It's still *technically* possible to hide those warnings on a per-module basis.
What good is it to know that a type doesn't have a certain instance?
I also had some trouble with Learn You A Haskell. I'll have to try swapping books 
It doesn't prevent `throwIO` of say `IOException`s, but it does prevent `throwIO` of `e` if `e` has an `Excepton` instance. This is a particular instance of a general problem and hence hiding `IO` and not importing `throwIO` don't solve the problem.
Thanks!
You could make something of the form `Dict (Exception e) -&gt; Void` (see https://hackage.haskell.org/package/constraints-0.10/docs/Data-Constraint.html#t:Dict and https://hackage.haskell.org/package/base-4.10.1.0/docs/Data-Void.html#t:Void). This not only ensures that the type has no `Exception` instance, but that it is impossible to define an `Exception` instance at all without causing a contradiction!
There is this package which collects all instances in scope and derives the complement from that https://github.com/mikeizbicki/ifcxt Moreover, one way to make an instance unusable is to make it overlap with another, e.g., by duplicating it (e.g., copy the module that defines it or maybe create a second path to it via a symlink).
Swagger, and in particular servant swagger, is pretty decent. It does run into some problems of orphan instances wrt generic deriving, so in our bigger projects we have a huge amount of newtype and hand written swagger schemas that may fail out of sync. But it is a very good way of communicating with your users, and for small projects it's probably perfect.
Yes, clearly the fully general monad comprehensions is much better than the anemic specialization to lists.
Agreed, then others could pick up the work if interested. Best to put all ideas out in a PIC state. 
Good to know that you find raaz interesting. Remember that it is still experimental. So there are rough edges but I have (tried to) anticipate some common problems in crypto libraries face and tried to incorporate the fixes in the type system as much as possible. There are two things that matter the most when you want to do bulk operations 1. Caching. 2. Alignment. To explain the 10 MB can you check what is your L1 and L2 cache sizes? Also there potentially some allocation overhead with bytestring as allocation happens in the heap (`malloc`). If all I want is to stream random bytes, it is sufficient to use a fixed size buffer that to on the stack (`alloca`). Which is what the raaz executable does. I would expect that to be a bit more faster but at the cost of using a much lower level code `fillRandomBytes` It looks to me that System.Random does not have an interface like `fillRandomBytes` so it is a bit unfair to compare with raaz. It is however a valid question why such an interface is not provided. 
Finding the learning material that suits you is key. At work, we have an FP learning group and we constantly see how some people prefer the hands-on approach on ‚ÄúLearn you a Haskell‚Äù while others rave about the theory-first method of ‚ÄúFrom First Principles‚Äù (myself included). I‚Äôm glad we have both book, though. Their styles appeal to different people, and at the end of the day we‚Äôre all learning!
The `e` is universally quantified because we want to enforce this property on users of library of helper functions for `EitherT` transformers. This library will then be used in a number of projects and the projects will each define their own error type. When coding in teams, it is always easier and more efficient to encourage correct practices by tools (in this case compiler errors) than it is by policy (coding guidlines). 
But before that, you could take a binary built on a different machine and just run it (we never got that to work)?
&gt; AMA What goes wrong? By which I mean, when a project doesn't work out, in your experience, what does that look like?
&gt; To explain the 10 MB, can you check what is your L1 and L2 cache sizes? It's 2.6 GHz Intel Core i7 (I7-6700HQ). L1 Cache: 32k/32k x4 L2/L3 Cache: 256k x4, 6 MB Each core has its own dedicated 256k level 2 cache and the system has 6 MB of shared level 3 cache. &gt; It is however a valid question why such an interface is not provided. /user/cartazio Hey, would you like to chime in? You [talked about version 1.2](https://www.reddit.com/r/haskell/comments/7ma9rd/in_your_professional_experience_how_suitable_is/drsrc7s/). I'm wondering if you're working on any speed improvements, such as those that can be attained by generating the values in chunks as opposed to one by one.
What were your mentors like were a student, and what do you wish to give your students that you didn't get yourself?
&gt; You can relax things a bit and give ‚Äúorphan instances‚Äù for a type in &gt; other modules, but in so doing, you lose typeclass coherence &gt; guarantees, and it‚Äôs now possible for different parts of your code to &gt; use different Foo instances, leading to subtle bugs. This isn't actually part of Haskell. GHC's implementation can allow this in certain circumstances, but the Haskell specification only permits one instance in a program.
&gt; You can relax things a bit and give ‚Äúorphan instances‚Äù for a type in other modules, but in so doing, you lose typeclass coherence guarantees, and it‚Äôs now possible for different parts of your code to use different Foo instances As far as I know this is impossible in Haskell.
You left out the `main` function in the first post.
&gt; This starts to fall apart anytime the type Foo is being embedded in another type. Coercion?
A new edition (blue cover) was published in 2016: https://www.amazon.com/Programming-Haskell-Graham-Hutton/dp/1316626229/. The original edition (yellow cover) from 2007 is now out-of-date. 
Damn it, I wish we had an FP learning group at work too :-/.
You clearly have enough of a mathematical background to compare with sets and predicates, so you've answered your own question haven't you? People who are not familiar with that mathematical notation will not have any intuition for that syntax.
&gt; You may think I‚Äôm biased but I‚Äôm really, honestly not. Of course you are biased, and you are entitled to be. Don't pretend that you are not while writing an opinionated piece. 
It's funny that you say LYAH is hands-on yet it doesn't have exercises.
Admittedly I am nitpicking, but my point is the opposite: Don't be sorry that you have an opinion, instead own it. Overall I liked what you wrote.
1) I'm not the author 2) I agree with your original point, no reason to convince me further. I see people do things like that all the time (disavow their own opinion for no other reason than it is an opinion). I was trying to point out how absurd the behavior is.
As an example: You can generate Datatypes, Aeson parsers , and Servant API descriptions from an OpenAPI spec (Which is routes + jsonschema) with swagger-codegen: https://github.com/swagger-api/swagger-codegen 
Maybe a tangential point, but it's always bothered me that one of the best patterns in Haskell‚Äîdumb data types, like V2 and V3 and so on in [linear](https://hackage.haskell.org/package/linear)‚Äîis kind of hampered by the fact that you are often forced to include a bunch of dependencies for instances (if you want to avoid orphans). [Data.These](https://hackage.haskell.org/package/these-0.7.4) is a good example of the kind of thing I'm talking about. I wish there was some way to include a dependency as "instance only", so it won't actually be imported if the person importing your library can't see the typeclass you're implementing. That way Data.These would have the same lack of orphans, but no laundry list of dependencies.
Now I get it. Dim witted and nitpicking - this is not my best day. please carry on :-)
I think this is more a reflection on reddit than you. Pretty often replies are only rebuttals.
I found them rather nice when learning, since I come from Python and used them there quite often
Do you have something to show us? ü§î
Sorry, I pasted wrong link for second element (copy-paste error). Fixed this just now.
In regards to using newtypes to add missing class instances: &gt; I also dislike this sort of workaround &gt; because it‚Äôs one of those non-obvious &gt; language-specific hacks [‚Ä¶] While I agree with the author regarding the problems with this solution, the solution relies on the inherent nature of how newtypes work, when did how a language construct works become a ‚Äúnon-obvious language-specific hack‚Äù?
This isn't by me, but /u/Icelandjack: https://gist.github.com/Icelandjack/0f94cbca3f5c5842f98ba2e7e9d07d49
It would be really cool if vinyl could use something like data Rec :: {-# MONOMORPHISE#-}(u -&gt; *) -&gt; {-# MONOMORPHISE#-}[u] -&gt; * where RNil :: Rec f '[] (:&amp;) :: {-#UNPACK#-}!(f r) -&gt; {-#UNPACK#-}!(Rec f rs) -&gt; Rec f (r ': rs) and basically end up with a flat tuple. Not sure how existential types or higher rank functions would work, though. 
I don't understand why orphans are such a big problem. If ghc globally checks for colliding instances (e.g: via propagating lists of instances in .hi files all the way up to the `main` module and checking at every compilation unit) - then the worst that can happen is a compile error when adding an instance. 
Note that the [concrete issue](https://github.com/haskell/hackage-security/pull/203) was fixed in the end. A patching policy is of course still a good idea in case the next such issue does not get fixed in a timely manner.
I don't have the code in front of me, and it wasn't my code, but... I've seen pattern synonyms which are type specializations of `unsafeCoerce` to go back and forth between heterogeneous vectors and unboxed vectors of `Word8`.
Do 4 but enshrine the sentiment that non-upstreamed patches are a source of shame. This is how it works in `nixpkgs`, and while it sounds a little hokey, I think it works quite well. It‚Äôs pragmatic in that you can fix the problem ASAP, but also mostly self-correcting because if you write a patch you are obligated to forever politely answer the question, ‚ÄúWhy haven‚Äôt you upstreamed this change?‚Äù Shedding this obligation by getting your patch merged or deleting it when upstream fixes the issue in another way is a healthy incentive.
Yeah, the idea behind Thunk is really neat. I think unpacking a value vs using a strict field only saves a single word (the header pointer) so it probably only really matters for small values. The bigger advantage of flattening Rec might be that the usual optimizations could kick in and pass the contents in registers, even across function boundaries. 
whoops. Added. Thanks.
They are completely subsumed by do notation. I find them redundant and annoying.
Woohoo! It's great to be back.
Perhaps Backpack could be a solution for customizing deeply nested structures. But you have to plan ahead, and it comes with its own overhead.
Unfortunately, [it is possible](http://blog.ezyang.com/2014/07/type-classes-confluence-coherence-global-uniqueness/), because detecting conflicting instances would force the compiler to always look at every file in your program, breaking incremental compilation.
weird its telling me this website is malware/spyware :\
This is basically how microgroove works. It's a flat array-backed vinyl-style record
Don‚Äôt both Rust and Swift solve this problem? Both have traits with associated types (like TypeFamilies); both allow instantiations in other modules; and both, as far as I know, have compiler checks to make sure exactly one instantiation is live in a given scope. 
I don't think it does that, but it seems like a simple solution? spj mentions scanning thousands of interface files to check for colliding instances. But it sounds like he may have missed the option to re-publish all imported instances in each .hi file, so you only have to look at the interface files you already loaded? What cases might it not work for? I might be missing something, but it doesn't seem problematic to me.
I'm pretty sure Rust will only let you implement a trait for a type if you control either the trait or the type.
if this is in the context of a CSPRNG... random &lt;= 1.1 definitely isn't suitable. I'm not sure if i can claim 1.2 will be either (at least for cryptographical randomness) i'm not sure what the reference code used for that comparison is... but digging around fillRandomBytesMT is the key primitive used, and https://github.com/raaz-crypto/raaz/blob/5a8d2ac86307b6cef5b0d9261d54e3162d59372f/Raaz/Random/ChaCha20PRG.hs seems to be where the core logic lives. Now, i could just be guessing, but if the benchmarks were using one of eg the quickcheck etc instances of a monad on top of system random, every BIND operation actually does a generator split on the RNG, which would be a source of some difference. theres also that STD Random is pretty bad wrt both quality and perf characteristics. I do think that if both RNGs are doing suitable unboxing by ghc and using a friendly sibling of the State Monad, there should be competitive or superior throughput by non CSPRNG codes ... I'm not doing anything explicility perf focused at the moment, but it is a side consideration of some choices. sometimes though its quality not speed that matters. ideally both, but thats not always easy :) 
That‚Äôs indeed a good approach, if you don‚Äôt need to be able to extract things from a `Focus` after it‚Äôs been constructed. 
I don't really understand the issue. Shouldn't the "owner" of a type also control that types instances? It seems like the instances really do belong with the type, as you are essentially writing functions specific to that data type. It would seem out of place to me to create an instance for a type/typeclass in another module.
I've thought about this approach as well. If the selector is also a phantom type, then this seems equivalent to giving the type class instances names. Perhaps every type class should include one of these phantom types by default? What are the potential downsides?
In practical terms, GHC *is* the Haskell language definition. IIRC Even the new Haskell language committee process was essentially just going to fold common/blessed GHC extensions into the new standard.
I have all 3 books (LYAH, HPFFP and PIH 2nd) as well as others. My most recent reading has been PIH 2nd. I find Prof. Hutton's explanations to be the clearest and stripped to the essentials. HPFFP is better for walking through the nuts and bolts, giving examples of the initially inscrutable errors one is likely to experience. LYAH is a fun read, but would benefit from exercises.
being biased is not the same thing as having an opinion.
Remote friendly?
For the tech lead - probably not as we have a team in Gainesville, FL to lead. However, we are remote friendly for a Sr Developer role for the right candidate. Phillip
Options 1 and 2 benefit the entire Haskell community. Options 3 and 4 benefit a subset of the community.
I am a student and I have already accepted a full time internship for the summer. But I won't be using Haskell there, and that makes me sad. I would like to submit a proposal for a Haskell gsoc project, but I am concerned that I will not have enough time over the summer to work on my project. Have there been successful Haskell gsoc or soh students in the past who were also juggling a full time internship?
What if package A created type Foo, and package B created typeclass Bar, and there is a natural instance of Bar for Foo, but package A and B don't rely on each other to work. Who creates `instance Bar Foo`? In my mind independent package AB should create it, with permission from A and/or B. 
Is the instance really necessary though? In my mind, either you are creating a new typeclass, in which case you add instances for datatypes you or others have created, or you are creating new data types, in which case you add instances for typeclasses you or others have created. I guess I have difficulty envisioning a scenario like you describe.
It should be pointed out that the team is already using Haskell for new development. Meaning this isn't one of those postings of "We're interested in Haskell, so come help us write our Java/Ruby/Python/etc code!". There really is Haskell in use, not just a dream of doing so. * Note that Haskell isn't the _only_ thing, just what the future is. It can't all be Haskell overnight :)
Option 1 is essentially "ignore the problem until it goes away". That doesn't benefit anybody, apart from reducing the workload on Stack maintainers. Options 2 and 4 are actually quite similar: make a fork and depend on it. 2 clutters Hackage with duplicate packages; 4 allows the Stack maintainers to implement the fix and make a proper PR, but use the patched version before the PR gets released / the fixed version is on hackage.
You could be right... maybe you could tease it out more. This seems like one of those things where there are weird corner cases lurking that will cause it to not work in general. I could be totally wrong about that though. 
It can indeed, try searching for `makeClassy`. The lenses are generated via TH instead of generics.
I'm not /u/ocharles, but I can share a couple thoughts on this question. I've seen a few things go wrong. 1. The student isn't committed to the project. Perhaps some other priority comes up in their life, or they misunderstood the time commitment. Whatever the reason, the student disappears for a week at a time or feels secretive or evasive, keeps their work on a local computer without committing changes, doesn't provide status updates or ask questions, and/or clearly tries to just cram work into a week just before each deadline. 2. The student doesn't make progress because the project was too ambitious. Perhaps the student volunteered to massively refactor or improve a code base they don't understand. (It's far safer to propose working on a concrete feature rather than "improve performance" or "clean up", because the latter often involves a level of big-picture understanding or dealing with technical challenges that aren't even known during the proposal. ) It doesn't help if the mentor is very busy or not an expert themselves. 3. The student creates something, but it doesn't meet the needs of the community for some reason. Maybe the API feels inelegant, or the performance is particularly bad, or the result is missing critical features. But we also can just make mistakes in selecting projects, and choose a project that, in retrospect, wasn't a very important idea in the first place. There are probably other failure patterns, but that's what comes to mind.
Yes, everything worked. You only need to be careful with LD_LIBRARY_PATH which should be free of paths like /usr/lib64. The only complication we had was with sssd since libnss_sss.so is not included in glibc package in nix. We ended up just creating an empty directory with a symlink to the shared library in RHEL and adding that directory to LD_LIBRARY_PATH.
This is nice! When you explain the GADT, there are some `Dict TypeLevelList`, which don't kind-check. For the rest, your explanations are good.
True - I meant that in the sense of starting from concrete, practical things you‚Äôll need when writing a program (ie. lists) and sprinkling theory here and there to explain why things work a certain way as opposed to starting with a solid theory base and then going into a more practical venture into writing programs.
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/GoFHmri.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) 
Yes, I'll try to fix this later.
I think I just applied for that position earlier today... Neat :) 
I'll try to find time for this on this weekend.
For the reference https://imgur.com/a/G5ePQ There are - `lens`y branch (without `lens` but other stuff like `pointed`, `lens` don't depend on `pointed`) - it's there because of `vector-instances` to define `AlignWithKey` for `Vector` - alternative is to depend on `lens` and have `AlignWithIndex` - `aeson` branch - `QuickCheck` What's the problem - Build times? With `stack` or `cabal new-build` caching your deps it shouldn't be common - unmaintainability risk? All packages are either rock solid or have good and responsive maintainers, for most packages it's both. - something else? I will be happy to address your issues. IMHO splitting packages only makes it worse. Too many people don't know about http://hackage.haskell.org/package/quickcheck-instances still. QuickCheck has 1800 revdeps, quickcheck-instances only 180 - I'm sure many instances are re-implemented across test-suites.
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/WwyoctH.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20du9mn8r) 
What would the advantage be? Would you be uploading the project-independent one to hackage or some github repo? If not it seems like just putting the more project-independent stuff near the top of the prelude is sufficient, as you can just cut and paste that part when making a new projec.t
I'm not a huge fan of the default strictness because it mucks up functor laws and the like and adds extra indirections when you want laziness. I think default laziness makes more sense. It might also make sense to reimagine what a record contains. Imagine data Box a = Box {unBox :: a} data Rec f as where Nil :: Rec f '[] (:&lt;) :: {-# UNPACK #-} !(Box (f a)) -&gt; !(Rec f as) -&gt; Rec f (a ': as) In memory, this is exactly the same as a lazy record (no extra indirection), but the fundamental operations now naturally deal in `Box`es instead of values. For example, rmapB :: (forall x. Box (f x) -&gt; Box (g x)) -&gt; Rec f as -&gt; Rec g as rmapB _ Nil = Nil rmapB f (x :&lt; xs) = f x :&lt; rmapB f xs We can use the boxy version to define both lazy and strict versions: rmap, rmap' :: (forall x. f x -&gt; g x) -&gt; Rec f as -&gt; Rec g as rmap f = rmapB (\(Box x) -&gt; Box (f x)) rmap' f = rmapB (\(Box x) -&gt; Box $! f x)
Rust ensures global coherence of trait implementations. They do this by limiting implementations in the following way: - you can only implement third-party traits for data structures defined in the current crate (ie. compilation unit/package) - you can only implement traits on third party types if the trait is defined in the current crate Tbh I would prefer a modular implicits approach, but Rust had been taking on enough innovation points at the time. And OCaml's modular implicits still haven't had any testing in the wild, so it remains to be seen how they hold up in the messy real world.
This is exactly why I wrote [superrecord](https://www.athiemann.net/2017/07/02/superrecord.html). It uses `SmallArray#` under the hood.
I think you mean it is *impossible*. If it were possible, then that wouldn't be very unfortunate üòâ
&gt; giving the type class instances names This is a language extension, I did not think that far. Though yes, basically, it's the way to emulate such feature.
I wonder if there is a backpack-based solution in there somewhere. Like, GHC could complain No instance for (C T). There's an instance in Data.C, but it's parameterized by &lt;some backpack thing&gt;. If you wish for that instance to be visible, include the right instantiation in your cabal.project (or wherever that would have to go) Of course, GHC could just do The Right Thing for us automatically, but then we lose separate compilation. Also this interacts pretty weird with the dependency solver, because we can't know in beforehand which packages we actually depend on.
Yes of course. Something like Mersenne could really be fast.
Wow, this `microgroove` looks promising. Wonder when the docs will appear
I would surprised if it exists like this. You should be able to use `toConstr` from `Data.Data` to come up with something. But it requires Data instances for f, a and b. There is also [dataToTag#](https://hackage.haskell.org/package/ghc-prim-0.5.1.1/docs/GHC-Prim.html#v:dataToTag-35-) which does not require any instances, but requires you to deal with primitives.
To be clear, I think the approach taken by these is 100% correct, sorry if it came across like I was criticizing the package. Orphans are worth avoiding, and many of the instances are non-trivial to write, so I‚Äôm thankful that they‚Äôre there. The issue for me is build times, so I really just wish (somehow) all the benefits could be provided without the downsides.
This is an interesting idea. What would you do with it?
I don't know if it exists, but it seem something possible to generate with `Generics`.
Salary?
What's exactly the problem with strict fields and the functor laws? Is it only: fmap (const x) . fmap (error) /= fmap (const x . error) and non-terminating arguments?
There's something in [`unification-fd`](https://hackage.haskell.org/package/unification-fd-0.10.0.1/docs/Control-Unification.html#v:zipMatch).
Does it look better as an inequation? (fmap . fmap) fst (zippy a b) ‚â§ Just a where the inequality is the smallest order relation such that for all x, Nothing ‚â§ x - It just hides the conditional in the definition of ‚â§ - Inequational reasoning is possible with monotonic or continuous functions - By tweaking the definition of ‚â§ we can keep the same inequation for lists, so `zip` can be seen as a special case of `zippy` in a finer semilattice structure.
This seems like fundamentally the same thing as what I'm considering.
I think the difficulty is you're conflating some sort of product which is defined only when the cases match with deciding whether those cases match in the first place.
It's called `zipF` in [fixplate](http://hackage.haskell.org/package/fixplate-0.1.7/docs/Data-Generics-Fixplate-Open.html), and the type signature is zipF :: (Traversable f, EqF f) =&gt; f a -&gt; f b -&gt; Maybe (f (a, b)) where `EqF` is a kind of "lifted equality" type class
This is to avoid boolean blindness and partial functions, which are probably worse than the problems caused by the conflation.
https://github.com/daig/microgroove/blob/12e58c6dbf6dd399b55fa098d38ef4169a7c34e3/src/Data/Microgroove/Rec.hs#L83 Type refining pattern synonyms a la EZYang
Definitely, I'm not saying this is the wrong way to approach it in Haskell or anything. But with respect to wanting to really understand the abstract structure, I think it makes sense to have a type index, then define the product for each index. [My thoughts in Agda](https://imgur.com/a/Vgiqv). But practically speaking, I'd go with something like what you have now or /u/fridofrido linked.
You may not personally care about the repeatability for your own system but as soon as you distribute your software it becomes relevant. The disadvantage is redundant copies of libc et al. but at least that's not frivolous, you only keep around the copies you want and on NixOS the redundancy disappears. Is a couple hundreds of MBs really worth the headache that comes with not controlling these dependencies?
And yet we use stack ; ) Not the same exactly but quite a similar situation.
What happens if you wrap `action` with a `lift`?
You should have an associative law.
Well you can switch between the unindexed and indexed forms, but ya I get what you're saying.
The difference is that you can still easily avoid being vendor locked into FPComplete's Stack tool for Haskell development by using Nix or Cabal or both. As long as tooling such as GHC or IDEs doesn't start requiring or depending on Stack we're fine, I think.
It's not obvious to me how but if it does then that's great!
That sort of thing. It doesn't even have to be a constant function; something like `Just` would do as well.
 I like to define type-classes to encapsulate these things &gt; {-# language TypeApplications #-} &gt; import Control.Applicative &gt; &gt; class Pairable f where &gt; pair :: Alternative g =&gt; f a -&gt; f a -&gt; g (f (a , a)) &gt; &gt; instance Pairable [] where &gt; pair [] [] = pure [] &gt; pair (x:xs) (y:ys) = ((x,y) :) &lt;$&gt; pair xs ys &gt; pair _ _ = empty &gt; &gt; main = mapM_ (print . uncurry (pair @[] @Maybe)) &gt; [ ([1,2] , [1,2]) &gt; , ([1,2,3,4] , [1,2,3]) &gt; ]
Your best bet is to look at the core produced by the various forms of your program.
Fixed it. Thanks.
Thanks!
Thanks for the corrections! I'll fix them as soon as I have time. The `TypeInType`extension was requested by GHC when I tried using the `[Type]` kind, but I could check this again to see if it was a suggestion or an actual request.
I'll check it! Thanks!
It's pretty close. It changes fmap f . fmap g = fmap (f . g) to fmap f . fmap g &lt;= fmap (f . g) The identity law stays the same, so you can't do anything too terribly weird. But inequalities are somewhat harder to reason about and code around than equalities.
Here is a quick demo of the tags method. Check it out: https://gist.github.com/calebh/d474ab9ef80ce907665c8bf3634a2af6
My gut feeling is that the problem is with `replicateM_`. You could try the [`streaming`](https://hackage.haskell.org/package/streaming) package (uncompiled): import Control.Monad import Control.Monad.Trans.State.Strict import Streaming.Prelude as S main :: IO () main = do print $ flip evalState 17 $ flip runStateT 'a' $ S.effects action action :: Stream (Of ()) (StateT Char (State Int)) () action = S.replicateM 100000000 (return ()) To quote from the README: &gt; If you want to tempt fate and replicate the irrationality of Control.Monad.replicateM, then sure, you can define the hermaphroditic chimera &gt; accumulate . Streaming.Prelude.replicateM :: Int -&gt; m a -&gt; m (Stream (Of a) Identity ()) &gt; which is what we find in our diseased base libraries. But once you know how to operate with a stream directly you will see less and less point in what is called extracting the (structured) value from IO.
When is it a good time in your career to learn Haskell? There are so many things to learn, don't know how to prioritize when to learn my first functional language. First job ~year
I just tested this with ghc 7.10.3. When compiled without optimization the program completes in 9 seconds, with optimizations in 2.5 seconds. So, there might be a regression in ghc 8.2.2.
Hopefully it's better than the Glassdoor site indicates.
I learned Haskell after releasing my first 'product' at my first job. That is, two or so years into my first job (my early twenties) I was gearing up to make a rewrite of the product with a new design and features. For various reasons management was OK with whatever I selected and Haskell fit the bill.
You've already defined a typeclass `Endofunctor` to make sure you can run `fmap` inside your category; similarly, you could define another typeclass making sure you can interchange `Fix f` and `f (Fix f)`.
I did something like that here: https://gist.github.com/sjoerdvisscher/11274093 Note that in some categories you also need something else than `Fix f`, f. e. in Nat. 
You may want to try this: https://github.com/ndmitchell/spaceleak or http://blog.ezyang.com/2011/06/pinpointing-space-leaks-in-big-programs/
Yes, but is there a way to define a fixed point for endofunctors in all categories `c` ? Because if I have to create Fix instance manually it would make it useless. 
you might want to have a look at https://haskell-miso.org/
You can use GHCJS (and then `servant-client-ghcjs` to automatically have functions to query yours API). Or Purescript (and then `servant-purescript`). Or Elm (and then `servant-elm`). 
I hope you're just trolling. `nix-env -i` purposely adds *extra* indirection to intentionally add this non-determinism as a convenience for those who want it. It's optional, and your other comments imply you know enough about Nix to know this. The corresponding docker functionality however isn't an extra feature atop a less non-deterministic feature but unavoidable.
Yeah, but what I'm looking for is a way to avoid Node and JavaScript altogether like I can with Eliom. There's even a widget toolkit and it works on desktop and mobile. Because I have concerns about Ocsigen/Eliom, I'm looking for the same level of comfort around Servant. http://ocsigen.org/tuto/6.1/manual/tutoreact http://ocsigen.org/tuto/6.1/manual/application http://ocsigen.org/tuto/6.1/manual/tutowidgets http://ocsigen.org/tuto/manual/start
Full disclosure, I've also looked into Ur/Web.
&gt; Because I have concerns about Ocsigen/Eliom Which one ? :)
Concerns: 1. AFAIU, it's still a custom OCaml compiler and dialect, rather than using something like ppx, which I admit might be a bad fit. Haven't looked deeper if this means I'd run into issues with delays when a new OCaml is released or if there's incompatibility when consuming regular OCaml packages in an Ocsigen project. 2. Lwt and how it compares to GHC's I/O runtime. Likes: 1. It has REST 2. You can write, and this is the defaul, write frontend in OCaml (Eliom dialect, to be specific) 3. Widgets already available 4. Doesn't make me install and deal with Node or npm at all. Huge plus.
My gripes with Ur/Web are the more serious bus factor and that MLton's compilation model is a bad fit for iterative web development.
I'm not saying you have to define a different representation for `Fix`, nor that you have to define a different implementation for `cata` got each Category. You should be able to use the same `Fix f` _object_ in every Category, but you can't use the `Fix :: f (Fix f) -&gt; Fix f` and `unFix :: Fix f -&gt; f (Fix f)` functions generated by the compiler in every Category, because those are only morphisms in the `(-&gt;)` Category. If you're willing to restrict your attention to the subset on Categories which are also Arrows (which is enough for Kliesli Categories), you can simply use `arr` to convert functions into morphisms.
&gt; MLton's compilation model is a bad fit for iterative web development. Yes, that's exactly what I mean by "no separate compilation". :)
1. An ocaml library will never be "incompatible" with Eliom. The current version of the eliom compiler calls the OCaml one. You can link OCaml library on the client or on the server as you want. As far as lagging behind releases: Ocasionally, but no more than other OCaml libraries. 2. I fail to see how Lwt is a concern, it's fairly good at its job, but I don't really know what's good about GHC's I/O runtime. Eliom already uses the ppx syntax, but that's not enough to give custom typing rules, so it does a bit more. :)
1. I guess I was misled by the blog post surrounding your PhD and the new Ocsigen compiler. 2. Just a point to consider, given that GHC is more like Erlang or Go's runtime. So, I guess if I use Lwt_preemptive and don't try to create too many threads, I might get into comparable concurrency territory in terms of scalability. I appreciate you taking the time to respond here. This might be an off-topic discussion inside r/haskell, but ultimately it can provide info for developers/users of existing Haskell frameworks.
Ocsigen as a whole: lot's of other people contributing. Lwt is used by a lot of people in the community. Js_of_ocaml's main maintaner is at Janestreet nowadays, etc. Eliom and ocsigenserver, on the other hand, are mainly besport affairs.
Can I run an Eliom app on Mirage? If not, how much/hard work do you suspect there is?
1. All I care about at this point is that if I decide to use Eliom, I won't have to deal with issues of a compiler that's easy to stop working for a period. Looking forward to the new compiler, which sounds like it will make stuff even easier and improve the language. 2. In abstract terms, regardless of implementation details, it's the concern of how many sessions I can run in parallel while making as efficient use of available processing threads as possible. How did you notice my question over in r/haskell, if I may ask?
Ah, that is a very good question. I tried to work on that a bit at some point. See [this](https://github.com/ocsigen/ocsigenserver/issues/54) and [that](https://github.com/ocsigen/ocsigenserver/pull/124).
If I can run it on Mirage, the question about Lwt will be moot since the deployment model would be spawning a Mirage instance per incoming request, on demand. No thread pools or security considerations of long running processes (leaks, side channels) to deal with.
I tried running the example code it's linked to, and it works fine without TypeInType. At least it does on my machine Unless my understanding (which, to be clear, is not very good) is substantially off, nothing here really has anything to do with TypeInType.
I have an addiction to type systems, I read both r/haskell and r/ocaml (and r/rust, r/types, ... :)
More like incompatible stacks, which should be converging soon.
Using mirage doesn't forces you to do that (and combining ocsigen+jitsu is yet another non-trivial thing).
No SML? I tried installing CakeML last week, and I failed. Just saying MLton's whole programing compilation model and PolyML's few optimizations aren't the only issues in that camp. They have a nice GC in SML#, but that project is easy to crash, though very cool. The built-in features show that it was created for Japan's HPC use.
Excited to hear that. If I could use Ocsigen on Mirage, I kinda would be sold :).
I know, but there are no threads in Mirage yet, are there? And what's the jitsu complexity?
The sooner the better in my opinion. If you come to Haskell from an imperative, OOP-style language then you're going to have to give up everything you've learned and pretend you're starting from scratch again. That's a hard sell if you're ten years into your career and think you know a thing or two about programming. If you take on Haskell earlier then there's much less risk and you will have fewer opinions.
I'm not trolling. I'm exhausted with writing bespoke nix-shells and default.nix's only to find that I did not correctly *for that specific dev environment* fully lock down dependencies, because `nix-env` doesn't really have a consistent policy for name bindings and that leaks into my nix code. It just bit me again, spot the problem with this nix-shell (which I created for a nodejs environment by mutating a very popular python dev environment strategy): ``` { pkgs ? import &lt;nixpkgs&gt; {} }: with pkgs; stdenv.mkDerivation { name = "ts-env"; buildInputs = [ nodejs ]; shellHook = '' npm install export PATH=$PATH:$(pwd)/node_modules/typescript/bin export PATH=$PATH:$(pwd)/node_modules/tslint/bin export PATH=$PATH:$(pwd)/node_modules/tern/bin ''; } ``` (and yes, I do have a default nix with node2nix run, but I can't here because node2nix doesn't support private repositories correctly in an edge case) This shell behaves inconsistently in the extreme, not because of a lack of specificity on dependencies, but because there is no consistent scheme for versioning packages in nix and so literally everything has to be treated differently. The only way to infer the version scheme is to poke around with nix-env queries and often to read the source code of the packages, which can be so big that you can't do it on github and need to maintain a local checkout. I maintain this is just a miserable user experience and that I'm using the tool nix intended me to use for development environments. And I'm tired of people telling me I just haven't used it enough, that I'm not reading the docs enough, or that I'm trolling. I'm not. 
yeah, perhaps a buffer writer opt in sub api would be a good way to go for improving random! thats a good idea
This looks great! I see from the haddocks that it even works with lenses as well :) Does that mean that if one used this internally in a library that any users of that library could use the library as if the data types were native ones or would superrecord details leak outside? (Not sure I worded that question super clearly...)
The strict state monad is so called because the state is strict, meaning that when you `put` a value, it is forced. You don't ever `put` anything into the state so that seems somewhat irrelevant. Compiling the program with `-O2` on 8.2.2 and it exits in a short amount of time. How exactly do you reproduce this? I think making a ticket on the [issue tracker](https://ghc.haskell.org/trac/ghc/) will get you a better explanation if there is a bug.
&gt; Haste.App FWIW, Haste seems like a research project that looks dead.
There are , of course, several ways you can do this. My preference in any scenario in which you are doing anything with two lists, is to use `zipWith`, and `zip` In this case: compareLists :: Eq a =&gt; [Int] -&gt; [a] -&gt; [a] -&gt; Bool compareLists indices left right = and . map snd . filter ((`elem` indices) . fst ) . zip [0..] $ zipWith (==) left right In order: The `left` and `right` lists are combined using (==), building a list of `Bool` results. The list of bools is zipped with an infinite series of ascending numbers beginning with 0, so, each bool gets it's index. That list is filtered to return only indexes in the input list of indexes. The filtered list of indexes and bools is then `map`ed to return only the bools The `and` function is used on the list of bools to see if all are true, if all Bools are True, that means those indexes were equal, if not, they were not. This implementation experiences unusual behavior in the following edge cases: 1) If indexes larger than either of the two lists are used, they are ignored 2) if either of the lists are empty, it will always return true. So you'd probably want some additional code to check for those cases and error, as realistically those are cases where a reasonable answer to the question isn't actually possible anyway.
&gt; The strict state monad is so called because the state is strict, meaning that when you put a value, it is forced. It would make a lot of sense if that was the case, but as the following example demonstrates, this is not what the "strict" in `Control.Monad.Trans.State.Strict` means: import Control.Monad.Trans.State.Strict -- | -- &gt;&gt;&gt; runStateT temporarilyUndefinedAction 0 -- Identity ((),1) temporarilyUndefinedAction :: State Int () temporarilyUndefinedAction = do put undefined put 1 
The real difference between the strict and the lazy state monad is the pattern match on the tuple in `s -&gt; (a, s)`. For the lazy state monad a lazy pattern match is used while the strict monad uses a regular pattern match. Comparing the monad instances makes this difference quite clear: https://hackage.haskell.org/package/transformers-0.5.5.0/docs/src/Control-Monad-Trans-State-Lazy.html#line-218 and https://hackage.haskell.org/package/transformers-0.5.5.0/docs/src/Control-Monad-Trans-State-Strict.html#line-215
[Apparently there's no `join` function, so you have to use an `MVar`.](https://hackage.haskell.org/package/base-4.10.1.0/docs/Control-Concurrent.html#g:12)
I wonder if the local instances created by `reflection` could have overlap and shadow the global instances, e.g. if I'm doing quickcheck tests and want to override the instance for `Double`.
Took me a couple minutes to fully understand what the bang pattern does. So essentially the lazy variant builds the ast on the heap in form of thunks and prunes the branches that aren't necessary to compute the result? I have a hard time coming up with a situation where this behavior justifies the memory usage. do undefined put () lazy = \s -&gt; let (s', a) = undefined s in ((), ()) strict = \s -&gt; let !(s', a) = undefined s in ((), ())
Whew, so I *did* get lost a bit in my various cases: It does exit in a short amount of time; however, it uses about 1G of total memory while doing so. Limiting the memory with -M1G will make it crash early. Alternatively, you can watch the effect in a more dramatic way by increasing the number of iterations by a suitable factor.
That's *very* interesting. If it's not too much of a hassle, would you be so kind to check how much memory those runs are using, respectively? If the space usage with ghc 7.10.3 is ok then I will open a ticket.
It looks like this has been ‚Äúfixed‚Äù in 8.4. I can‚Äôt reproduce it with the 8.4 alpha3 build while I can reproduce it with 8.2.2
Thanks a lot for you gut feeling: Indeed your example (after fixing import clashes) runs in constant time, and while it's not blazingly fast, at least it terminates. This suggests that the problem is really with the current ghc and/or base.
Thanks a lot! I will add that info the the ticket when I get around to submitting it.
So, as for the real project, I've decided to get rid of the problem by switching to `IORefs`. Yes, you heard correctly: `IORefs`. To be honest, I now really enjoy their clean interface and the guarantees that they are giving!
Indeed I could fix at least my example by replacing `replicateM` by a simple myReplicateM_ k action | k &lt; 0 = return () | otherwise = action &gt;&gt; myReplicateM_ (k-1) action 
Indeed I could fix at least my example by replacing `replicateM_` by a simple myReplicateM_ k action | k &lt; 0 = return () | otherwise = action &gt;&gt; myReplicateM_ (k-1) action 
I hope this is a correct summary of your results. * strict, 2 layers, -O0: crashes * strict, 2 layers, -O2: crashes * lazy, 2 layers, -O0: spends all time in garbage collection * lazy, 2 layers, -O2, correct behaviour, slow * strict, 1 layer, -O0, crashes * strict, 1 layer, -O2, correct behaviour, fast * lazy, 1 layer, -O0, spends all time in garbage collection * lazy, 1 layer, -O2, correct behaviour, slow
&gt; My gut feeling is that the problem is with `replicateM_`. Are you sure? `streaming` is warning against `replicateM` *not* `replicateM_`. The latter is fine, as far as I can see. I believe the problem is that `replicateM_` changed to use `*&gt;` in [`base-4.9.0.0`](http://hackage.haskell.org/package/base-4.9.0.0/docs/src/Control.Monad.html#replicateM) ([GHC 8.0.1, May 2016](https://wiki.haskell.org/Base_package)) but `transformers` did not specialise `*&gt;` to `&gt;&gt;` until [0.5.2.0](https://hackage.haskell.org/package/transformers-0.5.3.0/docs/src/Control-Monad-Trans-State-Strict.html#line-198) ([Feb 2017](https://hackage.haskell.org/package/transformers-0.5.3.0)), so we were living with a space leaking `State` Monad for nearly a year. 
I guess you are using `transformers &lt; 0.5.3.0`. If you're not please let me know what version of `transformers` you are using because I would be very interested to investigate!
This is exactly what I have been waiting for. I missed so much the convenience of OOP auto completion, where you type `vector.` + `tab` and you get all methods, that can be applied to it.
It's a modest step in that direction. For the sake of discussion I can make a little ladder: * Prefix-based name completion: We already have this in most editors. * Type-directed name completion: This is what I've implemented. It can just tell you "use this name or that name in this hole". * Combined name completion: This is also what I've implemented. This gives correctly combined names in an expression that you've made. * Expression completion: This is what Djinn (or something like it) could provide, and I'm excited about taking that direction. It can build intermediate steps to get from the domain to the codomain, using function composition, function application and case analysis. This gets to the "write my program for me" level. In parallel, case-splitting has been implemented in ghc-mod, although I haven't used it myself. I could certainly merge that code in if possible at some point. Completing my cases is something I've always wanted to be automated. Happily, I managed to implement this as a standalone module that just uses the GHC API. So ghc-mod and HIE could re-use it when I'm done.
Indeed I was on transformers-0.5.2.0, which is the one that ships with the current Stackage LTS, and updating transformers fixes the problem! This seems to have been the bug: https://hub.darcs.net/ross/transformers/issue/33 Alright, then my newest interpretation is for this to be purely an issue with transformers that has been fixed since (correct me if I'm wrong). I'm leaning then to complain at Stackage for leaving a space-leaking release in.
Alright, had a look at the dumps and they aren't too complex. In Core (basically desugared haskell) the main loop becomes this: Rec { -- RHS size: {terms: 27, types: 29, coercions: 0, joins: 0/0} $wloop $wloop = \ ww_s34d w_s349 w1_s34a -&gt; case tagToEnum# (&lt;=# ww_s34d 0#) of { False -&gt; case $wloop (-# ww_s34d 1#) w_s349 w1_s34a of ww1_s34h { (# ww2_s34L, ww3_s34M #) -&gt; case ww2_s34L of { (x_a2J9, s''_a2Ja) -&gt; ww1_s34h } }; True -&gt; (# ((), w_s349), w1_s34a #) } end Rec } w_s349 and w1_s34a are the states 'a' and 17 respectively. ww_s34d is the loop variable. Two things are notable here: - `(# ((), w_s349), w1_s34a #)` the outer tuple is unpacked so it will be passed in registers, the inner one is a normal heap allocated tuple because the cpr optimization doesn't work for nested types - $wloop isn't tail recursive! The second one is at fault for the leak. Roughly we are doing loop i state1 state2 | i &lt;= 0 = (((), state1), state2 ) | otherwise = case loop (-1) state1 state2 of result@(l, r) -&gt; l `seq` result Might as well dive deeper to check that our idea is correct. Optimized CMM: c3eE: /* get our function arguments */ _s3dY::P64 = R4; _s3dX::P64 = R3; _s3dW::I64 = R2; /* Check that we have enough stack space, gc otherwise */ if ((Sp + -24) &lt; SpLim) (likely: False) goto c3eF; else goto c3eG; c3eG: /* Check that we have enough heap space in case we return, gc otherwise */ Hp = Hp + 24; if (Hp &gt; I64[BaseReg + 856]) (likely: False) goto c3eI; else goto c3eH; c3eI: I64[BaseReg + 904] = 24; goto c3eF; c3eF: /* call gc with our function as return location */ R4 = _s3dY::P64; R3 = _s3dX::P64; R2 = _s3dW::I64; R1 = $wloop_closure; call (I64[BaseReg - 8])(R4, R3, R2, R1) args: 8, res: 0, upd: 8; c3eH: /* if i &lt;= 0 */ if (%MO_S_Le_W64(_s3dW::I64, 0)) goto c3f2; else goto c3eT; c3f2: /* we allocate ((), R2) on the heap and return that address and R3 in registers */ I64[Hp - 16] = (,)_con_info; P64[Hp - 8] = ()_closure+1; P64[Hp] = _s3dX::P64; R2 = _s3dY::P64; R1 = Hp - 15; call (P64[Sp])(R2, R1) args: 8, res: 0, upd: 8; c3eT: /* put the return address on the stack and make the recursive call */ Hp = Hp - 24; I64[Sp - 8] = block_c3eP_info; R4 = _s3dY::P64; R3 = _s3dX::P64; R2 = _s3dW::I64 - 1; Sp = Sp - 8; call $wloop_info(R4, R3, R2) returns to c3eP, args: 8, res: 8, upd: 8; c3eP: /* If the returned tuple is evaluated it will be tagged, i.e. `R1&amp;7 != 0`. Spoiler: It is always evaluated */ I64[Sp - 16] = block_c3eR_info; _s3e3::P64 = R1; // nop P64[Sp - 8] = R2; P64[Sp] = _s3e3::P64; Sp = Sp - 16; if (R1 &amp; 7 != 0) goto c3eR; else goto c3eV; c3eV: /* Returned tuple is a thunk, enter it */ call (I64[R1])(R1) returns to c3eR, args: 8, res: 8, upd: 8; c3eR: /* return */ R2 = P64[Sp + 8]; R1 = P64[Sp + 16]; Sp = Sp + 24; call (P64[Sp])(R2, R1) args: 8, res: 0, upd: 8; TLDR: We are way too strict and force the inner tuple when returning from each recursive call even though it's never a thunk. This is mostly because the `Constructed Prodcut Result` optimization breaks for nested data constructors. 
Indeed! You can write e.g. foo :: Integer foo = Just 1 &amp; _ or foo = _ (Just 1) And `:fill` produces `fromJust`. 
Correct. I should specify that "crashes" means $ ./weird +RTS -M1G weird: Heap exhausted; weird: Current maximum heap size is 1073741824 bytes (1024 MB). weird: Use `+RTS -M&lt;size&gt;' to increase it.
&gt; Alright, then my newest interpretation is for this to be purely an issue with transformers that has been fixed since (correct me if I'm wrong). I'm leaning then to complain at Stackage for leaving a space-leaking release in. I think you are correct. This should be reported to Stackage.
https://pastebin.com/Xh7qyyLN
Thanks! The 2.5 seconds are a bit odd, unless you increased the number of iterations or your machine was loaded at the time. https://pastebin.com/NaxM23fE
The salary is negotiable and competitive based on experience and the individual. :-)
I wouldn't put much stock in Glassdoor salaries with so little data, such as with ITProTV right now.
I don't really understand. 1. If that is the case, how is it possible to upgrade `transformers` *at all*? (On or off Stackage) 2. Can't GHC release a new version compiled against fixed `transformers`? 
It‚Äôs probably not. The guy messaged me privately saying it‚Äôs commensurate with experience and whatever else. Employers on this sub are so cagey and full of shit
If you depend on the `ghc` package you can‚Äôt upgrade `transformers` afaik but most packages don‚Äôt. As you mentioned Stackage only allows one version of a package so they have to choose the one that also works if you depend on `ghc`. GHC could ofc make a new release but afaik no `8.2.3` release is planned at this time. `8.4` will ship with `transformers-0.5.4` (or at least alpha 3 does).
I tried with the [nested CPR](https://phabricator.haskell.org/D4244) patch and it does a good job on this example. 
Cool what level are you at and how much do they pay you?
If they won‚Äôt give any data out we gotta go with what we see- which is Glassdoor. A talented functional programmer is not gonna relocate to Gainesville for 75k so it‚Äôs a wash anyways. 
This is much more constructive than cursing about it. I agree with the sentiment though. Disclosure: Affiliated with, but not employed by, the company in question. So I cannot 'officially' represent them, but I can tell you what working with the current development team is like.
I'm working with the team as a consultant. So I can't comment on pay, sorry. But it is a great team to work with. It is obviously no secret that we're fans of Haskell, but there are also Elm codebases. And I can talk more about tooling, environment, etc.
Awww man bad language on the internet! Not constructive! Ok so how much do you think they‚Äôre paying? And why is this treated as such hidden secret information? When places get cagey about this stuff it never means they pay good salaries.
You can absolutely comment on pay! You know more than we do
Interesting. Seems to be a weakness of Stackage.
Tangentially, it seems possible to define a `State` monad parametrically over strictness {-# LANGUAGE ScopedTypeVariables #-} module State where import Data.Functor.Identity import Data.Proxy newtype StateT strictness s m a = StateT { runStateT :: s -&gt; m (a, s) } type State strictness s = StateT strictness s Identity data Strict data Lazy class Strictness strictness where sseq :: Proxy strictness -&gt; a -&gt; b -&gt; b instance Strictness Strict where sseq = const seq instance Strictness Lazy where sseq = const (flip const) instance (Strictness strictness, Functor m) =&gt; Functor (StateT strictness s m) where fmap f m = StateT $ s -&gt; fmap (b@(~(a, s')) -&gt; sseq (Proxy :: Proxy strictness) b (f a, s')) $ runStateT m s instance (Strictness strictness, Monad m) =&gt; Applicative (StateT strictness s m) where pure a = StateT (s -&gt; pure (a, s)) StateT mf &lt;*&gt; StateT mx = StateT $ s -&gt; do b@(~(f, s')) &lt;- mf s b'@(~(x, s'')) &lt;- mx s' sseq (Proxy :: Proxy strictness) b (sseq (Proxy :: Proxy strictness) b' (pure (f x, s''))) instance (Strictness strictness, Monad m) =&gt; Monad (StateT strictness s m) where m &gt;&gt;= k = StateT $ s -&gt; do b@(~(a, s')) &lt;- runStateT m s sseq (Proxy :: Proxy strictness) b (runStateT (k a) s') runState :: State strictness s a -&gt; s -&gt; (a, s) runState m = runIdentity . runStateT m foo :: Strictness strictness =&gt; Proxy strictness -&gt; State strictness () [Int] foo _ = traverse pure [1..] mainStrict = do let (a, _) = runState (foo (Proxy :: Proxy Strict)) () print (take 10000 a) mainLazy = do let (a, _) = runState (foo (Proxy :: Proxy Lazy)) () print (take 10000 a) mainStrictWithState = do let (a, s) = runState (foo (Proxy :: Proxy Strict)) () print (take 10000 a) print s mainLazyWithState = do let (a, s) = runState (foo (Proxy :: Proxy Lazy)) () print (take 10000 a) print s Strictness consumes too much stack when interacting with lazy lists. Laziness consumes too much heap if you actually need to do something with the state. $ for STRICTNESS in Strict Lazy StrictWithState LazyWithState; do echo $STRICTNESS:; ghc -rtsopts -main-is State.main$STRICTNESS State.hs &gt; /dev/null &amp;&amp; ./State +RTS -M1m -K1k &gt; /dev/null; echo; done Strict: Stack space overflow: current size 33624 bytes. Use `+RTS -Ksize -RTS' to increase it. Lazy: StrictWithState: Stack space overflow: current size 33624 bytes. Use `+RTS -Ksize -RTS' to increase it. LazyWithState: State: Heap exhausted; Current maximum heap size is 1048576 bytes (1 MB); use `+RTS -M&lt;size&gt;' to increase it.
I've been wishing for something like this for a while and it's pretty exciting that you have a working prototype. I have to say, though, that the multiple-hole aspect seems a little weird to me. This may be because I'm used to code editors that present a list of potential completions at the edit point. One related bit of functionality that would be really cool to have is if the available completions included functions/operators that could themselves be used in the hole context if they were applied to the right inputs. I imagine if such a function/operator were selected as the completion, it would be inserted with holes for the missing inputs.
The budget for this position is $75k-150k depending on experience and ability, and since there is a lot of talk about pay, it will be commensurate with experience and ability, so if you have 90 days experience and limited ability it would be unreasonable to expect $150k. That's why it's not always productive to begin with those discussions. I hope this helps. -Phillip
Why would you hire a guy with 90 days experience and limited ability as a tech lead/senior developer? If your range is that wide it‚Äôs ‚Äúnot always productive‚Äù to even post the ad. Either you‚Äôre exaggerating the size of the range or you don‚Äôt actually know what you‚Äôre looking for.
I‚Äôm gonna take that back and say it‚Äôs actually super productive if that is the actual range. 75k if you‚Äôre not good or 150 if you‚Äôre great is very relevant information.
We do know what we are looking for, they are just hard to find. And in the process, we have found several who were beginning their careers and have hired them at the lower end of that range. We are still looking for someone to lead the team with more experience which is why it's posted. If you are interested, I would be happy to discuss the opportunity with you. It is a wide range as we've all quite a range of people express interest. Thank you for your input. I guess as a new startup and working hard to solve this challenge, we could do a better job with the posting. Phillip 
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/0qtsMS3.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) 
My [czipwith package](https://hackage.haskell.org/package/czipwith-1.0.0.0/docs/Data-CZipWith.html) has a class that seems _somewhat_ related to this, especially if you add an instance for some to-be-defined lifted-traversable class (if we take CZipWith as the lifted co-traversable). But I have no idea if that can be made work or if it matches your usecase particularly well.
A related feature was recently merged into GHC master, I believe. Typed holes will now include a list of valid substitutions in the error message: https://phabricator.haskell.org/rGHC26c95f46e679fb73e1ec2bec2be0801c72fd1449 https://phabricator.haskell.org/rGHC1c9208323b5ccd6796d00763ccba9d3b4a612358 I'm not sure how this relates, but perhaps that information could be used.
The lazy random monad is just a lazy state monad and you can use it and interleave to, for instance, lazily evaluate a random tree.
If your code needs to be in IO anyway and needs mutable values, IORefs are the right tool. If it doesn't need to be in IO, I would recommend STRef instead: it's virtually a drop-in replacement, but you don't have to put IO in places where it doesn't need to be.
I'm not the author, but I think you'd run into issues if the size of your data types exceeds the maximum size for a `SmallArray#` -- which is 128, so that's fairly unlikely.
I can understand general frustration with haskell job market, but I would also not attack the few companies trying to expand the number of full-time haskellers. Most people posting these jobs were probably in your shoes at one point and can probably relate to your goals.
*giggity*
According to [this tweet](https://twitter.com/mpilquist/status/964499772630499328) registration and attendance are free, but seats are relatively limited (100 seats total). If you want to hear 8 great talks and one talk by me you should register soon :)
[removed]
Oh, neat that it was solved that easily. With `*&gt;` for nested state we ends up as: \s1 s2 -&gt; let !(!(a, s1'), s2') = ma s1 s2 !(!(b, s1''), s2'') = mb s1' s2' in ((b, s1''), s2'') Which is only tail recursive if we now that neither tuple is a thunk. With `&gt;&gt;` we end up as \s1 s2 -&gt; let !(!(a, s1'), s2') = ma s1 s2 in mb s1' s2' which is always tail recursive. Following from that, changing the Applicative instance for StateT to instance Monad t =&gt; Applicative (MyStateT s t) where {-# INLINE pure #-} pure a = MyStateT $ \s -&gt; pure (a, s) {-# INLINE (&lt;*&gt;) #-} MyStateT smf &lt;*&gt; MyStateT sma = MyStateT $ \s -&gt; do (f, s') &lt;- smf s (\(a, s'') -&gt; (f a, s'')) &lt;$&gt; sma s' Also fixes the leak 
Ahhh of course! Then I'll just throw that into a map and construct my on-the-wire models. Thank you!
Option 1, and make it easier to go download older prebuilt binaries that don't have fancy new bugs. Don't over-complicate the problem and start ignoring the point of package versioning systems just to support the needs of the tiny percentage of the userbase that needs to be able to build the latest version of your build system from source at the drop of the hat. 
I have been convinced that this is impossible because the instances for `Double` may have been inlined.
Superrecord allows you to "cast" to real Haskell types of the correct shape. So you can either define these types at the API border or have your users define them and put the needed typeclass constraints on your library. I'd still recommend the former.
I agree, but I'd go as far as to say that Stack should prioritize never making an official release with un-upstreamed changes. This technique should be used solely for development.
This use of `Maybe`and the `Data.Vinyl` namespace are fine. Does `JsonField` really need those `KnownSymbol` constraints? Is it not possible to add these constraints to all function that need them?
You know, I hadn't even considered that. Initially, I just copied straight from the implementation for `Data.Vinyl.Derived.FieldRec`, which also has that constraint in there, but I don't really see why it's necessary. I guess I just assumed that, since whoever made that was clearly more experienced than me in type-level stuff, there was some good reason for it. I'll look into changing it!
What are your particular reasons for disliking Google out of curiosity?
`[Char]` is a type, not a class. `Num` is a class, not a type. Values have types, but not "classes", that doesn't make sense. Thus there are values of type `[Char]`, but "values of type `Num`" is a nonsensical phrase, because `Num`'s not a type. `Just "Haha"` has type `Maybe [Char]`. `Just 84` has type `Num a =&gt; Maybe a`. `a` is a type variable, which can in principle be substituted with any type, but `Num a` on the left of the big arrow `=&gt;` is a type class constraint. This says that for any type `a` that is an instance of `Num`, `Just 84 ` can be given the type `Maybe a`. Some instances of `Num` are, for example, `Int`, `Integer`, `Double`, so `Just 84` can be given any of the types `Maybe Int`, `Maybe Integer`, `Maybe Double`, and more. In particular, you can also define instances. So if you create a type `T` and make it an instance of `Num`, then `Just 84` can also be given type `Maybe T`.
So `Maybe` isn't a type. It's a *type constructor*. What does that mean? Well, to be precise, that means it has *kind* `* -&gt; *`. What are kinds? Well, if a *type* has *kind* `*`, that means that it can have values at runtime. So `Int` has-kind `*` (or, in Haskell notation, `Int :: *`), as does `Char :: *`, and `[Char] :: *`, and `Maybe [Char] :: *`. The other *kind* in standard Haskell is `-&gt;`. Much like function types, function *kinds* are used when we're ocnstructing things. So `Maybe` has-kind `* -&gt; *`, or `Maybe :: * -&gt; *`, or "`Maybe` is a type constructor expecting a type-of-kind-`*` before it can be a type-of-kind-`*`." So we can write: Prelude&gt; :t Just "haha" Just "haha" :: Maybe [Char] And we can write: Prelude&gt; :t Just (2 :: Int) Just 2 :: Maybe Int But, what about Prelude&gt; Just 82 Just 82 :: Num a =&gt; Maybe a This looks kind-of polymorphic. What's up with that? Well, `82` is a numeric literal. And numeric literals in Haskell are polymorphic! We can write `82 :: Int`, `82 :: Integer`, `82 :: Word64`, `82 :: Double`, `82 :: Rational`, and more -- if you can define a `Num` instance for your type, then you can use numeric literals with that type. So, we have a value `82 :: Num a =&gt; a`. And we have a function `Just :: b -&gt; Maybe b`. We want to see if we can apply `Just` to `82`. Does it type-check? Let's see -- can we unify the types in this expression? What constraints will we pick up? Just :: a -&gt; Maybe a 82 :: Num b =&gt; b For `Just`, that type says "You can pick any `a` at all, and I can make a `Maybe a` out of it." For `82`, it says, "You can pick any `b` at all, and as long as that `b` is an instance of `Num`, I can make a `b` out of it." When we unify that, we get `Just 82 :: Num c =&gt; Maybe c`, which means "You can pick any `c` at all, and as long as that `c` is an instance of `Num`, I can make a `Maybe c` out of it."
There are so many Comcast offices in Denver. They should have a conference out here -- I'd speak at it! :D
Maybe is a constructor of a type [Maybe a], where 'a' is another type. The 'a' stands for any other type, like Int or Char. When you give it a type, it becomes the resulting type of Maybe Int or Maybe Whatever. Types in haskell have many more properties and abilities than in most other languages.
It‚Äôs still a ghc bug. Do we really want to make ghc bugs into language features. The reason it‚Äôs not been fixed is that it‚Äôs a bit tricky to do since it has to happen at link time. 
Having instances be global is indeed a problem. But I‚Äôve not seen a cure that isn‚Äôt worse than the illness. 
Thank you very mcuh!!!
Great! By the time that lands, I can use that information from the error message. Users of older GHCs can use this. 
Do you recommend Cloud Haskell or Transient for Erlang like actor model programming?
I wish there were tutorials on this subject which didn't mention Paeno numbers. I find them incredibly useless due to how limited they are relative to their verbosity.
You're converting the same input into an integer. Your wrapper can be rewritten as wrapper :: String -&gt; Integer wrapper input = let i = read input :: Integer in myFunction i i i `read` doesn't have any side effect, it doesn't advance a pointer in the input string, nor does it consume anything from the input string. I don't know the exact read semantics, but maybe reading the whole line into a list of numbers will work. wrapper input = let ints = read input :: [Integer] in myFunction (ints !! 0) (ints !! 1) (ints !! 2) (ignoring any error handling when the input string doesn't contain at least three integers)
To give you an exact answer we need the exact format the numbers are presented in. But line-parsing can usually, for the scale of homework problems, be effectively done using list functions. So suppose the input is `"123 456 789"`, we can use `words :: String -&gt; [String]` to break on spaces, `fmap read :: [String] -&gt; [Integer]` to parse each word into an integer and finally, pattern match on the result to extract the three numbers. Look, it's like web-scraping, where the input format changes all the time -- unless it's a hard requirement for your code to be performant, robust and user-friendly in handling malformed or otherwise incorrect inputs, there is no point in trying to be clever about -- code exactly for the input format you were promised. Your teacher probably fixed the format to simplify that aspect of the exercise for you -- unless this is an exercise in parsing, of course :)
Has anyone come across this error: `AesonException "Error in $.packages.cassava.constraints.flags['bytestring--lt-0_10_4']: Invalid flag name: \"bytestring--lt-0_10_4\""` When running `stack build` recently? Not sure how to fix this...
To expand a bit on what others are saying, Num is defined as class Num a where (+), (-), (*) :: a -&gt; a -&gt; a negate :: a -&gt; a abs :: a -&gt; a signum :: a -&gt; a fromInteger :: Integer -&gt; a Note that last one: `fromInteger :: Integer -&gt; a`. Integer literals like `84` implicitly have `fromInteger` called on them. That way, you can write something like add5 :: Int -&gt; Int add5 x = x + 5 instead of having to write add5 x = x + fromInteger 5 This syntactic sugar is really, really nice when working with library-defined numeric types. 
I don't get the distinction you are making. How is using a newtype not writing code? I mean I guess using deriving is code generation...but the whole point of generic newtype deriving is that it sidesteps that.
I think this could be cleaned up a bit: wrapper :: String -&gt; Integer wrapper input = let [a, b, c] = read input in myFunction a b c Note that both your code and this code will only read a list of numbers this way if it is specified exactly as a list, such as "[1, 2, 3]". If the input string simply contains numbers delimited by whitespace, you could do something like this: wrapper :: String -&gt; Integer wrapper input = let [a, b, c] = map read $ words input in myFunction a b c 
Arbitrarily assign points a,b,c,d. v1 = b - a v2 = c - a v3 = b - d v4 = c - d Points form a square iff v1 dot v2 = 0 and v3 dot v4 = 0
If you mean how to implement it in haskell, then its mostly tuple unpacking and related arithmetic. You could save some effort with Control.Bifunctor. If you want a full solution lmk and I can code it up later.
Wait, wouldn't that just check if its a rectangle?
Haha yea youre right. Also check that |v1|=|v2| and then its a square check.
Ok. Thank you
The proof at the end can be done more generally as well. Whenever the initial algebra for `I + A ‚äó -` exists, written as `¬µX. I + A ‚äó X`, then that is the carrier for the free monoid, e.g. proposition 2.6 in http://www.fceia.unr.edu.ar/~mauro/pubs/Notions_of_Computation_as_Monoids_ext.pdf .
Couldn't it still be a [Lambert quadrilateral](https://en.wikipedia.org/wiki/Lambert_quadrilateral) with two equal sides?
Thanks a lot! And the syntactic sugar is really cool and makes things elegant!!!
I almost never want a global cache (I can only think of a cached `getCurrentTimeZone` as a counter example). I use a lot of caches, but I end up instantiating multiple versions often enough to want them closed: for instance, I'll instantiate a cache aiming at both dev and prod data, or something. Then I use something like `withCache :: Timeout -&gt; IO a -&gt; (Cache a -&gt; IO b) -&gt; IO b`, exposing `pollCache :: Cache a -&gt; IO (Maybe a)` and `ioWithCache :: Cache a -&gt; IO a`, and handle the internal mechanism with `TVar`s. Then on the other side I'll have `data App = App {tz :: Cache TimeZone, fibCache :: HashCache Int Int, ...}`. You expose `withApp`, and then you can pass your app to multiple threads, and gaurentee that within a given `Timeout` you only run `getCurrentTimezone` once. I've never really run into the problem of passing around the caches; you write logic code with pure inputs, and let your app code worries about what you want cached.
Just check the lengths of the edges. Don't 4d chess this shit. It's not that many calculations.
Cardboard boxes collapse without a top or bottom to hold the shape. Which is to say, parallelograms with equal side lengths exist.
Right, and also the angles. Any three points of a square will form a right triangle where the lengths of two of the sides are equal. Test all possible combinations of three points from the input. It's not very many combos. Point being the problem is solvable with elementary math, so don't get further in than you need to, just brute force it.
You can see the software running at https://haskell.org/news, btw.
**isRectangle** 1. Place all components of the coordinates, x and y, into list 2. Get unique elements OR sort then group 3. If the length of the resulting list is 2 you have a rectangle &amp;nbsp; **isSquare** 1. Check if it's a rectangle 2. Sort coordinates by x then y --- elements of list will be [bottom left, top left, bottom right, top right] == [(x1,y1),(x2,y2),(x3,y3),(x4,y4)] 3. if (y2 - y1) == (x3 - x1) then you have a square &amp;nbsp; It's overly complex, but it'll get the job done.
`getLine` is an action that reads a line from standard input and returns it as a `String`. Your `main` definition is an action that runs `getLine` only *once* (therein lies your problem), and connects its output as input to `print . wrapper`, which is a function that accepts a `String` and returns an action that will `print` the result of `wrapper` and return `()`. Supposing you want the numbers to be entered on separate lines, you can just run `getLine`+`read` three times, and omit the `wrapper`. With explicit use of the bind (`&gt;&gt;=`) operator: getLine &gt;&gt;= (\ line1 -&gt; getLine &gt;&gt;= (\ line2 -&gt; getLine &gt;&gt;= (\ line3 -&gt; print $ myFunction (read line1) (read line2) (read line3)))) In `do` notation: do line1 &lt;- getLine line2 &lt;- getLine line3 &lt;- getLine print $ myFunction (read line1) (read line2) (read line3) Note also that the combination of `read` and `getLine` is available in the standard library as `readLn :: (Read a) =&gt; IO a`: do num1 &lt;- readLn num2 &lt;- readLn num3 &lt;- readLn print $ myFunction num1 num2 num3 ---- This isn‚Äôt so important for your code, but in the future, you might see many different variations on this, for example with the Applicative operators `&lt;$&gt;` (`fmap`) and `&lt;*&gt;` (`ap`): print =&lt;&lt; myFunction &lt;$&gt; readLn &lt;*&gt; readLn &lt;*&gt; readLn Or one that avoids repeating `readLn`: do [num1, num2, num3] &lt;- replicateM 3 readLn print $ myFunction num1 num2 num3 
Assuming the points may be in any order, I would write a simpler function that determines whether four points *in order* (clockwise or counterclockwise) form a square; then rotations can be ignored, and there are three possible orderings to handle: A--B | | D--C A B \/ /\ D--C A--B / / D--C The helper function determines whether four ordered points ABCD form a square by checking that ABD and BCD form right isosceles triangles. This should be plenty to write a solution, so I‚Äôll refrain from posting code if you want to solve it yourself. And of course there are many possible ways to solve the problem. Another way is to find the center of the four points, choose one of them, rotate it 90¬∞ around the center repeatedly (hint: `iterate`), and check that the set of resulting points is equal to the original points.
You just nerd-sniped me so hard! Didn't think I'd be pulling out trigonometry this late at night. Anyway, I think I got one, it looks like this: http://moziru.com/images/kite-clipart-blank-20.jpg Corners could be: * a = (2.4, 1.8) * b = (0, 5) * c = (0, 0) * d = (-2.4, 1.8)
well there are infinite shapes it could be if you allow the sides not to be straight lines, but then the points don't really *define* a curved side as you don't have enough information for that
Can you give an example?
Could I do using Data.Vector? 
Well, you could, but `Data.Vector` isn‚Äôt really a mathematical/geometric vector, it‚Äôs what other languages would call an array‚Äîa contiguous sequence of elements with O(1) random access. If you‚Äôre thinking of the second solution I mentioned, you only need a few operations such as 90¬∞ rotations (`rot90 (x, y) = (-y, x)`) and vector difference (`diff (x1, y1) (x2, y2) = (x1 - x2, y1 - y2)`), not necessarily a geometry library. It‚Äôs also possible with complex numbers (where that rotation is just multiplying by *i*), although I think you‚Äôd run into precision problems because `Data.Complex` assumes floating-point.
I've seen universities teach haskell to first years. I've seen workshops that teach haskell to n-year veterans. It's never too early. Be aware that you might find it hard to get through at first, because it can feel really different to what you're used to.
Instead of trying to create the required list, you can create two separate lists; one with all the integer values and one with the squares only. Then, you can just get the first `x` elements of the second list, remove them from the first list and get the required element based on the index. You should just need to tune the value of `x` in order to be as small as possible (and for sure `&lt; index`). I don't know if this solution will satisfy your time requirements, but for sure it will be better than the one you tried. Also take into consideration, that the execution time is related to your machine. So 5 seconds in your machine, can be 50 second on my old computer ;)
```haskell nosquares :: (Enum a, Ord a, Num a) =&gt; [a] nosquares = diff [1..] (map (^ 2) [1..]) diff :: Ord a =&gt; [a] -&gt; [a] -&gt; [a] diff [] _ = [] diff xs [] = xs diff xs@(x : xs') ys@(y : ys') = case x `compare` y of LT -&gt; x : diff xs' ys EQ -&gt; diff xs' ys' GT -&gt; diff xs ys' ```
Sort the four points, then flip the first two. That gives you a traversal, in one direction or the other. So only two cases to check.
 Could you have 2 different Ord instances for two *different* data types in two different modules that happen to have the same name? Or two Map values for the same key in two different Maps from two different modules that happen to have the same name? It's the same question, because under my proposal an instance is just another kind of data, scoped by namespace. So, yes, of course. The only difference is that I have not proposed is a qualification syntax to allow having *both* instances in scope in the same module. But we could easily do that, too - if an instance is imported qualified, its methods are qualified.
I am not sure how you can enforce that the same data-type is used with the 2 different instances inconsistently, by accident. IOW: I don't understand how you implement this without losing global coherence.
If you enable my proposed extension, you are explicitly saying that you do not want global coherence. Instead, instances are imported, exported, and scoped, just like any other part of the type system. If your program tries to import two conflicting instances in the same module, your program is rejected, just as for any other part of the type system. Do you have a specific scenario where lack of global coherence is a problem?
Can you prevent a function from accidentally seeing both conflicting instances of the same type-variable at the same time?
When the extension is enabled, instances are not explicitly passed. They are exported and imported in the same way that other type system artifacts are exported and imported. As for anything else, the compiler checks whether export and import resolution results in identically named imports from two different modules, and if so, this is flagged as an error. The difference here is that the type variables are a part of the "name". So for example, you can import `Show Int` from one module and `Show String` from another without conflict. Whereas if `data Show a` is defined in two different modules, you cannot import `Show Int`
&gt; I think integer square root is in a GHC lib somewhere but since I &gt; already have binary searching I'll write it myself. http://hackage.haskell.org/package/arithmoi-0.6.0.1/docs/Math-NumberTheory-Powers-Squares.html
That's interesting. Thanks for posting. 
This blog post is something of a followup to my [SLURP blog post](https://www.snoyman.com/blog/2018/01/slurp) and some of its ensuing discussion. This post restates some of the private requests I'd made that led (in part) to the SLURP proposal. I've already shared most of these ideas on Github. However, a single comment in a mega-thread on Github is hardly a good place to write down these requests. This post recaptures those ideas with more explanation. It also contains a few other ideas. I hope to ultimately move the discussion to the proper official forums for these kinds of requests.
Good to know, thanks. Nitpick: the link to [*wires*](https://hackage.haskell.org/package/wires) here actually points to *reflex*.
Hopefully not too off topic, but what is the current state of generic programming in Haskell ([gmap](http://okmij.org/ftp/Haskell/generics.html) and friends)? This new work improves upon that current state of the art. Are people using gmap in the wild instead of map/fmap? Why not? Because it isn't well know? Not included in the standard library? Cumbersome to use? Hard to decipher type errors? Doesn't solve problem that people commonly encounter? Not used in tutorials? Other?
Your link to https://hackage.haskell.org/package/wires is actually a link to https://hackage.haskell.org/package/reflex. The other three links are correct.
Thanks, fixed.
I am writing an order matching engine in haskell, but I am not sure if the design is good enough. Basically when a new order is added to the book, I need to inform the user of all the effects that placing it triggered (such as orders that were executed, orders that were cancelled, etc), alongside with the updated book. I have something on the lines `placeOrder :: Order -&gt; Book -&gt; ([Events], Book)`. Does it make sense? Or should I somehow emit those events using a different approach (like, trying to infer it by "diffing" the book state).
FWIW, [frpnow](https://hackage.haskell.org/package/frpnow) is a fantastic, monadic FRP implementation.
It looks great, and the first thing that came to my mind was image processing, but then I noticed you have [a library for image processing](https://hackage.haskell.org/package/hip), too! Do you intend to eventually use this new `massiv` library for number crunching? Or are these libraries unrelated? Or is `massiv` a direct result of your previous experience with `repa` when writing `hip` and thinking "this could be better"?
As per the big slurp discussion, the pvp is a ‚Äúshould‚Äù in the upload guidelines, which means ‚Äúrecommended but not required‚Äù roughly.
It depends on your use case, but one thing I've been doing is doing as much validation as possible on the client, which gives the Haskell validation more leniency to have bad error messages. For example, the client is responsible for validating EIN numbers are formatted as NN-NNNNNNN (Where N is any digit, and the hyphen is optional) and giving a nice message to the user. The backend is then free to have an `EINNumber` newtype with a `FromJSON` instance that rejects values that don't match that format. That error message isn't customer-friendly, and it doesn't include every error with the request, but that's OK because users should never see that error. My motivation here isn't that the backend couldn't provide good error messages, just that it's a much better user experience for the client to instantly tell you things like "Your passwords don't match" or "EIN numbers must be exactly 9 digits long, with an optional hyphen after the second digit". That frees the backend to worry about things that the client can't enforce (e.g. security), and for its error messages to be worse. Broadly though I think your situation is a little underspecified: - Is this a JSON API to be consumed by e.g. a first party mobile app or third parties? My proposal above wouldn't work for third parties. - Is it important to give every failure back in the response, or just the first one encountered? - What kinds of things are your validating? Could your validations all happen in e.g. a `FromJSON` instance, or do they need to do e.g. database access. - What web framework are you using? (Different web frameworks might have different tools for this).
I'm sure arithmoi is much faster, but here's the simple isqrt I usually use: stable (x:xs@(x':_)) | x == x' = x | otherwise = stable xs isqrt x = stable $ iterate (\y -&gt; (y * y + y + x) `div` (2 * y)) 8
Json api consumed by angular currently. First error might be okay Some IO/db access, rest are mostly required or not Raw warp, handrolled digestive-functors currently, trying to pick a standardized replacement
[flogging that dead horse again, ey?](http://i0.kym-cdn.com/photos/images/newsfeed/000/540/686/696.gif)
I was about to start looking for something like this, thanks!
Thanks to Ertugrul for being upfront and helpful about this.
In my opinion, inhibition is a bad idea, mainly because not all wires are supposed to be able to inhibit. Inhibition introduces a sort of null pointer in the output of the wires that can block the entire network. Moreover, with inhibition there many different ways to do the same thing.
I think 1) clarifying that "PVP adherence is optional" is best addressed by the uncurated layer proposal (https://github.com/haskell/ecosystem-proposals/pull/6) which seems to be widely accepted and I intend action on soon. Doing something in the "meantime" seems superfluous. Regarding 2) "Hackage Trustee guidelines", Adam Bergmark who is both a hackage and stackage trustee, has told me that he is looking into drafting something up for trustees to discuss. Regarding 3) "Downstream projects" my understanding was that the understanding requested is already part of the mission statement of the ghc devops group? https://ghc.haskell.org/trac/ghc/wiki/DevOpsGroupCharter -- In particular, since hackage is coupled to cabal, and cabal is coupled to ghc, then the general concerns there flow downstream "naturally." In particular there is a release policies discussion that it seems came close to conclusion but never did, which should deal with the one place frictions arise (pace of changes and timing to test them): https://mail.haskell.org/pipermail/ghc-devs/2017-December/015173.html I believe that the consensus from this discussion, though it was never officially "stamped" has been already started to be implemented in the downstream library release policies of various packages? It would be good to clarify this. Regarding "maintainer guidelines" I think the entire point of the ecosystem-proposals repo is precisely so we have a place to discuss these things. However we also need to recognize that when people haven't yet fully fleshed out a proposal, then it is ok for them to say that we should hold off a bit on a proposal until they have. So the caret operator _will_ get such a discussion, but only when a full proposal is ready, etc. 
&gt; If there is a bug in a package, it should be fixed and the version be bumped, even if it's just a bug in the metafiles. The reason this is inadequate is discussed in the revisions FAQ: https://github.com/haskell-infra/hackage-trustees/blob/master/revisions-information.md#why-not-just-keep-uploading-new-versions
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [haskell-infra/hackage-trustees/.../**revisions-information.md#why-not-just-keep-uploading-new-versions** (master ‚Üí 5a30f96)](https://github.com/haskell-infra/hackage-trustees/blob/5a30f96e1bb1bdb1d71fba137cea149b1c197810/revisions-information.md#why-not-just-keep-uploading-new-versions) ---- 
Looks really cool! Might be nice to have a comparison with `accelerate` as well.
Absolutely, I'll be happy to hear about your experience using massiv.
Perfect timing :) Let me know how it works out for you.
&gt; Doing something in the "meantime" seems superfluous. From that proposal thread it appears as if the uncurated layer is still being hashed out. It seems as if it would be a good idea to clarify the PVP adherence guidelines now, as such a clarification would have a _very_ low time cost, and continue work on the uncurated layer in parallel. When/if the uncurated layer is finalized and put into practice, the terms can be updated again to reflect the new layering system. &gt; So the caret operator will get such a discussion, but only when a full proposal is ready, etc. Forgive me if I'm misinformed, but isn't the caret operator already implemented in cabal and shipping with GHC 8.2+? If there is no such proposal, or complete summary of intended behavior, this seems like it warrants a high priority discussion. If there _is_ such a proposal or summary, I would imagine that it would form the basis for a discussion and should be posted publicly in the ecosystem proposals repo. It's entirely possible I'm misreadign something here though, so feel free to clarify.
Yah, would be good to know if there are cases where the two are competitive. 
You hit it right on point. I did write the Haskell Image Processing library using Repa, but soon noticed many places where it could be improved. Unfortunately Repa isn't too active nowadays, plus some of the changes I wanted to make were too drastic that I know would be accepted by the library maintainers anyways, so instead I decided to do it from scratch. Next step will be transitioning `hip` to use `massiv`. I already started working on it and there is now a helper `massiv-io` package that can read/write images in the similar way `hip` does it: https://hackage.haskell.org/package/massiv-io
&gt; From that proposal thread it appears as if the uncurated layer is still being hashed out. No there's been no comment on the thread after over a week, which was the stated cutoff for "I guess everyone's ok with this then." So it is ready to implement imho. &gt;isn't the caret operator already implemented in cabal and shipping with GHC 8.2 Yes. And that part was fully discussed and documented. The point alluded to is that there are future changes with tooling to more fully take advantage of this and to give flesh to the idea that it is more than just "syntactic sugar" but has a different intent -- in expressing the idea that it is a "known-good" statement versus a speculative statement about what may break. The details of that future work are not pinned down, and also no action has been taken. The complaint is just that that _future_ work has no full proposal. I agree it has no full proposal. I want it to have one. But that proposal isn't read yet! So we can't have the discussion until we flesh out those details. That's all that herbert was poorly alluding to about "future plans not ready" -- that he doesn't want to discuss a proposal that isn't ready, so that there isn't confusion. When the proposal is ready, we'll discuss it. Until then it is not ready for discussion, because details are not pinned down, even though a general sense of what we want it to be is sort of understood by some people. There is nothing to discuss about the current operator, because it is currently just well-specified sugar. The future changes to the semantics and meaning of this operator, and perhaps other complementary operators, will be put forward in a proposal, when there is one ready.
I suggest you suggest that to michael, who has been insisting on hashing out the details of various PRs very publicly.
Have a look at [forma](https://hackage.haskell.org/package/forma). It does not provide any validation combinators for fields itself, but it does handle the ‚Äûall keys are strings‚Äú problem of JSON input nicely. Validation is applicative, so it collects all errors, and the ‚ÄöwithCheck‚Äò function allows a ‚Äûsecond pass‚Äú in which you can handle field interdependencies. 
Interesting. Does it allow for validating objects with objects inside or only top level key/values
You're correct as to the lack of clear guidelines for acceptance. That's why this is addressed explicitly in the last comment on that thread, which says effectively that it is up to maintainers to decide when proposals are "accepted" at this time, and that is what will happen here. &gt; To me, this seems like it opens the doors to break backwards compatibility with tools and/or build plans that already exist using this operator. The future plans aren't intended to break backwards compat, but just to widen the possible use for signaling in a backwards-compat way. But again, to discuss that means to discuss the future plans explicitly, and we can't do that, because they're not fully spelled out yet. But you are right that when we _do_ have this consideration, then that _will_ be an important component of that discussion. 
I spent the last hour or so playing with strictness annotations with no luck. Here are some things that I tried. I made the fields of DNAEntry strict: data DNAEntry = DNAEntry { identifier = !Text, content = !Text } This may have shaved a second or two off the compiled time, but had no effect on the interpreted time. I tried forcing the evaluation of the parser: process txt = case parseFASTA txt of Left msg -&gt; msg Right !entries -&gt; longestCommonSubstring $ map content $ entries I tried forcing evaluation of various steps in the substring search: step s = let !x = map (\c -&gt; let !n = T.cons c s in n) ['A', 'C', 'G', 'T'] in x I tried making the search loop strict in its arguments: longestCommonSubstring :: [Text] -&gt; Text longestCommonSubstring entries = go [""] "" where go :: [Text] -&gt; Text -&gt; Text go [] longest = longest go !current@(s:_) !_ = go (filter substringOfAll $ concatMap step current) s and I tried making substeps of the search strict: longestCommonSubstring :: [Text] -&gt; Text longestCommonSubstring entries = go [""] "" where go :: [Text] -&gt; Text -&gt; Text go [] longest = longest go !current@(s:_) !_ = let !s1 = concatMap step current !s2 = filter substringOfAll s1 in go s2 s None of these had a significant effect. I'm a newbie when it comes to Haskell performance, so there may be some obvious case that I'm missing.
No. It is a problem because each prior version would still be available to the solver. As long as a "metadata-only patch" involves a new version release, then the old versions still exist with the old metadata.
It almost sounds like this would be better solved by removing versions from the solver by marking them as broken. Something like "a.b.c marked broken. a.b.c.1 is released as a metadata version release" (as a hypothetical example). I guess I'm just failing to see why a separate mechanism *needs* to be adopted to deal with something that can also (seemingly) be solved by the same mechanism already in place.
But all those prior versions _aren't_ broken, except in a specific config. They're all good -- just with incorrect metadata! So why not just fix the metadata? After all, someone else may have pinned specifically to that version. Now do you want that version to be marked broken and to break that prior good plan?
&gt; Regarding 3) "Downstream projects" my understanding was that the understanding requested is already part of the mission statement of the ghc devops group? https://ghc.haskell.org/trac/ghc/wiki/DevOpsGroupCharter -- In particular, since hackage is coupled to cabal, and cabal is coupled to ghc, then the general concerns there flow downstream "naturally." In particular there is a release policies discussion that it seems came close to conclusion but never did, which should deal with the one place frictions arise (pace of changes and timing to test them): https://mail.haskell.org/pipermail/ghc-devs/2017-December/015173.html Others should point out if they disagree but it's not entirely clear to me that this should fall under the purview of the devops group. At this point GHC is only rather loosely tied to `Cabal`. If anything, I would consider GHC to be downsteam of Cabal.
The solver would opt for an older version if no build plan could be found with the current version. For example, if the older version omitted bounds that the new version included.
My point is a bit more subtle (though I confess I may have confused upstream and downstream here) -- it is that in the linked release policies discussion, there was a plan for libs bundled with GHC to sync their schedules to the GHC schedule to some degree. Ultimately, libs can do what they want, but GHC determines what versions of them get shipped with GHC, so that's the point of coordination...
Ah ok that makes sense, thanks for explaining!
I'm not sure how much GHC.Generics is used in real application, but it seems some useful libraries are using it. "aeson" and "generic-lens" are what I can name. This experiment is *very slight improvement*, and is not worth to propose compiler change (IMO), because it enables only `GContravariant` (not so much used) and some edge case of `GFunctor` (we already have DeriveFunctor which works amazingly well). But there may be something I didn't notice. It actually expands what Generics can do. I did this because primarily I felt it's low-hanging fruit which is not done with some unknown reason. And secondary, I'm interested in Generic2, and I think it's good to have newtype Arr2 f g p q = Arr2 (f p q -&gt; g p q) to enable generic Profunctor. So it was a kind of practice for me.
Some initial indications although it doesn't necessarily explain the difference: `$ stack build --profile &amp;&amp; stack exec -- ros-test-exe +RTS -p -RTS` `$ head ros-test-exe.prof` ``` Sun Feb 18 17:15 2018 Time and Allocation Profiling Report (Final) ros-test-exe +RTS -N -p -RTS total time = 10.97 secs (10975 ticks @ 1000 us, 1 processor) total alloc = 525,034,960 bytes (excludes profiling overheads) COST CENTRE MODULE SRC %time %alloc longestCommonSubstring.substringOfAll.\ LCSM src/LCSM.hs:16:33-47 96.8 53.9 shiftL Data.Text.Internal.Unsafe.Shift Data/Text/Internal/Unsafe/Shift.hs:53:5-64 1.9 0.0 longestCommonSubstring.step.\ LCSM src/LCSM.hs:19:23-32 0.7 41.2 takeWhile_ Text.Megaparsec.Stream Text/Megaparsec/Stream.hs:236:3-21 0.1 2.1 ``` 
Hmm. This is now more confusing. I get similar runtimes to you when building with --profile, but still ~40s runtime with no profiling. Why would profiling make the resulting binary run faster?
Marking `substringOfAll` `INLINE` made it seem to never finish or at least take longer than 40 seconds. I also get a long runtime if I eta contract `(\e -&gt; T.isInfixOf s e)` which makes me feel like maybe this is GHCi / profiling changing some sharing behavior.
Profiling can interfere with optimizations. If one optimization "goes wrong" but is prevented by profiling this would make sense.
What happens when you try it at each different level of optimization (-O0, -O, and -O2)?
The SYB-style of generic programming (that `gmap` is part of, and enabled by `Data.Data` in base) has been superseded by `GHC.Generics`, which is safer (`Data.Data` is dynamically typed), more expressive and more performant, all at the same time. SYB fails to handle any type that contains functions, `IO`, or polymorphism (among other things). So `gmap` is really just one way to implement `fmap`, but not a universal replacement. It doesn't help that `Functor` can also be derived by GHC. There are often types which don't fit the common pattern that can be captured by generic programming, so generics mostly offer an ad-hoc but convenient way to implement other more widely exposed interfaces. More concretely, the dominating practice for libraries is to provide some interface, e.g., a type class class Functor f where fmap :: (a -&gt; b) -&gt; f a -&gt; f b together with a generic implementation, that imposes some restrictions on the type `a`, which should at least be `Generic`. gfmap :: GenericFunctor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b So users can just write instance Functor A where fmap = gfmap while being free at any time to use another implementation in place of `gfmap`. Furthermore `gfmap` may often rely on other `Functor` instances, so it is generally not possible for users to just use `gfmap` directly without writing any instance. On a more technical note, the current approach to generics in Haskell is basically to have a representation of a data declaration at the type level, and a uniform way to map those representations to actual types. We can currently represent any regular ADT, and we're very close to have support for some GADTs. The difficulty to get here is probably due to the lack of dependent types. (Generics are trivial once we have those, but what I mean by "probably" is that there may still be a good solution without them.)
-O0: no profiling: 45s with profiling: 10s ghci: 4s -O no profiling: 46s with profiling: 10s ghci: 4s -O2 no profiling: 46s profiling: 10s ghci: 4s No significant differences. That's a little weird.
That one is my current facourite, it does have some room for improvement though. It could really use a function `planRepeated :: EvStream (Now a) -&gt; Now (EvStream a)` and the functions necessary to make it are use internally but not exported. On another note, have you tried my [GTK3](https://hackage.haskell.org/package/frpnow-gtk3) bindings for it yet? I've found its way of mixing FRP with imperative events works really for GUI libraries.
&gt;Until then it is not ready for discussion, because details are not pinned down, even though a general sense of what we want it to be is sort of understood by some people. This sounds like precisely the kind of discussion that would benefit from being had in the open.
Discussions without prepared proposals where details are worked through are often a mess, because there are lots of problems that arise due to just not having ironed things out, and those issues can be so multiple and distracting that it is hard to focus on the main issue. I don't understand this insistence that things that aren't worked out be discussed unclearly and poorly instead of just letting people take the time to organize thoughts and ideas towards a productive outcome. Good discussions when there are potentially many participants don't just _happen_ -- they take work. Throwing half-baked ideas out leads to confusion, and worse yet leads to people burning out their attention for a given thing before it is in a situation where that attention is best used. There is and should be a time for widespread discussions -- but that doesn't mean that every discussion has to happen all at once and regardless of if people feel equipped to have a concrete proposal to discuss.
A powerful editor or IDE with a good file tree explorer &amp; search. Scan quickly through all directories &amp; files once. Semantic source navigation configured (eg emacs + haskell-ide-engine, or VS Code + haskell-ide-engine). Or if not that, at least tag based navigation (generated with etags/ctags/hasktags). Identify the packages involved and how they related. For each one, review their cabal (or package.yaml) file to see what libraries/executables they provide. Review all haddock docs (if not published online, try stack haddock --no-haddock-deps --open). Especially get familiar with the main types and the module layout. Stepping through with a debugger is not so common in Haskell at the moment. There *is* a debugger built in to GHCI, which will do it. To get a bigger view of runtime behaviour, add some trace statements (from Debug.Trace) in strategic places. watch programs run with strace or similar. maybe generate a time &amp; space profile (see GHC guide).
Note that we can derive `Functor` with just `Generic` (not even `Generic1`). http://lpaste.net/362630 This extension makes it possible for the generic representation (`Rep1`) of a functor to also be a functor, in more cases than was previously possible. But as I've just shown, that is not necessary to implement `Functor` generically for the represented type. Furthermore, the `Generic1` approach doesn't handle types such as `data Foo b a = Foo (Either [a] b)`, does it? In the generic-deriving nomenclature, this is actually called [`GFunctor'`](https://hackage.haskell.org/package/generic-deriving-1.12.1/docs/Generics-Deriving-Functor.html#t:GFunctor-39-) (with a prime). If `Class` is a standard class in base (`Functor`, `Show`, `Ord`...), `GClass` is simply a duplicate of `Class` for generic instances to be defined for standard types without overlap, whereas `GClass'` is a class instantiated by the `GHC.Generics` building blocks `U1`, `M1`, `K1`, `(:+:)`, `(:*:)`, etc. as part of a generic implementation of `GClass`. Thus, `GClass` and `GClass'` play different roles, but `GFunctor` and `GFunctor'` happen to coincide exactly, because, as regular types, `U1`, `M1`, etc. are in fact functors.
Michael has the advantage here. He has a compelling narrative that all this airing out of the dirty laundry is necessary to get things moving. If he's wrong, then why has the Haskell tooling and documentation been so ostensibly sub-par for so long? And why is /u/hvr_ replying to even *a priori* reasonable posts with *sigh*s and *err*s? To us onlookers, the handling of the Cassava double-dash flag fiasco seems emblematic of *something*, even if it's not completely clear what. I'm not saying that /u/snoyberg is right, in fact I'd say his tone is downright shameful sometimes. But holy shit the people he's up against sometimes seem to be doing their best to obstruct progress in the Haskell ecosystem just because they can.
I would suggest to open an issue with https://github.com/haskell/hackage-server However a more detailed outline and use case might be helpful.
https://hackage.haskell.org/users/register-request has &gt; Most contributors to Hackage attach their real name to their packages and contributions. However, we have no wish to discourage contributors with personal or professional reasons for wanting pseudonymity. We ask only that such people choose a name (and username) that looks at home among a collection of real names; we will be unwilling to add Kittenlover97 to the package uploader group. Which I understand to say that contributors could choose to register an account on behalf of a company. On the other hand package can have multiple maintainers who can upload new version of a package to hackage. As a last resort there is also a package takeover procedure: https://wiki.haskell.org/Taking_over_a_package Not sure if this fully answers your question though. If you are looking for a group feature where a package is maintained by a group (company) and a group consists of one or more maintainers, I don‚Äôt think that feature exists yet.
Thanks! If you interpret it that way, it gives me good reason to just try the registration process, I suppose.
That's clever! I understand this specific use of `{-# INCOHERENT #-}` is safe, as long as there are no more incoherent `Functorial a b c d` instances, and we prohibit users to directly access `Functorial` methods. Is that "Generics are trivial once we have dependent types" above means, this can also be written in safer and cleaner way with dependent types?
Thanks! Those are good tips. To be clear, I'm aware that the code is not an optimal solution to the longest common substring problem. My question was more 'Why is ghci so fast?' than 'What can I do to make the compiled code faster?' The GC point is interesting. Is the GC 'turned off' or otherwise hobbled in ghci? I tried running the compiled code with `stack exec -- ros-test-exe +RTS -I0 -A100G -RTS` to try to disable the garbage collector entirely and the runtime didn't change.
I don‚Äôt know enough about GHCi gc to answer that sorry. But I‚Äôd expect it to be far more primitive than the one written in C/stg.
Yes indeed. The idea is to inspect the actual type definition instead of relying on some details of how unification works. 
Wow, it's so fast!
I'm explicitly _not_ asking for any control from downstream packages. All I'm asking for is a stated policy that tools besides Cabal and cabal-install are considered proper downstream components, and their needs be at least considered in making ecosystem changes.
Personally: I'm in favor of using the PVP for signaling breakage. I'm opposed to preemptive upper bounds, because I believe their value can be captured much more easily with a separate, explicitly mutable database of known good builds. I've detailed this in [The true root of the PVP debate](https://www.yesodweb.com/blog/2015/09/true-root-pvp-debate). Some people are opposed to the idea of semantic versioning in general. It's not my position, but I believe it comes down to the fact that, in a strongly/statically typed language like Haskell, the vast majority of breakage will be caught by the type system, and for the semantic changes, human error will give us a terrible signal-to-noise ratio anyway. As I said, it's not my position, and someone who _does_ hold it could represent it better than I do. To back up the "human error" part of things, consider how easy it is to [accidentally break compatibility](https://github.com/haskell/pvp/issues/22) in some cases.
&gt; Regarding 3) "Downstream projects" my understanding was that the understanding requested is already part of the mission statement of the ghc devops group? https://ghc.haskell.org/trac/ghc/wiki/DevOpsGroupCharter -- In particular, since hackage is coupled to cabal, and cabal is coupled to ghc, then the general concerns there flow downstream "naturally." Just because projects are somehow coupled does not mean they have the same mission statements. And there has explicitly been [pushback on issues around Hackage](https://github.com/commercialhaskell/all-cabal-files/issues/8) in the past stating that it works for cabal-install, end of story, implying that Hackage does _not_ consider other tools proper downstreams. If the intention is that Hackage and Cabal respect other projects as proper downstreams, why not say so?
Generally speaking, I agree that discussions on ideas that haven't had the time to properly gestate won't go anywhere productive, however in this specific scenario things don't seem quite as clear-cut. From what I can see, the caret operator was added in [Cabal pull request #3705](https://github.com/haskell/cabal/pull/3705). Above you say it was "fully discussed and documented", but there doesn't seem to be any reference to said discussion in either the implementing PR or [Cabal's readthedocs page](http://cabal.readthedocs.io/en/latest/index.html). I think it would have been better to propose adding the caret operator (i.e. `^&gt;=`) to Cabal's syntax alongside a roadmap for widening its use beyond simple sugar at some point down the line. From an outside perspective, right now it looks as if a breaking syntactic addition was made to Cabal's syntax without a clear path for how it might be used in the future. This is especially pertinent given that the time between the operator being put into production and the suggestions around widening its scope was about a few months. Again, from the perspective of a non-contributor trying to stay informed about changes to the build tooling, it looks like with a little more forethought and public discussion a lot of the questions that have been cropping up could have been publicly raised and answered.
I would propose that it only be marked broken _for the dependency solver_. If an existing build plan already has that version pinned, it would be unaffected.
As an example: support that `bar-1.0.0` says `build-depends: foo &gt;= 1.0`. Today, I'm working on a project which has pinned `foo-1.0.0` and `bar-1.0.0`. Everything works just fine. Tomorrow someone releases `foo-1.1.0`, which breaks `bar-1.0.0`. By one viewpoint, `bar-1.0.0` is broken, since it will allow a build plan with `foo-1.1.0` and then break. However, for my existing pinned versions, everything is fine. The revisions approach says that we should go back and change the dependency info in `bar-1.0.0` to indicate that it doesn't work with `foo-1.1.0`, and then everything is correct. Versioning pinning will still work (unless someone adds an unnecessarily strict constraint, which does happen on occasion). And the dependency solver will be able to continue working. However, I also think that allowing patch releases to hide early minor versions is a better approach, in that it doesn't require any kind of revision, works for pinning, and works for the dep solver.
I don't know where this meme of Haskell has no side effects came from.
https://xkcd.com/1312/
Not sure this will help, but anytime you run into a difference between ghc and ghci it's always worth looking into the difference between type defaults. In order to make things easier to use at the ghci prompt it uses relaxed rules for among other things monomorphism. Verify the types of everything, I'd bet ghci is being looser on some of the types than ghc is. Not sure why that would be faster, but it's really the only difference I can think of. Maybe someone who knows ghc a bit better can share the proper pragmas to bring the two environments closer together?
To me it seems like mandatory PVP (or any versioning scheme, for that matter) would solve this. A public package should not be allowed to say "I want version 1.0.0.0 instead of 1.0.0.1," because if PVP is being followed, that clause is saying "I don't want the bug fixes." For a public package, that should not be an allowable thing to say.
I liked the UTC+14 time zone one!
I was messing with the code you posted. On `data Foo b a = Foo (Either [a] b)`, which `Generic1` can't handle even with (:-&gt;:), it worked. Unfortunately, I found it break under polymorphic recursion. (There are some polymorphic recursion test cases in generic-deriving.) So, maybe both approaches have its own strength. data Weird a = Tip a a | Nest (Weird (Weird a)) deriving Generic *Main&gt; :t (gfmap :: (a -&gt; b) -&gt; Weird a -&gt; Weird b) &lt;interactive&gt;:1:2: error: ‚Ä¢ Reduction stack overflow; size = 201 When simplifying the following type: GFunctorial a b (M1 C ('MetaCons "Tip" 'PrefixI 'False) (S1 ('MetaSel 'Nothing 'NoSourceUnpackedness 'NoSourceStrictness 'DecidedLazy) (Rec0 (Weird (Weird (Weird (Weird (Weird (Weird (Weird
Yeah I don't see why inhibition wasn't just a wire returning `Maybe`
The difference between inhibition and returning `Maybe` is that you can more cleanly instantiate the `Alternative` and `MonadPlus` classes, or the arrow equivalents of the above.
Look out! I can factor primes in constant time!
Actually, I had come to rely on the idea of inhibition in my project for reactive agents. Each agent had a number of wires running in parallel, each responding to various behaviors signals coming in from the environment. On each step, a worker thread would apply the appropriate behavior signals to the agent, then check if there was at least one non-inhibiting wire. If they all inhibited, the agent would be deleted. 
I keep struggling to understand the [confusing situation](https://www.reddit.com/r/haskell/comments/7ki8jm/haskell_package_management_workflow_annoyances/drezblv/?context=3) with bounds. There appear to be different conflicting opinions (only lower bound, only upper bound, no bounds, or some combination). Why can't we standardize on one versioning policy that fits everyone's needs?
But you are in fact explicitly asking for control from downstream packages. You are in fact stating that GHC should never change in a way that breaks your tools. How is this not the very definition of asking for control? You are asking for a status that not even the Linux kernel has over GCC. And the way this is presented makes it seem like we go out of our way to break your stuff. It is completely unreasonable for me to have to vet my changes by compiling and running stack. I'm not a stack developer. Cabal, cabal-install and haddock are usually updated because they serve a purpose when building the compiler for which stack is wholly inadequate. If you decided to take a dependency on an upstream project, it is your responsibility to track changes to said project.
Packages don't have single plans. The solver picks a plan in the context of the sum total of all packages it seeks to solve. So a package might work perfectly well in one plan (because another package in the plan induces a constraint set that prevents an incompatible dep from being picked), but not work in another plan.
"He has a compelling narrative that all this airing out of the dirty laundry is necessary to get things moving." I think history has shown that this narrative is far from compelling. Let's leave it at that.
A PR to hackage to clarify that it is fine to have organizational accounts would be welcomed.
&gt; From what I can see, the caret operator was added in Cabal pull request #3705. Above you say it was "fully discussed and documented", but there doesn't seem to be any reference to said discussion in either the implementing PR or Cabal's readthedocs page. Cabal's manual actually has an extensive documentation of the syntax: http://cabal.readthedocs.io/en/latest/developing-packages.html?highlight=^&gt;=#pkg-field-build-depends &gt; a breaking syntactic addition was made to Cabal's syntax without a clear path for how it might be used in the future This was not a breaking addition. Files absent the syntax work fine. It is just an _addition_ and as such necessarily not backwards-compatible when used. At the moment it is just sugar, and the discussion on its addition was just about the usage of it as sugar. There is a perfectly fine usage for it _now_ which justifies it fully. The only question is how it might evolve in the future, which we can discuss when we are ready.
The "unbounded period" is likely to be a month or so at most. The whole point of the phased proposal is so that the first element can be implemented almost immediately. The ability to accept the field already exists, since it is an `x-` field. So the entire first phase is just changing the upload guidelines to reflect this. Which is the _exact same_ set of upload guidelines that you propose to be changed in another fashion. That's why I think we should just do the uncurated thing -- the changes are "conflicting" in a "git merge" sense.
&gt; We already have the caret operator. How can we now say that there's not a full proposal ready for it? I've explained this five times on this thread. We have the caret operator and it makes perfect sense as is. There is no full proposal for its potential future use and evolution. Note that the caret operator was in motion well before the ecosystem-proposals repo existed. I think that the future evolution of the operator, as discussed multiple times in this thread, absolutely belongs in that repo, as probably do other future plans, which we will have to figure out going forward. I think encouraging people to make use of it is a good thing, and this pattern should be continued.
Because, programmers.
Also, note that you can in fact add multiple Hackage accounts as maintainers for a package, so the take over process linked by /u/angerman isn't really required in a company setting, since you can just make it company policy to add all relevant employees as maintainers on company packages, that way any employee has admin rights over the package.
I actually think it is likely we will eventually see a Haskell CVE that is fairly similar to those we usually see in unsafe languages (i.e. not related to Missouri). The reason is that Haskell programmers generally are forced to use unsafe operations if they want *benign effects*. Let me explain. Consider a library like [bytestring](https://hackage.haskell.org/package/bytestring). For (good) performance reasons, it [https://github.com/haskell/bytestring/blob/master/Data/ByteString.hs#L792](uses `unsafePerformIO`) in a few places. Are those uses correct? I don't know, but probably, given the maturity of bytestring. What are the precise rules under which `unsafePerformIO` is safe? They seem pretty complicated. And if `unsafePerformIO` is used incorrectly, we risk not just writing a "function" that has side effects; rather we may [violate type- and memory safety](https://gist.github.com/ppetr/3693348). This means that a bug may not just lead to aberrant behaviour, but still have the program stay within the language semantics, it may cause the program to escape the language semantics entirely, and perhaps even have exploitable behaviour. In contrast, consider an impure functional language like Standard ML or OCaml. In these languages, it is idiomatic to write "pure" functions like in Haskell, but their purity is mostly a matter of convention - they may intentionally use side effects for performance reasons, and if the programmer screws this up, the result may be a bug. However, since these effects (notably, mutable references) are *safe* with respect to the language semantics, the result is never a potentially exploitable violation of type safety. It's an interesting situation: Haskell provides more powerful guarantees, but you have to be *extremely careful* when you break them, while Standard ML promises less, keeps those promises. Now, in Haskell's defence, while `unsafePerformIO` is more widespread than I'd like, it's nowhere as common as mutable references are in Standard ML. Further, the [Safe Haskell](https://ghc.haskell.org/trac/ghc/wiki/SafeHaskell) mechanism provides a way to indicate that your code is not using any of these potentially dangerous tricks. Also, Haskell provides pretty good support for abstracting away these dangerous uses in a few, hopefully carefully vetted, places. The [ST](https://hackage.haskell.org/package/base-4.10.1.0/docs/Control-Monad-ST.html) monad is a good example of this - it's essentially just an `unsafePerformIO` wrapper, but it carefully exploits phantom types to ensure that the use is safe (I guess? I'm not sure how fully specified the semantics of `unsafePerformIO` are).
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [haskell/bytestring/.../**ByteString.hs#L792%5D(uses** (master ‚Üí ed48e3d)](https://github.com/haskell/bytestring/blob/ed48e3d972c5718f7a4f5802a253ba1d93357bf0/Data/ByteString.hs#L792%5D(uses) ---- 
It seems that your comment contains 1 or more links that are hard to tap for mobile users. I will extend those so they're easier for our sausage fingers to click! [Here is link number 1](https://hackage.haskell.org/package/base-4.10.1.0/docs/Control-Monad-ST.html) - Previous text "ST" ^^Please ^^PM ^^/u/eganwall ^^with ^^issues ^^or ^^feedback! 
Technically a lot if ByteString's internals use [accursedUnutterablePerformIO](https://github.com/haskell/bytestring/blob/master/Data/ByteString/Internal.hs#L566). It's really worth it to read that doc comment and it also links to a couple bugs. The main differences between this and unsafePerformIO is that it doesn't black hole eagerly and allows inlining. It presumably also breaks horrendously for cafs? The rules aren't too complex, though. It is safe to run referentially transparent IO that may be partially aborted or started multiple times. The main points are: - No eager black holing means that multiple threads can start the action simultaneously - The lack of NoInline means that ghc can do full laziness and cse, E.g. Memoizing or aliasing your buffers It is safe to read from memory using accursedUnsafeUnutterableIO but you should check the core for anything beyond that.
&gt; The rules aren't too complex, though. It is safe to run referentially transparent IO that may be partially aborted or started multiple times. The main points are: What is the rule that prevents breaking type safety through IOVars? Clearly the rule could be "just don't do that", but I'm not certain whether there are other ways similar problems could sneak in.
For what it's worth with GHC-HEAD it's just as slow in GHCi. I condensed it into a self contained gist here: https://gist.github.com/AndreasPK/8e6f0cbf253f0930f4cda81e685ac136
Can you actually combine diacritics with emoji under Unicode? I‚Äôve never thought about it but you probably can?
That's really weird. For me, O0 and O1 differ significantly.
In my pc, -O0 performs similarly to GHCi. $ stack clean &amp;&amp; stack --silent build --ghc-options '-O0' &amp;&amp; \ time stack exec ros-test-exe real 0m4.575s user 0m4.648s sys 0m0.104s $ stack clean &amp;&amp; stack --silent build --ghc-options '-O1' &amp;&amp; \ time stack exec ros-test-exe real 0m59.197s user 0m59.291s sys 0m0.096s (Stack did not recompile library modules when `--ghc-options` option overrides defaults. I don't know why, but `stack clean` should and did work.)
The rule forbids you to write or allocate memory. Allocating internally is unsafe because the function can abort part way through without exception. Writing internally is impossible because you can't allocate internal memory to write on. That basically leaves only reading memory and I don't think it's possible to break type safety with that? Though you are right that something like this foo :: a -&gt; b foo a = accursedUnutterablePerformIO $ do writeIORef t [a] [b] &lt;- readIORef t return b where t :: IORef [a] t = unsafePerformIO (newIORef []) Does follow those rules but I'd argue that the unsafePerformIO part is unsafe, not the accursedUnutterablePerformIO.
In a sense, yes. In another sense, I was reading this chain (but more generally, the whole topic of discussion) as about policies regarding _core ecosystem infra_. So I took the above comment about Cassava as somehow attempting to relate it to the broader topic -- core ecosystem infra. I.e. the claim was made that it "seems emblematic of something, even if it's not completely clear what". But how can it be emblematic of something regarding core ecosystem infra if the package is itself _not_ part of that infra.
My next question is this. What's the advantage of the free monad over using typeclasses all the way, like in my [gist](https://gist.github.com/louispan/25a93676ea07de3fd542163cf59a1c64) above? Doesn't using typeclasses still let you test different behaviour? And separate the "program" from the implementation?
[I use continuations fairly heavily in dejafu](https://github.com/barrucadu/dejafu/blob/master/dejafu/Test/DejaFu/Conc/Internal/Common.hs#L112), for my testing implementation of concurrency. The typeclass instance [looks like this](https://github.com/barrucadu/dejafu/blob/master/dejafu/Test/DejaFu/Conc.hs#L119). This is mostly for historical reasons: I didn't understand free monads when I wrote the first iteration of this code back in 2015, but I knew enough about continuations and existential types to cobble together what I have there. But it's worked pretty well: continuations make it fairly simple to implement exceptions, for example. I don't use `ContT` however, as I slightly break the monad laws to make some infinitely looping programs more defined than they otherwise would be. eg: example = forever (return ()) This runs forever. dejafu uses an unlawful definition of `pure`/`return`, so it can be interrupted after a given number of iterations. But this is limited to monadic actions, it can't interrupt a loop in something with no effects.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [barrucadu/dejafu/.../**Common.hs#L112** (master ‚Üí c2a178b)](https://github.com/barrucadu/dejafu/blob/c2a178bd86f423e65f0bde62a5bc72c017d1afeb/dejafu/Test/DejaFu/Conc/Internal/Common.hs#L112) * [barrucadu/dejafu/.../**Conc.hs#L119** (master ‚Üí c2a178b)](https://github.com/barrucadu/dejafu/blob/c2a178bd86f423e65f0bde62a5bc72c017d1afeb/dejafu/Test/DejaFu/Conc.hs#L119) ---- 
&gt; T.isInfixOf Looks like `isInfixOf` has an `INLINE` pragma.
IIRC /u/Iceland_jack discovered that the `Freer` monad, which is used instead of `Free` by extensible effects libraries due to the asymptotic complexity issue among others, *is* in fact just continuations in disguise.
Blocks are needed so that there is a canonical record of which transaction happened before another. Without knowing which transaction came before another you can spend the same coin twice. Without blocks to prevent the same coin from being spent twice you would have to wait until the coin that was spent is spent by the recipient and recipients of recipients until it is hidden under enough proof of work so that no can rewrite the past by claiming a different order of events. Waiting until your recipient spends is time consuming and more importantly impractical because you cannot expect the recipient to trust that you won't pay that same coin to another person.
The problem is not that a dependency is too specific, rather that a dependency is not specific enough, and the dependency has released buggy metadata. For example: I have package A, which depends on feature X of package B. Now package B does a new release, changing the semantics of X. Because my dependency was not specified well enough, or the package B did not signal backwards incompatibility in their version number, now the compilation of package A breaks. The solution could be to bump the package B version to specify that it broke backwards compatibility. Unfortunately, the buggy package B had already been published, so it will forever be visible to the solver of package A, and so even if the new version is published, package A will still see the old version and think it is compatible with it. To solve this, hackage has opted to introduce package revisions. I think the better solution would be to pull buggy packages, but obviously since there's no consensus on semantic versioning that might not work out at the moment..
I've stated what I'm asking for: a statement of purpose, or whatever variation on that phrasing you'd like. Nothing more, nothing less.
GHC Proposal: https://github.com/ghc-proposals/ghc-proposals/pull/111
All I can say is that you've put words in my mouth that I did not say. My blog post expresses what I've requested. Your statement of my requests does not.
I got the news from https://www.nextbigfuture.com/2018/02/blockchain-3-0-with-directed-acyclic-graphs-dag-for-ten-of-thousands-of-transactions-per-second.html. Apparently someone is using this approach already: "One project in China looks to be taking a serious swing at being the leader in this space. IoT Chain (ITC) is built on DAG and can handle over 10,000 transactions per second. It has a strong vision, strong community, and is backed by leading blockchain funds like ChainFunder and FBG. IoT Chain has a solid shot at becoming categorized as Blockchain 3.0" 
Thank you for the complement. I am not an expert on `accelerate`, so forgive me if I get something wrong here. I find it clunky in the common use case, where you just need to use an array as a simple primitive, rather then to do some high performance computation on GPU or try to distribute it across many nodes. For example, if we simply try to create an array and than fold over it using `accelerate` instead of `vector` for instance, we now need to worry about the fact that elements must be lifted/unlifted, a scalar is now an element of a singular zero dimensional array, we have to use non-standard classes `Eq`, `Ord`, `FromIntegral` etc. not to mention the fact that we need special `if` and `case` handling. That being said, when you realize that you can run your Haskell program on a GPU, the overhead I mentioned above becomes very insignificant. Moreover, those are beautiful solutions to very complex problems, so that DSL doesn't look "clunky" anymore, especially when compared to an alternative of say writing CUDA code manually :) Some other drawbacks of accelerate for the simple case scenario I can think of: * you need to install at least one external dependency (CUDA, LLVM, etc.) in order to get the full benefit * custom data types with boxed arrays are not an option, for obvious reasons. Despite that `massiv` and `accelerate` try to solve the same problems, namely array processing, for the reasons described above, I think, they serve slightly different purpose. It would be nice to come up with some guide that people can use in order to decide which array/vector library is right for their use case.
I think the initial versions of Haskell had no side effects. Then they created a stream based IO system, and then later the monadic one we have today. That was obiviously before it was used for anything practical.
Yeah, and the longer I think of it, the more convinced I am that it should be `NOINLINE`.
The biggest problem with the DSL I have found for my own use cases is that with `accelerate` you can't keep state on the GPU in between your IO operation (because the API is "pure" only). For example, when you want to render a 3D scene to a raster image and show it on a monitor, you have to re-upload the 3D scene every time between frames. [Original thread here](https://groups.google.com/forum/#!topic/accelerate-haskell/s3FKh7ppe7M). I consider this the biggest blocker for accelerate, because most of my use cases need that.
It would be better if you share your exact code and then people can review and suggest specific improvements. Sometimes it's hard to give general suggestions. And sometimes those suggestions are not clear without underlying example.
All I can say is, I've stated my own conclusion based on the things you have requested in the past, and the way you and others went about requesting them. Regardless of what the posts say. Actions speak louder than words. I mean,yes, English is my 5th language, but this sentence quoted verbatim from your blog post, in big bold letters GHC, Hackage, and Cabal will strive to meet the needs of commonly used downstream projects, including but not limited to Stackage, Stack, and Nix. seems like a demand for control of the compiler. If I'm wrong, I'm wrong. But from my point of view, this is impossible to achieve without meaning the cart is leading the pony. GHC has been for the most part pretty neutral on this. That is, until someone decided that we weren't.
All of these issues tie together very closely. The explicitly stated idea in the Cassava discussions was that breakage to Stack users was acceptable. This attitude was further expressed when the issue of the caret operator breaking build plans popped up\*. So we have an explicitly stated non-interest in letting code work with Stack, and a concrete example of breakage to Stack being considered acceptable. All by a single individual who at least seems to have veto power on issues related to both Hackage and Cabal (see blog post links). This is why I'm asking for both maintainer guidelines and an explicit statement of caring about downstream. If a situation arises where there is a workaround for Stackage/Stack/Nix/something else, I would like to have a concrete mission statement saying "we'd like to work with these downstream projects." I would like maintainer guidelines so I understand who gets to make decisions, how decisions are made, how to know if pull requests have any chance of being accepted, etc. We can't look at each of these topics in a vacuum. There's currently an explicit statement of at least not caring about breaking Stackage and Stack, by at least some members of the Hackage and Cabal teams. A positive statement saying that, ideally, compatibility with those projects is considered a good thing would go a long way towards addressing those concerns. \* Yes, ultimately that was worked around, but only via override by other maintainers of Hackage.
I cannot in good faith do anything of the sort, and I believe most other people feel that way too. There is explicit discussion going on of _some_ plan behind the scenes to change the meaning of this operator, or do something else with it going forward. Perhaps I'll love those plans and support it. Perhaps I'll disagree vehemently. Perhaps it will break Stackage or Stack\*. If the operator as-is had been proposed, I would have been opposed to introducing a breaking change to the cabal file syntax for minor syntactic sugar. The motivation I've heard on both public and private channels has been all about "soft bounds," even though the implementation seems to have nothing to do with that. Like it or not, the messaging around this has been so completely confusing that users (like myself) are rightfully unsure about using this operator. I believe it's high time to get some version of a proposal on the table, or accept that people in general will be wary of the operator. \* Yet another reason why a statement of purpose regarding downstream projects would be a good thing.
I've seen these "just a month" kinds of issues drag out _many_ times before. I do not believe we should run our infrastructure in this way, it leads to unexpected delays throwing massive wrenches in everything. Is there something concretely wrong with the change in wording I've requested? All it does is clarify the status quo today.
If I understood the maintenance process by which decisions were made on Hackage and Cabal, and the people with ultimate decision making power had either acted in a way demonstrating that they consider the needs of Stackage, Stack, Nix, etc as important, or made a positive statement in that direction: that would be fine. I'd still favor an explicit statement, because we should be clear about goals. And I'd argue a simple paragraph in the `README.md` file in the respective repos is all that's needed. I could send a PR for both of these in under 10 minutes. My concern is that there is _not_ a clear decision making process. We've seen issues discussed with one maintainer and approved for work, and then PRs rejected by other maintainers. We've seen PRs silently rejected with other implementations by maintainers. We've seen open statements of hostility towards support for Stackage and Stack by maintainers (albeit in different projects). All of these make the current situation more than a little worrying. This is why I'm requesting this small statement of purpose. Assuming that downstream users are actually considered valuable by the projects (which your comment here implies), I don't understand why this would be controversial.
Ya, unfortunately there's no way to truly guarantee a change is following the versioning semantics, so what to do when someone uploads a package that violates the rules in a way we can't immediately detect? I agree it's a hard problem. Still, in the presence of non-malicious actors, I think the vast majority of cases (e.g., adding/removing items from the namespace on a bugfix bump), I don't think it's *that* bad. But maybe it is, I'm not the one who has these problems in my inbox all day, so maybe it's more of a mess than I think.
`check :: FL.LoggerSet -&gt; CFG.Config -&gt; String -&gt; String -&gt; IO () check loggerSet cfg base target = do logInfo loggerSet $ "checking " ++ base ++ "/" ++ target maybeApiKey &lt;- CFG.lookup cfg (T.pack "api-key") maybeSecretKey &lt;- CFG.lookup cfg (T.pack "secret-key") maybeMinAmount &lt;- CFG.lookup cfg (T.pack "amount") if isNothing maybeApiKey then do logInfo loggerSet "failed to load api-key" else if isNothing maybeSecretKey then do logInfo loggerSet "failed to load secret-key" else if isNothing maybeMinAmount then do logInfo loggerSet "failed to load min-amount" else do let (Just apiKey') = maybeApiKey let (Just secretKey') = maybeSecretKey let (Just minAmount) = maybeMinAmount let settings = RequestSettings { apiKey = apiKey', secretKey = secretKey' } maybeBaseAmount &lt;- loadBalance settings base maybeTargetAmount &lt;- loadBalance settings target maybeTargetPrice &lt;- loadPrice base target if isNothing maybeBaseAmount then do logInfo loggerSet "failed to load base amount" else if isNothing maybeTargetAmount then do logInfo loggerSet "failed to load target amount" else if isNothing maybeTargetPrice then do logInfo loggerSet "failed to load price" else do let (Just baseAmount) = maybeBaseAmount let (Just targetAmount) = maybeTargetAmount let (Just targetPrice) = maybeTargetPrice let targetValue = targetAmount * targetPrice logInfo loggerSet $ "available base: " ++ showDouble baseAmount ++ " " ++ base logInfo loggerSet $ "available target: " ++ showDouble targetAmount ++ " " ++ target logInfo loggerSet $ "target value: " ++ showDouble targetValue ++ " " ++ base ++ " at price " ++ showDouble targetPrice ++ " " ++ base candleSize &lt;- CFG.lookupDefault "5m" cfg $ configGroupKeyName base target "candle-size" let maybeRange = toRange candleSize if isNothing maybeRange then do logInfo loggerSet "failed to load candle-size" else do let (Just range) = maybeRange if targetValue &gt;= minAmount then do checkBuyOrSell loggerSet cfg settings base target range targetAmount sell Sell else if baseAmount &gt;= minAmount then do let amount = maxBuyAmount baseAmount targetPrice checkBuyOrSell loggerSet cfg settings base target range amount buy Buy else do logInfo loggerSet "nothing to do"`
`check :: FL.LoggerSet -&gt; CFG.Config -&gt; String -&gt; String -&gt; IO () check loggerSet cfg base target = do logInfo loggerSet $ "checking " ++ base ++ "/" ++ target maybeApiKey &lt;- CFG.lookup cfg (T.pack "api-key") maybeSecretKey &lt;- CFG.lookup cfg (T.pack "secret-key") maybeMinAmount &lt;- CFG.lookup cfg (T.pack "amount") if isNothing maybeApiKey then do logInfo loggerSet "failed to load api-key" else if isNothing maybeSecretKey then do logInfo loggerSet "failed to load secret-key" else if isNothing maybeMinAmount then do logInfo loggerSet "failed to load min-amount" else do let (Just apiKey') = maybeApiKey let (Just secretKey') = maybeSecretKey let (Just minAmount) = maybeMinAmount let settings = RequestSettings { apiKey = apiKey', secretKey = secretKey' } maybeBaseAmount &lt;- loadBalance settings base maybeTargetAmount &lt;- loadBalance settings target maybeTargetPrice &lt;- loadPrice base target if isNothing maybeBaseAmount then do logInfo loggerSet "failed to load base amount" else if isNothing maybeTargetAmount then do logInfo loggerSet "failed to load target amount" else if isNothing maybeTargetPrice then do logInfo loggerSet "failed to load price" else do let (Just baseAmount) = maybeBaseAmount let (Just targetAmount) = maybeTargetAmount let (Just targetPrice) = maybeTargetPrice let targetValue = targetAmount * targetPrice logInfo loggerSet $ "available base: " ++ showDouble baseAmount ++ " " ++ base logInfo loggerSet $ "available target: " ++ showDouble targetAmount ++ " " ++ target logInfo loggerSet $ "target value: " ++ showDouble targetValue ++ " " ++ base ++ " at price " ++ showDouble targetPrice ++ " " ++ base candleSize &lt;- CFG.lookupDefault "5m" cfg $ configGroupKeyName base target "candle-size" let maybeRange = toRange candleSize if isNothing maybeRange then do logInfo loggerSet "failed to load candle-size" else do let (Just range) = maybeRange if targetValue &gt;= minAmount then do checkBuyOrSell loggerSet cfg settings base target range targetAmount sell Sell else if baseAmount &gt;= minAmount then do let amount = maxBuyAmount baseAmount targetPrice checkBuyOrSell loggerSet cfg settings base target range amount buy Buy else do logInfo loggerSet "nothing to do"`
A statement of purpose isn't such a small thing. If a purpose is stated, then it must influence the actions people take; or else it was hardly a purpose. I don't necessarily think this is a bad thing; GHC ought to be controlled at some level by popular interests. But I think you have to admit that this statement of purpose would be utterly meaningless if it didn't also imply a certain level of control from downstream.
&gt;the only difference between \[the PVP and SemVer\] seems to be the number of components in the version number I see this stated frequently. It's not true. The number of version components is *a* difference between the two versioning schemes, but it's not the *only* difference. I outlined the differences, along with reasons why I prefer SemVer, in [this blog post](http://taylor.fausak.me/2016/12/28/problematic-versioning-policy/). Another problem not mentioned in the blog post is that the PVP is actually two things: 1. A policy for how to version Haskell software. 2. A policy for how to constrain the versions of your dependencies. SemVer is only the first thing. The second thing is where most of the disagreement comes from. 
Would it influence actions? Sure, that's the idea. I don't see how that leads to "control from downstream." Nowhere have I said, implied, or even wanted, a situation like: * Downstream users can force an upstream to drop a feature * Downstream users can demand that upstream implement something I have stated as clearly as I can what I'm requesting. I'm happy to clarify points. But I object to having words put in my mouth that I never said. For a contrasting example, consider Linux and userland. Linux explicitly states (and Linus religiously enforces) the idea that you _do not break userland_. That's a statement of purpose, or a mission statement, or whatever you want to call it. It does not at all imply at all that GNOME "controls" Linux. To be clear: I'm not asking for something nearly as extreme as "never break users of GHC/Hackage/Cabal." We don't have the same stability guarantees as the Linux kernel. I'm just giving an example of a statement concerning downstream which cedes no control of the project.
Mostly agree with this. I'm hesitant to use ^&gt;= until we know what the secret plans to make more impressive use of it are. I know what it means for cabal today, but it has been stated that there are some kind of future plans for it that involve more than that. That said, I do take issue with your comment that it was a breaking change. It broke no existing cabal files or builds. I don't see how you're claiming it was a breaking change.
https://pvp.haskell.org/faq/#how-does-the-pvp-relate-to-semantic-versioning-semver
I think you're reading more into the word "strive" than I would. I don't mind changing the wording. Strive here just means "attempt" or "try," not "must do so." GHC _has_ been a neutral part in this, I don't think I ever implied otherwise. I included GHC here because it's part of the Haskell upstream toolchain. This should further clarify that this request for clarification in intent isn't just about problematic areas. GHC has had no issues with in this dimension at all. I still think it worthwhile to explicitly state that it is intended to meet the needs of various downstream projects. In your opinion, is trying to meet the needs of downstream projects somehow problematic for GHC, Hackage, and/or Cabal?
&gt; I don't understand why this would be controversial. To be fair, I think there has been sufficient explanation in these threads that you should at least understand the opposing view, even if you don't agree. Others perceive this statement of purpose as a statement of control from downstream, and don't like that inversion of typical control. Whether or not you agree is a different story (I'm not sure I agree either). But I do think it's clear why it's a touchy subject.
It's a breaking change because previous versions of Cabal-the-library can no longer parse files that include `^&gt;=`. A non-breaking change would be, for example, the addition of a field `builds-with-packages: foo-1.1, foo-1.2, bar-1.5, ...`. It prevents older tooling from reading the files.
I've also been pretty clear that I'm not married to any specific wording, and I'm more than happy to hear someone express this in a different way that doesn't imply inversion of control to them. Is there some other phrasing that you or someone else want to recommend? If so, I'd be happy to hear it.
I think Linux -&gt; userland is a bit of an exceptional instance, and an equally interesting case is the one by /u/Phyx about GCC not being beholden to the buildability of the kernel, or glibc frequently breaking ABI, which serve as equally exceptional instances showing the other attitude in play. But I think I see your distinction now, and I think it'd better be made clearer. Your desired statement speaks only of stability, and nothing of additive / destructive changes. Is this correct? A statement of *stability* for downstream project would be *much* easier to get behind.
Commented with such a suggestion in the other thread :)
Ah I see. I'd argue that `cabal-version: &gt;=2.0` (which I believe is technically required for any file containing `^&gt;=`) is sufficient to allow these tools to fail correctly. I'd really rather not see old tools trying to guess or ignore new syntax like `builds-with-packages`.
I think there is some confusion here with respect to breaking changes around the caret operator. The operator itself was an addition to the Cabal library. It was backwards compatible in that it did not break or change the meaning of any existing Cabal files. However it did end up breaking Stack simply by being used: [https://github.com/haskell\-infra/hackage\-trustees/issues/120](https://github.com/haskell-infra/hackage-trustees/issues/120)
To format a code block here, add four spaces to the beginning of every line (the \`backticks\` only work within a single line). Also, you can edit the code into your original post, rather than adding it as a separate comment.
&gt; Downstream users can force an upstream to drop a feature But it already has, e.g. `&gt;=^`, so how can you honestly say that when you know the feature has essentially been blocked because of down stream. Linux &gt; userland is a very different case than GHC &gt; cabal or stack. The better comparison would be glibc and the rest which depend on it.
&gt; Is anyone aware of ways around this tradeoff? [mtl](hackage.haskell.org/package/mtl)
&gt; Is this correct? A statement of stability for downstream project would be much easier to get behind. We already strive for stability. But such a statement would preclude making changes that are good for the compiler. As an example, on Windows I have long sat on finishing making dynamic linking possible. Because while it would be good for the compiler, I dread and don't want to think about the dust up I know I'll get from stack folks because it would be hard to support in their use case. So such statements are the very thing I'm hesitant about.
Unfortunately, currently, Cabal-the-library cannot distinguish between "invalid cabal file" and "cabal file with too new a cabal-version field," which would be necessary to support this well. I understand that such functionality is in the works. For better or worse, new syntax _is_ currently ignored (with a warning) by older tools, such as the `custom-setup` stanzas.
How exactly has `^&gt;=` been blocked at all by downstream? The operator has been introduced, it's being used (even by `base`), and there are plans being made to further extend its meaning (which are still behind closed doors). No one in the Stackage or Stack teams has, to my knowledge, been consulted about the operator, had any say in its introduction or usage, or anything of the sort.
A test to know it a number is prime in O(logn) or even O(n) would actually be quite useful.
&gt;It wasn't a previously succeeding build that broke. That's exactly what it was, though. Running \`stack \-\-resolver nightly\-2017\-12\-01 setup\` worked with Stack \&lt; 1.6.1, then it stopped working when the new \`integer\-gmp\` was uploaded on December 4. This Stack issue \(linked from the Hackage trustees issue\) goes into more detail about the situation: [https://github.com/commercialhaskell/stack/issues/3624](https://github.com/commercialhaskell/stack/issues/3624)
Right, yea I didn't mean to imply *perfect* stability. But a statement that breaking changes will be avoided when avoidable and reasonable would be nice. The `integer-gmp` thing, for instance, should have been avoided altogether. There will of course be cases where it's just not reasonable, or it's more effort than it's worth. I think we should strive to allow GHC to develop aggressively though. If there were any kind of practical advantage to using `^&gt;=` in `integer-gmp`, I think it would have been better to let that happen than to prevent it because of Stack.
Hm. Well that's frustrating. Stack doesn't support `custom-setup`?
As a side note, cabal-install had a very similar issue, which was worked around by changing the contents of the `00-index.tar.gz` file: https://github.com/haskell/cabal/issues/4624
Stack does now, since it moved over to a version of the Cabal library that supports it. My point is that older versions of both Stack and cabal-install will ignore it and provide no support. You can argue this is good (better backwards compat) or bad (non-deterministic build plans). I'm just stating that it's the way the Cabal spec works today.
Oh I didn't know that it was due to `integer-gmp` being properly uploaded to Hackage. However, *that* of course is due to files on Hackage being allowed to differ from files in GHC, not `^&gt;=`. Still a very frustrating issue -_-
If you or someone else want to champion a policy around "avoidable and reasonable," I'm all for it. I'm simply not going to push that hard.
I noticed you have some experiments on SIMD in one of the branches. Did it work? Or were there problems?
Huh, I assumed it was O(sqrtn) as you can just do trial division, but wikipedia lists it as NP. Is it because testing an arbitrary prime with this method requires arbitrary precision arithmetic, which isn't a primitive op?
Indent code by 4 spaces to create code blocks in reddit (markdown). You could use pattern matching more often. It does the job of both `if` and `let` at the same time and more safely. case (maybeApiKey, maybeSecretKey, maybeMinAmount) of (Nothing, _, _) -&gt; logInfo loggerSet "failed to load API key" (Just _, Nothing, _) -&gt; logInfo loggerSet "failed to load secret key" (Just _, Just _, Nothing) -&gt; logInfo loggerSet "failed to load min amount" (Just apiKey, Just secretKey, Just minAmount) -&gt; do let settings = ... ... &lt;- ... ... Second, you can use the `MaybeT` transformer here so error handling doesn't push your code to the right. logInfo' :: MonadIO m =&gt; FL.LoggerSet -&gt; String -&gt; m () logInfo' logger msg = liftIO $ logInfo logger msg logFail :: (MonadIO m, Alternative m) =&gt; FL.LoggerSet -&gt; String -&gt; m a logFail logger msg = do logInfo' logger msg empty -- failure check ... = void . runMaybeT $ do logInfo' loggerSet $ ... -- 10 lines of code in 3 apiKey &lt;- MaybeT (CFG.lookup ...) &lt;|&gt; logFail "failed to load API key" secretKey &lt;- MaybeT (CFG.lookup ...) &lt;|&gt; logFail "failed to load secret key" minAmount &lt;- MaybeT (CFG.lookup ...) &lt;|&gt; logFail "failed to load min amount" let settings = ...
`n` is the length of the number in bits, not the number itself, so trial division is `O(2^0.5n)`. There are algorithms which work in polynomial time in regards to the length of the number, which means that it is strictly within P. As for trial division, you could perform integer division with a remainder and avoid arbitrary precision arithmetic. Your precision would be bounded by `kn` for some fixed `k`
I think he meant that core packages have been blocked from using `^&gt;=` by downstream. A change that the GHC devs would have liked to make cannot be made because of downstream. That's not unreasonable, but it's a question of in which cases this is appropriate. There will inevitably be changes that really ought to be made, even though they break stack.
Downstream \(Stack, Nix, ...\) is not asking for any control over upstream \(GHC, Hackage, ...\). Downstream is asking for an official policy stating that breaking stuff unnecessarily is bad. That seems pretty benign to me. In fact, SPJ has said: &gt;[GHC should strive to make life as easy as possible for downstream tools](https://ghc.haskell.org/trac/ghc/ticket/14558#comment:48)
`^&gt;=` has been reverted from the package has it not? even though it clearly was stack that was parsing something it shouldn't be. And afaik, this also broke `cabal-install`. but that was just fixed and people went on with their day. It broke LTS for some reason (I always assumed packages are checked before being chucked into LTS but ok, and base works because you had a patch to work around it if I recall). &gt; No one in the Stackage or Stack teams has, to my knowledge, been consulted about the operator, had any say in its introduction or usage, or anything of the sort. Stack is a consumer of Cabal, it's downstream from Cabal. It's not Cabal's responsibility to inform Stack of anything. I don't see why you think it is... The feature wasn't debated in private. The code wasn't pushed in by a back door. Stack developers have *chosen* not to pay attention to Cabal mailing lists and GitHub pull requests. Why is Stack not pro-active instead of re-active? In my opinion, Stack developers shouldn't be consulted at all, Stack developers should however be considered when they engage in dialog. To clarify. It should be Cabal says "We intend to do this" on their own communication channel. And it's Stack developer's responsibility as a consumer of downstream to say "hey what a minute, this would cause problems for us". I am 100% with you on features shoved in through back door, This should not happen. Ever. Everything should be publicly done and reviewed so reasonable time is given to object. I can even see myself agreeing that core packages shouldn't be changed and immediately uploaded to hackage. There should be a small grace period that code can be build and tested by interested parties. But I don't think the communication and burden should be on upstream. Where would that responsibility stop..
GHC hasn't been blocked from using the operator. A package was released to Hackage, well after the GHC release (weeks or months, I don't remember) which included metadata which differed from what was in the GHC repo. I _have_ requested that there be a grace period of a few months placed on using new Cabal features in Hackage to give tooling a chance to update. This isn't just because of Stackage and Stack. I maintain https://packdeps.haskellers.com, for example, and would love to have a little more breathing room. That request has been rejected. So clearly downstream does not have veto power here.
I'd rather you didn't duplicate the thread that /u/snoyberg and I have already had. I've already been taught the intended meaning of the request :) I do think it's different than the current wording implies though.
Will it be correct if I sumarise (your view) as follows. Adding a lower bound is putting a minimal expectation on the dependency and in some sense necessary while putting a upper bound is being unnecessarily conservative. But other than that something like PVP is okey/required. 
&gt; In your opinion, is trying to meet the needs of downstream projects somehow problematic for GHC, Hackage, and/or Cabal? No, but where we disagree is on how this should be done, and what the result should be when the two can't agree. As my post below explains, this shouldn't involve at all a mail to stack-devel or something to tell them of every plan.
&gt; ^&gt;= has been reverted from the package has it not? even though it clearly was stack that was parsing something it shouldn't be. And afaik, this also broke cabal-install. but that was just fixed and people went on with their day. I don't know anything about it breaking `cabal-install`. `cabal-install` _was_ broken by the addition of the `^&gt;=` operator, and they added a workaround to remove those cabal files from the 00-index.tar.gz file, see https://github.com/haskell/cabal/issues/4624. And yes, that was reverted, but there's nothing preventing GHC from using the new operator. In fact, GHC _didn't_ use the new operator: the cabal file uploaded to Hackage did not match the file GHC was using. &gt; Stack is a consumer of Cabal, it's downstream from Cabal. It's not Cabal's responsibility to inform Stack of anything. I don't see why you think it is... I think at this point you're intentionally misreading my post. You said that downstream has "blocked" upstream. This is a clear demonstration that downstream does not, in fact, have such power. I've expressed in the blog post and here how I think the caret operator should have been handled. &gt; The feature wasn't debated in private. The code wasn't pushed in by a back door. Stack developers have chosen not to pay attention to Cabal mailing lists and GitHub pull requests. I've addressed this quite clearly in my blog post. I believe more attention needs to be drawn to changes with major implications to the ecosystem. I _do_ follow the mailing lists, I regularly review the Cabal issue tracker and pull requests, and this one completely slipped by me. But more importantly: the fact that there was a greater plan for many new added meanings for this operator was nowhere to be found, at least as far as I know.
As if 10,000 people learn about `mtl` each day ;)
It _is_ in P. &lt;https://en.wikipedia.org/wiki/AKS_primality_test&gt;
**AKS primality test** The AKS primality test (also known as Agrawal‚ÄìKayal‚ÄìSaxena primality test and cyclotomic AKS test) is a deterministic primality-proving algorithm created and published by Manindra Agrawal, Neeraj Kayal, and Nitin Saxena, computer scientists at the Indian Institute of Technology Kanpur, on August 6, 2002, in a paper titled "PRIMES is in P". The algorithm was the first to determine whether any given number is prime or composite within polynomial time. The authors received the 2006 G√∂del Prize and the 2006 Fulkerson Prize for this work. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
Again, this is putting words in my mouth which I never said. I gave some clear points in my blog post. Mikhail [commented on my blog post](https://www.snoyman.com/blog/2018/02/haskell-ecosystem-requests#comment-3764905353) with a proposal that would perfectly address my requests. At no point was "email to stack-devel" part of any plan I discussed until you mentioned it here.
Current haskell has no side effects either. IO in haskell is 100% pure.
It really depends on how you define side effects. I can write code which asks the user for two inputs, adds them and prints the result. That is under very reasonable definitions code with side effects. Haskell uses its type system to seperate pure and impure code, so you can reason about the majority of the code as beign pure which is nice, but you still have side effects. Ofcourse due to this distinction, you can define purity such that it only applies to the pure parts of the language, in which case haskell is 100% pure, but that is sort of tautological.
&gt;The Cassava flag issue is unrelated to any of this because it is *not* a core package. To me, [the Cassava flag issue](https://github.com/haskell-hvr/cassava/pull/155) is related because it is a prime example of a core maintainer breaking Stack for no apparent reason and being unwilling to un\-break it. But of course you're right, Cassava is not a core package. For an example of similar behavior with a core package, look no further than [integer\-gmp\-1.0.1.0 needlessly requiring Cabal 2](https://github.com/haskell-infra/hackage-trustees/issues/120) for the caret operator. I know that you are familiar with that issue, but I'd like to provide a summary both to explain it to those that might not be familiar and to explicitly show the problem as I see it: * 2017\-11\-20: [GHC 8.2.2](https://www.haskell.org/ghc/download_ghc_8_2_2.html) is released. * 2017\-12\-04: I notice that base\-4.10.1.0 \(part of GHC 8.2.2\) isn't on Hackage and [ping Hebert about it](https://twitter.com/taylorfausak/status/937673574592208896). * 2017\-12\-04: Herbert uploaded [base\-4.10.1.0](https://hackage.haskell.org/package/base-4.10.1.0) along with some other wired\-in packages, including [integer\-gmp\-1.0.1.0](https://hackage.haskell.org/package/integer-gmp-1.0.1.0). * 2017\-12\-04: Someone notices that Stack fails when using a build plan that used to work and [opens an issue](https://github.com/commercialhaskell/stack/issues/3624). * 2017\-12\-06: I open [a GHC Trac ticket](https://ghc.haskell.org/trac/ghc/ticket/14558) because integer\-gmp doesn't have it's own issue tracker and Herbert suggests that bug reports be sent [directly to him](https://twitter.com/hvrgnu/status/938360575570083841), presumably via email. * 2017\-12\-10: It becomes obvious that the GHC Trac ticket isn't going anywhere, so I open [a Hackage trustees issue](https://github.com/haskell-infra/hackage-trustees/issues/120). * 2017\-12\-10: Herbert [revises integer\-gmp\-1.0.1.0](https://github.com/haskell-infra/hackage-trustees/issues/120#issuecomment-350561879) to remove the caret operator. * 2017\-12\-10: Herbert [blocks me](https://twitter.com/taylorfausak/status/939937217685966848) on both GitHub and Twitter. Through the entire process I tried to be polite and helpful. I feel that the response I got from Herbert was antagonistic and difficult. However I recognize that I of course am biased to favor myself, so I encourage others to read the links I shared and make up your own mind. My larger point is that the Cassava flag issue is relevant because it's indicative of how \(at least some\) core maintainers feel about Stack as a downstream project. 
I did a few experiments that looked very promising, so I do have some ideas and definitely will try incorporating SIMD into massiv in the future. It will require a lot of work, though, so I didn't even think of having support for those in the initial release.
Paper thread: https://www.reddit.com/r/haskell/comments/791qy7/linear_haskell_practical_linearity_in_a/
To be precise, the quadric complexity issue is solved by either using Church-encoding or a fast type-aligned sequence to implement `&gt;&gt;=`. `Freer` alone does not solve the issue.
Great, all the more reason for me to continue my work.
We can all go home now, my computer gets hot when I run a Haskell program which is a side effect :(
Without optimizations we have substringOfAll_aGM = \ s_a1us -&gt; all $fFoldable[] (isInfixOf s_a1us) entries_aGK With optimizations this bloats up to something enormous https://gist.github.com/Tarmean/6bb4cfe47ab735352dbc44c19d85ae7e Having huge functions isn't actually out of the ordinary for the Text and Vector libraries, though, since they use stream fusion which relies heavily on inlining all the things. From what I can tell this is mostly several copies of indices from [Data.Text.Internal.Search](https://hackage.haskell.org/package/text-1.2.3.0/docs/src/Data-Text-Internal-Search.html#indices). No idea how the Text search works, though.
Obligatory: http://conal.net/blog/posts/the-c-language-is-purely-functional
Codensity can also improve the issue.
Just as a note on terminology it is weird to call it side effects. Since it isn't a function but its own type whose purpose is effects. In that sense Haskell has no _side_ effects. 
that would render modern cryptography useless, destroying the entire cryptocurrency quasi-economy as well as basically anything which requires secure communication between two or more computers. (so, for instance, Visa, Amazon, etc.) it would be like the Great Depression or the fall of the Roman Empire.
Do we want to allow memes here? I generally haven't seen many. 
Still I wonder why uninlined `isInfixOf` doesn't suffer from the same slow-down. Probably (hopefully?) some cache effect and not a compiler bug.
I would love to see more material about machines. It seems like a really interesting point in the design space, but a lot of it feels way over my head.
Depends how large the constant is.
Your timeline can be condensed to the fact that things were broken _and then fixed_. Your timing on when you were blocked on GH is also incorrect -- you were blocked earlier -- you just noticed then. You also were subsequently unblocked after it became clear that the GH policy meant this might have an impact on some core issues. But your timeline omits this, conveniently. Also, since the issue was resolved positively, in the way you wanted it, on the _same day_ that the request was made in the proper tracker, i don't see why you're so hung up on this being a negative example? Simply because you think you were polite and herbert was not sufficiently polite in return!?
Your entire argument is you don't believe me when I say I will move on things in a timely fashion. This is exhausting because I have a record of moving on things in a timely fashion, which I am proud of, and the thing that most distracts me from moving on things in a timely fashion is when I have to deal with other conflicting requests that interrupt things.
I'm fine that people are wary of the operator until a proper proposal is presented. I'm not a huge fan of this fact, but I respect this as a reasonable consequence of the fact that a proposal has not yet been presented. You have to understand that everyone has limited time and brainpower, and to embark on writing and discussing that proposal now would get in the way of other things -- like, for example, implementing the uncurated proposal. I personally think, given how much concern you've expressed about the timing of the uncurated proposal being immediate, that I should direct my energy there first. Sorry, as an individual with so many hours in a day, I have to prioritize.
Hmm; maybe 10 years from now GHC will include a totality checker.
The proposal I linked to for candidate branches for ghc-bundled libs is _exactly_ what would provide this grace period, and I support it and want it to happen. That's why I find this whole thing so confusing -- the concrete thing is 90% of the way to being addressed.
&gt; that would render modern cryptography useless, destroying the entire cryptocurrency quasi-economy No it wouldn't. Cryptocurrency depends on ECDLP, not factoring integers. &gt; factor primes in constant time! This is eminently possible. A prime factors as itself. done.
This is not true, I have not said anything about your ability to move timely. This is about the fact that there are many people involved, and historically processes _have_ gone slowly in many cases. Also, as I've expressed elsewhere, the alternative proposals you've expressed in this thread have not 100% overlapped with the requests I've made.
I'm fine with memes if I can look at the image and, without looking at the title or needing some non-language-related knowledge, immediately see why it's relevant to Haskell. (Bonus points if it's actually funny) This post doesn't really seem to hit any of those points for me, unfortunately.
While that's a great proposal, it still does not fully address what I'm talking about, though it does go quite a far way.
It's a very small constant. Can be optimized to a single clock cycle in most platforms, sometimes even less.
Yes, and as I've said, the decision will be made via discussion on the `ecosystem-proposals` github. In this you and I appear to have been in full agreeance from the start!
Yes, it is expected that in the course of a productive discussion that sometimes the outcome is not exactly that originally proposed, but rather something else that addresses the underlying issue in a way that people agree can _also_ work. If your criteria for a good discussion is that everyone 100% agrees to everything you suggest, then that might explain quite a few things...
You did talk about his tone, writing "And even when he did fix it, he did so begrudgingly and made it apparent that he wasn't happy to be making the change." I would suggest that if you don't want to talk about tone, then _don't talk about tone_.
I might be slightly misunderstanding something, but if the possible signals a wire is outputting are known, would it be possible to just add an "empty" signal? Then when all wires are signaling empty, you delete the agent.
Hmm. When I specify -O0 in the package.yaml file under ghc-options and build with `stack clean &amp;&amp; stack build &amp;&amp; time stack exec ros-test-exe`I get the runtimes above. When I build with `stack clean &amp;&amp; stack build --ghc-options '-O0' &amp;&amp; time stack exec ros-test-exe` I get your results. 
(For the previous version of the paper)
&gt; it would be like the Great Depression or the fall of the Roman Empire. I think it would be more like Y2K on steroids.
Hello Haskellers! Is there a nice way of mapping over columns in [Frames](http://hackage.haskell.org/package/Frames-0.3.0.2/docs/Frames.html)/[Vinyl](http://hackage.haskell.org/package/vinyl-0.7.0)? I have pored over the documentation but not found a way of doing it. Maybe something can be done using applicative but it seems unnecessary convoluted. Cheers!
I don't think it would allow for that directly https://en.wikipedia.org/wiki/Uniqueness_type#Relationship_to_linear_typing &gt; A unique type is very similar to a linear type, to the point that the terms are often used interchangeably, but there is in fact a distinction: actual linear typing allows a non-linear value to be typecast to a linear form, while still retaining multiple references to it. Uniqueness guarantees that a value has no other references to it, while linearity guarantees that no more references can be made to a value. I have not read the proposal in detail though, so not sure.
How big? I work on a code base that's ~200kloc and any code aware tools I've used take up huge about of RAM and can cause my editor (Sublime) to freeze up. I've got used to perfecting my grep-fu.
isInfixOf uses `indices` from Data.Text.Internal.Search. indices also has `{-# INLINE indices #-}` even though it won't ever participate in stream fusion. Also, isInfixOf checks for singleton patterns and uses elem which fuses. But indices redoes that check and uses a non-fusing loop?
What do you really need from "erlang like actor model programming". Maybe passing some queues around will do just fine for your tasks.
I think you misinterpreted my statement. By "fully discussed and documented", I was referring to any proposal process and subsequent discussion around the purpose of such an operator prior to anyone implementing it in the linked PR. As best I can tell, any discussion that _was_ had would have taken place in the Cabal mailing list, which would be fine. However any discussion - should it have existed in the first place - isn't linked anywhere in the PR thread or in the documentation. So looking back at the history of events, it looks as if one day a maintainer decided that Cabal's syntax would be greatly improved by the presence of a new operator and just added it. With that context, I'm now concerned that any future changes to Cabal's syntax could be implemented similarly, and that any future changes to tooling in the wider ecosystem around such syntax could as well.
The work is done for that https://github.com/haskell/cabal/issues/4899 Cabal-2.2 will have three outcomes of parsing the file: - Success - Failure - Found `cabal-version:` but it's bigger than I know about, so refuse to do more. Important bit is https://github.com/haskell/cabal/blob/32fea06a1023a507d7dc16b29f542538c0b55e46/Cabal/Distribution/Parsec/ParseResult.hs#L45 runParseResult :: ParseResult a -&gt; ([PWarning], Either (Maybe Version, [PError]) a) running parser gives you list of warnings, and either a value, or a list of errors and possible recognized version. In other words, even parser fails it might give you a `cabal-version`. Importantly extracting of `cabal-version` is among the first things parser tries to do (and for `cabal-version: 2.2` it should succeed even before lexing the file). I hope that `stack` maintainers will start working on adopting `Cabal-2.2` soon. The branch is cut and release will be out hopefully in two weeks: https://mail.haskell.org/pipermail/cabal-devel/2018-February/010416.html
do you have a link? üôÇ Clean IO is not exactly easy to google
&gt; to us onlookers honestly, as an onlooker, I've read through most of these flame wars, and i'm still neutral, even wrt "whose public relations are better". *shrug*
Thanks! This was really helpful. What made you think to try `NOINLINE` on `T.isInfixOf`?
This is an antipattern: if isJust x then do let Just x' = x -- use x' else do -- handle nothing Instead, prefer: case x of Just x' -&gt; -- use x' Nothing -&gt; -- handle Nothing Your code has two points, where it acquires a bunch of `Maybe` values and then stops execution if any of them weren't found. You can replace the nested `if`s with this: apiKey &lt;- case maybeApiKey of Just key -&gt; pure key Nothing -&gt; error "Failed to find API Key" secretKey &lt;- case maybeSecretKey of Just key -&gt; pure key Nothing -&gt; error "failed to find secret key" You might notice that you're doing this a lot, so you might write: exitIfNothing logger maybeValue message = case maybeValue of Just a -&gt; pure a Nothing -&gt; do logInfo logger message error message and now can write: apiKey &lt;- exitIfNothing logger maybeApiKey "no-api-key" secretKey &lt;- exitIfNothing logger maybeSecretKey "no-secret-key"
Using errors can make hoping around maybe either exceptT a lot easier
I identified the inner loop and tried to hamper optimization by disabling inlining of some part.
Ah, that makes sense. I think my misunderstanding stems from the fact that for a lot of structures such as lists the number of bits linearly corresponds to the size of the structure, but for numbers the relation is exponential!
Ah, I'd forgotten that `mtl`-style has the `n^2` instances problem. I think `MonadEffect` style will have the same problem too, I'll have to experiment with it a bit more.
I must have missed something then. I understood your comment &gt; designing a suitable API should be possible as "it is not possible right now but could be made possible". Could you link me to a suitable example that shows how IO can be interleaved with CUDA-based accelerate operations without re-uploading the data? Otherwise, a minimal example that would demonstrate this ability would be: Upload array of 256^3 integers to GPU, and in a loop, read `(x,y,z,value)` from stdin, and add `value` to the the array at position `(x,y,z)`, after each step doing a reduction to print the current maximum of the array to stdout. If this is indeed possible, I'm super excited, because it means I could actually write my programs with accelerate. 
Copied the definitions into the file and it becomes reasonably obvious that the inline pragma on Data.Text.Internal.Search.indices is at fault. The problem is with the buildTable function which traverses the pattern and - builds a bitset kinda-bloom-filter so we can skip letters that don't occur in the pattern - finds the next occurrence of the second-to-last occurence of the last letter in the pattern so we can skip the in-between without having to build a complex transition table This leaves us with something like let buildTable i | done = (# secondLastOccurence, bitSet #) | otherwise = buildTable (i + 1) in let scanText = case buildTable 0 of (# skip, bitSet #) -&gt; ... We REALLY need to inline buildTable so we get jumprec buildTable i | done = let scanText .... | otherwise = jump buildTable (i+1) in jump buildTable 0 but we don't if we have the inline pragma on `indices`. I am not sure why this doesn't happen either way, though, since buildTable is only used once outside of the recursive call so this inlining is always save.
It suggests that conference submission deadlines are a little bit later than I thought (not that it matters much to me, since my advisor does not have the endurance for a pre-deadline all-nighter anyway).
I'm not deep in the ecosystem like many people in this thread are, so I probably don't know all the history, or how all the bad blood started, but to me it is blatantly obvious that hvr is just an asshole. I don't really know any other word that would fit. I didn't want to use that word, but there is no other word I can think of that describes his behavior accurately. He just seems to be opposing fixes and changes just because he can, or just because he dislikes some of the people who are proposing them. His responses to the cassava flag debacle were just ridiculous. Like a pouty child that just wanted to have his way.
While I won't comment on the issues between you and hvr, I don't think his behavior is acceptable as someone who seems to have so much power over packages that are core to the ecosystem. Decisions need to be made based purely on technical merit, not personal grudges, and from reading these exchanges(cassava flag, tfausak's links below), it seems pretty clear to me that he can't be objective. If he is not up to that task, then he needs to step down.
Good bot
What's the recording youtube link, if any?
Can you guys knock it off?
&gt; [When the numbers are sufficiently large, no efficient, non-quantum integer factorization algorithm is known. An effort by several researchers, concluded in 2009, to factor a 232-digit number (RSA-768) utilizing hundreds of machines took two years and the researchers estimated that a 1024-bit RSA modulus would take about a thousand times as long.[1] However, it has not been proven that no efficient algorithm exists. The presumed difficulty of this problem is at the heart of widely used algorithms in cryptography such as RSA.](https://en.wikipedia.org/wiki/Integer_factorization)
**Integer factorization** In number theory, integer factorization is the decomposition of a composite number into a product of smaller integers. If these integers are further restricted to prime numbers, the process is called prime factorization. When the numbers are sufficiently large, no efficient, non-quantum integer factorization algorithm is known. An effort by several researchers, concluded in 2009, to factor a 232-digit number (RSA-768) utilizing hundreds of machines took two years and the researchers estimated that a 1024-bit RSA modulus would take about a thousand times as long. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
I think you can use the length of the hypotenuse ie hypot A B C == hypot B C D Regardless of the order of the sides? Then you need only check two sides are equal for it to definitely be a square.. I think.
It's not clear to me how Free gets around the n^2 instance problem. It seems like it's just too inexpressive to get to the point that you'd need to address it, in the sense that it offers no story for how to compose the effects (Eff and so on being a kind of extension that adds this ability, but still without being as expressive as mtl style). Maybe that's what you meant?
Great job! Want to turn this into a [PR on `text`](https://github.com/haskell/text/pulls)?
This is also compounded by the fact that sometimes people will do their analysis in a way that the input grows linearly for numbers as well, so numbers are not always a 2^n factor (although I think it's a bad idea to write your algorithm analysis in a confusing way like that)
This looks like a pretty complex extension. Are features/extensions ever removed from GHC? Are most extensions designed in such a way that they have only local impact on the compiler? I'm surprised GHC has been able to frequently extend Haskell for so many years without the compiler ending up as a total C++-esque mess.
So we've reached a point where a comment calling a fellow haskeller an asshole gets upvoted (7 points at the time I'm writing this comment). Moreover by someone who explicitly acknowledges he or she doesn't have all the facts. Which could arguably have justified _not_ posting the comment in the first place. The technical disagreements are one thing, the tensions (or more generally the social issues) are another, but this?!!! It's time to stop the comments spree folks and take a bit of time off to reconsider this whole thing with more perspective. hvr has not always made the best decisions, but neither did Michael, Taylor and others, far from that. There are always two sides (or more) to such stories, as is often the case. Now, it would be great if the entire community could come together to ban that type of communication (not just this comment, but all those snarky remarks and aggressive tones that have been spreading all over the place), however major the actors are in the community. It's time we put a stop to this. At this point I don't even care anymore about anything technical. This is going way too far! And to be clear, I don't have any horse in this race. I have used stack(age) and cabal-install/hackage a lot, they're both great in some aspects and bad/appalling in others. But this isn't about that anymore. It's about how people have made the situation worse and worse, year in year out, until we reached this point. It's about time we stop handling things "the twitter way" ("hey, look hvr is at it again, let's go and boo him!" and so on) and act like grown ups again, I've seen my ~2yo son deal with conflicts better than what we're seeing here.
Have you tested building an image with Pandoc and Hakyll? Those take too long to build on demand in CI and are prime candidates for a Haskell based image.
This program is total
All you need to mess up this Chinese block chain is to use the same input for two transactions, and broadcast is simultaneously. Some nodes will receive these transactions in a different order and they will interpret history in a different way invalidating another chain of transactions than the rest of the network.
&gt; At this point I don't even care anymore about anything technical. This is going way too far! How do you suggest we deal with intransigent bad behavior when all else has failed? It seems to be like I'd rather somebody "snap to" and realize how toxic they're being rather than the options being limited to "do nothing" or "remove them as maintainers entirely."
I'm not quite sure what you're even asking about here. If you can parse your log format with standard regex then any decent implementation will be about the same. Regex matching isn't really significantly more expensive than string equality. In that case your bottle neck isn't going to be the parsing speed; It's going to be I/O which isn't going to be terribly different between the languages assuming roughly equal facilities are used (e.g. strict IO in Haskell). Have you actually done some profiling and found that the parsing speed is a signficant factor in your throughput?
Herbert doesn't have power over core packages. I've explained this repeatedly. The only people that think he does are the ones who are pushing grudges against him. He is not the central responsible maintainer of hackage, cabal, or anything else. When disagreements arise (and they will!) then his word is not the last word. He is one voice and contributor among many, and a very helpful and dedicated one at that. This penny-ante petty grudge match needs to cool it.
&gt; So you, an unknown person, who doesn't know the background, who doesn't know the people involved take it upon yourself to call someone who's invested lots of time into making the ecosystem better an asshole. Cassava isn't a core package, as such, he's free to do with it as he pleases. Of course, that much is clear. I wasn't arguing against his decisions, but his responses and his behavior. If one values such things, the reactions to the comments in that github issue are an indicator that I was not the only one with that opinion. &gt; I found Taylor's handling of his complaints rather childish, I also find it very childish that he forked "cassava" as "Cassava" basically relying on the fact that people will confuse the two while introducing possible problems for platforms which are case sensitive. I disagree. As far as I can tell, tfausak has been an active, contributing member of the Haskell/GHC community, and I think that it's a pretty strong statement to have one of the core Haskell people block you on two prominent media. I think this is made worse by the fact the person he blocked is someone who actually wanted/tried to report a bug with a package, after asking him to report bugs for that package directly to him. How is that supposed to happen if he has blocked all avenues of communication? &gt; Quite simply, time to put up or shut up. What have you contributed to the Haskell community? Or are you an arm chair quarterback? As much as I wish I could give you a never-ending list of commits to ghc, cabal, stack or something else prominent, I can't. I have, however, tried to contribute where I can, but I don't really feel obliged to try to appease you by listing those contributions. I would also like to ask you to clarify this question. Am I to understand that people who aren't contributing to the core Haskell/GHC ecosystem, but are nevertheless active members of the community, aren't entitled to an opinion on anything regarding these things? Maybe I really just shouldn't have used the word "asshole", but I did, and I'll let it stand. But I've been an active member of the Haskell community(in my way, which may or may not be of any value according to your own definition) for several years, and I don't think there's anything wrong with voicing an opinion about the behavior of someone who has the power to close bug reports and affect large parts of the Haskell ecosystem with his decisions.
Haskell's saving grace in this respect is that extensions usually have to play very well with each other in order to be accepted. Extensions do get removed, although not as frequently as I'd like. (I'm looking at you `TransformListComp` and `MonadComprehensions`.) See https://ghc.haskell.org/trac/ghc/wiki/LanguagePragmaHistory. &gt; I'm surprised GHC has been able to frequently extend Haskell for so many years without the compiler ending up as a total C++-esque mess. Have you journeyed through the code for the GHCi debugger? Probably not, since pretty much no one currently understands it - it is one of the parts of GHC that hasn't been touched in a while and is a bit of a mess. There are others... That said, given how old GHC is, it is quite impressive that it is so well engineered.
Take note that what had been a potential productive thread about moving forward with concrete proposals was essentially derailed by you into a shitshow of personal accusations and mudslinging. You need to cool it and figure out how to be a better contributor to the discourse, because this is _not useful_.
Also, the toxic behavior has not been just one side, are you ignoring this on purpose or...?
It's a little joke: factoring a number known to be a prime is easy: the number itself
&gt;The Haskell.org committee doesn't follow its own stated process or rules to keep people they are familiar with on the committee instead of running a nomination/selection process like they're supposed to. This is a slander. &gt; Past attempts to direct designers, frontend developers who want to do OSS contributions to Haskell.org has led to them being so ill-treated that they swore off ever doing anything for the Haskell community ever again. This is a lie. &gt; Hackage and Cabal are so egregiously mis-managed as open source projects that I have to wonder if the 2.0 version of the former wasn't primarily a wealth transfer than an attempt to serve the community. Hackage/IHG 2.0 alienated a lot of corporate sponsors too. This is also false. Please stop spreading lies.
Parsing speed is a problem with the older system I'm replacing, it's written in Java.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [tmcdonell/lulesh-accelerate/.../**Main.hs#L92** (master ‚Üí 35b4e65)](https://github.com/tmcdonell/lulesh-accelerate/blob/35b4e65b7ab0c167fed2ba11cc207c2c625f06b8/src/Main.hs#L92) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply duij51s.)
&gt; I think this is made worse by the fact the person he blocked is someone who actually wanted/tried to report a bug with a package, after asking him to report bugs for that package directly to him. How is that supposed to happen if he has blocked all avenues of communication? That timeline is incorrect, this is not what happened. This is what comes of making judgments on things you are not familiar with. (And this is what comes from taylor providing inadequate and partial information on an old dust-up that should be water under the bridge rather than trying to move forward in a productive fashion). &gt; Maybe I really just shouldn't have used the word "asshole", but I did, and I'll let it stand. Why would you do that, instead of retracting and apologizing in the name or productive and civil discourse? 
If we're to speak out minds, are you deliberately ignoring the fact that Michael Snoyman has consistently engaged in aggressive behaviour? His character assassination on Herbert has been prolonged over years.. His approach to community management has worn a lot of people out and created artificial divides. 
I agree with the other comment that if you need to parse log lines using a regex then many languages fit the bill. I do have a bit of experience in this area. A couple years ago I made [a library](https://github.com/frontrowed/postgrep) to parse PostgreSQL logs and do some rudimentary processing on them. Postgres log lines can have a fairly arbitrary format defined by the user, so we have a [parser for the log line format](https://github.com/frontrowed/postgrep/blob/58037b84b32d75fb067c3391c67616f33460cfe5/postgrep-core/src/PostGrep/LogPrefix.hs). Given the log line format, we can then use regexes to [parse the actual log lines](https://github.com/frontrowed/postgrep/blob/58037b84b32d75fb067c3391c67616f33460cfe5/postgrep-core/src/PostGrep/LogLine.hs) and [extract any info](https://github.com/frontrowed/postgrep/blob/58037b84b32d75fb067c3391c67616f33460cfe5/postgrep-core/src/PostGrep/LogEntry.hs#L24) from it. The main workhorses here are the `pcre-light` and the `conduit` libraries. I haven't actually run this code in about a year, but I'm bringing it up because I remember being pleasantly surprised at how blazingly fast it parsed log lines. In my opinion the code is very readable and flexible, yet the raw processing speed was definitely fast enough for our uses (we used this library to supplement `pgbadger`, and it parsed logs orders of magnitudes faster than `pgbadger` which is written in perl). I also remember trying to use `attoparsec` for the actual log line parsing in lieu of `pcre-light`, but I think I couldn't because postgres log lines can be parsed ambiguously given a poor log line prefix format. I couldn't wrestle `attoparsec` to deal with that properly, but regexes worked fine. Anyway I hope some of this info is useful to you!
&gt; So we've reached a point where a comment calling a fellow haskeller an asshole gets upvoted (7 points at the time I'm writing this comment). Moreover by someone who explicitly acknowledges he or she doesn't have all the facts. Which could arguably have justified not posting the comment in the first place. Maybe I shouldn't have posted it, and for what it's worth, I almost didn't post it. The thoughts that went through my head were essentially "Maybe I shouldn't bother voicing my opinion on all these big, deep ecosystem related things that I have had no hand in, or participate in this discussion between giants of the Haskell community" What I realized, however, is that while I may not have had a hand in all of this, I do have a horse in this race, as you put it. I write Haskell professionally(as of recently), and I have been using Haskell for many years now. This means that I do care what happens to the ecosystem, which, by extension, means that I have opinions about how things are handled, and how the people who are handling them act. &gt; The technical disagreements are one thing, the tensions (or more generally the social issues) are another, but this?!!! &gt; It's time to stop the comments spree folks and take a bit of time off to reconsider this whole thing with more perspective. hvr has not always made the best decisions, but neither did Michael, Taylor and others, far from that. There are always two sides (or more) to such stories, as is often the case. We are in agreement here. I would really much prefer that these things were handled professionally, and that there wasn't this constant smell of dirty laundry lingering in the corners every single time a discussion comes up about the Haskell ecosystem. I also agree that Snoyman hasn't necessarily handled everything as well as he could have. But when I read the links that have been posted in this reddit thread, I can't help but feel as if Snoyman is doing his best to be civil, while hvr is trying to oppose him at every turn. As for the rest of the comment, I guess I agree, even if it means that the mods go and delete my comment. My intention wasn't to start some kind of flamewar, but I don't think that people who are acting like assholes should just get away with it either. I also think that I may personally not attribute the same level of hostility to the word "asshole" as many others may do; I guess it kind of translates to something like "jerk" in my mind(which probably would've been a better choice in hindsight). This is probably because English is not my native language. For what it's worth, I don't have anything against hvr personally -- I am very fond of his works(especially cassava), and I have never been on twitter to "boo" him, or otherwise said anything about him ever before, on twitter, reddit or elsewhere. My comment was strictly based on my view of the behavior I feel he exhibited in the links that have been posted in this thread.
Maybe you can work through compiling and running the benchmarks (https://github.com/bos/attoparsec/tree/master/benchmarks) and possibly adding a few more?
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [bos/attoparsec/.../**benchmarks** (master ‚Üí 213dc8f)](https://github.com/bos/attoparsec/tree/213dc8fbc31697aaf3d4504a83243bd050c75d7b/benchmarks) ---- 
More anonymous downvotes. It is somewhat funny to see those contributing to the core ecosystem get routinely downvoted as they don't spent their time spreading FUD.
If my understanding is correct, there's work under way by Gershom to address some of Michael's points, and more to come to address some others. I fail to see how the toxic atmosphere helps addressing these points, or Gershom's work, or anyone really. Regarding your accusations, I don't have any proof so I can't just assume you're right (or wrong). Note that one could easily flip them around to make the other side look like "the bad people", and it would be just as likely to be true to me. :) I understand where your comment is coming from, having seen all the past public discussions, but I never saw a proof of this theory.
Slander requires intent/knowledge that what is being said is untrue. Are you sure that's what's going on here? It could be that two parties can observe the same events and come away with different conclusions and that also seems like a more charitable conclusion. It's also a more _useful_ since if you resort to dismissing (some of) what I said as slander you don't have any avenue for appealing to other people who see the same things I do.
Why would you want `MonadComprehensions` removed? I understand removing `TransformListComp` because I believe it is subsumed by `MonadComprehensions`. 
The FUD and fighting has been multi-partisan and recurring for a long time now. It seems inaccurate to me to try to characterize it as one-sided but it's possible you're in places I am not where it is one-sided. For my part, I read Reddit regularly, I'm in a couple Slacks, I'm on almost all of the Haskell.org hosted mailing lists, and I'm on the major Freenode IRC channels. Where are you seeing one-sided threads? I'd like to check that out and possibly talk to people if they're being unfair or unpleasant.
&gt; That timeline is incorrect, this is not what happened. This is what comes of making judgments on things you are not familiar with. (And this is what comes from taylor providing inadequate and partial information on an old dust-up that should be water under the bridge rather than trying to move forward in a productive fashion). I was not suggesting that he blocked him before he could make these reports, though I can see why my comment might be understood that way. Let me clarify and maybe add a bit: 1. If someone who is in a core position in the Haskell ecosystem asks people to report bugs directly to them for a core part of the Haskell ecosystem, blocking the same person who was asking is, to be frank, a dick move. It's not like there are thousands of people waiting in line to try to report bugs. 2. Bugs for the core packages like `integer-gmp` should not be going through someone via twitter or some other private service, especially if that person is prone to blocking the few people who are actually reporting bugs. &gt; Why would you do that, instead of retracting and apologizing in the name or productive and civil discourse? Mostly for posterity -- but please also see the last part of [this comment](https://www.reddit.com/r/haskell/comments/7yfdei/haskell_ecosystem_requests/duijjyi/). I admit that my words were poorly chosen, but it was not out of malice but rather because I'm not a native English speaker, and don't attribute as much hostility to that word as others may do -- something that I did not quite think through at the time. My intention wasn't to be hostile, so I'll edit the comment. 
Please see [this comment](https://www.reddit.com/r/haskell/comments/7yfdei/haskell_ecosystem_requests/duijjyi/). &gt; For what it's worth, I don't have anything against hvr personally -- I am very fond of his works(especially cassava), and I have never been on twitter to "boo" him, or otherwise said anything about him ever before, on twitter, reddit or elsewhere. I have literally never mentioned his name, as far as I can remember, in any communication of any sort in my entire life, so I most definitely do not have a grudge, nor am I pushing any agenda.
I can't imagine a meaningful use of `MonadComprehensions` and I haven't seen it used anywhere (unlike `TransformListComp`). The thought of having it around is also scary - I rely on list comprehensions mostly as type annotations to tell me when things are lists.
I don't have anything against you and didn't mean to sound like a jerk to you. If anything, your comment is a symptom of the problems that I'm begging the community to come together to fix. But there _is_ a lot more context to this whole story than just the few links spread around in this thread or even recently. And none of the parties are innocent. This is why I think it's essential that people just stop blaming the other party, making up fancy but somewhat realistic-sounding theories about power and control in the haskell community. We should go back to civilized discussions and to implementing whatever technical solutions we (the community) end up agreeing on to make matters at least a liiiiiiiittle bit better than they are today. And I'm hoping that we won't allow non-civilized discussions for much longer. People are even ignoring [SPJ's call for peace](https://mail.haskell.org/pipermail/haskell/2016-September/024995.html)... 
It makes me very sad that this is getting downvoted. Calling people names like this is never acceptable. We as a community need to be better than this if the wounds that have been inflicted are to heal. To quote Simon's [call for respect](https://mail.haskell.org/pipermail/haskell/2016-September/024995.html): &gt; It's fine to respectfully disagree with someone's judgement (i.e. 1). It's /not/ fine to imply that they have hidden (and bad) motives, or declare them incompetent or deliberately obtuse (i.e. 2). This has no place in our public conversations. The trickier the issue, the more careful we should be to express ourselves in a way that is respectful, and is visibly grounded in the assumption that the other person is acting in good faith.
Does reddit not support fenced code blocks? ```haskell example ```
Let me clarify what happened with the blocking. The github blocking was done with the understanding was this would _just_ prevent tickets on _personal_ not _organizational_ projects, and it was done earlier, in response to the Cassava mess. So the block was not _in response_ to the issue -- it was prior to it, and unrelated to it if I recall the timeline correctly. Also the fact of that block did _not _ prevent creation of the ticket in the correct repo. Once a request that was clear (as to what action should be taken -- i.e. a revision change) was made in the venue for that request (the trustee repo) then it was acted on almost immediately. The "me directly" part was about bugs in integer-gmp, not about the request for a revision. Very quickly, once the ghc thread narrowed in on the revision question, then a ticket was filed in the proper repo and it was handled. The intervening days were not due to nonaction on a clear request for a revision -- they were due to everyone trying to sort out the right course of action in the face of the issues, including exactly what fix would be appropriate with what consequences. If you read that lengthy GHC trac thread you will see clearly that many people besides herbert played a role in sorting things through, and the fact that this took a few days was due to needing to figure things out clearly, not to any sort of individual obstructionism.
Right. I was not saying you have a grudge, but I do think you have been mislead by those that do into seeing his role as different than it is in two ways: first as though he was the central arbitrator of anything (he is one contributor of many) and second through only forming an opinion of his work as a contributor through their cherrypicked links to interactions that they wish to cast in a bad light, while in fact as he is an active contributor you can find all sorts of helpful and friendly interactions from him all over the place.
Yes, that is a fair point. Thanks again for rekindling the release policy discussion!
The external dependencies can be a pain, yes. I had trouble installing LLVM on windows for example (though I am not a windows user, so had to experiment a bit). On the other hand, while GHC is an amazing Haskell compiler, it is not a very good Fortran compiler (; Thanks for the comments. Mostly that sounds like the impedance mismatch between the embedded language and regular Haskell. `-XRebindableSyntax` helps a bit, but yes `accelerate` is certainly a big hammer, which not every use case requires.
&gt; Well, less griping and more contributors would go a long way here As an outsider, my perception has been that something is wrong with the way all four of GHC/Cabal/Hackage/haskell.org are run and I think the lack of contributors is a symptom of some deeper problem. For example, the Rust community, which is significantly younger and smaller, has no trouble attracting contributors. I'm not going to try to diagnose what the root cause is or the solution should be, but you can't just keep doing more of the same and hope that the problem magically resolves itself. More importantly, if the administrators of those components relinquish their responsibility to identify and fix these issues then the responsibility falls on other more interested parties (including, but not limited to, Michael). So if you don't like Michael's proposals or you don't like forks of core infrastructure then you need to put forth a positive and proactive vision of your own instead of just reacting to complaints.
I think there's a very simple answer that hasn't been said yet. It takes a bit of geometric thinking, but the code is very easy. Two hints: * dot product of vectors (see u/Yungclowns's comment) * four constraints (see u/matt-noonan's comment)
*just*
It's a shame what these idiots have done to Haskell.
I haven't, but I'll give it a shot and post my findings. Methinks stack/ghc's incremental build capabilities more so than docker itself will aid in the build times for large Haskell applications -- will just have to preserve `.stack-work` between version bumps of the application being built. But yes, docker will allow building for many targets from one place. As for image size, the final image size should be 7MB+`size-of-haskell-executable`. Running `$ docker history hello-world-exe` IMAGE | CREATED | CREATED BY | SIZE -- | -- | -- | -- 113453229af9 | 24 hours ago | /bin/sh -c #(nop) ENTRYPOINT ["app"] | 0B e97177b495eb | 24 hours ago | /bin/sh -c #(nop) COPY file:510272c16b007ab0‚Ä¶ | 792kB e03eb537bc98 | 24 hours ago | /bin/sh -c apk add libffi | 51kB cf235046b394 | 24 hours ago | /bin/sh -c apk add gmp | 427kB 81e272ba10b1 | 26 hours ago | /bin/sh -c apk upgrade --update-cache --avai‚Ä¶ | 2.71MB 3fd9065eaf02 | 5 weeks ago | /bin/sh -c #(nop) CMD ["/bin/sh"] | 0B &lt;missing&gt; | 5 weeks ago | /bin/sh -c #(nop) ADD file:093f0723fa46f6cdb‚Ä¶ | 4.15MB
Cachin artifacts reminds me thay maybe Docker in combination with Nix would help for Haskell packages like those mentioned.
Maybe, maybe not. Without proof/evidence, to me this is as likely to be true as the evil haskell.org thing that Chris brought up. If this turns out to be true, then it would become observable and the suitable community reaction can take place at the right time. Similarly for the evil haskell.org thing. In my opinion, we should all just focus on facts, evidence, proofs. Nothing else. Someone claiming something without proof is never going to convince everyone and it'll just divide the community further. I don't want us to become the politicians of our respective countries.
This is an important and interesting discussion. I actually think GHC and Cabal have plenty of contributors these days, and the need for changes in release policy management is largely due to the need to scale up infra to deal with the amount of _stuff_ being done that makes demands on it :-) For haskell.org I assume you just mean the website, and not broader things? If so, I think that's an outlier because A) it probably doesn't actually _need_ that much more (though there are some nice-to-haves that never went anywhere) and B) the reason that work on it became difficult is a well known sordid tale not rehashing. With regards to GHC and Cabal I don't think the administrators of these components have relinquished responsibilities to "identify and fix" the issue of attracting contributors -- as evinced by their current health. Which leaves Hackage, which I do think lacks contributors. There are a variety of problems here, which I would like to address. First, I think that it never had a significant base to start with, and honestly basically fell into my and herbert's laps as the main developers became preoccupied elsewhere, so the infrastructure maintainers have had to more and more recognize themselves as _also_ the code maintainers, which is something that I've honestly just started to become more comfortable with. (The process of this started around mid last-year as it became clear to me that if I didn't work to shepherd through the last round of gsoc contributions, nobody else would do it. I essentially press-ganged herbert into taking over as deploy manager so I would have someone to watch over my sloppier impulses with regards to testing, someone to manage ops in general (as well as someone to conduct reviews back-and-forth with.) There are objective obstacles with hackage that are a pain to overcome -- a plain build is not very useful without some data, so you need to set up the `mirror-tool`. And the mirroring can be a bit bumpy as well. Then there is getting used to the conventions, such as ACID-State and the way in which data upgrades are handled, etc. And on top of that, there are the issues one has with any production web-application that make it harder to hobby-develop on -- including worrying about memory footprints and leaks, etc. So I think we need more documentation on how to build and test, more guides as to what code is where, etc. And, we _also_ need some vision (which ghc and cabal both have), which is hard because the vision relies on _usability_ above all else (along with playing a support role to broader ecosystem-wide long-term plans that necessarily involve changing hackage as a consequence). At some point I'd like to get together more of the former -- and the current round of hackage suggestions for GSoC are an attempt to articulate some of the latter, though much more needs to be done. I was talking to some folks about trying to also maybe have some hackage-workshops at hackathons to attract more contributors, etc. But again this is a fair amount of work, and the resources of the people who currently are responsible for this stuff is pretty finite -- so I would like to get this all underway, but I imagine it will take some time. I am hoping that the hackage and haddock proposed redesigns also get some momentum underway. (And also, I've been pleased to see a whole new infusion of work into haddock lately, which seems to be bearing some real fruit -- the activity on the repo and tickets is very heartening, as this is again a long-lived project where the original maintainers and developers drifted away some time ago). One reason, by the way, that I think comparisons with Rust are not useful, is that Rust has money to pay people to organize and educate others, and to also help put all the infra together necessary for others to collaborate. We have very little in the way of full time resources to that end. Another reason is _precisely_ that rust is a young language, and so there is the "greenfield" effect -- everything with the Haskell ecosystem is older, in need of constant renewal, has more details and sharp edges acquired over the years, and often encounters situations where maintainers are starting to withdraw but the process of realizing that the post needs to be officially passed to others takes some time to shake out. If you compare Haskell to peers in the _older_ elements of the free-software world, and the issues they face in seeking to draw in new contributors while maintaining institutional knowledge (and exclude from that the cases where projects are directly supported by large firms), then it seems much more fair, tbh. (I should mention by the way that "So if you don't like Michael's proposals or you don't like forks of core infrastructure" seems to imply that there are some specific things you think I don't like that are related to any of this -- I don't know of any at the moment, outside, of course of the dreaded revisions debate. So maybe there are things I wouldn't like, but I haven't thought about -- let me know so I can get on with not liking them :-P). 
I thought of this [and have advised others on this in the past](https://stackoverflow.com/questions/35019918/sum-all-numbers-from-one-to-a-billion-in-haskell/35020166#35020166) but I didn't see anything relevant that wasn't also manifestly typed.
Thank you for investigating this! This also worked for me: {-# NOINLINE isInfixOf' #-} isInfixOf' = T.isInfixOf longestCommonSubstring :: [Text] -&gt; Text longestCommonSubstring entries = go [""] "" where go :: [Text] -&gt; Text -&gt; Text go [] longest = longest go current@(s:_) _ = go (filter substringOfAll $ concatMap step current) s substringOfAll :: Text -&gt; Bool substringOfAll s = all (\e -&gt; isInfixOf' s e) entries step :: Text -&gt; [Text] step s = map (\c -&gt; T.cons c s) ['A', 'C', 'G', 'T'] $ stack exec -- time ros-test-exe 4.25user
Yeah, 10 years ago is exactly when I started hanging around, the atmosphere was lovely. I'm not sure we'll ever get back to that, but the current state of affairs, which stands at the very opposite end of the spectrum, is not going to work. I never would have imagined comments calling for respectful discussions having negative scores on this subreddit. This is appalling...
By the way, I have a C# implementation of the Slave part of wstunnel. The idea was to use it with Xamarin to build phone apps. If anyone wants it just ask :)
To the degree that some version of some possible plan was fleshed out (and there was much to be figured out then, and is still) it was in the course of discussions between myself, herbert, spj, and alanz during ICFP (with herbert not being physically present). My understanding was that SPJ and alanz as intermediaries were conveying _everything_ about those conversations to you, as they were the conversations regarding uncurated hackage, slurp, etc. in which this was also mentioned as a possible approach to reducing friction. So one reason that things were not further announced publicly is because _just like the uncurated discussion_ there was an attempt for principals -- _yourself included_ to come to some sort of rough consensus beforehand. On the other hand, it is possible that the conversations regarding this were not fully conveyed to you, as we've discovered subsequently that the telephone-game of going through intermediaries (even trustworthy ones) can lose quite a bit along the way. So perhaps that was the case here?
`cardano-sl` is a project that I've also recently started working on! Welcome :) The first thing I try to do is determine what kind of project it is. To do this, I look for a `stack.yaml`. `stack.yaml` is a way of documenting how a Haskell project works together. There are other ways (eg `nix`) but I'm not as familiar with them. Opening `cardano-sl`, we do have a `stack.yaml` file! Inside of a `stack.yaml` file, we're going to look for the `packages` stanza. For simple Haskell projects, it'll look like this: packages: - . That means we have a single Haskell package. A Haskell project may have multiple packages. A `package` has a single library, along with potentially multiple `test-suites`, `executables`, and `benchmarks`. `cardano-sl` is not a simple project -- it has many packages! Let's look at `wallet`. There is one canonical way to define a Haskell package: a `$PACKAGENAME.cabal` file. There's a new alternative called `hpack` that provides some syntax sugar that uses `package.yaml`. So I look for `cardano-sl-wallet.cabal`, and find it. The `.cabal` file will contain a series of `executable` stanzas. These stanzas describe the executables that can be built from the library. The `wallet` app has two: `node` and some swagger thing. Let's dig into the `node`. The first lines of the stanza are: executable cardano-node hs-source-dirs: node main-is: Main.hs This tells us that the `main :: IO ()` for the executable will be located in `node/Main.hs`. So let's look at that: main :: IO () main = withCompileInfo $(retrieveCompileTimeInfo) $ do args &lt;- getWalletNodeOptions let loggingParams = CLI.loggingParams loggerName (wnaCommonNodeArgs args) loggerBracket loggingParams . logException "node" . runProduction $ do logInfo "[Attention] Software is built with wallet part" action args From here, you can use `grep` and other tools to figure out where things are being called, and how the program is using stuff.
Sorry :(
Tsuru Capital is hiring, full time and intern positions are available. Haskell knowledge is required, experience with pricing futures/options would be nice but not necessary. Located in Tokyo, company language is English. Casual environment, nice monitors and a big coffee machine. Details how to apply are on our website. Feel free to ask questions here, in email or #tsurucapital on freenode.
This kind of problems have been experienced quite often. Most likely the difference is due to inlining/specialization taking place or not taking place. If different parts of the code are compiled using different optimization flag the code may perform poorly. You may want to force explicit optimization flags (same flags for everything) for your code as well as dependencies and see what happens. Take a look at this GHC ticket: https://ghc.haskell.org/trac/ghc/ticket/14208 . You may want to add your case to this ticket. Also see a similar problem reported here: https://stackoverflow.com/questions/46296919/haskell-webframeworks-speed-ghci-vs-compiled.
If you want to be generally treated like shit, go work for a trading company. It‚Äôs not worth the bullshit as there are far more interesting jobs without working for assholes who in turn make money for other assholes.
Thank you for your opinion. I don't feel like I'm being treated like shit and job is far more interesting than some of the previous jobs I did.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [tweag/linear-base/.../**Linear.hs#L46** (master ‚Üí f5927e1)](https://github.com/tweag/linear-base/blob/f5927e1fb03d9af04db7319e4436ed5a4fcad991/src/System/IO/Linear.hs#L46) ---- 
This is usually the case only with regex features that are not actually regular (e.g. lookbehinds). A truly regular regex can be executed in linear time.
Hours are usual - 40 with no required overtime. 30 days off you can take in any way you like - taking 4 weeks will be only 20 days, they do carry over to the next year. I'm working from home (and this home is in a different country), but I've been working here for a while - situation is slightly different for new employees.
Heads up that Hask isn't actually a category, but it's a useful fiction to pretend it is
Could we calm down on attacking companies posting job ads here? Especially if there's not a good reason. I think it comes from being a community of extremely independent people. Conformists aren't likely to end up excited about a language unique as Haskell, which can be a good thing. In this particular case though it hurts the community. We need these organizations for Haskell to succeed, it's counterproductive to be unwelcoming to them.
I was an intern at Tsuru capital several years ago and it was fantastic, I'd highly recommend working there, the team were great. If you want to _be_ an arsehole, you're doing a great job, keep it up, but you might want too back your bullshit with some facts.
I didn't structure the content at all, and had my font size too small so I unlisted the video. Will restream tomorrow with these mistakes fixed.
Everyone else seems to be advocating just using regexes, but I'm not ok with this, we have much better tools and ideally performance should also be good, if the parsers are designed carefully. Attoparsec is fast, and if you have a grammar which can use the Zepto parser module things may be even faster. As with all optimisation, you should start with something that's correct and then optimise, and I feel this is easier in Haskell than most languages - start out with the most obvious parser, which gives you a baseline for correctness, then work on an optimised version which you can test against the correct implementation. If you can share the structure you need to parse you can always get help on here or #haskell on freenode with improving performance - "make my code faster" is classic nerd sniping fodder :)
[These](http://hbtvl.banquise.net/series/Efficient%20parsing.html) serial blog post is great resource!
Something for everyone to remember: when there are conflicts within a community, and it is suspected that one or more actors have been persistently acting against the best interests of the group, [clarity in communication brings those people to light](https://status451.com/2016/08/09/too-late-for-the-pebbles-to-vote-part-1/). Sunshine is the best disinfectant. If anybody fancies themselves a Haskell historian, summarizing and tying together the past few years' feuds might be a very positive step forward. I don't think it would necessarily point to any particular malefactor, but it would really tighten the screws on those who would keep the Haskell community in such a sorry state.
I work for another trading firm and enjoy it. 40 hr weeks low stress
That is not an excellent comment. It contains false accusations against a variety of individuals and organizations. The accusations do not bring clarity, because they are false. They are not substantiated because they cannot be, they are lies. The only sorry state to be found here is the constant drumming up of drama by the usual few actors, which most people are sick of.
&gt;It contains false accusations against a variety of individuals and organizations. The accusations do not bring clarity, because they are false. They are not substantiated because they cannot be, they are lies. In the spirit of what I was just requesting, you could maybe substantiate these claims?
Could you try my GHC branch: https://github.com/ghc/ghc/tree/wip/T14068 I‚Äôd be interested to hear if it helps you. If it does, please tell us at https://ghc.haskell.org/trac/ghc/ticket/14068
All of this occurred _after_ `^&gt;=` was merged to Cabal's master, and (unless I'm misremembering) after Cabal 2.0 was released with the feature. It's hard to tell how much information was reliably communicated to me, but those conversations with SPJ were the first I ever heard of the "soft bounds" concept, and the mismatch between that and the documented feature caused me a lot of unnecessary work on trying to figure out what Stackage and Stack were supposed to do. That kind of uncertainty is exactly why a full proposal should have been put forward before the operator appeared in Cabal at all.
I would honestly be happy to offer something in return. In the private conversations we had\*, the idea of a 2 month or so grace period to upgrade downstream had been discussed, with the implication that Stack would guarantee to upgrade in that time. I don't think we ever formalized it completely, but I'd be happy to work something like that out. \* Again, I really prefer public conversations
Awesome!
I think your timing is correct here. However, that set of discussions is also just about the first time there were _any_ in depth discussions on any future plans about the caret operator. Before then it was just a vague idea. So to the extent there were discussions about how we might evolve it, you were included just about as early as anyone. The caret operator was not introduced to enable these plans, not least because they did not exist -- it was introduced absolutely as sugar, exactly as described on the ticket (https://github.com/haskell/cabal/issues/3705). So I just want to assure you that you were not "cut out" of any prior discussion -- as soon as a discussion began to be fleshed out, you were actually one of the people made aware of it.
As far as I can tell GHCi has the same default GC system as any other GHC program. The reason it behaves differently is that things get treated as GC roots in some circumstances where they wouldn't be in a fully compiled program.
I actually think it's a bad idea to use regexes in production systems. They are really quick and dirty which makes them great for one-off tasks. But in terms of readability and maintainability regexes are positively horrid. If your needs become even moderately complicated, the complexity of regular expressions can get really large. If this is a production system that needs to stick around and be maintained over time, then I recommend you steer clear of regular expressions and write a nice declarative parser instead.
I really like the idea of a state monad which you can keep track of all intermediate states while still only giving access to the most recent state to the program being interpreted. Not sure why I'd never thought of this before, but it seems so obvious. I also like the idea of providing debugging interpreters which print out every single action and its contents. I should use more Freer monads
The company language is English, but is the team Japanese? I'm a fluent speaker and this seems like a great opportunity.
Only three out of 12 are Japanese. (British) English is mostly spoken.
One of the best jobs I ever had was at a prop trading firm.
Thanks! Could you elaborate on the meaning of: &gt; 4. Three month working interview or internship in Tokyo
I don‚Äôt really use any special tools to explore Haskell projects: * Building haddock with source hyperlinks goes a long way * I like to use ghci combined with Atom‚Äôs fuzzy search on the project‚Äôs source. `:load` a module to get all its imports in scope. Then `:info` goes a long way. Type holes (`:: _`) help with figuring out local bindings‚Äô types. 
What's a fast type aligned sequence, and what's the church encoding?
I'd like to chalk it up as a communication gap, and I do see how the misconception can occur, which is why I've tried to work so hard to dispel it. Making it a standard practice to get changes that involve cabal file features and syntax discussed in a common place would I think help dispel the possibility of these sorts of slippages in the future, since the expectation would be "of course there's nothing behind closed doors" at that point.
I fully support this idea, and would be happy to lend any support I could to facilitate such public discussions.
Fantastic, you'll be hearing from me soon.
 nukeTheEntirePlanetFromOrbit :: IO () nukeTheEntirePlanetFromOrbit = justDoIt
Awesome! I've been following your work - Congrats on the release :-)
Sounds absolutely correct. Assuming you're writing an exchange (instead of merely mirroring another exchange so you can write a robot), then the events could then, for example, be used to trigger rule-based orders (OCO, stop-losses, etc). There's no harm in producing this information. Moreover, depending on your design, I suspect that the order matcher has to produce this event list internally, anyway. That is, an order comes in, matching occurs, a list of actions is produced (trades, cancellations, and insertions), this list is acted upon, and the order book is modified. So why not just expose that list anyway? Have you considered performance? That would be yet another reason why diffing is not a good idea. 
I recently had a look at all the Haskell FRP libraries I could find. I really liked the idea behind `Reflex.Dynamic` module: but unfortunately I couldn't figure out how to use anything in the `Reflex.Host` module (which seemed to be the only way of making the reactive system actually interact with the outside world)
The decision tree type at the end of the article reminds me of a type I had a lot of fun with last year (also for representing decision trees, but slightly more general) data DTree q r a = DLeaf a | DNode q (r -&gt; DTree q r a) I don't think I tried pretty-printing them: but I did have `Show` instances for parameters meeting a few constraints.
[removed]
I think we both know how Haskell works and how to do IO in it and that it is good, so we are basically discussing semantics (which can be interesting enough). In Haskell the type system seperates pure from impure, which I agree is super handy. When people say that Haskell is referentially transparent they really mean that the pure parts are. Ofcourse even being able to make that distinction requires a system like Haskell's where pure is seperated from impure. Consider the code with not free variables: getLine &gt;&gt;= \x -&gt; getLine &gt;&gt;= \y -&gt; f x y `x` and `y` cannot be swapped out for each other in `f x y` without changing the meaning of the program. From this fact we can conclude that either `getLine`, `&gt;&gt;=`, or both must be impure. You might say that monadic binds do not count (in monads which can do IO), but this is exactly just like saying "Haskell is referentially transparent except for where it is not". That being said I don't think saying something like that is as meaningless as it sounds, because "where it is not" is really well defined and easy to figure out in a Haskell program. 
I'm a big fan of attoparsec and I would first try that out, especially its streaming interface over either ByteString or Text. Especially when you expect the data format to change, compositional parsers can't be beaten.
I almost thought I'm on /r/espresso because of the picture...
I'm calling BS on the thread spinup time in rust. https://gist.github.com/spacejam/c70fa85479cc224e574e4cf3c06a280b Thread spinup time is negligible and you should be able to crank out far more throughput with standard idiomatic concurrent rust. Feel free to PM me if you need external help on this.
This is amazing! Even though unicode is cool, in this case I like `justDoIt`.
This. Don't switch to Haskell because of "maybe it can be faster than Rust or Go". It can compete quite well, but what Haskell mainly has to offer is an easy way to get something much better than regexes.
- `justDoIt`: Leave it to me! return ()
I have some examples of using the host interface [here](https://github.com/dalaing/reflex-host-examples) if that helps. 
Reminds me of "The 100" and "Terminator" among others. It's indeed scary.
&gt; When people say that Haskell is referentially transparent they really mean that the pure parts are. IO String is just as pure as String. &gt; x and y cannot be swapped out for each other in f x y without changing the meaning of the program. [...]. Referential transparency has nothing to do with swapping variables with each other. Consider this: runIdentity $ pure 1 &gt;&gt;= \x -&gt; pure 2 &gt;&gt;= \y -&gt; pure $ (-) x y I'm also not allowed to swap out `x` and `y` for each other in `(-) x y`, does this mean Identity is not pure? Of course not, because that's the wrong definition of referential transparency. &gt; You might say that monadic binds do not count (in monads which can do IO), but this is exactly just like saying "Haskell is referentially transparent except for where it is not". Again, no. Referential transparency says that `replacing an expression by its bound value does not change the meaning of the program`. That means that if you have `e = x` (the equals matters), you can replace `f e e` with `f x x` without changing what the program does. This is *not* a subtle semantic point, it's a _huge advantage_ in the way haskell does things. ________ Let me give another practical example of Haskell's pure IO, by once again comparing with Scala Future, which might appear similar (higher-kinded, can be used in the equivalent of `do`), but it's _actually_ side effectful. import scala.concurrent.Future import scala.concurrent.ExecutionContext.Implicits.global // Let's write the Future equivalent of IORef case class FutRef[A](var a: A) { def put(a: A): Future[Unit] = Future { this.a = a } def get: Future[A] = Future(a) } object FutRef{ def futRef[A](a: A): Future[FutRef[A]] = Future { FutRef(a) } } val ref: Future[FutRef[Int]] = FutRef.futRef(0) val writeOne: Future[Unit] = ref.flatMap(fr =&gt; fr.put(1)) val read: Future[Int] = ref.flatMap(fr =&gt; fr.get) val prog = writeOne.flatMap(_ =&gt; read).flatMap(i =&gt; Future(println(i))) *Evaluating prog returns 1*, and that's exactly what people used to side effects expect: if you refer to the same thing twice you get the side effect only once. Let's try and apply the substitution model, to see if referential transparency holds, by replacing `ref`, `read` and `writeOne` with their definitions: val prog2 = FutRef .futRef(0) .flatMap(fr =&gt; fr.put(1)) .flatMap(_ =&gt; FutRef.futRef(0).flatMap(fr =&gt; fr.get)) .flatMap(i =&gt; Future(println(i))) *Evaluating prog2 prints 0*. Replacing expressions by their bound value has changed the meaning of the program. Future is not referentially transparent. --------------- Now let's do the same with Haskell's IO: import Data.IORef ref = newIORef 0 writeOne = ref &gt;&gt;= \r -&gt; writeIORef r 1 readR = ref &gt;&gt;= \r -&gt; readIORef r prog = writeOne &gt;&gt; readR &gt;&gt;= \i -&gt; putStrLn $ show i *Evaluating this program prints 0*, which is exactly what I expect as purely functional programmer, but that confuses the hell out of imperative programmers at first. Thankfully, we can apply the substitution model, and replace the definitions of `ref`, `writeOne`, `readR`: prog2 = ref &gt;&gt;= \r1 -&gt; writeIORef r1 1 &gt;&gt; ref &gt;&gt;= \r2 -&gt; readIORef r2 &gt;&gt;= \i -&gt; putStrLn $ show i *Which also prints 0*. Replacing expressions by their bound value has not changed the meaning of the program: *Haskell's IO is referentially transparent (pure). --------------- This is not a point of principle, it's a massive benefit: I cannot apply the substitution model to `Future`, because it's side effectful, and therefore I need to use global reasoning. Even though I haven't flatMapped the same `ref`, the state is now being shared. Thanks to Haskell's pure IO (or Scala's IO, which we modelled after Haskell's), I can instead reason locally about it, because of referential transparency. Once again, Haskell has effects, not side effects. No. First, this is the definition of referential transparency 
Not an expert in Japanese labour law, but this sounds suspiciously like a probation period.
Low stress at a trading firm? What company is that?
Are you considering remote applicants?
Im writing a password encryption application, that keep my passwords encrypted in a dropbox file, and out of reach from sniffers. https://github.com/patriques82/keychain Now if you look at the Keychain.Core and the Keychain.Operations modules you see that there are many liftIO actions scattered across the code, since I need directories, files and other IO related stuff now and then. Im wondering if there is some smart way to factor out the IO stuff into some type class or similar that allows me to have pure code in the core and just have IO in the "outer" layer of the application? If somebody has other tips on how to improve the code, Ill gladly take note of these. 
I never understood what people meant by your first sentence. Supercompilation is a very specific technique which refers to speculating about possible values variables can take. There are some techniques you can currently use. If you really statically know the structure of your graph then you can build a GADT which has a type argument which mirrors the structure of the graph. Then you can write type class instances which dispatch on this argument and the compiler will unroll "recursive" things based on this. See for example, the [static-vector](http://hackage.haskell.org/package/fixed-vector) library. Programming in this style can be quite cumbersome as all your combinators have to maintain the proof about why the structure is statically known. In the Ocaml community there are several different implementations of staged languages which give you this kind of control but they each have a various trade off. It is also very easy to incorrectly stage programs and cause the compiler to loop. The implementations are usually quite cumbersome because of how the `Code` data type is interpreted. Typed template haskell is quite similar to how these staged languages work. Anyway, this is the topic I am actively researching at the moment so if you could share with me personally some details then it might be a nice example. 
This can be done. Give me a small example of a function you want to do this for, and I‚Äôll show you how to do it in Haskell.
I just looked up referentail transparency. I must concede that I didn't understand it correctly. It says that you are allowed to substitute and expression for its value. As you say this holds in Haskell (as far as I can tell). I thought it implied more powerfull kind of equational reasoning, were if two variables were bound by the same expression they could be used interchangeably. Note that in my example both `x` and `y` gets bound by the same expression, namely `(&gt;&gt;=) getLine`. In that way your example is not quite the same as you use two different expressions: `(&gt;&gt;=) (pure 1)` and `(&gt;&gt;=) (pure 2)`. But still, you are right. Haskell is pure in the sense that is referential transparent.
&gt; As I understood it, proper compile-time evaluation is supercompilation That's not what supercompilation is. For example, `fmap id :: Maybe a -&gt; Maybe a` does not reduce to anything under the normal evaluation strategy because no reduction even happens until all the arguments are applied, which at this point they are not. Supercompilation is an entirely different reduction strategy: in the above example, we would speculate on the value of the argument, which must be either `Just x` or `Nothing`, and in both cases, mapping the identity would just give back the original value, so `fmap id` is equivalent to `\x -&gt; x`, or, `id`. That being said, I'm totally with you that this should be available via a pragma, as it has an enormous number of uses. But no, it is not the same thing as just performing evaluation at compile time.
I believe all monads can be thought of as just continuations in disguise. The paper I read on it was entitled "Continuations: the Mother of All Monads," or something like that, but I don't have the link handy at the moment. I'll try to get it tomorrow
You can use TemplateHaskell to run an arbitrary expression at compile-time, and then [`lift`](https://hackage.haskell.org/package/template-haskell-2.12.0.0/docs/Language-Haskell-TH-Syntax.html#v:lift) the result into a literal: {-# LANGUAGE TemplateHaskell #-} import Language.Haskell.TH.Syntax (lift) import MyFib (slow_fib) slow_at_runtime :: Integer slow_at_runtime = slow_fib 40 slow_at_compile_time :: Integer slow_at_compile_time = $(lift (slow_fib 40)) 
Unlikely. Exceptions are possible, but very unlikely.
On the one hand what you say is true, on the other hand developing with more generic types allows for more code re-use and for types that further restrict what the code can do.
Me, an intellectual: `pure ()`
Well, if the functions within the `Monad` instance dictionary qualify as "continuations", of course all monads are continuations in disguise.
Type-aligned sequence: see the "Reflection without remorse" paper Church-encoding: see the "Free Monads for Less" blog post
That's not what I meant--also, the article was easier to find than I thought: http://blog.sigfpe.com/2008/12/mother-of-all-monads.html?m=1
The test file goes through as quickly as any other, but the code is highly non-optimized right now - proof of concept stage.
I haven't fully explored the design space yet, so I know there's more to be done, but I actually piggybacked off the extensible records library vinyl to make [vinyl-json](https://github.com/heptahedron/vinyl-json). It supports type-level field names, optional/required fields, serialization and deserialization to and from an extensible record type. I'm working on getting a polyvariadic function typeclass made so constructing one isn't as verbose, but it also has handy lenses to access individual fields using TypeApplications. Tell me if there's anything missing that your use case needs!
Don't feel comfortable saying but I do work in the back office so it's possible that's why.
Arnaud Spiwack suggests only doing it when you can see there's only one inhabitant. I think an alternative might be a second class offering `justDoItCarefully`.
The `...` in `MaybeT . return $ ...` has type `Maybe a`. So there are two cases, `Just x` and `Nothing`. `MaybeT m a` has both a `Monad` and an `Alternative` instance, so those two cases can be written more succinctly as `return x` and `empty` respectively: import Control.Applicative import Control.Monad.Trans.Maybe -- | -- &gt;&gt;&gt; runMaybeT just -- Just 42 -- &gt;&gt;&gt; runMaybeT simplerJust -- Just 42 -- &gt;&gt;&gt; runMaybeT nothing -- Nothing -- &gt;&gt;&gt; runMaybeT simplerNothing -- Nothing just :: MaybeT IO Int just = MaybeT . return $ Just 42 simplerJust :: MaybeT IO Int simplerJust = return 42 nothing :: MaybeT IO Int nothing = MaybeT . return $ Nothing simplerNothing :: MaybeT IO Int simplerNothing = empty If you don't have a concrete constructor in mind, and just want to convert an existing `Maybe a` to a `MaybeT m a`, then `MaybeT . return` does sound like a good way to do that. Another way is to use the [`maybe`](http://hackage.haskell.org/package/base-4.10.1.0/docs/Prelude.html#v:maybe) higher-order function to turn your `Maybe a` into `return x` or `empty`. I like to give this combinator a slightly more generic type than `Maybe a -&gt; MaybeT m a`: onNothingEmpty :: Alternative f =&gt; Maybe a -&gt; f a onNothingEmpty = maybe empty pure In addition to Monad stacks which begin with `MaybeT`, this combinator is compatible with any other Monad stack which contains a `MaybeT` somewhere in the stack, such as `ReaderT r (WriterT w (MaybeT m))`. This is the basic idea behind `mtl`: write operations which work with many Monad stacks. So `mtl`'s `put` works with any Monad stack which contains a `StateT`, its `throwError` works with any Monad stack which contains an `ExceptT`, and so on. By the way, `EitherT` is [deprecated](https://hackage.haskell.org/package/either-4.5/docs/Control-Monad-Trans-Either.html) in favour of `ExceptT`; it's the same thing except for the name, and the fact it's in both `transformers` and `mtl`.
Me, working in the sewers: `\RealWord -&gt; (# RealWorld, () #)`
Salary range?
Yes - I think these are at different levels however. All monads that *can be expressed* in pure code can also *be expressed* as continuations instead (and vice versa); whereas Iceland Jack merely did mechanical transformations like reordering arguments and such to arrive at "oops, this is just Cont". (It's in his tweets somewhere; I don't have time to look for it right now, sorry!)
You can force GHC to inline recursive function, when recursion depth is know at compile time, by turning it into typeclass: -- | Apply a function to all elements of a list. Type param @n@ is the length of the list. class Map (n :: Nat) where map :: (a -&gt; b) -&gt; [a] -&gt; [b] instance {-# OVERLAPPING #-} Map 0 where map _ _ = [] {-# INLINE map #-} instance {-# OVERLAPPABLE #-} (Map (n - 1)) =&gt; Map n where map _ [] = error "map: Not enough elements in the list." map f (x : xs) = f x : map @(n - 1) f xs {-# INLINE map #-} 
By encoding the recursion in the type and not in the term, you can at least inline everything, that can be a huge performance gain and in some cases results into a complete evaluation.
Supercompilation does not require evaluation to finish at compile-time; it can operate on partial information. For example, the usual map-fusion rule `map f . map g = map (f . g)` can in theory be automatically discovered and exploited by a supercompiler. However, in practice supercompilation has not worked well (especially when fully automatic), because the search space is enormously large, and it is very easy for a supercompiler to simply massively increase the size of the program by aggressive unrolling/inlining, without actually managing to make it faster. Template Haskell-like techniques don't have this problem, because nobody expects them to be automatic, or robust.
I was interviewed by a trading firm and "low stress" would definitely not one of the impressions I got after a whole day spent with the team.
I disagree. Here's a simpler case: if a function returns an `Integer`, will it be more reusable if it returns a `forall a. Num a =&gt; a` instead? No, it can be called in exactly the same calling sites, the only difference is whether it is the function or the callers who call [`fromInteger`](http://hackage.haskell.org/package/base-4.10.1.0/docs/Prelude.html#v:fromInteger). Similarly, an action which runs in a concrete Monad stack may be used from exactly the same calling sites, the difference is whether it is the action which makes itself polymorphic by using the `mtl` or whether it is the callers who convert the action's concrete Monad stack into the Monad stack they need. There is, however, one important subtlety. [`lift`](http://hackage.haskell.org/package/transformers-0.5.5.0/docs/Control-Monad-Trans-Class.html#v:lift) and [`hoist`](https://hackage.haskell.org/package/mmorph-1.1.0/docs/Control-Monad-Morph.html#v:hoist) can be used to add missing layers to the Monad stack, to account for the kinds of effects which the caller uses but the callee doesn't. But swapping two layers, because they happen to be listed in a different order in the caller and the callee's Monad stacks, isn't as easy. And rightly so: the order in which the stacks are listed affects the way in which the different effects interact! [For example](http://gelisam.blogspot.ca/2015/09/two-kinds-of-backtracking.html), with `MaybeT (StateT s m)`, the state survives backtracking, whereas with `StateTs (MaybeT m)`, the state gets reset upon backtracking. Other Monads, like `ReaderT`, can be put anywhere, they don't interact with any other effect. `mtl`'s constraints combine like a Set, they don't care about the order. So `mtl` is a bit error-prone, because when you write your action, you probably think of your effects interacting in a particular way, but your type claims that your action will work with all Monad stacks which contain the effects you have used, regardless of the way in which they interact. So `mtl` is better-suited for the situations in which you use Monads like `ReaderT` in which the order doesn't matter, and the situations in which your code genuinely does do something useful (but different) for all permutations of the effects you are using. In practice though, this isn't a problem because `mtl` code is often written with a concrete Monad stack in mind, as many applications have a top-level Monad stack in which everything will eventually run. The `mtl` code is _not_ written for reuse at all, it is written for the second reason you give: to restrict what the action can do, thereby making it clear from the signature which subset of the effects supported by this top-level Monad stack are being used by this particular action. And if they are being used for this specific task, `mtl` is more convenient than `transformers` because it doesn't require extra `lift`s and `hoist`s at the call sites.
I'm streaming again in an hour.
Do you sponsor visas?
&gt; Here's a simpler case: if a function returns an Integer, will it be more reusable if it returns a forall a. Num a =&gt; a instead? No. However what I said does not hold for all cases, and you correctly gave an example. To clarify: I did not mean to say that all the functions you'll write for type-classes will be re-usable, only that often functions you'll write for more general types will be reusable. This can't really be disproven by a counter-example like you provided, but rather you can get the feel that this claim is true by interacting with many positive examples. In the case of monad stacks, if you switch to use mtl-style and also enable the `-Wredundant-constraints` GHC flag, you may find on some functions than they don't use all of the monad stack's features, those functions may be more re-usable than the type previously told.
Are probation periods considered bad or illegal somewhere?
How do you think smileys get eyebrows?
Please make sure it's recorded. Thanks.
I have no idea.
Congrats then!
This is really common in the US from what I've seen. I like it because it let's lower skilled people get their foot in the door for internships over the summer and pad their resume slightly; it often comes with more wiggle room in salary and then you can negotiate up with a permanent position. But it's definitely something that doesn't work as well once you're out of college.
While the name is cute wouldn't "synthesize" be more accurate and informative?
Haskell is just like the gallium arsenide: it was, it is and it will continue to be the future.
I mean. I've worked at a telco that was extremely high stress. It's all going to vary job to job.
For those unfamiliar with gallium arsenide (including me before looking it up): it is an awesome semiconductor with many nice properties compared to silicon, but it isn't as widely known (or used) as silicon. Tangential fun fact: California lists gallium arsenide as a carcinogen ...
To be fair, a lot of things magically cause cancer specifically in California.
I understood the question as "what the point of not having unrestricted side effects", which might be wrong. If that is indeed your question, ask yourself how that would play in a non-strict language.
I see your point but your point has been said for the last 20 years yet the Haskell demand in Europe is close to zero. Why do you think It happens
Just gonna put [tensorlang](https://github.com/tensorlang/tensorlang) here for posterity ;) , not directly related to grenade but relevant. I tracked it [from](https://pseudoprofound.wordpress.com/2016/08/03/differentiable-programming/) [here](https://www.facebook.com/yann.lecun/posts/10155003011462143).
I really want to see this done with using rust as the low level cruncher. I feel like it could be a beautiful match, especially with rust developing some really powerful auto vectorization libraries. The Haskell library itself looks awesome, although I can definitely hear my coworker in my head saying something along the lines of "see? You need to get super ugly to get anything useful done"; not quite sure what a strong counterargument to that would be, yet.
That is the point! 20 years in the future, it will still be regarded as the future of programming. Or will be forgotten.
One would essentially have to take the monadic io and package it up in such a way that you can externally verify that it has no other references to it at any given time and then one can treat it as a uniqueness type. Alternatively, we could probably cheat as long as we cheat correctly/safely? But, I'm not versed enough in linear logic to say whether or not that's actually possible, especially in the face of concurrency
Another option is to try to install Haskell inside your existing non-Haskell workplace. There are usually plenty of small tasks which can be a good way to demonstrate its usefulness: replace a shell script, write a command line utility to process data, build a web server, etc.
too little
‚Äú**That which can be asserted without evidence, can be dismissed without evidence.**‚Äù - Christopher Hitchens 
You should take a look here: https://github.com/erkmos/haskell-companies . Just recently Wire was hiring in Berlin. The position was open for a long time.
The difference here is Intel essentially speculatively evaluated arbitrary IO code at kernel level permissions and then rolled back if the code branch wasn't executed or if, when executed, permissions weren't correct, etc. What that meant is that any io can be done at kernel root mode by tricking the CPU into evaluating your branch and then forcibly jumping out somehow to prevent that code from "actually" being executed; the mistake was two fold: 1. Executing non pure code speculatively with no sanity/safety checking at all 2. Not rolling back correctly if that branch fails to happen. (only safe to do for pure code) Both concerns are ones Haskell forces you to address; I'd be very surprised if GHC had any sort of thing like that happen. Much more likely would be a bug somewhere else in the compilation process rather than a bug in Haskell code.
Rust implementation reads per line via BufReader. I've not figured out how to write to stdout via multiple threads efficiently via the BufWriter. It was not my intention to call Rust out negatively; it's a pleasure to work with. 
How come, the way I see it haskell jobs demand in eu (uk, Swedish, Germany) is above the rest of world, have seen most of job openings posted here are eu based.
Thanks! This makes a lot of sense, and I hadn't considered the problems that would come along from a non-strict execution model. 
Love the image size here! I've tried to use Alpine before for Haskell executables, but I've historically run into issues as soon as I introduce Postgres into the mix. See this conversation: https://github.com/jml/servant-template/issues/21 I haven't had the chance to mess with your impl yet, but I'm curious what happens if you expand your app to include a database like Postgres. Does Alpine being non-glibc cause issues?
The principle advantage of monadic IO is not that it ensures safety (that is certainly a benefit) but rather that it makes I/O work nicely in a lazy language.
I am a Cardano developer so I would expect myself to have a good answer to this question, but I don't :( My two main struggles are: * Where does X happen? * If I want to add functionality Y, where should it happen? I usually answer the first question by running the node and grepping for log messages in code, or by trying to guess how the relevant function could be called (and then using tags to find it). For the second question, I just ask a colleague who wrote Y-related code. If you're not in our Slack, you can ask at our [Askbot forum](http://cardano-dev.askbot.com) ‚Äì it's not an official forum, but some of the devs happen to be there. If you're a newcomer to the codebase, you should first get acquainted with *what* Cardano does before trying to figure out *how* it does it. There have been at least two talks (overviews of the codebase) on that topic given internally, but I don't know when they will be published.
When people say that "When Haskell compiles, it just works", they are of course lying :-) There are many bugs that can slip through. However, when Haskell compiles, it is far more likely to work than in other, mainstream languages. I think it is primarily because of 3 factors: # Polymorphic functions Polymorphic functions are very restricted in what they can do, reducing lots of potential bugs. For a trivial example, the type `a -&gt; a` or `a -&gt; b -&gt; a` get a lot of potential bugs eliminated by the type checker. # Side-effects The above is only true insofar as `a -&gt; a` is a pure type. If it can also delete your hard drive before returning the given value, then there's a whole lot of potentially buggy values to account for. This is of course a contrived example -- but in practice, pure side-effect-free functions get a *lot* more assurances from the type checker than `IO` ones. The types are just so much *smaller* (have fewer values). So lack of `IO` really helps us with the vast majority of most Haskell program fragments that do not involve IO at all. It narrows the size of types by orders of magnitude, ruling out a huge chunk of wrong programs. # Sum types This is less relevant to your question, but is it one of the 3 factors. We can model our data very precisely (no implicit nullability is a consequence) so that code only has to deal with *actual* valid states, and the compiler enforces we actually *deal* with all of them.
&gt; Sure the type system will perhaps help me from using IO in a context where some code may be re-executed (such as STM or the like). There are a lot of contexts where you don't want IO to be performed. When I was writing web apps in Rails, sometimes I'd have to debug a weird performance problem, and I'd track it down to a **view** -- the dang HTML rendering code had an N+1 query problem. The view shouldn't be calling the database at all! The offending code was something like &lt;ul&gt; &lt;% user.posts.each do |post| %&gt; &lt;li&gt;&lt;%= post.title %&gt; - ( &lt;%= post.comments.count %&gt; comments )&lt;/li&gt; &lt;% end %&gt; &lt;/ul&gt; When I was initializing the view, I only provided the `user` object -- I didn't prefetch all the posts and comments. So rendering this view caused Ruby to make a query for all the posts, and then a query for each post's comment's. Hugely inefficient. In Haskell, a view is ideally a pure function -- `view :: Model -&gt; Html`. This can't perform any `IO`. Any information that the view needs must be represented in the `Model`. We'd acquire the model with something like `getModel :: Key -&gt; IO (Maybe Model)`, and this would be where all of the side-effects must occur. Since all of the IO would be kept in the same place, it'd be easy to spot the N+1 query problem: getModel key = do user &lt;- getUser key posts &lt;- getPostsForUser user comments &lt;- for posts (\post -&gt; getCommentsForPost post) pure (Model user posts comments) We can visually scan this and see that it makes one query for the user, one for the posts, and then N queries for N posts to get the comments. Because the concerns have been isolated, we can easily refactor to: getModel key = do user &lt;- getUser key posts'comments &lt;- getPostsWithCommentsForUser user pure (Model users posts'comments) Which removes the N+1 query problem. For me, improving Haskell's performance is much more straight-forward, and partially because I have control over effects. In web stuff, `IO` is the cause of poor performance, 99% of the time. Web requests take 10-100s of milliseconds, database calls take ages, etc. while rendering view code and performing calculations takes a tiny fraction. If the pure code *is* a performance problem, then it becomes super easy to write benchmarks and tests for the code to improve the performance -- after all, a pure function will always return the same input given the same outputs, so constructing cases is easy. Benchmarking and testing impure code is a huge pain in the ass, so I am encouraged to purify as much code as possible. When all of the impure code is isolated to a small surface area, it becomes clear which impure things are causing problems, and I can then make them concurrent. If the pure code benefits from parallelism, it is trivial to add parallel combinators (and, since it's pure, parallelism is deterministic, and not an additional pain to debug!). However, I think the most important part for me is: reducing the size of possible implementations. When I have a function `x :: Int -&gt; Int`, there's a bunch of stuff I can do: - ignore the input and return 42 - multiply the input by two - return the input unchanged - etc... but there's also a lot of stuff I can't do: - return a `String` - return a different number based on the current time - print the input to the console - open a database connection - write to a file - `sleep` - send an email or text message - spin up 100s of new AWS servers on my account - etc... If I see `x :: Int -&gt; IO Int`, then I know I have a lot more possibilities to be on the lookout for -- it can now do any of the above things! 
Honestly, IO by itself isn't terribly exciting. IO just gives us a way to know which functions are safe to call any time and which can potentially have hidden ordering rules. GHC is totally free to determine the sequence in which pure function calls actually happen at runtime. It is even able to sense when a call isn't necessary at all. This is absolutely not the case with an IO action. What's actually very exciting is that you can build your own types that stack extra constraints and capabilities atop IO. This is how things like STM can work without asking you to do lots of very careful manual work. As an example, try to increment an IORef from within an STM transaction in a way that doesn't use any `unsafe` functions. The STM library reserves the right to rerun your transaction multiple times, so it could be very bad if this kind of thing were to happen in a real program.
Searching for a Haskell job in Spain? Seems like a quixotic endeavour.
&gt; quixotic endeavour. Java is the windmill.
It's just isolating the side effects to specific parts of the application (in the simple case, to the outer edges of the application). By removing mutation as a primary feature, the programmer is forced to look for the pure FP solution. How useful this is, I think depends on the scale of the codebase. Larger codebases benefit much more from having most of the code be pure. The overall architecture scales properly, and the programmer can rest assured that no funky stuff will happen with the state of the app, as it grows.
I'm not really experienced with Haskell, but some tips nevertheless: - `Keychain.Core.encrypt` and `decrypt` are almost identical - I'd probably extract the common function and write them as simple calls. - Most (if not all) uses of `Keychain.Core.assertExist/Writable` provide a potential race condition. E. g.: write :: BS.ByteString -&gt; FilePath -&gt; App () write b f = do assertWritable f liftIO $ BS.writeFile f b The file may become readonly between the `assert` and `writeFile`, so `writeFile` would throw after the successful assert. One would need an atomic check-writability and write to solve the race, which is exactly what a filesystem does - so I'd do the Python thing of asking for forgiveness rather than permission. - If the types match, you can replace case ... of ... -&gt; liftIO $ ... ... -&gt; liftIO $ ... with liftIO $ case ... of ... -&gt; ... ... -&gt; ... - Some functions can be shortened, although that's probably a personal preference, e. g. `Keychain.Core.passwordApp` (didn't compile myself): passwordApp :: ([SiteDetails] -&gt; App ()) -&gt; PasswordApp passwordApp f = do mdata &lt;- fmap Y.decode $ decrypt =&lt;&lt; encryptedFilePath fromMaybe (throw "Incorrect password") $ f `fmap` mdata
I built https://github.com/seatgeek/docker-build-cacher in order to preserve things like .stack-work between builds. It is a real time saver for when you need to re-build the project in docker after adding a cabal dependency.
a program without IO is just a value, it would be useless. The point is to make effects explicit and isolate them so they can be reasoned about and tested. 
I've read this post like 5x today and I still don't understand the point that it's trying to make.
&gt; ... and all that after which we'll decide if we can offer you a permanent position. Would you be interested in candidates that are not interested in a permanent position, but only in the internship instead? I am currently looking for internships to strengthen my haskell skills and 3 months paid internship in Japan sound like an amzing deal to me, but I am not sure whether that is what you are looking for.
It varies depending on our success on the market. Base salary is bigger than you mention in your email, annual bonus is comparable to it.
&gt;&gt; to see those contributing to the core ecosystem get routinely downvoted as they don't spent their time spreading FUD. &gt; The FUD and fighting has been multi-partisan and recurring for a long time now. In order for this tu quoque argument to work you'd need to provide compelling evidence supporting it. Can you provide examples of FUD being spread by those contributing to the core ecosystem?
Worked for telco, can confirm.
Seems like a terrible name. Generic and hard to remember. Isn't accurate or informative, just broad.
Isn't forever actually IO a -&gt; IO b because it doesn't actually return?
`leftAsAnExerciseForTheReader`? :)
Don‚Äôt look for a haskell job. Look for a job you‚Äôll be happy with and where management has the stance that results speak for themselves, and use the tools that help you achieve those goals. If Haskell helps, cool! If not, you have a job you‚Äôre happy with! 
The `IO` monad is what the Command "design pattern" from OO-land wishes it could be. Because everything that does side-effects (when evaluated by the runtime) is a value, and you can compose these values much more effectively than with the OO version, you have an incredible amount of control over when you choose to execute actions vs. simply passing them around. Because it's the way everything is done in haskell, it's pervasive, so you don't have the architectural overhead of choosing what needs to be written "normally" vs. "as a command".
Hi all. Spent my spare time building this as a learning exercise in Haskell. It's my first proper project in Haskell. I wanted to try and build something fun, and I think a NES emulator was a good choice. It ended up being much harder than I anticipated (the PPU mainly). A lot of the code is fairly IO based and imperative in nature, and it's probably the thing I like least about the code. I started with a typeclass based approach similar to what Jasper talked about in his post on the [DCPU-16](https://jaspervdj.be/posts/2012-04-12-st-io-dcpu-16.html). I ended up just hard coding to IO to get the FPS I needed.
Thanks! I actually wrote a blog post going into detail about this a few years back :) https://blog.jle.im/entry/first-class-statements.html
Regarding installation instructions, you can install sdl2 with pacman if you're using msys.
Cool. I'm going to plunk down a whole bunch of negative sounding issue right now because those are easy and some things just pop out - but nice work, don't take the list as negative feedback! * Canon type is not exported. Without this users can't write a function type signature that uses Canon. * GCR_ type is also not exported. * Shorthand functions clutter the API (ex: `makeC`). If you don't want to delete them then consider collecting them in one part of the haddocks via an explicit export list and subsections using `-- ** Subsection name` in the export list. * "Multiply Function: Generally speaking, this will be much cheaper." Cheaper than what exactly? Are there benchmarks? Same with add and subtract haddock comments "more expensive" reads badly. If there is no point of comparison just call them "expensive" and drop "more". The explanation of why they are expensive is illuminating. * Why do I have to see cMul et al instead of a `Num` instance? * " If the Canon is not integral, return False" It is possible for a value of type Canon to be non integral? I guess it would help to have an explanation of what `v :: Canon` should mean to me. * Consider replacing error strings with a custom enumerated type with an instance for Exception. * Why is the mod operator (`%`) exported? Oh, looking at the source the Canon type is an instance of Integral. If/when you export the type the instances will be visible too, which will help. * Looking at the source, I see the GCR_ stands for something sensible. I hope that expansion makes its way into the description - it's often infuriating to the beginner when TLAs and other acronyms are left to stand alone. (I'm looking at you, Linux kernel short hand variable names!) As I said, I'm aware that this all sounds negative but don't mean to be down - I'd happily ignore the library if it wasn't interesting. Nice work, it's good to see more number theory projects popping up.
I think the author is simply confused about the purpose of Monads. They seem to think that Haskell claims that monadic computations improve upon imperative computations. Starting from this incorrect assumption, they compare the syntax of both, find them very similar, and conclude that there is no improvement after all. They then ask if a different syntax, in which the state-passing is explicit, would be an actual improvement, or if it would be worse. Answer: it would be worse. Haskell did not start from the imperative syntax and semantics, but from the explicit state-passing syntax, which can only express `State`-like computations, not `ST` and `IO`-like computations. The imperative syntax is indeed much more convenient than the explicit state-passing syntax, and attempting to emulate it led to the discovery of the monadic API. Thanks to it, we can now write [imperative DSLs](https://www.reddit.com/r/haskell/comments/7uqkxy/monthly_hask_anything_february_2018/dtqhcad/?context=1) inside our purely-functional programs. The author also says "I am deliberately ignoring the fact that parts of a program could be pure in Haskell". In doing so, they deliberately fail the see the only sense in which monadic computations do improve upon imperative computations: writing our imperative computations as a monadic DSL allows us to embed those imperative computations inside a pure program, rather than [the other way around](https://www.youtube.com/watch?v=iSmkqocn0oQ).
[This change](https://ghc.haskell.org/trac/ghc/ticket/14733#comment:9) fixes it!
Looking over your code I don't see any cases of excessive use of liftIO. Seems pretty normal. The only real reductions you can do is make sure to never do the same IO operations in multiple functions, but that's more about refactoring to reduce total code than making less liftIO calls. Make a single "write file" function and compose it all over your code, etc. If possible reduce the times you print to the console to a single point at the end of a computation rather than doing it multiple times through out. But you don't do much console IO either like. Code looks good. 
If you attempted to use that as it's type, the compiler would require you to provide a way to do `(a -&gt; b)`. What would the benefit be?
Post your code? might be a known bug, might be a novel one
A PSA, if you think you have a bug it doesn't take long to create a ticket at https://ghc.haskell.org/trac/ghc/newticket and the people are friendly
Congratulations! I don't understand these maths about factorization methods, but there is one thing to comment. You have one unified type to represent natural numbers, integers, rational numbers, and (rational number \^ rational number)s. And your operations can fail depending on which hierarchy their arguments belong. (e.g. `quotRem` must fail on non-integral numbers.) I think it is easier to have *separate types* which reflect mathematical natures of operations, rather than performing validity check in every function. For example, to reflect these hierarchy, Natural ‚äÇ Integer ‚äÇ Rational ‚äÇ RationalRoots (RationalRoots = Rational ^ Rational) Natural ^ Natural = Natural Integer ^ Natural = Integer Integer ^ Integer = Rational Rational ^ Integer = Rational Rational ^ Rational = RationalRoots RationalRoots ^ Rational = RationalRoots RationalRoots ^ RationalRoots = (not representable in this hierarchy) ... you can have something like: data Sign = Plus | Minus data ZeroOr a = Zero | NonZero a data NaturalCanon = N [ (Prime, NaturalCanon) ] data IntegerCanon = Z (ZeroOr (Sign, NaturalCanon)) data RationalCanon = Q (ZeroOr (Sign, [ (Prime, Sign, NaturalCanon) ] )) : : class NaturalPow a where powN :: a -&gt; NaturalCanon -&gt; a class IntegerPow a where powZ :: a -&gt; IntegerCanon -&gt; RationalCanon class RationalPow a where powQ :: a -&gt; RationalCanon -&gt; RationalRootsCanon And each implementation can rely on their contract represented by types. You don't need to check `crValidIntegral` if compiler can statically check it for you. Also they fits standard Haskell number classes (`Num`, `Integral`, ...) more easily.
It's almost like they're ahead of their time or something.. It's almost like....they're.....IN.THE.FUTURE
If I were in your situation, I would either look at France or look for related technologies - purescript, elm, scala, f#, ocaml. I suspect Haskell grows from university or meetup epicenters. If you were willing to wait out for a year or longer, you could plant the seed by starting a meetup in Haskell and getting people committed to getting better at it each month. Eventually some member of that group will just irresistibly want to use it for some tasks at work. Then, that person will switch jobs and some manager will give her the chance to use Haskell more. I have no idea how many companies have been born indirectly from the Boston Haskell meetup, but I am sure it has planted the seed many times. It's strange because often Boston Haskell doesn't even touch topics that are typical for tech meetups aimed at commercial software developers. 
`exerciseForReader` is not bad
Thanks for your reply! Sounds really interesting.
[A great article for those wanting to go down this particular rabbit hole :)](https://www.fpcomplete.com/blog/2017/07/to-void-or-to-void)
That's fun! It inspired me to take a stab at it. I always avoid `!!` when I can, so I decided to try a holistic approach, where all the non-whitespace characters are replaced at once: fractalize' :: Pattern -&gt; Int -&gt; Pattern fractalize' base = go where blank :: Pattern blank = fmap (const ' ') &lt;$&gt; base grow :: Char -&gt; Pattern grow ' ' = blank grow _ = base step :: Pattern -&gt; Pattern step = concatMap (map concat . transpose . map grow) go :: Int -&gt; Pattern go n | n &lt;= 1 = base | otherwise = step (go (n - 1)) Which immediately inspired me to try the exponentiation-by-squaring approach: fractalize'' :: Pattern -&gt; Int -&gt; Pattern fractalize'' base n | n &lt;= 1 = base | even n = fractalize (double base) (n `div` 2) | otherwise = fractalize (double base) (n `div` 2) `subst` base pmap :: (Char -&gt; Pattern) -&gt; Pattern -&gt; Pattern pmap f = concatMap (map concat . transpose . map f) subst :: Pattern -&gt; Pattern -&gt; Pattern subst base = pmap $ \ch -&gt; case ch of ' ' -&gt; blank _ -&gt; base where blank = fmap (const ' ') &lt;$&gt; base double :: Pattern -&gt; Pattern double base = subst base base 
This is fairly standard practice in Japan. One of the reasons is because it's near impossible to fire someone once they've been hired full-time. This gives the employer an opportunity to make sure the fit is right before hiring.
This only applies to whatever value with a Lift instance, which is not the case for a lot of types e.g. functions.
If you want to be even more helpful, run a quick search in the bug tracker before submitting.
Shameless self promotion... https://github.com/erebe/wstunnel 
Thanks for your input. The reason for wanting to factor out the liftIO stuff is actually not to purify the code. I think I frased the question poorly. To clarify; I want to factor out IO in order to be able to test the Core functions without doing IO. I have updated the code to include a class IOProxy class Monad m =&gt; IOProxy m where encryptFile :: FilePath -&gt; Password -&gt; App m BS.ByteString decryptFile :: FilePath -&gt; Password -&gt; App m BS.ByteString writeFile :: FilePath -&gt; BS.ByteString -&gt; App m () readFile :: FilePath -&gt; App m String getHomeDirectory :: App m String doesFileExist :: FilePath -&gt; App m Bool doesDirectoryExist :: FilePath -&gt; App m Bool And make an instance of it like so instance IOProxy IO where encryptFile f p = App $ liftIO $ BS.readFile f &gt;&gt;= CBC.encrypt (pack p) decryptFile f p = App $ liftIO $ BS.readFile f &gt;&gt;= CBC.decrypt (pack p) writeFile f b = App $ liftIO $ BS.writeFile f b readFile f = App $ liftIO $ P.readFile f getHomeDirectory = App $ liftIO SD.getHomeDirectory doesDirectoryExist f = App $ liftIO $ SD.doesDirectoryExist f doesFileExist f = App $ liftIO $ SD.doesFileExist f And the App type now looks like newtype App m a = App { runApp :: (IOProxy m) =&gt; ReaderT Password (ExceptT Error m) a } deriving (Functor) Now I think I can make test of App without doing IO.
No, not at all. It is always useful to learn how the wider world see our beloved language.
It may just be a badly chosen title, his real question seems to be: **Is pure functional programming a waste of time?"** ...since the monad argument shows that "pure FP" embeds "imperative" and therefore can't be easier to "reason about" than merely imperative code. Commenter *marco* relates his personal experience: "Imperative Haskell programs seem to be the norm."
I've opened [a PR for this and other platforms](https://github.com/dbousamra/hnes/pull/2).
Template Haskell, which is GHC's compile-time evaluation mechanism, has never placed any restrictions on recursion. It always trusts you and evaluates whatever you give it.
There is no difference in functionality between the examples. The name shadowing warning is telling you that the `xs` you are binding in the `x:xs` is taking priority over ("shadowing") the `xs` bound on the left hand side of the function definition. This means that on the right hand side of that case the `xs` used will be the shadowing one and not the originally bound one. The compiler warns about this because you may have intended to use the shadowed variable and inadvertently named the other variable `xs`. My preference is maximum' = \case [] -&gt; error "empty list" [x] -&gt; x (x:xs) -&gt; max x (maximum' xs) There's no repetition of the function name, and no binding a name only to immediately eliminate it. Edit: clarified shadowing explanation
Considering that it's your first haskell project, I'm very impressed! 
So, literally the probation period.
Merged! Thanks
AFAIK the only way to do IO as atomically as possible is to just do it and catch an exception if it fails - that would happen in the worst case, might as well use it for everything, as nothing of value (performance, correctness, ...) seems to be lost this way. If you want to avoid partial writes, you would write to a new file instead and, if successful, rename over the old one - renames are _usually_ atomic. In this case, the assert would probably be a UX thing - if stuff is wrong, don't waste time writing files or something. In the end, since your entire app is wrapped in an `ExceptT.tryE`, why not wrap in an `Control.Exception.try` as well / instead?
Isn't it ironic how the pot is calling the kettle black by suggesting motives of wealth transfer while http://haskell-lang.org was created to push users to FPComplete's services and where *you* conveniently winnowed choices to [your own book](https://github.com/haskell-lang/haskell-lang/pull/84/files) under the pretense of serving the community better...
Thanks. I've done small projects here and there, but nothing lasting more than a day
The thing with Haskell is that once you have a Haskell developer, the productivity goes to infinity and you don't need a new developer. Unless the original one dies, but they usually transcend instead ... 
Happy to see this project come to fruition, nice job!
RE point 4: If you read the whole thing it kind of becomes clear that that multiplication is much cheaper than addition. So it's not comparing to other libs, it's comparing the operations of the lib itself. I agree, that the documentation at the functions does not really convey that. I think this should be in the readme / module documentation, because it is important to know when to use this. To OP: very interesting library! I learned some new things today :).
I too did an internship at Tsuru and it was awesome. Applying there will be one of the better decisions in your life.
Thank you very much for your explanation. Your sample indeed looks pretty clean. 
Thanks for taking the time to look at the code and give feedback. I'll try to address these points today. There would be a range of reasons as to why (some of which are omissions :) ).
It'd be fun to port this to accelerate and go hunt for primes. :)
These are the relevant pieces I'd use to build a solution: Matrix has a function called toLists which returns a list of lists; a list of rows where each row is a list of columns. You can iterate over lists with the function called map, transforming one list item into a new list item. You can use the syntax [1..] to generate an infinite list from 1 to infinity. You can use a function called zip to take two lists and join them together, giving you [(a, b)]. You can use a function called filter to select which items from a list you want to keep, and which you want to discard, based on the function you give it.
One thing to note is that IO is also needed to ensure all functions are pure (referentially transparent). This doesn't really answer the question, since one can immediately ask "What's the practical benefit of pure functions?", but the other answers ITT answer that as well.
In my opinion, `justDoIt` is much more informative than `synthesize` or `inhabit`. But hey, there are lot of nice colors we could paint the bikeshed.
Another benefit on top of being able to have functions that you know *don't* do IO and having first-class use of IO types are that once you are familiar with the interfaces for IO the distance to using things like parsers, STM, Concurrently, and all the other useful monads/applicatives becomes a lot smaller. The interface gives some improvements over non-monadic IO, but it's worth knowing independently of that too.
Google NES ROMs. As far as I know, it's only legal to download them if you have a physical copy of the game.
A `FieldNamer` is a function that takes three arguments, the name of the field being the last one. -- Take all FieldNamer's arguments namerWithMapping mapping fieldNamer tyName allFNames fName = -- No reason to use show here case lookup fName mapping of -- is the name mapped? Just lName -&gt; [TopName lName] -- if yes, override it with its mapping Nothing -&gt; fieldNamer tyName allFNames fName -- otherwise, defer to the original fieldNamer
This is https://github.com/ekmett/lens/issues/762 I have used such a "smart" field namer in the past to automatically rename some names that are just too common or are keywords. [Example](https://github.com/ekmett/coda/blob/9c1902f860df5b62dda1b0b33df1fb14928279b0/lib/coda-lsp/Language/Server/TH.hs#L34)
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [ekmett/coda/.../**TH.hs#L34** (master ‚Üí 9c1902f)](https://github.com/ekmett/coda/blob/9c1902f860df5b62dda1b0b33df1fb14928279b0/lib/coda-lsp/Language/Server/TH.hs#L34) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dulnfzd.)
challenge 
As in, run your `main` function? The REPL works just fine for that. You can `SPC m s b` to open it, then call `main`. Or, in a different terminal window, invoke `stack exec -- nameOfYourExecutable`.
Yes as in run `main` function. `SPC m s b` is a lot to type and I have to switch window. `, c c` is way easier. Since I need to edit, run the program constantly, I prefer not to use REPL
Another approach: `SPC p T`. Usually this runs your test suites, but you can edit the command to whatever you want. You could change it from `stack build --test` to `stack exec -- nameOfYourExecutable`, and then the results would appear in a minibuffer, probably just like the Python method you're used to. You'd only need to change the command yourself once, since it remembers it for you. Then, your next calls to `SPC p T` can be very fast.
It can be done with list comprehensions: import Data.Matrix reduced :: (Num a, Eq a) =&gt; Matrix a -&gt; [[(Int,a)]] reduced m = [[(j,a) | j&lt;-[1..ncols m], let a = m ! (i,j), a /= 0] | i&lt;-[1..nrows m]]
There's also no reason not to have `(...) = justDoIt` without unicode.
Was at going to suggest you make a Mockable IO monad like that. Nice! 
is it? once you read one or two these discussions are basically the same thing over and over again. they are also filled with misinformation and people who didn't actually try writing any Haskell saying how it's not a language that can be used for anything practical.
Is it [this](https://twitter.com/Iceland_jack/status/961801401516118016) post you had in mind? or another one? I also searched for [freer](https://twitter.com/search?l=&amp;q=freer%20from%3Aiceland_jack&amp;src=typd) and [cont](https://twitter.com/search?l=&amp;q=cont%20from%3Aiceland_jack&amp;src=typd), but i don't think they contain the tweets you mean.
I guess the OP meant internships for mid-career professionals, aka 3-months sabbatical. Are you open to this?
It's important to note that executing python vs. executing haskell is sort of an inherently different execution model, and that this is a big chunk of why this experience is different. You can't just throw `ghc myfile.hs` into stdin and expect it to execute the file. You can get something sort of similar with stack, but it's still kind of a strained comparison. What you want is not impossible, but this is why this is a less common usecase, and why it might be harder to wire up the plumbing to get the behavior you're looking for. Realistically, my recommendation for you would be to define a simple test to execute and assess your program's behavior instead of just re-running main - It's not really hard to set up, plumbing already exists for that case, and it's probably a better habit to get into anyway.
On older GHCs you used to be access `Foo.data` from a module named `Foo` with qualified names, and construct such variable names with template haskell. This is no longer legal.
I just want to say that content like this is awesome and incredible, even though it sails right over my head. I consider myself a journeyman Haskell developer. It's good to know there are people out there pushing the limits of Haskell's type system and discovering cool new things. I know the materials are out there for learning about stuff like this, but I haven't had the need yet. If I ever do, it's good to know people like Oleg and Bartosz are sharing their findings. Thanks, y'all! 
Thanks! I really wanted to see *...both Monad and Applicative are monoids in the category of endofunctors...* in not sarcastic context ;) 
This whole conceit is fundamentally flawed. There is nothing wrong with imperative code - A series of specific instructions is always a legitimate model for a series of specific instructions to be executed by a computer. Storing state in a named reference is likewise, a useful conceptual model. Sometimes you care about state, giving said container of state a name in a fixed context so that it can be referenced explicitly in a human readable fashion is also useful. The problem is with uncertainty as to what state some reference is in, or whether or not a particular references previous or future states must be anticipated. Haskell provides the idea of encoding this in an effect. You've presented Haskell sans that effect by way of the free monad - So you've actually taken a solution to the problem of state, or at least, a proposed solution, and deliberately broken it, and then observed that the solution is flawed. This is like pouring a bucket of paint on a marble statue and critiquing the artists use of color. 
Thanks for your feedback. The difficulty is that these operations don't all guarantee one type or another. This is why you can query the CanonValueType to see what type of result you have. This is set when you do the computation so I don't think it's expensive. For instance, Integer ^ Rational could be an Integer or a RationalRootsCanon There is arbirtary nesting, so you can have Rationals ^ (Rational ^ Rational). This sort of thing I just label as Irrational. You could also have RationalRoots ^ RationalRoots. Also, anything in the Internals module is meant to be internal.
This is the real answer - rerunning `main` all the time isn't usually the best way to confirm behaviour/correctness.
You can use arithmoi directly :) This uses it for prime checking and conventional factorization.
&gt; The last thing to do is FromDay instance for Comp. Because Hask has rich structure, we can convert Day into Comp. Are you referring to strength? In section 8.3 in [notions](http://www.fceia.unr.edu.ar/~mauro/pubs/Notions_of_Computation_as_Monoids_ext.pdf) the conversion is done in math notation. &gt; I think we can define Free also using a Path of Arrows and a first element. I leave that as an extended exercise for a reader. :) Do you mean something like this for applicative: data UFreeApAlt f a where PureAlt :: UFreeApAlt f () RecAlt :: f x -&gt; UFreeApAlt f y -&gt; UFreeApAlt f (x,y) data FreeApAlt f x = forall t. FAA (t -&gt; x) (UFreeApAlt f t) and this for arrow: data UFreeArAlt' f i o t where HomAlt' :: UFreeArAlt' f i o () CompAlt' :: f l o -&gt; UFreeArAlt' f (o, l) (x, (o, l)) t -&gt; UFreeArAlt' f l (o, l) (o, t) data FreeArAlt' f x = forall i o t. FArA' (t -&gt; x) (UFreeArAlt' f i o t) ? I had this laying around from earlier experiments. Also, I swear I read a blog post about someone who benchmarked this first applicative representation and he had interesting results, but I can't find the post again. EDIT: ok found it: https://www.eyrie.org/~zednenem/2013/05/27/freeapp Also related to your bonus sections: the conversion from day to compose, which allows you to convert the free applicative to free monad, can be seen as a result of an adjunction. In the case of *applicative -&gt; monad* it is an adjunction between the `Id` functor and itself, giving a very clean conversion `FreeAp f ~&gt; Free f`. The adjunctions for *applicative -&gt; arrow* and *arrow -&gt; monad* involve `Cayley`/[`ArrowMonad`](https://hackage.haskell.org/package/base-4.10.1.0/docs/Control-Arrow.html#t:ArrowMonad), and `ArrowMonad`/`Kleisli` respectively. These give conversions `FreeAp f ~&gt; ArrowMonad (FreeArr (Cayley f))` and `FreeArr p ~~&gt; Kleisli (Free (ArrowMonad p))` respectively. (`Cayley` is what you call `StaticArrow`). This conversion can be done general with an adjunction between two monoidal categories, where one is a lax monoidal functor. We have a paper accepted at IFL'17 in which this last idea plays a part. But the space constraint made the section fairly dense, so maybe it could be interesting to clean up and share the haskell code as well. 
Thanks for the feedback. No offense taken. 1) Will fix 2) Will remove. It's internal 3) Will remove 4) Will clarify in documentation. The presentation sheds light as well. The trick that enables you to handle massive numbers is that you (mostly) represent them as list of prime/ exponent pairs. Multiplying numbers just requires a little list surgery whereas addition/subtraction (mostly) requires refactorization. 5) cMul ... Will explain in documentation 6) See CanonValueType. Will pad doc 7) Exception: Will look into this. 8) Will remove. It wasn't really needed internally either. 9) It's internal but will add make the comments more consistent in this regard. 
What would be the performance penalty when the emulator would have been implemented with a state monad instead of IORefs?
I have thought an exercise related to this, how do you do it? A function such that (extended n xss) is the matrix with n columns whose reduced representation is xss. For example, extended :: Num a =&gt; Int -&gt; [[(Int,a)]] -&gt; Matrix a Œª&gt; extended 3 [[(3,4)],[(2,5)],[]] ( 0 0 4 ) ( 0 5 0 ) ( 0 0 0 ) It's about building from the reduced, the original
I can't really suggest any particular sites. Lots of games run, but lots don't. I support Mapper 2, 3 and 7 roms. You can see which roms are compatible here: http://tuxnes.sourceforge.net/nesmapper.txt Mappers are basically custom memory modules that exist on the cartridges themselves, that allow referencing more memory than the NES originally shipped with. Sometimes they even do computation. A very clever idea, but a nightmare to emulate, since each mapper has to be emulated as well I recommend Super Mario Bros 1, Megaman, Contra etc. 
I think that was one of them, there were others around that time which laid out the relationship more explicitly.
Hmm. The biggest performance problems are in the PPU functions. Here is a profile trace: Thu Feb 22 07:37 2018 Time and Allocation Profiling Report (Final) hnes +RTS -s -p -RTS roms/tests/dump/spritecans-2011/spritecans.nes total time = 29.23 secs (29227 ticks @ 1000 us, 1 processor) total alloc = 27,249,619,544 bytes (excludes profiling overheads) COST CENTRE MODULE SRC %time %alloc handleLinePhase Emulator.PPU src/Emulator/PPU.hs:(68,1)-(108,33) 14.5 18.4 tick Emulator.PPU src/Emulator/PPU.hs:(29,1)-(55,15) 13.6 13.9 &gt;&gt;= Data.Vector.Fusion.Util Data/Vector/Fusion/Util.hs:36:3-18 6.4 7.2 renderingEnabled Emulator.PPU src/Emulator/PPU.hs:(346,1)-(349,22) 6.4 7.0 renderPixel Emulator.PPU src/Emulator/PPU.hs:(111,1)-(116,31) 3.4 4.6 primitive Control.Monad.Primitive Control/Monad/Primitive.hs:152:3-16 2.3 1.9 &gt;&gt;= Data.Vector Data/Vector.hs:343:3-24 2.3 1.4 getComposedColor Emulator.PPU src/Emulator/PPU.hs:(145,1)-(164,17) 2.3 1.6 getSpritePixel Emulator.PPU src/Emulator/PPU.hs:(126,1)-(142,15) 2.1 2.1 step Emulator.PPU src/Emulator/PPU.hs:(24,1)-(26,32) 2.1 5.5 fetch Emulator.PPU src/Emulator/PPU.hs:(167,1)-(175,13) 2.1 1.4 getSpritePixel.colors Emulator.PPU src/Emulator/PPU.hs:128:7-38 2.0 1.6 getBackgroundPixel Emulator.PPU src/Emulator/PPU.hs:(119,1)-(123,41) 1.9 1.5 step Emulator.CPU src/Emulator/CPU.hs:(24,1)-(36,38) 1.7 2.2 primitive Control.Monad.Primitive Control/Monad/Primitive.hs:88:3-16 1.6 0.3 handleInterrupts Emulator.PPU src/Emulator/PPU.hs:(58,1)-(65,35) 1.6 0.8 writeScreen.\ Emulator.Nes src/Emulator/Nes.hs:(589,51)-(593,39) 1.2 0.7 writeScreen Emulator.Nes src/Emulator/Nes.hs:(589,1)-(593,39) 1.1 2.0 throwIf SDL.Internal.Exception src/SDL/Internal/Exception.hs:(37,1)-(41,10) 1.1 0.0 fetchTileData Emulator.PPU src/Emulator/PPU.hs:(210,1)-(212,38) 1.1 1.0 readNametableData Emulator.Nes src/Emulator/Nes.hs:(321,1)-(325,38) 1.1 0.8 readPalette Emulator.Nes src/Emulator/Nes.hs:(332,1)-(333,70) 1.0 1.5 basicUnsafeIndexM Data.Vector Data/Vector.hs:278:3-62 1.0 0.4 fetchLowTileValue Emulator.PPU src/Emulator/PPU.hs:(192,1)-(198,25) 1.0 0.6 basicUnsafeNew Data.Vector.Mutable Data/Vector/Mutable.hs:(99,3)-(102,32) 0.8 1.2 basicUnsafeFreeze Data.Vector Data/Vector.hs:(264,3)-(265,47) 0.8 2.4 step Emulator src/Emulator.hs:(14,1)-(16,36) 0.6 1.0 liftA2 Emulator.Nes src/Emulator/Nes.hs:161:20-30 0.5 1.2 basicUnsafeWrite Data.Vector.Storable.Mutable Data/Vector/Storable/Mutable.hs:(143,3)-(145,49) 0.4 1.2 The PPU does: - 341 PPU cycles per line (where we load data from memory); - 262 lines (each line on the TV); - 60 frames per second. So it's quite a lot of computation happening. I've profiled other emulators (fogleman/nes), and hnes seems to be a good 2-3x slower atm.
I don't know tbh. I'm still new to the ecosystem. I originally implemented this using ST a rather than hard coding to IO, and the typeclass instance lookup was killing performance by a good 3x. I've heard suggestions from a friend, that IORefs can be quite intensive, and that perhaps I could bundle more into a single IORef. So instead of having an IORef for every field in the PPU, I could combine them into a datatype, and just blat the entire thing even when 1 thing changes. No idea if it would yield any better results though
https://archive.org/details/NESrompack The Internet Archive [has a DMCA exemption to preserve vintage software](https://archive.org/about/dmca.php). NES certainly should count as vintage :) BTW, interestingly, some people have uploaded huge PS2, Gamecube and Dreamcast collections to the Archive too. And they still are not taken down‚Ä¶
Agreed, I [fail to see the problem](https://stackoverflow.com/jobs?sort=i&amp;q=%5BHaskell%5D&amp;l=Europe&amp;d=20&amp;u=Km).
This is brilliant! I love the implementation, too -- data Variant (types :: [*]) = Variant Int Any This is exactly a sum type under the covers, anyway :) I bet this type would be great for implementing a more sane checked exception scheme, too.
Why not? We don't really care if you are a student or not, we do care if you can work on something useful
Thanks! &gt; I bet this type would be great for implementing a more sane checked exception scheme, too. That's what led me to develop this in the first place :) I've described it in a [previous post](http://hsyl20.fr/home/posts/2016-12-12-control-flow-in-haskell-part-3.html) that has been [discussed here](https://www.reddit.com/r/haskell/comments/5jc9mt/control_flow_with_an_open_sum_type/)
hello all, i have no experience with writing code. I've been looking into languages and Haskell has really captured my interest. I am beginning to sort through the learning material listed in this sub as well as other options I have come across while googling. Does anyone have any suggestions where I might start considering my lack of experience? Has anyone in this sub who has become proficient started from my position? A lot of options seem to require some basic knowledge in other languages like python or C++ which I don't possess. I've seen comments stating that it's good to start with a clean slate while others seem adamant about having a little experience. It's a little overwhelming but I am up for the challenge. I'm just having a hard time figuring out where I should start....
&gt; Different keywords. enum instead of data, so that people do not have to learn about algebraic datatypes. I don't really see the purpose of this rename. Who is this for? Are you going to tell a Java or a C programmer that these are like enums in those languages, except they're not at all like enums? Besides, the concept of an algebraic datatype isn't going to be new to any programmer, the terminology and the ways to express them are different, but whether you're calling it/using a tagged union or a closed set of subclasses or whatever, it's all more or less the same thing and not at all unfamiliar.
Yes, it's a brilliant implementation, and incidentally is the same as in [HList](https://hackage.haskell.org/package/HList-0.4.1.0/docs/src/Data-HList-Variant.html). The [previous post on variants](http://hsyl20.fr/home/posts/2016-12-12-control-flow-in-haskell-part-2.html) by /u/hsyl20 was what inspired me to explore using variants. Unfortunately, I ended up creating my own variant library because I wanted to explore using a different api, but I used the same [implementation](https://github.com/louispan/data-diverse/blob/master/src/Data/Diverse/Which/Internal.hs#L129)
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [louispan/data-diverse/.../**Internal.hs#L129** (master ‚Üí 7111596)](https://github.com/louispan/data-diverse/blob/71115963c0a7e71a76af914a03e9954b9134c291/src/Data/Diverse/Which/Internal.hs#L129) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dumiptr.)
&gt; circles = catMaybes . map (Œª(Shape x) -&gt; cast x) That's a good point, storing a collection of variants with the same shape should probably be simplified with using `Typeable`. I think variants still have a use case in control flow, where there may be no common shape to the types in the variant, but you want to be able to "mark" of each type that has been processed. For example, given handlers that can handle `a`and `b`, you can reduce a `V '[a, b]` so that it only contains the left over possibilities. processCommandAB :: (a -&gt; ()) -&gt; (b -&gt; ()) -&gt; (V '[a, b] -&gt; ()) processCommandA :: (a -&gt; ()) -&gt; (V '[a, b] -&gt; V '[b]) 
Many recent books are focused on the experienced programmer. I suggest classics such as "The Haskell School of Expression" or "Haskell: The Craft of Functional Programming" by Simon Thompson. In addition, and particularly because you are coming at this from a clean slate, I suggest you look for real-time help. #haskell on irc.freenode.net is a great channel for discussing Haskell and getting started. As for if you are alone, no you aren't. I don't know about on this sub, but I have met people who learned Haskell as a first language. It's a thing, particularly for people who pick up programming as a side skill after something else, like a Math degree.
Thank you! I'll look on amazon for those books. I really appreciate the feedback
Assuming that pairs on lists are sorted: extendRow :: Num a =&gt; Int -&gt; Int -&gt; [(Int,a)] -&gt; [a] extendRow n i [] = replicate (n-i+1) 0 extendRow n i pss@((k,a):ps) = if i==k then a : extendRow n (i+1) ps else 0 : extendRow n (i+1) pss extended :: Num a =&gt; Int -&gt; [[(Int,a)]] -&gt; Matrix a extended n = fromLists . map (extendRow n 1) Sorry, I coudln't think on something simpler.
Turn on `AllowAmbiguousTypes` like it tells you to :) That extension allows you to write functions that can't be called without explicit type applications. So, `field` has the type `(Functor f, HasField field s t a b) =&gt; (a -&gt; f b) -&gt; s -&gt; f t`, there is a type parameter `field` that doesn't appear in any of the parameters. This is impossible to call in standard Haskell, so it's not allowed. See here: https://downloads.haskell.org/~ghc/8.2.2/docs/html/users_guide/glasgow_exts.html#ambiguous-types-and-the-ambiguity-check
sweet! I should add a macro for https://hackage.haskell.org/package/enumerate-function-0.0.1/docs/Enumerate-Function-Invert.html#v:isSurjectiveM 
Very cool, I was thinking of working on a language that compiled to lua myself. I'm curious does the compiled code run in 5.1, 5.2, 5.3?
&gt; type Covers a = (a -&gt; a) -&gt; a &gt; When the function given to surjective returns a value, it should call a covers function specifying a pattern. `Covers a` seems to be `Cont a a`, where you're handling the same type you're producing; more interesting than my requiring the input and/or output types to be brute force enumerable :-) I just started reading module, so maybe my question doesn't make sense, but is that more powerful than something like requiring an explicit image and/or co-image as a list? 
 I just got it set up this month, but fwiw, the limitations are as expected: c ffi calls to native/os apis obviously don't build, ffi to pure c calls (i.e. for performanxe, not features, like text) require shimming but work, and ghcjs implements all (afaict?) memory operations from the `GHC._` modules. Every library I've wanted is present. Since you mentioned graphical interface frameworks, `miso` and `reflex-dom` are reasonably documented. This project configuration works great for me: https://github.com/ElvishJerricco/reflex-project-skeleton 
and this tutorial https://github.com/hansroland/reflex-dom-inbits/blob/master/tutorial.md along with these references https://github.com/reflex-frp/reflex/blob/develop/Quickref.md https://github.com/reflex-frp/reflex-dom/blob/develop/Quickref.md gave me enough info to make a GUI. 
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [hansroland/reflex-dom-inbits/.../**tutorial.md** (master ‚Üí 33c3124)](https://github.com/hansroland/reflex-dom-inbits/blob/33c312484498685908944835239716cd54f36a3e/tutorial.md) * [reflex-frp/reflex/.../**Quickref.md** (develop ‚Üí 6a8c4ef)](https://github.com/reflex-frp/reflex/blob/6a8c4efe9a181cc2d97900f5a7519c46cb13e0bb/Quickref.md) * [reflex-frp/reflex-dom/.../**Quickref.md** (develop ‚Üí f4820df)](https://github.com/reflex-frp/reflex-dom/blob/f4820df681b177fb950eb180aa97a81f855bd2aa/Quickref.md) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dumota3.)
I've used the [`Exists`](http://hackage.haskell.org/package/exists-0.2/docs/Data-Exists.html) type from the `exists` package to do just this in the past.
i can't expect anyone at my company to use it, tho i do enjoy it
Plus http://docs.reflex-frp.org/en/latest/
&gt; `Covers a` seems to be `Cont a a` The types coincide, but the meaning is completely different! The `a -&gt; a` is not a continuation, it is a crutch for `(Pattern a, a)`. I can't actually ask for a pattern, so instead I ask users to write `covers $ \{Pattern a} -&gt; {a}` and I use the pattern and the value separately. &gt; is that more powerful than something like requiring an explicit image and/or co-image as a list? Yes, since my approach does not require the domain and codomain to be finite.
Sorry, it is not ‚ÄúML‚Äù if it doesn't have signatures and functors. The type theory behind modules is actually rather subtle. You have to give proper answers to questions like: * What is the principal signature of a module? * When does a signature refine another? * What is the scope of an abstract type generated in a module? You can't just handwave these questions away with: &gt; Tomorrow will be a good day, good that I have everything planned in my ModuleSystem.
thanks! (Yeah, it wasn't clear whether it's relevant or coincidental, but it makes sense that it has its own name.)
The sentence from the README is a bit tendentious but the idea of extending `enum` to cover algebraic data types in general makes sense and creates a natural path for people coming from C/Java-style languages to understand what's going on. That's what Swift did and I think it's a reasonable decision, although I highly prefer the Haskell/ML syntax myself.
You have four points. If it is a square, then you can pick one point, and two of the other points will be the same squared distance away, and one wil be twice the squared distance. If this holds for two points, you have a square.
* I think you consider this http://haskellbook.com/. But consider that this will take some time, but well worth it. * Another option if you don¬¥t want to buy the book is to follow this learning path https://github.com/bitemyapp/learnhaskell from the Author of the tip above
Hmm, yeah, I wasn't sure how safe -XAllowAmbiguousTypes was, but it makes sense that it is required for type applications. After enabling, I ran into this issue at a call site: ./src/Main.hs:252:28: error: ‚Ä¢ Cannot apply expression of type ‚Äò(a0 -&gt; f0 a0) -&gt; s0 -&gt; f0 s0‚Äô to a visible type argument ‚Äò"_prob"‚Äô ‚Ä¢ In the first argument of ‚Äò(.)‚Äô, namely ‚Äòf @"_prob"‚Äô In the second argument of ‚Äò(^.)‚Äô, namely ‚Äòf @"_prob" . to Down‚Äô In the first argument of ‚ÄòsortOn‚Äô, namely ‚Äò(^. f @"_prob" . to Down)‚Äô | 252 | mapM_ print $ sortOn (^. f @"_prob" . to Down) $ probListFlat (do | ^^^^^^^^^^ So I tried giving some explicit type signature to `f` like you described: f :: (Functor f, GLens.HasField field s t a b) =&gt; (a -&gt; f b) -&gt; s -&gt; f t f = field ./src/Main.hs:349:18: error: ‚Ä¢ Expecting two fewer arguments to ‚ÄòHasField field s t‚Äô Expected kind ‚Äò* -&gt; * -&gt; Constraint‚Äô, but ‚ÄòHasField field s t‚Äô has kind ‚ÄòConstraint‚Äô ‚Ä¢ In the type signature: f :: (Functor f, HasField field s t a b) =&gt; (a -&gt; f b) -&gt; s -&gt; f t | 349 | f :: (Functor f, GLens.HasField field s t a b) =&gt; (a -&gt; f b) -&gt; s -&gt; f t | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^ In fact, I've tried some more variants as well, none of which compiled for me: -- f :: Lens s t a b -- f :: (GLens.HasField field s t, Functor f) =&gt; (a -&gt; f b) -&gt; s -&gt; f t -- f :: (GLens.HasField field (GHC.Generics.Rep s) a, Functor f) =&gt; (a -&gt; f b) -&gt; s -&gt; f t -- f :: (GLens.HasField field (GHC.Generics.Rep s ()) a, Functor f) =&gt; Lens s t a b Could you get it to compile somehow?
I'm real riled up to give it a try. We'll see if someone beats me to it :D
Ok, I think i found it: https://twitter.com/Iceland_jack/status/961477936334467072
https://haskell-miso.org/
Sorry, maybe I browsed your source code too much and confused public and internal API. And my comment was not objective, I just like to have tighter type in general. I said `RationalRoots ^ RationalRoots` is unrepresentable. That was not correct. But was close. Please let me explain. Let me shorthand `Q = Rational`. `Q^Q` and `Q^Q^Q` is representable in `Canon`. Because addition between `Q^Q` fails except for special cases, multiplication between `Q^Q^Q` fails in general. Also, there are `Q^(Q^Q^Q)` not representable in `Canon`. With `Canon`, none of the addition, multiplication, or power involving `Q^Q^Q` can be defined totally. So `Q^Q^Q` was representable, but the results of any common operations applied to them were not.
Yes that was it! Apparently I slightly misremembered w.r.t. what the connection is.
[removed]
&gt; I'd rather not have to use FFI &gt; scanning file data IN MEMORY. How do you expect this to work? Shared memory is certainly possible, but you AV software must have something like that ready, and it will very much look like using the FFI on the Haskell side. Please note that the FFI is really easy to use.
Seems like this would be far easier to implement as a [source](https://github.com/ghc-proposals/ghc-proposals/pull/107) [plugin](https://phabricator.haskell.org/D4342).
Just yesterday I finished one smallish project that used heavily `generics-sop` approach to sums and products and I was thinking about the 'tagging' approach. It would be neat to have something like: ``` data Product f (v :: [*]) apply :: (forall a. f a -&gt; a -&gt; r) -&gt; Product f types -&gt; V types -&gt; r ``` This would allow something like: ``` newtype Handler r a = Handler (a -&gt; r) handlers = Handler ("string: " &lt;&gt;) :* Handler (\n -&gt; "int: " &lt;&gt; show n) :* Nil :: Product Handler '[String,Int] v1 = V 1 :: V '[String,Int] apply ($) handlers v1 ``` 
&gt; you can't really reason about imperative code at all without some sort of model to base that reasoning on This is a great observation which is key to addressing the whole question, IMO. 
Awesome. Looking forward to using this. 
Do you mean something like [this](https://github.com/louispan/data-diverse/blob/71115963c0a7e71a76af914a03e9954b9134c291/test/Data/Diverse/WhichSpec.hs#L204)? it "can be 'switch'ed with 'Many' handlers in any order" $ do let y = pickN @0 (5 :: Int) :: Which '[Int, Bool, Bool, Int] switch y ( cases (show @Bool ./ show @Int ./ nil)) `shouldBe` "5"
To clarify `y` above is a variant of type `[Int, Bool, Bool, Int]` (position dependant). `show @Bool ./ show @Int ./ nil` is a record of type `[Bool -&gt; String, Int -&gt; String]`. `switch` is a function that inspects the variant and uses the record in `cases` to handle the different types in the variant, reducing the variant to a `String`
This is really easy with `process-extras`: https://gist.github.com/chpatrick/27aaef49fcc23541527c6e0bfa93a63d *Lib Lib&gt; :set -XOverloadedStrings *Lib Lib&gt; virusScan "not a virus" VSRClean *Lib Lib&gt; virusScan "X5O!P%@AP[4\\PZX54(P^)7CC)7}$EICAR-STANDARD-ANTIVIRUS-TEST-FILE!$H+H*" VSRVirusFound "stdin: Eicar-Test-Signature FOUND\n\n----------- SCAN SUMMARY -----------\nKnown viruses: 6418772\nEngine version: 0.99.3\nScanned directories: 0\nScanned files: 1\nInfected files: 1\nData scanned: 0.00 MB\nData read: 0.00 MB (ratio 0.00:1)\nTime: 27.715 sec (0 m 27 s)\n"
I see, this is using Seq; I looked into the `superrecord` mentioned above and they were using some kind of array. Seq is probably better for arrays that can be changed. 
I have been using tuples instead of a list: import Haskus.Utils.ContFlow -- continuations in the Variant type-list order testCont1 v = variantToCont v &gt;::&gt; ( \ (Circle r _) -&gt; "circle radius: " ++ show r , \ (Square _ w) -&gt; "square width: " ++ show w , \ Triangle -&gt; "a triangle" ) -- continuations in any order testCont2 v = variantToCont v &gt;:%:&gt; -- notice the "%" ( \ (Square _ w) -&gt; "square width: " ++ show w , \ Triangle -&gt; "a triangle" , \ (Circle r _) -&gt; "circle radius: " ++ show r ) &gt; fmap testCont1 shapes2 ["square width: 5.0","circle radius: 4.0","a triangle","circle radius: 1.0"] &gt; fmap testCont2 shapes2 :: [String] -- need some help ["square width: 5.0","circle radius: 4.0","a triangle","circle radius: 1.0"]
I decided to use Seq as my uses cases involves appending and changing the record, so I'm guessing Seq might have better performance characteristics, but I haven't really focused on benchmarks to check.
Thanks for your feedback. Indeed I should rework the annex to separate product/sum types. superrecord looks very interesting!
Yea the n^2 instances problem is solved by the `Eff` style libraries, not `Free` alone
I always thought this was dumb. That article just reinvents monad transformers and claims Cont is somehow unique in that. The `i` function the whole post is contingent on is just `lift @ContT`.
It would be even nicer if it wasn't limited to some maximum tuple size, but this is very good.
Same question, since mod system in some games only accept 5.2.
Upon reevaluation, it does seem to not be getting at anything as fundamental as I previously thought. The paper it cites [here](http://www.diku.dk/~andrzej/papers/RM-abstract.html) seems to say nothing more than that, for any monad that can be expressed in purely functional terms, you can express it in terms of continuations. Perhaps this notion was more surprising when monads were a relatively new concept for Haskell and the like.
&gt; Dependencies base (&gt;=4.8 &amp;&amp; &lt;4.11), lens, ... Even spamers need `lens` these days.
Actually I am just curious on how people work with distributed process on Haskell nowadays. ps: I completely agree with you that the vast majority of tasks could be solved with some local queues.
I have to solve for any number 
I tend to distinguish two parts of the ML family: - ML cousins: HM-inspired type system, strict, algebraic datatypes, emphasis on functional programming. This includes Scala, F#, etc. Haskell and Clean are a very far cousin. - ML descendants: The real deal, which contains all the above *plus* a proper module system with parametrized modules, module types and the ability for datatype abstraction. This includes SML, OCaml, 1ML, etc. Cousins are cool too. Hypatia is a very young cousin. :)
Well this is what astonish me. Haskell/Hackage is pretty niche, with specific procedures for uploading... Can a bot infer the right procedure for uploading? Even the .cabal file was modified correspondingly...
I'd be exceedingly impressed if this was an artificial neural network's work today. Maybe not so much in 5-20 years.
Look at the successive versions. The logo only appears a few versions in, same for the author, the homepage, etc. Looks to me more like someone toying around with hackage, descriptions.
Wow, Applicative parsers are all the rage this week, there are 3 posts on the topic! (plus a livecoding video which was posted here but didn't make it in this highlight reel)
OK. So then you need to limit `k` to small enough so that the log base `b` of `k` still has at least about 2 digits of precision in its fractional part. That will still work for bases `b` that are quite large. If you need to go beyond that, you'll need to use your own higher-precision algorithm for computing the logarithm instead of using the built-in `log` function.
Are you coordinating with the maintainer of vulkan on hackage to get this up there?
so your goal is to find a pair of natural numbers `k` and `n` which obey the following equation for some natural number `x`: x * 10^n &lt;= 2^k &lt; (x + 1) * 10^n if you take the logarithm of this equation you get: ln(x * 10^n) &lt;= ln(2^k) &lt; ln((x+1) * 10^n) ln(x) + n*ln(10) &lt;= 2ln(k) &lt; ln(x+1) + n*ln(10) ln(x) &lt;= 2ln(k) - n*ln(10) &lt; ln(x+1) ln(x)/ln(10) &lt;= 2*ln(k)/ln(10) - n &lt; ln(x+1)/ln(10) Note that for any `a`: `ln(a)/ln(10)` is equal to the base 10 logarithm of `a` Excellent. So your program can calculate `ln(x)/ln(10)` and call this `lb`, and calculate `ln(x+1)/ln(10)` and call it `ub`. Now do the following: * Start with `k = 0` and `n = 0` * If `2*ln(k)/ln(10) - n &gt;= ub` then `2^k` is too big, increment `n` and make our target bigger. * If `2*ln(k)/ln(10) - n &lt; lb` then `2^k` is too small, increment `k` * Otherwise, out number is within our target bounds, and we have searched all `k` lower than us, so the current `k` must be the minimum.
I see you filed this issue, which seems indeed like th correct thing to do. https://github.com/haskell-infra/hackage-trustees/issues/132
Can I use haskell to learn math while learning programming? I'm an imperative SWE now with a mech e background, and I fantasize about going to grad school for math/cs. Does learning haskell align with these loose goals at all?
No it doesn't have upper or lower bounds on many of its dependencies. We should kick it out of Hackage.
&gt; It doesn't litter your types with useless extra points when you don't need them. Can you clarify this? I'm not sure what "extra points" means here in this context and I'm curious to know :)
The problem is that once a spammer has worked out how to automate package uploading we're going to be inundated with spam packages for a while.
The intro is both adorable and informative, I like it!
[removed]
[Here you go](https://en.wikibooks.org/wiki/Haskell/Denotational_semantics#%E2%8A%A5_Bottom).
Here's that same closure type, from 7 years ago, closed as won't fix. Work around is "try it again".
Would a CAPTCHA system help?
It is strange how issues of Haskell Weekly sometimes end up with themes like this! Obviously I have a hand in it because I select the stories to feature, but all three of the posts were published in the past week. Synchronicity!
No. There were two packages on hackage: [vulkan](https://hackage.haskell.org/package/vulkan) and [Vulkan](https://hackage.haskell.org/package/Vulkan). I did not manage to go beyond vkCreateInstance with either of them and started a new project from scratch. As a result, the library is substantially different from those two. The new one is called [vulkan-api](https://hackage.haskell.org/package/vulkan-api). 
[removed]
Have you seen the `CoRec` type from Vinyl? It seems like it fulfills mostly the same purpose, that is, you could just have a list of these co-records.
`ln(2 ^ k) = k * ln(2)`, not `2 * ln(k)`.
Wait, what if I actually want to hack facebook?
What?? 
# P2 = SCANL (\A B - A+B) 1 P2 -- POWERS OF TWO DIGITS = CEILING . LOGBASE 10 DP2 = MAP DIGITS P2 LOWESTPOWER N = HEAD [ (K, M) | (K, (D, P)) &lt;- ZIP [1..] $ ZIP DP2 P2, LET R = D - DIGITS N, N == P `DIV` 10^R] #BETTER WITH BINARY SEARCH.
No problem. Thanks for looking at this. The next big revision will allow for expression trees (e.g. adding 1 to a Q^Q^Q would just be an irreducible expression) so there will be fewer failures. Initially, the expectation was that this would only be used interactively (mainly because the prospect of extremely slow factorization seemed to rule out using it from code). 
A great example of an application that runs in GHCJS is /u/cdsmith 's [CodeWorld](https://code.world).
Yes.
&gt;&gt; Since the check is entirely syntactic &gt; &gt; Can you elaborate? The `surjective` and `covers` functions do not have any runtime semantics. I simply look at the code within the `$(surjective [||\covers -&gt; ...||])` block, gather all the patterns given to the `covers` function, and complain if the patterns are not exhaustive. &gt; I wanted to make sure a quickcheck generator covered all parts of a complicated recursive GADT This seems ideally-suited for `surjective`, but while writing an example I realized that type inference isn't very good for case statements involving GADTs: data Nat = Z | S Nat data SingNat n where SingZ :: SingNat 'Z SingS :: SingNat n -&gt; SingNat ('S n) MissingCase :: SingNat 'Z -- error: 'p' is untouchable generatedCode = case undefined of SingZ -&gt; () Since I'm using typed TH, I think I should be able to generate an appropriate type signature in order to get rid of this "untouchable" error so we can get a warning about `MissingCase` instead. &gt; A purely syntactic approach would require structuring the code in a single large case statement, which would introduce duplication and make it harder to read. I don't understand. An implementation of `arbitrary` for a GADT such as the above would look like this: instance Arbitrary (SingNat 'Z) where arbitrary = pure SingZ instance Arbitrary (SingNat n) =&gt; Arbitrary (SingNat ('S n)) where arbitrary = SingS &lt;$&gt; arbitrary That is, the implementation is divided into multiple parts, none of which includes a case statement, large or small. Once I fix the above problem, I expect that writing this should be able to detect the missing case: instance Arbitrary (SingNat 'Z) where arbitrary = $$(surjective [||\covers -&gt; pure $ covers $ \SingZ -&gt; SingZ ||]) -- Warning: Pattern match(es) are non-exhaustive -- In a case alternative: Patterns not matched: MissingCase instance Arbitrary (SingNat n) =&gt; Arbitrary (SingNat ('S n)) where arbitrary = $$(surjective [||\covers -&gt; do r &lt;- SingS &lt;$&gt; arbitrary pure $ covers $ \(SingS _) -&gt; r ||]) Although, when I tried this, I got yet another strange error about a missing instance of `Arbitrary (SingNat n)`, so maybe I'm not out of the woods yet...
lens, servant, and you're all set ! 
`p2` is the powers of two: Œª p2 = scanl (\a b -&gt; a+b) 1 p2 -- powers of two Œª take 10 p2 [1,2,4,8,16,32,64,128,256,512] `digits` is a function that tells you how many digits long a number is Œª digits = ceiling . logBase 10 Œª digits 12345 5 Œª digits 123456789 9 It's got an off-by-one error, so it's wrong for actual powers of 10, but since we're only using it on powers of 2, it's accurate enough. `dp2` tells us how many digits long each power of 2 is: Œª dp2 = map digits p2 Œª take 10 dp2 [0,1,1,1,2,2,2,3,3,3] `lowestPower n` is a function that iterates through all the powers of 2, and finds the first one where its initial digits are equal to `n`.
This is not a matter of feelings. If the language's semantics doesn't allow you to *reason* about programs in a certain way, it is not ML. --- The Haskell approach is to enforce abstractions on the client side. For example, the type signature `sort :: Ord a =&gt; [a] -&gt; [a]` guarantees that `sort` won't, say, add integers, even if you pass it a list of integers. (Assuming sensible `Ord` instances, of course.) In this case, `sort` is the client of the `Ord` abstraction. However, Haskell has no means to enforce abstractions on the implementor side, beyond hiding data constructors, if that even counts. For instance, say you want to bootstrap a data structure `Herp` from another data structure `derp`, whose implementation details don't matter so long as you have an `instance Derp derp`. A plausible implementation of `Herp` looks something like: data Herp derp a = ... empty :: Herp derp a empty = ... cons :: Derp derp =&gt; a -&gt; Herp derp a -&gt; Herp derp a cons = ... uncons :: Derp derp =&gt; Herp derp a -&gt; Maybe (a, Herp derp a) uncons = ... merge :: Derp derp =&gt; Herp derp a -&gt; Herp derp a -&gt; Herp derp a merge = ... Haskell doesn't give you the means to make the concrete choice of `derp` not matter. Every bootstrapped instance of `Herp` will be a `Herp derp1` or `Herp derp2`, where `derp1` and `derp2` are concrete types that must be known by the user. Note that `ExistentialQuantification` does not help: data AnyHerp = forall Derp derp. AnyHerp (Herp derp) -- merge is outright unimplementable --- The ML approach is to allow you to decide whether you want to enforce abstractions on the implementor side (using opaque signature ascription) or on the client side (using functors). You can even use both at the same time, since the implementor of an abstraction is often a client of another: signature HERP = sig type 'a herp val empty : 'a herp val cons : 'a * 'a herp -&gt; 'a herp val uncons : 'a herp -&gt; ('a * 'a herp) option val merge : 'a herp * 'a herp -&gt; 'a herp end functor BootstrapHerp (D : DERP) :&gt; HERP = struct datatype herp = ... val empty = ... val cons = ... val uncons = ... val merge = ... end Note that the specification of herps does not mention derps in any way‚Äîderps are an implementation detail.
5 hours after first seeing it, I just realized why they put this package up on "Hack"age instead of some other place with more hits such as NPM. I'm dumb.
Thanks! I haven't really thought about the case. I meant something like FreeMonad m a = ArrowMonad (FreeArr (Kleisli m)) a but then, making (a -&gt; m b) without using fmap would need some massaging, so I don't know what one would end up with. IFL'17 looks like nice collection of sessions. I have to read proceedings! http://iflconference.org/ to save others a googling effort. 
Another aspect of this is "ML" refers to multiple things. * ML the calculus (HM System F + products, sums, recursive types) * ML the syntax (as opposed to C or Algol, the style of defining functions by cases) * ML the language (calculus + modules) Languages do not have strict parent/child relations with each other, each new language is technically created tabula rasa, but influenced by others. So historically, Haskell isn't truly a decendent of ML in any of the above categories, but Haskell did evolve along side of ML and is heavily influenced by it. At what point influence turns into memetic relation is subjective, but there's enough influence that I wouldn't say it's wrong to call Haskell an ML family language (where ML means the calculus or the syntax.)
&gt; FreeMonad m a = ArrowMonad (FreeArr (Kleisli m)) a I don't think the other way around will work. Yes that is the link, but those are the draft proceedings, which only required an extended abstract I think. In our case, the paper for the post-proceedings is much more refined, and probably for the others as well.
&gt;Against automated spam, I'd say most definitely. I've done work that involved web scraping, captchas aren't as much of an obstacle as you'd think.
Are you thinking of Google's reCAPTCHA?
 let x = raise UndefinedException in x end Looks bound to me. That trying to evaluate that binding causes an exception to occur isn't fundamentally different from waiting for the use of the binding. Computationally, the result is the same (here.) Ultimately, the type of `x` is a lie for any type I might try to give `x` as there will never be a value of that type for `x` to be. So there's effectively an extra point in every type even in SML. However, [fast and loose reasoning is morally correct](http://www.cse.chalmers.se/~nad/publications/danielsson-et-al-popl2006.pdf) in any language so ignoring the existence of exceptions is just as valid as ignoring the existence of undefined for doing induction on a data type.
(mfw ltu crosses over into hn)
I've never used it, or even seen it used.
What I meant by that was that I didn't want to have to implement it myself using FFI. It's really a time issue not a difficulty one. But it is nice to know FFI is "easy". Hope my "easy" == your "easy".
Who says I'm unaware? I'm saying your claim that `x` is not bound in the let body is wrong. It is bound. That call-by-value doesn't allow you to compute the body of the let is immaterial to my argument. `raise UndefinedException` occupies every type. It's semantically valid in any place a value of any type would be expected.
The variable `x` is bound *statically* to a `val` binder, of course. But it is never bound *dynamically* to any value.
Hey, I wrote that code! I guess I should expect to be questioned at immigration next time I fly to the US/UK?
&gt; However, fast and loose reasoning is morally correct No. Especially not if you want to reason not only about the correctness of the final result, but also about the time and space complexity of the algorithms that compute them.
I didn't claim that it was bound dynamically.
I won't comment on ‚Äúcoolness‚Äù, which is inherently subjective. But calling Scala an ‚ÄúML cousin‚Äù is abusive. At least F#, Haskell and Rust pretend to have ML-like datatypes, even if they fail for various reasons, and don't bother with modularity at all.
&gt; And when you use a variable in ML, you can be sure that it's bound to a value. Dynamically yes. If evaluation reaches a variable it's bound to a value. Statically, no. I can 'use' a variable that isn't bound to a value, that's the point of my let example. Haskell allows me to say yes, even statically, all variables are bound to values. In SML, inside a fragment of code a variable will have a value that matches the ones defined in its datatype. However, outside of that fragment I still have to account for the value we're attempting to bind being bottom. This is preferable to not knowing inside of a fragment, but it doesn't change the fact that you still have extra points in your types. 
I literally can bind them to variables though. To repeat (with your correction) let val x = raise UndefinedException in x end is a binding of an exception to a variable. It's a binding both syntactically and in the static semantics of SML. You're right that it's not a binding in the dynamic semantics.
Statically, you can't ‚Äúbind an exception‚Äù to a variable. The only information statically assigned to a variable is its type (schema).
Scala does have ML-ish features though. In particular with generics and interfaces, which have an expressivity that is very similar to functors (including abstract datatypes). If you look at [this paper](http://lampwww.epfl.ch/~amin/pub/big-step.pdf) on the DOT calculus, you see that formalizing Scala's core clearly exhibit similarity with some module calculus. Sure, it's not a direct ML descendant, but there is a certain kinship and inspiration.
&gt; For instance, say you want to bootstrap a data structure Herp from another data structure derp, whose implementation details don't matter so long as you have an instance Derp derp I believe the "Backpack" module system allows something a bit like this. For example, here's a signature for a effectful stream that requires the type to have `Functor` and `Monad` instances.
In a call-by-value, you have to be careful whether you are talking about a type as a collection of values or as a collection of expressions, since they have different denotations. The former denotes some object `X`; the latter, another object `T(X)`, where `T` is a monad, yadda yadda...
Can two applications of the same functor (in the ML sense) coexist in the same program?
You mean, like two Herps built out of different derps? Yes. For example, the [testsuite](https://github.com/danidiaz/streamy/blob/master/streamy-testsuite/tests/tests.hs) runs the tests specialized for three distinct libraries. The "functor applications" seem more lighweight in ML. In Backpack, they are done by lining up signatures with implementation modules in the [mixins](https://github.com/danidiaz/streamy/blob/master/streamy-testsuite/package.cabal#L62) section of the .cabal file. 
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [danidiaz/streamy/.../**package.cabal#L62** (master ‚Üí 2d16756)](https://github.com/danidiaz/streamy/blob/2d167569f3a3ea82eeb1b664d444a5c637618859/streamy-testsuite/package.cabal#L62) * [danidiaz/streamy/.../**tests.hs** (master ‚Üí 2d16756)](https://github.com/danidiaz/streamy/blob/2d167569f3a3ea82eeb1b664d444a5c637618859/streamy-testsuite/tests/tests.hs) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dunzldd.)
If functor application has to be done using Cabal, then I won't use functor application. But it's nice to know that it's possible. :-)
Market price for recaptchas is about $2 per thousand recaptchas defeated. How many captchas are we proposing to make package uploaders solve?
Sure. However, I'd note that only syntax objects can be written. You can't write values. Which is why as a writer of programs "binding syntax to variables" is really the most important denotation of "binding". The dynamic binding of variables to values is what we intend to occur, but it's not what we write. So long as exceptions and non-termination are not values you can write bindings that never actually bind anything, you can write fragments of code using bindings that never materialize (and in a call-by-value language you end up not materializing the computation that uses them.) 
&gt; Which is why as a writer of programs "binding syntax to variables" is really the most important denotation of "binding". The syntax for raising an exception isn't the exception itself. So my point still stands: you can't bind an exception to a variable (unless it's of type `exn`, of course). &gt; The dynamic binding of variables to values is what we intend to occur, but it's not what we write. ‚ÄúIntend‚Äù? No. It's what you have to *prove* occurs. Rigorously, not fast and loosely. &gt; So long as exceptions and non-termination are not values you can write bindings that never actually bind anything Again, if you don't want this to happen, prove it. In fact, I do something even stronger than this in my day to day programming: To the extent that I can (i.e., when I don't run into fundamental limitations of the type system), I prove that every control flow point in my program is reachable by a legitimate computation.
Yep, and you'll get a special "Bad Hombre" visa stamp
I see that your implementation strategy is to run retry failed transactions after a certain amount of time, using exponential backoff to avoid repeatedly running the same failing transaction (by the way, you're doubling the delay but you seem to have [forgotten](https://github.com/graninas/stm-free/blob/c5af4214c82db7dd843b12df479c50f75169bae1/src/Control/Concurrent/STM/Free/Internal/Impl.hs#L52-L62) to actually sleep for the delay's duration). This isn't guaranteed to work, since two transactions can cause each other to fail, in which case they will both double their waiting time, and then they will conflict with each other again. Randomized exponential backoff would be better, but in this particular case, there is a better solution. Since you know exactly which TVars and which transactions are involved in the failure, you can arrange so that aborted transactions wait in line for the relevant TVars to me modified (in the case of a `retry`) or for the relevant transactions to be completed (in the case of a conflict). This way you can use TVars as a notification mechanism, just like in the real STM. 
Sure thing! And also it would be great if someone finishes the work on [bindings-GLFW](https://github.com/bsl/bindings-GLFW/pull/44), which is used in vulkan-examples :) the version from that PR works on linux only.
Insightful comment, thanks! You're right, I forgot delaying :( I was thinking about notification mechanism, but I didn't understand why I should use it. Now I see! Will fix this in the meantime.
I'd be curious to see your source for that price, but assuming that's accurate, I'm guessing that comes from very low income countries' manual labor. And in that case, 0.2 cents per solved recaptcha, which is many orders of magnitude more expensive than not having one. How much does that cost? 0.00002 cents? And we only would have to raise that cost bar above the level of which these spam attempts are projected to earn for their creators. Suddenly they are losing money even trying. So I'd wager they'd stop pretty fast And afaik many automated spamming methods rely on the shotgun method: try to send as much as possible, and only a tiny fraction of spam sent actually result in a single dollar earned for them. So I think it is very much plausible that even 0.2 cent cost is a sufficient deterrent. Not to even mention the deterrent factor for the unsophisticated and generic attacker.
MTurk, right? well this is quite depressing. What do you propose to make Hackage upload a bit more trustworthy? 
&gt; unless it's of type `exn`, of course Make up your mind, can you or can't you. :P When I say "bind an exception", that's actually "bind a computation that raises an exception" as technically SML only allows you to bind the results of computations to variables. "I" can't bind values to anything, only the runtime can. "I" can only bind computations as represented by syntax. "My" intention is that a value gets bound by the runtime to the variable. The types that "I" use when writing code are different from the ones the runtime uses, while the runtime doesn't have extra points in its types, for "me" raising an exception is another possibility for an expression with a given type. * "I", "me", "my" being generic here for users of SML the language and not about me specifically. &gt; Again, if you don't want this to happen, prove it. Sure that's what you'd do. Create a mapping from the syntax to the semantics and prove that you never use syntax that has undesirable semantics. However, SML the language contains those bits of syntax and they are "legitimate" from the point of view of the specification of the language.
&gt; quite depressing Unless you are referring to low income people's situation and you are somewhat distressed by spam being able to overcome recaptchas, I'd offer [my arguments in this comment](https://www.reddit.com/r/haskell/comments/7ze9xo/hackage_spam/duo2508/) as a source of quite a bit of hope.
&gt; I'd rather not have to use FFI or write the file out to disk Why not just use tmpfs (assuming you're on linux, but other kernels have similar features) or a pipe?
feelings of lens inadequacy increasing.
&gt; When I say "bind an exception", that's actually "bind a computation that raises an exception" as technically SML only allows you to bind the results of computations to variables. You're contradicting yourself. In ML, you can't ‚Äúbind a computation that raises an exception‚Äù *precisely* because you can only bind results of computations, not the computations themselves, to variables. &gt; The types that "I" use when writing code are different from the ones the runtime uses, while the runtime doesn't have extra points in its types, for "me" raising an exception is another possibility for an expression with a given type. This isn't even contradictory, because it doesn't make sense. A type is inhabited by what the language's semantics says its inhabitants are, and nothing else. &gt; Sure that's what you'd do. Create a mapping from the syntax to the semantics and prove that you never use syntax that has undesirable semantics. No, that's not what I do. I use an existing programming language, not invent my own. What I do is, *when I implement an abstract type*, prove that the *implementation* cannot create illegitimate values. Then type abstraction guarantees that clients cannot create illegitimate values either.
&gt; Haskell allows me to say yes, even statically, all variables are bound to values. The price that you have to pay is that some of these ‚Äúvalues‚Äù are things you don't want to consider values. &gt; However, outside of that fragment I still have to account for the value we're attempting to bind being bottom. There is no ‚Äúbottom value‚Äù in ML's semantics. There are non-terminating computations, of course, but those aren't values. Your statement would be correct if you revised it to ‚ÄúI still have to account for the possibility that computations don't terminate‚Äù. And you do precisely that by *proving* that the computations you implement in fact do terminate.
Full solution: module Namer where import Control.Lens (FieldNamer, LensRules) import qualified Control.Lens as L import qualified Data.Map as Map import qualified Language.Haskell.TH as TH -- | A mapping of field names to lens names. type Mapping = [(String, String)] -- | A given FieldNamer but some field's lens names are overridden. namerWithMapping :: Mapping -&gt; FieldNamer -&gt; FieldNamer namerWithMapping mapping fieldNamer tyName allFNames fName = let mapMapping = Map.fromList mapping in if Map.member (TH.nameBase fName) mapMapping then L.mappingNamer (\s -&gt; [mapMapping Map.! s]) tyName allFNames fName else fieldNamer tyName allFNames fName -- | The given LensRules but some field's lens names are overridden. rulesWithMapping :: Mapping -&gt; LensRules -&gt; LensRules rulesWithMapping mapping rules = rules L.&amp; L.lensField L..~ namerWithMapping mapping L.camelCaseNamer 
&gt; There is no ‚Äúbottom value‚Äù in ML's semantics. This whole discussion is about the fact that you don't need one. Non-terminating computations provide all the same characteristics as bottom. val bottom = let fun x () = x () in x () end I've now defined a perfectly good bottom value. Here's Haskell's: fun undefined () = raise UndefinedException Haskell just lets you get away without the explicit thunk. If you feel justified in ignoring these two values, then you should ignore bottom in Haskell. They're exactly the same thing.
&gt; Non-terminating computations provide all the same characteristics as bottom. However, and fortunately, in a call-by-value language, variables stand for values, not computations! In other words, as a user of a variable, I can say: ‚ÄúSomebody else already took care of producing a value. I don't need to care how this value was produced.‚Äù &gt; `fun bottom () = bottom ()` What you called `bottom` is a value. OTOH, `bottom ()` produces a nonterminating computation. Obviously, `bottom` and `bottom ()` are different things. &gt; If you feel justified in ignoring these two values I don't. I deem it very important to prove that my procedures produce sensible outputs for every possible input‚Äîwhich, of course, procedural arguments make difficult. So I [redesigned my entire approach to programming](https://www.reddit.com/r/programmingcirclejerk/comments/7yvlwl/checkmate_haskal_nerds/dukdtp5/) to avoid procedural arguments whenever I can.
Here's a very very quick version {-# language BangPatterns #-} import Data.List lp2 :: Integer -&gt; [(Integer, Integer)] lp2 n = unfoldr f (1, 0) where f (!val, !power) = let next = (val+val, power+1) in if show n `isPrefixOf` show val then Just ((val, power), next) else f next main = print $ [1..1000] &gt;&gt;= take 1 . lp2 
Would nix support let me dev vulkan on macos?
Great post! I like how this starts with nothing and ends up with a passable JSON parser. I did something similar with my library [Derulo](https://hackage.haskell.org/package/derulo-1.0.0/docs/Derulo.html), except that I started with `ReadP` rather than from scratch. At any rate, [the parsing section](http://dev.stephendiehl.com/hask/#parsing) of Stephen Diehl's excellent "What I wish I knew when learning Haskell" is a great deep dive into this topic. 
&gt; SML cannot stop you from creating expressions that aren't "good" expressions for any type (even abstract ones.) So? Who cares? Nobody is complaining about the existence of non-terminating computations. Only about binding them to variables.
Hi, I have spruced things up a bit. Now at version 0.1.0.4. The two internal modules are no longer exposed now. There are now some nice divisor and "reflection" functions.
`If you don't have a concrete constructor in mind, and just want to convert an existing Maybe a to a MaybeT m a, then MaybeT . return does sound like a good way to do that` That's exactly what I have in my code. I tought there was a simpler way to do that. Thanks for the reply.
no
:(
Whoops, my bad. 
It's not mechanical turk, there are specialized services for this and they're way faster and more reliable than MTurk. &gt;What do you propose to make Hackage upload a bit more trustworthy? I propose we take a step back and not overreact to a one-off in a way that puts us _way_ out on a limb compared to how literally every other community handles this. If I wanted to investigate ideas for a process I'd start with asking the maintainers of https://crates.io do.
If you don't want to use a REPL you stick your final command (stack exec ...) in a makefile and use `SPC c c` to run the command. The first time you need to chose it, but once you select a make target, you can just rerun it with `SPC c r`. Alternatively `ghcid` can rerun your program automatically when you save a file. (I combine both by using ghcid in a makefile).
Absolutely; as far as I can see Crates.io just requires a github account to receive an API key, which indeed wouldn't prohibit spam submissions : https://doc.rust-lang.org/cargo/reference/publishing.html I can't see whether they have any human curators in the loop though.
Do values here mean "variables, but not expressions"? 
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [naushadh/hello-world/.../**hello-postgresql** (master ‚Üí 178ae1e)](https://github.com/naushadh/hello-world/tree/178ae1e8c40554f7495ee7049cff4f96fad868aa/hello-postgresql) ---- 
&gt; Languages do not have strict parent/child relations with each other, each new language is technically created tabula rasa, but influenced by others. I generally agree, but I am not sure "created tabula rasa" is true in any meaningful sense. The slate may have varying degrees of blankness, but I don't think it's ever even technically completely blank. And there are plenty of instances of languages with new names but only small differences from their predecessors; e.g. I think the total amount of difference between Pascal, Modula-2, and Oberon is less than between the various iterations of the language known as Fortran. Then there's the progression from CPL to C++, where there was both a clear, intensional, semantic and often syntactic thread linking them at each step, yet most aren't even aware any beside C and C++ are related. So while there are a whole lot of cases where nothing like a parent/child relationship exists, there are enough cases of clear ancestry for the family metaphor to be useful. The family tree just happens to be more of a tangled wall of shrubbery in the case of Haskell, ML, and their varied relatives.
Nowadays, `-XDataKinds` gives us a lot more power (particularly the magical `Nat` kind). How much closer would `-XDependentTypes` (especially its visible `pi` quantifier, `foreach (n : Nat) -&gt;`) bring us to implementing the Agda examples without any hacking? 
No need for captcha, just ask users to state the monad laws and you're good. This kills two birds with one stone: 1) no spam, 2) everyone will know the monad laws.
Hey! First of all, thanks for your library - it was a good starting point and a source for inspiration for me! Unfortunately, it was a little bit difficult to get into internals of the generate tool - that was why I started my own genvulkan. In fact, there are quite a lot of differences between the implementations, the biggest one being that I don't use regular haskell algebraic types to represent any of vulkan structures. On the other hand, I borrowed your approach to enums via pattern synonyms. 
&gt; I simply look at the code within the `$(surjective [||\covers -&gt; ...||])` block, gather all the patterns given to the `covers` function, and complain if the patterns are not exhaustive. There's nothing stopping the user from accidentally annotating with the wrong pattern, is there? e.g. [||\covers -&gt; covers $ \(Just True) -&gt; Just False|]] 
&gt;To be fair, a lot of things magically cause cancer specifically in California. Do you have examples of California labelling things carcinogens without scientific proof as you seem to apply? 
&gt;The Haskell library itself looks awesome, although I can definitely hear my coworker in my head saying something along the lines of "see? You need to get super ugly to get anything useful done"; not quite sure what a strong counterargument to that would be, yet. I've heard this before and suspect it's just a result of applying standards unevenly and over-representing "getting ugly" as though its the general case. I believe just about every language has to get ugly when performance becomes critical, the trick is that it's unreasonable if it's not your pet language. 
Thanks /u/sclv for promptly addressing the problem. I hope this spam means that Haskell is growing in popularity :)
Full dependent types give us most all of Agda's power. The only thing left at that point is syntactic sugar for being able to express more of what the typed core will be capable of (eg universes, universe polymorphism, convenient easy literal syntax in types, etc)
Thanks! I took a quick look at the Derulo code and it's nice and clean indeed -- very instructive!
Learning Haskell won't teach you math, but you can learn math and then use math problems as Haskell exercises, and so practice both things at once. You *might* be making it harder for yourself, though, trying to learn two things at once. Depends on how you learn best.
* You can't fire the rockets? You can clearly limit the effects that can happen. * Also look at all the machinery we have with Monads: While in most imperative programming languages you have to write the same loop over and over again in slightly different variations, in Haskell you properly abstract it. Even having something like `mapM` in an imperative language is worth it in my opinion. * Easy mocking with free monads or type classes.
You are right, one needs to tweak that in ways I don't know even make sense :) OTOH I'got snipped by Menendez' post, I'll try to find time to clean up my notes `code` into another post. Thanks for that link!
`[]` are not brackets, they are list delimiters. By definition `head [list1] == list1`. If you want to delimite expressions, use `()`.
I managed to fix my code by changing out head [list1] with [head list1]. I have no idea why this works but im not going to question it. Then you edited your comment and mentionned pattern matching, and I realised Ive been doing this wrong the entire time because im supposed to use "pattern matching and recursion". Is this simply making several overloaded versions of the function and having it automatically route to one or the other for empty lists? Is the code i have written so far appropriate still?
The case mergeLists x [] = [head x]++mergeLists (tail x) ([]) Can also be written mergeLists (x:xs) [] = [x]++mergeLists xs ([]) Which is equivalent to mergeLists (x:xs) [] = x : mergeLists xs ([]) Here `(x:xs)` is a pattern match. See also https://en.wikibooks.org/wiki/Haskell/Pattern_matching http://learnyouahaskell.com/syntax-in-functions
Indeed the effort to move GHC to Trees that Grow is still in-progress. Ultimately we have found that the `Data` instances which Trees TTG gives rise to are quite expensive to compile. You can find some [notes](https://ghc.haskell.org/trac/ghc/wiki/ImplementingTreesThatGrow) on this effort in the wiki. I'm not aware of any other users of the TTG technique.
so, the upcoming release won't have TTG implementation merged?
&gt; I can no longer forget, but there is still the risk that [I make a mistake when updating]. That's good enough of an improvement for me! Absolutely! &gt; I considered automatically transforming the expression given to `covers` into a pattern, but I decided that this was too error prone Yeah, and it doesn't work if the expression isn't generated using a pattern directly (e.g. `safeHead [True]`). What would be really nice (but much harder) is if you could attach coverage information to an expression in one place and aggregate it in another; e.g. t = covers $ \(Just True) -&gt; Just True f = covers $ \(Just False) -&gt; Just False -- Warning: incomplete ... not matched: Nothing test = $$(surjective $ [|| [t, f] ||]) Even nicer would be the ability to compose/transform patterns: t = covers $ \True -&gt; True f = covers $ \False -&gt; False -- Warning: incomplete ... not matched: Nothing test = $$(surjective $ [|| transformCovers (\x -&gt; Just x) $ map Just [t, f] ||])
&gt; ensure predicativity I think it's a way of ensuring logical consistency while granting some power, chosen so as to be predicative as opposed to some alternatives (like `Coq`'s `Prop`) &gt; out of reach anyway because of `Type : Type`? Type-in-type was chosen because it's the simplest way to handle the problem that universe levels also solve, and because Haskell was already inconsistent as a logic.
To expand on the colons, it is syntax for matching the first item of a list. `x:xs` binds the first item in the list to `x` and the rest of the list to `xs`. You can chain this, and match the empty list too, so `x:y:z:[]` matches a list of 3 items and binds the first to `x`, the second to `y` and the third to `z`
How fast does it need to be? An exponential solution is to generate lists of numbers 1 to n with all possible variations of signs and then filter out the ones that don't sum to zero. For fun, here's a possible implementation: zero = filter ((== 0) . sum) . mapM (\x -&gt; [x, -x]) . enumFromTo 1
The limitations of having unit as a return type will be more apparent with backtracking transactions. The standard `STM` is an instance of `Alternative`; the two branches of `(&lt;|&gt;)` must have the same type: you might need to use `retry` in one branch when you cannot return a meaningful value. - `()`: returns, with a non-interesting value - `Void`: doesn't return, it actually *can't* `Void` is usually mentioned as an arbitrary empty type, and a more common one in practice is `forall a. a`, as it is more flexible than `Void`. But I think `Void` makes the intent more explicit in discussions.
It's worth noting to anyone confused by why this works that mapM on lists is using the list monad which, in FP, can be thought of as modeling nondeterminism. (in imperative programming it's much more akin to nested loops)
So, what's your proposed algorithm?
 -- This is interesting problem. To solve it you need two or three tricks -- -- * Generalise problem, often general problems are easier to solve (surprisng, but true!) -- -- * Some branching recursive algorithms can be made faster by -- "Dynamic Programming" (read: smart "memoisation") -- It's easy in Haskell even in pure code using `MemoTrie` library -- -- * Optional: You can use soem math to make cut search space when it's clear -- there aren't answer import Data.MemoTrie -- First step is to generalise! zero :: Integer -&gt; [[Integer]] zero = sumsto 0 -- then we can have a recursive definition sumsto :: Integer -&gt; Integer -&gt; [[Integer]] sumsto 0 0 = [[]] -- trivial answer sumsto n 0 = [] -- no answer sumsto n m -- optimisation, cut the search -- if target sum is absolutely greater than max sum we can get by picking the -- same sign for all of the rest numbers. -- -- \sum_{k=1}^m = m * (m + 1) / 2 -- | abs n &gt; (m * (m + 1)) `div` 2 = [] sumsto n m = map (++ [m] ) (sumsto (n + m) (m - 1)) -- positive m ++ map (++ [m']) (sumsto (n + m') (m - 1)) -- negative m where m' = negate m -- now sumsto is recursive function and is called with same imputs -- so you can throw http://hackage.haskell.org/package/MemoTrie -- on it to memoize them. zeroMT :: Integer -&gt; [[Integer]] zeroMT n = sumstoMT (0, n) sumstoMT :: (Integer, Integer) -&gt; [[Integer]] sumstoMT = memo (uncurry go) where go 0 0 = [[]] go n 0 = [] go n m | abs n &gt; (m * (m + 1)) `div` 2 = [] go n m = map (++ [m] ) (sumstoMT (n + m, m - 1)) ++ map (++ [m']) (sumstoMT (n + m', m - 1)) where m' = negate m {- This is slow but I could wait for result on my machine: Œª *Main&gt; map (length . zero) [15..25] [722,1314,0,0,8220,15272,0,0,99820,187692,0] On the other hand, is much faster: Œª *Main&gt; map (length . zeroMT) [15..25] [722,1314,0,0,8220,15272,0,0,99820,187692,0] -}
Btw, this is 5th or so "do my homework / puzzle" post by /u/AprendiendoHaskell - https://www.reddit.com/r/haskell/comments/7zf0vr/obtain_lowest_power_of_two_with_a_given_leading/ - https://www.reddit.com/r/haskell/comments/7z49fp/reduced_representation_of_scattered_matrices/ - https://www.reddit.com/r/haskell/comments/7ydkib/generate_real_number_sequence/ - https://www.reddit.com/r/haskell/comments/7y8b93/determine_if_a_set_of_dots_form_a_square_in/ Though interesting, I'd apply SO rule (https://stackoverflow.com/help/on-topic) &gt; Questions asking for homework help must include a summary of the work you've done so far to solve the problem, and a description of the difficulty you are having solving it. 
Interesting. Indeed, the original `retry` has general type: retry :: STM a Ok, I see the point. I also think making instance of `Alternative` and `orElse` for my STML won't be that difficult.
Nope. But there are some preparatory changes, that will affect users of the GHC API. See https://ghc.haskell.org/trac/ghc/wiki/Migration/8.4#GHCAPIchanges
well.. I'd be interested in this, too. I think a native Haskell implementation ontop of Conduit might be a first start. Just to be clear, you want this: https://datatracker.ietf.org/doc/draft-ietf-rtcweb-data-channel/ right?? I made an audio streaming service (used commercially, and profitably in production) that would consume many RTP streams, than transcode them to mp4 audio with AAC and share them per HTTP (servant) and DASH. So I based this code on Conduit, which did the Job after fixing a memory leak (due to Haskells lazyness and my inexperience at that time). I have a pretty good feeling about the network performance. For another project I wrote a simple TFTP server, that we used to publish firmware images to Hardware Media DSP devices, and I liked how nice the libraries helped me write serialization code for parsing the messages. Not as nice as in Erlang or Scala but, good enough, and arguably better from a syntax aesthetics point of view. Especially the representation of State machines necessary for SCTP make Haskell a nice choice. Also, there are libraries for DTLS in Haskell, on which this could be based. Soo.. yeah, I think Haskell is a safe bet for this. 
Sure, the family metaphor is useful. When I say languages are created tabula rasa, what I mean is pretty simple. No choices of syntax or semantics are forced on you. There's no memetic inheritance necessary to create a new language. Of course, creators are not blank slates. We have certain biases, so we don't in truth create language from nothing. There's at least a set of first principles we work from (and often 2nd and 3rd and so on.) And a common reason for a new language to be born is "like x but y", so there are times when very clear lineage is present. In the case of the ML family (or rather should it be the ISWIM descendent family?) there is a fairly clear relationship, I think. At the very least both ML and Haskell have a mutual ancestor in ISWIM and Haskell via Miranda has some meme transfer from ML (notably type inference.) Which probably effectively makes ML the memetic grandparent of Haskell.
For that `n` my approach won't work anyway, as memory usage is at least linear. Maybe there is a *smart* math trick to count combinations in this problem, but I cannot think of it.
The paper for those interested: https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/trees-that-grow.pdf
That does look a lot nicer, as the current API is quite verbose. And not only is it nicer, pulling the `covers` calls outside of the quotation would allow me to use proper patterns instead of the lambda hack: t = Covers [p||Just True||] (Just True) and I think it would even make my API more composable, in the sense that if there is another `surjective`-like library which uses the same API, I should be able to use both on the same function: $$(surjective . otherLibrary $ [|| ... ||]) `transformCovers` also seems quite useful, and I don't think implementing it would even be that hard, as long as the set of patterns it encloses can be statically determined. Thanks for the suggestions!
Well this is simple to solve as a SAT search. All you need to do is encode your constraints logically. With SBV for example: aprendiendo's_homework_he_didn't_do :: Int -&gt; Predicate aprendiendo's_homework_hd_didn't_do len = do xs &lt;- replicateM len exists_ constrain $ sum xs .== (0 :: SInt32) constrain $ bAnd [x .== v ||| x .== negate v | x &lt;- xs | v &lt;- [1..]] return true after that you just `print =&lt;&lt; allSat your_homework` and you have the results.
I love mini language demos like this. They are invaluable to demystifying implementation of typed languages! For what it's worth, the parser would be a lot smaller with parsec or megaparsec. 
Asking the important questions! 
The only thread that has PVP, hackage, upper bounds and yet folks laughing I hope.
What is it?
Certainly! But not if you think in terms of total code (because parsec is itself an import with code...)
That's what you get when a comitee doesn't take "avoid sucess at all costs" seriously and designs a language that is useful for things besides implementing compilers for itself :)
https://en.wikipedia.org/wiki/California_Proposition_65_(1986)#Abuse
Clojure in /r/Haskell https://www.reddit.com/r/haskell/search?q=clojure&amp;restrict_sr=on&amp;sort=relevance&amp;t=all
 modifierDuree :: Etat -&gt; Char -&gt; Etat modifierDuree etat@(Etat _ d _) a | a == '1' = etat {duree = 1 } | a == '*' = etat {duree = d*2} We need to 'get at' the original `Etat`s `duree` property in order to multiply it by two and return a new `Etat`. So we use the `@` symbol when destructuring the argument, which binds the whole of the original `Etat` to `etat`, just like normal, but allows us to supply another pattern to it's right to further destructure the value, assigning it's `duree` to `d`. This could also be written as: modifierDuree :: Etat -&gt; Char -&gt; Etat modifierDuree (Etat n d v) a | a == '1' = Etat n 1 v | a == '*' = Etat n (2*d) v And it'd do precisely the same thing, just without using record syntax as sugar. If you want something that adjusts fields a little more dynamically, you could look into lenses, but I'd advise steering clear of that package until you have a solid understanding of fundamentals. A better idea might be to define a few 'setters' and 'getters' yourself: adjustDuree :: Etat -&gt; (Double -&gt; Double) -&gt; Etat adjustDuree (Etat n d v) f = Etat n (f d) v adjustNote :: Etat -&gt; (Note -&gt; Note) -&gt; Etat adjustNote (Etat n d v) f = Etat (f n) d v adjustVolume :: Etat -&gt; Double -&gt; Double) -&gt; Etat adjustVolume (Etat n d v) f = Etat n d (f v) adjustsmallNote :: Note -&gt; (Integer -&gt; Integer) -&gt; Note adjustsmallNote (Note n o) f = Note (f n) o adjustOctave :: Note -&gt; (Integer -&gt; Integer) -&gt; Note adjustOctave (Note n o) f = Note n (f o) Then you could: modifierOctave :: Etat -&gt; Char -&gt; Etat modifierOctave etat a = | a == '1' = adjustOctave (const 1) . adjustNote $ etat | A == '2' = adjustOctave (const 2) . adjustNote $ etat And so forth. This might be a little more verbose than you were originally thinking, but it's a much better learning exercise and it will also make your code easier to tweak and play around with as you explore more of the language.
Its a Haskell Party?
Sure but what does compile time evaluation even mean for functions?
sorry, as a lay person (albeit programmer) the formulas are hieroglyphics. Can someone please explain the typing rules?
From a very light skim, it seems to be an intensive introductory Haskell course. https://monadic.party
It's not so much "broken" as it is just straight up missing lol.
What is curry-style and church-style point? 
Most languages follow the church-style, on which the types of terms are a fundamental part of them. On the curry-style, types are just annotations that don't affect the meaning of the terms. That means that normal forms, equality etc. completely ignore the types. See the paper which is now fixed!
The first thing I notice that is confusing you is type signatures. In Haskell a function of two arguments looks like this: func :: a -&gt; b -&gt; c. Thus the type signature for choixLettre does not match. It needs to be: choixLettre :: Etat -&gt; Char -&gt; a. Now the question is what does a need to be? Well it needs to be the type of the functions you are using on the right hand side of the equals signs (modfierNote etc). This leads to the last potential confusion since you are using different functions on the right hand side you need to make sure that they all return the same type. That type will be what you substitute for "a" in the type signature I showed before. Hopefully this makes sense and sorry for any formatting issues I am on mobile currently. Let me know if you have any further questions!
Woops, it is now.
The RFC you linked is indeed the standard I would like to implement. Thank you for your detailed answer, I'll give the pure Haskell approach a shot! 
If I change the code to use Int, my code takes 12 seconds on my machine to write out a file containing all the 2,399,784 solutions for zero 28. If I use Integer, it takes 18 seconds.
Another way to cut it down is to filter out candidates within addNum, for example if they get so big that we know subtracting the rest of the numbers couldn't possibly add up to the same amount. So you could precompute the partial sums of [1..n], and use that number in the comparison. import System.Environment (getArgs) zero :: Int -&gt; [[Int]] zero = map fst . filter ((==0) . snd) . foldr addNum [([], 0)] . (\ns -&gt; zip (scanl (+) 0 ns) ns) -- tag each n with the sum of 1..n . enumFromTo 1 addNum :: (Int, Int) -&gt; [([Int], Int)] -&gt; [([Int], Int)] addNum (m, n) = filter (\(ns, s) -&gt; s &lt;= m) . concatMap (\(ns', s') -&gt; [(n : ns', s' + n), (-n : ns', s' - n)]) main = do (arg0 : _) &lt;- getArgs mapM_ print $ zero (read arg0) This gets it down to a bit more than 8 seconds for zero 28.
Personally, I like don't really like to include libraries in LoC counts. Sure, it might be cheating, but the point (to me) is to express a working concept succiently rather than reinventing the wheel. It's certainly up for debate on how purely pedagogical that approach is, though :)
Oh, sure, but another point is to make it easy to implement in other environments. Imagine, for example, that you wanted to implement it on Scheme, JavaScript, Go. If I used Parsec, you'd need an equivalent on those languages (most don't have). Since I used just plain functions, you can implement it anywhere else by braindead copying.
Signature you provided fixed that particular error but I got another error. At the end whole system just did not clicked together. Than I realized that one of the signatures was wrong. After I changed it, I end up with this signature: noContent :: MonadIO m =&gt; ActionCtxT ctx m () Thanks for the help!
I don't agree. Computer programming is absolutely full of mathematics. Formal languages, formal logic, computation theory, type theory, domain theory, relational algebra, abstract algebra, category theory, etc., etc. It may or may not be the kind of math /u/bushbud_lover wants to learn, but it's there.
I still have a problem with this though. Here is my current type signature for my views: postCollection :: (SpockState (ActionCtxT ctx m) ~ AppState, MonadIO m, HasSpock (ActionCtxT ctx m)) =&gt; B.ByteString -&gt; ActionCtxT ctx m () -- https://github.com/huseyinyilmaz/spyglass/blob/040b1b3daa6ac5c4255f884ea5b5b3ad7d7d7e19/library/Views.hs#L22 This type seems extremely complicated though. I suspect that I am missing some kind of constraint on my AppState. data AppState = AppState (STM.TVar (Map B.ByteString (STM.TVar (Trie B.ByteString)))) -- https://github.com/huseyinyilmaz/spyglass/blob/040b1b3daa6ac5c4255f884ea5b5b3ad7d7d7e19/library/Types.hs#L44 It feels like If I get the types right, I can just get following signature: postCollection :: MonadIO m =&gt; B.ByteString -&gt; ActionCtxT ctx m () Obviously I don't know what I am talking about. So I just wanted to throw this out there and see if somebody who has more experience can help me clean up my types. Thanks in advance.
I ended up adding this type synonym to have cleaner view signatures. https://github.com/huseyinyilmaz/spyglass/blob/f9a827f5384a0202bd9863124e7809b0ac27d42d/library/Types.hs#L48-L50
Great play on words!
Test the two diagonals. They should be of equal length and perpendicular to each other.
The first rule states that the "type" of types, *, is a kind, ‚òê. These are denoted `*` and `+` in the implementation, respectively. The second is a typical pi-type checking rule. It states that a function (Œ† x : t. t') is of sort s' (sorts being either * or ‚òê) if t is of sort s and assuming x : t allows us to conclude that t' has sort s'. Note that, in the implementation, Pi-types are denoted with `@`, so that Œ† x : t. t' would be written `@x t t'`. The third rule is for implicit quantification. It's nearly identical to the pi-checking rule, but ‚àÄ can only be a type. In the implementation, ‚àÄ is denoted `&amp;` so ‚àÄ x : t. t', would be written `&amp;x t t'`. The forth rule is a typical sequent calculus identity rule. The fifth is standard typing for dependent function application. If we have a function (f : Œ† x : t. t') and some input (a : t), then f a will have type t'[a/x], that is, the type t' with all occurrences of x replaced with a. In the implementation here, function application is explicitly denoted with `/`, in a form of polish notation so that parentheses aren't used. So, instead of writing `(((f a) b) c)`, you'd write `///f a b c`, instead of writing `((f a) (g a))`, you'd write `//f a /g a`. The sixth type-checking rule is for dependent intersection types. It's nearly the same as ‚àÄ. Again, it can only be a type. The seventh and tenth rules are typing right and left-projection for the dependent intersection type. It's very similar to typical sigma-type projection checking. If we have an (i : Œπ x:t. t'), then i.1 will have type t and i.2 will have type t'[i/x]. First and second projections are denoted `&lt;` and `&gt;` respectively in the implementation. The only thing that might be confusing here is that x is supposed to be of type t, and yet we're substituting it for i : Œπ x:t. t'. The main difference between sigma and intersection types is that any t and (for any a : t) t'[a/x] are both super-types of Œπ x:t. t', so i also can have type t. In the implementation, Œπ is denoted \^. The eighth rule is a typical checking rule for lambda-expressions. If assuming x : t allows us to conclude that t' : t'' (and if Œ† x : t. t'' is a valid term of some sort) then we can conclude that (Œª x:t. t' : Œ† x : t. t''). In the implementation, lambda expressions are denoted with a `#`. The ninth rule is a heterogeneous equality symmetry rule. If p is a proof that a = b, then œÇ p is a proof that b = a. In the implementation, this is denoted `~`. The eleventh rule is implicit function application. It's nearly the same as ordinary function application, but it pertains to ‚àÄ. Note that, in the implementation, implicit application is denoted `\`, and is interpreted similarly using polish notation. The twelfth rule is a kind of transport. If we have a predicate t' over x, and a proof that t'' : t'[a/x] holds, and a proof e : a = b, then we can prove that t'[b/x] holds from œÅ e @ x.t' - t''. In the implementation, this would be denoted `?e t' t''`. The thirteenth rule denotes a proof of reflexivity. For any x, Œ≤ x is a proof that x = x. This is denoted `:x u` in the implementation, where `u` is the program that the proof will be erased to. For more information on the erasure mechanism, I'll refer you to the publications on Cedille by Aaron Stump. The fourteenth rule is typing for heterogeneous equality. If x and y are well-formed terms, then x = y is a valid type. This would be written as `=x y` in the implementation, in polish notation. The fifteenth rule is a typing rule for let-expressions dealing with locally declared types. These are denoted with a `$` in the implementation. The sixteenth rule is a kind of pairing used to introduce dependent intersections. If u : t and u' : t'[u / x] and u and u' erase to the same terms (again, refer to the white papers for information on erasure), then the pair [u, u' @ x.t'] is of type Œπ x:t. t'. Note that pairing is denoted `|x t' u u'` in the language. The seventeenth rule is simply a way to replace a term t' with another term t'' when we have a proof t that t' = t''. To be honest, it's not clear to me how this is used, but it's denoted with a `!` in the implementation. The second-to-last rule is another typing rule for local let-expressions, but this time dealing with locally declared data. The last rule is for implicit lambda-expressions. It's nearly the same as those for pi-types. If assuming x : t allows us to conclude that t' : t'' (and if ‚àÄ x : t. t'' is a valid type) then we can conclude that (Œõ x:t. t' : ‚àÄ x : t. t''). In the implementation, implicit lambda expressions are denoted with a `%`. And that's it. If you need further clearification (including questions about the erasure mechanism, which I don't want to take the time to explain right this moment), feel free to ask.
When are the student scholarship results going to be declared?
Yes! If you study computer science you will learn a lot of math. If you sit down and self-study haskell, I don't think you will. 
Maybe u/b4zzl3 knows? 
We'll close the applications at the end of the month, give us around a week of March to choose the winners :)
We'll have both introductory and intermediate tracks, but it's a good approximation.
 choixLettre :: Char -&gt; Etat -&gt; Etat choixLettre x | x &gt;= 'a' &amp;&amp; x &lt;= 'g' = modifierNote x | x == '!' = jouerNote x modifierNote :: Char -&gt; Etat -&gt; Etat jouerNote :: Char -&gt; Etat -&gt; Etat I have cut down some cases just to make a point. Couple of stuff to notice: ChoixLettre is accepting a Char and is returning a function that modifies given Etat. Then all the modifying functions should have type signature that ends with Etat -&gt; Etat, and rest of parameters you feed it before. 
vscode with the HIE extension works for me on projects based on stack LTS 10 and I've been using it for a few months. The main problems for me are: 1. When the source code doesn't compile (e.g. when I'm in the middle of an edit), the tooltip and auto-completion don't work well for new code that I enter. This problem is probably not unique to HIE. 1. Sometimes the tooltip doesn't have the haddock link. On my dev machine, I maintain local haddock and hoogle data and run a local hoogle server. I don't know whether HIE depends on them and how well it will work if I don't have local haddock and hoogle.
Are haskero and HIE mutually exclusive? Hard to know how plugins interact. Is one or the other generally considered to be more actively used/developed?
More interested in the latter question I guess - I don't have a desire to run them together, just confirming that you would use one or the other. 
I'm using vs code with the Haskelly extension - great environment for Haskell development.
I have to ask, why? 
Thanks again. makes alot more sense now . i might pop back here later if i run into any more problems. Thanks alot for your help!
thanks for the answer, i understand now
Not any one reason, spacemacs is nice for what it is. It's more the accumulation of a thousand tiny configuration / UX inconveniences + hearing good things about vscode (even /u/edwardkmett uses it). I never would've thought I'd willingly use a MSFT editor either, but it is OSS at least https://github.com/Microsoft/vscode. Just want to see what's on the horizon and see how this relative newcomer has taken a stab at the editor game.
I've been messing with HIE for the past week. I like it quite a bit. LSP alone is enough reason for me to consider it the definitive future of Haskell editor integration. It still needs cabal `new-build` support for me, but Stack and cabal old-build support is pretty good. I've managed to set it up for a Reflex project, which was quite nice. AFAICT, HIE is *a little* more actively developed than Intero, but both are well supported by their maintainers. I think Haskero is a thin wrapper around Intero, so I wouldn't expect it to need much active development.
I thought Dante didn't require company-ghc. 
Will the monadic party have trampolines? :)
Will you help with rellocation if required?
Can any downvoters explain their downvote? Is there a factual inaccuracy we should k know about? 
Import literally everything in ghci with a memorable qualified import scheme to avoid anything but coding. 
https://www.explainxkcd.com/wiki/images/5/58/functional.png
I would suggest you to switch from spacemacs to plain emacs with intero. Spacemacs adds so many layers of complexity that often things go wrong and user has no clue how to fix things. In reality you need less than 1% of what spacemacs offers. It became the antithesis of what emacs should be. 
I am missing some fundamental understanding to decipher this. Can you point me in the right direction?
You might find this interesting: https://hackage.haskell.org/package/generic-lens-labels
This works for me on GHC 8.2.1: First.hs: module First where import Second class Foo a where foo :: Bar b =&gt; b -&gt; a -&gt; a First.hs-boot: module First where class Foo a Second.hs: module Second where import {-# SOURCE #-} First class Foo b =&gt; Bar b where bar :: b 
&gt; For me, an important principle is this: &gt; &gt; * **GHC should strive to make life as easy as possible for downstream tools** ‚Äî Simon Peyton Jones
True, but I don't know where to start on regaining some of the functionality spacemacs offers.
With melpa it is easy to install packages you need. Intero, magit, company mode and that's it. You already have a quite feature rich haskell dev environment. 
Is there a package for the window management that spacemacs provides?
Which one does the spacemacs provide? I would not be surprised that it is just a normal emacs package. 
I've heard about this, and like the increased clarity of it at call sites, but I was dissuaded from trying it because of plenty of "use at your own risk" warnings surrounding its mentions, IIRC related to an orphan/overlapping instance. Is that not to be worried about anymore? Or if it is, would you be willing to share an example where using generic-lens-labels can lead to an undesirable outcome?
You can try making an announcement here: https://www.meetup.com/HASKELL-SG/events/246341985/
I like to use spacemacs to edit text, so I setup a test project that doesn't touch my config. Thought I would share it since others might find it useful as well.
I was thinking more about going there and asking Moritz to allow you to make an announcement.