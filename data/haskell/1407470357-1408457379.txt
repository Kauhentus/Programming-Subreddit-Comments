&gt; Feel free to ask questions, if you have any. Thanks, though honestly this is one of those situations where once I know what question to ask, it's probably because I've learned the answer! Today was lamentably Haskell-free so I'm no closer than last night, but on the upside I do understand enough of this that I at least know what the shapes of the holes are for the rest! (Which is basically three out of the four imports -- I do know Maybe, by way of C# Nullable.)
&gt; We have to return an r at the end, and there is only two ways to create a value of that unfathomable r type: the return' and the yield'. I noticed this and thought it was very neat. It is an excellent use of Haskell's type system. There is, in fact, one more way to produce the "unfathomable" r, which is by never actually producing a result, and recurring infinitely. (For example, continue yielding the next integer.) Or by cheating with undefined/error, but that's cheating.
I just fixed a typo before posting this which changed the url. Hopefully the posted link is working for you now.
That's quite quick! It now loads much faster for me, and has visual feedback that something is going on. I was really confused at first in the previous iteration, I wasn't joking when I said I thought things were just broken.
[Free theorems](http://ttic.uchicago.edu/~dreyer/course/papers/wadler.pdf)
Those are theorems you get as a result from parametricity, I don't see how you can call "element-independent information content of a list" a free theorem since it's not a theorem at all even though one may imply the other.
You can just do the compiler's job and have each constraint become an explicit dictionary. That is a completely valid way of thinking of these functions since well.. it's how they actually get compiled! So the type of sort is really data Ord a = Ord {lt :: a -&gt; a -&gt; Bool, ... } sort :: Ord a -&gt; [a] -&gt; [a] sort Ord{lt = (&lt;)} xs = ... Just like in Wadler's paper.
As a heads-up, you would typically see `RankNTypes` as part of something of the sort: {-# LANGUAGE RankNTypes #-} Note that this is a language pragma, while rank-N types proper are an extension. You can in fact enable language extensions by other means (although I think language pragmas are by far the most popular, at least when it comes to packages). For instance, you could try the extension inside GHCi by using `:set -XRankNTypes` (and then trying something like `data WeirdInt = forall a. WeirdInt a (a -&gt; Int)`). The [GHC manual](https://www.haskell.org/ghc/docs/latest/html/users_guide/) is a good place to learn more about extensions (and incidentally pragmas as well). Some of the more substantial extensions are linked to a particular paper, too.
But type classes add additional constraints don't they, if you have `map a . sort = sort . map a` the comparison function will always be the same so that is a stronger statement than `map a . sortBy compare1 = sortBy compare2 . map a` (and same with `Data.Foldable.fold`) or am I missing something? Are there any papers on this or is there not a lot more to say about this other than "reify the dictionary and use normal parametricity"?
I always prefer the sum type approach. It is easy to ignore all other cases with a _ pattern match but the pattern exhaustiveness is always just a Wall/Werror away. Can't say the same for the subclassing mechanism. If there could somehow be a Haskell without exceptions in it I'd switch in a heartbeat.
I'll follow you there. The ergonomics in Haskell for handling exceptions are pretty poor in Haskell. I don't think they should improve because that would encourage broader use of them. To me exceptions just end up being tigers lurking in tall grass and rob you of the reasoning ability we strive so hard for in Haskell because anything in IO can blow at any moment and the type system cannot help you protect yourself. It sucks :(
I think it may be called a "spine".
www.cse.chalmers.se/~joels/writing/GPUFL.pdf
Shameless plug, but Chapters 2 and 3 of http://pubs.doc.ic.ac.uk/will-jones-phd-thesis/will-jones-phd-thesis.pdf aim to bridge one of the gaps (I think) you're describing, which in my mind is that between "Haskell 98 programmer" and "type system abuser". So no monads, comonads or performance tuning per se, but things like type classes, functional dependencies, rank-N types, overlapping and undecidable instances, phantom types, GADTs, etc. In essence, a subset of the tools useful for embedding domain-specific languages in Haskell, which is the overarching theme. Feedback gratefully received if you decide to read it and/or find it useful.
How about "the structure"?
This reminds me of the precondition for the information preservation law from [Control.Monad.Zip](http://hackage.haskell.org/package/base-4.7.0.1/docs/Control-Monad-Zip.html) which is essentially: () &lt;$ ma = () &lt;$ mb `() &lt;$ xs` is the same as `map (const ()) xs` or `[ () | _ ‚Üê xs ]` for lists and thus throws away their content and only compares the length.
Trevor McDonell, one of the Accelerate authors, has recently published his [PhD thesis](http://www.cse.unsw.edu.au/%7Etmcdonell/papers/TrevorMcDonell_PhD_submission.pdf) that contains a lot of example programs with performance comparison to CUDA code.
Not so! :) Making contributions is easy, just submit a pull request fixing a bug or adding some new functionality.
Just to add to that. The type signature syntax is designed around that correspondence: Ord a =&gt; [a] -&gt; [a] Vs Ord a -&gt; [a] -&gt; [a] It's just a slightly different arrow.
There's no standard name, but I would call it "the relationally invariant structure" if I had cause to talk about it.
I'm pretty sure it's always safe to delete everything in the .cabal folder. I'm not sure about partial deletion, but in the worst case you can just nuke all of it and start over. :)
Your definition of NP is not the same as the one in generics-sop. Perhaps a different choice of name to make this clear?
But the "element-independent information content" is exactly what the free theorem is talking about. Consider the free theorem for `[a] -&gt; [a]` applied to `reverse`; we have `forall (f::a-&gt;b): map f . reverse = reverse . map f`. By this theorem, the result of `reverse` on *any* list of length `n` is determined by its action on the list `xs = [0..n-1]`. Proof: Given any list `ys` of length `n`, the function `map (ys !!) [0..n-1] = ys`. By the free theorem, `map (ys !!) (reverse xs) = reverse (map (ys !!) xs) = reverse ys`. Note that we didn't assume anything about `reverse`, so this holds for every polymorphic function `[a] -&gt; [a]`. You can do a similar construction on any type that admits an enumeration of the values of that type.
What, you don't `sequncue` things? Alright, fixed :)
It's not the same as the one in `generics-sop`, but this is mentioned initially in the paper. But I can see the scope for confusion there, so I've renamed it "`HList`" instead.
Thanks. I got the example book using the SOP version as follows book1 :: NP I '[Text, Text, ISBN, Int] book1 = (I "Conceptual Mathematics") :* (I "Lawvere, Schanuel") :* (I "978-0-521-71916-2") :* (I (2009::Int)) :* Nil 
Also, there is an undefined p in your last gtoRow
I feel like I'm harassing you, but I am trying to get my brain around this stuff. I understand that in fields :: (All FromField xs, SingI xs) =&gt; NP RowParser xs fields = hcpure fromField field where fromField = Proxy :: Proxy FromField the fromField value is actually just a witness to the type class, and is not at all the same as the actual 'fromField' function in postgresql-simple? If so perhaps it should have a different name.
In fact this confusion causes a problem for gtoRow, the proxy needs to be renamed to avoid clashing with the actual toField gtoRow :: (Generic a, Code a ~ '[xs], All ToField xs, SingI xs) =&gt; a -&gt; [Action] gtoRow a = case from a of SOP (Z xs) -&gt; hcollapse (hcliftA p (K . toField . unI) xs) where -- toField = Proxy :: Proxy ToField p = Proxy :: Proxy ToField 
I lost the ability to distinguish what's advanced and what's intermediate anymore. Advanced: anything I haven't done yet.
It depends on your program, but it is possible to achieve equivalent performance. For ultimate performance, the main missing feature is probably the ability to express shared memory micro optimisations, which highly tuned libraries (linear algebra, FFT, etc.) will use to great effect. Accelerate does have an FFI though, so you can hook into these or drop down into raw CUDA if you want. The benchmarks in [my PhD thesis](http://www.cse.unsw.edu.au/~tmcdonell/papers/TrevorMcDonell_PhD_submission.pdf) have some examples.
Really well-done write up. Wow.
Here is a version based on the article http://lpaste.net/109050
I'm surprised anybody was able to find that d:
Possibly, but we were discussing the falling turnip implementation, weren't we? In that implementation, I feel like the innovation was combining the cells of the grid into neighborhoods and considering a neighborhood at a time instead of a cell at a time.
Just wanted to seize the chance to thank Bryan for all his amazing contributions to the Haskell ecosystem! Looking forward to hearing him speak at the Haskell eXchange next October :)
Wow, looks like you did, what I'd love to have done. Congrats!
But what can you do with numbers, if not compare them for equality?
I find I'm more likely to read through the source of a new package I use in Haskell than I am with the other languages I use, but I'm not sure if that's because of Haskell completely, or because Haskell is a language I have begun learning later in my time programming. Maybe I'd do the same with any new language now that I understand the importance of reading lots-- can't say yet.
&gt; Please let me know if you find criterion useful! Damn straight I find criterion useful. It's an excellent library, and this new release looks fantastic. The allocation and GC metrics look especially nice. Great work bos and everyone who contributed :) You're making the Haskell world a better place.
This should land in postgresql-simple. We could also adapt it to make haskell versions of sql composite types have FromField/ToField instances.
Spock's routing is faster than Scottys, and it provides more features like database support and sessions/workers/user-auth.
I think errors is post 2011. Broadly though, ezyang is pretty much up-to-date. I think the exceptions package is the big modernization there. Definitely use it whenever it applies! As for (4) I might be expressing the theoreticians distaste for fail in monad, but I also find it generally surprising and non-explicit. There are also a lot of monads out there which don't implement it‚Äînamely, every one of my application ones‚Äîso you end up with surprising, hard-to-debug errors if you assume too broadly that it's available. Use MonadPlus instead. It's got a smaller, less surprising surface area.
I wonder if it works on Windows?
That's really elegant. I love that. Thanks so much!
http://hackage.haskell.org/package/criterion has no docs right now; it looks like [the build failed](http://hackage.haskell.org/package/criterion-1.0.0.0/reports/). This seems to be happening a lot recently...you might want to push docs to Hackage manually (which you can do easily [with this script](https://gist.github.com/5outh/ca9bce3e112eb2eed98c)) Anyways, this looks awesome! I know criterion has been around for a while but the new webpage and tutorial are great. Thanks for all of your hard work, Bryan!
That's indeed [the same issue](http://hackage.haskell.org/package/criterion-1.0.0.0/reports/1/log)
To plug pigworker further on this, there's a wonderful set of notes for his "Dependently typed metaprogramming" lecture available online which describe dependently typed containers formally and neatly in the first few pages. http://www.cl.cam.ac.uk/~ok259/agda-course-13/ I recommend taking a look at them!
`Monoid` is a class for things that can be combined in a list-like way. It looks like this: class Monoid m where mempty :: m mappend :: m -&gt; m -&gt; m -- this is also written as &lt;&gt; Instances of Monoid must be associative and mempty must be the identity: that is: (a &lt;&gt; b) &lt;&gt; c = a &lt;&gt; (b &lt;&gt; c) a &lt;&gt; mempty = mempty &lt;&gt; a = a for any `a`, `b`, `c`. You can think of it as if `mempty` was the equivalent of `[]` and `mappend` was the equivalent of `(++)`. (That means that lists are monoids in that way, and therefore strings too). If `m` is a monoid, functions of the form `a -&gt; m` are monoids too: `const mempty` is the identity and `mappend f g = \x -&gt; mappend (f x) (g x)`. This is what is happening here. Experiment with these to understand it. 
Adding the dictionary as an explicit constraint changes the free theorem you get. For example, if you uncurry `sort` with an explicit dictionary you get: sort' :: (Ord a, [a]) -&gt; [a] Then you can treat that as a natural transformation between two "functors" (where the first is a functor, but not in the Haskell `Functor` type class sense): newtype OrdList a = OrdList (a -&gt; a -&gt; Bool) [a] sort'' :: OrdList a -&gt; [a] This would have a function analogous to `fmap` but with a different type: -- Note: I think you have to constrain the `Iso'` so that it is monotonic -- fmap' id = id -- fmap' (f . g) = fmap f . fmap g fmap' :: Iso' a b -&gt; OrdList a -&gt; OrdList b fmap' = ... -- Implementation elided ... and then you'd have the following free theorem: sort'' . fmap' f = (fmap . view) f . sort'' ... where `f` is some `Iso'`.
Wadler already covers the `Eq` type classes in section 3.4: &gt; This suggests that we need some way to tame the power of the polymorphic equality operator Exactly such taming is provided by the eqtype variables of Standard ML, or more generally by the *type classes* of Haskell. In these languages, we can think of polymorphic equality as having the type &gt; &gt; (=) : forall^(=) X. X -&gt; X -&gt; Bool &gt; &gt; Here `forall^(=) X. F(X)` is a new type former, where `X` ranges only over types for which equality is defined. Corresponding to the type constructor `forall^(=) is a new relation constructor: &gt; &gt; (g, g') ‚àà forall^(=) RelX. RelF(RelX) &gt; iff &gt; for all RelA : A &lt;=&gt; A' respecting (=), (g_A, g'_A') ‚àà RelF(RelA) &gt; &gt; A relation `RelA : A &lt;=&gt; A'` respects `(=)` if `RelA` relates equals to equals; that is, if whenever `x =_A y` and `(x, x') ‚àà RelA` and `(y, y') ‚àà RelA` then `x' =_A' y'`, where `(=_A)` is equality on `A` and `(=_A')` is equality on `A'`. In the case where `RelA` is a function `a`, this is equivalent to requiring that `a` by one-to-one. By comparison, the type `(X -&gt; X -&gt; Bool) -&gt; F(X)` yields the relation (g, g') ‚àà (RelX -&gt; RelX -&gt; RelBool) -&gt; RelF(RelX) iff for all (eq, eq') ‚àà (RelX -&gt; RelX -&gt; RelBool), (g eq, g' eq') ‚àà RelF(RelX) If you expand the relation `RelX -&gt; RelX -&gt; RelBool`, you get something slightly stronger than "respects equality": if (x, x') ‚àà RelX (y, y') ‚àà RelX then eq x y = eq' x' y' i.e., it's not just that equality implies equality, it's also that unequality implies unequality. I don't think this difference was intentional. Another apparent difference is that "respects equality" talks about a single polymorphic `(=)` function, whereas the expanded relation talks about two, namely `eq` and `eq'`. When proving something about type classes, you would of course instantiate `eq` and `eq'` with the same `(==)`, specialized to the appropriate types: `(==)_A` and `(==)_A'`, just as in Wadler's definition. In conclusion, the relation given by Wadler for the `Eq` typeclass is virtually identical to the relation obtained by replacing `Eq a =&gt; ...` with `(a -&gt; a -&gt; Bool) -&gt; ...`, so I would assume that similar replacements are also valid for other typeclasses.
While I agree with the main premise of the article, that not being able to rename reexported things is a deficiency of Haskell's module system, I believe there are some more alternatives to work around this particular issue. The various optics from the lens library give you first class representations of constructors in many (but not quite all) cases. Specifically Isos can represent newtypes, Prisms can represent variants, and Lenses can represent records. In theory, you can use Prisms in place of pattern matching. In combination with view pattern syntax, this should be a barely acceptable way of interacting with Prisms as patterns. Because Prisms, and other optics, are first class values they can be renamed and exported.
I think some of edward yang's work this summer might make it possible to rename exports going forward (though I could be wrong)
yes, those are totally safe to delete. I generally tell folks "its safe to delete ~/.ghc, but not ~/.cabal" because certain binaries will depend on static file assets in ~/.cabal, and can break if those get deleted. Eg happy and pandoc have this problem. 
&gt; Concerning the issue described here I am actually coming up with a proposal, which will be the subject of my next post, which I plan to post in the coming days. The suspense is killing me...
It's *very* quiet, mind you. But as I've (after years!) never gotten my head around to trying to spur something along, I've been hoping somebody eager would step in, or that it would snowball by itself. Through the power of magical thinking, I've also been hoping that eventually folks would look at this and realise ‚Äúholy crap! there's actually a whole lot of us here!‚Äù
You can re-export aliased modules like: module Lib ( module Connection , module Result ) where import qualified Lib.Connection as Connection import qualified Lib.Result as Result ~~If you then import 'Lib' and try to use the 'Failure' type, you'd get an error message like: "Not in scope 'Failure', perhaps you meant 'Connection.Failure' (imported from Lib.Connection), 'Result.Failure' (imported from Lib.Result)".~~ Edit: This is not in fact true.
That's what I get for telling myself "of course it'll compile!" at 3am.
Pull requests welcome, provided you preserve backwards compatibility with pre-Windows-8. My Windows VM is not allowing me to log in lately, so I can't add `GetSystemTimePreciseAsFileTime` support myself.
Pattern matching, for me, is much about exhaustiveness checking, which is lost with that approach.
Haskeller decides to write pong. Ends up writing an arbitrary ball/arbitrary surface/arbitrary physics simulator abstracted over time. Never finishes pong, but ends up with a cool blog post. So it goes. :P
This is exactly one use case for [pattern families](https://ghc.haskell.org/trac/ghc/wiki/PatternFamilies) which is an extension that I proposed that never went anywhere, see `Match` pattern family at the very end.
&gt; That brings up the interesting possibility of encoding any list-transforming function purely as a mapping between naturals. f :: [a] -&gt; [a] f (x:y:zs) = y:x:zs f l = l g :: [a] -&gt; [a] g = id As mappings between naturals, these functions are the same. They are both the identity mapping. As list-transforming functions, they are different. It turns out that element-type independent information is not quite captured by replacement of all elements with `()`. This was mentioned in a post-presentation question (from Wadler?) at ICFP 2013, IIRC. Instead, you have to provide a bijective mapping from "locations" in the container to elements of (for finite containers) Fin_*n* where *n* is the size of the container or (for infinite containers) **N**, the natural numbers.
Fantastic, thanks for figuring this out! I wonder, could this pattern be generalized? We often seem to have this issue of a slow transformer stack where we just very occasionally need some functionality. I vaguely remember reading about this sometime ago, how this is possible with `ContT`, presumably like you have done.
Well, this is the problem which the post is generally about. What are you trying to say?
That can't be a functor, I don't believe, given that it has both positive and negative occurrences of the variable. You could split them and have a profunctor, though. newtype OrdList a b = OrdList (a -&gt; a -&gt; Bool) [b] And then you might have a kind of dinatural transformation perhaps.
Also you will be able to make them bidirectional: https://ghc.haskell.org/trac/ghc/ticket/8581 So, this means that we will be able to provide aliases for constructors. Do pattern synonyms also allow record fields to be aliased?? Either way, this means we have the ability to give synonyms for types, values, and constructors. All that remains is classes, and perhaps records.
Just so that no one misses out, there are some comments from Rich Hickey (creator of clojure) at the bottom of the page
Lots of specialization and inlining, I imagine!
ghc commits log :) 
Numeric functions are implemented as type-class functions. Number data is not tagged but numeric functions are passed a dictionary of methods which give them access to the base numeric operations: (+), (-), (*), etc. For example, the squaring function: square :: Num a =&gt; a -&gt; a square x = x*x can be thought of as a function of two arguments: a Num dictionary and an number, and it uses the dictionary to look up how to perform the multiplication on the number. In C++ terms the dictionary is akin to an object's vtbl, but with type-classes the dictionary and value travel separately whereas in C++ the object and vtbl are bound together. In most cases the type of the number (and hence its dictionary) are known, so the compiler can compute method lookups at compile time. Together with inlining, this allows GHC to produce very efficient numeric code. SPJ gives a good introduction to type-classes in his talk: "Classes, Jim, but not as we know them": https://www.youtube.com/watch?v=6COvD8oynmI 
That is so interesting about the vtable traveling separately. Thank you so much for the great answer. Curiosity sated, lol.
I think once you introduce Godel encodings, every function in Haskell is a mapping from Naturals to Naturals. :)
&gt; we need a rank-2 type to quantify r, to say that a transducer from a to b is a transformation that takes a reducer to a specific r and returns another reducer to the *same* r. the description of rank-2 seems off there, or I am just parsing the sentence incorrectly, but I thought we needed rank-2 types to allow the function to return a reducer to *different* r, since without this extension, you can't write a function that uses its argument as *different* types in the same invocation.
Numbers are, indeed boxed/lifted. At least, things like Int, Integer, and Word64 are. Things like Word# are not, but I'm not sure if you can provide typeclass instances for those. The reason Haskell can approach C speeds is that the strictness analyzer (possibly with a worker/wrapper transformation) can make the functions your write on Int actually compile to functions that work on Int#. Getting the optimizer to always output the "right" Core/STG is an interesting problem (from both sides); it can sometime result in writing non-idomatic code in the pursuit of performance, although much of the time the right RULES and INLINABLE pragmas can get your there. You'll find the high-performance Haskell programs at [the benchmarks game](http://benchmarksgame.alioth.debian.org/) are fairly un-idiomatic. Some of them are almost like writing the C code in Haskell. (It's good Haskell can do that, it leaves open the possibility of things like Haskell device drivers; but, it's bad if that's the only way to get strong performance.)
Can we not effectively alias classes with ConstraintKinds?
You can already make "simply-bidirectional pattern synonyms" (https://ghc.haskell.org/trac/ghc/wiki/PatternSynonyms#Simply-bidirectionalpatternsynonyms). So you can write {-# LANGUAGE PatternSynonyms #-} module Option where type Option = Maybe pattern None = Nothing pattern Some a = Just a and then use None and Some either as values or as patterns. (Client modules will need to enable PatternSynonyms to use them as patterns, though.) That trac ticket is about "explicitly-bidirectional pattern synonyms" where the pattern synonym in a value context is actually a synonym for a general expression, not necessarily a data constructor.
You can, but you can't use the alias when defining an instance, though oddly you can when *deriving* an instance. Not sure in which direction that is a bug. (Edit: See https://ghc.haskell.org/trac/ghc/ticket/7543.)
Well, `[a]` and `[()]` have close to the same amount of information, but `[()]` has more. You know all of the elements are identical, you can use certain type class instances and you can turn an empty list into a list with one or more elements (which is something you can't do with `[a]`).
Yeah, dinatural transformation is probably the correct solution.
In the second code sample, do you intend for `blah` to be implemented differently for different types `a`? If so, it needs to be a class method of `One`! After all you could add instances of `One` for other types in other modules, and then how would you define the behavior of `blah` on those types? This doesn't really have anything to do with the associated data family. class One a where data Two a blah :: Two a -&gt; a instance One () where data Two () = A blah A = () instance One Int where data Two Int = B | C blah B = 3 blah C = 8 
[Why not both?](http://knowyourmeme.com/memes/why-not-both-why-dont-we-have-both) Seriously though, if the deriving generates correct code, we clearly need to be able to define an instance using an alias.
How well are these optimised away?
This post is ignoring `PatternSynonyms`. For example: A: module A where data Fail = F deriving (Show) B: module B where data Fail = F deriving (Show) C: {-# LANGUAGE PatternSynonyms #-} module C ( Fail(..), pattern AF, pattern BF) where import qualified A import qualified B data Fail = FailA A.Fail | FailB B.Fail deriving (Show) pattern AF &lt;- A.F pattern BF &lt;- B.F ghci: *C A&gt; let x@(FailA AF) = FailA A.F *C A&gt; x FailA F
No I'm not looking for different behaviour using methods, I'm doing this to play around with Haskell. Using GADTs GHC can infer that the type variable `a` is `()` in `One a -&gt; a` when the scrutinee is `A` because `A` carries a proof of `a ~ ()` so that's why this compiles: blah :: One a -&gt; a blah A = () -- a ~ () here In the latter example, `A` has the type `Two ()` (note: just like `A :: One ()` in the first example!), I want to know why **data** families (not type families) that are indexed on a type can't work as a proof that `a ~ ()` in this case. The constructor `A` has the same type in the first and second example. In fact it's possible to type the example I want by mixing the approaches and adding `One'` to the latter example: data One' a where A' :: One' () B' :: One' Int C' :: One' Int blah :: One a =&gt; One' a -&gt; Two a -&gt; a blah A' A = () blah B' B = 10 blah C' C = 42 so matching on the constructors is not an issue, only having them provide a proof `a ~ ()` and `a ~ Int` without piggybacking on `One'`. This should be possible since the constructor `A` uniquely determines the value of `a` same as `A'` but if it's not possible I would like to hear why
Interesting question. I didn't think about it, but it's definitely worth investigating. I guess, this could be another advantage of extensible-effects, it has this structure already. We need measurements. A lot of measurements!
OK so the type class is a red herring. You might as well ask about the program data family Two a data instance Two () = A data instance Two Int = B | C blah :: Two a -&gt; a blah A = undefined blah _ = undefined Here is a very implementation-centric explanation. Data constructors don't actually carry any type data at runtime. Effectively they are integer tags numbered sequentially starting from 0 for each data type. In your first program you define one data type `One a` with constructors `A = 0`, `B = 1`. If I'm looking at a value of type `One a` and I inspect it and find that it is the tag 0, then I can conclude that `a = ()`. But I can't conclude that a priori just from looking at the value without knowing its type: there is no `()` stored anywhere. In your second program you define two data types `Two ()`, `Two Int` with constructors `A = 0`, `B = 0`, `C = 1`. If I look at a value of type `Two a` and I see it is the tag 0, I can't conclude anything about `a`. A polymorphic function like `blah` also has no information about the type `a` at runtime; so there is no way for it to know whether it is matching an `A` or a `B`. Well, you might say, start numbering the constructors of `Two Int` at 1, so they don't overlap with `Two ()`. But I might define instances of `Two` in many different modules; how could I ensure that I never reuse a tag? From a higher-level point of view this is a closed-world vs. open-world problem. Pattern matching is only allowed when all the alternatives are known at the pattern match site, which can only happen in a closed-world setting like your GADT. For open-world "matching" there is type class dispatch, instead.
This is what ghc does, and not universally true. 
Let's figure it out! Since the same identifiers have been reused to mean different things, here are the versions I will be using: data One' a where A' :: One' () B' :: One' Int C' :: One' Int blah' :: One' a -&gt; a blah' A' = () blah' B' = 10 blah' C' = 42 class One a where data Two a instance One () where data Two () = A instance One Int where data Two Int = B | C blah :: One a =&gt; Two a -&gt; a blah A = undefined blah B = undefined Let's see, how are the two functions implemented? Types are erased at runtime, so we can't ever look at the type of a value to determine how the runtime should proceed. Of course, typeclass methods look like they break that rule, but we know that they are implemented via a dictionary, and the functions inside this dictionary know exactly which types their arguments are. So, how are our types implemented? `A'`, `B'` and `C'` are probably implemented via three tags, `1`, `2` and `3`. At runtime, when `blah'` received the tag `1`, it knows which branch to take. Good. Similarly, `A` is probably implemented via a tag `1`, and `B` and `C` are probably implemented via tags `1` and `2`. However, that means that when `blah` receives the tag `1`, it doesn't know whether this represents a value of type `Two ()`, `Two Int`, or any other future `Two a`. I guess that's why the code needs to be rejected. Wait a minute, why doesn't the typeclass help in this case? Doesn't the dictionary tell us which type we are dealing with? Well, different types will indeed lead to different dictionaries, but the dictionary's values are opaque, so we can't just ask it which type it is representing. All we can do is to call one of those opaque functions on one of our tags which we don't know how to interpret, and *it* will know how to interpret it. Therefore, if you want to implement `blah` using families, you need to do it like this: class One'' a where data Two'' a blah'' :: Two'' a -&gt; a instance One'' () where data Two'' () = A'' blah'' A'' = () instance One'' Int where data Two'' Int = B'' | C'' blah'' B'' = 10 blah'' C'' = 42 Thank you for your question, figuring out the answer was a fun challenge.
While type classes add this kind of functionality, keep in mind that the functionality is provided *to* the function/value with the class constraint, rather than imposed on it. When you write something like foo :: Ord a =&gt; ... what you're saying is that *if* you can give me an `Ord` instance for `a`, then ... So you don't break parametricity because the `Ord`ness for any given `a` has to be supplied externally, and you can't know, from inside `foo` what you're being supplied with. `foo` works the same no matter what `a` is. Here's a simple example. Suppose we have a type class for pointed types: class Pointed a where point :: a And we define a function foo :: Pointed a =&gt; a foo = point If we transform this into the underlying type-class-free form described above, it just becomes data Pointed a = Pointed { point :: a } foo :: Pointed a -&gt; a foo c = point c Now obviously this is parametric and behaves the same for any type `a`. Within the definition of `foo`, there's no way to know what `a` is. Oh sure, you know that it's an instance of `Pointed`, but that just means you know you have a value `x :: Pointed a` you can use. So what? That's no different that having a value `xs :: [a]`. Now scale this up to a monoid. Let's skip the class definition and just use the underlying stuff: data Monoid a = Monoid { mempty :: a, mappend :: a -&gt; a -&gt; a } mconcat :: Monoid a -&gt; [a] -&gt; a mconcat c [] = mempty c mconcat c (x:xs) = mappend c x (mconcat c xs) Again, no violation of parametricity. To make this *even clearer*, let's eliminate the `Monoid` wrapper, since all it does is bundle up some values: mconcat' :: a -&gt; (a -&gt; a -&gt; a) -&gt; [a] -&gt; a mconcat' z f [] = z mconcat' z f (x:xs) = f x (mconcat' z f xs) Now that obviously doesn't cause any problems with parametricity, so why should it cause problems if you bundle the first two arguments together in a type ala the `Monoid` data type? And why should it cause a problem if the type checker finds the value of `Monoid a` for you instead of you supplying it by hand? It obviously wouldn't. You learn nothing *new* about `a` because you're not presupposing `a` *does* have these things defined for it, but rather you're saying *if* they were definable then you could do this.
GHC realizes the injectivity of data families. That is pretty much their reason for existing. Moreover, `data` families are not only injective they are "generative", which means -- even in lieu of an instance! -- they are distinguishable from other inhabitants of their target kind.
If you look at the type of foldl :: (r -&gt; a -&gt; r) -&gt; (r -&gt; [a] -&gt; r) it splits cleanly in half, the two halves are 'reducers' the composite is a transducer. This composes for the same reason that `foldMap . foldMap . foldMap` composes and `traverse . traverse . traverse`, which are the designs that inspired lens. We can coin the notion of a monoidal reducer and transducer type Reducer a m = (a -&gt; m) or type Reducer = (-&gt;) with type Transducer a b = forall m. Monoid m =&gt; (b -&gt; m) -&gt; (a -&gt; m) You can also do the same for foldr. These fit into the framework of 'pure profunctor lenses' by making a profunctor. newtype L r a b = L (r -&gt; a -&gt; r) and feeding it to a pure profunctor encoded fold. Add enough newtype noise to make it fit the lens machinery and you can see a transducer as a special case of what you can say with a Fold.
I don't understand what's being said here. &gt; The insight I gained into my audience from these analytics is that Haskell programmers do more than just skim bullet points: they read the code. What is this relative to? Is there some data that states non-Haskell programmers don't read the code?
It [depends](http://stackoverflow.com/questions/12645254/ghc-code-generation-for-type-class-function-calls). One of the first thing GHC does is compile type classes away with a dictionary-passing translation (where the dictionary is a regular data structure[¬π](http://www.scs.stanford.edu/11au-cs240h/notes/ghc-slides.html#%2825%29) in Core). Passing a dictionary around is expensive so a partial evaluator will try to eliminate run-time dictionaries by making a specialised version of a function if its type is known at compile time (a specialised version is also made if you've used the [`SPECIALISE`](https://www.haskell.org/ghc/docs/7.8.3/html/users_guide/pragmas.html#specialize-pragma) pragma). I believe this works most of the time (I'm interested in more detail when the partial evaluator fails to specialise). Here the abstract from a paper on dictionary-free overloading, note that GHC is not dictionary-free but I believe many points apply: &gt; [‚Ä¶] partial evaluator can be used to avoid the need for dictionary values at run-time by generating specialized version of overloaded functions. This eliminates the run-time cost of overloading. [‚Ä¶] for all the examples we have tried so far, specialization actually leads to a reduction in the size of compiled programs. &gt; &gt; ‚ÄîAbstract of [Dictionary-free Overloading by Partial Evaluation](http://lambda.csail.mit.edu/~chet/papers/others/j/jones/jones94dictionaryfree.pdf) If that doesn't work the dictionary can still be inlined but if it matters you should look at the optimized Core code to make sure, having a specialised function then open the possibility of further optimization like checking strictness, tree shaking (no need to pass all of `Num a` if only a single method is used) eliminating boxing which could not be done with a run-time dictionary (at least in general). [HaskellWiki](http://www.haskell.org/haskellwiki/Performance/Overloading) and the [GHC user's guide](https://www.haskell.org/ghc/docs/7.8.3/html/users_guide/faster.html) have some more information on how to optimize type classes via annotations or the `SPECIALISE` pragma. jhc also has an interesting [approach](http://repetae.net/computer/jhc/jhc-reify-typeclass.html) to compiling type classes. **Edit:** Quote from SPJ: &gt; GHC does try to optimise the task of selecting a method out of a dictionary. However optimisation is a delicate art, especially when you are looking at inner loops. It may well be that GHC screws up because of some apparently minor change. The thing to do is to look at the Core (-ddump-simpl) for your inner loop and see what the difference is. &gt; &gt; ‚Äî[Cost of Overloading vs. HOFs](http://haskell.1045720.n5.nabble.com/Cost-of-Overloading-vs-HOFs-td3182437.html)
This API has no documentation, one version, and hasn't been touched in 5 years. I would recommend rolling your own, gleaning what you can from the source. It's only 100 lines or so.
Yes, it is valid to view it that way. But it is still 'ad-hoc' in the sense that the usage of the specific argument is not checked [by the type system](http://en.wikipedia.org/wiki/Ad_hoc_polymorphism).
Of course it will, of course, of course. But maybe ... ;p
Obviously it is useful. Very much. I am wondering though if you'd have plans to incorporate the approach following by Kalibera and Jones in their ISSM'13 paper "Rigorous Benchmarking in Reasonable Time"? (http://kar.kent.ac.uk/33611/7/paper.pdf)
Thanks for your answer, I guess the type class wasn't a red herring since gelisam's answer showed how the function could be implemented but as a class method instead. Regarding your closed/open world comment, it seems like this allows you to implement open GADTs? Is this a known pattern in Haskell and are there any interesting uses?
So we end up with the function I wanted with the type signature I expected and we just needed to make it a class method, fantastic! If I understand the implications of this, this approach has the *same* capabilities as GADTs only it's open to extension? I'll ask the same questions I asked rwbarton: Regarding your closed/open world comment, it seems like this allows you to implement open GADTs? Is this a known pattern in Haskell and are there any interesting uses? I tried to implement a small embedded language: class Eval a where data Term a :: * eval :: Term a -&gt; a instance Eval () where data Term () = Unit eval Unit = () instance Num n =&gt; Eval n where data Term n = Num n | Term n :+ Term n eval (Num n) = n eval (a :+ b) = eval a + eval b instance Eval Bool where data Term Bool = F | T | Not (Term Bool) eval F = False eval T = True eval (Not b) = not undefined -- CAN'T IMPLEMENT But the following terms eval :: Term () -&gt; () eval :: Term Bool -&gt; Bool will not type check citing "Overlapping instances for Eval Bool/Eval () arising from a use of ‚Äòeval‚Äô", I'm guessing that this is caused by the openness of `Term` so even though there isn't a `Num` instance for `Bool` or `()` one might get implemented later? Is there any nice solution to this without going all out and using `reflection`?
The author lurks here as gbaz1. My recollection is that he has a paper on the approach he used in his library that was too functional for any of the numerical conferences he tried and too numerical for any of the functional programming conferences he tried -- last year he also presented a session on it at Hac Boston. Sadly, we didn't record the session, but he should still have a copy of the paper available.
For the slide that mentions recompilation using FSNotify, I've done just that. https://github.com/schell/steeloverseer
This is brilliant. I've wanted something like this for ages. It's a great way to go for teaching. * First there was [stepeval](https://github.com/bmillwood/stepeval) ([demo page](http://bm380.user.srcf.net/cgi-bin/stepeval.cgi?expr=foldr+%28%2B%29+0+[1%2C2%2C3%2C4%2C5])), but the problem was you couldn't interactively decide which expressions to expand. And it was a custom Haskell evaluator, so it only supported a few things. * I tried using GHCi in debugging mode to trace execution of a statement, similar to this, like in Inventing on Principle, where it shows the evaluations as you type. It turns out this was very slow and GHCi (at the time) would quickly run into gigabytes of memory. But the actual concept was proven, at least. I didn't keep any code for this. * Later I played with [present](https://github.com/chrisdone/present) ([demo video](https://www.youtube.com/watch?v=oJhIvHtflbI&amp;t=0m50s)), which is good for expanding data structures, but it can't expand expressions with a substitution model. Looking at the source of [hs.js](https://github.com/stevekrouse/hs.js), I see you actually implement a [PEG parser](https://github.com/stevekrouse/hs.js/blob/master/haskell.pegjs) in JavaScript (peg.js is cool, used it myself) and [an interpreter](https://github.com/stevekrouse/hs.js/blob/master/ast_transformations_spec.js) (transformer) of the Haskell in JavaScript (with [cons](https://github.com/stevekrouse/hs.js/blob/master/functions/cons.js), [plus](https://github.com/stevekrouse/hs.js/blob/master/functions/plus.js), etc.). So it's a subset like stepeval, I'm assuming. The interactive part is jolly good, and the way it shows the type of each thing you mouse over, and the consistent use of color. 10/10 on the UI, guys. The live parser on the right hand side is not bad too. I think this kind of live interaction is a great way to efficiently build intuitions about such fundamental functions like this. I can see `scanl`, `unfoldr`, etc. working the same way. One thing that would be neat to demonstrate, would be that you write an expression like `fmap (+2)` and then below it you have a bunch of data structures printed out. `Maybe a`, `[a]`, `(a,)`, `Tree a`, `Vector a`, `(-&gt; a)`, `Void`, etc. and it shows live what happens when you manipulate the `fmap` first argument. This might really be a neat way to build intuitions about what fmap is. Similarly you could do it for return and &gt;&gt;= in Monad. These kind of classes are all about that kind of "small piece of code" -&gt; "applies to vast number of instances and they all do different things". For future work on this substitution teaching, has anyone played much with using the GHC debugger? Could GHCJS be utilized in some way? The substitution is hard to do well, I think. Requires some judgment about how expansion works. As Ben Millwood experienced writing stepeval. You've done a good job of it, in the scope of this small subset.
I actually didn't write this; I saw it elsewhere and thought I'd share it. :D I linked one of the authors to your comment in the [link's comment thread](https://news.ycombinator.com/item?id=8155703) over on HN, though. Hope you don't mind. Just thought they might be appreciative of your praise and feedback.
I somehow missed this killer feature come out. This is great and definitely gives another perspective on the subject problem. Thank you! Should be noted though that it's only supported since GHC 7.8.
Sounds like fun. Are the talks in English?
 module C ( AA(..), BA(..), A(..), af, bf, f ) where import qualified A import qualified B type AA = A.A type BA = B.A af = A.f bf = B.f data A = A Int f :: A f = let (A.A a, B.A b) = (A.f, B.f) in A (a + b) ghci A.hs B.hs C.hs *A&gt; import qualified C *A C&gt; :t C.af C.af :: A *A C&gt; :i C.af C.af :: A -- Defined at C.hs:12:1 *A C&gt; :t C.f C.f :: C.A *A C&gt; :i C.f C.f :: C.A -- Defined at C.hs:17:1 ghci C.hs A.hs B.hs *C&gt; import qualified A *C A&gt; :t C.af C.af :: A.A Problem here is that compiler sometimes doesn't show fully qualified name of data type if there is name collision - yes, it could be better. Isn't there some ghc ticket about this?
In golang, they use qualified imports which works well hand-in-hand with good naming conventions. import "bytes" ... if bytes.Equals a b ...
Not quite because of the openness of `Term`, but because of the openness of type classes in general. The culprit is the `Num n =&gt; Eval n` instance, whose right-hand side is way too broad. Haskell's mechanism for figuring out which typeclass to use is *type*-directed, not typeclass-directed. That is, the resolution of a constraint `Eq (Foo (Bar Int)` will first find an instance of the form `instance ... =&gt; Eq (Foo ...)`, of which there should be only one, and then the resolution continues by trying to satisfy the left-hand-side constraints. Since your right-hand-side is `Eval n` instead of `Eval (Foo n)`, it overlaps with every other `Eval` instance. There are at least three different ways to fix the problem, but each solution has a different disadvantage. First, you could ask GHC to use a different resolution strategy, by enabling the `OverlappingInstances` extension. This allows your code to typecheck as is, but the disadvantage is that this extension is considered [unsafe](http://stackoverflow.com/questions/10830757/is-there-a-list-of-ghc-extensions-that-are-considered-safe). Second, you could drop the `Num n =&gt; Eval n` instance and replace it with a more specialized instance for `Eval Int`: -- | -- &gt;&gt;&gt; eval (I 1 :+ (I 2 :+ I 3)) -- 6 instance Eval Int where data Term Int = I Int | Term Int :+ Term Int eval (I n) = n eval (a :+ b) = eval a + eval b This has the obvious disadvantage that more instances would need to be added if you also needed `Double`, etc., and the slightly less obvious disadvantage that only one of those concrete types will be able to use the `:+` constructor. Third, you could use a newtype to direct the resolution algorithm. The idea is to introduce a dummy type constructor whose role is to direct Haskell to the correct instance implementation. newtype Numeric a = Numeric { runNumeric :: a } -- | -- &gt;&gt;&gt; runNumeric (eval (Num 1 :+ (Num 2 :+ Num 3))) -- 6 instance Num n =&gt; Eval (Numeric n) where data Term (Numeric n) = Num n | Term (Numeric n) :+ Term (Numeric n) eval (Num n) = Numeric n eval (a :+ b) = Numeric (a' + b') where a' = runNumeric (eval a) b' = runNumeric (eval b) In my opinion, this third solution is the best choice. Its only disadvantage is that users need to call `runNumeric` in order to extract the number contained inside our dummy constructor.
Glad you're interested in this! Sorry the documentation is so sparse. It was code extracted from a draft paper, but the paper wasn't accepted and while I've given some talks on this, I've been too distracted by other things to really revive the project. The draft that the code came from is here http://gbaz.github.io/slides/ode-draft-2009.pdf And the slides from the OBT talk that you found the precis of are here: http://gbaz.github.io/slides/integration.html Hopefully between the two you get the idea. If not, feel free to msg me with whatever questions you want. To clarify the conference problem, one issue is of course that we could write a better paper -- we submitted it to very competitive conferences, and honestly it is a bit diffuse. The initial paper did get too lost in the numerical weeds for functional conferences, and in the process the elegance of the underlying idea got away from us. In particular, controlling for stability is a very distracting, very fun rabbit-hole, but hard to generate good general results about. We did not try with numerical conferences because competing on that plane is very hard. An ongoing project of mine is to try to understand the right types of math to present the result in a proper, general setting. The later slides present at least one genuine theorem about the soundness of the approach, and the right setting for such theorems is a good way to quantify a broader family of ODE solvers.
Haskell has qualified imports and explicit imports and they should really be the default. `import Data.List` should be deprecated. If you want that meaning, you should have to write `import Data.List unqualified (_)`. `import Data.List as L (concatMap)` should be the default form we use for pedantry. We should add individual renames extending the as syntax to work in a variety of locations like `import Package as P (Typeclass(method as m) as TC, Type(Constructor as C, field as f) as T, infixWord as (&lt;&amp;*|&gt;))`. Not exactly sure how renames would mix with wildcard '_' imports, probably just possible to bind a type class / type constructor / type / value to multiple names. Then, we should actually remove `import Data.List` entirely for one release before bringing it back with the meaning `import Data.List as List (_)` instead of the current/old `import Data.List unqualified (_)`. We should also have local imports, but that gets a little sticky, I think. I guess we'd allow them at the top of any layout block instead of just the module block... not exactly sure.
Shameless plug: http://www.iai.uni-bonn.de/~jv/Voi09b.html (Free Theorems Involving Type Constructor Classes)
I'd accept a pull request, at the very least. The paper is not very rewarding to read :-(
What do other Haskell implementations use?
Nice work! I am too fascinated by fractals. Especially by Mandelbrot - it's amazing how much complexity (and beauty) arises from such a simple underlying mathematical structure. A suggestion if you want to continue working on this project: Have you though about adding coloring?
Thanks so much. And also great to see the prior art -- we didn't have much time to investigate what was done before (we made this in a 24 hour hackathon). Indeed, when you go out of this subset things become trickier, like you mention with visualising data structures, but also conditionals, infinite lists, etc. I hope our UI can inspire followup projects!
I thought about color too; and it is probably possible without increasing the code size much. Most important I wanted to code to be as understandable as possible (especially for non-Haskellers), so features where very high on the agenda...
I made a pull request to directly output a png file using Juicy.Pixels . It's not really faster because there is a need of some "inline" annotations on the JP side, but you can directly get a nice PNG :)
Very nice! I made something similar years back, using the GD library. http://gregheartsfield.com/fractal-hs/
Thank you for the information, that there is such a game. Didn't know it before. ;)
The Current link is ``` https://www.haskell.org/platform/download/2014.2.0/Haskell%20Platform%202014.2.0%2064bit.signed.pkg ``` The right link seems to be ``` https://www.haskell.org/platform/download/2014.2.0.0/Haskell%20Platform%202014.2.0.0%2064bit.signed.pkg ```
Richard Bird actually wrote a paper --a "functional pearl"-- on Sudoku. http://ipaper.googlecode.com/git-history/d56c604033447bc92c777994e571a1eb36439099/Miscs/Richard-Bird/sudoku.pdf
The broken links on download pages now fixed - refresh the pages!
I don't know if somebody saw, but it seems to have been resolved now after half an hour.
Nice! Updating mine.
aa a f
Arch doesn't seem to have a haskell-platform package set anymore (did it ever?) Perhaps I should replace the link with https://www.archlinux.org/packages/?q=haskell -- thoughts?
Thanks for your efforts. Perhaps this would be a better link: https://wiki.archlinux.org/index.php/Haskell_Package_Guidelines#Haskell-Platform ?
I am so excited to see this work moving forward!
aa a f
Does anyone know how long it usually takes for the Ubuntu repos to be updated?
Any special instructions for upgrading on Mac?
I actually needed to check that before writing that comment, as to me it was unclear whether `ConstraintKinds` together with `TypeSynonymInstances`would allow it. Certainly my preference is to allow it!
done.
Personally, I like to uninstall all prior Haskell remnants first, which can be done by running `uninstall-hs all` (assuming you have that command from a prior Haskell Platform install). I also make sure my Xcode and system are both up-to-date. One thing to note: The `cabal` layout for installed packages has changed. If you ever customized `~/.cabal/config` you'll want to look carefully at the file `~/.cabal/config.platform` that is created *after* the first time you run cabal *after* you install the platform. If you've never customized the file, it should just update automatically after the first `cabal` run. 
Interesting, it's rather strange that standaline deriving works, whereas ordinary deriving / instances do not. It would be rather cool to be able to do this: type Classes a = (Read a, Show a, Data a, Typeable a) newtype T = T Int deriving Classes I often times end up copying big deriving lists. The only downside I can think of is that it makes it a little bit more obscure where a particular instance is coming from, and makes it harder to text search. Solving such issues is just a matter of tooling, though, so I'm in favour of a feature like this.
It's quite an interesting approach to modularity. I'd be interested to see how the ecosystem changes if/once this gets implemented.
If I was to install the haskell-platform would there be any reason I wouldn't be able to sanely upgrade the individual components (like ghc) as I please?
Thanks!
should i continue using [ghcformacosx](http://ghcformacosx.github.io) or replace it with haskell platform?
As far as I know, it's a tedious and error-prone process, and that is why Haskell Platform is there. But yeah, if you know what you are doing, then you can do it by yourself, too.
I feel like `base` would effectively have to be restructured. We would have to have the `String` type abstracted behind an API. Other types would also benefit, such as `[]` (list) and `TVar`/`MVar`/`IORef`. (The last one is a bit dubious, but all of those *could* fit a similar abstraction -- not-guaranteed-thread-safe-mutable-variable).
The biggest Haskell programs are not compiled with ghc. 
Well, jhc passes types around and dispatch based on these. You can also completely monomorphise the program (except for polymorphic recursion). 
okay sweet thanks, I'm sure I'll be able figure it out when it happens. 
Nice explanation. Upvoted for: &gt; ... As is common with Haskell, this results in what appears to be happy coincidence, but is actually the product of developing a language on top of such a consistent mathematical foundation.
I hope there will be some tangible progress at the end of the summer plus some more great explanations like this to motivate the community to continue to solve this. Imagine if instead of spending our time arguing about how to manage packages and figuring out how to fix broken installs we programmed our way out of this mess. You might consider mono-traversble including Textual rather than some kind of StringLike abstraction since it is built on top of the well known typeclass abstractions but just made monomorphic. Textual is then a sequential (a restorable Foldable: fromList . toList = id) with the monomorphic element being a Char. http://hackage.haskell.org/package/mono-traversable-0.6.1/docs/Data-Sequences.html#t:Textual
Sorry, but I just migrated that page on the wiki to [ArchHaskell](https://wiki.archlinux.org/index.php/ArchHaskell) (it was *not* about "Haskell package guidelines") and your link is now dead. Please link to [Haskell#Haskell_platform](https://wiki.archlinux.org/index.php/Haskell#Haskell_platform), part of the main Haskell article for Arch, instead of the packaging guidelines.
I guess that depends on the audience. I‚Äôm certainly prepared to give mine in English, but if everyone present speaks German it‚Äôd probably feel weird...
Excellent article. I was getting to grips with working with Applicatives and Functors today (first time doing anything useful with Haskell), and this has sped up my understanding tremendously, especially by providing a concise and intuitive explanation of the differences between Functor and Applicative and Monad. Cheers!
I'll prepare the slides in English - if there is German only crowd I think presentation will be German as well.
Replacing GHC in particular would discard all of the packages that the HP ships. Packages in GHC are not compatible between GHC versions.
that might take quite a while (or even be the next or Ubuntu version) - but there is a binary package right at the homepage that works fine with the current ubuntu and linux mint version (just tried ;) ) - just remove your old haskell-platform (or everything with haskell from synaptic), remove your ~/.cabal folder (rm ~/.cabal -rf) and follow the instructions (download the tar, unzip it from your download folder to the root: mostlikely `cd ~/Download` then `sudo tar xvf haskell-platform-2014.2.0.0-unknown-linux-x86_64.tar.gz -C /` and finally activate it `cd /usr/local/haskell/` and `sudo ./ghc-783-x86_64/bin/activate-hs /usr/local/haskell/ghc-7.8.3-x86_64/`) ... please note that the activate needed the root to ghc on my system and that the command to unpack the tar that was given on the homepage will unpack the platform into your download folder ;) (if you start in it) 
Wow thank you guys ... seems to be working with most things I use - right now the only thing I saw that causes trouble are - hdevtools - aeson (0.8.0.0 claims to break unordered-containers-0.2.4.0, stylish-haskell will install 0.6.2.1 that works fine for me right now)
Yeah, I went ahead and installed that after my post. I just like having the repo version of all programs when possible, so removal is easier if I need to do it.
Ok, point taken. So would a better wording be "to render a part of the Mandelbrot set as an image"?
true - but this one puts it into /usr/local and this is much better then the mess the repos caused in my system ;)
Excellent article! Thumbs up.
Yeah, that's fair. I guess all you'd need to do are remove `/usr/local/haskell` and clear out the symlinks to the binaries.
If you were developing a library and a user imported this module into a file with additional instances defined then your library would break if it compiled when there was only one set of valid instances availible.
Maybe you want a fundep from t to i?
This might be a detailed answer of what you are looking for: http://stackoverflow.com/questions/12220932/no-instance-error-with-multi-parameter-classes
The part about functors and applicative functors was really good, but in the part about monads, you got the type signature of (&gt;&gt;=) wrong. (&gt;&gt;=) :: Monad m =&gt; m a -&gt; a -&gt; m b -&gt; m b Should be: (&gt;&gt;=) :: Monad m =&gt; m a -&gt; (a -&gt; m b) -&gt; m b Of course, this means that the rest of the monad-explanation will need a bit of rewriting. ;)
Doesn't work on Fedora 20 (for me). Tried to download the source tarball and the deb7 bindist for amd64 (also tried the CentOS one, but didn't have gmp.3). Ran ./platform.sh bindist.tar.xz; Got the following error: cabal: The program 'hscolour' version &gt;=1.8 is required but it could not be found. I'm looking for a way to get ghc-7.8 for Fedora. Edit: As per this issue: https://github.com/haskell/haskell-platform/issues/98 I installed hscolour manually. Building now continues. Edit2: And apparently, activate-hs can only be run as root, even if I'm doing a local install. Why is that?
Don't forget to use this opportunity to [change your cabal configuration](http://www.vex.net/~trebla/haskell/cabal-cabal.xhtml).
No idea why I'd never looked into the Typeclassopedia before, but I have now, thanks to that link, and wow.
Pulled it in and updated the PDF.. Code-wise it is quite a bit cleaner, and it shows that Haskell has a great lib for rendering PNGs. Thanks!
I have incorporated your improvements -- thanks!
I would highly recommend not using haskell platform for OS X. Instead, install only the compiler and do everything in cabal sandboxes. I would also highly recommend nix (though note [this bug and workaround](https://github.com/NixOS/nixpkgs/issues/2689) for the moment) which basically gets rid of cabal hell. 
debian apt repos http://deb.haskell.org Ubuntu ppa https://launchpad.net/~hvr/+archive/ubuntu/ghc
Nice work, Mark!
Sorry it doesn't work on Fedora 20.. I'll add that to the notes. The root issue with `activate-hs`was reported, but I didn't want to muck with the logic that close to the release. There needs to be some sort of test that the current user can write to the `--prefix` tree. Pull requests welcome!
What's the reason for packaging cabal 1.18 instead of 1.20? 1.20 not stable enough yet?
The new look is still being worked on by /u/urinlaura90 - We'll update the look of the HP site as part of the whole Haskell branding overhaul.
Installing, either global, user, or sandbox, newer versions of the packages installed with the platform is a normal thing to do. If you work on multiple projects, many people find using cabal sanboxes to be the best solution (at the expense of some disk space and recompilation time.) The only thing you want to be wary of, is *never* let cabal "re-install" a package. (That is, re-compile and re-install the *same* version of a package, but with different versions of dependent packages.) It will ask before doing so; always say *no*. As pointed out below, upgrading GHC is a major operation... It is what the platform is there for! 
Yup! That's was why I laid it out that way. I'm planning an `uninstall-hs` tool for linux that'll do just that!
Thanks!
Fantastic article. I've been thinking about this for a while now and this has really cemented my understanding of the Functor-Aplicative-Monad hierarchy. 
GHC 7.8.3 uses Cabal-1.18, and the command line `cabal` tool packaged with the platform matches. While it is possible to have later versions of cabal installed that are beyond the compiler's bundled version, in consultation with the cabal maintainer, we decided that had too many pitfalls for the unwary. 1.18 has sandboxes, usually the feature people want out of installing a new cabal these days. You can build and install the newer cabal-install-1.20, but note that your package db will now have two different Cabal packages in it... not a problem unless you *also* compile something against the ghc package.
Minor correction. The equivalence of `a -&gt; b -&gt; c` with `a -&gt; (b -&gt; c)` is not because of currying, but because the `-&gt;` type operator associates to the right. Currying is the equivalence of `(a, b) -&gt; c` with `a -&gt; b -&gt; c`.
bike shedding the name: it should be open import Data.List as in agda (and others i suppose).
done. again. :-)
Correct. Hopefully the author is still around to correct this source of misinformation. OTOH, in GHCi `:t (&gt;&gt;=)` will always show you the authoritative type.
You mean, you built from source and it worked on Fedora, yes?
Precisely. The conceptual view here is as follows: if `U` has a left adjoint `L`, then its codensity monad (i.e. the right adjoint of `U` along itself) is `UL`. Apply this fact to the forgetful functor `U : Mon -&gt; Set`, and you get that the free-monoid monad 'F' can be obtained as `F = Ran U U`. Using the expression of a right Kan extension as an end, you get exactly what you wrote above. In fact, you can get any sort of free construction this way, see for example: https://hackage.haskell.org/package/free-functors. Unfortunately, making this precise in, say, System F, is not immediate, as far as I can tell. In fact, equating these sorts of constrained universally quantified types to ends requires quite a strong form of parametricity (essentially, parametricity over laws), that no one has actually proved yet, to my knowledge. 
Yes. I have ghc-7.6 installed via Fedora's package manager and used that to compile the latest haskell platform from source.
I have written a cool JSON-RPC library in Haskell called json-rpc. It is available in Hackage: http://hackage.haskell.org/package/json-rpc
that would be awesome :D
Is there a way to get the binary to work on a 32 bit machine?
typo: "but you [are] still saying no to modularity"
Are you planning platform releases for each GHC release? Say, for GHC 7.8.4 with Cabal 1.20?
&gt; There's no way to satisfy all of the constraints at play for this, regardless of destructive package updates. I'm still not getting it. The constraints cannot be satisfied for a single package foo. But if foo-1.0 and foo-1.1 are installed side by side, I don't see why the linker can't figure out what package to use for compiling what module (basically treating foo-1.0 and foo-1.1 as separate packages). 
The plan is to be *able* to track that closely if we want. It will depend a fair bit on what the GHC team's intention for each release is. In general, my hope is that release planning and coordination will be a discussion topic at this year's **Haskell Implementors' Workshop** in Gothenberg in September.
I think `lens` is a more powerful alternative to monotraversable. Basically, for any method in the `MonoFoldable` or `MonoTraversable` class, drop the `o` prefix and add an `Of` suffix and you have the equivalent lens combinator. Apply that combinator to the `each` lens and you recover the equivalent behavior. Example: -- mono-traversable function oforM -- lens equivalent forMOf each The advantage of `lens`, though, is that you can change `each` to a more specific lens. `each` is just a lens that uses type class magic to "guess" what you wanted to fold or traverse (just like `monotraversable` guesses what you wanted to traverse). However, you can replace it with a specific lens that indicates the specific depth at which you intend to traverse. For example, when traversing a `ByteString`, you might wish to traverse bytes or bits. `mono-traversable` locks you into traversing bytes, but with `lens` you have the option of traversing either one just by providing the appropriate traversal: forMOf bytes :: Monad m =&gt; ByteString -&gt; (Word8 -&gt; m Word8) -&gt; m ByteString forMOf bits :: Monad m =&gt; ByteString -&gt; (Bool -&gt; m Bool) -&gt; m ByteString
I don't think that it is possible to run the 64-bit binaries on a 32-bit kernel. I don't have a 32-bit build environment on my Ubunutu 12.04 system at present... but it would easy to build an bindist for that. I can probably get one set up next week (I'm on vacation at present, and without my usual cadre of machines.) 
I like Haskell. I really do. I understand its background too... but I can't be the only person who feels like he walked into the wrong room when he reads "transducers are monoid homomorphisms."
Theoretically it's possible to treat both of the packages as entire separate. But suppose you depend on two different versions of ByteString. One value can only have one data type. So you can't pass it between the functions of the two libraries since each expects it's own data type. You would have the very same problem that he addresses in his blog post, hence the idea to use something more natural like module signatures instead of (sometimes) arbitrary version numbers to ensure compatibility.
It's exciting, isn't it?
Ah right. Thanks for the explanation.
Yes, it is :-)
It's all fancy words for exactly what the code is saying. The Cayley representation says that structures often have an isomorphism with endomorphisms (which is the whole idea underlying "transducers" from the beginning) so we can expect that the monoid structure of `Monoid m =&gt; Endo m` is similar to the monoid structure of `m` itself. Then "split injective" talks about the properties of invertible functions. Basically it just says that the function `m -&gt; Endo m` has a left-inverse (called its splitter). So ultimately, it's a lot of words to say that rep :: Monoid m =&gt; m -&gt; Endo m rep m = Endo (mappend m) per :: Monoid m =&gt; Endo m -&gt; m per (Endo f) = f mempty ought to have per . rep = id
Nice! I was looking for that originally, but wanted it to be `[b] -&gt; [a]` not `b -&gt; [a]`! Thus, I kept getting stuck. This is the best. Edit: I wrote up some detail about this [here](http://tel.github.io/2014/08/10/typing-transducers/)
&gt; It's all fancy words for exactly what the code is saying. Yes, but they're fancy words that _I_ don't know _yet_.
I'm still a novice at Haskell, but there are a few things about the `Monad` section that don't seem quite right to me. First, there's the `m a -&gt; (a -&gt; m b -&gt; m b)` that a few people have mentioned. (As an aside, every time I complain about the gazillion fixity rules in Haskell and Haskeller's aversion to parens making it hard to read I'm told that it isn't a problem in practice... and then I see confusions like this that are caused precisely because of those two issues.) There's also this bit: &gt; Simply put, a `Monad` is a type that can do everything an `Applicative` can do plus handle unwrapping. However, it can't just unwrap values willy-nilly. It can only unwrap a value in a very specific case: while passing it to a function which returns a wrapped result. ... &gt; Consider a type like `Maybe`. If we were able to unwrap values at any point and return them directly, we'd be in trouble when we come across a `Nothing`. If, on the other hand, our type signature says we ourselves have to return a wrapped result, we can take the reasonable step of not unwrapping anything and simply returning another `Nothing`. *All* functors support unwrapping in a limited context. In fact, the kind of unwraping this article talks about doing with a Maybe works just fine with `fmap` (aka `&lt;$&gt;`): ghci&gt; let functorAdd3ThenDouble x = (*2) &lt;$&gt; (+3) &lt;$&gt; x ghci&gt; :t functorAdd3ThenDouble functorAdd3ThenDouble :: (Num b, Functor f) =&gt; f b -&gt; f b ghci&gt; functorAdd3ThenDouble (Just 5) Just 16 ghci&gt; functorAdd3ThenDouble Nothing Nothing Here the `5` in `Just 5` is "unwrapped" by the first (rightmost) `&lt;$&gt;` and given to `(+3)`. The result is then re-wrapped (by the same `&lt;$&gt;`), and unwrapped by the next `&lt;$&gt;` and passed to `(*2)`, etc. My understanding of the main* difference between `Monad` and `Functor` is that `=&lt;&lt;` (and it's flipped cousin `&gt;&gt;=`) expect to get already "wrapped" value(s) from the supplied function, and that/those value(s) are then "flattened" into the result. (In fact, some non-Haskell implementations of `Monad` call this operation `flatMap`.) This operation is therefore more powerful than `fmap` as it can remove values or turn a single value into multiple values, while with `fmap` *n* values in always results in *n* values out. I guess that "flattening" operation can sort of be thought of as one extra unwrapping, but that wasn't demonstrated in the `Maybe` example chosen. \* Other differences included flipping the arguments in `&gt;&gt;=` (making operations happen in the same order you read them in) and adding the `do`-sugar. Together these are probably why `Monad` is used in a lot of cases where its extra power isn't actually even needed. Edit: spelling/grammar/formatting
In my implementation of a GLSL eDSL, https://github.com/fiendfan1/Haskell-GLSL-eDSL, the main benefit is that you can specify the shader code and the data being sent to OpenGL at the same time. Instead of loading shaders, then creating and filling buffers with data, then binding each buffer, getting uniform locations and setting, etc., you just specify the data and the implementation to transform the data.
Brilliant
Continuing to chase the real problem: You can see that it's trying to upgrade text, why? $ cabal install aeson-0.8.0.0 --constraint='unordered-containers==0.2.4.0' --constraint='text==1.1.0.0' --dry Resolving dependencies... cabal: Could not resolve dependencies: trying: aeson-0.8.0.0 (user goal) next goal: text (dependency of aeson-0.8.0.0) rejecting: text-1.1.1.3, 1.1.1.2, 1.1.1.1, 1.1.1.0, 1.1.0.1 (global constraint requires ==1.1.0.0) rejecting: text-1.1.0.0 (conflict: aeson =&gt; text&gt;=1.1.1.0) So there you go! You should be able to install aeson-0.7.0.6 without issues as far as I can tell.
Good point, If you're actually generating shaders I can see how this might be nicer and safer than anything text based or whatever limit facilities GLSL offers.
I'm not sure I get how this actually works. Looking at your sample: position &lt;- inn vec3 ("position", objectVertices) This seems like the usual loose/stringly binding between shaders and their input data, like you wouldn't get a compile time error if you used a mesh without UVs with a shader that requires them?
Thank you :)
it's ok for now - I use sandboxes for everything other and for example snap is problem if I force reinstalls in the sandbox ;) .... ok just saw that snap just installes this package version :D
Well, the eDSL does 2 things: generates GLSL code, and logs each variable declared and the corresponding value. If I declare a variable to represent uv coords, the inn function requires that I give it a value (I guess you could give an empty list...)
Yes. The main reason for its existence in the current form is it composes with (.) ;) Hrmm that sounds familiar.
Yes, but if you understand what the code does, and you see the words, eventually you'll start to pick up intuition for what all that high-falutin vocabulary is all about.
Uh, yes, sorry, I'm blind. I get it now ;-)
To expand a bit on your last point, the main difference between `Monad` and `Functor` is that `Monad` has a "flatten" operation called `join` (this is all theoretical, I'm going to pretend you can define `Monad` instances in terms of `join` and `Functor` is a superclass of `Monad`): join :: Monad m =&gt; m (m a) -&gt; m a If we call `(&gt;&gt;=)` "`flatMap`" instead of "`bind`", then it hints at the fact that we're performing two operations: a "map" and a "flatten". We can already `map` as a `Monad` since all `Monad`s are `Functor`s, so the flatten operation is the actual new functionality. And indeed, given `join` as our flatten operator we can define `(&gt;&gt;=)` in terms of `fmap`: (&gt;&gt;=) :: Monad m =&gt; m a -&gt; (a -&gt; m b) -&gt; m b ma &gt;&gt;= f = join (fmap f ma) -- fmap f :: Monad m =&gt; m a -&gt; m (m b) As you mention, because we have this flattening operator (`join`) we can change the "shape" of the structure, which is something that `Functor` cannot do.
thanks for this reminder - most likely will join in wednesday :)
Great article and very easy read. Thanks
Depends on what you mean by "human-oriented". If you mean "like most humans communicate", well then, nothing. On the other hand, if you mean "conveying a very precise understanding of high-level abstraction such that another trained human can instantly understand the level of abstraction upon which you're operating", then it has everything to do with it.
I can totally understand that, most if not *all* the people you see talking about these notions actually felt exactly just like you. I've been around for long enough that I've seen some people begin learning haskell who later became respected contributors to the ecosystem and still are. Let me just throw below two "advices" I sometimes give, hoping at least one of them will be useful to someone -- although there's nothing really new here, I've seen similar advices given by other haskellers. 1) Don't be scared. I started learning haskell something like 6 or 7 years ago now, and back when I was just starting to feel comfortable with the basics of haskell, I stumbled upon posts about things like "right kan extensions" (I swear there was no single code-block that was clicking for me, let alone the explanations with those "fancy words") and you know what? I *understood* those posts, many years later, and even had a couple of discussions about these with the author. Of course, it hasn't been smooth. Every now and then I was heading back to these posts, trying to understand them, *again*, still not understanding at the beginning, and progressively developing some intuition as I gathered more knowledge about related things. And I'm clearly not the smartest guy of the community, so if I've managed to understand all these posts/papers I've struggled with for quite some time, you should be able to. Just don't be scared and impatient. 2) Why, for starters, are there so many people talking about mathematically inclined abstractions? There are a couple of good writeups about this, including a few by Edward Kmett. But if I have to sum that up in a sentence or two: an abstraction/notion generally lets you talk about a huge variety of objects (or "things" if you prefer) not one by one but about all at the same time (forgetting all those small unimportant details each particular object has), and the notions tend to imply a few interesting laws, which determine with more or less precision how all of the objects should behave when considered from the point of view of that notion you're interested in. And no matter the weather, what you ate for dinner or the color of the walls around, and more importantly, *no matter what any user of your code does*, your objects will still behave as dictated by the laws. Now that I've modestly and very quickly summed it up, here's the really big appeal I find in these *just from a software enginnering point of view*. By paying enough attention to these abstractions and given some experience with them, you can actually make it close to impossible for anything to go wrong in some pieces of code. And if the littles pieces never go wrong and you compose them (with the appropriate notion of "composition") to form bigger pieces, these most likely won't go wrong. I don't think anyone can reasonably ditch that kind of reasoning for its "non-realworldness".
No, no. Don't worry about it.
Parsec has to be the most grockable Haskell library. It is a joy to use.
I was going to ask you to expand on the last step, but I'll try myself. (note: `reverse :: [a] -&gt; [a]` is a (non-trivial) automorphism of `[a]`, so that implies there are several different isomorphisms possible for the theorem below.) Lemma: `[a]` and `forall r . r -&gt; ((r, a) -&gt; r) -&gt; r` are isomorphic with iso1 :: [a] -&gt; (forall r . r -&gt; ((r, a) -&gt; r) -&gt; r) iso1 [] e _ = e iso1 (a:as) e m = m (iso1 as e m, a) iso2 :: (forall r . r -&gt; ((r, a) -&gt; r) -&gt; r) -&gt; [a] iso2 f = f [] (\(l,a) -&gt; a:l) Step 1: `iso2 (iso1 x) = x` by induction on `x`. Base case: iso2 (iso1 []) = iso1 [] [] (\(l,a) -&gt; a:l) = [] Inductive case assuming `iso2 (iso1 as) = as`: iso2 (iso1 (a:as)) = iso1 (a:as) [] (\(l,a) -&gt; a:l) = (\(l,a) -&gt; a:l) (iso1 as [] (\(l,a) -&gt; a:l), a) = a:(iso1 as [] (\(l,a) -&gt; a:l)) = a:(iso2 (iso1 as)) = a:as Step 2: `iso1 (iso2 f) = f`. The free theorem of `f :: forall r . r -&gt; ((r, a) -&gt; r) -&gt; r` is &lt;roconnor&gt; @free f :: r -&gt; ((r, A) -&gt; r) -&gt; r &lt;lambdabot&gt; g . h = k . $map_Pair g $id =&gt; g (f x h) = f (g x) k Given arbitrary constants `e :: r` and `k :: ((r, a) -&gt; r)`, we will take g = \f -&gt; iso1 f e k x = [] h = \(l,a) -&gt; a:l First we check that these choices satisfy the precondition that `g . h = k . $map_Pair h $id` g (h (as,a)) = iso1 (a:as) e k = k (iso1 as e k, a) = k (g as, a) = k ($map_Pair k $id (as,a)) This means the free theorem implies that `g (f [] h) = f (g []) k`. Now we can conclude with: iso1 (iso2 f) e k = g (iso2 f) = g (f [] (\(l,a) -&gt; a:l)) = g (f [] h) = f (g []) k = f (iso1 [] e k) k = f e k Since `e` and `k` were arbitrary, by extensionality we have that `iso1 (iso2 f) = f`.
Since `Fold' a b` is also isomorphic to `a -&gt; [b]` (Has anyone actually proved this?) then Transducers and Folds are (morally) isomorphic.
I'd say the problem isn't specifically field accessors, but exposing the internal structure at all.
If you don't allow me to pattern match on an ADT that has no extratypical invariants to maintain, I will find you, and I will tell you how many times I uninstalled your packages and extirpated them from my dependencies.
Isn't it much simpler than that? data Listf a x = Nil | Cons a x forall r . r -&gt; ((r, a) -&gt; r) -&gt; r forall r . (Maybe (r, a) -&gt; r) -&gt; r forall r . (Listf a r -&gt; r) -&gt; r and then that final bit is `[a]` by the universal property of the list type? It's certainly the case for finite lists at least, right?
I'm always happy to offer `*.Internal` modules for those who are willing to accept the possibility of breakage in future versions of the library, but by hiding the representation in the recommended interface I am reserving the right to change whatever I want under the hood, and it's your problem if you choose to violate the contract I am offering.
For the finite case, yes. I mentioned this in reply to a previous post. The problem is that transducers are tied to left folds, so you get a pretty inefficient folding (always associated to the left, no opportunity to share work) compared to the monoidal notion of a Fold, and they don't extend to the infinite case in their current form unless you count infinite snoc-lists. Nothing keeps you from defining a Transducer via the foldMap signature or by taking the foldr signature and flipping it around to be symmetric: flip . foldr :: Foldable f =&gt; (a -&gt; r -&gt; r) -&gt; f a -&gt; r -&gt; r Then you can handle the infinite list case at least, and with the foldMap version you can handle infinite recursion anywhere in the term with a suitably lazy Monoid. Note that Fold in the infinite case is more appropriately isomorphic to `a -&gt; Free Monoid b` as in the present of infinite left recursion in the term structure the two aren't isomorphic making `Free Monoid /= List`. newtype Free p a = Free (forall r. p r =&gt; (a -&gt; r) -&gt; r)
How does C, and other procedurally-oriented programming languages which Haskell claims to outperform in the "understandability"/"elegance" departments, not immediately convey a very precise understanding of the program's raison d'etre to another trained human? Genuine question, not trying to start a war. It's just that Haskell caught my eye, and I've finished an advanced university course with it, but I still don't understand how all this super-abstract thinking actually helps. From what I've come to understand, actual "real world" programs written in Haskell are few and far between, and it's quite easy to see why - if I was going to write, say, a music player in Haskell I wouldn't know where to start. The community goes on about "thinking in different ways", but when does this thinking actually pay off, and how? I want to like/love Haskell, but articles like this one just make me sigh. C is ultra-elegant and understandable, fast and useful - so where is Haskell's edge? It feels like a super-mysterious japanese martial art - if you study it for 30 years, you will be able to knock out everybody instantly. But if you take Muay Thai you will be able to knock 99% of people out with a year of training - so that last percent of people never really comes into play. If you find the time and energy, please clarify.
Probably, but I'm not familiar with `forall r . (F r -&gt; r) -&gt; r` being isomorphic to `Mu F`. Is this obvious in some way?
Fair enough. A lot of packages on hackage hide their internal modules, which is a bit irritating to me.
It is just defining Mu F in terms of how you reduce it with F-algebras.
Not well documented, but there is an existing attoparsec CSS parser: http://hackage.haskell.org/package/css-text-0.1.2.1/docs/Text-CSS-Parse.html
Hmm, that seems to imply that the least fixed point of any (positive) functor exists in system F. While I expect the least fixed point for strictly positive functors to exists and the least fixed point of many other positive functors to exist, I didn't think that every positive functor had a least fixed point, and for that reason CiC and MMLT based dependently typed languages require strict positivity for their inductive types. Maybe system F cannot be naively interpreted in MLTT, or maybe some problem caused by dependent elimination that requires more than positivity, or something else?
No harm in that. 
I am surprised that nobody has come up with some sort of extensible records proposal based on lenses. Or maybe I just haven't looked. 
This is done in Idris and the original paper proposed "idiom brackets" so you could write `(| f a b c |)` rather than `f &lt;$&gt; a &lt;*&gt; b &lt;*&gt; c`
I like lenses, but the solution here is for you to export a `defaultConfig` value and have people update it with record update syntax to override defaults. When you add more fields you add the defaults to this value.
Both LambaCube 3D and GPipe are abstractions over GLSL and also OpenGL commands and state setup code. Using them you don't have to worry about (don't have control over) GL state changes, uniform setups, resource allocation, GLSL program binding. You just work with composable functions. These systems generate GLSL and OpenGL state modifier commands from the pure rendering pipeline function. (described by EDSL) So if you'd like to write GLSL explicitly you have to use other libs.
I don't think that completely fixes it either. If a user was doing this: defaultConfig { configFieldA = fieldAVal } ...then after the refactoring that code will no longer work. It will have to change to: defaultConfig { _configMC = defaultMConfig { _mconfigFieldA = fieldAVal } } But if the library exported a lens called configFieldA, then before the refactor the user would have had something like this: set configFieldA fieldAVal defaultConfig And after the refactor that code would still work because the definition of the configFieldA lens would be changed accordingly. The composability of lenses is a key part of preserving backwards compatibility.
Rest assured, at least one person (myself) got the joke. I thought it was hilarious.
Well, if it's any consolation, the idea of "transducers" has only been under public discussion for about a week. You could 100% understand the concept of monoid homomorphisms and still not know what this post is about because you missed a couple of days of Reddit...
I think glguy proposes not to introduce MConfig, but to just add fields to the original Config. If the users use accessors and defaultConfig consistently, then nothing would have to change. By the way, what is the right thing to do when there is no sensible defaultConfig (i.e., a mandatory configuration parameter)? I feel that accessors have an advantage here; with lenses you would have to construct an incomplete record before updating it (and might miss some fields). With accessors you can at least rely on ghc's warnings. EDIT: of course one could provide functions like mkConfig f1 f2 f3, but this is really messy when some fields share a type. 
But you can't use them safely like constructors, i.e: pattern match with exhaustiveness checking. Well, it's possible, but extremely cumbersome.
An alternative more modern approach, now that we have pattern synonyms, is that one can also expose an opaque pattern which doesn't necessarily reflect the structure underneath at all.
It seems I can get rid of the ambiguity by using [Data.Proxy](http://hackage.haskell.org/package/tagged-0.7.2/docs/Data-Proxy.html): class Unbox ty where type Unboxer ty :: * unbox :: Proxy ty -&gt; Unboxer ty instance Unbox Int where type Unboxer Int :: Int -&gt; Int# unbox Proxy (I# i) = i Yielding: Œª :t unbox (Proxy :: Proxy Int) 42 unbox (Proxy :: Proxy Int) 42 :: Int#
https://ghc.haskell.org/trac/ghc/wiki/Records/OverloadedRecordFields/Design#Lensintegration
How is this problem handled in other languages/package management systems?
It's consistent to allow fixed points of arbitrary positive type operators. However, it's inconsistent to allow that and classical axioms, for size reasons. (Eg, consider mu a. (a -&gt; 2) -&gt; 2, which makes sense as a type, but not as a set.)
We need to add this stuff to the lens haddocks, but in the meantime it might help someone to see some of these examples. There are a couple of benefits of writing things out like this. You get fewer dependencies if you just want to write these optics for upstream library consumers. Additionally it is more efficient to write your lenses manually than to use the `lens` combinator.
So, is it your opinion that "(Monoidal) transducers are monoid homomorphisms" is something that you either understand or not, but a person is unlikely to *believe* that they understand it if they actually don't? Is that the gain?
The cool thing about this is its also sufficiently general that it can be integrated with *all* lens libraries, not just `lens`. 
I would say it depends. In the string case, there is usually one main type used by the vast majority of packages. Otherwise: - For c#/java: the good practice is to use interfaces (~ existential types). - In SML, there is a powerful module system, considered too complex by some people (see [here](http://research.microsoft.com/en-us/um/people/simonpj/papers/haskell-retrospective/HaskellRetrospective.pdf) slide n¬∞61 -- SML functors) - In python, modules are just another type of object. 
maybe `coerce` should be put into `contravariant` and `prism` and `iso` into profunctors...
Concerning "multiple instances of the same package" and "unsatisfiable constraints" my guess is that they ensure backwards compatibility so that it is always safe to use the newer version.
That sounds surprising, because e.g. in Java people usually don't complain that third-party libraries are too well encapsulated. In fact the opposite complaint is more common, and exposing internals is seen as a mistake. Can you explain why Haskell folks think third-party types should have exposed internals?
That's great, thanks :) do you suppose there's any way to avoid having to give the Proxy argument? I played around with adding a second method to the type class with a default definition of partially applying `unbox` to Proxy but I couldn't get it to work
&gt; Maybe system F cannot be naively interpreted in MLTT, or maybe some problem caused by dependent elimination that requires more than positivity, or something else? You need an impredicative universe U to embed System F in MLTT. Classically, U is necessarily equal to 2, as /u/neelk pointed out. That gives you a model with just two types. To have non trivial classical models of System F you have to resort to those weird *algebraically compact* categories like CPO. Related question: is there a variation/subset of System F without this (IMHO rather undesirable) feature? Something where you can't just universally quantify over an arbitrary profunctor, so that ultimately only strictly positive functors are guaranteed to have an initial algebra?
Interesting. Using `return (a::ty)` as a concise way to produce a `Proxy ty`, then GHC typechecks this: Œª :t (\a -&gt; unbox (return a) a) (\a -&gt; unbox (return a) a) :: (Unbox ty, Unboxer ty ~ (ty -&gt; t)) =&gt; ty -&gt; t Œª :t (\a -&gt; unbox (return a) a) (42::Int) (\a -&gt; unbox (return a) a) (42::Int) :: Int# It gets wonky once turned into a binding though: Œª let smartUnbox a = unbox (return a) a smartUnbox :: (Unbox ty, Unboxer ty ~ (ty -&gt; t)) =&gt; ty -&gt; t Œª :t smartUnbox (42::Int) &lt;interactive&gt;:1:1: Couldn't match kind `*' with `#' Expected type: Int -&gt; Int# Actual type: Unboxer Int Kind incompatibility when matching types: Int# :: * Int# :: # In the expression: smartUnbox (42 :: Int) Weird message aside, GHC seems to expect kind `*` in a lot of places once polymorphism is involved (which makes sense given the restrictions put on `#`), and even abusing `(a -&gt; b) :: *` where `b :: #` appears to have limits.
The benefits aren't restricted to non-strict languages. Think of the headaches people have had in the past getting umboxed values into OCaml and F#.
Yes, this comes down to the fact that polymorphism over unboxed types is prohibited, because they don't have a uniform representation. The equation that arises from the type family instance is forall (a :: *)(b :: *) . Flip (a -&gt; b) ~ b -&gt; a which quantifies over *, so `a` cannot be unified with something of kind #. There's nothing to stop you defining type family Flip a where Flip (a -&gt; b) = b -&gt; a Flip (Int# -&gt; b) = b -&gt; Int# but this may or may not be useful.
I think Haskell gives more confidence in refactoring because the type-system can be better utilized, and Haskell is not mainstream, so its userbase consists mostly of people used to wading through cruft and being on the bleeding edge, without a sense of entitlement that a mainstream language engenders in its users that things never ever break or change.
I learned it from Gibbons: http://www.cs.ox.ac.uk/jeremy.gibbons/publications/adt.pdf
[Here](http://www.reddit.com/r/haskell/comments/1wgob8/bidirectional_patterns_synonyms_with_or/cf1wsh9?context=3)'s a comment illustrating the concept. I'd love to see how it works *in practice*.
I made some attempt to do a bit of this, but it's in the wiki not the Haddocks https://github.com/ekmett/lens/wiki/How-can-I-write-lenses-without-depending-on-lens%3F That page is definitely missing the 4-parameter formulations and a few other types, though. I do agree that the Haddocks are a primary location for this information, though. It'd be nice to add a "tutorial" module perhaps.
Particularly since the rest is derivable from fmap and pure.
The final goal was to make a type class with two methods: unbox :: Unboxer ty box :: Flip (Unboxer ty) where `Flip` is discussed in [this thread](http://www.reddit.com/r/haskell/comments/2d81tl/type_families_on_unboxed_types/). This is mostly for fun and I'm not sure if there is any benefit to functions that are polymorphic over unboxed values
Interesting :) since there is such a limited number of unboxed types something like this could be a short hand for defining a special for all of them: type family Flip a where Flip (a -&gt; b) = b -&gt; a Flip (a# -&gt; b) = b -&gt; a# Flip (a -&gt; b#) = b# -&gt; a But it may be of limited value as you pointed out. I was attempting to use `Flip` to allow something like: class Unbox ty where type Unboxer ty :: * type Unboxer ty = Flip (Boxer ty) type Boxer ty :: * type Boxer ty = Flip (Unboxer ty) box :: Proxy ty -&gt; Boxer ty unbox :: Proxy ty -&gt; Unboxer ty with instance Unbox Int where type Unboxer Int = Int -&gt; Int# box Proxy = I# unbox Proxy (I# i) = i as is discussed [here](http://www.reddit.com/r/haskell/comments/2d7arf/type_class_with_unbox_prim_prim_method/).
What I meant was that with lens you have the choice of using a type class or not. If you want to use a type class you use `each`. The only difference is that its slightly more characters, but there's no fundamental difference from monotraversable: oforM = forMOf each
There's a paper I've been meaning to read: James and Sabry *The Two Dualities of Computation: Negative and Fractional Types*. I imagine they will talk about it a bit there.
Looks really cool, and very simple. Looking forward to trying this out!
I need to check this out. It's sort of natural to think "Hey, we've got sum types, and product types! What the heck do difference and quotient types look like?" I googled for "quotient types" a couple of weeks ago and found nothing really interesting. This looks really cool, thank you!
The fact that to use these "resource-managed pipes" we have to wrap the entire pipes API is rather unsatisfying. Can they not be slotted into the existing pipes infrastructure somehow?
They are only added to the file which you pass as an argument. I'm interested in hearing ideas for a more targeted approach.
djv: I'm planning on adding a machine-parseable version of the profiling output. What kind of format would be easiest to use for your tool?
This is cute, but it has big drawbacks that I think outweigh its value, and I personally wouldn't use it in practice for any real work. The problem with wrapping every subexpression in an SCC annotation is that it distorts the behaviour of the program. SCC annotations are (a) expensive and (b) disable a number of compiler optimisations. So for instance, if you annotate a small arithmetic expression, it will suddenly become quite a bit more expensive than it "really is". For example, a small loop involving 64-bit integer addition takes 6 ns on my laptop in a criterion benchmark, 15 ns with `-prof`, and 21 ns with an SCC annotation. These annotations tends to be lumpy and uneven in their effects, so that if you have a lot of annotations turned on, the things that you're being *told* are taking a lot of time don't necessarily correspond to the things that really *do* take a lot of time. The approach that I've converged on with SCC annotations over the course of a number of years is to use them very sparingly, and with considerable skepticism when trying to interpret their results.
Agreed about the distortion issue. The tool is meant to be used as a first pass when profiling your code. My intuition is that code is usually computationally expensive because it is called in a loop. In such cases adding SCCs might exaggerate the percentage of time taken but it will still show which part is the slowest. For example the 15 to 21ns difference that you mentioned. The disabling of compiler optimisations is what worries me more and haven't really thought about. I would be interested in seeing examples where adding SCCs completely distorts the profiling report.
For my case a csv formatted output would have been perfect. In general people might need to parse the tree structure of the profiling report. For those cases a format which supports nesting would be better.
This paper was previously discussed some on reddit [here](http://www.reddit.com/r/haskell/comments/25ujqk/is_there_any_implementation_for_this_paper_about/).
Seeing as you were the one that 'got me into this mess' with your wonderful post on the types of data, I'll definitely have to give it a read.
This definitely looks like something I need to read all the way through. I can only hope I've got enough understanding to actually do that.
&gt; Monoidal transducers are easier to understand than transducers. They can be interpreted categorically... *eyes glaze over* *shakes head* *eyes clear* I think you may have lost some of the audience there, but it is nice to have a categorical analysis of Transducers.
&gt; extremely beardy quasi-philosophy I believe the established epithet for anything that mentions category theory is "abstract nonsense". There's no need to invent your own terms when the structure they describe is already isomorphic to an already named structure. ;)
This is a config structure, so getters and setters **are** the meaningful operations.
A warning, the ideas in this paper don't totally "work" (yet?). There seems to be some substance there, but the picture is not entirely clear.
Be aware that the language in which they use these types is reversible, and their interpretation (that `-a` is `a` flowing backward and `1/a` is `a` imposing a constraint on its context) depends somewhat on that structure, but the types themselves could be used in other ways. In a dataflow language, for example, you could use these types to talk about ‚Äúturns‚Äù in the flow of computation. That might let you express cycles, or maybe futures. You could also use them to model continuations, by interpreting them as ‚Äúawaiting‚Äù future values. Perhaps /u/edwardkmett has some ideas here. :) 
I'll take it with a grain of salt then. I am interested to see how it handles 1 / (1 - X) as the type of lists as a sort of 'simple test'.
That sounds like a very interesting test case, I don't think the paper pursues that direction at all. Do post if you figure something out!
Patches welcome :-)
Have you seen [ghc-events-analyze](http://www.well-typed.com/blog/86/)? I've been meaning to check it out. If it's a better solution for you I wonder if it wouldn't be too difficult for djv to support that as well.
There is a very limited form of subtyping going on for (-&gt;). If you go back to older GHC's it would show up as `?? -&gt; ? -&gt; *`. We don't show it that way any more. Simon has some plans for how to play with the encoding of the kind that makes the need for the limited notion of subtyping we still have go away and get replaced by parametricity. We copied his idea into Ermine and they seem to work very nicely, so it should go smoothly for GHC when and if he decides to finish killing the existing sub-typing there.
That's perfect! Thank you! The groupBy seems kinda strange (from source) but I will propably give it better look tomorrow.
Has anyone got a good setup for running profiling during continuous integration and making the results available as some kind of report?
`pipes-safe` is more powerful, but more complicated. For example, `pipes-safe` lets you catch and recover from exceptions and also finalizes resources more promptly than the solution in the blog post.
I was a little vague, sorry! I believe that many Haskell abstractions make programs easier to reason about, in the same way that it's easier to follow a program written with function calls and for loops than one that used goto for everything. Of course, more abstraction is not always better; any abstraction has to pay for itself by making other stuff easier. Pretty much everyone thinks recursive functions are worth the complexity, for example, though that used to be controversial. I've managed to convince myself that stuff like monoids and functors are also a net positive. There's a lot of wacky stuff that comes out of the academic/theoretical side of the Haskell community that I'm not too sure about yet -- but I'm happy to wait and see. BTW, the 'transducers are monoid homo' stuff is a bit of a red herring: 'transducer' is a new coinage by the Clojure people, and a bunch of folks have been trying to find the equivalent abstraction in Haskell. Since I know a lot about monoids and a bit about homomorphisms, I learned things from just the title that I didn't get from the original blog post. (eg. the circumstances when computations using them can be safely parallelized, and how to do it.) Whether it's actually *worth* knowing this stuff, though, depends on whether 'transducers' turn out to be nice abstractions in practice -- and the jury's going to be out on that for a while.
Eh, with out a runnable semantics, its just the isomorphism, which is basicly just the reverse of the derivation, and because you don't have exponentials actually doing anything with a list is really hard. (I think..this stuff is weird and makes my head hurt)
Thanks! Currently, all it does in terms of NLP is syllablization, and I chose to write that function myself, in order to ensure that it works as expected on words that are common in Divan poetry. (the tests should cover that) Also, I don't have much experience with NLP, or with the NLP libraries in Haskell, but I would like to see how it would be enhanced. Another possible change might be using Data.Text instead of String. It would be very helpful to read someone else's changes on this code, since I'm a Haskell beginner.
This is friggin' beautiful. Can't wait to try this out.
Thanks, very interesting. Besides the awkwardness, I would suppose that requiring the prisms to be completely polymorphic is pretty limiting in practice. 
I proved this [here](http://www.haskellforall.com/2014/07/equational-reasoning-at-scale.html) in Appendix B. Technically, the proof was for `Monoid`s, but you can generalize it to `Category`s by just replacing `mappend`/`(&lt;&gt;)` with `(.)` and replacing `mempty` with `id`.
This post is a goldmine for me. Thank you so much! After poring over the paper for a little, I have a suspicion I can use this along with trace to represent iterating over a list. But I could be completely wrong - there's still so many concepts in this paper that I need to understand before I can really grok it.
Is it possible to imagine a pipes-safe API on top of managed resources ? From all pipes apis, pipes-safe is the one I know the least. Probably because I kind of feel that it is the less beloved one (maybe wrongly so). It is rather difficult to know when you actually need it (this is surely my lack of concrete and practical experience with the pipes ecosystem). If I remember correctly there was still a feature missing in order to match conduit error handling ? Anyhow this reddit thread is probably a terrible place to discuss this ;-) Sorry about that.
Additionally in node modules are installed locally by default (like sandbox). Dependencies of modules are installed inside the parent module (recursively). That way every module has the dependencies of the version it wants and conflicts won't happen. Also because of the dynamic nature of javascript there is an implicit module signature that most often stays the same.
Could you briefly share the core of the idea? This sounds like it *might* be related to something I've been thinking about for the past half year.
&gt; Probably because I kind of feel that it is the less beloved one (maybe wrongly so) This perception is correct. It's still due for one last major improvement. It's the only one of the core `pipes` libraries that I consider incomplete. The most appropriate abstraction to build `pipes-safe` on top of is actually [the `Resource` monad](http://www.haskellforall.com/2013/06/the-resource-applicative.html) (now part of `resourcet`). The reason why is that you need to preserve the open and close actions.
Also note that Cabal now has a `hyperlink-source` setting in the `haddock` section, so you don't have to specify `--haddock-hyperlink-source` manually "every bloody time". (If you don't see the setting, you're probably using a .cabal file generated by an older version. Delete it and run `cabal update` to get the fancy new template.)
I think the problem with any hierarchical sampling profiler is that you still need to maintain a call-order stack like the CCS, which is going to be pretty intrusive for a language like Haskell. I'll wait and see what improvements 7.10 brings, but I'm a bit skeptical that this can be done quickly.
Keep in mind that groupBy expects an equivalence and that elements are grouped by matching against only the first member of the group, so while using (&lt;) might work out in this case, it will often lead to surprises.
&gt; Technically, the proof was for Monoids, but you can generalize it to Categorys by just replacing mappend/(&lt;&gt;) with (.) and replacing mempty with id. Unintentionally relevant, I *just* published a blog post (first in a looong time) about the similarities between Monoid and Category. http://unknownparallel.wordpress.com/2014/08/11/similarities-monoid-monadplus-category/
As kamatsu points out, "idiom brackets" let you locally overload the whitespace syntax to mean `(&lt;$&gt;)`/`(&lt;*&gt;)`. (The "Applicative" class used to be called "Idiom" back in the day.) The problem with overloading whitespace more globally is that we run into the problem of ambiguity over which instance to use when type checking. The same problem shows up with `do`-notation, and is why we define newtypes for monad transformer stacks, giving the instances for the newtype instead of trying to lift everything implicitly. Too much polymorphism keeps the type checker from working as a debugger.
I don't think so, actually. IMO, the big problem is that it requires your datatypes to be represented as a sum-of-products, and that representation exposed to the type system. [Conveniently](https://ocharles.org.uk/blog/posts/2014-08-07-postgresql-simple-generic-sop.html)...
Division by zero is handled because they use meadows instead of fields. *Edit:* IIRC, they don't mention anything about meadows in that paper though. The problem is that fields, in general, are non-algebraic and non-constructive. There are [multiple ways to constructivize fields](http://ncatlab.org/nlab/show/field), even without getting into alternatives to fields (e.g., meadows).
I think it's important not to take the "Applicative is parallel" analogy too far. It can also be useful for inherently serial tasks. For example, consider the following function: import Control.Monad.State import Control.Applicative data Tree = Leaf Char | Branch Tree Tree data LabeledTree = LLeaf Int Char | LBranch Int LabeledTree LabeledTree deriving (Show) label :: Tree -&gt; LabeledTree label t = evalState (label' t) 0 where label' :: Tree -&gt; State Int LabeledTree label' (Leaf c) = LLeaf &lt;$&gt; incr &lt;*&gt; pure c label' (Branch lt rt) = LBranch &lt;$&gt; incr &lt;*&gt; label' lt &lt;*&gt; label' rt incr = modify succ &gt;&gt; get The labeling of the right and left branches cannot be done in parallel: the left branch must be done first to get the correct index to start the second branch.
Would it be possible to just add the parent ID as a field to every record in the CSV file? I do something similar for the [profiteur](http://jaspervdj.be/posts/2014-02-25-profiteur-ghc-prof-visualiser.html) tool.
Do note that there are two independent interpretations of "`1/X`". On the one hand we can think of it as *division* of `1` by `X`; on the other hand we can think of it as the *reciprocal* of `X`. While these notions coincide in popular structures like the Real numbers, they diverge in popular structures like matrices. In particular, given the matrix equation `X == I + A*X`, the solution (if it exists) is `X := (I - A)^{-1}`. While many square matrices have two-sided multiplicative inverses, there is no unified notion of division on matrices since multiplication is non-commutative. This suggests that what we really want for handling the case of linked list is reciprocal types, not type division. It's telling to look at how these operations are encoded for [combinatorial species](http://byorgey.wordpress.com/2014/08/10/readers-wanted/).
Oh? Do you just mean that Idris has idiom brackets, or that it actually overloads *everything* on the choice of `Applicative`? If it's the latter, could you share a reference?
there is another straightforward construction that says "if you can do something involving subtraction and division, according to certain rules, but the result involves neither, then the result can still hold" http://arxiv.org/abs/math/0212377 That certainly handles your list structure example, and many other cases. And instead of applying to a vague language we might yet develop, it works in Haskell right now :-) 
Yes, see [here](http://www.reddit.com/r/haskell/comments/25ujqk/is_there_any_implementation_for_this_paper_about/chohp1x).
Also in Java you can always cheat with reflection. In Haskell that's actually somewhat harder :-)
I'd love to have an accurate non-hierarchical profiler. 
This is great! It might shed some light on why lists represented as 1 / (1 - X) feel dishonest, but the Taylor expansion 1 + X + X^2 + ... still works. I have a feeling posting in /r/haskell is going to give me quite the reading list of fascinating academic papers. I am in no way complaining!
Idris has "dsl" blocks in which everything is interpreted to be part of an applicative.
Make a new 'sort', lets call it boxity. data Boxity b = BoxityVar b | Boxed | Unboxed data Kind b a = KindVar a | Kind b a -&gt; Kind b a | Type (Boxity b) | ... Now where you quantify over kind vars, let you quantify over boxity vars as well. Currently we use something like having (-&gt;) :: ? -&gt; ? -&gt; * with magic subtyping relationships: ? / \ # * Now we can make variables that can quantify over boxities, so we can replace the use of (?) in (-&gt;) with (-&gt;) :: forall (a :: Boxity) (b :: Boxity) = Type a -&gt; Type b -&gt; Type Boxed ... all suitably dolled up to likely keep the change mostly invisible for users. This basically gets rid of bounded subtyping and replaces it with another universe level with very weak structure.
Actually setting up ghcjs is **much** easier than it used to be (at least on OS X and Linux). For this reason I am not sure if anyone is still using the vagrant stuff and it is probably out of date and broken (it was good back before ghc 7.8 was released, when just compiling ghc HEAD and the various packages needed for ghcjs to run was a bit of a nightmare). Please try the [installation steps](https://github.com/ghcjs/ghcjs) and file an issue or hop on #ghcjs if you have any trouble getting it working. That said if anyone has time to set up a Vagrant (or better yet Docker) script that would be much appreciated. Start with something that installs GHC 7.8.3 and then add the ghcjs [installation steps](https://github.com/ghcjs/ghcjs).
I'm finishing up a blogging snaplet and plan on writing a series of articles on the construction of it and my own website as a whole (built in snap with backbone, react, and requirejs). Keep an eye on this sub I'll post it sometime in Sept.
Whats a meadow?
Same here. I also did something similar in the ghc-time-alloc-prof package.
Please try GHCJS if you have time.
Briefly, MonadPlus is a special class of monoids. A monoid can be seen as a category with one object, or, conversely, a category can be seen as a generalized monoid with notions of "target" and "source". So they're all really closely related. We can pretty quickly identify all of these, too. This newtype lets us view any MonadPlus as a Monoid newtype PlusMonoid m a = PM (m a) deriving (Monad, MonadPlus) instance MonadPlus m =&gt; Monoid (PlusMonoid m a) where mempty = mzero mappend = mplus And this one lets us view any Category and choice of object as a Monoid, essentially focusing on the subcategory of that single object‚Äîa monoid there. newtype Focus c a = Focus (c a a) instance Category c =&gt; Monoid (Focus c a) where mempty = Focus id mappend (Focus f) (Focus g) = Focus (f . g) 
I had tried and failed with the installation steps, but that might be because I was trying on Windows. I'll try on Linux, maybe that will work easier.
How much of Hackage can Haste build? I'm trying to port specific Haskell projects into JavaScript to run in the browser, so there are a lot of libraries which would need to build, thus the appeal of GHCJS. I'm not just looking for a general functional approach to JS, I'm pretty strongly wedded to [Elm](http://elm-lang.org) for that.
The Grothendieck group construction lets you turn a commutative monoid into a group -- applied to the naturals, it gives you the integers. A variant of this construction, called the geometry of interaction construction, takes a monoidal category (with a fixed point operator, aka a trace) and produces a compact closed category. This is super important in semantics, since it explains where higher order behavior comes from. A really clear and concrete explanation of it can be found in Samson Abramsky's paper *Retracing some paths in process algebra*.
Agreed, there's still use to that! As soon as 7.10 hits, I'll see if I can adapt my native code profiler.
&gt; I put ‚Äúfree‚Äù in quotes because I do not claim to actually understand what this means in category theory, nor do I claim to be using that term correctly in the category theoretic sense. I‚Äôm pretty sure I‚Äôm somewhat close to that definition, though. Ow
Really want to see ghcjs on nix. 
i'll have a go at it in the next week i suppose.
I don't think State buys you much : update :: Transaction -&gt; Book -&gt; Book update (Transaction Buy p q) = (cash -~ toAmount p q) . (shareCount +~ q) update (Transaction Sell p q) = (cash +~ toAmount p q) . (shareCount -~ q) update (Dividend amt) = \b -&gt; b &amp; cash +~ toAmount amt (b ^. shareCount) update (Split ratio) = shareCount *~ MkQty ratio Then you can combine them in either way : position :: [Transaction] -&gt; Book position trs = appEndo (F.foldMap (Endo . update) trs) (Book 0 0) position' :: [Transaction] -&gt; Book position' = foldl' (flip update) (Book 0 0) 
He didn't do that bad at all. His "free monoid" is in fact the free monoid over one generator `[()]`, his "free MonadPlus" is the free monoid, and his "free category" *is* basically the free category over a graph with objects in `Hask`. If would have been better to use `[a]` as a free monoid over sets and `ListT` as a "free monoid over monads", but this is alright really.
Data.Map.Strict will evaluate both the keys and values into WHNF, but only when the map itself is forced. The drawHands loop never forces the map. It just inserts a value and loops again. Add a ! to the m argument of drawHands; "where drawHands n' !m =" and you'll see a slight speedup. You can remove the ! from r. And an exercise for you; drawHands only uses IO to call randomHands exactly "n" number of times. You can break out the use of IO; use replicateM to call randomHands "n" number of times, and then create the map using fromListWith (+) without being in the IO monad.
A strict map ensures that your keys and values are evaluated whenever your map is. But it doesn't ensure that the map itself is evaluated to start with. And your `drawHands` code doesn't evaluate its maps, just lazily builds more of them, so by the time it returns the final map a terrible number of thunks have been built.
I think that‚Äôs a sensible elaboration on /u/danharaj‚Äôs words: "[‚Ä¶] pattern match on an ADT that has no extratypical invariants to maintain".
&gt; This makes them like "value objects" in Java, which only have constant fields‚Äîchances are you expect those to be completely exported. Well, not always. For example, Java's String is opaque, and its internals have changed between JDKs without breaking clients, while Haskell's String has an exposed implementation as [Char] and many people dislike it. I think Java's approach makes more sense in this case.
The magnitude of the distortion if you have SCCs everywhere is such that you will give completely bogus results, of the "not even wrong" category. Seriously, just don't do this: you will guarantee ably and predictably send people off on attempts to optimize completely irrelevant code, just because you're giving them pretty pictures that they don't know are fundamentally flawed. There, I hope I've stated this strongly enough this time.
I've been using profiling a lot lately, on a huge application, and while it's not the easiest tool to use, I do find that when I fix something that shows up in the profile that does translate to improvements in performance on the non-profiled code. But you have to be careful how you use it: start with an SCC on main, and gradually add more SCCs to drill down into the expensive parts of your program until you have enough information to fix the problem. Try to avoid putting SCCs on tiny functions and in low-level libraries. If you compile with -fprof-auto everywhere you're going to get distorted results. 
A lot of the time improving the functions the profiler blames helps, but sometimes it's just a red herring. While having just a few SCC annotations is good, you have to have some clue where to put them. A simple sampling profiler would help with this. 
Indeed the original code was like that (there is a function to *fold* up the stats just above it in the gist) - but due to the 8GiB trouble ;) - I wanted to break it down to look into it in the debugger - that's why I expanded it that way ;) But I did it using forM and the fromListWith is nice (did not see it before) - thank you!
Thank you both - I still have to get used to the lazy stuff
hmm ... can it be that there replicateM won't be possible (I think it uses foldR internally and so it will build up the 1.000.000 thunks too(?)) - I get the same memory leak no matter what I try with the map - for example: rankStats :: Hands -&gt; [(Text, Int)] rankStats = sortWith (negate . snd) . M.toList . foldl' count M.empty . map classifyRank where count !acc r = let m = M.insertWith (+) r 1 acc in seq m m randomStats :: Int -&gt; IO [(Text, Float)] randomStats n = percentOf . rankStats &lt;$&gt; replicateM n (randomHand defaultDeck) where percentOf :: [(Text, Int)] -&gt; [(Text, Float)] percentOf = map (\ (x,c) -&gt; (x, 100 * fromIntegral c / fromIntegral n)) 
Awesome, I'll be looking forward to it!
I'm a ways behind you ( chapter 7 ) but I find the book to be great. With a universe as dense as Haskell, I think the book does a good job of introducing the reader to Haskell development. It refers you to hackage and src where appropriate and introduces real world projects via the time machine store. Theres not going to be 1 definitive text. I suggest RWH and LYAH ad companion texts.
How does it compare to LogicT? My first impression is that your ListT is less efficient (looking at `&gt;&gt;=`) and more limited.
Nice! The ListT transformer (done right) is essentially what one would need to implement F# asynchronous sequences in Haskell (http://tomasp.net/blog/async-sequences.aspx/). I wonder if that abstraction would be useful for Haskell programming?
I liked the book. I agree it lacks polish/quality to an alarming extent sometimes. But it gets a lot more right than it does wrong. For example, it shows off the Text library and Conduits.
How's this differ from the Pipes implementation?
It's more basic. It should be possible to implement Pipes in terms of it, thus lightening its API. We should ping /u/Tekmo on this one.
I got to chapter 6 and gave up. The writing is obviously from someone for whom English isn't a first language. That's OK, but it shouldn't have passed the review stage. My biggest problem, however, is with the technical content. Some of it is just plain wrong, while some of it is poorly explained and misleading. For a book aimed at Haskell beginners, this is a major issue. Ironically, I learnt a lot by fixing these as I went through the book. I tried to submit errata to the Apress site, but kept getting errors on their forms and gave up with that as well. The book also falls down on it's basic premise: "Beginning Haskell - A Project-Based Approach". There's no project. There are some references to the "Time Machine Store" but most items are left as exercises to the reader, and there isn't even a set of source code for the finished project. The book only cost me $15 during a special offer. If I had paid the current price of $34.99, or bought the printed book at $49.99, then I'd be looking for a refund.
Correct. And keeps the very last number.
Funny thing. I've done [this proof](http://r6research.livejournal.com/22105.html) before, but I didn't understand it at the time. To me it was some long series of steps involving many Greek letters. Now I see that it is working through the proof that `T a := (a -&gt; 2) -&gt; 2` has `P := forall a. (T a -&gt; a) -&gt; a)` as its (least) fixed point since `T` is a (positive) functor. Well, not exactly P, but a variant of P with parametrically baked in.
That's fine if you already understand stuff ---- not so much if you're trying to learn from it!
Note that you don't need a separate type class for `ListMonad`. You can already implement `cons` in terms of `MonadPlus`: cons :: MonadPlus m =&gt; a -&gt; m a -&gt; m a cons a ma = return a `mplus` ma
I gave an example for how to also add `await` functionality [here](http://stackoverflow.com/questions/25070740/if-monadplus-is-the-generator-class-then-what-is-the-consumer-class/25082997#25082997). However, `ListT` is a subset of `pipes` functionality, not the other way around. As an example, you can't implement a `Producer` with a non-`()` return value in terms of `ListT`.
Well, yes, but `mplus` requires a bit more of packing-unpacking fuss compared to `cons`. And since it's the most basic operation I expect it to have a notable impact on performance. Looks like we need a benchmarking suite.
I liked the book. But - as others said - it contains a lot of errors, and the "project" in the title is a bit misleading. It's more like a unified theme of selling "Time Machines". The author seems very passionate, and that's cool in my opinion. It's , forgiving the errors, a nice introduction to what the community of haskell is currently using: cabal sandboxes, lenses for records, conduit for IO, persistent for databases, Scotty as a micro web framework... And several "unanimously" accepted GHC Extensions. As such it's a nice view of what is being used on "modern" haskell. The big problem is that beginners are the one that suffer the most on errors, and the book is full of them. But I'm a beginner and I found it to be a fun reading - I just read it as a high view of haskell libraries/extensions.
it's pretty shitty at the moment, but i have a (sort of) library for doing precisely this sort of things (writing api wrapper / clients): https://github.com/intolerable/api-builder dunno if it's flexible enough for use with the strava api since i only really use it for my reddit and dota2 clients (i'd be glad to make changes to make it more useful though) and it's probably a bit late now since you've written everything but it might be worth a look (docs are a little lacking in places, its a work in progress)
The result of "randomHands" is a big list, and you only want a small part of it. When you run replicateM, also reduce the value to only the key for the map. randomStats :: Int -&gt; IO [(Text, Float)] randomStats n = percentOf . sortWith (negate . snd) . M.toList . makeMap &lt;$&gt; replicateM n (do !r &lt;- classifyRank . head &lt;$&gt; randomHands defaultDeck 1 return r) where makeMap lst = M.fromListWith (+) $ map (,1) lst percentOf :: [(Text, Int)] -&gt; [(Text, Float)] percentOf = map (\ (x,c) -&gt; (x, 100 * fromIntegral c / fromIntegral n))
The default name `ghcjs-boot` expects for node.js is `node`. Some distros install it as `nodejs`, in that case you need add `--with-node nodejs` to the boot command. I'll add a note to the installation instructions. (The Windows build is indeed broken. I've fixed most of the issues recently but I still need to update the build tools archives) 
I bought the ebook version - if it is just a matter of typos, those can be corrected and ideally an updated ebook version released. But, I have less Haskell experience (i.e. I'm a beginner and re-reading LYAH right now) so I can't really comment on the technical content.
[T.S.M.N.W.A.](https://www.youtube.com/watch?v=eSyVbd6ug2Y) but otherwise, fun stuff!
Another how does this differ from... http://hackage.haskell.org/package/List?
&gt; a category can be seen as a generalized monoid with notions of "target" and "source". I don't know if it helps anyone, and it certainly didn't help me at the time, but while I was participating the the ICFP 2014 programming contest, I realized that categories are just indexed monoids. I was introduced to indexed monads by /u/Tekmo while he was developing pipes, and since then I've been playing around with them in different contexts. In this particular case, I thought I was building a indexed monad, then realized all the "return values" were just `()`, so I could probably just drop that, and deal with the objects as some kind of "indexed monoid". At the time, I hadn't heard of indexed monoids, but my subconscious started putting together the laws for indexed monoids and I thought they were amazingly similar to the category laws. Later on, I was able to verify my intuition at nLab. newtype Blur a i o = Blur a instance Monoid a =&gt; Category (Blur a) where id = Blur mempty (Blur f) . (Blur g) = Blur $ f `mappend` g Hide the "Blur" constructor and use the `i`/`o` phantom types to control exactly what compositions you want.
&gt; It didn't occur to me to look for a library like this! even if you had when you'd started writing, it probably wouldn't have existed :) &gt; I'll see if Strive can be implemented using your API builder. if it can't, please let me know ‚Äì there are at least two APIs I know of that it can't handle (but im working on them)
I'm getting no audio. Is it all mixed into the stereo channel that fell off my headphones? Are slides available separately? It's a lot faster for me to go through some slides than to watch the whole video.
Well, don't you get something like that with *pipes* (and *pipes-concurrent* for the async part) ?
Neat. This is basically replacing the direct subkind relationship between `?` and `*`/`#` with the polymorphic instantation relationship between `forall (b :: Boxity). Foo b` and `Foo Boxed`/`Foo Unboxed`? (Is there some accepted term for this quasi-subtyping-ish relationship between quantified types?) (As for whether it's related to the thing I was thinking about... still a very strong *maybe*. It's the sort of (maybe even precisely?) reverse question of adding first-class polymorphism, or type parameters which can be instantiated by types-which-are-not-known-at-compile-time, to a language (Rust) which is *unboxed* by default and (so far only) implements polymorphism using monomorphization.)
Thanks again - but "randomHand" in this case is just a list of 5 cards and this way I lose the nice reuse of rankStats as well but I'll give it a try ... yes that does indeed work as well, with only about 4times the memory of the *low-level* version above - I think the trick is just to force the Text - this should sum up to about just the memory size I still think it should be possible to generate the random hands "on demand" and use something like foldl' to keep the memory really low, no matter the size of the input set - maybe using sequences or something But of course this is not really possible right? The type system should prevent this kind of side-effect escaping the IO monad so either do it inside or return the complete bulk?
Check out Simon Thompson's latest edition of [Haskell: The Craft of Functional Programming](http://www.amazon.com/Haskell-Functional-Programming-International-Computer/dp/0201882957/ref=sr_1_1?ie=UTF8&amp;qid=1407858201&amp;sr=8-1&amp;keywords=thompson+haskell). I have a few other recommendations [here](http://reinh.com/notes/posts/2014-07-25-recommended-reading-material.html).
The audio is unfortunately one-sided (the right track is the one you want to listen to). I downloaded the video and watched it using vlc. I can't see [any slides](http://www.jonmsterling.com/talks.html) but we can ask /u/jonsterling gently. :)
Oh. It's unfortunate. However we still can make a simple conversion from `ListT` of "list-t" into a `Producer` of "pipes", which means that the `ListT` of "pipes" functionality potentially could still be transferred to the "list-t" library. producer :: Monad m =&gt; ListT m a -&gt; Producer a m () producer = lift . ListT.uncons &gt;=&gt; traverse_ (\(h, t) -&gt; yield h &gt;&gt; producer t) However it looks impossible to provide an inverse conversion without access to the internals of the "pipes" API. Or is it, btw? My intention with "list-t" was to bring something neutral in respect to the choice between such libraries as "pipes", "conduit" and "machines". I think we could all benefit from having this as a sort of "lingua franca" between those libraries. That is besides the functionality that this library brings on its own.
IMO, API of "List" is overcomplicated with a questionable type class. Besides fixing that "list-t" provides more instances of standard classes and is maintained. However essentially the implementation of `ListT` itself is similar, yes.
Why not? Isn't it a valid unicode identifier? Are you perhaps saying that "‡≤†" does not count as a letter, thus a non-operator identifier cannot contain it?
Apparently it's "kannada letter ttha", so yes, it is a letter. http://unicodelookup.com/#%E0%B2%A0/1
A couple of years ago I posted http://lpaste.net/75725 to poke fun at unicode operators and identifiers.
Haskell names can contain special characters, and this module defines this function as follows: ‡≤†_‡≤† :: String -&gt; a ‡≤†_‡≤† = error
Reminds me of http://en.wikipedia.org/wiki/Kind_Hearts_and_Coronets 
I love the acme category on hackage.
Yes, it's a letter, but its category is "letter, other". A Haskell varid has to start with a lower case letter, so to be valid it should have category "letter, lowercase". So I claim any Haskell compiler accepting this is wrong. 
https://xkcd.com/927/
[Image](http://imgs.xkcd.com/comics/standards.png) **Title:** Standards **Title-text:** Fortunately, the charging one has been solved now that we've all standardized on mini-USB. Or is it micro-USB? Shit. [Comic Explanation](http://www.explainxkcd.com/wiki/index.php?title=927#Explanation) **Stats:** This comic has been referenced 712 times, representing 2.3894% of referenced xkcds. --- ^[xkcd.com](http://www.xkcd.com) ^| ^[xkcd sub](http://www.reddit.com/r/xkcd/) ^| ^[Problems/Bugs?](http://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me) ^| ^[Delete](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=delete&amp;message=delete%20t1_cjo5ovs)
I see. At least ghci won't let me use it as an upper case letter too! Prelude&gt; data ‡≤† = Foo &lt;interactive&gt;:9:6: Malformed head of type or class declaration: ‡≤† Prelude&gt; data Foo = ‡≤† &lt;interactive&gt;:10:12: Not a data constructor: `‡≤†' 
How would you want to attribute PC addresses to source code in a sampling profiler? That's the tricky part. We're planning on integrating the work of Peter Wortmann and others which does this by annotating every subexpression and maintaining the annotations through the compiler, but it's quite complicated and hard to ensure that it doesn't interfere with optimisations.
 &gt;&gt;&gt; isAlpha '‡≤†' True &gt;&gt;&gt; generalCategory '‡≤†' OtherLetter `OtherLetter` is treated as 'lower case' as it isn't an `UppercaseLetter`. This lets more languages be used in variable names, since many don't have case at all.
Technically a letter, other isn't valid at all in a varid which only allows small or large characters. I wonder if GHC just shortens to `isLetter c &amp;&amp; not isUpper`.
Change the Haskell report then, because it says: uniSmall -&gt; any Unicode lowercase letter This seems quite clear to me; "other" is not lowercase. Don't get me wrong, I'd like it to be legal. But as a Haskell implementor I try to follow the report. 
Yes, that's exactly the bug in ghc. 
While this is funny, it's not applicable here, since "list-t" does not compete with any of the three existing standards.
So `MonadPlus` could in theory be removed without too much change in functionaliy but `Category` is necessary due to being more powerful.
&gt; which means that the ListT of "pipes" functionality potentially could still be transferred to the "list-t" library For the sake of what???
Am I the only who thinks that it should be `‡≤†_‡≤† = undefined` (or, alternatively, `‡≤†_‡≤† = error "‡≤†_‡≤†"`)? Requiring a `String` after ‡≤†_‡≤† feels pretty strange to me.
&gt; My intention with "list-t" was to bring something neutral in respect to the choice between such libraries as "pipes", "conduit" and "machines". I think we could all benefit from having this as a sort of "lingua franca" between those libraries. That is besides the functionality that this library brings on its own.
In the [thread you linked](http://www.reddit.com/r/haskell/comments/2bpsh7/a_simple_monadic_stream_library/cj7sqtw) I proposed that `MonadPlus` should be the lingua franca of `ListT` apis.
‡≤†_‡≤† ^you ^*would* ^think ^that
Yes, that is the tricky part. I did a very crude mapping of PC to function (for ghc) by finding the closest preceding symbol that made sense. This helped me identify some hot spots. Just having some location info with every symbol in the object code would go a long way. You can maintain locations that don't interfere with optimizations, but of course, the more optimizations you do the less reliable they get. 
Nit: Linearity wasn't actually shown in the beginning. You also need to show d_q(a*f(x))/d_q(x) = a*d_q(f(x))/d_q(x) For all a real numbers. This is easy to show -- write out the definition and pull out the a. Sorry about formatting, doing this from my phone.
Derp, that was it. Thanks!
Makes sense to me. I seem to recall this getting changed in GHC at some point after some folks complained about not being able to use japanese identifiers, but that is only a vague recollection. [Edit: Found it! https://ghc.haskell.org/trac/ghc/ticket/1103]
Technically it was a deliberate extension added in response to another feature request. It just never got propagated upstream to Haskell'.
I recommend just leaving it out. It's optional, after all. That being said, I solved the same issue a couple months back. The errors all arise from changes in the GHC API between releases. Using the GHC API docs from the [current](http://www.haskell.org/ghc/docs/latest/html/libraries/ghc/index.html) and [previous](http://www.haskell.org/ghc/docs/7.6.3/html/libraries/ghc-7.6.3/index.html) release for comparison, I replaced the erroneous code with whatever seemed appropriate. It was a terrible hack job. I'm not really knowledgeable about the GHC API, so I'm not really sure why it worked (or even *if* it worked). EDIT: [Here's](http://lpaste.net/109251) the diff.
I've sent an email to the Haskell' mailing list inquiring about making the change official.
There is a nix package for GHCJS somewhere, but I have to admit that I'm not familiar enough with nix to know where to find or how to install it. The authors of the nix package provided some useful feedback when I was improving `ghcjs-boot` two months ago, it makes far fewer assumptions about installation locations and program paths now.
It's not a complexity, but a feature. One can easily make specialized stream implementations by simply deriving the classes, the same way you do with `MonadReader` or `MonadState`. newtype MySpecializedStream m a = MySpecializedStream (ListT m a) deriving (ListTrans, ListMonad, Monad, MonadPlus) 
(author here) There was indeed an editing problem and quite some typos got in. This has been corrected and an updated version is available from the Apress site.
Awesome :D I looked up [ghcjs on nixpkgs](https://github.com/NixOS/nixpkgs/search?q=ghcjs&amp;ref=cmdform) but could only find a reference to `ghcjsBase`, not a formula for it. I am a nix noob though so will ask on the #nixos irc channel and report back.
Looking at the state of PR and issues hdevtools looks like it needs some maintenance love. Anyhow it does not compile on 7.8 so if you have installed the latest haskell-platform on windows you might be out of luck. See this issue https://github.com/bitc/hdevtools/issues/24
&gt; Unicode; the land where comparing after calling toUpper is different from case insensitive comparison. Blame people, Unicode merely tries to encode the mess of human languages. &gt; On a related note I wonder if this is deficiency in the Haskell report, it seems that allowing non-upper non-lower characters into the non-first position would be beneficial. Unicode provides the ID_Continue property, so the report could just use that. 
APL to the rescue! Something like APL squish quad `‚å∑` could be considered "upper case" and thus serve as a type and constructor name starter for caseless alphabets.
Excellent!
&gt; As such it's a nice view of what is being used on "modern" haskell. &gt; The big problem is that beginners are the one that suffer the most on errors, and the book is full of them. I'd also note that within its space -- viz, books that try to teach modern Haskell, currently popular libraries, etc -- it doesn't have a lot of competition. Arguably its only real competitor is Real World Haskell, which if anything is far more error-prone simply by being that out of date.
Hint: apt-get install fonts-knda to see it in all its glory
He did [another talk about vinyl at BayHack a couple months ago](http://vimeo.com/95694918), and [the slides for that are online](https://github.com/VinylRecords/BayHac2014-Talk/blob/master/Talk.pdf).
What about those of us that purchased the Kindle version from Amazon?
It's a funny kind of letter, because it's neither upper nor lower case. 
[Building a link shortened with Haskell and Snap](http://vimeo.com/m/59109358) - a nice video from NY Haskell group
Honestly, the spec is all fucked up too (yeah, Unicode is hard etc eÕ§tÕ≠cÕ®). √© is valid right now, xÃÅ is not.
&gt; acme-missiles library: Cause serious international side effects
&gt; [Don't use if your code is not pointless enough.](https://hackage.haskell.org/package/acme-pointful-numbers-0.1.2.4/docs/Acme-Pointful.html)
Maybe is the action on objects and fmap the action on morphisms. The source and target categories are both Haskell (Types as objects; functions between them as morphisms).
Wow, didn't know that and never heard anything from Apress. (I've been checking its errata page once per month only to see ‚ÄúNo errata are currently published‚Äù.) Where do I find the updated version? When I go to ‚ÄúYour eBooks‚Äù I see it listed with the date 2/2/14 which is when I bought it. *edit:* I downloaded it again and it does indeed seem to an updated version ‚Äì I really wish Apress' routines were better, though.
&gt; (author here) Sorry if my posts sound too negative. One can really feel that you put a lot of effort and passion into the book, and that passion keeps me reading. The introduction to Lens is great! However, it feels like there's a little bit of polish missing, especially those type errors should have been caught. Was there a deadline or something similar? Was there some kind of internal review process before publishing?
I purchased the book shortly after it was published. The good part is that it is up to date and covers topics such as cabal sandboxes or the lens library. For me the major letdown however was the fact that there is no project going on at all. It seems almost like someone suggested to include a project when the book was pretty much done so that they could put in a catchy subtitle. I am afraid that as it is the book is a bit of a missed opportunity. It would have needed at least one extra round of editing and proof reading.
Carter (cartazio) is working on a numerical computing library but I don't think Haskell has an equivalent for Numpy. You do have the **statistics** library, which is great and I use it often but the tools for matrix manipulation just aren't has mature I think (someone please correct me if I'm wrong). Pandas is just a user-friendly interface on-top of Numpy and Scipy while providing a few extensions to the underlying data structures provided by numpy and some "baked in" statistical functions. I use Pandas primarily for Time Series manipulation and depending on where Carter's numerical computing library is I might build a similar time-series manipulation library on-top of that. There's exciting stuff coming for Haskell in this world but it's trailing some other languages a bit.
I have merged some of the pull requests on hdevtools into my own [repository](https://github.com/merijn/hdevtools) and so far it works for me with 7.8.3 and the new platform. Several others have done similar things, I don't really have any plans to take over maintenance on hackage (I have no clue whether bitc is still working on it). Feel free to clone from github and build locally, though.
I'm a little wary of duck / structural typing. Developers, myself included, tend to use short, somewhat ambiguous, labels for field names like "id", "name", and "count". So, I see structural types like `{ name : String }` and `{ count : Int }` being abused. That said, these slides have convinced me that *something* like this would be awesome to have. There's been plenty times where I wanted to have `Maybe` or some parser functor etc. across my (lazy or strict) record type, for one reason or another.
Thanks for the tip on Carter's library, I'll keep an eye on that. &gt; Pandas is just a user-friendly interface on-top of Numpy and Scipy while providing a few extensions to the underlying data structures provided by numpy and some "baked in" statistical functions. Right, and it seems like Haskell would be equally if not more capable of achieving a similar goal on top of BLAS or whatever :) By the way, [hmatrix](http://dis.um.es/~alberto/hmatrix/hmatrix.html) seems promising.
From my (very limited) experience with pandas, DataFrames are, roughly speaking, lists of records. And you can drop, slice and combine columns very easily. This would be difficult to do with Haskell records in a type-safe manner... Maybe something like Vinyl could help? 
`Hask` is the category of Haskell types (objects) and functions between them (morphisms). The type constructor `Maybe` and the corresponding implementation of `fmap` form a functor from `Hask` to `Hask`. The former maps each type `a` to `Maybe a`, the latter maps `f : a ‚Üí b` to `fmap f : Maybe a ‚Üí Maybe b`, with the functor laws holding.
I can post the slides soon! There were a few errors in the ones at the talk that I would like to correct, but I'll do my best to have them out shortly. EDIT: Here are the slides on Slideshare (http://www.slideshare.net/jonsterling/galois-tech-talk-vinyl-records-in-haskell-and-type-theory), and the source to the slides is here: https://github.com/VinylRecords/Galois-Tech-Talk.
I pretty much agree with everything you say. &gt; The big problem is that beginners are the one that suffer the most on errors, and the book is full of them. I think most of the errors are spelling/grammar, and while I find them cosmetically annoying, I don't think they've ever hindered my learning. There have been 1-2 code errors that gave me pause and made me take some time to figure out what was wrong; the other code errors have been too minor to matter to me. Overall I don't think the errors have substantially detracted from me learning Haskell, so I still recommend the book. As /u/cunningjames says it's the current best in class - teaching modern Haskell with useful, popular modern libraries.
Like many gaps in the Haskell ecosystem, building a simple matrix library is not technically that hard it's just a matter of having the right incentive structure in place to get the library built. A lot of Haskell library development is motivated by academic or hobbyist work so it tends to incentivize interesting novel technical approaches to problems, and not so much boring engineering and polishing work. So we end up with a lot of undocumented partial prototypes exploring the design space of things like typed-dimensionality or optimization but not a whole lot of robust solutions that just solve the simple case. But when Haskell libraries do come to fruition they tend to be the 'the right solution' and much higher quality. Python is sort of the "dual" philosophy to Haskell, and both approaches have their merits. Don't know enough about Carter's library to comment deeply, but from some googling it seems like he's trying to explore a much much larger design space than a simple library like NumPy which is just a simple dense matrix, a bunch of loop operations, and bindings to a subset of BLAS. 
In what way is the typeclass questionable? * the "runList" action is just like your "uncons", but with the dedicated "ListItem" sum type rather than "Maybe" which imho is nicer * "cons" is just like your "cons", but includes Tekmo's suggested default implementation based on MonadPlus and has the comment explaining that it exists in the class for performance as you've mentioned below. * "joinL", which your package lacks, is necessary to implement many list operations, for example "tail". While your tail is of type "(Monad m, ListTrans t) =&gt; t m a -&gt; m (Maybe (t m a))", List's tail is the much more ordinary "List l =&gt; l a -&gt; l a" and can replace Prelude's "tail" for ordinary lists while working just the same for monadic lists. Btw - which instances do you feel are missing? I'll gladly add them to "List". Cheers
Seems to have been fixed
Imagine that you are manipulating a list of records in ghci. You want to drop one of the columns and combine two other columns into a new one. All of this without having to explicitly define a type for the new record. How to do that? Haskell nominal typing of records makes it difficult. Some kind of structural typing / row polymorhphism would make it easier. For example, you could have a generic function that adds a column to any record, or drops an existing column. Kinda like type-changing assingment, but where the type change involves adding/removing columns.
&gt; we are usually only really interested in "endofunctors" We does not include /u/edwardkmett: non-Endo-[Functor](http://hackage.haskell.org/package/categories-1.0.6/docs/Control-Categorical-Functor.html).
totally doable, its just the tooling isn't there yet 
This has always been my impression. Either that or you have to go stringly-typed and let everything be maps. I spent a little time trying to copy Vinyl to embed the relational logic into Haskell at some point. Later I reflected it'd probably be a better place for pandas :s
Why?
Even there there is a certain injectivity on objects. Sjoerd Visscher is the one person I know who managed to get a fully general Functor in in Haskell. There is a slightly more general notion in http://github.com/ekmett/hask but it has the same shape issue.
The "fields wrapped in functors" thing was very cool.
‚òû‚òÄ‚òú :: String -&gt;a
Of course one can do this sort of thing in Haskell, especially with all the type-level programming available in 7.8. It's just that a dataframe is a very dynamic heterogeneous structure by design, so it tends to take more work to model in a static type system. Adding/removing heterogenous columns could be done with a HList/Vinyl like structure but then inference tends to break down and becomes difficult to use interactively inside of GHCi. The strength of something like pandas is that you don't have to worry about the type or shape of data at all, it automatically aligns and casts as needed using Python's fast-and-loose everything-at-runtime approach. How to replicate that experience in Haskell is an open question.
yup, I've a strictly grander goals than "just wrap up blas and do dense arrays only". trying to focus on release engineering right now :) I've put ~ 2.5 years of thought into the basic design, and i've been iterating on the implementation details for 1.5 years as is :) Every extant numerical computing / data analysis tool chain has a strong and needless forced dichotomy between library provided routines (batteries) and what people can easily do in userland without breaking out C. (even ignoring issues of intelligibility of performance tuned code in many of these settings). I want tools that are about ease of battery manufacture, not "how many batteries for things i want are prebuilt". Because I'd rather be able to easily (and quickly) implement performant (and intelligible!) algorithmic math than play the "did someone write the exact procedure I need in enough generality that i can use it for my problem while having good code quality and ease of install". I want tools where you can easily reflect all your problem specifici structure into your algorithm when you really care about performance and precision that more generic solutions (that will be on hand) can't provide. I want to be able to add new array formats (eg what if i want sparse symmetric k banded matrices?) easily in userland, and have all my generic codes work correctly on them out of the box! I want the abstractions of my libraries to give a shared vocab for not just the mathematical structure, but for all the folk lore performance tricks to also become more unstandable by dint of that shared vocab! I just want to write algorithmic math, have it be high level, extensible, and fast. And I want tools that I'd still happily use in a decade. will share more once I cut an alpha (which will only be suitable for expert haskellers), though documentation (outside of my huge 1315 lines of comments for currently 2386 lines of code) wont really happen till the beta (whose release should be a bit more wider audience of usability) Turns out that for mathematical array computation, generality vs performance aint a trade off, its a synergistic super hero duo that mutallly reinforces one another! 
I'm all for changing the report. 
if you wanna have a db in memory (and have writes be relatively cheap, vs a full on OLAPy build once, read heavy workload) tables is a neat package http://hackage.haskell.org/package/tables proper Time series stuff is slightly different from an array layer, and that its at all pseudoworkable on top of numpy arrays requires a lot of secondary structures and machinery. A (simplified) analogue of what time series analogue of my array api would look like in the pure case might be something like the following (bear in mind i'm still mulling how to do this nicely and this is just an off the cuff sketch) class (Ord (Key table)) =&gt; OrderedKV table where type Key table :: * type Value table ::* type Address table :: * keyRange :: table -&gt; (Key table, Key table) key2Address :: table -&gt; (Key table) -&gt; Maybe (Address table) address2Key :: table -&gt; (Address table) -&gt; Key Table nextAddress :: table -&gt; Address table -&gt; Maybe (Address table) nextKey :: table -&gt; Key table -&gt; Maybe (Address table) -&gt;Maybe (Key table, Address table ) sliceTable :: table -&gt; (Address table, Address table) -&gt; Maybe table readTable :: table -&gt; Address table -&gt; Value Table this is (roughly) all you need for having a pretty generic time series structure (that works on various time scales/types), at least ignoring how you build up the structure to begin with. I'm still playing around with the right details for a time series data structure, but i'll probably add something roughly like this after my alpha release. 
Just a joke.
I only knew the answer because it caught me out too.
Oh, interesting. I saw that their purposes overlap but figured if both were listed, there must be some benefit to using both. If not, yeah, hell with it.
This is exactly the task I was too daunted to tackle, thanks! Line numbers didn't match up to the diff which is yet ANOTHER layer of uneasiness, but hey, it compiles and everything seems happy. :)
In programming language theory. That's not what that is. In theory, types are Scott-domains and functors map Scott-domains to Scott-domains and continuous functions to continuous functions.
Yes. The new Haskell Platform build system is written in Haskell. It must be compiled by an existing Haskell development environment in order to build the new one. While it significantly complicates the process on Linux, (I guess that) it simplifies the process of creating Windows and OS X builds. With the new Haskell Platform release, my development environment is even further from standard, but here is how I do it in case you are interested: http://www.extellisys.com/articles/haskell-on-debian-wheezy
Once you think about it, having cases is pretty weird ‚Äî or at least how they're used in English. Why have two symbols for each letter, and then rarely use this to your advantage? All of these sentences only use a capital at the beginning, adding no extra info at all ‚Ä¶
I wish I knew about tables earlier...lol. Thanks cartazio :)
I think Haskell (in a few years, something like Idris, more so) is *much better* suited for numerical computing. Like another commenter said, Haskell's ecosystem is slowly but surely gaining momentum.
Putting aside Haskell matters, ‚Äòfunny‚Äô is not the word I would use. Writing systems without case aren‚Äôt rare. Take the following scripts for instance: - Arabic (abjad) - Hangul (featural alphabet) - Hebrew (abjad) - Tamil (abugida) - Thai (abugida) They all share the same features of: a) being alphabetical in the wider sense b) being in wide use today c) having no notion of case. As best as I can tell, Unicode classifies a code point as lower- or upper-case when it‚Äôs part of a writing system that has a notion of casing, but not otherwise.
I love how this thread went from ‚ÄúThat‚Äôs cool‚Äù, to ‚ÄúWhy is that legal?‚Äù, to ‚ÄúWhy does English even have uppercase letters?‚Äù
Maybe related to bidirectional programming ? http://stackoverflow.com/questions/13404208/in-pure-functional-languages-is-there-an-algorithm-to-get-the-inverse-function/13404681#13404681
A few years ago, I wrote a basic FFI to a sparse matrix solver. It worked well for me at the time. I haven't used it since so it has probably bit-rotted. It's [here](https://github.com/tdox/hcholmod) in my GitHub repository.
&gt; In category theory isn't F a single object rather than two? If ¬∑ denotes the pair-forming operator then we can say a pair f ¬∑ g (where f is a function on types and g a function on morphisms) is a functor if it satisfies so and so. In that way it is "a single object". If we *do not* have the pair-forming operator then a functor is not a mathematical object but some construction we are describing in the surrounding text. Namely a symbol which is overloaded to doubly denote: a function on morphisms and a function on types. Or, you could just say a functor is a function from (the union of types and morphisms) to (the union of types and morphisms) and which maps types only to types and morphisms only to morphisms. And in that way it is again a mathematical object. In the end it doesn't matter which way you do it because you get the same result. If you consider functors as pairs of functions then the pair Maybe ¬∑ fmap is a functor. PS I had to write "types" which are the interpretation we have in mind for the objects in a category because of your use of the word "object" to refer to a mathematical element. Hope it doesn't cause confusion.
I just realized why you might be further confused as well. In Haskell you declare a type transformer to be of class Functor by giving a function on functions, a high-order function, that satisfies, together with the type transformer, the postulates of what is called a functor in the theoretical sense. So that's a different although very closely related use of the same word.
not sure I can follow that sketch of a neat idea. care to flesh it out slightly? in particular, what do you mean by "data"? also, as a general proposition, the most general setting i know of for this stuff (not that i claim to know the setting at all well) is deformation theory: http://ncatlab.org/nlab/show/deformation+theory
Very clever. Especially the closing credits. 
I've been using ghc-events-analyze, and I like it quite a lot. It's better than grepping through the eventlog, which I was doing before ghc-events-analyze came out. The one real drawback is that ghc-events and ghc-events-analyze themselves need to be optimized. I regularly generate eventlogs that are about 500mb, and ghc-events-analyze needs a ton of RAM (8-12GB usually) to process them. And I know there's a big thunk leak somewhere because I hit the stack space limit (ghc-7.6). It probably wouldn't take too much work to fix this, but it hasn't been enough of a bother yet for me to want to invest the time.
After the 3rd or 4th time you've wasted hours trying to optimize a bottleneck, only to discover you were working on entirely the wrong function, you may be convinced. I have encountered this exact phenomena far too often for me to start by annotating everything.
No, capitals are also used to mark proper nouns too. Personally I prefer the German way of capitalizing the first letter of every noun.
W00t! Finally my mother tongue is in the Haskell subreddit! :-) Yes! 
I think this is likely to interest at least some parts of the Haskell community, and a Haskell tool (QuickCheck) plays a large role. However, I'm not sure we mention the word "Haskell" even once, so please let me know if I shouldn't post this here.
&gt; The Applicative constraint in your quote is unfortunately just plain wrong. Well, that's why I've chosen this quote :P. It's an example of one of the constraint errors in the book. Stuff like this _will_ confuse beginners and shouldn't pass review. Heck, my post shows that there are _different_ constraints on `&lt;$&gt;` shown in the book. After reading RWH, LYAH, PCPH and the Yesod book, I concur with /u/ReinH and wouldn't recommend BH:APBA, at least the first edition.
Haha! Hello! 
The scripts that have uppercase and lowercase are called bicameral scripts and the ones without (like Kannada above) are called unicameral scripts. One of the reasonings for the existence of the 2 types of scripts might have to do with writing on papyrus and other leaf based manuscripts (papyrus, palm leaf manuscripts etc.,). If one writes letters that are horizontal and vertical on a palm leaf then there are chances of the leaf breaking at those points. To get around this the South Indian writing systems evolved alphabets with very little number of vertical and horizontal lines. Here is something called a [grantha script](http://en.wikipedia.org/wiki/Tigalari_alphabet) an alphabet system which was uniquely used to write on palm leaf manuscripts. Also these kind of writing systems were faster to write using than ones with lots of horizontal and vertical lines like the most ancient [brahmi script](http://en.wikipedia.org/wiki/Brahmi_script#mediaviewer/File:Schrift_brahmi_tabelle.gif). I am suspicious that this might be also the reason to evolve an upper case and lower case systems 1. for faster writing and 2. to reduce the amount of page tearing. Sorry for going so offtopic. Back to our regular Haskell programming. 
Haven't listened yet, but looking forward to hearing the first episode. Excited to see what future episodes will cover! :-)
&gt; For building editable GUIs, we generally want some notion of a path, a function for looking up the value at a path, editing a value at that path, and a way of resolving screen positions to paths, say. But that's a zipper! A zipper isn't an "imperative API" for traversing a data structure, but rather a _reification of a context_. The _context_ of a subpart of a data structure is effectively a path to that subpart. Zippers are powerful because they allow you to do things like traverse an AST or other structure and perform modifications and optimizations by pattern-matching on the context surrounding that structure. For example: a while back (for pedagogical reasons) I wrote an implementation of splay trees using zippers. The implementation was predictably not as fast as a C version implemented via pointer updates; however, the code was, to my reading, comparatively clear about what was happening, as the logic of a splay tree is, "Traverse the tree until you find the node you're looking for, and then pull it back up through a series of pointer swaps." The zipper merely reifies the traversal as a data structure, and then you rebuild and modify the tree as you traverse the zipper. ([Code can be found here](http://lpaste.net/109288), although be warned it has some rough edges‚Äîhowever, I invite you to compare `splay` to a corresponding C implementation.) This is of course something of a toy example, but the principle scales to any problem that involves examining and manipulating contexts of data structures. EDIT: changed link from pastebin to lpaste‚ÄîI had no idea how bad pastebin's Haskell syntax highlighting was.
&gt; In what way is the typeclass questionable? In summary. The sole premise behind your API seems to be to target both `ListT` and `[]`. I highly doubt the usefulness of that and I see the burden on the API. E.g., `joinL` seems to exist only for that reason. The `ItemM` does not make things simpler and it renders your class underivable. &gt; the dedicated "ListItem" sum type rather than "Maybe" which imho is nicer This is clearly subjective. IMO, it's a needless complication. &gt; "joinL", which your package lacks, is necessary to implement many list operations, for example "tail". While your tail is of type "(Monad m, ListTrans t) =&gt; t m a -&gt; m (Maybe (t m a))", This is simply not true. `uncons` is enough to implement it, hence, I could have implemented this as a transformation in my package as well: tail' :: (ListTrans t, MonadPlus (t m), Monad m) =&gt; t m a -&gt; t m a tail' t = lift (uncons t) &gt;&gt;= maybe mzero snd However I deliberately didn't do so, because it's simply wrong. There is a difference between an empty tail and an absense of it (a tail of an empty list), which in your case is encoded as the same thing. The functionality of your `tail` is now achievable with `drop 1` in my package. &gt; Btw - which instances do you feel are missing? `MonadBase`, `MonadBaseControl`, `MFunctor`. --- Overall, having many disagreements with the design decisions in your package, I just felt it would be easier to roll another one, than to argue. So I honestly don't see much of a point in having a debate. And in the light of getting aggressively downvoted by some opinionated gentlemen in this thread I kinda lose the motivation.
The current plans are homotopy type theory and Idris for the next two, but let's see what happens.
In the case of pure `newtype`s it's free, but in the case of monad transformers our data is wrapped in a monad, which needs to be executed to unpack the data.
There isn't really a Haskell equivalent. For CSV I would use cassava (https://hackage.haskell.org/package/cassava). For an extended example of its use and some moderate sized data analysis including drawing maps (in Haskell) see here: http://idontgetoutmuch.wordpress.com/2013/10/23/parking-in-westminster-an-analysis-in-haskell/ (the map is right at the end BTW). For matrices you have hmatrix as has already been mentioned (now with type literals to check, at compile time, compatibility of matrix operations). At work, I use a package which allows me to quasi quote R, passing in Haskell data structures and receiving back Haskell data structures. So I have full use of data frames (not that I have felt any need for them) and pretty much all known statistical functions (e.g. I needed Nelder-Mead a few weeks ago). This will be open sourced "real soon now". Not much help if you are using Python rather than R for data analysis though.
Kannada letter ttha ("‡≤†") is in the Unicode category "letter, other". A Haskell varid has to start with something in the Unicode category "letter, lowercase", and thus I conclude that this is not legal Haskell. As /u/edwardkmett pointed out in another thread, GHC has allowed OtherLetter to count as LowercaseLetter since ~2008 or so, letting folks use Japanese words and the like for variables. The question in my mind is is whether GHC has become the de-facto standard here or whether this could be considered to be a bug in GHC. 
Bounded + Enumerable isn't quite Finite. At the very least, Finite is more inclusive. For instance, consider a semi-lattice without a unique bottom. Maybe minBound picks some least element and enumFrom rolls up the chain it's in‚ÄîFinite would be required to roll up *all* chains. Edit: oh, I wasn't thinking! This is an even better example if you have a full lattice! Bounded then has an obvious and correct implementation, but Enum either can't work or won't enumerate everything. Yet, the lattice is still easily Finite.
&gt; &gt; For building editable GUIs, we generally want some notion of a path, a function for looking up the value at a path, editing a value at that path, and a way of resolving screen positions to paths, say. &gt; &gt; But that's a zipper! A zipper isn't an "imperative API" for traversing a data structure, but rather a reification of a context. What you are describing sounds more like a lens to me. 
Seems strange that the module `Safe` should contain crashing functions.
This would be cool class Functor f where f :: (a -&gt; b) -&gt; f a -&gt; f b instance Functor Maybe where Maybe f (Just x) = Just (f x) Maybe _ Nothing = Nothing ghci&gt; Maybe (+1) (Just 5) Just 6
I concur. Won't these exact variants defeat the purpose of the library? 
And ancient Egyptians indeed didn't write out the vowels usually.
I've been doing some musical analysis, and I've found zippers fairly useful. I have a series of rules which get applied at every location with a musical score to try and analyse it for harmonic 'goodness' :-) For example, one rule might say 'warn if the harmonic interval between this note and the next is dissonant'. Another rule might need more context, and would need to look at the previous and following intervals as well (meaning it has to check four notes instead of two). Another rule says 'its an error to have the same note sounding concurrently in multiple voices, unless they actually duplicate each other for their entire duration, in which case its OK'. Zippers provide a very useful thing to pass into each rule, to say 'here is the location in the music I want you to examine, but you can use the Zipper to check out as much of the surrounding context as you need to'.
See also this http://timbaumann.info/posts/2013-08-04-hakyll-github-and-travis.html and this http://wesleyhales.com/blog/2013/03/29/Fun-with-Static-Site-Generators-and-Travis/
In the case of Latin, capital latters were the original: It evolved [from stone-chiselling over hand-writing on parchment/paper to print](http://upload.wikimedia.org/wikipedia/commons/thumb/c/c0/Evolution_of_minuscule.svg/1280px-Evolution_of_minuscule.svg.png). And capital latters are, indeed, very nice to chisel: If you're in a hurry (e.g. graffiti) you can make everything that's round angular, it can't change the meaning. With most of those that look lower-case, some form of upper case was used at least for the start of chapters and sentences, even if it was just "write them bigger". As such, minuscules are a readability and writability improvement over the original forms. The Cyrillic script not having proper minuscule forms (outside of cursive) a thing I never got over. Learned to read it fluently, still throws me off with it's complete small-caps look.
I don't see how you would ‚Äúgo left‚Äù or ‚Äúgo up‚Äù with the proposed solution
Note that `errors` re-exports only the total functions from the `safe` library
I've had that experience (not in Haskell; C). I still start with everything unless I already think I know where the bottleneck is, and sometimes I guess wrong and still have to profile the whole thing.
Manually unpack the sdl2 package somewhere and then `cabal configure -v3`, as suggested in the error message?
Lesson seems somewhat similar to the Fizz Buzz "pearl" in a recent monad reader. Functional Programmers: Don't forget you can use higher-order functions.
wicked!
Link? 
That sounds amazing. Thanks for doing this!
I'm not sure if we've banished HoTT from the subreddit, but last I checked Idris is still welcome here and I know I'd personally love to hear more Idris news. EDIT: Clearly, I'm mistaken somehow. I've getting quite downvoted, but I'm not quite sure why.
I'm not an expert, but I think you're right. I remember reading somewhere that lenses sort of evolved from the idea of a zipper ([perhaps it was this](https://www.fpcomplete.com/user/psygnisfive/from-zipper-to-lens)).
Very nice! Keep them coming!
I'm convinced the algebraic equations that define mutually recursive types can be seen as varieties (in the sense of algebraic geometry) and that we can then mimic the construction of [derivations](https://en.wikipedia.org/wiki/Derivation_(abstract_algebra)) and then, maybe, [K√§hler differentials](https://en.wikipedia.org/wiki/K√§hler_differential).
Are you in the right thread? I didn't see any "lesson" here.
See the third paragraph onwards, where I abstract over splitAtExact_ to produce takeExact etc.
As someone who has attempted to build a text editor atop zippers, I agree entirely. They lead to some very interesting puzzles, for example, how do you represent a range selection? How about a second-order zipper! http://blog.poucet.org/2007/07/higher-order-zippers/ What if you've got a zipper of paragraphs atop a zipper of characters, for more efficient rezip / paragraph-level operations? How do we combine such a multi-level zipper with second-order zippers? Very curious indeed. Oh no, now you want multiple selections? What now!? In the case that your application will only ever need the capabilities of a zipper, then things can work out very elegantly. However, as soon as you need a more complicated context in the datatype, things get tricky. Certainly not impossible, but now you're spending days tinkering with creating new datatypes that fit your set of desired operations, rather than getting things done. So, zippers are very cool, and have their uses, but I also wouldn't go around recommending them for most pragmatic programs, as they are likely to outgrow the representation.
I know lenses and zippers are different. However I'm talking about this &gt; we generally want some notion of a path, a function for looking up the value at a path, editing a value at that path, and a way of resolving screen positions to paths Now "a function for looking up the value at a path, editing a value at that path" is exactly what (a map from path to) a lens gives you. (On the other hand "notion of a path" and "way of resolving screen positions to paths" have nothing to do with zippers.)
I recently wrote a small programming language that uses a zipper for type inference: http://5outh.github.io/posts/2014-07-21-molecule.html I can't really say that it's the best solution to the problem of *type inference* (it isn't) but it was a neat experiment and did work pretty well. I also think that zippers have an obvious application in self-balancing trees, e.g http://en.wikipedia.org/wiki/Red‚Äìblack_tree, where you need to keep track of how to order your tree based on the structure above you. Zippers aren't always the right solution to the problem, but I think they do make sense a lot of the time. Alas, I agree with techtangents in the comments on the page -- it would be nice to have a concrete example comparing/contrasting ways of going about traversal. The quote /u/kvensiq pointed out also rung a bell for me, because AFAIK what is being described is exactly a zipper. :)
Ah. Sorry. I misunderstood your disagreement.
In the particular case of functions on natural numbers we can compute the inverse of a function as follows (I am pretending that `Integer` is the type of natural numbers): inv :: (Integer -&gt; Integer) -&gt; (Integer -&gt; Integer) inv f n = find 0 where find k = if f k == n then k else find (k + 1) That is, `inv f n` searches for the first `k` such that `f k == n`. We need to think a bit about what happens if `f` is a bijection which maps ‚ä• (the undefined value) to something other than ‚ä• (hint: it ain't a bijection). In general this may not be possible -- we would have to impose requirements on the domain and codomain of the function.
Hmm well it is indeed a nice use of generalisation but I don't see how it's similar to the Fizz Buzz pearl. http://themonadreader.files.wordpress.com/2014/04/fizzbuzz.pdf
If you use bijective functions as your building blocks, sure: data Bijection a b = Bijection { to :: a -&gt; b, fro :: b -&gt; a } instance Category Bijection where id = Bijection id id (Bijection f finv) . (Bijection g ginv) = Bijection (f.g) (ginv.finv) -- building blocks inc :: Num a =&gt; Bijection a a inc = Bijection (+1) (subtract 1) toString :: (Show a, Read a) =&gt; Bijection a String toString = Bijection show read -- let you make stuff like addThreeAndShow :: (Num a, Show a, Read a) =&gt; Bijection a String addThreeAndShow = toString . inc . inc . inc and just to show it works: ghci&gt; to addThreeAndShow 4 "7" ghci&gt; fro addThreeAndShow "8" 5 
Keep them coming!
Ah, I can only comment on where the lesson was, not how it's similar to fizzbuzz. Thinking more, you probably could come up with an abstract stack machine, generalise in some way, and eventually come up with the same generalisation - but I just thought a bit and came straight to the result.
Aren't zippers comonads (extract = get zipper focus, extend = go left/go right)? Then there should be a whole class of problems that comonads are good at solving, that should apply to zippers as well. Perhaps the problem is trying to choose too big of a problem for a zipper to solve; maybe there are more "internal" uses of zippers than have comonadic semantics i.e. configuration builders, command builders, iterators. are we saying that we don't like comonads, or are zippers just a version of comonad that isn't as useful as in the general case?
Crashing functions can sometimes be useful to implement safe abstractions on top of. The biggest problem is that they are a default in Haskell. We are trying to start a convention that partial functions be suffixed with "Ex" (headEx, etc), where Ex stands for exception. http://hackage.haskell.org/package/mono-traversable-0.6.1/docs/Data-Sequences.html#v:tailEx In classy-prelude, head only operates on non-empty structures. Safe is a great start, but marking non-empty structures is a much better way of doing things once you get used to it. http://hackage.haskell.org/package/mono-traversable-0.6.1/docs/Data-MinLen.html#v:head
Building SDL on Windows requires a few modifications to the cabal file. * Download the mingw variant of the SDL 2.0.3 development libraries, and extract them into a path such as "c:\sdl-2.0.3". * Extract the "sdl2" haskell package with `cabal unpack sdl2`, and edit the sdl2.cabal file. * Delete the "pkgconfig-depends" section. * Add "SDL2main" to the "extra-libraries" section *before* "SDL2". * Add "c:\\\\sdl-2.0.3\\\\include" to the "include-dirs" section. Double backslash characters are intentional. * Add "c:\\\\sdl-2.0.3\\\\lib\\\\x86" to the "extra-lib-dirs" section. Adjust this path if you are using 64-bit ghc. 
&gt; In summary. The sole premise behind your API seems to be to target both ListT and []. I highly doubt the usefulness of that and I see the burden on the API. Correct summary. I think it's good for reuse and for API familiarity to use the same API for ListT and []. Less of a cognitive load on the user if it is the same API. &gt; The ItemM does not make things simpler and it renders your class underivable. What do you mean by underivable? &gt; However I deliberately didn't do so, because it's simply wrong. There is a difference between an empty tail and an absense of it (a tail of an empty list), which in your case is encoded as the same thing. The functionality of your tail is now achievable with drop 1 in my package. My "tail" is compatible with Prelude's "tail". Your "tail" is more equivalent to [safe](https://hackage.haskell.org/package/safe)'s "tailMay", which is definitely a fine and useful function, but List's API is deliberately compatible with Prelude. "drop 1" isn't the same as it doesn't cause an error for empty lists. &gt; &gt; Btw - which instances do you feel are missing? &gt; &gt; MonadBase, MonadBaseControl, MFunctor. I'm unfamiliar with these. I tried to keep dependencies low for List. If there's a need for those I'll gladly add them in either in List or a separate package.
ekmett added a zipper component to `lens` based off of http://hackage.haskell.org/package/zippo, but I'm not sure where it lives now. I also wrote another lens-based zipper library here http://hackage.haskell.org/package/pez. I don't maintain either anymore, but looking at the zippo source might be interesting.
&gt; there is a small point it wants to convey: **Functional programmers! Remember higher-order functions!** (from the fizzbuzz PDF) While the pearl goes through several other steps to motivate things, at the end you basically get a higher-order function at the core. It is a small lesson; but very similar to the generalization in the article.
http://themonadreader.files.wordpress.com/2014/04/fizzbuzz.pdf if you haven't already seen the link from /u/tomejaguar .
I took those articles as a starting point and modified them to rely more on modern cabal. In particular I use a cached cabal sandbox to bring the Travis deploy time from ~15 minutes to under one minute.
Thanks Rein! (It's been submitted to iTunes, just waiting for it to pop up)
Yes, it was my point to show that a function like `inv` cannot exist. Please show me how you would define `inv` so that it works for all invertible functions (and gives garbage for the others).
Just because zippers are comonads doesn't mean that all comonads are zippers; I don't think you can extend the logic that way (i.e. just because `Identity` isn't super useful doesn't mean Monads in general aren't), but I agree with everything after your first sentence for the most part. Zippers are totally useful in many areas; they aren't the best solution to *every* problem but I don't think anyone was asking them to be and the author's reasoning isn't very clear (possibly even a little bit wrong).
&gt; There are a lot of things that can be done. But just because something can be done doesn‚Äôt mean it should be done. This all sounds obvious, but our industry is full of people wasting time solving hard problems created artificially, which could be solved trivially or sidestepped entirely just by revisiting earlier assumptions. Hard problems created artificially? You mean like "how do I do side-effects in a language where nothing changes?" The research should be done. And people need to play and experiment with new techniques to see if they are useful. To argue they shouldn't is stifling, unimaginative, and harmful in the longer term. Also, we're going to have to dock points for the title not being "Zippers Considered Harmful."
There is a loose relationship between zippers, comonads, lenses, and delimited continuations. A zipper is a delimited continuation that has been reified as a data structure, [as Oleg explains](http://www.haskell.org/pipermail/haskell/2005-April/015769.html), and [every zipper is a comonad](http://cs.ioc.ee/~tarmo/tsem05/uustalu0812-slides.pdf). Lenses can be represented as the Store comonad, [which is related to the van Laarhoven representation used in `lens`](http://bartoszmilewski.com/2013/10/08/lenses-stores-and-yoneda/). Both are comonadic, and comonads have something general to do with [computation based on contexts](http://blog.sigfpe.com/2006/12/evaluating-cellular-automata-is.html). Practically, zippers and lenses both allow you to do simple updates of deeply nested structures. Zippers give you very efficient properties when you are updating several times within a given context, as they can maximize sharing of other parts of the structure, which is something lenses don't give you for free. Lenses give you lots of combinators for prodding, manipulating, extracting from, and destructing complicated nested structures in a compositional way, while composition of zippers is unwieldy and difficult. In the "editable GUI" example, I can imagine either (or [both simultaneously](http://hackage.haskell.org/package/zippers-0.2/docs/Control-Zipper.html)!) being used to good effect.
It currently lives at http://hackage.haskell.org/package/zippers
If you don't have the ability to move the focus around in relative terms, you really are working with lenses instead of zippers. It might be worth considering your API from that viewpoint to see if it simplifies anything.
 (+1) &lt;$&gt; Just 5 Why do you want to be tied to `Maybe`?
Science is full of complicated and difficult artificially-created problems. If people talk about them long enough they gain respectability even though they shouldn't have had it to begin with. Sometimes it's just hard to see the forest through the trees and people lose sight of what's important. Doesn't happen as much in computer science as it does in modern mathematics, though, thankfully.
I can't disagree with "the research should be done", but not every Haskell program should be a research project. I once saw someone give "some kind of 2D zipper" as the answer to "how do I represent a character that can move around a rectangular array of grid cells?" when it was clear that neither party involved had any idea what such a thing could look like. Well okay, if you want to figure out some way to adapt zippers to your particular problem and save a logarithmic factor, that sounds like a fun research project and maybe you can write a functional pearl about it. In the meantime, there's `Map (Int,Int) a`. In general (and I think the author alludes to this) people tend to suggest things based on how cool or trendy rather than on their technical merits or how applicable they are to the question under discussion. "`vector` is fast because it has fusion" is basically our "MongoDB is web scale". FRP is something that not many people actually use yet but that gets recommend often because, well, it's FRP. I could go on but I don't really want to start a discussion of the libraries that everyone thinks are the most overrated, so I'll stop there. I realize this general phenomenon is not going to go away, but it helps to be aware of it.
&gt; Doesn't happen as much in computer science as it does in modern mathematics, though, thankfully. Hah! Are you telling me completed enriched noetherean higher multicategories on flat sheaf-modules aren't the end-all-be-all of algebraic homopology? ;)
&gt; What do you mean by underivable? With "list-t" you can do the following: newtype T e m r = T (ReaderT e (ListT m) r) deriving (ListMonad, MonadPlus, Monad) With "List" you can't. The reason is the utilization of an associated type family. &gt; My "tail" is compatible with Prelude's "tail". Your "tail" is more equivalent to safe's "tailMay", which is definitely a fine and useful function, but List's API is deliberately compatible with Prelude. "drop 1" isn't the same as it doesn't cause an error for empty lists. Oh. Well, then it's another design decision I'd disagree with, since I'm one of those who strongly oppose APIs with partial functions. IOW, IMO, the `head` and `tail` functions of the `Prelude` are a mistake and should be like their "safe" counterparts instead. Hence I intentionally didn't follow that design decision. So I think it's good that we just have two libraries going alternative paths.
Someone else commented something to this effect on the post, and [I replied](http://pchiusano.io/2014-08-12/zippers-not-useful.html#comment-1541183526) that I consider the term "zipper" to refer to something much more narrow. I'd be curious to know if other/most Haskell programmers have a more general (well, IMO I would say "diluted") definition for the term "zipper" than what I am talking about.
Is there a transcript available?
as a teaser, heres the haddocks from a build earlier today http://bit.ly/prealphadocs theres a bit more I need to add mind you :) 
Thanks for the link - that was a good video. I need to loop back and try Snap again now that I grok more of haskell.
I agree with this comment totally; I read the article as "don't use zippers because you can always do things better in other ways," but I think the point was supposed to be "don't use zippers if there are better, simpler approaches to the problem at hand."
Your name is somewhat misleading: this should actually work for (some) infinite types too, as long as the law still holds(all elements are reachable in finite time), and you change the pattern matching . It should be possible to extend it to Finite a =&gt; Finite [a] for instance. It would get arbitrarily slow, but that's inevitable I guess. A perhaps bigger concern is that it would give inverses for surjective but non-injective functions: but I would say that's a feature :&gt;
Slides here: http://conal.net/talks/bayhac-2014.pdf
&gt; You can view Int as the set Z ‚à™ {‚ä•} where ‚ä• is the value that denotes a looping computation. We can view Haskell functions of type Int -&gt; Int as mappings from Z ‚à™ {‚ä•} to Z ‚à™ {‚ä•} But this is wrong, as there is one extra condition: functions must be monotonic on definedness. Even more than that: the only funcions that don't map bottom to bottom are the constant ones, which are clearly not bijective. That means that if we don't care about what happens when non-bijective functions are given, we can fulfill the assignment: since all relevant functions map bottom to bottom, we can act as if the funcions were Z -&gt; Z, and implement `inv` by searching Z.
I answered that question in my other reply. I'm on mobile, so sorry if I fucked up the formatting.
To tell the truth I don't have that strong an opinion about the particular instance of zippers either. When I initially read the article I saw the juxtaposition of "some unsuspecting newcomer to FP gets pointed to zippers" with "just because something can be done doesn‚Äôt mean it should be done" and thought it was about not using the "cool" tool when there is a more practical, simpler alternative. (Which does seem to be the conclusion of the article.) But now I see that section can also rather directly be read as "we shouldn't even study things like zippers", so I see where your original post is coming from now.
&gt; Crashing functions can sometimes be useful to implement safe abstractions on top of. Isn't it more natural to implement unsafe operations on top of the safe ones? &gt; Safe is a great start, but marking non-empty structures is a much better way of doing things once you get used to it. Ok, NonEmpty works for head/tail/init/last. What about applying tail twice? What about `(!!)`?
Unfortunately, we don't have a transcript. If somebody would like to make one, I'll post it to the site. 
Thank you!
Thanks!
Thanks for listening!
I think that more than one more round of proof reading/editing was needed. I have picked up and submitted over 20 errors just from the first 1/2 of the book without any from the code.
I had to patch SDL_platform.h in order to get it working on my system. You may find [this](http://stackoverflow.com/a/22600114/349384) useful. Be careful, as the developer distribution comes with three different include directories (why?), and you need to make sure you patch the correct one and/or all of them. If it helps, this was my installation procedure (requires pkg-config): 1. Unpack the SDL development libraries somewhere. 2. Patch C:\path\to\sdl2\x86_64-w64-mingw32\include\SDL\SDL_platform.h 3. set PKG_CONFIG_PATH=C:/path/to/sdl2/x86_64-w64-mingw32\lib\pkgconfig 4. set PATH=C:\path\to\sdl2\x86_64-w64-mingw32\bin;%PATH% 5. cabal install sdl2
I was not expecting videos of this! Awesome!
We are in fact working with the same definition of a zipper‚Äîbut I don't see how the `(Tree a, Ctx a)` representation fails to meet the criteria of "some notion of a path, a function for looking up the value at a path", etc. Not only can it do everything you've described there, but it does it with better sharing, better data locality, and a cleaner interface. I suppose that was the crux of my comment: the structure you had in mind apparently wasn't a Huet-style zipper, but everything you said in that sentence describes a Huet-style zipper. I suppose I was in turn somewhat confused by the offhand representation of a "zipper" over JSON values that you described in your article as being associated with the type `Cursor -&gt; Either Err a`, which is very much _not_ a Huet-style zipper. Similarly, describing a Huet-style zipper as an "imperative API" makes no sense to me. I suspect this is why both of us believed the other to be operating under a loose definition of 'zipper'.
[boomerang] (http://www.seas.upenn.edu/~harmony/) 
thats actually super cool
If you take non-empty to be the fixed point of `Either a (a,_)` then you could have a nice definition as tail :: NEL a -&gt; Either a (NEL a) tail = fmap snd . project The code is the same as safe tail on lists!
Yeah, the Haskell identifier grammar should really be based upon UAX#31 
Sorry. I forgot to say. Definition: Let f be the unique functor such that every well-pointed topological space has a covering map to the fibrant topological category of a sheaf of open windings. We will call this f the homopology of that sheaf.
This reminds me of my scala "Just fucking do it" operator. def ‚ïØ¬∞‚ñ°¬∞‚ïØ‚îª‚îÅ‚îª[T,U](value:T) = value.asInstanceOf[U] Naturally, this never actually makes it into source control. Purely a prototyping hack with an @deprecated annotation so my build server would reject it if I accidentally left it in.
[I've actually thought of doing this before](https://hackage.haskell.org/package/acme-dont-1.1/docs/Acme-Dont.html), though it was going to be a synonym for `void`, which... is nonsensical.
Okay, I think I get what you are saying, that Huet zippers also fit that general description I gave there. Maybe I'll see about clarifying the post. Just to clarify, my remark about zippers for JSON parsing was that parsers receive a zipper into the JSON document (the `Cursor` type in `Cursor -&gt; Either Err a`). It's just the input type of the function that is the zipper, not the whole type itself.
&gt; Ok, NonEmpty works for head/tail/init/last. What about applying tail twice? Checkout MinLen above, it goes beyond NonEmpty and encodes (a minimum) length in the types. So you can apply tail twice, just make your function require MinLen (Succ (Succ Zero)) I am not saying one should never use Safe, I still use it. It is just that NonEmpty/MinLen is what I look to first and it solves the problem better a majority of the time. Is your experience different?
It's not a synonym for void. They have the same type signature, but `void` performs the side-effects and tosses away the return value. `don't` doesn't even perform the side effects.
I know, I meant to suggest that when I thought of creating a `don't` function, it was going to be a synonym of `void`. Then I realised that... it wasn't :P.
Thank you. I didn't know hakyll existed. Good chance to get some more exposure to Haskell and finally set up a development blog.
What's up with snap ? Does the io-stream project goes well ?
How about avoiding head and tail functions, and just pattern matching on your list ? I don't think I have ever used such functions in 4 years of Haskell.
An example has recently come up on SO: http://stackoverflow.com/questions/25255917/how-to-improve-the-zipwiths-performance-in-haskell See [note 2] in my answer for details of how the profiling report is distorted in that case, basically compiling with `-fprof-auto` is enough to break stream fusion of some vector processing.
Huh. I wrote a [splay tree](http://hackage.haskell.org/package/splaytree) that uses exactly the same technique. I ended up not using it for much, so the API is woefully incomplete. And I think some of the zipper handling can be simplified a bit. But still, it works very well. I guess instead of a reified zipper I could pass along a reconstruction function, but that kind of code tends to be write-once-read-never IMHO.
I don't think anyone should be allowed to suggest a 2D zipper until they first derive the correct type (preferably by differentiating the original structure's type). After that, if one still thinks it the best solution, then by all means go ahead and implement it. 
Thanks, I see. The garbage consists of non-termination though, which is probably unavoidable. And as you say, there has to be a requirement on the range to be enumerable, since such `inv` would not work on `Float -&gt; Float`.
AFAIK, HoTT is not banished from this subreddit. Why would we do that?
Thanks, I will have a deeper look into denotational semantics. I editted my post.
Sure, I mean, technically every algorithm implemented on a computer is linear time because we're just using DFAs, but even if your float was of unbounded length (using some non IEEE format), it would still probably be enumerable via diagonalisation.
 toStream xs@(Cons x r) = x : toStream (r xs) should be instead toStream xs@(Rec x r) = x : toStream (r xs) I think.
I really like Python. The difficulty of refactoring is a pain point in an otherwise well designed, neat, elegant language. Making annotations and therefore the mypy type checker easier to use or standardised seems likes a good idea. The problem of course is that I doubt I'd have the restraint to use it everywhere. I tend to only use type systems when they're forced on me because it usually makes getting something working quicker for me.
By "data" I mean values in inductive types other than the natural numbers. I'm wondering if there's a kind of analysis for such functions which allows us to look at how their output varies with respect to changes in their input. Functions defined as folds often exhibit a rather precise relationship between the result of a function for some data and its result for slightly smaller data. Here's another question. Suppose I have two functions f,g :: Tree -&gt; Tree where Tree is the type of unlabelled binary trees. Suppose further that I promise f x = tf, g x = tg where tf and tg are built only from Leaf, Node and x. What would be an adequate testing strategy to tell whether f and g coincide on all inputs? (I think I know the answer.) Now what if I tell you that f x = let y = cata lf (\ a b -&gt; nf) x in tf, g x = let y = cata lg (\ a b -&gt; ng) g in tg, where lf, nf, tf, lg, ng and tg are again built just from constructors and the variables in scope? (I wish I knew the answer, and maybe I'll do some digging, both by contemplation and empirically.)
That's correct; I've updated the post. Thanks!
I highly doubt Python will jump on that train (even though I'm convinced such an additional syntax would benefit Python) as long as GvR is deciding Python's fate.
Look at the author of the email.
is there some open source code demonstrating this?
Here is a paper about meadows: http://arxiv.org/pdf/0901.0823.pdf
wow... I'm speechless... 
I was pleasantly surprised, too.
I think it's a bit worse than standard instrumentation. Cost centres prevent some optimisations from happening, while other optimisations are simply disabled in order to preserve the cost attribution. The -fprof-auto flag is careful enough to insert CCs only around functions which are not marked INLINE. https://www.haskell.org/ghc/docs/latest/html/users_guide/prof-compiler-options.html
Even when you pattern match on a list, you need to account for the possibility of an empty list. With NonEmpty/MinLen, you get a static guarantee that you won't encounter an empty list.
I worked on mypy for a couple days at Dropbox Hack Week in July (with Jukka, Guido and others) so, while I'm hardly a mypy expert, I can try to answer any questions people here have about it.
We wouldn't. I'm not sure where that idea would have come from, or why bss03 was downvoted for the confusion for that matter... Anyway, yes, submissions like this are officially welcomed, in case that still needed clarifying.
It seems similar to Closure's type system for JavaScript by analysis and some annotations. Is that an accurate comparison?
It's not. It doesn't try and analyze untyped code (other than in the body of functions that have signatures), and it is a whole language implementation not just the type checker (but the type checker can be run separately sort of how you would use dialyzer, which is what I recommended in my talk).
The syntax is already in Python 3, it just doesn't yet have any standard usage: http://legacy.python.org/dev/peps/pep-3107/
how was your experience of using haskell with ios? how far back does the lack of template haskell set you in practical terms? It seems like it pops up quite frequently as a dependency.
Why is it a whole language implementation? Do you plan on using the types to make optimizations?
Well, I'm not the author of mypy, so I can't tell you precisely why, but that is one of the reasons enumerated on the site: http://mypy-lang.org/ From my conversations with Jukka, I believe he started off with a language that was not at all compatible with Python http://www.alorelang.org/ and then built mypy which was nearly compatible with Python, and then he made mypy fully compatible with Python 3 syntax.
"After" is what everyone said in my category theory classes. It works for me.
How does it deal with functions that are used in the body of a type checked function but aren't annotated themselves? Eg: using numpy functions.
Different episodes will weight those aspects differently. But philosophy is an interest of ours.
Yeah, let's never do Clojure again, even if takes [multiple years](https://www.destroyallsoftware.com/talks/the-birth-and-death-of-javascript)
So I see there have been a couple hundred commits in the past month since I last looked at mypy, which is great, but means I might be giving you out-of-date information. So keep that in mind :) You're right that there is parametric polymorphism in that, for example, you can define a function composition function with the mypy equivalent of `forall a b c. (b -&gt; c) -&gt; (a -&gt; b) -&gt; (a -&gt; c)` as its type. You don't actually have any parametricity guarantees about the function because mypy allows `isinstance`, or `... is None`, etc., on a variable of any type. But you can instantiate your composition function at any triple of types, so it is parametrically polymorphic in that sense. I think the knowledge that the function type constructor `-&gt;` is covariant in its result and contravariant in its argument(s) is sort of baked into the way type inference works in general. You can also define classes that have type parameters, and then give those classes member variables and methods whose types depend on the parameters. At least as of a month ago, mypy only supports invariance here, with variance annotations a wishlist item. So there should be no soundness issues in this area, with invariance the only option. There are also some built-in types with special variance rules. Tuples of a specific length are covariant in each variable. There are some built-in "interfaces" like Iterable that are covariant too. The latter is an ad-hoc special case in the type checker, or at least it was when I implemented it. :)
Ideally you would write some type annotations for your dependencies that live in a separate module and are only used during typechecking, like [this](https://github.com/JukkaL/mypy/blob/master/stubs/3.2/heapq.py) for example. Otherwise I think functions without type annotations are treated as taking arguments of type Any and producing a result of type Any, so you would use `cast` to tell mypy what type you expect the result to be. (`cast` is sort of like `unsafeCoerce` for typechecking purposes but at runtime it is just the identity. After all mypy does not replace python's built-in type (or tag if you prefer) checks.)
I'm not familiar enough with the Closure compiler to compare the two, but basically mypy is a glorified lint tool that requires you to annotate your code with type information, yes. It would only be similar to the typechecking part of Closure; it doesn't emit a new python program or anything like that.
When will the FP Haskell Center use Stackage ? I guess the answer is somewhere already but on the IDE environment setting I only see quite old builds I can choose from.
Hello! I'm a bot who mirrors websites if they go down due to being posted on reddit. [Here is a screenshot of the website](http://i.imgur.com/voUrsYA.png). *Please feel free to PM me your comments/suggestions/hatemail.* _____ [^FAQ](http://www.reddit.com/r/Website_Mirror_Bot/wiki/faq)
I have the impression/intuition that this trick is exactly Polymorphic Higher-order Abstract Syntax (PHOAS), in a different setting where we don't notice at first that we're handling bindings. Which sheds a different light on your closing paragraph: &gt; However, this approach is limited to cases in which it only makes sense to have one cycle. This is not true anymore when we consider datastructures which are not linear: a cyclic tree may have different cycle in different branches, some of which may be nested. It is then necessary to fall back to a setup managing syntax with binding. I would guess that: - the presentation can be extended to multiple cycles (I would try to consider that the phantom index `b` is a world, with an inclusion relation between worlds, as in e.g. Nicolas Pouilllard's thesis) - you are already using name binding techniques!
I've fixed the mentioned #1884 issue that prevents this from working in Cabal sandboxes in the ghcjs branch of https://github.com/ghcjs/cabal already. a `remote-repo` field in a sandbox config replaces existing repositories. We ran into the same problem preparing hackage.ghcjs.org (as a transitional repository adding GHCJS support to existing packages) and sandboxes. If there's interest and concensus that this is how the `remote-repo` field should be handled, I'll send a pull request to upstream cabal and backport the change to 1.20 
See also: [Functional programming with structured graphs](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.307.3874)
Seems like that just pushes the conflicts from the record type declarations to the field name declarations, but I don't get to have my cake and eat it, too. Independent that type of change, there are several other advantages to way vinyl works over our currently record system.
A couple things. 1. What exactly have you made or done? 2. Do you think *at all* about the effect your words have on other people? Everybody knows they need to be shoving stuff out the door. Encountering dicks like you never helps. Sidebar: I've noticed people that have struggled through a real project tend to be more sensitive/kind to others. Those that haven't ever gone through that experience are oft more capable of being thoughtless.
This has virtually all been done by Dedekind already (if you squint just a little); are you claiming originality??
I say there is nothing wrong with wearing both hats. There is a time and a place for research and "just playing", regardless of whether what you are doing seems "useful". If no one did that, then as someone else alluded on this thread, we might never have all the great ideas and techniques we now have for functional programming! There is also a time and a place for putting on your "engineer" hat, being very skeptical, and deciding on the right tool for the job. I suppose my post is more coming from this perspective, but research and just playing have their place too, and I also like doing those things. My pet peeve is when people respond to newbies with a "researcher" type answer (go check out &lt;fancy technique&gt; for this!!), without communicating clearly that this is what they are doing or thinking seriously about whether &lt;fancy technique&gt; is really the best tool for what the newbie is asking. I feel this is somewhat irresponsible. I think you nailed it in your [earlier comment](http://www.reddit.com/r/haskell/comments/2deyp2/paul_chiusano_solving_the_wrong_problems_and_why/cjpb29z). The response I'd like to see in those situations is something like: "a `Map (Int,Int) a` is probably the simplest thing and would definitely work. For fun, your own enrichment, etc, you might want to look at zippers, &lt;blah blah, discussion and links to interesting theory, etc.&gt; And they may even prove useful in your situation, you'd have to play around and see!" Also, here's [a relevant Paul Lockhard quote](http://www.maa.org/external_archive/devlin/devlin_05_08.html). He's talking about pure vs applied maths, but it's the same sort of thing: &gt; Another thing that strikes me is how often I am placed on the wrong side of some sort of Pure vs. Applied, or Art vs. Technology debate. I have always found these to be false dichotomies. Mathematics is an incredibly rich and diverse subject. Can't we enjoy it in all of its many shades and textures? Besides, what could possibly be more useful than a lifetime of free entertainment? &gt; &gt; As Salviati says, just because I object to a pendulum being too far on one side doesn't mean I want it to be all the way on the other side. I seek a balance. Can we not have Theory and Practice, Beauty and Utility? I may be a so-called "pure" mathematician, but that doesn't prevent me from enjoying electronics and carpentry (and oil painting too, by the way). And yes, artists are problem-solvers. And problem solving is an art! By the way, many people (such as myself) enjoy drawing and painting and playing music for fun; not all artists are in it for the money. &gt; &gt; The Pure/Applied distinction is one that I loathe. It is the creative/mindless distinction that I care about. Whether you are proving an abstract theorem about group schemes or calculating an approximate solution to a differential equation, you are either being a creative human being pursuing your curiosity or you are mindlessly following a recipe you neither understand nor care about. That's the issue for me.
Does your Haskell code call back into Java via JNI much? I guess maybe not, since you're using SDL. I played around a bit with GHC on Android and started thinking about what it would take to wrap the Android API (or any other Java API) in a Haskell API. One issue would be managing references to Java objects: JNI gives you local references which are only alive for a single native code call (Haskell in this case) and you can only have a small number at a time, and global references which have unbounded lifetime but must be deallocated explicitly. Ideally the Haskell programmer would be freed from worrying about any of these details. If you (or anyone else) has experience with this in a real-world setting, I would be very interested to hear about it.
If I am not mistaken, with PHOAS we would need to introduce a constructor for variables and then be forced to define substitution. Here `CRec`'s type is really the one of a fixpoint combinator rather than the `(a -&gt; F a) -&gt; F a` characteristic of PHOAS. If I had to draw parallels with other techniques, I would rather mention [difference lists](http://hackage.haskell.org/package/dlist-0.7.1/docs/src/Data-DList.html#DList) because that is the spirit behind what is stored in `CRec`.
Hm, indeed. I watched that clip and he argues "can static types check for feet and inches, can it check for null fields?" The answer for both is yes. That part of the talk is poorly researched, which is a pity because he influences a lot of programmers. On the other hand, it seems that Bob's talk addressed those issues and seemingly van Rossum changed his mind quite readily, which is good.
I use both nix and stackage. They are both amazingly good. An interchange format for stackage sets that compile that would allow for two way synching between nix and stackage would be great. Currently a lot of Haskell libraries have build failures and a lot of build time is wasted trying to build things that does not work. I think a dedicated Haskell solution like stackage over time can develop more sophisticated solutions to the search problem of finding all subsets of packages*versions that build.
This is a long-overdue second post about the implementation of [ethereum-haskell](https://github.com/bkirwi/ethereum-haskell), an autodidactic Haskell project of mine. Compared to [the first post](http://ben.kirw.in/2014/06/24/barely-functional-1-rlp/), this is longer and a bit more technical -- as always, questions and critique are welcomed. I'm still trying to figure out how one might use QuickCheck to test generic functions (a la GHC.Generics) if anyone has any suggestions?
Functors aren't a collection of functions directly. They're two mappings, the object and arrow mappings `Ob` and `ar`. As for `(r -&gt; _)` as a Functor: 1. The object mapping `Ob t = r -&gt; t` takes objects of Hask (types) to objects of Hask again (function types from r to that type) 2. The arrow mapping has to take morphisms in Hask to morphisms in `r -&gt; Hask`, in other words it takes Hask morphisms `(a ---&gt; b)` to (r -&gt; Hask) morphisms `((r -&gt; a) ---&gt; (r -&gt; b))`. Here I use long arrows to differentiate morphisms from functions... although, genuinely, Hask morphisms are represented internal to Hask as functions. In fact, we'll abuse this to represent the arrow mapping internally as well as a function of type ar :: (a -&gt; b) -&gt; ((r -&gt; a) -&gt; (r -&gt; b)) which happens to be composition ar ab ra r = ab (ra r) Thus, functors of this brand‚Äîi.e. endofunctors on Hask‚Äîare captured in `Functor` since `fmap` essentially has the type of the Hask-internal representation of the arrow map of such a functor class Functor f where fmap :: (a -&gt; b) -&gt; (f a -&gt; f b) instance Functor ((-&gt;) r) where fmap = ar
Don't drop implied insults and then whine about people's "reading comprehension" when they call you on it, especially when you're the one who started the argument.
Recall the categorical definition of a functor: it's something tha associates objects with objects, and morphisms with morphisms. &gt; kinds of type * are the objects and the morphisms are normal Haskell functions. Basically, yes. &gt; As far as I can gather (-&gt;) is considered to be the morphism between objects (since a-&gt;b still has kind *) and functional composition is the morphism between functions Kinda. -&gt; isn't a morphism between objects. Recall that a functor takes a single object from the target category and returns a single object in the destination category. -&gt; takes two objects and returns a single object. We need to partially apply an object to -&gt; in order to get a functor. Once we partially apply r, we have a mapping from Haskell types to Haskell types - that is to say, a mapping on the objects in Hask: it associates Int with r -&gt; Int, String with r -&gt; String, a to r -&gt; a, etc. This is why ((-&gt;) r) is a functor, not (-&gt;). Additionally, fmap maps morphisms to morphisms. For example, it will associate the morphism (a -&gt; b) with the morphism (r -&gt; a) -&gt; (r -&gt; b). So it should be pretty clear that (-&gt; r) and fmap form a functor on Hask: you have a map from objects to objects and a map from morphisms to morphisms. Given that fmap id == id and fmap f . fmap g == fmap (f . g), we have a functor on Hask by definition. &gt; You don't call (Just 5) a functor, you call Maybe a functor. There's a good reason for this: Just 5 is a particular value. Maybe is a type constructor or kind * -&gt; *. So Maybe is a mapping from objects in Hask to objects in Hask: it maps Int to Maybe Int, String to Maybe String, etc. Notice that that's exactly the same as ((-&gt;) r). We could say that "functions are functors", in a loose sense, but it's probably more precise to say that "functions from r" form a functor. This isn't quite like saying "Just 5 is a functor"; the equivalent of that is that "the length function is a functor".
Definitely worth reading as well is the "sequel" to that paper, [Abstract Syntax Graphs for Domain Specific Languages](http://ropas.snu.ac.kr/~bruno/papers/ASGDSL.pdf)
On a related note, this guy is in the process of writing cavestory in Haskell: https://github.com/chebert/cavestory-haskell (Although he is currently on hiatus)
But functions from r don't form a kind of * -&gt; *, they are still of kind *. (-&gt;) :k: * -&gt; * -&gt; * ((-&gt;) Int) :k: * -&gt; * (I'm using :k: as :: only for kinds) This would represent the set of functions F of type (Int -&gt; a) which has kind *. Functions don't form a Functor in any way, they're the morphisms of the category with functor ( ((-&gt;) r) , fmap ). My confusion is the fact that everyone says that Haskell functions themselves form a Functor.
"Functions from r" in this case should be interpreted as `((-&gt;) r)`, which is indeed of kind `* -&gt; *`. This maps from an object `a` in Hask to an object `r -&gt; a` in the subcategory of Hask which is the target of the `((-&gt;) r)` functor. The mapping on morphisms provided by `fmap` takes a morphism `a -&gt; b` in Hask to a morphism `(r -&gt; a) -&gt; (r -&gt; b)` in the aforementioned subcategory. And yes, in case it wasn't obvious, the use of Category Theory terminology in Haskell in somewhat sloppy. The two mappings described above constitute the functor in the proper sense. Haskell programmers will often refer to an unapplied type constructor `(-&gt;)` or, even worse, values whose type includes that type constructor, as "a functor". The latter usage is even more prevalent with `Monad`, which is unfortunate. If it helps, `((-&gt;) r)` is basically the [covariant Hom functor](http://en.wikipedia.org/wiki/Hom_functor) except with Hask instead of Set.
I'm new to Haskell too, but i think it's a good idea to use camel case for functions names (cardChar). replace repeating functions like the (cardChar) function with (case of) cardChar r = case r of Ace -&gt; 'A' King -&gt; 'K'
Ah. So e.g. differentiating a function on the reals says "approximately this much changes per change in input." Similarly, is there a way to read a differential of a polynomial functor as saying "if i change the data this functor is over by 1 (adding e.g. a Maybe), then how much does that change the 'size' of distinct elements after applying the functor". Or if not the formal "zipper" derivative, is there another related formula that tells me this. Yeah, that's a good question! gotta tie in to the calculus of finite differences I figure. I guess another related question, or perhaps reformulation of your second is: We are given a function on some polynomial data of some sort. Its result can be the same domain, or it can be in a more restricted domain such as Integers. Now, is there a theorem that says if we test this function on N inputs, for some N, we can then determine this function uniquely? (Obviously, no). Now, what if we know something more about the form of this function. And is there a relationship between "forms of functions" and the resultant value of N. It seems to me that there must be good results here (which have to do with how far the function can "look"). 
&gt; The big picture, as you can probably imagine, is exhilarating: Haskell is now one of the very few statically-typed, high-level, functional, compiled languages that work, without substantial changes to the code, on Linux, Windows, Mac, Android and iOS. So cool. It's really exciting to see someone working with this!
Done...committed the changes.
Objects of Categories as Complex Numbers http://arxiv.org/abs/math/0212377 If you have an infinite set equipped with a nice-enough isomorphism, you can give it a complex cardinality. For example, the set of unlabeled binary trees satisfies T ‚âÖ 1 + T¬≤, so |T| is a sixth root of 1. T‚Å∂ has cardinality 1, but it's an infinite version of 1. T‚Å∑ is isomorphic to T---not just because they're both countable, but nicely isomorphic in the sense that it's constructed entirely from uses of the isomorphism above. Similarly, binary strings satisfy B ‚âÖ 1 + 2B, so |B| = -1 and there's a nice isomorphism between T¬≥ and B. 
What are the main changes?
This is the first time I've heard about Yampa being used in a while. Very interested in seeing FRP used at a larger scale.
Thanks for posting this! As some one who uses both haskell and python for work this is awesome!
looks like no more Maybe everything!
No, definitely not dead. I changed jobs recently and some other things, which means that I haven't had the time I was expecting to work on it. I'm still planning a release with the Snap 1.0 but I may try to put something out before then as well since it's been so long. The website was having some issues I haven't had the time to debug, but it's almost certainly a domain issue as the site hosted using github pages (which can still be seen [here](http://christopherbiscardi.github.io/snap-for-beginners/).
Hmm the gists no longer load?
I know him IRL, I think he may have abandoned it unfortunately.
That's great news, I really hope it gets backported and merged into 1.20!
I guess there are advantages to having haskellers at Facebook
Another interesting way to encode these structures is captured by the `bound` library. There's a lot to explore with these various ways of encoding these kinds of references inside the language. http://lpaste.net/109425 http://hackage.haskell.org/package/bound
I'm new to Haskell, but I recognize this as Hodge theory for the update operators. Interesting how the very Haskell approach to implementing CAs leads to building a topos of sheaves, as a "God API" for dealing with a simulated world. Will be thinking a lot about this I'm sure.
I could barely read the second half
what's bad about artificially created problems? isn't it also nice to just do something because it's interesting, not because it has applications somewhere? (and there might also be some applications you don't know when doing the research itself.)
That's (almost) what they do in the second part of Ghani et al.: after rejecting the extensional approach, they turn to type-level de Bruijn levels (basically your solution, except that `bound` deals with type-level de Bruijn *indices* and it lets you weaken entire subtrees rather than having to traverse them all the way to the variable nodes). I'm still not quite happy with this one: you can mark a node as being the beginning of a cycle and never use the variable introduced that way because you finish using `Nil`. To decide whether a structure is cyclic or not, you have to go down the tree all the way to the variable node rather than stopping at the `Mark` constructor. [My take on the binder approach](https://github.com/gallais/potpourri/blob/master/haskell/cyclic/CyclicListBinder.hs) uses GADTs to guarantee that there is *at most* one pointer introduced and that if it is introduced then it *has to* be used. I don't like types which lets liars get away with saying one thing and doing another. :)
Is there a practical use for Comonad *other than* cellular automata? The only other one I can think of is zippers.
people always also bring up image transformations as well
The layout on this page is broken. The navigation covers up content.
Changing `https://` to `http://` makes it readable for me.
There's the dataflow stream transformer comonads, but last time I heard about them they were "really inefficient" and there'd been no further examination.
Store shows up often in the implementation of Lens. IStore is the signature functor for Moore automata. Not *exactly* practical, perhaps.
Could you elaborate on that? I'm loosely familiar with those terms and I think it'd be very illuminating to me to see how they connect to this post.
Often, it's more verbose. Not to say this is the author's standpoint, but I've heard the argument that case statements are sometimes more clear than pattern matching. I would say, if you have a sum type like a card, say: data Card = Ace | King | Queen | Jack | NumCard Int and you're doing something simple like trying to show it, then `case` matching is better. In the case of something significantly more verbose, like writing a DSL for a game, for example: data Command = Move Direction | SetUpTheBomb | DrinkPotion then a function that handles updating the state of the game might be better written in pattern matching style, e.g: handleCommand (Move dir) = do some multiline complex behavior handleCommand SetUpTheBomb = {- you get the idea -} -- ... and so on That's usually how I design things, at least. If I'm trying to do something simple like `show`ing, case statements work fine and are less verbose. But I'd hate to encapsulate handling complex logic in a case statement, just because the indentation gets out of hand and it's harder to tell where one statement ends and the other begins.
I really wish cross-compiling were easier, though. Currently, you need to build ghc from scratch to target each platform - you can't compile a windows executable and an arm executable with the same ghc binary AFAIK. It would be nice if we could build a cross-compiling ghc that could build for *every* platform.
&gt; ... he's not thinking about type systems in languages like Haskell Obviously not, but he is speaking of "typechecking in compilers" in general. The relevant quote from the talk: &gt; The typechecking in compilers is actually very weak. Yes it can tell you that you shouldn't be putting an integer into a string field... or something like that. But does it tell you that you are mixing inches and feet or inches and centimeters? Does it tell you that a certain variable can be null and that you should check for that before just using it? No! His second claim about null is correct for certain languages, but is not an inherant property of static type systems in general. It is worrying when prominent people make false statements. Guido is a programming language creator and as such influential when it comes to languages. Listeners to his talk may take his claims for truth and build their view of static typing on false premises. That is not a good thing. 
The purpose of the library is to wrap crashing functions in a way that's easier to use - making them safer (you get non-crashing variants) and easier (if they do crash, the error is better). If I were doing the same thing today I would name all crashing functions in the module trailing with Ex, or put them in a different module.
Hmm, I thought hdevtools had the advantage of staying running, so it didn't need to spin up and load your code each time it was queried. Does ghc-mod have that ability now?
Are you on mobile? Did they used to load for you and then stop?
Wow, I didn't know this existed. The whole thing is hilarious. Thanks for sharing!
Sure thing. My e-mail's in the comments of the code. Drop me a line, and let me know what you need.
Doesn't work with https everywhere :( EDIT: if you can change the reddit button js link to use https it should be fine
It reads fine after judicious application of firebug.
Innate to be that guy, but if you are willing to go down the rabbit hole, using lens and representing the conversions as prisms might be a fun exercise.
Actually do check out everything. At least ‚ÄòA Simple Category-Theoretic Understanding of Category-Theoretic Diagrams‚Äô is hilarious too
Or minimal application of the element inspector.
Because I'm a Haskell noob and I've used Rake in the past. I definitely plan to go to cabal eventually (patches welcome!) but it's not at the top of my list right now.
&gt; Copyright is maintained by the individual authors, though obviously this all gets posted to the Internet and stuff, because it‚Äôs 2014. Hah.
probably, the number of IO operations is greatly reduced so I would bet that it would be a lot faster. Still probably not at C++ levels though ;)
If you can't justify it then the structure and the problem might as well be arbitrary What you said is not a good justification by any stretch of the imagination, because I can come up with an indefinite number of arbitrary structures and postulate difficult conjectures, and logically there is the possibility that might have an application somewhere some day. It's kind of obvious that you have to use good judgement to direct research.
fractals of category diagrams!
I think the rhetoric needs to change. "It preserves the structure" is literally meaningless. Why don't we just call the laws what they are. The first --at least in Haskell if not in theory-- is that `fmap` has `id` as a fixed-point and the second is that `fmap` distributes over composition. That's it. Any appeals to intuition are a waste of time and the only thing that should be done is (i) talk about what we can reason of `fmap` with only these simple laws and (ii) look at models for `fmap`, i.e. functions which satisfy the laws.
"It preserves the structure" is not meaningless. The terms "preserve" and "structure" are both meaningful in this context. This meaning is made rigorous in category theory. There are multiple ways to consider what the laws "are". I prefer the category theoretical interpretation that gave us the term *Functor* in the first place: fmap sends arrows to arrows in a way that *preserves* their *structure* (identity and composition).
&gt; ‚ÄúA Simple Category-Theoretic Understanding of Category-Theoretic Diagrams‚Äù by ‚ÄúStefan Muller‚Äù is a work of incomprehensible abstract nonsense, and as such, constitutes a valuable contribution to the field of category theory. Amazing.
The same rhetoric is also used in algebra when talking about homomorphisms and fmap actually satisfies the homomorphism laws (checking this from memory not but I think I'm correct). I always find the abstract algebra perspective a little easier than the categorical one, mainly because I've studied it, whilst category theory is still on my todo list.
Yep! A functor is a category homomorphism. I actually said "in algebra and category theory" before I changed it. Category theory *is* algebra, after all. It's the "abstract algebra of abstract functions" (Awodey).
Oh man, reading the SIGBOVIK proceedings are a highlight of my year
&gt; Heterotopy Type Theory: A defense of the traditional foundations of mathematics well played &gt;Many authors mistakenly refer to these types, even informally, as ‚Äúproofs of equality.‚Äù This is not about equality. It is about our freedom to practice type theory as we see fit, and we will defend this freedom against any onslaught of political, or mathematical, correctness. This paper takes a firm stand against the liberal attack on identity, instead offering a pure account of identity types, called heterotopy type theory, which reaffirms their original intent. We hope this document will be enshrined as a part of the Constitution of the Association for Computing Machinery [1], to protect the august institution of identity against future attacks.
I suspect the llvm backend could help with this? Once you have llvm bit code you can (hopefully) compile to any platform. 
It would be especially difficult if you wanted to make a cross-platform wrapper as well due to the typing :\
I wonder how much effort it'd take for ghc to call the target platform's ghc for template-haskell
That's because the phrase is defined, monolithically, to mean it satisfies the laws; otherwise it has no meaning. That makes it completely obsolete. What tsahyt says about homomorphisms is true, but that does nothing except draw an analogy. Still obsolete. You can't say "`fmap` *preserves* their structure* without first defining (preserves structure) = (satisfies the laws). And in a category theory there is no polymorphism per se so `id` is a function from objects to morphisms and the law becomes &gt; `fmap[id[A]] = id[fmap[A]]` which means that "`fmap` distributes over `id` and composition" is the most concise and actually meaningful way of characterising functors. The terms "preserves" and "structure" are in general mathematically meaningless. Not so for "fixed-point" and "distributes over".
I really don't understand why everyone is missing the point in what I said. I said "it preserves the structure" is meaningless not that you cannot give it meaning. (Ignoring the other aspects of a functor) think how ridiculous it is to say "f is a functor iff it preserves the structure". That is completely meaningless until you define (preserves the structure) = (satisfies functor laws). Which means the phrase is superfluous, since you could have started with "f is a functor iff it satisfies the functor laws". Not to mention that the functor laws can already be characterised in English (see my other post) without having to introduce the monolithic "it preserves the structure". PS It's as if I have said "'the square root of 9 is pork chops' is meaningless" and you replied with "no, because see, we can define 'pork chops' as 3".
And the example of 'pork chops' is generous because that is a noun-phrase whereas 'preserves the structure' is a predicate. Nobody has taken it upon themselves to define the verb 'preserves' or the noun 'structure' separately.
Yep. Had to kill it for the sake of my sanity. document.getElementById('navigation').remove()
Note, 65 is in PDF page numbers. In actual page numbers on the bottom of the document pages, the page is 57, and 65 is cryptojokes.
Minor error: The line &gt; fromItem (List [address port id]) = is missing a few commas, no?
[Fixed Gfycat Link (HTML5 &amp; GIF)](http://gfycat.com/AbleDismalIndianpangolin) ^v1.5 ^| ^[About](http://www.reddit.com/r/GfycatLinkFixerBot/wiki/index) ^| ^[Banlist](http://www.reddit.com/r/GfycatLinkFixerBot/wiki/banlist) ^| ^[Code](https://github.com/Gawdl3y/GfycatLinkFixerBot) ^| ^[Subreddit](http://www.reddit.com/r/GfycatLinkFixerBot) ^| ^[Owner](http://www.reddit.com/user/Gawdl3y) ^(Problems? Please message the owner or post in the subreddit.)
Source: https://gist.github.com/kasbah/b6e638655f18c1e11a2c
I'm enjoying going through it at random. So far, my favorite paper has been "What, if anything, is epsilon?", which samples lots of github programs to find out what values programmers use for epsilon. Among other things, it turns out there are a lot of dumb programmers. :-) I also love the article on "Unit-Test-Based Programming".
It is appended/extended. Because you can specify css, I think it would be great for that. If you clone and load some of the html in examples/, there are inline links which connect the macros refs to their definitions.
Eugenia Cheng's n-cats book has a nice discussion of what she calls "The Data-Structure-Properties (DSP) trichotomy". Eugenia Cheng and Aaron Lauda, [*Higher Dimensional Categories: an illustrated guide book*](http://cheng.staff.shef.ac.uk/guidebook/guidebook-new.pdf) p. 5
Nice catch! Fixed, thanks.
Pretty cool. Will definitely be using this when I embark on a toy project next time.
You are now my favorite bot. 
Thanks to Racker [ccarter](https://github.com/ccarter) for fixing the 1.2/1.3 incompatibility :) [Bloodhound TravisCI](https://travis-ci.org/bitemyapp/bloodhound) [TravisCI yml for Bloodhound](https://github.com/bitemyapp/bloodhound/blob/master/.travis.yml)
Actually it would be cool to be able to replace code, works better for Haskell syntax, too. That's how tutorials in e.g. School of Haskell are laid out. Add a flag to interpret up to a label, and you've got an interactive tutorial which you can play with in ghci while the code progresses. 
Might be good to have a "before" and "after" pairing
It would be nice to see a thorough blog post response to this thread. And just as important as "what chapters are no longer relevant" is "what chapters would be missing today?" 
This [Stack Overflow answer](http://stackoverflow.com/questions/23727768/which-part-of-real-world-haskell-is-now-obsolete-or-considered-bad-practise?rq=1) knows. (As of writing, last updated June this year.)
Is there a way we could use tex with this? I think tex requirements is a feature, not a bug.
From the code of generic-deriving: -- | Sums: encode choice between constructors infixr 5 :+: data (:+:) f g p = L1 (f p) | R1 (g p) This states that **:+:** is an infix operator that associates to the right (because of the 'r' after 'infix'). In other words: a :+: b :+: c gets parsed as a :+: (b :+: c) so this will behave consistently unless the declaration of the operator changes.
I don't understand the claim of &gt; There! Now the f is forced to take or leave its input x, and so we can breathe easy knowing that filter2 returns a subset of its inputs. Can I not call filter2 (Just . (+1)) [1,3] and get [2, 4] ? If the case Just y -&gt; y : filter2 f xs were changed to Just y -&gt; x : filter2 f xs that problem goes away. But there's still nothing in the type enforcing that the definition of filter2 in fact only returns a subset of its input. Obviously this is distracting from the real point of the post and mostly tangential. EDIT: Reading on, it seems that the final solution also suffers from the problem of NOT returning a subset of the inputs, given a "predicate" that doesn't only return either (Just (its input)) or Nothing. Am I missing something?
Essentially, a field where we define `0^{-1} = 0`. See [arxiv:0804.3336](http://arxiv.org/abs/0804.3336v1), [arxiv:0806.2256](http://arxiv.org/abs/0806.2256v1), etc
&gt; Notes: One programmer defined epsilon as 1.0-18 , that is, -17, probably meaning 1.0e-18 . Four used literal 0 for epsilon. Another defined epsilon as 1 / 100000 , which uses integer division and results again in 0. Owing to the strength and creativity of C# developers, here we saw our largest value of epsilon so far, 700 , and the declaration with the most significant digits: 0.0000000000000002220446049250313080847263336181640625 I don't even.
This is not about how the operator itself associates, but rather how GHC implements the `deriving Generic` mechanism. If I were to write the data declaration data T = A | B | C deriving Generic GHC would generate a `Generic` representation corresponding to that data type which is roughly of the form `a :+: b :+: c`. At present, it appears to generate `a :+: (b :+: c)` (which you can verify by writing the above declaration in a file with the appropriate pragmas and then compiling it with `-ddump-deriv`) but the manual, [as linked above](http://hackage.haskell.org/package/generic-deriving-1.6.3/docs/Generics-Deriving-Base.html#g:9), indicates that a user should not rely on a particular nesting being generated. Among other things, this means that GHC could hypothetically change the way it implements `deriving Generic` and consequently the [serialization strategy presented here](http://www.haskell.org/ghc/docs/latest/html/users_guide/generic-programming.html#idp25226064) would produce a different encoding of the same value when run with a different version of the compiler. The question being asked is not, "Is the nesting of these expressions reliably the same?", because the manual clearly indicates that you can't rely on a particular nesting. The question is, rather, "In light of the fact that the compiler can choose an arbitrary nesting, should the serialization example be revised to produce a consistent serialization regardless of how the expression is nested?"
Looks really neat, I like the macro stuff! You might be able to draw some additional inspiration from [rundoc](https://github.com/tarleb/rundoc): it uses pandoc to do the hard work and allows to replace part of the code by running dynamic interpreters. Unfortunately it's author (me) is a lazy fuck and has currently little time for the project.
If you mean the source code, it's in [Haskell](http://www.haskell.org).
Yeah I agree. This is very cool and seeing the transition from before to after would be even cooler.
Well, it should be, considering the subreddit we're on.
No, not really. For Haskell it's pretty normal.
Release notes: https://github.com/aristidb/aws/blob/master/README.org
My two cents is that I think it should be an error to redefine a code block. If you want to amend a code block, there should be a separate operator for that.
I bought the book recently, after already knowing Haskell, and when I flicked through I didn't find much was actually out of date apart from remarks at the beginning+ends of chapters explaining which packages to use (i.e. I recall a chapter claiming mtl was rubbish and to use some new emerging library which is now dead).
Because it loads faster. I'm a busy man, I want my animated images now, dammit!
Urge. You are absolutely correct. Serves me right for posting so late at night. Will fix shortly! Thanks for pointing out!!
Oooh this is great, thank you.
Ho hum.
There's some partial support for IAM temporary credentials (no code for refreshing them yet), but only for S3 and SES right now, I think.
"Preservation of structure" means that `Just` gets mapped to `Just` and `Nothing` gets mapped to `Nothing`. So, the inability to get a `Nothing` result does (sort of) demonstrate preservation of structure. But, perhaps I am missing your meaning?
Well, this (and the quote in the reply) convinced me to read everything.
Do you plan to keep the snapshots? So if I start a project today using a snapshot, would I be able to use the same snapshot in 1 or 2 year? Or do you plan to purge some snapshots time to time?
The next "aws" release will bring IAM temporary credentials support to DynamoDB as well. :) Well, I need to test it first, of course, but the code is there...:D
I was just looking at this library yesterday morning (for SQS). Great to see more work on it!
Congrats Aristid, nice job!
We have no plans to ever purge snapshots.
Currently, the builds are only being tested on Linux, though I'd love to get them tested on Mac and Windows (and other platforms) as well. If anyone's interested in helping out with that, please be in contact with me.
Thanks! The beta of stackage is certainly the best tool I saw since I started using Haskell to fix the cabal hell issue. Not only it resolves most dependencies problem but also the time lost in re-compilation at each new project creation. Thanks for you insight and your dedication.
Oh hmm. I thought there was GPL contamination in the dependencies, but looking again now I only see LGPL. Might just check it out myself, then...
I finished reading it recently and got a lot out of it.
Done! 0.10.2 brings support for STS/IAM temporary credentials in all services. (It's not 0.10.1 because I accidentally uploaded the wrong thing for 0.10.1.)
You end up repeating the function name n times. When it comes to refactoring you have to change them all. 
A real, non-toy version of this actually WOULD be useful. Just because C# has side effects and you can't use the IO monad to enforce purity doesn't mean composing together a program and then running it later wouldn't be useful (although perhaps async jobs already accomplish this well enough).
&gt; We‚Äôre going to start with the simplest version of type theory, a system called ST. ST is defined axiomatically. The axioms are, mostly, very similar to the basics of axiomatic set theory ‚Äì especially to the NBG formulation. It looks like the author doesn't have a clue what type theory is about. You cannot have an axiomatic type theory. That is definitionally *not* a type theory. This blog post is just promoting a boring axiomatic set theory and has zip to do with type theory.
Yes I should have titled it differently - perhaps "IO Monad explanation using C#". But now I can't seem to change the title on reddit.
haha I wasn't implying you had to do it today ;) I am sure the community appreciates your effort!
Is there a blog (or something) to follow your project?
Read [this Wikipedia article](https://en.wikipedia.org/wiki/ST_type_theory). Also, be prepared to be haunted by Russell's ghost.
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**ST type theory**](https://en.wikipedia.org/wiki/ST%20type%20theory): [](#sfw) --- &gt;The following system is Mendelson's (1997, 289‚Äì293) __ST__ [type theory](https://en.wikipedia.org/wiki/Type_theory). ST is equivalent with Russell's ramified theory plus the [Axiom of reducibility](https://en.wikipedia.org/wiki/Axiom_of_reducibility). The [domain of quantification](https://en.wikipedia.org/wiki/Domain_of_discourse) is partitioned into an ascending hierarchy of types, with all [individuals](https://en.wikipedia.org/wiki/Individual) assigned a type. Quantified variables range over only one type; hence the underlying logic is [first-order logic](https://en.wikipedia.org/wiki/First-order_logic). __ST__ is "simple" (relative to the type theory of *[Principia Mathematica](https://en.wikipedia.org/wiki/Principia_Mathematica)*) primarily because all members of the [domain](https://en.wikipedia.org/wiki/Relation_(mathematics\)) and [codomain](https://en.wikipedia.org/wiki/Relation_(mathematics\)) of any [relation](https://en.wikipedia.org/wiki/Relation_(mathematics\)) must be of the same type. There is a lowest type, whose individuals have no members and are members of the second lowest type. Individuals of the lowest type correspond to the [urelements](https://en.wikipedia.org/wiki/Urelement) of certain set theories. Each type has a next higher type, analogous to the notion of [successor](https://en.wikipedia.org/wiki/Successor_function) in [Peano arithmetic](https://en.wikipedia.org/wiki/Peano_arithmetic). While __ST__ is silent as to whether there is a maximal type, a [transfinite number](https://en.wikipedia.org/wiki/Transfinite_number) of types poses no difficulty. These facts, reminiscent of the Peano axioms, make it convenient and conventional to assign a [natural number](https://en.wikipedia.org/wiki/Natural_number) to each type, starting with 0 for the lowest type. But type theory does not require a prior definition of the naturals. &gt; --- ^Interesting: [^Type ^theory](https://en.wikipedia.org/wiki/Type_theory) ^| [^Per ^Martin-L√∂f](https://en.wikipedia.org/wiki/Per_Martin-L%C3%B6f) ^| [^Personality ^type](https://en.wikipedia.org/wiki/Personality_type) ^| [^Dependent ^type](https://en.wikipedia.org/wiki/Dependent_type) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cjs89i1) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cjs89i1)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
Yes, this is a good answer. I'm looking for a more comprehensive community answer than 1 person can provide. Also, a white-list of chapters would be more useful to me than a blacklist. Here's a classification based on qZeta's SO answer. Please add to it: Chapters * 1-10) 1 * 11-13) ? * 14-15) 2 * 16-18) ? * 19) 3 * 20-21) ? * 22) 3 * 23) ? * 24,28) 2, but Parallel and Concurrent Haskell is better * 25) 1 * 26) 2 * 27) 2 
&gt; although perhaps async jobs already accomplish this well enough Actually not really; C# asyncs are not very composable. The main reason is that a value of type `Task&lt;T&gt;` represents a task *that is already running*, at which point it is too late to compose it. Contrast with F#'s asyncs, which can be composed using monadic combinators and *then* run with `Async.Start` and the likes.
Ok I take that back it's not the author who's got a whacky notion of type theory, it's Mendelson. At least, in that type theory has moved on a great deal since the time of Russell and ST type theory is a whacky throwback. At any rate, it's not really relevant for anyone interested in type theory. That's really the important thing.
I've been thinking of trying to get a working version of TST (and NFU?) in Haskell for a while. I'll be interested to see where this goes.
`joeblessyou`, if you mean the funny stuff at the end like so run engine $ scene &lt;~ fraction1 engine ~~ fraction2 engine ~~ Window.width engine ~~ Window.height engine they're just fancy utility combinators defined in helm : https://github.com/switchface/helm/blob/master/src/FRP/Helm/Utilities.hs#L111
This exists to some degree in [XsharpX](https://github.com/NICTA/xsharpx/blob/master/src/XSharpx/Terminal.cs). It only supports terminal operations right now (I am sure they would accept patches to extend it and also to add other kinds of IO (e.g. FileIO)). Could then extend it further by making use of coproducts to combine the grammars together, or by adding trampolining so other monadic functions (e.g. `forever`) don't immediately overflow the stack.
Why can't you have an axiomatic type theory?
Type theory has developed since its roots in Russell into something fairly different. Rather than merely assigning types to something and that's that, modern type theory, sort of post-Curry-Howard, post-Martin-L√∂f, and post-Dummett, has found philosophical foundations as much, if not more so, in Gentzen. That is to say, modern type theory seeks to do more than merely assign types, but to *justify* types, both in what they get assigned to, and, more importantly, in what types there are in the first place. The focus on justification is key. It's not ok to simply assert things, even if doing so is consistent -- we must show that the assertion is justified at a fundamental level, which means we must exhibit certain proof transformations that show that the assertions neither create nor destroy information in a particular way (which turns out to be beta and eta rules). It ultimately comes back to the question of what counts as an acceptable logical connective, and the question of what the meaning of these things is.
This got me even more confused! :) I thought that the answer would be something like: current type theory subsumes logic and is very expressive, so you can't write it as axioms of a logic (and especially not a weaker logic like first order logic, even using axiom schemata). Your "can't" seems not to relate to the *ability* or defining such object but whether it is philosophically justified. But, if one wanted, would it be *possible* to write a foundation for some "modern" type theory using something like first order logic? Also. Why can't you *justify* types with an axiomatic construction? Or even give a proper meaning to your types (isn't model theory enough?)
I got sucked into some other engine changes, but I plan on making FreeCell this week. I'll e-mail you once I make some progress.
What kind of composition are you talking about, basic monadic bind is straightforward: public async Task&lt;T&gt; Bind&lt;T,U&gt;(Task&lt;U&gt; value, Func&lt;U, Task&lt;T&gt;&gt; f) { var temp = await value; var result = await f(temp); return result; }
Too many steps to sustain interest in trying out.
Thank you! I'm glad it's working well for you.
I've never heard of these options before (I haven't checked the yesod install guide in over a year) What makes these options significant?
Those are instructions for developers I guess. Or do you mean the actual programming environment has too many steps? I can't comment on that as I haven't checked it out yet.
Note that /u/Kaidelong talked about &gt; composing together a program and then running it later The problem with C#'s `Task` is that it represents a single occurrence of a computation. Starting a task is a destructive operation (if you try to start it twice, it throws an exception). It is not meant to be stored away in a data structure and run whenever you want. And most relevantly in our case, it cannot be composed twice. If I write the equivalent to your `bind` in F#: let bind value f = async { let! temp = value in return! f temp } then I can do the following, which would raise an exception in C#: let someComputation = async { return 42 } async { let! x = bind someComputation (fun x -&gt; x + 2) let! y = bind someComputation (fun x -&gt; x - 1) return x + y } The closest thing to `Async&lt;'T&gt;` in C# would probably be `Func&lt;Task&lt;T&gt;&gt;`, but writing and composing those would be atrociously verbose.
I am curious as well, and after investigation I found from [this discussion](https://github.com/kazu-yamamoto/ghc-mod/issues/253) that `ghc-modi`, rather than `ghc-mod`, is supposed to help with invocation times. I don‚Äôt know about Sublime Text, but it appears that [using `ghc-modi` in ghcmode-vim](https://github.com/eagletmt/ghcmod-vim/issues/48) remains an open issue :( There is a pull request with preliminary support tough.
Using these options, as well as optimization: 2 (in the cabal config) results in the same consequences. A second attempt with `--disable-optimization` succeeded quickly and only hit the usual 1.3GB of RAM used. 
This is most likely an example of the SpecConstr blowup bug: https://ghc.haskell.org/trac/ghc/ticket/8960 Try -fno-spec-constr or -O, and maybe put a comment on this bug :)
I am so looking forward to it. Let us know when you do. 
So... have one of the inputs be Nothing? 'CAuse that's the only way `fmap f input = Nothing`.
That's what I was thinking, yes.
The thing is that saying "preserves structure" is a great way to package what the laws do into a concept that is intuitively easy to understand: saying "`fmap` distributes over `id` and composition" is great for formalizing what it *exactly* means to "preserve structure", but is harder to understand, as it's not obvious why that's important if you are not used to category theory beforehand. That's why "preserves structure" is a metaphor used everywhere in math when talking about mappings which preserve laws. We don't teach basic arithmetic by starting with the Peano axioms and building up either. You say that we are defining "preserves structure" = "satisfies the laws", but "structure" is a term that means something in its coloquial use, and describes what the laws do.
On a related note Ive found that persistent entities takes an incredible amount of memory to compile which makes it very difficult/tedious to build and deploy on heroku. We ended up splitting all our entity definitions into a separate package to deal with the issue, but I would really love to see as much work as possible going towards reducing memory consumed by ghc. Slow compile and deploy cycles is probably the #1 problem we're facing with haskell (at least, relative to nodejs)
http://hackage.haskell.org/package/mueval can certainly evaluate Haskell. I'm uncertain about exposing host app functionality, but it seems a reasonable thing to be able to do.
good - thank you!
I think thats not a good idea. Maybe you are looking for eps¬π to balance out rounding errors. ¬π http://en.wikipedia.org/wiki/Machine_epsilon .
I'd say slow compile times is my number one complaint with Haskell in general. I recently ported some the core computations of a Haskell program to the GPU, and the best thing about that was the turnaround time for a change going from 30sec to &lt;100ms. It's just incredible what a difference in productivity this makes! When I had to work with the Haskell code, it basically went like this: "Oh, it seems there's some precision error, let me try something" (stares at cabal compiling for 30sec) "Damn, forget a minus" (stares at cabal compiling for 30sec) "Let me comment that debug output code quickly" (stares at cabal compiling for 30sec) "Let's try clamping that one value to see if its overflow causes the problem" (stares at cabal compiling for 30sec) ... (That 30s is a measured, not an exaggeration. And that's for a small 2.5k LoC app...). At least the ~20sec link time is gone since dynamic linking got fixed on OS X with 7.8.3.
http://hackage.haskell.org/package/dyre may be relevant too.
What's the reason you're not using GHCi? That usually has a much quicker turnaround time.
I never found ghci to be useful beyond figuring out a tricky one-liner. Besides, the interpreted code is often so much slower so that what you gain from faster compile times is lost again waiting for your app to startup.
Cant answer for the poster above, but I personally use the repl all the time. Im eagerly waiting for https://github.com/haskell/cabal/issues/1715 (happily fixed!) to land in cabal since i run into this one snag every now and then, great experience overall though. We never-the-less build &amp; deploy frequently, its just a fact of life in an early stage startup.
Thank you!
Mirror?
Certainly. Using the [hint](http://hackage.haskell.org/package/hint) library, you can interpret arbitrary haskell expressions, and you can decide which modules are in scope via [setImports](http://hackage.haskell.org/package/hint-0.4.2.0/docs/Language-Haskell-Interpreter.html#v:setImports). If your app is installed in `~/.cabal` or if you [give hint the path to your sandbox](http://hackage.haskell.org/package/hint-0.4.2.0/docs/Language-Haskell-Interpreter-Unsafe.html#v:unsafeRunInterpreterWithArgs), you can give the name of one of your own modules. However, that might not be enough for your purpose. For example, suppose you are writing a game and the exposed API is for scripting an enemy AI. You might want to have an action getPlayerPos :: IO (Int, Int) which would fetch the information from some `IORef`. But such an action cannot be defined on the top-level: the function would either have to take an additional input containing this `IORef`, or the function would have to be defined dynamically, with the `IORef` hidden in its closure. If you want to do the latter, then you will have to expose this dynamic function to hint. One way to do this is to wrap the user's script in a function accepting the dynamic function as input, like this: $ ghci &gt; import Data.IORef &gt; playerPos &lt;- newIORef (0,0) :: IO (IORef (Int,Int)) &gt; let getPlayerPos = readIORef playerPos &gt; let wrapScript script = "(\\getPlayerPos -&gt; " ++ script ++ ")" &gt; &gt; import Language.Haskell.Interpreter &gt; :{ | let interpretScript script = do | setImports ["Prelude"] | f &lt;- interpret (wrapScript script) (as :: IO (Int,Int) -&gt; IO Int) | lift (f getPlayerPos) | :} &gt; &gt; let userScript = "getPlayerPos &gt;&gt;= print &gt;&gt; return 42" &gt; runInterpreter (interpretScript userScript) (0,0) Right 42
Thank you very much! 
I wonder if anything's changed with respect to /u/icculus's point of view since he stated that back in 2011
(==) . Numeric.IEEE.succIEEE
&gt; depending on whether it is in memory, in the x87 register stack, or in one of the SSE/AVX registers. Doesn't matter. It will be forced to vanilla IEEE when you convert it to int.
Oh my, it's possible to do multiline ghci inputs? This will change my life!
Wait, with a 2.5 (2 point 5) kloc app you are getting a 30 second reompilation hit *and* Haskell isn't recompiling the whole world? (i.e. is only recompiling what changed). Granted not all apps are created equal. In Scala land if you abuse/push the type system to its limits then a small application can take a relatively long time to compile. Presumably Haskellers don't bother building until they've gotten to a point where they can no longer keep in their head the set of made changes and need to see the result; rinse/repeat until deployment. Anyway, kudos for hanging in there, Haskell must deliver hugely at the language level to keep the hair in your head ;-)
the lack of currying is [explained here](http://cdsmith.wordpress.com/2014/06/25/big-changes-coming-to-codeworld/)
Thanks for posting this. Rolling your own compiler seems crazy to me but if you need it you need it
Maybe you didn't disable optimization? (cabal builds with optimization by default). When fixing type errors, you can get much faster turnaround time if you use the repl and then only do `:r`. If you use emacs with haskell-mode, you can even run the repl inside emacs and get jump to error line! 
In the example 'workflow' I gave, none of the issues were type/compile errors. You're absolutely right that just checking for types/syntax is significantly faster, and I know about the various editor integrations and flymake type setups. They help, but if you're i.e. trying to optimize code you have to compile with all optimizations enabled to benchmark the results. Also, if your app does a fair amount of computation, running with optimizations disabled often causes enough slowdown to negate any benefit from the faster compile time.
I really hope so. I always assumed floats are IEEE-encoded when inspecting them for that kind of bitwise hackery and never had a problem.
Yes the point would be to be able to change the state of the running host app or cause some other side effect dynamically through scripts so I don't think importing package would work as the definitions would be separate instances from the host app unless maybe the package was built as a shared library/DLL although I'm not sure about that. The idea of using parametrized lambda strings is sort of okay but does seem like almost a stringly-typed solution and would need more work to generalize like you'd have to parse the script and find any references to those host app "exported" functions/actions and do the mapping. I can't imagine this working well with file scripts and say if you wanted to support some of the functionality of ghci like :t to get the type of a host app function. You was saying that this is one way to do it do you know of any other ways? 
Yeah I thought about that but I'm not sure if this would work with packages that didn't contain only pure functions &amp; type declarations. The point of having an embedded scripting language is to able to change the state of the host app or cause some other kind of side-effect. If the package had internal mutable variables, I think the one that the host app linked to and the one that interpreter uses would be different instances? I think maybe there would be a way to share state by building the package as shared library/DLL but that solution isn't the most ideal.
Worth noting this is not just correct but handles infinity and NaN better than any nonsense hackery.
There is an isIEEE member of RealFloat you can check.
Those affect the dependency solver in cabal. If GHC itself is eating a gajillion bytes of memory it's something else.
Ah well, in that case, there is not really much you can do. I also had that problem with an application I wrote, where one module which contained some TH-generated lookup tables (256 entries =&gt; big case expression) took over a minute to compile with optimizations (I think GHC has problems with big case expressions. Libraries dealing with large ASTs such as haskell-src-exts also have that problem iirc). 
To get cabal to load, try --ghc-options="-fdefer-type-errors"
Yeah, including Lennart who has written a couple of Haskell compilers in his life and has been around pretty much since Haskell was born I think?
I think you may have submitted this to the wrong subreddit.
He wrote Haskell compilers before Haskell was born...
You might not want to check out how succIEEE is implemented then.
Unfortunately it's implemented in C. 
It is crazy, an I was sitting next to Lennart when he did it. There are good reasons why we did it, but it was about trade offs and compromises, not the ideal solution. 
Doh! thanks for noticing
Now this sounds like a Chuck Norris joke =)
I had a similar problem, except for me it suggested adding `--force-reinstalls` to fix the issue, which as it turns out causes the install to jump out of the sandbox and break all your packages.
Why wouldn't it work? The host app first loads the scripted code and then *runs* it, and there you get all the effects. E.g. assume the context of your app is captured by a type newtype App a = App { runApp :: StateT AppState IO a } deriving .... and that this `App` type is in a package that the interpreter can load. Then your scripts could be Haskell values of type `App ()`. You could load them and execute them with something such as (untested): executeScript :: String -&gt; App () executeScript s = do action &lt;- liftIO $ runInterpreter $ do setImports ["App.Monad"] interpret s (as :: App ()) action -- --&gt; here you get all the effects
&gt; If the package had internal mutable variables How? I can only think of two ways to construct mutable variables in Haskell: via an `IORef` (or some other reference), or via FFI, to use the global variables from another language. But as I was trying to explain in my other comment, you can't define a global `IORef` in a module, so it doesn't matter whether that `IORef` would be shared between multiple clients of the module or not. The closest you can achieve is to define an `IO` action returning a new `IORef`, but then of course executing this action twice would yield distinct `IORef`s; again, whether the same instance of the module is shared between multiple clients or not. Now that I'm thinking about the problem further though, maybe we can use `unsafePerformIO` to produce a top-level `IORef`? Let's see if it ends up shared. -- GlobalVar.hs module GlobalVar where import Data.IORef import System.IO.Unsafe {-# NOINLINE globalVar #-} globalVar :: IORef Int globalVar = unsafePerformIO (newIORef 42) Creating a .cabal file, installing hint and the resulting package to `~/.cabal`... Okay, now we're ready to test: $ ghci &gt; import Data.IORef &gt; import GlobalVar &gt; writeIORef globalVar 43 &gt; &gt; import Language.Haskell.Interpreter &gt; :{ | runInterpreter $ do | setImports ["Prelude", "Data.IORef", "GlobalVar"] | action &lt;- interpret "readIORef globalVar" (as :: IO Int) | lift action | :} Right 43 Looks like the module instances are shared after all! *edit*: but that was only because I was inside ghci. When I call `runInterpreter` from a compiled program, the module instances are indeed separate.
`hint` was already mentioned by others, but I thought you might find my shenanigans helpful. A simple module to try it out module TestHint(run) where import Prelude pow :: Int -&gt; Int pow x = x * x run :: Int run = pow 2 The module for running the interpreter module Test where import Language.Haskell.Interpreter testHint :: Interpreter String testHint = do get searchPath &gt;&gt;= liftIO . print set [searchPath := ["."]] loadModules ["TestHint.hs"] setImportsQ [("TestHint", Just "T")] getModuleExports "TestHint" &gt;&gt;= liftIO . print eval "T.run" run :: Show a =&gt; Interpreter a -&gt; IO () run i = runInterpreter i &gt;&gt;= print HTH. 
these aren't subsets but possible splits into two sets (ok one component will give you all subsets) It's a basic recursive algorithm - here is a simpler version for just the subsets: subsets :: [a] -&gt; [[a]] subsets [] = [[]] subsets (x : xs) = let parts = subsets xs in [ x:ys | ys &lt;- parts] ++ [ys | ys &lt;- parts] there are two cases: - the list you are looking at is empty - well obvious there is only one subset - the empty set/list (so return `[[]]`) - the list is non-emtpy in the second case it looks at the first element of the list and then recursivley creates all subsets of the remaining list (the tail) - and then there are two possibilities: either the first element `x` is into a subset or it isn't - so the result consists of two parts: each subset from the recursive call **with** `x` and each of those again **without** `x` - these are exactly the parts that gets concatenated `++` (here in the list-comprehension form) The code you gave just consists of a slight alteration of this - the two parts will assing `x` to either the first element in the tuples or the second.
I tried this with gelisam's [example](http://www.reddit.com/r/haskell/comments/2dsh1d/embedding_a_haskell_interpreter_in_a_haskell_app/cjswd4q). I made a tiny library package which contains an exported global IORef, package imported in the host app and used in a tiny script to modify the variable. It did not modify the version linked in the host app, it was being treated as two separate instances.
Oh, good point: I didn't actually ever get to run `yesod devel`. This error output is all from trying to get dependencies installed. I should edit that to make it less confusing. 
Hi I wasn't so much as looking for an interpreter library but more about how to deal with exporting a subset of the host app's functions/actions to scripts to modify the host app's state or cause some kind of side-effect in the host app like what other scripting languages for apps are used for rather than purely to evaluate Haskell expressions in isolation. 
Yeah, I'm starting to understand, thanks
Great, thanks for replying.
Hm.. I think you could do this: data AppState = AppState .. deriving (Read, Show) deserializeAppState :: String -&gt; AppState deserializeAppState = read serializeAppState :: AppState -&gt; String serializeAppState = show Then export your app API in modules and load them into your interpreter (`loadModules`). Your interpreter needs then serialize your application state, run the script and return it. Deserialize the AppState and you're done. If you want, I can come up with a more complete example. 
Are you certain that you provided the actual command you used in the original text? Your `cabal` output says: Backjump limit reached (change with --max-backjumps). But if you actual compiled with `--max-backjumps=-1`, that error should never occur.
That sounds like a cabal-install bug to me. Can you open a bug report for that?
I don't have a concrete application this is more hypotheticals and some ideas so don't worry about it. The only problem I can see with this solution is scaling up to larger app state. Imagine something like a 3D modeller with a embedded scripting support, it's not unusual for a user to write a script which does some sort of transformation on a 3D model represented as a triangle mesh already loaded in memory. With this solution you could end up serializing/deserializing quite a large amount of data. 
This totally worked. Honestly, I'm not sure why it didn't occur to me to try a simpler install command, but in hindsight, it seems like the obvious thing to do. I think I was actually frustrated by repeated failures to install stuff. Of course, package management is very tough, but I have been frustrated by cabal before. At any rate, I thank you for the suggestion.
Yep, for large states and/or often calls to the interpreter this won't scale well. Keeping your state in an IORef as suggested by others will very likely be more efficient. However, if you'd need to decide for something more concrete, I'd recommend evaluation (i.e. benchmarking) anyway. :)
I actually tried the original command and then I tried it without `--max-backjumps=-1` and I received the same output in both cases. I can do it again to demonstrate.
It's more a question of interfaces. Hard to argue bit-fiddling should consistently work outside of modules specifying the bit level representation, precisely because corner cases like NaN and infinity exist. 
Lennart wrote the first Chuck Norris joke? TIL!
I am currently writing a blog post on how to resolve all these issues. On mac, 1. install Haskell using the following script (don't forget to clean your install, rm -rf ~/.cabal ~/.ghc and uninstall ghc/cabal before launching the script) https://github.com/yogsototh/install-haskell 2. Then do cabal install yesod-bin yesod init my-project Then don't use the command given at the end. Don't use cabal sandbox. Instead do cd my-project cabal install --only-dependencies --enable-tests . yesod devel 
He may well have. He's done a lot of stuff. ;)
Yep. I don‚Äôt know why this isn‚Äôt a more widely taught feature. Unfortunately, the editing and history are still line-by-line.
Is there a way to use it with wai/warp? Something similar to wai-websockets interface where I could just combine my Scotty server with your socket.io using runSettings?
Checking the issue tracker it seems more likely that I ran into [this](https://github.com/haskell/cabal/issues/2004) bug actually. I had initialized a sandbox above the project directory to install yesod-bin in.
Well written, and I like the use of type holes to show how this great new feature of GHC can be used to assist in writing code. But what about the `MonadTrans` instance with an implementation for `lift`? I feel that would really tie up the article nicely. 
My app works that way, you talk to it over a socket. I originally used hint, but switched to the GHC API when it turned out to be easier and faster (hint wanted to reload everything on each expression). The API is a module that imports a lot of internal functions, but it also imports some other modules interpreted, so you can write longer functions in a file, and shorter ones in the REPL. The interpreted modules wind up being a toolbox of functions useful in the REPL.
Where is the tutorial? I only see the examples.
According to the last paragraph of the article, `MonadTrans` will be in the next article in the series. If this one was anything to go by, it should be good :)
 &gt;Unfortunately, the editing and history are still line-by-line. There's your answer. Once you start entering multi-line definitions, you are probably better off typing them into a file. 
Natural language is great for teaching semantics. Code is better for teaching syntax. I listened to a haskell cast recently where Don Stewart talked about how programmers learn haskell very quickly in production environments when they can just look at the code that's already been written. But as reddit haskellers we often don't have that luxury. This post goes to an extreme by removing natural language entirely, but nonetheless tells a story through the progression of examples. After writing this code over a few hours today I made the tutorial in 10 minutes. I'd love it if someone went through and did something similar for lenses, postgresql-simple, scotty, and all the other libraries I'm trying to learn. 
You use `ghci` until your code type-checks and then you compile with `cabal`.
The difficulty in writing a parser for me is rarely parsing correct syntax ‚Äì it's rejecting invalid syntax. This parser happily accepts a bunch of invalid numbers (such as XXXXVVVVVVIIIII). Could someone expand on how you normally reject invalid syntax? Do you run a separate pass where you exclude invalid syntax, or do you try to restrict the valid parses in the parser?
Just for fun: source in CodeWorld: http://codeworld.info/#P5rG3HT7jd4LFLaACiz5Mrw== :) Another tree generator I wrote a few days ago, too: http://codeworld.info/#PrF9cPa97GYm_ftkUPJs1MA== 
I explained how Haskell functions work to a 14 year old and they got used to lack of parens almost instantly. I helped a 15 year old with his Scheme homework and he adjusted to the syntax difference very quickly (and favorably). 
But that's never the bottleneck (OK, at least not for me). I can just compile with -fsyntax-only and -O0 and such, and getting a type error back is much quicker than a full build anyway. The problem is if you actually need to run your program to keep working on it. Like, when your optimizing and you need to run your benchmark to know if what you did worked, or your working on something graphics related and need to see the result. For the former, you obviously need all the slow -O2 -fllvm and such flags to measure, for the latter, any time gained from faster, unoptimized builds is often lost once you factor in the slower startup and runtime from your unoptimized code.
I've got my ghci set to automatically use multilines. So if I type a `let` clause, for instance, hitting enter will take me to the next line rather than running the line. I just hit enter twice if I know I'm not going to need that. If you want to try it, just put `:set +m` in your `~/.ghci` file.
Hello everyone! I've just released a tool that should be of help to those struggling with doc lookup in IDE's while working on Haskell projects. This is a pre-release on github, as I'm hoping to get some feedback from you before it goes to Hackage. Feedback or code-review (via comment, issue, or pull-request) is welcomed and encouraged. 
Yes, I know -- that's why I said: &gt;However, if you are using this to control for rounding, flushing an 80-bit register-resident double to memory (which means rounding it to 64 bits) will obviously not yield good results.
Sweet, although it'd be nice if it could automatically set the package scope by searching the current directory and each parent directory for a .cabal file. 
I notice that the Haddocks aren't showing up for 0.10.*
[Repost?](http://www.reddit.com/r/haskell/comments/2bhdvs/building_monad_transformers_in_haskell_part_1/)
Wow, that's a quick turnaround.
Chuck Norris is so fast, he can run around the world and punch himself in the back of the head.
We have quite a few resumes to go through already. To make the first round it is best to be early. That said, we regularly have open positions, so feel free to contact me at any time.
Note also https://twitter.com/PLT_Hulk/status/276723978234646529
Great to see that the team is growing! I miss Haskell.
It does complicate things occasionally yes, 30 is not really all that many (we have 73 explicit dependencies at the moment, 205 total). Perhaps obvious, but: * use cabal sandboxes * use cabal freeze * use cabal sandbox add-source (for your own modules and forks) * watch out for packages that depend on c libraries It's definitely a bit tricky to deploy with a heroku buildpack too (if you go that route and have trouble send me a private message, I can give a hint).
PS That said, I've had 85 explicit dependencies on a NodeJS project. I'm sure that's still quite small compared to many other code bases.
As an additional question, can you see how `partitions` differs from: partitions' :: [a] -&gt; [([a], [a])] partitions' [] = [([], [])] partitions' (x : xs) = do (ys, zs) &lt;- partitions' xs [ (x:ys, zs), (ys, x:zs) ]
If you use multiple modules (files), ghc --make (the default) only (re)compiles the ones that have changed or have changed dependencies. I would consider that incremental compilation, ala SBT.
I shall answer your call.
It depends on your target audience, but in the case of [language-puppet](http://hackage.haskell.org/package/language-puppet) I just distribute pre-compiled binaries ...
could you open a ticket on the haskell-platform github project issue tracker and/or email the haskell platoform maintainers?
He asks for comments in his comment section but blog comment sections are interminable to use. &gt; Let‚Äôs say you want to create a constrained monad type class, in which point and bind may be called only with types that satisfy a given type class. How would you express this in Haskell? You can‚Äôt (at least not easily), because type classes are magical! Is this not doable with a `class Monad m =&gt; RestrictedMonad m`? &gt; In Haskell, the community‚Äôs preferred solution is to create wrapper types (newtypes) and define the instances on the wrappers instead. You can then ‚Äúforce‚Äù the compiler to choose the instance you want by using the right wrapper type. It works, but it‚Äôs totally ad hoc and relies on convention rather than compiler. I would like to see some justification for why this approach is "ad-hoc". He just dismisses it without explaining why. &gt; If you emulate type classes using ordinary values, then while you gain a whole host of benefits, you also lose some. I don‚Äôt personally think the tradeoff is worthwhile. Again, he cut short of actually explaining why. &gt; No Newtype Abuse. There‚Äôs no need for the ad hoc practice of abusing newtypes to force Haskell to select the ‚Äúright‚Äù type class instance. (In fact, I‚Äôd like the compiler to forbid this abuse of newtypes for instance selection.) Why is such use "abuse"? Why ad-hoc? Why forbid it? Articles should contain reasons for views. &gt; Probably named instances (which, for example, Purescript and Idris already support!). I would be interested him exploring the (previously mentioned in this comment) Scrap Your Typeclasses approach. The laws as a set of quickcheck properties is a fun idea. It could be neat if the compiler could reject instances that don't satisfy laws. Though, I wouldn't restrict such functionality to just type-class instances. Why not any function?
e.g., image processing.
Yeah, in that case there's not much you can do with the current tools. `cabal` does do incremental builds, but I assume that you are compiling something that is deep up the module chain and forcing recompiles of everything else.
It'd probably be worth while to show how to exclude parsing some varieties as an exercise, but ancient usage of the numerals varied widely according to Wikipedia.
&gt; It could be neat if the compiler could reject instances that don't satisfy laws. Reminds me of [this](https://github.com/pseudonom/phantheck) interesting approach to checking QC properties at compile time.
&gt; &gt; No Newtype Abuse. There‚Äôs no need for the ad hoc practice of abusing newtypes to force Haskell to select the ‚Äúright‚Äù type class instance. (In fact, I‚Äôd like the compiler to forbid this abuse of newtypes for instance selection.) &gt; &gt; Why is such use "abuse"? Why ad-hoc? Why forbid it? Articles should contain reasons for views. It does seem to be to be an "abuse" to have to introduce a new type just to resolve instance selection. It's not clear exactly what it's an abuse of though. Maybe me.
As for your question about deploying code, I (and I think most people) only deploy binaries. You build the binary in a VM or otherwise identical system as production, and then you push just the binary. All the haskell dependencies are linked statically, so the only things you need on the production server are standard C libraries (libgmp, libffi, etc). They should be the same versions, but that's why you build in a similar environment.
Using a proxy or similar? It might be because of https://github.com/haskell/HTTP/issues/68 
My mistake. I searched for the link and didn't find it. I'm sorry. 
~~Blog post says Haskell has first class typeclasses. Haskell most definitely doesn't have those though, you cannot take type classes as parameters or yield them as results; Template Haskell fills the void when typeclasses have to be generated.~~
Sounds like SBT sub projects, an absolute must for any non-trivial app, cuts down hugely on incremental build times.
Nevermind. I'm not mad. :) Hint: Reddit should tell you that the link was already submitted iirc. Also, if you click on the grey link next to the article ("jakubarnold.cz") you'll get to [this](http://www.reddit.com/domain/jakubarnold.cz/) site where you can what got already submitted.
haskell platform is supposed to just work‚Ñ¢, and this is especially important for windows! it could be a network connectivity issue as someone else here mentioned 
Not having looked too closely yet, what's in here that's not already in the Dash system for Haskell, which I use all the time?
&gt; So even though I couldn‚Äôt create two `Monoid` instances for `Integer`, one for addition and one for multiplication, I would be allowed to create two classes that derive from `Monoid` and define instances of each class for `Integer`. That‚Äôs not the right approach. The monoid of integers under addition (‚Ñ§,+) and the monoid of integers under multiplication (‚Ñ§,√ó) are not ‚Äúsubmonoids of the integers‚Äù. (There is a concept of submonoids, but it‚Äôs not relevant here, and I think it would only be relevant in a dependently typed language.) The very concept of a monoid is as *parameterised* by a function, so you want to be able to refer to functions at the instance level: class Monoid a (mappend :: a -&gt; a -&gt; a) where mappend mempty :: a law leftIdentity = forall x. mappend mempty x = x law rightIdentity = forall x. mappend x mempty = x law associativity = forall a b c. mappend a (mappend b c) = mappend (mappend a b) c instance Monoid Integer (+) where mempty = 0 instance Monoid Integer (*) where mempty = 1 The same goes for other algebraic structures: instance (Monoid a add, CommutativeGroup a mul) =&gt; Ring a add mul where law leftDistributivity = forall a b c. mul a (add b c) = add (mul a b) (mul a c) law rightDistributivity = forall a b c. mul (add b c) a = add (mul b a) (mul c a) 
I misunderstood the blog post, he in fact says the exact opposite
I view it in the opposite way: it's just encoding *more* things in the type system. Don't think of it as just "resolving instance selection" but as changing a property of the type. This makes the most sense with things like `Ord`, where you're specifying a new canonical ordering, but it still makes sense for other types. It gets a bit weird with things like `Monoid` where there is no reasonable notion of "canonical", but I think that's just a practical tradeoff and not an "abuse". Personally, I like this more than using a module, where the underlying function is actually independent of the type. So you can actually have two different versions of `foo : a -&gt; string` *for the same a*. Not so great, in my view. It's just having more stuff in types.
For my (usually proprietary) Haskell apps, I haven't had a need to care directly about the number of dependencies (although I do care about their quality, and a big number of them makes it harder to assess the quality of each one). For my open source projects (which happen to be mostly libraries) I try to reduce the number of transitive dependencies. [Here's an example of how I do it.](http://ro-che.info/articles/2014-03-03-tasty-0.8#dependencies)
It should be a combination of Prolog, Plankalk√ºl and Perl (obviously).
There is some work to replace it: https://ghc.haskell.org/trac/ghc/wiki/LightweightConcurrency
They are mentioning this topic in one of the haskell casts, I think the one with Simon Marlow. From what I recall, the problem was inter-thread locking and access. There was an attempt to rewrite it in Haskell, but that threading issue complicated things a lot.
&gt; Let‚Äôs say I want to take two type classes, and programmatically combine them (and possibly one or more values) to create another type class. class Foo a where foo :: a class Bar a where bar :: a class (Foo a, Bar a) =&gt; FooBar a where ta-da
The idea of having quickcheck-integrated "laws" associated with type classes is fascinating. I wonder if LiquidHaskell could be used to accomplish the same sort of thing.
&gt; The role requires physical presence on the trading floor in London. Remote work isn‚Äôt an option. Aw. :( Telecommuting needs to get way more popular.
I get it now and I think this is a pretty cool solution :) I have a few Qs: * [elaforge mentioned something about hint reloading everything for each expression? ](http://www.reddit.com/r/haskell/comments/2dsh1d/embedding_a_haskell_interpreter_in_a_haskell_app/cjtavh4) * Does Mueval support coercing to values out the box? 
&gt; You was saying that this is one way to do it do you know of any other ways? &gt; &gt; &gt; &gt; I would recommend to use the other way I suggested: to expose a function which accepts the IORef as an argument. One indirect way to do this is to hide the IORef inside a ReaderT or a StateT, as suggested by /u/jcpetruzza . When it clicked in my brain I prefer this solution a lot, lot more
Do you mean `-fno-code`? `-fsyntax-only` seems to be a GCC thing (from googling).
I'm not sure what you mean about 'dash system'? One impression that I got from Dash.app, and the current public docset repos (e.g. http://sanfrancisco.kapeli.com/feeds), is that with exception of some fixed version core packages that have been combined for Haskell , there wasn't really a solution handling the *arbitrary package* dependency case confronted by Haskell developers in reality. Having only a finite, non-approximal group of docsets to choose from really just wasn't going to cut it. So that is what dash-haskell is about, although it is still rather naive with respect to all the other dash docset tools out there.
I would love to see this cast. Would you happen to have a link? I wasn't able to find the exact one.
Wow, this looks like a great effort. They really hit the hardest issue first. I would expect work to happen on bit wise operators prior to Concurrency. I guess no one ever accused a haskeller of being afraid of complexity :)
Ok, as a long time dash user, I'll look forward to this
I use Vim conceal mode occasionally, but one thing I've found that consistently bothers me is indentation. Since `-&gt;` and the unicode arrow take up different amounts of space, you end up having to mentally adjust indentation when writing. And then the indentation looks weird to someone who *isn't* using a conceal-enabled editor! I'm not sure how you can solve this though. As a result, I'm not convinced conceal is a good idea, even if it is convenient and pretty.
I agree, I had to stop using it because of that. This issue is supposed to be easy to solve in Emacs, but that turned out to be tricky also.
There is surely a problem* with indentation, I should mention it in the issues section. On the bright side, when I read terse functional code, I prefer to have somewhat clunky indentation, instead confetti of special characters. :) In the end, it is true that you have to always switch concealing on and off to be sure that the code is OK. * I'm actually recalling I saw a fork that tried to solve arrow indentation issue for vim... I'll try to incorporate it if possible.
base-4.7.0.1 should be part of packages distributed with ghc-7.8.3, although I would really like to know if it works with older versions of base. Try relaxing the versions of base and ghc in *dash-haskell.cabal* to see if you can get it going. Unfortunately I don't have a proper setup to test against a good variety of ghc and base versions.
Oh I'm on 7.6.3. I should probably just update GHC
Well, it does not completely solve the issue. It just tries to not break indentation for most common cases. E.g. '-&gt;' will become '‚Üí&lt;space&gt;'. Same for other arrows. Once you use something longer, e.g. 'forall', things will break again, and preserving space for long words won't look quite good. Alas. I wonder if emacs users found anything better than that? Anyway... I found the commit that I was looking for and checked-in into Conceal Plus bundle... Spaces for arrows and :: are now preserved by default.
Thanks for the link!
I can't help but feel a bit... underqualified. Good luck to all applicants! Working in Haskell would be an amazing opportunity for sure.
Code like this is fairly common: map :: (a -&gt; b) -- ^ A function -&gt; [a] -- ^ A list of valus -&gt; [b] -- ^ The output map f xs = ...
The author of this blog is another victim of the Dunning-Kruger effect when it comes to math. 
They get conflated often enough, particularly by users that advocate for spaces everywhere instead of using tab characters. I do find that alignment can help readability in some cases, but it is often a pain to maintain, and can make patches really unnecessarily ugly. I *still* find myself doing it sometimes in Haskell, but yeah, normally a bad idea.
I hear that a lot, but I have never seen a way to stop doing it. It's too useful for readability, alignment.
this is presumably not "programmatic"
I actually have had trouble with the Heroku buildpack‚Äì no stdout/stderr output was being captured. Have you seen that issue?
To me that usually indicates that you could split the project into several packages that only really need a subset of dependencies. For me, I have a project that interacts with several web APIs, for example, so I had http-client &amp; http-client-tls as dependencies. What I have done for those pieces of code is to move them into API client libraries, since they're somewhat standalone. Pulling little things out like this lets me shove more of the dependencies out of the main package, and really just import the functionality that I care about. You still get the dependencies transitively, of course, but at least it's factored a bit better.
I had that problem! you've got to change the buffering setting for IO; I think it's something like hSetBuffering or something, but I am on my phone right now.
I think there are "abusive" cases of `newtype`, but mathematically speaking: monoidal Integers under addition and monoidal Integers under multiplication are two very different objects, and so it makes sense for them to be different types (IMHO).
I think I remember a similar article in The Mathematical Experience by Davis/Hersh.
first comment on this side ... most likely trollish ;) 
Ahhh that actually makes sense. So he's saying that classes are not first class values that can be combined with a combinator. I believe that with constraint kinds this is actually possible. You could write something like: type Both c1 c2 = (c1, c2) Also... something something type families.
[This recent post](http://degoes.net/articles/principled-typeclasses/) reminded me of a paper I read last week, and a search didn't find it ever having been posted here. So hey! Sharing.
&gt; if you click on the grey link next to the article ...wow. I always just assumed that went to the base site. Because I mean, that's what the link text says, and there's not even a hover tag to tell me different...
Don't be an ass.
‚ÄúThat's a bad idea‚Äù is the excuse tool writers use for edge-cases they don't want to support. I say the same thing when talking about structured-haskell-mode.
I'm way more comfortable with this stuff than looking at the current RTS C code...
Seems a bit pointless to me. There's a similar mode for Emacs. It looks a bit prettier, but complicates everything else. "Long" code doesn't bother me when I can edit it easily. However, I approve of the editor support rather than people writing UnicodeSyntax directly in the file, which alienates anyone trying to read and edit their code.
For "-&gt;", "=&gt;" the resulting arrow in emacs has the width of two characters, which I think looks good.
Thais true for monoids, but what if you climb higher to rings? Will you have a separate type for them, too?
Maybe you want this? convertString intsIn = let s = [convertChar x | x &lt;- intsIn] in if and [isJust x | x &lt;- s] then Just (catMaybes s) else Nothing
I'm nowhere close to being able to answer that, I just wanted to see what kind of discussion arose from this since there was a lot of interesting commentary on the other one. :)
I don't think the syntax _has_ to be awkard. Haskell is quite flexible, so if you're unhappy with how it looks, you can always add an operator and change your naming conventions, e.g. dog # getLeg 1 # getToe 2 And for the more general point: I'm afraid I'm not entirely sure what the phrase "object-oriented" means here, as it's an overloaded phrase that means very different things to different people. I sometimes write code that looks vaguely like both your examples, but I'd argue that the former example is not "object-oriented" at all, and the latter example is only object-oriented according to [some definitions of the term](http://www.cs.utexas.edu/~wcook/Drafts/2009/essay.pdf) and [not others](http://userpage.fu-berlin.de/~ram/pub/pub_jf47ht81Ht/doc_kay_oop_en). The Haskell language provides various ways of accomplishing what objects (purport to) do in other languages: you can selectively export functions, types, and constructors in order to provide a public interface to achieve encapsulation (as in your first example), and build lots of small functions with well-defined semantics in order to increase modularity, and use typeclasses or higher-order functions to implement ad-hoc polymorphism. So it's not unreasonable to encounter a program that doesn't use any kind of "objects" at all and yet still features the advantages attributed to object-oriented design.
Parts of the runtime system could be written in Haskell. But other parts, like the GC, has to be written in different language since there's no way to write Haskell that is guaranteed not to allocate.
&gt; Is this not doable with a class Monad m =&gt; RestrictedMonad m? No, but it _is_ doable with constraint kinds and associated types.
Getting the laws associated with the type classes should be doable today using annotations. https://ghc.haskell.org/trac/ghc/wiki/Annotations The challenge I guess is to write the ghc module that does something useful with those annotations. One possible mechanism is to define "hidden" functions in the type class dictionary. These functions are not available normally, but available under some special circumstance. Then quickcheck code could be hooked into such hidden functions by, say a ghc module, and the compiler could invoke these hidden functions in various places.
There's a bit of topology in it too, maybe? The domain of one map has to be the codomain of another - could think of this as a way for functors to "respect a topological structure". I approached categories from the algebraic viewpoint mainly, as topology was more of a weak area for me. But with simplicial methods in higher cats it seems like... maybe there really is some topology going on? *shrug* This isn't disagreeing with you at all, if it's even meaningful at all. (I'm still a bit too Fraggle for all this meaningful stuff, but I'm working on it!) Btw, "structure" in algebra brings to mind stuff like modules over principal ideal domains. In that case there's a so-called "structure theorem", which comes about by some sort of 'generalized fooling around with matrices'. Importantly, it gives a nice canonical decomposition in terms of some particularly simple examples. Seem to remember that... this might involve transfinite induction. But there should be plenty of other decomposition theorems around with no TI. (&lt;-- very Fraggle, sorry) So the idea is... a homomorphism preserves all this stuff too, and then the whole "structure" thing is also the nut inside the shell. (The Yoneda lemma was once the slogan of Noether and Artin.) The shell might be ...er, coordinates? Gah, I dunno. I'm in the 4th chapter of LYAH, but convos help. I'm also taking a break from the book cause I have the flu :-/ Seemed like this all going somewhere and was gonna make sense, lol. 
But you may have to write dog # getDogFoot 1 # getDogFootToe 2 cat # getCatFoot 1 # getCatFootToe 2 Or you'll need to write some boilerplate like class GetFoot animal foot where getFoot :: animal -&gt; Int -&gt; foot instance GetFoot Dog DogFoot where getFoot = getDogFoot ... 
Regarding hint and reloading, it follows the same policy as ghci. Roughly, every time you use `runInterpreter`, it is equivalent to running ghci from the command line, so packages will be loaded. If you use `loadModule` to load a file `F.hs` (similar to `:l F.hs` in ghci) and later you modify `F.hs` (i.e., if its last modification date changes), then next time you try to interpret something, you will see a reloading.
But how? What design pattern do we use to break down bigger problems into smaller ones? Is there a totally different way?
Why are you using a `elem` [0..25] rather than a &gt;= 0 &amp;&amp; a &lt;= 25 ?
See [the Typeclassopedia](http://www.haskell.org/haskellwiki/Typeclassopedia) for some examples. e.g. where in an OOP language you would use the composite object and null object patterns, in Haskell you often implement `Monoid` instead.
You are doing it wrong. You don't need OOP. In order to understand that, you have to forget about it and learn Haskell, like if it never existed.
You'll need to learn the appropriate abstractions ("design patterns") for Haskell. However, it's not that strict: Haskell is very flexible, so multiple approaches can be used to solve the same problem; the only question is where the trade-offs are (readability, maintainability, extensibility etc). There are a few approaches: - You could go find packages that solve a similar problem and see how they attacked it. - Or you can come here and ask a specific question! --- Some comments in regards to the example you've shown: 1. Using `IORef` for counters is quite natural if you want to keep track of a long impure counter. However, abstracting under a data type seems quite overkill unless you want to add additional, non-trivial functionality (i.e. takes more than a line to write). Right now, you are writing more words just by unwrapping/wrapping the data type! If you really want a separate type for it, a `newtype` would be preferable here. I would probably just use a plain `IORef Int` myself, or define an alias `type Counter = IORef Int` if I want the types to be self-documenting. 2. This one is even more overkill. Wrapping entire `IO` actions in a data is useful only if you want to alter the behavior later on *dynamically* (at run time). This can potentially hurt performance. And if you don't expose the constructor for `Counter` publicly, then the solution is semantically isomorphic to the previous solution, except possibly less efficient. (Now I realize this is probably just a toy example for you to play with, so the critique may not apply that well. Take it with a grain of salt.)
Related and solves the indentation issue: http://www.reddit.com/r/haskell/comments/23g9dv/oc_haskell_programming_font_with_ligatures/
*It was OBP I tried, not OOP -- No inheriting, virtual function stuff.* *I don't use them unless the API demand.* When I was learning Haskell, I wasn't bearing OOP/OBP in mind. I thought OOP/OBP are for imperative languages. But when I then try to write some bigger / realtime-based programs, code gets complicated and hard to read. Things like video games, web servers, and those that involves data structures like hash-table, linked-list when in imperative world, cannot be done in a purely functional way without extra time-complexity cost. (except tree structures) So OBP is one of the solution I tried.
If you want to discard any Integers that aren't part of your encoding, you can use `mapMaybe convertChar intsIn` (you have to import `mapMaybe` from `Data.Maybe`).
There has been some work done to address that issue, see [OverloadedRecordFields](https://ghc.haskell.org/trac/ghc/wiki/Records/OverloadedRecordFields).
I don't really use OOP, to be honest, I hate those virtual function stuff. The usabiliy is OK, but readability is not. The reader need to jump between files very often, which makes code reading less efficient. Rather than virtual functions, I prefer "switch case". So thank goodness, Haskell-ers have ADT. 
You probably need to recompile vim with the "big" feature set. sudo port install vim +big (The vim that comes with Mac OS X does not have the "conceal" feature, nor does the MacPorts default variant.) PS. iTerm2, use it!
Modules are the way you split things apart. You have fine grained control over what things to expose from a module and how many to import from one. Everything can be done in a purely functional way without extra time-complexity cost, in functional languages like Haskell. Often times the time complexity is actually lower than the one you would expect, given lazy evaluation. If you don't appreciate functionality separated from data, interface separated from types, modules and lazy evaluation, maybe you are not ready for Haskell yet.
&gt; The major red flag I see in your example is the IORefs ‚Äì you should probably &gt; try to rephrase your example so that it's not using mutable state. I know I don't really need "IORef" in something as simple as a "Counter". But what if this is something like a hash-table? For example, in a long-term running key-value service that supports inserting, deleting and searching? You can't implement this with something like a immutable hash-table without extra time cost, right?
&gt; What design pattern do we use to break down bigger problems into smaller ones Functions.
As others have said, you need to forget the OOP mindset to write idiomatic haskell. Dont worry about ugly syntax, because of haskells referential transparency its pretty much always possible to write combinators that makes the syntax nice. For your second example, you can get OOP-like access to datastructures by using lenses. Lenses also has some big advantages over access in most other languages since they abide by laws that allows you to abstract out more patterns. Your last example could look something like this using lenses: dog ^. getLeg 1 . getToe 2 Lenses really shine when you have a datastructure with multiple levels of nesting. For example if you wanted a map with two keys that points to a counter, so: type MyCounters = Map String (Map String Int) Writing an increment function for such a structure in javascript would look something like this: function increment (k1, k2, m) { if (m[k1]) { if (m[k1][k2]) { m[k1][k2] += 1 } else { m[k1][k2] = 1 } } else { m[k1] = {} m[k1][k2] = 1 } return m } We have to write a lot of logic for checking whether or not a key is in the map. Some of this logic would have to be repeated in a getCount function. In haskell we could write a lens that points to the specified count: myCountersLens :: String -&gt; String -&gt; Lens' MyCounters Int myCountersLens k1 k2 = at k1 . non M.empty . at k2 . non 0 This code says exactly what it does, it firsts looks for k1 in the structure, if its not there, use an empty map, then it looks for k2 in the submap, if its not there use 0. We can trivially use this lens to create an increment and getCount function: increment :: String -&gt; String -&gt; MyCounters -&gt; MyCounters increment k1 k2 = over (myCountersLens k1 k2) (+1) getCount :: String -&gt; String -&gt; MyCounters -&gt; Int getCount k1 k2 = view (myCountersLens k1 k2)
I had the Emacs mode going at one point. Then I gave a live coding talk at a Haskell meetup and had to field so many questions I just disabled it forevermore. At least, unlike Unicode Mode, conceals don't contaminate your source. But if you're going to share your view you ought to recognize that it's very non-standard.
&gt; Modules are the way you split things apart. You have fine grained control over what things to expose from a module and how many to import from one. I thought that pure function could do everything, and side-effect related code should be minimized. Then I wrote some video game program, in a way like this: runGameStep :: GameState -&gt; UIEvent -&gt; GameState But when 'GameState' gets more complicated, the code became hard to read. &gt; If you don't appreciate functionality separated from data, interface separated from types, modules and lazy evaluation, maybe you are not ready for Haskell yet. What really matters is not whether "functionality separated from data" or "functionality combined with data". It is "functionality related to data-type", which makes 1. documentation eaiser to search (so that you can use auto-completion) 2. namespace easier to name. (so that you could shorten "getDogFootToe(getDogFoot(dog))" into "dog.getFoot().getToe()" 
http://dl.acm.org/citation.cfm?id=1017481 is the original paper by Oleg and Chung-chieh Shan.
I see this, but what if it's not a Counter, but a FIFO queue? if you write something like type Queue a = [a] dequeue :: Queue a -&gt; Queue a dequeue = drop 1 It would be much less effecient. ========================== **Edit on 19th Aug** Sorry I gave the wrong example. This "drop 1" always cost constant time. It's "enqueue" that costs linear time in this implementation type Queue a = [a] dequeue :: Queue a -&gt; Queue a dequeue = drop 1 enqueue :: Queue a -&gt; a -&gt; Queue a enqueue x = (++[x]) And this is the case that "dequeue" cost linear time type Queue a = [a] dequeue :: Queue a -&gt; Queue a dequeue = init enqueue :: Queue a -&gt; a -&gt; Queue a enqueue x = (x :)
Sadly, currently it is not possible to enable both concealing and double-width glyphs in Vim (at least through `ambiwidth`).
Yes, seeing ‚çü instead of `&lt;*&gt;` can be really confusing!
&gt; Also, should I be using Maybe's, or throwing Exceptions? You should be using `Maybe`s.
Not particularly actually, because the list values are shared between the input and output lists we aren't reallocating the entire list every time we pop off an element.
You're assuming an operational model. Haskell is optimized for immutable data operations. It wouldn't be so inefficient as you assume!
What do you mean by "less efficient"?
You may have to write like that. Many people find it irritating but personally I don't find it a big deal.
&gt; But when 'GameState' gets more complicated, the code became hard to read. Perhaps you could give an explicit example.
You're thinking operationally. You're talking about procedures when in Haskell the term "function" corresponds to mathematical functions (although equality is not extensional since two functions which map the same points to the same points can still differ in, say, complexity). The closest thing to the classes of object-oriented programming are abstract data types. In Haskell these appear as type classes. And actually most type-generic reasoning done in research is with the use of type classes (e.g. Monads, Idioms, etc.) not with considering the algebraic structure of the type.
You want an (amortized) constant time FIFO queue? Look no futher than the [Edison library](http://hackage.haskell.org/package/EdisonCore-1.2.2.1/docs/Data-Edison-Seq-BankersQueue.html) or [Okasaki's paper](https://www.cs.cmu.edu/~rwh/theses/okasaki.pdf). While there may very well be a logarithmic cost to performance in strict, immutable languages, it is unknown whether that also applies to lazy, immutable languages, since replacing a thunk with a value is a limited form of (referentially transparent) mutability. Plus, if you avoid things like IORef/STRef/MVar/TVar you get persistent data structures for free.
That's one solution. Another is to use a different font. ;)
If you really want to do OO in Haskell, Look into [OOHaskell](http://code.haskell.org/OOHaskell/) (with [description](http://arxiv.org/abs/cs/0509027)). Even SPJ has done some work with [inheritance in Haskell](http://research.microsoft.com/en-us/um/people/simonpj/Papers/oo-haskell/).
Actually, the study found that competent people were more capable of accurately gauging their own absolute skill level, but systematically overestimated the competence of others. Other studies have made further interesting observations. The effect is actually much more nuanced than the Internet thinks it is. There's a deep irony in overconfident interpretations of the Dunning-Kruger study.
I took issue with a lot of things in this. First: &gt; [...] rather than being based on an axiomatic foundation of first order predicate logic, is based almost entirely on ideas of computability and computation. These are not mutually exclusive. There's no good reason not to employ the predicate calculus except willful ignorance. &gt; The key to Banach-Tarski is that it‚Äôs built on an operation that‚Äôs impossible in reality. This is misleading. It doesn't have the physical model we expect it to, that is all. It only tells us that our expectations are silly and we shouldn't have had them in the first place. &gt; There‚Äôs a pretty strong argument that that entire process is an abuse of the underlying axioms. What. &gt; The constructivists say that things like Banach-Tarski and the Russell paradox are part of the same basic problem: defining things not by how they can be created, but by how they can be described using some fuzzy airy-fairy gibberish derived from axioms. This is so incredibly misleading it undermines the whole thing. Banach-Tarski Paradox and Russell's Paradox. One of these things is not like the other. The latter one describes an inconsistency; the former is something that a group of people say is "unintuitive" mainly because of the language used in describing it. &gt; That the only way you can really consider a statement to be true is if you have concrete proof: the only way to prove that something exists is to show a concrete example of it. This is the reason computer scientists don't care about constructivist nonsense. A theory is useful if it has a model. Being pragmatic we concern ourselves with the useful. If it has a model and can be used for abstraction nobody is going to care if it postulates the existence of an "unknowable". Well, nobody who isn't a mathematician that spends too much time philosophizing. (Sorry if I can't conceal my contempt.) PS Your unapologetic use of acronyms with no definitions is really off-putting.
That sounds like a feature request: constant-space Haskell dialect (or FC or Core...)
http://www.haskell.org/haskellwiki/GHC/FAQ#Why_isn.27t_GHC_available_for_.NET_or_on_the_JVM.3F
I meant cost more CPU time. Did I use the wrong phrase? (Pardon me, I don't speak English natively.)
Would such a dialect be really better than C ?
What is the caret for?