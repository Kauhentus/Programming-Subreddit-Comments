Fixed point in time something something. It's complicated.
Alligator eggs is painful. :(
I'd be happier if there was a way to have an explicitly free type variable (like `const :: a -&gt; _ -&gt; a`). Without that, `IO ()` is much easier to spot, even if it might require some wrapping.
`()` is easier to spot in a type signature than a free variable. That being said, I prefer the free variable. It is not only a matter of convenience. A type of `m a` where `a` is free means *any* `a`. Sure, you can create `()` out of any `a`, but the intended semantics are "evaluate this computation, ignore the value" rather than "evaluate this computation, ensure the value is unit". At the call site, you can still use `void` or `(() &lt;$)` if you wish to be more explicit.
I prefer m a, and that agrees with my general Haskell philosophy of "ask for what you need and nothing more". I find this makes composing things simpler, and that in turn encourages a more modular approach to development.
That's true. If I pass `return undefined`, the type `()` doesn't imply whether the value will be forced or not by the HOF. Whereas with `a', you know it cannot be.
I have developed my own GP package in Haskell (not on hackage) based on [combinators](http://www.cs.bham.ac.uk/~wbl/biblio/aspgp06/fsb-meo-combs.pdf) for my undergraduate fourth year project. In the end I wanted something that worked as I was being graded on it, so simply used Haskell to write C code equivalents of the functions created. However, I don't think it would require much work to interpret the functions in pure Haskell though. But this has certainly motivated me to revisit the project, just to experiment and see a comparison between tree-based GP and this combinator based variant. 
&gt; Whereas with `a', you know it cannot be. You don't really know that unless you are programming in Haskell 1.4.
Also, sometimes when you want `safeHead` you can solve it better with pattern matching. Compare: case safeHead xs of Nothing -&gt; e1 Just a -&gt; e2 a case xs of [] -&gt; e1 a:_ -&gt; e2 a The exception is when you really wanted a `Maybe`
I'm going to buck the trend and say `m a`. The `m ()` solution often requires someone to take a perfectly good monadic action and fmap over it to put a `()` on. If you are taking such an action, it is something you are probably going to be running a lot, and the overhead of that needless `fmap` can be non-trivial for complex monadic actions. Somewhere along the way folks decided they were going to make my code slower by requiring me to put `void` all over it. I'm not a huge fan of this trend.
Did you actually check the core or benchmark? Where is this supposed speed hit coming from? As far as my experiments go,`fmap (const ())` is a no-op.
I agree with this, and will further argue that you should strive to make your functions as general as possible. When `m a` works, `m ()` just adds a pointless restriction. Suddenly, if you users have a non-unit monadic action they're forced to take the extra step of using `void`. Making your users jump through a hoop because it makes the type signature prettier isn't a good trade-off.
Trivia: The type of `forever` is `Monad m =&gt; m a -&gt; m b`. What would be its type if we chose the other convention? Answer: `Monad m =&gt; m () -&gt; m Void`. In the contravariant position (argument of a function), we use the fact that `()` is the terminal object (i.e. we always have a function `a -&gt; ()`). However, in the covariant position (result of a function), we use the fact that `Void` is the initial object (we always have a function `Void -&gt; a`).
I was just splitting a hair and trying to explain that there is a mismatch between Haskell semantics and "termination". 
Yes. `fmap (const ())` can only be a turned into no-op when the instance of the `Functor` is known at the call site at compile time. If you write decently polymorphic code this only happens with a _lot_ of inlining and that isn't always possible.
In this case I would write `IO unit -&gt; IO ThreadId`; see my [other comment](http://www.reddit.com/r/haskell/comments/1jqjv3/for_an_arbitrary_monadic_action_which_type_is/cbhdtom).
Performance concerns may be secondary to other concerns. If you believe the () is less error-prone (e.g: makes it less likely to ignore results that should not be ignored), then it might override the slowness.
I would also really love to see his work.
The best way to learn the Lambda calculus might be to implement it - Lennart Augustsson's - Lambda Calculus Cooked 4 Ways (www.augustsson.net/Darcs/Lambda/top.pdf‎) isn't exactly a gentle intro but I found it very helpful.
`m a` is also more natural in that it is likely to be the inferred type of whatever the definition is. To get an inferred type involving `m ()` I'd think you'd need either a pattern match on `()` (weird) or a type ascription (ugly). (This is in part due to the type of `&gt;&gt;`, but the same arguments apply as to why its natural type is `m a -&gt; m b -&gt; m b` rather than `m () -&gt; m b -&gt; m b`.) I don't necessarily think the inferred, most general type is always the best one to export, but I do think it's a good default.
Not in what appears to be the latest base: [link](http://www.haskell.org/ghc/docs/7.6.3/html/libraries/base-4.6.0.1/Prelude.html#v:seq)
&gt; The older version still uses the ExistentialQuantification extension for encapsulation, but I think that without the Monoid constraint on the data type, that dependency could be omitted also. Not if you want the `Applicative` instance as written, but if you instead switch to a specific function it can work: data Fold a w b = Fold (a -&gt; w) (w -&gt; b) sum' = Fold Sum getSum length' = Fold (const $ Sum 1) (fromIntegral . getSum) fold (Fold t c) xs = c (foldl' mappend mempty (map t xs)) instance Functor (Fold a w) where fmap f (Fold t k) = Fold t (f . k) zipFold (Fold tL cL) (Fold tR cR) = Fold t c where t x = let l = tL x; r = tR x in l `seq` r `seq` (l, r) c (wL, wR) = (cL wL, cR wR) zipFoldWith f a b = fmap (uncurry f) (zipFold a b) average = zipFoldWith (/) sum' length'
I only use polymorphic-in-MonadIO inside application code, or utility libraries meant to be used in cases where they will be lifted anyway. They are the same thing, type-wise, so it's a bit stylistic.
Perhaps if we had special types `Top` and `Bottom`, things like this would become clearer. Top is the supertype of all types (something of any type can fill a hole of type Top), and bottom is the subtype of all types (something of type Bottom can fill a hole of any type). const :: a -&gt; Top -&gt; a forever :: Monad m =&gt; m Top -&gt; m Bottom callCC :: MonadCont m =&gt; ((a -&gt; m Bottom) -&gt; m a) -&gt; m a I've seen "Top" called "Anything" or "Any", and "Bottom" called "Nothing" or (it's essentially the same as) "Void".
If the constructor is hidden, then there is no way to get a value of that type, which suffices.
For a monadic action, isn't is plausible that `&gt;&gt; ()` is going to be cheaper than `fmap (const ())`, or at least more obviously consistently so?
Next up: comefrom
I was going to say m () until I reread the first sentence properly, and realised you were asking about a function *parameter*, NOT a function *result*. So I shall say m a.
Can't that be inferred for `_`? It looks to me that `_` is `Bottom` when it's covariant, and `Top` when it's contravariant with the ehm... 'entire' type. We already have `OpenKind` and `ArgKind` in GHC, which I think refers to the same concept.
Interesting, I hadn't thought of using `_` for both Top and Bottom.
Terminating infinite lists? Huh?
 type Bottom = forall a. a
dead link :(
Aha! It's because somebody posted my fork on Hacker News: https://news.ycombinator.com/item?id=6154908
Consider "forkIO", for example. Ignoring the created ThreadId is fishy. It's a likely source of leaked threads. Explicitly ignoring it is nicer, though an even nicer combinator would be "withForkedIO" which killed the thread at the end of the given action. Consider also: foo (gameState &amp; player %%~ inputNewState) If `foo` ignores the input value, and only uses the action, the inputed state would be thrown away. If you were forced to throw a `void` in there, it'd make the bug more apparent. What would you suggest for this case?
That's an interesting question. I don't really know, but I counter with this: What could you really do with an `Iso Foo _`? If all you know is that you have an isomorphism between `Foo` and *something*, you can't really *do* much with that isomorphism. If you have a *non*-isomorphism, then you can do something interesting. data NotIso a b = NotIso (a -&gt; b) (b -&gt; a) notId :: NotIso a _ -&gt; a -&gt; a notId (NotIso to from) = from . to In this case, I think an existential would be the correct way to think about it? notId :: forall a. (exists b. NotIso a b) -&gt; a -&gt; a Consider data SomeNonIso a where SomeNonIso :: forall b. (a -&gt; b) -&gt; (b -&gt; a) -&gt; SomeNonIso a notId :: SomeNonIso a -&gt; a -&gt; a notId (SomeNonIso to from) = from . to `SomeNonIso a` looks a lot like the partially applied `NotIso a` Top describes "a hole of type (forall a. a)" while Bottom describes "a value of type (forall a. a)". So I'd say `_` in an invariant position describes either "a hole that can only be filled with a value of type Bottom (forall a. a)" or "a value which can only fill a hole of type Top (forall a. a)". However, we need some way of remembering that the result of `to` can be used as an argument to `from`. ---- This seems to be stepping towards type-level lambdas. It would be easy to instead just say "you can't use type-level `_` in a type-invariant position". Or if we're really lazy, then we can consider`_` in any type signature to be a macro for generating a fresh symbol, without bothering to think about any of this theoretical stuff, and it would Just Work.
I think there are some strange non-printing chars in that URL. [Try this.](http://www.augustsson.net/Darcs/Lambda/top.pdf)
=(
Yep, that's what I meant to get at -- just got sloppy with describing it. I see the libraries proposal is already underway!
How does this compare to HSpec? Seems like it fills a very similar role.
 import Test.Tasty.SmallCheck as SC import Test.SmallCheck as SC import Test.Tasty.QuickCheck as QC import Test.QuickCheck as QC import Test.Tasty.HUnit import Test.HUnit I'd suggest just re-exporting the entire modules `Test.SmallCheck`, `Test.QuickCheck`, and `Test.HUnit` from their tasty counterparts.
I doubt I'll be spending time on that (because I personally don't use that), but I'll be happy to receive patches. Also, that's why I focus on extensibility — everyone has their own ideas on how to structure and run their tests.
Haha, good find. By the way, is it ok if I add you to the contributors file? What's your email?
Saying something is a successor give the impression you are the maintainer of the prior package and have deprecated that package. Unless I misunderstand the maintainer graph, I think it is more appropriate to call this a "competitor" or "alternative" to test-framework.
C version is not allocating anything and haskell version allocates a list. So the comparison is not valid.
Shouldn't that be optimized away?
There's a hint in the comments of that SO post: &gt;And nicely, GCC or Clang with optimizations turn the whole loop into `movabsq $500000000500000000, %rsi` The C version does not even execute a loop. Why can't GHC be this smart? Well for starters, the whole computation is lazy, so optimizing the loop away would just be incorrect. I don't know if there's any way to finagle GHC into optimizing away the list and the loop.
There's also a version under the same name in [Data.Function.Pointless](http://hackage.haskell.org/package/pointless-fun) which also has some other compositional combinators you may be interested in. The `(.!)` operator is essential for proper composition in the strict subcategory of Haskell. And the `($::)`, `(~&gt;)`, `(!~&gt;)` trio work excellently for writing your own "derived" instances, whether as actual instances or for anything that looks like derived instances tend to.
N.B., this notion of tacit programming is just lifting everything into a reader monad. For which see [Ralf Hinze's talk](http://www.cs.ox.ac.uk/ralf.hinze/WG2.8/26/slides/ralf.pdf)
Maybe it could be. But how many special cases should the compiler handle? In scala community a similar discussions concerning for comprehensions never end.
&gt; the whole computation is lazy, so optimizing the loop away would just be incorrect. No: the whole computation is referentially transparent, so optimising the loop away is perfectly valid.
How are you just translating monads, currying, etc. to valid JS like that?
forkIO is mostly covered by "withAsync/withForkIO" as we both mentioned. However, in some irregular patterns, you might have non-nested forkIO calls, in which case various bracketed functions may not fit the bill. Also, State/StateT may not be appropriate for all cases -- if the function takes it as an argument, but does not return a new value (though gameState may be a bad example of that). Of course, if State/StateT were always appropriate, we wouldn't need all the ~ operators, only the = operators from lens. But we do need the ~ operators, which is evidence that throwing around parts of the input has some disadvantages.
with `-fllvm` real 0m0.087s without, real 0m0.998s 
If GHC doesn't supercompile something, you can do it yourself with TH. I wrote a small description of how to do it [here](https://github.com/quchen/articles/blob/master/useful_techniques.md#poor-mans-supercompiler).
I agree, I was surprised to hear that Max was working on a successor, only to be a bit confounded that this isn't written by Max at all.
What can this do that test-framework can't? It looks to me like tf can do everything that tasty can. Is the improvement simply a more solid foundation? The pattern support looks like a notable improvement, which I seem to be trading for JUnit XML output to give to Jenkins :) Overall, it looks like a great library - thanks!
&gt; Language culture that actively takes advantage of the above two features and discourages stringly typed programming Stringly-typed programming is still alive and well in Haskell programs. Using `String` (or similar) for error messages will probably persist for a long time to come.
`listToMaybe`
While I agree that those problems aren't completely solved, I don't think the solution is requiring every other library to adopt the `()` return value convention. That's just papering over the problem instead of fixing it at the source.
Oh my bad. I don't really know what I'm talking about.
Palpably mad? Op's version is perfectly reasonable and idiomatic. Writing your own recursive helper functions for a trivial computation is not very nice.
I am amazed and pleased that you apparently think it makes so much sense for there to be a type class for seq that you assumed it was so. If only... 
This is pretty much the quintessential example of a benchmark I don't care about. I've _never_ had to add up 1 billion statically known numbers, nor do I mind manually using a formula for `sum [m .. n]` if it ever occurs in a program. I wouldn't complain if GHC optimized this, but when it comes to the implementors fixing this benchmark vs. doing just about anything else, I'd pick the latter.
hspec is a small patch away from parallel test execution, there was just concern that turning it on by default could be problematic. You are right that hspec doesn't pass through options. AFAICT tasty is fullfilling about the same role as hspec or test-framework, so I doubt it makes sense for any to be a provider to another. That being said, you might want to figure out how to use the hspec matchers rather than sticking with obfuscated operators like `@?=` 
'palpably mad' was due to irritation at having to write out this kind of stuff for the 1 zillionth time, and anticipating the usual totally pre-cut pre-formed brain-free system of objections; it was admittedly excessive; I apologize. In any case, writing recursive functions by hand is one thing; expecting miracles from Data.List is another. As rwbarton points out, and as I would have, the vector package, *the obvious next step*, the absolute go-to import, when dealing with Unbox instances like Int -- it is among the ABCs of *idiomatic* contemporary haskell, JUST AS MUCH AS DATA.LIST, makes it possible to dispense with hand written recursion. --Though suppose it hadn't made it possible? The hand written recursion is *perfectly idiomatic,* and no fancier than the C nonsense. Or so I was thinking.
I don't think that the name has any connotation with testing even though it is only a vowel change away. You might consider actually naming it testy, although the Javascript Testacular framework recently changed its name to Karma.
minimalistic, awesome, useful, builds on existing stuff, love it! libs and tools like this made me fall in love with the ruby community, almost a decade ago -- and now it is the haskell community giving me this same feeling; but pure-FP, lazy and HM-typed!
I disagree that it requires buy-in from everyone. This isn't an all-or-nothing issue. Every library that follows this convention makes bugs in its use less likely. More buy-in =&gt; less bugs. Haskell doesn't have linear types, so we don't have good type-system defenses from accidental loss of values/information. And we probably won't have them in Haskell (for the general case). So just err on the side of safety: more superfluous GHC errors rather than more runtime bugs.
I also agree. roche and max are both people whose code I'd tend to trust, but it would be good to have word from max one way or the other on if he'd encourage people to switch over or not (i.e. does he really intend to leave test-framework unmaintained, or is it just in maintenance mode at the moment, etc).
Alright. You convinced me.
Oh, I couldn't agree more—the good performance I'm after is just a tight nonallocating loop. It's a bonus if the LLVM backend can precompute the sum of the arithmetic series (and it's a pretty silly optimization, IMO). I tried replacing + by xor in my `vector` Haskell version and in the original C, and made the upper limit of the loop a command-line parameter. Now I get indistinguishable runtimes from `ghc -O2 -fllvm`, `gcc -O3` and `clang -O3`, around 1.44s. `ghc -O2` is about a third slower and `gcc -O3 -funroll-loops` is about a third faster—I couldn't figure out whether there was an LLVM equivalent to `-funroll-loops`.
These tools might be useful, though they both require a JVM. &gt;Lambda Animator is a tool for demonstrating and experimenting with alternative reduction strategies in the lambda calculus. Eager languages reduce arguments before function application. Lazy languages reduce arguments, if needed, after function application. Runs as an applet in the browser. If you are prepared to trust it it can use Graphviz. There is a bibliography on the bottom of the page; http://thyer.name/lambda-animator/ &gt;The Penn Lambda Calculator is an interactive, graphical, pedagogical computer program that helps students of natural language formal semantics practice the typed lambda calculus Downloadable executable. http://www.ling.upenn.edu/lambda/
I started writing up that post into a library [here](https://github.com/Gabriel439/Haskell-Foldl-Library/blob/master/src/Control/Foldl.hs) and I just wanted to mention that you can fuse away the intermediate list allocation using a rewrite rule: {-# RULES "unfoldr/foldl'" forall f gen b0 . fold f (unfoldr gen b0) = case f of Fold step x0 done -&gt; let go b x = case gen b of Nothing -&gt; done x Just (a, b') -&gt; let x' = step x a in go b' $! x' in go b0 x0 #-} If you write the list production in terms of `unfoldr` this fuses away the list and produces code equivalent to the hand-written loops mentioned elsewhere in this thread, bringing the running time down by a factor of 10. You also get better performance if you explicitly annotate the list elements as `Int`s. Unfortunately, this does not play nice with list enumeration syntax. List enumerations get immediately inlined to a function that is not exported, so there is no way I can optimize that, but I can provide an alternative `enumFromTo` function written in terms of `unfoldr` that gives very good performance when you fold it. If you combine that with the `llvm` backend that will precompute the value at compile time and give performance identical to C.
You ought to ignore everyone else, both in this thread and the other threads you've looked at. The correct answer is to not do something stupid like use a loop/list/enumeration/etc. to sum from `m` to `n`, but instead to whip out your pencil and paper, or your Wolfram Alpha, and figure out the closed form, which as anyone knows, is `(m+n)(n-m+1)/2`, or fixing m = 0, is of course `n(n+1)/2`. Programming should never be used as an excuse to avoid thinking about your problem.
Yes, I agree. Using a `String` as an error message is only wise if you have no intention of (meaningfully) recovering from the error.
Finding optimizations for poorly designed programs is a stupid thing to spend time on. If you're going to write a program that does this, you deserve for it to be slow. That's the price you pay for being too lazy to think about what you're doing.
is not forall a. (exists b. NotIso a b) -&gt; a -&gt; a isomorphic to forall a b. NotIso a b -&gt; a -&gt; a ? Why then is this an issue? `_` could just be interpreted as introducing a new name universally quantified at the prenex level, and this handles all the cases the way one would expect.
Sorry, I now see how that may be confusing, but that didn't occur to me when posting. To make it clear, the package is by me (Roman Cheplyaka), and I am not the maintainer of test-framework (Max Bolingbroke is). tasty isn't endorsed by Max, and test-framework isn't explicitly deprecated by Max either. My intended message was "If I were you, and was using test-framework, I would consider switching to tasty." The reason is that test-framework appears to be unmaintained, and its future is not clear. So I still think that «successor» is more appropriate here than "competitor" or "alternative", but with the above qualifier attached. It would become a competitor if Max returned and resumed his work on test-framework.
&gt; What can this do that test-framework can't? From the end user perspective, not much. My first priority was simply to replicate test-framework, but with a cleaner codebase. If you used golden tests (test-framework-golden), then tasty-golden provides an additional `--accept` option to update golden files with the current test output. I submitted the patches to do the same with test-framework a long time ago, but they have never been released. (test-framework supports patterns as well, so it's not really an improvement.)
`(-1) ^ n` should run in O(log n) time. Try running `(-1) ^ 9999999999999999999999999999999999999999999999999` at the `ghci` prompt.
Speaking personally I wouldn't want to touch a library called testy... Unless I had a particularly hairy codebase that is.
Thanks for your replies, they are very useful, really! But consider that the other thread discussing this matter, afaik, is from 2009, so it's perfectly reasonable to expect that things changed in 4 years... Moreover, I am not exactly sure why using Data.Vector is any different from using Data.List, given that the elements are Ints, but I am obviously missing something!
Yes, your solution works very well, but it's pretty obscure... to be honest, I too thought that foldl', being strict, would have had this kind of optimization.
I would also add that it makes sense if you have to send an arbitrary exception over, say, a network connection, as in that case turning it into a `String` is your only option.
Interesting! I had never thought of using Void in that way. Would you consider that to be the *preferable* way (compared to `forall a. m a`) to signal that an input argument needs to be an infinite loop? (I also wrote [another post based on this](http://www.reddit.com/r/haskell/comments/1jsth7/for_an_infinite_monadic_loop_which_type_is_better/).)
Nonsense, exponentiation is not O(n) in the exponent.
As long as you don't screw up existing/shadowed names forall a b. (exists b. NotIso a b) -&gt; a -&gt; b -&gt; (a, b) notId (NotIso to from) a b = (from (to a), b) forall a b. NotIso a b -&gt; a -&gt; b -&gt; (a, b) -- bad! Should be easy to avoid though -- using existing techniques It does seem rather easy, which makes me wonder why GHC doesn't have any extensions enabling "exists" syntax, if it's so easy to desugar into foralls.
Void might be more appropriate once it's in the base package. (Currently it's not.) The traditional approach is certainly to just use a fresh type variable, as evidenced by `forever :: m a -&gt; m b`.
Thanks for the tour, but can someone explain to me the part about: &gt; not (3 &lt; 5), evaluates to True Is that an error or have I missed something fundamental?
there's more work to implement haskell code one than c code one because I'm trying to support polymorphic. for example, if a sub tree required type "Num a =&gt; a -&gt; a -&gt; a" ,then "rem::Int-&gt;Int-&gt;Int" should be valid tree node But I still can't find out a method to verify such situation. your works seem good, I'll read it deeply later
Not my work I should say. My work was based off that paper
Looks like a great opportunity. I wonder what impact the requirement to work in the Singapore office will have.
 import qualified Data.Vector.Unboxed as V main = print . V.sum . V.enumFromN (1::Int) $ 1000000000 Compiled with `ghc -O2 -fllvm` I get real 0m0.003s so on my system it's definitely killing the loop.
`Data.Vector` is designed for numeric computations, so a lot more work has been put into optimizing it. They're structured differently as well, and have Unboxed instances. They're also guaranteed to be finite. This makes them easier to optimize. `Data.Vector` uses what's known as Stream Fusion, which can optimize away intermediate representations completely. For instance, `sumOfSquares = V.sum . V.map (^2) . V.enumFromN (1::Int)` won't create any Vectors at all in memory (using `-fllvm -O2`) and will compile down to a simple loop. That work hasn't been done on lists, either because of a lack of need or increased complexity.
"Unsafe" as in, unsafe IO, in the context of Haskell, is not the the unsafe functions you must be referring about here, because those are not actually unsafe. They just do IO. Secondly, the amount of IO one does is not related to the programming languages, but to the purpose of the application you are writing. It may feel like you do a lot less IO in haskell, because people don't generally use it for such applications. Just because you can formally prove properties of (pure, what you refer to here as "safe") functions in Haskell, doesn't mean anything in the context of crashes. More importantly, it distracts, because you can have formally correct, but very faulthy haskell code (space leaks or infinite recursion). **A safe haskell function is one where you can mathematically reason if, if it succesfully calculates, that what it calculates is what you want to calculate**. **It makes no garantuees about not crashing, or even succeeding in calculating** 
It has been done, actually: http://hackage.haskell.org/package/stream-fusion
I was using the term safe in the same way as you were using it in the last paragraph when you wrote "Haskell is no safer". The point was that Haskell functions don't have many failure modes functions in other languages do and in particular many functions in Haskell don't have any of the classic failure modes, not even the ones you can produce in Haskell. Many functions are in fact guarranteed to terminate in Haskell (assuming of course that the Hardware or the OS don't interfere), many functions do not produce space leaks, none access memory they are not supposed to (outside of the FFI),... The point was that I would prefer a language which does its best to reduce the number of possible failure modes in practice, even if it doesn't succeed in 100% of all edge cases.
Didn't have time to look over your code but I noticed you asked about mdnsIp = runGet getWord32host $ BL.pack [224,0,0,251] And thought I'd redirect you to https://github.com/sebnow/haskell-network-address or something similar if you're working with IP addresses since it's nice to have some type safety on them (without having to roll your own).
So it has! I was aware that work had been done, but I didn't know what the current state of it was. Thanks for alerting me. Do you whether this will make into the standard libraries, either the way it is now or as a complete replacement of the current `Data.List`?
I prefer returning `m a` when returning a non-existent member in positive position. Then it can simply be unified with `m Void` when I need it to be. Having to use `fmap absurd` or `vacuousM` has a similar performance impact to needlessly requiring `m ()`. Also, this is what you get when you use this on most existing combinators anyways. traverse (const Nothing) :: Traversable t =&gt; t a -&gt; Maybe (t b) 
I think people decided against it see, e.g. http://ghc.haskell.org/trac/ghc/ticket/915 I have seen more helpful discussions of the plusses and minuses somewhere I cant find. But with lists, stream fusion is a giant win for some things and then pretty bad for others -- `concat` and `concatMap` are thematized on the ticket and elsewhere. By contrast, the going ghc build-foldr optimization eats these for lunch. In the miracle of `Data.Text` and `Data.Vector.Unboxed`, though, everything comes together; they should be viewed as elementary first-week Haskell; the use of lists for unboxed things like Char and Int has *not* been 'idiomatic' for some time now, but just a known mistake. One should think of `Data.Text` and `Data.Vector.Unboxed`, which are not one whit more difficult to use than the Prelude though certain things are of course excluded, as the way this kind of optimization was given to us and as the standard library implementations of the things they implement. Many features of ghc were developed to make the `vector` package possible. 
Why's that? Spent a week there, and liked the city overall, and it's a great hub to explore Asia!
Except it's not really a performance problem at all, it's simply a bad design problem. Even if we throw reason out the window and accept that this is a perfectly reasonable design, the programs aren't equivalent, so they can't be compared for speed. One is a for loop construct, the other builds up a giant list only the consume it. A comparable C program (like the one that this gets compiled to) would build up a huge stack of structs, and then decompose them. But that's obviously an enormously stupid solution for the simple task of summing over a list. So even *if* we accept that we should ignore the closed form, we still have a gigantic design problem, namely that you're using lists for no darn reason! Either way you've refused to think about the problem and you deserve everything you get.
http://www.reddit.com/r/haskell/comments/1jrj7o/sum_11billion_in_haskell/cbi1q7n
**tl;dr:** Don't ask for a () if it is not to use pattern matching (or seq) on it. Let explicit appear that you _won't_ evaluate the result. To me it's just not aesthetics matter: "m ()" and "m a" will have different behaviors, and will tell different things to the user of your API. Taking an "m a" as a parameter tells that you will _not_ pattern match on the result of the action, i.e. if your monad is lazy that the side effects _won't be executed_. Using "m Void" tells the same and would be maybe clearer for people who don't want to search through the type of the function to see if the "a" is used elsewhere in the signature. "m Void" would also enforce that interesting results don't get accidentally lost, but that is a different concern. I think BTW that forkIO is misleading in this way. By asking for an "IO ()", it gives the false impression that it will pattern match on the unit. See the following code: forkIO $ do putStrLn "One." unsafeInterleaveIO (putStrLn "Two.") It will just print the "One.", not the "Two.". It means that forkIO arbitrarily forces you too return a unit, on which it could pattern match but doesn't do it. This type of cosmetic use of (), which is scattered amongst all the Haskell libraries, wrongs the user as to the way his action will be executed. The behavior of forkIO would've been rendered explicit if its type were: forkIO :: IO Void -&gt; IO a So, for short: - "m ()" = "I will execute your action and pattern match its result, so every side effect encountered in the way of its "computation" will be executed" - "m a" = "I will execute your action according to your monad's evaluation method (i.e. if your &gt;&gt;= doesn't evaluate enough, that's your problem), and forget about whatever result it may return, so beware" - "m Void" = The same, except "I will refuse to silently turn out your result, tell explicitely you don't need the result" **EDIT:** There is also the heavier ```(forall a. (Monad m) =&gt; m a)```, which _forces_ the action to return _an undefined result_ (so passing an "m Int" for instance will be refused). But that is the same than "m Void" and it's way less convenient (and it will require extensions).
It's an error.
&gt; Have you ever seen a programmer attempt to destructure/parse a String in an attempt to recover structural information about the error message? Who does this in Haskell?
&gt; concat and concatMap are thematized on the ticket and elsewhere http://www.ittc.ku.edu/~afarmer/hermit-opt-haskell13.pdf
You need to turn the `-1.0 ^ n` into `(-1.0) ^ n`. Yes, there is a difference. O(log n) vs O(1) will be measurable when there are a lot of terms (and that expansion converges quite slowly, so needs a *lot* of terms. And then there's the prefactor. But this isn't due to `(^)` scaling linearly: % time ghc taylorpi.hs -e 'taylor 5' 3.14159 ghc taylorpi.hs -e 'taylor 5' 2.12s user 0.03s system 99% cpu 2.156 total % time ghc taylorpi.hs -e 'taylor2 5' 3.14159 ghc taylorpi.hs -e 'taylor2 5' 3.49s user 0.03s system 99% cpu 3.534 total % time ghc taylorpi.hs -e 'taylor 6' 3.141592 ghc taylorpi.hs -e 'taylor 6' 14.93s user 0.07s system 99% cpu 15.021 total % time ghc taylorpi.hs -e 'taylor2 6' 3.141592 ghc taylorpi.hs -e 'taylor2 6' 27.84s user 0.11s system 99% cpu 27.989 total % time ghc taylorpi.hs -e 'taylor 7' 3.1415926 ghc taylorpi.hs -e 'taylor 7' 117.81s user 0.30s system 99% cpu 1:58.26 total baraddur% time ghc taylorpi.hs -e 'taylor2 7' 3.1415926 ghc taylorpi.hs -e 'taylor2 7' 254.19s user 3.48s system 99% cpu 4:17.91 total 
It's no more cheating than `build/foldr` shortcut fusion that GHC uses in `base`.
I know, I know \^\^. But for once, it wasn't an intricate use of ContT leading to illegible code ;)
If GC is involved, it is allocating. Allocating in a tight loop with intermittent GC's. Whereas the C one does in-place updates inside registers. GHC should optimize it to do the same.
The programs are certainly not semantically equivalent, because one is written naturally in an imperative language and one is written naturally in a functional language! What's the most natural way to solve this problem in Haskell? I'd certainly say `sum [1..10^9]`. I don't know much about Haskell, but from my experiences with it I have some fuzzy expectations that I can write what seem like ridiculously large data manipulations (often infinite!) but have my ass saved by the laziness of the language when I later extract the information I'm actually looking for.
That's besides the point. This is a good benchmark of simple looping constructs (as long as you show you're comparing apples to apples). Using a more complicated example that doesn't have a fast closed form is a waste of time here.
The fact that allocation&amp;GC are involved here is basically a deficiency (or a potential for improvement, if you'd like) on the Haskell compilation side of things. 
Of course this benchmark is silly. But the idea is that it represents potential for optimization that will also work for non-silly loops. If GHC can't optimize this case, it's likely there are variants of it which *are* useful which it will fail to optimize as well. This case is simpler than the variants, so it's nice to focus on that.
Like I said below, you're not comparing apples to apples, so even if we accept that poorly designed code ought to be efficient, it's not comparable. Codensity's `go` example is the nearest thing to a `for` loop in pure settings (so near that it's essentially how Scheme's `for` loop macro works). And that's quite fast indeed, at 0.09s. The whole question is just the result of not thinking things through (or worse?).
I like the requirement for `m ()`, because the annoyance of adding "void" calls is nothing compared to a runtime bug when accidentally losing information in the result of the inner computation.
Codensity's tail recursive `go` version below has been the standard functional analog of for loops for probably 50 years. It's just completely unacceptable to do huge amounts of data construction like this just to sum a list. You wouldn't do it in C, even tho it's *just as easy to do*, so why would you do it in Haskell? It's nonsense. &gt; I don't know much about Haskell, but from my experiences with it I have some fuzzy expectations that I can write what seem like ridiculously large data manipulations (often infinite!) but have my ass saved by the laziness of the language when I later extract the information I'm actually looking for. Sure, except here you're asking for an answer that has to construct and look at a billion pieces of data. Laziness isn't going to save you the cost of a billion mallocs, etc. if you *must do them all*.
If the standard `go` style loop is competitive, that's great! If the same code, using unboxed vectors is competitive, that's great too. (I got the impression that both are closer, but still not competitive). But I don't think it is too much to ask for a simple composition (`sum` and `enumFromTo`) to be fused away into a `go` style loop.
&gt; this was written in Haskell What do you mean by "this"? The background of the video? Where is the code for it? \^\^
Just out of interest, how much do positions like this tend to pay?
Most people can't just pick up and move to another country, I'd assume. It's also an expensive place to live, and it's not for you if you don't like hot and humid weather :)
There is no public salary level, it tends to be negotiated on an individual basis. I don't have any idea what the salary range will be, but almost certainly higher than you would get working as a programmer doing something outside the finance sector.
I mean the video is the output of an executable running realtime, using OpenGL to produce the graphics, and said executable is compiled by GHC. The source code is not public, mostly because it is an ugly hack, not because it's secret or anything. Some minor reusable libraries developed for this and similar projects are on Hackage. The executable can be found and downloaded by following the links on that page.
What's "Mu Haskell"?
What an incredibly stupid article! I think HuffyP should muzzle their CTO for writing one non sequitur after another. It's obviously clear that this silly CTO is not a programmer nor know anything about functional programming.
No. Non-silly loops do work that cannot be computed statically or with a closed form. For that you want good numeric performance and deforestation and such. And I do care about that. But that is not what this benchmark measures. This benchmark measures GHC running a loop against GCC printing a statically determined constant.
Well, there were variants of the C code compiled to an actual loop, which significantly beat the GHC loop. It was apples to apples, assuming the allocation/GC overhead can be fused away.
Does anyone know which finance companies actually use Haskell? I've read some old posts as well as "Haskell in Industry", which make it seem that Haskell is pretty widespread in finance, although people usually only drop names like Tsuru Capital and Alpha Heavy. I've also read that a lot of companies that used Haskell (like Credit Suisse) have moved on to other languages. 
Scala is not a lazy language. The semantics of Haskell is very different.
Standard Chartered's in-house, strict dialect of Haskell.
Standard Charter's dialect of Haskell, with some 'different' strictness properties and other quirks. The linked video is a public presentation on it.
Oh boy, a lot of these don't look good.
I couldn't hear/understand what he was saying in the video, so it wasn't helpful.
As far as actual institutions go, CS were the big name that used Haskell but shifted largely over to F#. There was a small Haskell-using group in Deutsche, but that's not going on anymore either. The scattered usage I know of is more generally in smaller shops like Tsuru and Alpha Heavy. In the financial services industry, there is a FP group at Standard and Poors/CapitalIQ that does Haskell-related stuff. There will be a talk at CUFP: http://cufp.org/conference/sessions/2013/edward-kmett-sp-capital-iq-functional-reporting
I couldn't get ghc and llvm to penetrate through the Applicative instance but the answer is something like {-# INLINE[0] fold #-} {-# INLINE[1] (&lt;*&gt;) #-} Then e.g. main = print $ F.fold (liftA2 div F.sum F.length) (F.enumFT (1::Int) 1000000000) worked fine, $ time ./a 500000000 real 0m0.088s the same time as `F.fold F.sum` alone. I was using enumFT x y = unfoldr op x where op z = if z &gt; y then Nothing else Just (z, succ z) {-# INLINE[1] enumFT #-} to hide my `unfoldr`. 
It's just Haskell, through a slightly different implementation that gets a couple of things wrong - if you know Haskell, you know Mu.
Singapore has extensive subways, underground malls etc. You can easily avoid going outside for most of the time.
Cool. That's what I had assumed. I don't personally see a huge need for it - `vector` and `text` are excellent. `Vector` and `Text` for data, Lists for control, all that good stuff. I don't know enough about the workings of stream fusion to know what the disadvantages of including it by default would be, so I thought I'd ask. Thanks for updating me.
This is nice. I think I can steal this so that the left folds reuse `build/foldr` fusion and play nice with list enumeration syntax.
&gt; I couldn't hear/understand what he was saying in the video, so it wasn't helpful. Care to mention what it gets wrong? And right for that matter!
The problem is we have `(&gt;&gt;)`, `(*&gt;)` and `(&lt;*)`, so your safety net is a lie that only applies if you stick to a rigid monadic block-oriented style that looks like something out of a FORTRAN programming style manual. I already find myself forced to use `(*&gt;)` and `(&lt;*)` in parsing combinators to work around the "benefits" of this warning, when I neglect to use the ever so useful parsing result for some combinator like: char :: Char -&gt; Parser Char Which is most of the time unused, but where you do *occasionally* want to use it, so now I'm forced now to take up two names and complicate all of my parser code, or clutter it up and make it look like Scala code.
Programming Languagues is a fantastic MOOC that focuses about 80% of time on functional programming. It starts in Oct, though
Probably this one: https://www.coursera.org/course/proglang I took it, the functional programming contents are quite comprehensive and well taught. I dropped out towards the end, when the course starts teaching OOP.
If there is a 'benchmark', the command line parameter rwbarton included is obviously part of it.
It is Haskell 2010 + a silghtly better import resolution algorithm (local functions are preferred to imported ones), no Char, String as an abstract type, strict by default. Everything else is the same. Of these, the better import resolution is a definite improvement, everything else is more a matter of convenience for implementation.
Parser combinators are a good point. They don't tend to return "resources" or error codes as results. I am not sure the general leakiness (which I agree exists) is a good argument against particular instances (each of which is a choice between convenience and safety). I can relate to both sides of the debate. I can see how the (noticeable) convenience lost is more important than the (relatively minor) safety lost. My hope is that the IDE of the future can make this issue go away (e.g: by auto-adding the "void" calls where needed, and showing the type being thrown away).
I meant https://www.coursera.org/course/proglang as /u/k0001 says. But this particular Programming Languges class is awesome too (though for a completely different reason).
Seems like you are missing the equivalent of $quickCheckAll or $(defaultMainGenerator) from test-framework-th. I don't want to write out lists of my tests by hand.
Using do-notation to simulate a list is a pretty gross hack. List syntax is pretty nice.
there also Odersky's on Coursera, as well as Dan Grossman's
It is absolutely normal to have difficulty with even basic problems when you are just getting started. Take your time and do not be discouraged. The very beginning where you're at now is the most difficult, in my opinion. LYAH is a great introductory text and working through practice problems is necessary to learn the basics, so I'd say you're on the right track. If you post what you have so far or ask more specific questions about the problem you're stuck on, it will be easier for the community to help you.
If you don't know how one should think to write functions, maybe it helps to read similar functions that others made and try to understand them. How about you try that with [Data.List](http://hackage.haskell.org/packages/archive/base/latest/doc/html/src/Data-List.html)'s source code? That, and tinkering a lot with ghci, trying things that you think may help, and seeing what happens. Are you making the functions with pattern matching or higher-order functions? The second ones help to shape your brain a bit more.
Too much to ask? Probably not. But as other people have said in this comment thread, there are bigger fish to fry.
you can just read answers. don't try to figure stuff out on your own, especially if you're learning something new. For example, if you're learning japanese and you don't understand bat shit of what that japanese girl is saying, but translator next to her translated for you, just take the translation instead of trying to understand japanese, that you don't know bat shit about. You'll make bad habits. You'll think that kawaii means something completely not related to kawaii at all... etc. But, do talk to the japanese girl A LOT. Or let her talk to you A LOT, even if you need translation constantly, the more you're exposed to japanese, the easier it will be for you to learn japanese. Just keep going with haskell practices. Look up answers. Search online. Ask haskell programmers. Read haskell source code. Be exposed to haskell. A LOT. 
oo nice so like a custom haskell version of ocaml but haskell?
The typical way I've always done fib is like: fib :: [Integer] fib = 0 : 1 : zipWith (+) fib (tail fib) and a cursory run computes (fib !! 10000) quickly, with only a slight delay on (fib !! 100000). Something I notice about your fibonacci function is that it calls last, (or your variant of), which is O(n) (iirc), on each recurse of fibList. So it would end up being something like O(n^(2)) if I'm not mistaken.
I'd work there but I feel under-qualified when compared to whats out there, no large scale haskell project experience, no publications, no PhD.
Are you based in Singapore?
Here's a simple echo server using pipes-network. import Control.Proxy import Control.Proxy.TCP main :: IO () main = serve (Host "0.0.0.0") "8000" $ \(socket,_) -&gt; runProxy $ socketReadS 4096 socket &gt;-&gt; socketWriteD socket The first line serve (Host "0.0.0.0") "8000" $ \(socket,_) -&gt; is not actually pipes-related, but just a convenience function that sets up a thread for each connection. this line sets up the actual echo server: runProxy $ socketReadS 4096 socket &gt;-&gt; socketWriteD socket socketReadS creates a source (a "Producer" in pipes lingo) that reads from the socket, and feeds (with &gt;-&gt;) to socketWriteD, a "Consumer" that writes everything it receives back to the socket. 
Here's a more sophisticated example I threw together for somebody else. It is a client / server application where multiple clients can connect simultaneously to the same server. The server keeps a continuously updated tally of how many keystrokes each client has typed. I split it into two pastes. Client code: http://lpaste.net/84020 Server code: http://lpaste.net/84018 This uses the libraries on Hackage. I will release an updated version when the 4.0.0 series of libraries hits Hackage.
Is this really clearer than taylor :: Int -&gt; Double taylor limit = fi (go 0 1 0) / 10.0^limit where go :: Int -&gt; Double -&gt; Double -&gt; Int go n a b = if truncate (b*size) == test then test else go (n+1) b (b + term) where test = truncate (a*size) term = 4.0 * (parity n) / (2.0 * (fi n) + 1.0) fi = fromIntegral size = 10.0^limit
Try thinking of it in a more declarative way. We want to take the current element (head) and the rest (tail) list while it is the same and put this in it's own list. pack (x:xs) = [x:(takeWhile (==x) xs)] ghci&gt;pack "aabbcc" ["aa"] Then pack the list that is the rest of the list until it doesn't equal x and append it. pack (x:xs) = [x:(takeWhile (==x) xs)] ++ (pack $ dropWhile (==x) xs) ghci&gt;pack "aabcc" ["aa","b","cc"]
Perhaps there are several differences between Ocaml and Haskell other than strictness.
&gt; I didn't know how Haskell lists are linked - if they are indeed singly-linked with no tail pointer then that would explain it. Even if they were doubly-linked lists, appending to a list would still be O(n), because the whole list needs to be copied (in case there are any other references to the list). By contrast, *prepending* to a singly linked list is constant time because the new list can share most of its structure with the original list. You can build doubly-linked data structures in Haskell if you want, but they tend not to be very useful for this reason.
Is that in UK? I've asked a couple of people in Tokyo and they don't know anything about Haskell (though of course Barcap is big enough that it's plausible one team is using Haskell but other developers don't know about it).
It took me about 6 months of solid effort to get to the "intermediate" level of Haskell programming, and I considered myself a pretty decent programmer already. It takes time, just like with any skill. You have to work on it, develop it, and eventually things will start to "click". Don't get discouraged because it's taking a while either; Haskell has a steep learning curve. It'll take you a long time to get passed the first hurdle, but after that the more difficult concepts come more and more quickly. As for problem 9, let's break down the problem into manageable cases. First of all, we want to work on lists of a generic data type, not just strings (lists of chars), so we know that our type signature will look like: pack :: [a] -&gt; [[a]] Now, the problem statement says that we need to group identical successive elements together, which means that we have to be able to tell if one element is equal to another. Sounds simple, but not all data types can be compared for equality. Luckily, there is a typeclass that ensures that we can use `==`, and that is `Eq`. So we can add this constraint to our type signature pack :: (Eq a) =&gt; [a] -&gt; [[a]] It is important to figure out what types you need for a function first, although there are many cases when you have to change type signatures halfway through to adjust for an unforeseen obstacle. Now we can move on to what the function does. `pack` is supposed to group up consecutive, equal elements. What's a good function for taking elements from the front of a list that meet a condition? We could write our own, but this is such a common function that it comes in the built-in Prelude functions, and is called `takeWhile`. We can use this function as follows: pack xs = takeWhile (== head xs) xs But this isn't yet correct, this would return a value of type `[a]`, not `[[a]]`. It is, however, what the first element would be, so if we added pack xs = takeWhile (== head xs) xs : undefined our code would at least type-check and compile. Now, we want to repeat this for the rest of the list `xs`. Here is a great example of when to use a recursive function. We already know the code we need to calculate each element, we just want to keep applying it to the list until we run out of input. To do this, we can use `takeWhile`'s companion function, `dropWhile` (hint: `pack (dropWhile ...)`) All that's left is to define the trivial case. So all this together is pack :: (Eq a) =&gt; [a] -&gt; [[a]] pack [] = [] pack xs = takeWhile (== head xs) xs : pack (dropWhile ...) I'll let you fill in what gets passed in to `dropWhile`. Now, this is a fairly naive solution. It'll get the job done, but it won't be too fast about it. As bonus, see if you can avoid using `head` altogether by using pattern matching, and see if you can figure out how to loop over the list once (using `takeWhile` and `dropWhile` loop once each).
I like to bust out this one: fib n = (phi**n - phi'**n) / sqrt 5 where phi = 1/2 * (1 + sqrt 5) phi' = 1/2 * (1 - sqrt 5) It won't work for your n = 1000 case though.
I completely agree. Stating tests as lists feels unnatural.
The other posters here are right: you certainly shouldn't feel bad about not being able to think your way through these problems yet. I would highly recommend some sleep. Things have a way of looking easier if you sleep between attempts to wrestle with them. I'm not going to write the function for you, but I will try to describe how I'd approach the problem. A good way to start thinking about writing a function is to think about its types. `pack` takes a list and produces a list of sublists. pack :: [a] -&gt; [[a]] This gives us something to start with. Let's assume that we don't need a helper function, and `pack` is simply recursive (we can change our minds, later). We have two options for the bottom of the function: pack [] = [] pack [] = [[]] We can decide which to use, later, if it turns out to be important. The next case is pretty easy, as well: pack [x] = [[x]] Typically you'll need to be looking for break-points in your sequence, where the first two elements in a list are inequal. So we need to include a case where there's only one element in the sequence. For the rest, again noting the need to look at the first two elements, the overall form of the recursive bit can look something like: pack (x1:x2:xs) = ... where (sub:subs) = pack (x2:xs) ---- That's most of the way to the result, actually, and this is purely thinking about the structure of your inputs and outputs and the need to consider two pieces at a time. I hope I explained the process well enough that you'll be able to apply a similar logic. Feel free to ask any questions if I was unclear.
With a doubly-linked list, you usually also have a tail pointer for the last element in the list. It is certainly more expensive in memory usage, but it also means that working on the back of the list has the same performance as the front, and it is faster to insert a list in the middle.
Yes. I was talking about appending, not accessing elements near the end.
Clicky link: [Data.List source code](http://hackage.haskell.org/packages/archive/base/latest/doc/html/src/Data-List.html).
If your eyes aren't bleeding, you have untapped reserves of learning effort. That's my big tip as a fellow "retard".
Thanks. I was not able to install network-address due to dependency issues. But I used iproute package which provides similar types on IP addresses.
I think it's all london? Here's the paper they did: http://www.lexifi.com/downloads/frankau.pdf
You can always do an "imperative" implementation of the fibbonacci sequence if you are worried about performance. Its not as pretty as the tying-the-knot version but it should have similar performance andmight be easier to understand if you are just starting out. Basically you use a recursive function for your loop and store your state in function arguments #imperative version a, b = 0, 1 while True yield b a,b = b,a+b --Haskell version fibs = loop 0 1 where loop a b = b : loop b (a+b) My mind is a bit rusty today so I can't tell for sure if there isnt a space leak in that addition but thats a bit more advanced anyway and the algorithm I'm using should be pretty similar to the versions where people tie the not and define fibs in terms of itself.
Doubly linked lists only really work if they are mutable. In Haskell everything is immutable so you can only really add things to one side. In a strict functional language this often means that you need to build lists backwards and reverse them in the end (or use special data structures) but due to Haskell's lazynaess you can instead build the list "all at once" and never actually use the appending operation. Haskell lists only let you append to the front, meaning that to append to the end you usually need to build the list backwards and then reverse it (or use a different data structure with fast appends). However, in this case we get around the limitation because the lazyness lets us build the list "all at once". Since we never actually see a Nil terminating part of the list at the end of the list we arent actually appending things.
/u/please- had an interesting solution with `zipWith`. I personally prefer an arguably simpler solution that leverages `iterate`, which simply calls a function on a value, and then calls the function on the value of that call, and so forth generating an infinite list (e.g. [f(x), f(f(x)), f(f(f(x))), ...] fibs :: [Integer] fibs = map fst $ iterate (\(x, y) -&gt; (y, x + y)) (1, 1) &gt;&gt;&gt; take 10 fibs [1,1,2,3,5,8,13,21,34,55] &gt;&gt;&gt; fibs !! 20 10946 It's quite fast. It's also very simple once you understand how the individual components tie together. Calculating `fibs !! 100000` takes about 0.85s from the ghci repl and about 0.35s compiled.
The `zipWith` version takes advantage of laziness. The list is infinite and references itself, but later values only require earlier values, so laziness lets us get away with it. Here's my favorite variant that also utilizes laziness: fib :: [Integer] fib = scanl (+) 0 (1:fib) I think it's a bit easier to reason about since there's less going on, but they do essentially the same thing. `scanl` is basically `foldl` if we keep all the intermediate steps. Now we're working with 2 lists: `fib` and `(1:fib)`. Like `foldl`, `scanl` starts with the accumulator (0), so the two lists look like this: fib = [0,..] (1:fib) = [1,0,..] We then perform the fold operation `(+)` on the accumulator (0) and the first value of `(1:fib)`, which is 1. 0+1 == 1, so now: fib = [0,1,..] (1:fib) = [1,0,1,..] Our accumulator is now 1, and we perform the fold operation on the second value of `(1:fib)`, which is 0. fib = [0,1,1,..] (1:fib) = [1,0,1,1,..] Our accumulator is again 1, and we continue down the list: fib = [0,1,1,2,..] (1:fib) = [1,0,1,1,2,..] Our accumulator is now 2, we add 1 and get 3. The pattern should be pretty clear at this point: we take the last generated value, add it to the next value of `(1:fib)` (which happens to be the second-to-last value generated), and that produces the next value. Repeated infinitely. Except we're lazy, so we only go as far as anyone needs us to. So if someone does `fib !! 10000` to find the 10000th number in the sequence, we generate values up to the 10000th one then stop. This also runs in linear time since we essentially only run down the list once, so on top of being super short it's also quick.
As you have probably witnessed in this thread, the Haskell community is very friendly and eager to try to help. Remember to use that resource!
Depends on where you're working. $300k+ USD base isn't too abnormal around SF for senior tech positions (non-finance), though the bonuses are lower: targets of 15-50% are typical. Stock goes on top, and may or may not be worth anything. Good independent contractors are making a lot more than that. Some people here on /r/haskell are in that camp. Singapore makes up for it a bit with a lower cost of living (imo) and a different set of weekend travel destinations than are available around the US, it does seem like it would be fun for a couple years. Semi offtopic... but the bigger shocker for me was the pay delta for healthcare jobs. My wife talked to a a couple people when we were out in Singapore a few years back regarding positions that would pay $100-150k USD here, super specialized healthcare work with 7+ years of school and a difficult board exam, and they only make &lt;$45k SGD out there.
Oh this make me happy! Just started going through /r/dailyprogrammer with some haskell after (yet another) disappointing experience with lisp. Thanks for sharing :)
That's not the point, int is being implicitly converted to a double when you call pow(double, double) and depending on whether you want to have deterministic floating point will also affect the result. And as you say youself, Haskell `(^)` has no equivalent.
I have taken Grossman's course, and I recommend it if the individual has not take a similar course that was well taught or if person prefers a dictated schedule, and I think that Grossman does an excellent job of communicating that information. However, for the autodidact I think the book *Concepts, Techniques, and Models of Computer Programming* by Peter Van Roy and Seif Haridi is more valuable as an overview of programming paradigms or even just as a reference. That textbook is among a handful of techical books that I feel are actually well written with a minimum of verbosity; it is 936 pages long, but considering the breadth of domains the book covers that is actually quite terse, a property that all books among my list of quality texts have. What I like about the way the book approaches the material compared to Grossman's class is that it does not skip around between languages and does not use an industry language, which allows the reader to focus on the concepts instead of dealing the particular implementation of those concepts. The language in question is not a massive language on par with Java and C++, and the teaching tool comes with its on ready made and tailored environment, further freeing the reader from external and irrelevant details, and the authors are able to write with all variables that the reader will encounter ahead of time. With respect to learning **just** functional programming I actually find it much more efficient to read and work through the theory upon which the functional programming languages are derived. The ancillary skills picked up with learning the language in which mathematics is discussed has massive dividends elsewhere and permeate in many areas outside of programming. That familiarity prepares an individual to not only expand their horizons in mathematics and programming but also increases their purely analytical skills, an area that is frustratingly weak amongst programmers that only focus on the marginal coverage programming has. That is the reason I can only shake my head at the meme that mathematics has no value in the context of programming and should not be emphasized in curriculum beyond an juvenile level. That the Haskell community does not exhibit that behavior and generally encourages real understanding of the concept rather than a specific case is one thing I love about the opinionated nature of not only the language but its enthusiasts, enthusiasts that are largely not only competent in the domain but extremely inviting and friendly towards the nascent individual. Math is beautiful and universally applicable, something that the Haskell community embraces rather than shying away from, and its backing theory is something that does not take years and years of grinding just to even get to a serene moment in. I wish more people had that experience so that math was not so abandoned so early by the population by and large and perceived as having no value for them. There is a section in *The Little Schemer* entitled "Oh My Gawd: It's Full of Stars," a phrase which given only seven words I could no better describe the moment when everything just clicks and all of the hard work comes together and the beautiful elegance and synergy of maths comes together.
side note: "retarded" is very offensive (it's ableism at its best). the haskell community tends to be very nice. we should keep it that way. also for newcomers.
Different implementations do it differently. But basically, first you parse a module ignoring fixity. For instance, you can parse operators as if they all had the same fixity. Then you resolve the imported modules and find out what they bring into scope. And then you re-process all infix expressions with full information about operator fixity (this has to be done in a scoped way since operator can be declared locally). 
&gt; I also didn't know that zipWith could behave like that when applied to the function infinitely - thanks for the tip! * Try to manually expand the definition and you will see that it behaves correctly. Remember that laziness makes Haskell try to compute the *outer* values before the inner ones. In a language without laziness, the expression would just halt the program since it would try to calculate the *last* number before the first one. * If it helps with understanding, you might want to know that GHC performs "sharing," i.e. the `fib` names in the `zipWith` call are just pointers to the original `fib`. This does not matter for the end result, but I assume it makes the function more efficient. * If you need intuition on how it works, all you have to do is realise that it performs 0 1 1 2 3 5 + + + + + + 1 1 2 3 5 ? ----------- 1 2 3 5 8 i.e. one list shifted to the right added with the original list to give the next number in the sequence. You see how it uses the `8` from the sum to fill the `?` spot. Those were the three key insights for me at least.
Yes, especially when the performance concerns are minor and are completely insignificant most of the time. And anyway, Edward is now taking steps to help mitigate the performance concerns at least in some cases. However, Edward also pointed out that the clarity of code concerns actually cut both ways. While `m ()` might be a little more clear at the declaration site, it also can cause some obfuscation at the use site. So the question of which is clearer on balance really depends on the particular code in question.
Nice posts. Perhaps it's because I just haven't gotten the Tumblr thing yet, but it seems to me that Tumblr isn't the best place for these kinds of posts. I find it very awkward to browse and navigate among the exercises.
Tumblr is easy enough to use and i don't have to put too much work on setting things up. The only better alternative is host my own blog... which would be a pain...
You're using GHC 7.6.1, I'm using GHC 7.6.3. I find that `UTCTime` *is* an instance of `Show`, but it is defined in a different module. The fix should be for the instances to be exported along with the type. For now, import Data.Time instead of Data.Time.Clock.
Ew, floating point. Well, this is useful if you need to know the *approximate* value of large fibonaccis. Here's another fun fact that allows you to compute the *exact* value of the nth fibonacci in `log n` time (not counting the overhead of large integer arithmetic): Consider the 2x2 matrix `A = [[0,1],[1,1]]`. It is easy to see that A^n = [[fib (n-1), fib n],[fib n, fib (n+1)]] Since `A^(2*n) == A^n * A^n`, this gives us a formula for `fib (2*n)` and `fib (2*n+1)` in terms of `fib n` and `fib (n-1)`.
Oh you’re right. That’s weird, I don’t find the module that defines such an instance.
Could you elaborate? BTW: Hspec uses do-notation to create a forest (not a list) + it's just an ordinary writer monad. So I don't see how this is a "pretty gross hack". It's really just a writer monad.
It's an orphan instance in `Data.Time.LocalTime`. Yes, orphan instances lead to trouble in many ways. I think the author did it that way because of a circular dependance problem. It would be tricky to define just enough of `LocalTime` in a separate module without any mention of `UTCTime` to be able to define the `Show` instance. If you can solve that puzzle, I suggest submitting a patch.
Hspec has support for parallel spec execution, check the documentation: http://hspec.github.io/
If you're just adding numbers, using strictness annotations is rarely wrong. I think your example can, but will not, leak in most cases. It'll build up a thunk when you force the spine, but once you're forcing a single element a lot of shared values will be calculated. Here are the first 3 fibonacci numbers (starting with 1): loop 0 1 = 1 : loop 1 (1+0) = 1 : (1+0) : loop (1+0) ((1+0)+1) = 1 : (1+0) : (1+0)+1 : loop ((1+0)+1) (((1+0)+1)+(1+0)) That could be accomplished by `length . take 3` for example. Now if you demand the 2nd list element, you evaluate the (1+0), and the whole thing becomes = 1 : 1 : 1+1 : loop (1+1) ((1+1)+1) There are still unnecessary thunks around, but it's not as bad as before. If you want to get rid of the thunks, there are a few easy options: - ``fibs = takeWhile (`seq` True) $ loop 0 1`` will force every element to be calculated when you traverse the list, hence no thunk buildup. - `loop a !b = b : loop b (a+b)` makes b strict using a BangPattern, which has a similar effect. - ``loop a b = b`seq` b : loop b (a+b)`` is identical to the previous line (note that infix `seq` has `infixr 0`).
Nice! It's spelled "exercise", not "excercise", though.
mappend there is implemented with the Eq instance, that's a weird way to save a very tiny amount of code.
Already fixed it, thank you.
I really wish Haddock instance entries would link to the definition of the instance.
The title is a bit misleading, the blog post is regarding how difficult it can be for beginners to start using Haskell on Windows (a recurring theme, se for instance [gundersen.net](http://www.gundersen.net/haskell-on-windows/). His particlar gripe is with installing Leksah from Cabal. Interesting read though, could perhaps be useful for maintainers of the Haskell Platform.
The problem I have with this post and all posts like it that the reason things work on Linux and Mac is because people have either given of their time or their money to make it happen. Windows users: make it happen. I am no stalwart of open-source software but I submit patches from time to time to projects I care about and the projects I don't care about are left alone. Moaning and saying "I shouldn't have to think for this to work" never solved anything.
[The solutions to the first exercise](http://dailyhaskellexercise.tumblr.com/post/57051641046/even-odd-split) are both broken...
Thanks for the example. I wanted to try it, but for the recent libraries that cabal pulled in, the function serveFork that the server code uses does not exist. It took a bit of digging to find out it has been renamed to serve. I switched "serveFork" to "serve" and now it runs fine. :)
No, the `n` here is not the fibonacci number; it's the index of the fibonacci number in the sequence. So it really is `O(log(n))`. The large integer arithmetic factor may actually have the effect you are describing, but that factor must also be tacked on to any other integer-based method.
I think we need something to allow us to download and install MinGW RPMs from OpenSUSE or RedHat as part of the cabal install process on Windows (if we have to put it in Setup.hs for now that is fine). For instance here is the OpenSUSE URL http://download.opensuse.org/repositories/windows:/mingw:/win32/openSUSE_Factory/noarch/
You've misunderstood the point of the exercise. It's not about odd and even elements. It's about odd and even positions in the list.
Haskell Hoodlums is held in BarCap in London.
That's strange. Why? What about data Pair a = Pair a a data IrregularInductiveType a = C a (IrregularInductiveType (Pair a))) 
Yeah it's fine but has only a very limited package support - for example nothing GUI like aside from web
To store Fibo(n), the n-th Fibonacci number, you need O(n) space, which requires O(n) time to be written. This is independent of what algorithm you're using to calculate Fibo(n) - even if you have some oracle that gives you Fibo(n) in O(1), you still won't be faster than O(n).
D'oh! Very true!
To the OP: if you want help on this kind of things, avoid negative words like "crippled". They don't give a positive impression about you and does not actually help you getting help. Also, there isn't a single error message in this blog post, just random rambling from which it is impossible to deduce what is/was the problem. You if have a problem installing Leksah, don't blame the Haskell platform. I presume you had problems installing GTK+ or other dependencies which is outside of the scope that cabal does. And isn't pretty much every language like this on Windows? Maybe your vanilla Python installation is easy to do, but if you get serious, you'll have to install virtualenv or other dev tools. You'll be knee deep reading manuals and struggling to understand how package dependencies work in this particular environment. I won't even get to the Ruby + RVM mess. In my experience, Haskell Platform on Windows has worked quite nicely.
I think of the zipWith version as implementing the recursion relation `f (n+2) == f n + f (n+1)` and the scanl version as implementing instead the relation `f (n+1) == sum [f k | k&lt;-[0..n-1]] + 1`.
You make a very good point that the type `m a` implies that there will be no pattern matching on the result and so provides an additional piece of information, but I don't think that it is actually a universally held understanding that the type `m ()` implies that there *will* be pattern matching on the result.
Ah, I see.
You get cleaner code and better test groups without it, plus TH isn't really powerful enough to do this and both of those splices cheat by actually scanning the file on disk in crude ways. Porting `test-framework-th` should be a simple matter of changing one import, though.
&gt; Windows users: make it happen. ... &gt; Moaning and saying "I shouldn't have to think for this to work" never solved anything. I agree with you 100%. There are two cultural issues here. First of all, your average Windows user is not an open source contributor. They expect someone else to build their software and either pay for or pirate it. Some might go and drop a bug report and call it "a contribution", despite the fact that the report is not helpful and is written in a negative or downright abusive tone (like this one). Second of all, a Windows user is probably used to closed source software from big anonymous corporations. The only way to (try to) influence them is to write negative blog posts. You can't approach the developers on mailing lists or IRC. I'm saying this because from my open source experience, complaints and contributions from Windows users are disproportionate compared to their numbers using the software in question.
What you really want is posterous. Too bad twitter bought it and killed it. Manuel Chakravarty, a prominent member of our community, is in the same boat. He was using tubmlr, then switched to posterous, then got burned when twitter killed it. Now he is back on tumblr. Still, even on tumblr, [his blog](http://justtesting.org/) seems more usable than yours. Can you make yours look like that? The founders of posterous created a [clone of posterous](https://posthaven.com/) which they promise won't be killed, ever. However, it is for pay. If you want to move to an actual blog and make it be fun and not a pain, try one of the various Haskell solutions out there :). Look around on hackage and ask around.
Yeah, it’s obvious it oughtta be that way.
I think the [archive](http://dailyhaskellexercise.tumblr.com/archive) makes it easy enough.
Haskell platform isn't crippled at all, GTK is crippled. GTK is a nightmare to get working on windows.
Looking at his blog comments, it seems like he is expecting cabal to install GTK for him. Ultimately his problem is that he doesn't have pkg-config installed so cabal can't find it. In other words, he is expecting cabal to work as his system package manager. **This problem has got nothing whatsoever to do with the Haskell platform.** It's just a noob (who is also loud and offensive) who refuses to read the manual or look at the big picture. This would have been immediately obvious had he added the error messages he was getting, not rambling on and on about how he was frustrated doing it.
Type :i UTCTime in ghci.
 Prelude Data.Time.Clock&gt; :i UTCTime data UTCTime = UTCTime {utctDay :: time-1.4.1:Data.Time.Calendar.Days.Day, utctDayTime :: DiffTime} -- Defined in `time-1.4.1:Data.Time.Clock.UTC' instance Eq UTCTime -- Defined in `time-1.4.1:Data.Time.Clock.UTC' instance Ord UTCTime -- Defined in `time-1.4.1:Data.Time.Clock.UTC'
As you can see, no `Show` instance here. I think I’ll see it if I add the module `Data.Time`, but hm, it’s quite confusing!
I'd assume the issue is that polymorphic recursion can be tricky, and your replacement is no easier.
That's OK, int to double conversion is lossless so a C compiler could optimize `pow(-1, n)` to `parity(n)`. But mine don't. Should I conclude that "[i]t's all these little things that add up and make [C] piss slow most of the time"? Seriously, your example is really bizarre.
This is, for me, usually the primary reason for hidden modules.
Well, I can't speak dirextly to hspec, but for tf/tasy I can see it going very much like binary's Put. Every primitive would be in () in the monad, meaning you're never using the features of the monad, and it's just a hack so you can use do notation.
Here is my short [step by step guide](http://fluffynukeit.com/2012/11/installing-leksah-on-windows-7/) to installing Leksah on Windows 7. I, too, had issues getting it to work without issue, but I've been using it quite effectively for months now. 
Oh, nice! Thanks.
You might be interested in Agda's mixfix definitions.
I think the preferred way to do it on windows is to download an installer which takes care of installing the required dependencies(by just downloading the required libraries to the same directory as the exe, not caring about if they already are on the system or not) Indeed, the download page on leksah.org says: &gt;Install from Hackage if their is no specific installer/package available for your OS But there is an installer available for windows, so that is probably what he should have used instead.
&gt; I don't think that it is actually a universally held understanding that the type m () implies that there will be pattern matching on the result. No, you're right. This is why the use of "m ()" is now misleading in Haskell. But that is very subtle, because both Void and () mean "no information" (in one case you are sure to have no defined value, and in the other case you are sure to have always the same defined value, so the amount of information is the same: zero). It is only their behavior wrt evaluation, _and therefore execution_ (since in Haskell execution in conditioned on evaluation). Add to this the fact that Void is recent and that it's not even in base. You should always check the documentation when you see a "m ()", or don't try to rely on other evaluation than the one provided by your monad &gt;&gt;= or the ```seq```s you would have added yourself. Now, when you design something, if you can provide new information for free why would you do without it?
But for a Haskell beginner like the author, the FP Haskell Center would be perfectly fine.
Don't worry, for the Linux version I had to manually edit the makefile, install hidden dependencies and pray :)
You're welcome!
But that's like saying you're not against children. No one would take a stance against good name choices. It's a matter of what's good and bad for children/variable names.
Confirmed. There is no single evnironment so hostile to a beginner. It's like Linux in 90's. Crap and WTF's are flying loud and high. This language would have been better if it had a decent IDE. And there will be no IDE (except maybe a text editor on steroids) because this language is not homoiconic. The same is true for C++, Python and other crap. 
Here is a simple program I wrote that acts as a simple proxy between a client &lt;-&gt; server https://github.com/mhitza/inspection-proxy/blob/master/Main.hs
&gt; There are only two hard problems in Computer Science: cache invalidation and naming things. This is the truest thing ever. I've been hosting a biweekly math-for-fun session at my local hackerspace. Being a programmer has ruined me, as I tend to go back and rename things after I've written them on the board, because I thought of a more descriptive name mid-way through a proof :)
Yes, do it with Data.Time loaded.
&gt; Started hanging out in #haskell, which taught me a load of things by accident +1 to this. Not just about haskell either. I learned a lot of accidental things about PL in general as well as abstract math and logic. Great find.
Just finished the GADT article, few mistakes I found: On page 9, the `evaluate` uses `error` even though `fail` or just `Nothing` would be more appropriate, even more so since two paragraphs above the author says: &gt; Furthermore, evaluation will fail if the input expression is incorrect, so we have to use type `Maybe`. There's a typo on the page 22: &gt; Indeed, the longest path from the root node (containing alternating red-black nodes) can only be twice as long the shortest path (containing only **red** nodes). And on page 24, `type Tree a = forall bh. Node a bh` is a universal type, not existential. In this case, `Tree a` represents a `Node` of every possible depth, not some specific, but unknown one. `data Tree a = forall bh. Some (Node a bh)` would be okay, I think. Otherwise, it's a great introduction!
That's a quite comprehensive example! Just in case it is not clear from the example, I'd like to mention that the tools exported by the `Control.Proxy.TCP.Safe` module are the ones you want to use if your program needs to establish new connections *within* a pipeline, such as in /u/Tekmo's client example, when he uses `connectWriteD`. If you don't need that feature, then `Control.Proxy.TCP` is fine enough and a bit simpler, just as in the echo server example by /u/jhickner in another comment in this thread. Also, keep in mind that a lot of backwards-incompatible work is being made in the whole Pipes ecosystem for the `pipes-4.0.0 `release, which will be simpler to use than the current version, so this and other examples will be changing substantially soon. In the case of `pipes-network`, you can see the current work towards this goal in the `pipes-4.0` branch here: https://github.com/k0001/pipes-network/blob/pipes-4.0/src/Pipes/Network/TCP.hs
30 years ago there were better environments than most today's toys like Pyhton, Ruby or Haskell. Just like author of this blog I find such poorly maintained environments an insult to my intelligence. If you like to code with clubs and stones go ahead, but do not advertise Haskell as something better. As I look at Haskell community I can see a lot of steam but there is no trace of the train. 
In Idris language (probably next haskell) Applicative is indeed superclass of Monad.
Getting GTK2hs working on the Mac is a nightmare as well, I've never managed to do it.
[Choo choo!](http://www.haskell.org/haskellwiki/Applications_and_libraries#Haskell_applications_and_libraries)
I don't think it's a Prelude issue. It's in the essence of Haskell itself being a lazy language: if you traverse a lazy list while keeping a reference to its start, it will have to unfold in memory. So you need tricks like this to only traverse the list once, so you don't keep references to visited nodes.
I guess this fact is due to list being the free monoid.
You're right. That's a much better approach. I never thought of that.
I have a static blog generated by hakyll, but make things right still takes a bit time... I was looking for posterous at the beginning of the project and figured it got killed. :( I'm going to test posthaven today... 
No, I'm in London, but work closely with lots of people in Singapore and visit yearly.
I don't know. When starting out I had a hard time distinguishing where the boundaries of who was responsible for what were. If one has never used gtk and the only thing that depends on it is leksah and you are just trying to do a little haskell development then how do you tell who is responsible. When I first was trying to get started "reading the manual" usually beget more reading of manuals. As a windows developer everything was completely foreign. There is a lot of implicit knowledge that is useful for working with haskell from build system to how libraries work. While I didn't write an angry blog post about it I certainly did my share of complaining.
When I came from Windows to the *nix world it was quite a rude shock. While I was comfortable building things from source I expected any reasonably large project to have installers that precluded me from having to think. I certainly expected things to be the same over the fence and it was not. There is just a huge gap in expectations about how things should work that everyone needs to recognize. Secondly there are lots of open source projects on windows. There are lots of contributors. There is certainly no warning from either haskell or leksah that it will not be a first class experience. If its not going to be (I have never used haskell on windows so I can't comment) then it should be noted. This will at least set the expectations of new users appropriately. If as a community we have not supported windows in a way that windows developers are comfortable with then it may be that it is not worth the time and effort to do so. Personally I don't really believe in cross platform applications as they are so difficult to build. Trying to revive a visual studio addin for haskell would seem like the most natural option from an adoption perspective but I am not sure there are the resources to engage in that right now. I guess to sum up *nix developers have a very different set of expectations from windows developers. I do expect them to be frustrated. Personally I would rather have them write about it (angry or not) rather than just silently abandon their efforts. As a community we can collect many more interesting data points. 
Somebody explain the theory behind this. If Haskell's type system is turing complete how is it that type checking terminates? So theoretically I could have a set of types that form a loop and then type checking never terminates.
where does it say that it is turing complete? from my delves into the type system so far, quite a few corner cases you could move into when setting up types are disallowed - you can relax those with compiler directives, but this can then lead to the compilation not being able to complete.
You can compute stuff with non turing-complete languages
I know that but he's doing factorial so if you get the recursive case wrong it will never terminate. Which means you can write loops and things that never terminate and it's all happening in the type system so the type system supports arbitrary computations it looks like.
They use UndecidableInstances as well as other extensions. UndecidableInstances is (as it says in the name) makes instances undecidable in general. I think it makes the type system Turing complete or at least it does with the other extensions enabled.
This comment is only tangentially related to the OP but I suspect you folks would be able to provide an answer... If type systems of programming languages keep getting more powerful and expressive, what happens to the divide between your "regular" program and the program run at the type level? Who watches the watchers? Does the type system need a type system? Are we not being as awesome as we could be writing "regular" type safe code?
In languages like Agda, types and values are essentially the same sort of thing. Types have types and so on; all "small" types have the type `Set` and `Set` has the type `Set1` and so on.
Most likely it won't compile if you get the recursive case wrong. Either that or its using enough extensions to make type checking turing complete.
Great work either way, I just wouldn't switch without something to replace those methods. Maybe I am doing it wrong, but in my current project I have 30-ish quickCheck functions, that are 1-5 lines each. I don't want to write them all inline in big list, as that would be hard to follow, and its not very DRY to have to write out the list of functions by hand, when a convention could do. I agree that the TH solutions kind of suck though, I'm having some issues with test-framework-th right now.
The concept itself is just mind blowing.
&gt; So many years passed and it still smells like BS. Much like your comments, 90% of which are obnoxious content-free trolling in this and various other subreddits. Knock it off.
You don't need a Turing complete language to compute a factorial. 
Haskell's type system is not turing-complete, but GHC's is.
FYI, on OSX you want homebrew.
fixed, thx
you don't need qq if you use a matcher instead of == 2 `shouldBe` 3 It is just missing the location information. Hmm, is there a way to produce a binary operator from TH ? 2 $shouldBe 3
I've been thinking about it and have concluded that you are absolutely right that my point wasn't really adding anything because if you are sending information over a network connection about an exception whose type you will not know in advance then it most likely falls under the category of an exception that you are not going to recover from.
No, if I understand correctly Cloud Haskell uses tables that map Strings to serialization and deserialization functions. These tables are generated at compile time via. Template Haskell using the `remotable` function, but you still have to manually pass them as an argument when initializing Cloud Haskell. You also have to use either `mkStatic` (for values) or `mkClosure` (for closures) on a value when you send it over the network to lookup (at compile time, using Template Haskell) the String identifier for the type and the serialization functions. This is discussed both [in this paper](http://research.microsoft.com/en-us/um/people/simonpj/papers/parallel/remote.pdf) and in the documentation for the module [Control.Distributed.Process.Closure](http://hackage.haskell.org/packages/archive/distributed-process/latest/doc/html/Control-Distributed-Process-Closure.html).
That is the case only if you are working in a particular concrete monad transformer stack. If you are writing polymorphic code and the monad isn't known and the function can't be inlined you'll get pretty awful core.
Is this kind of the Haskell equivalent of C++ metaprogramming? Like [this](http://aszt.inf.elte.hu/~gsd/halado_cpp/ch06s04.html) kind of stuff where you get the C++ compiler to compute things at compile time.
Dependent types and absence of bottoms, so having to prove termination/productivity seem to go together, but is that a law of nature? Is there or could you have a language with pi and sigma types *and* general recursion? (I'm guessing where the requirement creeps in is because of equality proofs? So maybe you would need to consign them to a separate `Prop`/`Constraint` kind like Haskell does... ?)
&gt; Windows users: make it happen. OK then. To whom should I pay and how much to get a working 64-bit GHC for windows? Is there a contributors foundation or something?
:D ... great one - hope you do not mind if I put it in my repertoire (the picture - not the goto)
Well-typed and the Industrial Haskell Group would be my starting point. Maybe the FP complete guys would be interested in doing something as well. Of course, unless you have pretty deep pockets one individual isn't going to cover that whole cost, so you would need to organise a concerted crowd-funded effort or significant commercial sponsorship.
But my point is that I much prefer `==` over ```shouldBe```. However, I also meant for it to work for any boolean expression, simply by pretty printing the AST and substituting arguments for their `Show` rendering: let a = 4; bs = [0..3] in [assert| a `elem` bs |] would say something like Failed: 4 `elem` [0, 1, 2, 3] Notice how this would automatically work for any boolean predicate on `Show`able types.
Idris is a language with dependent types that seems to be playing more and more with general recursion and non-totality. I don't know too much about what they're learning there, though.
Sure, but then that tends to lead to layered universes of types and a broad breakdown of the distinction between type and value layers.
I just wanted to randomly say thanks for taking the time to create and post these.
&gt; It won't work for your n = 1000 case though. Actually, Haskell is a great language for making that formula practical. We just need to represent the formula `a+b*sqrt(5)` as an algebraic object: data Fib = Fib { a :: Rational, b :: Rational } instance Show Fib where show (Fib r 0) = show r show (Fib r i) = show r ++ " + " ++ show i ++ " * sqrt(5)" instance Num Fib where fromInteger i = Fib (fromInteger i) 0 Fib x1 y1 + Fib x2 y2 = Fib (x1+x2) (y1+y2) -- cross-multiply the terms: Fib x1 y1 * Fib x2 y2 = Fib (x1*x2+5*y1*y2) (x1*y2+x2*y1) negate (Fib x y) = Fib (negate x) (negate y) instance Fractional Fib where -- do division via multiplication by the conjugate: z / Fib k l = z * Fib k (negate l) * fromRational (1/(k^2 - 5 * l^2)) fromRational i = Fib i 0 sqrt5 = Fib 0 1 phi = 1/2 * (1 + sqrt5) phi' = 1/2 * (1-sqrt5) fib n = (phi ^ n - phi' ^ n) / sqrt5 and the tl;dr: *Main&gt; fib 10 55 % 1 *Main&gt; fib 100000 2597406...875 % 1 Though to be honest, this is the same basic trick (log(n) exponentiation) as the matrix multiplication mentioned by yitz.
Oh, they've already did it: this is what the IHG's page says: From version 7.6.1 of GHC on, 64-bit Windows is a supported platform..
http://i.imgur.com/YXXVxpb.png Like that? Some colorschemes (like solarized, which I use here) do that automatically.
Check vim2hs/autoload/vim2hs/haskell/syntax.vim: you can write your own regex to define a new syntax type and highlight it as you wish. 
syntax highlighting is not part of the colorscheme but of the syntax 'plugins'. lbolla pointed out the right part of vim2hs that does it.
oh, most things are wrong, some seem not a big deal to me. great comment of yours.
There are many differences between OCaml and Haskell. Strict Haskell is miles away from OCaml.
Yes, I meant it an as understatement. Anyway, I hope SCB starts hiring Haskell programmers in Tokyo.
There's no deeper reason to not having Π/Σ and general recursion, just that when all your programs possibly are their own proof of correctness inconsistencies smell funnier than in Haskell. As /u/tel said, there's Idris that supports general recursion and dependent types, but even there partial and total functions are distinguished, and only total functions can be used in types so the type checker doesn't loop forever. A separate `Prop` for propositions from an "inconsistent" `Type` of non-propositions would be a non-solution IMHO, just averting your eyes from the real issue. If I can't prove the empty set is the false proposition and the singleton set is the true proposition, then something's wrong. Perhaps `main` should be of type `Partial (IO a)`.
I'll second that! I enjoy reading them every week.
Thanks for the nod! Glad you read them :-)
You don't translate monads, currying &amp;c, you use these features of haskell to generate a string that is valid javascript. As an example that might be easier to understand, imagine you have a complex system written in haskell, which uses all these features of the language you mention. When you run it, it gives you some sort of textual output, say a detailed report of your webstore's inventory. In this case it is easy to see that monads, currying and so on were not translated to an inventory report, they *generated* an inventory report. In the same way this library helps you write programs that generate valid javascript.
[`listToMaybe :: [a] -&gt; Maybe a`](http://hackage.haskell.org/packages/archive/base/latest/doc/html/Data-Maybe.html#v:listToMaybe) EDIT: oops that is not actually the function that you want.
Your mistake here seems a good counterargument to the "types tell you everything" crowd.
Parsing Haskell code seems like over-kill though if it can be avoided. It is easy enough if you let-bind everything expression to a simple variable before putting it in the QQ or otherwise limit what is in the QQ, but then you are trading off one inconvenience for another. How about taking == as an argument. 2 $must == 3 a $mustBe elem bs
I also let bind helper functions that are used for multiple tests with in a describe block. Also do notation makes multiple levels of describe nesting easy.
On the contrary, the type still tells you something. For example, just looking at the type I can guarantee you that: fmap f . sole = sole . fmap f ... no matter how `sole` is defined; even if it is a partial function!
What do you use that function for? I can't think about any use for it. Maybe you use it do do something that others do differently.
I'm doing my masters thesis around precisely that question. My result is the language [Caledon](https://github.com/mmirman/caledon).
Thanks for the in depth explanation. What was throwing me off was the pattern match for a single element. 
But not if it uses seq.
I would have called the function something like perhapsJustHead. Sounds like some kind of naughty negotiation though.
We actually used to have programmers in my group Tokyo, but shut down that office - so I can't imagine it will happen anytime soon.
Yeah, `listToMaybe` came to mind immediately but then I looked at the implementation.
Just squint really hard and it works
You might have colour schemes that don't supply enough different colours, though.
I like it. I added it to [data-extra](http://hackage.haskell.org/package/data-extra). I tried really hard to find a way to do this in terms of other combinators and couldn't see a way. Meanwhile, I couldn't help trying to write this without any ifs, and the best I got was: sole = (&gt;&gt;= listToMaybe) . traverse id . zipWith ($) (return:repeat (const Nothing)) Have mercy on my _sole._
I believe if you craft something horrible that would make the type-checker loop it will eventually terminate because some maximum recursion depth is exceeded. Not sure if that happens in all cases or just some.
couldn't you just put it in `~/.vim/after/syntax/haskell.vim`?
Look ma, no ifs! sole xs = do [x] &lt;- return xs return x Combinators: sole xs = guard ((() &lt;$ xs) == [()]) &gt;&gt; listToMaybe xs
Reminds me of the function [the](http://hackage.haskell.org/packages/archive/base/latest/doc/html/GHC-Exts.html#v:the) which is curiously exported by GHC.Exts.
So you are **not** translating Haskell to JavaScript, you are just using a set of Haskell functions to determine what your JavaScript will be? You still have to respect JS's semantics.
Just because of the potential jokes, this needs to be in the standard library.
not as of 7.4.1 http://www.haskell.org/ghc/docs/7.4.1/html/users_guide/release-7-4-1.html
What could possibly go wrong? sole x = [a | [a] &lt;- Just x]
*I am talking with a lot of certainty about how the library works. I am pretty sure that I am correct, but I have not actually used the library* &gt; You still have to respect JS's semantics. This is handled by the library. You work on a level of abstraction where the functions you use to generate javascript takes a haskell representation of the animation you want to make(That is, a value of the type `Figure`), and turns it into javascript(This is probably a bit simplified). That is what is cool about it. As you can see in the OP, the library also allows you to work at lower levels closer to the generated javascript, but at all levels (at least the ones that are exported, i.e. you have access to, maybe not the *primal* level) the library makes sure that valid javascript is generated.
Here's [the proposal](http://haskell.1045720.n5.nabble.com/Proposal-Remove-Show-and-Eq-superclasses-of-Num-td4808712.html) for removing the constraints, if anyone is interested.
It isn't true though, in Haskell the type system is typed (and extensions are making it more precisely typed, too). Types have benefits beyond early error detection and performance (which are indeed less important in the type level), such as checked documentation, potentially better error messages, and probably more.
In contrast, the [the](https://github.com/edwinb/Idris-dev/blob/master/lib/Builtins.idr#L53) in Idris stdlibs is ingenious though.
&gt; only predicate = RSet { universe = [], predicate = predicate } the `only` function looks wrong, the universe shouldn't be empty, because then the set is also empty. Rather, it should be full, so only predicate = RSet { universe = [minBound..maxBound], predicate = predicate } What is the intended behavior of `mappend`? I would expect something like this to hold observe (x &lt;&gt; y) == f (observe x) (observe y) for some function `f`. Should `f` be the intersection? Then why do you concatenate the universes?
&gt; Then why do you concatenate the universes? You would need an Eq instance to take the intersection of the universes. I suppose taking the union is the only thing guaranteed to work.
Maybe the [`lattices` package](http://hackage.haskell.org/package/lattices) has something?
I wish somebody would change pattern match failures to use `mzero` instead of `fail "Pattern match failure"` so I could use them with a clean conscience.
With this data type `mappend` might be called something like `interfere`, and be interpreted to mean that the two sets which aren't actual realized, and "observed" values interfere with each other. As well, `observe (x &lt;&gt; y)` should not equal `observe x &lt;&gt; observe y`, or for any f, `f (observe x) (observe y)`, similarly to how for vectors `|u + v|` does not equal `|u| + |v|`. I don't think I explained myself very well so please ask for more clarification. Edit: An example of what I mean by interference: allowance = 0.1 delta = 0.05 string = do time &lt;- possible [0.0, delta .. 1.0] height &lt;- possible [-1.0, -1.0 + delta .. 1.0] return (time, height) sinWave = only $ \(time, height) -&gt; abs (height - sin time) &lt;= allowance cosWave = only $ \(time, height) -&gt; abs (height - cos time) &lt;= allowance waveOnString = observe (string &lt;&gt; sinWave &lt;&gt; cosWave) 
Yes, but the stronger statement is a complete straw man. I don't think I've heard anybody, let alone a "crowd", say that the types tell you everything.
As in `the Int 3`, right? I like it.
“Applicative ⇒ Monad proposal” is approved in Haskell actually. But first… you need to make ghc to issue warnings for Monad instance without Applicative/Functor instance… So look at the comic :)
Yeah, I saw that. I think it's related to an SQL-like extension of list comprehensions. Nearly but not quite what I'm looking for. Nearer than listToMaybe though :)
Under Haskell98 (and I *think* even 2010), yes, but GHC's core libraries violate this.
When an error is unrecoverable and does not need to be handled correctly, an exception can be the easiest way to let the program crash. For everything else I tend to use Either and stuff from Control.Error
Oh yeah, you're right. Thanks, I always wondered what that was for.
Oh, if we're adding to a library, maybe we should give it a more general type. sole :: MonadPlus m =&gt; [a] -&gt; m a sole [x] = return x sole _ = mzero Or even sole :: (MonadPlus m, Foldable f) =&gt; f a -&gt; m a sole = join . liftM (maybe mzero return) . foldlM (maybe (return . Just) (const . const $ mzero)) Nothing But that's untested and I'm not sober and that's probably not terribly efficient.
It seems from this description that you might want to keep it as two separate functions - one to check if the result is ambiguous (more than one element), and another to give you Just the head of the list. The function that would combine them would look something like (m a -&gt; Bool) -&gt; (m a -&gt; Maybe a) -&gt; m a -&gt; Maybe a which looks like it's probably only a transformation or two away from a standard combinator. With the 'sole' function written as 3 conditions returning a Maybe, you're losing the information of why it failed, and so if you decide to add in error messages later, you'll need either a redundant check for ambiguity, or you'll need to modify the function. With the two functions + combinator approach, when you want to add error handling, you can just switch out the combinator.
Since you can only catch exceptions in the IO monad, you can mostly treat exceptions the same way you would treat non-termination and `error "foobar"`. As such, it's not as bad as the situation in Java/C++, since it's not possible to use exceptions for control flow outside of the IO monad.
 and how about having different color for type constructor and data constructor? is that possible? to emphasize that one is of the Type realm and the other of the Term realm.
 let const2 c = \_ _ -&gt; c in list (list (const2 Nothing) . Just) Nothing where `list` is from [list-extras](http://hackage.haskell.org/packages/archive/list-extras/0.4.1.1/doc/html/Data-List-Extras.html)
I blame cosmic rays.
`list` is also in `data-extra` =)
So I got the folds to play nice with build/fold fusion by changing `fold` to use `foldl'` implemented in terms of `foldr` (i.e. the same way `Data.Foldable` does for `Data.Foldable.foldl'`). The only non-`INLINABLE` pragma I had to use was an `INLINE` (no specific phase) on `fold` and it works now. You can see the code that I got to work here: https://github.com/Gabriel439/Haskell-Foldl-Library/blob/comparison/src/Control/Foldl.hs That's a test branch I'm using to test out list fusion alternatives. The key function is `fold3` which fuses away your example: import Control.Applicative import qualified Control.Foldl as F main = print $ F.fold3 (div &lt;$&gt; F.sum &lt;*&gt; F.genericLength) [(1::Int)..1000000000] The other two functions (i.e. `fold1` and `fold2`) were just to compare to Data.List.foldl' and Data.Foldable.foldl', neither of which fuse. However, this fusion property is lost when you switch the list to `Double`s. I have no clue why this happens and I was wondering if you had any clue why: import Control.Applicative import qualified Control.Foldl as F -- This does not fuse away the list main = print $ F.fold3 ((/) &lt;$&gt; F.sum &lt;*&gt; F.genericLength) [(1::Double)..10000000] However, the performance on `Int`s is still pretty satisfying and I'd be willing to settle for that.
The [`thyme` package](http://hackage.haskell.org/package/thyme) has time-value instances for the [`AffineSpace`](http://hackage.haskell.org/packages/archive/vector-space/0.8.6/doc/html/Data-AffineSpace.html#t:AffineSpace) and [`VectorSpace`](http://hackage.haskell.org/packages/archive/vector-space/0.8.6/doc/html/Data-VectorSpace.html#t:VectorSpace) type classes from the [`vector-space` package](http://hackage.haskell.org/package/vector-space). `thyme`'s documentation claims that it was specifically designed for use with timestamps.
`DiffTime` from http://hackage.haskell.org/packages/archive/time/1.4.1/doc/html/Data-Time-Clock.html
Cool story, bro. If you can learn how to express your oh-so-edgy ~~ *opinions* ~~ without being a petulant little jackass, be my guest. Just whining about how *nobody understaaaands you* won't accomplish anything.
Your `Monad` instance seems suspect to me since `(&gt;&gt;=)` discards the `predicate` of its left parameter. Also, your pattern for using this seems to be combining an `only` with a `possible` and `observe`ing the result. I think that would be more directly expressed as just applying filters: only pred = All . pred observe f xs = filter (getAll . f) xs fizz = only (\x -&gt; x `rem` 5 == 0) buzz = only (\x -&gt; x `rem` 7 == 0) fizzBuzz = fizz &lt;&gt; buzz results = fmap (\x -&gt; observe x [0 .. 100]) [fizz, buzz, fizzBuzz] That seems to more clearly separate the roles of `only` and `possible` rather than giving them the same return type.
If the absolute base monad is `IO` (which the immediate base monad being an instance of `MonadIO` is meant to imply), then the code is "in `IO`" in the sense that, when the definitions of each the monad transformers that got used are expanded, the resulting type is visibly isomorphic to a `Kliesli` arrow in `IO`. For example: MaybeT (RWST r w s (ErrorT e IO)) a ~ RWST r w s (ErrorT e IO)) (Maybe a) ~ (r, s) -&gt; ErrorT e IO (Maybe a, s, w) ~ (r, s) -&gt; IO (Either e (Maybe a, s, w)) ~ Kleisli IO (r, s) (Either e (Maybe a, s, w)) EDIT: By the way, `monad-peel` is defunct; `monad-control` is its successor.
The Haskell standard mandates it, but it is a mistake. 
This is actually applicable to a project I'll be setting up shortly. Is there an easy way to lump in an arbitary number of constant args, like the Haskell equivalent of a list for "modifiers"?
Interesting, thanks.
We changed that. It was getting in the way of many perfectly good instances of Num, like instance Num a =&gt; Num (e -&gt; a) which were in common use. It was a mostly bloodless coup, however, as you can modify old code by just adding the (Eq a, Show a) constraints as needed, and code that is correct for the current implementation that has the superclasses will continue to work in the old implementations.
Same boat. I admire FP Complete's work but do not feel I am ready for full time work yet.
I once implemented something similar to this, which I called a **frame**. The idea, however, is that I needed to have a hypercube type that was like an indexed by a subset of the elements of some "point" type. A simplified version (no enforcement of uniqueness nor assignment of `Int` indexes) goes like this: {-# LANGUAGE RankNTypes, ExistentialQuantification #-} {-# LANGUAGE GeneralizedNewtypeDeriving #-} module SimpleCube ( Cell -- private constructor , getPoint , Frame -- private constructor , getCells , Cube -- private constructor , makeCube , runCube ) where -- | A 'Cell' is just a @p@ value tagged with a phantom type parameter -- @s@. The idea is every use of a 'Frame' will get its own @s@. newtype Cell s p = Cell { getPoint :: p } -- | A 'Frame' is a set of 'Cell's tagged with the same @s@ index. newtype Frame s p = Frame { getCells :: [Cell s p] } -- | A 'Cube' is a 'Frame' plus a getter that can only be called with -- 'Cell's from the 'Cube'\'s 'Frame'. data Cube p v = forall s. Cube { getFrame :: Frame s p , get :: Cell s p -&gt; v } makeCube :: [p] -&gt; (p -&gt; v) -&gt; Cube p v makeCube points eval = Cube (Frame (map Cell points)) (eval . getPoint) runCube :: (forall s. Frame s p -&gt; (Cell s p -&gt; v) -&gt; r) -&gt; Cube p v -&gt; r runCube k (Cube frame getter) = k frame getter [I learned this trick in StackOverflow.](http://stackoverflow.com/questions/13078307/how-to-index-an-element-type-by-a-source-container-value)
of course. but if it's in the right file, it might get upstream :-)
Aside from the fact that you could of course mimick Maybe (or even implement it) in most OOP languagues you will always need some kind of exception handling if you talk to hardware or 3rd party libs ... so where is the point?
Right, no one's ever been able to convince me that exceptions are valuable, except for unrecoverable errors of the runtime system. I'm happy to be persuaded otherwise though, if someone wants to have a go!
I'm not sure why you would want the universe to be a property of a particular set rather than a property of the type. You could define a newtype around whatever underlying element type you have, write custom Enum and Bounded instances that only return the restricted universe you want, and possibly make smart constructors of your newtype to prevent illegal values from being created.
What’s your issue?
In my hungover state, I worry about the overhead of `toList` when the input is already a list. -- These use foldr rather than foldMap to avoid repeated concatenation. -- | List of elements of a structure. toList :: Foldable t =&gt; t a -&gt; [a] {-# INLINE toList #-} #ifdef __GLASGOW_HASKELL__ toList t = build (\ c n -&gt; foldr c n t) #else toList = foldr (:) [] #endif Is that enough for `toList` to be optimised away when GHC knows the input is a list? There's no explicit comment either way.
No, you are pretty much right. You do still want exceptions though. Concrete example: I have networking code that handles client error by disconnecting the client. The ability to just throw an exception to terminate the handler for a client is much nicer than explicitly wrapping everything in Either, since I don't plan to recover anyway.
Some reasonable uses of exceptions are: * division by 0 * deadlock, non-termination * programmer has not implemented this yet You *could* handle each of them with a Maybe monad, but doing so would be horribly inconvenient. There are some (few) instances where it makes sense to capture these exceptions. For example, when running untrusted code in something like lamdabot or ghci. An exception in this client code should not crash the entire program. Secondly, it is important to still do cleanup correctly when an exception happens, so flushing buffers, killing worker threads, closing files, etc. Sometimes the easiest way to write such code is to catch all exceptions and rethrow them.
Exactly. I find it particularly cool that you can essentially *implement* type annotations within the language itself.
This is like dream job for me(especially the GHC part). I'm also in school but still applied because 1) I had a 3,5 month internship starting on September and I could work full-time at that time and 2) I will be graduated shortly after that internship(after one more semester, which is 14 weeks in my school) so I can continue working full-time shortly after my internship. But unfortunately they never responded my emails. (edit: I sent first email about a month ago) This is heartbreaking and annoying when you didn't even get a "no sry" answer to your application for an important job for you.
it could be very sweet when you have no chances to relocate and you only have Java jobs around you(this is roughly my situation here)
I tried it. I like it :)
I love the code formatting for your blog -- may I ask how you achieve this? Also, nice post! Very simple but touches on a lot of topics and is clear and easy to understand. 
It's really a non-problem. You can always switch out your let bindings with case bindings. It just hints at the fact that let and case are two very different beasts.
Thanks for the feedback. Glad you enjoyed it. As for the formatting: I wrote the post in markdown and let [pandoc](http://johnmacfarlane.net/pandoc/) take care of the pretty printing. There are a bunch of color styles it supports out of the box. Take a look at the [examples](http://johnmacfarlane.net/pandoc/demos.html#examples) page, and check out #18. Then just make sure you use the fenced code block syntax and tell it it's Haskell source a la [github](https://help.github.com/articles/github-flavored-markdown#syntax-highlighting).
Awesome! I had been using the base Time library and was simultaneously grateful as to how well thought-out it was and annoyed at the minor inconveniences. It seems like thyme keeps the good and fixes the bad.
McGill.
Your Monad is not a Monad. It has `a` occurring in both positive and negative position. Therefore `fmap` (`liftM`) for your Monad is forced to be not correct before we even consider the rest.
I avoid exceptions like the plague. I almost always use Maybe or Either instead of exceptions as much as I can. However, something in the back of my mind says that there are probably situations where you just don't want to mess with exceptions until later. If "mess with exceptions" includes running in MaybeT/EitherT, then maybe exceptions have their place. But the EitherT monad transformer is so convenient, so maybe that's not a good reason.
A related but legal construction is Martin Escardo's seemingly paradoxical and wonderful 'infinite search in finite time' monad. newtype Search a = Search { runSearch :: (a -&gt; Bool) -&gt; a } there you give it a predicate and it'll give you an element that satisfies that predicate -- if one exists -- or some arbitrary element of a otherwise. return = Search . const then represents the singleton set. This is related to Hilbert's epsilon and I've generalized it to newtype Search f a = Search { runSearch :: f a -&gt; a } for any `Contravariant` functor `f`: http://lpaste.net/69758 This variant lets you retrieve the 'best' member according to some ranking and perform other tricks. You can see the predicate and the implicit non-empty list of as that you have to test against the predicate, but now everything is lined up in positive position.
Wait, what about honest IO exceptions like trying to write to a file on a disk that is full? Are we all assuming in this discussion that exceptions = pure exceptions?
It took me a while to understand that the monoid structure for the scheduling problem was just to throw away the schedules, but keep the sorted lists and merge them, then recompute the schedule (lazily as needed). That means the parallel trainer produced by HLearn is just a parallel sort followed by a serial scheduling assigment. (To be fair, when the number of tasks grows much faster than the number of scheduling-processors and the number of real processors on which our algorithm runs, the sort does dominate the runtime.) HLearn looks like a nice piece of machinery, but I think it's always good to try to understand what the machinery is actually doing, and in some cases the machinery turns out to not really be necessary. But this did make me wonder, is there a good parallel sort implementation on Hackage? A quick Google search didn't find anything relevant.
Interesting, does it play well with IO support from the mtl package?
Paredit for Haskell mode would be awesome.
All you have to do is modify `IO` and `ErrorT` so that `mzero` raises a descriptive error.
Interesting idea, I like the example provided in their page https://github.com/Icelandjack/Capabilities/blob/master/src/Capabilities.hs : you can restrict a function that reads from a text file to a specific path, blocking that function from messing up with the rest of your filesystem. I wish they had provided more advanced examples. I'll keep my eye on this.
But this is such a common scenario, that I think some solution out-of-the-box in Prelude. As Tekmo shows in his post this is not inherent of Haskell, with the proper implementation Haskell laziness shouldn't be an issue in many similar cases.
What do you mean by 'mzero raises a descriptive error'? Where's the difference to the current usage of 'fail'?
Hello everyone. I worked on this project for the last couple of evenings and I would like to know what you think about it. Any remark on the code, the idea or the documentation are very welcomed. Thanks!
.. and someone else discovers Lawvere theories / free monads.
I've worked remotely for long periods a couple of times, I'm currently doing so. As long as the rest of the team is comfortable with you working remotely and most communication happens in writing, it's perfectly fine. Personally, I find myself working more efficiently when doing it remotely, and overall, it seems to me that you are implicitly forced to be organized in the way you work so that your colleagues are aware of what you are doing, which is a good thing. It's also the case that I live in a city where programming jobs are basically non-existent, so remote work or relocation are about my only choices. I wish more companies were hiring remote workers, it works great once you embrace it.
That's correct. You're using monads as you would use the builder pattern in OOP. There are a bunch of functions that correspond to JS functions and you can use Haskell to generate JS in a macro kind of way. It's all statically checked, and the library does some basic stuff for you like closing brackets and other boilerplate, but in the end you're still writing JS. Here's a tutorial for a library that uses the same approach for HTML. http://jaspervdj.be/blaze/tutorial.html 
You can remove `fail` from the `Monad` class and make `MonadPlus` responsible for that behavior. As a bonus, you can write code generic over `MonadPlus` using pattern match failures.
I do like the idea. The one thing that I consider a royal mess is that different sites have different password policies. Limitations on special characters, Minimum number of capped letters, length restrictions. I'll be stoked when there is a password generator that can handle that.
Hey! Thanks for the feedback! Actually, you can already go and implement a password Schema to do just that. It uses combinators to specify the type of passwords that are generated. EDIT: A more user friendly approach could be to have some kind of specification language, regexp like. I'll have to think about this!
Well, you can always make the program shut down. Don't need exceptions for that.
The only way you can be persuaded is by writing tons of code with error checking. It becomes a mess of checks and recovery at each statement, to the point where you can't see the logic for all the error checks. In many cases it doesn't matter where the error comes from (but you still need to catch it) so it's much cleaner to have a single toplevel catch and then write all the code inside it without worrying about error checking. In Haskell monads can give you some of this, but at the cost of making all your code monadic (and it only really works if all your errors use the same facility, e.b. Maybe). The truth is that there is no one true way to check errors. It depends on the context. Exceptions do kinda suck, but so do error codes, and all other proposed error checking mechanisms. You need all of them and then pick the one that makes most sense at the time. EDIT: it comes down to good taste. A rule of thumb is - if the operation you're implementing has a reasonable chance "failing" in normal operation (e.g. hash table lookup), you should probably make that part of the caller's logic by wrapping the result in an error code. If the failure is extremely unlikely and not worth handling explicitly at each site (e.g. network disconnect, OOM, etc.) an exception will be cleaner.
If you need to fail with a more descriptive error message just use the mechanisms specific to that monad to attach a more informative error message. But pattern matches can only fail in one way, so there is no need for anything more complicated than `mzero`.
Because...?
I'm sorry to be harsh, but this is an absolutely horrible idea; note that I am not saying you shouldn't write it, and I am not criticizing the code, but I don't think that you or anyone else should use it. Every so often someone comes up with this same idea: for example, a recent /r/crypto [post](http://www.reddit.com/r/crypto/comments/1jq9lg/is_there_a_password_security_solution_like_this/). It's bad because it violates [Kerckhoffs's Principle](https://en.wikipedia.org/wiki/Kerckhoffs%27s_Principle) and [Shannon's Maxim](https://en.wikipedia.org/wiki/Claude_E._Shannon#Shannon.27s_maxim) (okay, so they're same thing...); In principle, an attacker could collect a bunch of your derived passwords, and use them to gain information about your master password. From a crypto implementation perspective, **never EVER use normal hash functions on passwords**, even with salts; you should be using a [KDF](https://en.wikipedia.org/wiki/Key_derivation_function) like PBKDF2, bcrypt, or scrypt. The "correct" way to solve the password multiplicity problem is to use a password manager like [KeePassX](https://www.keepassx.org/), because then the _only_ vectors of attack are the encryption on the password database and guessing the password. This also has the benefit of being able to change the master password if necessary. If you just want to toy around, this looks like a fun project. If you want to build something practical, though, look at [pwsafe](http://nsd.dyndns.org/pwsafe/). Again, sorry to be so harsh, but bad crypto is a big problem.
Actually, it's such that the functions `a -&gt; M b` form a category. Edit: in a sufficiently natural way
Hey, thanks for the feedback. Indeed you are correct, you can have at most the entropy of the seed password. What I meant to show is that the entropy would be 115 if you were to pick at random one of the possible password. This is in fact an upper bound. Assuming you have a strong enough seed password, this bound can be reached. An attacker trying to crack a database of hashed password will have a hard time finding the original passwords, as they probably do not appear in any dictionary. However, an attacker could indeed try to crack the seed password using a generated one as you showed. However, if the seed password is strong enough, they might not be able to do so practically. Again, this depends on how secure is the seed password. If the seed password is easily crackable, then all the system is vulnerable. 
They're isomorphic, and &gt;&gt;= makes the connection to do notation via desugaring more obvious. [Real World Haskell](http://book.realworldhaskell.org/read/monads.html#monads.do) has good examples.
&gt; functions of the form a -&gt; M a form a monoid It's important to note that it's a monoid _over endofunctors_ and not over sets, as would be assumed if you didn't elaborate. [This](http://stackoverflow.com/questions/3870088/a-monad-is-just-a-monoid-in-the-category-of-endofunctors-whats-the-problem) SO post elaborates. &gt; the operation of the monad should take the form (a -&gt; M a) -&gt; (a -&gt; M a) -&gt; (a -&gt; M a) Actually, the analogue to `mappend` takes the form (m (m a) -&gt; m a) and is called `join`; it rarely comes up in programming (one such instance is `concat`) and defining `(&gt;&gt;=)`, which comes up often, in terms of it is inefficient, so we stuck that one in the class. This is also part of the reason we don't use `(&gt;=&gt;)` for the definition. As for where `(&gt;=&gt;)` (which has a more general type signature than you gave) comes up, it comes up when you are composing functions whose output is monadic. I can't give a natural example off the top of my head, though; sorry.
Hey! No need to apologise :) ! Thanks for the remarks and the links! I will try to switch to a KDF instead of SHA-512. EDIT: I did the switch to Scrypt and I changed the documentation and added a disclaimer.
Don't mind if I jump on this bandwagon :D I'm also hiring a contract Haskell engineer for remote work. The project is a big data/analytics gig. PM me if you're interested!
It's a little confusing because when people use the word monoid, they can be referring to a set with an associative, unital, binary operation, but they can also be referring to a more general concept: an object in any "monoidal category" with an associative, unital, binary operation. The sense in which a monad is a monoid is the latter more general sense. For monads, the operation in question is actually join :: Monad m =&gt; m m a -&gt; m a You can get an idea of how it's "binary" since it reduces two applications of the monad into one. Monads have a cool feature that they define a new category with the same objects as our usual category of sets and functions, but whose morphisms with source a and target b are functions f :: a -&gt; m b. This category is called the Kleisli category, and as with any category, it has a composition operator, (&gt;=&gt;) :: Monad m =&gt; (a -&gt; m b) -&gt; (b -&gt; m c) -&gt; (a -&gt; m c) This operator is related to the "bind" operator (&gt;&gt;=) in the same way that the usual function composition operator (.) is related to the usual function application operator ($). In mathematics, monads are usually defined by their join and return operations which act like the binary operation and unit in a general monoid, but in Haskell, monads are usually defined by the bind (&gt;&gt;=) operator and return. Either way is sufficient and you can derive them from each other. Similarly, it would suffice to define the composition (&gt;=&gt;) operator instead of bind. An excellent resource for understanding monads from a mathematician's perspective is the Catster's monads presentation on Youtube &lt;http://www.youtube.com/watch?v=9fohXBj2UEI&amp;feature=c4-overview-vl&amp;list=PL0E91279846EC843E&gt;.
&gt; …if you look at it conceptually there is no catch at all, it’s really I would say sound design. There is absolutely no catch there. That’s what makes monads, and category-theoretical things, difficult to explain to Joe Programmer. Simplicity isn’t necessarily easy to understand at such a level of abstraction—people regularly name-drop physical and mathematical results like E = mc^2 and e^iπ + 1 = 0 without any real understanding of what they mean or how deep they are. &gt; …in a real program when you write to the output or read from the input you can see that it has changed in the middle. So the proper way or one way to model that is by something called resumption. Resumption is a function that you give the input then it returns a value and another function, another resumption… Is he talking about continuation-based I/O? I thought we found out that that’s a bad idea. The question seemed to want an answer about linear types—how you can model an impure function as consuming a RealWorld token, which must be consumed exactly once, and returns a new, updated RealWorld. &gt; …you use it often as a tool for thought, you do that on a white board as the design, so you kind of know “These are the effects, they are in this part of my program and I can reason about this” and you are careful as a developer that your effects are localized in that part of the program but you don’t need your compiler or your language to support that. Why not both? If you have a powerful reasoning tool, then I think it should go in your language so that you can *verify* your reasoning. 
When he says Monoid he means in this sense: -- Pseudocode instance (Monad m) =&gt; Monoid (a -&gt; m a) where mempty = return mappend = (&gt;=&gt;) Not to discount the `join` interpretation but I didn't want him to think his interpretation was wrong.
Ok, yes, endomorphisms in any category form a monoid in that way. In fact, endo-1-morphisms in any 2-category form the objects of a monoidal category, which is why we can define monads as monoids; starting with the 2-category of categories, functors, and natural transformations, passing to the monoidal category of endofunctors, and looking at monoid objects :-)
Now I have two problems
I use `&gt;=&gt;` a lot to write point-free functions. printFile :: FilePath -&gt; IO () printFile = readFile &gt;=&gt; putStrLn
&gt; This category is called the Kleisli category, and as with any category, it has a composition operator, &gt; (&gt;=&gt;) :: Monad m =&gt; (a -&gt; m b) -&gt; (b -&gt; m c) -&gt; (a -&gt; m c) &gt; This operator is related to the "bind" operator `(&gt;&gt;=)` in the same way that the usual function composition operator `(.)` is related to the usual function application operator `($)`. It's useful to point out at this point that `Control.Arrow` has this type: newtype Kleisli m a b = Kleisli { runKleisli :: a -&gt; m b } This type has a `Category` instance like this: instance Monad m =&gt; Category (Kleisli m) where id = Kleisli return (.) = (&lt;=&lt;) This allows us to use the actual `(.)` operator to compose `a -&gt; m b`-style operations. The analogue to the `($)` and `(&gt;&gt;=)` operators is then this: applyKleisli :: Monad m =&gt; Kleisli m a b -&gt; m a -&gt; m b applyKleisli f = (&gt;&gt;= runKleisli f) ...which we can also read as `Monad m =&gt; Kleisli m a b -&gt; (m a -&gt; m b)`—turn the Kleisli arrow into the corresponding function. The above needs these imports: import Prelude hiding (id, (.)) import Control.Category import Control.Arrow import Control.Monad 
The current definition of `Monad` class Monad m where (&gt;&gt;=) :: m a -&gt; (a -&gt; m b) -&gt; m b (&gt;&gt;) :: m a -&gt; m b -&gt; m b return :: a -&gt; m a fail :: String -&gt; m a reflects that it's designed for desugaring `do`-notation, except for `return`. The official reason that `fail` exists is to have some sensible implementation for pattern match failure inside `do`-notation. I'd much prefer to see something like this: class (Applicative m) =&gt; Monad m where bind :: (a -&gt; m b) -&gt; (m a -&gt; m b) join :: m (m a) -&gt; m a (&gt;=&gt;) :: (a -&gt; m b) -&gt; (b -&gt; m c) -&gt; (a -&gt; m c) since all these functions are equivalent: bind = (&gt;=&gt;) id join = bind id f &gt;=&gt; g = join . map g . f bind f = join . map f join = id &gt;=&gt; id f &gt;=&gt; g = bind g . f Of course, `(&gt;&gt;)` needs to have the appropriate counterparts as well, so that it may be optimized. We could even reduce the definition to `type Monad m = (Applicative m, Category (Kleisli m))`, but Haskell's current type instance system would make this very cumbersome to work with. ~~While we're on this topic, I'd also like to a redefinition of `Applicative`~~ class Functor m where map :: (a -&gt; b) -&gt; (m a -&gt; m b) class Pointed m where point :: a -&gt; m a class (Functor m, Pointed m) =&gt; Applicative m where zipWith :: (a -&gt; b -&gt; r) -&gt; (m a -&gt; m b -&gt; m r) (&lt;*&gt;) :: m (a -&gt; b) -&gt; (m a -&gt; m b) ~~since `(&lt;*&gt;)` (a.k.a. `ap`) and `zipWith` are equivalent:~~ zipWith f xx yy = map f xx &lt;*&gt; yy ff &lt;*&gt; xx = zipWith ($) ff xx
&gt; [This](http://stackoverflow.com/questions/3870088/a-monad-is-just-a-monoid-in-the-category-of-endofunctors-whats-the-problem) SO post elaborates. TL;DR: class Monoid a where mempty :: a mappend :: a -&gt; a -&gt; a class Monoid a where mempty :: () -&gt; a mappend :: (a, a) -&gt; a class Monoid a where mempty :: 1 -&gt; a mappend :: a * a -&gt; a ---- class Functor m =&gt; Monad m where return :: 1 ~&gt; m join :: m * m ~&gt; m class Functor m =&gt; Monad m where return :: (1 ~&gt; m) a join :: (m * m ~&gt; m) a class Functor m =&gt; Monad m where return :: 1 a -&gt; m a join :: (m * m) a -&gt; m a class Functor m =&gt; Monad m where return :: a -&gt; m a join :: m (m a) -&gt; m a
&gt; The only way you can be persuaded is by writing tons of code with error checking. Hmm, possibly. I still think I'd prefer to use `Either String` as a sort of lowest common denominator.
Hey I like this mode!
Is anybody else having trouble with the links to the introduction to the GHC architecture article?
&gt; From a crypto implementation perspective, never EVER use normal hash functions on passwords, even with salts Other than trying to avoid writing crypto code in general, if there already are libraries for it, what's wrong with a "normal" hash function? Aren't KDFs just hashes too?
1) You're more or less right about how the scheduling problem works. The only minor correction is that the run time isn't affected at all by the number of bins (if you use the right data structures). 2) HLearn has a parallel sort. It's provided by the [SortedVector](https://github.com/mikeizbicki/HLearn/blob/master/HLearn-datastructures/src/HLearn/DataStructures/SortedVector.hs) data type. Let's say you have a list: &gt;xs = [1,5,3,-10,12,22] :: [Int] You generate a SortedVector by: &gt;train xs :: SortedVector Int You do it in parallel by: &gt;parallel train xs :: SortedVector Int Details of how the `parallel` function works are provided in the TFP paper linked to from the gthub page. It essentially divides the run time by the number of processors available. 
Normal hash functions are designed to be fast. Key derivation functions are designed to be too resource intensive to brute force
I expected that the passwords would look like skabadoobeedapabwilidobiobiblpiwpbiwilbibwbiwplbdoopdoop
In fact, it can generate all kinds of password, the one given are just generated from a single schema. Another schema for instance generates 78 ascii chars. Another a 5 word passphrase. Etc. EDIT: Okay... I get it! http://www.youtube.com/watch?v=p-oCfg8cgMc
Could you fix it to build in Hackage so we can browse the haddock?
I suggest to simply output multiple encodings of the password: one with special characters, and a longer one with base32 encoding. This is a good UI requiring nothing of the user. You can always add more encodings; but they should all be hard-coded in the publicly available scat to make the utility most useful. Allowing the user to specify a schema is no good, it breaks the intended use here. You are supposed to be able to install scat on a random machine without calling home. If you have to download the custom schema from a networked machine you lose the advantages.
&gt; Another a 5 word passphrase Doesn't this require the machines to have the same dictionary?
actually no, it uses its own list of words (the 7776 words of the Diceware list)
Yes, that's a good point. I designed the schemas to be easily described using combinators, so it should be pretty easy to come up with your own and hard code it into the program.
Wouter Swierstra himself also wrote a similar library: http://hackage.haskell.org/package/IOSpec
I am not convinced that division by zero deserves an exception.
Good point. Looks like I screwed up by not looking at the `Applicative` laws long enough.
It seems doubtful to put `zipWith` in place of the antiseptic `liftA2` since the zipping picture doesn't fit with the `ap` derived from the Monad instance. Lists are the obvious case, but only one: &gt; zipWith ($) [even,odd][2,3] [True,True] &gt; [even,odd]&lt;*&gt;[2,3] [True,False,False,True] `zipWith` is a method of the `MonadZip` class, in "base" `Control.Monad.Zip` which governs the use of parallel monad comprehensions; `MonadZip` is basically the 'other Applicative' as `MonadPlus` is another 'Alternative' 
It seems wrong, in that I think things that are *not* syntactically legal[1] but that the compiler will accept will get in there as well. [1]: except where the language is defined in reference to the compiler, in which case there are no such things, of course.
This is really awesome. Perfect example of how composable Haskell code can be using type classes. Here's a bit of an example of messing around with it if it helps anybody get started: https://gist.github.com/ljsc/6200472.
It's a small joke referencing: &gt; Some people, when confronted with a problem, think "I know, I'll use regular expressions". Now they have two problems. ... with the implication that regular expressions are substituted with category theory.
Hello, sorry I'm late to the party! I'm the author of the package along with Daniel. The person who uploaded it (without me knowing) is a friend who did so after I tried to think of a low-key announcement :) “brand-new step in securing functions” is not quite what I had in mind. The idea originated as a project in a course called Language-Based Security and future goals include making it a monad transformer and adding further capabilities (such a threads, IORef, directory access, …). This is my first contribution to Haskell and I appreciate all comments and suggestions. 
KDFs are likely keyed hashes (i.e. slightly above "normal") and they're also designed to avoid various attacks like key extension and brute force. In other words, while KDFs might exist in the space of all hash functions, only hashes with good properties ought to be used for key derivation.
Is there a wiki page or blog post somewhere which discusses the practical ups and downs for these? I've seen various critiques in responses here and on mailing lists, but it's very distributed. There are also a smattering of descriptive blog posts which don't really get into the practical experience.
Parsec. Parser combinators in general.
Lens/fc-labels. Composable getters and setters like you can't imagine in most languages.
QuickCheck. It exists in other languages, but typeclasses, strong typing, and the nature of values in Haskell make it incredible.
Pipes/Conduits/Machines/Iteratees/Enumerators/"Composable-fuzzy-happy-fun-tunnels". Like Clojure's reducers libraries, but much more powerful and safe.
mtl. While monads/monad stacks are considered to be a burden often, it's kind of remarkable how mtl lets you have the compiler describe exactly what kind of context is needed for a heavily layered computation.
I don't see a way to add or subtract two TimeDiff. Is there one?
A category is just a monoidoid :)
Haskell tutorials.
Chesus, I am not alone! Heating laptop and core-eating GHC processes on Windows 7 - but no ghci-s are running. (I was doing some memory-consuming XML processing before)
I don't see advantages of Lens over native getters/setter in, for example, C#. Could you point, please?
That has been cloned to other languages by now though. 
I'm not familiar with C#, so perhaps I'm wrong that it's unattainable there. Broadly, though, (a) lenses generalize getting/setting up into bidirectional programming, (b) compose, (c) are first class, and (d) generalize using typeclasses. For instance, while it'd be fairly easy in most languages to, say, looking into an object's `dict` slot and then take the "foo"th entry and access each element individually, lens lets you build that generally lens = dict . ix "foo" . each where `lens` can now be passed in as an argument to arbitrary functions, allows you to read, write, and modify the elements (without using mutability), and, if `dict` is written generally, can be used on any value which "has a dict containing set-like things of objects". All in a typesafe manner.
I find a lot of the standard library stuff, monads, monoids, applicatives etc. just so useful that my programming in other languages calls out for abstractions like that.
The Yesod web framework uses the type system to prevent you from doing stupid things. It's a refreshing contrast to RoR and the other dynamically typed "agile" web frameworks that are hip these days.
Take a look at: http://www.haskellforall.com/2013/05/program-imperatively-using-haskell.html?m=1
I'm very partial to [`sbv`](http://hackage.haskell.org/package/sbv) which provides a high-level interface to several different SMT solvers. One of the particularly nice features is that you're not tied to any particular solver--many of the features work across multiple solvers. This is important because the most popular solvers, Z3 and Yices, are not licensed for free commercial use.
Still can't get over the fact that they read in the "wrong direction".
Didn't you end up with 3 problems in pipes? ;-)
Same here. For function application, having the value on the left and the function on the right might be the inuitive way round (at least for people whose language reads left to right) but given that historically it's been the other way round I regretfully think that lens should not break this convention.
Yes :)
It has been like this, on OS X at least, as long as I can remember &gt; import Control.Concurrent.Async &gt; a &lt;- async $ mapM_ return [1..] &gt; wait a ^CInterrupted. At this point, it's still using a maximum of memory, but if you exit with &gt; :q it all goes away. If you cmd-q the terminal, it carries on thinking about [1..] 
I only like it as a tradeoff for having the automatic type upgrade property. If lenses were newtyped to get their own category instance I don't think the combinators would be able to be as general as they are.
But they don't! They are just functions, and that's the order they compose. The only confusing part is that they don't apply to the value it looks like they should apply to. They ultimately are applied to a function, instead. \f -&gt; dict (ix "foo" (each f))
It doesn't really break from the convention, as demonstrated in my other comment in this thread.
Software Transactional Memory.
For those who gets the 404 error, I recover an html version from the google cache (http://bit.ly/14hY3NX) : https://github.com/matovitch/haskell-gap
I want to add "purity" here since STM is vastly more powerful when you can constrain the types of functions passed in.
I'm sorry, but no one is ever going to convince me that it's a good idea having a language where some code flows from left to right and some from right to left, *with no syntactic distinction whatsoever*. Nobody has ever written a Category instance that works like this and for good reason. I understand the appeal of not having to introduce a new type and force people to hide the Prelude (.) and import Category, but i just don't think it's worth it. I'm kind of baffled how otherwise reasonable people can make arguments like "omg, this is so elegant, it looks just like java!".
Interesting example of [priming][1]: after seeing "dynamically typed" I then read 'fragile'. [1]: http://en.wikipedia.org/wiki/Priming_(psychology)
&gt; I'm sorry, but no one is ever going to convince me that it's a good idea having a language where some code flows from left to right and some from right to left, *with no syntactic distinction whatsoever*. I'm sorry, but did you actually read what I said? The directionality is exactly the same as normal. Have you ever done anything like this? fmap (pure f) It could have been written like this: (fmap . pure) f This is exactly the same pattern. Note that the order didn't change.
The Applicative interface does not buy you anything over the monad interface, does it?
I haven't dived into it very deeply. Can someone explain what the free monad / datatypes a la carte approach buys over just using type classes? E.g., using some of the same examples: class Monad m =&gt; Stdin m where hGetChar :: Handle -&gt; m Char hGetContents :: Handle -&gt; m String class Monad m =&gt; Stdout m where hPutChar :: Handle -&gt; Char -&gt; m () type TTY m = (Stdin m, Stdout m) instance Stdin IO where hGetChar = IO.hGetChar hGetContents = IO.hGetContents instance Stdout IO where hPutChar = IO.hPutChar functionThatOnlyWrites :: Stdout m =&gt; Foo -&gt; m () or the secure read example: class Monad m =&gt; SecureRead m where secureRead :: FilePath -&gt; m (Maybe String) instance SecureRead IO where secureRead filepath | "secret" `isInfixOf` filepath = return Nothing | otherwise = liftM Just (readFile filepath) untrusted :: (Stdout m, SecureRead m) =&gt; FilePath -&gt; m (Maybe Int) untrusted filepath = do -- code is the same 
What's a mon?
I don't see what that has to do with this. The issue is that I can compose two vanilla Haskell record getters as `foo . bar` but if I use van Laarhoven lenses that becomes `bar . foo`. What's even worse is that people try to sell this as a feature by writing stuff like `bar.foo .= x` and proclaim that Haskell is awesome because it kinda looks like Java. The only reason I thought that it was acceptable to have the (.) operator overloaded is that people where always careful to put spaces around it when they composed functions. But i guess that's not true anymore. 
I do wish Pandoc wasn't so monolithic. Fantastic library, though.
The `(.)` is not being overloaded here. It's just normal function composition. I think you are confused because most lens libraries *do* overload `(.)` by using `Category`. The thing with lens is that it's really not the same kind of lens as you are probably used to and, in my opinion, are probably better off not even being called lenses. They are really just glorified semantics editor combinators, and thinking of them in this way make it a lot more clear why the "reverse" composition makes sense.
I'm talking about how (.) is both a regular function and namespace separator. Some people have argued that this is bad and composition should use some other name. 
I don't understand what namespace separation has to do with this. The only syntax in Haskell where `.` is used for namespace separation is when modules are involved, which has nothing to do with lenses. Also, that case *is* syntactically distinguished, by capitalization (`Foo.` is the beginning of namespace separation, whereas `foo.` is composition).
Interesting :) And I would agree; I used to be a big advocate of Django, but these days I always hope to hear that websites/applications are built with Yesod or Lift or something more solid than Django or Rails.
Amen. If you've ever debugged a race / deadlock in C++ / Java, then STM is immediately liberating.
Doesn't have to do with namespaces here. (.) :: (b -&gt; **c**) -&gt; (a -&gt; **b**) -&gt; a -&gt; c somelens :: Functor f =&gt; (a -&gt; **f b**) -&gt; s -&gt; **f t** (.) here :: ( (s -&gt; f t) -&gt; **u -&gt; f v**) -&gt; ( (a -&gt; f b) -&gt; **s -&gt; f t**) -&gt; (a -&gt; f b) -&gt;u -&gt; f v completely normal composition, I don't know how you could have a syntactic distinction. Nobody ever said composition always has to be covariant, or that there's any significance at all to a direction. &gt; I'm kind of baffled how otherwise reasonable people can make arguments like "omg, this is so elegant, it looks just like java!". Well if they're reasonable every other time and you're seemingly the only person who doesn't think they're reasonable now...
You know, like a Pokémon, a Digimon, a Hegemon….
Applicative has more possibilities: class Functor f where map :: (a -&gt; b) -&gt; (f a -&gt; f b) class Functor f =&gt; Applicative f where unit :: f () map0 :: a -&gt; f a pair :: (f a, f b) -&gt; f (a, b) map2 :: (a -&gt; b -&gt; c) -&gt; (f a -&gt; f b -&gt; f c) (&lt;*&gt;) :: f (a -&gt; b) -&gt; f a -&gt; f b (&lt;*&lt;) :: f (b -&gt; c) -&gt; f (a -&gt; b) -&gt; f (a -&gt; c) unit = map0 () map0 a = map (const a) unit pair (a, b) = map ($ ()) (map (,) a &lt;*&lt; map const b) map2 f a b = map ($ ()) (map f a &lt;*&lt; map const b) f &lt;*&gt; a = map ($ ()) (f &lt;*&lt; map const a) pair (a, b) = map0 (,) &lt;*&gt; a &lt;*&gt; b map2 f a b = map0 f &lt;*&gt; a &lt;*&gt; b f &lt;*&lt; g = map0 (.) &lt;*&gt; f &lt;*&gt; g pair (a, b) = map2 (,) a b f &lt;*&gt; a = map2 ($) f a f &lt;*&lt; g = map2 (.) f g map2 f a b = map (uncurry f) (pair (a, b)) f &lt;*&gt; a = map (uncurry ($)) (pair (f, a)) f &lt;*&lt; g = map (uncurry (.)) (pair (f, g))
By itself, no, although the `Applicative` functions could have specialized implementations for performance, potentially even with asymptotically significant improvements. Also, the `Applicative` combinators happen to be pretty convenient for certain things relative to those provided for `Monad`. Once `Applicative` is a proper superclass of `Monad` then there will really be no point in contrasting them.
Do you know of an example where the Applicative instance has implementations that outperform the ones of the Monad instance?
[Diagrams!](http://projects.haskell.org/diagrams/) It's a DSL and library for making vector graphics. Imagine making grahics iteratively and recursively. The design is really well done, too. You can arrange things relatively to other things.
Seconding Aeson. Python's `json` comes close in convenience but is obviously not type-safe.
&gt; I suppose you could have a datatype that describes all possible examples of ill-typedness, This is precisely what /u/stevana has [here](https://patch-tag.com/r/stevan/proglang/snapshot/current/content/raw/ProgLang.agda) because it makes for a much better choice when it comes to displaying error messages. I expect that one can quite easily (exercise... :D) prove the typechecking to be decidable using the information available in the counter-proofs.
The `persistent` and `esqueleto` libraries, which make ORM-style programming not only convenient, but type-safe.
In theory it buys you assurance of non-monadicity.
monads. can't have enough burritos. 
"Obviously"? Python is not typeless, it just doesn't do static type checks. The problem is not so much that Python doesn't support types (because it does), but that the `json` library has no way of asking it to decode into a given type; the 'magic dictionary property' thing doesn't do what Aeson does, but if there were a method that took an additional parameter to specify a class to instantiate for the result, then we'd be about as close as possible in Python.
Do you know of a tutorial somewhere to learn more about using SMT solvers?
&gt; ⊢ x y : A I want to parse that as "x applied to y is of type A". Does this say instead "both x and y are of type A"?
Haskell effectively does checked, structural JSON schema decodes with a much richer schema language. That's a bit above most JSON libs elsewhere. If you want to run schema free you can always just destruct on the Value type.
I think you could just use [Control.Arrow](http://hackage.haskell.org/packages/archive/base/latest/doc/html/Control-Arrow.html#g:2) "&gt;&gt;&gt;" to compose them in the opposite order if its more pleasing to you.
I know. I'm just saying something similar *could* be done, even though `json` doesn't do it.
What do you mean solid? A lot of big companies like facebook still most of their work in php and it's not like their servers are going down all the time. Funcational development might have prevented some bugs along the way, but it doesn't guarantee a stable website. 
Anything where `(&lt;*&gt;)` acts like `zip`, combining two structures with a fixed shape in parallel. The simplest example is an infinite stream, e.g. `data Stream a = Stream a (Stream a)`. The issue is that the second argument to `(&gt;&gt;=)` takes a single element of type `a` and produces a `Stream b`, but to maintain the `zip`-like behavior when creating the final result, the position in the structure needs to be consistent. So for the first input item you take the first item of the stream it gives you, for the second input item you take the second item from the stream... and it just keeps getting worse. Meanwhile, `(&lt;*&gt;)` gets two `Stream` arguments and can just do the sane thing and walk down both in parallel. Note that state machine-style thingamajobbers like iteratees and the automaton arrow, e.g. `data Automaton a b = Automaton (a -&gt; (b, Automaton a b))` behave like infinite streams for this purpose (except you'll waste more CPU and less memory). The `ZipList` `newtype` has the same issue, but it isn't a valid monad anyway due to being variable-length. The standard list monad *doesn't* have this problem as such because it keeps all the full cartesian product instead of taking the diagonal and throwing away everything else.
Oh oh, sorry, not disagreeing there.
I've written a blurb on here that went into the whys and wherefores of how they aren't really any better -- and are in many ways worse -- than the status quo, but there isn't much.
As a outsider, or somebody who has written Haskell but stopped before learned what a comonad is, 'capabilities' is a much more accessible *name* than the two you just mentioned. This may be re-inventing the wheel, but there is incredible value in making something easier to both explain and intuit. I know you're a big deal and your opinion carries a lot of weight, but frankly this sort of attitude is what puts people off of Haskell. Conversely, ideas newly fashioned in the manner of the OP can advance Haskell and the vast benefits it brings to programming.
Ah, sorry, I forgot the comma! I meant `both x and y are of type A`.
Find a good colocation space if you really have trouble with the distinction. Lonely yes, if your entire social life revolves around work. I personally found that by working from home a.) I'd be more efficient at work and have to work less (results may vary) and b.) My social life revolved more around true friends than more artificial relationships created at work. I also tend to get out more in the evenings, since I am not socially burned out from work (as a huge introvert the work day really took its toll).
Yes and rwbarton was making a joke on n-categories.
Ed Kmett's automatic differentiation library
I think his point is that it's weird for (.) to behave _conceptually_ like (&gt;&gt;&gt;) on lenses despite not being overloaded and it "making sense" if you look at the implementation. He doesn't want (&gt;&gt;&gt;); he wants (.) to behave in conceptually the same way as it does elsewhere.
Well, that is unfortunate, but in order to use it as a module path separator, you need to leave the spaces out, and the thing on the left has to be a module name. In my opinion (.) is *good* for composition (it's easy to type and visually only second to whitespace in terms of quietness, and composition is extremely important). It was a bad choice of module path separator though, for the very reason that composition is so common and important. I'm not sure what other symbol we might've chosen, perhaps | would work. In any case, lenses are functions, and the dot is function composition here in both cases. The reason that it's "backwards" has to do with precisely the sort of functions being composed. A simple lens into a field of type t in a structure of type s is a function (Functor f) =&gt; (t -&gt; f t) -&gt; (s -&gt; f s). By choosing f to be the identity functor, this essentially becomes (t -&gt; t) -&gt; (s -&gt; s) which gives a way to modify the field, which if we supply a constant function, gives "set". By choosing f to be the constant functor which gives t always, this essentially becomes (t -&gt; t) -&gt; (s -&gt; t), and by supplying the identity function to that, we get "view". Note that the field type is to the left of the topmost -&gt; in the type of the lens, i.e. the argument to a lens is something regarding the field it lenses into, rather than being the overall structure directly. With a simple field extractor, you'd have the type s -&gt; t, and the argument to that is the whole structure, and the result is the extracted field, which is the opposite of what's going on with a lens. This is why the composition of lenses happens in the order that it does, with the lens acting on the outermost structure first in the composition chain. Algebraically, we have view (l . l') = view l' . view l
&gt; Python is not typeless Yes it is. Well, okay, it only has one type but everything has a type so technically true. What you are calling "types" should be called "cases" "sorts" or perhaps "classes." It is a fundamentally different thing. The "sort" of a thing is an operational fact. The type of a thing is a static fact. In python we might operate with assertions about what the sorts of things are, but that is no different than any other kind of dynamic assertion. Python is probably typesafe (as a monotyped language that does not go wrong) and it uses sorts heavily ("stronly sorted"?) but we should not claim that these sorts are types. They are not. Not having types means you don't have type directed code generation (which is what typeclasses are). 
Euh, not really. You could try going through the examples that ship with sbv or reading some of the z3 documentation, but neither is really a tutorial per se.
you should use two judgments then. Also you probably should stick in some gammas, since iinm eq-reflect is (more?) kosher for closed terms (than open terms?).
Perhaps Facebook's engineers want to start developing in something other than PHP (and maybe they do for certain utilities/features/etc.). From an IT/IS Management perspective they are going to potentially have a very difficult time justifying the business case that retooling their entire platform in a new language is going to produce a greater return in value than the expense to make the switch. It's not impossible, it just takes a very strong business case to justify migrating all of that code to a new platform and re-tooling all processes and infrastructure to support that new architecture.
What if `Foo` is a constructor?
My statement was more one of frustration, as this has been a recurring theme, and was perhaps more dismissive than it should have been. I was bitten by this bug myself 3-4 years ago. Oleg himself was bitten by just a couple of weeks ago. What happens is someone discovers Lawvere theories and writes a post, an article or a paper about them, throwing them out there as an alternative to monad transformers. Then a bunch of people get excited about it and go beat their head against it. Much effort is wasted and then things go back to the way they were with a few more people aware of how the transformer stack is precisely the interpreter for such a theory that you have to write anyways and they give you more flexibility in the representation. New ideas add value, don't get me wrong. Lawvere's thesis on the topic was a great advancement.. for 1964. However, I had just a few days prior explained in [very great detail the issues with this approach](http://www.reddit.com/r/haskell/comments/1j9n5y/extensible_effects_an_alternative_to_monad/cbcwbsa), so I was somewhat exasperated to see it pop right back up. All that said, I do use a very similar approach when it comes to mocking myself, but I don't think using Data types a la Carte is the best way to model the large sum type, as I'd spent hours describing just days earlier. In isolation I confess that response (and probably this one!) makes me look like an ass. ;)
Facebook codes in php, but it is transformed into C++. https://developers.facebook.com/blog/post/2010/02/02/hiphop-for-php--move-fast/ Why? Because php doesn't scale.
We're fixing that soon! The proposal has been accepted. It'll definitely be in for 7.10, and _maybe_ even possibly 7.8.
You're right both times, thanks, although I didn't want to explain UIP and related stuff anyway. I'll try to make the blog post clearer without making it confusing. Btw, I'm assuming you mean &gt; So all identity proofs in *ITT* are equal to Refl, I always disliked the terminology though---the fact that "extensional type theory" means "type theory with equality reflection". In my head it simply means that functional extensionality holds :). Also, for what concerns the `no` constructor in `Check`, I don't think that having explicit evidence of impossibility of a typing for a certain syntax would be any more simpler than defining a data type data BadTerm {n} (Γ : Ctx n) : Set where var : (v : ℕ) → v ≥ n → BadTerm Γ lam : ∀ σ → BadTerm (σ ∷ Γ) → BadTerm Γ _bad·_ : ∀ {σ} → BadTerm Γ → Term Γ σ → BadTerm Γ _·bad_ : ∀ {σ} → Term Γ σ → BadTerm Γ → BadTerm Γ _nat·_ : ∀ {σ} → Term Γ nat → Term Γ σ → BadTerm Γ ... And then work with that data type to derive that no type can be assigned.
But those exist in Clojure and Akkka?
Everything which actually works is full of ugly hacks. Working code is still hugely valuable. If you're the author, please consider making the source code of your Haskell demos available on GitHub! 
I was chewed out recently for suggesting that if someone wanted to handle exceptions thrown from pure code then they should catch the exception in IO. The impression I got was that there was a way to catch such exceptions purely, but after looking for a while I haven't seen anything that would do this without returning an IO type. I want to check if that is actually correct. Is there a way of, say, catching an exception caused by calling "head []" and returning Maybe a, or Either Error a, or something pure without resorting to unsafePerformIO? Please give an example if you know one.
Things I love in Haskell that don't exist elsewhere at all: My stuff: * `ad` - automatic differentiation as a library. This is something that has been done in other languages, but the way we can do it in Haskell can use quantification to avoid tricky issues with ["perturbation confusion"](http://conway.rutgers.edu/~ccshan/wiki/blog/posts/Differentiation/) that arise in other languages. These arise when you take the derivatives of functions that take derivatives. There you have to be very careful to get things in the right order. In Haskell, I can make it so you can't get the wrong way to type check! This has been used to make robots see the world, calculate risk in financial models, do back-propagation in neural networks, layout railroad tracks, model the response time of neurons in the back of the head of a locust strapped to a table as objects loom overhead, accelerate ray tracing, speed up Bayesian networks, and a whole host of other things that people just never bothered to tell me about. * `lens` has been described as jQuery for abstract data types. It takes the things we already know how to do in Haskell and recasts them into a more composable form. It takes the familiar `+=` from C/C++ and turns it into something strange and wonderful. * `mtl` -- not really mine, but I maintain it these days. The `mtl` is one of the most under-valued bits of software engineering in the Haskell community. It is the glue that binds together lots of code written by lots of other people. It provides a compositional way to safely just 'bolt on an extra bit of functionality' into your program and keep programming. Nobody else has it so good. It provides what is in essence a dependency injection framework that you can actually reason about. Even other languages that have "Monads" or "Workflows" you don't get this. In, say, Scala monad transformers are nigh unusable, because of the need to annotate function calls with stuff like `pure[({type λ[g[_], a] = StateT[g, S, a]})#λ](x)` due to the fact that type inference works in scala right up until you actually need it to. In Java/C#/F# you can't even "think the thought" as you can't abstract over higher kinded types. * `speculation`. Microsoft Research wrote a gigantic paper on how to implement a weak version of it in C#. I wrote it as a 1-liner.. in a reddit reply to having read the paper.. and it works better than the original. Then we were able to proceed to hack the compiler to make it work better for STM. It abuses the fact that because sparks aren't garbage collection, it can throw away the *running process* once it is known it won't use its result! If you have a 70% accurate guess at the value of 'a', you can use it to get almost 70% utilization out of an otherwise idle core to turn seemingly sequential processes into parallel ones, safely. spec g f a = let s = f g in s `par` if g == a then s else f a * `bound` makes it much easier to deal with name capture when designing programming languages. It turns name capture into a monad transformer that can be written in 20 lines of Haskell and lets you get back to programming your type-checker without second guessing if you properly dealt with all the name capture issues! Other people's stuff: * `diagrams` is amazing. * `criterion` is the gold standard for benchmarking. * `darcs` really showed the world what it would mean to have an actual consistent theory of how patches only make sense in a context, and how to commute a patch over another patch automatically. It lost the fight for dominance, but I think it helped raise the bar. It was the first application with wider distribution than the Haskell compiler that it was written in. * `pandoc` is used everywhere. * `xmonad` is a great gateway drug for Haskell. It is concise. The config file is written in Haskell. It is useful. You get immediate feedback. It is ridiculously well documented and showcases good style with heavy testing, and compositional mostly pure design. Stuff that has analogues elsewhere, but where the Haskell version is "just better": * `QuickCheck` has been ported to many languages. The original author even makes a living maintaining a version of it for Erlang, but the Haskell version is by far the easiest to use due to the power of typeclasses (relative to the erlang version) and the less noisy syntax (relative to the scala version). * `STM` only really works if you can control the _non_-STM side-effects. Witness the fact that Microsoft went off and spent years trying to get it into the CLR in a universally consistent way before giving up completely. When I use `STM` in other languages I always feel like I'm holding a live wire. When I use `STM` in Haskell it is a no-brainer. * `containers` and `unordered-containers`. This isn't strictly unique. In many ways the HAMT implementation in Clojure is more powerful. But they do stand out for me in terms of library design. When I go to look for containers in, say, the various dialects of scheme or lisp I almost universally come away disappointed. Whereas in Haskell, we have an embarrassment of riches. * `snap` and `yesod` make it easy to make fast type-safe web applications. By increasing type safety you can be much more confident that your application won't go wrong at run time, and by building on a functional foundation there are fewer moving parts that can go wrong. 
See for example my blog post at http://unsafePerform.IO/blog/2013-05-01-simply_typed_lambda_calculus_in_agda,_without_shortcuts/ for a type checker that uses `Relation.Nullary.Decidable` from the standard library to give a total decision for well-typedness for the simply-typed lambda calculus.
Why the mix of narrow and broad transcription? That's just weird.
I know, but that's sort of the point -- equality reflection isn't that special if you restrict it to closed terms. IINM, all proofs of Id(M,N) are refl if M and N are closed terms, meaning it's decidable, and not all that useful to have equality reflection since you could do it by definitional equality. The power of equality reflection coms from the ability to forget non-trivial proof terms. Hence why I think it's important to have the gammas in the inference rule you gave. It ain't equality reflection unless it's got the gammas.
You can use `Prelude.(.)` and still have them compose the traditional way round, essentially by CPS transforming. Just change `a -&gt; b` into `(b -&gt; r) -&gt; (a -&gt; r)`.
Try http://rise4fun.com/Z3/tutorial/guide
But *that* breaks the traditional meaning of ((&gt;&gt;&gt;)) where data flow should go from left to right, but instead with lenses it will go from right to left.
&gt; But they don't! They are just functions, and that's the order they compose. Then they shouldn't have been implemented like that. It's already hard enough to persuade people to use Haskell. Having functions compose right-to-left, but functionlike-helper-things compose left-to-right is just another nail in the tyre of Haskell's high-performance racecar.
&gt; Python is not typeless, it just doesn't do static type checks. He didn't say "typeless" he said "not type-safe".
Unfortunately, this one went up immediately onto Slashdot. That's a place where Haskell, FP, type systems, and the principled approach that underlies them all take a serious public beating on a regular basis. Could you please write a short post there that says how wonderful these kinds of things are in general, and an elevator summary of why, just that this library doesn't really add anything and FP languages like Haskell can already do all that only better? Or something like that. If I try to write the post, I'm sure I'll get it wrong. You can post as Anonymous Coward if you're worried about the flames this is likely going to attract. Post back here so anyone who happens to have any mod points can upvote you.
Could you please upload this to hackage? That's the standard way to share Haskell projects. It's nice if you can share a github link too - in fact there's a field to do that in your cabal file - but the basics is hackage. That makes your project searchable among all Haskell projects, and makes it much easier for people to install it and contribute to it. Once you have an account, it takes literally a few seconds to upload - just use "`cabal upload`". Thanks.
&gt; Wait, what about honest IO exceptions like trying to write to a file on a disk that is full? That's not an exception. That can just return `Left DiskFull`.
hmm i don't get it. doesn't the `fromFile` combinator require the diceware/pokemon text file to be present at run time? EDIT: actually those files are quite small. how about using TH to incorporate them into the executable, or even just hard-code them as Haskell modules?
here's a tough one: i'd really like an android version of this, and i'm sure there are many who would want an iOS version. well, i guess i can just ssh into one of my VMs somewhere out there, but still...
I disagree. What you call "sort" is called "type" in the Python community, the language spec, and even things like `TypeError`. Python types are fairly strict, and there is certainly more than one of them; their semantics are radically different from any static type system (including Haskell's) though. Python being a dynamic language, it binds types to values, not names, and it performs any type checks at run-time, when the value is used, instead of at compile-time. This is the essence of the whole dynamic vs. static typing thing, but it doesn't mean we can't call Pythons types "types". By that same logic, you could demand that we stop calling Python's functions "functions", because they aren't pure and may take more than one argument.
&lt;3
I'm the author. Seriously, you don't want to see that code. And I mean, really seriously :) It's not only that it is hackish. But, like, 10 lines long fragments of the code are already infamous in the very small circle of people I dared to show it...
Thanks. Could be something like that this time. At least one time yesterday, I had another thread that was capturing keyboard input. After ctrl+c the thread kept running and prevented me from being able to type ":q" so I had to terminate by closing the terminal window. Previous times I'm not sure that was the exact case though.
[parallel](http://hackage.haskell.org/package/parallel). Not sure if it counts because at its heart it just exposes GHC functionality (through two functions), but a library in this form is certainly unique to Haskell.
What have you actually tried...?
Reading documentation for all of the above-mentioned.
Would mmap help at all? http://hackage.haskell.org/packages/archive/mmap/0.5.8/doc/html/System-IO-MMap.html
What's the problem? Just make a datatype for your struct and make the Binary instance for it: there is already a Binary instance for floats, so you shouldn't have any problem(though you mentioned 64-bit: you are aware 64bit floating point numbers are actually Doubles, right?). If your struct was just three concatenated floats you would do: data MyStruct = MyStruct Float Float Float instance Binary MyStruct where get = do a &lt;- get b &lt;- get c &lt;- get return $ MyStruct a b c put (MyStruct a b c) = put a &gt;&gt; put b &gt;&gt; put c Done that, you can use `encode` and `decode` with your datatype. [You can see more complicated examples in the docs](http://hackage.haskell.org/packages/archive/binary/0.7.1.0/doc/html/Data-Binary.html).
This SO answer might also help: http://stackoverflow.com/questions/6976684/converting-ieee-754-floating-point-in-haskell-word32-64-to-and-from-haskell-floa edit: If you go to the bottom, it seems that alloca doesn't necessarily have to call the system allocator. Sorry, I don't know much about the FFI
One program writes a struct to the file, the other program wants to read it and reconstruct the same values. On the same computer. The fact that the FP representation on x86 is IEEE-754 is irrelevant. I just want to "convert" a buffer containing a machine-compatible representation of some value (in this case, float or double) and get out a Haskell type.
Indeed it would. I'll consider that.
Go with `Data.Binary.IEEE754` and see if there's any reason not to like it -- it just works.
It's still parsed as though it's a module. You have to put a space, as in `Foo .`, if you want it to be used as a value.
Something is not getting through. Would you also "reverse" the way functions like `fmap`, `mapM`, `first`, `traverse`, etc. compose? I sure wouldn't! And there is nothing more or less special about them than the functions in lens.
&gt; Now, Data.Binary has its own serialization format [for floating point numbers, some nasty `decodeFloat` thing, not the machine's native format], so I can't use encode and decode functions.
No, I don't think I'm arguing on a technicality at all. I'm arguing that the reason people are getting so confused is that they are pretending lenses are supposed to be like record accessors from other languages, but they aren't. If you actually think of them as function transformers instead of record accessors, they are actually a lot easier to use and understand. My ability to do this would be hindered with the proposed CPS transform. It would incline me and others more toward pretending they are some black magic. It would get in the way of using them in a more traditional Haskell style (e.g. I frequently use `_1`, `_2`, etc. as a kind of `mapM` for tuple components, instead of as a "lens"), as opposed to this weird "lens" style where we pretend we're in some imperative language.
What do you mean: people are copying your work? Or people are laughing at it? If it's the former: awesome; your work must be good, after all! If it's the latter: screw them; what demos have they written with it? There are not nearly enough complete, runnable, non-bit-rotted examples of high-performance game/demo code in Haskell.
foo.c #include &lt;stdio.h&gt; struct point { double x, y; }; int main(void) { struct point points[10]; int i; for (i = 0; i &lt; 10; i++) { points[i].x = i; points[i].y = i * i + 0.5; } fwrite(points, sizeof(struct point), 10, stdout); } bar.hs import Control.Applicative import Foreign import qualified Data.ByteString as B import qualified Data.ByteString.Internal as B -- EDIT: CDouble would be technically correct data Point = Point { x, y :: Double } deriving Show -- EDIT: In the real world, use an FFI tool to automatically generate -- this so that it matches the layout of your C compiler instance Storable Point where sizeOf _ = 16 peek p = Point &lt;$&gt; peek (p `plusPtr` 0) &lt;*&gt; peek (p `plusPtr` 8) main = do b &lt;- B.getContents -- or use mmap-bytestring let (fptr, offset, len) = B.toForeignPtr b withForeignPtr fptr $ \ptr -&gt; do coords &lt;- peekArray 10 (ptr `plusPtr` offset) :: IO [Point] print coords You could also use storable vectors: import qualified Data.Vector.Storable as V main = do b &lt;- B.getContents let (fptr, offset, len) = B.toForeignPtr b let vec = V.unsafeCast (V.unsafeFromForeignPtr fptr offset len) :: V.Vector Point print vec 
Ranges in general behave weirdly for floating point. 0/0 is NaN, so the most logical thing is for the sequence to be empty. 1/0 on the other hand is positive infinity, which would lead you to expect to get the list [0.0]. Because of the weird behaviour of floating point ranges though, you end up with [0.0,Infinity,Infinity]. Here are more examples of weirdness: [1,3..2] -- Integer ranges, returns [1] [1.0,3..2] -- Same range with floating point, returns [1.0,3.0], despite 2 being less than 3 [1.0,3..10] -- Returns [1.0,3.0,5.0,7.0,9.0,11.0], despite 11 &gt; 10 
Should all compilers give the same results about these expressions, or the standard is not clear about the range notation in such cases? It would be surprising to find out that the range notation is defined differently for Integers and Floatings.
~~http://www.haskell.org/haskellwiki/Functor-Applicative-Monad_Proposal~~ My mistake! 
Monoid, not monad. :-)
[Here](http://www.haskell.org/onlinereport/haskell2010/haskellch6.html#x13-1310006.3.4) is what the language standard has to say about the definition of arithmetic sequences. It is my view that it is the programmer's responsibility to ensure that in an expression `enumFromThenTo a b c`, `c` is actually in the arithmetic sequence beginning `a`, `b`, ..., at least to within a reasonable tolerance if using a floating-point type. And, I suppose, that `a`, `b` and `c` are actually numbers rather than infinities or NaNs. If you cannot ensure this, then you should define the arithmetic sequence you want yourself, being aware of the dangers of inexact computations, infinities, etc.
All compilers should give the same results for things that don't have `NaN` and `Infinity` in. The range notation is defined differently for `Integer` and `Float`. Details are in [the Report](http://www.haskell.org/onlinereport/haskell2010/haskellch6.html#dx13-131001). I'm not 100% sure how precise the wording is meant to be taken for the exceptional values...
&gt; Then there's Data.Binary.IEEE754, which seemingly does what I want, except that it calls alloca for each conversion Do you now what alloca is? It is just some arithmetic on stack pointer. Not to mention that you don't even tried anything. &gt; PS: Now I'm just frustrated and want to switch from Haskell [I'm just doing a test project] to some language where manipulating native data is not so.. arcane. Yeah, go away from language that has perfect library for what you need. 
&gt; The sequence enumFromThen e1 e2 is the list [e1,e1 + i,e1 + 2i,…], where the increment, i, is e2 − e1. The increment may be zero or negative. If the increment is zero, all the list elements are the same. According to this definition [0, 1/0 ..] should yield [0, Inf, Inf, Inf, Inf, ...] since Inf - 0 = Inf, n * Inf = Inf but we see that the result is [0, Inf, Inf, NaN, NaN, NaN, ...] Doesn't this violate the standard?
Well, sure, you certainly could do that. I'm curious whether it has any advantages aside from syntactically reversing the order of composition to pay for the additional complexity of types. I'm usually hesitant to accept complicating types just for vanity's sake at the term level (especially when lenses already have so many type parameters).
Say `f = Identity`, and the instance looks like: instance Monoid a =&gt; Monoid (f a) where mempty = Identity mempty (Identity x) `mappend` (Identity b) = Identity mempty In this case your `apDefault` is not well-defined.
Actually, the code generating the password is pretty straightforward once you have a hash. It's just a succession of modulos and divisions on an integer hash. It's implemented in Haskell as combinators (in the module Scat.Builder), but it could be a typically be a generator object in some OO language. For instance: gen = new Generator(hash) x = gen.next(10) // A positive number below 10. y = gen.next(5) 
Just forked the repo - the code doesn't compile. What's this module?https://github.com/redelmann/scat/blob/master/src/Scat/Schemas.hs#L32 I'm assuming it contains the definition for getDataFileName? Thanks!
+1 type safe non-“stringly” typed queries are essential for trustworthy database code.
&gt; Do you now what alloca is? I know what it does in C. I have no idea what it does in Haskell. Does Haskell's runtime even have a notion of "stack"? &gt; Yeah, go away from language that has perfect library for what you need. When something feels like trying to fit a circle into a too small square, I always consider the possibility that I'm using the wrong tool for the job. Be it a wrong library, wrong approach or a wrong language. Also, I don't need to try anything to see from the documentation that pieces don't quite fit together.
The same occurred to me regarding the JSON package, and nowadays the Aeson package. I convinced a dyed-in-the-wool don't-need-no-static-types Common Lisper-for-decades colleague of the benefits of Haskell's static typing, in that the Haskell JSON libraries are actually parsers and JSON is the token format, and that the return value of the `decode` function has a type that specifies _how_ to parse and validate, in a composable way. In contrast, in CL the state of affairs is much worse, there's no way to know what to decode into (an association list? an object (of what?)? a decimal? an integer? a vector or a list?), and that Lisp treats `null`, `[]` and `false` all as `nil`, you also have an encoding problem, too. I think that this was the first time that static types had been presented to him as a helpful informational driving force of programs, rather than some kind of gatekeeper that just gets in your way. Aeson and QuickCheck might be interesting use-cases to break down walls of prejudice.
Just as an aside: Good code example, but honestly this is still pretty unsafe. Aside from the particulars of the IEEE754 format, the alignment of your structure isn't guaranteed to be what you think it is (in this case, 16 bytes wide,) and padding means you're just asking yourself to hit ridiculously stupid bugs down the road. If you add pretty much anything, this could all horribly break. You can pack it and force an alignment, but honestly it's simpler and less error prone to map a buffer and do pointer arithmetic.
The true beauty of STM is that it eliminates most of the accidental complexity of concurrency. Code that "should" just work does, without having to worry about weird concurrency bugs that only occur infrequently in production. The kind of code where STM fails due to livelock is code that would never work at all with a naive approach to concurrency and the programmer would be forced to deal with it properly anyhow. That's never going to change, since no coding technique can save you from having to know wtf you're doing (except for, perhaps, not writing any code at all).
&gt; By that same logic, you could demand that we stop calling Python's functions "functions", because they aren't pure I see no problem with this. 
&gt; I'm arguing that the reason people are getting so confused is that they are pretending lenses are supposed to be like record accessors from other languages I wonder where they ever got *that* impression from...
Sure, it's unsafe, I can't run it on a PDP-11. "Unsafe" just means you should make sure you know what you're doing, it doesn't mean you shouldn't do it. &gt; You can pack it and force an alignment, but honestly it's simpler and less error prone to map a buffer and do pointer arithmetic. Hmm? That sounds like it needs exactly the same kind of assumptions about data format, while creating more possibilities for human error, if you are going to do pointer arithmetic all over the place. If the struct is something more complicated that might have padding, you could use one of the c2hs-type tools to automatically derive a Storable instance that matches the layout of your C compiler.
This module is generated by cabal and allows access to data files. If you use cabal to build the project you should not have any problem.
It was merely a historical accident, as far as I see it. Earlier lens libraries were explicitly intended to work this way.
You don't, but I don't think the Python community would give you a lot of love. And most of the programming world would scratch their heads and wonder what the hell you're talking about.
Reid Draper will be presenting on Cloud Haskell for the Chicago Haskell group on Sept 14th. The website has more details.
I'm kinda curious what's going on here: λ floor $ (1/0 :: Float) 340282366920938463463374607431768211456 -- equal to 2^128 λ floor $ 1/0 17976931348623159077293051907890835356329624224137216... -- long number -- equal to 2^1024 -- similar, but negative, numbers are returned for -Infinity (-1/0) λ floor $ 0/0 -26965397022934738615939577861835371004269654684134598591014512173... λ (floor $ 0/0) &lt; (floor $ -1/0) True I blame IEEE
Let me clarify: Types in Haskell and Python are the same concept - they tell us which operations we can and cannot perform on values. Their *semantics*, however, that is, how exactly they are applied and checked, differ radically. The "type" concept itself isn't different (except for minor details), but how it is applied is. Similarly, the "list" concept is common to both languages, but Haskell implements them as lazy singly-linked lists, while Python uses strict arrays. An integer is an integer, a string is a string, and you can't perform integer operations on strings. That's the essence of a type system, and both Haskell and Python have one and enforce it. Just because it's done at a different stage of processing doesn't mean one is a type checker and the other is a sort checker. It makes a lot of sense discussing how Haskell and Python apply types, what the advantages and disadvantages of each approach are, etc.; but refusing to use the (accepted) name "type" for the common concept and insisting that only Haskell types are real types is silly (see also [this](https://yourlogicalfallacyis.com/no-true-scotsman)).
&gt;By that same logic, you could demand that we stop calling Python's functions "functions", because they aren't pure and may take more than one argument. Yes. Actually, I half agree with you. When working in python I will gladly use the words "type" and "function." Fine. But, in a comparative discussion of languages it confuses things. Python "functions" are procedures (some might actually be functions, but that is a different matter). "Dynamic Types" are not types--they are sorts! I really wish we could just banish that phrase, as it really does make things confusing. We have a ton of theory to understanding both types and cases. To understand why your usage leads to horrible confusion data Mono = Fun (Mono -&gt; IO Mono) | Pair Mono Mono | Bool Bool | Num Integer | String String data MonoException = MonoException String deriving (Show, Typeable) instance Exception MonoException mnot :: Mono mnot = Fun $ \x -&gt; case x of Bool b -&gt; return . Bool . not $ b _ -&gt; throw $ MonoException "type error" `Mono` is a type that is similar (although still less powerfull) than the single type used in Python. But, while `Mono` is a type, its cases are not. Yet, here we have code that is python ish, complaining about a "type error" if we hand it something of the wrong sort. I claim the language of "types" here is confusing. Not so confusing if you only use python, but terribly so if you want to be able to perform comparative design. The conclusion of my original comment stands though. Mono typed lanugages don't have type directed code generation. This means no "return type polymorphism" or its dynamic equivalent, which means extra syntax for things like parsing JSON. 
Which is why I wouldn't actually bother, of course. I quit using Python after learning Haskell for multiple reasons, and the Python community's general confusion about programming is no longer my concern.
On the same machine and same C compiler, it's no more unsafe than any FFI marshalling code.
This might work, but I think you need parametricity to prove it. 
I love acid state and am integrating it into a project right now but deep inside me I fear the havoc it will impose on my app if I have to start doing schema changes with safecopy to add new features.
I'd include at least "numeric accuracy in typical cases" and "performance" ahead of "behavior on infinities" in my list of design criteria for the implementation of `enumFromThen`. (Not that I am convinced the existing implementation is so great, won't (m+m-n) lose a little precision unnecessarily when m-n is small compared to m, as is common?) There was also a libraries proposal about reimplementing Double's Enum instance last month (email subject "Proposal: fix Enum Double instance").
Here is a counterexample: newtype List a = List { unList :: [a] } deriving Show instance Monoid (List a) where mempty = List [] List xs `mappend` List ys = List (xs ++ ys) pfList :: Monoid a :- Monoid (List a) pfList = Sub Dict instance Functor List where fmap f (List xs) = List (map f xs) instance Applicative List where pure = pureDefault pfList (&lt;*&gt;) = apDefault pfList *Main&gt; (,) &lt;$&gt; List [1, 2, 3] &lt;*&gt; List ['a', 'b'] Loading package newtype-0.2 ... linking ... done. Loading package constraints-0.3.3 ... linking ... done. List {unList = [*** Exception: /home/manzyuk/Projects/liftedMonoid.hs:16:37-76: Non-exhaustive patterns in lambda 
Ok. :) Just out of curiosity, what would be the "correct" transcriptions?
It wouldn't be for vanity, it would be for consistency, and it wouldn't need to be a type parameter, it could be universally quantified inside the type synonym.
It's getting through perfectly well, I just don't agree with it. I wouldn't reverse your suggestions because they're already covariant!
It's not un-type-safe, but it is *exactly* subtyping; I touched on this in my [post on Spellcode](http://pthariensflame.wordpress.com/2013/07/07/introducing-spellcode/). My current love of constraint-based subtyping is in large part due to the extremely powerful use of it in `lens`.
Assuming the file never leaves the machine.
Facebook developed the Thrift infrastructure which enabled them to write their backend services in over a dozen languages (including Haskell and OCaml). They might actually have more C++ than PHP overall. There is no shortage of hate for PHP in Facebook, to the point where a central page on PHP in the internal wiki proclaims "This language makes no sense." They get by using strict style guidelines, code reviews, and the hard work of dedicated dev-tools and test-engineering teams. (I worked for Facebook in 2010.)
Ah right, yes, there was an original point you were making, totally lost track of that one. Yes, you are right, return type polymorphism makes for very concise syntax in many situations; in the JSON example, the next best thing in Python would be to explicitly pass the type (or sort, if you prefer) that you want to receive.
Oh, right, sorry.
I don't think that's enough. What if you take `f a = (a, List a)`, where the monoid instance uses the one for `a` on the left component, and list concatenation on the second?
Comments like this makes writing open source software so rewarding.
&gt; except for, perhaps, not writing any code at all That's how I'm learning Haskell! Ugh, I should start a project...
&gt; We also show that with Mio, McNettle (an SDN controller written in Haskell) can scale effectively to 40+ cores, reach a thoroughput of over 20 million new requests per second on a single machine, and hence become the fastest of all existing SDN controllers. Holy shit.
Can you explain the implications to a layman?
That's awesome. Thrift sounds like the right infrastructure for the job.
How is mapM :: (Monad m, Traversable t) =&gt; (a -&gt; m b) -&gt; t a -&gt; m (t b) or first :: (a -&gt; b) -&gt; (a, c) -&gt; (b, c) different from _1 :: Applicative m =&gt; (a -&gt; m b) -&gt; (a, c) -&gt; m (b, c) with respect to the ideal direction of composition? That type signature is just a specialization of the `_1` lens. Why would you reverse composition for `_1` and not `mapM` or `first`? It's not that the first two are covariant and the last is contravariant, as you are claiming. They actually have the same variance. Edit: Here's a more directly comparable pair of functions: traverse :: (Applicative f, Traversable t) =&gt; (a -&gt; f b) -&gt; t a -&gt; f (t b) traversed :: (Applicative f1, Traversable f, Indexable Int p) =&gt; p a (f1 b) -&gt; f a -&gt; f1 (f b) `traversed` can be specialized to *the exact same type* as `traverse`: traversed :: (Applicative f, Traversable t) =&gt; (a -&gt; f b) -&gt; t a -&gt; f (t b)
Or how STM was written off in .NET for being impractical, because it needs to track every variable for mutations.
I have a dedicated 40+ core server running a haskell web spider, and it is difficult to get it to use even close to all the available resources.
I sort of meant as a regular individual. I edited. :P That said, can you try with HEAD? I'd assume that 40+ core server is production, so maybe you can't try it there. But just a thought (and the results would be interesting probably. As dcoutts said, the paper really addresses multicore scalability, specifically.)
I tried to find something nice to say, but really can't beat that comment :-) Great work guys! 
Also, it would be nice to see if it improves any of the Haskell web frameworks in [these benchmarks](http://www.techempower.com/benchmarks/).
Yes! I just moved to Chicago. I've been waiting for a Chicago Haskell group. :)
Would you explain how are we computing the third value? Instead of 0+Inf-0 = Inf The value should be computed as Inf+Inf-0 = Inf Unless I missed something in the logic. 
I want you to write up a focused blog article on this point. I don't know how to best get it across, but it's fundamental to realize that Haskell types and functions are similar to mathematical types and functions which are non-arbitrary things. There are lots of posts floating around about untyped===unityped, but few that focus on what it means to have something really be a *type*.
OK thanks I'll check it out!
&gt; No. The compiler is always free to change alignment and packing properties however it wants. Actually, it is not. Each architecture has a "natural" alignment for each supported data type and the compiler will use that unless you tell it otherwise. Sure, the compiler is free to choose a larger than necessary alignment, but that would be considered a waste of space and 'bad' generated code (IIRC, it's called "quality of implementation issue") so no compiler does that. &gt; Floating point is already peculiar due to the nature of IEEE precision modes and all other kinds of crap (ARM is different than PPC is different than x86 etc.) Precision modes, etc. have nothing to do with the bitwise representation, which is precisely defined by the standard. When transfering binary IEEE floats between two conforming architectures, you only need to take care of endianness. If some CPU does an incorrect thing when encountering something "weird" (NaN, denormal, negative zero, infinity..) it is not a conforming architecture and probably is anyway unsuited for any kind of serious numeric work. &gt; Realistically I'd say you should just not serialize doubles and use fixed-point, but that's a bit besides the point. Bitwise interpretation is identical on all IEEE-754 conforming architectures. No reason not to use it.
Yes, I have considered encoding the binary data to hex or BASE-64 and compressing it. I want to do it directly as an exercise because I code in C++ for living, and I'm evaluating Haskell as a replacement for C++ for my hobby projects. So I want to see how it stacks up to C++ when I need to get some dirty s*t done that's otherwise rather trivial in C++. Actually, I'm using an intermediate data file because it is generated from an underlying data source from which something sensible can be extracted only through COM interface. Since HDirect is currently very broken, I had to write a C++ program to extract the interesting bits and dump it to an intermediate file.
It is implemented as numericEnumFromThen n m = n `seq` m `seq` (n : numericEnumFromThen m (m+m-n)) 
okay. soon.
The Zen of Python was an easter egg (see also `import antigravity`). That said, it has mostly sound advice (advice that, especially wrt to namespaces, Haskell should have listened to - I'm looking at you, awful record syntax).
The "speculation one-liner" is presumably what eventually became specBy :: (a -&gt; a -&gt; Bool) -&gt; a -&gt; (a -&gt; b) -&gt; a -&gt; b specBy cmp guess f a | numCapabilities == 1 = f $! a | otherwise = speculation `par` if cmp guess a then speculation else f a where speculation = f guess in the Speculation module?
It's very cool, but limited to the category of problems which don't require IO.
Yep. There it is a little more anal retentive about not spawning off background threads when there aren't any to be had.
What are some good textbooks for learning how to develop compilers well? Contributing to GHC interests me, but compilers/interpreters are the one area of computing that is effectively dark magic to me still. There are a number of recommendations on [Hacker News](https://news.ycombinator.com/item?id=136875).
(x-comment from my reply in HN) I've read most of LYAH, and I have no problem understanding mathematical concepts, but man, Haskell people truely don't know how to convey this "awesomeness" they keep talking about. 1. Where do you use automatic differentiation? I've done machine learning, signal processing, etc., but never even heard of it until now. Why should I care? Your pitch should include that (especially when the wikipedia article doesn't really provide real use cases). 2. What's special about lenses? I tried reading http://www.haskellforall.com/2013/05/program-imperatively-us... but there's no summary of what this is about, and from the first paragraphs it seems like a Haskell workaround for setters and immutability. Again, I feel like the community is not pitching these things correctly. People like me start reading, don't understand the point of it all, and give up. And I can go on... What is the target audience of these features (or Haskell itself)? Is it people like me, or is it more hardware validation engineers, automatic proof system developers, database people?
The floating point serialization of the binary library is platform independent - it works on the same computer, a different computer, any computer. That's what it's designed for, because that is a very common use case, and it does it well. Calling this behavior "arcane" just because it doesn't happen to be what you need right now is disingenuous. Fortunately for you, Haskell does also provide the tools needed for your case. However, you are quick to dismiss the easiest potential solutions without any real justification. To begin with, why are you rejecting straightforward serialization of your data as text? In my work I serialize and deserialize files in the hundreds of megabytes all the time. On modern hardware, and with fast languages like C and Haskell, you are talking about a difference a few seconds at most. If you need to process hundreds of gigabytes, though, then you may have a problem. Then you reject the IEEE754 library, which is designed to do exactly what you want, because it "calls `alloca` for each conversion". So what? Try it - I very much doubt that will be an issue for you, as explained by others here. If you want to throw out everything that has ever been done without seriously considering it, then go ahead and re-invent the wheel. You may come up with a new approach that others can benefit from. But please don't sneer at good work that other really good people have done.
Actually, use `NominalDiffTime`. From the comments in that module: "If you don't care about leap seconds, use UTCTime and NominalDiffTime for your clock calculations, and you'll be fine."
*Re: AD* Automatic differentiation says that we can use the chain rule and the fact that we build up functions out of more primitive functions to calculate the derivative of a function at the same time as we calculate the answer for a constant factor slow down. In general for a function of n inputs and one output, it increases my space complexity to match my time complexity, but usually costs a fixed factor of 3x or so to give you the derivative of the output with regards to all of the inputs. This means you can do gradient descent with the actual derivatives rather than approximating them by wiggling the answer a bit. Dan Piponi won an *Oscar* for using a very simple version of it to run 3d camera models backwards to fit them 3d actors into scenes in movies. If you've done machine learning you've tripped over back propagation. Back propagation involves sitting down and cranking out what the derivatives of the function would be. Automatic differentiation gives you back propagation for your neural network for free, independent of acyclic topology or choice of 'hat' function. *Re: Lenses* It is a fairly tricky sell to explain lenses to someone coming to Haskell from the outside, but you can think of `lens`-style lenses as providing you with a "swiss army knife" of highly composable combinators for getting down into nested structures of all sorts of types and changing out one or more targets in a principled way that enables the types to change without losing type safety. Within the community it has become popular as it leads to very concise code that you can still reason about. An example is that you can work with `bytestring` through the bytestring API, which provides a sort of c-like dense array type, and consists of what is probably 50+ functions that collide with almost every name in the `Prelude`. Or you can do so by using the lens combinators that are designed to with everything and the 2 or so bytestring specific lenses/isomorphisms that `lens` provides. Use of the library goes from colliding with everything in sight to working with 2 names that don't. This doesn't capture the feeling of working with lenses, but is a very "bread and butter" example of how it makes things suck less for users. *Re: Audience* As for the audience of these features. I tried deliberately to take an interesting cross section. AD gets used a lot in finance for e.g. figuring out why your portfolio made or lost money, computer vision, cleaning up images using maximum entropy deconvolution, training neural networks, statistical modeling. I find it particularly interesting because it lets me throw away decades of hacks in graphics work. It definitely is the sort of thing a computer guy who cares about crunching numbers will use more than one who is writing web apps. *Re: My pitch* I didn't give it a whole heck of a lot of thought as a pitch for a larger audience. Someone asked an open question about what it is in Haskell that I can't find elsewhere, and so it amused me to take a few minutes to answer. I certainly wasn't expecting a huge influx of questions and emails from folks from HN. =) Haskell itself is a general purpose programming language. It finds use in a lot of areas. I work in finance. I like to write compilers. I also like to crunch numbers, play with graphics, and I learned category theory by playing around in Haskell. I have a highly mathematical bent, having reinvented myself into more of a mathematician from a programmer later in life, so most of my examples are of that sort. I can say that I personally find it incredibly rewarding, and that is coming from someone who before he found Haskell had been programming for 15 years and had grown rather progressively more disillusioned with finding "anything new under the sun". A large part of the reason I still enjoy programming is because Haskell exists.
The thyme package is basically the same as the time package, except implemented to be fast and with less features. Unless high speed is an issue here - which OP didn't mention - why not just use the existing convenient `Num` instances in the time library?
I think that'll happen once 7.8 is released. See https://plus.google.com/115504368969270249241/posts/C3Cak1Ndk5w. Let's hope it makes a big difference. It's interesting to see how well Go performs, since that also has lightweight threads. Do you know if the existing benchmark implementations already use the 'yield' trick?
mm, turns out `floor $ 0/0` is equal to `negate $ 2^1024 + 2^1023`, guess it doesn't have a sign bit or something.
Thanks for the reply! 1. AD: After reading your reply and http://justindomke.wordpress.com/2009/02/17/automatic-differentiation-the-most-criminally-underused-tool-in-the-potential-machine-learning-toolbox/ , I understand that it's a "better" symbolic diff. (although it operates differently) technique. I'll look into it further, but I have to say that in my experience, little as it is, ML people don't spend much time / paper-space differentiating their loss functions. Maybe it's more common in the classical NN community, with back-propagation and so on, but I didn't encounter it in modern NN like convolutional networks or RBMs. So I'm still not sure that the problems Justin Domke mentions actually exist. 2. Lenses: Yeah... I still don't get it. I'll try to re-read that article. 3. Audience: I know nothing about finance applications, but the use cases you mentioned are interesting. Can you link me to an article? As for CV, MEM, etc., I guess it might have some advantages over manual work or running it through Mathematica. Thanks for the reply! 
Good point. 
There's also Hidden Features of Haskell: http://stackoverflow.com/questions/211216/hidden-features-of-haskell
&gt; Calling this behavior "arcane" just because it doesn't happen to be what you need right now is disingenuous. 1. Default format of Data.Binary is not native. 2. I called manipulation of native binary data arcane because it's not obvious from documentation how to get pieces to fit together. I like rwbarton's 2nd solution best. 3. Definition of arcane from webster: "known or knowable only to the initiate". I think rwbarton's (esp. 2nd) solution qualifies. Unfortunately, it's not inferrable from documentation, which justifies the "arcane" attribute. &gt; To begin with, why are you rejecting straightforward serialization of your data as text? I'm not convinced that (printf . scanf) will be an identity (under bitwise equality) when one of the two is performed by a different runtime. Until you prove this to me somehow, you can cut out your "argument" about speed and cheap storage. Converting FP numbers to/from decimal is a non-trivial affair, and which has been a topic of a number of research papers. As I wrote elsewhere in this thread, I also want to evaluate how well suited Haskell is for use in the "wild" where you don't have the luxury of choosing file format. For example, the petroleum industry often uses data files with IBM FP format. There's no package for that, and only approach like rwbarton's or mmap will work. rwbarton's 2nd solution is "THE" solution I was looking for and trying to achieve myself.
hence, for third term m=Inf, n=0, so third=Inf+Inf-0=Inf 
You should rethink the name, it might return unpleasant results if your users search for it ;-)
As you might have guessed, my native language isn't English :) I wanted to find something shorter for *password scatterer*, and I thought about *scat*. Would this really be an issue?
One thing this paper clearly shows is the importance of RTS/runtime inspection tools like ThreadScope and the GHC EventLog, both for development as well as operations purposes (JMX might well be one of the best things available in the Java ecosystem, from an industrial application perspective).
So to be clear, Mio will replace the old "new IO manager" by default in GHC 7.8? I.e., if I compile with GHC 7.8, I'll be using Mio at runtime?
Hopefully the superfluous names can be sorted out as mentioned in the proposal. Having 2-3 different names for every operation just sucks.
&gt; Default format of Data.Binary is not native. Native to what? The format is designed to be platform independent. That means that if you serialize using this library on any OS and on any hardware architecture, you can deserialize on any other OS and hardware and get bit-equivalent IEEE floats, assuming you mind the bit resolution. I know that's not your use case, but it's a common use case, that's what it's for, and it works well. &gt; I'm not convinced that (printf . scanf) will be an identity You're right. Besides the fact that `printf` and `scanf` are considered by many to be unidiomatic for any use in Haskell for other reasons, decimal representations of floating point are subject to round off errors. Instead, you would use the `decodeFloat` and `encodeFloat` functions to represent the exponent and mantissa separately as exact integers. Then, the classic "easy" way to serialize/deserialze the integer tuples would be using their `Show` and `Read` instances. But nowadays (and especially in your situation) one would prefer the fast builders and readers included in the `Data.Text` library, or some other combination of fast build/read/IO libraries. Besides the fact that the code for text-based formats is by nature easier to write and maintain, there is also the advantage that having your data in human-readable form often saves a lot of time. We find that those two advantages can easily offset the occasional inconvenience of processing and moving around larger files. Though again, it depends on your scale; with terabytes or petabytes, the situation might be different. And of course, if you are tied to a specific file format, then you are tied to it. &gt; I also want to evaluate how well suited Haskell is for use in the "wild" where you don't have the luxury of choosing file format. It's a matter of taste. I find it pleasant and convenient. For example, I wrote a library to parse the binary Olson time zone files found on most Unix-like systems. I used the binary library. I found that I could just read the spec and translate it almost word for word into Haskell. &gt; For example, the petroleum industry often uses data files with IBM FP format. There's no package for that... If not, then it's not a big deal to write it. If you can, please share by uploading to hackage. &gt; and only approach like rwbarton's or mmap will work That's certainly not true. rwbarton is an excellent Haskell developer, and his solutions are good. But he is starting with your assumption that you will not be using one of the several much simpler ways to do it. Some of them weren't even mentioned yet in this thread, because you started off the discussion by rejecting even the use of `alloca`. If you haven't first tried the simpler ways and done some benchmarking, then using this kind of extremely low-level approach is very likely a massive premature optimization.
Yep, my braino 
&gt; Native to what? The underlying machine, of course. As if I had not already explained this. &gt; If not, then it's not a big deal to write it. - cut - &gt; rwbarton is an excellent Haskell developer, and his solutions are good. Indeed an excellent communicator, not just an excellent Haskell coder: unlike you, who is trolling about how I should use text format or some package instead, he TAUGHT me HOW to implement that "not a big deal" instead of telling me to do something else. If I could, I would vote rwbarton up 1000 points and you down for 1000 points. This is how much I value the contributions of you two. Now, EOD. You have not contributed anything of value to me, and I've wasted enough time on you.
If you want to "reach in and modify an array without doing a full copy" in Haskell, just go ahead and do it. Seriously, it's fine to use mutable arrays, they're not in the Haskell '98 standard library, but they do come in the libraries that ship with most Haskell implementations (I only use GHC, which has them, and I know Hugs has them, but mutable arrays are so useful I'm sure they're provided elsewhere too). See the [Arrays HaskellWiki page](http://www.haskell.org/haskellwiki/Arrays) for more information on the various sorts of arrays available in Haskell.
Would a Standard respecting definition be as follow, or are there some considerations to take into account and I missed: numericEnumFromThen n m = n `seq` m `seq` (n : intermediate m (m-n) 1) where intermediate first step num = first + num * step : intermediate first step (num+1) 
We've actually had a mailing list w/ Google groups for quite some time but I've always meant to set up something more accessible. Finally pulled the trigger and set up the site. We're glad to have you!
Sometimes you want to use the most efficient algorithm which indeed involves in place mutation of an array. That's what ST is for
First off, if you want to mmap the input file, you can skip ByteString entirely and use mmapFileForeignPtr from the mmap package. &gt; How was I supposed to know about it? You're not, really—the module has an `{-# OPTIONS_HADDOCK hide #-}` pragma. I'm not a big fan of this. It's not like the existence of Data.ByteString.Internal is a secret. I usually use the documentation on Hackage (in this case http://hackage.haskell.org/package/bytestring). There, the module shows up but the link to the documentation goes nowhere, not much better. I actually used `:browse Data.ByteString.Internal` in ghci to find the function I was looking for (toForeignPtr). You could avoid ByteString entirely by allocating your own buffer (Foreign.mallocForeignPtrArray) and using low-level IO (System.Posix.IO.fdReadBuf, er, I don't know whether this works on Windows) and handling partial reads and so on, but that's awfully inconvenient compared to using Data.ByteString.Internal. Perhaps it would be best for Data.ByteString.Internal.toForeignPtr to be moved to Data.ByteString.Unsafe.
Yes (if you link with `-threaded`).
Yes, one of the things we worked on during the [parallel ghc project](http://www.haskell.org/haskellwiki/Parallel_GHC_Project) was improvements to ThreadScope to help Kazu and others with just these kinds of issues. It's not polished, but we can now integrate data from linux perf with the RTS eventlog and look at it in ThreadScope. This has the potential to really help with looking at system &amp; I/O performance issues (e.g. can see syscalls, when threads are de-scheduled by the kernel etc).
To be super-clear, Mio builds on and makes major changes to the existing "new IO manager", rather than completely replacing it. I love the paper, but I also feel it claims to be a somewhat bigger thing than it really is.
That sounds pretty awesome. Would love to give that a go.
I don't know for sure what the other benchmark implementations use.
Anyone knows whether McNettle is available somewhere? (Outside the research group, that is)
Wow, thanks a lot! This has been the most helpful and informative answer!
Neither: the code is rather funny, but in a very scary way. Believe me, it's not code anybody can learn from, except maybe learning how no to do stuff It is also bit-rotten and non-high-performance (today's machines are so friggin fast that performance doesn't really matter for this simple stuff). And the OpenGL binding I use is a personal fork from like 5 years ago (since when the "official" binding was completely rewritten)
"...and then much of the code changes in Fay would probably be in simple string generation code." The author thus demonstrates complete and utter ignorance of what is required to create or re-target a compiler. The differences between languages (human or computer) are far more profound than just "which string in language X or language Y corresponds to this concept in language Z."
I wholeheartedly agree with this. Threadscope played a huge role in discovering the various anomalies that we ran into. This work would have taken much longer without it. Many thanks to all the Threadscope developers and maintainers! 
I see a problem with this plan: The output would be Perl.
Yes, I want to make it clear that Mio is definitely not a complete rewrite of the "new IO manager"; rather, the "new IO manager" was a starting point for Mio (a very nice place to start from, indeed) and much of the structure of the "new IO manager" is unchanged in Mio. We called it "Mio" because calling it the "new new IO manager" or something like that felt awkward, so we just made up a new name. But I hope that does not give the impression that it is a total rewrite.
There's actually an embedded domain specific language for solving PDEs that even generates CUDA code: http://en.pk.paraiso-lang.org/
Not just "stops scaling" but in some cases performance per core worsens, or even performance overall worsens with every additional core. For web applications that can be deployed on hundreds of single-core VMs if scalability is needed, it's not a problem, but for bigger servers that need to scale up, it's been a waste of resources to give your machine multiple cores. This appears to reverse the situation, I look forward to seeing the new benchmark numbers when 7.8.1 is released.
w.r.t doing work directly in Haskell on big arrays it turns out that lots of calculations can be streamed, so using `vector` or `repa` or `accelerate` can usually let you focus on the bigger picture in your algorithm and still maintain quite aggressive performance.
I don't anticipate the superfluous names going away any time soon. The migration would be too painful. Having default definitions in terms of the others in the superclass at least means you can eventually phase them out in your own code though.
Hello everyone again! Some news for those who might be interested: As I previously stated, the current version of scat uses Scrypt, which is way more secure than on the first version. Also, very important point, scat will now strongly encourage people to use, in addition to their password, a random string a characters, adding way more entropy (128 bits are encouraged). The idea is that this code is kept either on a hardware key, or on a piece of paper in your wallet for instance. More details in the README for those interested. As a side note, I do not plan on changing the name. I hope people will first think of password scatterer or scat singing instead of anything else! The program is now available on Hackage, and the documentation has been updated on GitHub. As always, I welcome any remarks you might have. Cheers
Yes. the `io!` macro blows up on me at runtime, not compile time. It helps but isn't enough.
...you ~~never~~ always know whether the size of the interval can perfectly divide the size of grid. FTFY
0/0 is Not a Number - no, really, they call it that, NaN - and as such, it has a metric boatload of conventions about its handling. Not the least of which is that no NaN is **ever** equal to any "other" NaN, even itself! Charming, that.
Sure, but that's where `floor/round/ceiling` comes in: λ (0/0) == (0/0) False λ floor (0/0) == floor (0/0) True λ floor (0/0) == (negate $ 2^1024 + 2^1023) True
In before, "Do you even lift, bro?"...
Has there been any comparison with something from Erlang land? My boss is curios.
http://elm-lang.org/docs/Signal.elm (lift section toward the bottom)
I only found this: https://github.com/AndreasVoellmy/nettle-openflow
That is the old (Deprecated) `Sytem.Time`. `Data.Time` is the new library which has the `Num` instance.
It's not *quite* the same API, but aside from that, `thyme` has instances for `AffineSpace`, `VectorSpace`, and `AdditiveGroup`, like I mentioned. The fact that the "intermingling" operations are type-classed is a huge advantage over `time`, and one that allows the various utilities in `vector-space` to be used to great effect when manipulating time-values. In theory, one could just use `time`, but, by the same reasoning, in theory, one could just use `old-time`, or even just use raw `Int#`s for all values; that doesn't make it a good idea. :)
Thanks. That was going to be my next question. I mostly do statistics-style regression models on mortgage data, which naturally runs in the few tens of GB range, small enough to fit on a single machine, if you're careful. I have to be extremely careful with my main workhorse, R, to keep it from consuming all RAM. Python's pandas does better. One of my Haskell fears is that I wouldn't be able to control memory use sufficiently, but it seems like there would be a way to make it work.
I know I did a double take at the title. Especially at the thought of stackoverflow trying to explain lifting.
Haskell doesn't celebrate it with a pseudo-philosophical list of random things that sound more intelligent than they are. `import antigravity` is fun, though. Oh, and record syntax is indeed awful (although I love how record fields double as accessor functions) - but nothing `Control.Lens` can't fix.
There we usually turn to streaming approaches. I routinely crunch a lot of numbers that don't fit into memory at the same time by using `iteratees`/`pipes`/`conduit`/`multipass`/`machines` and just touching the records as they fly by, and setting up for multiple passes if need be. I mean, that is what sufficient statistics are for, right? =) Basically just steal tools from the hadoop/bigdata crowd. ;) The trick is realizing that most calculations you want to work with have a formulation as a monoid or a group or just some online streaming fold and that you can gather up all of those operations into a larger streaming fold that does everything at the same time. It is far easier for me to do this in Haskell where I don't have some primitive data frame type that does so much for me that I'm crippled without it than it is in R where if I'm not using the built-in structures I'm deaf, dumb and blind! =)
I never understood why that's all necessary, Haskell has it, too. Also, I like Elm's applicative operators. ` scene &lt;~ Window.dimensions ~ Mouse.position ` is pretty cool.
I can't believe nobody mentioned [EKG](http://blog.johantibell.com/2011/12/remotely-monitor-any-haskell.html). I used to work as a DevOps and this would be my wet dream. The fact that you can add this kind of monitoring to **ANY** process is insanely awesome.
There was an [initial announcement](http://www.reddit.com/r/haskell/comments/1fvtbf/fp_complete_launches_haskell_in_real_world/) of this competition a couple of months back. Just wanted to let you all know that is indeed happening -- and we are putting up a $1000 first prize every month. We'll keep it going if there are good entries. Note that a submission should include tutorial material as well as code -- as with School of Haskell, we are trying to help increase the supply of great sample code and great applied lessons. Unlike most programming competitions, the focus is heavily on applications that solve real-world problems, as opposed to clever advanced coding or abstract concepts. We're working to spread the word that Haskell is not just for research but is also ready for practical use -- and to lower the barriers for new users.
Switching to C++ makes sense given the OP's expertise. We might be Haskell fans here, but we are not bigots. And, if you want a a library to be cross language, you would have to give it a C shim anyways. That said, ofcourse you can allocate a buffer and then write to it in Haskell! Use the ST monad or work in IO!
Have you checked out? http://www.haskell.org/haskellwiki/Applications_and_libraries/Mathematics
For every complaint the author has about "in Haskell you can't-" to which you want to say "but you can...", consider this: Is it the author's fault that learning to do things in Haskell in the style he wants is difficult? Even if someone else knew of solutions to his/her problem that doesn't necessarily mean it should be expected of the author to go to great lengths to find these answers.
"Numerical problems" is pretty non-specific; the answer depends on the type of problem you're trying to solve. I'm a mathematics student that likes to have fingers in all kinds of different pies, so here's my experiences with the main classes of math problems I've solved with computers. * The "32 gigs of arrays per node" problems. I've worked with a variety of simulations: electrodynamics, supersonic fluid dynamics, semiconductor physics, and Monte Carlo quantum chemistry. I didn't require anything fancy like adaptive mesh refinement, so I was perfectly happy crunching 4 gigabyte arrays in Fortran as long as I stuck to shared memory parallelism. Unfortunately, MPI is a huge PITA, especially in Fortran. I would also not write anything from scratch that required fancy data structures in Fortran or C/C++. /u/edwardkmett seems to be the man to ask about doing this kind of stuff in Haskell. * The "experimental mathematics" problems. If I don't require a particular mathematical software package, Haskell is my go-to language for experimental pure mathematics. It's a good environment for generating conjectures in, especially if you're working with algebraic, categorical, or combinatorial doodads. * The "data analysis/ugly symbolic computation problems". I just use Sage or Mathematica for these. The main thing stopping me from using Haskell more is the pending release of GHC 7.8, and the relative immaturity of distributed Haskell computing. It will have all kinds of SIMD goodies, more mature DPH, better array fusion, a better IO manager, type-level naturals, and probably lots of other things.
I did. I am interested packages like rungekutta http://hackage.haskell.org/package/rungekutta 
I think the Zen of Python was taken much more seriously than it was meant to be. And I agree that `Control.Lens` is great, but does not change the fact that you need to manually "namespace" your fields by prefixing them (not to mention the odd business of starting your field names with '_'). So, ok, you have "real functions" instead of accessors, but most of time you just want accessors, and it would be trivial to turn a chain of accessors into a real function with a lambda. Not to mention the fact that you have a number of alternatives to `Control.Lens` - that's a big Lisp, everything-is-a-library, syndrom.
We have much in common. I am a mathematics student, too. I have learned about PITA, and I am doing parareal now. Parallel computing is one of my interests or called major. Thanks for your comment.
I find using haskell only without any other packages will not get enough accuracy. Maybe it is my mistake, but I want more things like the package you recommended.
I prefer that approach also. Exceptions are very nice for the case where you want to have the program exit violently on a particular error, because you can pretend that error doesn't exist and let the exception mechanism take care of things. However, if you want to handle some exceptions (but not others), or you have to take care of cleanup (e.g. you're implementing something like async), I've always found exceptions to be difficult to reason about and generally inhibiting understanding. And a common source of bugs. This seems to be true for any language that supports exceptions, leading me to believe that exceptions are the culprit. Aside from that, there are two good reasons to use exceptions. Async exceptions are the only mechanism (in Haskell) for thread pre-emption. This means they're the only means for implementing certain concurrent communication algorithms. This is admittedly a niche use. The other reason is performance: ghc cannot as yet always remove the extra indirections caused by Maybe/Either/etc. If the failure needs to bubble through many layers or functions, the impact can be quite high.
I think you need to be much more specific in what you are looking for, before you can have better answers to your questions. Accuracy in what sense?
It's not possible without unsafePerformIO or similar. This is by design, the paper [A Semantics for Imprecise Exceptions](http://research.microsoft.com/en-us/um/people/simonpj/papers/imprecise-exn.htm) describes in detail why this is so. Basically, the problem is that it's often possible for pure code to throw more than one type of exception, and without an evaluation order (which Haskell leaves unspecified) the specific exception that's thrown may be non-deterministic. GHC resolves this by restricting exception handling to IO, which allows exception handling to have reasonable semantics. 
`compensated` was written so i could summarize over a few billion numbers without destroying the precision in my significand. Basically `Compensated Double` doubles the available significant bits by a fact of 2 to ~100, though it doesn't increase the exponent range. `Compensated (Compensated Double)` gives you ~200 bits of significand.
That was how I understood the situation. I recommended that the exception-throwing code be lifted into an IO context and the exception be dealt with there, or if there was certainty of how the 'pure' code would be used, then unsafePerformIO could be used with caution. I'm beginning to think I may have misunderstood the question though. Maybe they were asking about writing a pure function that could throw an exception on certain inputs rather than an existing function that could not be wrapped purely (such as head). Thanks for the feedback.
Yes. When I do my version of Runge-Kutta method, I find the precision does not meet my demand. I spend some time to figure out whether there is some mistake with my code. So I suspect that it has something to do with the precision problem. So that is what I am asking.
If you are talking about numerical methods then I think the answer is yes. I am not sure what you mean by high accuracy. Most languages, Haskell included, allow you work with Doubles, IIRC the level of accuracy is about 10^-16 e.g. if you optimise your code you may see changes of this order in the answers. I use repa (http://hackage.haskell.org/package/repa) and yarr (http://hackage.haskell.org/package/yarr). There may be others. I believe I am getting results comparable in performance to C but hesitate to make this claim until I have run more experiments. You can see some of my attempts here: http://idontgetoutmuch.wordpress.com/2013/08/06/planetary-simulation-with-excursions-in-symplectic-manifolds-6/ http://idontgetoutmuch.wordpress.com/2013/02/10/parallelising-path-dependent-options-in-haskell-2/ http://idontgetoutmuch.wordpress.com/2013/01/05/option-pricing-using-haskell-parallel-arrays/ http://idontgetoutmuch.wordpress.com/2012/04/22/the-implicit-euler-method/ http://idontgetoutmuch.wordpress.com/2012/04/01/solving-a-partial-differential-equation-comonadically/ I should also mention the fact that you can do Automated Differentiation in Haskell without having to change your code too much. Here's an example: http://idontgetoutmuch.wordpress.com/2013/05/31/neural-networks-and-automated-differentiation-3/ I'd be very interested in seeing the examples http://www.reddit.com/user/kidnapster talks about in Haskell :-) 
Sounds like you needed some mentoring/advice on how to design a concurrent build system. Did you try shake?
&gt; For every complaint the author has about "in Haskell you can't-" to which you want to say "but you can...", consider this: But it is worth pointing out that if X is possible, allowing the statement _X is not possible_ to go unchecked gives others the wrong impression. God knows there are enough strange ideas about what Haskell can(not) do and how it works &amp;mdash; we hardly need more.
Hmm.. Yeah, I guess it'll work for Metropolis-Hastings as well... I'll read more about it. Thanks!
Incidentally, I had the problem of summing over many small numbers and took a different (not terribly haskell-y) approach: put all the numbers to be summed into a priority queue. At each step, pull the two lowest numbers from the queue and push their sum back into the queue.
I feel like it would help if you gave some example project ideas. The idea of having a competition where you're just allowed to submit "anything" is a little weird to me.
Great! I've got a hopefully suitable project I've been working on that I might enter. edit: the list of things they are looking for is sadly devoid of games :(
You have learned about Pain In The Arse? Pretty sure that's what kidnapster means by PITA in "MPI is a huge PITA".
That's a fair point. My intent was mostly to discourage people from saying OP is retarded, which is divisive and a poor way to take criticism.
 "I want to be able to build a list of objects of varying types that implement the same interface without having to re-invent virtual tables every time I do it (type classes don’t quite solve the problem)." Maybe extension [7.4.5. Existentially quantified data constructors] (http://www.haskell.org/ghc/docs/latest/html/users_guide/data-type-extensions.html) is enough for solving this problem. More detail description is [wiki] (http://www.haskell.org/haskellwiki/Existential_type). 
I don't see how C++ is a good choice when you need to call into it from other languages. Calling into C++ is a major pain, half the language features used in APIs are impossible to use from other languages,...
&gt; and the relative immaturity of distributed Haskell computing There is the HdpH DSL which is suitable for irregular symbolic computing. It is described in [Reliable Scalable Symbolic Computation: The Design of SymGridPar2](http://www.macs.hw.ac.uk/~pm175/papers/Maier_Stewart_Trinder_SAC2013.pdf). It is effectively a distributed-memory load balancing scheduler that supports a small set of DSL primitives for creating irregular tasks. The DSL will be familiar to anyone who has used the monad-par API. It is 100% Haskell, and uses the network transport layer re-factored away from CloudHaskell. That paper describes a binding to the computer algebra language GAP. These bindings are not yet open source, although your symbolic applications could equally be written in pure Haskell. HdpH hackage documentation: http://hackage.haskell.org/packages/archive/hdph/0.0.1/doc/html/Control-Parallel-HdpH.html
"Learning about PITA" == "School of Hard Knocks"
I second that. Sometimes Haskell is perfect for prototyping/specifying the algorithms, and when settled you can port it to some other language that is better in line with other project constraints. I'd bet he'll come back to Haskell next time he has a cool project idea, since prototyping in C++ is hell.
The precision problem is something you generally need to solve if you're inventing a new algorithm. Picking a different language won't help much except for giving you numbers with more precision, which is not what most people mean when they talk about making their algorithms more numerically pleasant.
He has used about shake and decided to write a competitor to it. The concurrency bits in Shake were far from trivial (and got way harder once I started thinking about arbitrary user exceptions, which I suspect he hadnt got to). I think they would have been immeasurably harder in c++ though as I am always using first class IO () values. 
Is there no decent deprecation mechanism in Haskell which could facilitate this?
Why would anyone ask other Haskellers for help/ideas before ditching the language? /s
and preferably one that works without fuss on Windows
Maybe what I mean by PITA is not the same as what kidnapster said. Thanks for pointing it out.
Yeah, I mean PITA by (P)(ITA). It is my mistake.
Ok, I am not so confident of my code. I will reconsider it someday.
I know symplectic method. I learnt symplectic from his papers http://math.nju.edu.cn/~XYWUMATHNJU/ . Your blog has given me a good guide. I do PDE, but I have no knowledge of neural networks. Thanks very much!
This is a bad example. In Haskell, due to laziness all those first sets will be ignored. ... I'm trying to think of a good example. I'm sure there is one, but I haven't thought of it yet.
SDL_main is gone, and there is no fun left in writing bindings for SDL any more.
Ok, here are some sample ideas -- still quite broad because we don't want to limit people. Data analysis, business-logic related apps such as pricing, trading, rating engine, database manipulation, hardware/software design and simulation, high-performance computing on CPU or GPU, web application or mashup, web services providing a machine-consumable API, web front-end to a pre-existing application, transforming text documents into structured data or structured data into another format, processing JSON or XML Data, searching and filtering unstructured text documents, implementing security using cryptography, interfacing existing GUI frameworks to Haskell applications. Etc. Here are a few random pretend examples. An e-commerce website that runs a store and takes orders. A stock trading app that analyzes data streams and updates the target prices of different stocks, making buy/sell decisions. A back-end for a mobile app that accepts JSON transactions, handles business (or game or technical) logic, and records results. A workforce or logistics optimization app that accepts large numbers of constraints on various discrete and continuous variables, then assigns optimal tasks to each worker, vehicle, etc. A high-reliability programmable tool for a common category of sysadmin tasks. A parallel, highly correct, high-performance simulation of gene splicing. An intranet website that exposes a workflow tool formerly only accessible through a desktop app. By contrast we're not looking for general or academic research, mathematical theory, abstract computer science, Haskell syntax lessons, or general concepts like monads.
Care to go into more detail with your comment?
Yeah this is a fairly common technique so long as you can afford to do something `n log n` in the size of your numbers. When `n` rises beyond fitting in memory that ceases to be a viable technique. =) Another is to batch things up by size and maintain 2-3 windows worth of mantissas as sums. The benefit of the `compensated` approach was that it it just some extra operations to shuffle around all the 'error' into a second Double and then work them as pairs. Knuth gave us an error free transformation of `a + b` into `x + y` such that `x = fl (a + b)` is the closest floating point approximation to `a + b` and `y` is the error, because `a + b = x + y` with real honest to goodness equality and no loss of information. By using that, and carefully doing compensated multiplication we can just work in enough precision that we can cover pretty much any problem that would, say, arise in finance, because the errors just can't cascade far enough to drown out your significant digits. More easily, Kahan has a compensated summation technique that lets you add a stream of numbers using a second Double to accumulate error. This has the benefit of reduced overhead relative to the `compensated` approach, because it is unidirectional and the only extra state is an extra `Double`. `compensated` is what you get if you upgrade Kahan's technique to try to make it more free form and nearer to associative, so that you can work over parts of your data in parallel. 
SDL used to hijack main() via a macro to call SDL_main, which led to a lot of fun when dealing with other languages/libraries that also wanted to interfere with main. I can't remember the exact details of why it was such a pain, but I do remember reading some very... interesting .h files when I was writing bindings for libtcod, which uses SDL as a backend.
Oh... so you're saying it's no longer fun because they've removed the challenge. Is that it?
and on mac
Do you have the `libtcod` haskell bindings up somewhere?
https://github.com/bluepeppers/haskell-libtcod They are bindings for the core and half the base modules of tcod, but there's no documentation/tests/non-trivial examples. I haven't had time/inclination to touch them of late (Finally mustered up the courage to learn OpenGL properly), so while they'll probably compile, I would be unwilling to bet on them linking/running successfully. I should probably add a README with a warning.
Well writing bindings is pretty boring, especially if you're not changing the library's API in the binding (and I try not to). I think the most fun I had last time I did some bindings was setting up an emacs keyboard macro to deal with the constants.
This post series was what introduced me to the notion of a right seminnearing. The fact that we can view one of sets of Alternative laws as the same as right seminearring means we can view parsing through this same lens. That has stuck with me ever since. Bernardy and Claessen have a paper for ICFP this year that continues to make use of this fact. http://www.cse.chalmers.se/~bernardy/PP.pdf There they work with a non-associative right seminearring, and collecting the results of all associations of it for the terminals in your document to makes it associative and is the act of parsing a context free grammar!
Thanks! A coworker of mine, Elliott Stern, had just started a rogue-like in Haskell for fun. I jokingly called it "Rogue-Sham-Bo" as his combat resolution technique is to have everything throw rock/paper/scissors. He was just about to write some kind of `ncurses` front end the other day, but `libtcod` would be much prettier.
Depends how large the library is. Once you have a bit of experience, it becomes very methodical to produce the bindings for functions (structs can be simple, but often have small complications that you have to deal with). More difficult/time consuming is writing tests (you can write incorrect bindings with Haskell and have it compile) and documentation. And there's always questions of multithreading, garbage collection and the like, which many C libraries simple ignore. These are more interesting, but usually only have to be worked out once per structure. I would say that Haskell is fairly decent to write bindings for (compared to some other languages) as things like `alloca` and `peek`/`poke` are very well designed, though the fact that there seem to be about 3 different tools to do the same thing (hsc2hs, c2hs and another that I think I'm forgetting) is a tad confusing when starting out.
and how did sodium performed compared to the rest?
As libSDL.org make a dll available, SDL has always been one of the easiest bindings to get working on Windows. Things only start to get difficult if you want extension like font support (sdl-ttf) or whatever.
Yeah, Kahan summation would have been easier and faster; I didn't know about it at the time. It's really cool that you've generalized it.
&gt; Switch Text.height to use px instead of em ಠ_ಠ 
That looks pretty interesting, especially the load balancing bit. I haven't done anything really fancy like adaptive finite elements or multigrid on a cluster, but load balancing would be a huge boon in that setting. The only thing I've seen like that is [charm++](http://en.wikipedia.org/wiki/Charm%2B%2B). Unfortunately, network-transport-tcp is just way too slow on a supercomputer (MPI takes advantage of special Infiniband features and such). I've thought about writing a [GASNet](http://gasnet.cs.berkeley.edu/) network transport layer for Cloud Haskell, but never found the time or motivation to work with the FFI.
They'll have a cost still. Not a huge one, but nonetheless. The binds at least should cost something..
The answer for most people will be productivity. Sure someone learning the language will be able to build their app, but how much longer will it take them if they are repeatedly getting stuck. Haskell is not a language to learn while doing a project that you want to ship quickly since you will be learning how to program again.
There are many bindings for those libraries. E.g. hmatrix package can use ATLAS and MKL
Yes, haskell is a fantastic substrate for numerical computation of all sorts. In addition to all the great work and libs people have already mentioned, I should mention that later this month I'll be releasing a take on what think is a REALLY NICE numerical computation substrate that I hope to convince the general haskell community to adopt. Its the open source element to what i've spent the past year developing at my wee company, Wellposed. 
Please see [the discussion](https://groups.google.com/forum/?fromgroups#!searchin/elm-discuss/specifying$20size$20of$20text/elm-discuss/3Iz-HpV1QRg/oHPoqWDgrmEJ) of why we did this, and perhaps give some commentary on why you find those arguments unsatisfying. Different design choices are needed for CSS and Elm. When you are working with a language that allows you to *create* abstractions, it is nice to have access to low-level tools (as mentioned in the discussion thread).
I really like the concept of FRP and how Elm does things. But I strongly feel that what is holding it back is the requirement that EVERYTHING must be done in Elm. We really need to find a way to use ordinary CSS files (as a bare minimum) as well as HTML.
I think the point here is that it's only dynamic languages that really need to call the compiler dynamically, and Python at least has a well developed FFI to C++. If it's already there, the difficulties of calling C++ is far less relevant.
I think this is getting closer. [This proposal about markdown interpolation](https://groups.google.com/forum/?fromgroups#!searchin/elm-discuss/interpolation/elm-discuss/6QIaKL4tbFY/xUIB0El6YX4J) has has made it much clearer to me how Elm and everything else can live happily together. Here are some brainstorms in that direction (no parser support yet though!): * [Wiring up a field](http://www.share-elm.com/sprout/520a797fe4b098db14c5460e) * [Rewriting](http://www.share-elm.com/sprout/520a797fe4b098db14c5460f) the [physics example](http://elm-lang.org/edit/examples/Intermediate/Physics.elm) I'll be exploring this more in the coming weeks :)
&gt; Sounds like you needed some mentoring/advice on how to design a concurrent build system. Yes, I did go through several iterations of the design. But in any other environment (C#, C++, Java, JavaScript) the computer would have been able to help me find the faults in my design. GHC gave me little help other than "a deadlock occurred". Similarly, I would have been able to debug the infinite loop issue in another environment, but not GHC. &gt; Did you try shake? Yes. I am building something similar to Shake (as [ndmitchell mentioned](http://www.reddit.com/r/haskell/comments/1k8x5h/capn_proto_capn_proto_v02_compiler_rewritten/cbmshcz)). I even used Shake's concurrency design to get a few ideas. 
Haskell has debugging facilities. They're not as good as other languages, but you do have to debug much less, so it more than evens out. I use [TraceUtils](http://hackage.haskell.org/packages/archive/TraceUtils/0.1.0.2/doc/html/Debug-TraceUtils.html).
Have you seen the `vector` package?
With only em, it's impossible to position or size things properly relative to pixel sized elements. With only px, it's impossible to position or size things properly relative to text elements. It is not something you can abstract away no matter how powerful your programming language is, and it's hardly a kludge. With the decision to use px for text elements and not going with a TextHeight datatype, it seems like Elm is moving away from being a tool to implement dynamic websites, and focuses on being a system to implement games and animations. I see no reason it couldn't be both, so I'm a bit puzzled why this decision was made when there was a perfectly good suggestion in that discussion thread (the TextHeight datatype).
My feeling was that they are equivalent, but the TextHeight data type introduced a more complex API. At worst, this is a misunderstanding, not a secret philosophical plan. Can you explain your case in more detail? Is the idea that the size of `em` depends on the particular typeface you use? My understanding was that browsers default to (1em = 16px) and that zooming in and out will scale the font no matter what units it is defined in. If there is always a correspondence between em and px, why are both needed? Can you be specific about where the problems will arise, perhaps providing links and examples? edit: if it is true that (1em = 16px), I'd expect people to write code along the lines of this: em = 16 -- you can use this throughout the whole program title = Text.height (3*em) (toText "Hello") pic = image 200 (6*em) "yogi.jpg"
Dude, Edward, take a day off or something. No one can be this smart *all* the time...
I hope people now appreciate the [HaPy](http://www.reddit.com/r/haskell/comments/1jnsfb/hapy_python_to_haskell_foreign_function_interface/) project even more.
&gt; My understanding was that browsers default to (1em = 16px) No, it is 16 *points* wide.
Can you find relevant links? Or explain this in more detail? Is it relevant to the idea that: there is no reliable mapping between em and px? Maybe the [CSS3 spec for lengths](http://www.w3.org/TR/2013/CR-css3-values-20130730/#lengths) is a useful place to go?
I'm on a phone right now but check font-size on MDN. Mozilla says an em is *probably* 16px wide but you need pixels to have certainties. I don't think any spec enforces em-pixel ratios.
No, I had not. I'm looking into it, and it seems related to what I was describing. Thank you.
Your `unique` already exists in Data.List as `nub`, and it has only an `Eq` constraint. `capitalize` would be better written as `map toUpper`. Also, the problem you are tackling is what `lens` already does: it also can derive lenses automatically, which is what you want.
Learning to use GHCi is not going to hinder your ability to compile and run executables. Don't worry about it. 
Note that `map toUpper` would "uppercase" every character, but the author's `capitalize` function only uppercases the first character.
Oh, sorry, didn't notice it wasn't recursive.
Yeah, the compiler is *leaps and bounds* ahead of where it used to be.
Currently, Elm supports embedding itself in a traditional page. This sorta encourages a pattern of creating Elm "widgets"
&gt; if it is true that (1em = 16px) `em` (and `ex`) depends on the text size of the given element. [Example](http://chrisdone.com/em.html). On my Firefox, the red is wider than the blue by 3px~.
Thanks for the feedback! I've been working on understanding the lens library. I'm still not ready to use it in any of my projects. Isn't nub O( n^2 )? unique should be asymptotically faster, I think.
You appear to have missed the point, in that non-dynamic languages don't need to link to the compiler. Whether other languages have a C++ binding is irrelevant (though my understanding is that calling C++ from lua isn't too difficult either.) And since the original author is taking this on, it would be easier for him to write a C wrapper to the C++ code if/when it comes to that.
Anything interesting about the example graph?
Edward Kmett's productivity drops off exponentially once he runs out of single letter variable names.
You're welcome!
I've been thinking about this idea since forever. It's good to hear that it's not a completely unsensible one. The other flavor of it I was thinking about is that packages could have so-called weak dependencies. A weak dependency wouldn't have to be present to build the package, and the parts of the package which require it would be built iff it were present (at a declaration rather than module granularity). This wouldn't *directly* obviate orphan instances, as in the third party case a given instance would still have to be provided by one of the upstreams, but most of their disincentive to doing so would be removed. Some benefits over mixin modules would be avoiding the combinatorial explosion of them, and no need for magical bringing instances into scope. (In the mixin modules scenario: How stable would an arrangement where the maintainer of parsec-parsers is neither the maintainer of parsec nor parsers be? Could there be multiple distinct mixin packages for parsec and parsers? How would the system decide between them?) A thorny question is: how would "parts of the package which require the weak dependency" be determined? Would it be programmer annotated at the appropriate points in each source file? I think Backpack would be an excellent fit here: if all of the relevant entities from the weak dep are explicitly specified (a "hole"), then this could be determined automatically. (There are also things like: * presumably a package with a weak dep would have to be rebuilt if the weak dep were installed afterwards * would things-that-require-the-weak-dep only include things which mention entities from it in their types, or also just in terms? * presumably things which are not built would not be silently dropped, but poisoned, to avoid accidental name clashes and so on...) What do you think about this?
This is kind of why i switched to the 'glb of two packages may not be the same as the product of both' approach. It gets messy.
To be fair it'd been done long before me. =) The main contribution I have is that my construction appears to be able to be iterated, unlike previous designs. There is `qd` in the c/c++ ecosystem that provides double-double and quad-double arithmetic using a similar construction, though unlike `compensated` they can't just keep going. ;)
It sounds great! I hope to learn about it!
Ken Shan is not only a computer science, he's a linguist! His dissertation was on continuations in natural language semantics. :) &lt;/linguist&gt;
Seminnearing is the task of giving seminars.
printf debugging didn't help with my concurrency issues, either. It did help me realize a problem in one of my random experiments, but besides that I could not isolate the deadlock or any useful information.
I have a quite large 3D game I'd like to enter that kinda does some of these things; e.g. parsing .obj models, reading config files, relying on high performance matrix multiplies, and opengl performance. Is it suitable? I'm kind of surprised games weren't explicitly mentioned because it's an area Haskell sees virtually no commercial use in, but is a pretty huge industry.
Not that I know of, I went on graphviz.org and looked for the first medium-sized graph I could find. Finding a graph with more well-defined clusters will probably be more interesting.
Actually a game back-end is among the examples I gave. A game would be a reasonable thing to submit.
The scoring criteria lean toward apps that solve a problem or meet an applied need, but a game is a valid entry as long as it complies with the contest rules on the site. Even better if it contains significant components or techniques that can also be used for non-game things.
&gt; single letter *type* variable names.
At a [nearring conference](http://www.algebra.uni-linz.ac.at/Nearrings/conferences.html)?
Why not? Did it cease to reproduce? What about ThreadScope?
Presumably your title should be "FRP is not real time ready yet"? I suspect the author is hinting at the combination of FRP + laziness.
There is "hard" real time and "soft" real time. What you mean by saying that a very good garbage collector should be efficient and good enough for gaming purposes is perfectly possible, but reflects "soft" real-time requirements. Haskell and most other languages are not "hard" real-time -- they cannot guarantee with 100% certainty that all operations will take a predictable amount of time. That doesn't mean they are unsuitable for gaming, just that you might hit some situations where it's impossible to be absolutely sure the user won't experience some sudden latency. This is often due to the garbage collector running at unexpected times and "stopping the world," but as you point out, not all garbage collectors work this way. Additionally, non-GC languages can be just as indeterministic, e.g. using `malloc()` and `free()` in C generally lead to unpredictable pauses. I don't know enough about the GHC garbage collector to comment on its suitability for gaming. In general "soft" real-time can satisfy probability requirements.. e.g., if the GC is manually triggered often enough during game pauses one could state that there is some probability of an in-game pause which might be low enough to satisfy requirements. In other words, a pause could happen, but it won't ruin the gaming experience though some users may notice it. This is different from e.g. a motor controller where a pause could cause a physical machine to become unstable, or an audio program where a pause could lead to audio glitches. Either scenario is considered unacceptable (in a professional setting) and therefore these applications have "hard" real-time constraints. By the way there is a Haskell library for hard real-time programming called [Atom](http://hackage.haskell.org/package/atom). However, it accomplishes this by not actually using the Haskell run-time, but by being a DSL that generates a C program intended for installing on a microcontroller. As a side-note, a cool hard real-time language worth checking out is [FAUST](http://faust.grame.fr/). It is not a general-purpose language like Haskell, but it is a functional DSP graph definition language. That is, you have functional variables that you use to express a DSP block diagram, and this is used to general real-time C code. (It is targeted to audio applications but is actually fine for any 1D DSP application.) It can also generate for a host of other targets, VSTs, PureData objects, JavaScript, Java, LLVM... It can even generate a latex document describing your code in mathematical syntax.
But how does the small cost compare to taking a free monad and filtering out all the useless `set` nodes?
Well you do the latter once, and the former each time you run it. So here you'll have to run it a zillion times and have a zillion sets for the numbers to work out better with the algebraic encoding, but the point isn't the cost in this exact case -- rather its just to establish the general principle.
Good answer, but its worth adding that in hard real-time settings, devs are often willing to trade a fair bit of performance for hard constraints -- speed takes a backseat to predictability. This follows from what you've described, but I've found that explaining this helps people to conceptualize why the problem domain is different and not just "more of the same, better".
The guy with the evil eyes in the bottom-right corner scares me. :O
Thanks for the answer! I read that the GC usually pauses between 1ms-5ms. I know that it is different for every program, but still I need some numbers to make some calculations. Let's say my game runs and 60fps that means every frame needs 0.016666667s of calculation right? A gc pause would increase this to ~0.017666667 which results in 56fps. So it s a performance loss of ~4fps or 7%. For me that is a pretty big hit. But thinking out loud, would be it possible to write my main game loop in atom and the rest with normal GHC Haskell? Then I wouldn't get any performance hit in my game. Let's also say I have 10 monsters running around in my game. Every monster runs on his own thread. I know that the GHC is concurrent and thread local, so every monster gets garbage collected on it's own right? By writing my main loop with atom and using threads for everything else gc pauses shouldn't really be noticeable. (in theory) 
I think you might be conflating two different definitions of the term "real-time". I assume that since you're talking about making a game engine, you're talking about "real-time" in the sense that you want a game engine that runs fast enough to be interactive, i.e. be able to react "in real time" to user inputs. There's no reason why Haskell can't be used for this. This other thing with "real-time predictability", however, comes from a much older, more strict definition of real-time [(wiki)](http://en.wikipedia.org/wiki/Real-time_computing). This definition of real-time doesn't necessarily have anything to do with interactivity; it has more to do with what kind of guarantees we can make about a system in terms of meeting its deadlines. Of course, if your deadlines are defined appropriately, a real-time system in this context could also be interactive in the way you're talking about. That we've adopted the term "real-time" for both of these uses is unfortunate, because it can cause exactly this kind of confusion. What you've read is actually saying that FRP doesn't make any of the strict guarantees required by this second, more technical definition of "real-time". However, if all you're concerned with is interactivity, then FRP is just fine for what you're doing.
Right, and there are considerations other than speed -- communication between real-time and non-real-time portions of a program must be handled with great care. In general it's hard to guarantee hard real-timeness statically so mistakes can be introduced by bad IO management among other things.
You should hopefully not have a GC pause every frame. I have no figures on this at all, but I would assume something like once every ten seconds is more reasonable. But if you want me to be completely frank, it sounds like you should have a lot bigger problems than exactly consistent framerates. How computationally intense is your game going to be, that makes you think you can only have 60 frames per second on a normal computer? Are there any other optimisations than "Using a language with manual memory management" that you have considered? 10 monsters is *nothing* unless you have some real heavy AI/pathfinding to them.
Oh right I thought it would run every frame. I just made an example to calculate how big the impact could be. If I have 1ms pause every 10 sec, then I could have 0.1ms pause every 1sec. (Assuming that it would be linear) or I could run it every frame and then the performance impact would be very marginal. (no random fps drops) But it's hard to plan an engine if I don't know anything about the pause times yet :(
At 60 FPS, your users will *not* notice an 1 ms pause every 10 seconds. I play Counter-Strike, which is a fairly competitive PC shooter, and my framerate regularly swings between 45 and 60 frames per second and I wouldn't know unless I looked intently at the framerate counter. Hell, my framerate even depends on which direction I look in the game world! Computer games, even competitive ones, have nowhere near real-time requirements. I don't think yours does either. Yes, planning an engine is really hard. I mean it. Very, very hard. It doesn't get easier if you let trivialities trip you up before you've gotten to the real meat. Language choice is one of your least concerns unless you're already done with much of the other stuff. If you against all odds would notice problems with Haskell when you are done planning your engine and have started implementing it, *then* you might want to consider the Haskell version a prototype and do the real thing in C++.
For soft-realtime, GHC is more-or-less ok. Also note that you can manually trigger the collector using `System.Mem.performGC`, possibly resulting in more predictable time/space behaviour.
Your calculations seem okay, but unfortunately I can't answer about the GHC run-time specifics since I just don't know. However, your approach is along the right lines for one way to think about a hard real-time-compatible GC. If the GC pause can be guaranteed not to take more than _n_ milliseconds, then you could conceivably "schedule" it into the per-frame update. Time-constrained GCs are not that common however, and then you have to think about guarantees that it will successfully actually collect garbage faster than it is generated. I think there is one available for Java as a commercial product but can't remember its name. Another approach is a constant-time GC, however this is certainly not available in GHC or most other languages. (GC operations must be guaranteed to run in O(1), i.e. independent of the amount of memory being collected) One open-source O(1) GC was developed for [audio applications here](http://users.notam02.no/~kjetism/rollendurchmesserzeitsammler/), and imho gets far less credit than it deserves. Note however that one very important constraint for it to work is that the maximum memory used by the program at run-time should be specified up-front. Otherwise a system call would need to be made to enlarge the virtual memory of the process. (I'll note that when talking about hard real-time, it's impossible to actually dissociate the language run-time and the operating system. Even a well-engineered real-time GC won't be hard real-time when running under a non-real-time process scheduler.) &gt; But thinking out loud, would be it possible to write my main game loop in atom and the rest with normal GHC Haskell? Possibly but I believe there are no mechanisms to handle communication between an Atom program and a Haskell program. It is generally intended to generate code that will actually run on a microcontroller, so it's really best to think of it as a separate program. In this regard, it's no different than just writing your real-time portion in C and communicating with Haskell over a socket. However, you'd probably end up writing the majority of your game in C at that point. Keep in mind also that the real-time guarantees of Atom and similar languages are actually constraints on things the language can do, so it might not be the most expressive language for writing game code. Unfortunately there's just no silver bullet. :) Likely a better approach is just to pre-allocate things as much as possible and avoid hitting the GC at all, though this may be difficult in Haskell. Slab or block allocators can be useful and are often used in gaming. (Usually in C++) 
Probably he means FRP doesn't work well with laziness. The combination can cause a "time leak": http://blog.edwardamsden.com/2011/03/demonstrating-time-leak-in-arrowized.html Basically you can make values which depend on your event history. Since Haskell is lazy, it won't compute the value until you use it, at which point it will go through the whole relevant history to calculate your value. Not only does holding the event history waste memory, but you get a "time leak" where it takes much longer to compute this value. Computing it incrementally would take the same time, but would be split across hundreds of frames. Evan explains why Elm isn't Lazy: http://www.testblogpleaseignore.com/2012/06/22/the-trouble-with-frp-and-laziness/
Factor your expected garbage collection times into your budget by amortizing it into the frame cost rather than as an extra concern. Think of it as just something else that'll happen like physics, rendering to the screen, setting up sound, etc. It takes from your time budget like everything else. In practice, when you start developent it won't be an issue and by the time you are tuning to hit frame-rate guarantees the particulars of your engine will outweigh any general purpose worries about garbage collection. Keep in mind .NET code runs the vast majority of the games in XBox Live Arcade, and that runs in a garbage collected setting. (In fact, the version on the console runs in a particularly lobotomized garbage collector.) In any event Haskell is nowhere near unique in this regard. ;) [Edit: I should have said Xbox Live Indie Games apparently]
That's me, and those are upside down lambdas!
1. I don't believe you can make a funD with a name that is interpolated in from outside. This is probably the most you can use the built-in syntax. import Data.Generics makeChanger2 :: Name -&gt; Q [Dec] makeChanger2 field = do let fixName n | nameBase n == "change" = mkName $ "change" ++ capitalize (nameBase field) fixName n = n upd = [| \x value -&gt; $(recUpdE [| x |] [ fmap (field,) [| value |] ] ) |] r &lt;- [d| change x = $upd x |] return (everywhere (mkT fixName) r) 2. Something like the following doesn't do any error messages, but it is shorter: makeChangers :: Name -&gt; Q [Dec] makeChangers dataName = do seqConcat . map makeChanger . unique . getConNames =&lt;&lt; reify dataName where getConNames = everything (++) (mkQ [] $ \x -&gt; case x :: VarStrictType of (n,_,_) -&gt; [n]) seqConcat :: Monad m =&gt; [m [a]] -&gt; m [a] seqConcat m = liftM concat (sequence m) The extension -XPatternGuards can sometimes help with flattening out case expressions. Here it only helps helps a bit because now you only have to give one fallback case: case info of TyConI dec | DataD _ _ _ constructors _ &lt;- dec -&gt; result constructors | NewtypeD _ _ _ con _ &lt;- dec -&gt; result [con] _ -&gt; fail "Not a data name." 
If you make any OS calls (which you will) then you cannot be hard real-time unless you are using a hard real time OS (you aren't, unless you are using an embedded processor). Even with flat out C with everything pre-allocated you can wind up spending milliseconds in a driver, file operation, network, swapping, whatever. In any event no game today has hard real time requirements. Given general benchmarks I've seen optimized Haskell would generally be after C, and C++. This might not be true for games, but I would guess that Haskell would be no worse than most other languages in any case if laziness is avoided. 
See [here](https://docs.google.com/forms/d/1FzWbzGm6odYWxJZcU3GFHlS3lVFTBOI1-M1c87CjOFg/viewanalytics); basically `haskell-indentation` mode is the most popular; but the other two are used widely as well. However, it's dangerous to force such a change upon users; I'd rather add it as a fourth indentation mode, and let users decide which indentation scheme they'd want to use.
I was there for this talk. Feel free to ask questions and I’ll recall what I can.
My asteroids clone is clocking up at a whopping 4% GC over 1000 seconds of game play. Certainly nothing to worry about!
BTW, this is another wonderful application of automatic differentiation that I hadn't thought about before: differentiating formal power series/generating functions for combinatorial problems.
Would you put a price/number of new paying users on this task?
Are there papers available about this? I'd particularly like anything about compiling CUDA dynamically (possibly JIT) and running it as you go.
Many of the more popular Haskell FRP frameworks like Netwire don't cause time leaks.
Here's a Haskell implementation of a PDE solver for path dependent options (an Asian call but it would be easy to make the payoff generic): http://idontgetoutmuch.wordpress.com/2013/02/10/parallelising-path-dependent-options-in-haskell-2/. When I have time I was hoping to implement an Ising model but if you have any interesting suggestions, I'd be happy to consider them.
Here's an example of doing regression using AD in Haskell: http://idontgetoutmuch.wordpress.com/2013/04/26/regression-and-automated-differentiation-4/. Warning: I haven't benchmarked it.
I have asked several ML people why they don't use AD but never got a satisfactory reply. I think one of them muttered performance but wasn't able to substantiate this. Here's an NN implementation using AD in Haskell: http://idontgetoutmuch.wordpress.com/2013/05/31/neural-networks-and-automated-differentiation-3/.
Try: http://www.cse.unsw.edu.au/~chak/papers/CKLM+11.html http://www.cse.unsw.edu.au/~chak/papers/LCGK09.html http://www.cse.unsw.edu.au/~chak/papers/MCKL13.html
&gt; Keep in mind .NET code runs the vast majority of the games in Xbox Live Arcade That's very much untrue. You're talking about Xbox Live Indie Games, which is not the same as XBLA.
For comparison with SBCL have a look at Paul Khuong's [paste](http://paste.lisp.org/+2YUM).
Fair enough. It was a statement based on half a memory about XNA and XBLA deployment and my Xbox has been red ringed of deathed for enough years that the distinction has long been lost on me. ;)
Are you sure? Someone here said that you can trigger the GC manually with "System.Mem.performGC", so you can have a GC every frame. I also watched John Carmack's talk Part 4 where he said that it is perfectly viable to have the GC run every frame. But I think he was talking about GC's in general. Edit: Ah okay I think you mean that even if we can trigger the gc every frame we don't know how long the GC will pause right? Also I thought that GHC is concurrent and thread local. So only things that you allocate in your game loop will get collected. 
The GC isn't incremental, so it'll do a full GC each frame if you call that, which is very expensive (it involves traversing the whole heap). You need an incremental GC where you do just a small number of steps per frame so you can control costs (e.g. perhaps you only get through the full heap once every 100 frames or something).
I would suggest going on with RWH, which you don't have to read in order necessarily once you've understood LYAH. Monads aren't something there's a real world application for that's purely centered around them - they just appear everywhere, and play a prominent role in current Haskell. In that sense, your question is similar to asking for projects "to understand loops" in imperative languages. What I did was play around with monads: write twenty-line programs using Maybe, List, Reader, Writer, State, IO and their APIs (`ask` for Reader, `tell` for Writer, ...). RWH goes through the process of building a non-monadic parser the way it makes sense, then uses it to parse a binary file, and a couple of chapters later remarks that secretly it *is* a monadic parser if you replace just a couple of the operators. The whole thing is introduced so that monads appear naturally, they aren't anything magical put in. 
Lua has an incremental GC (and LuaJIT has a better one on the way), but has kind of dreadful performance for other reasons (very dynamic). Azul has their C4 GC which is fully concurrent. That's kind of what you want. It requires a patch to the Linux kernel to cooperate, as I understand it. Honestly I'd settle for a non-compacting concurrent collector. For games, fragmentation isn't a huge deal (you tend to not have a lot of GC'd data anyway, tens of megs - most of the data are huge textures, sounds, meshes etc. which you would treat differently, and would be able to use virtual memory these days to avoid fragmentation for those). The holy grail is a system which requires no GC by default (using some kind of linear types for predictable and high performance deallocation), but still has one, and where each thread can have its own completely independent GC *and* a different GC algorithm. So if you have a "background" thread that doesn't care about pause times because it's not running on a fixed frame rate (e.g. most AI and script logic could run like this - even if most engines don't, yet), you could have a super efficient stop-the-world GC with no write barriers etc. For threads that need more predictable behavior you could choose between an incremental algorithm, or a concurrent one, and for high-performance threads where every microseconds counts (e.g. the rendering thread) you'd avoid GC (and indeed most allocations at all) altogether. Rust is a language that will at least hit some of these points. 
Do you use map on lists? fmap for functors is the same thing - in fact, [] is a functor. In lists, fmap (/map) "map"s a function over all the elements in the list. When you have a list of values and think, "golly, I want to apply a regular function to everything in this list", you're using functors. Maybe is the easiest functor I know of, and it's a lot like a list. It either has Nothing or Just something. So when you have a Maybe something value, but you want to apply a regular function to it, you do what you did with lists: you fmap the function over it. fmap is just a way to apply regular functions to values that have some extra context - for Maybe, they might have failed; for Either, they might have failed more descriptively; for [], there might be any number of values. That's an informal explanation, a zillion people here can give better ones than I can. Might cover applicatives / monads later, gotta go to work.
If lens seems to difficult to understand, you could try your hand at one of the other libraries that fix this "record problem"(Not sure what it's called in general, but this issue is a known one.) I myself tried fclabels and found it very easy to use. It, too, uses TH, and as you can see by the examples in the [documentation for fclabels](http://hackage.haskell.org/packages/archive/fclabels/1.1.7.1/doc/html/Data-Label.html) it is very easy to use. 
Very nice - a solver for "Untangle" from Simon Tatham's puzzle collection. Or is it just a nicer version of the puzzle itself?
Does this seem really odd to anyone? The implementation of `lookupDefault` basically assumes lookup will fail if it takes the hit of evaluating the default value all the time.
It's worse than odd. It's totally wrong. Of course it shouldn't evaluate the default argument, it's goes totally against the spirit of Haskell. 
&gt; Is it the author's fault that learning to do things in Haskell in the style he wants is difficult? It's not learning how to do it in Haskell that is hard. The hard part is *unlearning* your deeply set ways of thinking from another paradigm. This person does appear to have had some success in learning Haskell - yet still views the world through the narrow lens of OO-think.
Inconsistent at least h&gt; lookupDefault Nothing 1 $ fromList [(0,undefined)] Nothing h&gt; lookupDefault undefined 0 $ fromList [(0,Just ())] *** Exception: Prelude.undefined 
I think the fact that the first doesn't return undefined when using HashMap.Strict is a bug that has been already reported against unordered-containers. fromList puts values inside a HashMap, and it says they must be evaluated, but here they are't.
How about we get to peek in your code, to give you suggestions?
I only have my latest 'project' online so that will have to do: https://github.com/mobj11/Haskell-Card-500. The readme isn't the best, but what I'm trying to do is to implement a rommy-like card game. The rules are not the important thing here, more the experience gained from trying to structure the entire thing in some clever way (quite sure I have failed there though) and implement the function required for drawing, discarding and playing cards etc. It's far from finished so quite a few loose ends and perhaps not that well-commented either but feel free to comment on anything you find. EDIT: Thanks for the comments cdxr
Worse. Strict only on else branch
Or basic common sense, really. Even Scala explicitly adds by-name annotations to the default argument on the corresponding functions in its standard library. I'm disappointed that this behavior persisted even after several people (including me) pointed out how ridiculous it was when it first appeared. 
For the first No-Prize, if we define swap16 k = unsafeShiftL (k .&amp;. 0x00000000FFFF0000) 16 .|. unsafeShiftR k 16 .&amp;. 0x00000000FFFF0000 .|. k .&amp;. 0xFFFF00000000FFFF swap8 k = unsafeShiftL (k .&amp;. 0x0000FF000000FF00) 8 .|. unsafeShiftR k 8 .&amp;. 0x0000FF000000FF00 .|. k .&amp;. 0xFF0000FFFF0000FF swap4 k = unsafeShiftL (k .&amp;. 0x00F000F000F000F0) 4 .|. unsafeShiftR k 4 .&amp;. 0x00F000F000F000F0 .|. k .&amp;. 0xF00FF00FF00FF00F swap2 k = unsafeShiftL (k .&amp;. 0x0C0C0C0C0C0C0C0C) 2 .|. unsafeShiftR k 2 .&amp;. 0x0C0C0C0C0C0C0C0C .|. k .&amp;. 0xC3C3C3C3C3C3C3C3 swap1 k = unsafeShiftL (k .&amp;. 0x2222222222222222) 1 .|. unsafeShiftR k 1 .&amp;. 0x2222222222222222 .|. k .&amp;. 0x9999999999999999 shuffle = swap1 . swap2 . swap4 . swap8 . swap16 unshuffle = swap16 . swap8 . swap4 . swap2 . swap1 The `swapN` functions just exchange the locations of pairs of blocks of N bytes, so we can see that `swapN . swapN = id`. Proving `shuffle . unshuffle = id` and `unshuffle . shuffle = id` is simplicity then: shuffle . unshuffle = ( swap1 . swap2 . swap4 . swap8 . swap16 ) . ( swap16 . swap8 . swap4 . swap2 . swap1 ) = swap1 . swap2 . swap4 . swap8 . ( swap16 . swap16 ) . swap8 . swap4 . swap2 . swap1 = swap1 . swap2 . swap4 . ( swap8 . swap8 ) . swap4 . swap2 . swap1 = swap1 . swap2 . ( swap4 . swap4 ) . swap2 . swap1 = swap1 . ( swap2 . swap2 ) . swap1 = swap1 . swap1 = id unshuffle . shuffle = ( swap16 . swap8 . swap4 . swap2 . swap1 ) . ( swap1 . swap2 . swap4 . swap8 . swap16 ) = swap16 . swap8 . swap4 . swap2 . ( swap1 . swap1 ) . swap2 . swap4 . swap8 . swap16 = swap16 . swap8 . swap4 . ( swap2 . swap2 ) . swap4 . swap8 . swap16 = swap16 . swap8 . ( swap4 . swap4 ) . swap8 . swap16 = swap16 . ( swap8 . swap8 ) . swap16 = swap16 . swap16 = id 
There are functions in basic.hs where you could make use of the State monad. For instance, the function `drawCard` could be written with the signature: drawCard :: Int -&gt; State Game () The `()` means that the function does not return any additional information, it just changes the state of the implicit `Game` parameter. After some more refactoring, you might find a useful datatype to return instead of `()`. That is when the utility of the State monad will become truly apparent.
Even reading the linked mailing list thread I can't see the rationale for the change in `lookupDefault`. Can anyone explain?
I have another piece of advice for you. You define a few functions with the following structure: meldCompareLength :: Meld -&gt; Meld -&gt; Ordering meldCompareLength a b = if length a &gt; length b then GT else LT This pattern can be written as: import Data.Ord ( comparing ) meldCompareLength :: Meld -&gt; Meld -&gt; Ordering meldCompareLength = comparing length It is so short that it usually isn't defined as its own function. You can just use `comparing length` wherever you need it in your code, and an experienced Haskeller will understand it. Edit: I just realized that your function actually does the opposite, it returns LT when the length is greater. I don't recommend defining functions like this because it is the opposite of what most readers would expect.
Usually the problem with open source projects not working on Windows is that nobody who uses Windows wants to put the work into the project.
Technically I just gave away the first No-Prize a few hours ago to Sanjoy Das, but you are the first to provide a full proof of identity. I hadn't personally done it either. Nice! I'll mention it as an honorable mention and probably include the proof under a Spoiler tag in the next post. =) This is very much what I was looking for when I started this series.
Thanks for posting about this. I just realized the variant of Uniplate that is embedded in the lens package has the same issue. Argh!
"Consistency" "Everything else in there forces the arguments all the time, so so should `lookupDefault`."
I think that Probabilistic Programming monad is cool.
Thanks for the reply; it's information like that which really helps. Have had a hard time figuring out what it all could be used for, since I have no one to tutor/guide/help me :(
Whaaaa? Lazy function arguments is definitely what I *always* expect when coding in Haskell.
That definition just seems short-sighted: define the module as a value-strict Map and it becomes clear that this behavior falls outside of that mandate.
Actually I am programming in Rust since 3 months and it is a very promising language but I am not completely convinced. I started with Haskell two days ago and I really enjoyed writing code in it. I am also not completely convinced of having a JVM to run my games. I probably will write the core in Rust and then switch to Haskell for the logic.
The typeclassopedia will be probably helpful if you don't know about it yet: [http://www.haskell.org/haskellwiki/Typeclassopedia](http://www.haskell.org/haskellwiki/Typeclassopedia)
Uhh, that might come in handy in a month where I have forgotten all those definitions and laws. The exercises are probably a good thing to do as well. Thanks!
It depends on the sizes of your graphs and your RTS settings.
I personally agree. It almost cripples the module for me as the whole point I have for using it is that it is faster than `Data.Map`, but now I have extra boxes to fiddle with.
I'm not sure if it's a comment on how unfortunate it is to have strict default or (wrt to the time needed to pinpoint the issue) the sad state of the Haskell debugging ecosystem.
Came here to say this... even Scala, a strict language, gets this right!
With the first issue of the error, it took me about an hour to find, once I realised the relevant change was the unordered-containers version. Not great, better tools would have made it instant, but not fatal. With the second issue, it took me about a minute to find, once I knew there was a problem and once I realised the relevant change was the unordered-containers version, mostly because I had seen it before, but with a bit of profiling I suspect I could have found it in a few minutes. 3rd party libraries changing is never the first thing I think of, and it took a long time to realise there was a performance problem, since in general that is hard to test for. The debugging isn't great, but it wasn't the problem here.
And also Adam! And Andres, Bas, Neil and Gracjan. Pretty good lineup there!
Some background: even experienced Haskellers frequently ran into performance problems due to the lazy nature of the `Data.Map` API, most commonly by using one of the updating functions e.g. `insertWith` incorrectly, a strict API was born (`Data.Map.Strict`). Anecdotally it has solved many performance problems. The question now was: in what way should the API be strict? There are a couple of ways the `Data.Map` API can be strict: 1. Data type key-strict. 2. Data type key- and value-strict. 3. Functions key-strict (implies (1)). 4. Functions key-strict and data type value-strict. 5. Functions key- and value-strict (implies (2)). `Data.Map` is already (3), mostly for performance reasons (so the key can be kept unboxed in loops). The choice is between (4) and (5). I argued for (5) to avoid having the users have to try to think hard about which values will be evaluated when, something that people tend to get wrong, and to make it simpler to explain the strictness properties of the API (i.e. I don't want to have to have subtly different strictness properties on a per function-basis). I could consider making default values a special case, if people think it's a good idea. We should have a better reason that abusing the default value to get better error messages because we lack stack traces though.
Neil's own blog post had an example of why a stack trace was insufficient. And should people really have to go through Maybe to do nontrivial work in their defaults? In general, "harder to explain" has been used to justify all sorts of crap languages and APIs. Things with simple models are easy to explain, but please don't sacrifice actual good behavior for negligible gain in explainability. Would you define ifthenelse :: Boolean -&gt; a -&gt; a -&gt; a as strict in its arguments in a strict language with optional laziness? I really hope not... And this function is just a shortcut for a condition check like that.
From what I can tell, everyone thinks making default values lazy is a good idea - and not as a special hack. I assumed keys are always strict, and when using the strict module, values that go in the Map are strict - simple and predictable. I never guessed that anyone might even think of making all functions taking values strict in their value argument. The linked blog post gives two excellent practical examples, neither of which are because we lack stack traces. However, I don't think its a hack about stack traces etc. - it is purely about having the semantics that people expect.
Does Haskell make any useful guarantees for hard real-time systems?
Made a compiler in F# using the FParsec library last semester (CS student) and it was a really good experience even though I'm not a F# fanboy. It was the only way to get the rest of my group to say ok to some functional programming as they feel comfortable with the .NET framework. I have considered to reimplement some parts of it in Haskell (not going to reimplement half a years work made by five people on my own) focusing on monads, functors and such, but not sure how to do that the best way possible. Hopefully the links might help me there, so thanks! EDIT: F# is also a really (imo) ugly mix of a lot of paradigms, it's like working with a mutant language! Using a pure and elegant language as Haskell might be a completely new experience. Think I'm going to create a parser to see if I'm right.
Yea, I felt like cheating the entire semester while all the other groups were fighting with their classes, objects and what not. So glad I was able to work with the paradigm I feel most comfortable with, even though I'm fairly new to FP.
Noted. And yeah, dependencies changing behaviour underneath you can be nasty.
What about spine-strictness?
Performance issue, or correctness issue? And where is the variant of Uniplate inside Lens? I'd be curious to take a look - some of lens is certainly somewhat equivalent to Uniplate in some sense.
This is odd behavior. If the user of `lookupDefault` wanted the "default" to always be forced before performing the lookup, then the user can do this manually with `seq`. I understand that the point of `Data.HashMap.Strict` is to be able to promise certain big-oh behavior, but in this case, promising that the default will be evaluated is just pointless. If we are forcing the computation lookupDefault def k m then if the lookup is a "miss", the result should be `def` and we will start forcing `def` anyways, no need to guarantee it will be forced earlier.
&gt; Performance issue, or correctness issue? It was problem 2 from your post. The code `lookupDefault (hit ! x) x mp` occurred directly in the same place in the `lens` code. &gt; And where is the variant of Uniplate inside Lens? I'd be curious to take a look - some of lens is certainly somewhat equivalent to Uniplate in some sense. It used to be fully contained in its own module, but has since been spread around the package quite a bit as the variants on the uniplate combinators have become more ingrained in `lens` culture. e.g. `parts`/`partsOf` which takes a traversal and gives you a lens that views its targets as a list began life as a `uniplate` combinator. Similarly `holes`/`holesOf` also began life as a `uniplate` combinator, but the lens culture kept warping it. The tuple of value and replacement function have been replaced with a variant on the `Store` comonad, because it enables you do lean on the rest of the comonad-transformers tool chain. By the time it was generalized to support indexed traversals as well it had speciated into something admittedly barely unrecognizable. Back several releases ago when it was first introduced it was much more obviously the same idea as the `uniplate` version. ;) [`Control.Lens.Plated`](http://hackage.haskell.org/packages/archive/lens/3.9.0.2/doc/html/Control-Lens-Plated.html) exposes the moral equivalent of the `Uniplate` class as `Plated`. class Plated a where plate :: Traversal' a a This comes into scope when you import `Control.Lens`, so every lens user gets access to the vast majority of uniplate functionality out of the box. This necessitated a few name changes, but for the most part stuff is recognizable. On top of that, [`Data.Data.Lens`](http://hackage.haskell.org/packages/archive/lens/latest/doc/html/src/Data-Data-Lens.html#uniplate) exposes `uniplate` and `biplate` combinators. `uniplate` is the Data.Data version of the uniplate combinator, and is offered (via `DefaultSignatures`) as the default definition for `plate`. They are pretty much just your hit-map code turned into `Traversal` form. uniplate :: Data a =&gt; Traversal' a a biplate :: (Data s, Typeable a) =&gt; Traversal' s a `GHC.Generics.Lens` also has a `tinplate` which is a weak version using `GHC.Generics` instead. The `lens` versions provide 2 major benefits. 1) It turns out to be a fair bit faster using the `Traversal` approach, because I don't have to build up an explicit data type of Zeros, Ones and Twos, but I can just `(&lt;*&gt;)` my way through stuff in the environment. But to get there I had to concoct the `Bazaar` comonad that is used to implement some of the crazier traversals that arose through generalizing everything. That in turn helped lead to the current understanding of lenses, traversals, isomorphisms and prisms in terms of profunctors. 2) Many of the combinators can provide `fooOf` variants that work with any `Lens` or `Traversal` or `Setter` where the types can meet the requirements. e.g. rewrite :: Plated a =&gt; (a -&gt; Maybe a) -&gt; a -&gt; a rewrite = rewriteOf plate is just defined in terms of rewriteOf :: ASetter' a a -&gt; (a -&gt; Maybe a) -&gt; a -&gt; a This means it can be used on things that could never support the full `uniplate` API, e.g. because they might contain functions in such a way that a `Traversal` cannot be defined.
The only reason for lookupDefault to exist is to abstract case lookup key map of Just val -&gt; val Nothing -&gt; default so any policy that requires different strictness is confusing. The main reason for Data.Map.Strict to exist is to avoid retaining thunks in maps, I.e. (2). (3) has a far better justification than you give, as functions can avoid calling compare on key arguments only in the case of maps, so I'd rather explain the principle as saying that arguments will be strict if they would be evaluated on all nontrivial calls anyway and it helps performance. Do you have any arguments that being strict in values as (5) is actually a more useful semantics or avoids errors - even supposing for the moment it's simpler, why would anyone reason about it? I think being lazier in values (while retaining (2)) can't possibly cause the kind of errors Data.Map.Strict exists to avoid - if the value isn't retained in any map but builds up excessive thinks then you must be retaining something elsewhere, and then it's more confusing than useful that some map operations happen to have a side effect of seq-ing some values.
I'm aware of the sieve, but this certainly is not the direct translation of the it to Haskell code. For instance, there is no "marking" of composites that are deemed to be multiples of primes. The expression 'p*p - x' which appears in the definition of 'q' is not the kind of thing that appears in the sieve either. Though maybe there is some obscure connection there. In any event, one way to help me gain understanding is to tell me what the parameters to the sieve function represent, or give me some loop invariants for them.
Pull request submitted. https://github.com/tibbe/unordered-containers/pull/69/files
Thanks! This is is very helpful.
There are libraries that can do this. Here are two that come to mind: * [MemoTrie](http://hackage.haskell.org/package/MemoTrie) uses lazily generated tries to do by-value memoization. * [stable-memo](http://hackage.haskell.org/package/stable-memo) uses `StableName`s to do by-identity memoization. Neither quite subsumes the other. In some situations you may want one, and in others you may want the other. I like both. There are also other memo combinator libraries; these are just my favorites (disclaimer: `stable-memo` is mine).
I'll take a look at fclabels. Thanks!
&gt; The uglymemo[1] package is the closest Haskell has to a language pragma. Could you explain what you mean by that? I looked at the package and its source code but I don't see what the connection is to a "language pragma".
I don't see anything that makes `uglymemo` any closer to a language pragma than, say, `data-memocombinators`.
Well, it uses unsafeIO but in a fairly safe way just like GHC would do if it had the feature and you'd use function annotations like inline. Hmm, maybe that's just called a pragma. 
This is not true. Data.MemoUgly.memo :: Ord a =&gt; (a -&gt; b) -&gt; (a -&gt; b) Data.MemoTrie.memo :: Data.MemoTrie.HasTrie a =&gt; (a -&gt; b) -&gt; (a -&gt; b) Data.StableMemo.memo :: (a -&gt; b) -&gt; (a -&gt; b) `MemoTrie` is implemented in a purely functional way and still does not require the user to do any more work than `uglymemo` does. If anything, `stable-memo` is closer to a "primitive" than the others are, since it doesn't even require a type class constraint and is a little more intimate with the garbage collector and memory model of GHC (however, unlike the other two, it memoizes by identity instead of by value). Also, it is not correct to say that `uglymemo`'s `memo` is not pure just because its implementation uses `unsafePerformIO`; its interface is still pure, just like the others.
Ah, I didn't realize that they all used unsafe functions. The solutions in the wiki all pass a state. Have you used your memo package with concurrency? Edit: Wait, how does MemoTrie work? It doesn't use unsafeIO. I'm very confused right now.
They don't all use unsafe functions. `MemoTrie` and `data-memocombinators` are implemented in a completely purely functional way. They're just lazily generated tries under the hood. I have not personally used `stable-memo` with concurrency, but I don't have any reason to believe it would not work. I don't have any reason to believe `uglymemo` wouldn't work either.
Could anyone explain why this answer doesn't mention fmap? When I read the question my first thought was that you could define an fmap instance for lifting. Is this a misuse of fmap? 
&gt; Edit: Wait, how does MemoTrie work? It doesn't use unsafeIO. I'm very confused right now. I suspect you didn't really understand the wiki page. :) Everything there uses roughly the same trick. There is some "already full" container with lazily generated values which the resulting function just indexes into. The only special thing about `MemoTrie` is that it's designed to be more generic; the container is a lazily generated trie. There is no state you have to pass around to keep using the memoization. As long as you always use the same result of `memo f`, it's memoized already. The container is stored in its closure, already hidden away from prying eyes.
MemoTrie uses laziness. For example, consider: memoBool :: (Bool -&gt; a) -&gt; (Bool -&gt; a) memoBool = fromTrieBool . toTrieBool toTrieBool :: (Bool -&gt; a) -&gt; (a,a) toTrieBool f = (f False, f True) -- note: pair cells generated lazily! fromTrieBool :: (a,a) -&gt; (Bool -&gt; a) fromTrieBool (f, t) False = f fromTrieBool (f, t) True = t Now, using it: memoF = memoBool f -- = fromTrieBool (toTrieBool f) -- = fromTrieBool (f False, f True) The first time you call `memoF False`, the first cell of the pair will be evaluated. Any time after that, the cell is already evaluated so it never needs to be re-evaluated. All MemoTrie does is make a typeclass to allow implicitly generating from/to trie for many different types. 
&gt; They're just lazily generated tries under the hood. How does that work? Can the trie disappear when used in a strict function? Would using it in separate threads mean having two tries? 
&gt; Can the trie disappear when used in a strict function? I'm not sure what you mean. &gt; Would using it in separate threads mean having two tries? No. It's just normal lazy evaluation.
&gt; The container is stored in its closure, already hidden away from prying eyes. That makes sense, thanks for that. I don't really understand why GHC would keep everything in memory, though.
No worries. I think I understand it now. 
&gt; Actually, we don't care to try avoiding all collections, only major ones. It boils down to the same thing in the practice. It's hard to *exactly* guess at where the generations will land and *only* do allocations that will be garbage by the next minor collection and therefor won't get promoted to the old generation. So you want to avoid pretty much all allocations that survive the innermost couple of scopes anyway, to avoid accidentally promoting them to the old generation (thereby eventually triggering a major GC). And in those rare cases where allocations would be cheap (because they're allocated and immediately garbage right after) they'll usually be short-lived enough that you might as well put them on the stack and not allocate at all.
`data-memocombinators`, `representable-tries` and several other packages provide variations on this functionality, either through `unsafePerformIO` or by playing delicate games about making structures full of lazy places to put the data as you evaluate things. It turns out there are a lot of interesting points in the design space. So when Simon and the others were deciding how to implement memoization, what they decided to give us instead were a bunch of primitive tools that could be used to implement whatever solution was appropriate to your particular problem rather than try to build a one-size-fits-all notion of memoization and ignore the things that don't fit. You may want to read ["Stretching the Storage Manager"](http://community.haskell.org/~simonmar/papers/weak.pdf) by the Simons and Conal it goes into quite some detail about what language support was added for memoization. The tools that they added there are incredibly useful for a wide array of tasks that just happen to include memoization but aren't limited to it.
Not a direct response to you, but I would like to remind the /r/haskell community that we all started from a position of ignorance at one point, and given that I would suggest that downvotes should not be applied so liberally to someone who said something wrong but followed it up with eagerness to improve his or her understanding.
mysql is not such a bright idea. if you want to be serious about data integrity, go postgres. and even if you value the (arguably better) replication features higher than data integrity, use mariadb. you don't know what new surprises mysql (oracle) might bring.
This solves a practical problem in a straightforward way, but I am bothered by a few details: * The abbreviations of `Representational`, `Nominal`, and `Phantom` are gratuitous. * `@` is appropriated for something unrelated to its usual pattern meaning of “as”, when it could just as well be `::`—and thus `Representational` etc. could be types of kind `Role`. * It remains possible to write data types that will typecheck but have surprising runtime behaviour unless you explicitly intervene (the `Set` example). 
I use Acid-State by itself, but since you ask this question my guess is you have so much data it won't fit into memory. Some explanation of what your data is, how you intend to use it (what kind of queries) would be useful to get better answers.
Could you please explain what kind of surprise mysql may bring? 
Could you just really quickly tell me what the difference between by-value and by-identity is? I have some vague idea but it doesn't quite fall in place.
Indeed, I don't know that expecting people to mark the roles of their type-variables is going to work out given the fact that it looks ugly, requires GHC 7.8, and only affects matters to people using GND in the special case when the wrapper has a different instance for a typeclass than the wrapped type. Having said that, in my experience GND is *extremely* handy when you need it, so I am glad that they are doing what they can to fix its problems.
I agree with points one and two. Regarding three, note that you can even break Set's invariants without GND. See for example [this comment on stack overflow](http://stackoverflow.com/questions/12735274/breaking-data-set-integrity-without-generalizednewtypederiving/12744568#12744568).
Havoc? It shouldn't be harder than with a traditional database.
That's a non-example. You cannot have multiple instances of the same class&amp;type like that. It's a bug in ghc that this is not flagged as an error. 
How about this as a little idea I'm working on: Taken from the idea of Event Sourcing: You store atomic events that happen to your data model. Assuming an accounting package you might have events like * AmountMoved Acct Acct Double -- from, to, amount * AccountAdded String (Maybe String) -- account name, parent account Current state is basically a left fold over the events. Continuing the example * BalanceSheet = Map String Double Current state supports the concept of snapshots so that you don't have to play back form the beginning of time, but just from the last event that's been folded onto the state. When a new event is added it will be written to the database (postgres in my case, but could be redis, elastic search) and also folded onto whatever states are currently in memory and care about this particular event. It can also publish the latest state (or a transformed event) out to the connected client over a socket, ØMQ, websocket whatever. Some of this is captured in the cqrs package http://hackage.haskell.org/package/cqrs, but after a conversation with Tekmo I'm going to build mine around pipes. I'd be interested to hear people's opinion on this idea.
Usually the term "pragma" is reserved for things that change the behavior of the compiler so that it processes the source code and/or generates the machine code in a different way. For example, the `LANGUAGE` pragma lets you enable extensions to the Haskell language that result in the source code being interpreted differently, and the `INLINE` pragma tells the compiler that it should always inline a particular function no matter what rather than apply a heuristic to see whether to inline it. Merely writing code that does something radical "under the hood" is not considered a pragma because it is just normal Haskell source code that happens to use some unsafe functions.
Good point. All our container data structures are spine strict to have easier to reason about big-O bounds. A non-spine strict map would e.g. have a O(1) `insert` just to have some random function e.g. `null` take O(n*log n) some time later.
Ah, I get it. Thanks!
Well, multiple instances of the same class and type are fine of course, as long as they're not in the same codebase. So should it be an error to have multiple instances _in scope_ even if they are unused? Or should usage somehow be tracked outside of class constraints?
It would be great if strictness/non-strictness would appear in the types like in Scala.
Replace "explain" with "reason about" and my argument still holds. People were surprised by the behavior of the lazy `Data.Map` API in the sense that they used it incorrectly and created buggy programs. Search for `insertWith` on StackOverflow for some examples. I felt that (5) would be best to address the issues, but perhaps (4) is better*. \* In fact, I think what's really needed is to address the issue strictness at the language level, perhaps by allowing strictness annotations in function types. 
Aside: Contrary to what the blog post says,`Control.Monad.State.Strict` is not strict in the state, only in the sequencing. This is another example of an API that gets used incorrectly all the time by seasoned Haskell programmers due to surprising laziness.
Maybe I'm lacking coffee, but why is this here? Someone decided to learn haskell... is this newsworthy?
I am not familiar with CQRS, but it seems a great idea 
Yes, grab a cup of coffee! :-) Then add his blog to your newsreader and maybe you'll have an interesting read every now and then. I'll bet a day-to-day write up of someone learning the language can be valuable for newcomers without the time to play with Haskell daily.
&gt; The bug – which rightly is a flaw of the theory, not the implementation – is one of the reasons that modules with GND enabled are not considered part of the Safe Haskell subset. (The other reason is that GND can be used to break module abstraction, a subject which is not considered further here.) But SafeHaskell already permits breaking module boundaries, doesn't it? Using `StandaloneDeriving` you could add orphan instances to an imported type and get access to hidden internals. I should try it and file a bug ...
I learned about them by writing small programs with them, I played around with the Reader, State monad etc. Then later I discovered monad transformers when I was reading the Scheme in 48 Hours tutorial, I think it was then I really understood monads. Right after I had used monads in a "real" program I came back to the monad tutorials and now they made sense. I maybe don't know monads on a category theory level, but I can use them and I'm happy with that. :-)
Data.Persistent.Memoization from the package TCache also uses stableNames to memoize the result of an expression (in any monad). It has also a timeout, after which the expression will be re-evaluated. I use it to reduce the number of system calls and to cache dynamic web pages in Web applications.
Thanks for the tutorial, gonna try translate it for Helm. :-)
This kind: http://bugs.mysql.com/bug.php?id=17943 Historically, the Postgres philosophy has been to get it right first, make it fast later. Mysql is more make it fast now, maybe get it right later.
And for what it's worth, stable-memo is a library implementation of the memo function described in that paper, in a couple forms and with a slight generalization. 
I sometimes use downvotes simply to prevent the spread of disinformation. I had originally downvoted some of his comments that were factually incorrect, and when I saw that he had edited some, I removed the downvote. I don't really see my actions as disrespectful toward ignorance, merely productive for the newbie community at large. 
bit_slayer did a fine job explaining the differences. If your next question is "why would you want by-identity memoization?", the stable-memo page on Hackage has a few reasons listed. 
i was thinking about oracle not making regression tests available, not publishing useable changelogs (that's why debian is now updating mysql so aggressively) and consider oracle's history of pushing people to pay for something. that remark should be understood to underline the recommendation to go with mariadb if you need a mysql-like solution.
Sure but a time- or space-leak is not the same thing as GC pauses being a problem. In a leak, it is not memory that should be collected, but rather that references to memory remain lying around in unevaluated thunks. So I would say these are two separate but equally relevant issues.
I have also played with even sourcing. It has also an additional advantage: events are easier to replicate than whole states, so horizontal scalability is much easier. I´m developing too an event sourcing functionality by using package Workflow, which has logging and recovery services. To make the folding of events more general, I define all events and the state in a ADT type and define a Monoid instance for them. then the state is the mconcat of all the events. 
I feel like structures that are strict in their values should be spine-strict, but structures lazy in their values should be spine-lazy.
I don't think using `::` instead of `@` is a good idea. Type variables still have kinds, and these kinds are orthogonal to what role they have: I can have a `* -&gt; *` variable of any role, presumably.
&gt; You think people are going to sacrifice type safety because the type safe version "looks ugly"? If you read my comment closely then you will see that I in fact listed multiple reasons.
Okay, I'll address the others, too: &gt; requires GHC 7.8 Big projects are very used to using CPP to enable different things in different compilers. &gt; only affects matters to people using GND in the special case when the wrapper has a different instance for a typeclass than the wrapped type I think you may have misunderstood: roles do not solve the problem of going wrong at runtime because you switched class instances under GHC's feet, and the problem they do solve has to do with two types using the *same* instance when they shouldn't.
The title is a bit misleading though: I was hoping to find "tales from the real world side", namely some success story of a guy getting paid to write Haskell every day :=)
We must be desperate for Haskell news...
It's the summer slump.
When GHC looks for an instance, it does not look at the context of class declaration (the part to the left of the =&gt;), it only matches on the class head (the part to the right of the =&gt;).
 instance Category cat =&gt; Monoid (cat a a) instance (Monoid a, Monoid b) =&gt; Monoid ((,) a b) They will overlap when there is an `instance Category (,)`. It's not really worth checking whether this instance actually exists, so the possibility is enough for a Haskell compiler to complain about it.
It's bullshit but some people need it to motivate themselves, not unlike, "I'm getting better everyday in everyway" nonsense.
Hmm, the fact that there are so many memoization libraries makes me think that memoization might be harder than I thought. Yesterday, I wrote [this simple memoized implementation of minmax](https://github.com/gelisam/tic-tac-top/blob/defcf48251bfe8af2c16d392c91658275f5260cc/src/Main.hs#L91) by generalizing the wiki's [dynamic programming example](http://www.haskell.org/haskellwiki/Dynamic_programming_example#Available_in_6-packs.2C_9-packs.2C_20-packs). Basically, you don't need to list subproblems earlier than their dependencies, you can just list all the subproblems in any order, stuff them inside an array, and let laziness do the rest. For example: fibs :: Array Int Integer fibs = array (0, 100000) [(i, fib i) | i &lt;- [0..100000]] fib :: Int -&gt; Integer fib 0 = 1 fib 1 = 1 fib n = fibs!(n-1) + fibs!(n-2) -- lookup memoized values main = print (fib 100000) To me, this technique is simple and intuitive, so if it works, I don't need a library. But does it work? Here is what I think it does, please correct me if I'm wrong. The first time fibs is referenced, an array of size 100000 is allocated and filled with lazy thunks. Then, whenever an element is looked up, the thunk is evaluated and replaced with its result. Thus, the next time the same element is looked up, we won't need to repeat the computation. Simple memoization through laziness! **edit**: After reading the other threads on this page, I am now convinced that the above technique works, but I now realize that it has limitations, notably the Ix constraint on the type of the input, which clever memoization libraries manage to bypass in different ways.
This is not the same as the Scala situation. The types in Scala effectively tell you the calling convention of the function, with call-by-value being the default, and an annotation existing for call-by-name (this, of course, does not tell you whether the function is actually non-strict in the particular argument). `Eval` was a burden in the case you mentioned because it represents the license to evaluate values of arbitrary type, and that license must be piped through to all the right places in the program.
Yes, of course you're allowed multiple instances if you don't use them in the same program. But multiple instances (used or unused) in scope in Main should be an error.
OK, so in scope in the same file. Yes, that sounds like it would be a good thing.
Let me make some cosmetic changes to the code from the wiki to make it a little easier to follow primesSA = 2 : prs where prs = 3 : prs' prs' = sieve prs 3 [] sieve (p_n : ps_n) x_n fs_n = vs_n where q_n = (p_n*p_n-x_n)`div`2 a_n = accumArray (\b c -&gt; False) True (1,q_n-1) [(i,()) | (s,y) &lt;- fs_n, i &lt;- [y+s, y+s+s..q_n]] ts_n = [i*2 + x_n | (i,e) &lt;- assocs a_n, e] gs_n = (p_n,0) : [(s, rem (y-q_n) s) | (s,y) &lt;- fs_n] us_n = sieve ps_n (p_n*p_n) gs_n vs_n = ts_n ++ us_n Now, let's work through a couple primes to try and observe how the algorithm works. primesSA = 2 : prs prs = 3 : prs' prs' = sieve prs 3 [] sieve prs 3 [] = sieve (3 : prs') 3 [] = vs_0 p_0 = 3 ps_0 = prs' x_0 = 3 fs_0 = [] q_0 = (p_0*p_0-x_0)`div`2 = (3*3-3)`div`2 = 3 a_0 = accumArray (\b c -&gt; False) True (1,q_0-1) [(i,()) | (s,y) &lt;- fs_0, i &lt;- [y+s, y+s+s..q_0]] = accumArray (\b c -&gt; False) True (1,3-1) [(i,()) | (s,y) &lt;- [], i &lt;- [y+s, y+s+s..3]] = array (1,2) [(1,True),(2,True)] ts_0 = [i*2 + x_0 | (i,e) &lt;- assocs a_0, e] = [i*2 + 3 | (i,e) &lt;-[(1,True),(2,True)], e] = [5,7] gs_0 = (p_0,0) : [(s, rem (y-q_0) s) | (s,y) &lt;- fs_0] = (3,0) : [(s, rem (y-3) s) | (s,y) &lt;- []] = [(3,0)] us_0 = sieve ps_0 (p_0*p_0) gs_0 = sieve prs' 9 [(3,0)] vs_0 = ts_0 ++ us_0 = 5 : 7 : us_0 sieve prs 3 [] = 5 : 7 : us_0 prs' = 5 : 7 : us_0 us_0 = sieve prs' 9 [(3,0)] = sieve (5 : 7 : us_0) 9 [(3,0)] = vs_1 p_1 = 5 ps_1 = (7 : us_0) x_1 = 9 fs_1 = [(3,0)] q_1 = (p_1*p_1-x_1)`div`2 = (5*5-9)`div`2 = 8 a_1 = accumArray (\b c -&gt; False) True (1,q_1-1) [(i,()) | (s,y) &lt;- fs_1, i &lt;- [y+s, y+s+s..q_1]] = accumArray (\b c -&gt; False) True (1,8-1) [(i,()) | (s,y) &lt;- [(3,0)], i &lt;- [y+s, y+s+s..8]] = array (1,7) [(1,True),(2,True),(3,False),(4,True),(5,True),(6,False),(7,True)] ts_1 = [i*2 + x_1 | (i,e) &lt;- assocs a_1, e] = [i*2 + 9 | (i,e) &lt;- [(1,True),(2,True),(3,False),(4,True),(5,True),(6,False),(7,True)], e] = [11,13,17,19,23] gs_1 = (p_1,0) : [(s, rem (y-q_1) s) | (s,y) &lt;- fs_1] = (5,0) : [(s, rem (y-8) s) | (s,y) &lt;- [(3,0)]] = [(5,0),(3,-2)] us_1 = sieve ps_1 (p_1*p_1) gs_1 = sieve (7 : us_0) (5*5) [(5,0),(3,-2)] = sieve (7 : us_0) 25 [(5,0),(3,-2)] vs_1 = ts_1 ++ us_1 = 11 : 13 : 17 : 19 : 23 : us_1 us_0 = 11 : 13 : 17 : 19 : 23 : us_1 prs' = 5 : 7 : 11 : 13 : 17 : 19 : 23 : us_1 us_1 = sieve (7 : us_0) 25 [(5,0),(3,-2)] = vs_2 p_2 = 7 ps_2 = us_0 x_2 = 25 fs_2 = [(5,0),(3,-2)] q_2 = (p_2*p_2-x_2)`div`2 = (7*7-25)`div`2 = 12 a_2 = accumArray (\b c -&gt; False) True (1,q_2-1) [(i,()) | (s,y) &lt;- fs_2, i &lt;- [y+s, y+s+s..q_2]] = accumArray (\b c -&gt; False) True (1,12-1) [(i,()) | (s,y) &lt;- [(5,0),(3,-2)], i &lt;- [y+s, y+s+s..12]] = array (1,11) [(1,False),(2,True),(3,True),(4,False),(5,False),(6,True),(7,False),(8,True),(9,True),(10,False),(11,True)] ts_2 = [i*2 + x_2 | (i,e) &lt;- assocs a_2, e] = [i*2 + 25 | (i,e) &lt;- [(1,False),(2,True),(3,True),(4,False),(5,False),(6,True),(7,False),(8,True),(9,True),(10,False),(11,True)], e] = [29,31,37,41,43,47] gs_2 = (p_2,0) : [(s, rem (y-q_2) s) | (s,y) &lt;- fs_2] = (7,0) : [(s, rem (y-12) s) | (s,y) &lt;- [(5,0),(3,-2)]] = [(7,0),(5,-2),(3,-2)] us_2 = sieve ps_2 (p_2*p_2) gs_2 = sieve us_0 (7*7) [(7,0),(5,-2),(3,-2)] = sieve us_0 49 [(7,0),(5,-2),(3,-2)] vs_2 = ts_2 ++ us_2 = 29 : 31 : 37 : 41 : 43 : 47 : us_2 us_1 = 29 : 31 : 37 : 41 : 43 : 47 : us_2 prs' = 5 : 7 : 11 : 13 : 17 : 19 : 23 : 29 : 31 : 37 : 41 : 43 : 47 : us_2 us_2 = sieve us_0 49 [(7,0),(5,-2),(3,-2)] = sieve (11 : 13 : 17 : 19 : 23 : us_1) 49 [(7,0),(5,-2),(3,-2)] = vs_3 p_3 = 11 ps_3 = (13 : 17 : 19 : 23 : us_1) x_3 = 49 fs_3 = [(7,0),(5,-2),(3,-2)] q_3 = (p_3*p_3-x_3)`div`2 = (11*11-49)`div`2 = 36 a_3 = accumArray (\b c -&gt; False) True (1,q_3-1) [(i,()) | (s,y) &lt;- fs_3, i &lt;- [y+s, y+s+s..q_3]] = accumArray (\b c -&gt; False) True (1,36-1) [(i,()) | (s,y) &lt;- [(7,0),(5,-2),(3,-2)], i &lt;- [y+s, y+s+s..36]] = array (1,35) [(1,False),(2,True),(3,False),(4,False),(5,True),(6,True),(7,False),(8,False),(9,True),(10,False),(11,True),(12,True),(13,False),(14,False),(15,True),(16,False),(17,True),(18,False),(19,False),(20,True),(21,False),(22,False),(23,False),(24,True),(25,False),(26,True),(27,True),(28,False),(29,True),(30,True),(31,False),(32,True),(33,False),(34,False),(35,False)] ts_3 = [i*2 + x_3 | (i,e) &lt;- assocs a_3, e] = [i*2 + 49 | (i,e) &lt;- [(1,False),(2,True),(3,False),(4,False),(5,True),(6,True),(7,False),(8,False),(9,True),(10,False),(11,True),(12,True),(13,False),(14,False),(15,True),(16,False),(17,True),(18,False),(19,False),(20,True),(21,False),(22,False),(23,False),(24,True),(25,False),(26,True),(27,True),(28,False),(29,True),(30,True),(31,False),(32,True),(33,False),(34,False),(35,False)], e] = [53,59,61,67,71,73,79,83,89,97,101,103,107,109,113] gs_3 = (p_3,0) : [(s, rem (y-q_3) s) | (s,y) &lt;- fs_3] = (11,0) : [(s, rem (y-36) s) | (s,y) &lt;- [(7,0),(5,-2),(3,-2)]] = [(11,0),(7,-1),(5,-3),(3,-2)] us_3 = sieve ps_3 (p_3*p_3) gs_3 = sieve (13 : 17 : 19 : 23 : us_1) (11*11) [(11,0),(7,-1),(5,-3),(3,-2)] = sieve (13 : 17 : 19 : 23 : us_1) 121 [(11,0),(7,-1),(5,-3),(3,-2)] vs_3 = ts_3 ++ us_3 = 53 : 59 : 61 : 67 : 71 : 73 : 79 : 83 : 89 : 97 : 101 : 103 : 107 : 109 : 113 : us_3 us_2 = 53 : 59 : 61 : 67 : 71 : 73 : 79 : 83 : 89 : 97 : 101 : 103 : 107 : 109 : 113 : us_3 prs' = 5 : 7 : 11 : 13 : 17 : 19 : 23 : 29 : 31 : 37 : 41 : 43 : 47 : 53 : 59 : 61 : 67 : 71 : 73 : 79 : 83 : 89 : 97 : 101 : 103 : 107 : 109 : 113 : us_3 So let's see if we can now figure out what the arguments to `sieve` and the various `where` variables mean. * `p_n` - The next prime we're going to process. * `ps_n` - The rest of the primes. * `x_n` - The lower bound. We've already found and recorded all the primes less than this. * `fs_n` - A list of all the primes we've seen so far, each paired with some offset (which I'll get to later) * `q_n` - Half the distance between our lower bound and the square of the current prime. As we see later, we only need half because we're skipping the even numbers. * `a_n` - Here's the sieve. We're checking the next `q_n` - 1 odd numbers to see if they're prime, relative to the primes we've seen so far. We know the evens aren't prime, so we skip those. We know the `q_n`'th isn't prime, because that's the square of the current prime. By default, we'll set all the values in the sieve to True. If any of the primes we've already processed has a multiple in the range, we'll set that index to False. Here's where the offsets come into play. The offset for each prime is half the difference between the square of the previous prime and the last odd multiple of that prime (we don't care about even multiples). This will always be negative, but the sum of the offset and the prime will always be positive (otherwise it's not the offset of the *last* odd multiple of the prime). So now we just step through the range we care about, incrementing from the offset by the paired prime, marking off the multiples we find. * `ts_n` - The primes found by the sieve. We just transform them back from being half-offsets from the lower bound. * `gs_n` - The new list of known primes. We update the offsets for each prime to be relative to the square of the current prime. * `us_n` - The recursive call to process the remainder of the primes, with our new lower bound being the square of the current prime (which could equivalently be defined as `x_n + q_n`), using our updated list of primes * `vs_n` - The return value - the primes we just found, followed by the primes found by the recursive call. This definitely isn't the easiest code to follow. Some more intelligible variable names and comments would go a long way. 
Yes, 4 is what I'd expect.
In my opinion, it should have been implemented as an inline pragma, like this: data Ptr a {-# ROLE R #-} = Ptr Addr# Ignored by compilers which don't know about it.
It seems nice, have you compared TCache to acidstate？ 
Re: the last question. If you want an open sum type for events, it's probably going to be error-prone in practice -- you really want very hard guarantees about forward and backward compatibility and I suspect that it would be hard to maintain such comaptibility with the event definitions "sprinkled about" rather than defined in a single sum type.
But until there is actually a `Foo (,)` instance, there's no overlap. The issue is that there is a Monoid instance for both `(x a a)` and `(,) a b`. I'm deliberately renaming the type variable from `cat` to help reduce confusion, because the name of the type variable is irrelevant. `(x a a)` and `(,) a b` unify at `(,) a a`, so there is overlap between these instances. Typeclass selection in Haskell *only* looks at the right side of the `=&gt;`; when you say `instance Foo a =&gt; Bar (T a)` you are not saying "`(T a)` is an instance of Bar as long as `a` is an instance of Foo." You are saying "`(T a)` is *always* an instance of Bar. If you end up using this, please add `Foo a` to the list of constraints to be solved for." So with the listed instances, if I do test = (1 :: Int, 2 :: Int) `mappend` (3,4) The instance of `Monoid` to use for `(,) Int Int` has to be chosen *without* knowing whether or not `(,)` is an instance of `Category` or `Int` is an instance of `Monoid`.
Maintaining a coherent definition and types of your events can be advantageous. for example you can rely on some properties of them: You can make them commutative, so the order of events may not matter. that is very good for clustered servers that replicate mutually their events and applicate them without regard for the order. You can define your events to have inverses, in a class HasInverse, so you can roll back an event by applying the inverse event. That may be good when another update (for example a payment) in the same transaction fail. But these advantages depend on properties of your event set. If your development assume these properties. Maybe you can add more event definitions as long as they agree with the properties that you code assumes.
Oh, certainly. However, that's just a question of requiring appropriate type classes for your event type and perhaps using QuickCheck or similar to test the implementation(s) -- I understood the question to be about using an open sum rather than a closed sum type. (But maybe I misunderstood.)
I was just going through a paper by David Wise that recommended the same approach. In my case I'm going to need the most significant set bit as a mask and whether it is at an even or odd position. Right now, I'm doing that through smearing and taking the parity of the smear, and reusing the smeared version to get the MSB mask en passant. If I can get both more efficiently from a tabled lookup, I would love that. :)
I've been wanting to use GND to automatically create associated type families for me. For example, class C a where type T a instance C Int where type T Int = Bool newtype NT = NT Int deriving C This is disallowed by GHC 7.6, I assumed for some reason relted to the unsoundness of GND. Would roles allow for automatically deriving these type families now?
Incidentally, this instance was recently added here: https://github.com/diagrams/monoid-extras/blob/master/src/Data/Monoid/Endomorphism.hs I plan to make a new release of monoid-extras which includes this instance soon.
AcidState doesn't seem like a good fit for a cache - you don't really need the persistence and recovery there, right?
In retrospect, I have to agree. 
This is a ghc bug. The Haskell 98 standard says this should not work. GHC's particular implementation strategy allows it to.
When I ask ghci for the kind of a type constructor, are the roles involved notated somehow in the output? Can I declare a type constructor to have an argument of kind * -&gt; * whose parameter has a specific role? How would I write that? (Would I ever want that?)
The rate at which you produce content and code is both overjoying and depressing. I can't even use reddit as an excuse. You do more here than I do, too. Maybe I should get some amphetamines... or whiskey.
The problem is that the alternative (explicitly writing out all instances) is more portable than the proposal.
Ed is like Neo, he can see the... matrix :=)
Oh man, come on, you can't just bait us like that and then just leave us hanging... tell us how to break things! =)
&gt; I was considering writing immutable data structures in JavaScript in order to tease out ideas for what might be a better UnderscoreJS/LowdashJS, but I get disheartened seeing how well it has already been done. All you have to do is ditch JavaScript. https://github.com/swannodette/mori
`Data.Map` is spine strict but value lazy. If we made it spine lazy reasoning about asymptotic performance would be tricky indeed (Okasaki talks about how to do so in his book).
I think there is a deeper issue here. There are many axes on which we can make a datatype lazy or strict and we simply do not have a flexible way of filling in all possibilities. I think spine lazy data is useful sometimes so it should be available, and the use cases of spine strict data strongly overlap with value strict data. In particular, I think most people would agree that spine-lazy/value-strict is a poor choice in almost any use case. Perhaps Repa does it best with its type indices... Also, see http://blog.ezyang.com/2011/05/an-insufficiently-lazy-map/
There's a mirror. http://hdiff.luite.com
A bit late here but: As for the module abstraction problem, wouldn't that be more cleanly solved by restricting newtype deriving to require the constructor to be in scope?
The ISP is aware of the issue and is working on it. The sys-admin is in contact with them, but he didn't know how long a fix would take when I left the office.
Hackage is currently hosted by Galois. They gave advance notification on the Haskell infrastructure list that their ISP would be down for up to 4 hours , and more likely be down for on the order of 4 minutes, for some network maintenance. As danharaj notes, luite maintains an excellent mirror that has great up time. If you'd like to help out, hackage2 is nearly ready for release, and could use more contributors likely, and likewise there's many other core tools and infrastructure that welcomes more people helping out even in little tiny ways. One of the things I'm repeatedly amazed by (and sometimes horrified by) is how much core ecosystem work is done by a relatively small pool of folks. It makes me wonder how much more amazing stuff could happen in the Haskell ecosystem (both more and sooner), as more people help out. Every little bit of help makes huge difference. It may take a wee bit of time to learn the nitty gritty of the projects you want to help out one, but all the people involved in the core tools/libs etc love more people getting involved and helpin out. 
The papers don't go into much detail about how to actually do that, but I'm the main dev so feel free to ask questions. The mailing list is probably a better place for that though. https://github.com/AccelerateHS/ http://groups.google.com/group/accelerate-haskell
uh? My copy of the email puts the Galois outage several hours away, and at a maximum of 30 minutes (not 4 hours). I think this is unrelated.
Awesome! This article was a fun read and had beautiful interactive results at the end. Would love to read more in this style from Edward in the future!
I actually need the recovery there in case of server break down, just like AOF in redis
Ah. Yep, that makes sense.
I fail to see how going against the semantic conventions of the language is at all principled.
Thank you *so much*! Seriously, that is going to be very very helpful to the community