With great enthusiasm! I personally started from ML languages (F#/Ocaml/Standard ML), the drop wasn't as hard.
The only thing I'd like to add to the other replies is that you should make sure to join the #haskell IRC channel on irc.freenode.net and hang around. There are lots of people there who are always happy to answer beginner questions.
I've been a professional python program for a number of years now, but thanks :) always solid advice. My coworkers have sometimes scratched their heads when I break out `itertools.tee` and `functools.partial`, etc, but I make sure that it reads easily enough and they tend to like it.
Haskell isn't really something you can start hacking at like PHP though :s
Sorry, I forgot a `join` in that `print` expression. You can probably see how to fix it. Indeed, the `(&lt;*&gt;)` implementation is a little obscure, but it's the same as `liftA2 (&lt;*&gt;)`. liftA2 (&lt;*&gt;) :: (Applicative f, Applicative g) =&gt; f (g (a -&gt; b)) -&gt; f (g a) -&gt; f (g b) Or, specifically in this example, liftA2 (&lt;*&gt;) :: IO (IO (a -&gt; b)) -&gt; IO (IO a) -&gt; IO (IO b) Since this works with arbitrary applicatives `f` and `g`, we can be confident that the effects of the two applicatives won't be mixed up—there can be no `join` going on in there, because it's not possible to `join` two arbitrary applicatives! So the type hints that we will get the right semantics. Since we're working on `IO` values, the instance can actually be written like this: Fetch f &lt;*&gt; Fetch x = Fetch $ liftM2 ap f x Then we can say liftM2 ap f x = do f' &lt;- f x' &lt;- x return (ap f' x') = do f' &lt;- f x' &lt;- x return (do f'' &lt;- f' x'' &lt;- x return (f'' x'')) and this makes sense: we first evaluate the "outer" `IO` actions, and then return an `IO` action that evaluates the "inner" ones, and as a result gives the application.
I really wanted to make this comment as well but I figured equally that response would be &gt; muscle memory So I refrained. It was really distracting!
Also, if you define the (illegal) `Monad` instance I gave for `Fetch` and set `(&lt;*&gt;) = ap`, you end up with something equivalent to Fetch f &lt;*&gt; Fetch x = Fetch $ do f' &lt;- join f x' &lt;- join x return (return (f' x')) which is clearly not what we want.
Yeah, it's *way* easier than PHP. It makes sense. :P
I'd argue that what you're calling generalization is the exact opposite. Property A is a generalization of property B if the things that have property A are a superset of the things that have property B. As you go from `Functor` to `Applicative` to `Monad`, the number of things that can implement each decrease, not increase. On the other hand, the number of things they allow you to do increases. This makes them more powerful but less general.
For tooling, that definitely is. Java has fantastic tooling. 
Incredibly astonishing editing skills. I don't think I could even think as fast as you type. :O
Ah, that was yesterday - I'm glad to hear it went well!
How about a lazy tattoo that only gets applied when people inspect it, and then never tell anyone where it is? That way you can avoid the regrets of having a tattoo of a language nobody cares about in 20 years, but also have the boasting rights of having an innovative lazily applied tattoo. You can say “I learned Haskell and all I got was this lazy tattoo.” There might be issues if the tattooist dies before you show it to anyone, but lazy IO is pretty tricky like that. Riffing on that, if you can't be dissuaded, perhaps you can have ⊥ as a tattoo and then you can claim the tattooist did indeed die before applying the tattoo.
I'm sure any serious answer will be pointless
Sudden death stops being a problem once you know how to apply the tattoo to yourself.
you mean point free?
&gt; Elixir runs on the Erlang VM so it is not going to be a purely functional language. Just a quibble, but that doesn't necessarily hold. After all, Haskell runs on x86 machine code which is far from purely functional.
Don't do it. If you need someone to tell you what meaningful tattoo is you shouldn't get one. My advice is that this is a horrible idea and you should not do this.
So if Elixir is 'like' Clojure in the sense that they fit the same needs and sets of problems, my next question is what languages are most 'like' Haskell? My guess would be the ML family, particularly the nonobject-oriented versions like Standard ML?
Sounds cool, but I doubt there will be too much attention from the Haskell community if it's dynamically typed. 
How would you detect / define breaking changes? Removing / changing exports == breaking, adding exports == non-breaking?
How is `foo.bar()` so fundamentally different than `bar foo`? Seems like a matter of syntax to me. What you really need is a solid IDE that can display documentation when you put your cursor on the method, and lets you jump to the definition.
I always thought that Hoogle generates the binary databases from the text ones. Do you mean that Hoogle's information in general is not complete / accurate? My tool here is certainly far from perfect in general. For instance, it would need a much smarter type comparison engine: func :: Num a =&gt; a -&gt; a func :: (Num a) =&gt; a -&gt; a func :: Num b =&gt; b -&gt; b would be considered three type changes for func.
Like the PVP defines it (http://www.haskell.org/haskellwiki/Package_versioning_policy). Basically, if an exported entity in one version disappears or changes its type signature in the next version, then that is a breaking change.
Maybe that ( should be a $ because otherwise the lack of a closing paren would make me freak out. 
&gt; How is foo.bar() so fundamentally different than bar foo? What should be the type of "bar"? Different classes can have methods called "bar" with different argument lists and return types. I think making "foo.bar()" a synonym for "bar foo" is one of those clever ideas that lead to trouble later on. Look at Haskell's problems with conflicting record accessors. The dot notation is just a better idea.
&gt; I will vote with the others that this may be a bad idea. My 30-y.o. friend got a tattoo when he was 18. I think this is kind of the point of a tattoo — to embarrass yourself in five years for the rest of your life.
Aha, that makes sense. Thanks!
&gt; What should be the type of "bar"? The type of bar is whatever it is, just like the type of bar is whatever it is with foo.bar(). Can you explain in concrete terms why "foo.bar()" is better than "bar foo" in terms of coming up to speed with an unfamiliar codebase? &gt; they wouldn't exist if Haskell had used the dot notation to begin with They would. Consider what the type of: example x = x.bar would be, and you'll see that making record accessors not be functions makes little difference, since you can immediately make a function record accessor out of it again.
how about `λ&gt; let 2+2=5 in 2+2` from http://codegolf.stackexchange.com/a/28794/14822 . 
&gt; I think "example x = x.bar" shouldn't compile, unless it has a type annotation. Then you didn't really solve the problem that Haskell's new record system solved. &gt; Regarding readability, "foo.bar()" tells me that "bar" is defined in the class of "foo", while "bar foo" doesn't tell me that, because "bar" could be a function defined anywhere. I don't see how that does you much good. In Haskell it's actually easier to find out where things are defined, since in Java `foo.bar()` could be an interface call, whereas in Haskell `bar foo` is the lexically bound `bar`. Any IDE worth it's salt lets you go to definition anyway, so the point is moot.
I'm so tired of all this Elm crap. Elm has nothing new to offer whatsoever - and its **absolutely retarded** Signal type makes it unusable for any non-trivial application. Fix your language before you fill the internet with useless hype and guides for something that will never be used in any serious project in its curent state. Until you figure out how to implement a proper Signal type you can fuck off and stop spamming your crap.
aww. that's bad. is this known/documented somewhere? maybe that's a worthy feature request for ghc...
&gt; Then you didn't really solve the problem that Haskell's new record system solved. Meh. I think it wasn't worth solving. Do you ever actually want a type like "anything with a bar() method" in your program? Would you write such a type by hand if inference wasn't available? If not, why do you want to infer it?
I thought the ability to do `x { getName = something }` was a property of the constructor, not the record syntax. Man, that sucks.
There is a difference. When bar could be defined as a field for different records, what would be the type of example in this definition? example x = x.bar It cannot simply be inferred, since there can be multiple types that do not unify. However, in GHC 7.10, we get [Overloaded Record Fields](https://ghc.haskell.org/trac/ghc/wiki/Records/DeclaredOverloadedRecordFields) which offer a way to solve this. Also relevant: [Dotpostfix] (https://ghc.haskell.org/trac/ghc/wiki/Records/DeclaredOverloadedRecordFields/DotPostfix)
I think Elixir is nice that it's drawing people to learn about OTP and the actor model, but really I'd be just as inclined to learn Erlang itself. Recently on the Mostly Erlang podcast, some of the creators of Erlang mentioned that Haskell had influenced some of it's language features. Putting aside the typing differences, when you look at some of the syntax for pattern matching it does look a bit Haskell-inspired (where it allows you to split a function definition by cases, where in most ML's you need to put all the cases in a single expression). I'm interested in the question of why not just learn Erlang itself, if that is the platform you are interested in.
That's what I said I think?
For example if you have a record with the field bar, and another record with the fields bar and baz, you may want to write functions that work on both types as long as that function only uses bar.
Yes, the objects of List are not all singleton lists - they're homogenous lists. But that's okay, because there is no requirement that a functor F: C -&gt; D maps the object(C) -&gt; object(D) surjectively. So it's no problem at all that there are more objects in List than just the singleton lists. As you state above, the mapping from morphisms of Hask to morphisms of list is given as follows: Given f in morphism(Hask), with f having domain a and codomain b, F(f) :: [a] -&gt; [b], while formally a map between singleton lists of type a and singleton lists of type b, is taken as a polymorphic function between lists of length n in [a] and lists of length n in [b] for utility. That complete's the characterization of our functor from Hask -&gt; List. Now a monad (http://en.wikipedia.org/wiki/Monad_(category_theory)) is an endofunctor (remember List is a subcategory of Hask) from Hask -&gt; List, equipped with two natural transformations (bind and return). So you are correct to say that return is a natural transformation from (Identity :: Hask -&gt; Hask) -&gt; (F :: Hask -&gt; Hask), not F itself. But there is no extra information to specify about return. It maps the identity functor to F by just applying F to the result of the Identity functor. It doesn't encode the mapping from a -&gt; [a]; F has already done that.
The empty tattoo. 
I'm intimidated by the Erlang syntax. I hate ugly code and shun C syntax languages as well as PHP and Perl. I don't even like the look of Lisp syntax. To gauge how shallow I am, here is my polyglot study list: Ruby, Python, Haskell, Lua(?), CoffeeScript(?), and Elixir(?). Yes I know I can't get around JavaScript and that CoffeeScript is worthless without knowing JS. You can tell from this list where my priorities lie. Obviously I'm an amateur and not to be taken seriously, but I'm not alone. Also, I've read reviews from seasoned programmers who developed successful projects in Erlang who still don't like the language, find it unforgiving (to the programmer, not the application), would prefer another tool if they could absolutely get away with it, and some even had regrets about using Erlang despite delivering a successful project. Obviously this is opinion, but I see that as a red flag.
You're not the first one to want something like this. 
Any particular reason why `cabal.config` is part of the repo? I had to delete it in order to build with ghc 7.8.2 instead of ghc 7.8.3 
I usually prefer composition to subtyping. If I can't use composition, I'll use nominal subtyping, with explicitly declared relationships between types. Structural subtyping is pretty much never what I want.
Another thing that's become apparent to me as I think about this specific example is that while Haskell seems very formal w.r.t. types when compared to other programming languages, it's actually very informal when compared to mathematics. This makes the language more powerful though. For example, the map function wouldn't be very interesting in programming if it created a function that only worked for singleton lists. But it makes it very difficult to think of Haskell in terms of Category theory without having to make apologies for certain things not lining up quite right.
Ok. :)
(!!) &gt;&gt;= &lt;$&gt; What does this do?
I think people are being overly judgmental of your desire to get a tattoo. That being said, it really depends on your reasons for getting a tattoo - for example, I could only see myself getting a Haskell tattoo if it was part of a larger ornamental piece. I'd probably use something with a lambda symbol for the cool factor.
You would be right. Also, VHDL is surprisingly similar to Haskell in its execution model, even if everything else is different.
If you have long arms and a high tolerance for pain, you might also want to consider getting a COBOL tattoo.
It fails to parse, because `&lt;$&gt;` isn't in parens ;)
They _morally_ satisfy those laws -- the results should be the same if you substitute those definitions in. Its just the case that they might use a more efficient _implementation_.
Very nice! It took me a long time to learn this stuff slowly picking it up by reading other people's code. It'd be neat to see follow-ups comparing Scott and Boehm-Berarducci encodings, too.
&gt; Yes I know I can't get around JavaScript and that CoffeeScript is worthless without knowing JS. I don't think you have to learn JavaScript by itself to use prettier JavaScript layers like CoffeeScript. Of course, it's true that if you learn CoffeeScript you can probably write plain old JavaScript (hideous as it is) but I wouldn't consider that quite the same. Incidentally, have you ever looked at [LiveScript](http://livescript.net/)? It's similar to CoffeeScript in syntax but improves on it substantially in my opinion. It also borrows a lot of concepts from functional programming like currying functions. Under the hood it *is* running on the JavaScript model, but as a Haskell programmer I found it pretty palatable and pleasant to use. 
It becomes a much easier feature request after kind equalities go in.
In _Haskell_ a functor with a capital F is constrained to be unique if it exists. You can very this by checking that given `fmap id = id` the other functor law follows by free theorem. 
I never really know how to deal with specifying versions for my packages. Like, how do I know what versions I need? I only know it works with what I've got, so I simply did a 'cabal freeze' and committed that. I mean, I could've added &gt;= version_I_used to the .cabal for every package, but that would've probably caused you the same issue.
My first thought would've been that I like to see this on the Hackage website, or that Haddock docs should have some kind of 'history' to them!
You could have ⊥ on your ⊥?
Not exactly Haskell, but I think this is appropriate: http://en.m.wikipedia.org/wiki/Fixed-point_combinator#Fixed_point_combinators_in_lambda_calculus That's a mind-boggling and beautiful equation of the lambda calculus, which underlies all functional programming. And it has a hidden connection to Haskell: it was discovered by the man whose name was chosen for the Haskell language. Plus, a tattoo is appropriate because it'll probably take several decades of staring at it to understand it :-)
After a certain amount of time you'll be at a point where you'll be trying to understand a bunch of foundational typeclasses (Monad, Functor, Applicative, etc.). It will be a lot to work through. Don't worry if you don't get everything on the first pass. Go through it in multiple passes, over time you'll understand more and more, and each next thing will also be easier to understand.
Your false accusations here amount to libel. Read the very link you mentioned. The process for incorporation from that very link: &gt; 3. Incorporate your Nonprofit Organization &gt;a. Prepare and file articles of incorporation with the Secretary of State &gt; * Articles of Incorporation for Nonprofit Corporations explains what to include in your articles in order to qualify for 501(c)(3) tax-exempt status. &gt; b. Create the bylaws etc. and down at #6 there is "Apply for tax exemptions" (i.e. file at the federal level) We are only through #4 so far. **The standard process of legal incorporation is filing Articles with the state as a *prerequisite* to federal filing**. I think you are entirely confused by the line that says "Articles of Incorporation for Nonprofit Corporations explains what to include in your articles in order to qualify for 501(c)(3) tax-exempt status." What that means is merely: the Articles must be written so that they will explain your tax-exempt purpose for when you *later* apply for 501 status with the IRS. We are not a scam, we are testing with only *fake* money, we are accepting donations for our development, we do not have a single interest-bearing account (although if we do someday, that would be perfectly fine and not at all scammy). Your entire speculation is based on you misunderstanding how U.S. incorporation process works combined with a willingness to rush to gross conclusions and post public accusations.
congratulations, this is the dumbest, most vapid thing I've ever read (and that's up against significant competition).
Yea, I think all of you misunderstood my question! I don't want to know your still interesting reasons to not to have some kind of tattoo...thank you to point it out!
As a Haskell programmer I prefer the Scott encoding since it doesn't bake a recursion scheme into the representation. So it's just like data types in Haskell. 
That's quite a good idea, man! Not bad at all!
I really thank you, man! I'm glad to be in your stupid classification nobody cares!!! Tips for the future: if you don't give a shit of a question, just keep living your little life...haskell users don't really need your opinion!
that's what I'm looking for!!!
Fucking. Genius. 'nuff said!
I don't mean I need a tattoo with some sort of meaning concerning my life or human being...I what to know if there is some kind of funny/tricky snippet of code!
I've seen that site when I was researching whether somebody had already implemented a Hackage diff tool. From what I can tell, it's only a textual diff between releases? Digging through every single implementation source modification, moved around block of code, comment update, change to a non-public API etc. is quite different from instantly getting a succinct list of actual API changes.
Sorry, total Cabal versioning newb here ;-( So, why is the cabal freeze thing a problem? Wouldn't you just build the thing in a sandbox and get the exactly right packages? The current HP is quite old, and I don't have it installed.
Yeah sorry I may have been too concise. I was more thinking about adding an API diff bit to that tool. The thing I like about hdiff is that it's public. I've been using it for years, so if there's a way to integrate your API diff bits to that, that would be great! I'm pretty sure Luite would be open to adding something like that if that makes sense (maybe I'm overlooking something not obvious).
&gt; I simply want to know what haskell users think about it!) you asked, i told you what i think .... 
Just....just read the "tips for the future" section, and everything will be fine! My post is not like "go out of here, you stupid asshole!"...if you have something USEFULL to share, do it. Otherwise, please don't spam your meaningless opinions.
In itself it’s nothing special, but when it’s used well it can be amazing.
The versions might be constrained by something else, and if you use a freeze file it's very likely that there won't exist a valid build plan. The freeze file also won't be used if someone installs your package from another directory or through hackage. Dependency errors from freeze files are a bit confusing since they are marked as "global constraints", it should perhaps be fixed in Cabal but for now it is what it is. Resolving dependencies... cabal: Could not resolve dependencies: trying: hackage-diff-0.1.0.0 (user goal) next goal: HTTP (dependency of hackage-diff-0.1.0.0) rejecting: HTTP-4000.2.17 (global constraint requires &lt;4000.2.17 || &gt;4000.2.17) rejecting: HTTP-4000.2.16, 4000.2.15, 4000.2.14, 4000.2.13, 4000.2.12, 4000.2.11, 4000.2.10, 4000.2.9, 4000.2.8, 4000.2.7, 4000.2.6, 4000.2.5, 4000.2.4, 4000.2.3, 4000.2.2, 4000.2.1, 4000.2.0, 4000.1.2, 4000.1.1, 4000.1.0, 4000.0.10, 4000.0.9, 4000.0.8, 4000.0.7, 4000.0.6, 4000.0.5, 4000.0.4, 4000.0.3, 4000.0.2, 4000.0.1, 4000.0.0, 3001.1.5, 3001.1.4, 3001.1.3, 3001.0.4, 3001.0.3, 3001.0.2, 3001.0.1, 3001.0.0, 3000.0.0 (global constraint requires ==4000.2.17) Dependency tree exhaustively searched. In my case I have the following in my cabal config, to make my life easier for a variety of reasons, which clashed with your freeze file ``` constraint: transformers == 0.3.* constraint: mtl == 2.1.* constraint: network &gt;= 2.4 &amp;&amp; &lt; 2.5 constraint: HTTP &lt; 4000.2.17 || &gt; 4000.2.17 ``` I'm guessing network and HTTP works fine with my preferred versions, transformers and mtl don't because you use Except. If you want Except you can add a dependy on transformers-compat to support mtl 2.1.*. But if you want ExceptT I think you'll just have to stick with the newest major versions of both. 
This all seems very cool in a type-magic kind of way, but can anyone give an example of why or when using Church representations would be advantageous or useful compared to the naive approach?
I use Except because Control.Monad.Error is marked deprecated in the most recent version. I also use modify', which was not present in mtl 2.1.*. So I guess there's not much I can do to make it compile on your setup if you restrict mtl to a specific version, cabal.config or not. But yes, it seems like a good idea to remove the cabal.config and replace it with version bounds in the .cabal.
I think it would be wonderful if this functionality ended up being available right in hdiff / hackage / hoogle / haddock / cabal etc. If anybody wants to use the code, needs some help with it etc., let me know!
&gt; k -&gt; hom would be less powerful than hom ~ Hom. k -&gt; hom would say there is only one for a kind, but not also enforce that it matches what Hom says, which I need to refer to it later. &gt; &gt; That particular fundep can probably be removed, effectively recovering it on the backswing from the shape of Hom. That matches what I would have expected. I thought there might've been some subtlety I was missing. &gt; I've been slowly removing the need for kind -&gt; type constraints from the hask codebase, just because they are somewhat surprising, and with this much bizarre code, anything needlessly surprising jars the audience for no reason. (For what it's worth - which is not much - `hom ~ Hom` was more bizarre to me at first, likely due to the invisible kind indexing which /u/sjoerd_visscher noted.) &gt; with this much bizarre code I get a similar feeling when reading this code (and also `lens`) as when I and others go to preposterous lengths to implement or encode things in C++ which are basic bread-and-butter language features in Haskell. The language is being stretched to its limits, and you can practically hear the creaking. Do you by any chance have an idea in your head of a hypothetical future language in which these ideas would be easier to express (or come built-in), without the hacks and boilerplate, or is Haskell at the frontier of what's possible?
There are two different encodings, the Scott encoding and the Boehm-Berarducci encoding. They coincide on non-recursive types and differ on recursive types. Roughly, the Scott encoding gives you pattern matching, while the Boehm-Berarducci encoding gives you foldr. Also note that in a strict language the non-encoded and encoded versions of a type would not be isomorphic. The encoded version is lazier.
I didn't come up with it - it's a brain teaser a friend of mine showed me. We had a lot of fun, first just trying to figure out how to resolve the types, and then figuring out the behavior. Took a couple hours of head-scratching!
Thank you :) Oops! Corrected.
Paul's talk at the Boston Haskell meetup. Sorry for the video quality, he posted very thorough notes here: http://gist.github.com/pchiusano/444de1f222f1ceb09596
You can think of this as a natural way of transforming types into "continuation passing form". This is useful if you'd like to understand the representations of `LogicT` or `attoparsec` `Parser` which use continuations for efficiency. More generally than than, you can think of Church-like representations as being sort-of dual to standard data type definitions in the sense of standard data types are focused on how to build up a type while Church-like representations are focused on how to pattern match on it and eliminate it (initial v final representations). So even if the applications are not immediately useful, they get to be useful quickly.
Well for god's sake run it through the ghci before you get it. Who knows what these lunatics are gonna give you. Still don't do it. It's gonna end up irrelevant. 
That would be me! All I can say is don't late-night code golf and drive...
What you did is far better the labor-intensive arcane incantations being proposed in this thread. Keep working on writing great code, don't martyr yourself at the altar of cabal.
Is there any reason whatsoever to conjecture that the latest versions of everything are mutually compatible? If they were, cabal version hell wouldn't exist.
&gt; dynamically typed Meh. Sorry.
Can't wait, thanks for working on this kind of stuff! ;-)
Sounds cool. How is this different from cabal behavior in the latest stable release? Is step 4 the key change? 
dude you cut your hair
Indeed, in dependently typed languages they are just functions.
I keep chasing after what a decent successor would be for Haskell for the kinds of things I like to write, but I confess the outlook of those efforts is still murky. The power to weight ratio of the Haskell language and ecosystem is very very hard to beat. Such a system would have to be a huge step up to compensate for that loss and I'm not seeing many deep wells of potential to make such a big difference. Of course, this could change tomorrow. I do think there is a point at which Haskell is bad at abstraction in a manner I can make rigorous, but becoming better at it requires trade offs I don't know that we as a community would accept.
Yes, in current Cabal, when you do step 4, the new install of q-1.0 overwrites the old one, so it's no longer possible to build both versions of r.
I love the "plug-in" example. More and more I'm beginning to notice that there seem to be two extremely common forms of composition: parallel composition (thing 1 and thing 2 occur "simultaneously"), and dependent composition (thing 2 "depends" on thing 1). Speaking abstractly, I believe this is merely the difference between a commutative Monoid and a non commutative Monoid.
&gt; Like, how do I know what versions I need? I'm glad you asked! I wrote a tool for this purpose, [cabal-rangefinder](https://github.com/gelisam/cabal-rangefinder), which just binary searches through the versions of all dependencies to find a point in the past at which things break. I'll send you a pull request with its results.
Oh, neat! That's a pretty time consuming process, I imagine. Setting up the dependencies in the sandbox for my tiny project already takes an eternity.
Yes, it takes a lot of time (which is why I haven't sent you this pull request yet), but at least with my tool, it's computer time instead of human time.
Or trace monoids!
Damn. Someone beat me to it.... &gt; We can compose things that do not even remotely resemble functions, such as proofs! Clearly, some of us are not the intended audience.
[huh](http://cdn.hark.com/images/000/006/085/6085/original.0)
Is there a newtype in base providing the general `Monoid` instance for `Applicative`s? I have never been able to find it. Any reason why there shouldn't be?
You could make it easily enough. Personally I find liftA2 far more explicit.
&gt; You can think of this as a natural way of transforming types into "continuation passing form". CPS, Church encoding, and Yoneda are all basically the same Kan extension... so, not only can you think of it this way, it really is this way! ;)
I could make any of the newtypes in Data.Monoid easily enough, but they're provided by base anyway.
I believe `reducers` has this as `Ap`.
Would be very interested to read more about these thoughts, if ever they gel into a form suitable for expounding upon to a largeish audience.
[Indeed](http://hackage.haskell.org/package/reducers-3.10.2.1/docs/Data-Semigroup-Applicative.html). Thank you.
Looks real promising! Is this effort part of some GSoC or work towards Backpack? Does it extend to more than dependencies, such as flags and compilation modes?
It's infrastructural work for Backpack, so the intent is for it to extend to record arbitrary information (though right now we're only planning for it to let us distinguish between different choices of Cabal dependency resolution and implementations of Backpack holes)
... can't ... tell ... if ... satire ...
Yeah it is. But actually i am now really interested why ``[-10..-5]`` doesn't work as one would expect.
When you want to specify `-N` (with `N` being positive, of course) you shall use `(-N)`.
It's because `..-` is interpreted as a single operator. Haskell allows you to define your own operators made out of various non-alphabetic symbols, so the parser is thinking here that maybe you were trying to use an operator called `..-` that you forgot to declare, or forgot to import from wherever. (Maybe that doesn't answer your question?)
So let's define `(..-)` and use `OverloadedLists` to make this correct Haskell!
Yeah, it would. Haskell likes to think of almost any string of non-alphanumeric characters as a potentially user-defined operator. This includes something silly like `(..-)`. Consider the following &gt;&gt;&gt; let (..-) = (+) in [1..-10] [11] which works because if you inline the definition of `(..-)` you get `[1 + 10]`. Typically, this strong preference for user-defined operators works *great* but it causes some trouble around the few edge cases like enumeration syntax sugar and "negation priority" which causes `(-2)` to mean "negative two" instead of the function section `(\x -&gt; x - 2)`.
&gt; Jenn Schiffer is a neapolitan JavaScript developer who is good in math. Like, she took a lot of math classes beyond calculus in college and grad school. And she uses math. She also took a class about the history of rap and rock and she loves rap and rock. 
Isn't this pretty much homotopy type theory?
Crap, I had `undefined` in the back of my mind somewhere but forgot about it at some point. I think this is "fixed" by requiring the transformation functions to fully evaluate their arguments, since that would treat any value containing a bottom to be treated like a bottom, essentially conflating all bottom-ish values to one. Full evaluation maybe is overkill, some kind of partial evaluation could be enough.
Cool blog post. A minor quibble: there's a lot more than two possible monoids for Int. E.g., (minBound, max) 
Yes, I'm rereading this and realizing most responses (and my own) have the idea that we should try to dissuade you from getting a tattoo... but we're assuming something about you (e.g. that you're learning Haskell and think you're in love with it, and want to commemorate that with a tattoo). But hell, you might not even be a programmer - you might just be some guy who thinks code looks cool and you want us to help you brainstorm. I hope you didn't feel too judged here, by the low number of upvotes. It's your life - have fun and do what you want. If you think a haskell tat rocks the hardest, we are honored :) I still kind of like my suggestions, I think the Y combinator suggestion is especially good, because that one is really a pearl in the field and maybe you can learn it enough to explain it to me &amp; others :)
Remember the `default` which states which one of the types should actually be used when one of the isomorphic ones comes up. Avoiding ambiguities is exactly why that part is there. That should resolve all of the problems you mention. Still, I don't know how I feel about Either being able to flip itself whenever needed. When I had the idea I didn't even realize how radical the implications could turn out.
Suppose `A` and `B` are isomorphic: f :: Bool -&gt; ? f True = a :: A f False = b :: B Very quickly you realize that there must be one "canonical" form. Suppose we make A canonical. Then f :: Bool -&gt; A f True = (a :: A) f False = [b_to_a] (b :: B) (The square brackets indicate a function added implicitly.) But here's another issue. Suppose we have a very simple declaration x = (b :: B) What is the type of `x`? It could be either `B`, or it could be x = [b_to_a] (b :: B) :: A So this makes us lose the property of "standard" Haskell (no GADTs, in particular) that every term has a "principal type" - a most general type that encapsulates all other possible types. And this means that we can't, for example, do type inference on the above term `x`. Without burdening type inference, we can achieve a similar effect by creating a typeclass: class IsA b where toA :: b -&gt; A fromA :: A -&gt; b And then use those class functions explicitly. Additionally, if you're still interested in the implicit conversion: now, if we return to the term `x` defined earlier, it indeed has a principal type! `IsA b =&gt; b`. Idris allows [implicit conversions](https://github.com/idris-lang/idris-tutorial/blob/master/content/misc/args.tex), which do essentially what you are asking for. I'm not sure how issues such as the ones I bring up above are handled. It may be worth asking some of the developers of the Idris compiler about how that is implemented. I'm inclined to say because of all the various possible downsides, and the relatively small upside potential, that this is probably not a great idea. (Also, take note of philipjf's comment that those types are *not* actually isomorphic in Haskell). Now, it looks like you're mainly motivated by the tuple example. Nested tuples are a bummer because there are so many ways to associate them and because performance will be bad (each tuple is another pointer to chase). Much better (due to associativity, and performance) is to go with *n*-tuples instead. I wonder if any library has boilerplate for generating `n+1`-tuples from `n`-tuples? Possibly *lens*, if anything...
1. My college's Programming Languages course was taught with Racket, a Scheme dialect. I loved this course, it was really the class that re-invigorated my love for programming/CS because I was feeling disillusioned at the time. 2. Haskell in particular makes me lean more on the type system in other languages to make my programs as safe as possible by dis-allowing values according to their types (my main programming language at work is C). Functional programming in general helps me create more general functions and think more in terms of composition. Learning about "state-threading" monads (which is how I visualize IO/State/Reader/Writer) keeps my code cleaner and easier to understand, and I feel like I have a much firmer grasp on exactly what is going on as my code executes even in imperative languages. 3. I work primarily with high performance computing. With CUDA, thinking in terms of what your kernel outputs and how to make the inputs independent of each other for each thread being run is the same feeling as creating a pure function that can be mapped on a list of data. In my current embedded HPC environment which is focused on computer vision and image processing, much of what I do is about composing multiple algorithms in a graph and keeping each "node" (function) in the graph separate from each other. Also in general, I think people who get interested in Haskell and functional languages get interested in compilers. Learning about the different compiler optimizations that are enabled by equational reasoning helped me out a lot with HPC. Like Stream Fusion--basically that `map f . map g = map (f . g)`. Simple insights like these pay big dividends in every day programming.
The way I thought of it the `default` would be the canonical you speak of. I don't really see what you are getting at with the second example. The principal type is the canonical type. Type inference would work pretty much like it did before, except that when it arrives at a type it would look for types isomorphic to that one and then choose the one declared as canonical. So in your case the one chosen would be x = [b_to_a] (b :: B) :: A I am also not quite sure whether I like where my idea is going, but it was at least worth a shot. As philipjf posted, `undefined` really puts a dent into everything.
Let's see, from my cursory understanding of HoTT, I think that HoTT would indeed allow you to obtain a witness of `a ~ b` from an `Iso a b`. But I think that in dependently-typed languages such as HoTT, such witnesses usually need to be applied explicitly, via a function such as `subst :: a ~ b -&gt; f a -&gt; f b`. In Haskell, however, equality constraints are applied automatically: {-# LANGUAGE TypeFamilies #-} swapUnit :: (a,()) ~ ((),a) =&gt; (a,()) -&gt; ((),a) swapUnit x = x -- | -- &gt;&gt;&gt; main -- ((),()) main :: IO () main = print (swapUnit ((),())) I assume that the compiler doesn't carry around a dictionary corresponding to this constraint, but instead always uses `unsafeCoerce`. Let's assume a similar constraint `a ~~ b` implemented via an `Iso a b` dictionary. The compiler would of course have to raise an error whenever the conversion is ambiguous: whether to use `id` or `a ~~ a`, which of two `a ~~ b` constraints to use, which of two distinct transitivity paths to use, etc. Also, `a ~~ b` would not imply `f a ~~ f b`, unless perhaps in the special case where `f` is known to be a `Functor`. I think such a feature would actually be rather nice! Haskell requires an unusually high number of explicit type conversion functions, and it would be nice to allow the compiler to infer them when unambiguous.
I feel like I would be able to get started more if I actually knew how to write text to a `&lt;p&gt;` tag or something. Alerts and console logs don't convince me to use it. The "how to get started with grunt" part succeeded, and had appropriate detail. I am sure many other resources for purescript give a copy-paste solution to show hello world in an element, but it would have been nice to do something like a timer updating a single value on screen with the DOM.
This looks a lot like the Hask library, hmm.
I don't actually know any resource that have that. You could checkout purescript-react, particularly the examples folder for that. I believe there are also some basic jQuery bindings (example here: https://github.com/purescript-contrib/purescript-jquery/blob/master/examples/test.purs.hs). I've also noticed this on github which you could use or see as an example of how you'd wrap the basic dom functions: https://github.com/andreypopp/purescript-browser-dom. I think you're right and I should add an example of that. Purescript is young and there isn't a fully fleshed out dom manipulation library yet I don't think, it's a great language though and if anyone's interested please get involved and help out on the library situation :)
Haskell's motto is “Avoid success at all costs”. 
In regards to game modding, I have an unfair prejudice against Java (which minecraft uses), since several many (mostly young and first-time-hopeful-programmers) try to tackle problems without any idea or experience in how to express their desires. Granted, my first experience with java was runescape private servers, I felt like I despised it then too. (Java was the third or forth language I was exposed to) Moral of the story, learning how to barely use a language (especially as a first language) for the sake of games results in many giving up and quitting--in my experience. One of the biggest problems is that self-taught-amateurs don't know how to think about organizing information. I was once in that position myself, only second-semester university courses helped illuminate me. 
That is a joke motto though, right? I'd always thought it was, but after seeing this get downvotes im not so sure...
Of course `(a,())` has a few other extra points too; e.g. `undefined` vs `(undefined,undefined)` vs `(undefined,())`.
This was very informative. Thanks! 
I thought it was a great idea. Sometimes you just ignore what others think and do what you think is right.
I think the phrasing was a joke, but the idea wasn't. Haskell has always aimed to do "what's right" according to a particular vision of what's right and has often done so at the rather great expense of what would make obvious, practical, immediate sense. The result is something interesting new and wonderful. Smoothing out the rough edges until it's "popular" is ongoing work.
It would have been nice is there was something a little less trivial shown here; if you're going to use terms like 'at scale', then you should probably show something with more scale than a simple key logger app. It's really not clear that this stuff lets us do anything more advanced/useful from this post. I would have loved to see how this could be used to build say a large scale automatically parallel computation system from smaller, trivial parts, or, anything at all that demonstrated how we can have correct, complex systems with high assurance. I feel the article misses a huge opportunity (which users of other languages have spared no time picking up on in other discussion threads such as on Hacker News).
The problem still remains. Your original problem was that you had multiple isomorphic representations and you'd like to just choose one, following isomorphisms whenever someone else chooses a different one. The new problem is that you have multiple isomorphisms (on the same pair of types) and you'd like to just choose one, following the iso-2-morphisms whenever someone else chooses a different one. It's the exact same problem, just one level up. The big problem here is that the things you really care about are *representations*, not *types*. A single type can comprise multiple representations, and does so whenever it has non-trivial endo-isomorphisms. In general, we cannot consider a datum to be a value, because the same value may represent different data when considered as being in the image of different isomorphisms. (Consider the `Either()()` example above, or little-endian vs big-endian interpretations of machine words, or any other favorite example of a type with non-trivial endo-isomorphisms). Specifying a datum requires specifying both the value itself and some trace of *how* that value represents the datum (e.g., which isomorphism it's viewed as being under). Keeping track of these traces gets hairy really quickly, but it's required for actually solving the problem of ignoring the choice of representation.
Yes-ish. "Avoid (success at all costs)" is pretty much the Haskell model. We want the language to succeed, but we do not want to sacrifice the things that make Haskell the language it is. Referential transparency, (fast and loose) equational reasoning, explicit effects, etc. are basically not up for discussion. A better module/package/dependency system, better waysw to deal with records, best-in-class performance, etc. we all want; we just are looking for the "right" way to do them. "(Avoid success) at all costs" is a joke. Only the "trolls" want to preserve the sentiment of Haskell as an acedemics-only language, were you have to be a genius to write Haskell and executing Haskell programs in practice is ignored and the language is treated as simply a way to produce ever more abstract papers. Sorry about your downvotes.
all true. 
This is similar to the way pipes and conduits can be composed "vertically": do a &lt;- pip1 pip2 mkPip3 a But, they can also be composed "horizontally": src $= cond1 =$= cond2 =$ snk In horizontal composition, the two objects are run in tandem, as co-routines, trading control at await/yield pairs. In vertical composition, the two objects are run in sequence, as monadic actions or composed functions, where the result of the first action is available (but possibly unused) by the next action. 
Sounds legit to me. Maybe it's not a popular idea, but Haskell seems particularly suited to a few types of game-related things. Procedural generation for example; L-systems are easy in Haskell and guys like the procedural-world blogger make extensive use of them for stuff like architecture. I think a decent voxel engine/game/toolset would be a pretty cool thing to build in Haskell. It's also a good language for DSLs, so scripting shouldn't be a problem. So my new question is, what kinds of games is Haskell well suited for? Edit: The intent shouldn't be to make a game for Haskell to become popular, but to make a game that harnesses Haskell's power. 
Was not expecting to hear go terminology there, but very well said!
Yeah it would be a great addition to hdiff. I've been promising hosting haddock documentation for ages on hdiff (to make it a more complete fallback when hackage is down), and this would be a good reason to actually build the documentation. It would be nice to generate a diff between each version of the package and one version earlier, and then show a color coded version list for the kind of changes (with an extra warning color for potentially breaking changes when the major version number has not been increased). Clicking a link would then show the diff, where the user can then select other versions to compare with. I'm pretty busy with GHCJS at the moment though, so it could take some time before I get to it.
I was working with another student on [this](https://github.com/RTS2013/RTS) project. He was writing the client in JS and I was writing the server in Haskell. It's an RTS game with modding support using Safe Haskell and it has procedurally generates maps. If people have an interest, I could write up a lot more documentation. His client wasn't working so I started to write one in Elm. So far, it just creates some perlin noise maps, some units that can move across the map and collide with one another. Pathfinding is almost done/workable. The client is really basic ATM. Here's a [blog](http://scroungeworld.blogspot.com/) on the project.
How would you handle polymorphism here? The intersection of polymorphism with this seems to make type inference damn impossible.
Look at Warcraft 3 modding. That's where I was first exposed to programming.
Sound good! If you want to use my code for this feature, don't hesitate to contact me with any questions/request when you get around to adding this.
Oh, you’re right! Thank you for pointing that out :)
Why dont languages continue design in a way that will break users, but standardize the new designs infrequently, so that its less of a problem? 
Exactly! I picked cuibe world because it does all of that and is still awesome. Ive also been dabbling with the same question, because its obvious that haskell has a ton of power but I cant figure out what a lot of that power is good for
I have a lot of time for the rest of the summer, experience in frontend and backend web stuff, and id love to h elp out on this game! Some documentation would probably be nice, or just an overview on what I need to get started. Are you and your partner no longer working on it? I read snippets of your blog posts and its clear youve got a very deep vision for the game design here, which ill read more of later because i also love examining &amp; developing game designs. 
Why is it taking os long to develop better records/modules/packaging? ive read a lot of different explanations but what irks me is that it also doesnt seem like there are all that many people working on it
Is this ever going to be implemented in vanilla cabal?
Be cause the userbase won't follow to the new design, or else they'll move slowly and with a lot of bitching. Case study of the first: Perl6, the second: Python3
I would but i dont think im smart enough to really help. im not in the know about caategory theory and as much as people say you dont need to know category theory to really contribute (or at least to use it) i cant find much interesting stuff to work on that doesnt seem to require it. also i dont really want to learn category theory. 
People are working on updating the GHC strictness analyzer. This means that there will be better optimization of the currently naïve lazy code: https://ghc.haskell.org/trac/ghc/wiki/Commentary/Compiler/Demand https://ghc.haskell.org/trac/ghc/wiki/Commentary/Compiler/StrictnessAnalysis Obviously for now Haskellers just use judicious strictness annotations to tune their code. http://www.haskell.org/haskellwiki/Performance/Data_types#Strict_fields https://ghc.haskell.org/trac/haskell-prime/wiki/BangPatterns As for keeping statistics, how about you memoize the results so you don't have to recompute anything? http://www.haskell.org/haskellwiki/Memoization As for libraries, this is pretty nice: https://hackage.haskell.org/package/strict-identity Also these pages in general for more info on ghc strictness/performance: http://www.haskell.org/haskellwiki/Performance/Strictness http://users.aber.ac.uk/afc/stricthaskell.html EDITS: Better links, fixes, updates, etc. 
*grabs of all my yes and throws at ezyang*
cabals not super interesting... ill admit im setting myself up for defeat by saying i dont want to learn category theory but i want to do something interesting in haskell, but itd still be nice if there were something that fit that category. When i think of working on cabal i just think of ironing out a lot of annoying structural problems... but if its not like that id really like to work on cabal because thats the worst part about the haskell experience for me. although ive also heard people describe versioning in cabal as NP-complete... which i didnt understand but i figured was true (i understand what NP-complete is [not the complete part] but i dont understand how versioning fits that description)
No, because HoTT builds on top of intensional type theory, which does not implicitly transfer between equal types, for the obvious reason that in HoTT (and in this case) there may be multiple ways to do so.
&gt; cabals not super interesting Well then, there's your answer to &gt; Why is it taking os long to develop better records/modules/packaging? 
Any cross platform UI library would be great. 
I like this. Maybe it can be taken a step further by letting cabal-install upload successful build results to Hackage.
You could do worse than start by asking on the mailing list http://www.haskell.org/mailman/listinfo/cabal-devel
Or, with stronger typing, ((near)semi)ring(oid)s. Or various other things, such as monoidal categories.
I'm still working on it, but no, my former partner isn't. I'm more than willing to go over the project and help get you started. We could write down the steps we take to help others get started as well. It should be a simple cabal build for the server (after you install dependencies).
&gt; parallel composition (thing 1 and thing 2 occur "simultaneously"), and dependent composition (thing 2 "depends" on thing 1) Sounds like the difference between Applicative and Monad to me.
Backpack is a bunch of related features, but the relevant one here is say you're writing a Cabal package which uses some module 'Plugin': you want the user to be able to provide an implementation of that module. So you specify it as a hole, give it a signature, and then someone else who build-depends on your package can provide a module with that name to fill it in.
&gt; btw, I think you should use Integer for the amounts, it could be usefull wrapping an integer within a newtype with a phantom datatype arguments so you can avoid mixing by error BTC and USD amounts So you would use `Amount USD` or `Amount BTC` in Type signatures and pass values to functions as `Amount 3` or `BTC 3`? And `Amount USD` would represent cents and `Amount BTC` would represent Satoshis? 
Yeah I'd totally be into abstracting the data structures out into a separate package so that applications can easily use multiple markets. I just made my data structures match CampBX's API for easy JSON parsing, I don't have an attachment to them.
You can only make this optimization when you know that evaluating the expression will terminate, or that you will eventually evaluate the expression no matter what. Otherwise you risk transforming a terminating program into a non-terminating program. GHC tries to infer the latter (the strictness analyzer that AisRauli talks about).
So basically, the naive version doesn't work or is very difficult to get to work correctly. Is there anything that could be salvaged by putting some sort of restriction in place? For instance disallowing isomorphisms which don't specialize the arguments of the type in some way, together with only allowing one isomorphism per pair of types. Or am I again just pushing things one level up?
&gt; requiring the transformation functions to fully evaluate their arguments That might break down on potentially infinite types, like lists. As far as I can tell, the right requirement is that the function's result can be pattern-matched to arbitrary depth as long as the arguments satisfy the same condition. I think such functions are known as "productive". For myself, at some point I started calling them "co-total" and it stuck.
Could you give me an example showing how this could happen? (I mean, transforming a terminating program in a nonterminating one). Or the theoretical reason, if you prefer!
honestly you don't need to know any category theory to make a difference in Haskell. you don't even need it to program in Haskell... not sure who is telling you otherwise :) 
Well, the most trivial example would be: main :: IO () main = print (head [1..]) This program "obviously" terminates in a lazy setting, printing 1. If GHC would make the infinite list strict it'd get stuck evaluating the list until it ran out of memory and crashed. Hence a terminating program has been turned into a non-terminating one due to a strictness change.
Currently I'm aiming for somewhere in the middle of September, but that date is not set in stone yet. Proofreaders would definitely be welcome, shoot me an email darthdeus@gmail.com :)
Consider the infinite list of primes: primes :: [Integer] primes = sieve [2..] where sieve (p:xs) = p : sieve [x|x &lt;- xs, x `mod` p &gt; 0] There are more efficient ways of generating this list, but that is besides the point. This list is useful. For example, you can ask for the 10th prime: primes !! 10 This works just fine in Haskell, since it won't try to evaluate past the 10th element of the list. However, if you try to force the evaluation of the whole list of primes, then it will never terminate, because the list is infinite.
If a is a monoid you have the Writer monad.
Why not GTK and [lGTK](http://www.haskell.org/haskellwiki/LGtk) ?
nitpick: QT -&gt; Qt actual helpful response: There is [hsqml](https://hackage.haskell.org/package/hsqml) which allows you to use QML for the front end and Haskell for the backend. While I haven't finished it yet (procrastinating and all that), I've written a game with it quite easily (after first writing the frontend in gloss, SFML, freegame, and Helm). Robin (the library author) is also extremely helpful. The Reichert Brothers have written a few tutorials (like [this one](http://reichertbrothers.com/blog/posts/2014-06-06-hsqmlstocqt-haskell-qml-data-visualization.html)) which demonstrate how to use the library, and there are a few examples written by Robin himself which I found helpful. Even the [landing page](http://www.gekkou.co.uk/software/hsqml/) has a concise example. As for the rest of Qt, I haven't found myself missing it. I think the closest we have is hsQt, but I'm not sure that's been updated since version 4.8 of Qt (which is now at 5.3).
Woa, the animations accompanying the post look neat! Which tool did you use?
some integration with cabal is necessary.
i think that the `Right` based monad and applicative instances are a mistake. of course you can model exceptional things with Either, but that's not the only thing you need a sum type. so i by far prefer Except to Either for that use case. a better (not (yet) possible) Functor instance for Either might be fmap :: (a -&gt; c, b -&gt; d) -&gt; Either a b -&gt; Either c d because it maps over both parts of either. (i know about bimap.)
&gt; I think all programmers care about the correctness of their software. Naw, I use Haskell instead of Idris because Worse is Better and I want to Just Ship It.
AFAIK, there aren't that may people working on it. The number of Haskell developers is far smaller than the number of Python developers, e.g. That said, the new overloaded records are (I think) scheduled for GHC 7.10, and that should make records a lot less painful. Modules and packaging. There was positive news this weekend on the cabal solver. Modules in the style of the backpack paper will take longer because there are few people working on it.
If you want it to be a functor to/from a category other than Hask, you'll have to use the [Functor type class from data-category](https://hackage.haskell.org/package/data-category-0.6.0/docs/Data-Category-Functor.html#g:2). Otherwise, we are stuck with the existing functor.
the isomorphic thing is a red herring here. this is really just a special case of implicits, and implicits are a bad thing :-P
This is just my opinion but -- I think it is clear that the current records implementation sucks. Everyone can agree on that -- we want something better. In some ways, that is a good place to be. If we change the record system to something 'better', then we are going to be stuck with that. We are unlikely to get a third chance to get it right. Historically, there have been a lot of interesting record proposals -- but none that was clearly the best solution in every way. So, it is easier to stay in the 'everybody agrees it is broken' state than move to a 'some people think it is good enough now' state. In the 'everbody' state, you still have the hope that something better will come along. 
i couldnt tell if the new overloaded records were going to be a language extension or built in
I'd like to point out that there's actually some really incredible research just recently done in this area (which, e.g., gives the same laziness suggestions on Okasaki datastrutures as Okasaki does): http://www.ccs.neu.edu/racket/pubs/esop13-c.pdf
You should do it as long as it has no cost.
I am pretty sure if someone came up with a Language that actually were consistently 40% faster than C and so on, it would be adopted immediatly. That is the reason why Fortran still has some success, because it can consistently beat C in numeric code.
The site displays nothing with JavaScript turned off...
Makes a lot of sense...
It is going to be a language extension and only available in GHC for a while. But, NoNPlusKPatterns and heirarchical modules used to be language extensions, too. The Haskell report will likely not contain overloaded record fields for a while. I think the feature can be presented without the FldTy and UpdTy type families, but if not the overloaded record fields extension will have to wait until type families make it into the report. Like most languages, changes and improvements start elsewhere and eventually move into the language specification over the course of several years. Moving as quickly as I think the community would be comfortable with, I wouldn't expect to see overloaded record fields in a report before 2017 and probably not until 2020.
It would be nice if the Elm program could be run to produce a static page in case JS is turned off.
It was always meant to be interpreted as the latter, as far as I know, with the second interpretation as a joke.
"Popularity at all costs" refers to the practise of implementing only features that are popular for the sole reason that they are popular, and refusing features that are impopular. It is certainly sound advice to avoid that practise.
While I was never *opposed* to learning category theory, I also did quite a bit of Haskell hacking before really understanding much category theory. I certainly didn't understand what "a monoid in the category of endofunctors" was, even when I was working my way through some mtl-based experiments. However, like you, I found the most interesting parts of Haskell were often described succinctly in terms of category theory. That was reason enough to learn some category theory for me, and really, it's not that *hard* it's just very *abstract*.
I didn't expect this would be a requirement for Elm, I thought it was aimed more at "apps" than contenty documenty sites? Anyway, yes, for contenty sites, like your blog, it would be good to be able to send the basic HTML and then afterwards load up JavaScript, for search engine and general browsing user experience improvement. Many sites (*cough* Google+, Twitter *cough*) suffer from fat-clientitis. I can imagine writing a larger contenty site in Elm would recreate a similar fat-client issue. 
[jMacro](http://hackage.haskell.org/package/jmacro) allows you to write web apps that feel stateful, but are stateless on the server side, so you don't have to synchronize. If you have time, [this video](https://www.youtube.com/watch?v=rtfbQJGQj0Q) talks about the abstractions used and has a couple of demos. Perhaps some of the same techniques can be used for your purposes, even if it's not a web application.
Would you prefer if I said something along the lines of "Programmers in purely functional languages have a unique tool for verifying the correctness of their software"?
You can use Control.Lens and Template Haskell to generate lenses for a record. Exporting all the lenses from the module is then just a matter of typing a lot of names or learn your editor of choice well enough to use its macro capabilities.
I disagree with your point regarding client-side computations (infosec guy here). From the application point of view, the browser is untrusted, and you should carry as much computing as you can on trusted assets. From an accessibility perspective, client-side computations will often cause more problem than they solve (think about Braille readers, or even just turning JavaScript off). [EDIT] Made point clearer; thanks to /u/dllthomas.
*Why don't we have better records?* Records have taken so long to improve because every one of the proposed improvements comes with a cost that makes it worse for some scenarios than the status quo. Rather than double down on the wrong solution, we've taken a rather slow minimalist path here. Trex, HList, the new extensible records work of Adam from last summer's GSoC, lens have all effectively addressed the problem as libraries for different usecases, but nothing subsumes all of them right now. I can't point to a single design in the space that "is the best", so why should we standardize on a bad design? *Why don't we have modules?* This one has more hope for progresss. When Haskell was initially put forth, it wasn't obvious that it was going to be possible, let alone that it would be possible with a full ML style module system bolted on top. Once Haskell started working, it turned out there are some tricky interactions between the things that makes Haskell Haskell (typeclasses!) and module scopes. We've had some work going on in the background -- Backpack is trying to become a module system for Haskell for instance. However, it currently absolutely doesn't even try to address the typeclass issue. And that is what keeps it a non-starter. Until someone comes up with an way to handle modules that doesn't cost us typeclasses, we'd rather not have them than double down on the wrong solution. *Packaging?* This one is tough. Personally I think we do a good job these days. When I joined the community in 2006 there was nothing and installing Haskell packages was hell. Then Duncan and company came up with Cabal. Overnight developing Haskell went from a one-off activity you could kinda get to work to something you could collaborate on. Is it perfect? By no means. But we now have thousands of packages. We have stackage helping keep the bleeding edge of it working together. It isn't clear to me what a better solution looks like, but here at least there are definitely incremental improvements possible. We could stand to support some things "internal" dependencies on packages that don't tie the package version bound to yours to reduce diamond dependency problems. We could stand to support a nix style package store that can hold packages built with multiple options, something to do binary build distribution, etc. But in practice if you look around at comparable languages, it isn't clear that there is a solution waiting out there that is night and day better than what we have.
Perhaps something even less divisive: "It is often valuable when programming to have a tool for verifying correctness in software..."
Is type isomorphism over recursive algebraic types even solved? I remember seeing something about isomorphisms for (co)inductive types, but it seems like adding function types would cause significant problems...
That's right and they can form alternative semirings of their own, like the tropical semiring.
Nice! I especially liked that the high performance was possible due to purity and immutability.
1. We did some scheme in my Jr. level programming languages class. As the project we had to implement a language, and I implemented a functional language. This would have been 2001, IIRC. I wasn't told about Haskell, and forgot about function programming for several years. I discovered Haskell around 2009 and really enjoyed it writing my first [real Haskell program](https://github.com/stephen-smith/ai-contest-2010) in 2010. 1. With the tools I'm allowed to use at my current position, not much. The C++ compiler has problems with templates, and we normally target Java 1.4 on that side. That said, sometimes I am able to use the new abstractions I've learned. I have an optimization to one of our existing programs that I want to implement later this year; it is based on a better understanding of operational/free monads. 1. I only write in impure languages to pay the bills. At home, I stick to Haskell and Idris; both can compile to JavaScript if I really need to.
I said that becuase some operation rounding errors could be dangerous, so having it wrapped you could implement them yourself making sure they have the propper behaviour you are looking for. They could represent whatever you feel more comfortable with but internally have exactly the same precision as the exchange have; for example you could have: btc:: Double -&gt; Amount BTC btc x = carefullRound (x*satoshisInABtc) mBtc:: Double -&gt; Amount BTC mBtc = .... usd:: Double -&gt; Amount USD usd = .... cent:: Double -&gt; Amount USD cent = .... -- btc 0.003 == mBtc 3 --&gt; True -- btc 0.02 + mBtc 0.3 --&gt; Ok -- usd 12 + cent 50 --&gt; Ok -- mBtc 0.2 + usd 12 --&gt; Error instance Show (Amount BTC) where show (Amount x) = show x' ++ " BTC" where x' = (round $ fromIntger x*k/satoshisInABtc)/k k = 10^numOfMaxDecimalDigitsYouWantToShow So you are dealing with BTCs and no satoshis, but internally you are using satoshis. I've never used CampBX, but the ones I used had bigger precision for fiat currency than cent (I think had at least 4 decimals). 
I just added diffing with local packages, so you can write something like hackage-diff conduit 1.1.5 ~/tmp/conduit-1.1.6/dist/doc/html/conduit/conduit.txt and get a diff with the number of breaking changes.
Very helpful! Does the regular State monad deserve a mention here? As a newbie I see ST in there and wonder how it relates to State. I found a SO answer that discusses it, but it feels like it should be in here.
IIRC, all Applicatives have a reversed form, where the effects are combined right to left even though values are still defined left to right. So, "uniqueness" doesn't happen with *any* Applicative.
I think a small Haskell logo &gt;λ= could be tasteful, and not entirely Haskell-specific. I mean, right now, it would clearly be the Haskell logo, but if in 20-25 years everyone's forgotten about Haskell and we all write in Idris# for iOS it would still be relevant as a combination of the bind operator and the anonymous function introducer. Even if bind goes away, it's still the non-dependent function meta variable on top of the ASCII rendering of a reversed total order operator.
D3 is responsible for a lot of the wow factor in animations these days https://github.com/mbostock/d3/wiki/Gallery
Alternatively, consider something like a "jam" where the object is not to create a huge game, but rather just smaller PoC's
I've heard Qt described as the 'only cross platform toolkit that doesn't make me want to throw my computer out the window'. I want to believe. In the past i've found windows support for gtk to be less than ideal. how is it these days? 
I really wish Elm were a Haskell library instead of its own language, though I understand the reasons for it being so. Would anyone be interested in exploring/building a similar capability on top of Haste/GHCJS (ideally both with glue wrappers)? I would love to discuss if so and we would have a direct use-case for it at work. One could probably cherry pick an existing FRP library as the base, build the DOM wrappers, port the good stuff from Elm and reach something of an Elm port on Haskell. Just a thought... Unfortunately, both Haste and GHCJS are still immature in their own ways, which would probably be the main challenge for such an endeavor. 
I haven't used in on windows for years but I found it ok even at that time :)
A simple rule of thumb I'd say is that `State` is whne you have _a single thing representing some state_. You can have a list as a state and "modify it" by changing the state, which works fine for some cases. `ST` on the other hand gives you arbitrary mutable references. For example if you want to have a computation where you have 4 mutable variables, you could either have a `State` with a tuple of 4 elements as the state, or use `ST` and have 4 `STRef`s Also the reason why I didn't include `State` in the final version of the article (initially I had it there) is that it doesn't actually give you mutable references, unlike `IORef`, `MVar`, `TVar` and `STRef`, which made it feel that it doesn't belong in that article :) Maybe I was wrong.
Have you seen [Shade](https://github.com/takeoutweight/shade)?
From when I once hacked on the compiler, I remember this tree diffing approach was already employed. How is virtual-dom different? I can see the benefits of an Elm api though.
I don't think this is exactly what you're looking for, but there does exist [a tool for haskell to elm translation](https://github.com/JoeyEremondi/haskelm), which lies somewhere along the same lines. 
Thanks for pointing this out - I now remember a post about Shade, but had forgotten about it completely.
Thanks for having me on the podcast guys, it was an absolute blast! Happy to answer any extra questions here. Happy listening, hope my ramblings make some sense :) Little bit of errata: It was [Real World Haskell](http://book.realworldhaskell.org/read/using-parsec.html), not LYAH where I got excited about `parsec`. I should also warn there could be a large part of BS in the mathsy section - I'm happy to be called out there ;)
It is the same idea. Diffing has been in Elm since really early on, but I did not imagine it was anything special and never wrote anything about it. Things always seemed fast enough so I just focused on other stuff. Diffing seems to arise naturally if you have the following constraints: 1. You want the `view` to be a pure function (`Model -&gt; Element` or `Model -&gt; Html` or whatever else) 2. You need to deal with the DOM at some point. So this was a good marker that the world sort of agrees this may be a good way to do things. I ended up not writing about this in the post because the only people who know how Elm's renderer work probably read it in the source code ;) Also, the fact that an `Element` has a known width and height means that it is not as trivial to use the `lazy` optimization because it's harder to get at that size information. I think it could be done, but I have not tried yet. I've not really heard of anyone struggling with speed so we are focusing a lot on interop and tooling at the moment (e.g. elm-html, adding [time travel debugging](http://debug.elm-lang.org/) to elm-server, making elm-get more powerful, etc.) which I think will be higher impact.
Do you happen to know who's working on the strictness analysis for GHC? I thought it was a dead project (clearly I was wrong as that wiki page was last updated 5 months ago). I ask because some of my PhD work deals with strictness analysis and I would love to help! I'll ask on the ghc-dev list as well since that's probably a better venue. Cheers!
&gt; HsQML is [...] easy to compile. I cannot say the same for qtHaskell in my experience I had the opposite experience: I really want to try out HsQML, but I can't get a recent version of *Qt* (not HsQML) to build on OS X 10.6. With the version of Qt I can install, however, I did manage to [compile qtHaskell](http://gelisam.blogspot.ca/2012/04/building-qthaskell-on-os-x.html).
[MFW](http://i.imgur.com/s28PsRv.gif)
The browser is untrusted? Depends on your point of view. The user really has to trust their browser but why should they trust your application? And even from the point of view of the server, an untrusted client doesn't mean the server needs to do everything. At this point, I should give an example to make this concrete. Problem is, almost everything is an example, from animating menus to decoding video. Here's one: Do you use a random password generator website? Do you use one that generates the password on the server? Also, your comments on accessibility are misleading at best. Screen readers work fine with JavaScript. Check out the w3c's accessibility guidelines if you don't believe me. 
Yay Ollie, now you are officially an Haskell celebrity :P Looking forward to listening/watching the episode!
In the first few seconds I learn that there are org-mode parsers in Haskell. Oh joyous day!
Indeed we are - we do a lot of processing of our event stream by walking over the stream as a `Pipes.Producer`. We have PostgreSQL set up in `LISTEN`/`NOTIFY` mode so we can broadcast changes to that table to various daemons - such as daemons that send emails. We can then tack on a `Consumer` to work out if these events are interesting, update a local state model, and send emails or do whatever they need to. It's a really nice model for reacting in real-time to database changes.
For the record, we implemented a similar approach in the reactive dom interface in Eliom (the reasons were not only efficiency though, it has more to do with not refreshing nodes uselessly).
I've said it before and Ollie has said it again at 12:50 ish "It's stupidly easy to hire haskellers" :-)
I wrote that tool! If you have any questions or problems using it, feel free to send me a message. I'll warn you, you might have to tweak the code generated, since I haven't had a chance to update it in a while. I'll try to time the next update of the library to the next Elm release.
This is very true. Haskelm was really meant more for sharing Algebraic types and operations on them between a Elm frontend and Haskell backend, and automatically translating basic functions based on those. Lots of Haskell features are unsupported, like anything having to do with typeclasses.
Also note that a similar optimization that GHC will attempt to do is avoid generating thunks that are never analyzed. For instance starting from `head [1..]` and getting to `1`. I am not sure the analyzer is there at the moment (well to my knowledge it can handle this transformation in -O2, but I don't know how it handles [1..] offhand to know if can unwrap it correctly).
So it uses a DOM 'display list' implementation. Cute.
Welcome to 2014.
&gt; That might break down on potentially infinite types, like lists. Requiring each component be in head normal form would accomplish what is being asked for without that problem.
I would be very interested to learn more about your "own views" and in what ways the major frameworks are advantageous/deficient. I'm only getting started with the major frameworks so I'd find this kind of insight valuable.
What are the reasons for it being its own language?
Here is my quick take: - Make different design choices, experiment with new things, diverge from Haskell as you please - Move fast: You can cater to a smaller feature base, sidestep hard blockers - Simplicity in the short term: Hardwire lots of JS-specific things into compiler - General ease: It's often easier (more fun?) to start from scratch - Branding/ecosystem: You control the whole thing, forge in your own vision
Guess we'll have to invite Chris on again ;)
I am also an infosec guy. I agree that you don't want to be deriving security from javascript, but you ultimately **must** trust the browser. There is a chain of trust going from the server to the browser, and you can't re-establish it once broken (a bad TLS cert, for example), nor can you increase it on the other end. However, the browser has total control over what the user sees and does. If you don't trust that, you have nothing to go on. 
[You should never store currency amounts in floats or doubles](http://stackoverflow.com/questions/3730019/why-not-use-double-or-float-to-represent-currency). And yes, Bitcoins are actually integer [multiples of satoshis](http://bitcoin.stackexchange.com/questions/114/what-is-a-satoshi). Generally speaking, values in USD are integer multiples of cents (some financial institutions may use smaller divisions but I don't think you would need to). Some people use the datatype "decimal" to store USD values in two decimal places (the underlying representation can still be an integer). There's a [Decimal](http://hackage.haskell.org/package/Decimal) package on Hackage. [This package](http://hackage.haskell.org/package/penny-lib-0.4.0.0) uses it to store money amounts so it seems fine.
Wow this is really great news! Being able to install packages without (or at least with greatly reduce) fear of breaking existing packages is a major step to reducing "cabal hell" PSA. When cabal asks if you want to force-reinstall and break packages, you almost undoubtedly do not want to do that. You need to uninstall and reinstall. This script helps: https://github.com/glguy/GhcPkgUtils/blob/master/Unregister.hs
I'm flattered that you linked to my wee strict-identity package. But as a rule, strictness is only needed in inner loops and in data structure definitions, and any application of strictness should be deliberate and justified with thoughtul benchmarks and or heap profiling. I'm not aware of anyone actively working explicitly on strictness for ghc right now, at least not publicly. And I kinda follow ALL the public channels wrt ghc :-) When debating how strict or lazy something should be, write down the desired semantics of your code on undefined and infinite structures, as well the semantics in the finite case. Then if there's more than one valid implementation, benchmark/profile them and pick the best one! At some point performance engineering is a creative human endeavor, because there's lots of tradeoffs to be made and only you can dictate what's acceptable 
That script... would be pretty easy to implement properly in ghc-pkg itself...
I totally agree that a game in Haskell (or at least a pure language) could have a large impact. Though, I think there needs to be a very good incentive to show people WHY you'd choose a pure language. For a while now I'm working off and on on a multiplayer library. It works relatively simple, but requires you to write the game logic in a pure function. Initially I wrote the library in JavaScript, but writing pure functions in js was quite a pain and I was always worried I made a mistake so that the game logic was having side effects. Basically I need the game to be pure because of the multiplayer mechanism I have implemented. The multiplayer functionality requires you to implement an update function of type [Input] -&gt; State -&gt; State. Input can be (for instance) key presses, player join/left. Once you have that, multiplayer (or rather state synchronization) just works. That means it's fairly easy to create small multiplayer games with it. (I can go into the details a bit more, but am on my phone right now) I'm thinking of making/gathering generic simple art assets that can be used to implement these simple games. That way it's easy to get started for anyone to create games on top of it. What's even more awesome is that, because it's pure, third party games can be hosted from my server without me worrying too much about security. I think it could potentially spawn a nice ecosystem of people playing, creating and learning. I can give some more info tomorrow (I'm on my phone now). Please let me know if there's interest. Edit: this is a game I've implemented as a testbed for the library: http://wsball.softwarebakery.com/
That argument does not work, because (at least ignoring bottoms) there are Applicatives for which effects commute, like `Maybe` and `Reader a`. I suspect at least `Maybe` has a unique instance (again, ignoring bottoms).
The original Elm paper that I read suggested pretty heavily that Elm wanted to stay *far* away from laziness as well.
Thanks for the suggestions! Been digesting your answer for a bit. I get that I can use Reader/local as an easy way of closing over an environment, but doesn't that make the new environment immutable? I could pass the args in this way, but then any locals defined in the function would be in a separate env, which is a bit weird. Python's global keyword lets you muck with globally scoped vars too, so it makes this....interesting. 
One additional interesting case is that there are while generally monads are more "powerful" than applicatives, there are some exceptions where the applicative is just as powerful as the monad. The best example is function types: instance Functor ((-&gt;) r) where fmap = (.) instance Applicative ((-&gt;) r) where pure = const f &lt;*&gt; g = \r -&gt; f r (g r) -- Look, Ma, I can do it with `Applicative` and `id`! instance Monad ((-&gt;) r) where return = pure ma &gt;&gt;= k = join (fmap k ma) where join = (&lt;*&gt; id) 
Because a) there aren't many people working on these problems at the moment (but there are people working on them) and b) those who are working on them want to, as usual, avoid (success at all costs); it is important to many haskellers that correct and robust solutions are found for important problems, and if there's some solid theory behind it that's even better. We don't want half baked solutions basically.
I thought of this gif so many times ...
Check out "Optimistic evaluation", work by Robert Ennals and Simon Peyton Jones about 10 years ago. Added link: http://research.microsoft.com/en-us/um/people/simonpj/papers/optimistic/
Yes; sorry. I will edit my comment.
The really nice part is the switch from a static template approach with "holes" that are filled with reactive code to a more dynamical and modular widget approach where both data and rendering are dynamically changed depending on the events. That is the right thing to do IMHO. The template approach has made difficult and non composable all web developments (in the name of a poorly understood separation of concern)
From the user point of view, the server is untrusted (it will perhaps store your password in cleartext). From the application point of view, the browser is untrusted (the user will perhaps inject JavaScript in the page to generate arbitrary inputs). This means that as long as all the computations are done on server-side, you (as the application developper) stay in a trusted environments you control. Once it's out, you are exposed to tampering, code injections and API abuse (or misuse). Point taken regarding accessibility. I had that in mind to explain what I wanted to say, but that was maybe not the best argument.
What is your chain of trust? You are sending some data to a client program and expect it to render your website as you designed it. You really have to consider the browser as out of the trust scope, and must make security decisions accordingly (even when using TLS connections, you never know how the browser is configured and if the tunnel is under attack, maybe with the notable exception of client certificate authentication). I do not think this is about trusting the render, but rather about limiting what you expose (of your application) to the outside world. If you perform more internal computations, you will be able to have private programming interfaces that only you can use. If you rely on external computations (e.g. JavaScript), you will have to expose those APIs, increasing the attack surface.
Also [hplayground](https://github.com/agocorona/hplayground)
Bit painful to read this, isn't it? It's like a joke without any punchline.
I've been using Yesod for the past few months for building my new application and it works amazing. I'm always surprised how easy it is to integrate new things and sometimes even *extend the framework* a little bit. You do not need something to be a *Yesod plugin* just to be able to integrate it really well. I'm a Ruby developer who spent the past 5-6 years in Rails, built a lot of apps, and now I'm thinking of abandoning it completely for Yesod. It just feels much less hacky and once the app compiles I'm pretty confident it works, unlike with Rails where I have to spend a lot of time testing everything. My suggestion would be to just go for it and learn along the way. So what that you won't be able to use 5000 plugins that you could use in Django/Rails, at least you'll learn something along the way and maybe realize that those plugins don't give you much value after all.
please do :-)
The diff process would not be necessary if the non relevant code is not executed in the first place. I suspect that in Elm the events are not constrained to the part in the DOM tree where they appear. This is probably due to the declarative syntax, with no closures in order to make code and the interpreter more simple and clean. In a static template-based application, this is fine. But in dynamic applications, the whole template has to be refreshed, and this triggers the execution of code that would not be necessary to be executed. All this is pure guessing, so forget if this is not the case.
Well, this is more or less what you said in the point 1 and 2 above
Yes! This is such an important fix. Hoooraay
If you implement an FRP library as a Haskell EDSL that has a runtime in JavaScript producing strict code is trivial. Such a EDSL approach has several (mayor, unrelated) drawbacks though.
Could it output just the necessary data so that IDEs could use it? I'm thinking more like, using it in Emacs with flymake.
It is more or less the "xmonad effect". That said, xmonad certainly targets a narrower audience than a popular game would.
Excellent!!!
agree they made the post
I'm beginning to suspect that Haskell is a veiled attempt by mathematicians to see how much abstract math they can force programmers to learn, and we're falling for it hook, line and sinker ... lol
Okay, so you have now reduced the problem of making one particular programming language popular to the problem of making one particular game popular. I have two questions: * Is the new problem really easier? (ex: [Nikki and the robots vs Haskell](http://www.google.ca/trends/explore#q=nikki%20and%20the%20robots%2C%20Haskell%20language&amp;cmpt=q)) * Is Cube World even written in Haskell? &gt; I dont think the open source community has ever banded together to work on something with almost no purpose besides entertainment. Really, you have never heard of any open source game, despite their [very large number](http://en.wikipedia.org/wiki/List_of_open-source_video_games)? Gee, I wonder if that has anything to do with question #1. *edit*: okay, that was mean, I'm sorry. Some games do get popular, and of course you can't succeed if you don't try. Best of luck with your game project; it would be great if your plan worked, I just wanted to point out that it requires more than just hard work.
I'm working on a GUI library called [Threepenny][1]. My plan is to move it to Haste/GHCJS at some point, but this is still a long way off. Volunteer and all that. [1]: http://www.haskell.org/haskellwiki/Threepenny-gui
School's out for over a month so I thought I'd write a blog post to try and make sure I'm still literate when I got back. The general idea is that a program will take a list of statements like CREATE TABLE tbl (col INT); ALTER TABLE tbl RENAME TO atbl ALTER TABLE atbl RENAME col TO col1 and give back CREATE TABLE atbl (col1 INT) I'm not entirely happy with my explanation of lenses but I didn't want to end up writing a lens tutorial. I'm also not happy with the length - I was going to split it into two posts but having one post with the boring parsing in and the other post with the fun migration bit in sounded like a bad idea.
This sounds just like nix (which is fantastic news). Can't wait for this to get in. 
Having gotten used to Haskell, I sometimes find it surprising just how many useful programs can be typechecked using only unidirectional, monomorphic, local type inference like this. It’s also surprising how many programs (and programmers!) can get by without laziness. Recently I had a problem like this: type Label = String data Instruction = Frob | Jump Label type Block = [Instruction] type Graph = Map Label Block data FlatInstruction = FlatFrob | FlatJump Int type FlatGraph = [FlatInstruction] flatten :: Graph -&gt; FlatGraph flatten = ? flatten (fromList [ ("A", [Frob, Jump "B"]) , ("B", [Frob, Frob, Frob, Jump "C"]) , ("C", [Jump "A"])]) == [ {-A-} Frob, Jump 2{-B-} , {-B-} Frob, Frob, Frob, Jump 6{-C-} , {-C-} Jump 0{-A-} ] -- or [ {-B-} Frob, Frob, Frob, Jump 6{-C-} , {-A-} Frob, Jump 0{-B-} , {-C-} Jump 4{-B-} ] -- etc. Where you want to flatten a map from labels to lists of instructions, referencing each other by label, into a single flat list of instructions referencing each other by index. I don’t know offhand how to do this in one pass without laziness. 
We didn't even have 2013.4
Yeah, this is the type of project where Haskell really shines. ASTs as (inductive) data types, along with laziness, make it really easy to express a lot in a very small amount of code. Combine this power with parser combinators (a la `parsec`) and you have a pretty strong suite of tools for manipulating programming languages.
Hurrah for zippers! At the risk of blowing my own trumpet, in [my thesis](https://personal.cis.strath.ac.uk/adam.gundry/thesis/) (section 2.4) I used a zipper to do elaboration for Hindley-Milner (type inference plus generation of the corresponding System F term).
I believe I've seen your thesis floating around -- I'll have to give it a read!
I tried Yesod and it wasn't for me. Maybe I don't have much fun doing web apps, since I did it so long for a living, but I expect things to go smoother. Instead, I was fighting Cabal and trying to divine what magic TemplateHaskell had in store for my code. On top of that, I was running into the usual issues with a small(ish) software project: having to hunt for docs and code examples, missing and limited functionality, etc. Finally, I got frustrated and threw my hands up. I love functional programming, and I do my best to eat the dogfood, but this was not a case where it gave me any kind of leverage to get the job done. I am still hopeful for a brighter day in the future. But I just didn't have the patience for it in its current state.
I found life a bit easier just using Scotty, although it is a little less type-safe.
A thing you might want to keep in mind when writing articles like this: Use examples that make sense. "key1" with "value1" isn't particularly helpful if you want to talk to the readers' intuition to help them understand. Switching "key1" for "username" and "value1" for "denvercoder9" is surprisingly more effective in making the reader understand the concept at hand. It takes a little more effort on the writer, but relieves every reader from an equal amount of effort. Having names like "value1" or "foobar" is bad practise when writing explanations on complex subjects, unless you're intentionally trying to be really abstract.
Yes, you're right, thought abot it too. Will edit, thank you
Looks like a monomorphism error. Add this line: `{-# LANGUAGE NoMonomorphismRestriction#-}` to the top of the source file and see if that fixes the problem.
Yeah, I saw that in the error message. I tried it before I posted here, but it didn't help; just removed the NoMonomorphismRestriction note from the error message. I've updated [the gist](https://gist.github.com/joefiorini/df6813cdf6ac560c1ede) to show the error after adding that option. Thanks! 
This seems broken: ghci&gt; let stmts = map parseStatements ["CREATE TABLE tbl (col1 INT);", "ALTER TABLE tbl RENAME TO atbl"] ghci&gt; singleStatementFromMigrations stmnts [Create "tbl" (CreateTable (M.fromList [("col1", Column Int [])]))] Shouldn't that be "atbl"?
I had a similar experience as a Yesod newbie, and I later found that the reason was that I was coming at it as a non-Rails-Django programmer and an advanced conventional Haskell programmer. Keep in mind that the whole idea of Yesod is to start with the Rails-Django model and improve it by leveraging the power of Haskell. I think that OP, as an experienced Django developer, might not run into that problem at all, like /u/progfu.
Yep, thanks! I'll fix it.
No laziness required: flatten :: Graph -&gt; FlatGraph flatten g = map lookup `concatMap` vs where (ks, vs) = unzip $ assocs g indexOf = fromList . zip ks . scanl (+) 0 $ map length vs lookup Frob = FlatFrob lookup (Jump label) = FlatJump $ indexOf ! label Edit: Ah - didn't see your "in one pass"
Do you mean: without laziness and also without (explicit) memoziation?
I'll give that a try. A bit of background though on why I'm using template the way I am: I tried to setup my own wrapper for defaultTemplate to essentially provide a layout for all of my other templates. I was having trouble getting that to work, so I decided to just make template = defaultTemplate, figuring defaultTemplate should mostly "just work" since it's part of happstack-hsp; that way when I'm ready to try implementing my layout again I can just change the implementation of template and not have to update any more code. Perhaps part of my problem was continuing to use XMLGenerator as m, not ServerPart. I'll look into that. Thanks!
I don't know HSP well enough to really diagnose this. I'm pretty sure if I knew the type of defaultTemplate I could figure things out, but I don't know what package you pulled that in from.[1] Anyway, looks like defaultTemplate can work for any "m", but the particular choice of `m` establishes what `StringType m` is. (I.e. StringType is a type family.) However, your code is sufficiently general to also work for any `m` AND has a type that doesn't involve `m`, but unbound type variables can't be eliminated (it leads to ambiguity) so you get this ambiguous type error. Generally this is solved by adding either a type annotation or a call to asTypeOf that effectively fixes the value of `m` to what you want. Here's a smaller example of code that as a similar error, though not involving a type family: main = getLine &gt;&gt;= (print . read) Here's *a* way to fix it: main = getLine &gt;&gt;= (print . (read :: String -&gt; Int)) [1] And this, among other reasons, is why I always import either qualified or explicitly.
by the way, the type signature of the function runReader is actually: runReader :: Reader r a -&gt; r -&gt; a where the Reader r a is the Reader, the r is the environment, and the a is the result :) Great post though, and keep it up :) we need more people to post things like this.... their own road to understanding, for others to follow and relate to :) 
&gt; [1] And this, among other reasons, is why I always import either qualified or explicitly. Yes please, *especially* for bug reports and help requests!
Thanks bss03! [defaultTemplate is part of happstack-hsp](http://happstack.com/docs/happstack-hsp-7.1.0/doc/html/happstack-hsp/src/Happstack-Server-HSP-HTML.html). It's type is: template :: (XMLGenerator m, EmbedAsChild m headers, EmbedAsChild m body) =&gt;String -&gt; headers -&gt; body -&gt; m (XMLType m)
Well, in one explicit pass. The second pass exists in the lazy “look up where this will have ended up in the result” version, it’s just spread over the use sites of the result. But of course your solution is just as good and perhaps easier to understand.
The exact approach you mentioned of gathering statistics at runtime doesn't really work for GHC where most information about the code is not present at run time and there is a focus on static analysis of written code over dynamic analysis of running code. It's more a feature of dynamic language implementations, Microsoft's experimental JavaScript compiler did something like this. Byte code based implementations also retain a lot of useful data at runtime and if they are aware of "lazy" vs "strict" you could perhaps implement it at the JIT or VM level. GHC does use different, static approaches though. You may not be able to determine whether the strict or lazy approach is better from actual empirical data, but let's say you have a function like `foldl` where you can statically prove that it always have to consume the whole list. In this context GHC may as a rule of thumb turn it into `foldl'` because stricter versions are generally better when you can statically prove that it has to be strict in the end anyway. This approach will never be able to produce results quite as good as a clever watchdog at runtime that rewrites the program but you don't need anything other than the compiled program running at runtime either, which is a performance boon in itself and (if GHC could cross compile) would also be helpful as far as portability goes by producing a simpler program. TL;DR: there is such a facility, but it makes the decisions while the program is being compiled, not while it is running, so it can't be as smart as you may think it is.
It looks like the opposite to me. When you get a "type variable is ambiguous" it means it's too general and it doesn't know which specific instance to pick.
Even if FORTRAN beat C in every conceivable way then people would still not completely rewrite their old code bases overnight. Backwards compatibility will also become a concern as people start trying to mix C and better-than-C together and that usually means compromises in better-than-C as accommodation.
Your not stuck in string hell. There is no such thing as a string hell. This just a type error which will become more clear to you after gaining a bit more experience.
Disclaimer: I would consider myself a very solid beginner/work-in-progress intermediate programmer in Haskell. 1. I had been programming for about 8 years in various imperative languages, before I picked up a book on FP. A very senior person at work was constantly praising this language called "Haskell". I wanted to know what all the fuss was about, so I picked up the smallest book I could find (Hutton's haskell book). I figured I'd go through it over the weekend, because, what the hell... it can't be that different, can it? Boy, how wrong I was. It took me a couple of weeks just to get through that book, and it didn't even scratch the surface. I have since read LYAH and am in the middle of working through RWH. I have watched and read through tons of tutorials online, and keep tabs on the haskell mailing list at haskell.org. It has been about 2.5 years since I first started FP in Haskell, and it has been a beautiful journey. I have also written one 2D game in haskell, and participated in 2 programming contests at work (placed 5th and 1st respectively, coded from scratch in Haskell for both). 2. My Haskell journey has been a mix of pleasure and pain. The pleasure comes from learning so many new concepts, quite honestly, I feel like I learnt programming all over again from a very different perspective. And it is so much more elegant in haskell, the code I write tends to *just work*, which is a feeling I've gotten from Haskell in general. The pain comes from dealing with the libraries out there (too many libs, too little QC) and the lack of tools or performance analysis know-how. Nowadays, anytime I prototype something, it is always in Haskell. Haskell lets me think at the algorithmic level rather than lower levels. I write very little boilerplate, and like I mentioned, the code usually is far more elegant. The other day I wrote an interesting fold which took me 20 minutes to write... and I felt like it was the most beautiful line of code I've written in a couple of months. 3. I've attended a course on functional programming in C++, which discussed std::accumulate, std::transform, std::function, std::bind, lambda functions in C++, and so on. I even programmed some project euler problems using these. However, they just completely lack the elegance and ease of use that Haskell provides. TMP is another area where knowledge of FP will help you, because TMP bears much resemblance to functional programming. If you want my advice, pick up a book on Haskell. LYAH is a fantastic first book, which presents everything in a very friendly manner. It lacks exercises, but you can program project euler problems for that. You will learn more in 2 months of Haskell, than probably the prior entire year or two of imperative programming. I can't help but feel that Haskell programming is the way programming is actually meant to be done. It is the elegant ivory tower, to the mud hut that is C++. I'm ready for the flaming/cussing now, because discussing programming language superiority is often like talking about religion (I'm agnostic/almost atheistic if that matters)...
Or add parentheses to stop the confusion. I hate how the standard Haskell lint tool is so aggressive in whinging about not-strictly-necessary-for-compiler parentheses.
There certainly is such a thing as string hell. It's an overabundance of stringly-typed interfaces. I agree this isn't it, however. 
I added a working version to the gist -- not sure how to permalink to it, but it is linked to the stepcut name. The primary issue here is that as we generalize code, the type errors become more and more difficult to understand. Choice comes with pain! If you diff the original version and mine you can see the changes needed. But, there were basically three issues: 1. you need to import `HSP.ServerPartT` if you want to use the `XMLGenerator` instances for `ServerPartT`. This is annoying -- but the alternative is to require that you install `happstack-server` even if you just wanted to use `hsx` with `snap`, etc. That is clearly not an acceptable solution either. This is a fatal flaw with Haskell type classes in general. We have a bunch of libraries that create useful types. And we have useful classes as well that those types could inhabit. But where do we put those instances? If we put them in the library that declares the type or the library that declares the class we might create a bunch of dependency hell for people that don't even care about that particular instance. But if we put it in a third library, then forgetting to import the extra module leads to mysterious errors. If anyone has a better idea I would love to hear it! 2. the type for `defaultTemplate` is very general. And many of your uses of it don't make the type specific enough -- so the compiler can't figure out which instance you mean to use. By giving `template` an explicit type signature the ambiguity is cleared up. As Haskell sneaks up on dependent types its ability to infer types decreases. That is an unavoidable fact of life, AFAIK. 3. there were some minor issues with you calling `ok &lt;$&gt; toResponse $ foo` instead of `(ok . toResponse) =&lt;&lt; foo' which became clear once 1 &amp; 2 were resolved. Overall -- this issue highlights a very difficult question for maintainers. Do we opt for optimal flexibility at the price of more complex type errors when the users inevitably make mistakes. Or do we exclude a bunch of potentially useful power in pursuit of simple error messages. With great power comes horrible error messages!
The only `m` that I can find that has all those instances is something like `HSPT XML IO` or possibly even `HSPT XML Identity`. Try annotating your template function with a type signature that fixes the `m` to be one of those like: type MyXMLGen = HSPT XML Identity template :: (EmbedAsChild MyXMLGen h, EmbedAsChild MyXMLGen b) =&gt; String -&gt; h -&gt; b -&gt; MyXMLGen Data.Text.Lazy.Text Either that, or use some operation that effectively fixes that type, like passing the whole do block through `runIdentity . runHSPT`.
And I hate the way it complains about unambiguous tabs. It's relatively easy to drop the rules you don't agree with, though.
The OP doesn't want edit capability at all it seems.
What did you run to uninstall it and rebuild packages that depend on it?
I recommend not doing that &amp;ndash; you'll get overlapping instances everywhere.
For those experts in making transformers, could you take a look at this stack overflow question about [transformer(s) corresponding to a monad](http://stackoverflow.com/q/24515876/2008899). In particular, we'd love a better answer than [the bounty winner](http://stackoverflow.com/a/24844388/2008899) or a more complete proof/disproof of the bounty winner.
Good article --- I just wish all these blog articles were easier to save as PDF documents. They never come out right when I just try to print from the browser. I always have to get the source HTML, throw it into Dreamweaver and delete all the crap
I'm wondering if anyone knows if people have solved the n^2 complexity of sum types that is an issue with both scott and church encodings. For example, the type OneOf a b c d = First a | Second b | Third c | Fourth d Would be encoded as first x a b c d = a x second x a b c d = b x third x a b c d = c x fourth x a b c d = d x Thus, with a sum type of size n, we have total encoding size of n^2, thanks to the repetitive arguments a, b, c, and d. Anyone know if someone has come up with a clever solution to removing this redundancy without explicit data types? 
Except for typeclass instances, right? :P
Have you tried [Readability](https://www.readability.com/) or any of the similar extensions? They usually do a pretty good job. Haven't tried with code though.
I'm not really an expert, but the `append` example in the [wikipedia article about Idris](http://en.wikipedia.org/wiki/Idris_(programming_language%29) is a good one: The type signature guarantees that the resulting Vector has the correct size.
The simplest example is that Haskell's type system can't show that 1+1=2. With a lot of extensions, it can be done, but not without a bunch of hacks.
I wish it was stupidly easy to get a job writing Haskell. Or Idris. I'll even take Scala, Frege, Fay, or Elm. F# or F* in a pinch; but I'm a big fan of UNIX-like systems, and not so much a fan of the MS Windows ecosystem.
The canonical example for me is the type-safe printf in Idris. You can't do it in Haskell even with extensions (apart from template-haskell, but that's cheating).
A "simple" type of object we pass around in dependent types is the dependent pair. This is a peculiar little value at first - it pairs up a value, but lets us use that value *to determine the type of the second value*. Why would we want to do such a thing? It turns out to come up all the time when we're trying to prove things. David Christiansen [recently wrote a Fizz Buzz solution in Idris](https://gist.github.com/david-christiansen/3660d5d45e9287c25a5e), and the dependent pair crops up even is something this simple. One thing David does is "prove" that some of his code is correctly determining whether a number is "fizzy" - that is, whether it's a multiple of three. The type of this function is: fizzyCorrect : (n : Nat) -&gt; Fizzy n -&gt; (k : Nat ** n = 3 * k) What this is saying is that given a natural number `n` and a `Fizzy n` object, we can construct a proof saying that "there exists a number `k` such that k * 3 = n", which is a proof that `n` is a multiple of 3. By doing so, we have a constructive proof that `Fizzy n` does indeed encode the fact that `n` is "fizzy" - a multiple of 3. It's this type of dependence that is very difficult to express in Haskell. We *can* do a lot of it, but it requires a ton of machinery. On the other hand, this is very natural in a language that was designed up front for this - like Idris.
Vector size can be [done in Haskell](http://hackage.haskell.org/package/sized-vector-1.4.1.0/docs/Data-Vector-Sized.html), which I think makes doing it in Idris it less obviously impressive (even if it requires no extensions and just works in all senses) to the casual viewer.
Oh? How clean are we talking about?
[It's actually possible to do it in Haskell](https://www.fpcomplete.com/user/konn/prove-your-haskell-for-great-safety/dependent-types-in-haskell). OP's question is dangerous: some ~~twisted minds~~ playful hackers could take the examples provided here as challenges. My personal guess would be something along the lines of [Induction-Recursion](http://en.wikipedia.org/wiki/Induction-recursion_\(type_theory\)).
Tradition indicates `newtype` is the fix. newtype MsgPackViaJSON a = MsgPackViaJSON { unwrap :: a } instance ToJSON a =&gt; Packable (MsgPackViaJSON a) where ... instance FromJSON a =&gt; Unpackable (MsgPackViaJSON a) where ... You might also turn on newtype deriving and pass-through some other instances.
[This API](http://hackage.haskell.org/package/sized-vector-1.4.1.0/docs/Data-Vector-Sized.html) is surprisingly clean.
NICE --- had no idea about this, will check it out.
Thought I had every extension out there :-) ....will check this one out ... thanks
Here's [the previous discussion about it](http://www.reddit.com/r/haskell/comments/291wel/formatting_typesafe_printflike_library_by/cigswfd?context=3), I think this discussion shows where Haskell falls short. It's a type-system thing rather than a syntactic thing.
Is this related? http://lpaste.net/92328#line24 On line 24 I returned a pair of both the value and the number associated with it, this forces my _implementation_ to be correct, not just the external type to seem correct. I.e. I could pattern match but return the first item in the vector instead of the second, but with the tuple it forces them to match up: the compiler won't let me write a "wrong" implementation (I commented out examples that wouldn't compile). Or is that just something that seems similar and is totally unrelated?
Fair deuce, the API is pretty clean, though the Nat implementation seems like the sort of hack that's too clever for its own good.
unfortunately the msgpack maintainer is quite inactive these days... the package needs to have some of it's dependencies updated :( On top of it, since the goal of msgpack is to be a "binary json", it can use a lot of what's done in Aeson (see e.g. yaml package)
This is an outstanding idea. Laziness is not a good default for most "real-world" programs; Haskell get's away with it because GHC is so good. Adding a Strict Pragma would make it much easier to read fast Haskell code - one could simply sidestep all the headache involved in making sure your program is sufficiently strict. 
A neat way to stretch your mind about this type is to figure out why these are called ("big") sigma types even though sigma means sum and we're clearly talking about a product. But consider the sigma type `Sigma Bool (\b -&gt; p b)` where p is a type function from Bool -&gt; Type and the second component of the Sigma type just says that we take the *value* of the first and use it to determine the second. Now p has to be defined on both True and False so there are two possible ground types it could take. Which means that this type is isomorphic to Either, but where the tag and the value have been separated.
It would be nice if I could apply this per function. I find that usually there is one loop I want to be very strict. 
I really hope to see this implemented in 7.10 :P
&gt; Laziness is not a good default for most "real-world" programs I would strongly disagree with that sentiment, partly because I think appeals to the "real-world" are disingenuous and don't really advance the discussion. What I will say is that while there are some problems with the current implementation of laziness in GHC, I'm not necessarily convinced that they are intractable. The open problems with the implementation, I think, are best solved by better education about lazy evaluation actually works and better Prelude defaults. On the technical side, lazy-by-default makes it much easier to encode *both* evaluation strategies within the same language than the inverse does ( strict-by-default ). It's easy to force a lazy argument, but we can't unforce a strict argument. 
I've realized since running posting this you're right, it's not actually a string issue, it's an ambiguous typing issue.
I use laziness to optimize my programs all the time, like literally :/ much, much more often than strictness.
Thanks for the clear explanation! I tried applying your changes to a simpler example [Gist here](https://gist.github.com/joefiorini/a3d45d0bd762e0616b50), to help me understand what's going on. I get what you're saying in your comment, but I wasn't able to get it working. I'm getting down to a type issue. It looks like the the compiled XML is returning an HSTP XML (ServerPartT IO) (and changing the return type of "template" to that causes a different error). I've tried different combinations of "unHSTP" and "unXMLGenT" to unwrap the ServerPartT IO, but it hasn't helped. It looks like I did exactly the as what you did in your comment, do you have any ideas?
What happens when haddock extracts documentation from a Strict module? Showing all the strictness annotation explicitly? I'd much prefer this kind of translation be done as a pre-processing tool, not as a pragma.
Xelatex is the more modern, better software anyway. ;) But that looks really cool. I assume you can configure how you want the document to appear – e.g. change the default font?
I think you might be right about IR! I've tried to fake it in Haskell a few times, and I just don't think it's possible...
It's important to remember that being able to express non-termination does *not* make your type system inconsistent. There are two ways to do this: 1. A language like Agda or Idris can do it in a very weak and syntactic manner, by providing the syntax of general recursion, and then requiring proof when you try to execute it. But this isn't quite `fix`, so we'll skip this and come back to it shortly. 2. In a system like Nuprl/CTT, the `fix` term or the Y combinator actually has a type! And CTT is believed to be consistent. The typing of this term is very similar to how one would do it in Agda (basically, there is a type of partial functions, which means `the result has this type if it converges`). But the difference is that the underlying terms of Nuprl are unityped, and so the y combinator is expressible, not just as a symbol or a primitive axiom like in System F + fixpoints, but as an actual function built from first principles. Not only do they have these "partial" types in Nuprl, but additionally, you can use fixpoints to construct (unityped) fast general-recursive algorithms, and then prove by induction that they have a total type. The approach which seems to be presently in vogue is to bake the termination proofs directly into the terms for typechecking, and thence for runtime, extract a term which contains an honest-to-god fix in it. Really, the approaches are analogous, but they come at the goal from opposite ends: in one case, you extract a clean general recursive term from a more complicated term which has its termination proof baked in, whereas in the other case, you provide an external justification that the clean general recursive term is the computational content of a particular proposition (in the sense of the realizability interpretation of types). I am told by Conor McBride &amp; others that the intrinsic pre-baked approach already suffices for the use-cases where the extrinsic one has long excelled.
I think the "lack of documentation" is mainly an issue for people not used to Haskell's type system. There exist purists in the Haskell community who are adamant that types are all you need, which is largely true. The issue with documentation is that people coming from dynamic languages expect a *different type* of documentation, focused mainly on extensive example code and tutorials. To "fix" the documentation "issue" is to allow new people to lean on things that are familiar to them (in this case documentation style) to learn something that is unfamiliar to them (Haskell's type system). Whether this type of change should be enacted is up for debate. Personally, I agree that the type system is massively valuable but I also think that it's useful to enable people unfamiliar with the type system.
Laziness is absolutely the correct default. It's what makes code reuse work.
It could work like the INLINE pragma
I don't think it's just a case of people not being used to the type system. The types are a good documentation if you are looking at a specific function and the types themselves are specialised enough (eg. FilePath for file paths). The problem arises when you try and link things together or you're just getting started. Firstly you'll have little idea of what the entry point is. Secondly you may have two functions you want to glue together but they have a slight type mismatch - quite often you may have to convert one thing to another, or there's an extra argument or something. In this case you will have to trawl through the docs for the right function that gives you the type/data you need. This is not only difficult but really inefficient in terms of time! You can speed this up if you know what you're doing but it's still a lot more work than reading a well written tutorial
&gt; There exist purists in the Haskell community who are adamant that types are all you need, which is largely true. It's true in some cases. You can understand ``Data.Traversable`` pretty well from just types alone. On the other hand ``Network.Socket`` is completely opaque if you just look at the types. Personally, I think types mean that we have to write a heck of lot less documentation but they don't mean that Haskell code is automatically self-documenting. It's still *very very very important* ( can't stress this enough ) to provide the canonical simple example that indicates the entry points to the library and basic usage at bare minimum.
Ah, the reason I skipped this one is because it uses somebody else's website to do the work --- works well though
This will be abused, and we'll still get questions about performance because rampant strictness is at least as bad as rampant laziness, given Haskell's syntax. Still, as long as the issues raised in the proposal can be addressed (even if that just means documenting you-can't-do-that), it doesn't sound like a bad feature.
We don't include strictness annotations in Haddock, except on data types, so we won't include those implicitly generated by the pragma either.
Or, you could just use bang patterns. :P
I don't think there's anything stopping us from adding such a pragma.
I don't like that, because then if, during refactoring, I move code aroudn in the same file, it changes meaning. It's going to bad enough when code changes meaning when I move it out of my *.Internal module (or something like that).
I.... don't think I can really agree with this. I think types are nice, but they are definitely not sufficient documentation. Looking at the types of individual functions might be helpful, but piecing together how to use a library overall (one important goal of documentation) and how to deal with the pitfalls you might encounter and the recommended philosophy and approach and design principles cannot always be inferred from the types. And if they were, it would take a lot of work. and this is even assuming that you could infer what a function does from its type, even if you understood the type. Sometimes (read: many times) the Haskell type system isn't even strong enough to allow you to do that! For an example from Prelude, compare the type signatures for `all` and `any`. Same signature, opposite things. In real world libraries, a lot of times the types simply aren't strong enough to give a picture of what the function is even supposed to do. types are good documentation, but they are definitely not sufficient, and do not make the need for good documentation magically go away. 
&gt; types mean that we have to write a heck of lot less documentation but they don't mean that Haskell code is automatically self-documenting I see types (at least in part) as documentation that the compiler can understand. There is a difference between `Double -&gt; Double -&gt; Double` and `Distance -&gt; Duration -&gt; Speed`, even if all those types are defined as newtypes over `Double`, and this kind of technique is the preferred way of documenting Haskell code. In a perfect world, the type system would be powerful enough to express anything you could possibly want to document, but we live in the real world, so we have to resort to less rigid forms of documentation to fill the gaps. The same goes for any language, really; it's just that Haskell's type system is more expressive than average, so a good programmer can document better with it than with that of most other languages.
&gt;Once you spend some time with Haskell, this situation turns out to be pretty nice... Right, but main problem is here that you must spend much time than for other languages 
Yes, data types is a good documentation, but it's good only for data. It doesnt tell you how to use it, in what cases and etc...
If the browser is out of your trust scope, it is not suitable for carrying communication to your end user, and you should stop using the internet. You have lost before you started. The chain of trust is: I trust the server that initiated this TLS session is the right one -- and one that I trust, I trust that no one can alter the bits being sent over TLS, I trust that my operating system is delivering the right bits to my browser, I trust that my browser is rendering html+css+js as expected, I trust that my OS is sending pixels to my screen, keys and mouseclicks appropriately to/from my input devices. If any of those things are broken, you cannot trust a web application. I'm not advocating for an increase in attack surface, and in fact, that's the whole point. If anything, pushing more work to the client reduces attack surface, because your server no longer needs to be a complex amalgamation of UI code with security conscious API code -- you can focus solely on the latter. 
&gt;On the abstractions, you do &gt;&gt; main = do &gt; getLine &gt;&gt;= putStrLn &gt;The `do` here is not needed. Instead: &gt;&gt; main = getLine &gt;&gt;= putStrLn Yes, we no neef 'do here', will fix this &gt;I'm also not sure it's equivalent to the python example and it's definitely not equivalent to the `echo` command. I didnt mean echo as unix command, i just meant it in getting echo of input &gt;Perhaps as an example you should use `cat` instead... Yes, maybe, but i wantef to show the simplest example &gt;Which should also better illustrate your point as that's a pretty different approach to the program. Of course a real `cat` would handle errors, but adding that in would just make the example too complex. 
I'm sorry, but I can't agree with the "types are all you need" statement. Not at all. f :: [a] -&gt; [a] -&gt; [a] This can do way too much for the types to tell me anything about the behavior. f could be `(++)`, it could be `tail`, it could be a specialized form of `intersectBy`. It could also just concat the first 2 elements of each list. If we specialize `[a]` to `[Int]`, then this could perform any arithmetic function between the lists in an element by element fashion. It could also add them pairwise. There is a fair amount of ambiguity in just type signatures, especially as they get more specific (e.g. `a` to `Int`). I realize that the name is going to be a big part of the signature, but not all names are that descriptive, and operators are going to make things even more confusing. There needs to be examples and english language documentation as well as type signatures.
While I like the idea of getting more / simpler control about strictness, I generally dislike adding features on a per-module basis as long as module == file.
Polymorphic strict bindings should probably be introduced even if `Strict` isn't used.
Neato. What happens if I write this? &gt; let x = x in x Presumably it will say "recursive patterns are not allowed in strict mode" or so?
Haskell is for programmers that love to think (about Haskell things like computations, transitivity, tail recursion, ...), you'll find one "think lover" (about that) on a thousand programmers (and I'm generous). Compare golang learning curve with Haskell learning curve... ridiculous. This is the main reason (my opinion). Others like lack of documentation only become worse the situation but (I think) in general if you are willing, it's only a discomfort.
tail recursion is a "haskell thing"? :)
I think the pragma would work using the function name {-# STRICT function_name #-} You would get a warning/error if the pragma had no matching function.
It's easy to write that example more like Ruby/Python using do notation. main = do data &lt;- getLine putStrLn data
Yes. We discovered that we could do this while thinking about how to implement this pragma, but they're really separate issues.
I'm not necessarily talking about moving the whole function, but lifting a binding out and putting it somewhere else. It could be a case, where, or let and if moved to another function even in the same file the semantics change.
Right now the wording is "Recursive bang-pattern bindings aren't allowed", so it would probably change to "Recursive bindings require tilde-patterns in strict mode".
Could we split this in `StrictFunctions` and `StrictData`? My guess is that would in most cases be sufficient.
Sounds interesting. Look forward to updates.
The funny thing about dependent pair is that you can use it to implement full haskell-like data types using only simple sum types (or enum, that is data Foo = A | B | C, without arguments) by making the type of the right part (with the "content" of the variant), depends on the left part, containing the tag. This insight was for me the "click" that allows me to understand dependently typed languages.
One of the things that has frustrated me is poor integration with 3rd party libraries. For example, I need to issue SQL to a MySql database. After installing mysql-simple, making sure everything compiles and links ok, ghc promptly bombs with SIGSEGV. I found similar issues to GUI toolkits and the like. Any time you need FFI to some library, look out!
Sounds like a valuable community project to me :) I'm certainly in favour of getting more people to use Haskell. We did have some examples to illustrate early library code (Prelude.*), though most of the definitions were well known in the community, so didn't really need examples. Now there are many more libraries and hopefully a broader user base. 
What about top level variable (not function) bindings? Currently they're not allowed to be strict.
O Christopher Done author of the amazing [formatting](http://hackage.haskell.org/package/formatting) library praise not type-safe `printf` for the Holey Monoid is far grander. (Disclaimer: I've never seen Idris's type-safe `printf`. I just love the Holey Monoid :)
I added a working gist. The essential gist is: diff -u /tmp/Fo.hs /tmp/F2.hs --- /tmp/Fo.hs 2014-07-23 16:47:53.829818303 -0500 +++ /tmp/F2.hs 2014-07-23 16:55:02.005804484 -0500 @@ -1,4 +1,4 @@ -{-# LANGUAGE FlexibleContexts, OverloadedStrings #-} +{-# LANGUAGE FlexibleContexts #-} {-# OPTIONS_GHC -F -pgmFhsx2hs #-} import HSP @@ -19,8 +19,8 @@ main :: IO () main = simpleHTTP nullConf $ hello -template :: (EmbedAsChild (ServerPartT IO) body) =&gt; body -&gt; ServerPartT IO XML -template body = defaultTemplate "" () body +template :: (EmbedAsChild (HSPT XML (ServerPartT IO)) body) =&gt; body -&gt; ServerPartT IO XML +template body = unHSPT $ (defaultTemplate empty () body :: HSPT XML (ServerPartT IO) XML) hello :: ServerPart Response hello = ok . toResponse =&lt;&lt; template I removed the `OverloadedStrings` option because it makes the string literals like `&lt;% "this." %&gt;` ambiguous. To enable `OverloadedStrings` you would need to annotate the literals with a type like `&lt;% "this." :: Text %&gt;` or `&lt;% "this." :: String %&gt;`. It would be nice if there was a PRAGMA like `StringLiterals=Text` that would give the literals a specific type instead of relying on context to infer the type. There are three changes to `template`: 1. I changed the `EmbedAsChild` instance to be `HSPT XML (ServerPartT IO)` instead of just `ServerPartT IO`. 2. I called `unHSPT` to unwrap the `HSPT` monad. 3. The call to `unHSPT` caused the compiler to lose some important type information -- namely the `XML` type in `HSPT XML`. So I had to add an explicit type signature to the call to `defaultTemplate` so that the compiler could know the correct type. Trying to keep code general tends to result in ugly type errors and requires you to give the compiler hints various places. So it is often times better to just nail down the types: template :: XMLGenT (HSPT XML (ServerPartT IO)) XML -&gt; ServerPartT IO XML template body = unHSPT $ defaultTemplate empty () body That version removes the class constraint entirely and so it no longer needs the extra type annotations. Also, it seems unlikely that we are going to do anything with the XML aside from turn it into a response -- so we can clean up the code a bit by moving the `toResponse` call into `template`: template :: XMLGenT (HSPT XML (ServerPartT IO)) XML -&gt; ServerPartT IO Response template body = toResponse &lt;$&gt; (unHSPT $ defaultTemplate empty () body) One design method to use is to start your app by putting together your monad transformer stack like: type MyApp = HSPT XML (ServerPartT IO) And then implementing the `template` function for that specific type. Then `hello` would have the type: hello :: MyApp Response This also makes it easier to monkey around with your transformer stack later. Though, ultimately, transformers stacks are annoying -- which is why things like the `Idris` `Effects` library looks appealing. 
If you want your blog to be read you shouldn't go for fixed-width fonts! Other than that I think that the list is well ordered. I strongly agree that the lack of good documentation and the lack of batteries included are the main problems with the language, which is funny because they aren't *language problems* but rather *ecosystem problems*. The language is actually well designed, it's not that hard to learn and it just works, which is fine if all you want to do is FizzBuzz or the 99 Haskell problems. But if you want to learn your way into querying a RESTful webservice that gives you some JSON data you want to crunch you will have a hard time, only to discover that you ended up using libraries that can't do HTTPS or UTF8.
Is there a sensible semantics for this? What happens if they throw an exception or don't terminate?
There's a `default-extensions` property of Cabal build targets, so you can also use it as a per-project feature.
&gt; haven't been shown a compelling example of a codebase that would be significantly simpler if Haskell were strict by default. It's not going to be significant, because with bang patterns (or tilde patterns, for explicit laziness) it's trivial to depart from the default.
I have a tangentially related question: is there a principled way to reason about the number of inhabitants of a polymorphic function? The simplest examples I can think of are f :: forall a. a -&gt; a -&gt; a and g :: forall a. a -&gt; a -&gt; a -&gt; -&gt; a which have 2 and 3 inhabitants respectively. A more complicated example is h :: forall a b c. (b -&gt; b -&gt; c) -&gt; (a -&gt; b) -&gt; (a -&gt; a -&gt; c) (with thanks to Tony Morris - this is from his workshop at the Brisbane Lambda Jam) The nearest I've been able to figure is that for each universal there's a sort of abstract dimension that gets introduced, and that n parameters of the same type in the negative positions are treated like nth complex roots of unity. There's something puzzling and kind of compelling about all of this, and I'd really like to understand it better. Is this something that is well understood? Does anyone have any pointers on where I could find out more?
&gt; On the technical side, lazy-by-default makes it much easier to encode both evaluation strategies within the same language than the inverse does ( strict-by-default ). It's easy to force a lazy argument, but we can't unforce a strict argument. This is quite debatable. Here's quite an evident way of encoding a lazy version of `Int` in a strict-by-default setup: `() -&gt; Int`. I find it way more reasonable and consistent with the semantics of the language than bangs all around. Mix this with a contextual memoization and you'll get a full reproduction of thunks with a benefit of being more explicit about what they really are, thus granting the user with a better understanding of the program's performance.
that's just nonsense. have you looked at wreq? even good old http-client is not that bad.
Are bang patterns really that bad? They're about the lightest-weight notation for adding an effect that I can imagine. And what is a "strict function"? It would be interesting to see some real code that would benefit from this. Maybe the real issue is that strictness seems to be the one way to communicate a host of different things to the compiler. Sorry to be an armchair critic. 
i think haskell has a quality of being at once beautifully simple and bizarrely enigmatic - to a longtime imperative programmer, at least. after a couple days of tinkering, i feel like i both know the language, but somehow cannot mentally trace out how to accomplish anything beyond the very trivial. my mind is fiercely imperative. this will pass, no doubt. cabal is cranky and doesn't handle versioning terribly well; the "platform" distro for windows is quite old, and it's cabal doesn't support sandboxes; if you skip the platform and grab the latest binaries, you wind up in a hit-or-miss situation where your build environment won't match what ghc/package authors expect and likely symbols won't link. (maybe just updating cabal and sticking with old ghc would help here, but i'm not yet very familiar with how dependcies are all strung together in the tooling to say) i wish there was more middle-ground stuff. maybe there is. but i feel like there's basically a lot of material on a) "here's how you print a string, and look guys, GADTs are sweet!" and c) superhuman cateogry theory-badassery. hard to locate any level-b) stuff. practical, small project based tutorials. how to solve realistic but basic software-based problems in haskell. (that said, i just found Jekor's youtube series on developing a haskell application, this might be just what i need.)
&gt; a use of a + b is (a -&gt; r) * (b -&gt; r) that said, it is like equality r^(a+b) == r^a * r^b which is quite obvious. But the other one &gt; a use of a * b is (a -&gt; r) + (b -&gt; r) seems to be wrong in arithmetics: r^(a*b) == r^a + r^b
Why isn't this the difference between Functor and Applicative? Using parsing as an instance of Applicative, don't I have to parse and consume the first chunk of the input before I can move on to the second chunk? (dependent) Otherwise I don't know how much of the input was consumed by the first operation. Whereas with Functor, I could fmap over all of the things at the same time (parallel).
...so many monad/monad transformer/lens/etc. tutorial, but lack of the library documentation 
I'm failing to see how anything you said competes with what I said and as a result, in what way you disagree.
Ah. Sorry. It was with your opening: the "lack of documentation" is an issue not only with people not used to Haskell's type system --- it's also an issue for everyone, familiar or not.
yea, that's why I said it was *mainly* an issue for newcomers, considering that the context is "why Haskell doesn't have more adoption". I didn't say it was *only* a problem for people not used to the type system. I feel like most people in this thread skipped over what I actually wrote and went straight for "Just Type Signatures Aren't Good Enough!!". I didn't say "types are all you need"; I said types convey a lot of information and newcomers don't necessarily have the ability to access it. They also expect different forms of documentation because of their "upbringing", which is beneficial to them if provided.
Oh! Thanks for pointing that out. I was blind to the fact that it could be misinterpreted... knowing where I was going.
Yeah, there's something really subtle here mentioned in one of the footnotes: if you have a product `(a, b)` then you ought to be able to eliminate that product by using both `a` and `b` together. In other words, you ought to be able to produce a dependent goal by consuming `a` first and then `b`. Which is just the adjunction: (a, b) -&gt; r == a -&gt; (b -&gt; r) This also gives you your algebra r^(a*b) == (r^b)^a == (r^a)^b I really wish I knew how to better talk about this distinction—I find my current approach lacking. I find mentioning duality really important, but I don't want to bring in a lot of mechanics. (Edit: I edited the post to try to be a little less bombastic and leave more room for this issue. I also edited the footnote to directly mention the adjunction)
Add four extra spaces to the beginning of each code line if you want proper code formatting.
I bet there's some kind of ghc api or separate library for type parsing and comparison
I noticed that with just the primitives provided (1, 0, +, x, -&gt;), I can only construct a type with finitely many inhabitants. What do we need to add to reach outside the land of the finite? I looked up the definitions of Mu and Nu... newtype Mu f = Mu (forall a . (f a -&gt; a) -&gt; a), data Nu f = forall a . Nu a (a-&gt;f a), but I can't shake the feeling that there's an easier way to make it happen.
I think, though, that the problem with the documentation runs much deeper than just mismatch between expectation in reality. I can't really see that "*The* issue with documentation" is this mismatch. I believe that the issue is much deeper than this mismatch --- that it's just bad in general, even *if* users come in, used to "this style" of documentation.
`Mu` and `Nu` are exactly the way you do it, but their types are a little weird. A slightly easier one to grasp is called `Fix` newtype Fix f = Fix { unfix :: f (Fix f) } Fix operates over a "signature" functor, `f`. So we want to think about how data types and signature functors relate. Here's `List` data List a = Nil | Cons a (List a) data Listf a x = Nilf | Consf a x so basically you replace recursive self-references to the type with a new type variable. Now, a zero-element list is the type Listf a Void since the only constructor you can use is `Nil`. A one-or-fewer-element list is Listf a (Listf a Void) since you can use `Nil` and `Cons`. A two-or-fewer element list is Listf a (Listf a (Listf a Void)) -- 2 or fewer Listf a (Listf a (Listf a (Listf a Void))) -- 3 or fewer ... Clearly to get the lists we know and love we'd like to just plug an infinite number of layers of `Listf` together. And that's exactly what `Fix (Listf a)` is Fix (Listf a) ~ Listf a (Fix (Listf a)) ~ Listf a (Listf a (Fix (Listf a))) And we can eventually show that these three are all equal: List a Fix (Listf a) Mu (Listf a) Nu (Listf a) though the latter two are a little more difficult to grasp.
You said: &gt; To "fix" the documentation "issue" is to allow new people to lean on things that are familiar to them (in this case documentation style) to learn something that is unfamiliar to them (Haskell's type system). The phrase "lean on" implies that fixing it is the same as supplying a crutch. &gt; Whether this type of change should be enacted is up for debate. Personally, I agree that the type system is massively valuable but I also think that it's useful to enable people unfamiliar with the type system. Enabling people unfamiliar to a system is the entire point of documentation. I fail to understand how there can be a debate over whether that is a good idea.
While it is probably a lot more work than even the proposed default instances, it would be nice to have co-derivable instances. For example, if a type has an instance for Foldable from recursion schemes, it can be given an instance of Foldable from base, and vice-versa. I think that's what we want in this case, really. If you can serialize to JSON you should be able to serialize to MsgPack *and vice-versa* same for deserialize.
Dopes will be dopes. This is for non-dopes! Plus, flags can probably be made that will detect problems with over-strictness. E.g. some simple termination checks, positivity warnings, etc.
I think this is sort of like an -fdefer-type-errors deal. Not something you'd use regularly but neat whenever you for some reason need it and definitely a good selling point for GHC.
I recognise your name and get the feeling you should know this stuff better than I do, but have you read this [blog post on why `() -&gt; Int` isn't a good idea](http://augustss.blogspot.com.br/2011/05/more-points-for-lazy-evaluation-in.html)?
I'm not 100% sure it has the answer you're looking for, but you may want to look into the literature on parametricity, which is a key property in your claim. For example, in Java, it's not totally clear that there are only two inhabitants of `forall a. a -&gt; a -&gt; a`: one could, for example, use introspection to check whether `a` is `Int` and then behave as `(+)`, and behave as `const` otherwise, which is a very different behavior than the two you probably had in mind. I'm not totally sure whether anybody has studied the connection between enumeration and parametricity. Now that I say the word "enumeration", it occurs to me that this sounds very similar to stuff Brent Yorgey has been studying connecting insights from combinatorics to more familiar programming languages concepts. So you might look into some of his papers, too; e.g. "Species and functors and types, oh my!" is probably a good starting point. But no promises that you find something specifically solving the problem you ask. If you don't find a paper... perhaps you could write something up yourself and publish it. ;-)
&gt; And we can eventually show that these three are all equal: &gt; &gt; List a &gt; Fix (Listf a) &gt; Mu (Listf a) &gt; Nu (Listf a) Depends on what you mean by equal. They are all isomorphic in Haskell, ignoring bottom. Pretty sure if Hask wasn't a CCC, Mu and Nu need not even be isomorphic. *mumble mumble* colimits and limits, initial and final *mumble mumble* expression problem.
Yep, they're hand-grenade "equal".
Indeed, when people say "laziness is a bad default", what they often really mean is something like: * I find it less surprising when my code is slow because it's too strict. * I have accepted the need to be explicit about laziness because it's always been required in my past experience. * Because the need for laziness has always forced me to write entangled spaghetti such as mixing unrelated computations into the same loops, I've accepted the need, and no longer even see the alternative.
There's some good work linked from [here](http://stackoverflow.com/questions/9190352/abusing-the-algebra-of-algebraic-data-types-why-does-this-work) as well. I had the feeling that either something by Conor McBride or by Brent Yorgey would have some answers. I guess I should keep clicking and reading. I'm not sure I'll look too far beyond those materials though - if there turns out to be a topological interpretation I'm not sure I'm adequately prepared to dive in :)
I'm in full support of this feature. While 90% of the time BangPatterns are perfect for this, there is still that 10% when I can't help but feel that strictness by default along with ~ to annotate laziness would be cleaner
I have thoroughly enjoyed this as well as your previous posts. Keep it up!
what is it about Haskell's syntax in particular that will make it slow compared to other strict by default languages?
&gt; rampant strictness is at least as bad as rampant laziness, given Haskell's syntax I think many will disagree with you on this one. The problem with laziness is not that it can sometimes give bad performance. Rather, it's that reasoning about *why* a program exhibits bad performance is often more difficult.
Indeed there's a huge opportunity at the moment for companies willing to let Haskell people do Haskell. Candidate quality is much higher (and no one else is hiring them), the code will be more robust and you'll need a much smaller team. Its all win as far as I can see!
Whilst the benefits of laziness for compositionality are well known (see Augustsson or talk to Conal), it is a common misconception that strictness may be added easily to a lazy language. In fact, Haskell does not support this behavior in any coherent way. Let me also note that the proponents of laziness are always arguing from the POV of compositionality &amp; (as Ed Kmett is so fond of saying) "termination in my lifetime". However, with laziness, there is no way to reason by induction on terms. (At this point, you are probably thinking of linking me to "Fast and Loose Reasoning is Morally Correct"—please stop your copy-paste-o-matics and read the rest of what I have to say.) Now, in case you have come to think that by application of bangs in data type definitions and functional patterns that you have recovered the ability to reason by induction, think again! Consider the following: data LazyNat = Z | S LazyNat So we've defined a pretty horrifying data type: in addition to the natural numbers, it contains a countably infinite number of exotic elements. For each successor, there is an additional bottom lurking (`S _|_`, `S (S _|_)`, etc.). There is a least element as well (undefined), and a greatest element (`fix S`). We can solve some of this by making the `S` constructor strict: data StrictNat = Z' | S' !StrictNat Now, this is much better: there is only one exotic closed term of type `StrictNat` (undefined). This extra term is still, of course, enough to scuttle any hopes of reasoning by induction. But it gets worse! There is actually no way to know whether a function is strict in its arguments from its type, since bang patterns go, well, in the patterns. But let us ignore that deficiency presently. Consider: damn :: LazyNat -&gt; String damn !n = anotherFunction n At first glance, one would expect that a term like `damn (S undefined)` would properly diverge. This is precisely what the principle of mathematical induction would have you think hope for! That is to say, we would hope to be able to characterize the observable behavior of the open term `anotherFunction n` entirely on the basis of which canonical natural numbers `n` could possibly refer to. But instead, we won't diverge at the call-site of `damn` but rather somewhere else deep and inscrutable. This is because strictness only means that bottoms are preserved. It doesn't mean you can't hide a hot potato somewhere deep in a structure, and chuckle to yourself when the world burns down in a deeper execution path, or even worse, another thread. It gets weirder: class Damn a where damn :: a -&gt; String instance Damn LazyNat where damn !x = "asdf" instance Damn StrictNat where damn !x = "asdf" (Just as an aside, what if I had chosen to make one instance of `Damn` lazy in its pattern? How would you know I had done this?) OK, so the messed up thing here is the following. What would you expect the result of `damn (fix S)` (`S` is the lazy constructor here) to be? Well, since `fix S` is a diverging term and `damn` is strict in its argument, it seems like it `damn (fix S)` should diverge. Think again! This does not diverge in fact. (Note that `damn (fix S')` does nicely diverge however). Even `seq` doesn't suffice to restore the principle of mathematical induction, since there is no way to say "evaluate this thing to normal form"; all `seq` does is take you to weak head normal form. Basically, it normalizes to the closest constructor and then stops. There are a number of *ad hoc* approaches to reducing data to canonical form, but these are not general: they must be defined for every type. Does all this matter? If you just care about performance, then Haskell with strictness annotations (and careful profiling to see where you *actually* need them, and where you don't) is perfectly up to the task. People who say otherwise are very likely spreading FUD. But the importance of strictness has little to do with performance, and *everything* to do with predictable behavior in the presence of partiality and other effects (which Haskell has in droves, whatever you like to pretend)—behavior which is amenable to the strongest tool that we have, which is mathematical induction. Induction is about knowing who to blame when things go wrong: induction is about being able to say, "I know this much: if it gets to this point, it won't fail here at the very least". It's a real shame to trade this for other kinds of expediency.
I don't really understand this line: &gt; Products are constructed from values of both their left type and their right type together and used when there is a use for either values of their left type or their right type. Is this getting at the fact that both a and b are always defined, so if you always need to have the choice of which to use, you have to use a product type? If that's the intuition, then I don't think "either" and "both" have the right meaning to convey that. It seems like "choice" might offer a better intuition than "use"? Later, in the section that is supposed to clarify this issue, you say a "use" of a is a function a -&gt; r. So a "use" of a * b is a function a * b -&gt; r. How then does that imply that a use of a * b -&gt; r is equivalent to (a -&gt; r) + (b -&gt; r)? For example, if a is integers, and b is imaginary integers, and r is complex numbers, are you saying that the function a * b -&gt; r can be represented as either a function that just takes the integer and returns a complex number or the function that takes an imaginary integer and returns a complex number? That doesn't seem to make any sense. Or perhaps I'm misreading the section and a * b -&gt; r is not a use of a * b, and it only counts as a use if it is a * b -&gt; (a -&gt; r) + (b -&gt; r), but there doesn't seem to be any explanation for why that would be the case. I suppose it is intended that this is captured by footnote 4, but that doesn't really clarify it in my mind. If a can take on 10 values, and b can take on 10 values, why can I only get 20 values out of a use of it (a -&gt; r) + (b -&gt; r) rather than 100 values?
Oh...I mean that tail recursion isn't a worthwhile thing to worry about in Haskell. You shouldn't really be thinking about it at all when you program Haskell, for the most part.
You can't combine two `Functor`s.
Haskell *detect* tail recursion but can't achieve it and the programmer must to think about it to get better performance. http://stackoverflow.com/search?q=tail-recursion+haskell Or a specific example http://stackoverflow.com/questions/3429634/foldl-is-tail-recursive-so-how-come-foldr-runs-faster-than-foldl
cf [counting linear lambda terms](https://byorgey.wordpress.com/2011/01/26/counting-linear-lambda-terms/) and [redux](https://byorgey.wordpress.com/2011/02/08/counting-linear-lambda-terms-choice-and-correspondence/)
the link you posted had nothing to do with tail recursion though :( it was only with laziness/strictness...in fact, it stated that tail recursion was irrelevant.
&gt; To allow local instances you must give up unique resolution. You have to be careful about what is meant by "local instances", and where they can be declared (is it module level or value level). Your "somehow unify A.T and B.T" is the crux of the issue. In a Backpack-style design, a "shaping pass" is run prior to type checking which determines, a priori, whether or not A.T unifies with B.T; it's not a property that the source language can change after the fact. If you can say "with instance blah" inside an expression, the situation is different, of course.
really? ("The performance gain came from the tail recursive implementation") try with this http://stackoverflow.com/questions/21205213/haskell-tail-recursion-version-of-depth-of-binary-tree Anyway looks tail recursion is relevant in Haskell... ;P Google &lt;&lt; "haskell" "tail recursion" / "haskell" := 30k / 5M = 6%o "java" "tail recursion" / "java" := 62k / 505M = 0.002%o (to avoid non programming sources) "haskell" "programming" "tail recursion" / "haskell" "programming" := 26k / 3.4M = 8%o "java" "programming" "tail recursion" / "java" "programming" := 13k / 21.6M = 0.6%o 
The latter are intended to be for flow control -- they don't generally carry data. Historically, there's not much rhyme or reason for the way `base` is arranged. Just go with it; it's better than Java.
&gt; they don't generally carry data. Every `Applicative` is a `Functor`, so if `Applicative` doesn't generally carry data, then `Functor` should follow? or maybe I got it wrong?
I can really relate to your standard-library argument about `base` lacking the "batteries included" features. Every time I try hacking up something in Haskell instead of Ruby or Perl I need to reinvent all those small standard operations I have at my fingertips (i.e. without ahving to remember in which packages and/or modules such basic things a hiding) in other languages, ever tried - replacing a sub-string, - splitting a string on e.g. "`,`" into at most 4 strings, - pad a string to n characters (left/center/right aligned), - print a number in octal or hex base (optionally `0` padded), just to name a few things I believe a standard library should provide support for out-of-the-box without having to jump through hoops. ---- Minor nitpick though as to one of your examples: I'd translate import sys data = sys.stdin.readlines() print data rather to import Control.Applicative main = do data &lt;- lines &lt;$&gt; getContents print data 
How did you discover wreq back when you were a newbie?
Is it possible to define a manifold of types? It seems like it should be.
Sounds really syntactically noisy though.
That seems even worse, as then when you look at a `.hs` file you wouldn't even know its semantics are completely different.
Could you make the element of the list be a pair of the value and its index in the list? Hmm I guess then each element of the list would be a different type? 
That doesn't seem to be true though, since f :: forall a. a -&gt; a -&gt; a has 2 inhabitants, regardless of what `a` is. The inhabitants are f x _ = x and f _ y = y (or at least, it has 2 inhabitants which are functions that terminate) My understanding is that the `forall a` effectively means "regardless of what `a` is". I don't think you can get 16 different version of f just by using it with Bool, for instance.
The Yoneda lemma is your friend here: (∀ x . (a → x) → f x) = f a for any (covariant) functor `f`. In particular, for your simple examples: ∀ a . a → a → a = ∀ a . a × a → a = ∀ a . (2 → a) → a = 2 where the last step is just an application of Yoneda. The more complicated example is also quite easily tackled: ∀ a b c . (b → b → c) → (a → b) → (a → a → c) = ∀ a b c . (b × b → c) → (a → b) → (a → a → c) = ∀ a b . (a → b) → (a → a → b × b) = ∀ a . (a → a → a × a) = ∀ a . (2 → a) → a × a = 2 × 2 = 4 where we used Yoneda three times, once for every type variable. 
It sort of is like that: `Cons :: a -&gt; n -&gt; List a n -&gt; List a (S n)` Each cons in the list is the value and its index. Not enough, sadly.
That is pure awesome - thank you so much.
I'm not sure what you're saying here. The post seems to state that (a,b) -&gt; r == (a -&gt; r) + (b -&gt; r) which is of course not true. The duality between sums and products works another way: one is a colimit and the other a limit. The universal property of the limit gives you: s -&gt; (a,b) == (s -&gt; a, s -&gt; b) which I believe is the correct dual of (a + b) -&gt; r == (a -&gt; r, b -&gt; r)
The fact that "take", or "map" or "zip" are defined for any list and works for infinite lists is a good exemple of how non-strictness enhance expressivity.
Thanks, this post greatly helped in understanding the intuition behind the Lens type. Waiting for the rest of the series!
That's an interesting question. `StrictData` is usually more important than strict functions. Worth experimenting with.
Lazy streams are indeed useful, but you can of course have them in a strict language too. The question is what dominates and which default gives you fewer bugs.
Maybe its too late now to put them in the same namespace. those general typeclass are generally in either data or control. if i can't find it in one i try the other. *oogling helps here. according to a bit of github investigating i did, idris puts functor into a namespace called Prelude.Functor and agda has a toplevel namespace Functor.
I'm pretty sure the author screwed up and went too far with the dualities and generalizations. Okay, maybe not screwed up, but went with the minimal requirements for implementing a function (in that, when you have a tuple (a,b) you could use just a or just b, but forgot to mention that you could, as you point out, use both). So really, `a * b -&gt; r` would be equal to, uhhh, `(a -&gt; r) + (b -&gt; r) + ((a*b) -&gt; r)` which sounds a little bit redundant, recursive and silly. I believe there is a way to not use `*` on both sides, but it hasn't come to me yet. EDIT: Cf. this post above: http://www.reddit.com/r/haskell/comments/2bj7it/let_me_tell_you_about_the_types_of_data/cj6cgga
The canonical Idris example is stolen from Cayenne. :)
latter ("(avoid popularity) (at all costs)", "avoid (popularity at all costs)") = "avoid (popularity at all costs)" Did you mean *former*?
Is your tone born out of frustration? Frustration is what drives newbies away. Too bad it will not get heard. (The second half of your post raises some very valid points.)
&gt;you will surely agree Yes.
&gt; On the technical side, lazy-by-default makes it much easier to encode both evaluation strategies within the same language than the inverse does ( strict-by-default ). It's easy to force a lazy argument, but we can't unforce a strict argument. On the contrary. In a lazy language you can never know whether a function evaluates its argument strictly or not. However in a strict language you can have an explicit thunk datatype in whose absence you know an argument is evaluated strictly.
&gt; One branch of logic, intuitionistic logic, tries to cope without the ability to prove without construction. Thus, in intuitionistic logic the structure of data is better preserved… but one must also do without ex falso quodlibet which can be painful. Inituitionistic logic is fine with ex falso quadlibet. You must mean the law of the excluded middle (tertium non datur, if we're sticking to latin). Nice piece otherwise!
I did screw up and indicated a tightness that isn't true. The other tight equality would be `a * b -&gt; r == a -&gt; (b -&gt; r)`. I'm going to back through and be more accurate indicating that you *can* use a pair by having a sum of uses instead of must. The thing I was really getting at comes from natural deduction where there are two rules for eliminating a product and two rules for introducing a sum a x b a x b a b ----- ----- ----- ----- a b a + b a + b but trying to describe this with functions swept actual details under the rug and lead to inaccuracies. In particular, while you can translate those bars into a function they are not the only things of that shape which can be translated into a function. For instance a x b ------------- contract/duplicate a x b a x b ----- ----- use left | use right a b ------------- construct product a x b uses contraction to show how we are able to use `a x b` twice.
If you set up your .ghci file and set the $EDITOR variable to vim, you could just: `&gt;`load up your file in ghci `&gt;`play around `&gt;`type `:e` which will automatically open the file in vim `&gt;`change stuff as you wish `&gt;` `ZZ` out from vim into ghci and continue playing around Might want to enable persistent undo in vim, though.
Yes, [Cayenne](http://en.wikipedia.org/wiki/Cayenne_(programming_language\)), written by some clever chap.
How is `fix S` a diverging term? For `S'` I buy it.
In Haskell2010, orphan instances are too valuable to kill. In some cases, they are the only reasonable way to adapt data from one package to the interface of another package in your new application. Using a newtype wrapper without GeneralizedNewtypeDeriving can be quite painful, assuming it is always possible. With GeneralizedNewtypeDeriving... I'm still not sure how I feel about needing to define SubTypes, (Co)Arbitrary, and (Co)Serial instances right next to each of my data types just so I can run "internal" tests.
I feel like strict tuples should be in base even if `Strict` never arrives.
In Haskell the line between data and control is pretty blurry. Incredibly blurry actually due to laziness. I think the Control/Data division is kind of a painful historical accident. Nobody suspected just how blurry it would get.
But fold isn't defined on every lists, whereas in a strict language it is. Moreover, explicitly lazy streams are easy to write in strict languages too.
I've slightly clarified the wording. Do you think it helped?
This paper is labelled as "Version 8" in a footnote, and I'm wondering if he has published these ideas before of if this is just "revision 8" of this particular paper. He has certainly given talks on these ideas for a number of years, e.g. his Google Tech Talk "Faith, Evolution and Programming Languages" [1] [1] https://www.youtube.com/watch?v=8frGknO8rIg 
Can you provide some historical background on how the division was defined?
I'm stating something weak but spoke about it strongly and I've rectified the wording now to try to be a bit more clear. Invoking a bit more Haskell, I'm trying to talk about the following function zap :: (a, b) -&gt; Either (a -&gt; r) (b -&gt; r) -&gt; r zap (a, _) (Left f) = f a zap (_, b) (Right g) = g b which indicates that a sum can be *a* use of a pair. Unfortunately, I spoke as though its the only use, but obviously that isn't true since we have things like `uncurry (+)`. Really, the way to talk about it would be to invoke `curry` curry :: ((a, b) -&gt; r) -&gt; (a -&gt; (b -&gt; r)) which indicates that we could choose to use the two sides sequentially. This is more general than `zap` since we have embed :: Either (a -&gt; r) (b -&gt; r) -&gt; (a -&gt; (b -&gt; r)) embed (Left f) a _ = f a embed (Right g) _ b = g b but talking about `curry` would have been a longer sidenote than I felt worth tolerating. Hopefully, with this softening of wording there's a better compromise.
Not really. There may have been something in ["Being Lazy with Class"](http://www.scs.stanford.edu/~dbg/readings/haskell-history.pdf) but I don't remember offhand. If you're looking for history then it's worth the (longish) read though!
&gt;we cannot produce the right-side value **right** using Inr Is that bold 'right' not superfluous or does it mean 'correctly' in this context? Anyway, I guess you mean you can't get 'it :: Unit + Void' using Inr (b/c no value of type Void)? But you could get 'it :: Void + Unit' using Inr T?
Superfluous. And that's exactly correct :)
It's just the eighth draft of the paper. By the way, don't imagine that propositions as types is Phil Wadler's original idea - this is a pedagogical survey paper.
Erm, you're right—what I means is that it has no meaning and you would diverge if you tried to find out which number it is.
Tail recursion is taught in HS classes using Java (source: AP CS). Today's programmers *should* understand recursion, especially tail recursion. 
Funny you should ask... I just posted the first Release Candidate: http://projects.haskell.org/pipermail/haskell-platform/2014-July/002964.html I usually let this bake for about 48 hours before announcing to a wider audience. But if you're intrepid, try it! Do keep up with the thread on that list, to see how it is working for folks. As for the website... contributions of content and updates welcome! It is in the repo, and in the branch new-build, built from templates, which need content updates. Pull requests welcome! 
Because if I hide a bottom in there, your inductive proof will be falsified.
Timing update: Release Candidate 1 went out last night. Expect an RC 2 (if needed) in two days. Expect final release by end of next week (8/1).
But there is only one bottom, and the inductive proof can take account of it easily.
To say that it "has no meaning" seems ridiculous.
While I wouldn't want to go back to before hierarchical namespaces for modules, I will advocate that *most* modules named Data.Foo and Control.Bar should be renamed to simply Foo and Bar, respectively. For example, Data.List is a Control.Monad; it's not a meaningful differentiation in a non-strict-by-default, pure-functional language.
&gt; it's better than Java Yeah, well, that's just, like, your opinion, man. I actually don't have *too* much trouble finding stuff in Java, and I think there is some value in organization-based package naming. It would make my life easier if hoogle would index more of hackage by default. Half the time I use hoogle it's because I'm trying to find something outside of the platform. Hayoo to the rescue!
This is beautiful, thank you! Do you know if counting the inhabitants of an arbitrary polymorphic type is a decidable problem?
You can indeed add the bottoms as another case in your inductive proof. The point is that the usual induction principles assigned to particular mathematical entities will not apply in Haskell.
I've long described the problem of make as being Applicative rather than Monadic and got empty stares. Now have a page to refer them to :)
Smells like ornaments. Calling /u/pigworker! :)
I'm glad nobody said that we had "scuttle[d] any hopes of reasoning by induction"! You have an argument here, why burden it overblown, inaccurate phrasing? The difference between allowing GR in function definitions and lazy data types is really interesting, but it keeps getting buried in imprecise language.
&gt; regardless of what a is. Careful... &gt; I don't think you can get 16 different version of f just by using it with Bool, for instance. `Bool -&gt; Bool -&gt; Bool` most definitely has 16 instances in a total language: f0000 _ _ = False f0001 x y = x &amp;&amp; y f0010 x y = x &gt; y f0011 x _ = x f0100 x y = x &lt; y f0101 _ y = y f0110 x y = x /= y f0111 x y = x || y f1000 x y = not $ f0111 x y f1001 x y = x == y f1010 _ y = not y f1011 x y = x &gt;= y f1100 x _ = not x f1101 x y = x &lt;= y f1110 x y = not $ f0001 x y f1111 _ _ = True It has several (at least 8) more in Haskell.
I think it would be helpful if you instantiated the `f` for each application of yoneda; I can't _quite_ follow the more complicated steps.
I'm not sure, actually. Note that you can get all recursive types this way, so if indeed there is an effective counting procedure, it is quite powerful. Some other people here might know more about this.
That's at least correct. The point is, Haskellers do things like casually adding infinity to `Nat`, which they can work with using familiar tools like `Ord`. But you're right that these aren't the usual inductively defined natural numbers. So where does this get us into trouble? It is not that such expressions have no meaning. I'd like to see a focus on where it comes back to bite me that I couldn't define precisely the usual inductive `Nat`. In what ways does dealing with that omnipresent recursion differ from reasoning about programs that use `let rec`? That would be really valuable (for me) to learn more about.
&gt; Close inspection of the transmitted program shows it contains curly braces—it is written in a dialect of C! module Main (main) where { main = do { putStrLn "Really, Wadler?"; putStrLn "Did you forget that layout is optional, in Haskell?"; putStrLn "Braces and semi-colons, ahoy!"; putStrLn "Maybe aliens speak Haskell."; }; }
Not necessarily. It is totally possible for every A to be a B, most B's to have property X, and most A's to have property not-X. I don't really see that applying here, though - most Applicatives and many Monads certainly seem to carry data.
Anytime I've wanted strict tuples, what I've actually wanted is unboxed strict tuples. Those are built in and have a much nicer syntax than `Strict`: (# a,b #)
I don't think anyone will disagree that strict numeric code is (almost) always faster than lazy numeric code. Numeric code is pretty much all I write. That said, I don't see myself using this pragma at all. I think it will make reasoning about my numeric code harder, not easier.
That's a good point. I usually forget about them.
If you add Category to Applicative, you also get the same power of Arrows.
&gt; But you're right that these aren't the usual inductively defined natural numbers. So where does this get us into trouble? This is well stated. If we're talking about whether structures in Haskell model mathematical quantities, our discussion of such things is always in some idealized subset of the language. Indeed, what's the problem here if that subset corresponds well to the programs we use Haskell to write? Haskell is first and foremost a general purpose language, not a system for doing constructive mathematics. To do that kind of work you really want to abolish general recursion and partiality entirely and install a totality checker, but that's not a compromise I think Haskell is ever going to make nor do I see a reason why it should given its design goals.
To first approximation, [everything with the words "functional pearl" in it](http://www.haskell.org/haskellwiki/Research_papers/Functional_pearls). A short list of ones I've read and enjoyed * Brent Yorgey's *Monoids: Theme and Variations* * Janis Voigtlander's *Bidirectionalization for Free!* * McBride and Patterson's *Applicative programming with eﬀects* * Gibbons, Lester, and Bird's *Enumerating the Rationals* ([I talked about this one here](http://tel.github.io/2014/07/09/calkin_wilf_for_early-ish_haskellers/)) * Mark Jones' *Composing Fractals* * Conor McBride's *i am not a number--i am a free variable* * Stephanie Wierich's *Type-Safe Cast* * Koen Claessen's *A Poor Man's Concurrency Monad* * Ralf Hinze's *Explaining binomial heaps* * M. Douglas McIlroy's *Power series, power serious* * Hutton and Meijer's *Monadic parsing in Haskell* * Gerard Huet's *The Zipper* * Jeremy Gibbons' *The Third Homomorphism Theorem* * Stephan Dolan's *Fun with Semirings* Also the entirety of Bird's book [*Pearls of Functional Algorithm Design*](http://www.amazon.com/Pearls-Functional-Algorithm-Design-Richard/dp/0521513383) is great. For an example, take a look at [his sudoku solver](http://www.cs.nott.ac.uk/~gmh/sudoku.lhs) And more...
Prof. Pierce maintains a '[Great Works in Programming Languages](http://www.cis.upenn.edu/~bcpierce/courses/670Fall04/GreatWorksInPL.shtml)' page that probably has lots of overlap with the answer to your question. These days a lot of the 'common ground' for functional programmers (and TPL more generally) is found in textbooks and you won't have to go directly to research papers in order to get a good grounding. This is in contrast to even just a few years ago where it was a little more difficult to get pedagogical material. Dr. Matt Might has a good list for graduate students in programming languages [found here](http://matt.might.net/articles/books-papers-materials-for-graduate-students/#pl).
I've found having an awareness of the algebra of types, even if I don't have a deep understanding, to be really helpful for designing libraries; much more so than trying to take cues from category theory.
Not really relevant to the article, but I had a question about Shake. Why does the only run interface take a `Rules ()` and return an `IO ()` instead of taking a `Rules a` and returning `IO a`? I was looking at using it as a library for the elm build system but needed to determine some information at build time (can't remember the specifics).
* Stephen's Diehl blog post constists some haskell [papers]( http://www.stephendiehl.com/posts/essential_haskell.html) * A Git annex repository of papers by (ocharles)[http://www.reddit.com/user/ocharles] - (papers)[https://github.com/ocharles/papers]
Laziness by default means you don't need to contort yourself to decompose problems into "generate a data structure" and "process a data structure". For the most trivial example (See [this paper](http://www.cse.chalmers.se/~rjmh/Papers/whyfp.pdf) for many more examples): and :: [Bool] -&gt; Bool and = foldr (&amp;&amp;) True map :: (a -&gt; b) -&gt; [a] -&gt; [b] map f = foldr (\a bs -&gt; f a : bs) [] all :: (a -&gt; Bool) -&gt; [a] -&gt; Bool all f = and . map f Notice how none of those definitions fall back to recursion on list constructors. They re-use existing code, though `foldr` is barely different from explicit pattern matching. `and` short-circuits appropriately, thanks to laziness. `map` streams appropriately, thanks to laziness. Here's the really important part: `all` short-circuits. It doesn't even use `foldr` directly. It is implemented in terms of `and` and `map`. It's re-using code. And it's still efficient. And that requires laziness. So then, the important question is "Why does it require laziness by default?" And the answer to that is that programmers are the other kind of lazy. They don't break from the default behavior, unless the default behavior is obviously broken for their cases. So the author of `and` and `map` in a strict-by-default language is going to make them strict, and not care. The author of `all` is going to write it from scratch because it's impossible to re-use `and` and `map` efficiently - and never going to even think about it twice. That's just the way it works, right? You might argue that I'm talking about a social issue, not a technical issue. But in the real world, social issues are technical issues. Laziness enables more code reuse. Lazy by default means more code reuse by default. And that's a *huge* win in the real world. Now, it's true that there are performance issues caused by inappropriate use of laziness. But in my experience (maintaining a large production web service written in haskell), those performance issues are pretty easy to track down and fix. You just need to understand the evaluation model. It's really no different than needing to understand the evaluation model of a strict language to track down problems created by performing calculations too eagerly. Laziness by default is a big win. It means that vastly more code is re-usable without having to do anything special. And the downsides people complain about vanish once you get a bit of experience.
Do you actually need many strictness annotations on your strict numeric code?
I probably add a bunch more than I need to be careful. It's often more a reminder to myself which variables I want to have preevaluated than instructions to the compiler. I've written a lot of functions where some parameters need to be lazy and some might benefit from added strictness.
The basic approach is to write all the data you need into files. - Have something that computes your rules: `myRules :: Rules ()` - Create the result you want in a file: runShake ... $ myRules &gt;&gt; want ["file"] result &lt;- parseFile "file" doStuffWith result You can also do recursive traversals in this style. For example, to collect all files in a project (reachable from the project roots): "//*.dep" *&gt; \out -&gt; writeFile out =&lt;&lt; computeDirectDeps (out -&lt;.&gt; "src") "//*.deps" *&gt; \out -&gt; do imports &lt;- readFile (out -&lt;.&gt; "dep") -- "recursive" call depss &lt;- mapM (readFile . (-&lt;.&gt; "deps")) imports writeFile out (nub (concat depss)) "//.projectdeps" *&gt; \out -&gt; roots &lt;- getProjectRootModules depss &lt;- mapM (readFile . (-&lt;.&gt; "deps")) roots writeFile out (nub (concat depss)) There's probably a way to wrap this up in a nicer higher-level API. 
The fact that `Rules` takes any type argument is mostly there so you can have a `Monad` instance for `Rules`, but morally it's a `Monoid`. Shake usually refers to `Rules ()`. I could make it `Rules a` to `IO a`, but you'd have to compute the `a` value without referring to anything in Shake, since the `Rules` cannot obtain any information about actions. If you can dig up the specific problem that you had with elm (or even if you can fake up something based on what you can remember now), I suggest you ask it on [StackOverflow](http://stackoverflow.com/questions/tagged/shake-build-system).
I'm using Gentoo Linux and this all was done with their package manager, portage. I recommend it to anyone as a cure for cabal hell. I don't remember the exact commands I have used, they were rather ordinary and straightforward.
For me as a novice who's in his second year of learning Haskell, it's nice to see Hutton and Meijer's paper on the list. Thanks to Eric's greatest Functional Programming series on Channel9 I was introduced to Haskell (and he's going to do it again this year as a MOOC). Graham's "Programming in Haskell" was my first and so far the only Haskell book. I keep returning to it over and over - what a great book! (sorry, can't say such words about "Learning Haskell for great good").
I [competed last year](https://bitbucket.org/krakrjak/icfp2013-junkfood) and in the process it really made catamorphisms and (generalized) anamorphisms "click". I think we finished somewhere the the middle of the pack, even though our attempt wasn't much more than brute force. I look at this (and other) programming competitions as learning opportunities. You have a specific goal, a tight timeframe, but (generally) the techologies you use are wide open. With that in mind I encourage everyone to participate. I will be competing again this year. We'll probably use bitbucket again for the central git server. My "partner in crime" was and is one of my roomies, so collaboration was face-to-face, but we welcome other team members; I figure Google Hangouts are a way to collaborate, but I'm open to ideas. Since we don't want to pay for BitBucket hosting, I believe that limits the team size to 5 (committers). I have the requirement that our solution use Haskell, but other languages and technologies are welcome. I'd like a Haskell (or Idris) solution to end up on top this year. PM me or find me on G+ and stack overflow if you want to join my team. I'll be the central point of contact initially and I plan on setting up the BitBucket repo this evening, so it will be ready for cloning and comitting by the time the contest starts.
can you write the readfile/writefile example interface for that?
My papers repository could probably do with some organization - I'm open to suggestions here. It's both a place for me to dump things that I may or may not read, but it's also got some really enlightening papers in too. I worry that it's only useful if you already know what you're looking for (which is kinda the point, it's meant to be a convenient way to get that PDF you want without faffing around on Google)
Okasaki: Purely Functional Data Structures; good technique for reasoning about time complexity of non-strict code in persistent contexts. Also: http://cstheory.stackexchange.com/questions/1539/whats-new-in-purely-functional-data-structures-since-okasaki
bss03 explained this thoroughly, but it's better to think of `f` as being the function which which operates without knowledge of `a`. Thus we can say something more like "`f {A}` has two instantiations without taking advantage of knowledge of `A`" where `{.}` is System-F type application.
There's also Extempore, if you're going the lisp-way. Check out this recent presentation by the author, Andrew Sorensen: https://www.youtube.com/watch?v=yY1FSsUV-8c
agreed! In a lot of fancy haskell projects, the rough pattern is "strict in the small, lazy in the large". This is very strongly so in numerical domains, but its holds pretty broadly too
Here's a list I keep: &lt;https://secure.plaimi.net/~alexander/haskell.html&gt;. It is graded on "difficulty". Suggestions welcome!
As we're hosting the hackathon in our Berlin office the [factis research/Checkpad](http://www.checkpad.de) team is coming. I'm personally looking forward to improve our [docker](http://docker.io)-based continuous integration/delivery system [Frozone](https://github.com/factisresearch/Frozone) with tight integration of Git and Darcs. Some darcs hacking would be also cool!
@stepcut251 Thank you so much for your patient explanations. I think this was hard for me as a beginner because I was trying to wrap my head around a lot of concepts while also dealing with the opaqueness of HSX code generation. Your explanation solved my problem. The scariest part: I think I even understand it. Let's see if I can explain it. First, breaking down the type signature of the template function: template :: XMLGenT (HSPT XML (ServerPartT IO)) XML -&gt; ServerPartT IO XML From what I can gather, this kind of type signature is a composition of abstractions. Basically, we have a few different behaviors working together here: 1. Translate literal XML into function calls 2. Render XML from translated function calls 3. Insert rendered XML text into response These responsibilities seem to correspond nicely to the types in our signature: 1. XMLGenT 2. HSPT 3. ServerPartT So we're composing these 3 specific abstractions to get the behavior we need. But, since HSP &amp; Happstack are standalone abstractions, one doesn't know about the types of the other. genElement (the top-level function from HSX) is generic enough to insert its XML into any monad (returns XMLGenT m (XMLType m)). In this case m is ServerPartT, since it's inserting into a Happstack response, right? I'm guessing the fact that the type signature so closely matches the actual flow of the abstractions is by design? Is that what some Haskell devs mean when they say "all you need is the type signature"? Finally, as for the type conversion, I kind of get it. I can tell that I'm specifying that "body" is a value of type "XML", but that unXMLGenT &amp; unHSPT both work on generic types (probably because it can deal with many types of XML nodes: children, parents, entire documents, etc). genElement (the top-level function that HSX inserts in place of literal XML) returns "XMLGenT m (XMLType m)". Nowhere along the line does anything actually say the wrapped value has to be an XML, so by the time it gets to something expecting an XML, it fails because it doesn't know if it has one. Does that sound right? Also, I took your suggestion of the monad transformer but instead of making it an App variable, I called it "WebPage", since I figure the cross-section of HTML-rendering &amp; HTTP handling makes that an apt description for "HSPT XML (ServerPartT IO)". Would it make sense to have a type alias like this built into happstack-hsp? Seems like it might lower the barrier to entry that I ran into here.
I generally import these kinds of papers into Mendeley, then I can search and organise quite easily.
You can throw exceptions from Shake actions, which bubble up to the thing that called shake, which might be how you indicate type checking failed.
["Seven trees in one"](http://www.math.lsa.umich.edu/~ablass/7trees.pdf), because it's hilarious.
Can programming be liberated from the von Neumann style? * http://web.stanford.edu/class/cs242/readings/backus.pdf The next 700 Programming Languages * http://thecorememory.com/Next_700.pdf A correspondence between ALGOL 60 and Church's Lambda Notation Part 1 * no link :( The Hindley-Milner Type System (pick one of the following) * http://ac.els-cdn.com/0022000078900144/1-s2.0-0022000078900144-main.pdf?_tid=924acc8e-11e4-11e4-81ba-00000aab0f27&amp;acdnat=1406063593_751828af14ff6a8484a84e19bd0b18a6 * http://homes.cs.washington.edu/~mernst/teaching/6.883/readings/p207-damas.pdf * https://en.wikipedia.org/wiki/Hindley%E2%80%93Milner Why Functional Programming Matters * http://www.cs.kent.ac.uk/people/staff/dat/miranda/whyfp90.pdf How to make ad-hoc polymorphism less ad hoc * no link :( Lazy Functional State Threads * http://homepages.dcc.ufmg.br/~camarao/fp/articles/lazy-state.pdf Typeclassopedia (pick one of the following) * http://www.haskell.org/wikiupload/e/e9/Typeclassopedia.pdf * http://www.haskell.org/haskellwiki/Typeclassopedia Applicative programming with effects * http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.22&amp;rep=rep1&amp;type=pdf The Essence of the Iterator Pattern * http://www.cs.ox.ac.uk/jeremy.gibbons/publications/iterator.pdf The rest is just for fun, but I think really shows the awesomeness of Haskell. Composable Memory Transactions * https://research.microsoft.com/en-us/um/people/simonpj/papers/stm/stm.pdf Supero: Making Haskell Faster * http://community.haskell.org/~ndm/downloads/paper-supero_making_haskell_faster-27_sep_2007.pdf 
&gt; The contest organisers are: Nicolas Wu (co-chair) (zenzike), Duncan Coutts (co-chair) (dcoutts), José Pedro Magalhães (dreixel), Richard Evans, Martin Lester (mariusz), and Marcelo Sousa (mabs). Sounds like an Haskelly Oxford thing to me!
frozone looks interesting. can you tell us some design decisions? in what way does it differ from e.g. [drone](https://github.com/drone/drone) that also uses docker for the base image?
Wow! Literal haskell never made sense to me (what's the point of it?) until I read that sudoku solver. Now I see its purpose. Thanks for the list!
A more recent paper that I think is really nice, and approachable for beginners: Fritz Henglein, Ralf Hinze: Sorting and Searching by Distribution: From Generic Discrimination to Generic Tries But really, I'd recommend any functional pearl, and [these papers](http://www.cis.upenn.edu/~bcpierce/courses/670Fall04/GreatWorksInPL.shtml) 
What kind of task will the programmers be given? I couldn't find that information on the contest website.
Recursive bindings, for one, but they will be explicitly disallowed, probably. I can't actually think of another *syntax*, but things would certainly be different if [a] was strict, for example.
GHC has an excellent garbage collector and functional languages are no worse at garbage collection than other languages. However, Java or .NET's GCs are tuned for those languages, and not to the allocation patterns of something like Haskell, so GC performance of a Haskell program on those platforms may suffer. But all serious GC's use parallel generational-copying collectors just like GHC does, but GHC's constants are tuned better to accommodate lots of tiny allocations, as is common in FP. In general, I have found that hacker news is useful only as entertainment. Take anything you learn from there with a massive grain of salt.
Languages based on immutable data structures need to allocate new structures all the time instead of modifying old ones. Since the old copy (or parts thereof, due to pervasive sharing) is often not needed anymore, those languages tend to produce a lot of garbage. On the other hand, I remember reading that the immutability simplifies the logic of the garbage collection, so even though more collections occur, they might be faster.
What I meant was, that if you have a function with signature f :: forall a. a -&gt; a -&gt; a it has 2 inhabitants, and it doesn't matter if you use f True False later in your program. I should have been more precise in my use of "using it with" :)
We won't know until the contest starts, although you can look at past tasks. They are varied, but almost always strike me as difficult. Last year we were asked to, given some input/output pairs of 64-bit numbers and an AST size, guess the program in a "toy" language that include lambdas and folds. The language our guess was written in didn't vary, but there were hundreds (thousands?) of different sets of I/O pairs. BTW, you can only take a maximum of 5 minutes to solve any particular set of I/O pairs.
The statement cames from using a functional language in a runtime system with a GC tuned for an imperative/OO language (.NET). FP languages allocate more often; on the other hand, in an imperative language it may not be possible to compact the heap by moving objects around in case the program depends on address order, for example, so allocations tend to be more expensive. I believe that in OCaml and Haskell allocation is very cheap (essentially bumping a pointer)
thanks
It's wrong in types too
Interesting, I've heard GHC's compiler described as state-of-the-art quite a few times.
It was scary but not to the degree of "including (general) lamdas and folds" lol. The language is actually 1st order.
Most of it certainly is. It seems like multiple times per year, the GHC team integrates more optimizations or features based on recent academic results. The GC (really more part of the run-time rather than the compiler) is no slouch either, but I don't think it has received the same level of improvements.
I don't recall a considerable restriction on lambdas; I could be misremembering though. I do recall the fold being rather limited, and having programs be limited to at most one fold. Maybe the lambda was only allowed as part of the fold? I don't recall having to handle a variable bound to a lambda, though I'm fairly sure my interpreter handled that case.
That's a good point that I didn't realize when writing that post: the "proper" `IOA` example doesn't even use the `Arrow` interface, only `Category`.
I found last year's contest here: http://icfpc2013.cloudapp.net/ (Search for "0. Syntax"). The keyword `lambda` was essentially a decoration for the fold. This language didn't really have lambdas ;)
If you can't trust some random uninformed commenter on HN, who can you believe?
Yes, this is almost certainly what the post was trying to say. In OO languages, you often have a choice where when you want to modify an object, you can either mutate it or replace it with a new one. Replacement is sometimes better because it keeps your changes private (and thus easier to reason about) instead of making them visible to other code that also had a reference to that object - but of course a lot of the time you do want the change to be visible elsewhere, and more to the point here, allocating a new object does have a cost. The only difference here when talking about functional languages is that they make the choice for you, so you're going to be allocating a bunch. It's fun to profile Haskell programs because often they allocate dozens of gigabytes of memory in a matter of a few seconds but don't actually use any more memory - they just churn through a *lot* of allocation and collection. In an eagerly evaluated pure functional language you could in fact write a generational garbage collector where you can collect younger generations without considering older ones at all, because there can't be any references from an older generation to a younger one. Haskell's laziness, however, does let you [create those references](http://www.haskell.org/haskellwiki/Tying_the_Knot), so GHC [doesn't get to use that particular trick](https://ghc.haskell.org/trac/ghc/wiki/Commentary/Rts/Storage/GC/RememberedSets).
Functional languages that run on VMs not designed for functional languages suffer some. Why? We pay a tax for a feature we're not using: The ability for every field to be mutating all the time. Also there are space leaks you [need cooperation from the garbage collector to solve](http://homepages.inf.ed.ac.uk/wadler/topics/garbage-collection.html) in lazy languages, so using the host VM's garbage collection blindly usually means ignoring those issues. Languages like Haskell that have their own GC on the other hand? GC there is pretty solid. It gets a ton of benefits relative to other languages. Immutability means that we can reduce a lot of the work spent 'rescanning'. In fact if you look a little farther afield, the garbage collector for Erlang is _amazing_. Because they are both strict _and_ purely functional they can do the entire collection in one pass, which is a thing of beauty. 
The lists seem to have missed one of my favorites The Essence Of The Iterator Patten http://www.comlab.ox.ac.uk/jeremy.gibbons/publications/iterator.pdf
Quite enjoying that sudoku solver. I also found I prefer my transpose to Data.List.transpose, although I have not benchmarked it: transpose [] = [] transpose as = foldr (zipWith (:)) (repeat []) as Edit: Benchmarking done, about 3x slower than Data.List.transpose on my machine using 7.8.3 and -O2 and forcing the computation with a sum of the elements. Even with the all powerful GHC I suppose abstraction comes at a price.
How about this? import Data.Char import Control.Applicative isPal :: String -&gt; Bool isPal = ((==) &lt;$&gt; id &lt;*&gt; reverse) . map toLower . filter isAlpha
I always like transpose = Data.Traversable.traverse id
I agree with your approach of using the the Data.Char predicates. Here's how I implemented it: import Data.Char (toLower, isAlpha) isPal :: String -&gt; Bool isPal s = cleanedString == reverse cleanedString where cleanedString = map toLower $ filter isAlpha s Here I used `filter` to get rid of characters like space and punctuation (but also numbers, which may not be what you want. Then I map to lowercase characters, which I then compare vs the same string reversed. This would go through the string multiple times; in practice that's not a big deal and almost certainly isn't for a palindrome function
Btw, there's an extra = in isPal ==
You could go to http://hackerschool.com ! Definitely got me a lot of interviews that I never would have otherwise - I'm likewise self-taught.
Try to convert [Typeclassopedia](http://www.haskell.org/haskellwiki/Typeclassopedia), got errors: pandoc: Error producing PDF from TeX source. ! You can't use `macro parameter character #' in math mode. l.592 ($)&lt;/code&gt; &lt;span style="border: 1px solid # 
For general use, it's probably as good as you get in a production language runtime system. It has been very heavily tuned, and a lot of work has gone into implementation/measurement/development over the years. For specific purposes (e.g. real-time, parallel), there are better memory management techniques, but they're not commonly (if at all) provided with language implementations.
And most importantly, Erlang has small processes with copy-on-send semantics, which means that garbage collection only needs to look at one small process heap at a time.
I second that. "Programming in Haskell" also was my first Haskell book and shows the author's long experience in teaching. I would also like to mention "Real World Haskell" as a great second step after PiH. It goes easy at the beginning, only introducing you to what's needed and then diving deeper into more advanced topics in later chapters. Next book for me will be "Beginning Haskell" .. heard it's a bit more up-to-date with libraries and techniques. 
Which version of pandoc are you using? pandoc --latex-engine=xelatex -o typeclass.pdf http://www.haskell.org/haskellwiki/Typeclassopedia https://www.dropbox.com/s/qhu2r6bncxo90kl/typeclass.pdf You can also specify --toc to generate your own table of contents and use a filter to remove the generated one if your were systematically converting wiki pages. 
Your transpose has different outcomes than the one in Data.List. Trying `transpose [[1, 2], [3, 4, 5]]` I get `[[1, 3],[2,4]]` from your function and `[[1,3],[2,4],[5]]` from Data.List.transpose edit: updated with actual result
http://homepages.inf.ed.ac.uk/wadler/topics/monads.html Philip Wadler's Paper about Monad!
Yeah, they 'stop the world' but they have a lot of little worlds, where we have (more or less) one big one.
Recursive bindings will be allowed. See the (now updated) design.
Simon has updated the proposal with much more detail.
See the (now updated) proposal. We'll keep CAFs lazy. There isn't really a good time to evaluate them eagerly. I guess one could try to define some evaluation order for top-level things (and people using top-level IORefs have wanted that for some time), but that's out-of-scope for this design.
The proposal has now been updates to cover this.
Thanks!
&gt; Hi, I am a newbie to Haskell … &gt; … but this is my first haskell program … I think your example is needlessly complicated given OP's situation (using Applicative, the `((-&gt;) r)` instance and pointfree style), I'm guessing [MaxGabriel's solution](http://www.reddit.com/r/haskell/comments/2bnwbw/need_help_bool_to_identify_palindrome_in_haskell/cj77v44) is much clearer to a newcomer. *(That being said `(==) =&lt;&lt; reverse` or what hdgarrood said allow you to omit the `id`.)*
The nice thing about programming in German is you don't need to debate naming convention.
Normally i am a big fan of programming contests, but this looks somehow boring and too unclear. 
When your objects are immutable you don't need a write barrier.
Ghc could allocate all strict data on a separate heap and avoid the write barrier and the remembered set. Efficient Haskell uses strict leaves anyways.
Haskell has been used as a host language for generating safety critical code in a smaller language (like C). See atom and copilot. https://leepike.wordpress.com/category/copilot/ Have a chat with lee pike about it.
Sorry, yes, I wasn't thinking too clearly and [left out a whole ton of details](http://stackoverflow.com/a/10531030/476408). In particular, it's traverse of sized vectors.
Do have any specific examples of optimizations or features you consider state of the art that GHC's GC doesn't have? 
To me it seems like there are almost two levels to the contest. One is basically writing a pacman AI, the next is analyzing the code of the ghosts and adapt the AI around that(at runtime). The second level seem way, way more involved than the first, but it is not completely clear to me how much of an advantage you can achieve by analyzing the AI, especially when you only have a few days. But as /u/sclv says, these sorts of questions are probably what makes this competition unique.
Are you sure you can analyse the code of the ghosts? Can't it be arbitrary and inaccessible to the lambdaman CPU?
You know, I feel the same way, though I didn't realize it until I read your comment. 
And of course, if you want to generate some C, then prove that C is equivalent to the resulting object code from a compiler - well, there's a formally verified compiler [that can do that!](http://compcert.inria.fr)
pandoc 1.11.1, upgrade to the latest(1.12.4.2) solves the problem, thanks.
No. I have just quickly glanced the rules. After a second glance, maybe the ghost AI is given before hand, and constant through the entire competition?
Using a totally different underlying machine as well
I'd be very interested in participating in a german haskell community. It saddens me that functional programming isn't more common when talking to other students.
I think it looks really nice and will involve lots more programming than most coding contests (which are typically better named algorithm contests.)
Thanks!
Well, there are several startups trying to pave the ground for bigger firms. Then people come together regularly in [meetups](http://www.meetup.com/Regensburg-Haskell-Meetup/). You can also start out one at your local uni. Last but not least, there is [HacBerlin2014 coming up](http://www.haskell.org/haskellwiki/HacBerlin2014).
I hope the OP at least gives you a Thank You for solving their homework.
Good point, mine only works for matrices. Edit: Although I do get `[[1,3],[2,4]]` for your example: it truncates the list of lists to a matrix
1. This is a real practical concern; Haskell with explicit memory management wouldn't be Haskell anymore, and using Haskell in realtime-critical applications is a challenge to say the least. As much as I love writing Haskell, I would not choose it for realtime applications. 2. Since GHC is written in Haskell, it is probably easier to verify than a compiler written in, say, C. However, I don't think anyone has ever actually done it. While Haskell is relatively unsuitable for writing the application itself, it can still be a powerful tool in constructing it though, as others have mentioned. The language shines at building parsers and compilers, really.
Sorry, you're right, that's the output I got too
Every topic about Haskell problems have the same structure. Some practitioner says "Looks promising. Guys, could you please fix documentation and write tutorials so I can get something done?". Then a die hard haskeller step out with response "No, I'm fine and you are too stupid to use it anyway." and get upvoted to the sky.
Yes, pure functional programming languages are insufficient for Applications with Guaranteed Time Bounds (well it's possible to have the garbage collector timeout after certain time limits but then the application can run out of memory unexpectedly and that's not so good for safety critical applications). This makes me sad because I really love using functional programming and I can't use verified programming languages such as Coq or Agda for programming and existing verified imperative languages such as Frama-C and Ada Spark are kind of weak and hard to get started in without buying support. Ada Spark 2014 looks interesting though. It might be possible to write an OCaml program that allocates all of its memory at startup and then afterwards never allocates any memory at all. As well, you could have a hierarchy of OCaml programs with isolated heaps that monitor each other for failure (sort of like Erlang). However, you wouldn't find much support or tooling in the community for this. You might have to stick with C and Ada. I'm assuming you have looked into Ada Spark and Frama-C? Those kinds of verifiers can significantly reduce bugs. If you have a RTOS which strictly enforces separation in time and resource usage you could have some tasks that have Guaranteed Time Bounds written in C or Ada and others that don't need Guaranteed Time Bounds written in other languages.
Being able to express that type of dependence seems quite useful... Why is it so hard for haskell to do so?
Here are three even more down to earth dependent pairs: 1. Variable length arrays can be modeled as a pair of an int (length) and a fixed length array of that length. 2. A discriminated union can be modeled as a pair of a tag, and a payload. E.g. for Maybe you have the tags "Just" and "Nothing" and the type of the payload depends on the tag (nothing for Nothing, and a single value of type T for Just) 3. Existential types (and closures) can be modeled as a pair of a type, and a value of that type.
ATS might suit your purposes.
And, similarly, a Pi type `Pi Bool (\b -&gt; p b)` gives rise to a pair.
Type inference becomes impossible in general, having any haskell expression on the type-level means it's much harder to show that the type expressions terminate, unification becomes much harder to do as you have lambda terms in types, etc. 
You can pretty easily use equality proofs in Haskell: data Eq :: a -&gt; a -&gt; * where Refl :: Eq a a subst :: Eq a b -&gt; a -&gt; b subst Refl a = a 
Yes, pause times of the parallel GC are not so nice. If you want an absolute bound on pause times, you'll have to trade quite a bit of performance for it, though (on-the-fly GCs cannot move data, or you need an expensive read barrier). I think a more realistic approach is a mostly-concurrent GC (there's an IBM paper which I can't find, there's JVM's CMS collector, and the new Android runtime also has one). They can't give you absolute bounds on pause times, but they're "real-time enough" in practice (e.g., for web servers, best-effort style applications). Those mainly need a modified write barrier which GHC already has for generational GC.
Maybe for cases where you don't want users to define their own instances?
Well, the basics of it at least, sure. Every time I've tried to make use of those they've driven me into weird twisty dead ends.
I have been collecting papers here http://www.dohaskell.com/type/papers and here http://www.dohaskell.com/type/light%20papers, but I wouldn't say every Haskeller should read all of them :P
It seems to me that the the transpose of a ragged array should be undefined. Otherwise you have to give up the idea that transpose.transpose == id It seems that the transpose in Data.List.transpose doesn't follow this, giving Data.List.transpose $ Data.List.transpose [[1,2],[3,4,5]] == [[1,2,5],[3,4]] which is unfortunate IMO.
I can see then only exporting the ~~class type and no functions~~ functions and no class. This guy seems to have a case for that: http://stackoverflow.com/questions/17849870/closed-type-classes However, exporting just one or two functions with the class while leaving other undefined ones will cause people to define incomplete instances. I say the compiler should stop you if you try to NOT export all undefined functions. Or at the very least show a warning. edit: some clarification
I've used them quite effectively. It can be nicer than Agda because the GHC simplifier is more powerful than Agda's rewrite mechanism Pattern guards are often quite useful, e.g: zeroPlusNEqualsN :: SNat n -&gt; (Zero :+: n) :=: n zeroPlusNEqualsN SZero = Refl zeroPlusNEqualsN (SSuc n) | Refl &lt;- zeroPlusNEqualsN n = Refl
The intro mentions a two-player mode, but the spec does not. Edit: oh the spec mentions a queryable lambdaman-2 position
Have you looked into [Rust](http://rust-lang.org)? It brings a lot of Haskelly goodness to a very low-level imperative systems-oriented language.
Interrupt 2 kind of does. I guess our ghost AIs will have to fight up to two Lambda men? *edit*: and I guess the undocumented parameter given to the lambdaman AI will be an integer indicating whether it is the first or second player.
Perhaps the forthcoming [fuse](http://hub.darcs.net/ertes/fuse) library?
Over on Y-combinator there was someone complaining that the spec looked too precise and long. But we've been fielding questions ever since it started on little details. Its actually really important to be precise and clear. The downside is a rather long and formal-reading specification. Our goal has been to make it fun, and I think there are some fun ideas in here if you can look past the minutia of the game rules.
Not a very great advantage, it's not like you can just port over your existing Java code :-)
Oh, yeah. Well partly. The second example in the chosen answer looks more helpful since then you can explicitly define the types in your signatures (as opposed to letting the compiler infer them). 
How about [folding](http://hackage.haskell.org/package/base-4.7.0.1/docs/Data-Foldable.html#v:toList) your structure into a list of independent IO actions, would that work for you?
Ooh okay, that makes sense. I'm not sure I'd want that tradeoff then. Having not used Idris, does it suffer from those drawbacks because of the type dependence? 
Thank you. Seems to be a bit of an overkill, but still an interesting project.
This is exactly where I wish we had ML modules! 
Note that you can build any `ListT` (not just the one in `pipes`) with only a `transformers` dependency. For example, here is how you would implement a `ListT` analog of `Pipes.Prelude.stdinLn`: -- stdinLn :: ListT IO String stdinLn :: (MonadTrans t, MonadPlus (t IO)) =&gt; t IO String stdinLn = do eof &lt;- lift isEOF if eof then mzero else do str &lt;- lift getLine return str `mplus` stdinLn That will type check as any `ListT` out there and do the right thing for all of them. I gave an extended talk on this at BayHac and I'm working on writing it up into blog post form.
&gt; It would require traversing the whole data structure to produce this list, which would defeat the purpose. I don't understand why. Wouldn't the list be produced lazily, like everything else? &gt; BTW, it's impossible to suffice a Foldable instance with a mutable data structure. Mutable structures? I must not understand what you are looking for, then. Is laziness even compatible with mutability? After all, laziness is the [hair shirt](http://research.microsoft.com/en-us/um/people/simonpj/papers/haskell-retrospective/HaskellRetrospective.pdf) which keeps Haskell pure.
&gt; Unlike the obnoxious ListT in transformers, the ListT in the List package, for example, is not "ListT done wrong" http://hackage.haskell.org/package/List-0.5.1/docs/Control-Monad-ListT.html I think there are some other similar "ListT done right" packages; I'm not sure which is best. Thank you. &gt; There is also FreeT ((,) a) m r which is another name for "ListT done right", of course, though you'd need to define some of the combinators you mention. I had such a feeling, just couldn't wrap my mind around it (yeah, [`wrap`](http://hackage.haskell.org/package/free-4.9/docs/Control-Monad-Trans-Free.html#v:wrap) :) ). Please elaborate on such a usage of `FreeT` (no need for the free monads introduction though). E.g., what's the purpose of the tuple functor `((,) a)`? Would be nice to have some links to where this is discussed. &gt; You should consider though that pipes might really be the way to go: the pipes library simply does not bind you to an ecosystem'. &gt; The Producer type is extremely useful even if you never use the (&gt;-&gt;) (it took me a while to see this). As soon as you start to work with it, the Producer concept in pipes starts seeming a better type than "ListT done right" anyway. ListT m a scraps the the return value in Producer a m r; but this has a million uses. It's not as much the dependencies that I implied by the "ecosystem" and am bothered with. It's the following: * "pipes" has popular competitors like "conduit" and "machines". Since I need this for a fairly basic library (["stm-containers"](http://hackage.haskell.org/package/stm-containers)), I feel that dictating a choice of a general streaming library to a user would be unfair. * I try to stick to the most simple abstractions that fit the bill. Since I don't need the producer's return value, it's obviously a case for a simpler abstraction. The API I'm looking for would provide a simpler monadic composition (e.g., `stream1 &gt;&gt;= stream2FromElementOfStream1`), which is just like a list monad, but due to "pipes" having that extra return value, the composition is done on another level and is more involved. IOW, the API I'm looking for is basically a higher level of abstraction. I imagine there could be some proxy APIs which would perform transformation to specific general streaming libraries at a cost of `O(1)` complexity. E.g.: Stream STM (k, v) -&gt; Producer (k, v) STM () 
&gt; Wouldn't the list be produced lazily, like everything else? In a monad it will have to traverse the whole thing prior to executing the following action. &gt; Mutable structures? I must not understand what you are looking for, then. Is laziness even compatible with mutability? After all, laziness is the hair shirt which keeps Haskell pure. [See this](http://hackage.haskell.org/package/stm-containers-0.1.2/docs/STMContainers-Map.html). The data structure lives in the `STM` monad. As I already mentioned it's definitely possible to achieve the laziness with streaming libraries like "pipes". All I'm looking for is a simpler abstraction.
I don't get it.
Out of curiosity, what Haskelly goodness things do we get from Rust?
Chris Okasaki's *Purely Functional Data Structures*
Yes, the bind you are looking for in `stream1 &gt;&gt;= stream2FromElementOfStream1` is `for`: for stream1 stream2FromElementOfStream1 I wonder if `Tekmo`s idea could be carried out on a large scale: map2AnyListTStreamLikeThing :: MonadPlus m =&gt; M.Map k a -&gt; m (k,a) map2AnyListTStreamLikeThing = M.foldWithKey (\k a b -&gt; return (k,a) `mplus` b) mzero Then, to show it can specialize to the `ListT` in `pipes`: &gt; mymap = M.fromAscList [(1,"1"), (2, "2")] &gt; runEffect $ enumerate (map2AnyListTStreamLikeThing mymap) &gt;-&gt; P.print (1,"1") (2,"2") Is it that any `ListT`-like thing will have a `MonadPlus` instance that makes sense of this? 
Mainly a better type system and immutability by default, which for me are two of Haskell's defining features. EDIT: Also some syntax sugar like pattern matching and type inference.
Sorry, I was being obscure. Genuine modules allow you to separate an API from its implementation. There could be a "effectful streaming" API and all of pipes, conduit, and what-have-you could target it. If your stm-containers depended upon that interface alone then users could later decide which underlying implementation they like the most. [Tekmo pretty much suggested this exact thing, but translated to Haskell](http://www.reddit.com/r/haskell/comments/2bpsh7/a_simple_monadic_stream_library/cj7sqtw). Unfortunately, you need existentials to make this behave conveniently... and then you start to get into a roughly rough patch with inference.
Nice! But I had to use google translate page, since I don't know German ; - ;
Tail recursion is still important in Haskell, you just have to additionally watch that you're not building a big inner structure of thunks while you do it and that your tail recursive function is collapsing its arguments as soon as it can. It gets more complicated but it doesn't become irrelevant. This is what `foldl'` does and it is indeed efficient, the tail call behaves like a goto as it should. The other consideration is that another efficient recursive case opens up in the form of "co-recursion" (we need a better name for it) which uses a call to a lazy data constructor to only build a data structure as far as it is demanded. This is what `foldr` does when its given argument is a constructor or a function that ends up as a call to a constructor at the outer level.
True, but if you make that undefined behaviour then you also give up the `transpose . transpose = id` property. At least this way you get to keep a weaker `transpose . transpose . transpose = transpose` property. Also it seems that Data.List.transpose is supposed to work with infinite rows and infinite columns (as well as ragged rows), not sure if any of the other proposed implementations have that property.
Try this tutorial, it's both useful and very fun: https://ocharles.org.uk/blog/posts/2013-08-01-getting-started-with-netwire-and-sdl.html 
&gt; because their workflow is typically to pick up tools and get moving quickly rather than to stop and understand its foundation. I personally think that Haskell could support this workflow and make it safer. 
I don't think there's much damage in writing an example or two even if just for brand-new Haskell users.
&gt; In a monad it will have to traverse the whole thing prior to executing the following action. I was thinking of `[IO a]`, a list of independent IO actions, not `IO [a]`. &gt; As I already mentioned it's definitely possible to achieve the laziness with streaming libraries like "pipes". Ah, I get it, you want streaming, not laziness. Something like the following, right? A monadic action which gives you the next element, and a further action which you can call if you need more? data Iterator a = Iterator { runIterator :: STM (a, Iterator a) } 
Why is that, could you elaborate?
I tried out the book and I keep seeing '↑' symbol. What is the keyboard equivalent of it? 
Rust is nice, but still a long way away from being production ready.
Nice blog!
This assumes parametricity, doesn't it?
Well, in that case, I can explain /u/random_crank's `FreeT ((,) a) m r` suggestion: that representation is isomorphic to a version of `Iterator a` which can also signal the end of the stream by returning an `r` instead of an `(a, Iterator a)`. type Iterator a r = FreeT ((,) a) STM r data Iterator a r = Iterator { runIterator :: STM (FreeF ((,) a) r (Iterator a r)) } data Iterator a r = Iterator { runIterator :: STM (Either r (a, Iterator a r)) }
The simple-conduit library on Hackage may have just the power to weight ratio you are looking for, as it is essentially a streaming library built around a short-circuiting form of ListT. See me (johnw) on IRC if you'd like more info. 
&gt;You could reduce the amount of tests needed by proving the code instead in the SPARK subset of Ada. When you mean 'proving the code', do you mean C++ code? Is this even feasible in large codebases? 
Oh, incidentally, this is how you do a dynamically typed EDSL in a DTPL.
A discussion over at the python sub: http://www.reddit.com/r/Python/comments/2bbve4/what_can_python_learn_from_haskell_video/
Yep.
He goes to far, tries to get too much, and loses goodwill because of it. If he just focused on one of the stronger points, like adding static optional typing, I think it would have a better chance.
I thought this was fascinating. 
There is a way around that: add the GHC_PACKAGE_PATH to the package.conf.d in your sandbox, for example GHC_PACKAGE_PATH=/path/to/sandbox/packages music2ly. Alternatively, install from source using music-util as described here http://bit.ly/1nqscJn. This always sets up the suite in a local sandbox.
So the "signature functor" for `[a]` is data Listf a x = Nil | Cons a x or, in other words, `Maybe . ((,) a)` pretending we could compose types with `(.)`. If you're interested in genuine streams, however, we can eliminate the `Nil` constructor and now we just need `((,) a)` as the signature functor. By signature functor I mean that `[a]` is isomorphic to `Fix (Listf a)` where data Fix f = Fix (f (Fix f)) -- tongue twister, I know then we can produce the free monad over `f` data Free f a = Free (f (Free f a)) | Pure a which gives us our "`Nil`" back, now called `Pure` and holding a value of type `a`. Here `Free ((,) a) a` is the same as a non-empty list, `Free ((,) a) ()` is the same as `[a]`, and `Free ((,) a) Void` is an infinite stream again -- Inlining `Free ((,) a) r` gives us Stream data Stream a r = Next (a, Stream a r) | Terminate r Finally, we can use the general construction for a free monad transformer data FreeF f m a = Free (f (FreeT f m a)) | Pure a newtype FreeT f m a = FreeT { runFreeT :: m (FreeF f m a) } which puts `m` effects in all the right spots in order to be "`ListT` done right". So, Fix ((,) a) -- is the type of infinite streams Free ((,) a) r -- is the type of streams which have "return values" FreeT ((,) a) m r -- is the type of ListT done right
I have been working on one. It is subpar compared to the competition but the code is simple enough, perhaps educational. https://github.com/gatlin/FreeStream
The whole point of giving answers like that to people asking you to do their homework is that their grader will see that they clearly didn't do the work themselves.
How do you know its length without traversing it?
There are a couple of times you'll want to do this. Sometimes it happens when you don't expect other people to write instances of the class and you have implementation-specific operations that might not preserve some invariant that people normally shouldn't call. Often these methods will be exported in another module marked Unsafe or Internal. If you do intend for people to be able to write their own instances of the class you can provide default implementations of the hidden methods so that they aren't undefined by default. You'll see this pattern in a couple of places: http://hackage.haskell.org/package/profunctors-4.0.4/docs/Data-Profunctor-Unsafe.html This package has two unsafe methods on the Profunctors class for performance purposes. These are only available in the Unsafe module. See also: http://hackage.haskell.org/package/array-0.5.0.0/docs/Data-Array-IArray.html#t:IArray IArray has a number of unsafe methods that are not exported by default. It is important to export the type class always, however, so that people can write down the type signatures when they use methods from your class.
I agree, you can almost feel the spite when the guy at the end asks if static type checking is worth it.
I don't, and I didn't claim I would. I only claimed that a data structure (String,Int) -- where the Int is known to be the length of the String -- suffices to check for palindromness with a single traversal. And note that (String,Int) is a weaker requirement than having a full blown array with random access.
It is more like a guy grappling with two different languages with two different design philosophies. Being more pragmatic and understanding about the trade-offs would a better use of his time than trying to meld the two together all willie-nillie.
TBH - the original poster is better off changing jobs than trying to change a whole company culture to use their favourite language/languages.
Check out Evernote Clearly, it's a local browser extension so doesn't call out to anybody. It's pretty good but sometimes messes up non-alphanumeric symbols in code fragments.
Actually Pandoc is generating LaTeX markup only, not PDF 😊
That's not the feedback that I got from the many people that came up to me after the talk this week.
Provably no data races except in code marked "unsafe". 
Yes. This is why all top-level bindings must have a type signature in Idris, as it uses bidirectional type checking.
Latter in your comment, former chronologically.
Don't think that's necessarily a result of the talk. Types are just a intrinsically polarizing issue in the profession, at least at the current moment.
GHC's run-time system contains a lot of C code, and the correctness of that code is a valid concern. There have been efforts to move as much as possible into libraries which can be written in Haskell; however, things like the garbage collector will for the foreseeable future be written in C. 
Yes, sure; still, GHC contains relatively more Haskell code and less C (or C++) code than, say, gcc or clang. The C part is probably equally hard to verify, but the Haskell part is going to be easier.
Since traverse f . map g = traverse (f . g) you can just use transpose = getZipList . traverse ZipList as in https://github.com/ekmett/lens/blob/master/src/Control/Lens/Traversal.hs#L373
Ah, yes, I looked at the type of `traverse id` and then figured out that putting a ziplist on the inner list should work. Wasn't really thinking about what the id was doing in there.
Also, note `sequenceA = traverse id`. =)
That can be better handled by giving it an unexported superclass. class Foo a where .. class Foo a =&gt; Bar a where .. just don't export `Foo` and now they _can't_ make an instance of Bar.
What can Haskell learn from Python? That's probably a more interesting question for Haskellers.
Or if you misuse the concurrency primitives. (Or, does that not count as a _data_ race? I'm not sure how specific that term is.)
Very insightful! Okay. As I now understand the `((,) a)` functor is a way of encoding the `Cons` constructor of `Listf`. I still however don't understand the purpose of the `r` in `FreeT ((,) a) m r`. IOW, if I were to declare `ListT m a` as a type alias to something based on `FreeT`, what would it be? Where would the `r` go? BTW, I've found a project, which takes [a different approach](https://github.com/tapuu/FreeList/blob/26481ec3303d9e51a19ee65b4b2c530a517c257c/src/Control/Monad/Trans/FreeList.hs#L21), which could be expressed with the following alias: type ListT m a = FreeT [] m a What will you say about that?
If you want more of this approach, and/or if you're in search for design metodology for haskell libraries, read Elliott's (very, very good) paper [Type class morphisms](http://conal.net/papers/type-class-morphisms/type-class-morphisms-long.pdf).
I didn't make it too far, but here are a few `FreeT/[]/Producer` isomorphisms http://lpaste.net/108183 ; then some applications to `STMContainers.Map` fwiw -- I'm not clear enough what you are looking for, so it's flailing a bit (and misread foldM as a right fold, I think). I really wanted to see how far Tekmo's `MonadPlus` idea would work, but didn't get far.
&gt; Where would the r go? It goes into the `Pure` case of `FreeF`. So, we have effectively an `r` annotating the `Nil` case, making this representation a bit larger than what we'd expect of a `ListT`. 
The non-exported methods could have default methods that you want to force people to use. 
Have you tried using lists instead of Seq? You can use the specialized IntSet instead of Set Int. 
`FreeT [] m a` is definitely not similar to `pipes`. You could call it `ListT`, I suppose, but it's not very similar to `[a]` at all. To compare `FreeT ((,) a) m r` to `ListT` we can do newtype ListT m a = ListT { enumerate :: FreeT ((,) a) m () } where choosing `()` as our return means that `Pure` acts like `Nil` again (just like how `Free ((,) a) ()` is isomorphic to `[a]`). We need a new monad instance, though, one such that `ListT Identity a` behaves like `[a]`.
Yes, lists performed even worse for me (but perhaps I didn't utilize them correctly?).
&gt; We need a new monad instance, though, one such that ListT Identity a behaves like [a]. Seems like many other instances as well, since they target the `r`, but we need to target `a`. So it looks like it'll make more sense to just implement it directly: newtype ListT m a = ListT (m (Maybe (a, ListT m a))) 
Yep, that's equivalent! Effectively, each inner monadic action needs to discover either the end of the list or a single value and the next stage.
A quick run with the profiler suggests all the time is spent in `compare` (i.e. in sorting): COST CENTRE MODULE %time %alloc compare Main 82.3 89.9 next_states Main 8.2 6.4 search Main 5.6 0.8 search.xs Main 2.9 2.5
I can also confirm that switching to `IntSet` brings performance into line with the Python implementation. Moreover, if one switches from the ordered `Data.Set` to unordered `Data.HashSet` (with a naive `Hashable State` instance) the Haskell implementation runs about five times faster, python 2.9 sec original Hs 4.2 sec IntSet 2.8 sec HashSet and IntSet 0.5 sec The final source can be found [here](https://gist.github.com/bgamari/33e1b89ec99552abe27c).
A few general observations/questions: - When you say slower, how much? 50%? 10x? Did you compile with optimizations enabled (-O2)? - Did you profile (-prof -fprof-auto, +RTS -p)? It seems you spent most of your time doing compares for the sort. - Data.Map and Data.Set are not hashed containers (Data.Hash[Map|Set]) - You should fix warnings (-Wall)
What an odd language quirk. Good to know
`containers` implements ordered containers (hence the `Ord` constaint on the element types of most operations). `unordered-containers` implements containers via hashing.
Thanks, it does work a lot faster now! It seems that using HashSet is impossible without IntSet. Am I correct? Is that because *Set Int* isn't hashable?
&gt; Having that without breaking the Haskell semantics is difficult That's an interesting statement. Could you please expand on that?
This is a set of libraries we've written at [Zalora](http://github.com/zalora). Some of you may have heard about what we're doing already but long story short we're replacing monolithic PHP systems with some sweet and modular webservices in haskell, where something written in one webservice could even be reused in another. Servant came out of this brainstorming and has been serving us very well so far, so please do not hesitate to try it out and send any feedback our way! Trailer: We rely significantly on TypeFamilies and DataKinds and our resources carry the list of operations (add, list, etc) they support at the type-level. You can define your own operations in just the same way the standard ones are defined. It really lets you forget about any kind of web-related boilerplate and focus on your models. It's entirely extensible.
Thanks!
If we assume that `Data.Set` is written is Haskell, then to use the built in hashing there would have to be a way to do the hashing, i.e., a function of type `hash :: a -&gt; Int`. But if such a (non-constant) function existed parametricity would break down. In order to restore parametricity, it would have to have type `hash :: (Hashable a) =&gt; a -&gt; Int`. But then we're back to what I said, your `State` type does not implement any such type class. So to use some kind of built in hash, Data.Set could no longer be written in Haskell. This is make it highly suspicious in my mind, but not only that. What would this magic built in hash function do? Haskell is a non-strict language so we can give the hash function an unevaluated thunk which must give the same hash value as when it has been evaluated. The only way (I know of) to do that would be to use the address of the thunk/value. It's doable but interacts very poorly with a copying GC. (If you want such beasts ghc provides `System.Mem.StableName`.) 
How would you hash a function, for example?
The `IntSet` isn't hashable either. The code describes how to hash a `State` by turning the `IntSet` into a list. You can do that for `Set` as well.
How did you get a breakdown into individual functions? I compiled and ran with ghc -fforce-recomp -prof -O2 perf.hs &amp;&amp; time ./perf +RTS -pa -RTS and I see 94% was spent in Main, but I don't get any further breakdown. What am I missing?
To nitpick: You can hash all functions to, e.g., 0. It would break down for functions anyway, because I assume you'll still need an `Eq` instance even if you were to use a built in hash. 
Use `-auto-all`.
Seems like a fun place to use dynamic programming via lazy arrays: courses :: Array Int (Courses, Credits) courses = fromList [f (getCourse i) | i &lt;- [0..maxCourse]] where f :: Course -&gt; (Courses, Credits) f c = (1 + sum (map (\p -&gt; fst (courses ! p)) (getPrereqs c) , getCredits c + sum (map (\p -&gt; snd (courses ! p)) (getPrereqs c)) Then you jus need to find the value with the lowest number of courses with credits above the min in the array. Courses and Credits could just be type aliases for Int most likely. The functions getCredits and getPrereqs use the input maps you stated to return the worth of the course and a list of the Int values for the prerequisite courses. Should be pretty damn fast because the values for each course are only ever calculated once. Another nice example of tying the knot. Apologies if the code is errant, I typed it on my iPhone late at night. The pairs can easily (and should be) transformed into being calculate in a single foldl loop. Finally, this assumes no cycles (I wouldn't put it past some universities to has cyclic prereqs though). Edit: Fixed formatting; difficult to see the results in alien blue
Or, if you want to stay away from ST, [unordered-containers](http://hackage.haskell.org/package/unordered-containers).
I know that, but they could have no default implementation, which is what I'm showing.
Close. 1. translation from literate XML to function calls is handled by `hsx2hs`. This is a straight desugaring process. It is useful to note that while it rewrites the literate XML into functions, it does not actually provide those functions or even give them type signatures. It doesn't specify what the types of the arguments or the resulting value are. In order for something to happen, you need to import another library that actually contains implementations of those functions. They could be functions that generate an `XML` type -- or they could just generate strings, javascript code, etc. 2. at the moment, the only library that provides an API compatible with `hsx2hs` is the hsp library. It provides a bunch of type classes like `XMLGen`, `EmbedAsChild` and `EmbedAsAttr` which support monadic XML generation. However, the details of the translation are dependent on which monad instance you are using. Things like the types of the string values `(StringType m)` and the type of the thing being generated `(XMLType m)` are dependent on which `m` we are talking about. In something like `HSPT XML`, we are dealing with `Text` values and generating the `XML` type. But in the `JMacroT` monad we are actually generating `JExpr`. The `XMLGenT` type is a bit of ugliness which exists entirely to make the type-checker happy. Specifically, we need to have this instance: instance (EmbedAsChild m c, m ~ n) =&gt; EmbedAsChild m (XMLGenT n c) where asChild m = asChild =&lt;&lt; m Which allows us to embed some generated XML inside a parent element. With out the `XMLGenT` we would just have: instance (EmbedAsChild m c, m ~ n) =&gt; EmbedAsChild m (n c) where asChild m = asChild =&lt;&lt; m Unfortunately, that instance is so general that it matches just about everything making it impossible to create other useful instances. :( `XMLGenT` is exactly isomorphic to `IdentityT`. To actually generate some XML, we need some monad that has an `XMLGen` instance. We could, for example, create an XMLGen instance for `ServerPartT`. In fact, that is what the `HSP.ServerPartT` module does. But, having to explicitly create an `XMLGen` instance for whatever monad you happen to be in is annoying. So, we introduce the `HSPT` monad -- which is almost exactly like the `IdentityT` monad. The only difference is that `HSPT` has an extra phantom parameter that allows use to specific the type for the xml value that is being generated. By default with supply the `XML` type which is based around lazy Text. But you could provide an, instance XMLGen (HPST MyXmlType) instead. 3. To convert the `XML` value to a `Response` we just use `toResponse` which uses the `ToMessage XML` instance. This does not actually require an monads. We have `ServerPartT` in there mostly because that is the base monad that `simpleHTTP` demands we use. But it doesn't directly contribute anything to the XML generation. So, in summary: 1. `XMLGenT` ugliness to make type checker happy about `EmbedAsChild` instances 2. `HSPT XML` - monad that actually has the `XMLGen` instances 3. `ServerPartT` - monad we are stuck inside because of Happstack, but doesn't contribute anything As for the type of `template` -- when we are using the `XMLGen` stuff and we have something like: [hsx| &lt;div&gt;some element&lt;/div&gt; |] the type is going to be something like: XMLGenT m (XMLType m) In order to be able to embed that fragment in some other code, the type-checker needs to be able to find an appropriate `EmbedAsChild` instance. By given `template` a very specific type for the `body` value -- it makes it dead simple to find an appropriate `EmbedAsChild` instance. I could be useful to provide a type alias like that. Though there are already so many types and type aliases I am not sure that more makes things better. One attempt to do that on a larger scale is the `happstack-foundation` library: http://hackage.haskell.org/package/happstack-foundation-0.5.5/docs/Happstack-Foundation.html Which also adds in type-safe URLs, acid-state, thread-local state, etc. Looking to pieces `hsx2hs` and `hsp` it is clear that most of the complexity comes from `hsp`. It would probably be nice to create an library like `hsx2hs-blaze` which just generates HTML via `blaze-html` and doesn't do anything fancy at all. One issue with hsp is that there is a lot to learn up-front -- but once you understand, it is pretty easy to use. Creating more options and asking people to decide what is right for them before they understand what is happening isn't really making things easier. I am thinking Happstack needs to provide fewer choices not more.
Awaiting the blog post eagerly, especially if there's a hackage release accompanying.
If they don't have a default implementation, the whole thing doesn't make much sense. But Haskell doesn't stop you from doing every nonsensical things. If you want a warning for it, ask for a ghc enhancement.
Fair enough, and I hope it goes well.
If only the difference was just 23%... I'd be very happy about that ;-)
Oh, and the sorting could be done this way, but I'm not sure it's any clearer: sortBy (flip compare `on` snd) It does read nicely, though.
Multi-line strings
Which automatically inserts cost-centers for your functions!
If only Haskell actually protected against segfaults... I'd be very happy about that ;-)
I'm fairly sure Haskell already has multiline strings: multiline = "line 1\ \line 2\n\ \line 3\n" No-one ever does that when you can just use `unlines`, though. 
In that case `const 0` is a perfectly good `hash :: a -&gt; Int` :)
Better known as "Conal".
As yes, thank you.
Presumably those `compare`s are from reading and writing the `Set`s and `Map`s.
My bad... Some of his [impostors](https://github.com/conal/MOOC-Course-Description/commit/4b7121ea0054e0589cabc0c79755a68d24db747d) are moving around. :)
Yeah those are horrible. Haskell needs triple quotes.
There's a bunch of quasiquoters for this. {-# LANGUAGE QuasiQuotes #-} import Text.RawString.QQ str :: String str = [r| foo bar baz |]
You can shorten that to: sortBy (flip (comparing snd))
It's a joke hinging on the fact that in German nouns are always capitalized, just like Haskell typenames and constructor names.
Well, i didn't get it. Maybe because i'm german. ;-)
Funnily, this is roughly how matrix transposition is defined in the [Applicative Programming with Effects](http://www.soi.city.ac.uk/~ross/papers/Applicative.pdf), and is actually just `sequenceA` for an appropriate definition of applicative for lists!
Will definitively check it out!
By the way, OCaml, a strict language, doesn't allow `let rec x = x`. But does allow a recursive list: `let rec x = 1 : x`.
Conan the denotarian. 
Every time someone posts this I notice that lecture 11 doesn't have the slides available.
Computer Scientist with out fear!
Silk [announced their REST framework](http://www.reddit.com/r/haskell/comments/29h32i/announcing_rest_a_haskell_rest_framework/) a while ago, and now this. Great stuff!
One day you will want to have two effects running in the same do block. There are a number of approaches to handling this. Monad transformers are one. Typical things + IO at the bottom with something else like... state, or reader. + Parsec over some underlying effect. + Some state to track over a trifecta parser (I like parsing) You can do transformers, you can do extensible effects, you can try flattening the effects. Maybe you'll never need it. That's fine too.
An ``Int`` is just a value, no relation to a function at all. During computation it's a value in a CPU register just like in C or many other language. The ``Integer`` type uses a GMP integer ( or similar library ) library representation as a variable-length array of digits. SPJ has an old, but [good paper](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.50.231&amp;rep=rep1&amp;type=pdf) where he talks about unboxed/boxed value representations of literals in Haskell.
Literals are literals. Variables are variables. When variables have a type where the principle connective (i.e. the top level type constructor) is `(-&gt;)` then they represent a function. Finally, *function literals* are introduced by `(\... -&gt; ...)` syntax. So, if we're being totally strict then *only* `(\... -&gt; ...)` is a function, but some variables have function types. Everything else isn't even kind of a function. But if we're being less strict then it's fun to call `id` a function instead of a "variable with a function type". And if we're being really loose then it's kind of neat to notice that all kinds of syntactic forms look like they could participate in application... so they're all "kind of" functions, right? --- Personally, I think the "less strict" style is best.
One note about the slides that may not be obvious: the `μ` symbol says "I am giving a *representation* a *meaning*. So `μ :: Image -&gt; (Loc -&gt; Color)` should be understood as saying, given a representation of an Image, the semantic meaning of this Image representation is a function from Locations to Colors. (He had commented that it was a bit less than ideal because it was hard to tell the difference between the two in the above notation - because the first thing (`Image`) is a type, and the second thing (`Loc -&gt; Color`) is a mathematical idea (that doesn't even have to be computable per-se, just well-formed - these two things might have been clearer to see if for example they were in different colors). But I was still a bit confused on the whole thing - I don't believe it implies that any of this code is intended to be executed - it's just a description of the model. And it was a bit confusing when μ appears on the right hand side of some of the equalities, but not others.. I think when it does it's because anywhere a representation like Image appears it has to be preceded by a μ. the type classes morphisms paper uses a bracket notation that makes the scope of the meaning function clearer I think.
Which books? Might's list is theory texts, not including Haskell applications.
http://en.m.wikipedia.org/wiki/Homotopy_type_theory ?
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Homotopy type theory**](https://en.wikipedia.org/wiki/Homotopy%20type%20theory): [](#sfw) --- &gt; &gt;In [mathematical logic](https://en.wikipedia.org/wiki/Mathematical_logic) and [computer science](https://en.wikipedia.org/wiki/Computer_science), __homotopy type theory__ (__HoTT__) attempts to give an account of the [semantics](https://en.wikipedia.org/wiki/Semantics_(computer_science\)) of [intensional type theory](https://en.wikipedia.org/wiki/Intensional_type_theory) using the framework of (abstract) [homotopy theory](https://en.wikipedia.org/wiki/Homotopy_theory), in particular [Quillen model categories](https://en.wikipedia.org/wiki/Quillen_model_category) and [weak factorization systems](https://en.wikipedia.org/wiki/Weak_factorization_system). Conversely, intensional type theory forms a logic ([internal language](https://en.wikipedia.org/wiki/Internal_language)) for homotopy theory. &gt; --- ^Interesting: [^Type ^theory](https://en.wikipedia.org/wiki/Type_theory) ^| [^Intuitionistic ^type ^theory](https://en.wikipedia.org/wiki/Intuitionistic_type_theory) ^| [^Univalence ^axiom](https://en.wikipedia.org/wiki/Univalence_axiom) ^| [^Set ^theory](https://en.wikipedia.org/wiki/Set_theory) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cj8wawu) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cj8wawu)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
Years ago, when SPJ said that "the next Haskell will be strict", he didn't realize tibbe would accept the challenge!
How much mileage would one get from creating two flavors of some existing Data.Foo module, one with Strict and one without?
Refactoring manually, or using editor/IDE macro? The latter should be updated to do the right thing.
A simple tool or editor plugin could toggle between the three modes of expression.
I meant more like a Banach space of the generating functions of types.
I assume for being too literal with the word "brain"
The primary things coming to mind: - Calling or being called by a C API. You have to deal with raw memory and there's no general mechanism available that can guarantee you're not accessing deallocated memory, have a wrong buffer size etc. - Any high performance code dealing with arrays / raw memory that can't bear the cost of having a bounds check on every single memory access. That's not just indexing, but you might need to write a Storable instance to have a record stored tightly in an array. There's nothing really preventing you from screwing up there. - A fault in the implementation of the memory-safe primitives you're using. In the end, most safe data structures like bytestrings use raw memory and plenty of unsafe* operations under the hood. There's no magic type system feature ensuring memory accesses to be valid, we simply have to rely on their bounds checking / memory management logic to be correct.
I think you got the right idea, but just to give you a few more examples: - A computation that might need to abort early and also read from a file (MaybeT/ContT over IO) - A computation that can fail with an error message while also modifying a mutable array (ExceptT over ST) - Having access to some global state while accumulating log data (ReaderT over WriterT) - A conduit pipeline with some shared state between the stages (StateT over ConduitM)
You need to give it a fake sha256 on the command line in your case. Just add --sha256 &lt;fake hash&gt; and when you try the resulting nix expression it will print the calculated hash for you.
I know but damn man, I only have so much time to try out cool new web service libraries.
The choice is not necessarily between mutable and immutable, as a HAMT based container (Data.HashMap) would've been another option. Other languages such as C++ also have their default map container implemented as a O(log n) binary tree instead of an O(1) hashmap, so it's really not that odd.
Thanks. The generated default.nix still looks like it requires hand-editing however. (Doesn't look like the example default.nix /u/ocharles presents.) $ nix-shell error: cannot auto-call a function that has an argument without a default value (`cabal') $ nix-shell -A cabal error: cannot auto-call a function that has an argument without a default value (`cabal') $ nix-shell -A haskellPackage.cabal error: cannot auto-call a function that has an argument without a default value (`cabal') I may be better off writing a script to generate default.nix and my cabal file from the same data.
You need to provide args. Take a look at shell.nix: https://github.com/ocharles/engine.io/blob/master/engine-io/shell.nix
The generated default.nix is suitable for inclusion in nixpkgs but not directly usable from nix-shell if not through nixpkgs. It's a slight difference you'll get at some point. Try: nix-shell -E '(import &lt;nixpkgs&gt; {}).haskellPackages.callPackage ./default.nix {}'
I think one of the main reasons why Haskell one-liners are notoriously unreadable is because of direction reversals within one line. The part (==) &lt;$&gt; id &lt;*&gt; reverse is read left to right, but the whole line is read right to left because of the `(.)`'s, yet map toLower and filter isAlpha are read left to right once again! It would be so much better to just reject the composition operators and write consistently in one direction: filter isAlpha &gt;&gt;&gt; map toLower &gt;&gt;&gt; ((==) &lt;$&gt; id &lt;*&gt; reverse)
I don't know drone well. My colleague Alexander Thiemann evaluated it. As I understand Drone always uses a docker base image, clones the repository into that image and runs the build scripts. It doesn't build the base image and if running the build scripts takes a long time you can keep a directory between builds... but that will result in unclean builds that could possibly fail after a clean. With Frozone (which is based on our Dockercook tool) you can split the build process into many Dockerfiles that only see parts of the repository. Because they only have access to some files of the repository the result of the build only depends on those files. For all builds where these files don't differ the cached build result can be used. We split our build into these steps 1. Ubuntu base image 2. apt-get install core dependencies (PostgreSQL, C libraries, etc) 3. install Haskell compiler and needed libraries 4. perform build preparations (code generation) 5. build our base library code 6. run unit tests on base library 7. build core components 8. run unit tests on core components 9. build everything else and run integration tests If I change a non-core component Frozone sees that everything else stayed the same and starts the build process at step 9 - everything else is cached. I get my test results very fast because only a small fraction of the code needs to be built and tested. If I want to switch to a new PostgreSQL version I change the Dockerfile at step 2 and everything following it needs to be rebuilt. Frozone manages caching in a way that ensures that both branches have everything cached that was built for them. So if I commit another patch to my new PostgreSQL branch fixing a unit test from step 9 I have a fast build. If a different developer working in the main branch fixes a bug his build will also start at step 9 - but using the cached results from the main branch with the old PostgreSQL installed. By writing Frozone build descriptions in your project you can decide how many build steps you want to have. Every build step result is cached. More steps means more caching and faster builds. 
Clicking the &gt;&gt;&gt; button gives a popup filled with "*** Exception: Prelude.undefined". I can't tell if it's a joke or an actual error.
Thanks. That downloads dependencies (the first time) and starts a shell for me. But I can't run cabal or ghci from that shell. What am I missing? Edit: Oh, cabal2nix doesn't include cabal-install...
No, literals are not functions. I can't speak for all compilers, but none that I know of treat literals as functions. 
You might be interested by [Paeno numbers](http://www.haskell.org/haskellwiki/Peano_numbers). You can treat function as data and data as function. But it would be rather ineffective, so it is not implemented that way. Also, it all depends on your definition of function. For recursive data, there are recursive functions. It is like worms and butterflies.
All functions in Haskell have exactly 1 argument, and strings and integers are not functions.
This is a good effort, but as far as i can understand the developer still has to write boilerplate code for the Create, Read, Update, Delete operations. As far as [i can see](https://github.com/silkapp/rest/tree/master/rest-example/example-api) also [rest](http://silkapp.github.io/rest/) from Silk requires all the boilerplate operations to be rewritten for every resource. This is behind the results of modern frameworks like [Python Eve](http://python-eve.org/) and [Node Sails](http://sailsjs.org/), which generate the basic operations automatically and provide a way to customise. Sails also provides Socket IO endpoints for free corresponding to every REST endpoint
This sounds like some very useful instructions if one were interested to try to convert GHC's build-system to Shake. Especially the part about leaving the previous build-system untouched and just add the new Shake-based system as an optional overlay.
No in servant an operation is written just once and you can use it on all the types you want. What you write for your types has no boilerplate in it. its the bare minimum that you cant avoid, even if you were building a webservice manually you'd write code that is equivalent.
That isn't a coincidence - the motivation for writing this up is GHC. 
I'll use this little space here to express why this has been released despite the fact that Silk released *rest* a few weeks ago. First, when I first heard about *rest* at Zurihac, I already was thinking about the core ideas of *servant* and found out there were some interesting differences between the two approaches. I really felt that since they quite deeply explored their approach, I should do the same with mine just for the sake of seeing where that leads, so that's the second reason and here's the result. I'm also thinking about generating the corresponding client JS code automatically for an operation as well as adding some machinery to generate pretty API docs -- thanks to *rest* for the inspiration -- no idea about when that will land in servant yet though.
Does this mean that GHC will soon have a Shake build-system ? That would be awesome :-)
Hi All, thanks for the truly helpful comments. As assumed by http://www.reddit.com/user/c_wraith, this was an "assessment item" that I did in my programming course about 2 years ago. In short, I failed that assessment, hence upon revisiting my notes on the course led me to attempt at solving it again; post-studies. I am honestly fascinated by how haskell arguments are structured, and hence am just trying to gain more understanding into this old yet useful lingo. And where else is a better place to post my confusions/questions then to such a forum where I am guaranteed help from people with experience like you all. In saying that, please note that I still have a lot more questions that I'd like clarity on but will only post them after "Mr. Google" and other sources fail to assist me. :)
In my experiences the difference is actually smaller. It all depends on what kind of code you are writing, though. I often make some minor efforts to reduce allocation, but it seems a lot of people (reasonably) don't even try.
Thanks for the D implementation. However, I fail to see how the Python version is "better". All you did was change the dictionaries to lists, which deviates from the question.
Yes, all the code I've shown doesn't help answer your original questions. The Python code I've shown solves the search problem without __ eq __ and __ hash __, and stores less stuff in closed_states. And the data structures like course_credits are now simpler, they are a list of frozensets instead of a dict. Often it's better to use the simplest data structure that does the job. The sorting is done only once. The resulting code is shorter, simpler, uses less ram, so in my opinion it's better.
I believe that at some point in the past the compiler they use for the [Reduceron](http://www.cs.york.ac.uk/fp/reduceron/) actually did something like that. But I didn't find anything interesting in a quick glance over the current memos on the site.
The Reduceron converted ADTs to a Scott encoding, but not Integer literals. 
It would indeed be awesome. But... it would add yet one more barrier to entry for bootstrapping.
GHC already needs GHC to bootstrap, so what's the additional dependency?
The implementation of readFile is clearly wrong here. It should not be relying on the file size being either true or stable, but rather as a hint. Must remember to fix.
The Reduceron is loosely based on my BWM machine, and it treated integer literals specially. All data other data types were Scott encoded (but I didn't know it was called that back then). 
That's so entirely unlike anything I've experience with Haskell, I simply can't imagining Haskell code getting anywhere near that fast for things that are not completely I/O bound. For any kind of numeric code / HPC (image processing, ray tracing, physical simulations, data compression, machine learning etc.) I usually find that my idiomatic Haskell code is 20-100x (!!!) slower than naive (not super tuned with SIMD intrinsics) C/C++ code. Getting it within 'acceptable' 2-5x slower usually requires going to extreme lengths, many hours/days of optimization, profiling, unsafe primitives, often writing code that is actually more verbose and less safe than equivalent C/C++ code. Just a few random anecdotes: - I found on multiple occasions that the most optimized repa code I can produce running on 4 cores can't beat completely straightforward C on a single core - Using very idiomatic 'forM_ [0..n]' expressions can mean a slowdown of 10x (https://ghc.haskell.org/trac/ghc/ticket/8763) - I recently asked about how to efficiently implement byte level processing for a compression program with conduits (http://www.reddit.com/r/haskell/comments/29nvsx/how_to_get_good_performance_when_processing/). The idiomatic solution was orders of magnitudes to slow. 'Fast' Builders were orders of magnitudes to slow. The only solution I could come up with basically uses ugly, low-level memory primitives and does not preserve the nice await/yield semantics of the conduit monad. - Monad transformers. They often have enormous overhead. Just varying the placement of the liftIO in my last anecdote made a difference of 40x (!). Basically, you can't do performance critical things like looking at each element of a large vector while in a monad transformer stack and expect performance to be anywhere near reasonable. - We've all seen that qsort example. The performance of using and allocating many temporary singly-linked lists with 16b pointer + GC overhead makes this perform way, way worse than a real qsort. Implementing a real, in-place qsort doesn't look any better than the C code. And that's actually a good thing, cause often the Haskell code ends up looking worse in such comparisons. - Think of a simple lens example, like the pong one, where we use a ReaderT+StateT and lenses to simulate imperative style class-state manipulation, i.e. "m_interation += 1; m_frame_idx += 1". This code would compile to two instructions working on tightly packed member data in C++, but in Haskell it would involve something like this: chase down the pointer to the old iteration count, allocate a new interation count, GC the old one, allocate a new state record, copy all the pointers from the old one, GC the old one, execute all the monadic bind unwrapping/rewrapping stuff, repeat for the 2nd variable, make some virtual typeclass calls (depends on how much inlining happened, I guess) etc. This is going to be like an order of magnitude slower, at least. Of course this is not how you'd write an inner loop in Haskell, just illustrating how much overhead there often is in such idiomatic code. - I wrote a 6502 emulator. Even after extensive tuning, inlining and specialization the overhead was too large to bring this within the same order of magnitude as a naive/basic C/C++ emulator I found that trying to match C++ speed generally requires writing extremely ugly C-in-Haskell type code, and then certainly still not getting within 23%. It's often things like trying to store a tree in a tightly packed array to avoid using 64bit pointers and scattering the nodes all over the heap. It's trivial in C++, but in Haskell I first have to manually write a Storable instance. And the implement my own equivalent of a growing std::vector. It's actually more work than doing it in C++, more error prone, and still not getting the same performance. This is my biggest 'Why I can't use Haskell for task XYZ'. Id really like to use Haskell for virtually everything, but I often can't. I find that often when I write the skeleton of my algorithm (getting data into Haskell, make a pass over it, get data out of Haskell) it's already way to slow to be usable. Before any actual computation has been done, just moving stuff around, monad transformer overhead, GC/allocations, etc. Then I spend 2-3 days trying to optimize it, and give up. I often catch myself thinking 'I literally could've implemented that algorithm in asm by now'. I'd love to learn more about writing fast Haskell code and what I could do to improve the situation, but right now, I don't see a sane way of getting within &lt;5x of C code for the things I'd like to do without actually losing most/all benefits (readability, conciseness, safety, elegance, productivity, flexibility) that Haskell normally provides.
Overloaded literals are a special case of polymorphic constants (like `minBound` and `mempty`). You can think of any such constant as being a function and indeed their constraints may be reified into arguments to an actual function in the compiler's internal representation — that doesn't mean they *are* functions however.
I still do most of my Haskell programming manually. But, yes, I believe the fix-up is a mechanical transformation.
Could someone file a bug (I'm on my phone)?
Good call, I just [opened an issue](https://github.com/haskell/bytestring/issues/26).
Actually, you'll find that the stuctures in the music-suite are quite related to the ones in a DAW. There are the Voice/Track/Score types, which are basically sophisticated version of MIDI tracks, Segment/Behavior/Reactive which could represent fade curves, and so on. Anything related to modelling the music is game for music-suite. However for a full DAW, you also need an efficient real-time DSP engine, and a GUI, both which are out of scope for this project.
~~Issue filed: https://github.com/haskell/bytestring/issues/27~~ Michael filed an issue a few minutes before me: https://github.com/haskell/bytestring/issues/26
It has now been fixed.
Alas, I'm unlikely to do that, and perhaps no one ever will. I'm sharing what I know to make it easier if someone does decide to do it, but it would still be a big undertaking. 
The benefit of writing C in Haskell is that you can call it from within Haskell without either (A) blocking the RTS (if you import C code using `unsafe`) or (B) incurring a few hundred nanosecond overhead (if you import C code using `safe`). I'm okay with investing more time extra time writing the performance intensive part of my code in Haskell, even if verbosely, if it means that the rest of my code can be written in high-level Haskell code. Also, if you have an algorithm that takes more than a few microseconds to complete, then you can just implement it in C and import it into Haskell using the `safe` FFI. The overhead of the `safe` call to C-land will then be a negligible overhead.
Look at the type of [`withHTTP`](https://hackage.haskell.org/package/pipes-http-1.0.1/docs/Pipes-HTTP.html#v:withHTTP): withHTTP :: Request -&gt; Manager -&gt; (Response (Producer ByteString IO ()) -&gt; IO a) -&gt; IO a It's obvious that something strange is going on. Why would `withHTTP` needlessly use continuation passing style? If it could have the following type instead, it probably would: withHTTP :: Request -&gt; Manager -&gt; IO (Response (Producer ByteString IO ())) Therefore, I conclude that `withHTTP` (and probably `withManager` also) behave like `withFile`, in that all the IO must happen inside of its body. If you try to use the body to return the handle and then use the handle, you get an exception: &gt; import System.IO &gt; h &lt;- withFile "foo" WriteMode return &gt; hPutStrLn h "hello" *** Exception: foo: hPutStr: illegal operation (handle is closed) So I assume that returning a producer and then using it must be similarly incorrect.
Hmmm... Yes this makes sense. I suppose the point of pipes (or one of them) *is* to control where resources are acquired and released, isn't it? Thanks.
Sure, we're in complete agreement there. I thought Haskell's FFI is actually rather nice and I used quite a bit. My rant is really just about how poor idiomatic Haskell performs for the kind of numeric/HPC code that I often want to write, and how poor Haskell often supports dealing with mutable, densely packed data like the mutable records stored in an array example I gave. If I can isolate the performance sensitive part in a few functions, the Haskell + C approach works very well. But if I imagine writing a high-end game or a Monte-Carlo path tracer or something entirely in Haskell, I don't think this kind of split would be very successful. There's just too much of the core algorithms and data structures that I'd need to keep on the C side. For instance, the 6502 emulator I wrote, I think I'd need to pretty much rewrite the entire emulation code in C or simplify the code to only run in plain IO/ST, use no fancy abstractions, nothing that can't be completely inlined away etc. I did not find any other way to get it even within the same order of magnitude as straightforward C code.
My bad then, i will have to read the docs better
+1 one http-conduit + aeson/attoparsec. Excel has a fairly simple C API that you should be able to write an FFI wrapper around.
You are not the only person I've seen make these sorts of complaints, so I'm sure there's something to it, but I also see a lot of evidence that you write code in styles that I often don't. I tend to put a lot of effort into using the right data structures, I don't really use lens (not due to performance, but just because I don't really like the style), I think the common quicksort example is awful, etc. I see no reason that well written Repa code should be as slow as in your experiences unless you've missed one of the important performance tips in the documentation (in which case I expect awful performance). I've even beaten hand optimized C on non-io bound realistic applications, on many occasions with just sequential Haskell code, if such an anecdote is worth anything to you.
I actually wrote like 3-4 different versions of that repa code, wrote my own version based on async, added inlining and strictness, compiled with the LLVM backend, tweaked compiler options and still wasn't able to even get anywhere near naive C code. I learned a bit more about Haskell since then, maybe I'd do better at it today. I just started using lenses like a week ago, so it's not that ;-) I mean, forget about conduits, lenses, monad transformers and immutable data structures. I can't even match C performance if I forgo all of the things that make Haskell nice for me and just write really basic, mutable imperative-style code. I absolutely can't figure out how I'd take an optimized piece of numeric C code like a video/image codec, a realtime raytracing kernel, a fluid simulation package, a chess engine or something like that, rewrite it in Haskell and get anywhere near the same performance. I'd be impressed if the result is just 3x slower after lots of optimization work. I'd be dying to learn how to do that, especially without actually making my code be even more verbose and cryptic than the optimized C version is. If somebody would write a book about 'HPC with Haskell' that actually delivered on performance, they'd have my 50 bucks for sure...
&gt; you can do extensible effects Only [7 libraries use extensible-effects](http://packdeps.haskellers.com/reverse/extensible-effects), whereas [1112 use transformers](http://packdeps.haskellers.com/reverse/transformers). Why is this so? What's wrong with extensible-effects?
e-e is 1 year old. mtl is 8 (and mtl evolved into transformers 5 years ago)
This is actually a more general idiom than `pipes`. You can see another example of this in `System.IO.withFile`: withFile :: FilePath -&gt; IOMode -&gt; (Handle -&gt; IO r) -&gt; IO r You give `withFile` the file to open and whether to read/write/append the file and then it gives you access to a `Handle` within the scope of the continuation. The typical use will be something like: import System.IO withFile "infile.txt" ReadMode $ \handleIn -&gt; do withFile "outfile.txt" WriteMode $ \handleOut -&gt; do -- Here you might copy from `handleIn` to `handleOut` ... Once the `withFile` scope is closed the `Handle` is invalid, so you're not supposed to return it from the block and use it outside of the block. Technically, nothing prevents you from doing this, so it's really more of a convention and it's not enforced by the type system. You might wonder why `withFile` does things in this way at all, if it does not enforce `Handle` safety. The reason why is that it does enforce exception safety and it's very similar to a `using` statement from C#.
Also, the way you would change your code to propagate the safety is to change it to: downloadRedditExample :: (Producer ByteString IO () -&gt; IO r) -&gt; IO r downloadRedditExample k = do req &lt;- parseUrl "http://www.reddit.com/r/haskell.json" withManager defaultManagerSettings $ \m -&gt; withHTTP req m (k . responseBody) In fact, this idiom of `(a -&gt; IO r) -&gt; IO r` is so common that there is a monad for it, the continuation monad: newtype ContT r m a = ContT { runContT :: (a -&gt; IO r) -&gt; IO r } So you could actually write nested `with`s as ordinary binds in this monad: handleIn &lt;- ContT $ withFile "inFile.txt" ReadMode handleOut &lt;- ContT $ withFile "outFile.txt" WriteMode -- Here you might copy from `handleIn` to `handleOut` ... In fact, `forall r . ContT r IO a` is so common that the `mvc` package has an [even further specialized type](https://hackage.haskell.org/package/mvc-1.0.1/docs/MVC.html#t:Managed) for this purpose, called `Managed`. newtype Managed r = Managed { with :: forall x . (r -&gt; IO x) -&gt; IO x } I've been considering factoring this type out into its own tiny package because it's incredibly useful. Then the type of your function would just be: downloadRedditExample :: Managed (Producer ByteString IO ()) downloadRedditExample = Managed $ \k -&gt; do req &lt;- parseUrl "http://www.reddit.com/r/haskell.json" withManager defaultManagerSettings $ \m -&gt; withHTTP req m (k . responseBody)
Thanks :) Could you elaborate on what you mean by 'squarely in the development process'? Did you see that `Web.Herringbone` exports `precompile :: Herringbone -&gt; IO &lt;list of errors, if any&gt;` - does that satisfy the requirement of processing everything before production? I guess Herringbone is most useful when you're using languages that compile to JavaScript which are implemented in Haskell, so that you only need to `cabal install --only-dependencies` and not worry about versions getting out of sync, or `$PATH`, and so on. The `Web.Herringbone.Preprocessor.StdIO` module lets you create a preprocessor very easily from a command line tool that can read input via stdin and print it to stdout, maybe that should also feature in the README...
aka Codensity IO r
I don't know your excel workflow, but this is really all you can tell you without specifics: I suppose you have a url where your data is stored, grab it raw as it is and dump it to a file using http-conduit, consult the hackage documentation for details. If you managed to do that, you need to parse the data for usage i.e. convert the external encoding to an internal encoding you can work with. Find out how the data is encoded externally e.g. xml/json/zip/base64/... Define your datatypes which should represent your data in haskell, the internal encoding. Then use a library depending on the external encoding to do the actual conversion. That would be aeson for json, xml-conduit for xml or parsec/attoparsec for custom stuff, ect.
You still can't cause a data race, you will get a immutable object out of it. Or you need to lock it to get a mutable reference. These are all implemented in unsafe block. So you need to make sure that everything that is expose from the block is safe.
To offer a different anecdote, the naive repa code I wrote performed slightly better than the naive c code I wrote. Furthermore, the repa code parallelized decently across multiple cores.
Yes, it seems to be fixed on github. But [this](https://github.com/music-suite/music-score/commit/89b5b406486ea9938cc54baa29d0a06ed663aa1f) commit is not yet available on hackage.
Well, MSYS or Cygwin would still be needed for any FFI, and GHC's runtime is written in C and C--. If it were rewritten to pure C-- then it might be possible. Not sure how you'd deal with needing libgmp and ncurses though.
Thanks again. I agree that it makes more sense to ship compiled/minified/optimized files to production. I will update that example. Regarding preprocessor composition - a preprocessor is currently `ByteString -&gt; IO ByteString`(there's a bit more to it than that, but it'll do for now) so I could probably add a `Monoid` instance, so you can do`purescript &lt;&gt; minify` or something. I'll have a think about that. I'll also think more about differing dev / production builds. :)
Not much better. For a language that prides itself on terse syntax, that probably shouldn't have been accepted.
I'd seriously love to see any piece of code like this!
Yeah I've defined that type too (with another name) in one of my projects, under a different name but it's the same idea. Maybe it would be worth putting into its own package :-)
See my `shell.nix` above, and also the sibling `default.nix`. It should give you enough material to get yourself something equivalent. Feel free to come along to #nixos on FreeNode if you need more help :)
What happens if you want to calculate `s 3`? You need to calculate `s 0`, `s 1`, and `s 2`. What do you need for `s 2`? You need to calculate `s 0`, `s 1`, **again**. You end up with a ridicolous high number of evaluations. What if you instead [memoize](http://www.haskell.org/haskellwiki/Memoization) the evaluated numbers? s r = sList !! r sList = map sWorker [0..] where sWorker 0 = 0.2 -- replace with your value sWorker r = (1 - p) * sum [g i * (s (r - i)) | i &lt;-[1..r]] -- call to s This will be much faster, since every value needs to be evaluated only once and can then be retrieved from the list.
&gt; an effort to build an automated forex trading system based on the MACD indicator As someone who has written trading systems for a living - I hope you don't intend to trade real money with this.
Also, maybe you can avoid the linear indexing of the memo list by doing this: http://stackoverflow.com/a/3209189 
There is still a dependency on a tool which determines the target machine's characteristics, e.g. word and pointer sizes, etc. For cross-compilation this is simply a cross-C-compiler (+`nm`) otherwise autoconfig. Ideally autoconfig should be scrapped first (and obtain the infos just like in the cross-compilation case). Are there "feature checking" facilities in Shake already? These could simply be dependencies like config/wordsize: obtainWordSize &gt; $@ or such.
Care to elaborate? While I'm at it: Can you recommend books/articles on automated trading systems?
I really don't believe there's much benefit in reinventing the `autoconf` wheel (or rather: replicating the countless hours of work that went into [GNU Autoconf](https://www.gnu.org/software/autoconf/) is wasted effort imho). What I'm after is more modest: Just replace `make` with `shake`. I think there's at least some benefit here, as GHC needs to handle a more complex situation than `make` was designed to handle, and which `shake` promises to make super simple...
I can only agree with you, the jpeg decoder in Juicy.Pixels was a pain to get relatively efficient in Haskell, and only ran around 2x the times of jpeglib (not jpegturbo). It boiled down to a c like style of writing which is not really pleasant to practice in Haskell.
Or just do a `sWorker r = ((p-1) *) . sum . zipWith (*) sList $ map g [r, r-1 .. 1]` to avoid any indexing time whatsoever. That makes calculating s_i only take O(i) .
I asked, because in your comment, the "latter" and "the second interpretation" seem to mean the same thing, which is the "avoid (popularity at all costs)", so I was confused.
You can certainly write such a dependency in Shake. Shake doesn't yet contain an obtainWordSize function, but I think there should probably be a package on top of Shake (shake-autotools?) that has all those things defined. I don't have much experience with writing those kind of things, so will leave it to others.
If I wanted a shell against say GHC 7.6.3, what would change?
Maybe the post about [lazy dynamic programming](http://www.reddit.com/r/haskell/comments/26jh2w/lazy_dynamic_programming/) can help?
As noted in the Stack Overflow answer cited below, this solution will incur a linear indexing time due to the use of `(!!)`. Using the logarithmic time lookup proposed in the answer will likely improve things substantially.
If you are on Linux you can use the `time` command.
I wrote on ghc: :set +s
Maybe: nix-shell -E '(import &lt;nixpkgs&gt; {}).haskellPackages_ghc763.callPackage ./default.nix {}' 
Hm, interesting, when I tried `s 100`, it took less than a second in ghci.
Why are you using `Float` instead of `Double`? 
The little known [mmtl](http://hackage.haskell.org/package/mmtl) package. Essential feature: no quadratic blowup in the number of MonadTrans instances. Transformers uniformly transform other monads.
I completely agree with this. If we attempt to replace the `autoconf` functionality we rely on in GHC today with Shake, we're just going to end up reinventing autoconf, and we will - believe it or not - make an even *shittier version of autoconf*. Nobody will win and it'll just add more maintenance burden for us. People all over the world use autoconf for a reason, and that's because it has had countless hours of effort put in to deal with bizarre cases and environments correctly (yes, including things like cross compilation). People use GHC in 'bizarre' environments (read: atypical machine configuration). There's simply no way in hell we'll get all the nonsense to support these sanely correct, and it will make any maintainer's life in that realm about a million times harder.
What implementation did you use?
Well, the accuracy seems slightly off, but using `Vector.Unboxed` and `n = 1000` I get the following: &gt; time -p ./testalgo 1.8301515387155585e234 real 0.00 user 0.00 sys 0.00 I had to use Doubles though, as precision suffers a bit for some reason even at n = 20 import qualified Data.Vector.Unboxed as V probs :: Int -&gt; Double -&gt; [Double] -&gt; Double probs n p gs = (V.! n) $ V.constructN (n+1) f where f partialvec | V.null partialvec = p | otherwise = let m = V.length partialvec indexed = V.zip (V.enumFromStepN (m - 1) (-1) m) partialvec in (1 - p) * (V.sum . V.map (\(i, x) -&gt; gvec V.!(i) * x) $ indexed) gvec = V.fromList gs main = do putStrLn . show $ probs 1000 0.7 [1..2000] 
You're welcome!
A quick glance at the API seems to indicate little difference from `mtl` (intensionally so). What about this API is better?
it's less efficient unfortunately. other than that, it's pretty nice. there are some vocal critics though. look for previous reddit discussions about it. there a a few.
I'm pretty sure the HAMT research wasn't done when Data.Set and Data.Map were written, and that they have a different API.
No that it matters right now, but I'm fairly sure `transformers` works with any Haskell 2010 compiler, but `Eff` looks like it needs at least RankNTypes. Does it work / can it be used without PolyKinds?
mtl has worked for me. What about it falls flat?
Does it have to be fake? Or can I run `cabal sdist` and then sha256sum the resulting compressed tarball?
Complaints I have heard are that it 1) encourages monolithic monads for the whole program, 2) requires judicious use of `lift` and 3) can encounter problems when you have multiple monads of the same type in the hierarchy.
That is possible, but I am giving them the benefit of the doubt.
Haskell 2010 does, as long as you don't use anything out of Foreign.* and don't use the FFI. These "crazy" packages (like ByteString or Text) that need to use the FFI or unsafePerform IO -- those are the problem. /s :P
The only disadvantage of this is that if OP is running these in ghci (and it seems that he is), if `probs` is called with a different `n`, all values must be recomputed every time. With the list implementation, calling `probs 5001 ...` after `probs 5000 ...` should be instantaneous. Also `print = putStrLn . show`
What do you mean by "generating a function"? You can `seq` a function (or if you prefer, an expression with function type). It means evaluating that expression until it is of the form `\x -&gt; ...`. It sounds like you have a list of partially-applied functions and you want to arrange things so that `seq`ing the elements of your list will evaluate the supplied arguments, so that you can apply a parallel evaluation strategy to the list. If your function is `f :: A -&gt; B -&gt; C -&gt; R` and you want to provide the first two arguments, you can achieve this by strictly applying your function: [(f $! a1) $! b1, (f $! a2) $! b2, ...] 
well, the list implementation was too slow according to OP. However, using a Map to index would be a good solution, depending on use case edit: whoop, just saw the implementation without indexing, which is very cool. I thought somehow he wanted to supply a list with observed values as gs, in which case that wouldn't work..
The Extensible-effect paper claims it's more efficient than stacks of monads. I couldn't figure out where the inefficiency should come from. ([past discussions](https://pay.reddit.com/r/haskell/search?q=extensible+effects&amp;restrict_sr=on&amp;sort=relevance&amp;t=all), EDIT: [most enlightening comment](https://pay.reddit.com/r/haskell/comments/1j9n5y/extensible_effects_an_alternative_to_monad/cbcwbsa))
A question on this. What do you miss (if anything), in term of standard monad functions, when you use extensible-effects over mtl in your code?
I don't use extensible-effects because a) I know transformers well and b) it doesn't work as well as the idris thing. It's mostly a matter of easy composability, even if you use transformers you end up with one stacking order, and generally "one big monad to rule them all", even if you composed it.
Yes, that worked. Thanks 
Probably you are looking for Erik Meijer's [Introduction to Functional Programming](https://www.edx.org/course/delftx/delftx-fp101x-introduction-functional-2126) course using Haskell which is starting from October.
It's great to see this written down somewhere. I often mention it in passing while talking about the advantages of explicit side effects, now I have a link :)
Slide 34 seems to advocate Haskell programmers stealing C programmers skin and wearing it as a "kernel suit".
you are right. I was looking for Erik Meijer. Thank you very much &lt;3
I admit I just skimmed over it, but I really like the enthusiasm and sense of wonder you keep up during the article. It's exactly how I felt when discovering that things like 'statements' and 'control flow operations' are just thin syntactic sugar over a bunch of library functions rather than fundamental, magic, build-in features like in most other languages.
&gt;s is basically composing functors of free monads, and then using the free monad of the new functor, correct? Huh. I've heard of extensible effects, but never figured out how they worked until reading this sentence. This makes perfect sense. Neat. 
When you come from an imperative background one of the most difficult things to shake off when you switch to haskell is the idea that something happens after a line of code has been evaluated. When I read imperative code I go line by line and feel that safety of knowing the new state inside the computer that exists because of that line. In haskell this is doubly wrong. Firstly because, like the article states, each line is not a command, it's just a part of a recipe that could be executed at some point. And secondly, because of lazy evaluation it might never happen at all !
I personally really like [this approach](http://ro-che.info/docs/2014-06-14-extensible-effects.html) by /u/roche. It's admittedly not a "perfect" API, but it's a clear win over mtl and doesn't incur the quite punishing overhead (plus the the API complexity) of extensible effects. 
I think 1) is just lazy programmers (myself included) being lazy. Just throw everything in to one context and you stop having to think about what context you are in. Works great until you need refactor or divide responsibilities. For 2) `mtl` "improves" `transformers` by not having type classes automatically insert calls to `lift`. Because of that, the only `lift` class left are the really important ones, or the ones in your custom type class instances. I feel it would be *difficult* to build on top of `tranformers` without have judicious use of lift be part of (at least the extension) API. 3) Also strikes me as an *existential* problem with trying to auto-insert a bunch of `lift`s into what is essentially `transformer`s code. You can avoid the problem by just sticking to `transformers`.
I'm not a native speaker, so I might be using the wrong words. Sorry for any confusion!
Oh, I had `sWorker` and `sList` at top-level.
It should be `r` elements.
I was hoping to not have to do that. Oh well, thanks for the response.
Interestingly this seems to compile when I avoid `:.` in favor of explicit tuples. Seems either like a bug or very weird behaviour. It's as though the extra type function is preventing the reduction of the whole term..
Oh bleh, actually this is much simpler than that! It's a fixity issue. It's parsing `a :? b :. c` as `(a :? b) :. c` which is wrong. Adding `infixr 5 :?` fixes everything :) Sidenote, you can drop the `:`s and use prettier operators like `==` and `?` on the type level. `:` is still taboo since it'll clash with the promoted list constructor.
Awesome! You're right--I should have looked at those parens closer in the error message. Thanks
Yeah, that's pretty good. I was also looking for a reason to explore type families
Right, I only mentioned literals specifically because of the title of the post. That said, I don't know which is more wrong, thinking of polymorphic constants as functions or as constants.
with my last implementation it takes 0.25 seconds for n = 1000
Haskell. The language where one of the pinnacle features is best explained by not talking about it, and we're not even kidding.
Given the access patterns here, wouldn't it be better to do it with iteration over the list rather than repeated indexing of any structure? I.e. something like sWorker r = (1-p) * (sum . zipWith (*) gs . reverse . take r $ sList) This is O(r) rather than O(r\*log(r)) (assuming the earlier entries are already computed), right?
Thanks, I'll definitely keep that in mind and consider rewriting some things. the main reason was that I think it's more important to see the IO as representing the idea of what it does, abstractly, instead of holding a concrete computer instruction. this sort of also allows you to see how the same IO can be compiled to different backends and contexts, too :)
What exactly you should do depends on what exactly you want, but you could also consider: data VariablePart = PostStuff ... | GetStuff ... data Resource = Resource VariablePart ... Add type variables or and members as needed.
Also, lenses don't work well with 1).
typo: &gt; printing actually ever “happens” of you evaluate putStrLn "hello" &gt;&gt; putStrLn "world" s/of/if/ &gt; ourself form scratch s/form/from/
This is also a solution. But, unlike the type family solution or tomejaguar's solution, if `PostStuff` and `GetStuff` have similar fields `a` except that some subset of `PostStuff`'s fields are `Maybe a`, the field names either need to be prefixed with `get` and `post` or `GetStuff` and `PostStuff` need to be namespaced in separate modules. I guess this could be smoothed over with lenses, though?
Not exactly. data Lazy = Lazy Int data Strict = Strict !Int newtype Newtype = Newtype Int lazyBottom = Lazy undefined strictBottom = Strict undefined newtypeBottom = Newtype undefined -- test1-3 are all _|_ test1 = case lazyBottom of Lazy x -&gt; seq x () test2 = case strictBottom of Strict x -&gt; seq x () test3 = case newtypeBottom of Newtype x -&gt; seq x () -- test5 is _|_, but test4,6 evaluate to () successfully test4 = case lazyBottom of Lazy _ -&gt; () test5 = case strictBottom of Strict _ -&gt; () test6 = case newtypeBottom of Newtype _ -&gt; () -- this distinguishes between a newtype and a strict datatype -- test7 evaluates to (), but test 8,9 both are _|_ test7 = seq lazyBottom () test8 = seq strictBottom () test9 = seq newtypeBottom () 
It's definitely more than "-&gt;" just looking like a logical connective. Look up the [Curry–Howard correspondence](https://en.wikipedia.org/wiki/Curry%E2%80%93Howard_correspondence).
Great post. I've been recently picking up Haskell and a understanding of monads, and this article helped me concretize this understanding further. There's one typo in one of the 'aside's, s/stin/stdin/
Are you sure you are running with the latest version of the transformers package? The [documentation](https://hackage.haskell.org/package/transformers-0.4.1.0/docs/Control-Monad-Trans-State-Strict.html) for 0.4.1.0 has modify' 
It was waiting on GHC 7.8 to be production-ready. What's the confusion?
thank you! :)
This only works with sequence_ , otherwise you have to change the type to IO [()] &gt; helloworldhelloworld :: IO () &gt; helloworldhelloworld = sequence [hello, world, helloworld]
http://hackage.haskell.org/package/ad is pretty awesome, and makes is pretty easy to write optimization routines on top. theres a lot of neat things in the works that aren't ready for prime time, but can you use AD today. 
I am new to Haskell and kinda curious, how does one benefit from installing Haskell Platform in OSX compared to installing cabal and ghc himself?
No. Ajhc is closing now. We choose ATS language in the next iteration, and are trying merge Ajhc's CLHs into jhc.
The idea of a "zero-argument function" has no actual meaning in Haskell due to its evaluation strategy. In say Scheme, you can actually simulate call-by-need by wrapping things in functions of no args aka thunks. In Haskell since everything is by-need, this sort of thing is done by default. Say you write a function `g` of type `Int -&gt; Int` and it has a special value when called with 0. In cbv languages, you could write a wrapper around g that calls it with 0 and just call the wrapper like so: `(define g-wrapper (lambda () (g 0)))` Then instead of littering your code with `(g 0)` calls, you can just call `(g-wrapper)`. But if you just type `g-wrapper` into your repl, you'll get something like `#&lt;procedure&gt;` since it isn't an integer that can be printed. It's a first-class function value. Try to convert this example to Haskell and the types make it very clear why it doesn't make sense. What is the type of the following function? `gWrapper = g 0` It's `Int`. Not a zero-arg function `-&gt; Int`. Just `Int`. Whenever you place `gWrapper` in your code, `g 0` will be called in the same way as above.
The timing of "missing" 2013.4.0.0 release didn't sit well with the timing of GHC 7.8... We either were going to put out a release that used the same GHC as the prior, with almost all the same libs... or wait for the 7.8 release. The 7.8 release took much longer than expected, and, in turn, there was some delay in getting the HP libs to all match. It seemed the better call to not expend all the effort to put out a 2013.4.0.0 release, for only a small incremental gain. That said, it still was a hard call (and one I flip-flopped on a bit), as consistentcy in release is important (as your comment makes clear.) The good news of all this is that I used the delay to significantly re-write the Haskell Platform build system. The new system can now completely go from GHC bindist to HP installer in one automated command. This will mean that effort to make a HP release is signficantly less, and we can release them on a regular basis, and track updates to GHC more frequently.
Well, that shows what I know :p I thought constantly taking the first n and reversing them would slow things down, clearly it doesn't really. 
I might've missed something, but how can you ignore the return value of forkIO (which is ThreadId) when you have stated that bothPar's return value is IO () &gt; bothPar :: IO () -&gt; IO () -&gt; IO () &gt; bothPar x y = forkIO x &gt;&gt; forkIO y
I'm using a special `forkIO` that is of type `IO () -&gt; IO ()` that i introduced earlier, heh. Perhaps I should just not use the same name...that's probably a better route.
I saw the type and I tried to look for the declaration for it but I couldn't find it. Was your example meant to be just theoretical?
postgres 9.4 comes (not as) close (as one might hope, but is still a big improvement). you can also do many things with rules. postgres is great. my postgres crud daemon (in haskell) is using postgres functions for updates most of the time.
have a look at http://ro-che.info/articles/2014-06-14-extensible-effects-failed.html for details regarding performance of extensible-effects as free monads.
Oh, that would explain this. Thank you, and no problem :)
This also seems like a good time to use closed type families.
"Built with the new shake based build system" - cool! Can you point me at the Shake script you use?
I've edited the post to make it more clear :) I'm calling it: makePar :: IO () -&gt; IO () makePar x = forkIO x &gt;&gt; return ()
I see in https://github.com/chrisdone/ghc-server you say "Generally it should replace use of GHC, GHCi, hlint, hdevtools, ghc-mod in one big program that you can communicate with via a simple s-expression/JSON-based protocol." How do you see integration with tools such as HaRe, which could both make use of the lower level features and expose commands via the protocol?
See the source to the new hptool command: https://github.com/haskell/haskell-platform in the new-build branch
The platform is very useful on Windows systems thank you very much for this great work. For people like me, it would take ages to get things right on this system without the platform.
For all the tools that are libraries (hlint, haskell-docs, HaRe, structure-haskell-mode, haskell-names, etc.), I would just import the library and expose some commands. I don't really want much "business logic" in a monolithic codebase. HaRe sits ontop of the GHC API's monad stack, so it would ideally it could just run in the same GHC session.
I *love* this idea. Thanks again GHC devs, 7.10 is looking to be an exciting release :)
Thanks, but FRP doesn't have that much in common with the arrow calculus described in the paper.
Great! Could you explain how is the kernel specification in Haskell intended to be used?
In particular - the web site sites that the open source release includes "all the proofs". Where are the proofs? Does the Haskell specification constitute a mathematical proof in some way? Or is there also some Agda or Coq code somewhere?
&gt; If there is no canonical instance ofσ for Σ and S then for any expression Φ[x] satisfying Σ;x:σ|= Φ[x]:Bool, and any u:VΣJσKS with VΣ;x:σJΦ[x]KS[x←u] =T, there exists w:VΣJσKS with w6=VΣJσKSu but VΣ;x:σJΦ[x]KS[x←w] =T. I.e. Some objects cannot be named. This is Voldemort's theorem. (Sorry for losing the math formatting.)
All proofs in the project, shown as green arrows in the picture, are machine-checked in the interactive theorem prover Isabelle/HOL. http://ertos.nicta.com.au/research/l4.verified/approach.pml Isabelle/HOL allows to turn executable specifications directly into code in SML, OCaml, Haskell, and Scala. http://www.cl.cam.ac.uk/research/hvg/Isabelle/overview.html 
Exactly, just having a single sentence about the plan on the website would have saved a lot of confusion. In general, there are so many great things happening in the community-all is needed is just some better PR.
Part of the success of the other languages in this respect is having a canonical matrix/array library. As of yet, there doesn't seem to be a real consensus in Haskell on such things, or there's just not enough people working in this problem space.
/u/cartazio is working on just this problem.
That whole discussion was about overlapping instances. What about incoherent instances? It would be cool if the pragmatic also included a number. {-# overlapping 0 #-}, to guide the compiler in which order to try the options.
I think they are in the l4v project : https://github.com/seL4/l4v
the role of pragmas in modern haskell has become totally insane talk about a slippery slope....someone long ago should have imposed some discipline when the first pragma was suggested. we can't even discuss "programming in haskell" anymore...its now about "programming ghc" and the weird none-haskell dsl of pragmas 
That did it, thanks!
Don't put any pragmas and write in Haskell98/2010, what's your problem?
it always amazes me how difficult installing hlearn is 
Libraries.
you can't anymore. pragmas have become deeply embedded in hackage libs if people tried to version mainstream c++ through the preprocessor, there would be a riot
SPJ and the other MSR folks have always been very upfront about the fact that Haskell is a research language and GHC is a research compiler. Pragmas allow language researchers and enthusiasts to experiment with new features in production without splintering into 80 different Haskell compilers. To me, it seems like a pretty reasonable solution to the problem of the many different purposes which GHC serves. What would you prefer?
The problem is that people think it's an important concept that's hard to learn. It's not important, nor hard to learn. Once you understand haskell's type system, it takes about 3 minutes to explain in its entirety. But if someone demands that they be force-fed an explanation before they understand higher-kinded types, they're going to be miserable.
http://www.reddit.com/r/haskell/comments/ja27r/sel4_a_formally_verified_version_of_the_l4/ Paging /u/kamatsu and /u/808140 2 years ago they seemed really disappointed the source wasn't available.
Just after I spent 3 hours using black magic to install gtk of course :D
That's all fine. I am asking about some use cases. For example, let's say I want to develop some functionality on top of the kernel. Can I first do it in Haskell against the low-level Haskell kernel spec and then reimplement it in C for high performance? Or, let's say I'd like to learn about the kernel and "experience" working with it by first programming against the Haskell spec? In what ways, other than "by reading it", can the Haskell kernel spec be used? 
Every monad has `Monad m =&gt; join :: m (m a) -&gt; m a`, and a valid monad transformer has to yield a monad (when composed with a monad), so yes.
Actually I got my statement wrong. I'm going to delete it and resubmit.
Couldn't they integrate them into the syntax? 
Better to explicitly declare the precedence list than to rely on spooky action at a distance comparing arbitrary integers.
This is basically [`squash`](https://hackage.haskell.org/package/mmorph-1.0.0/docs/Control-Monad-Morph.html#v:squash) with `m` restricted to `Identity`.
How does a library pragma hurt your program?
Urk, so I have to CPP all the libraries I want to work with two consecutive GHC versions without deprecation warnings :(
Be more conservative in your choice of libraries. :) The core libraries use very few pragmas (e.g. `BangPatterns`).
Perhaps a change in terminology would be appropriate in order to accomplish this. {-# OVERRIDABLE #-} instance Monoid [a] where ... {-# OVERRIDE Monoid [a] #-} instance Monoid [Foo] where ... 
Seems like an inevitable cost of having the language improve over time.
It's basically almost equivalent to GHC's arrow notation ("proc" etc). Section 3 of the paper has the (very simple) translation from Arrow Calculus into GHC's "proc" notation.
I just reread the first three sections of the paper. One reason there may be no implementation is because the paper shows that arrow calculus and classic arrows, so-named in the paper, are equivalent. So the arrows are the same, and the benefits of paper would accrue mainly to the implementors of the arrow extension, who already knew some of the results.
it doesn't if you never need to extend or modify a library...except this is a common practice these pragmas aren't sitting in the heady fringes of esoterica, they are fully accepted as part of mainline haskell development. we all understand the world isn't perfect, that a few hacks here and there to make things move forward are to be shouldered periodically, but haskell has adopted these hacks on a level no other language has...odd for a language that is touted as occupying a loftier realm than other tools
Not much. Locked in versions of libraries. It's fine right now, but in a few months it'll be better to just install ghc+cabal.
i have no idea why haskell people can't sit in a room and move the LANGUAGE forward C did it C++ did it Java did it even PHP! and these are tools with huge amounts of code in the wild to worry about. the fact that the haskell community pushes change through ghc pragmas speaks volumes regarding the stewardship of the language you seem like a smart guy tibbe, why not just take the reigns and make some people upset? (a necessary precondition for meaningful change) you have to admit haskell would be better off without this stuff
&gt; activate-hs tool (new!) for switching between installed versions That's really sweet! Thanks for adding this.
if haskell is a research language, why the aversion to breaking changes? many of these pragmas have been around for a while, they are fixtures of development now. if indeed you were correct, that this is a research initiative...that would tell me that breaking programs was acceptable if the core language moved forward with the best thinking in research, because hey, this is just a tool for thinking in code. so I think you actually have it 100% backwards...the heavy use of pragmas seems intent on making sure code never breaks...which is a viable goal for an industrial tool, but implemented poorly. it would be like C++ developers versioning the language through the preprocessor...which would be nuts. the right way to do it is to either write a spec first, or a spec that mirrors development of a reference toolchain, and then all toolchains can implement new features while supporting backwards compatibility. this is how C++ has moved forward, and it has worked. C++11 introduces new features that are clearly spelled out, and compiler designers integrate those into the same codebase that supports C++03 etc. the big danger here is that haskell and ghc can never be considered separately. the haskell community has basically zero insurance against ghc development going off of the rails or being hijacked or some other bad outcome. people here rip on Go a lot...well guess what, even as young as it is, Go already has two completely independent toolchains, and one more for llvm is on its way.
I'm also curious about this question. If you've come to any conclusions I'd love to hear them!
Well, for any monad transformer that has `runT :: t m a -&gt; m (s a)` and `T :: m (s a) -&gt; t m a`, you've got: fmap (T . I) . runT :: t (t I) a -&gt; t I (t I a) join . fmap (T . I) . runT :: t (t I) a -&gt; t I a 
&gt; Export a newtype for your custom monad stack / transformer that includes Reader, but not it's constructors. Then most of the code that uses your Monad(Trans) doesn't need to know the reader argument, only your runAwesome/runAwesomeT needs to know. I'm a bit new, could you show what this looks like? Wouldn't it be in effect the same as writing `foo :: SomeType -&gt; ReaderT a m ()` ?
Or, if we believe that this pragma is complete and good to go, just get rid of it and leave the syntax. 
&gt; I think the pragmas are there for ideas which are not proven in the wild in my mind, these should then be limited to "beta" versions of ghc, released into the wild long enough for library authors to migrate and test. in my view, most of this stuff should be absent from released, stable versions of ghc. furthermore, these changes should be reflected in a spec, not a single implementation. &gt; Is there something you'd like to see changed that has not been? thats a tough call because removing support for many of these pragmas at this point would result in ghc and haskell moving backward considerably in the absence of a spec. i would probably offer the tougher call of not trying to integrate sweeping, fundamental changes like strictness into the toolchain (pragmas or otherwise). at this point, haskell/ghc is so overloaded conceptually that it is almost impossible to grasp the totality of the toolchain...imho (which i am sure is a minority opinion), things like strict vs lazy are better left to distinct toolchains...meaning, you want a strict language, go find one
The userbase is tolerant of breaks in backward compatability and having half-baked features brought into the mainline to be tested. Why would you give up an asset like that and lower the language to the level of things like C++ and Java which need years to implement small changes?
I've document my steps here: http://wiki.ocharles.org.uk/Nix#how-do-i-use-cabal2nix-for-local-projects
Why would the languages be better off without the ability to casually add and remove features? Pragma's grant GHC the ability to throw an extension out in the wild, let it get some use see how it interacts with other features. If it plays well and doesn't either cause trouble or push the language into some design corner then it get put in the standard. If it's useful in certain circumstances but causes issues in others then it can hang around as a pragma until the community comes up with a better option, instead of what happens in one of the shackled languages you listed where the feature is wrought in stone and can't be removed because it's been standardised.
1. I learned functional programming in my first year course in Computer Science in 2006. We used Miranda. I picked up Haskell in around 2008 while I was bored at a co-op job. I now use Haskell for most things. 2. I find that I'm much better at writing small functions that do one thing really well. Furthermore, I prefer to use immutable objects when I can to avoid introducing too much mutation into my code. 3. I do! A few years ago I became interested in template meta-programming in C++. My experience with functional languages allowed me to make that conceptual leap to type-level programming much easier. C++ templates are simply L-systems (like ferns), and they're pure functional. The syntax is kind of like a pointy lisp with some extra noise.
Ah okay, so that is consistent from before. Well thanks to you and your team. Perhaps gave Jhc a renewed spark. I know that I certainly played with Ajhc. I cannot wait to see what your team does with ATS.
"Custom" monad: http://hackage.haskell.org/package/cryptsy-api-0.2/docs/Cryptsy-API-Public-Types-Monad.html Example API (does not mention `Manager` type): http://hackage.haskell.org/package/cryptsy-api-0.2/docs/Cryptsy-API-Public-Market.html Final step, turning "custom" monad into IO still doesn't mention `Manager` but that's just because there's a "good default": http://hackage.haskell.org/package/cryptsy-api-0.2/docs/Cryptsy-API-Public.html If you use `ReaderT r IO ()` each of your function signatures has to mention `r`. Not so with a newtype wrapper or type alias. This example is not even the full opacity; I didn't wrap with a newtype, just used a type alias; it's not a correctness issue if users want to use constructors directly, I believe. This is just the first example I had in mind, since I wrote it.
&gt; The reason experimental features are typically included via pragma isn't that GHC is supposed to be an industrial tool that seems to be the defining tension. the haskell community otherwise seems bent on promoting real industrial use. it would be interesting to hear from people like FPComplete, who are engaged in industrial education and promotion...how these decisions impact their work
So are you for or against wearing C programmer skin? Supple, not usually exposed to much sunlight.
This package has some very weird `build-depends`... that can't be right: if impl(ghc==7.2.*) build-depends: ghc &gt; 7.4 if impl(ghc&gt;=7.4) build-depends: -- Hard versions ghc ==7.4.* || == 7.6.* || == 7.8.*, 
Isn't that what Haskell 2010 did? [Section 12.3](http://www.haskell.org/onlinereport/haskell2010/haskellch12.html): &gt; implementations are also encouraged to support the following named language features: &gt; &gt; PatternGuards, NoNPlusKPatterns, RelaxedPolyRec, &gt; EmptyDataDecls, ForeignFunctionInterface &gt; &gt; These are the named language extensions supported by some pre-Haskell 2010 implementations, that have been integrated into this report.
EDIT: * more than every 12 years. 
I'm fine with wearing it. It's the *stealing* that I'm concerned with.
The first `if` is weird, I know. I used to support 7.2 but I dropped it recently. Not that I have any users, but if there are any trailing users of GHC 7.2, specifying this `build-depends` explicitly yields a better error message from Cabal. Although next time I upload to hackage I'll change it to &lt;7.4. With the second build-depends with the ghc constriants by themselves, you get error messages about completely uninteresting packages even if you're on an old GHC that'll ultimately be rejected by ghc-server, this saves people wasting their time if they don't read the package description.
Exactly!
What is going to happen in few months ?
It's no more difficult than learning any other language feature.