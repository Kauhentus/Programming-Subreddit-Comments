Beautiful!! A git clone, then stack ghci let degreesRadians a = a * pi / 180.0 onscreen $ contourF (\a b -&gt; sin (degreesRadians a) + cos (degreesRadians b)) (-100) 100 (-200) 200 10 works great on macOS. Thanks again.
&gt; `sequence :: Monad m =&gt; [m a] -&gt; m [a]` That's not a list of *monads* (plural). That's a list of `m a` where `m` is a *monad* (singular). The `Monad m =&gt;` is outside of the brackets and applies to the entire list. You cannot stick an IO and Maybe (two different monads) to the same list, nor does it make much sense. A list of monads (plural) would be something like `[ Monad m =&gt; m a ]` but of course that's not legal Haskell (you can get around this with extensions). &gt; You can sum over many numeric values if they are all the same kind of number. That's exactly the point. You can have a list of the same kind of number and this makes sense, but a list of *any* numbers doesn't. The command type class allows you to have a list of the same type of command, for example a list of sort commands, or a list of print commands. This may have some limited utility but generally people want a list that can hold any kind of command.
That sounds good, only `Apply` is currently outside of our hierarchy 
It's kind of silly, but I switched to git primarily because of the [fugitive](https://github.com/tpope/vim-fugitive) plugin for vim. I tried to write something similar for darcs, but at the time the the command line interface was ill suited to such a project — this may have changed over the years. I too am looking forward to pijul.
Can you explain how darcs cherry picking is better than the git one ? Googling darcs cherry picking didn't yield any useful result.
&gt; hub.darcs.net is a solid alternative for github. I wouldn't call it a solid alternative. It lacks most of the extra features of github IMO (like github pages, app integration, webhook, code review). I'm not sure if integrating a CI like Travis is even possible in darcs.net as of now.
De? Not sure I understand...
Consider the following git situation $ git pull upstream $ echo hack &gt;&gt; foo $ git push upstream -- Say some one else in the meanwhile pushd a nonconflicting change Push does not go through because we need to either rebase or merge and push. in darcs it will work fine. Similarly in the other direction. Cherry picking is great in darcs in git it is a joke. 
Can you think of any other idealised monads? The [blog](https://theorylunch.wordpress.com/2012/11/08/an-introduction-to-ideal-monads/) states &gt; # `Either` &gt; Also the `Either` monad can be derived from an ideal monad. This is just a generalization of the `Maybe` case. but that requires `Const val` to be idealised? type EITHER a b = Lift (Const a) b pattern LEFT :: a -&gt; EITHER a b pattern LEFT a = Other (Const a) pattern RIGHT :: b -&gt; EITHER a b pattern RIGHT b = Pure b 
Thank you for the solution with GADTs, you opened my eyes to something I had not previously understood with GADTs and type variable which only appears on the constructor but not on the type.
Yea having an `Idealized` instance on `Const` is similar to the `Monad` instance it could have had: instance Monad (Const x) where Const x &gt;&gt;= _ = Const x The idealized instance would be fine if it weren't for that `Applicative` relationship. The solution is just to have another `Const` type with different instances, just like how we have a different type in the `validation` library for the Applicative instance of `Either` that uses a Monoid for the `Left` type.
Cocode
Ha, thanks! 
Ah, I see. Let's assume first of all that you've connected the haskell `length` function with the `len` measure with the following type length :: xs:[a] -&gt; {v:Int | v = len xs} The subtyping query LH produces at the call to `f` will then be {v:Int | v = len xs} &lt;: {v:Int | v &gt; 0} which translates into the SMT query v = len xs ==&gt; v &gt; 0 which is invalid, so LH would reject the call to `f`. The reason, intuitively, is that we don't know anything about `xs`, so we have to accept the possibility that it could be empty, which would violate `f`s precondition. We can convince LH that the call is safe by adding a precondition to `compute`, e.g. compute :: {xs:[a] | len xs &gt; 0} -&gt; Bool Then our subtyping query will additionally have an environment reflecting `compute`s precondition xs:{[a] | len xs &gt; 0} |- {v:Int | v = len xs} &lt;: {v:Int | v &gt; 0} which translates into the SMT query len xs &gt; 0 /\ v = len xs ==&gt; v &gt; 0 which **is** valid. (Note that LH can infer this precondition of `compute` for us depending on how it's called in the current module, we don't necessarily have to write it ourselves.) A more interesting example would be if we rewrite compute to case-analyze `xs`, e.g. compute xs = case xs of [] -&gt; ... y:ys -&gt; f (length xs) The `:` case tells LH that `len xs = 1 + len ys`, so we end up with a slightly different query xs:{[a] | len xs = 1 + len ys} |- {v:Int | v = len xs} &lt;: {v:Int | v &gt; 0} or len xs = 1 + len ys /\ v = len xs ==&gt; v &gt; 0 which is also valid (assuming that `len` is non-negative, which LH knows).
If you do this in a library, it's a good idea to make a module like module MyModule.Internal where data Protected = UnsafeMkProtected { getProtected :: Int } makeProtected :: Int -&gt; Maybe Protected makeProtected i = do guard (i &gt; 0) pure (UnsafeMkProtected i) so that people can access the internals, if they need to, and if they understand the risks. It is really frustrating to have to fork a library just to implement something that the original maintainer didn't think of.
Type checking can probably be done the way you suggest. But for something like `intero` that shouldn't be necessary. You can do all checking at about 10,000 lines/s so with caching of already processed modules the interactive response time is very low. Ghc might not be this fast, but it should be. :)
I believe Pijul deserves much more attention. Unlike Darcs it can be not only cool, but also useful. 
I'm not sure functional programming is a reason to use darcs. Why would you care what language your tools are written in? There are lots of other good reasons to use darcs, though. I wish it had succeeded over git. It's clearly the simpler to to use, and with the same effort, most performance problems could be solved. That said: as others have mentioned, GitHub is the decision factor. The more projects use GitHub, the better it gets. Not just in features, but in probability that your potential collaborators are already on the platform.
The big thing in Darcs is not having to needlessly care about patch ordering. You can, normally painlessly, push and pull subsets of patches out of order, things that like.
Thanks! will go through it.
&gt; if you're a responsible software engineer who cares about correct solutions Then you'd know that while tests can provide evidence that code is incorrect, they cannot prove code is correct. Instead, you'd focus on getting Idris, Agda, Isabelle-HOL, Twelf, Coq, and the like where you can actually produce sound, machine-checked proofs of correctness. Even ignoring the correct thing to do (see above) and focusing on tests, BDD has never saved me any time over unit tests or property tests, and I've yet to find a team that writes their acceptance criteria as BDD tests. The English-ish syntax is just useless overhead until the requirements come in like that.
This is the price you pay for abstraction. The more abstract your type is, the less you know about expressions of this type, as they allow more representations. You are right, that the existential typeclass pattern allows for more abstract types, but it's essentially just an overly complicated way to do what can always be done much easier via records. GHC may generate more efficient code for existential typeclasses. I would like to see a proof of that, however.
For the record, the `Bind` class has induced `&lt;.&gt;` as a law, and `Const` violates it (there isn't `Bind (Const r)` instance. https://hackage.haskell.org/package/semigroupoids-5.2/docs/Data-Functor-Bind.html#t:Bind And if you want to state the law without relying on `&lt;.&gt;`, there are laws for `&lt;.&gt;`, where you can inline the definition
&gt; Did you maybe reply to the wrong user/comment here? Perhaps so, or maybe I just misunderstood you. You have said &gt; You just gave two good examples where lists with type classes do make sense but I didn't have "lists with type classes" in mind at all. I meant *a list of instances*. The term is of course incorrect, because an instance is a type, not a value, but that's exactly the point. There are no lists of instances. When one wants a list of different *things*, these *things* cannot be instances, and vice versa.
I wonder if it has a connection to [`Bound`](https://hackage.haskell.org/package/bound-1.0.7/docs/Bound.html#t:Bound) class Bound (t :: (Type -&gt; Type) -&gt; (Type -&gt; Type)) where (&gt;&gt;&gt;=) :: Monad f =&gt; t f a -&gt; (a -&gt; f b) -&gt; t f b boundJoin :: (Bound t, Monad f) =&gt; t f (f a) -&gt; t f a boundJoin = (&gt;&gt;&gt;= id) v class Idealize (f :: Type -&gt; Type) where (&gt;&gt;~) :: f a -&gt; (a -&gt; Lift f b) -&gt; f b idealJoin :: Idealize f =&gt; f (Lift f a) -&gt; f a idealJoin = (&gt;&gt;~ id) **Edit**: The documentation starts with &gt; We represent the target language itself as an **ideal monad** supplied by the user, and provide a `Scope` monad transformer for introducing bound variables in user supplied terms.
??? I was responding to the selling-point aspect of GP's argument, and by no means claiming BDD was a panacea nor that it would prove solutions. It's just a really useful way of encouraging teams to A) start with a common definition of what correct looks like, and B) having tests being at the forefront of development, rather than an after thought. Just because you've never seen a team that writes their acceptance criteria as BDD tests doesn't mean it doesn't happen. The "English-ish syntax" are SUPPOSED to be your requirements that are humand and machine readable, so it sounds like you either don't know what you're talking about or have been doing it wrong.
Haskell functions are pure, meaning their output only depends on their arguments, not "hidden" variables. This includes `placeShips`, your user provided function. This means that if the non-decreasing recursive call to `placeShips ships board` is ever evaluated, evaluation will not cease (or only cease when resources are exhausted). So, there's at least one problem with the `placeShips` you've provided. Are you sure that the first argument to `placeShips` is a finite list of ship sizes, and not (e.g.) a infinite list of random values like you might get from calling [`randoms`](https://hackage.haskell.org/package/random-1.1/docs/System-Random.html#v:randoms)? If that were the case, the solution would be "simple": main = do gen &lt;- newStdGen print (placeShips (randoms gen) (mkBoard 10)) placeShips = placeShips' [5, 4, 3, 2, 2] placeShips' [] _ b = b placeShips' (s:ss) (x:y:d:r) b | isShipPlaceable s x' y' d' b = placeShips' ss r (placeShip s x' y' d' b) | otherwise = placeShips' (s:ss) r b where x' = x `mod` 10 y' = y `mod` 10 d` = (d `mod` 2) == 1 
For *that* implementation of the parser, it doesn't. Since the parser just checks command heads in turn, it'd be straightforward to write one that does. Generalizing, there's a few places where I've ranged over a sum type to build something (usually a function or a map) that maps back to that sum type, in order to get completeness checking.
I also appreciate the darcs-like features git and mercurial have grown over the years. `hg record` and `git add -p` at least make me feel somewhat comfortable using those systems. But as far as the actual model of repos and patches goes, and reasoning about them, darcs still wins in a landslide in my mind. I'm constantly googling how to do even the most slightly non-standard thing in `git`, and in mercurial i'm typically compelled to fall back to a visual tool while in darcs I never felt the lack of one.
&gt; There are no types in the compiled executable (C++ or Haskell). Unless you build with RTTI, but to my knowledge that's not actually relevant to the topic at hand...
FWIW, the value is not "contained" within `IO`. An `IO a` is more like a *function* that, when run, will produce side effects and return an `a`
Here's some: `tar x`, `tar c proj`. I don't understand this "`tar` is confusing" meme.
I don't know how darcs achieve those things, but for now, my main issue with git is when you rebase / amend a shared branch. As far as I know this workflow is a nightmare in git. On the other hand, it works "well" with mercurial and [Evolve](https://www.mercurial-scm.org/wiki/EvolveExtension). Perhaps the "patch model" of Darcs may help.
Sure, but that's only the "and the kitchen sink" parts of `tar`. The bread-and-butter use cases are pretty damn simple. `tar` packs/unpacks a directory into a single file. `tar x &lt;foo.tar`, `tar t &lt;foo.tar`, and `tar c file ... &gt;foo.tar`. Everything else is just there for convenience / performance reasons.
Good point anyway.
Good question. We seriously need this. How does one watch a thread in Reddit?
It depends on the project/context but I would suggest that instead of trying to sit down and design everything and then implement things off of a checklist, you instead sit down and try to get one small thing working. Once that's working, you try to get the next small thing working, and so on. At each step decide on the functionality you want to implement and then do only what's necessary to implement that piece of functionality. When you have the functionality working, go back and refactor what's there to address anything you learned/changed while implementing the new functionality. Haskell's type system makes refactoring trivial. If you proceed in this manner then you will never have a "wrong model" since it will only have exactly what it needs to support the current functionality. As a concrete example: I'm working on a program to do introspection of directories full of certain types of data. Ultimately the program I want to write will need to know about various formats of files, be able to read them and extract important information, summarize that information, etc. In the long run it will also need to traverse sub-directories recursively. But I didn't start out by sitting down and trying to design anything or think about free monads and DSLs or what I would need for testing. I sat down and wrote something like this: module Main where import System.Directory ( getDirectoryContents ) main :: IO () main = getDirectoryContents "." &gt;&gt;= putStrLn . show which just listed all the files in the current directory. I can add sub-directory traversal later, but I'd rather get to the core of the problem and work on reading the data first. So, then I changed `main` to something like: main = getDirectoryContents "." &gt;&gt;= putStrLn . show . filter ((==) "data1.csv") which effectively hardcoded it to look at only that one file... and I proceeded in this fashion, initially just storing the list of files that I read as a list of strings, then at some point changing them to a list of tuples with the first argument being the filename and the second argument being the name of the detected format, then eventually upgrading that tuple to a `KnownFile` data type that held those two pieces of information in addition to 4 more pieces of information. The program has evolved quite a bit -- it now understands how to read twelve formats and can pull the summary data out of four of them. But every step of development has been proceeding as I've described, making one change at a time and continuously (and mercilessly) refactoring along the way. Especially for learning projects I would suggest this approach over the "big upfront design and planning" approach.
&gt; Will it merge better? Probably -- that's darcs' major claim to fame. Git is frankly just a setting and policy away from doing it as well as darcs does, but it would have to implement that policy first, and darcs' philosophy of "order only matters when it has to" is pretty deeply embedded, so git might be playing catch-up in the game of patch commutativity for a long time if it bothered. That said, I found darcs unusably slow, and it has basically zero tooling, so it's not in my toolbox either.
The same law, but replacing `(&lt;.&gt;)` with `(&lt;*&gt;)` should be fine. EDIT: Oh wait but requiring `f` to have `pure` is the same thing as giving it a Monad constraint. So that's no good either. Sounds like `Idealized` really needs to be based on a version of `Lift` that correctly uses `Apply`
`Handler` is an instance of [`MonadIO`](http://hackage.haskell.org/package/base-4.9.0.0/docs/Control-Monad-IO-Class.html#t:MonadIO). You can therefore use `liftIO :: IO a -&gt; Handler a` to turn an `IO` action into a `Handler` action. For instance, if you have `examples :: IO [Post]`, you also have `liftIO examples :: Handler [Post]`. By the way, questions which are more 'help me' than 'discuss this' generally fit better at /r/haskellquestions. Also, please include a minimal example (that is, a piece of code which demonstrates your problem and can be run as-is) with your questions if possible.
I'm slowly developing a mental image of this zoo, it's fascinating
As with any parallel program, tuning GHC's GC parameters can help here. However, you are quite right that things don't scale as well as we would like. See, for instance, [#9221](https://ghc.haskell.org/trac/ghc/ticket/9221).
**Here's a sneak peek of [/r/haskellquestions](https://np.reddit.com/r/haskellquestions) using the [top posts](https://np.reddit.com/r/haskellquestions/top/?sort=top&amp;t=year) of the year!** \#1: [Typical commercial Haskell dev environment?](https://np.reddit.com/r/haskellquestions/comments/610jon/typical_commercial_haskell_dev_environment/) \#2: [Call-by-need is asymptotically optimal w.r.t. time. Is there a counterpart for space?](https://np.reddit.com/r/haskellquestions/comments/5lvzw5/callbyneed_is_asymptotically_optimal_wrt_time_is/) \#3: [Where can I learn about function naming conventions?](https://np.reddit.com/r/haskellquestions/comments/4ww75f/where_can_i_learn_about_function_naming/) ---- ^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| [^^Contact ^^me](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| [^^Info](https://np.reddit.com/r/sneakpeekbot/) ^^| [^^Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/5lveo6/blacklist/)
It certainly is. There are certain situations when such behavior is inconvenient (e.g. database schema changes) but with code it's the best way to proceed.
Does DARCS offer better rebase/merge workflow? Could I pause a massive rebase and start working on something else?
Yes, that's exactly right. In other words, `liftIO` is a class method of `MonadIO`.
It's not the "existential antipattern"; it's the "existential *typeclass* antipattern". In the latter you do data AnyWidget = forall w. Widget w =&gt; AnyWidget w which is completely pointless. The former is just "using an existential", which is fine, and not generally objected to. https://lukepalmer.wordpress.com/2010/01/24/haskell-antipattern-existential-typeclass/ 
[removed]
If you want to use it as doubles you could converted it to radians any way using the `toRadian` function. 
This isnt a legitimate instance. return a &gt;&gt;= f = Const mempty &gt;&gt;= f = Const mempty /= f a It only works for Const () as there is only one choice of x.
D'oh! Forgot monads have `return` =P I suppose part of my argument works for `Bind`, assuming a conforming instance of `Apply` that doesn't use a semigroup?
I think I may have left the concept homeless after I broke up category-extras. I'd be okay with moving it into the `free` package. There are a couple of engineering choices to be made. Would it be better to keep the category-extras approach and the Lift like approach used here by folks, or would it be better to do something like the recursion-schemes package does, and let us take a monad that happens to be ideal that we have today, and then derive monad products for them without requiring them to have this specific constructor on the outside? e.g. it seems to make sense to do something like move the existing MonadIdeal to a new name, maybe not the right names but something like: data Ideal f a = Pure a | Ideal (f a) class Bind m =&gt; Idealized m where {- or maybe Apply -} idealize :: m a -&gt; (a -&gt; Ideal m b) -&gt; m b class Monad m =&gt; MonadIdeal i m | m -&gt; i where _Ideal :: Iso (m a) (m b) (Ideal i a) (Ideal i b) instance Idealized Proxy where idealize _ _ = Proxy instance MonadIdeal Proxy Maybe where ... instance Functor f =&gt; MonadIdeal ... (Free f) where Then we could talk about ideal products of things like Maybe or Free f or other things that happen to be ideal, but which aren't encoded as `Ideal f` directly.
To see or refute the connection, take a stab at showing that given an instance of `Idealized f`, `f` is a left monad module over `Ideal f`. You'd need to show m &gt;&gt;~ return ≡ m m &gt;&gt;~ (λ x → k x &gt;&gt;= h) ≡ (m &gt;&gt;~ k) &gt;&gt;~ h or require these as laws.
&gt; there's a few places where I've ranged over a sum type to build something (usually a function or a map) that maps back to that sum type, in order to get completeness checking. can you provide an example? 
You can also use RSS by adding .rss to the end of most reddit URLs. https://www.reddit.com/r/haskell/comments/6ae4yq/what_resources_exist_on_designing_a_haskell_or/.rss
&gt; Start simple, refactor when needed, not before. This is something I wish I had known at the outset. Refactoring in haskell is *far* easier than refactoring in python or Java, often to the point where it's best to just define your abstractions/data types and then start writing code.
I think it's nice to have functions like `move :: Shape a =&gt; a -&gt; a` that statically guarantee that a moved rectangle is still a rectangle, a moved point is still a point, etc. With a single record type `Shape` you can't have that. That said I think we're mostly in agreement.
yeah, i didn't get the point of the mutual recursion (for subcommands?) or the implicit dependency between fields (why does is the lazy string better than a function?). why not have command be a name and an invocation, and invocation be a function? 
The arguments about network effect, github, polish, eco system, .. are all true and the answer to why isn't it used more. Let me make another point. People love to talk about patch theory and how that is superior. I find that analysis shallow. Fact is that the text/line based approach to source code changes can detect dependencies only by very crude and local heuristics. Git then takes the conservative approach and says all changes are totally ordered, while darcs maintains only a partial order based on if it can detect a dependency. Both approaches are bound to be incorrect, or rather suboptimal for git, sometimes. The way to improve version control and change management is IMO to improve the heuristics detecting change dependencies and conflicts by taking language specific knowledge into account. The choice between darcs and git is essentially a matter of policy and to me the correct one seems to be instead of being optimistic and incorrect like darcs to be conservative and assume a dependency or a conflict unless the opposite can be proven.
I didn't mean to pretend it's a *big* downside, but there are certainly other compilers that support vanilla Haskell, whether or not no one uses them.
This is a nice post, but you should reconsider your use of sum types with records when modeling commands. It breaks things at runtime :( if you call an accessor on the wrong command.
Think of Apply as IdealApplicative: Lift f (a -&gt; b) -&gt; f a -&gt; f b can be split into a call to fmap or &lt;.&gt; by case on the first argument. so a Lift based on Apply rather than Applicative would be a suitable starting point for Ideal. you can choose the other one to Lift just as well, but the implementation is a little more annoying.
I think Don's [The Design and Implementation of XMonad](https://xmonad.wordpress.com/2009/09/09/the-design-and-implementation-of-xmonad/) is great!
You are too modest. I think you'd do a great job :)
Hmm, interesting. I haven't used Existential qualification too much, but it looks like it's what I need. Are there any downsides/pitfalls to using the Existential qualification extension?
This sounds like dataflow programming. You might also like [To Dissect a Mockingbird](http://dkeenan.com/Lambda/), a graphical notation for lambda calculus.
Along the same lines, is there a way to view a Haskell program as a computational graph? It's my understanding that a Haskell program is technically just one big composite function so it should be possible to create a visualization graph of the program like OP described
Thanks. I believe we will make good progress here! Anyone willing to lend a hand is certainly most welcome to do so.
From the hip - Serializing/tokenizing a representation of a graph sounds like the hard part. Transforming a graph to a series of functions sounds pretty easy to do in Haskell, but capturing the contents of a drawn area concretely sounds quite difficult. Depending on how you feel about it, I might investigate some combination of ghcjs and electron, or, depending on your strengths, just electron, with Haskell as a psuedo backend over localhost or file IO. Parsing JSON in Haskell is ridiculously simple (aeson is your friend), so if you already have the webdev chops to pull off a canvas UI to json interface, putting it all together would be pretty painless. This of course has the added benefit of transferring to a webapp with considerably less sweat and tears, if that's of interest. But in my experience, Haskell GUI frameworks​ have a tendency to get extremely non-haskellian very, very quickly, which will likely get problematic and painful.
What you're talking about are string diagrams. Any given monoidial category admits one along the lines of the diagrammatic matrix calculus you linked to. Here's a [paper on them](https://arxiv.org/pdf/0908.3347.pdf). Hask is treated, in practice, like a cartesian category, where the tensor product of linear algebra is replaced with the cartesian product. This fact is the basis for the [concat](http://conal.net/papers/compiling-to-categories/compiling-to-categories.pdf) library. There might be potential in interpreting concat programs diagrammatically. One thing to note is that these diagrams denote *point-free* programs. They don't allow full representations of the lambda calculus. There are nice diagram calculi for the lambda calculus (my favorite are the [bubble diagrams](https://www.youtube.com/watch?v=PtgYKXLwnBY&amp;ab_channel=dasuxullebt), but there are [many alternatives](http://bntr.planet.ee/lambda/work/visual_lambda.pdf)). However, I've never seen one incorporate types in a nice way, and none have a clear connection to string diagrams. This is something that's been investigated for a long time, but there aren't any clear answers. The most successful approach is probably Scratch and its diagrams, which does suggest a [nice representation of types](https://stevekrouse.com/types-are-shapes-d6af1e83192f), but it doesn't fit well with everything else. We have a mish-mash of partial solutions which don't fit well together. I wouldn't be discouraged by this, though. Due to the lack of hard answers, coming up with diagram calculi is more of an art than a science. You might very well be able to come up with an elegant, unified approach. 
Most tar implementations support quite a bit more than what POSIX / Single UNIX Specification require. It's been a while since I needed to use any of the tar options unique to HP-UX or AIX, but they had a lot of options, too.
&gt; Git then takes the conservative approach and says all changes are totally ordered, while darcs maintains only a partial order based on if it can detect a dependency. You are using fairly technical terms "partial order" and "total order", but not in a way that is consistent with what I understand them to mean. Could you please be more explicit about what is the total order between changes (commits?) that Git imposes? I hope I'm wrong, but when people misuse terminology like this in causal conversaion it's often because they are trying to sound more authoritative / accomplished. (And, it generally works, because confidence is often confused with competence.)
&gt; Can you think of any other idealised monads? There is also the coproduct of two ideal monads.
I think you'll want to wait for the generation of the interface file (".hi") of the dependencies, but the good thing is by this time the type checking is done, so your mucking about with killing everything if you get a type failure isn't necessary. If you don't wait for the interface file, you also won't be able to inline functions. GHC likes to inline small functions across modules even if they're not explicitly marked inline, although if you're compiling in non-optimised mode you may not care about this. But if you're not optimising, it probably won't take as long to generate the interface file, as it just needs to type check and a non-optimised build doesn't even dump out any code into the interface file for inlining (so no inlining of course). So in theory, once you generate the instance file for A, I'm pretty sure you could start compiling B, whilst the object file creation is still happening for A. At least I think this is the case, I'm no expert on GHC internals. 
I am not OP but the way I would like to put it is the following difference in philosophy: 1. Git and its cousins manage temporal chronology, i.e. "At this point of time the repository is this and by the way reached here thus" 2. Darcs and its cousins like pijul and camp manage semantic chronology. "Currently the repository supports these features and bug fixes" (semantic chronology is not standard words, but grant me some liberties) While this characterisation is not exact, 1. On the Git side for example, one can sort of achieve this by having a main trunk, tracked by master say and keeping each feature addition/bug fix as a non-fast-forward merge to the main trunk. 2. On the darcs side one can get temporal history a la git by religiously tagging. Now which one does one prefer. I largely prefer the darcs way as I think this is how humans like to view their repository: "What bugs are fixed or what features have hit the head". If bug-1 and bug-2 are independent, I really do not care about which of them entered repository first. My opinion is that this is the way one should study human history as well, I guess. "For example, was the great economic depression the main reason for the rise of fascism". rather than the exact dates when the Wall street collapsed. Of course we do need some temporal sense to historical events -- that is historical knowledge. But if that historical knowledge has to become historical wisdom we need the more abstract semantic chronology.
So basically: prototype/spike/exploration and use power of Haskell to refactor it to production level as you learn more about problem domain instead of Doing The Real Thing From Scratch (as in some other languages)?
Yes, though in all honesty this is how I approach a lot of problems in other languages as well, it's just not as easy as in Haskell.
There is [glance](https://github.com/rgleichman/glance).
A post I made a while ago might be helpful, [targetting c--](https://www.reddit.com/r/haskell/comments/5infeh/how_popular_is_c_and_is_it_a_good_idea_to_target/?st=J2K7KAVE&amp;sh=fa861b00).
Should that not be class Bind m =&gt; Idealized m where idealize :: m a -&gt; (a -&gt; Ideal m b) -&gt; m b 
I've been reading Functional and Reactive Domain Modeling, which uses Scala, and I've been finding it really interesting.
The original web (http://www.cminusminus.org) is dead. Here is an academic old version, whith manuals and a Quick C-- compiler: http://www.cs.tufts.edu/~nr/c--/code.html See also https://ghc.haskell.org/trac/ghc/wiki/Commentary/Compiler/GeneratedCode 
Remindme! 2 days "This is one way."
There is definitely interest in Reykjavík, I don't know about other places in Iceland (inb4 what other places). I started a meetup a while ago and finding interested people or companies to host us was never an issue.
Edimburgh, Gothenburg, Glasgow.
You are aware that "human and machine readable requirements" is otherwise called "source code", aren't you? But whatever, I have read the discussion and seem to understand now. Those feature steps are intended to be written before development is started. Then for them being of some specific language makes sense, because they are not tied then to any specific language, API, or testing technology. The language could look a bit less weird (how does it even address typos and wording variations?), but we have what we have.
&gt; You are aware that "human and machine readable requirements" is otherwise called "source code", aren't you? I'm trying to ignore the patronising tone. Not all teams are comprised solely of developers; a lot of developers out there have marketing/business/test/design colleagues that have a stake in the solution too, and it's these teams that benefit greatly (but not just them) from a common vocabulary and a process that drives a test-first mentality. The other thing I've not mentioned is tooling: if your CI can say "this requirement is not satisfied/broken" when a change is pushed, that sends a very strong signal as to what's wrong and why. Maybe you're suffering from a language barrier. There is of course a spectrum of "really awful Gherkin" to "perfect Gherkin", but in reality you still get a lot of use out of the middle. I fail to see how this is "weird": Given I have a basket with these items: | sku | quantity | price | | 1001 | 5 | 1 | | 1002 | 2 | 13.99 | When I submit my purchase Then the price should be "32.98"
Do you have links to these communities' primary online presences?
Do you have a link to this community's primary online presence?
Any Haskellers in Tübingen?
I in no way meant to sound patronizing, it was a paraphrase of know [joke](http://www.commitstrip.com/en/2016/08/25/a-very-comprehensive-and-precise-spec/). Speaking seriously: even if the feature script looks like a human text, it still is a source code. It does not essentially differ from: provide_basket (Sku,Quantity,Price) [(1001,5,1),(1002,2,13.99)] submit_purchase assert_price_equal 32.98 same way, it exactly specifies which steps should be done. The interpreter does not perform natural language analysis, it just matches with known patterns (which should be specified). If a pattern was not written, but it failed to be recognized, the test would not pass. If an action were mistakenly recognized by some other pattern, it may have unexpected result. There is, however, a difference. In the latter case, the actions are matchable by editors, so that if there are several steps `provide_basket`, it would be harder to write in other place `provide_cart`. Compare it with "I have a basket with these items:" vs "there are items in a cart:". Also it would be much easier to navigate from the usage to definition and back, when it comes to maintaining the tests. It might be that the Gherkin tools support such operations somehow, but with code-like representation it all comes for free, while in human-like it sould be some bleeding edge of text processing. That's what I meant when I wrote "weird".
1. Integer literals like `5` in Haskell are interpreted as something like fromInteger (5 :: Integer) Obviously this is a slight lie since that expression itself contains an integer literal, but you can't really express the 'raw' integer value that's actually used. `fromInteger` is a member of the `Num` typeclass, so you need to implement that to be able to use integer literals for your own types. 2. When specialised to the expression `foldMap (+5) [1, 2, 3] :: Sum Int`, `(+5) :: Sum Int -&gt; Sum Int`. Since `Sum Int` is a monoid, the `a` and `m` in `a -&gt; m` both get specialized to `Sum Int` like we need. EDIT: Answered the second question
If it's any help, here's the [cminusminus site from the Internet Archive](https://web.archive.org/web/20150408151744/http://www.cminusminus.org).
Thanks for a great article. Really a pleasure to read.
Why do you think that one cannot navigate the history in darcs ? Even SVN can do that. As for bisect: http://darcs.net/Using/Test 
We have a pretty big meetup in Warsaw https://www.meetup.com/Monadic-Warsaw/ and the community is very friendly.
https://www.meetup.com/it-IT/got-lambda/ http://wiki.portal.chalmers.se/cse/pmwiki.php/FP/FPWeeklyMeeting
&gt; graph of commit was isomorphic to a graph of delta That's not 100% correct. As far as having all versions stored and accessible, it doesn't matter if you store all snapshots, or a few as a single snapshot plus enough relative deltas. However, with Git, the "obvious" graph is between commits, and the parents of a commit are explicit and the contents of "the patch" are determined by the parent but with Darcs/Pijul, the dependencies of a patch are implicit, and the dependencies are based on the contents of the patch (i.e. vice-versa from Git). Because the edges are explicit in Git, they tend to reflect the temporal workflow of the team. A commit will often have a parent that "touches" completely different files, and the amount of branching / parallelism is generally proportional to the number of developers, with perhaps some artificial branching enforced (merge --no-ff) for tracking purposes. Because the edges are implicit in Darcs/Pijul, a patch to a file that hasn't been changed in 2 years will depend on a patch from 2 years ago. A patch that touches a dozen rarely changed files, might depend on two dozen patches with some quite far in the past and some fairly recent -- or it might depend on only one other patch; content is king and what matters most is what was changed and how recently the previous version was established. As you can imagine this can make for an interesting dependency tree. So, while delta vs. snapshot is a holographic isomorphism, that's not the graph that Git (and SVN and CVS, etc.) users are looking for.
&gt; Why do you think that one cannot navigate the history in darcs ? Because I've not been presented with a visualization tool like gitk or qgit, but for darcs. I did some searching and it looks like neither darcs nor pijul has one, though camp has a very simple one available. I certainly know history is maintained and I can do some jumping around on the command line, but I've not seen a good dependency visualization like a GitHub network graph or what I get with QGit/GiTK.
Sadly not :( Wish I could attend but Haskell'17 deadline in 10 days...
If you are asking what I think you are, then there isn't a difference. The difference between two content trees would differ in only insubstantial ways between the various diff utilities. But, the output of `git diff` isn't the only thing Git records in a commit. It's not only thing Darcs records in a patch either. Git doesn't store patches. It calculates them based on the snapshot and parents recorded by a commit, and the snapshot(s) recorded by the parent(s). Darcs does record patches. It then calculates what other patches to depend on based on what changed (I'm not sure if it stores these calculated "parents" or not). So, the dependency graph can look very different, even if the patches look the same.
I'm willing to contribute, whether it be patches, testing, etc. My knowledge of compilers is lacking but I know Haskell!
I haven't used any of these so maybe I shouldn't comment, but a lot of people seem to be using [PureScript](https://github.com/purescript/purescript) or [GHCJS](https://github.com/ghcjs/ghcjs) Here's a blog about a few different options: http://mutanatum.com/posts/2017-01-12-Browser-FP-Head-to-Head.html It also includes [Elm](http://elm-lang.org/) Again, I haven't used any of these, so I'm not sure how well they'll fit your needs nor how well any of them interact with Yesod.
If you want overloading, the first step would be to split the class into two parts, one for each module. Both functions have no relation to each other, there is no reason for them to be together. Otherwise, why not use just a record with the fields you defined and manipulate it explicitely? It seems to be a composable data! It is at least a profunctor.
If a member of a type class doesn't mention all the type variables, then using that member will *always* cause a ambiguous instance error. Proxy arguments can be used to constrain the instance when you don't need a value. For example: toRequest :: p b -&gt; a -&gt; Request getResponse :: p a -&gt; ByteString -&gt; b However, it does seem to be like you either need to (a) add bidirectional functional dependencies, (b) split into two typeclasses, or (c) avoid using type classes at all.
If type of response depends on type of request that two type classes are not possible, because `-XFunctionalDependencies` should be used. This way it's ok to use two-argument type class. class Requestable a b | a -&gt; b where 
The [*Reykjavík Functional Programming*](https://www.facebook.com/groups/reykjavikfp/) Facebook group is where we advertise all our events
One of the Fay maintainers recently posted his tool (sort of a package manager) for purescript: https://github.com/chrisdone/purify Perhaps it means they'd moved on? 
With [`Data.Functor.Reverse`](http://hackage.haskell.org/package/transformers-0.5.4.0/docs/Data-Functor-Reverse.html) you can get a ‘reverse’ view of your data structure &gt;&gt;&gt; import Data.Functor.Reverse &gt;&gt;&gt; data V3 a = V3 a a a deriving (Functor, Foldable, Traversable) &gt;&gt;&gt; toList (V3 1 22 333) [1,22,333] &gt;&gt;&gt; toList (Reverse (V3 1 22 333)) [333, 22, 1] &gt;&gt;&gt; traverse_ print (V3 1 22 333) 1 22 333 &gt;&gt;&gt; traverse_ print (Reverse (V3 1 22 333)) 333 22 1 
I've never been so I can't recommend it
Sure... that would be an appropriate use. Not sure if that's what he wanted?
The two types you wrote are *exactly* the same. So, if you want a reversed list, why not just build a list in reverse order? The 'Context' type you wrote is just a variant of a list zipper, which is more easily written as data Context n = Context { _view :: n, _left :: [n], _right :: [n] } where `Context` is a structure representing a particular point in a list that contains `_view` prefixed by `_left`in reverse order and suffixed by `_right` in regular order. If the type names help you keep them separate in your head, that's fine, but you can just use a `newtype` around list as well.
Thanks, nice to see this "wayback machine". Anyway the Tufts website has a couple more pages.
I personally disagree. The latter specifies your intent much more precisely IMO. You want a state monad with state of that monad equal to Foo. 
My usual approach is to think of types as problems and expressions as solutions to these problems. The question is, what "problem" is a requestable object supposed to solve? A `Requestable a b` is anything that provides you with two things, * a function that turns an abstract type `a` into a `Request` and * a function that parses an abstract response type `b` from a bytestring. The simplest type I can think of that would do that is data Requestable requestable response = Requestable { toRequest :: requestable -&gt; Request , getResponse :: ByteString -&gt; Maybe response } With that data type you can implement your "solutions" userName :: Requestable User Text userDetails :: Requestable User UserDetails carOwner :: Requestable Car User and maybe some combinators for convenience. -- Maps over the input of a requestable. lmap :: (y -&gt; x) -&gt; Requestable x r -&gt; Requestable y r -- Maps over the output of a requestable. rmap :: (a -&gt; b) -&gt; Requestable r a -&gt; Requestable r b -- Creates a larger requestable from two smaller ones. combine :: Requestable x a -&gt; Requestable y b -&gt; Requestable (x, y) (a, b) Edit: removed bogus combinators
The solution was fine and everything worked as expected and the code is as per the below: -- file Server.hs postList :: IO String postList = .... getPosts :: Handler [Post] getPosts = liftIO postList :: Handler [Post] but I would like to guarantee that my code read files from the files system and do not do any other IO, for that purpose I have created a new type -- file HandleIO.hs newtype HandleIO a = HandleIO { runHandleIO :: IO a } deriving (Functor,Applicative,Monad) and the below helper function openF :: FilePath -&gt; IOMode -&gt; HandleIO Handle openF path mode = HandleIO (S.openFile path mode) readF :: Handle -&gt; HandleIO String readF handle = HandleIO (S.hGetContents handle) closeF :: Handle -&gt; HandleIO () closeF handle = HandleIO (S.hClose handle) so now I can use the above function in my `Server.hs` file instead of using `plain IO monad` so the `Server.hs` page will be as the below postList :: FilePath -&gt; T.HandleIO [T.Post] postList path = do handle &lt;- H.openF path ReadMode str &lt;- H.readF handle let result = P.parse (P.beautify &lt;$&gt; P.parsePosts) str return $ fst . head $ result getPosts :: Handler [T.Post] getPosts = undefined -- stuck here !! in this case I am not able to use `liftIO` in `getPosts` function since I want to make it form `HandleIO [Post]` to `Handler [Post]` and the signature of of `liftIO` is `IO a -&gt; m a` I am not sure if I am using the right approach.
Right, but it does matter to me for finding the hidden / unintentional inter-dependencies behind the mysterious bugs I find and resolve at work. Also, the graph that gets generated by Git and it's explicit parent pointers, and the graph that gets generated by Darcs/Camp/Pijul and it's implicit content dependencies is very different and one may be more useful for what I want to do. (I tend to think the Darcs/Pijul one might be harder to navigate, since I think it's have a higher branching factor; but it'll actually reveal the problems I look for faster.) Also, I like the pretty graphs, and being able to jump into the (optionally annotated) source at any point in history.
obligatory "those are the combinators of a strong profunctor" comment *edit*: no they aren't, `combine` is like `bimap` but strength is only `Arrow.first`
We are few and far apart but I know at least 3 haskellers working in companies here in Gtb. Including myself :D
Cambridge is home to three prominent Haskellers (two Simons and one Neil) but they're not involved much in the offline community. 
Good luck with that! I started attending PwL only recently but I've been at GotLambda a number of times now, so perhaps see you there soon ?
Really, would you share which compan{y,is}? Or, if not public, in a PM? Do you program Haskell daily?
I was impressed by the London meetup, and here in Paris we start having some momentum as well. https://www.meetup.com/haskell-paris/
I hope so! Need to put it in my calendar. What dates should I save?
I was able to convince myself that Apply follows as IdealApplicative. I haven't yet sat down to work out Bind. [Edit: Oh, you meant the use of b rather than a. Fixed.]
The world is full of both artists and programmers that benefited greatly from training on technique. The world is also full of artists and programmers that didn't. Artists unfamiliar with technique sometimes create beautiful and novel new works of art. Programmers unfamiliar with technique create unmaintainable balls of crap.
ByteString specification in Coq: https://github.com/jwiegley/bytestring-fiat
I've moved onto PureScript and GHCJS. I don't know about the other maintainers.
It is exactly what I want - type response depends on type of request. Thanks. 
I am totally not experienced enough. Better apply anyway.
I did about 4-4.5 hours a day for 8 months. Wish it had been around then :)
In this case you can use `FunctionalDependencies` and add `| a -&gt; b` to the definition of your class or use `TypeFamilies`, and define class Requestable a where type Answer a :: * toRequest :: a -&gt; Request getResponse :: ByteString -&gt; Answer a and give instances accordingly.
It's never foolish to have strong preferences, it's only foolish to let those strong preferences cause you to act in a way that's against your best interest. Eg. refusing to take a job programming Java while you are starving. If you're not starving, program in whatever language you want, take whatever jobs you want, etc. If saying you like Haskell or Python is the wrong answer in the eyes of some individual then that's a reflection on that individual, not on you.
It is very rewarding to take ideas from Haskell and applying them in other languages, there are lessons that run deeper than how exactly they manifest in Haskell. During interviews you can explain *what* it is you like so much about Haskell, has it changed the way you program or view programming whatever the language? Check this thread out &gt; [*How did Haskell make you a better programmer?*](https://www.reddit.com/r/haskell/comments/3absc6/how_did_haskell_make_you_a_better_programmer/)
I'm planing to post more about: - ghc on raspberry pi (hence the setup + SDK) - ghc on android - ghc on iOS over the next weeks (depending on how much time I find write) Testing will be very welcome, especially as there might be bugs throughout the whole stack (ghc, libraries, llvm, platform), and cross compilation is not very well exercised on all those platforms. Another item that is looking to become increasingly necessary is a virtualized file system layer in Haskell. I also believe we need experience reports/blog posts wrt to using cross compilation.
You miss 100% of the shots you don't take - Wayne Gretsky - Michael Scott
&gt; porting graphics engine to haskell sweet. afaik, haskell's laziness and non-incremental gc cause latency problems; you don't know when garbage is collected, and it can be hard to know when some thunk is forced. (but i don't know much about graphic engines). also, ghc itself (not gcc) performs most of the optimizations anyways, right? 
You want a function that translates from `HandleIO a` to `Handler a`: handleIOToHandler :: HandleIO a -&gt; Handler a handleIOToHandler (HandleIO action) = liftIO action -- action :: IO a As for your design in general, it's quite sensible. I'm personally not sold on the value of breaking `IO` up into different categories of effects, but if you feel that it helps you avoid errors, go for it. By the way, check out [`withFile`](http://hackage.haskell.org/package/base-4.9.1.0/docs/System-IO.html#v:withFile). It's a safer alternative to `openFile`/`hClose` which prevents you from forgetting to close a file you opened (as in `postList`).
Yea it's not really GCC that's the backend. The default code generator goes straight to assembly after Cmm. There is a code generator that goes from Cmm to C, and then uses GCC. But it's not as good, and uses some arcane GCC-specific flags.
Those still aren't enough to select an instance for `getResponse`, because they allow for two different request types with the same response type. This either calls for a two-way dependency (`a -&gt; b, b -&gt; a`) or a pair of type families. class (Request b ~ a, Answer a ~ b) =&gt; Requestable a b where toRequest :: a -&gt; Request getResponse :: ByteString -&gt; b Alternately, if the implementation of `getResponse` does not depend on the type of `a`, you could use a superclass. class Response b where getResponse :: ByteString -&gt; b class (Response (Answer a)) =&gt; Requestable a where type Answer a :: * toRequest :: a -&gt; Request 
I think the simple answer to why lambda calculus is important in the functional programming world is that functional languages often use denotational semantics to define how they work. Where imperative languages almost always take a less rigorous approach relaying on some combination of reference implementation and operational semantics. So, roughly speaking, since functional languages define what any given code will do by explaining how that code gets transformed into lambda calculus it can be beneficial to understand lambda calculus. It is after all, sort of like the assembly language of functional programming semantics. Don't approach it like you need to learn it to write code with it. Just learn the rules of lambda calculus and move on. Just having learned those rules, even if you can't recall all of them exactly, will help when learning Haskell later.
Can you give an instance where darcs merge any more dangerous than git ? If patch-1 and patch-2 commutes in the textual sense (i.e. the edit different files) but semantically they are dependent (one changes the function name in the header file and the other in the source) then you can add explicit dependency of patch-1 on patch-2. See `darcs record --asks-deps`
Original file: Line 1 Line 2 Line 3 My Branch: Line 1 Line 1.5 Line 2 Line 3 Upstream: Line 1 Line 2 Line 2.5 Line 3 Darcs will merge with no complaints as: Line 1 Line 1.5 Line 2 Line 2.5 Line 3 With git, the behaviour here depends on your choice of merge strategy. The darcs behaviour is often what you want, but I'd want to at least be made aware of changes in such close proximity. BTW, darcs record --asks-deps is fantastic, exactly what I wanted, thanks for the info!
metafunctor is a kind of Troll. Best not to worry about it.
The fact that the git's behaviour here depends on the merge strategy itself should indicate that a version controller cannot really get this correctly. In the git setting what decides which merge strategy should be used ? At least darcs is consistent here in terms of what it does. Closeness of edits is not something that can be be easily decided. Is it 1 line or 2 line or 42 lines. What if the changes have 100 comment lines in between which are not usually semantically relevant. When you pull in changes from remote repository then darcs asks for confirmation of each patch. You should be careful at this point That said I remember something along this lines discussed in the darcs community. I do not recall what it was and what was decided.
This resonates so much with me right now. I've been a front-end dev for close to 7 years now. Lately I've gotten so into FP and FP practises, when I found myself looking for work recently after a contract ended, I'd go interview for front-end roles - perm or contract - and I would talk about my love of functional programming and of category theory and mathematics (I'm not anywhere near an expert, I can barely get by, but it still fascinates me and I want to keep learning) and I would always receive a response like "We loved your skill and energy but we don't think this role will be challenging enough for you", over and over. Recently I started hiding it just so I could find work to tide me over, but I feel like I'm crushing my soul. It hurts. I'm still looking. But I think I might have out-grown front-end, but not grown enough to jump straight into a FP language role, as much as I desperately want to. If anyone has any advice for me, please share. But please be kind, I'm already so hurt from my recent string of bad luck.
I think the first question is: where do you draw the line between the technical stacks you know you won't be happy working with and the ones that you might enjoy. There are many functional styles: untyped (Clojure, JS, ...), typed with a poor kind-system (OCaml, F#, ...) or typed with a rich type and kind system (Scala, Haskell, Purescript,...). All these approaches lead to very different styles in practice. Be precise on what you want and dont want, just for yourself. Then there are words that can affraid people like "functional programming", " monads", "pure", etc. Recruters wants two things: they want a productive dev who will integrate their team painlessly and a dev that write code most people can read and modify. Obviously if you write monadic code in C#, F#, you must exepct to see most of devs uncomfortable with your code. So don't say these words. Instead talk about why there are important and which problem they solve. Dont say you want to be functional, but that you write modular, context-free code to maximise code reuse and reduce the bug surface. Don't say pure, instead say you like explicit code that don't rely on complex state analysis to be understood. Don't say say monads but you say you know how to make the code more maintainable and generic. Functional prpgrmaming is still a new thing in the industry. Don't expect recruters to know what FP is. Instead use common words and general ideas. PS: Many Haskellers are happy doing Scala at work. Look out the typelevel.scala comunity, you'll feel at home. 
Sometimes an indirect route works best. You can carve out space at a company doing what you're good at so you have the leeway to do what you want to do. If you're the front end guy at a non-technology-oriented company, they often won't care if you use ghcjs or purescript as long as you deliver the end product.
While I fully understand the choice, I wish GHCJS could be as convenient as fay or elm for a quick one-file script.
I think you're right. I've just spent my whole career in short term contracts, I haven't learned how to bide my time yet. Maybe I just gotta give it a shot. Thanks for the advice, it is very much appreciated. 
Interesting projects. How does _bookkeeper_ differ from standard records besides syntax? The ability to modify an attribute?
What is the work environment? Is there an office? If so, where? Remote?
1. You could use Data.Conduit.Process to do piping between processes you spawn: https://github.com/snoyberg/conduit/blob/master/PROCESS.md 2. To spawn a process in a shell environment you can use https://hackage.haskell.org/package/process-1.4.2.0/docs/System-Process.html#v:shell
It's a special case of a [zipper](https://wiki.haskell.org/Zipper).
Thank you :) I have, Elm was my stepping stone into Haskell. I went from Elm to PureScript and now can understand Haskell fairly well, struggling to find things to build with it in my spare time but I'll get there. Thanks for your comment!
There, I fixed it for you: *"~~Religious~~ beliefs receive harsh judgement."*
Learning Haskell made me a better Python programmer. Mainly by avoiding stateful code as much as possible and isolating side effects to explicit code sections. Unfortunately, it is frustrating to not be able to easily replicate functionality that iteratees and Haxl give you. Not to mention error prone refactoring, even when using type annotations.
I'm in Glasgow and not aware of anything. I'd be interesting in coming but can't find anything on a search either. Could you share details?
"Functional programming" is vague. It makes you sound like a trend follower. Talk in terms of what your priorities are and what they say about you. Haskell meets my priority of maintenance (purity isolates effects, and static types isolate types of things from each other) and producing reasonably fast code (GHC is an optimizing compiler). Other priorities like fast iteration and ease of editing, both of which Haskell sucks at but which Clojure or Common Lisp meet, are secondary priorities.
In my place of work all things called "functional" are met with some kind of scorn. Even though things they don't know are functional that they've started to use they often love (I primarily work in Swift). For example they have no problem with flatMap (ie bind) for arrays and optionals as that is in the standard library. But I implemented it for Either the other day and got told my code was "obtuse, and no one in the team I've showed it to can think of any use for it". It's infuriating. When I go to conferences if I mention FP people glaze over. I see people complain in the work chat about "everything needs to be trendy and functional these days" despite what they see only being the tiny surface of FP — they see FP as something like NoSQL, something that has come out of the woodwork recently that everyone has latched onto and will go away again before long. You can even see it in the Swift development itself. There are potshots at Haskell in some of the official Swift talks. And they don't even understand the awesomeness they are getting close to but refuse to put in the language. I don't know what the answer is. Programmers have always been religious in their beliefs and I don't know how to change that.
And with Well Typed, no less ! : )
I read the comment you deleted, I just wanted to say I enjoyed it, and I did write a reply, and please feel free to share any more advice you may have. It's certainly not unwanted by any means!
I also like _Rust_ programming language. I'm using `ripgrep` constantly and already forgot about `grep` and `ag`. I think _Rust_ is very nice for performance-critical applications. But Haskell is still the best in multi-threading area due to `STM` and other nice RTS features. I want to see bright future in easy Rust-Haskell interop.
Religious people also receive a lot of harshness from other religious people.
It's only bizarre if there is a right answer.
As an author of an "mtl alternative" that supports multiple readers: &gt; need to throw away the functional dependency I didn't. &gt; teach yourself to accept overlapping instances in your code without throwing up They don't violate any of the nice properties of the `Constraint` category, and their "most specific match" semantics work great for this particular use case. I recall you had an example where they prevent a poly-kinded instance, but a `Monad` by definition has kind `* -&gt; *`, so it doesn't cause any issues in practice (I had to add a single kind annotation once, it wasn't that bad). &gt; they all come at the low low price of not being able to infer types for them pretty much at all you go to access the environment with anything that is polymorphic in the argument [`ether`](https://hackage.haskell.org/package/ether) has superb mtl-like type inference as long as you use explicit tags. Works with polymorphic environments just fine. The reason why the `lens` + `mtl` solution falls apart is that it doesn't allow me to peel one layer with `runReaderT` and remain in a polymorphic environment. E.g., with Ether if I have ``` f :: (MonadReader Foo r1 m, MonadReader Bar r2 m, MonadReader Baz r3 m) =&gt; m a ``` it's possible to write `g = runReaderT @Foo f r1` and get ``` g :: (MonadReader Bar r2 m, MonadReader Baz r3 m) =&gt; m a ``` removing a single constraint at a time. On the other hand, if with `mtl` + `lens` I have ``` f :: (MonadReader env m, HasFoo env, HasBar env, HasBaz env) =&gt; m a ``` I can't satisfy one `HasFoo` constraint at a time.
I think they not yet figure out WTF is "OOP".
Well, it's kinda foolish to love anything that won't love you back, right? For example, being able to play music is great, but additionally having a high preference for playing music over other jobs is pretty much a curse that sets you up for failure with high probability. You'd be better off with the skill but without the curse. Having a love for programming makes more sense, because programming pays well today if you're any good, but FP isn't desired by employers the same way other programming skills are. It's fun to learn, but if you've fallen in love, get over it. Love is nature's code for investment, and there's no point investing with low return.
We had a thriving community in Utrecht, but, other than the [Advanced Functional Programming Summer School](https://afp-2017.github.io/) (which I highly recommend!) and the [reading club](http://foswiki.cs.uu.nl/foswiki/Center/ReadingClub) at the university, I'm not sure if you could say that a non-academic community still exists. The [Dutch HUG](https://wiki.haskell.org/Dutch_HUG/Meetings) was pretty successful while it lasted, if I do say so myself.
time to extend this haskell beer meetup :)
It was a terrible point. You didn't miss anything. I was just ranting on very little sleep.
Agreed. At the very least combining is 3 gets us a start. Suggest we put it on meet up and keep it at the Panton arms.
I for one have a hard time adjusting to different languages. All my c++ code looks like std::vector&lt;int&gt; a {1,2,3,4,5,6}; std::transform(a.begin(), a.end(), a.begin(), [](const auto c) { return c * c; }); and I get dirty looks by other c++ programmers for this style.
Not sure where zifter fits in yet, but might help.
&gt; Isn't [Haskell] a pure representation of Lambda Calculus and Category Theory? Er, no, what makes you say that? Neither of those has type classes for example.
Huh. I've always viewed fast iteration and ease of editing as solid benefits from using Haskell for anything that takes, say, more than an hour of coding. Maybe I've been out of the Python game for too long...
To say that programming languages are "just tools" would mean that humans approach problem solving the same way no matter which "tool" they use. But in reality languages shape up your thoughts, your approach to problem solving. The way you would structure your program and your solution to the problem in haskell is very different than what a csharp or java programmer would've done. Even though all these languages provide means for encapsulation, composition, inheritance, the outcome is not the same. 
"Out Of Persistantdatastructure"
I think we're on a different page here. I'm not talking about the `HasFoo` stuff. Just saying that the reason `MonadReader` has a functional dependency is to allow inference when possible, and that requiring tags everywhere is no better than throwing that out and requiring explicit type applications. As soon as you move to `HasFoo`, yea the boilerplate becomes equal, except I guess you can unambiguously get the whole environment in one `ask` and no explicit types with `mtl`.
There's a world of difference between iterating fast a simple solution so you know you are solving the right problem, and iterating fast a complex solution so you can fit it well to the problem. O measurable difference is how often you are throwing all your code away. Python excels on the first, Haskell is much better on the later.
What you want is probably [`syntax`](http://hackage.haskell.org/package/syntax) library. It allows you to have both — parser-combinators and pretty-printers — at once. It has [`syntax-example`](http://hackage.haskell.org/package/syntax-example) package with examples of simple lambda-calculus parser. Takes this: (\f-&gt; (\x -&gt; f ((x) x)) (\x -&gt; f (x x) ) (\x -&gt; "test") (\y -&gt; y +2.0e13)) And produces this: Abs "f" (App (App (App (Abs "x" (App (Var "f") (App (Var "x") (Var "x")))) (Abs "x" (App (Var "f") (App (Var "x") (Var "x"))))) (Abs "x" (Lit (LitStr "test")))) (Abs "y" (App (Var "y") (Lit (LitNum 2.0e13))))) \f -&gt; (\x -&gt; f (x x)) (\x -&gt; f (x x)) (\x -&gt; "test") (\y -&gt; y 2.0e13) It can produce both: AST and pretty-printer result as you can see. Whole parsing module with comments, [main module with running functions and parser](https://github.com/pavelchristof/syntax-example/blob/master/Main.hs) has about ~100 lines. And the main parser itself is very concise: -- | An atom is a variable, literal or an expression in parentheses. atom :: SyntaxText syn =&gt; syn () AST atom = _Lit /$/ literal /+/ _Var /$/ name /+/ parens expr -- | Parses a list of atoms and folds them with the _App prism. apps :: SyntaxText syn =&gt; syn () AST apps = bifoldl1 _App /$/ S.sepBy1 atom S.spaces1 expr :: SyntaxText syn =&gt; syn () AST expr = _Abs /$~ S.char '\\' /*/ S.spaces_ /*/ name /*/ S.spaces /*/ S.string "-&gt;" /*/ S.spaces /*/ expr /+/ _Let /$~ S.string "let" /*/ S.spaces1 /*/ name /*/ S.spaces /*/ S.char '=' /*/ S.spaces /*/ expr /*/ S.spaces1 /*/ S.string "in" /*/ S.spaces1 /*/ expr /+/ apps 
Stroustrup and Sutter [agree with you](https://github.com/isocpp/CppCoreGuidelines/blob/master/CppCoreGuidelines.md#Res-lib).
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [isocpp/CppCoreGuidelines/.../**CppCoreGuidelines.md#Res-lib** (master → b10ffdf)](https://github.com/isocpp/CppCoreGuidelines/blob/b10ffdf55fed3cdc8067b7ef2e5ba020678c2b62/CppCoreGuidelines.md#Res-lib) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dhgxwma.)^.
The FP.AMS meetup is full of Haskellers, too
&gt; topological sort of the patches to feel git like. But possibly it can do more. All these arguments have nothing to do with how the history is kept. Even if a topological sort is applied, it wouldn't feel git-like, because the dependencies stored are different. With Git the edges are based on parent pointers, and git records those independent of content changes. With Darcs the edges are based on content changes, whether or not darcs records them separately.
Everytime I see a comparison between programming languages and carpenters I'm reminded of [this gem](https://eev.ee/blog/2012/04/09/php-a-fractal-of-bad-design/). &gt; I can’t even say what’s wrong with PHP, because— okay. Imagine you have uh, a toolbox. A set of tools. Looks okay, standard stuff in there. &gt; &gt; You pull out a screwdriver, and you see it’s one of those weird tri-headed things. Okay, well, that’s not very useful to you, but you guess it comes in handy sometimes. &gt; &gt; You pull out the hammer, but to your dismay, it has the claw part on both sides. Still serviceable though, I mean, you can hit nails with the middle of the head holding it sideways. &gt; &gt; You pull out the pliers, but they don’t have those serrated surfaces; it’s flat and smooth. That’s less useful, but it still turns bolts well enough, so whatever. &gt; &gt; And on you go. Everything in the box is kind of weird and quirky, but maybe not enough to make it completely worthless. And there’s no clear problem with the set as a whole; it still has all the tools. &gt; &gt; Now imagine you meet millions of carpenters using this toolbox who tell you “well hey what’s the problem with these tools? They’re all I’ve ever used and they work fine!” And the carpenters show you the houses they’ve built, where every room is a pentagon and the roof is upside-down. And you knock on the front door and it just collapses inwards and they all yell at you for breaking their door. &gt; &gt; That’s what’s wrong with PHP.
Hmm, I understand your point... I'm not trying to say that the "essence" of a list gives rise to only the monad instance. I'm trying to say that people use that "essence" to obtain the knowledge needed to implement pure and bind functions. In the same way that people need to know how lists work in order to implement Monoid, Functor, Applicative. Typeclass instances for a type are written based on that type "essence", which gives them different powers, or else they would be equal and without interesting usages. I mean... typeclasses instances implementations are different because the types behind them are different! Thanks for your answer, it make me realize that this is something that goes way beyond monad :o
[removed]
Where will convergence of lisps and type theories end up? Will there be a lisp designed for some type system with solid type inference? Can modern computational theories be bent to verify the continual normalisation fundamental to lisps? Etc etc.. Despite the nice inference properties of, say, the modified HM type systems, I'm often hamstrung by languages with a mere two outward evaluation stages, compile+run. Going outside the language for code-gen is unfortunate and complicated. (Really excited about Idris' trade-offs for this reason.) 
How would this look like in the presence of OverloadedStrings?
To accompany the good advice here on how to present your interest in functional programming, here is a contrarian viewpoint that lies further along the spectrum: http://jspha.com/thoughts/2016/11/30/there-is-no-functional-programming-only-culture . It makes for good food for thought, even if you don't subscribe to the author's call to arms.
I see this sentiment often. "They're just tools, use what gets the job done". But have you considered why a person chooses to do a job in the first place? Why is a carpenter a carpenter? Why is a programmer a programmer? Because the carpenter enjoys working with the tools belonging to that profession. Likewise for the programmer. If a tool or any other element in your work can make your job more pleasant, then it's not unreasonable to think that there can be a preference of such.
Well if we are going off into allegories, then i'd compare csharp and java for example with a normal screwdriver and power screwdriver. Very different tools that look and act nothing alike. But regardless of which you chose you would do the work exactly the same way. They would not change neither your approach, nor your thinking about the problem. Now enter a 3D printer. And suddenly the way you think about the problem changes, dictated by new abilities this tool opens up for you. 
I'm going to say it: SPJ is outrageously inspirational
No, the hammer and the screwdriver was the right analogy (not allegory, which is a story with a moral message). A hammer and a screwdriver both allow you to do more or less the same thing just in a different manner: attaching two things together with nails and screws, respectively. A screwdriver and a 3D printer do entirely different things, which is not at all like Haskell and Java or C#, all of which are Turing Complete and do *precisely* the same set of things, just in a different manner. Haskell is just a programming language. It's great but it's not some magical silver bullet either.
Makes sense, but functional-paradigm languages share features that meet priorities like maintenance. Considering the rarity of functional language jobs compared to OO, I'm happy to do whatever functional-style job I can get my hands on. But I do agree with you that it is better to give practical reasons for being interested in FP. 
wow, good job, the app looks super nice and usable! I haven't taken a look at the code but I love it when people publish example applications like this. Not only have you finished a project -- something I have a lot of difficulty doing -- but by publishing it you'll help others learning the same tools. 
Maybe core libraries could provide these somehow, either indirectly through extra-*-files or through some direct mechanism, if cabal provides a way to publish c headers
Unrelated, but I was curious about Neil's note about being conservative about language extensions generally? Not trying to debate it, just curious to learn if I should be considering more factors than I already do, before using one.
As it happens I was modifying them today so now they are a lot more robust than when I wrote that blog post. There's also a powerscript one for Appveyor I'm still debugging. 
I almost mentioned ["curryrs"](https://github.com/mgattozzi/curryrs) in my post, check it out. I totally agree that Haskell is better for asynchrony, it's still the best language for asynchronous programming in my opinion. I'd use it for any mostly-IO workload like a server, but for end-user software I don't think you ever need the full asynchronous ability that Haskell gives you and Rust's gimped equivalent is enough.
No one wakes up in the morning and wants to use CPP (: . I haven't read a lot on advanced techniques for avoiding CPP. I assume using cabal flags to swap out a source-dir/module to compile different code for different versions of dependent libraries is the best people have come up with.
I'm interested in reading the original typeclass proposal email from Philip Wadler called "Overloading in Haskell" which is mentioned in this talk. I can't seem to find it anywhere. Does anyone know where I can find it?
This is a really good idea.
[Pinchot](https://www.stackage.org/package/pinchot) - write grammars, not parsers. You write out your grammar; uses [Earley](https://www.stackage.org/package/Earley) to create a parser; you can also pretty print the parse results.
I really liked this comment you have made: &gt; It suggests a nice intuitive picture: In f a, the a values, if any, will be found in a context whose nature depends on what f is I will modify the gist to reflect this notion. Thanks for your review :) 
I have already updated the gist to follow your suggestion :D
The new compact regions feature in GHC 8.2 would be a great fit for something like loaded graphics (3D models, textures, etc.) as it wouldn't be touched by the GC.
The new compact regions feature in GHC 8.2 (out soon) could be a great fit for something like loaded graphics (3D models, textures, etc.) as it wouldn't be touched by the GC. 
You know, if I were to ask that question in an interview, a lack of opinion would be the worst possible answer. Don't be ashamed of having opinions. Just remember to ramp up your excitement about them slowly, rather than starting out overbearing. I find haskell to be the best language I've used for writing any software that doesn't have requirements that garbage collection interferes with. It is easily my favorite programming language. But I start out with "I like it" before I start telling everyone about all the small details of why. The details can wait for the listener to start showing interest in hearing them. :) 
I'm not convinced it would work even for new versions going forward For example, how would the GHC 8.2 release know what macros to export to provide compatibility with changes introduced in GHC 8.4?
Wow; an impressive amount of polish for the result of a university course. Great work!
What makes it inconvenient for a one line script?
Reactions: + SPJ has a cat named Haskell! + He wants Haskell Weekly News back, but it already is (https://haskellweekly.news/). It even has the endorsement of the old one (https://wiki.haskell.org/Haskell_Weekly_News). + The audience is really quiet. + He talks about how polite Haskellers are and how special this is. I hope the community can start living up to this again. IMO you can express emphatic disagreement while still being polite, these things are pretty orthogonal. + Oh snap! The audience wakes up and immediately launches a surprise attack. "Whatever happened to the formal semantics of Haskell?" SPJ's answer is interesting (timestamped link: https://youtu.be/re96UgMk6GQ?t=28m54s). + It's a little weird looking at a slide that claims `f (print "yes") (print "no")` doesn't make sense in Haskell (from here: https://youtu.be/re96UgMk6GQ?t=31m30s). To me that makes perfect sense! It took me a minute to realize SPJ is talking about a hypothetical Haskell with side effects and `print :: ()`. I'm worried this may make people think Haskellers have trouble manipulating IO actions, when in fact the opposite's the case. Overall a great introduction to Haskell's history, thought that's not surprising since SPJ is one of my favorite public speakers.
Declaring a preference for functional programming should be a huge plus for an employer. The functional style has made my Ruby and Python coding worlds better because of: * Far fewer local variables (usually none) * Immutable data structures * functional-style transformations instead of loops * Far fewer if/thens (usually just one or two in any program) IMO these all result in fewer lines of code with fewer bugs, easier to test, and it's easier to understand. Ruby's syntax makes all this especially easy. E.g.: def titleize self .tr('_', ' ') .split(/[[:space:]]/) .map { |w| titleize_word(w) } .join(' ') .capitalize_first_letter end
If you don't mind my asking, do you also have roles for devs without much Haskell expertise but who want to move into a Haskell role? That's my situation at the moment, front-end dev for many years, really been getting into haskell in my free time over the last year or so, not much in the way of expertise in it but I've realised Pure Functional Programming is where I need to be in my career. For my sanity, at least.
Great job! Can anyone explain the difference between the `gtk` package and `gi-gtk`? I'm interested in writing some GUI apps myself, and I've always thought that `gtk` is the way to go, but now I'm confused. PS. I don't know GTK, I'll be learning it on the way.
Though it probably wasn't in the 60s.
What's wrong with that?
And He also launch missiles... 
You can use a state monad to transport the pipe handler so that you can construct over this state, a monad that uses `&gt;&gt;=` for piping shell commands quite literally. (No joke)
I don't follow: #ifdef ApplicativeMonad would be backwards comptible to GHC 7.4 if 7.8 introduced it. I agree this is not introducible in any post-AMP compiler though. I really like /u/hvr's package feom his comment for this reason! 
Using `read` is ok, but you may want to use `readMaybe` [instead](https://hackage.haskell.org/package/base-4.9.1.0/docs/Text-Read.html#v:readMaybe_)
Make that 4. I am also close to that Clojure-y thing (NonDysFunctional Programmers) and we're open to hosting Haskell talks (or any functional language for that matter).
Foolish would be having a strong preference for using C as a high level programming language.
Brno has a fairly decent Haskell community with ties to academia and industry and also a [meetup](http://fpbrno.github.io/).
Your code has an unhandled pattern case, if the list isn't an exact multiple of 3. You might want to make another function that splits a list to chunks and then act on the chunks, returning an error or `Nothing` if it fails. Another approach is to ignore the remainder if it's not divisible by 3. In addition the suggestion to use `readMaybe`, you could then use `mapM` to a return a `Maybe [Article]` to easily combine them. In general, if an existing traversal functions (e.g `map`, `foldl'`) exist, it's generally better to use them rather than to roll your own loop. It lets you split the concerns of actions on the data structure ("iterate the list, map and combine") versus actions on the data specific to your case ("this is what to do on each item and this is how you combine any two items".
It's funny you call SPJ attitude 'balanced and scarce', when he's one of the purist who prefered keeping evil IO out until it was pure enough to get into haskell. You either don't understand this community or come here to bash it.
Don't know why this is being downvoted, people who did probably haven't read any of SPJ's papers. 
You can use my `turtle` library for this purpose. In addition to providing convenient wrappers for running subprocesses `turtle` also tries to reimplement as much command-line functionality natively in Haskell. For example, the way you would write this natively in Haskell would be: {-# LANGUAGE OverloadedStrings #-} import Turtle main :: IO () main = ("111 " &lt;|&gt; " 222") &amp; limit 1 &amp; stdout ... but if you preferred to use subprocesses you could write: {-# LANGUAGE OverloadedStrings #-} import Turtle main :: IO () main = empty &amp; inproc "echo" ["-e", "111 \n 222"] &amp; inproc "head" ["-1"] &amp; stdout 
Hi, you represent the tape as a list and index. In functional programming we often use a zipper datastructure. In this case you would represent the tape as two lists. One the tape left of the head and one for the right. Then moving the head means just removing one element from the top of a list and adding it to the other. Use a stack for the brackets. When you reach the closing bracket, push the head of the stack and jump to this position. I don't think, that a fold alone is enough for executing the program. Just write your own recursive function.
Hey I am interested for this .... I am looking for a long time to find a job with Haskell.Nice to hear that its available in India.
Am based in Bangalore, still a Haskell newbie and still learning. But it's great to see this.
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [quchen/articles/.../**write_yourself_a_brainfuck.md** (master → f9ae32e)](https://github.com/quchen/articles/blob/f9ae32ea9ff8a9737d868494f4abd128a026090d/write_yourself_a_brainfuck.md) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dhigitt.)^.
metafunctor is a known troll. Don't feed it.
Are you considering remote candidates?
Yep. Truth is impopular
Please upvote this guy. He need it
MIN_VERSION_GLASGOW_HASKELL(X,Y,Z,W) is only defined on GHC 7.10.1+ The other MIN_VERSION_foo macros are all supplied by either ghc or cabal.
I have though of a function that splits the list to list of tuple `[String] -&gt; Maybe [(a,b,c)]` - if this what you mean by chunks - and then I map over that list to get `Maybe [Article]`. I loved the separation of concerns but again I will run into the same issue while defining `chunks` function: chunks:: [String] -&gt; [(a,b,c)] chunks lst = case lst of [] -&gt; [] (a:b:c:xs) -&gt; (a,b,c) : chunks xs it is a lot easier to reason about this function in term of (iteration, map and combine) even though I am sold into doing it using traversal function 
Yes we are.
This is hands down my favorite podcast out of my dozen different ones. Thank you thank you!
Regarding the loop, you could transform the linear list of operators into a very simple "AST" where a loop would actually be a list of instruction in itself. You can have a look at my own implementation to see how that would work (type definition: https://github.com/remusao/Hodor/blob/master/src/Language/Brainfuck/Internals/Instructions.hs#L11). To interpret the program you could map over the list of instructions while updating a state: https://github.com/remusao/Hodor/blob/master/src/Language/Brainfuck/Internals/Interpreter.hs#L55 You don't have to use the State Monad, your approach of keeping track of the state explicitly should work just fine. As already suggested, you can use a Zipper to represent your memory tape. In which case it will just grow as needed lazily. Also, any character not part of the Brainfuck language should just be ignored, and not cause an error. Otherwise you won't be able to run programs containing comments.
Way way less than the other way around, except perhaps in comment sections on the internet.
Is the W field for RCs or something? If so, how do the RCs and the real versions line up?
Overlapping instances strike me as a huge mistake. They're fragile, they're error-prone, and they're intrusive. They also clash with associated types, so library authors must consider the possibility of breaking user code if they add an associated type to an exported class. Ugh.
&gt; They also clash with associated types, so library authors must consider the possibility of breaking user code if they add an associated type to an exported class True, unfortunately. Type families need overlappable type instances as well in order to support the same range of use cases, ha-ha :) &gt; They're fragile, they're error-prone, and they're intrusive. I have a particular use for them where I don't find those statements true. A single `{-# OVERLAPPABLE #-}` instance for lifting monadic operations by default is a working, robust, simple, extensible/modular solution to the problem of N*M instances. With the stigma attached to -XOverlappingInstances, I'd gladly use an alternative solution, but there isn't an alternative solution that works equally well. I'd like to stress modularity here. If package `foo` defines a class `MonadFoo` and package `bar` defines a transformer `BarT`, I either write an orphan `MonadFoo m =&gt; MonadFoo (BarT m)` or there needs to be a dependency between `foo` and `bar`. However, if `foo` provides `instance {-# OVERLAPPABLE #-} (MonadFoo m, MonadTrans t) =&gt; MonadFoo (t m)`, the problem is solved.
Interesting that so many of these are machine learning or AI focused. Imagine if Haskell had a mature data science stack.
Aww. No love for the narwhal?
I agree. For me the type context is too general to have a good way to label it concisely. Maybe people who have studied it more have better words. I just use container since it's the most common context I come across.
I mean you do get some weirdness like "Const ()" is a functor, as is "Id" but often it makes sense to include the kind of stupidly trivial examples in a concept, and don't really have a problem with trivial functors being trivial containers. The fact Map and Set aren't functors is a problem..but is kind of an implementation detail not a conceptual issue. Its just that the implementation doesn't work with types without computable comparison. Also there is a "strict" meaning of "container" that doesn't include IO, or free monads, or (-&gt; r) for all r's. (IIRC it ends up translating to polynomial functor sometimes with added equalities. I.e. "cycles" are lists where you identify all rotations, or equivalently forget where the list starts, and bags are lists (or some trees) where you forget all positional info, i.e. this often implemented as Set in haskell)
Please pm your profile/resume. Thanks :)
We should certainly keep in touch, nonetheless. Happy to see you pick up Haskell.
The easiest filter is if any of those companies go to the trouble of having one of their Haskell developers post here, then they are more likely to be committed to it. From what I hear from some people on this forum, many people are looking for a transition job where they can do some Haskell as well, as a first step, as long as they know the ratio they are getting into up front.
Umm... Maybe I'm missing something but that seems to give you absolutely nothing. As the parent said, neither function mentions both type parameters, so even the FD approach has no benefits over two typeclasses. Like what can you do with that and the above functions in the OP that you can't do with: class Requestable a where toRequest :: a -&gt; Request class Respondable a where getResponse :: ByteString -&gt; a
The easiest way probably would to cut out the loop body and then recursively run that substring with the current state. Alternatively build an ast representation that would be easier to interpret. You also might want to look into zippers because indexing into lists is a O(n) operation which becomes very expensive very quickly when done repeatedly with large indices. I attempted a write a straightforward version but I feel like every time I try to write a brainfuck interpreter in haskell it becomes slightly less readable. {-#language FlexibleContexts#-} import Control.Monad.IO.Class import Control.Monad.State import Control.Lens hiding (translate) import Control.Monad.Loops import Data.List (break) data Tape = Tape [Int] Int [Int] empty :: Tape empty = Tape (repeat 0) 0 (repeat 0) shiftLeft, shiftRight :: Tape -&gt; Tape shiftLeft (Tape ls c (r:rs)) = Tape (c:ls) r rs shiftRight (Tape (l:ls) c rs) = Tape ls l (c:rs) evalProgram :: String -&gt; IO Tape evalProgram s = execStateT (translate s) empty translate :: (MonadState Tape m, MonadIO m) =&gt; [Char] -&gt; m () translate ('+':xs) = current += 1 &gt;&gt; translate xs translate ('-':xs) = current -= 1 &gt;&gt; translate xs translate ('&lt;':xs) = modify shiftLeft &gt;&gt; translate xs translate ('&gt;':xs) = modify shiftRight &gt;&gt; translate xs translate ('.':xs) = do c &lt;- use current liftIO $ putChar (toEnum c) translate xs translate ('[':rest) = do let (xs, ']':ys) = break (==']') rest untilM_ (translate xs) ((==0) &lt;$&gt; use current) translate ys translate (_:xs) = translate xs translate [] = return () current :: Lens' Tape Int current f (Tape l c r) = (\c' -&gt; Tape l c' r) &lt;$&gt; f c 
But that gives you: getResponse :: Requestable a =&gt; ByteString -&gt; Response a So `a` is never actually directly mentioned explicitly in the type signature, so not only does the response type not depend on the request type, but you actually can't compile the above without `-XAllowAmbiguousTypes`, and can't use it without `-XTypeApplications`.
I mean that makes it work, but that seems no better than two typeclasses, what does it give you exactly? Can you give a concrete example of what it can do that two typeclasses can't? Fundamentally we still have `getResponse :: Requestable a b =&gt; ByteString -&gt; b`, so the response type does NOT depend on the request type.
Is that logo lambda icon + type errors? very cool.
Yeah it's 'bind' and lambda
I think the W was an attempt to learn from the cabal macros. Folks have been known to release "fixes" in patch level releases that are in the 4th place and screw up in such a way that folks need to actually care about that digit. I'm not aware of whether or not release candidates are being rigorously tracked now.
This is my desktop background right now http://i.imgur.com/HyFyV.jpg
I'm in a similar position :) Still very new to Haskell. I'm very interested in this as well. I'd love to keep in touch. If possible, can you share your story on why you guys decided to use Haskell as opposed to other mainstream languages?
I think we know Sibi P!
I'm not sure which of the three things I described that you are referring to. For the solutions with two functional dependencies or two type families, the response and request types have a one-to-one relationship. Each can be derived from the other. In particular, with two type families, the signature you give is equivalent to `Requestable (Request b) b =&gt; ByteString -&gt; b`, which works fine. In contrast, having two classes creates no relationship between requests and responses. Without knowing more about the original problem, it isn't clear which one is preferable.
It should work with functional dependency or inject ive type family (the type family declaration doesn't have to be inside the class). However I don't see what stops you adding a type signature, you surely can write x = getResponse bs :: Response Int Can't you ? (But I havent tried to compile it so maybe I'm wrong)
Just an advice, not an improvement: you can [search this](https://github.com/search?q=MonadTransControl&amp;type=Code&amp;utf8=%E2%9C%93) in github to file some usecases to help you understand it.
Looks like Machine learning and AI are the only jobs haskell or otherwise. So Data science stack might be really good to have. Whatever happened to the HLearn? From a brief look at it, it seemed to hit all the correct chords, monoids and homomorphisms all the things we like, and could revolutionise practical machine learning Disclaimer: I am not very familiar with Machine Learning.
BTW, what is MonadBaseControl for? I could never quite get it..
I think maybe that's the point of this request.
See yeah. Something totally different like that could work. I actually suggested something essentially equivalent elsewhere in the thread. And regardless of type family injectivity or data families or whatever, you are fundamentally going to not be forcing stuff to get paired up. Not with the original approach that is. The request is throwing away the inputted type, and the response is not looking for an inputted type. So you may as well use two separate type classes, it will be equal in power. 
It's all in the types. You are in the path of a dangerous negationism of Typolatry that may result in your expulsion from the community
The docs say this was based on the [control-peel]( https://hackage.haskell.org/package/monad-peel-0.2.1.2/docs/Control-Monad-Trans-Peel.html ) which has slighly more documentation, so you may want to start there. I have never needed the `monad-control` package, so I'm not the one to do documentation for it. If you don't understand what this package is for or how it is supposed to be used, what makes you think this package will be useful to you? Is there a way you could solve your problem without it, for example, using just ordinary monad transformers? From the types and the few pieces of documentation I can see, I can kind of guess how it is supposed to work. It looks like the [`peel`]( https://hackage.haskell.org/package/monad-peel-0.2.1.2/docs/Control-Monad-Trans-Peel.html#v:peel ) function was renamed to [`liftWith`]( https://hackage.haskell.org/package/monad-control-1.0.0.5/docs/Control-Monad-Trans-Control.html#v:liftWith ), and the return type of [`peel`]( https://hackage.haskell.org/package/monad-peel-0.2.1.2/docs/Control-Monad-Trans-Peel.html#v:peel ) was renamed to the [`Run`]( https://hackage.haskell.org/package/monad-control-1.0.0.5/docs/Control-Monad-Trans-Control.html#t:Run ) type. The point of it seems to be to allow you to generalize a way to evaluate a monadic action in the transformer layer just below the current layer. For example, if you have a Stateful monad `MyMonad` which wraps a transformer `StateT` and you don't want to expose the constructor for `MyMonad` but you do want to provide a way for users of your code to still evaluate a `StateT` directly within your `MyMonad`, then you can instead instantiate [`liftWith`]( https://hackage.haskell.org/package/monad-control-1.0.0.5/docs/Control-Monad-Trans-Control.html#v:liftWith ) function which would return a version of `runStateT` that you can bind to a temporary variable (e.g. `run`) which you can then use to evaluate the lower-level `StateT` monad within your `MyMonad` by evaluting `run` in your `MyMonad` procedure. 
It's a bit shocking to me to be honest. The Go code is all reasonable, there's no benchmark-hacking in it. Yet even then, the fastest Haskell implementation, Yesod/MySQL, would require _four times_ the hardware to handle the same load. If Postgres were a hard requirement, Yesod would require _seven times_ the hardware. Lets say you have two 60GB [Linode Instances](https://www.linode.com/pricing) for your Postgres master and slave, and four 4GB servers for application-level stuff, written in Go. Your annual bill is $6,720. If you rewrite in Haskell/Servant, you'll need five times the number of application servers, and so your annual bill will go up to $10,560. Relative to developer time, particularly for debugging, it's maybe not awful, but it still makes Haskell a hard sell.
Well, remember: these are microbenchmarks. It would be interesting to compare all Haskell frameworks with "raw" solution based on `warp` and `postgresql-binary`, to get the "cost of abstraction". (There is `wai`, but it doesn't implement db-benchmarks) E.g. routing like https://github.com/TechEmpower/FrameworkBenchmarks/blob/670058a14cbc9eb28229762df23e5642211ca845/frameworks/Go/fasthttp/src/server-postgresql/server.go#L44-L61 is fast, but not something I'd like to write. Also content-negotiation is costly, and optimising for specific db-workload (e.g. the go version above prepares the SQL query, which in this case is probably a win - but not in all real-world situations). 
"I don't like this" seems like a pretty bad reason to make a breaking change to Prelude.
That looks like it, thanks!
There is a bits-bytestring package that provides the Data.Bits class.
So useful! Every time I encounter a new concept I look for it on GitHub, gists, Google, .. until I feel comfortable with it
I don't know why you're mischaracterizing my argument as "I don't like this". The argument is that no one uses it, and hence it pollutes the namespace, thus it should be removed. 
At all three of the real companies for which I have written real Haskell in exchange for real money we have used the `Prelude` in our *real programs*, believe it or not! It's quite a pain to have the useful name `init` reserved by a useless `Prelude` function.
I'll present a variant of the parent's argument I think is stronger: The Prelude is unacceptable for either education or production use. Also, it changes so slowly that this will continue to be the case for a long time (for evidence of this let's take your `init` suggestion: shockingly good, obvious, easy change, still hasn't happened yet!) For these reasons instead of directing our energy at improving the Prelude, the community should pick one of the alternate preludes to standardize our educational material around and make the default for production code.
I think I'm helping
I would politely suggest to you that you're not helping. You're clearly smart and you have good ideas about how the community should evolve. I think your efforts would be very well spent documenting, or indeed writing, important libraries.
I think these benchmarks reflect how many developer-hours have gone into each of the implementations, as opposed to some truth about the language it's implemented in. The relevant question, to me at least, is: how many man-hours of work has gone into each implementation (and everything it builds on top of)? An optimized C-library with a Python wrapper around it doesn't exactly speak about the performance of Python. It just means there are a lot more Python developers out there than Haskell ones. 
Whole truth is often uncomfortable. People like to flock around feel good, superficial/partial truths. Anyone attempt to expose the "uncomfortable" parts will be met with the reaction you yourself getting around here...Not implying that what you are doing is the same. But the reaction seems similar.... 
The narwhal is the best! https://wiki.haskell.org/File:NarleyYeeaaahh.jpg
*Personal opinion ahead* In my experience with only one "implicit" parameter it's easier just to explicitly state it like you are already doing. Why constrain yourself to use "ask", "asks", "mapReader" and monads for just one parameter? If you have a more complex "context" that is required for most of your program to work then I would go for a reader. 
Thanks, I can confirm that this has what I need.
I looked through `Prelude` and the only thing that I think *really* shouldn't be there is `init`. It'll be easiest if I just pretend it doesn't exist and everyone can go about their lives unperturbed.
&gt; Personal opinion ahead This is my personal opinion too. I would start reaching for monad once manual parameter passing has become far too cumbersome.
There are unofficial bindings for TensorFlow: https://github.com/tensorflow/haskell I have just played around a bit with this so I can't say how the Haskell bindings compare with Python support.
&gt; For the on-site role, you have to be allowed to work in England. Are you including EU citizens with that? I don't mind moving to england, and currently I'd be allowed to work there, but naturally I don't know what the situation will be in a couple years from now. In any case, I'm looking for full time employment starting next year. I have a couple of years toying around with Haskell in hobby form, some professional experience in form of a one year internship at IBM (Java/Scala, no Haskell), and I'll (very likely) have a master's degree in CS by the end of the year. Is that roughly an acceptable profile for you?
I'm happy to sign up to that team. I just think that the "get `init` out of the Prelude" team should be 10 times bigger than the former.
Just to add &gt; An optimized C-library with a Python wrapper around it doesn't exactly speak about the performance of Python. True, but it tells you exactly how fast you can expect a web-app written using that Python library to perform; and the source code will tell you how complex it is to achieve that speed. This isn't testing languages in general, it's testing platforms for web-app development. However Python is a straw man. The overwhelming majority of language implementations _don't_ do this. If you look at the source, you'll see most are short and idiomatic. It's worth wondering why the very functional [Scala/Fintrospect/MySQL code](https://github.com/TechEmpower/FrameworkBenchmarks/blob/master/frameworks/Scala/fintrospect/src/main/scala/FortunesRoute.scala) is twice as fast as the imperative [Haskell/Yesod/MySQL code](https://github.com/TechEmpower/FrameworkBenchmarks/blob/master/frameworks/Haskell/yesod/yesod-mysql-mongo/src/yesod.hs#L244-L254)
IIRC, previous discussions about these benchmarks in this subreddit ended up concluding that these benchmarks are more about JSON parsing, DB performance, etc than the actual performance of the web frameworks. It's kind of similar to how in a triathlon swimming speed is pretty much irrelevant as long as you don't drown. It's the bike and run that determine the race.
It doesn't *explain* MonadBaseControl, but I wrote a [brief tutorial](http://haskell-distributed.github.io/tutorials/3ch.html#monad-transformer-stacks ) segment with a motivated example of how to use it in Cloud Haskell. It is only superficially related to Cloud Haskell and doesn't require any knowledge of Cloud Haskell to follow it. 
*What* isn't true? I described three methods for avoiding ambiguity when you have a two-parameter class with some methods that don't directly use one parameter or the other. Using two functional dependencies or two type families creates a one-to-one relationship between the types such that you can correctly identify the instance even when only one type is available. In particular, if I have instances `Requestable A B` and `Requestable X Y` and a function `foo :: Requestable a b =&gt; a -&gt; b`, I know that `foo` isn't a function from `A` to `Y`. Any use of `getResponse` and `toRequest` in `foo` must use the same instance. Whether or not that is useful depends on what the OP is trying to do. I never made any claims that such an API forces you to use the methods together. Obviously, once you have converted something to a ByteString, you are free to decode it with whatever method you have handy.
It just seems weird, because GHC patch levels *are* the third component, and (to the best of my knowledge) there is no fourth.
Except literally every comment by this guy is the same thing. The only thing he's ever said is that he hates this community for being too academic, always in some colorful and outlandish way. He's never talked about any other topic, like his opinion on a library, or providing help to a beginner, or asking how to accomplish something. It's this same inflammatory content with the same inflammatory wording every single time. I can't think of any explanation for such behavior other than that he's an actual troll, whose purpose on this sub is solely to make people angry and get downvotes.
Do you know of better benchmarks?
I agree a data science stack would be very good to have. Like many projects HLearn was a one-person project, so it kind of faded as Mike had other priorities. I think there were some technical issues around subhask that were never quite resolved. Performant code in Haskell remains a tough (but probably not intractable) nut to crack. However, if nothing else the success of python/julia that good-enough solutions can go a long way even if the perfect solution remains to be found. The right abstractions for data science also haven't been figured (not just in Haskell, but any language). It's just that other communities are more willing to move forward with dynamically typed crashy interactive scripts where as haskellers are scratching their heads over things like a dependently-typed tensor library and the right way to do a numerics typeclass hierarchy.
&gt; I'm fairly certain the number of man-hours that have gone into the Go stack (from compiler and up) are at least an order of magnitude more than the case is for GHC (and up). Go is ten years old and sponsored by Google. GHC is 25 years old and is sponsored to a lesser extent by Microsoft Cambridge. I don't think the order of magnitude is as large as you suggest. I think the difference is where the work-hours have been focused. &gt; Benchmarks ... don't reflect much on the language, but primarily on the number of man-hours that have gone into tuning the underlying implementations (of compilers, libraries, etc.). So one should judge a development-platform based on the _potential_ quality of the apps a _hypothetical_ implementation might create in the future? There will always be a trade-off between correctness and performance. That does not mean performance shouldn't matter in the here and now. Ultimately, I worry sometimes that the broader community sees Haskell as a language for building tools to build apps instead of a language to builds apps outright. Imagine you're a contractor. You're competing with a Java contractor. You both send in estimates. Yours requires five times the number of EC2 instances to run. How has Haskell helped you in that situation?
This is one of those things where I think the work outweighs the benefit. Why put ourselves through a whole deprecation cycle? It just a bunch of effort that will leave us with straggling packages or old code that doesn't build anymore. All for the sake of freeing one variable name. Don't get me wrong, I agree that it'd be nice. I just don't think it's a reasonable use of anyone's time, and the annoyances it'll cause by breakage will far outweigh the annoyances currently caused by its presence.
Well, it's easier to call a normal function than having to call it within a reader monad context (that you need to create). It might be easier to test. I would write the function as you did. If you really find it tedious (as API user) to pass the file around you can still the actual function in reader monad.
I still haven't seen a really compelling use case for `MonadControl`/`MonadBaseControl`. Most of the uses that I've seen could be replaced with `MonadError` or `MonadManaged` (both of which have nice laws that instances obey) For example, take [this post](http://www.yesodweb.com/book/monad-control) which was linked elsewhere in this thread. You could use the `managed` library to change the type of `withMyFile` to: myFile :: MonadManaged m =&gt; m Handle ... and you can also change the type of `sayHiError` to: sayHiError :: (MonadError MyError m, MonadIO m) =&gt; Handle -&gt; m () ... and then use them both like this: both :: (MonadManaged m, MonadError MyError m, MonadIO m) =&gt; m () both = do handle &lt;- myFile sayHiError ... and then instantiate `both` to this specific monad transformer stack: both :: ErrorT MyError Managed ()
Does this work with the bracket pattern?
Ah, so laziness is involved too :P
I don't think I'm mischaracterizing it. I certainly agree that less namespace pollution is a good thing, but I don't see how it's enough of a good thing to justify a breaking change; Especially considering in the few instances where it might actually bother you there's a trivial workaround.
I doubt very much there will be any breakage because I doubt that any important package uses it. Still, I take the point that it would be quite a lot of work even to prove my doubt.
I remember seeing weird stuff when someone forgot to add an instance of some `takusen` class. For your specific example, I think the most principled option today is to use two type families, `IsGoodWrapper` and `IsGoodWrappee`, and an auxiliary class, `NonDefaultStateMonad`.
Yeah, I knew about `ujson`, but I still found it surprising that a python implementation would get those results. If I remember correctly the standard deviation was pretty high in that test for that framework, so is average even a fair metric to use? I've built a whole bunch of apps using python3 and flask and one servant app. None of these had high traffic so it wasn't a question of maximizing instance value, but the servant app seemed a lot faster than the various flask apps (and wrk confirmed that). 
At first I was surprised to see you say this, because I didn't know `init` was partial. Now I'm surprised to find that `init [] /= []`. Who thought that was a good idea? I still wouldn't say it's worth a deprecation cycle to remove `init` alone from the prelude. But I'd be very much in favor of a sweeping deprecation cycle for removing *all* partial functions from prelude. It isn't significantly more work, but it *is* significantly more beneficial.
We wouldn't avoid making this change if it broke only `Acme` packages. We would avoid making this change if it broke every package. Therefore there is threshold at which a package becomes "important" enough to stop this change going through. Have you any idea how we can classify this threshold?
could be worse http://i.imgur.com/tavuLmx.jpg 
[PURE POWER VIA MONADS](https://wiki.haskell.org/wikiupload/7/75/Pure_Devil_Haskell.png)
I think `Reader`/`ReaderT` makes more sense when combined with other monad transformers. In isolation I think it's clearer to thread values explicitly
I tried, but I couldn’t figure out why monad-control is designed this way. My understanding is that most monad transformers have the form data FooT m a = FooT { runFooT :: context -&gt; m (f a) } for some `context` and some `f`, and that makes it possible to upgrade functions like `withFile` from IO to `FooT IO`: withFileT :: FilePath -&gt; IOMode -&gt; (Handle -&gt; FooT IO a) -&gt; FooT IO a withFileT path mode body = FooT $ \context -&gt; do withFile path mode $ \handle -&gt; do flip runFooT context $ body handle I would thus have implemented MonadBaseControl as a bijection from `m` to `Context m -&gt; StM m a` for two associated types `Context` and `StM`, but instead monad-control chose to abstract over parts of the above logic, resulting in withFileT :: MonadBaseControl IO m =&gt; FilePath -&gt; IOMode -&gt; (Handle -&gt; m a) -&gt; m a withFileT path mode body = do st &lt;- liftBaseWith $ \runInBase -&gt; do withFile path mode $ \handle -&gt; do runInBase $ body handle restoreM st So `runInBase` is the partially-applied `flip runFooT context`, which cleverly hides both `runFooT` and `context` from the body and thereby ensures correct usage. Given this, I really don’t understand why `restoreM` is a separate function instead of being baked into `liftBaseWith`; it seems like all this does is enable incorrect usage, like attempting to revert to a previous state in order to cancel some side-effects, instead of making sure that it is only used to bind the `f a`’s effects at the point of the computation at which they would have happened if we didn’t have to work around `withFile`’s monomorphic API.
HLearn uses an alternative, incompatible Prelude, and I think this is enough to kill it. A pity, because lots of good ideas are used in SubHask. Also, there are some GHC limitations that limits the expressivity of the method. The limitations are cited in the source, they can be found by searching for 'trac'. A notable one is the absence of contraints with existential quantifications. PS: Thanks for your latest work with crypto and cartography, it's really valuable for all.
Yeah it would be really cool to have a tool that can find all uses of some identifier on Hackage.
`init`'s the only one that I can't imagine ever being used. If you want to replace it with something with a less valuable name that's fine.
Yea those are all among the partial functions that I was talking about =P That's why I said I'd be in favor of a more sweeping deprecation
JSON parsing and ORM performance are two of the biggest bottlenecks in web frameworks
or at least /r/haskellquestions 
Why not put a warning on all non-total list functions that can be replaced by using `NonEmpty` for a start?
&gt; And I'd wager that given the 10,000+ packages on Hackage, it wouldn't be hard to find 50 that use `init`. I'd be very surprised if there were 50 packages published in the last year or so on Hackage that use `init`. 
Can you state plainly what lesson *you* are drawing from these benchmarks? You seem to be suggesting that significant time hasn't been spent on performance in the haskell libraries that make up that stack, while also suggesting that more developer hours is unlikely to close that performance gap. I'd disagree on both those points.
Yes, but then they should call it a JSON (de)serialization benchmark or a DB access benchmark, not a web framework benchmark. When something claims to be an "X benchmark", it should focus primarily on the performance of X. This disconnect may be more pronounced in Haskell since web frameworks are so interoperable.
Many, many people (maybe the majority) are doing full time software development for far less and using languages that are at the top of these benchmarks. And that doesn't even count the huge number of people with other expertise for whom coding is just a part of the job. IMO the goal should be making Haskell the cheap plastic alternative, not the luxury item.
+1. While many are focusing on why `init` might be bad for reasons similar to `head`, the author correctly focuses on it's valuable name and it's empty set of use cases. `head` is a convenient function; `init`, while nicely filling out the head/tail/last set, is just never functionality I reach for, especially on a type where it isn't (or doesn't seem) confluent. On the other hand, I (relatively) often want to name variables init, and I dutifully find another name each time I see that error. This assumes the author and I are correct that the current uses on hackage are countable on two hands 
In 2014 Petr Pudlak and I did a 5 months long study on combining Haskell and foosball daily. I can report both were found to improve quality of life.
&gt; using languages that are at the top of these benchmarks Or at the bottom. A lot of web applications use *Rails* and *Django*.
Congratz OP =)
Programming generically over an interface only works if you can also reason generically over that interface. Using an example from your post, how would you prove that: mapT (f . g) = mapT f . mapT g Intuitively, this functor law seems like it should be true, but how would you prove that it is true for all instance of `MonadTransControl`? Edit: Also `mapT` already exists as `hoist` of the `MFunctor` class from my `mmorph` library
Thanks. Do you have an opinion on the [Tekmo style](https://www.reddit.com/r/haskell/comments/6b2krt/can_somebody_please_document_monadbasecontrol/dhjsudy/)?
&gt; Go is ten years old and sponsored by Google. GHC is 25 years old and is sponsored to a lesser extent by Microsoft Cambridge. GHC has had SPJ and (for some time) Simon M working on it roughly full time. I bet Go has had more than 10 developers working on it full time during its life.
I think your point is fair enough. At this point I think I'd be happy if everyone agreed that `init` doesn't belong it `Prelude` but we're going to leave it there because it's too much bother to remove it.
I think a growing percentage of the Haskell community feels that way about each of the partial functions, myself included :) It'd probably be worth it to remove all of them in one effort though; I wonder how much support we could gather for that?
Thirded.
Agreed. While I thank TechEmpower to do this, I really regret that they did not start off with more "realistic" scenarios. Adding FWs would be harder, but the results might then make some sort of basis to base a FW choice on. Now it really does not say much except for those FWs repeatedly in the "the top" or "the bottom"
The documentation for the [layers](https://hackage.haskell.org/package/layers-0.1/docs/Documentation-Layers-Overview.html) package has a great overview of many approaches to the problem of abstracting over monad transformers. You can't use it to learn `MonadBaseControl`, but you _can_ use it to understand what `MonadBaseControl` is about, and how it compares to other approaches. It's a bit outdated, but still a very valuable read imho, check it out.
If you're using `tails` or `isSuffixOf`, then you've already imported Data.List, so having init in there is fine. 
You’re right, I can’t prove it. :) Though intuitively, I feel like it *should* be impossible to write a `MonadTransControl` instance that violates that law without also violating one of the existing `MonadTransControl` laws. I don’t know nearly enough to attempt to demonstrate that, though.
&gt; which every application is supposed to do This assumes knowledge about the deployment environment and business goals of an application.
4 is a guess and 5 builds on 4 concluding "evidence". I don't really have any opinion on the "init" issue, but I do think that presenting actual numerical evidence is preferable to characterising your guess as such!
&gt; I feel it helps me to to avoid error but it is quite tedious since I have to create several function like `handleIOTOHandler` every time I have to translate an IO like , lot of boiler plate. This is the price you pay for the additional safety. Haskell programmers often have to choose between a convenient but error-prone solution and one that involves a bit more boilerplate while defending against more errors. Which balance you prefer is up to you, of course. &gt; Thanks for the informations My pleasure. :)
Thanks, this seems very nice, but I'm unable to get it working. I did: cabal install syntax git clone https://github.com/pavelchristof/syntax-example cd syntax-example ghc Main.hs But gave me this: Failed to load interface for ‘Control.Category.Structures’ Failed to load interface for ‘Control.SIArrow’ Perhaps you meant Control.Arrow (from base-4.9.1.0) Module ‘Data.Syntax’ does not export ‘Seq’ Failed to load interface for ‘Data.Syntax.Attoparsec.Text.Lazy’ Perhaps you meant Data.Attoparsec.Text.Lazy (from attoparsec-0.13.1.0) Module ‘Data.Syntax.Char’ does not export ‘SyntaxText’ Failed to load interface for ‘Data.Syntax.Printer.Text’ So after I noticed that there was a cabal file in the directory, I went with cabal install but to no avail. If I compile Setup.hs, I can try running Setup --configure, however it tells me that there are missing dependecies. Even if I do install them using for example "cabal install lens", it will still complain. While I like functional programming, I still need to understand how to use packages with haskell
`grep` finds it 48 times in the Agda codebase (vs. 111 `foldr`, 57 `foldl` and 21 `foldl'`) for instance. Not sure how many of these `init` are in a module already importing `Data.List`.
In general I agree with you. We should be trying to make Haskell more accessible to more people. But in this particular case I think "making Haskell the cheap plastic alternative" = spending scarce Haskell developer hours doing nothing more than hacking the benchmark. I personally have more important things to do with my time.
As a note, your expectation would arise from the statement [(x,y,z) | x &lt;- [1..] | y &lt;- [2..] | z &lt;- [3..]] Note that you couldn't use x in y or z
Quick git grep on our codebase showed that it's used in about 15 places.
Yeah there's an awful lot of FP (and Haskell) experience necessary advertising that turn out not to be the case. It's being used as a filter by companies and recruiters for getting more qualified? candidates.
The next question, of course, is: of those straggling or unmaintained packages, how many of them build with base 4.9 anyway? Either due to simple PVP bounds issues, or other changes like the FTP? I don't think there's a good way to answer that question unfortunately. Except, of course, building all of Hackage and seeing what breaks.
That's pretty cool. I wrote a bf interpreter as my first haskell program, and it's almost exactly the same as yours (looks like I'd shortened some names because I felt guilty running off the side of the page). It's here if you're curious: https://gist.github.com/oisincar/be2990aa67f1ff08d3a54fd1fc03e159 I think for an inefficient-but-short program, there's not much better you could do. Your (our) method of running an operation on a single object in the list will be very slow. And mine finds matching brackets in linear time (instead of caching), although tbh I can't see what yours does (does it handle brackets?). If I was to do it again, I'd parse it as a tree so loops could be much faster (and be pre-computed, like [-] always sets the register to 0, [-&gt;++&lt;] adds twice the current register to the next one to the right, etc.), and as others have mentioned Zipper pattern makes tape updates much faster than splitAt.
Thanks for reading the gist. I hope this document is useful for your (and everyone here) purposes. If you identify any suggestion, I will be glad to hear them and make corrections.
Entirely aside from this proposal (which seems quite sane, even obvious, to me), I find this whole line of reasoning about old code deeply problematic. That doesn't mean I think it's false... just that I wish it were false. I wonder what changes would be needed to make it false! For instance, clearly existing code can be fixed up with entirely mechanical and backward-compatible changes -- namely, adding an unqualified import of `init` from Data.List in any module where it occurs. Would it be sufficient to automatically fix all code on Hackage? Probably not, given increasing commercial (and private) uses of Haskell, but it would go a long way. What if there were a universally accepted upgrade tool available, that picked up mechanical changes like this from a repository (perhaps distributed with packages and mentioned in the cabal file... or perhaps in a centralized repository akin to Hackage itself), so people could apply the same safe upgrades to their private code? Of course, this is even *more* excessive work for this one proposal, but I wonder if some framework like this might be reusable, and address backward compatibility concerns in the future. In my experience, businesses with large internal code bases rely heavily on exactly these kinds of automated refactoring and cleanup tools to keep their older code relevant and usable. The open-source world, on the other hand, just plans to throw things away and move on to the new thing often enough to avoid it becoming too large a problem. I think Haskell is something that's proven itself worth not throwing away.
&gt; It isn't significantly more work, but it is significantly more beneficial. Also, significantly more controversial. I, for example, would probably be part of the opposition on that one. On the other hand, removing `init` seems to be an obvious win. (I guess, to be fair, I did remove `head` from [my own custom prelude](https://code.world/doc/Prelude.html). But, only to replace it with other partial functions! And the main reason was that when you're a kid drawing pictures, `head` is valuable naming real estate. This concern definitely doesn't apply in most Haskell development.)
Well, we have a few regulars from TU Munich, so that helps. You don’t have to have a foundation of related jobs, as you say they are basically non-existent if you search for more than one or two people at a time per city. So just start something and see who shows up. Being based in a local hackerspace certainly helps.
&gt; In particular, if I have instances Requestable A B and Requestable X Y and a function foo :: Requestable a b =&gt; a -&gt; b, I know that foo isn't a function from A to Y. Any use of getResponse and toRequest in foo must use the same instance. Whether or not that is useful depends on what the OP is trying to do. Right, and that approach of actually attaching the `getResponse` and `toRequest` instances together, through say a tag, is the one I suggested elsewhere and the one approach in this thread that I actually agree with. I am purely arguing against the other approach that has been suggested far too many times for something that as far as I am concerned is relatively useless, particularly considering half the examples given in this thread don't even compile. &gt; I never made any claims that such an API forces you to use the methods together. Obviously, once you have converted something to a ByteString, you are free to decode it with whatever method you have handy. I mean surely what the OP wanted is making sure you decode with the right thing based on the input type. I would be extremely surprised if all they wanted was the very limited guarantees that such a "forgetful" approach gives you.
But if you keep the signatures in the OP and just add a functional dependency that doesn't even compile without `-XAllowAmbiguousTypes`, and isn't usable without `-XTypeApplications`. So that really doesn't seem like the right way to do it. You fundamentally need to a type level tag of the input type, so that the type of `getResponse` actually depends on the original input.
Why in the bathroom specifically? Do they like to fuck with it when guests go in or something... Now you're just giving me ideas.
Very nifty :) But doesn't seem to handle nested brackets, from what I can tell?
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [caiorss/zhserver/.../**Zotero.hs** (master → 200c739)](https://github.com/caiorss/zhserver/blob/200c7391bfa572b4a4a2b861ee5a1ea9c77ea695/src/Zotero.hs) * [caiorss/zhserver/.../**DBUtils.hs** (master → 200c739)](https://github.com/caiorss/zhserver/blob/200c7391bfa572b4a4a2b861ee5a1ea9c77ea695/src/DBUtils.hs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dhkntwo.)^.
I'm against what I perceive as a dangerous sectarization of the Haskell community that is threatening the future of the language and converting it into a playschool for cool kids, irrelevant even in the academical sense. 
I was thinking of having a datatype for a "step" that would be able to take a dynamic amount of arguments. Something like [safe-printf](https://hackage.haskell.org/package/safe-printf). Then, you need a mapping from this "textual" representation of the step to the function that does the actual stuff the step is supposed to do. After that, we need a way to look up and execute steps. I think a simple Alternative interface should suffice as a starting point.
Didn't know this existed. Thanks.
Oh, and since you just use plain functions, you can create multiple variations of your data collections: data Keys = Keys { a, b, c :: PackageKey } keys = PkgKeys (pk "a") (pk "b") (pk "c") where pk n = PackageKey n "0.1" keysVer = PkgKeys (pk "0.1") (pk "0.2") (pk "0.3") where pk v = PackageKey "name" v So you could have one test which uses the first values to test for different names and the other values to test for different versions in this case.
No, not at all. The bathroom has no windows, and the intention was to have a nicer lighting than a standard cold white lamp. That didn't quite work out, though, because the colored light is too dim, so now it has become a strange oddity.
I also can't build it. Sad :( This library is very old, unfortunately... Last commit was 2 years ago. So it's not maintained, sadly. Thus, it may be problematic to build it with latest `ghc` and latest versions of packages. Probably this `syntax` library needs more attention.
I have certainly used `init` on many occasions. Admittedly, it would have been cleaner to use a total alternative. But unlike `head`, which has the convenient safe alternative `listToMaybe`, there is no immediately available safe alternative to `init` in the base library. You have to roll your own. And, you know, we Haskellers are lazy by default. So I probably use `init` at least as often as I use `head`. Here is a counter-proposal: 1. Add `takeEnd` and `dropEnd` to `Data.List`. These have precedent; they already exist in other libraries with list-like functions, such as `Data.Text`. (Or some variation on exactly what functions we're adding and what they're called, but please, let's not get bogged down in bikeshedding.) 2. Add a warning for uses of `head`, `tail`, `init`, and `last`. This would be a big step towards finally purging those evil functions from our code. It's a concrete first deprecation step without the breakage pain. It would annoy people like me who are lazy and who also want warning-free code, but we deserve it.
So you looked at his comment history, huh? &gt; postmarxist puritans who think that IO is some evil introduced by Capitalism did I just find metafunctor's alt account or what?
In that case you may be right. But at least now you know (as well as me, didn't heard about it earlier) that there is /r/haskellquestions subreddit. Probably better place to ask small questions.
For Data Update, we are the bottom! We are literally the slowest entrant that still managed to complete the benchmark. And even our *best entrant* there (Servant) is scoring 49req/s, when there's stuff in JS/Go/C#/C++/Kotlin/Scala all clocking in at over 2000req/s (with zero errors).
What could the reason be?
I'm not restricting myself to inferable types, I just think that in this situation the fact that the types can't be inferred is indicative of a bigger problem. And yes I fully realize that the final composed function has a similar type. The difference is that with my setup the components actually have enforced correctness, instead of just relying on you artificially restricting the type signature AFTER you combine them.
Keep it very simple and focused. Pick a widely used algorithm (eg QuickSort, Merge Sort, Dijkstra's Algorithm or whatever) and compare performance of the algorithm implemented in lazy vs strict Haskell, or lazy Haskell vs some other language. (eg F#, C, Python, whatever)
That would also work. The traditional safe alternatives are `listToMaybe`and `take 1` for head, and `drop 1` for `tail`. `uncons` is new to `Data.List`. I imagine `uncons` is not nearly as widely used, and anyway it's really just a shorthand for using two of the usual ones at once: uncons = (liftA2.liftA2) (,) listToMaybe (pure . drop 1) But I'm fine with `unsnoc` if people like it better. EDIT: Fixed type of `uncons` as combined `listToMaybe` and `drop 1`.
No. sorry. That would compromise my anonymity. I want to preserve my freedom.
It's difficult to speculate on the future, so yes, for now we certainly include EU citizens in that. In terms of profile: it's difficult to judge from a few sentences. We generally tend to hire people with a bit more Haskell experience, but an understanding of Haskell and functional programming concepts is always more important than knowledge of the latest libraries etc which can relatively easily be filled in. So I suggest that if you're interested, you just try to apply. However, right now we're looking for people who can start this year, not next. We hope to be hiring again and more in the next few months, so this doesn't rule anything out.
The application-level code doesn't look obviously inefficient, so my best speculation is that our lower level network stack is just not that performant. Unlike most of the scripting languages or JVM, which bind their network APIs to a native implementation, afaik Haskell's network stack is actually written in Haskell. But that's just speculation.
I've only read up until the 33rd page, and I am a bit confused so far. Maybe my question is answered later on in the pdf, but even then it may be interesting to talk about this for others too more explicitly here in the comment section. So, if the left hand branch of the tree has two `(++)`s, and I think of a naive implementation of thunks within GHC, then yes, every time I try to pull out a value of the upper thunk, I will only find a pointer to a lower thunk, which will give me a pointer to the list. Extraneous steps galore. But, and here comes my question: Can GHC not notice somehow that we are traversing the same pointer-structure again and again? Can it not walk this tree with a stack, remembering where it came from, only having to dereference a single pointer at a time, and returning one level up the stack when done at the lowest levels? I hope my question makes sense, if not, please say so and maybe I can provide a code example of what I mean.
Thanks for getting back to me on this. So, basically it's still worth applying, with the understanding that I'll probably not be considered for the jobs currently posted because of the timing.
I would very much be interested in collaborating on getting a cucumber equivalent for Haskell up and running. The github.com/sol/cucumber-haskell has a decent write up on what to attempt. I say we fork and start prototyping :) 
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [blitzcode/hue-dashboard/.../**HueREST.hs** (master → 6d97472)](https://github.com/blitzcode/hue-dashboard/blob/6d97472fad7b71ac6e3a16e227ad9ccf4e35a795/HueREST.hs) * [blitzcode/hue-dashboard/.../**HueJSON.hs** (master → 6d97472)](https://github.com/blitzcode/hue-dashboard/blob/6d97472fad7b71ac6e3a16e227ad9ccf4e35a795/HueJSON.hs) * [blitzcode/hue-dashboard/.../**HueSetup.hs** (master → 6d97472)](https://github.com/blitzcode/hue-dashboard/blob/6d97472fad7b71ac6e3a16e227ad9ccf4e35a795/HueSetup.hs) * [blitzcode/hue-dashboard/.../**HueBroker.hs** (master → 6d97472)](https://github.com/blitzcode/hue-dashboard/blob/6d97472fad7b71ac6e3a16e227ad9ccf4e35a795/HueBroker.hs) * [blitzcode/hue-dashboard/.../**WebUIREST.hs** (master → 6d97472)](https://github.com/blitzcode/hue-dashboard/blob/6d97472fad7b71ac6e3a16e227ad9ccf4e35a795/WebUIREST.hs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dhkxevs.)^.
It sounds like your code does more than my package does (or did when it still worked). A package doesn't have to be complete to be useful! Almost anybody programming Hue will only need the above list.
~~Could it be some kind of FFI or concurrency issue? I haven't looked too closely into the hasql code base, but think it relies on libpq. It'd be interesting to see how a pure-haskell version would perform.~~ *Actually, my guess is multiple pipelined requests in the fast frameworks, vs. waiting for confirmation in the Servant version. (Sequentially, I guess, if it's only 49 tps..) Round trips and commits take time.* The [wire-protocol](https://www.postgresql.org/docs/current/static/protocol.html) doesn't seem too overwhelming, you'd cover the important bits "just" by supporting the extended query protocol in binary mode? Then again, I wouldn't know where to start, even on building a minimal benchmarking/comparison suite.
&gt; I expected beta reduction would deal with that How? map fn1 . map fn2 reduces to \xs -&gt; map fn1 $ map fn2 $ xs and then the expression doesn't reduce further, because `map` wants to pattern-match on `xs` but we don't yet know whether it has the form `[]` or `(x:xs)`. Even then, suppose we were dealing with infinite streams and didn't care about bottoms, we would then be able to reduce a bit further: \xs -&gt; map fn1 $ map fn2 $ xs = \xs -&gt; map fn1 $ fn2 (head xs) : map fn2 (tail xs) = \xs -&gt; let xs' = fn2 (head xs) : map fn2 (tail xs) in fn1 (head xs') : map fn1 (tail xs') = \xs -&gt; fn1 (fn2 (head xs)) : map fn1 (map fn2 (tail xs)) And now if you were trying to prove that `map fn1 . map fn2` was equal to `map (fn1 . fn2)`, you'd invoke the definition of `(.)` to rewrite the head to `(fn1 . fn2) (head xs)`, you'd use induction to rewrite the tail to `map (fn1 . fn2) (tail xs)`, and then you'd invoke the definition of `map` to rewrite the rewritten cons as `map (fn1 . fn2) xs`. Definitely more complicated than just beta reduction! Furthermore, a proof assistant might be able to find those steps if you asked it to help you to prove `map fn1 . map fn2 = map (fn1 . fn2)`, but why would an optimizer try that? There isn't even an a-priori reason to suspect that `map (fn1 . fn2)` would execute more efficiently. We know that it's better because we know that map traverses the entire list, and that traversing it once is better than traversing it twice because we won't have to construct an intermediate list, but for an optimizer to figure that out, it would need to know about functions which traverse their entire data structures, and have some fancy analysis which looks for them. Very definitely a lot more complicated than just beta reduction!
[Here](https://hackage.haskell.org/package/base-4.9.1.0/docs/src/GHC.Base.html#map) is the definition of map with its rewrite rules if you are interested. I'd be pretty surprised if that optimization didn't fire, can you give an example?
Java also has a stop the world GC with roughly the same pause times. I'm not sure of how much more frequent those stops are in GHC, but I've not heard any indication that it's all that much worse than Java. Haskell's collection libraries and it's GC are very well tuned for each other. `containers` is really good about sharing and the GC is really smart. I look forward to the day that GHC does implement GC optimizations with linear types, but there are currently no plans to actually do that work, if I understand correctly.
Fourthed
The snap version of that benchmark isn't even streaming the responses, and it's using `StdGen`, we should probably spend five minutes to optimize it.
&gt; [...] without the help of explicitly written rewrite rules.
&gt; [...] without the help of explicitly written rewrite rules.
Sorry, my reading comprehension is apparently terrible. Then the answer comes down to 'GHC can't know about type class laws unless we tell it about them'.
GHC optimization usually breaks down for recursive expressions. It has some heuristics for optimizing them but they are not reliable. This is why high-efficiency Haskell code usually uses a non-recursive representation under the hood which GHC can better optimize and then transforms to a recursive representation at the last moment For example, consider the following recursive definition of the list type: data List a = Cons a (List a) | Nil There are two ways that you can transform that into a non-recursive type: {-# LANGUAGE ExistentialQuantification #-} {-# LANGUAGE RankNTypes #-} module Example where data ListF a x = Cons a x | Nil -- Non-recursive representation #1 newtype List1 a = List1 { unList :: forall x . (ListF a x -&gt; x) -&gt; x } -- Transform back into the recursive representation toList1 :: List1 a -&gt; [a] toList1 (List1 g) = g (\la -&gt; case la of Cons a x -&gt; a:x Nil -&gt; [] ) -- Non-recursive representation #2 data List2 a = forall x . List2 x (x -&gt; ListF a x) -- Transform back into the recursive representation toList2 :: List2 a -&gt; [a] toList2 (List2 seed step) = go seed where go x = case step x of Cons a x' -&gt; a:go x' Nil -&gt; [] ... and you can define a `map` function for each type: map1 :: (a -&gt; b) -&gt; List1 a -&gt; List1 b map1 f (List1 g) = List1 (\h -&gt; g (\la -&gt; h (case la of Cons a x -&gt; Cons (f a) x Nil -&gt; Nil ) ) ) map2 :: (a -&gt; b) -&gt; List2 a -&gt; List2 b map2 f (List2 seed step) = List2 seed step' where step' x = case step x of Cons a x -&gt; Cons (f a) x Nil -&gt; Nil Note that everything so far is non-recursive. The `List1` and `List2` types are not defined in terms of themselves so they are non-recursive types. Similarly, the `map1` and `map2` functions are not defined in terms of themselves so they are not recursive functions. Now you can show that for both `map` implementations: example1 :: List1 a -&gt; List1 a example1 = map1 id example2 :: List2 a -&gt; List2 a example2 = map2 id ... GHC will optimize away `map id` to `id`: $ ghc -O2 -ddump-simpl -dsuppress-all -fforce-recomp Example.hs ... example4 = \ @ a_apc ds_Xrh @ x_XqC h_Xo2 -&gt; (ds_Xrh `cast` ...) (\ la_anJ -&gt; h_Xo2 la_anJ) example1 = example4 `cast` ... example2 = \ @ a_ap3 ds_dqW -&gt; case ds_dqW of _ { List2 @ x_apU seed_anN step_anO -&gt; List2 seed_anN (\ x1_anQ -&gt; step_anO x1_anQ) } ... which if you clean up is equivalent to: example1 (List1 ds) = List1 (\h -&gt; ds (\la -&gt; h la)) example2 (List2 seed step) = List2 seed (\x1 -&gt; step x1) ... which is essentially the identity function in both cases with the only difference being eta-expansion. Carefully note that GHC did that without the assistance of rewrite rules. GHC's normal optimization rules sufficed. Now compare that with `map id` for ordinary recursive lists: example3 :: [a] -&gt; [a] example3 = map id GHC won't optimize that in isolation: example3 = \ @ a_ap3 -&gt; map (id) ... and when it does optimize that in other situations it's only because of rewrite rules
I grew disillusioned with the techempower benchmarks, and stopped working on them, but I never put down in words why, so here it is. When working on them, I started noticing what was slow about the `servant` implementation. The first thing, which had a pretty large impact, was that for `plaintext`, `servant` by default adds the encoding, whereas other applications don't. The RFCs are pretty clear about this being wrong for `text` media types, but, feeling a little dirty, I added a new content-type without the encoding. This is a situation we can't win - we try to make it really easy to do the right thing, and hard to do the wrong one, but, since in this case everyone is doing the wrong thing and it's faster, I did the wrong thing, the result being that there are more lines of code and `servant` apps come out looking more complicated. Then the obvious next thing is content-type negotiation. Realizing it was huge factor, but that I'd again need to write more code in order to do the wrong thing, I stopped. Note that these factors are usually very small in most applications, but in cases where the application is basically not doing any work, which is what the benchmarks test, they're really significant (again, just the extra "charset=utf-8" that `servant` was adding to the headers in plaintext was a surprisingly big difference!) Then routing. You'd think that if you're claiming to test web applications, you'd be pretty serious about routing, no? I don't *object* to having part of the tests be database and JSON serialization, etc., but routing is much more what I think of as the core of a web framework. The others are important for web applications, but they're often not part of a framework per se. But guess what - these tests aren't actually testing routing! First, there are only about 5 routes to begin with, so it's not like you're going to notice if you're using a framework that's linear rather than logarithmic (and indeed, `fasthttp` *is* using linear routing, and `servant` has logarithmic routing). And there's more: *you can have separate applications for each route*! So you can just have each application handle one route, and even if your routing is exponential, no one will notice! Even if you don't abuse this deliberately, the fact that many implementations don't run all tests means that they unwittingly get a performance boost! Then the other thing is just that testing in an environment that's like the actual environment used allows you to tweak RTS and DB flags to make sure they're optimal. But this doesn't feel like improving the libraries so much as doing well in the benchmark, which, while I don't think is exactly *immoral*, isn't something I want to spend too much time doing. I think actually picking the right number of connections to the DB is going to make a pretty big difference, though I'm not sure. Then there's the fact that you're looking at reqs per second. There's also latency. Admittedly latency is pretty bad for the benchmarks that do DB access, but the only test that's *purely* the framework (and not other libraries), which is `plaintext`, `yesod` and `servant` are leading vs. Go frameworks (though `snap` is to me the more impressive in terms of stddev). And this is still against applications that are not doing content-type negotiation. Ultimately, I do care about performance, and this benchmark is quite helpful. But funnily, I feel like it was helpful to me more in learning why I should ignore it than why I shouldn't. E.g. I still don't know why the DB tests are slow, and that indicates there's more investigating to be done; but the parts which I do feel like I understand the performance difference (primarily, content-type negotiation) I don't really want to "improve"! And routing - well, routing we have to rely on our own benchmarks for.
Not sure how Haskell's GC works (surely this is taken into consideration), but JVM's GC (Scala, Clojure etc) doesn't really care about short lived allocations, which plays nicely with functional languages. 
Type class laws? There are no type classes here.
Check his website, there is a section on Haskell or Ad Hoc Polymorphism. I think the paper was called "making ad-hoc polymorphism less ad-hoc". 
My thoughts exactly, not just because of Haskell but because SPJ got a lot of public exposure with the Computing at Schools initiative... 
It's part of XML-DSig, which is part of the SOAP and SAML world. Those kinds of old-style huge enterprise protocols would require significant corporate funding to do a brand new implementation in Haskell, even just for bindings. We need some of that stuff for our enterprise customers, and we'd love to have it in Haskell. But for now we're making due with third party SaaS solutions via REST APIs. If at some point that isn't enough, we'll probably create a wrapper in a language with good existing library support for that stuff and provide a REST API for the specific things we need.
The reason why I thought it is because there has been lately some discussion on the topic of supercompilation which uses only beta reduction under the hood. That means that if beta reduction cannot deal with this neither can supercompilation which is surprising when you consider it has "super" in its name. 
Indeed, I doubt that you can make supercompilation work reliably by only using beta-reduction under the hood. Do you have a reference to the work you had in mind? The work on partial evaluation typically uses various kind of eta-expansions (in GHC those would be the case/case transforms etc.).
&gt; ... original typeclass proposal email from Philip Wadler After som digging around I found it in the most obvious place, on Wadler's webpage: [Proposal: Overloading in Haskell](http://homepages.inf.ed.ac.uk/wadler/papers/class-letter/class-letter.txt)
Yes, thank you. I actually just stumbled upon it. I don't understand how I missed it during my first try.
As the error message helpfully tells you, a type annotation is needed. Depending on what type you use, the ceiling and square root functions will have different implementations, because you are using type classes here. The GHCi REPL will try to make a good guess (and sometimes it guesses badly) for your types. GHC on the other hand wants you to spell it out. Type this into your REPL to get the type signature you need: `:t isd`
I'll try to give this a stab, I may not give the best explanation so feel free to ask questions. to any more experienced haskellers: please feel free to critique the bits I get wrong **TLDR** ```map (x `comp`) [1..(ceiling$sqrt $ fromIntegral x)]``` You can use GHCi to help you debug this function, I'll go through what I did to get it working. λ&gt; let isd x = map (x `comp`) [1..(ceiling$sqrt x)] λ&gt; :t isd isd :: (Integral a, RealFrac a, Floating a) =&gt; a -&gt; [Bool] the `:t` command in Ghci will output the type of any given expression. Here we see one of the 'scary' things for newcomers in haskell, your _number_ is polymorphic. This method takes a type which satisfies _all_ of `Integral`, `Fractional` and `RealFrac` and produces a list of booleans. So far so good. When we try to apply it to a value in ghci though, that's where you get your ambiguous type error. If we look at the type of a numeric literal we get: ``` λ&gt; :t 10 10 :: Num t =&gt; t ``` Haskell parses your numeric literals in a polymorphic fashion, and avoids choosing a concrete type for it. However, GHC has also kept the type as generic as possible. All we know about this literal is that it satisfies the `Num` class. When we give ghc the expression `isd 10` it will try to find the most generic type which fits there. In this case, the type needs to be `Integral`, `Fractional` and `RealFrac`, unfortunately, GHC doesn't know of any type which fits the bill. Why does your number need to be both integral and a fraction? If we look at `comp` you use `mod` which has the type `Integral a =&gt; a -&gt; a -&gt; a`, and `isd` uses `sqrt` which has the type `Floating a =&gt; a -&gt; a` and `ceiling` with type `(Integral b, RealFrac a) =&gt; a -&gt; b`. Fortunately, there is a simple fix, adding a call to `fromIntegral` allows you to convert an integral number into any other `Num`. λ&gt; let isd x = map (x `comp`) [1..(ceiling$sqrt $ fromIntegral x)] λ&gt; :t isd isd :: Integral a =&gt; a -&gt; [Bool] Why doesn't haskell automatically cast numbers for you? There are a lot of reasons but essentially, it wouldn't be safe, imagine if it casted `260` into `Word8` (a byte)? You'd have incorrect results with no way of knowing. Why does it work when you hard code `x` in a `let` expression in `ghci`? I'm not certain, but I wouldn't be surprised if it has to do with the subtle ways in which `ghci` differs from `ghc`. 
Eta-expansion happens in the splitter, Issue 1 in Section 3.5: "learning from residual case branches". The way one goes from optimizing a case stuck on a variable to optimizing each branch with static information on the variable value (in exchange for duplication of work) is typical of eta-rules on sums.
Should be pretty easy to import data.list.init then? It's not like the function is going away, it's just moving out of a global namespace (which I think is a good idea)
&gt; Liquid Haskell What cases, specifically? What can't you express with refinement types? 
There's actually a lot of work on this! These kinds of transformations are generally difficult to find, though usually genericity *helps* because of the corresponding [theorems for free](http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.38.9875) (which are easy to find, as their name suggests!). "Free" theorems only take you part of the way though. There's a general class of transformations called [deforestation](https://wiki.haskell.org/Correctness_of_short_cut_fusion), many of which we have techniques to discover automatically, including the map/map one.
But the amount of man-hours for ur/web are at least an order of magnitude less than Haskell. Ur/web finishes in the top 20 in a couple benchmarks, and from a glance I believe it beats every haskell entry 
Interesting, I need to look into the networking libraries a bit more. Are there good reasons to write it in haskell (safety, runtime integration, etc.)? Otherwise it's way too big of performance hit
http://blog.tweag.io/posts/2017-03-13-linear-types.html specially this: http://tldrify.com/n5u
I'm reading as we speak! It looks really awesome, even more so (if I understand correctly) since it requires no effort to actually build proofs (I'm basically assuming some kind of SAT solving wizardry is able to automatically find the proofs for anything expressible on that language). I'd, though, be highly interested in an usage such as: 1. Community makes a standard type for "Ethereum token contracts" (easy); 2. We also make a type synonym: type ZeroSum = (ctr : TokenContract ** ∀ (txs : [Transaction]) -&gt; sumOfBalances (computeState txs ctr) == 0)` (also easy) 3. Contract developer programs a member of that type (possibly hard, but doesn't matter); 4. He publishes that program, i.e., a proof the contract is zero-sum; 5. The community rests assured that the contract doesn't violate the zero-sum. Point is, this, and other, characteristics are easy to state and verify with dependent types. The fact they might be hard to prove is irrelevant;. What interests the contract users is that, as long as the dev provides such proof, we can rest assured the contract absolutely won't violate it. I was under the impression that dependent types were perfect and natural for that. Am I making sense? Are liquid types also fit for this?
They're not implying that GHC will do GC optimizations with this. You will be able to use linear types to ensure the correctness of manual usages of `malloc` and `free`, hence their mentioning of `malloc`. The idea being that you can do the memory management of large data sets yourself, and guarantee you're right with linearity. GHC won't do any of this for you. The fusion aspect is something I hadn't thought about. That will be a marginal improvement, if anyone finds sufficiently useful rewrite rules.
But what if I also wanted to have a (Container Int), for instance, in the same program? 
Tha'ts the point. The point is that malloc and free (or better, some abstractions over them) can be used in very fast and type safe and efficient _libraries_ that everyone can use, for network communications, database access and JSON Serialization. For example, to be on top of these benchmarks. As fast as the ones of imperative languages like Go and C++. 
Well, the slight upside is that `hlint` will at least tell you when you're doing things suboptimally :)
The "shape" of `Container a` is strictly more general than `Container (x -&gt; y)`. We use yet another wrapper so the underlying type has the right shape, and the exposed wrapper has the right kind. newtype Container' f a b = Container' (Container (f a b)) instance Category f =&gt; Category (Container' f) where id = Container' (Container id) Container' (Container x) . Container' (Container y) = Container' (Container (x . y)) Of course the wrapper makes the type much less usable. In practice, we either give up on abstractions that don't fit our type and look for more adequate ones, or we change the type to fit the existing abstractions.
Sorry, you can't have `Container Int` with kind `*` and `Container (-&gt;)` with kind `* -&gt; * -&gt; *`, what would the kind of `Container` be? `Container :: (a :: k) -&gt; k`?
I agree. And in fact the evidence from this discussion is that `init` is indeed used.
It's a guess because it's a guess. Turns out it's a wrong guess and plenty of commenters in this thread have used it in practice.
Pipelining basically means sending a bunch of requests in one go, not waiting for acknowledgement from the other party between messages. There are probably lots of plausible explanations for what we're seeing, but I think this and/or some concurrency problem is *one* way to explain the 49:2000 difference.
Your point about routing is an excellent point, I think. It's something I have wondered myself many times about particular web frameworks. I kind of wish we had a more framework+language centric way of measuring performance that didn't get confused with JSON-serialization libraries or DB connectors. Is this still useful, though? It seems that the *promise* of TechEmpower is that it says, "here's how people normally build web apps using language X with framework Y". (To that end, I often think TechEmpower is missing an HTML-template-rendering endpoint/test as well.) Of course, another problem is that as soon as you build such a thing, it becomes a target and therefore no longer useful, I suppose. You also have to make probably arbitrary distinctions between so-called "microframeworks" and "frameworks" and pure web servers which aren't traditionally thought of as frameworks. It occurs to me, though, that if you had a different benchmark project (which doesn't have to describe itself as a competitor to TechEmpower), you could also put in a matrix of which projects adhere to various parts of the spec (but that could also be taken as shaming other projects if most don't adhere too closely to the spec).
`15/05/2017` is division, not a date. Try asking ghci what type it has (`:t 15/05/2017`).
Indeed; see [this artifact](https://cogumbreiro.github.io/jlamp17/) for an example of Coq formalization compiled into Javascript code. Here the Javascript code is used as a demonstration of the code supporting an academic paper, which may not be what the OP had in mind.
Ah, you're right. I'm rusty, clearly. :) Either way I think my general point still stands however; you probably really don't want this as a general optimization, and even if you did, you'd probably have to carefully control it to actually get the effects you want in general.
So, if you go the dependent type route I think it's pretty doable. Morte is already a calculus of constructions and Dhall can turn on dependent types like Morte since it's a pure type system. The slightly tricky part is adding universe polymorphism to support fancier dependent type features but I think that shouldn't be too difficult to implement. If you go the refinement type route it will be harder to implement (and distribute since you need to bundle an SMT solver), but much easier to write proofs and the types will be much nicer. If you think it's possible I would try this route because if the goal is to specify a contract language then you want the types to be as readable as possible so that people can verify that the contract is enforcing what they think it is enforcing
&gt; `succ 2017 → 2018` Because 2017 is a *number*, not just an year.
That might be the obvious route I was missing and I'm starting to like it. But how would I get actual contracts after I've written them as Coq datastructures? Isn't that the same as compiling Coq to the EVM?
The reasons are that it shouldn't be "way too big of a performance hit." Haskell can be very fast. I really don't see a reason that Haskell itself should be the performance bottleneck for these things, considering we have to use the GC for the things we'd be sending to C anyway.
I actually can't think of a single case where you wouldn't want `fmap` fusion. The functor laws and parametricity guarantee that `f` and `g` will both be called with the same values, in the same order, the same number of times, regardless of whether you use `fmap f . fmap g` or `fmap (f . g)`. Meaning that the only possible difference between the two is that the structure of the functor has to be traversed and rebuilt an extra time with `fmap f . fmap g`. EDIT: I guess `bottom` is one case where you have to at least consider the difference, if the functor is strict in its values. But `bottom` already makes strict functors violate the functor laws, so I don't think this matters.
This enables local use of `RebindableSyntax` data Syntax = Syntax { (&gt;&gt;=) :: S s () -&gt; (() -&gt; S t ()) -&gt; S (s + t) () , return :: () -&gt; S 1 () } mySyntax :: Syntax doStuff :: S 3 () doStuff = do let Syntax{..} = mySyntax () &lt;- return () () &lt;- return () return ()
You would only need to write a function that takes a description of the contract and prints a representation of it (as a text or binary file), and compile and run the Coq program that uses this function. No need to compile arbitrary Coq terms to EVM, this is more like a printer of EVM bytecode.
Basically the calculus of constructions alone isn't enough to do basic things like a type-safe `head` function. You need to add more powerful features like inductive constructions or universe polymorphism in order to be able to express that and usually when people say "dependent types" they actually mean these additional features. For example, Idris has implicit universe polymorphism which is why it can do these sorts of things Universe polymorphism is basically replacing `Type` and `Kind` with a `Natural`-indexed tower of types (i.e. `Type 0`, `Type 1`, `Type 2`, ...) and the ability for something to be polymorphic in the index, such as: ∀(n : Nat) → ∀(a : Type n) → ... Once you have a tower of `Type`s you can begin to encode things like type-safe `head` It's called "universe polymorphism" because `n` is like the "universe of types" that you are using and the above type is "polymorphic over the universe"
Oh, neat, `Day`s *do* have an enum instance! λ&gt; import Data.Time λ&gt; let date = fromGregorian 2000 10 03 λ&gt; date 2000-10-03 λ&gt; succ date 2000-10-04 λ&gt; :t date date :: Day `UTCTime` time values don't, though. λ&gt; now &lt;- getCurrentTime λ&gt; now 2017-05-15 22:39:46.62058 UTC λ&gt; succ now &lt;interactive&gt;:15:1: No instance for (Enum UTCTime) arising from a use of ‘succ’ In the expression: succ now In an equation for ‘it’: it = succ now 
Ah! I get it. Simple, and makes total sense to me. Wouldn't that, though, require a very weird way of programming directly on opcodes and proving things about them? I need to digest that for a while I guess.
&gt; So you agree that lineal types solve the problem. Well... Thanks. Not really? If you write a bunch of manual mallocs and frees, you can get a little more safety out of linear types. But you're still at risk of early frees and borrowing is difficult to do. Without proper compile time analysis of linearity, linear types don't buy you much; but they do buy some small amount. &gt; unlike other languages Huh? Pause times being proportional to the number of objects on the heap is par for the course. Go is the odd one out on this. I'm having a hard time understanding your argument here. Are you saying that longer application lifetimes mean worse GC performance as time goes on with GHC? Because that's not really true, unless you expect your server to be leaking memory (in which case, yes, leaks increase pause times). Pause times are proportional to objects on the heap, which is, again, par for the course.
How much worse is it just to accept `Type :: Type`, as `DependentHaskell` will do? Obviously it means that it's possible to falsely prove anything, but a relatively small amount of discipline on the programmer's part is enough for this to be ok, right?
Here's one such case: data T a where T1 :: a -&gt; T a T2 :: Int -&gt; T Int instance Functor T where fmap f (T1 x) = T1 (f x) fmap f (T2 x) = T1 (f x) This doesn't have `fmap id = id`, so you might think it's a silly function, but it's expressible, so we can't assume the Functor laws (for fear we might compile, but not run)
I'd be really interested to hear how that could be exploited like that!
It seems simpler: map f (map g xs) map f $ case xs of [] -&gt; [] (x:xs') -&gt; g x : map g xs' case xs of [] -&gt; [] (x:xs') -&gt; f (g x) : map f (map g xs') Which of these translations is hardest?
Yes, I believe Liquid Haskell uses z3 (a SAT solver) by default.
[Use it to prove bottom](http://www.cs.nott.ac.uk/~psztxa/g53cfr/l20.html/l20.html) and then use that to type your contract `giveMeAllYourMonies : GivesYou 20` via bottom-elimination.
What "full codebase"? Put the above in a `let`. Of course, you can't turn on `Set : Set` locally, but here we're talking about a hypothetical language that would be `Set : Set` always.
The documentation says the only important thing: it directs you to `lifted-base` which is the user-facing bit. this package is just guts, and complicated ones at that.
We have greatly shrunk the gap between the kinds of things you can express with LH refinements and "full dependent types". This work is still in flight, but see this for an example: https://twitter.com/RanjitJhala/status/857564398600245248 With it, it should be quite possible for LH to specify and verify the kinds of things you're describing above. 
I suspect that Ur/Web doesn't meet the requirements set by the OP: it doesn't have a full dependent type system and is pretty specifically tailored to web applications.
You probably still want to keep the initializer in `IO`, too. If you don't pair it with the finalizer using `bracket` you can have an exception in between the `initialize` and the `using` which prevents the initialized resource from being disposed of correctly
Of course that’s a good excuse to skimp on documentation.
What do you want documented!? To do what?
Good introduction to how instance resolution works. I definitely tried the same erroneous code when learning Haskell—seems like a bit of a problem with the language that it consistently trips people up. For the prefix notation, you can add parentheses to make it valid Haskell: ArgCount ((-&gt;) a b ) ArgCount ((-&gt;) x (y -&gt; z)) Likewise, this: ArgCount (m a) ArgCount ((-&gt; a) b) Should be written as this: ArgCount (m a) ArgCount (((-&gt;) a) b) Since if we had type operator sections, it would be `(a -&gt;)`, not `(-&gt; a)`. And I wish we did—it would make it easier to introduce the functor/applicative/monad instances for `(-&gt;) a`.
Improved compiler performance always sounds good to me ;) Thanks GHC team for all your hard work!
Backpack!
There will be a dramatic redesign of the current codebase once we hit 1.0 and are planning on both performance optimizations and also corresponding with a formal specification pipeline. We are also planning on heavy use of liquid haskell for 2.0
There's already a Lem formalization of the Ethereum virtual machine, led by /u/pirapira of the Ethereum Foundation. https://github.com/pirapira/eth-isabelle (cc /u/SrPeixinho)
It's more about being able to write a "smart contract" with some potentially complex logic and then provide guarantees to users of the contract in the form of dependent types.
I wonder how Redis (via [hedis](https://hackage.haskell.org/package/hedis)) would perform? As I understand, the author made pipelining front and center from the library's inception.
I think people here and Haskellers generally could learn a bit from a write-up explaining how you chose to divide up this large application, when the team gets a moment.
Lineal types bring all that compile time analysis. Read the documentation. Well Elvish, thank you for the chat. I can't do more. We reached the hard stuff of the elvish' mind.
Something like this: https://gist.github.com/ehamberg/e9132db561548125c8bfed00f4444a29 (I have only tested the macOS build.)
I personally think like the `SubHask` style definition where there is no value in between `a` and `succ a`, I think I have seen that definition in broader mathematics as well. Under such a definition the only allowable definition of `succ` is the absolute smallest increase in time possible. IMO the instance for `Double` should be the same, perhaps `[x .. y]` can desugar to something besides `Enum` if people really want it to work for `Double` values. Unfortunately at the moment both `Bounded` and `Enum` do not have `Ord` as a superclass. You could argue that `Bounded` should have something like `POrd` as a superclass and thus for now just `Eq`, but `Enum` under a more mathematically rigorous definition gives rise to a compatible `Ord`.
You state you want to prove properties of your programs, and then specifically talk about dependent types. Dependent types are only one means of proving program correctness. For instance, Isabelle/HOL is not dependently typed, but is able to extract verified code to OCaml, Haskell, SML, and Scala. OCaml and Haskell can be further compiled into Javascript, and Scala can interact with the entirety of the JVM ecosystem. See [this document](https://isabelle.in.tum.de/dist/doc/prog-prove.pdf) for a gentle introduction. Further, there's been a specification of the Ethereum Virtual Machine recently completed in (Lem, then extracted to) Isabelle, by Yoichi Hirai from the Ethereum Foundation. They managed to prove some simple smart contracts correct in Isabelle. See [here](http://fc17.ifca.ai/wtsc/Defining%20the%20Ethereum%20Virtual%20Machine%20for%20Interactive%20Theorem%20Provers.pdf) for more details.
Compact region still not working.. EDIT: it works, sorry guys!
Ticket [`#12369`](https://ghc.haskell.org/trac/ghc/ticket/12369) is related, would let you define newtype family Container (a :: k) :: k newtype instance Container (a :: Type) = Container1 a newtype instance Container (f :: k -&gt; Type) (a :: k) = Container2 (f a) newtype instance Container (f :: k -&gt; k' -&gt; Type) (a :: k) (b :: k') = Container3 (f a b)
Ad compact regions: the thing lives in IO; can it be used with `unsafePerformIO`? Any reason not to do that?
Thank you for the detailed and marvelous explanation. absolutlety I will update my code to use the Reader monad, as you mentioned, ignoring the database connection will be useful to write function that use the database connection without giving it explicitly , furthermore, it will be a good reason to use `Monad transformer`. Thanks again, awesome. 
I see, I think it will be good to use it in my case, since the `HandleIO` is a special case for `IO` monad which only access the file system
I'm curious, in which sense?
It's a fairly small change to `Parser` to produce a `Parser [Article]` rather than a `Parser [String]` and trying to convert `[String]` into `[Article]`. Writing your own parser is good for learning!
Interesting... I'd imagine that the time between when the resource is actually allocated and when the initializer returns a Haskell-ized data type is probably quite a bit greater than the time between binds in my definition. That's not to say you're wrong; the initializer should probably be in `IO` so it can be used in the `bracket`. But I don't think that fully solves the problem, does it, since I'm guessing vanilla `bracket` doesn't handle that problem very well either? Also, I think the finalization will always be deferred until the end of `runManaged`, right? So this: runManaged $ do x &lt;- bracket' initialize finalize body ... -- A bunch of other stuff won't call `finalize` until after "a bunch of other stuff." That seems like a major drawback.
Yes, I believe they will be merged eventually. The diffs that are still open will very likely see a few more iterations before being merged though.
The Coq backend has been used in anger precisely once: about four years ago by an MSc student. There's also no manpower available to fix any of the problems in that backend (and there's a few serious ones, due to Lem's design being biased towards HOL), so whoever uses it is essentially on their own.
Did [unpacked sums](https://ghc.haskell.org/trac/ghc/wiki/UnpackedSumTypes) end up making it in? The wiki says yes, but it has not been mentioned in the announcements.
Ahh, yes, I somehow missed those. I'll be sure to mention them in the final release announcement. Thanks!
&gt; I found, in some libraries, functions were translated into the identity function (f x into x). Which libraries were those, out of interest? That shouldn't have happened... How did you start using Lem, by the way? Through TT?
See line 545 here: https://bitbucket.org/Peter_Sewell/lem/src/cc0920354a50e805f0b968b2cbd72f36d2b874eb/library/word.lem?at=master&amp;fileviewer=file-view-default#word.lem-545 I understand what you mean by TT, yes, he had told me about Lem. The direct cause: somebody else saw me translating Coq into Isabelle/HOL and he started the Lem version, and that worked for me.
Oh, that's bad. I will try to fix that...
Indeed, people were concerned that a `NFData` constraint is easily circumvented since it is user-provided. In the future we will indeed want to have some sort of class as you suggest. Perhaps you'd be interested in giving implementing this a shot?
As someone who has suffered through multiple 24+ hour build times on a RPi3, I am pretty excited about this.
I think that scenario is incredibly unrealistic, as your "application-level stuff" is not what's being benchmarked here. Remember that lots of people run their "application-level stuff" in python, lua, php and similar, but what's being benchmarked here are shims over C-libraries. So yes, haskell might be 4 times slower than C, but python will be 20 times slower than Haskell, because none of that C-wrapping will help the application level stuff.
I'll probably look into it in summer vacation. Diving into ghc codebase takes quite some hours :)
Where is the User Guide for this RC? I can't find it in the obvious places. 
FastHttp is [written in Go itself](https://github.com/valyala/fasthttp/blob/master/http.go); likewise the Scala and Java libraries. Not everything that tops these benchmarks is "C in disguise". Moreover, as applications like Postgrest indicate, a big part of any web-app genuinely is just generating SQL queries from HTTP requests and returning the result as JSON. If that's as slow as these benchmarks suggest, any minor business-logic isn't going to make a visible improvement. To put things in perspective, my toying-at-home languages the last two years have been Haskell and Rust. Over at /r/rust they're already talking about profiling hyper to look for unnecessary sys-calls, and incorporating a new async Postgres library for round 15. I expected to see a similar debate here in /r/haskell about optimisation opportunities: it hasn't happened. 
I think the hardest part is where you relate the first one with the last one to get a fused loop. Just guessing 
Is there some fundamental reason why ASLR causes problems here? My understanding is that compact regions are basically a contiguous block of memory and loading that should work regardless of ASLR. You probably need to adjust pointers but that seems doable and is probably necessary even without ASLR.
I did a school project with haskell on an rpi years ago. Even with the lack of tooling and functionality in general (no TH, no cabal, had to use hugs instead of ghci, the networking libraries didn't work out-of-the-box for whatever reason so I had to shell out to python -_-, the gpio library crashed due to flushing input pins which I had to patch locally, ...) it was still very pleasant! Can't wait to take another stab at it.
Well, sure, and some packages you might want just don't compile at all on the pi at the moment (eg: aeson). However, if thats the only Unix device you've got in your house (such as myself), then it's still an option someone might want. Without having tried it, I'm pretty sure that this cross compile stuff wont work on windows just looking at how much it's assuming you have unixy build tools, though perhaps it will with a full cygwin suite or something like that.
Agree. I bailed on developing in haskell for the pi a few years ago because of the compile time resource requirements. Haskell runs pretty well on there once you can get it to build...
Well my suggestion of using Haskell directly on the pi doesn't prevent anyone else from cross compiling or not :P
I would think that we'd *at least* remove `last` along with `init`, in order to prevent symmetry breaking. I'm also comfortable with removing `head`/`tail`, but those at least make some sense because we are using cons lists, not snoc lists.
Haha of course not, I just meant I don't want that to be the only choice and cross compilation not be pursued because of it :D
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [rumaak/room-model/.../**Main.hs#L67** (master → 6ba1a25)](https://github.com/rumaak/room-model/blob/6ba1a257db9f94940f08939084c3d9dcb32d5e37/src/Main.hs#L67) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dhn6qdm.)^.
Yep, I've been there a few times, and even talked there. I think we should start a purely pub based meetup - those are the best technical discussions - and certainly the NonDysFunctional programmers would be welcome to join in.
I am using ArchLinux on all of my machines, including a pair of Rasberry Pi2/3. So far I am quite happy with the Haskell resources provided by Arch. Also, I have found [ArchLinuxARM](https://archlinuxarm.org/) much better suited to my needs than Raspbian Jessie. There are `ghc-8.0.1`, `cabal-install-1.24` and about 480 precompiled haskell related packages for the ARM platform, see in the [package repository for ARM](https://archlinuxarm.org/packages). There is no `stack` yet for the RPi but I did not bother to compile it. I mainly develop on some x86_64 platform then on the RPi just use cabal, first with --dry-run, and try to utilize the precompiled packages. Admittedly, it is not a racing car but for my toy IoT programs is OK so far. The RPi-s operate as headless servers, but just for the kick of it I set the RPi3 up and tried with my usual`xmonad` based desktop environment with a large monitor (1920x1200) and it was surprisingly pleasant. Nevertheless, I keep an eye on how the cross compiling for ARM is evolving. 
Thanks I'll look it up
Over-(ab)use of CPP will break the tooling. There are - http://hackage.haskell.org/package/base-compat and [other compatibility packages](https://github.com/haskell-compat/base-compat#other-compatibility-packages) help with unified `Prelude` via `Prelude.Compat` and other stuff, so you can off-load CPP-ing to them. - Also http://hackage.haskell.org/package/base-orphans is sometimes useful.f
You might want to take a look at [this](https://www.reddit.com/r/haskell/comments/6b3dlt/techempower_benchmarks_14_released_with_servant/dhl500h/) comment above.
You really need to read [this](https://www.reddit.com/r/haskell/comments/6b3dlt/techempower_benchmarks_14_released_with_servant/dhl500h/) comment already. You are drawing some very wrong conclusions.
This is the wrong twitter handle. The correct one is [@zw3rktech](https://twitter.com/@zw3rktech)!
I haven't tried this with windows. There is however [Android NDK for Windows](https://developer.android.com/ndk/downloads/index.html). As the [Raspbian SDK](https://medium.com/@zw3rk/making-a-raspbian-cross-compilation-sdk-830fe56d75ba) was modelled to be somewhat similar, I believe it should be able to do this for windows as well. Regarding the build tools, I don't think there is anything outside of `autotools` and `make` involved. Both of which I believe are required for building haskell on windows as well. If someone wanted to give this a try, I'd be happy to lend a hand where I can.
Unpacked sums subsumes this optimization in one sense and not in another. :) data T = C1 !Int | C2 Is represented as `(# Int#, Int# #)`, where the first int is the tag. In other words we avoid any allocation, which is great. However, in the case of `Maybe a` we'd like a representation which is either a pointer to an `a` (or a thunk) or a null pointer for a total of a single word, so there's still some room for improvement there. Summary: if you use strict, unboxable types in your sums (e.g. no polymorphic or lazy fields) unboxed sums will work well. For polymorphic and lazy cases there's still (lots of) work to do.
You could, but having a cross-compiling is nice from an ergonomics perspective. 
Yes ;-) It is!
It's worth telling someone, no idea how receptive they'll be. 
I feel like there is room for improvement in terms of the ghc-options. For one site, I get around 2700 req/s on my laptop with vanilla Yesod compiled with ghc 8.0.2. But, if I use ghc 8.2.0-rc1 and the llvm backend, it jumps up to 3900 req/s. 
Agreed, but also see my comment [here](https://www.reddit.com/r/haskell/comments/6b3dlt/techempower_benchmarks_14_released_with_servant/dhnqyms/?utm_content=permalink&amp;utm_medium=api&amp;utm_source=reddit&amp;utm_name=haskell). The future is bright!
I absolutely could sketch up a better benchmark, but it would be very hard to implement and enforce consistency. 
Question: is using cereal for JSON a reasonable idea? Or is that just a bad idea. 
&gt;There will always be a trade-off between correctness and performance. That does not mean performance shouldn't matter in the here and now. Correctness will *always* have an overhead. 4 times slower? No. But don't expect safety with performance equal to that of C++. &gt; I worry sometimes that the broader community sees Haskell as a language for building tools to build apps instead of a language to builds apps outright. Imagine you're a contractor. You're competing with a Java contractor. Serious question: is this a reasonable expectation? If we closed the gap with scala would we ever be able to get it equal to Java? 
&gt; Lua, Java, Go and JavaScript Apples to oranges. Lua shouldn't beat Haskell. But JavaScript and Java are unsafe, and Go is a systems programming language. &gt; That this suggests an unusual difficulty in hitting performance targets using Haskell. I think it suggests yesod has had a lot of eyes and servant has not. As I said elsewhere in the thread, ghc is making more performant code with the next release.
a field that's polymorphic over unboxed types (like `data X (u :: #) = ...` or something) doesn't work because the types' sizes are different (like ByteArray versus Word), right?
Note that internal libraries are broken in this release. Should be fixed next RC.
I've complained about this for ages and I've used it for work for awhile.
I'm not sure. Does GHC allow for such polymorphism today? In general we could deal with unboxing in polymorphic cases if GHC compiled polymorphic code by monomorphize is by default. I don't know if it's possible to start doing that at this point however.
Quite fun indeed, and I find the hole interface interesting (I'm not sure about just showing holes on hover, but I guess it would be easy to have a recap. of the empty holes of the current function in a right pane or something). I also hadn't realized that RedPRL had adopted cubical, it's a fast moving project!
Ooooh I love it
There really should be some annotation to declare "this instance complies with the class laws"
no, doesn't typecheck
Honestly that's what having the instance at all is supposed to do =P
Thanks :)) I think searched for it, but haven't found anything (will try Hoogle next time) 
Yes, why not?. This should not a sect. One programmer can use a language ecosystem and not being blind about the problems
An `Enum` instance for `Double` would lead to 1/ε levels of pain for new users, bizarre behavior of `[1..x]` and other such nonsense.
Awesome stuff, thanks to all who contributed! Based on my small bit of testing Im *really* looking forward to developing with 8.2's new error messages. Not only are they pretty as hell, but they're quite well optimized for the fact that usually the first thing I want to look at when something goes wrong is the *code* in question rather than the type error. 
Why would you go through the trouble of compiling libffi if it's included with ghc? EDIT: Ok, never mind, I didn't read the entire thing.
I don't have enough experience in this field to make a really constructive explanation, but I can try to sum my current thoughts. To start with, the Haskell development overall feels slower (having to restart the program each time is kind off frustrating, although it makes debugging easier) compared to interpreted PHP. Second, templating. Maybe is just me, but loading .html file and inserting output there feels always better for me than writing the page in templating style. Having a module for these could work out, but I haven't tried that yet. Third, support. Development on Windows (yeah, still didn't switch to Linux) was pain in the beginning. After few weeks of Cabal hell (not the dependency one) I decided to learn Stack. Really good tool, but I spent some time learning it and realizing, that even after all in my project is done, the final executable won't work without stack installed (but it should work after getting some PATH work done). I don't know how well do Haskell frameworks perform, but I believe, that currently even "perfect" code doesn't reach the performance of top web languages like PHP and Java. I would be glad to hear different look on this subject, so feel free to confront me :)) 
[`fromDynamic`](http://hackage.haskell.org/package/base-4.9.1.0/docs/Data-Dynamic.html#v:fromDynamic) has a `Typeable` constraint. With a concrete type, the compiler can find the relevant instance. With the polymorphic type, you need to add the constraint `Typeable a =&gt; ...` to your function.
Thank you!
Thanks, I'll have a look at it :)
Thanks! I thought I was misunderstanding some internal GHC magic with the automatic Typeable deriving but the solution seems so obvious in retrospect whoops
It's running *compiled code* at compile time, and it can do *all kinds of IO*. :-) As this is a longer topic, I plan to devote the posts next week solely to TH, it's relation with cross compilation and introducing and explaining how this can/is being solved.
From "Easily working with JSON" to Lens and TH in a handful of lines. /u/5outh , perhaps you could add an introductory part that shows how to achieve the same getting/setting logic just by using functions? I feel the post lacks a "why bother" section.
Thanks for the reply (and for the work on the benchmarks). You're right that with e.g. routing tests the solution is to simply add it. But the reason I didn't think it worth it to bring up the other issues is because, as far as I can tell, there's simply no easy way of fixing them. As you say, there are tons of frameworks (and I think also no easy way of contacting the authors of the individual benchmarks?); anything that breaks all of the current tests is, I'd imagine, off the table. I guess there could be a longer-term solution - e.g. adding content-type negotiation or whatever it might be to all tests as separate tests altogether, and then over the course of a few benchmark cycles deprecating the older ones - but that too seems unpalatable. I should say - I am very impressed with quite a bit in these benchmarks, and am happy they exist. I just think that further work on my side (just the web framework, not serialization, templating or DB drivers) on getting better results will primarily boil down to doing well on the benchmark rather than improving performance of the framework itself.
They are the same type. The second syntax also defines a record accessor for the type. A record accessor can be used as a function, for pattern matching, and for "record update syntax".
Why are partial functions bad? For me personally, whenever I ran into a partial function exception, it pointed to design errors in my program or plainly wrong code that didn't behave as I expected and needed to be changed. A function that always works would mean having to deal with the alternative that it yielded Nothing, but what if you clearly expect your program to always yield something at that spot? I think that's akin to a beginner mistake of always catching Exceptions in Java instead of throwing them on to where they are to be processed in a meaningful manner. Some things are not meant to be handled in the program, but rather to make it crash and clearly point out that something went wrong. Errors in a program that are not recognized as such can corrupt everything that follows and make the real problems harder to find. The set of the four functions head-tail-init-last is not only well established in the reports that founded the language (in the standard prelude), they are also dual to each other. Removing one of them would mean throwing that duality out of the window, and removing all four of them would mean an extra step for getting access to some of the most common list functions that are a good window into the language for many tutorials too. If init', init2, etc. are not good for you, there are many ways of importing modules -- You could add an import for Data.List "as L" (without "qualified") to have the access path at hand when you need it: L.init . tail $ "hello" - and then just use the word "init" for your own symbols as you wanted. :) 
I really liked the fact you included exercises with the tutorial. And it's great to see more Spock tutorials out there! Thanks!
Actually, all credits for this one go to Bryn: https://github.com/agrafix/Spock/pull/122 :-)
When you know a signature works, but another does not, you can ask GHC to infer the signature for you, using `_` as the signature. For example: update :: _ update = ... GHC will complain and write you the infered signature. This is usually really helpful.
Is there any reason not to add an explicit rule for `map f . map g` to GHC?
Since default GHC uses its own code generator, I have this wish that we could escape the typical C/C++ toolchain model and cross compile like Go users. Correct me if I'm wrong, but everything but the final link step is done by GHC and it looks possible.
I'm afraid I do not know how Go cross compiles (and how it contrasts to the typical C/C++ toolchain model you mention) to compare it to ghc.
Thanks Gabriel! By the way, by default the editor shows the probes/holes that you have made in a pane, but I just hid it because my screen is not so big. We're using VS Code as the main editor, and one disadvantage is that their diagnostics pane is still pretty crappy: for instance, multiline things don't display properly. So, at some point we will probably want to make our own pane that works a bit better.
Great tutorial and I really liked the exercises. Might be worth noting that there is an `object` missing [here](https://www.spock.li/tutorials/rest-api#adding-people), though: errorJson :: Int -&gt; Text -&gt; ApiAction () errorJson code message = json $ [ "result" .= String "failure" , "error" .= object ["code" .= code, "message" .= message] ]
It's a type equality constraint. Here's a blog post about them: http://blog.infinitenegativeutility.com/2017/1/haskell-type-equality-constraints
Please excuse my ignorance then. Why is OP saying that this optimization does not happen, followed by the ensuing discussion about how it is not possible for GHC to optimize it?
Thanks!
Good point! I will add something about that. FWIW, in so many words, when working with APIs there are typically a lot of overlapping fields in response types. The main motivation of using auto-generated lenses here is to avoid that issue in code. Template Haskell is there to get two things done at once (serialization/deserialization) with no extra code. Happy to write all that up in the post, thank you for the feedback.
I don't think the OP asked why the optimization does not happen, but rather, why it cannot be optimized *without the help of explicitly written rewrite rules*. So my understanding is that there is an explicit rewrite rule near the definition of `map` and thus the optimization does happen.
I suppose that's true, maybe just mentioned for completeness.
I haven't used `cereal` for JSON. I've always used `aeson`. They [use the library at Facebook](https://code.facebook.com/posts/745068642270222/fighting-spam-with-haskell/) to process billions of JSON BLOBs, so I figure that if it's ever "slow" for my purposes, I'm probably using it wrong (going back and forth between `Text` and `ByteString` a lot, for instance, is a mistake I've made in the past).
Good points, I definitively think the 140ms latency vs 2-3ms is a serious issue. I'm guessing that it's possible to fix it though. A 100x latency disadvantage is serious, but say a 4x efficiency penalty I'd gladly accept to get some extra robustness. 
&gt; But JavaScript and Java are unsafe, and Go is a systems programming language. While compiled, Go's heavy runtime --including built-in green threading, async, deadlock detection and GC -- means it's no more a systems language than Haskell. The unsafeness of Java is overblown: with the additions of streams and optional in versions 7 &amp; 8 and especially if one uses one of the safe dialects like Kotlin a large chunk of potential errors disappear. It's still not perfect, but one shouldn't underestimate it. JavaScript is a tragic disaster. I honestly don't think GHC is the primary culprit here. I think it's the IO, DB and web stacks. It's conspicuous how the Haskell solutions tumble down the rankings as soon as the database is involved. 
Types like `MyType` are usually wrapped in a `newtype` to write instances for them instance Functor MyType instance Applicative MyType instance Monad MyType but this means you have to deal with annoyingly wrapping (`MyType`) and unwrapping (`runMyType`), when learning out it can help to have a 'parallel construction' using a simple *type synonym* type MyType a = String -&gt; [(a,String)] `return` now becomes return :: a -&gt; MyType a return a str = [(a, str)] rather than a clunkier `newtype` return :: a -&gt; MyType a return a = MyType (\str -&gt; [(a, str)])
I must mention this gem &gt; type Parser a = String -&gt; [(a, String)] &gt; &gt; A Parser for Things &gt; &gt; is a function from Strings &gt; &gt; to Lists of Pairs &gt; &gt; of Things and Strings! &gt; &gt; — [Dr. Seuss on Parser Monads](http://www.willamette.edu/~fruehr/haskell/seuss.html)
 env GOOS=linux GOARCH=arm go build helloworld env GOOS=windows GOARCH=amd64 go build helloworld env GOOS=android GOARCH=arm GOARM=7 go build helloworld All available without installing or configuring special tools or environments.
First off - any result other than 101 means that the code is broken. So your code is broken. STM by itself doesn't break like this. That's the whole point of it. So clearly, no. You cannot use `unsafeIOToSTM` like that. STM transactions can be retried in more places for more reasons than you expect. I don't know where it's going wrong exactly, but it doesn't really matter. There are two main ways to mix STM with IO actions without this problem. The first and simplest is just having your STM action return an IO action. This works well when your STM actions will never need to retry depending on the result of IO. If you potentially need to control whether an STM action retried depending on the result of an IO action, you can use the [stm-io-hooks package](https://hackage.haskell.org/package/stm-io-hooks). It still limits you to only running IO actions after the STM logic completes, but it will retry the whole thing if the IO action throws an exception.
Ahh, it seems that I never pushed them. Let me fix that.
&gt; (To that end, I often think TechEmpower is missing an HTML-template-rendering endpoint/test as well.) For what it's worth, the Fortunes test requires the use of server-side templating. See requirement #9 of test type #4 (Fortunes) here: https://www.techempower.com/benchmarks/#section=code The Fortunes test results is also the default view. We attempt to classify implementations as either "realistic," meaning they represent realistic use of the framework and underlying platform, or as "stripped," meaning they have been engineered to the very specific requirements we state. Though to be clear, we do rely heavily on the community to help identify implementations that should be considered stripped, and as you can imagine this is a subjective measure leading to some debate among contributors. Ultimately, you are right that it's inevitable that people will tend to target the very particular things we measure, and we attempt to deal with that. Of the current test types, the Fortunes test exercises the broadest set of functionality that one might expect in a web framework (request handling, routing, database connectivity, ORM, JSON serialization, XSS countermeasures, templating, response handling). And we intend to add more test types as time permits. I have found many people focus on the Plaintext test only, which is a bit too narrow a view. The Plaintext test was added to satisfy some community participants who wanted to show off their ultra high-performance request routing/processing capabilities. It's the only test run with HTTP Pipelining enabled, making it the least useful to developers who are evaluating frameworks. On the flip-side it has the highest numbers, making it the most "exciting" from some points of view. From our perspective, the frameworks that pull ahead in Fortunes are among the most interesting.
That's too bad... I can't see how `stm-io-hooks` would help unfortunately. I would like to be able to get a retry or commited response after preparing a transaction, like a normal transactional database basically. Thanks!
Yep, I believe this is the main offending function: http://hackage.haskell.org/package/template-haskell-2.11.1.0/docs/Language-Haskell-TH.html#v:runIO For awhile now I've been wishing for a "pure TH" that did not have access to IO. We still need the IO part, but it would be really nice to have a non-IO part. Maybe break it into two monads: `Q` and `QIO`. It seems to me that this would give several benefits: * Better cross-compilation story for packages that don't use `QIO`. * Better security for code playgrounds like the in-browser Haskell code widget at https://www.haskell.org/ * Faster GHCJS compilation for TH because it pure TH could be compiled and run for the build machine rather than using Node.js to run it in the host.
&gt; Correct me if I'm wrong, but everything but the final link step is done by GHC and it looks possible. This is incorrect, GHC doesn't do any compiling to any executable format. The default backend spits out assembly code which is assembled by an outside toolchain. So actually compiling *and* linking is done by another toolchain. The exception to this is the GHCi backend which supports a byte-code target. It's a double edge sword, but it's massively simpler to do this than writing out the binaries ourselves. It's not a technical limitation, just strictly a resource and focus thing.
Thanks I didn't know that. This makes me wonder whether it spits out assembly code that's fed to gcc/clang. What format is it in? I mean this would explain more than just the need for `ld` why mingw is required for GHC on Windows.
This does not use the API definition features of Spock am I correct?
Well, I have looked at the requirements a number of times but I totally missed that the fortunes benchmark requires rendering a template. Thanks for dropping by and clarifying stuff. I definitely have no desire to disparage your work: it always makes for interesting discussion and I think it's done in the right spirit (but it's a Herculean task, in a way, which will attract controversies).
The simplest examples are usually related to the lack of immutable data and insufficient lambdas in python. Deep copying becomes tedious after a while. Ruby for example has decent support for that even though it's also just by convention to avoid mutation. 
Since IO actions can be aborted and re-executed many times, you can do it with unsafeIOtoSTM if 1) your action is idempotent: the result do not change if it is re-executed many times with the same parameters, and if the parameters change, the result is the one of the last execution 2) launch it in a different thread: mv &lt;- newMVar () unsafeIOToSTM . forkIO . withMVar mv $ const myIdempotentTask Since the additional thread will not be aborted if the transaction abort and retry, and each retry launch a new thread, you have to use blocking in the new thread (but the STM transaction continue being blocking free) 
What's preventing us from having an inline-optimized pragma?
I don't know the specific variant it writes out, but it's nothing that MASM shouldn't be able to handle. The only reason we are currently tied to mingw-w64 is because it makes things simpler. Historically, C compliance on the Microsoft compilers have been (well, no need to sugar coat it) quite crappy. Also some assumptions are being made in the rts about the linker that was used and some specific ld quirks. But it's nothing that can't be factored out. I think at some point in the past the RTS was compilable using CL. That just bitrotted. Now that there are free versions of the toolchain available, and C99 and C11 compliance are much better than maybe we could get it working again.
actually, I take it back, we do use gnu specific extensions such as .gnu_linkonce, but nothing that again can't be work around.
No, it does not. We should add a note about this and (hopefully) write a tutorial about how that works... 
&gt; application’s confinguration. In *confinguration*?
Leave empty lines around your code blocks :) I'm going to give you one tip: when you see a type error, the first thing you should try is delete the type signature and see what happens. What happens in your case?
https://twitter.com/bitemyapp/status/861631849734569984 :P
I thought about this, but one problem is that this check is also used for cooperative scheduling: &gt; Note that HpLim may be set to zero arbitrarily by the timer signal &gt; or another processor to trigger a context switch via heap check &gt; failure.
And here is another: https://github.com/tittoassini/zm 
Follow-up: I checked out the *Fortunes* test again based on your recommendation. Normally that's the test I don't look at much, but I came to it with a new appreciation and was pleased (relieved? edified? not sure the right word) to see that the handful of frameworks I have personal experience with are in the ordering I would expect them to be in generally. At any rate, it seems to match my expectations and my experience a little more than the others.
That video was awesome with a great background score. I have been long waiting to try one of the PRL family of proof assistant. NuPRL is honestly unusable. 
I'm tempted to say we need a better "language" for this stuff. Even disregarding the need to inline things optimized sometimes, the phase number approach we have now for controlling the ordering of inlining and rewriting is a major pain. I think there needs to be more abstract semantics for this stuff. And FWIW, `-fexpose-all-unfoldings` will put the optimized unfoldings of every function into the `.hi` file, but obviously that doesn't apply on the per-function level. I'm unaware of any version for doing that to one function. Which is another problem: What if I want to expose all the **un**optimized unfoldings in a module? I have to go through and manually add `INLINABLE` to all of them. The fact that you have to choose between whole module+optimized and single function+unoptimized is not very good.
I think relying on the LLVM backend would be a far better approach. The less we ask GHC to do, the better; for the GHC devs, for modularity, and for expanding to support whatever LLVM supports.
Your first line says "the concatenation function accepts two lists of possibly different types and returns a list of pairs". Instead it should say "the concatenation function accepts two lists of the same type and returns another list of the same type". Fix it. Your second line says "the concatenation function is the result of calling an auxiliary function on two empty lists". Your third line says "the result of calling the auxiliary function on two empty lists is an empty list". Putting these together, we get "the concatenation function is the empty list". That can't be what you meant. Fix it. And so on.
Super cool! I should totally update my [HPi](https://github.com/WJWH/HPi) library some day. GPIO from Haskell for the win!
It looks like this is a different, simpler case but I wonder if https://ghc.haskell.org/trac/ghc/ticket/13379 changes improve or affect this.
I'd suggest posting to the ghc-devs mailing list, or directly to the GHC trac. Sounds like a cool idea.
I'm unsure what preparing means here but that sounds easy. (transaction &gt;&gt; return Commited) `orElse` return Retry I also agree with u/djfletch about TChan.
Your third line says "the result of calling the auxiliary function on any two arguments is ??" Your fourth line says "the result of calling the auxiliary function on two arguments, the first of which is empty and the second is non-empty, is the second argument". No matter what you write in place of ??, the fourth line will become unnecessary, because the third already handles any two arguments. So there's something wrong with your code. Fix it.
Upon spending more time looking at the code the only problem seems to be how runTx is trying to inspect the transaction for Retry responses. Applying orElse like above and simplifying we get something like this (untested). runTx counter i o = do req &lt;- readChan i case req of Done -&gt; return () Inc -&gt; do resp &lt;- atomically $ flip orElse (return Retry) $ do cnt &lt;- readTVar counter writeTVar counter (cnt+1) return (Response cnt) writeChan o resp runTx counter i o
I've been working on a different part of GHC, so this may be wrong. I believe all desugaring occurs before type analysis. `[x..y]` desugars to `enumFromTo x y`, and the `Enum` instance is selected at that point. 
Not saying that's a bad goal, but isn't LLVM still unable to accommodate Haskell fully? I mean, LLVM barely is sufficient for Rust's needs and they are struggling with important optimizations they're unable to express in LLVM IR. Haskell has an even harder time because of GC. Don't get me wrong, I like the idea of LLVM as the backend since it would give us good code generation for more architectures and maybe in some cases better optimization, but the current GHC LLVM backend is not a general win, only in certain cases. Is GHC LLVM actively developed or just maintained?
LLVM does not stand in the way of the GC or any of GHC's runtime in anyway. It's just an alternative to assembly for the Cmm layer. GHC (and Rust) don't really treat it as the main optimizer; mostly just as a cross-platform assembly. That's why it's not usually a performance win; the optimizations are mostly already done by GHC among the earlier stages. I'm unaware of cases where the LLVM backend creates a non-negligible performance loss. Point being, I don't see any real performance difference here, but I do see an opportunity to really simplify how GHC works. In fact, the LLVM backend is already a much much simpler chunk of code than the native backend, with basically the same performance (occasionally better, for very C-like Haskell code)
Yes that is still a problem. My understanding is that LLVM does not guarantee stability in its textual IR format, but I thought there was a binary format that they *do* guarantee stability for. Is this not true? Wouldn't it be much better to use that than to embed specific versions of opt and llc?
One thing you can take a way from the original post is that `GHC` does quite good job by default; when you don't try to help it (i.e. tell to `{-# INLINE #-}`). 
Yes... and I am saying that it could desugar to something different...
In what cases is that a problem?
Wouldn't guard pages still work? At worst the scheduler would have to compare the fault address with zero right?
Functions definitions work like this: `myFunction arg1 arg2` You defined `conc` as being equal to `conc'` with two args - But you didn't define any args for `conc`. That's your first major issue. The second major issue seems to center around how one properly destructure a list. This gave me a lot of trouble when I first started, so no shame here. There's nothing super mind blowing about it, it's mostly just a syntax thing, so I'll just give you a cheat-sheet. xs -- matches anything x:xs -- matches one or more, assigns first item in list to x x:[] -- matches exactly one, assigns first item in list to x x:y:xs -- matches two or more, assigns first to x, second to y x:y:[] -- matches exactly two, assigns first to x, second to y [] -- matches an empty list There are two key concepts here. One: aFunction one two You haven't given `aFunction` a pattern to match, really, you've just told it that the first argument shall be called `one`, and the second shall be called `two`. Since there is no pattern, those will match any case, and so any subsequent definitions of `aFunction` would never be invoked. So if you want to use a pattern to destructure, you should open with one of the patterns you're going to use. Second key concept: `1:[]` makes a valid list, `[1]` So, that means we can infer that in any pattern `x:xs`, `xs` could be empty. That also means that x:xs cannot match a list that could be empty - an empty list by itself has no `:`. So `x:xs` and `x:[]` will both match on [1], but not []. Hopefully this gets you on a good track. I'd concentrate on trying to solve `conc` before you bother with `wow`. Once you've figured out `conc`, `wow` will basically write itself. 
Wait what? Is that new error message rendering?
Fixed it. Now i receive the error (possibly incorrect indentation or mismatched brackets)
Your second line now says "the concatenation function is the result of calling an auxiliary function on a certain value that I haven't defined anywhere". Your third line says "the result of calling the auxiliary function on any argument is that argument". Putting these together, we get "the concatenation function is a certain value that I haven't defined anywhere". I think you should stop treating this as a game of whack-a-mole and try to understand what you want to say. For example, here's a description of a non-tail-recursive function that solves your first problem. It has two lines of code, described as follows: * The result of calling the concatenation function on two arguments, the first of which is an empty list, is the second argument. * The result of calling the concatenation function on two arguments, the first of which is a list consisting of a head and a tail, is a list whose head is the head of the first argument, and whose tail is the result of calling the concatenation function on the tail of the first argument and the entire second argument. Think through it in English and verify that it's a correct description of what concatenation should do. Then translate it into code and make sure that it works. Post it here. Then we can move on to tail recursion.
It probably outputs a human-readable file containing assembly code, which is then made into a binary and stripped by the toolchain. 
I think someone should complete the `wai` benchmark using `hasql`. I think having a benchmark that's doing the bare minimum (no content type negotiation, no log n routing (since there are only five routes), no nothin' fancy) would be the best way to prove whether or not we currently have the tools to be fast without substantial work optimizing the low level network stack.
It's basically push-button cross-compilation. It's a lot more pleasant to work with than cross-compilation with ghc. Partly because of TH and partly because of custom setups. 
Why not pg-simple? Is hasql generally more optimised?
This is what i managed to do conc :: [Integer] -&gt; [Integer] -&gt; [Integer] conc [](x:xs) = (x:xs) conc (x:xs) (ys) = x conc (xs:ys) I will work on it more until you respond. I know that the problem is on the last line...
&gt; I strongly feel that there is nothing wrong with the benchmark. The benchmarks looked okay to me too, and I think it would be great if someone really dug in and figured out where the performance bottlenecks are. I can't help except to give the advice that the quickest way to get a good sense of what's going on is probably to manually insert eventlog logs throughout libraries and experimenting turning different chunks of code into noops. It's likely to be very time-consuming, but I'm sure you could come up with some good patches as a result. Good luck!
Hmm. Are you on windows? It might put the executable somewhere else. 
&gt; There is something wrong with the Yesod and Servant code (or underlying Haskell Postgres libraries). I'd guess it is the postgres; as you've pointed out yesod is competitive in earlier benchmarks. Possibly related to `persistent` though? What do benchmarks look like for other postgres libraries?
&gt; It’s pretty much guaranteed that you’ll try to build the deepseq package. This has actually been the source of rather confusing (and invalid) bug reports against `deepseq`. Why does Stack insist on rebuilding an (according to meta-data) incompatible version of `deepseq` rather than just using the one preinstalled in GHC's package db? 
It is what the servant benchmark uses. 
In this code: conc2 :: [Integer] -&gt; [Integer] -&gt; [Integer] conc2 akk1 akk2 = conc2' [][] where conc2' [] akk = akk conc2' (x:xs) akk = conc2' x (xs:akk) Do [] stand for empty lists or for any list, like _? I know that the code is not yet correct.
It's the standard wisdom for read/write databases that are written to by multiple applications, sure. If you have a database that's only accessed by a single application, it's less important and it's common to depend primarily on the application. In fact, the application really _has_ to ensure integrity because the only alternative is that the database sometimes fails transactions. Nowadays, a single application owning a database is very common, and it often provides services to other applications. Writing consistent data is mostly a correctness issue. Now Haskell is good for correctness but it's no magic bullet. You can be correct in other languages too.
You can still skip the template haskell and use generics when you want to adjust fields. instance Aeson.ToJSON VerificationError where toEncoding = Aeson.genericToEncoding $ Aeson.defaultOptions { Aeson.constructorTagModifier = Aeson.camelTo2 '_' , Aeson.allNullaryToStringTag = False , Aeson.sumEncoding = Aeson.TaggedObject { Aeson.tagFieldName = "reason" , Aeson.contentsFieldName = "extra" } }
https://ghc.haskell.org/trac/ghc/ticket/13714
Probably because it's listed in the lts snapshot... Why the downvotes? It's a legit question. 
Ok, I did it! conc1 :: [Integer] -&gt; [Integer] -&gt; [Integer] conc1 akk1 akk2 = conc1' akk1 akk2 where conc1' [] akk1 = akk1 conc1' (x:xs) akk1 = x : conc1' xs akk1 I am very grateful for your help, thank you a lot!
* Set up the infrastructure to run things locally (with the DB on a different machine). * Check whether you reproduce the results. If not, maybe the flags/connections aren't optimal for the actual benchmark infrastructure. If so, try tweaking RTS flags and connections (though understand you should be optimising for the testing setup, not for your machine). * Check whether using Vector instead of lists (both the `servant` and `yesod` code use lists) improves performance. * Check whether using a newer LTS/GHC helps. * See if async writes are an option (allowed by the rules). If so, try them. * Try pools with different parameters. * Actually profile the code. * Go chase down library improvements. Note that for the multiple DB queries, it seems to me like the `yesod` benchmark is breaking the rules somewhat, which state: This test is designed to exercise multiple queries, each requiring a round-trip to the database server, and with each resulting row selected individually. It is not acceptable to use batches. It is not acceptable to execute multiple SELECTs within a single statement. The fact that there's only one `runPg` seems to suggest it's actually running them all together (I could be wrong, though). It's unclear to me whether the ruby code is doing the same, but it is using only one connection from the pool, which is allowed and reasonable, so the `servant` code could be fixed to not take and return connections for each request (same for `updates`). From the results, it seems pretty clear that the focus should be on serialization and DB. The simple `plaintext` test is doing fine. 
I think u/snoyberg did a good job answering this question already. TL;DR: &gt; Stack and Stackage take a very limited definition of wired-in packages to allow as many packages to upgrade as possible. https://github.com/haskell/deepseq/issues/34#issuecomment-296426661
I downvoted because this feels like u/hvr_ taking pot shots against Stack. They asked a question that makes Stack look bad even though they already (should) know the answer. 
If your list is accurate, I suggest you file a bug report against stackage.
Saying what, exactly? That's the output from `ghc-pkg`. Are packages included that should be excluded, or the other way around? 
It's incomplete. The official `ghc` package (unfortunately) depends on more packages. See also https://github.com/ghc/ghc/blob/master/compiler/ghc.cabal.in#L53
I see. However, it is still constructive. Because of that I learned [which packages and why are considered built in](https://www.reddit.com/r/haskell/comments/6bqfk9/testing_ghc_release_candidates_with_stack/dhp0641/). I suspect that many more stack users are unaware of that detail.
What is the social contract between ghc users and ghc? One might be that any use of `INLINE` which improved performance significantly (with caveats, maybe, e.g. related to phased rules firing) is considered a runtime performance bug. Another would be that GHC should be able to handle inlining all the things and that these long compile times are a compiler performance bugs. IMHO the situation vis a vis inlining is not good mostly because there is no social contract, not because marking things inline is difficult or because compile times (currently) are long. The current situation leads to sluggish pace of improvements (because issues don't make it upstream) and puts library writers in an impossible position of trying to have both performance and sometimes babying the compiler.
I will run benchmark on it tomorrow and I will see then. Till tomorrow ;)
The `extra-package-dbs` flag is not well documented. :(
It's even incomplete for GHC 7.10.3. While `ghc-7.10.3` didn't depend on `deepseq-1.4.1.1` directly, it was part of the transitive dependency closure.
Wouldn't `a' = getCompact . unsafePerformIO . compact $ a` work?
Also, if you want to add async reads (notify) to haskell db libraries, that would almost certainly be huge for the benchmark, but for the community in general too
I'm confused. How exactly do you derive induction, Scott encodings, etc., by just extending CoC with type universes?
"mucking about with stored procedures" gets pretty close to a false dilemma, IMO, even if there *is* a school of DBAs that inisist on gating all db access through SPs.. My approach is to write the business logic in Haskell, read and write data using sql (by all means, go with some ORM if you prefer), and using the tools offered by the db to ensure my app isn't making a mess. FK constraints are a no-brainer, non-nullable fields, check constraints, and perhaps occasionally a trigger are great for ensuring data integrity, and they're all really cheap when used appropriately. Of course, you also want a sane (normalized) data model to make it harder to represent invalid/inconsistent states. Call it security/correctness in layers, or whatever.
Oh I see
Afaik hasql uses the binary protocol, while pg-simple uses the text protocol and therefore has the overhead of parsing queries.
The only place where you can *guarantee* data integrity is in the database. Therefore, not doing it there is simply wrong, since you're effectively saying that it's okay for your database to represent invalid states. For most cases you won't need stored procedures anyway; simply using unique/foreign key/not null constraints will do the job.
Do you think it is a bad idea to just start with [calculus-of-constructions](https://github.com/maiavictor/calculus-of-constructions)? Same thing except I fully understand the implementation and running JS is easier...
Other recent post on the subject if anyone is interested https://www.reddit.com/r/haskell/comments/6b3dlt/techempower_benchmarks_14_released_with_servant/
&gt; There isn't even an a-priori reason to suspect that `map (fn1 . fn2)` would execute more efficiently. We know that it's better because we know that map traverses the entire list, and that traversing it once is better than traversing it twice because we won't have to construct an intermediate list Isn't that taken care of by laziness?
&gt;On the other hand, Haskell is in a particularly good position to statically guarantee that application code always handles data correctly, and there's an obvious dev time gain vs mucking about with stored procedures etc. But this guarantees nothing about what *other* applications do to your data. Or am I missing something?
Hm, but the action doesn't have to succeed eventually. We can run `action` with inconsistent data and aborted half way through. orElse (unsafeIOToSTM action) (return ()) Also idempotance doesn't cover the absence of error with incorrect input, I don't think. 
&gt; I don’t know whether the answer is a smarter compiler, better tools for analyzing performance, better diagnostics about how a program is simplified, or some combination of all of the above, but I’d really like to be able to keep these sorts of pragmas out of my application code and still be confident I’ll get reasonable performance. My preferred solution would be tooling. Any change to the compiler's behaviour needs to be *very* reliable, since it'll be applied to all code. For anything tentative, I'd rather have the compiler default to the most obvious/naive version, and allow pragmas when we know it'll help. That gives us two concrete problems: - Finding out which pragmas will help where. This seems like a linting problem to me: a tool is run infrequently over the codebase, and suggests possible improvements which may or may not be ignored. This could either look for known patterns, or make suggestions based on some automated profiling, or both. - Verifying that included pragmas are still required. This seems like a testing problem: provide a benchmarking function and measure the pairwise difference between having a pragma enabled/disabled on a variety of inputs, e.g. generated by QuickCheck. Fail/warn if some improvement threshold isn't reached, e.g. `INLINE on foo should give 15% time reduction, measured 3%`.
If you've got `Semigroup` imported, you need to hide `(Data.Monoid.&lt;&gt;)`, because it overlaps `(Data.Semigroup.&lt;&gt;)`.
I'm a fan of integrity constraints in the DB. Multiple applications, multiple version of a single application, etc. Of course, you may not want to pay for the overhead of ACID (or close) of lots of CHECK constraints, or rather you might not want to pay for it *there* and instead accommodate the weaker guarantees elsewhere in the system design. I'm also a fan of stronger guarantees encoded in the Haskell types. But, even there I practice defensive programming to an extreme that might be considered paranoid. If it's not reflected in the type, I check every property before I assume it. I think this is also one of the reasons I like `Either e` and `Maybe` monads; they actually free me from a lot of the `null` checks in my Java, or the `int` return value checks in my C.
Yes and no. In a strict language, you'd start with a big input list in memory, then you'd traverse it to construct a big intermediate list. At that point, the first list might become eligible for garbage collection, then you'd traverse the big intermediate list to construct a big result list, and then the intermediate list would become eligible for garbage collection as well. With lazy evaluation, if the entire output list is used, we perform the same steps, but in a different order. Suppose the output list is printed, one value at a time. As each value of the resulting list is forced, the corresponding value of the intermediate list is forced, which in turn forces the corresponding value in the input list. Then the same thing happen for the second element of all three lists, and then the third, and so forth. So in this scenario, laziness allowed us to minimize the maximum memory usage: if the input list can be generated on demand and the output elements aren't kept around, we only need to keep a single element in memory at a time. But if what we want to minimize is the amount of time spent on the computation, not the maximum memory usage, then laziness doesn't help in this case. We have to perform the same number of computations: in particular, for each element, we deconstruct the input cons, we apply `fn1`, we allocate a cons for the intermediate list, we deconstruct this intermediate cons, we apply `fn2`, and we construct the resulting cons. With the fused map, we don't have to perform this superfluous construction and destruction of the intermediate cons. Laziness can, of course, also help to reduce the number of steps, when a thunk doesn't end up being forced. But this doesn't seem relevant in the distinction between `map fn1 . map fn2` and `map (fn1 . fn2)`.
Just to confirm: connection pools are allowed and strongly encouraged.
Looking at the implementations for json serliaisation, the ruby code for it is either using a wrapper around a C library or a Java library, while the Haskell one is native Haskell (mostly). So I wouldn't expect the typical ruby is slow thing to be true. Also don't under estimate the penalty for Text over ByteString.
If I were you, I'd benchmark hasql against some control Postgres driver (jdbc postgres, Go postgres) and see how hasql stacks up. If hasql is much slower, see how connection pooling is done. Make sure you mimic the jdbc and Go methodology for connection pooling. Then maybe respond here so we can help parse the benchmark and profiling results?
Oh man, I've spent quite some time trying to go through the TH implementation and fully understand it. Same for GHCI and how TH uses linking through GHCI. I'll be eagerly awaiting any posts you may be able to write.
It's our responsibility to use the tools we have available to keep our data safe and consistent. It'd be foolish to abstain from the wonderful power and expressiveness of a database just because we have a passable type system.
That's literally something that's being slowly rolled out. Look how long AMP took.
why do servant and yesod use lists instead of vectors?
Several examples looking like that: hostname &lt;- init &lt;$&gt; readProcess "hostname" [] [] In every case it makes sense and adds no performance penalty. 
In what way was your function not tail recursive? Or would that require like f xs [] = xs f xs (y:ys) = f (foldr (:) [y] xs) ys
My function was tail recursive modulo cons, but not tail recursive. I'm not sure what exactly the OP was asked to do, but my guess is the usual tail recursive concat involving reverse. Not that it's better in any way.
I mean everything that works before works now, with at worst very minor changes. Such as `() &lt;$`. But now we can actually get information out of guarded Applicatives when the predicate does happen to be true. 
Then maybe also have `when_` with type: when_ :: Bool -&gt; f a -&gt; f () when_ p s = if p then () &lt;$ s else pure () To mirror `traverse_`.
Thanks for the write-up! Waiting eagerly for next week's section on Template Haskell. 
Ok, the benchmark does not work, no idea why, but I think you be right
I would echo what most have said here that you should always have data integrity constraints in the DB but with one amendment to what everyone else is saying: Everyone else says something to the effect of "unless you can't" for various reasons. But I would say; if you can't use constraints on your data *are you sure you should be using a relational database*? You should revisit your architecture and decide why you think you need an RDBS. Red flag answers would be things like: "because that's what everyone seems to do", "because SQL is nice", "I want a server to store the data", etc. Red flag in the sense that either they signify bad practice (e.g. the first example) or there are superior options that achieve the same thing (e.g. the next two examples). 
Btw, something I've learned only recently: `void == () &lt;$`. The name `void` is questionable IMO, and I guess due to the usage of "void" in imperative languages.
Glad you like it!
Think of it as writing a REST API: * `Inc` - increases counter, returns current value. You can call that as often as you like (for a kv-store that would be `Get` or `Put`) * `Done` - finishes transaction; response is either `Commited`, which means that the transaction was successful, or `Retry`, which means that the transaction must be retried. Like Postgres' `PREPARE TRANSACTION` basically.
The Writer monad
There are lots of other words that would be off limits by that logic: `return`, `map`, `class`, `instance`, `null`, `singleton`, `Functor`, constructor, function, `sequence`, ... Just accept that different languages and cultures sometimes use the same words for different concepts, and move on.
Yes, if you do `-keep-tmp-files` you'll see the assembly file produced. Stripping is an optional step, Cabal always does it, but GHC itself doesn't by default. Nowadays we also produce an assembly file that mimics behaviour of `-ffunction-sections` and `-fdata-sections` on linux variants and macos. Windows will be enabled soon (8.4). This allows us to link in only functions and data definitions that are actually used vs a slight link overhead. But that's why the assembly file has so many sections.
The second paragraph was only about your condition 1) seeming stronger than idempotence with the varying parameters part. Secondly I believe we need to include control flow dependency, for example I believe the action has to be nop here. when (a==b) (unsafeIOToSTM action) :: STM ()
Ok. Since `Inc` wants a result outside of STM that doesn't map well to one transaction. But we can split it up into two types of transactions, one for each request. `Inc` seems straightforward. `Done` would need to save the old reads and perform an STM action to check ok and write. One way to do that would be to check the values of the reads, but that has the [ABA problem](https://en.wikipedia.org/wiki/ABA_problem). So instead you can add some infrastructure and include time stamps with your store items. The commit action can be represented directly, with type `STM Response`, and would change with each `Inc`. Edit: No, this doesn't take into account that the writes of the current transaction should be visible to itself. So more transaction logic is needed. But I think the two types of transactions approach is still valid, just need a more explicit transaction log for `Inc`.
Haskell uses "void" as the unique map to the one-element (terminal) type. On the other, "Void" seems to be the canonical name for the uninhabited (initial) type. So it is *one* culture that uses the same word for two almost completely opposite concepts. If we have `absurd :: Void -&gt; a`, then I guess a better name for `void` would've been `trivial :: a -&gt; ()`.
In theory it should be also possible to use it as universal data structure data Foo a b = Foo a b data Bar a b c = a b c toHList2 : Foo a b -&gt; (a, b) toHList3 : Bar a b c -&gt; (a, b, c) ... -- this won't work for sums toHList : any {a[N]} -&gt; ({a[N], }) -- this will toHList : any {a[N]} -&gt; ({ ({a[N], }), }) 
 apply2 : (a1 -&gt; a2 -&gt; x) -&gt; (a1, a2) -&gt; x apply3 : (a1 -&gt; a2 -&gt; a3 -&gt; x) -&gt; (a1, a2, a3) -&gt; x ... You can already [do something like this](https://github.com/agda/agda/blob/master/src/full/Agda/Utils/TypeLevel.hs#L104) with type-level programming. And using `t ~ Arrows (Domains t) (CoDomain t)` (both type families defined in that same file), you can have `t -&gt; Products (Domains t) -&gt; CoDomain t` without any proxies (if you're willing to enforce that `x` should be a non-arrow type). data Foo a b = Foo a b data Bar a b c = a b c toHList2 : Foo a b -&gt; (a, b) toHList3 : Bar a b c -&gt; (a, b, c) The codomain of these `toHList` functions look like an (open) type family to me (it's probably better to express it in terms of lists of types so that you can reuse the `Products` idiom).
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [agda/agda/.../**TypeLevel.hs#L104** (master → 0397ac6)](https://github.com/agda/agda/blob/0397ac6e2c31a073910a6bcdf4cd3795dd990b20/src/full/Agda/Utils/TypeLevel.hs#L104) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dhpsoxm.)^.
&gt; They are, quite literally, everywhere—whether you take advantage of their useful properties or not. Obviously, you should. 
What would you use it for? That's the question I come back to every time I think about heterogeneous lists.
You don't need volatile load. You just keep going and on first load that try to access it, it'll throw exception - do GC then continue.
First class records are quite nice.
Good point actually.
No, and it also ignores `akk2` which is a bad sign. I wrote to you in the other subthread, check it out.
I don't know how that would be used. There isn't much you can do with "any type". What I'd really like to do¹ is to declare a list like: forall a. SomeClass a =&gt; [a] And have the list hold different types of that class. 1 - It's probably there somewhere. Most likely I just don't know how to write it.
I think that would convolute your code. All this really achieves is converting a `Bool` into a `Maybe`. It just happens to have effects in the process. Since you supplied the `Bool` in the first place, you can just use `if`. However, as I said elsewhere, I think having a function in `base` to easily convert `Bool` to `Maybe` would help, along with a version of `when` that just operates on `Maybe` directly (`sequenceA` would work for this, as /u/phadej pointed out). 
* As a replacement for array (if you know its size). Because you cannot access element out of bounds. * As a replacement for dictionary (if you know its fields). Because you cannot access not existing field. * As a boilerplate eliminator. Because if you can convert any data into HList you can write one function for HList and automatically derive it for all data (with relevant shape).
Yeah, basically I'd have to implement concurrent transactions from scratch. Since STM already uses [TL2](http://www.disco.ethz.ch/lectures/fs11/seminar/paper/johannes-2-1.pdf) afaik, I was hoping I could leverage that instead of having to do everything myself.
Can you post your implementations of length, sum and reverse?
Aha, right you are.
What happens for transactions that are "doomed", they have seen inconsistent data but haven't received the `Done` signal? Should `Inc` (or `Get`) give invalid values? GHC's STM implementation is lazy about validation so it is probably not a good fit for what you want. If users of your API are expected to be robust to inconsistent values from `Get` (one `Get` gets a value from version A of the world and a second gets a value from version B) then it would be fairly simple to build without any `unsafeIOToSTM`, but you would have to do your own logging. I think data base systems often offer some relaxed semantics to improve performance such as not allowing transactions to "read their own writes". For example if you do `put x 0 &gt;&gt; get x` you will get the value of `x` before the `put`. This means that you don't have to search the log for writes when you do a `get`.
that led me to his religious declaration: http://www.willamette.edu/~fruehr/haskell/ChurchLFP.jpg so worth it.
Yes, you just have to make sure that an access happens within the heap/stack + guard page since after the guard page the memory could be in use again. My suggestion was to use some kind of stack banging at the next 4k boundary then 8k if needed etc, but it might not be necessary as you suggest. In my implementation I used such a technique and I think some JVMs do something similar. But it all feels quite like a hack to me. I would still be very interested in the performance implications since it is hard to tell what the costs of the manual checks are. I will do some benchmarks if I have time.
How would you verify the reads quickly? `Eq` is not very practical imo. `StablePtr`? Or version values, at which point it's almost TL2 anyway?
Those checks are too often. Haskell is a language where it's much often to allocate new data on the stack or heap. In imperative languages you just reuse existing memory cells with overwritting them - so the allocation there is much more rare and on larger chunks. So if we want to make Haskell faster we'll have to make some OS specifics from time to time. It's not a hackery! 
3 - This could be a problem, that the default size will go from 1k to 4k if there a lot of threads eventually.
It means that the instances can be passed around as values and that you can have multiple different named typeclass instances. Think of the instances as dictionaries/record values like they are implemented internally in GHC. In Haskell you can emulate those with Edward Kmett's reflection package.
Great example. Yah I agree with you. I just think making `when`/`unless` do both effects and conversion to Maybe is not as generally helpful as having two things that do each part separately. If you want both you can just compose them in a helper function in your app.
I think originally it came from [this post by Brent Yorgey](https://byorgey.wordpress.com/2009/01/12/abstraction-intuition-and-the-monad-tutorial-fallacy/) and people just ran with it.
There's more: lfhelpers = LFHelpers{..} where pk n = PackageKey n "0.1" pkg key deps = ([key], Package "0.1" (RemoteFile "" "") deps []))
[Applicative-Monad Proposal](https://wiki.haskell.org/Functor-Applicative-Monad_Proposal), the changes implemented in GHC 7.10 that made `Applicative` a superclass of `Monad`.
Yes, I used the existential quantification syntax, but this is not what I meant. I was talking about something like this working: {-# LANGUAGE ExistentialQuantification, MultiParamTypeClasses #-} module Main where class Test a data Test1 = Test1 deriving(Show) data Test2 = Test2 deriving(Show) instance Test Test1 instance Test Test2 main :: IO () main = let list = [Test1, Test2] :: forall a. Test a =&gt; [a] in print list The above has the following error, because the types are different: Main.hs:15:11: error: • Couldn't match expected type ‘a1’ with actual type ‘Test1’ ‘a1’ is a rigid type variable bound by an expression type signature: forall a1. Test a1 =&gt; [a1] at Main.hs:15:35 • In the expression: Test1 In the expression: [Test1, Test2] :: forall a. Test a =&gt; [a] In an equation for ‘list’: list = [Test1, Test2] :: forall a. Test a =&gt; [a] I was looking for something that didn't generate that error, and still let me map through it.
[Applicative-Monad Proposal](https://wiki.haskell.org/Functor-Applicative-Monad_Proposal). Basically, the process of making `Applicative` a superclass of `Monad`.
X-Post referenced from [/r/fsharp](http://np.reddit.com/r/fsharp) by /u/abhiranjan [ClearTax is hiring #fsharp engineers in Bangalore](http://np.reddit.com/r/fsharp/comments/6by0t6/cleartax_is_hiring_fsharp_engineers_in_bangalore/) ***** ^^I ^^am ^^a ^^bot. ^^I ^^delete ^^my ^^negative ^^comments. ^^[Contact](https://www.reddit.com/message/compose/?to=OriginalPostSearcher) ^^| ^^[Code](https://github.com/papernotes/Reddit-OriginalPostSearcher) ^^| ^^[FAQ](https://github.com/papernotes/Reddit-OriginalPostSearcher#faq)
Love the new website design ! 
length: ownLength :: [a] -&gt; Int ownLength = ownLength' 0 where ownLength' a [] = a ownLength' a (_:xs) = ownLength' (a+1) xs sum: summe :: [Integer] -&gt; Integer summe = summe' 0 where summe' akk [] = akk summe' akk (x:xs) = summe' (x+akk) xs reverse: invER :: [a] -&gt; [a] invER akk = invER' akk [] where invER' [] akk = akk invER' (x:xs) akk = invER' xs (x:akk)
Haskell doesn't have map syntax. It has data structures and functions. Lens syntax with maps can be pretty nice: `someMap ^? ix someKey :: Maybe SomeValue`
The Reader comonad ;P
Thanks for the feedback! :)
And it must be taken care to isolate the effect until the transaction finish. For example, if the IO action retry many times and write to a file, other thread can access that file in the middle while it is in an inconsistent state. I mean that there is no general mechanism to combine STM and IO for all the cases but there are ways to make it for some controlled cases.
No, the last line is not tail recursive, it has to be `conc' ... = conc' ...` To make that work you need to reverse one of the lists first. Maybe try with my hint.
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [goldfirere/glambda/.../**Exp.hs** (master → 2149a62)](https://github.com/goldfirere/glambda/blob/2149a6229cb7151520bd32b123021e159e73c9e9/src/Language/Glambda/Exp.hs) * [goldfirere/glambda/.../**Unchecked.hs** (master → 2149a62)](https://github.com/goldfirere/glambda/blob/2149a6229cb7151520bd32b123021e159e73c9e9/src/Language/Glambda/Unchecked.hs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dhqhrsq.)^.
You can actually do that: {-# LANGUAGE ExistentialQuantification, Rank2Types #-} module Main where data Test1 = Test1 deriving Show data Test2 = Test2 deriving Show data Crap = forall a. Show a =&gt; Crap a list :: [Crap] list = [Crap Test1, Crap Test2, Crap 23.3, Crap "foobar"] printCrap :: Crap -&gt; IO () printCrap (Crap a) = print a main :: IO () main = () &lt;$ traverse printCrap list Everyone who writes code like this deserves to be slapped, in my opinion, but you can technically do it. Better yet if you understand that this a complete misuse of type classes, that type classes are no interfaces, and that `list :: [String]` holds the exact same kind of information as the list in the above example.
Thanks for the references, I will definitely check them out, they look very interesting. The only thing is that I would rather overcome difficulties with what I am trying to do now than switch to a completely different solution. That would probably be less educative.
Hey, we're glad to discuss these sort of topics directly with the candidates, but currently not in public.
Send us an email some time guys charles.hoskinson@iohk.io
They wrap?
By that logic we shouldn't have `replicateM`, as you can just compose `replicate` and `sequenceA`: replicateM = (sequenceA .) . replicate and we also shouldn't have `replicateM_` replicateM_ = (sequenceA_ .) . replicate I really don't see why there are so many objections, my `when` is strictly more powerful than the existing one, and `when_` is pretty much the same, just with a more general type signature. And there is plenty of precedent for such a thing with `replicateM` and `replicateM_`.
I think that is a very valid point, although `trivial` isn't sufficient for our purposes, we really want `trivial &lt;$&gt;`, which is less concise than `() &lt;$`. So we probably need a separate function for dealing with `Functor`s: absurd :: Void -&gt; a trivial :: a -&gt; () vacuous :: Functor f =&gt; f Void -&gt; f a forget :: Functor f =&gt; f a -&gt; f () Or something like that?
Well, that certainly works. But now, instead of `mapM print [Test1, Test2]`, the code is full of crap. I fail to see how it's any better.
&gt; are you sure you should be using a relational database? Agreed. If you opt not to use contraints, a fast key-value store with a consistency model that matches your application could work quite well.
I don't think we have overloaded characters either. It would potentially be useful to do so to offer proper support for encodings like Big5 and Shift JIS.
Is there a thing where we can put our email in a box and press a button to get digital mail updates and previews?
EDIT: I'll just post my notes file here. tl;dr I'm having a really hard time reproducing the horrible performance Haskell had in these benchmarks. EDIT: I opened an issue here on tewfbm: https://github.com/TechEmpower/FrameworkBenchmarks/issues/2800 Findings: * in single postgres query we lose reqs/sec when upping concurrency whereas sinatra does not And now notes: * single postgres query ** haskell *** 1 **** 64 cody@zentop:~$ wrk -t4 -c64 -d10s http://localhost:7041/queries Running 10s test @ http://localhost:7041/queries 4 threads and 64 connections Thread Stats Avg Stdev Max +/- Stdev Latency 2.69ms 718.44us 42.62ms 98.15% Req/Sec 6.02k 247.94 8.18k 95.04% 241216 requests in 10.10s, 41.82MB read Requests/sec: 23883.28 Transfer/sec: 4.14MB **** 256 cody@zentop:~$ wrk -t4 -c256 -d10s http://localhost:7041/queries Running 10s test @ http://localhost:7041/queries 4 threads and 256 connections Thread Stats Avg Stdev Max +/- Stdev Latency 11.88ms 15.73ms 426.80ms 99.10% Req/Sec 5.97k 485.05 7.99k 92.46% 236534 requests in 10.02s, 41.01MB read Requests/sec: 23610.48 Transfer/sec: 4.09MB ** sinatra *** 1 **** 64 cody@zentop:~$ wrk -t4 -c64 -d10s http://localhost:8080/db Running 10s test @ http://localhost:8080/db 4 threads and 64 connections Thread Stats Avg Stdev Max +/- Stdev Latency 15.12ms 2.47ms 36.92ms 86.09% Req/Sec 1.06k 164.77 3.52k 88.53% 42417 requests in 10.10s, 9.17MB read Requests/sec: 4199.96 Transfer/sec: 0.91MB **** 256 cody@zentop:~$ wrk -t4 -c256 -d10s http://localhost:8080/db Running 10s test @ http://localhost:8080/db 4 threads and 256 connections Thread Stats Avg Stdev Max +/- Stdev Latency 52.58ms 1.99ms 65.99ms 90.78% Req/Sec 1.22k 64.08 1.29k 74.50% 48598 requests in 10.02s, 10.51MB read Requests/sec: 4851.77 Transfer/sec: 1.05MB * Database update results ** haskell *** 1 **** 256 cody@zentop:~$ wrk -t4 -c256 -d10s http://localhost:7041/updates Running 10s test @ http://localhost:7041/updates 4 threads and 256 connections Thread Stats Avg Stdev Max +/- Stdev Latency 23.85ms 9.15ms 233.72ms 88.59% Req/Sec 2.71k 382.10 3.85k 82.71% 107548 requests in 10.02s, 18.64MB read Requests/sec: 10729.77 Transfer/sec: 1.86MB ** sinatra *** 1 **** 256 cody@zentop:~$ wrk -t4 -c256 -d10s http://localhost:8080/updates Running 10s test @ http://localhost:8080/updates 4 threads and 256 connections Thread Stats Avg Stdev Max +/- Stdev Latency 185.62ms 43.17ms 377.42ms 82.13% Req/Sec 355.46 187.42 646.00 60.53% 13685 requests in 10.03s, 2.99MB read Requests/sec: 1363.94 Transfer/sec: 304.72KB 
I think they're just joking around with this teaser.
That wasn't exactly my point but I can see how it came across that way. I most certainly agree that having many "simple" combinations are worth having around.
Sorry I forgot how to explain monads after I learned about them.
What happens if we completely ignore utf-16 and go for only utf-8? We could have a serialisation library that works directly with Builders and generate bytestring. Not a Text -&gt; Utf-8 etc.
What does the stdDev field mean. That the figures stay withing the given precent most of the time? If that is the case it is okey but if it means that stdDev is thatmuch of the mean that means these figures vary gigantically and hence are not really reliable.
Just autogenerate the lens chapter with Template Haskell and we can target arbitrarily depths
Professional Haskeller here -- when we hire, we're looking for a few things: * can you solve problems without reaching for IO? * do you understand monads and monad transformers? * how familiar are you with common libraries? * can you play type-tetris? * how well can you model a problem in terms of types? You'll notice we're not looking for "real world" Haskell experience here; we know it's hard to find professional Haskell developers. We figure if you can do well on the above bullet points, we can teach you the rest. PS: We're (http://takt.com) hiring! If you (or anyone else reading this) feel like you meet the bar, please message me and get in touch!
(troll) If you can't see that monads are just like burritos, you clearly don't understand monads yet and need to read more monad tutorials.
Things I test for when I interview Haskell programmers: * Can you decompose larger problems into smaller problems? * Can you equationally reason about code? My rationale is that these two qualities promote code that is easy for other people to maintain, debug, and reason about These are not technically specific to the Haskell language, but if you master those two areas then you can pick up any functional language fairly quickly
Where I work we have a couple more: * Can you write good property tests (ie QuickCheck). * Can you write code that avoids partial functions (ie has incomplete pattern matches, calls "error" etc)? 
I am not saying the figures are unreliable (as yet). It is important to know what the stdev field means in any case. I am confused because there is a +/- infront of stdev (which is usually a positive quantity).
Do you have any table of contents so we know what will be in the book?
&gt; Can you write code that avoids partial functions (ie has incomplete pattern matches, calls "error" etc)? People actually do that? I mean I know I did when I was learning, but yikes. 
So you were able to explain monads before you learned about them?
That's because it's fundamentally bad code. It's like writing if (x == true &amp;&amp; !(x == false)) { ... } in C++, overly complicated and unnecessary. The right way to write the example above is this: list :: [String] list = [show Test1, show Test2, show 23.3, show "foobar", lazyFileInput, cycle "foobar"] This is a list of objects/thunks/closures whose common property is that they'll evaluate to a list of characters. The `String` is the common interface, the underlying object my be anything, a few characters, a lazy IO input stream (from `readFile`), or an infinite stream via `cycle`.
I couldn't figure this out: is Intermediate Haskell going to be a teaching book with sequential structure (i.e. first chapter picks up after a beginners book, and then each chapter builds on the precious one) or is it going to be more like a collection of independent tutorials? 
I hope your book covers a discussion of Haskells evaluation model and strictness annotations from a semantic point of view, rather than just as a performance optimization. Intermediate Haskellers should know when a function or data type field is supposed to be strict and how to use `case of` and `seq` to achieve that.
It bothers me that such a simple and beneficial change causes such problems. I wish there was some sort of automatic way to manipulate old code bases so that they still compiled ok after various changes are made. Such as automatically replacing `when` with `when_`. Because yeah I don't have a magic way of avoiding old codebases from having warnings. And IMO `whenMaybe` is a sub-par solution, since it is pretty much objectively more powerful than `when`. Although don't you have to specifically ask GHC to warn you about this? I'm pretty sure it's not a warning by default. 
How are you planning​ on preventing the "dead trees are immediately obsolete when it comes to describing how to use libraries and best practices" thing that seems to plague programming language books? I imagine it'll be even worse in a language like Haskell with it's rapidly changing third party library ecosystem.
Nice job! Too bad there isn't a way to change the status code.
Really great!
Because they want to be paid for their work.
 {-# LANGUAGE EmptyDataDecls #-} {-# LANGUAGE FlexibleContexts #-} {-# LANGUAGE FlexibleInstances #-} {-# LANGUAGE GADTs #-} {-# LANGUAGE GeneralizedNewtypeDeriving #-} {-# LANGUAGE MultiParamTypeClasses #-} {-# LANGUAGE QuasiQuotes #-} {-# LANGUAGE TemplateHaskell #-} {-# LANGUAGE TypeFamilies #-} FFS
[this image might help](https://byorgey.files.wordpress.com/2011/05/monad_tutorial.jpg) (in seriousness: yes, it is an inside joke. One drawing attention to how everyone explains monads with odd and usually unhelpful metaphors)
Do you happen to know a project where a beginner like me can see the points you mentioned in action? Idealy, it would be a) code with the current best practices and b) solving a problem I can understand (i.e. something web-related, something with databases, some program that models an company environment) as opposed to, say, the inner workings of lenses. Such a pointer would be greatly appreciated! :)
Oh great! Another book for me to wait ages for!
&gt; It bothers me that such a simple and beneficial change causes such problems. I wish there was some sort of automatic way to manipulate old code bases so that they still compiled ok after various changes are made. Such as automatically replacing when with when_. Because yeah I don't have a magic way of avoiding old codebases from having warnings. I think I understand your pain ;-) I always have the same reaction when the rename `fmap` to `map` and `map` to `mapList` or `map @[]` discussion comes up. A compiler-based (GHC-API) refactoring tool, should be able to safely handle such cases. It might get a bit more involved though, if the libraries still want to support older ghc versions. Here one would probably need at least some `#ifdef`s, which may add some complication for maintenance. #if __GLASGOW_HASKELL__ &gt;= NEW_VERSION import Control.Monad #else import Control.Monad hiding (when) #endif // Here might be more imports. #if __GLASGOW_HASKELL__ &lt; NEW_VERSION when :: Applicative f =&gt; Bool -&gt; f a -&gt; f (Maybe a) when p s = if p then Just &lt;$&gt; s else pure Nothing when_ :: Applicative f =&gt; Bool -&gt; f () -&gt; f () when_ p s = if p then s else pure () #endif &gt; And IMO `whenMaybe` is a sub-par solution, since it is pretty much objectively more powerful than when. What bothers me with `whenMaybe` is mostly the broken naming convention with `_`-suffix. IMO, having a systematic and consistent nomenclature can make remembering (or rather reconstructing) names much easier for newcomers and more experienced Haskellers alike. It's almost embarrassing for how long I was avoiding to learn about `traverse`. If it would have been called `mapA` instead, I would have immediately thought of `mapM` and `liftA`. &gt; Although don't you have to specifically ask GHC to warn you about this? I'm pretty sure it's not a warning by default. The warning isn't enabled by default, but comes from `-Wunused-do-bind`, which is included in the `-Wall` collection.
Great link.
Wow, I'm glad you guys exist :)
This is a difficult question that eventually led us to the decision to cover more theoretical topics than we originally expected. The plan is to introduce not only libraries themselves, but the ideas behind them and coding patterns used in them, and it works on multiple levels: * understanding the design principles can help people reinvent the same libraries on their own, and reinvention is a very powerful learning technique (speaking from personal experience). * even if library code will diverge, the ideas will persist. As an additional measure, if the book becomes at least moderately successful, we intend to keep the digital version up to date.
Yes, there will be three chapters that tackle this topic from different angles. First, there'll be a chapter on how things are represented in memory, so strictness can be understood as forcing a thunk prematurely. Second, there'll be a chapter on using strictness in practice to achieve the desired performance characteristics. And third, there'll be a chapter that relates exceptions and non-termination to their denotation (bottom), so strictness can be understood as a different treatment of the bottom value.
Yes, chapters do have dependencies between them and the structure is mostly sequential. For example, we cover `transformers` and functional dependencies before `mtl`. But we also try to minimize those chapter dependencies, and we generally prefer to repeat some explanation briefly than to refer the reader to a completely different part of the book. When teaching, repetition doesn't hurt. Cross-references between chapters do.
Yes, we have a Slack channel for early access readers and coauthors. Drop /u/peargreen a letter for an invite. We'd appreciate any comments, questions, reviews, or other contributions.
&gt; teaching is a complex skill completely independent from domain knowledge Sure. I wasn't implying (for instance) that Intermediate Haskell will make an intermediate Haskeller out of _everyone_ – that would require a pedagogical genius indeed. However, having information in one place is going to be a huge improvement over what we currently have, and it will make learning easier for curious and motivated self-learners. (And given that there aren't that many incentives to learn Haskell otherwise, everyone who wants to become an intermediate Haskeller probably fits that description to some extent.)
ah ok - btw the snippet looks like it's permutations of 123456789 - excluding the 0)
There's a newsletter now, so you can subscribe by email too: http://intermediatehaskell.com/#want
Correct. The permutations range [1..9], so excluding 0.
and the only _magic number_ is *381654729* then? Your code is a bit hard to read and I too am a bit unsure how to help out - but the problem is solvable using *brute force* and maybe you can have a look at https://www.stackage.org/haddock/lts-7.2/base-4.9.0.0/Data-List.html#v:permutations and see if you can write an algorithm using this (IMO getting the permutations is the hardest part) If you want I can post an solution using this but I don't want to spoil it for you
If everything goes well, only one age :)
That was a typo, sorry. Fixed now.
Yup that looks like the number. Anyhow I'm not supposed to brute force, but to use backtracking. forgot to put that in OP
can you describe in words what you wanted to do?
I understood the 'section' part differently (see my partial answer below), but your understanding match closely with the provided code ;) As a funny note, we get the same result using both our understanding of the 'section'.
you can have a deadlock easily using sanctified base STM primitives: atomically retry is a deadlock.
The old adage says data outlives the code. Not all applications accessing the same data will be written in haskell. 
It's an external tool https://github.com/fpco/ghc-prof-flamegraph
The main difference is you don't have to give up Data.Map with reflection due to reify generating a fresh type, where the approach taken in this paper still kills it dead.
Good post. Unfortunately when I tried to print the page, the menu appears on every page so I can't read some text. Is it possible to access a printable version?
Thanks!
Remember that cc-nc is an option: While freely available, it'd only be the authors who can make money off it, e.g. from prints -- many people want prints, and if you have a half-way decent run it's not hard to have a generous profit margin without making the book actually expensive. Just make sure to not let yourselves be ripped off to pay for "editing" that doesn't deserve the name, shareholder profit and whatnot. As long as you offer your books for less than what twice what printing it on a laser printer, a needle, some thread, glue and cardboard would cost you'll always sell them.
RemindMe! 9 months
It took me a long time to understand the evaluation model of GHC. I wrote down/spoke about the important details here: http://h2.jaguarpaw.co.uk/posts/haskell-programs-how-do-they-run/ You may find it a helpful reference.
Is this what you are looking for? {-# LANGUAGE TypeOperators #-} module Main where infixr ::: data (:::) a b = a ::: b data Nil = Nil class ToStrings a where toStrings :: a -&gt; [String] instance ToStrings Nil where toStrings _ = [] instance (Show a, ToStrings b) =&gt; ToStrings (a ::: b) where toStrings (a ::: b) = show a : toStrings b list = "foobar" ::: 2 ::: 32.3 ::: '?' ::: ["Hello", "World"] ::: Nil main :: IO () main = putStrLn . unwords . toStrings $ list -- Output: "foobar" 2 32.3 '?' ["Hello","World"] You might also want to look at the `Text.Printf.printf` function: ghci&gt; printf "Args: %s %c %d %.3f\n" "foobar" 'a' 3 pi Args: foobar a 3 3.142
I had the exact same priblem and the folks over at IRC helped me out. They forced me to write the reader monad type-classes by hand. And then write a function that is in the reader monad, call it via runReader, and expand every expression by hand on paper another intuition is that it is basically the type signature of a lambda function with exactly one unapplied argument -- your reader environment. 
Great. Gonna do just that. 
Reminds me of [bound](https://www.schoolofhaskell.com/user/edwardk/bound).
Intermediate Haskell is meant to be exactly that — the book to read after LYAH. If there was a good resource I could recommend, there'd be no urge to write Intermediate Haskell. For now I think the best option is to start a small project, see how far you can get with the knowledge you already have, identify the missing pieces and look for resources that cover those individual topics. A good high level overview of things to learn is ["What I Wish I Knew When I Was Learning Haskell"](http://dev.stephendiehl.com/hask/).
I'm tremendously excited by the number of Haskell resources coming out, especially at an intermediate level. Haskell is my first programming language and I genuinely believe learning it well would have been impossible without the Haskell Book. Now, there are _three_ intermediates on the way? What riches!
To explain 'section', I mean this: You loop k for the total length of n, so in my case 9 times. You then take k digits of n in each loop, and then that number you took needs to be divisible by k. Example: 123456789 Say you're at k = 4, your section is: 1234 and should be divisible by 4, so 1234 % 4 == 0 Looks like your answer does just what im looking for
IMO at least mentioning profunctor lenses, perhaps just a few paragraphs about them, would be worthwhile.
Great! I wish you guys all the best. I'll probably​ drop you a line after I finish going through Haskell from first principles; I'm always on the lookout for quality learning materials and have a huge interest in helping improve them.
Reflection keeps global uniqueness intact by generating types, right? If you speak of coherence do you also mean global uniqueness or only that something like the diamond property holds?
This looks good! I have few minor comments though: 1) the `c` function should care about Unicode characters and split up the returned Integer into a list of `Word8` 2) When I tried running this with `take` on `/dev/random`, I got `*** Exception: /dev/random: hGetContents: invalid argument (invalid byte sequence)`. Is there a way how to read every possible byte sequence? 3) This works on a `FilePath`, but does it work on a `Handle`? The data in question might come from any file source - socket, pipe, ...
At first I got really excited because I thought this article was going to be about reducing a servant API type-check error into something readable. My friends and I call those errors *typenado*s and they are incredibly hard to parse. This is still cool, of course!
The operational semantics of something like Core or STG. Probably closer to STG actually. I don't recall ever seeing those papers. If I had it would have saved me a lot of time! I tried to learn about STG instead which, it turns out, was probably too complicated to be worth it.
What I've read so far is fantastic, but there seems to be one minor omission: the `Identity`, `Const`, `Sum`, and `Product` functors are not, by themselves, quite sufficient to encode all algebraic datatypes. You also need V1 and `U1`, or alternatively `Void` and `()`.
My solution is a hack to avoid proper handling, but if you really want to treat your input as a list of bytes converted to Word8, use the `bytestring` module, this is its purpose. The `readFile` function suppose an input stream in a specific character encoding (your system default) and convert it to `Char` which is a representation of unicode. So, as far as I know, you cannot read a file using `readfile` which is not valid in your system default encoding (which is utf8 most of the time), and I bet that `/dev/random` can generate invalid sequence of bytes. However, it will work on a `Handle` or a `FilePath` depending on your usage of, respectively, `hGetContent` or `readFile`. Actually, you can use the functions of `System.IO` an open the file in binary mode. Prelude System.IO Data.Char&gt; f &lt;- openBinaryFile "/dev/random" ReadMode Prelude System.IO Data.Char&gt; map (fromIntegral . ord) &lt;$&gt; take 10 &lt;$&gt; hGetContents f [159,203,117,94,192,243,166,24,205,135] `openBinaryFile` set the decoding to `char8`, which, well, does not decode. Be careful with lazy IO ;) 
Of course, but it still seems arbitrary that I can get the better performance version `runSTArray` only for a very specific use-case..
Surely `Functor` is too general. Seems like it would have to be `Traversable`.
Would be nice for it to be mobile friendly too!
I don't know, but you can probably find some good tips and tricks here: https://www.willamette.edu/~fruehr/haskell/evolution.html
If you like the container analogy, you can see a function as a Map or a dictionary, except on can contain an infinity of key value pairs. When you map over a function just just map over the values (which it can return). Note, a function is THE reader monad.
Neat. Let me try to state what I think the "big idea" is, and /u/bitonico can correct me, expand etc. perhaps. My summary would be: There are operations on environments that are expensive. But they are only sometimes needed. So we reify the operations on environments in a well-typed way (using GADTs) and only force the operations when needed. In a sense we move from environments to _actions on environments_, and so in jargon this is a sort of firstifiation of a yoneda/cayley/continuation view of environments. Alternately, this is to environments, roughly, what ropes are to strings :-)
That woud be an accademic version. I guess Haskell is more fitted to that style
What libraries dealing with binders and scope exist? I only know bound and unbound. I haven't used either of them yet, but I would like to explore this problem space some day. Now that I think about it, searching for "binder" turns up [a few more relevant results](http://hackage.haskell.org/packages/search?terms=binder). Is there anything in your post that might be packageable as a library? Until recently, I thought variable bindings was more or less a solved problem, but, after discussing and reading about it, it seems on the contrary that we are still struggling with scope and substitutions when implementing new shiny calculi or proving theorems about them.
That's a good question. I don't think a good library to implement such calculi exists currently. I also just know about bound and unbound. What is in my blog post could definitely be factored out in a library, really what you're building is a toolkit to work with type-level naturals and various constructs involving them, but you could easily parametrize everything over your `Syntax` type. I was talking to Ed Kmett because maybe we can recover some of the Monad/Traversable stuff for those structures too, but my brief attempts to recover even a simple Functor instance for `Env` failed.
I don't know, some of the things in there, eg implementing your own Peano arithmetic, could be put to good effect in an enterprise setting. I mean how much can we really trust the numbers that come with the compiler?
&gt; An intermediate resource doesn't need the hand-holding of beginner material, but it can still benefit greatly from careful ordering and clear, not-entirely-trivial examples. While we're at it: have you got any examples of intermediate-level books that you really like?
That doesn't appear to be the case? https://aphyr.com/about
So a question this poses to me is can we write an "algebraic theory of environments" or in pure haskell terms "if we were to abstract over environments by giving them purely as a typeclass with laws, what would that be". (bearing in mind that because of the type-level stuff going on, we might not be able to give an actual typeclass in haskell today, though we certainly could in agda I guess :-))
It is really confusing when you are writing handlers for an API type and you forget, for instance, to define one and the compiler dumps out the entire definition of your API type (all routes). I wonder if there's something that can be done about that to make it more friendly?
While that's an interesting idea, adding additional keywords or syntax to the language I think would be best handled as an RFC or proposal to... I think one of the committees?
*I'm attempting to have an actionable and constructive discussion around the TechEmpower benchmarks a second time. I strongly feel that there is nothing wrong with the benchmark.* The thing is, the TechEmpower "benchmark" is the extremely flawed concept. It is not a benchmark, it is pure stupidity and lacks any meaningful or scientific approach. What is deeply worrisome is that people in the Haskell community don't see this, as it should be pointed out by others over and over again already. The only way forward is to write and publish a meaningful, scientific benchmark, which I will do in the future. The constructive part: So anyone who see that TechEmpower is deeply flawed and wants to work on a paper on how to do a benchmark, please write a comment below or a private message to me. 
My line of reasoning was that you have to "run" the `ST` action for each element of the container, therefore it has to be `Traversable`. However, now that you have asked me to explain my reasoning I begin to wonder whether, since there are no visible side effects, one could essentially `fmap` over all the elements and have the "run" be done lazily.
Oh, my bad.
I think you likely *could* do the lazy freeze, but it seems like a giant hack. Would it offer more real expressiveness?
The `fix error` comment is amazing. I'd never seen that before.
[10892](https://ghc.haskell.org/trac/ghc/ticket/10892) is a very annoying limitation, I encountered it when creating an [applicative-but-not-monad](http://hackage.haskell.org/package/plan-applicative) and it made composing ()-returning actions much more verbose.
While the TE benchmark might be flawed, it does seem to highlight some very real performance issues. I fully agree with the OP that we should figure out what's going on here.
Someone in the industry of making-haskell-difficult-to-learn did not like my answer. How a answer like that can receive negative votes instead of a great number of positive votes? I can not believe it. If all your self-confidence lies in the secrecy of the recipe of the Haskell burritos, then you have serious psychological and social problems folks.
 fizzbuzz :: [(Integer, String)] -&gt; [Integer] -&gt; [String] fizzbuzz spec = fmap $ fromMaybe &lt;$&gt; show &lt;*&gt; matcher where matcher = foldMap toMatcher spec toMatcher (i, s) x = s &lt;$ guard (x `rem` i == 0) main = let divs = (3,"Fizz") : (5,"Buzz") : (7,"Boom") : [] list = [1..200] in mapM_ print $ fb divs list In haskell we don't have to trade extensibility for nice code ;) 
* Don't use lazy IO, you will suffer. * As guibou said, don't use `Handle`, as that deals with encoding and buffering, which you especially don't want for reading `/dev/random`. Use `Fd`s instead (suggested below). * If you want to read ByteStrings, use [`fdRead` from the unix-bytestring package](https://hackage.haskell.org/package/unix-bytestring-0.3.7.3/docs/System-Posix-IO-ByteString.html) * If you don't want any ByteStrings involved, use [`fdReadBuf`](http://hackage.haskell.org/package/unix-2.7.2.1/docs/System-Posix-IO-ByteString.html#v:fdReadBuf) to read into a `Ptr Word8` buffer * In both of the above, don't forget to loop if you've read less bytes than desired
If the set of keys in your state is fixed, a record would likely be your best option: data S = S { fooCount :: Maybe Int, barCount :: Maybe Int, bazSet :: Maybe (Set ?) } The `Maybe`s are necessary if you want to work with partially defined states (i.e. where some components are absent). If at some point you can be sure that the state is fully defined, the following may suit you: data S foo bar baz = S { fooCount :: foo, barCount :: bar, bazSet :: baz } type SPartial = S (Maybe Int) (Maybe Int) (Maybe (Set ?)) type SComplete = S Int Int (Set ?) If the set of keys is variable, you need something like a `Map` (from `Data.Map`). You can then collect the possible types of your values in a sum type: data V = CountV Int | SetV (Set ?) type State = Map String V
I agree, these benchmarks are useful. There a things in the Haskell ecosystem that are known to be very slow, such as `String` based functions in `postgresql-simple`, or very slow compared to the "optimal" way of doing it, such as any use of `Handle` instead of `Fd`s, `binary` parsing, uses of lists where vectors would be appropriate, and so on. Even the most basic microbenchmarks highlight these inefficientcies effectively.
Is there a way to have a typeclass constraint on the "value" portion of a Map? That is, so that a type like V could be implemented as a typeclass instead and thus be more easily extensible?
&gt; hysterical reasons This is just great haha
One way to make it more manageable is to define APIs piecewise, by groups of a few endpoints that deal with the same kind of entity. I wish we had a more robust and scalable solution, but I'm not aware of any, yet. Not saying it's not possible though!
Not only that, Traversable didn't exist at all for many years after this interface was designed. :) edit: Actually, they came sooner together than I remembered. ST was around since 1994 or so, but runSTArray is a more recent addition -- it showed up in GHC 6.4 in 2005. Traversable appeared in GHC 6.6 in 2006. It's only been much more recently though that Traversable started seeing a great deal of use, after the advent of lens.
[propellor](https://propellor.branchable.com/) allows any type to be included in its Info, by using Data.Dynamic. The result is a multi-typed monoid. I wrote it up here: &lt;http://joeyh.name/blog/entry/birdplanesupermonoid/&gt;
Yes; that would be an [existential type](https://en.wikibooks.org/wiki/Haskell/Existentially_quantified_types). For a typeclass `C`, you can create a type whose values are the values of any type that is an instance of `C`. This is a natural idea in object-oriented programming, but in Haskell, existentials tend to be more hassle than they're worth. Perhaps you could give some more details on what you intend to do with your state -- how is it constructed, how is it used? These questions determine what a suitable model would be.
No. It only compiles with GHC 6.8.3.
Storing a significant amount of data as `[Word8]` instead of a packed format like a ByteString is likely to waste a lot more memory than a 4k block.
&gt; ByteStrings are never guaranteed to be freed. Whether they are freed depends on whether other, totally unrelated ByteStrings are also freed. Yes, I understand that, but it seems it could never use more memory than if the allocation size for each individual ByteString was 4k. It seems unlikely that it would use more memory overall than the `[Word8]` approach. If you're in a situation where you want to have a massive amount of tiny ByteStrings, then ShortByteString (also from the ByteString package) would probably be a better option.
The `Monad` instance of `Reader` (and `(-&gt;)`) allow for manipulation of results for all possible values of an argument. For example, this data type has 3 (usual) values: data Three = A1 | A2 | A3 As a result, manipulation of `Three -&gt; a` is really like manipulation of `(a, a, a)`. All those functions you mentioned work in bulks like this. It's not obvious, but, say `join :: (Three -&gt; (Three -&gt; a)) -&gt; a`. It's really like `join :: ((a, a, a), (a, a, a), (a, a, a)) -&gt; (a, a, a)`, and its definition would be: join ( (a, _, _) , (_, b, _) , (_, _, c)) = (a, b, c) i.e. to take the diagonal of a square matrix. Why? Well, remember when I said 'all possible values of an argument'? Those functions all use the same value of the argument, and just passes it around, so it's not surprising that `join(X)[i] = X[i][i]`, (ab)using array notation.
Yep. You convinced me. There are no mathematical guarantees, there may be errors that we have to test and this is terrible. I will convince my boss for stopping our Web services with Haskell and use PHP instead. So that we will concentrate our Haskell work on playing with Category Theory and Lenses and traversing and folding lists and write blog posts about stupid and useless things. Simon,please, erase the STM library since it is written in C and I'm pretty sure that there are no guarantees that it is free from deadlocks. That is a menace for our Haskell kindergarten.
I'd qualify that with "you shouldn't use records in sum types because it creates partial functions" (although GADTs can change that situation). But your example _is_ more readable with the records.
&gt; Then the 4000 Bytes are wasted, forever (for the lifte time of the application), they cannot be reclaimed. Yes, I understand. However, my point is that it can't use more memory overall compared to if every ByteString allocated used at least 4k memory. See what I mean now? I just want to be clear that I'm not trying to downplay the problem - I can definitely imagine scenarios where it would be a very significant issue. &gt; Regarding ShortByteString, I am not sure how much it really helps in practice. Fair enough.
Oh, yes. I'll take a look at that TypeOperators extension. Thanks.
nice! This is actually reminiscent of the datatype for a lambda calculus with explicit substitutions that was defined in ["Explicit Substitutions for Higher-Order Syntax" by Neil Ghani, Tarmo Uutsalu and Makoto Hamana](http://www.cs.ioc.ee/~tarmo/papers/merlin03-hosc.pdf) (in particular see Section 6 for the Haskell code and sections 1-5 for the category theory). That said, your presentation is factored a bit nicer to be useful as a library and maybe hiding the explicit substitutions under the hood. I'd been thinking about adding something like this to `unbound-generics` (although because that library is monomorphic, it wouldn't be quite as transparent) as a way of quickly short-circuiting repeated `open`/`close` operations (that substitute a fresh free variable for the outermost bound variable and vice versa).
 do a &lt;- foo b &lt;- bar return (a + b) Desugars to `(\a b -&gt; a + b) &lt;$&gt; foo &lt;*&gt; bar`. It eliminated the `return`, and would also do so for `pure`. I find it unfortunate when the compiler has to special case on regular Haskell functions. For example, in the first release, if I had used `$` instead of parens, GHC wouldn't have used Applicative; it wouldn't have known that the last expression was really just `return`, because it looks like a call to `($)`. My suggestion is that instead of special casing on `pure` and `return`, it should use `in`: do a &lt;- foo b &lt;- bar in a + b
return is pure, always. Until a couple of years ago, Applicative was not a superclass of Monad, so we had two different definitions that always did the same. Now, since we have Applicative =&gt; Monad, return is redundant. (And keeping pure instead of return was chosen because it's higher up in the Functor/Applicative/Monad hierarchy.)
With AMP in place, all `Monad` are `Applicative` (and _must_ be), and I believe all lawful instances have `return == pure`. Thus, if at the end of a `do` sequence you use `return` or `pure`, for all lawful instances they are equivalent. A keyword would be nice for the last statement to not special case `pure` and `return`, but I think that's better suited to a different proposal. 
So the values in your state is heterogeneous. Are the operations on that state homogenous? If so, you can set your state to be a record the operations instead. Dumb example of what I mean: maybe you're writing a navigation system and your state is one of Car, Bicycle and Pedestrian. But the navigation system doesn't really care about the state in and of itself. All it really cares about is "which set of roads are legal for this mode of transportation?" and "how fast does this mode of transportation cover this distance?" and "how much does this mode of transport cost to travel from here to here distance?" Those questions are the same for each mode of transportation, what differs is the value of the answer. So store the questions as functions in the state instead, and bam, your state is homogenous again! 
system-filepath I think
Hi, thanks for the response. I think I've got the idea, and updated my program to use what I think is a zipper (but I also keep the head separate from the two lists), making all operations O(1). [Like so](https://github.com/IsaacWoods/BF/blob/048551926957fad525fd2aac7cd103d064c67eab/Main.hs)?
Sure. I have a set of clients with different configurations. These configurations affect which events must be captured and acted upon by the event loop. An event is passed into the event processing object resulting from the configuration, which can refer to the previously-tallied state in order to make decisions *or* can update the tallied state for later events to keep track. A simplistic example would be an event loop in a car. A car model might be configured to turn on the oil light after 7500 miles, or 5000 miles, or 3000. Every time a `mile_traveled` event occurs, the "state" is updated and persists. If the `mile_traveled` tally in the state is a multiple of the configured interval, then the light is turned on. This is much simplified, of course -- I have many events and many clients.
I think the real point is that `runSTArray` offers a safe and efficient interface to `unsafeFreezeSTArray`; `freeze` has to copy the array to guarantee safety in unrestricted use.
Perhaps we should get `ApplicativeComprehensions` instead?
I use [csv-conduit](https://hackage.haskell.org/package/csv-conduit) rather than cassava lately, so it hasn't gotten my notice.
It's deprecated and shouldn't be used. The authors itself said that: http://www.yesodweb.com/blog/2015/05/deprecating-system-filepath
This is the sort of thing it might be fun to do an analysis of. A plot of usage versus pull request response/development activity, or something.
Maybe scroll through results of hackage matrix builder from when it was still running, or observe libraries that have high download counts but haven't been added to stackage Otherwise, I would analyze GitHub issue creation vs closing rate
Old open PRs are a great criteria for automatic analysis
&gt; **Nit #349:** Now, when you say "tensor" do you actually mean [tensors](https://en.wikipedia.org/wiki/Tensor)? or do you mean multi-dimensional arrays? &gt; &gt; **Hint:** "Tensor"Flow only actually deals with multi-dimensional arrays. Same is true of APL and most every other array-programming language/DSL I've seen. If you only mean to handle multi-dimensional arrays, then something like [Jeremy Gibbons' recent paper](http://www.cs.ox.ac.uk/people/jeremy.gibbons/publications/aplicative.pdf) gives a very nice presentation of the standard approach for this sort of thing. Representable functors and MDAs are quite closely related, though they don't give you all the linear algebra stuff you'd expect from a library that handles actual tensors. If you mean to handle tensors, I have a library in the works from some years ago that I've been meaning to polish up and finally publish. Doing a single implementation correctly isn't too difficult (she says, after having done that work when noone else seems willing to), but turning it into type classes so you can overload the implementations gets quite tricksy since vectorspaces have far too much structure so it's hard to disentangle things into a nice type class hierarchy. Not having a good solution to the type classery is why I've let the library languish these past 5 years. If you're primarily interested in using type classes/families to overload implementations in order to eek out performance, then you should prolly take a look at the work on Repa and Accelerate. They've been focused on this particular task for the case of arrays/MDAs.
There are still 98 packages on Hackage that require it though.
Updates to keep it going with Stackage are the only activity though - https://github.com/hvr/cassava/commits/master So it's maintained but just barely.
If only field names in sum types generated us functions `... -&gt; Maybe ...` (or maybe even `Prism`s!).
Why should they if the package works well enough for them?
A proof assistant take on this might generalize to any set of prime numbers. But to fit the extreme framework, it could define prime numbers through their relationship to prime ideals.
Or, perhaps even better, records and sums were two different things altogether. ;)
In a sense, yes: if you have a function defined in terms of those, you can translate it to any algebraic data type. That's the basic idea behind [generics](https://stackbuilders.com/tutorials/haskell/generics/).
Fwiw, `system-filepath` had a nice lean dependency footprint, basically just `text` over what's already pre-installed with GHC: $ cabal install --dry Resolving dependencies... In order, the following would be installed (use -v for more details): text-1.2.2.1 system-filepath-0.4.13.4 whereas `path` pulls in quite a few packages: $ cabal install --dry Resolving dependencies... In order, the following would be installed (use -v for more details): base-compat-0.9.3 dlist-0.8.0.2 integer-logarithms-1.0.1 mtl-2.2.1 primitive-0.6.2.0 random-1.1 stm-2.4.4.1 text-1.2.2.1 time-locale-compat-0.1.1.3 transformers-compat-0.5.1.4 vector-0.12.0.1 hashable-1.2.6.0 tagged-0.8.5 exceptions-0.8.3 uuid-types-1.0.3 unordered-containers-0.2.8.0 scientific-0.3.4.12 attoparsec-0.13.1.0 aeson-1.2.0.0 path-0.5.13 Personally, I will refuse a forced migration from `system-filepath` to `path`.
plugins provides hotswapping(!), but doesn't built for me on any platform (even NixOS, which is listed as supported in thr .cabal). it has a recent upload, but i didn't find an issue tracker on darcs, and the issues on github have been ignored. http://hackage.haskell.org/package/plugins 
Yeah, but there's still a *few* things that [depend on it](http://packdeps.haskellers.com/reverse/system-filepath).
Yes, this package seems to be good replacement. I like it. Though some people may not be aware of it...
Those dependencies which don't come with GHC come transitively from exceptions (providing MonadThrow). Otherwise, the rest are for providing instances of class like aeson (to provide FromJSON and ToJSON instances), hashable (for that instance) and deepseq (NFData). Libraries which most people probably already have installed. The alternative is orphan instances and user inconvenience. 
&gt; My suggestion is that instead of special casing on pure and return, it should use in This is a great idea.
Another package is nonce: https://github.com/prowdsponsor/nonce It would be nice if the pending PR for cryptonite can be merged and a new release be made.
Ever. The argument is that apart from beauty, not much is to be gained by removing return from Monad. I’d like to see it gone too, but I get the point.
Most packages with »wl-pprint« in their names. Some have unresponsive authors (e.g. wl-pprint), some are on life support without development. Edward maintains ansi-wl-pprint, but there is no feature development. Anyway, I took matters into my own hands: http://hackage.haskell.org/package/prettyprinter (surprised the name wasn’t taken)
If you find anyone willing to take it over, or if you want to yourself, I'm sure he'll be willing to hand over the permissions necessary. If you want to put out a call for help, that could be a way to do so..
no, i'll try that. an email is listed. 
Why is it all a single loop? Does it need to be? The problem seems much simpler if everyone just gets the state relative to their needs. Then you just set listeners on your shared event channel to dispatch out to Stateful components listening for the events. 
Would something like this work for you perhaps: {-# LANGUAGE ExistentialQuantification #-} data Any b = forall a . Any a (a -&gt; b) some (Any a f) = f a t :: [Any Int] t = [Any 9 id, Any 2.0 fromEnum, Any "Hi there!" length] s = sum . map some $ t I recently wrote a library based around this idea, dunno if it's super helpful yet though. https://hackage.haskell.org/package/shade
Missing the point of this thread as per initial ancestor comment, fully aware that AMP is in place. Discussing instead about how before ApplicativeDo, `return` had type `(Monad m) =&gt; a -&gt; m a` but with it enabled, it might instead be "sugar" for `(Applicative m) =&gt; a -&gt; m a` or even `(Functor m) =&gt; a -&gt; m a`.
In general, your best bet is to go google the name of the missing library, find install instructions for windows, make sure the install directory of the new program shows up on your PATH enviornment variable somewhere, and try again. EDIT - It's probably an easier problem to try to figure out whether or not you can get the underlying lib to work in another context in windows, and then see if there is a significant difference between "stack env" and your standard env.
Maybe Adam Chlipala's "[Certified Programming with Dependent Types](http://adam.chlipala.net/cpdt/)"?
you need Applicative, Functor isn't enough, no?
Why do you suppose bos' packages were never transitioned over to new maintainers? Did he disappear, is he OK?
bos is careful about giving them up. Something like `text` requires a high degree of interface stability and not every prospective maintainer can meet that criteria.
Off the top of my head: * `lens` uses `TypeInType` to [define a higher-rank kind](https://github.com/ekmett/lens/blob/6f8e5539fccca2e07a3e0b54bb0ec3364f9f3d52/src/Control/Lens/Type.hs#L461) for one of its type synonyms in the `Lens` hierarchy: type Equality (s :: k1) (t :: k2) (a :: k1) (b :: k2) = forall k3 (p :: k1 -&gt; k3 -&gt; *) (f :: k2 -&gt; k3) . p a (f b) -&gt; p s (f t) * `singletons` uses `TypeInType` extensively. One particularly nice application of `TypeInType` is the definition of the `SingKind` class: class SingKind k where type DemoteRep k :: * fromSing :: Sing (a :: k) -&gt; DemoteRep k toSing :: DemoteRep k -&gt; SomeSing k This is a _kind_ class, which is only possible with `TypeInType`. Before, `SingKind` had to be defined in a kludgey way involving proxies. * GHC 8.2 will feature a redesign of `Typeable` with a type-indexed `TypeRep`: data TypeRep (a :: k) where TrTyCon :: Fingerprint -&gt; TyCon -&gt; [SomeTypeRep] -&gt; TypeRep (a :: k) TrApp :: forall k1 k2 (a :: k1 -&gt; k2) (b :: k1). Fingerprint -&gt; TypeRep (a :: k1 -&gt; k2) -&gt; TypeRep (b :: k1) -&gt; TypeRep (a b) TrFun :: forall a b. Fingerprint -&gt; TypeRep a -&gt; TypeRep b -&gt; TypeRep (a -&gt; b) This requires `TypeInType` as this `TypeRep` GADT is indexed not just by the type variable `a`, but also by the _kind_ variable `k`.
The last commit on `text` was like, 2 hours ago. There are only 4 PRs up, where one is orthogonal at best, another is seriously underbaked, and another hasn't fixed up issues requested for over a year. Doesn't seem very unmaintained to me.
Given Haskell's laziness, I'm not sure what the advantage of Any would be. If anything, it causes problems because the result will be recomputed every time you call `some`. Instead of having`Any a f`, why not simply apply `f` to `a` directly?
Software Foundations is a great resource: https://www.cis.upenn.edu/~bcpierce/sf/current/index.html Covers both Coq, and some PL semantics
In other words you have all the power of pattern calculus in Haskell? 
I think it might be worth just pinging the maintainer again on the issues. After all there are only two of them and they are quite old so maybe they have just missed it. I send them a PR a few days ago and got it merged very quickly so they’re definitely not completely unresponsive.
&gt; A plot of usage versus pull request response/development activity, or something. I'm not sure that's a good metric. Bug fixes? Great. Necessary. But if a library is well-designed, there will only be so many edits you'll make down the road. 
&gt; Kind of similar to the Rust problem, where's the compelling platform to go with the language. What about it helps my team and I deliver better quality software faster. Really? Rust isn't as mature as C++ but it's faster and has the libraries. 
So, I admittedly haven't tried it, but the readme raised some red flags: &gt;Impure function calls in pure functions are detected and raise errors at runtime. Why? I realize type inference is *hard*, but I'm over here in Haskell which has purity checked at compile-time *and* has monads to manage impure code. &gt;Inherently, programs written in Tisp run concurrently and can run parallelly. Parallelism is a subset of concurrency, no? Though I admit automatic concurrency is cool as heck. &gt;Dynamic typing This seems like a huge mistake. &gt;The concept of "Time is Space" What does this have to do with Haskell? &gt;Everything is a value; no object system That sounds cool.. It's homoiconic? &gt;Simplicity Go's simplicity strikes me as huge weak point, actually. Lots of reinventing the monad and code generation that weakens compile-time checks. In general: why was go chosen for a language? I'm not familiar with the libraries, but what does go offer by way of parser libraries and the like?
A dev platform isn't just the libraries and the merits of the language. There's also the wider ecosystem to consider and how it interacts with the workflow of your development teams. It's a bit off topic here here, but there are some cons that prevent me from confidently recommending Rust for large projects at the moment.
&gt; Libraries which most people probably already have installed. Btw, the notion of "having a library already installed" doesn't apply to [`cabal new-build`](http://cabal.readthedocs.io/en/latest/nix-local-build-overview.html) anymore the way it did to the way Cabal operated previously. In the past, a major problem of cabal was that the install-plans it picked were stateful as the packages you previously installed into the *user* package db had a major effect on which install-plan the solver picked, harmed reproducibility of install-plans as well as causing the dreaded breaking-reinstall situations. So this is one of the major killer features of cabal's new nix-style features, which also earned it the name ["no-reinstall Cabal"](http://blog.ezyang.com/2015/08/help-us-beta-test-no-reinstall-cabal/). So with the new nix-style paradigm, the only things you have already "pre-installed" are the packages that are in the *global* package db, i.e. (`ghc-pkg list --global`), which is typically only populated by your GHC installation or your Linux distribution (c.f. Debian) in a self-consistent way (i.e. no broken packages), and thus can be considered a more or less static part of the system environment.
You are right, it depends on your generating and pruning function.
Really nice introduction to the benefits of `Monad` and `do` syntax.
Can we count hackage.org? Package main pages make no sense - no one votes for anything, and the GitHub link is hard to find. 
&gt; no one votes for anything Here's the current top-10 of no one voted for packages on Hackage: - 20 votes for `lens` - 12 votes for `servant` - 12 votes for `aeson` - 10 votes for `megaparsec` - 9 votes for `pandoc` - 9 votes for `optparse-applicative` - 9 votes for `hspec` - 9 votes for `base` - 7 votes for `reflex` - 7 votes for `reactive-banana` However, the voting feature has been getting a serious overhaul, and I hope we can bring it online soon in one of the next hackage-server feature deployments.
I get the premise of the title, and I even agree with it. However the article simply shows the outline syntax of different languages without giving any explanation of any the monadic solution is better, e.g. by showing the type signatures of the Haskell snippets or explaining how the Monad API is implemented in each case.
I'm working on a music composition library called Mezzo (https://hackage.haskell.org/package/mezzo – the Hackage version is a bit old, I'm planning to update it in the near future with some new features). The idea is that the library statically enforces rules of music composition, and turns musical mistakes (such as dissonant intervals) into compiler errors. This is possible because a representation of the music is stored on the type level, and the rules are implemented as type class constraints. The representation is essentially a type-level matrix of notes, and to ensure that all rows and columns have the same size (which is necessary for the rule enforcement to work), I'm using a [promoted vector of vectors](https://github.com/DimaSamoz/mezzo/blob/master/src/Mezzo/Model/Prim.hs#L157) to store the music, which is manipulated by the composition of [term-level musical values](https://github.com/DimaSamoz/mezzo/blob/master/src/Mezzo/Model/Music.hs#L66) (with an algebraic structure similar to Haskore). As vectors are GADTs, this is only possible with GADT promotion, enabled by `-XTypeInType`. I use GADT promotion in other places too, such as indexing [type-level chords](https://github.com/DimaSamoz/mezzo/blob/master/src/Mezzo/Model/Harmony/Chords.hs#L58) by their size (e.g. a triad would have the kind `ChordType 3`) or [promoting harmonic grammars](https://github.com/DimaSamoz/mezzo/blob/master/src/Mezzo/Compose/Harmony.hs#L47). In general, TypeInType seems to come in handy if you do any kind of complex type-level computation or static verification, and don't want to be hindered by the distinction between types and kinds (after all, they are all checked at compile-time). While it would be too involved to give concrete examples from the library, the best demonstration of what `-XTypeInType` enables is heterogenous type-level if expressions: type family If (b :: Bool) (t :: k) (e :: k) :: k where ... type family HIf (b :: Bool) (t :: k1) (e :: k2) :: If b k1 k2 where ... `If` is a normal, homogenous if-expression (i.e. both the then and the else branch have the same kind), and it is used to statically determine the return kind of `HIf` from its argument. Note that `b` is a type variable of kind `Bool`, while `k1` and `k2` are kind variables – this is only possible because types and kinds are the same thing. While I don't think this example has a great practical value (or at least I haven't found one), it slightly blew my mind when I saw that it compiles :) 
PureScript has that. There's good and bad for both: * The good for PureScript is avoiding this problem. You can just write `data Foo = Bar {id::Int} | Mu {id::String}` without additional ceremony. * The good for Haskell is that you have nominal typing. It's not possible to, without manual conversion, go from a nominal record e.g. `Person` to an "anonymous" record like `{"name":String,age:Int}`, as it is in PureScript. The disadvantage of first-class records is the same as tuples; aside from throwing information away (like instances), it lets you combine things that shouldn't be combined. E.g. `{"id":Int,"title":String}` could be anything (from a forum post to a category or a node in a network), so the fact that this could be accidentally passed to a function expecting a different meaning is a downside avoided in Haskell. In PureScript everyone ends up doing this: newtype Foo = Foo { foo :: Int, bar :: Char } but then you can't access the record without unpacking the `Foo`. How to get the best of both worlds isn't clear. There is some `Newtype` class with an `unwrap` function. I would maybe add some special support to define nominally tagged records. So on both sides you do have trade-offs. 
Yes, I completely agree with that point. So perhaps my comment was more to show how you can turn heterogeneous data into a homogeneous structure. It would need to be adopted to the specific situation. Perhaps adding a constraint on the `a` in `Any` (e.g. for `increment`ing, `count`ing etc..) would make this useful for the asker. Perhaps you should have a look at the example I provide in the package I wrote: https://hackage.haskell.org/package/shade-0.1.1.1/src/Example.lhs Like I said, I'm still not convinced that it's useful. However, I did manage to use it to solve a similar problem in a compiler I wrote for a university course. Here I wanted new backends to be able to define their own `Parser a` and a function from that `a`.
Although I mostly agree with you, and although the article is mostly nice if you already "get" monads (and thus is somewhat circlejerky), it _does_ show how the same concept, down to the syntax, applies to many different situations, Thus, the problem that it solves, even although it's not immediately apparent, affects all the mentioned examples. In that form, it does bring across what an elegant abstraction it is.
To be fair, most of my issues with Rust are less about the language and more about the current state of the tools available for it. Esp. on Windows. 
I think it actually varies depending on where you are in the world, though Coq seems a bit more popular. Both are worth learning, in any case. - Coq has dependent types, Isabelle does not. - Isabelle has a really nice structured proof language called Isar, Coq afaik does not. - Isabelle is a much more general engine for proof and program extraction of which Isabelle/HOL is the most popular system. There is also Isabelle/ZF. - Isabelle/HOL uses classical logic, whereas Coq works with constructive logic out of the box. This is a significant difference. - Isabelle/HOL has the powerful `sledgehammer` tool for automatic proof. I don't know of anything like it in Coq, though it does also have some automatic tools. - Isabelle/HOL has a much smaller trusted kernel than Coq, which has a much larger base system. Thus Isabelle/HOL can be more closely scrutinised and trusted to be correct, so that if it says a proof is true it is near certain that it is so. Isabelle also has a nicer IDE, imo. I think what I've said is true, but I also think my preference is showing as well, heh. So, maybe take what I say with a half grain of salt. Edit: with regards to NuSMV, CBMC, and things like that, I have no idea. Those are different realms of verification, and not something I am as familiar with. It's an area worth learning the theory for, and playing around with, anyway.
He is OK. He is just very busy with his projects. Mostly I remember some issues about `double-conversion` package when very needed PR wasn't merged since 2015 until December 2016. I'm talking about this PR: https://github.com/bos/double-conversion/pull/13 https://github.com/haskell-infra/hackage-trustees/issues/87 
Well, maybe it's not so important for `acid-state` package because it doesn't have so many issues and PR's. But I don't like when issues and PR are hanging for years. For example, `containers`: https://github.com/haskell/containers `unordered-containers`: https://github.com/tibbe/unordered-containers I can't imagine everyday programming w/o using data structures and algorithms from these packages. They are SOOOO important. Packages with containers should have first-class support for every language in my opinion if you want this language to be widely used. But they have a lot of issues and PR's from 2014 and even 2011-2012 =\ For example, `O(n)` size function for `HashMap`. I think community should put more effort into such packages.
Yeah, development activity was not a good suggestion. Though if a library is done and needs no edits, it probably won't be getting any pull requests (if it is on a VCS), so that should capture "done and dusted" libraries to some degree. 
Nice!! Showing where monads can make a practical difference.
One of the unstated challenges is that components have to be packed together by type, as you say, in an array. For simple lookup simply storing them in an Entity record with a Type -&gt; Maybe (Element of Type) should be easy enough.
And what problems would remain?
Something like https://gist.github.com/pakoito/00db608857b0a96ea8829387f1a5c785#file-kecs-hs-L19 That happens internally inside the World, where an array of Components is registered at startup and is filled at runtime. Then, for every system you register for a set of Types, you have to call a function like that to look it up. The challenge is the genericness and componentization. If it was done just for one case of a world for one game it should be mostly trivial.
[This stackexchange post]( https://softwareengineering.stackexchange.com/questions/267549/license-of-library-used-by-one-library-include-in-my-project) suggests your understanding is correct, but I don't have any experience with distributing binaries to tell how seriously this should be taken.
I like the simplicity of demonstrating the ergonomics of monads contrasted with the naive and ad-hoc solutions. I feel like this presentation style would resonate well with my colleagues who are interested in fp but scared by terminology.
I think it's a great motivator. Doesn't look like a tutorial, but just an explanation of what makes monads useful.
Adding a new type of component might require a recompilation of your program, but you shouln't need to change the *implementation* of the "black box world" itself, is that correct? The "black blox world" might even have been supplied by a library you don't control. It's like a generic repository for components.
Correct, that's the idea. AFAIK in the OOP version there shouldn't be a problem adding a new type to the system at runtime, due to the type unsafety and matching based off reified types available thanks to reflection. That in Haskell seems too far away, and I'd rather not have that feature in exchange for type safety.
/u/ChrisDone2's point, I think, is that the mentioned dependencies are likely already transitive dependencies of your app. And even if not, that it's likely already built and therefore likely won't add time to your build. Either way, you're not addressing the last point, which was: &gt; The alternative is orphan instances and user inconvenience. How do you propose avoiding orphan instances if not by adding a few extra de facto standard dependencies to `path`?
PureScript is, in my mind, a good example of how this should be done. If you'd like to do this properly you might find some of our scripts useful too. See, for example, our LICENSE and the contents of the license-generator directory in the repository: https://github.com/purescript/purescript
&gt; However the article simply shows the outline syntax of different languages without giving any explanation of any the monadic solution is better What you're saying is true, but I also believe is the exact flaw with all of the 1000 monad tutorials out there. Presuming that the clearest way to communicate info about monads is through type signatures and laws. To someone experienced in haskell and accustomed to gaining information this way it is useful. To someone who isn't it's like handing someone a dictionary when they still are working to internalize the alphabet. Additionally every monad tutorial spends 80% of its time one the solution and 20% on the motivation. And usually presents the motivation in a language the user is less familiar with, so even if they've frequently encountered the problem in "their own" language they may not pattern match on it immediately and recognized it. This is probably the only Monad tutorial I will ever point to someone who is curious about Monads. "Read this first, understand the problem. That will give you the motivation, and the landscape - then read any of the other tutorials online and it will make sense." The the perspective of the average haskeller someone might disagree. But taken from the viewpoint and a learner with an imperative background this is 100% on the spot. My only quibble would've been to find a way to present the monadic solution cleanly in an imperative language. Honestly an excellent teaching reference. Kudos to the author.
Also I tried cabal-dependency-licenses but I had problems with mismatches of versions of the Cabal library (don't remember the details, sorry). I ended up resorting to a hacky approach that just extracts the relevant information via string manipulation.
The problem of having extensible data and operations at the same time is known as the "expression problem". Search for that and you'll find several potential solutions. That said, I don't know if there's one that's commonly accepted as probably-best-for-most-cases, and maybe something nicer is possible for the specific case of entity-component-system architectures.
 do a &lt;- foo return (bar a) This will desugar to only use `Functor`
What's the matter, for example, with Löh and Hinze's [open data types](https://www.andres-loeh.de/OpenDatatypes.pdf)? How come they have not even been proposed yet, let alone implemented? Infinitely simpler to understand than "Data Types à la Carte."
Does that satisfy your criteria? http://lpaste.net/8801501803372871680
Hum, actually, I don't think it does! It should provide a `Prism` so that it can be applied to entities that do not have a component, but I don't think that is expressible in haskell with such a typeclassy approach ... I'll think a bit more about it.
That's a great solution, definitely the closest I've seen so far. It ticks all the boxes except one, so let me present it below. How would I approach it if I wanted the components of the same type to be packed together in an array, and each contained one association id (i.e. an Int) and you could traverse the different arrays for intersecting elements with the same Int. Bad Haskell sketch: type EntityId = Int data Weapon = Weapon { _damage :: Int , _minlevel :: Int , _id:: EntityId } deriving (Show, Eq) data PhysicalItem = PhysicalItem { _durability :: Int , _weight :: Int , _size :: Int , _id:: EntityId } deriving (Show, Eq) type EntityStorage = ???? allWeaponsInSystem :: EntityStorage -&gt; [Weapon] -- ??? traverseEntitiesForSword :: EntityId -&gt; EntityStorage -&gt; (Sword -&gt; ()) -&gt; () Could traverseEntitiesForSword even be generalised for any entity assoc?
I do not get what is bad about unpacking...
fmap bar foo. thanks! 
If you store each component in its own array, then what is stored in `EntityStorage`? Wouldn't the type of `traverseEntitiesForSword` simply be EntityId -&gt; (Sword -&gt; r) -&gt; r which traverses all of the relevant component arrays (`Weapon` and `PhysicalItem` in the example) and builds a `Sword` to pass to the given function? Edit: Oh, and to answer your second question, you could associate each component with its vector using a typeclass: class IsComponent a where componentVector :: Vector (EntityId, a) -- or some other typeclass that lets gives you a 'a -&gt; EntityId' dict Then, you could write a function that works for any `(a ::: b ::: c ::: ...)` (really just a tuple) so long as each of the elements is a member of `IsComponent`.
Use a Tchan (or similar) as your queue and subscribe Stateful components as listeners, organized by the state that they care about. If a listener gets a signal it doesn't care about, it just discards it and loops back to listening again. If it gets new data, it updates it's state, kicks off whatever function it needs to kick off, and loops back. Then you have a single sum time being broadcast on your shared channel, representing different events, and different Stateful "widgets" that consume events, store state, and kick off downstream routines based upon their current state. To close the loops, just send an AppQuit signal or something and make them all return () instead of re executing themselves. 
As mentioned, some of them are maintained, but none is actively being developed. Here is what I think `wl-pprint-annotated` lacks: - Uses String, not Text - »Doc« type is abstract, and not exported from an internal module - making it incompatible with everything else - Some undocumented definitions, most docs taken from the parent package (presumably ansi-wl-pprint or wl-pprint) - No obvious way to render to HTML (well, there is no *obvious* one, which is why I think it should include helpers) - No obvious way to render to ANSI terminal either (making it a hard choice over ansi-wl-pprint) Especially the last one was what I was missing everywhere, because for some time I thought about writing a nice browser GUI for my [STGi](https://github.com/quchen/stgi). I then started modernizing wl-pprint, hoping one day I could merge into the main module; I also added extensible backends, making ANSI/HTML support just a relatively small addition to the prettyprinter instead of its own entire library. Unfortunately, the wl-pprint maintainer is unreachable (I gave up after multiple emails over half a year), so while waiting I kept documenting, optimizing and what not. Anyway, that’s how we got here ;-)
Here goes an uninformed comment, as I hadn't heard of pattern calculus before now. Going by what [this page](http://bondi.it.uts.edu.au/) and the links therein suggest, I guess you can do it in Haskell, though not necessarily with the same expressivity (e.g. needing a fair bit of machinery for things that are first-class in their language).
Rust sounds great. I can see how it gives much nicer guarantees than C++. I just don't see any reason for it to be faster than C++. I do see reasons for C++ to be faster, though, because it allows doing some things that Rust disallows or at least makes very difficult (e.g: intrusive data structures).
Fair enough. I haven't used it on windows. 
&gt; I just don't see any reason for it to be faster than C++. &gt; &gt; I do see reasons for C++ to be faster, though, because it allows doing some things that Rust disallows or at least makes very difficult But, on the other hand, safety means you can write more complicated code that's still fast. It will never be faster than assembly, but it may well be the only way for a human to write complex software like a [terminal emulator](https://github.com/jwilm/alacritty). Rust says "yeah, we're more limited, but you were never going to write that algorithm in C anyways because it would be a pain in the ass."
Can someone provide an example of some OO code that demonstrates what we are trying to replicate in Haskell? It's rather hard to do it in the abstract.
Copyright does not distinguish between a file and a collection of files, so including the license information in a separate file, or files, distributed alongside the binary, is ok. (IANAL)
Congrats and thanks!
&gt; Isabelle has a really nice structured proof language called Isar, Coq afaik does not. Coq does have a structured proof language actually, known as C-zar. Unfortunately it's always been somewhat buggy (in ways that limited its expressiveness, though without introducing unsoundness) and recent releases of Coq have only broken it further. OTOH, the latest releases of Coq also introduce 'structured proof scripts' that should partially obviate the need for a separate Czar-like language.
All instruments must be of the same type, or you wouldn't be able to store them in a container, that's the flaw with what I proposed.
Thanks. Is there a self-contained example that gets to the heart of the matter?
You can use an existential wrapper to store them in a container. That's a bit annoying, but a lot less annoying than working with complicated type machinery.
Understandable. I don't blame anyone for not reaching for rust, but I will say it's found it's way into my workflow. 
Incidentally I had the same problem. So I just "stole" cabal-dependency-licenses' code and [integrated](https://github.com/DanielG/cabal-helper/blob/master/CabalHelper/Licenses.hs) it into [cabal-helper (Hackage docs)](https://hackage.haskell.org/package/cabal-helper-0.7.3.0/docs/Distribution-Helper.html#v:pkgLicenses) which takes care of all that cabal version mismatch trouble automatically. It is a library though so you'd need like three lines of code to actually spit out the information. Maybe I should just send a PR to cabal-dependency-licenses to turn it into a shim around cabal-helper.
What if the lists are all homogeneous? List of components by the same type, and a map from component type to a list of components of that type. That works :D
What remains to be seen? My point is directly comparing individual projects to gauge the usefulness of a programming language ecosystem is unhelpful, especially when you compare projects from different generations of computing. Does rust have an FFT library that can outperform Fastest Fourier Transform in the South? Does it have a game engine more performance orientated than UE4? No? Are those good reasons not to use Rust, because of some arbitrary, subjective standard I pulled from my rear where the sun don't shine? An optimiser's strength comes form its ability to perform a multitude of mediocre optimisations that a human would find tedious. That in itself is only a part of the performance equation. Rust can't magically fix any dataflow issues you may have, make your broken architecture design magically brilliant or solve your memory and data locality problems. In my eyes Rust does make the process of writing code simpler, it isn't a silver bullet for how people make fast software systematically. ripgrep is fast because burntsushi understood what is is to make a fast regex library and to apply it to searching files. Not because of Rust. 
Yeah, that works, but you're essentially forfeiting (b) in one way or another by saying that your lookup from component type to list of components can fail at run-time. So you're doing a form of dynamic dispatch based on the run-time component type.
It doesn't with open type families. All possible lookup elements are known, even if they're just empty lists. See: https://gist.github.com/pakoito/00db608857b0a96ea8829387f1a5c785
Thanks for your work on getting a GUI toolkit more accessible from Haskell! I think this effort is an inspiration to get people involved (myself included) 
All monads are sequential though. Even the tardis monad is sequential; it's just reversed. Anyway, I can't think of a single monad that gets more complicated with `do` notation. And besides, I think beginners should learn how to use `do` notation for rudimentary tasks like IO and state before actually learning how monads work. This article acts as a nice motivating bridge across that gap.
Sequential insofar as function composition is. 
What's your point? Function composition *is* sequential.
In the US, Carnegie Mellon, Princeton and Pennsylvania are all good options, just judging from the people I know in those universities. McGill in Canada is good too I think. Also in Germany MPI and TU München are good options too. I have no idea about fees/scholarships or anything though, so you'll have to figure that out yourself.
Parallelism is emphatically orthogonal to concurrency - all you need for concurrency is the ability to block and return. In particular, many concurrent programs can run on a single core in a way you would notice the concurrency, while it's not clear the same is true of parallel programs. 
And? It's still faulty reasoning to say that because in one case, a faster thing was implemented in language X therefore X language is faster. The causality is weird, and it's just as superfluous as when C and C++ developers do it amongst themselves. If I wanted to make something faster than ripgrep, for whatever reason, the programming language its in wouldn't be my first consideration. 
Simple. It already exists! https://themonadreader.files.wordpress.com/2014/04/fizzbuzz.pdf
Related to open data times is the lovely recent work on the Key Monad: http://people.seas.harvard.edu/~pbuiras/publications/KeyMonadHaskell2016.pdf
Great post. Provides a great counterargument to the idea that monads are useless abstract non-sense, which seems to be a common belief among SEs that don't like FP.
global uniqueness is the term that wadler uses in this paper for what I usually refer to as coherence. For me that gets bound up in every diagram in the category of constraints commuting.
Env x a itself isn't a functor, but given access to an associated "Syntax a" that you want to map over at the same time you can finish mapping over Env in the continuation. You need the associated 'Syntax' to figure out if each Var is a "real" Var in some sense. The trick is in building the right polymorphically recursive CPS'd function to actually get it all to go through. There are a few messy cases to handle, complicated by the mixture of weakening, composition, etc. We might have better luck trying to just play around with doing the mapping over Weaken given Syntax first, then go back and fill in the rest after we have a nice building block.
You can concat symbols in 8.2?! Exciting times ahead.
It actually turned out to be something different. This won't compile: {-# LANGUAGE ApplicativeDo #-} foo :: Applicative f =&gt; f () foo = do pure () -- Needs monad constraint! pure () Not sure what's up with that, but it works if I bind the first statement out: {-# LANGUAGE ApplicativeDo #-} foo :: Applicative f =&gt; f () foo = do _ &lt;- pure () pure () EDIT: Actually here's an example that errors unexpectedly in the `where` block: foo :: Applicative f =&gt; (Int -&gt; f ()) -&gt; Int -&gt; f () foo f 0 = f 0 foo f n = bar where bar = do _ &lt;- f n foo f (n - 1) --- error: • Could not deduce (Monad f) arising from a do statement from the context: Applicative f bound by the type signature for: foo :: Applicative f =&gt; (Int -&gt; f ()) -&gt; Int -&gt; f () at /tmp/dante854Uay.hs:5:1-52 Possible fix: add (Monad f) to the context of the type signature for: foo :: Applicative f =&gt; (Int -&gt; f ()) -&gt; Int -&gt; f () • In a stmt of a 'do' block: _ &lt;- f n In the expression: do { _ &lt;- f n; foo f (n - 1) } In an equation for ‘bar’: bar = do { _ &lt;- f n; foo f (n - 1) } --- Even adding `ScopedTypeVariables` doesn't help: {-# LANGUAGE ScopedTypeVariables #-} foo :: forall f. Applicative f =&gt; (Int -&gt; f ()) -&gt; Int -&gt; f () ... where bar :: f () ... Same issue.
Thanks, fixed :)
First, thanks for your contribution and hard work! The material seems to be very interesting and useful. I've converted it to pdf and uploaded them [here](https://drive.google.com/drive/folders/0ByGJZkRGs2GWYzAtNlVCblVfaVE?usp=sharing). Maybe you can add them to your folder.
Would something like Patreon allow you to continue work on it?
People want extensible systems like this because their languages lack simple sum types. Given sum types, there's no need for all this silly machinery, as an application developer.
Hi Alexander, You might want to check out Leanpub - Rainer Grimm told me that it is a great way to self-publish a book. They don't have an army of editors to put behind you, but you can always find people willing to help - to review the material and find errors and such. I've bookmarked this for when I get the chance to read it. I really liked the topic you wanted to cover. Cheers, Ivan 
What you want to do is add something to the `Context` that you pass to your call of `loadAndApplyTemplate`. For example, you could do something like constField "references" (map show listOfReferences) &lt;&gt; defaultContext Of course, that `listOfReferences` is what you're after. Reading some of the code in [Hakyll.Web.Pandoc.Biblio](https://hackage.haskell.org/package/hakyll-4.9.5.1/docs/src/Hakyll-Web-Pandoc-Biblio.html#pandocBiblioCompiler), the one that jumps out is `biblioCompiler`. It tries to get the the file location from `getUnderlying`, which we don't really want - we want to be able to hand it over manually. So, perhaps just copy that definition and add a parameter for a filepath: biblioCompiler :: FilePath -&gt; Compiler (Item Biblio) biblioCompiler fp = do makeItem =&lt;&lt; unsafeCompiler (Biblio &lt;$&gt; CSL.readBiblioFile fp) That gets you a list of `Reference`s. You'll probably want to render it according to a particular CSL style. This is where it seems to get a little hairy: [](https://hackage.haskell.org/package/pandoc-citeproc-0.10.4.1/docs/src/Text-CSL-Pandoc.html) has the code that handles the rendering of the references with a particular CSL file, in `processCites'`. But that function assumes you want to put the references below a Pandoc file. It also does some stuff to search through the Pandoc file itself for inline citations and such. I couldn't seem to find a function that just takes a CSL style and a bunch of references and dumps out a formatted list - it always get sent to `citeproc`, which always assumes a Pandoc document. What you *could* do, is pattern match on the `Reference` itself and do some manual formatting by yourself, bypassing the entire Pandoc rendering chain. You'll lose the ability to use arbitrary CSL files, but at least you can be sure that, given a `.bibtex` file, it will be rendered and made available in your Hakyll template with the key `$references$`. EDIT: If you'd like, I can give a more detailed explanation on how to get the rendered bibtex file (as a `String`) into a document, as well as getting the file path of the bibtex file from the metadata of a Markdown file that Hakyll processes, so that you can link a Markdown file with a bibtex file.
Right (even though I am not sure how that would work), but can I have components that do not implement `AttractiveInstrument` now?
Pandoc allows citation wildcards in a `nocite` metadata field. So you can pass `processCites'` this pandoc document (here given in Markdown): --- nocite: '@*' bibliography: 'mybib.bib' ... and it will give you a Pandoc document that just contains a bibliography with all the entries in `mybib.bib`. I don't know anything about Hakyll, but I hope this helps. 
I'm excited about this topic as it's an area that's vacant in terms of documented "war stories" or established best practices. I have begun commenting on some of your chapters in the shared document in hopes that this'll help improve quality and prevent the project from stagnating.
It is quite obvious that sum types in themselves do not solve the expression problem.
I wrote a blog post tackling monads from a similar angle: http://vaibhavsagar.com/blog/2016/10/12/monad-anti-tutorial/!
&gt; I am nevertheless curious whether this representation is backed by some rigorous mathematics or it is just Haskell inability to express variadic functions / typeclasses in the language nicely. Well it is a valid functor, yes, but if your question is whether there's any mathematical justification for blessing *that particular functor* (with a lowercase f!), you're going to inadvertently ignite a firestorm. Whether something *should* be declared for instance search just because it's a valid instance is a contentious topic. Ultimately, the typeclass/instance model simply does not fully reflect the mathematical shapes these concepts are named after, even in the limited domain of Haskell programming. Lots of things are functors in more than one way, and there's no reason why you shouldn't be able to simply state which functor you want to use. This is basically what `Lens` is: e.g., `over _2 (+2) ("hello",3)`. That `over` is essentially `fmap`, `_2` is specifying which functor instance you want, and `(+3)` is the function you're mapping.
I thought that the fact that they are packed by type was an implementation detail. Is there a reason this is important, other than performance?
No, just performance. Packing by entity is easier to implement, you just need a Type -&gt; Maybe element map per entity which is easier to implement.
Can you just use [vault](https://hackage.haskell.org/package/vault-0.3.0.7/docs/Data-Vault-Strict.html) ? It would require some setup code in IO though.
Of course, I agree 100%.
Thanks ,I corrected the mistake. 
My question is why the default functor * for `(,)` is not `map :: ((a,b) -&gt; (c, d)) -&gt; (a, b) -&gt; (c, d)` * for `(,,)` is not `map :: ((a,b,c) -&gt; (d,e,f)) -&gt; (a,b,c) -&gt; (d,e,f)` * for `data F a b = F Int a b` is not `map :: ((a, b) -&gt; (c, d)) -&gt; F a b -&gt; F c d` etc.
We should! We could do something like: match "posts/*" $ do route idRoute compile $ do bibtex &lt;- load "files/bib.tex" rendered &lt;- renderPandoc bibtex ctx = constField "references" (itemBody rendered) &lt;&gt; defaultContext pandocCompiler &gt;&gt;= loadAndApplyTemplate "templates/default.html" ctx This does the following: * For each file in the "posts" folder, * load the "files/bib.tex" file, * render it with Pandoc, * put the rendered string in a field called "references", which we can refer to in our template files with `$references$`, * call `pandocCompiler` on the post itself (we're looping over posts, remember?) * pass the string result from that into the "default.html" template, which can then access it with `$body$`. This is all from memory, so if something doesn't work, just ask.
Does it? I've loved everything I've found there. I do tend to only purchase books that are set at "pay what you want", though. If I'm not impressed, I didn't lose anything. If I learned something, I go back and purchase. I'll even double check for an author donation page. Key example might be PureScript by Example by Phil Freeman. Packt Publishing, on the other hand, I avoid like the plague. I've been bitten too many times. _edit: Corrected spelling of Packt Publishing (was Pakt)_ 
Me too, [one ticket number before yours](https://ghc.haskell.org/trac/ghc/ticket/13740#ticket), and with a smaller test case. I think it's perhaps related to the fact that `&gt;&gt;-` has no precedence.
Hmmm, I really hope I haven't confused the two... Edit: I have. Original post updated.
You are right, it is a language design choice but not a feature in itself. for example calling JavaScript more feature rich than Typescript isn't correct, because the latter at least offers you warnings when your types don't match.
Small note, during the introduction you make a distinction between imperative, functional and object-oriented programming. Sure in some senses there is a difference but concider that there is not a well defined line between these. Doing stuff in iside the IO monad is imperative. And even though probbably a bit tedious you could do object-oriented functional programs as well. Coding in a functional way is even preffered in some cases in languages such as C as for example gcc will optimize these cases rather well! Best of luck with your book! 
that's just `($)`. λ&gt; :t ($) ($) :: (a -&gt; b) -&gt; a -&gt; b
Because you can't make a `Functor` (capital F!) instance like that. Try it, it won't type check. `(,)` accepts two arguments, and `Functor` operates on things which have only one type argument, e.g., `(,) a`. This is what I mean by the intuitive functor (small f) operating on both is not a `Functor`, hence, the controversy over "blessing" the unintuitive (but valid) `_2`. For what it's worth, Haskell usually calls the two argument version of Functor `Bifunctor`, and it has `bimap` ([see here](https://hackage.haskell.org/package/base-4.9.1.0/docs/Data-Bifunctor.html#v:bimap)) which behaves as you describe.
And that's it. I published the half of my book online. https://www.reddit.com/r/haskell/comments/6ck72h/functional_design_and_architecture/
So there is no other reason but typeclasses are not variadic?
Finally got some minutes to get back to this. Thank you for the exhaustive answer. So if I understood correctly, `len` has some special definition which says that it is always positive and LH relies on that instead of any inductive property?
Then all the modules need to know about each other, and your "world type" needs to list all the constraints. So it's not modular anymore? (also all the typeclasses need to return Maybe a)
Whew...I'd love to see someone try to teach *that* to a Haskell beginner on their way to monads.