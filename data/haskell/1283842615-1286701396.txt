With any luck the 64-bit codegen will work on OSX too!
do you have any interesting links about the new inliner (or at least a list of pros/cons)?
&gt; LALR parsers are constructed at compile time (unlike parsec) How is that done? Do you mean they are compiled into some efficient data structure the first time they are evaluated, or does that use some template haskell magic? **[edit]** After looking at the documentation, both options are available (at runtime and template haskell). Impressive! I guess the main difference with parsec from a programmer point of view is that (if it is really like happy) it allows for left recursive grammars, which you probably meant by "you write happy-like grammars". Am I right?
You are correct. :)
What new type checker?
I'll just throw in some keywords, as the details are tricky. Here goes: codata, corecursion/coinduction, and maybe bisimulation. That is, you *can* write down the semantics if it produces codata. You can then compare results with an equivalence relation which itself is also codata.
As you said, operational semantics are the perfect tool for reasoning about interpreters. If you want to analyze a compiler (that is, from one language to another one), denotational semantics will be more suited. For example, ⊥ is a first-class citizen, so you can prove that ⦃p⦄\_RM = ⊥ ⇒ ⦃p⦄\_BF = ⊥.
I personally think that the installing experience with HP is pretty decent; what I'm still looking for is some cabal install --global function to install it equivalent to a system-wide package (this should be then the same result like the .pkg's for apps proposed by chak in first post) What **really worries** me is the inherent complexity in getting a GUI working in Haskell on OS X (and Windows). I think Linux is much better there, as GTK+ is well adapted to Linux, more-or-less working on Windows and a pain in OS X. In this regard I'm highly impressed by the [Leksah](http://www.leksah.org/) people which managed to build a .app which is kind-of decent. I'd highly recommend leaning towards [Qt](http://qt.nokia.com/) but didn't manage to get qtHaskell running although trying hard -- I used it extensively with C++ and it's a breeze. It would be strategically wise as it's the only cross-plattform toolkit that seems to be accepted and working, License is LGPL (only wxWidgets is "easier" from a legal standpoint) and Nokia/Intel invest so that cross-plattform compatibililty is there... implementation wise I was already looking at [PySide](http://www.pyside.org/) to see wether one can just jump the bandwagon of their bindings generator...
you can write an interpreter with a bounded number of computation steps (that returns Nothing when it reaches 0) and then prove things about such interpreters by using infinite limits of increasing number of steps. But the canonical way is to use natural semantics encoded as a relation (relations don't have to be "terminating"). proving things about compilers is hard. it's extremely hard when either source or destination language isn't just a toy. if both, source and target, languages aren't toys - you better hope your name is Xavier. The problem is amount of work it takes, it's huge. I can't imagine doing it in Agda (iirc it only supports manual proofs). proof automation is crucial in the beginning. in the end, the hardest thing is an automation that doesn't take ages searching for proofs. I suggest you take a look at this: http://pauillac.inria.fr/~xleroy/courses/Marktoberdorf-2009/ if you insist on using agda you probably won't care for the proofs, but there's still nice info on the semantics encoding. edit: looks like X.Leroy made available another materials: http://pauillac.inria.fr/~xleroy/courses/Eugene-2010/
You may find "Operational Semantics Using the Partiality Monad" by Nils Anders Danielsson (University of Nottingham) useful to look over. http://www.cs.nott.ac.uk/~nad/publications/danielsson-dtp2010-talk.pdf
I've heard about codata/corecursion, but I've never found a good tutorial for the subject. Can you point me to any?
"cabal install" is not a package manager. For extra libraries it will serve but you cannot let "cabal install" upgrade or update an installed library without being an expert. Today's scenario: Install HP and upgrade regex-posix to 0.94.4. regex-compat is still compiled against the old HP version of regex-posix. Recompiling regex-compat fixes this but now "ghc-pkg check" shows haskell-platform is "broken". Luckily this is just a placeholder and no code is broken, but in general one has to recompile everything above an updated package and "cabal install" can not even do this. God help you if you touch anything that the "ghc" package depends on. Linux distributions have package managers that can cope with this. Apple Mac OS X has pkg/mpkg/distribution packages that I have never seen used for these scenarios -- the philosophy is too different. Linux makes hard things simple and Apple makes only simple things possible.
Yup.. I just redid this for 6.12.3. ghc from source then haskell platform from source.
And we can move these discussion to http://www.haskell.org/haskellwiki/Mac_OS_X_Strike_Force where they will have more life.
And we can move these discussion to http://www.haskell.org/haskellwiki/Mac_OS_X_Strike_Force where they will have more life.
And we can move these discussion to http://www.haskell.org/haskellwiki/Mac_OS_X_Strike_Force where they will have more life.
Would you post that offer to have someone take over on http://www.haskell.org/haskellwiki/Mac_OS_X_Strike_Force ?
I'm doing exactly this for the language http://liyang.hu/pub-model.xhtml using the technique described in http://liyang.hu/pub-cccctc.xhtml . Using coinduction for bisimulation, so not restricted to terminating terms. Thesis due in a month.
Hmm, nothing small and tutorial-like pops into my mind atm. I've mostly learned about it reading Yves Bertot &amp; Pierre Castéran: *Interactive Theorem Proving and Program Development* and by playing around with coq. You may also find http://www.jucs.org/jucs_10_7/total_functional_programming interesting. There, the concepts of codata and corecursion are introduced in the context of a simple language without dependent types. 
You may want to look at [ Coinductive big-step operational semantics](http://pauillac.inria.fr/~xleroy/bibrefs/Leroy-Grall-coindsem.html) from Xavier Leroy and Hervé Grall. I think this is exactly what you're looking for: coinduction to model both terminating and non-terminating programs. However, as pointed out by Paczesiowa below, this is only useful if you need to reason about properties of non-terminating programs. If not, just model the reduction relation as an inductive relation and prove that a if *p -R1-&gt; v* then *|p| -R2-&gt; |v|* where *-R1-&gt;* is the register machine reduction relation, *-R2-&gt;* that of brainfuck and *| |* your compilation function.
For that kind of compilers (from a very simple language to a very simple language) that may be OK. See [A type-correct, stack-safe, provably correct, expression compiler in Epigram](http://lambda-the-ultimate.org/node/1631) for instance.
http://www.haskell.org/haskellwiki/Simonpj/Talk:OutsideIn
dead link :( do you have a copy? edit: nvm, found it.
The contest is [here](http://ai-contest.com/).
I have been wondering whether someone could write a wrapper library in Haskell where you can transparently choose between Haskell-bindings for wxWidgets, Qt, Gtk, Cocoa, Win32- etc. So that you could write your UI once and pick the Haskell-binding which fits best.
I want as many Haskell entries as possible this time!
I attended the lectures that Xavier gave. Make sure you look at the slides on that page (Non-obvious link under the TOC). The section "Notions of Semantic Preservation" describe how to say what "correctness" means. The short answer: deterministic forward simulation.
"GTK-OSX is provided under the terms of the GNU General Public License, Version 2" Are they serious about GPL opposed to LGPL?
Thanks!
The package is probably going to evolve during the contest. If you see room for improvements, fork away (and please send me a pull request with your changes)!
I didn't notice any mention of Haskell on the site - is Haskell already supported?
I have a sneaking suspicion that at this point, subtype constraints would make certain aspects far simpler -- in particular by allowing many more expressions to possess a principal type. On the other hand, they'd probably introduce far more complexity than they eliminated, and would thoroughly change the character of the language as well...
It was last year (GHC 6.8.2!) and certainly will be this year (hopefully newer). Since the contest doesn't even really open until Friday, *no* language is officially supported yet. :)
I believe it was rewritten because the old inliner choked on the kind of Core that stream fusion code was generating: it generated code that worked, but it wasn't fast, due to e.g. missed unboxing opportunities and other shortcomings. The new inliner does a much better job. It's not clear that it has any disadvantages.
 import Text.ParserCombinators.Parsec import Control.Monad.State showPrompt :: IO a showPrompt = undefined type WTF = Int type MyState = String parser :: Parser WTF parser = undefined evalInput :: MyState -&gt; WTF -&gt; IO MyState evalInput = undefined state0 :: MyState state0 = undefined type Repl a = StateT MyState IO a repl :: Repl () repl = do liftIO showPrompt input &lt;- liftIO getLine case parse parser "" input of Left e -&gt; liftIO $ putStrLn $ "Error: " ++ show e Right d -&gt; do state &lt;- get newState &lt;- liftIO $ evalInput state d put newState repl main = runStateT repl state0 not tested. and that output doesn't really work in your version
Thanks. This just feels right and beautiful. I will try it out and do some more reading on Control.Monad.State.
I wouldn't call it right or beautiful, it's called the haskell way.
Can't wait to implement some of the more [advanced strategies](http://ai-contest.com/advanced_strategy_guide.php "derp")!
please communicate this properly so distros don't think there's some reason to hang on to the 6.x series...by that i mean make it clear that the major version bump doesn't signify any greater risk than any other haskell release
Thanks everyone for your helpful replies -- I'll have something to read in my spare time for weeks to come:) If anyone is interested in the actual RM -&gt; BF compiler, I've posted [part 2](http://gergo.erdi.hu/blog/2010-09-07-from_register_machines_to_brainfuck,_part_2/) to my blog.
So where is it? :)
Whaaaat ? Is there an ongoing contest already ? Count me in as a haskeller (still a weak one) again ;-)
&lt;JOKE&gt;So there will not be LLVM support in 6.x? Features always gets delayed.&lt;/JOKE&gt;
http://citeseerx.ksu.edu.sa/viewdoc/summary?doi=10.1.1.105.4086
I think the explanation of why all the inlining in filter is a saving could be clearer. So we use the transformation to enable filter to be inlined. Where exactly do the savings accrue? What is their absolute or relative significance? You mention we avoid the overhead of storing `p` in the closure `filter p xs` but this is a pretty insignificant difference compared to creating the closure `go xs`. It's just one word extra and one more function parameter. Almost certainly the greater effect is that we get a call to a known function (the predicate) rather than an indirect call through a pointer. Inlining the predicate completely only saves the (small) function call overhead. I'm not convinced that using the INLINE hammer is necessary or a good idea in the `filter` example. GHC will already inline it if it thinks it is a good idea. If you do not use INLINE then GHC may decide to inline the wrapper but not the worker (e.g. if it's large or there's little specialisation benefit). That'd be perfectly reasonable. By hitting it with the INLINE hammer you're always inlining the worker too even if that's not really of much benefit.
Distros should provide the [Haskell Platform specification](http://code.haskell.org/haskell-platform/haskell-platform.cabal). Don't try to guess what is stable, just follow what we specify is stable.
I see nothing wrong with the way you're doing it in your original post. There's nothing beautiful about that State Monad solution, and that's just excessive for an example this simple. I'd still recommend trying it as a learning exercise, of course. One thing though, you can replace do { newState &lt;- evalInput state d; return newState; } with evalInput state d 
Sounds like a good exercise; you might also want to look at [Shellac](http://www.cs.princeton.edu/~rdockins/shellac/home/), however - it's really good.
I do this often enough that I find myself defining this over and over: foreverFrom x m = do {x' &lt;- m x; foreverFrom x' m;} Used as foreverFrom initialState $ \state -&gt; do newState &lt;- stuffWithState return newState
And thanks to Vincent today is a good day for Haskell cryptography. There's Crypto-API, certiciate, cryptocipher, and asn1-data. WOW!
You might also want to add error handling to that monad stack e.g. something like newtype StateErrorIO e s a = StateErrorIO { runSEI :: (StateT s (ErrorT e IO) a) } deriving (Monad, MonadIO, MonadState s, MonadError e) type MyError = ... type Repl a = StateErrorIO MyError MyState a I have a couple of blog posts on the state monad which you might find useful: http://mvanier.livejournal.com/5406.html http://mvanier.livejournal.com/5846.html 
I'm in the habit of trying to write my code like this (absent the `INLINE`) in general. Not only is it likely to be more efficient, but not passing around a redundant argument just feels cleaner. 
&gt; In the Haskell space, Gtk2hs has always had problems on Mac OS X (and I don't see gtk-osx helping so far) and wxWidgets seems mostly unmaintained. On Twitter, njbartlett suggested that QtHaskell would be worth looking at. Of these three options, the known problems with Gtk2hs seem to be the most daunting. I don't know anything about QtHaskell. So maybe the best options at this point are: * Investigate QtHaskell, particularly for cross-platform support and native/pretty look; and * Inspire &amp; round up help for Jeremy to maintain wxHaskell. 
I found [A Tutorial on (Co)Algebras and (Co)Induction](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.37.1418) a very pleasant introduction to the basics.
You can definitely do this for simple GUIs. The problem is that you are limited to the lowest common denominator of all the supported GUI frameworks. Moreover, GUIs on some platforms are expected to follow a set of UI guidelines (certain menu points need to be present, certain behaviours are expected, and so on). This is especially important on the Mac, as common Mac applications place a strong emphasis on usability and Mac users, as a result, have come to expect that high standard, including conformance to the many UI standards set by Apple.
I think there's a backside though -- testing becomes harder as you can't test the inner "go" functions directly.
I suggest you see the [LambdaCalculator](http://hackage.haskell.org/package/LambdaCalculator) for an example.
You are right, it's not straightforward. Eclipse's SWT has done a decent job at this. So i don't see why such a project couldn't be succesful.
This would better be named iterateM_ wouldn't it? 
When defining monad stacks, lifting becomes a pain. I suggest defining it in a way similar to this: newtype MyMonad a = MyMonad { runMyMonad :: StateT s (ReaderT r (....)) a } liftNameOfStateLayer = MyMonad liftNameOfReaderLayer = MyMonad . lift and such for each layer. The name isn't necessarily "state" or "layer" but the name of the state you want to be changing, or the environment you want available. Then instead of "liftState $ .." and "liftReader $ .." you can use named lifters, you can have multiple StateT's or any other transformer of the same type, etc. 
in addition, not being able to discover the type signature of the go functions like you would a top level function from ghci, is a little annoying too.
I don't really understand either of these complaints. The go function is an implementation detail of the outer function. You test it by testing the outer function, just as you would if it had no inner go function. You discover the type of the outer function, just as you would if it had no inner go function.
It'd probably be `flip iterateM_` actually. That was my first thought as well, but I reconsidered after thinking about what the implied `iterateM` would do. It could also be defined with something like `foldM_ (flip ($)) x (repeat m)` but that's not really helpful. I've written several variations on an `unfoldM` function, and would be inclined to use something like that in this case, myself.
Not to mention, the inner function has the same type as the outer function, just absent an argument or two.
Showed up in the nick of time! Awesome.
it's not specific to this case, I just have a general wish to be able to query the type signature of functions in where clauses
Making darcs available as a hackage library has been really helpful in letting the community propose cool features as standalone tools rather than having to bundle everything into Darcs. We're also starting to see an ecosystem developing, little bits and pieces at a time. Darcswatch (now using Darcs library), DPM, ipatch, darcs-fastconvert...
does it have any advantages over using darcs-git.py?
i'm also interested in this sort of thing, particularly how you go about proving correctness of compilers like this where are you going to post your thesis?
&gt; Almost certainly the greater effect is that we get a call to a known function (the predicate) rather than an indirect call through a pointer. This is, in my experience, the big win of the static argument transformation. When you inline such that some function argument is specialized, GHC's optimizer can do a much better job. I ran into this quite a bit in vector-algorithms, and if you look at the source, you'll see that almost every algorithm uses this transformation several times, and for this reason. I had a chat a while back with one of the GHC devs (Max Bolingbroke, I think) about getting GHC to do this itself, but I think it still isn't in. For ordinary arguments, it's not that big a savings, and in fact is actually a detriment if you only have one static argument, as I recall. But it tends to be a big win if the argument is a statically known function, which allows further optimizations. I guess a satisfying heuristic still hasn't been found, though.
To be honest, I think the improvements that bos mentioned will garner more general interest than the type checker overhaul. But yes, a worthy 7.0 indeed!
I've rewritten most of the explanation now. Let me know if you find it clearer. I ran a number of benchmarks to try to tease apart where the different performance gains come from. For example: inlining `even` into the body of `test` gives us an additional 7% improvement over just calling `even` from within the body. This is a much smaller gain than replacing the call to an unknown function `p` with a call to a known function `even`. I also tried to be more nuanced in describing when to use inlining. I would like to talk about reading Core at this point but I haven't introduce Core in the post and doing so would likely make the post much bigger. I plan to write about Core some time in the future. That being said adding `INLINE` pragmas does help more often than I would like. As you know they're used quite frequently in bytestring, text and, I believe, vector. Perhaps the inlining thresholds have been tuned using a benchmarking set (e.g. nofib) that's sufficiently different from those libraries.
A good IDE lets you do this.
Yes much better! :-) I know INLINE is used a lot in bytestring etc, I'm not totally convinced they're all needed. I don't think we've ever properly checked.
Yes, there's probably some of both. The same applies to the `UNPACK` pragma, which is sometimes applied where it has no effect (e.g. on polymorphic/function fields). The next GHC version will warn in those cases. Unfortunately it isn't as easy to introduce a warning for unnecessary inlining as GHC can't really promise which functions it will generate unfoldings for in future versions.
The code is rather boring when you saw c++ version.
True, the code follows closely the C++ one. I'd like to change it a bit, probably use a Haskell library for the random numbers instead of the erand48 C function. If you have any idea, please submit a patch. And make it faster too.
So iPhone + Haskell = OK? Nice
The new stylesheets look great! Good work everyone!
Nice. RyanT5000 should be ecstatic right about now. I may pick up that Mac for iPhone development after all. I guess it served its purpose. It largely derailed any groundswell behind compiled Flash for iPhone, and then Adobe's own code quality issues have left it on life support on other platforms.
One can of course add type signatures in where clauses to document them when they get tricky as well :-)
It looks very nice, I like to see the more common web theming between haddock and haskell.org. Hackage could also really use a face lift. It will be much easier attracting new developers to the language with an integrated, consistent web experience. Thanks for all the hard work, it looks great. 
&gt; [...] , for instance to provide Failed to parse (Can't write to or create math output directory): O(1) &gt; mutable arrays. http://new-www.haskell.org/haskellwiki/Functional_programming EDIT: actually, it's all over the place. Just open the next semi-technical wiki page.
Hackage facelift coming soon: http://althack.org/hackage/hackage3.png
Agreed. It looks great!
And the color scheme for that will change to match the one being used on the new haddock and haskell.org pages.
There's only one problem I have with it: the contrast of the link colours against the background is a bit too low. The current #C4451D could be replaced by #C42F00 (which is completely saturated) making it easier to read, with minimal effect on the overall appearance. In the Haddock stylesheet, there's an even worse choice of #AB6954 for visited links, which against the grey background used for type signatures, while not the worst possible choice, is needlessly unclear. Rather than going closer to the background colour for visited links, one could make the text a bit darker (closer to ordinary text): #801E00 would work well there.
Still "as long as the resulting apps do not download any code" which seems fairly arbitrary. Why should I agree to let someone as wishy-washy and selfish as Apple decide what I can and can't do with my code? 
It's not in the least arbitrary. It has a specific purpose. Read "as long as the resulting apps do not download any code" as "as long as you do not attempt to circumvent the AppStore as the sole code distribution platform for iOS" (except for HTML5 executed in WebKit). Apple is a public company. Public companies must be selfish — they do have a contractual obligation to be.
Now, if only the Haskell iOS tool chain wasn't even worse than the Mac OS X one.
Excellent news! Thanks to everybody involved in this!
Exactly... we all know code is data is code, so no content whatsoever could ever be downloaded.
I think the damage has been done. Apple have made it abundantly clear that if you develop for their platform then they can - and will - whizz on your bonfire if it's in their interests, and they can get away with it. I have nothing against the app store itself, the problem is, it's the only one.
Still a pretty hostile place when Apple appears to want to treat customers like criminals. Is anyone doing Haskell work on Android?
I think it's backwards. Apple wants to protect its customers from criminals and sub-qualitiy applications. For developers it's unfortunate that Apples main customers are users.
I think this new trendy web 2.0 look is strictly worse than what it's replacing.
This is great, thanks everyone!
Why does that wiki page say Java has closures?
fonts are bytecode
Unless of course you're of questionable moral integrity and don't mind supporting the axis of evil jailbreakers. Not nearly as large a community as the regular app store, but as a result the Cydia store is way emptier and releases on it are a lot more visible! Some people have made some really good money there :)
The Android UI has to be in Java. But there is a toolkit for native code on Android. It might be wiser to go with Scala, for which there are some tools available.
Looks awesome.. I am replacing my bookmark with the new site..
Agreed. I'll wait for the compiler to figure out how to optimize this.
Technically, every language with first-class objects has closures. And Java even has some moderately non-verbose syntax for them, in the form of anonymous classes. ...that doesn't make Java functional in the same sense that my driving license doesn't make me a car mechanic, of course.
The internets.
Judging from http://www.pragprog.com/titles/btlang/source_code … Had a glance at the Haskell source code. It looks terrible.
Apple do seem to consider those customers who want to 'jailbreak' the hardware they've bought as criminals.
 class (JSON v) =&gt; DBTy a k v | a -&gt; k, a -&gt; v where == class (JSON v) =&gt; DBTy a k v | a -&gt; k v where
"Static typing" is an example of something macros can't really *practically* do. At least not with any more ease than just implementing a compiler/type-checker from scratch.
There's also http://hackage.haskell.org/packages/archive/generator/0.5.4/doc/html/Control-Monad-Generator.html that me and yairchu started (But later Yairchu continued to maintain alone). Also meant to implement "yield" on top of continuations.
Licenses don't apply to copyright owners. So if some guy releases two packages that interact in conflicting ways, it is still legal - because the copyright owner isn't bound by the license. Additionally, building executables from BSD-using-GPL3 is legal, only *distributing* them isn't... I hope cabal only warns about and doesn't completely disallow conflicts. 
Not inlining recursive definitions is one thing, but why can't GHC do one of: * Automate the static argument transformation when possible * Specialize a definition for a particular use for some of the given arguments (This is similar to inlining, but there's no problem applying this to recursive definitions)
The static argument transformation is not always a win. I presented one case where it almost always works (higher-order functions) but if applied to a monomorphic function with no higher-order arguments it's often a loss due to the cost of allocating a closure for the worker function. I believe the reason the transformation isn't in GHC at the moment is that they haven't been able to find a good enough set of heuristics yet. As for your second point, I also like automatic specialization of polymorphic functions. A specialized version would be generated at the call site. This is very similar to how C++ templates work. Taken one step further we could also specialize data types at the call site to create and unboxed version of e.g. [Int]. 
[Here](http://github.com/jaspervdj/planet-wars-haskell) is Haskell a starter package written by [some Haskellers](http://github.com/jaspervdj/planet-wars-haskell/network/members).
Or you can use this linkcode free link: http://www.amazon.com/gp/product/0521513383
don't the linkcode's get auto added by reddit so they can help support themselves at no loss to you in any way whatsoever?
Right, but we don't *want* to support Reddit. It's not like it's any use whatsoever.
Judging from the cover I bet there's going to be a sudoku solver
Oh cool! You learn something new every day. :)
I loved the fake shock at the lack of function execution.
_What the \_ is happening here?_
If leksah would run from dmenu then I'd be more likely to use it.
There was a problem launching from dmenu that has been fixed in the leksah-head darcs version. See the "Development" section at the end of the download page for details on building it. http://leksah.org/download.html
Of course I was confused, I'm a horrible Haskell programmer and I did this staying up way too late at night. :)
Even if you are not interested in the Coq derivation this paper is interesting for it's clear presentation of the STG machine and core language.
Works like a charm - thanks!
I found this http://assets.cambridge.org/assets/bookpageresult.jsf?conversationId=238085 to get an idea of the book contents
Yesod really seems to do things the way I like, but I have one big issue. Do you seriously need to restart it every single time you make a change? Is there no way to have it automatically recompile and reload the files you change?
That's a great example of something I would like to get into Yesod 1.0. Turbinado had such a feature, and I've heard that Snap is working on it as well. Technically, it's nothing to do with Yesod: it's a question of setting up SimpleServer correctly. I'll do some more research into getting this working, thanks for the reminder. Edit: One of the big features of Hamlet 0.5 was that most template changes don't require a recompile, which can really speed up development time. However, actually code changes all still require a manual recompile.
michael, congrats on the continuing great work on this project its great that you have placed an emphasis on high-quality docs. just a general comment...i keep trying to employ either yesod or snap in place of perldancer...but i still find haskell getting in the way instead of helping me. i still find yesod to be...how can i say...far away from the model. i suppose i see web development as requiring rapid development, and the best toolkits enable this without getting in the way too much. with the current generation of haskell toolkits, i still find myself focusing more on the haskell than the problem domain itself i can't speak for other people, but i would prefer abstractions that sacrifice performance in exchange for rapid development. its most likely that yesod and snap are fast enough...focusing further on performance serves no purpose, most web apps are not gated on the performance of the toolkit but on the performance of the storage layer. anyway, i know yesod is a work in progress, keep up the good work
it still doesn't allow for providing arguments in a different order or omitting some of the arguments that have default values. if you want nice code at the call site - you can always use comments: main = renderBox {-XPos-} 2 {-YPos-} 4 {-Width-} 50 {-Height-} 60 edit: or meaningful names: main = renderBox xpos ypos width height where xpos = 2 ypos = 4 width = 50 height = 60 solution 1 (though it has high starting cost - but probably nothing that TH couldn't solve): http://www.cse.chalmers.se/~hallgren/Thesis/main.html#parameters_impl solution 2 ("Oleg already did it"): http://okmij.org/ftp/Haskell/keyword-arguments.lhs
Is your sending address the same as the one you used to subscribe?
&gt; Additionally, building executables from BSD-using-GPL3 is legal, only distributing them isn't... It's perfectly acceptable to build and distribute binaries that use BSD-using-GPL code, because BSD is GPL-compatible. But if somebody comes along and modifies the BSD-licensed code, they must also release their changes under a GPL-compatible license, unless those changes remove the GPL'd dependencies. Depending on the exact nature of the dependencies, removing them can vary anywhere from "no big deal" to "for all practical intents and purposes, this BSD-licensed code is under the GPL". The problem is that when BSD-licensed code has a GPL'd dependency, it puts further restrictions on what people normally expect to be able to do with BSD code. So a warning can be helpful.
Can you give me a specific example of a problem that is difficult to solve with Yesod but easy to solve with &lt;insert other framework here&gt;? It would give me a good idea of how to try and improve things. I have two thoughts for why Yesod would seem to have "Haskell baggage" so to say: * If you're used to dealing with Perl, Haskell will seem foreign, and therefore the Haskell pieces will be more irritating. * Yesod relies heavily on the Haskell type system, so to some extent Haskell creeps into Yesod quite a bit.
Yes, at least I think so. I subscribed via google groups and tried sending using the google groups web interface too, but even this did not work. The original sender address of the bounce message is the same too, plus I don't have problems with any other mailing list (where only members are allowed to send) using the same account... I tried resubscribing, just in case i did something funny, but still same problem. Even tried with ...@gmail.com and ...@googlemail.com mail domain before a while, though 
Hmm, it would be nice if we could use Template Haskell to automate this with something like `funcName arg1 $(funtype arg2)` (where arg1 is already acceptably typed), but I don't see a convenient way to do that without wrapping the whole function with TH, which is a bit extreme. (Seems like I always hit this problem when I try to use TH...)
Haskellers, almost by definition, would use Haskell instead of R. In addition to the snark which I could not resist, does anyone know enough about both languages to know how feasible some sort of library in Haskell would be to replace R? In all seriousness one wonders if a dedicated R-like language is something not really necessary in a world with increasingly flexible languages and increasing language integration (like the JVM family). I would be happy to hear about why this is not true of R.
The advantage of R over Haskell, at present, is that it's highly domain-specific. That is, there's already a great volume of libraries for doing work in statistics, and so on. Not to mention that sweet frontend that they have. I don't remember what it's named, but I have it installed.
The vision of Data Parallel Haskell would give efficient and concise ways to build the kind of computations that statisticians needs. That vision's day is dawning but not quite here.
i can't really cite anything specific, other than, as you point out, yesod leans heavily on the type system. i may also have a mental barrier to surmount here - admittedly the models i am comparing are from languages that are duck-typed. historically in these environments, the typing i really care about happens in the database anyway, once again, i do like yesod, please keep up the good work
pre-ordered. this is exactly the sort of material i am still willing to buy in printed form
&gt; "Oleg already did it" This should be a T-shirt.
I use both languages extensively. As pointed out by many others, the main thing R has going for it is CRAN, the vast repository of packages for R. It would be hard to duplicate this-- often the packages are written by the original inventors of statistical methods, and re-implementing them would require a lot of specialized knowledge. Aside from that R has some other niceties for dealing with data. It handles missing data gracefully, as well as categorical and ordinal data. It has a syntax for specifying formulas (e.g. cancer ~ age + smoking). It can define types dynamically: you can read in a data file, without defining the types for its contents ahead of time. These are just a few off of the top of my head. Still, you can do a lot in Haskell, sometimes better than you can do in R. My current research project involves doing most of the heavy lifting in Haskell. Shameless plug: check out the development version of hs-linear-algebra, now with (partial) LAPACK bindings (at http://github.com/patperry/hs-linear-algebra ). 
&gt; This should be a T-shirt. I dunno. If it were true that it should, in fact, be a T-shirt then, by definition, Oleg would already have one.
I like it. Why not follow google into the future but can I ask how this differs from say the autocompletion in Leksah? (Serious question; I don't see the differences, is it that you could search for type sigs too?)
There are a few things that R has that are very appealing to users. 1) CRAN. Both for libraries and data. Models are frequently written in other languages, such as fortran and c. Installing libraries works really well. 2) The data.frame object has a lot of functionality built around it. Still not that nice to use. There are other "domain specific objects" that provide a common target for extension writers. 3) The domain specific modeling language. Haskell could do most of this with a combinator library. 4) Support for graphics is very good in my experience. 5) Good documentation. The language itself is *not* that much fun to use and tracking down bugs in a library is scary and hard for the typical user. A lot of modeling libraries are not production quality. There has been some work done on parallelization but I am not up to date on that. I would love to see a wrapper that covered 50% of CRAN and allowed you to do modeling with dph and repa support and nice graphics. Scripts in R would not have to be readable in the new language. Parallelism alone would attract users. Type safety and elegance would be icing. Dare I suggest a name? OK, H. 
I'll give it a shot, cf [my blog comments](http://docs.yesodweb.com/blog/yammer-screencast): 1. you want to let users customize the app's ui, say by installing customised stylesheet or templates in ~/.app/skin/ 2. you want the app to recognise plugins at startup or even while running, preferably without you having to list them, and definitely without having to recompile I'll stop there until I've had more experience with yesod, but the above are things I was used to in the python or ruby worlds. They're probably just going against the grain of haskell ? 
Just to provide an alternate view point to the last point, I am in the exact opposite boat. The amount of time I spend on initial development is insignificant, and popular web frameworks seem entirely focused on making this smallest part of the job easier/faster. I do care about performance as I do write web apps that actually do more than shuffle things in and out of a DB. The very reason I am interested in frameworks that make good use of a strong statically typed language is because all the popular frameworks are designed for the "I just want to get something up and running quick" crowd.
+1 to "Oleg already did it"
You can now get it on a tshirt: http://haskell.spreadshirt.com/ (zero commission, until we can redirect donations to haskell.org)
Haha.. this is awesome :)
If you have ideas for other fun/humorous Haskell slogans, post here, and I'll add them. 
-fcontext-stack=1024
The way I (and I believe most people) use R is that I start with some data in a particular format and iteratively munge it. Sometimes I backtrack because I didn't get good results with a particular pass. One common pattern is leaving a lot of randomly structured breadcrumb files around as the data is being sliced and analyzed along the way. It's extremely interactive and supports discovery of methods, rather than just running known canned models. That said, it would be wonderful to have that sort interactivity (even non-statistically) with Haskell, then with the ability to compile down to hairy-fast code once the user has figured out how to get from A to B.
GAP has the same problem. The underlying programming language in GAP is not as nice as Haskell, but the library is practically irreplaceable. It's really quite annoying that the originators of computer algebra systems almost always feel the need to invent a programming language along with the library, and they end up with an awesome library mired in a mediocre language, often with poor ability to interface with code written in other languages nicely.
&gt; and I'll add them. ... in the type system.
&gt; People learning Haskell often have the idea that evaluation order is not important because it does not affect the calculated result. It is no coincidence that beginners end up floundering around with space leaks that they do not understand.
The monad law haiku: Monad axioms: Kleisli composition forms a category. 
Is Lesksah's autocompletion context sensitive?
CAS writers usually are mathematicians, and the roots of most big CA systems go way back - you can clearly see that GAP was written when Pascal was big (in education, at least). That said, there is a lot that GAP gets right, especially with changing algorithms on the fly when new properties of an algebraic structure are discovered. Also the whole machinery of properties that can be checked whether they have been computed already is pretty cool (a.k.a. Maybe Bool :-) I've tried mimicking some of this stuff in Haskell with limited success - it's kinda clunky but works after a fashion. I should take that project up again (damn Minecraft!)
But those libraries are written in C/C++/Fortran?
Yesod looks really cool. But I feel like Hamlet is great for prototyping or usage, when you are designer and developer. Sometimes one has to work with designers who have no clue about Hamlet and can produce only HTML with their beloved Dreamweaver... It would be great, to have Hamlet like support for HTML and XML(since sometimes data need to be send in XML) too in yesod.
I think you can more or less paste straight HTML into a hamlet template. There may be some limitations, like javascript with $ needs quoting.
fmap fmap fmap
I use SHJS to highlight Haskell in my blog. http://nix-tips.blogspot.com/2010/09/javascript-highlighter-for-haskell-code.html
Try [nabble](http://old.nabble.com/Haskell---Haskell-Cafe-f13132.html)?
Anybody can explain why they are fun? :P
{-# INLINE main #-} or {-# LANGUAGE C #-}
This is a great video demonstrating the tool, thanks. I'd like to point out that HaRe is still being maintained by the HaRe team, and has recently been put onto hackage: http://hackage.haskell.org/package/HaRe
By the way, regarding the automatic recompile, you might want to see [wai-handler-devel](http://hackage.haskell.org/package/wai-handler-devel) and the [accompanying blog post](http://docs.yesodweb.com/blog/wai-handler-devel/).
It's great software, with one fundamental flaw: It isn't based on haskell-src-extensions. It's hard to find pure Haskell 98 nowadays.
Haskell: cuz who wants something dysfunctional?
Porting hare to Haskell 2010 is a huge undertaking; it would require us to rewrite the main transformation engine, together with traversal strategies and not to mention all of the refactorings themselves. Unfortunately, haskell-src-extensions doesn't provide us with static semantics, and we also require comments and layout information to preserve the appearance of programs. The only real solution is to port HaRe to the GHC-API. Simon Thompson and I have been discussing this for a long time, and I think the only way to approach this is with the help of the community, once we have a tractable porting plan in place.
Haskell... avoiding success since 1990 fucn Haskell programmers do it... at most once, and then cache the result I see space leaks 
Are you saying you didn't subscribe via majordomo? http://haskell.org/mailman/listinfo/haskell-cafe
&gt; warm fuzzy thing Advantages: It's damn subtle. :-)
Okay, totally OT, I know, but I'm struck... I find it so depressing to read about this "very old language" which was first released in 1990, the year van der Jeugt was born, and I turned 20. ugh. And here I am just learning it! And then I watch a video tutorial on Leksah, and that guy looks like he's twelve (no offense intended. I promise, someday you'll take it as a compliment). Yikes! I guess it's good though. If there are lots of young folks fired up about haskell and making a difference, then perhaps success will come despite those best efforts otherwise.
Referring to a Haskell as an old language confuses me, too. Is Java “old”?
This makes no sense. See Felleisen for what expressiveness means.
I was more amused by the fact that Haskell would come out as "fast, rather than expressive" in any conceivable experiment than I was taking the author seriously.
Great language shootout. Avg. gzip code size vs speed. The Haskell entries are *faST*
Comparing the lengths of programs optimized only for speed is a bit suspect. If a language lets you double the length of a program for a 5% performance improvement, the shootout will gladly accept the longer one. It would be interesting to see a version of the shootout that kept the whole Pareto boundary for each language -- any version shorter than all the ones faster than it or faster than all the ones shorter than it. That would give a much better sense of the speed/expressiveness tradeoffs each language makes. 
purely lazy 
I think it would be informative if we could enter a new language: haskellShort. Secretly (but not very) the same language as haskell, but with the programs optimised for length rather than speed.
yes, because we need more pointless, unreadable code. how about sending beautiful code? the kind of code that first comes to your mind, doesn't care about efficiency and makes you go "awww" like a picture of a small kitty?
twelve year olds understanding monads are pretty motivational, it helps closing reddit and focusing on code.
Sweet post!
The [original content](http://www.youtube.com/watch?v=4I7VZV7elnY), including a proper link to the project home page in the description.
As the maintainer for the EclipseFP IDE, I would like to offer my help. We already use the GHC API in the scion library, and adding refactoring support through HaRe to the IDE would be a huge bonus.
I don't like it, it completely misses the structure of the format and tries to force semantic validation on the parsing phase. why not something like this (but I'd prefer postponing validate step after parsing) {-# LANGUAGE NoMonomorphismRestriction #-} import Text.ParserCombinators.Parsec import Control.Applicative hiding ((&lt;|&gt;), many) import Data.List whitespace = skipMany $ char ' ' lexeme p = p &lt;* whitespace data Arg = Arg { key :: String , value :: String } deriving Show keyP = lexeme $ many1 letter valueP = lexeme $ char '\"' *&gt; many1 (noneOf "\"") &lt;* char '\"' argP = (\k _ v -&gt; Arg k v) &lt;$&gt; keyP &lt;*&gt; lexeme (char '=') &lt;*&gt; valueP argsP = many argP data ImageTagProperties = ImageTagProperties { src :: String , width :: Maybe Integer , height :: Maybe Integer } deriving Show lookupArg :: String -&gt; [Arg] -&gt; Maybe String lookupArg k args = value &lt;$&gt; find ((k==) . key) args imageTagProperties = argsP &gt;&gt;= validate validate arguments = case lookup' "src" of Nothing -&gt; fail "no src arg" Just x -&gt; return $ ImageTagProperties x (lookupInt "width") (lookupInt "height") where lookup' x = lookupArg x arguments lookupInt x = read &lt;$&gt; lookup' x 
Not sure what you mean. Java and Haskell are nowhere near each other. If you're talking about the rainbow table at the bottom, it's talking about pareto-optimal languages arguing that they all sit on the pareto frontier when considering performance and expressiveness. But I'm not sure how Java got on that list - it's not on the pareto frontier - it's dominated by Haskell and GCC on the graph. 
What's important to the working programmer is a) What fraction of the code is obviously correct b) What classes of programming errors are caught by the compiler; I'm not sure any formal definition of expressiveness can capture a), since it's a pretty woolly idea, but I think one everyone who has come to Haskell from other languages will recognise. 
thanks, that worked
do it in a monad and remain pure
In case anyone is interested, I've started working on a new Haskell k-nucleotide in the shootout. I did the existing one and it's shamefully ugly code. Here's some work on writing it as beautiful Haskell-worthy code. It needs to be sped up a bit still. http://hip-to-be-square.com/~blackh/haskell/k-nucleotide.hs
Gotta love it when someone goes through the trouble to compile all of this data together into a nice visualization, and then slaps a big ol' caveat on it like "This is basically an artificial measure that won't accurately represent anything"
I remember I shaved off one character in my prime generation code at the expense of making the algorithm exponential. Edit: The trick was that instead of checking for the divisors `[2..n]` is is sufficient to recursively check only the prime divisors up to n: `(p n)`. Of course, without sharing, the recursive call cause an exponential blow up.
I agree, the thing being parsed is a series of key/value pairs, not a specific list of keys with attendant values.
**RI-DI-CU-LOUS** 
I would have to agree that it does try and force semantic validation into the parsing phase but, in my defense, this is my first foray into the world of parsers. Why is that really such a bad thing? I know that it happens to be very handy from my perspective. As for completely missing the structure of the format; well maybe I have, but I thought that I got all of the data out in a format that seemed nice at the time. How did I miss the structure of the format? I am trying to read over the code you provided here and perhaps what you mean will come to me.
Worth it.
&gt;Why is that really such a bad thing? For the very reason that you posted; you're sitting there screwing around with order in a place where it shouldn't matter. It also shows poor compositional breakdown, in my opinion. Much better to specify "here's how to parse an attribute" and "here's what attributes I expect on this tag" than to try to munge that into one function. Yes, at least you extracted the idea of "order irrelevance" but you still have a function doing both those things at once, and the need to abstract out "order irrelevance" fundamentally arises from that munging so I'm not inclined to award too many abstraction points for pulling an order-irrelevizer out of the resulting mess. :) If you continue down this path, as you try to push your technique further and further the complexity will continue to explode. The permutation is cute an' all, but even if Haskell is making expressing it easy enough it still will have performance issues pretty quickly if you encounter multiple layers of "unordered" stuff.
&gt; Why is that really such a bad thing? because it's not modular, it's hard and error-prone: * this time it was relatively simple to write a parser for it (you're lucky that some people wrote a paper about it and implemented it in Parsec library) * you still had to define two similar parsers (for src and width), but they have the same syntactical structure * there are things that are incredibly hard to parse (what if there was supposed to be 2x as many width params as src ones?), but they are absolutely trivial to implement as another validating phase - you just write a simple tree traversing function (which haskell excels at) that transforms one version of the tree (preferably with loose types - e.g. RoseTree (String,String)) to another one, with much more rigid structure. * writing custom validation you can generate much better error messages, while relying on parsec mechanism will collapse errors and you can get an error like "missing comma or you forgot src argument", where those errors should be treated differently * besides the usual source of errors resulting from doing two things at once, you have to carefully slap "try" everywhere, forgetting doing so can result in an error. * it's also probably slower, such parser will backtrack something crazy. * I don't know about iCal (I've only generated some, haven't parse any), but that html code should be parsed as html (or sgml), because that its syntactical structure. html parser should be available in libraries, you can reuse it (with your custom validation step).
For those who haven’t tried it yet, I suggest looking at the graphs when the vertical scale is a *linear* speed measure (either real time or CPU). The default logarithmic scale makes it look like there’s a lot of variation in speed across languages, but the reality is that there seem to be a small number of clearly separate tiers of language performance, and for most applications one language in a tier is probably going to be as good as any other. Do note that there seems to be a bug in the graph rendering with the linear vertical scale, though: the faster languages appear to be plotted far higher than they should be, so are even better relative to the slower languages than they first appear.
What I would like to see in the language shootout: For every language x, also have an entry called "idiomatic x". It's great to see Haskell doing fairly well, but I'm a little afraid it's all pretty meaningless as it won't be the kind of Haskell I would ever write.
This is shared by many (all?) OO approaches which permit non-abstract methods in classes which allow derived classes. Good candidates in Java are streams such as `java.io.InputStream` and its descendents. It's the reason why inheritance of behavior results in some loss of encapsulation.
So.. what is their argument for comparing *gzipped* source code? Repeating "public static void" twenty times is not bad for expressivity?
Gzipped? Why are they comparing length *when gzipped*?
I'd say that the total safetly is not possible as it would be termination problem. Consider class (Ord a, Num a) =&gt; Funny a where fun x = nofun (x - 1) nofun x | x &lt;= 0 = 0 | otherwise = fun x The use is safe. However IIRC it is not possible to detemine if recursive function terminates.
Looks like the haskell code gets worse with more cores?
http://en.wikipedia.org/wiki/Kolmogorov_complexity
So "expressiveness" is low kolmogorov complexity of source code. Makes some sense but I don't see why anyone would practically care about that kind of expressiveness as opposed to source code length. 
Whitespace.
&gt;At least not with any more ease than just implementing a compiler/type-checker from scratch. This would be a typical Schemer's approach.
That's true, but to be fair, the set of OO-capable programming languages under a niceness ordering is bounded below, by C++. Ugh.
You don't even really have to graph it linearly, you can see it in the summaries. Using common languages as the barriers, For [single core](http://shootout.alioth.debian.org/u32/which-programming-languages-are-fastest.php), there are, broadly, the Compiled Languages, from C++ to Lisp/Go, going from 1-4ish. There are the compiled scripting languages from Clojure to Lua or so, from 11 to 25-ish. And then there are the interpreted languages in the rest of the graph, from 30x slower than C to 90x slower than C, and the Ruby MRI outlier. The big exception in that analysis is the astounding presence of LuaJIT comfortably nestled in the compiled languages, and Javascript's creeping out of the "compiled scripting language" category and heading towards the compiled languages as well. (Please don't nail me too hard on the exact terms I'm using; for instance, Erlang is not normally considered a "scripting language" but in terms of compiling it in a lot of ways it has more to do with scripting languages than C++, for instance in the way its typing is so weak as to be almost nonexistent.) On the [multicore](http://shootout.alioth.debian.org/u32q/which-programming-languages-are-fastest.php) it's even more clear, there's fast, from C to Pascalish in a fairly smooth gradient, and there's slow, from Clojure to Python 3. (And there's "probably unusable" for JRuby, PHP, and Perl.)
As far as I know, the google group is read-only.
&gt; edit: or meaningful names: How about using `-XRecordWildCards`: data Params = Params { xpos :: Int, width :: Int } main = renderBox Params{..} where xpos = 2 width = 50
Old pastes are gone?
He wrote that he'll import them sometime this week.
I missed that bit. Thanks.
Simon Peyton-Jones just added an `INLINABLE` pragma to GHC. This pragma allows us to do exactly the thing we want, namely make sure that GHC generates an unfolding so the function can be inlined, without forcing the inlining unconditionally. 
HLint online would be awesome, I've wanted to do that before, but integrated in to a pastebot would be even better.
Well, it's not possible to determine *in general*. Specific cases may be possible, such as the one you give; how else would you know that it's safe? Consider also that finding the big-O time complexity of an algorithm implies a termination proof. I would go so far as to conjecture that any *specific* program for which halting behavior cannot be proven must contain either an embedded Turing-complete interpreter or a serious bug. On the other hand, if one wants that sort of termination proof, it's probably time to set Haskell aside and move on to Agda...
Ooh, I want to play with this!
&gt; proven What is or is not provable is relative to a given deduction system. There is no absolute notion of provable.
Sounds like a great idea, I'm not subscribed to the mailing list but I do hang out on [#haskell](irc://irc.freenode.net:6667/haskell)
Er, yes, sorry, that was poorly worded. I meant something much more informal, more along the lines of "what the programmer is capable of reasoning about in a practical sense" rather than "what is formally provable". Apologies.
What about creating a diaspora seed? I don't know if this is a good or bad idea, I'm just throwing it out there...
achievement unlocked: you have written a monad tutorial!
This is actually a really cool idea: authored a monad tutorial, posted to hackage, replied to the haskell-cafe, used the lambdabot, blog posted to r/haskell, answered a question on stack overflow, etc. Sounds fun!
Nooooo.... no more monad tutorials.... please
"Haskell hackers garbage collect in parallel"
Nice! The split between *mtl* and *transformers* with *monads-fd* is going away. This will enable more packages to work with each other and move Haskell forward faster (not that we aren't moving fast already...). If you're the author of a package that depends on *mtl* or *monads-fd*, please follow this proposal and update your package when the proposal is applied. Go Haskell!
isn't this just a sidestepping of the real problem?
What problem are you referring to?
The sooner this happens the better!
I don't really know, but there had to be some problems if someone decided to fix this (maybe mixing libraries that depend on both solutions? orphan instances?), there are multiple parsing/xml/whatever libraries and the diversity shouldn't be considered a problem. if fundeps are the real problem, maybe they should be disabled (unless this mtl stuff is the intro for just that). I hope they don't disable fundeps, what will I abuse?
so now i can have a sparsely populated, near-dead social network that is purely functional?
Anyone have any luck building it? Resolving dependencies... Configuring amelie-0.1... Warning: This package indirectly depends on multiple versions of the same package. This is highly likely to cause a compile failure. package cgi-3001.1.8.1 requires mtl-1.1.0.2 package Takusen-0.8.6 requires mtl-1.1.0.2 package MonadCatchIO-mtl-0.3.0.1 requires mtl-1.1.0.2 package regex-base-0.93.2 requires mtl-1.1.1.0 package hslogger-1.1.0 requires mtl-1.1.1.0 package formlets-0.7.2 requires mtl-1.1.1.0 package applicative-extras-0.1.6 requires mtl-1.1.1.0 package amelie-0.1 requires mtl-1.1.1.0 package MissingH-1.1.0.3 requires mtl-1.1.1.0 package ConfigFile-1.0.6 requires mtl-1.1.1.0 package haskell98-1.0.1.1 requires random-1.0.0.2 package MissingH-1.1.0.3 requires random-1.0.0.2 package Takusen-0.8.6 requires random-1.0.0.2 package QuickCheck-1.2.0.1 requires random-1.0.0.2 package amelie-0.1 requires text-0.7.2.1 package blaze-html-0.2.3 requires text-0.8.1.0 package blaze-builder-0.1 requires text-0.8.1.0 package time-extras-1.1.4 requires time-1.1.4 package random-1.0.0.2 requires time-1.1.4 package Takusen-0.8.6 requires time-1.1.4 package random-1.0.0.2 requires time-1.2.0.3 package hslogger-1.1.0 requires time-1.2.0.3 package amelie-0.1 requires time-1.2.0.3
The main issues with `mtl-1.1.*` : - Not Haskell 98 (fundeps and more) and thus GHC-specific. - no `Applicative` instances The idea of `transformers` is to have the Haskell 98 parts of `mtl-1` as a separate package and have the unportable `monads-fd` and `monads-tf` packages separate. Since this split was initially just an experiment, `mtl` remained as is. However, now that the experiment has worked and many packages are using `transformers` + X it is just the natural next step to merge the experiment back into the stable platform. The issue of `monads-fd` vs `monads-tf` still remains, though.
&gt; (I guess that Maybe and Identity are one of the few in which is does not matter). Indeed. Maybe and Identity (and Reader) are among the so-called commutative monads. A monad is commutative when `liftM2 (flip f)` = `flip (liftM2 f)`.
It's a really bad idea. Diaspora is amateur hour.
&gt; Not Haskell 98 (fundeps and more) and thus GHC-specific. this makes sense. &gt; no Applicative instances I don't get this one. I though that typeclass mechanism had this advantage over OO languages with interfaces and their implementation, that you could take someone elses data type, a different typeclass and provide an instance yourself. now it seems that's not the case, so we're back to "provide all the instances in the world for your data type". and this is the problem that should be solved on the language level, not by shuffling code around in libraries.
Very nice article, thanks.
&gt; I though that typeclass mechanism had this advantage over OO languages with interfaces and their implementation, that you could take someone elses data type, a different typeclass and provide an instance yourself. You can but this creates an orphan instance: an instance that is defined nor in the module that defines the datatype nor in the module that defines the type class. Orphan instances have some [disadvantages](http://www.haskell.org/haskellwiki/Orphan_instance).
so the language problem of orphan instances is being solved by shuffling library code. I don't think it's the right way.
Alright, this is getting a bit silly. So if I click on the link, I can find out that memory usage and speed have been improved, and a bunch of other things... but not what it is. No problem, just click on Hackage, right? Oh. Apparently it's a rewrite of hp2ps. But I have no clue what the heck hp2ps is, so... yeah, I'm still clueless as to what this piece of software does. Oh, Hackage has a home page link.... ugh, but it's to a gitorious page. I suppose I could google for hp2ps, but I think I've reached the end of my curiosity. Best of luck, hp2pretty users.... whoever and whatever you are.
It's not quite that bad. You can define a new typeclass and instances for someone else's data types. The thing that is discouraged is for me to marry your typeclass with someone else's data type without injecting my own newtype wrapper.
&gt; But I'm not sure how Java got on that list - it's not on the pareto frontier - it's dominated by Haskell and GCC on the graph. It isn't dominated. Java 6 Steady Sate code is shorter than C GNU gcc and it is faster than Haskell GHC.
Related: How to talk to your children about sex there comes a time for every novice haskell programmer to have a talk about the binds and the returns.
The font doen't display properly..
I just saw that. I'm not quite sure what to do about it. The slides look fine on my PDF viewer.
I implemented digit-recognition a few years ago (though, admittedly, using [fann](http://leenissen.dk/fann/) ), taking input from a tablet, and it worked great in all of those cases (I trained it on a bunch of classmates + family members) -- I think noisy training data really helps, but I don't know!
Nice talk! And thanks for referencing my monad tutorial blog posts!
Yeah, all those monad tutorials. What's the big deal, a monad is just an endofunctor over a category associated with two natural transformation mu (1 -&gt; M) and eta (M^2 -&gt; M). 
there are databases out there of handwritten digits with tens of thousands of normalized samples. a search result: http://yann.lecun.com/exdb/mnist/
Slide 8 has an error, or maybe I'm still sleeping. Prelude&gt; return Nothing :: Maybe Int &lt;interactive&gt;:1:7: Couldn't match expected type `Int' against inferred type `Maybe a' In the first argument of `return', namely `Nothing' In the expression: return Nothing :: Maybe Int In the definition of `it': it = return Nothing :: Maybe Int Return type of that is `return Nothing :: Maybe (Maybe a)`; I think it should have been rather `Nothing :: Maybe Int` 
I want to polish it a bit before releasing on hackage. In particular, I don't manage to catch stack overflows. Is that possible? (I've tried Control.Exception.try without success.) 
Very cool! No, you can't catch stack overflows as far as I know...
Too bad. I guess I could just call ghci as an external process and parse the ouput then.
Hmm. What happens if you stack overflow on a different thread? Does that take down the main thread?
Nice to see someone reddit'ed my post. :)
Absolutely. *fap fap fap*
True, but that didn't require a lot of googling to find out. &gt; hp2ps––heap profile to PostScript
you can catch it on Linux with [libsigsegv](http://libsigsegv.sourceforge.net/) but I don't know how you would recover from that!
Am I missing something here? All i see is "A is for..."
Click on it.
Awesome!
Why, that one doesn't rhyme at all!
That was aswesome.
"C" could use some work. "Z" is beautiful.
I'd love to see this merged into hoogle
Aayy!
Please, let's not add to the number of Linux-only Haskell tools.
But, but… Z doesn't rhyme!
you never know...
Y is beautiful.
Eh, a tool that's single-platform is still better than it not existing at all...
I sent patches that solve stack overflow. But two new problems: It searches over *all* matches including ones that don't quite have the right type. How can we filter them efficiently? Ctrl-C doesn't work, so you need to Ctrl-Z and kill...
A code execution option (like codepad's) could be great, too
In general, that's true. But in this specific case, I'd rather have a multiplatform tool that crashes on stack overflow than a single-platform tool that prints a cute message ;)
I don't think there *is* a language problem. Perhaps you could put orphan instances in the same category of goto statements: you shouldn't use them in general, but there are some cases where they are very useful/necessary. I'll give two: * Breaking up a large single module into multiple sub-modules can sometimes necessitate an orphan instance, or at least be easier to follow with orphans. For example, the default Yesod site template uses an orphan for this reason. * When defining properties with QuickCheck it can be useful to define an orphan instance. I consider this (relatively) acceptable since there is no production code based on this that might break. Or at least, if you structure things properly, your non-test code shouldn't break. &gt; I though that typeclass mechanism had this advantage over OO languages with interfaces and their implementation, that you could take someone elses data type, a different typeclass and provide an instance yourself. That's still the case without orphans: you can declare a Foo datatype, and I can create a Bar typeclass and create a Bar Foo instance in my code without any orphans. That is a case not addressed well by typical OO inheritance/instantiation.
On page 7: "Braces around the function name (func) make it infix" Did you mean "prefix" instead of "infix"?
you call something that shouldn't be used an advantage? maybe instances should be named and imported/exported with proper hiding. because that newtype wrapper thing is awful, writing "foo (BarWrapper bar)" looks like "fooBar bar" for custom "fooBar" function created for bar. and that looks like print_int from ocaml. we can't have that!
I think you're confused about what an orphan is. An orphan is when: * module A defines datatype Foo * module B defines typeclass Bar * module C defines instance Bar Foo Those are the things that are dangerous and should be avoided except in special circumstances. The advantage of typeclasses over OO which is touted is the ability to do this: * module A defines datatype Foo * module B defines typeclass Bar *and* instance Bar Foo In Java, the equivalent is impossible, ie: * package A defines class Foo * package B defines interface Bar and makes Foo an instance of Bar I don't think *anyone* touts orphan instances as one of Haskell's strengths versus OO languages.
I know what an orphan instance is. there's a great explanation in comments section (by augustss) and I agree with Lennart: http://lukepalmer.wordpress.com/2009/01/25/a-world-without-orphans/
I really liked M.
What about a tool that crashes on all platforms but recovers on Linux?
Generally people just bound the search explicitly. That would also give a hand on how the search really uses the stack, and may guide future improvements. That said, I'd rather have the tool on hackage with stack overflows that not at all. Nice work!
Then I was confused by your comment: &gt; you call something that shouldn't be used an advantage? I'm pointing out that orphans should not (in general) be used, but there is *still* an advantage to type classes versus general OO inheritance/instantiation.
"O" made me laugh out loud ;-) 
I just released it on hackage, with Jun's patches to catch stack overflows. It can take some time to look for a function depending on the size of your hoogle database.
What are "non-begative powers"?
They're powers that are either dositive or nero.
What about the ibadgernary ones?
I though that prefix was when you stuck the operator *before* the arguments like : + X Y. And infix is where you stick it in the middle : X + Y. In Haskell, braces around a function like (func) make it infix so you can write X func Y. Am I wrong?
Next step would be to have ghci easily usable with graphical libraries (show and interactively modify graphs, GUI, 3D) which would be a fantastic experience for learning and visualisation.
+1 to concatMapM. I've written this function in many separate projects. [Even GHC has it!](http://darcs.haskell.org/ghc/compiler/utils/MonadUtils.hs)
(First: the characters ( and ) are called parentheses, not braces. Braces are { and }. Brackets are [ and ], sometimes called square brackets to be clear. We sometimes call &lt; and &gt; angle brackets.) You put parentheses around a function name when defining it if you want it to be an infix operator. In my (admittedly limited) experience, this is usually done with functions whose names are symbolic, rather than alphabetical; for example: -- Define some new infix binary operator called +&gt; (+&gt;) :: a -&gt; b -&gt; c x +&gt; y = ... So deech is right. However, you can still use such an infix function as a prefix function by writing the parentheses at point of use, such as: z = (+) x y Again, this is usually (I think) seen for operators/functions with symbolic (non-alphabetic) names - as in this example. So fizruk is right. Perhaps confusing matters here further is that backticks around an "ordinary" prefix function allow it to be used infix: func :: a -&gt; b -&gt; c z = func x y z' = x `func` y There are several library functions for which this particularly makes sense, such as Data.Map.lookup. Hope this helps clarify matters! 
Slide 9 has an error. The type signature for (&gt;&gt;=) in the Maybe monad should be this: Maybe a -&gt; (a -&gt; Maybe b) -&gt; Maybe b
I really don't see the need for "isWindows". When you're creating an executable for a platform, that information is in the build environment, and can be directly used to e.g. select different implementations of modules. This should all happen outside Haskell code. The idea that making this a built-in could eliminate run-time comparisons is besides the point. In fact, I find the idea of catering to currently popular platforms by writing references to them into language standards to be unfortunate, too. I agree that the other bugs are bugs and should be fixed.
In Haskell, a function whose name consists entirely of symbols is an operator. One that starts with a lowercase letter and otherwise consists of letters, numbers, and a few extra "name" characters like _ and ' is a normal function. A normal function name can be infix-ised using backticks. An operator can be prefix-ised using parentheses. Any function can be defined either prefix: ($) = id or infix: op `on` f = \x y -&gt; op (f x) (f y) and its usage is unrelated to its definition: the only significant thing is whether the name consists of symbols or letters.
You're right! I will fix that. Thanks!
Fixed! Thanks!
I would much prefer to use normal Haskell conditional than #ifdefs. Requiring that there be a static build environment rather than allowing portable code (like yhc bytecode) is also a bit unfortunate. I happen to disagree with the particular choice of `isWindows` but we do want something similar that is better than strings. Cabal uses an enumeration of OSs (which is imperfect but still a lot better that strings).
What build error are you getting?
I absolutely agree that you shouldn't generally be using ifdefs. import OS (thing_that_varies_depending_on_os). No conditionals whatsoever, neither Haskell, nor CPP. &gt; an enumeration of OS That's also the wrong thing. What should matter are "is this feature available" and "how is this feature accessed", ala autoconf.
As I pointed out, foldMap is a general purpose replacement. I don't feel the need for a specialized concatMapM at this point so much, but on the other hand, it took me quite some time to learn to use when to use Traversable and Foldable -- now I find them indispensable.
&gt; I absolutely agree that you shouldn't generally be using ifdefs. &gt; import OS (thing_that_varies_depending_on_os). That's fine for things that are basically portable concepts/features that happen to be implemented differently on different platform. Many things do fall into this category. &gt; No conditionals whatsoever, neither Haskell, nor CPP. Well, there's probably still some CPP or conditional building of whole modules if each OS-specific module refers to system features that only exist on that system (e.g. C libs). &gt; That's also the wrong thing. What should matter are "is this feature available" and "how is this feature accessed", ala autoconf. In my experience a significant number of such tests amount to `isWindows`. There's also cases where essentially you have different configuration profiles based on OS (like Cabal's default install dirs) and an OS enumeration works well there.
Shameless blog promotion!
foldMap is not a replacement; consider the types: foldMap :: (Foldable t, Monoid m) =&gt; (a -&gt; m) -&gt; t a -&gt; m concatMapM :: Monad m =&gt; (a -&gt; m [b]) -&gt; [a] -&gt; m [b] To unify the two, you'd need a Monoid instance for: Monad m =&gt; m [b] Which would technically be possible -- but much too general to add to the base libraries. Even in its monoid form, concatMapM still can't be replaced by foldMap without a nasty instance along those lines.
no shame in that, if you don't post it - dons will.
 Ord a =&gt; ElemType a -&gt; [a] -&gt; [a] couldn't you hide it again by moving ElemType to the constraints (and preferably with merging Ord by superclass constraint)?
Like so? class (Ord a) =&gt; ElemClass a where et :: ElemType a sortP_6 :: forall a. (ElemClass a) =&gt; [a] -&gt; [a] sortP_6 xs = case et of { ElemType e -&gt; runST (f e) } where That works, but now you have to write instances for `ElemClass`. They're all instance ElemClass T where et = ElemType Evidence but you still need to write them. Unless I'm missing another trick...
maybe some instance (that mutable array thing from ElemType ctor) =&gt; ElemClass a where ? 
I have had this problem before with STUArrays before. I think the [Vector package](http://hackage.haskell.org/package/vector) doesn't suffer from the issue because its classes are designed a bit differently. **edit**: spelling
Hmm... you're right. Not sure how I missed that before. foldMap only generalizes concatMap, not its monadic counterpart.
Why not do that yourself? Why would it be helpful for others to stop doing work they're good at?
You mean like make a DSL for hard real time [monitoring tasks](http://hackage.haskell.org/package/copilot)? Or perhaps you mean you want easy to use [SSL tunnels](http://hackage.haskell.org/package/secure-sockets)? Perhaps a declarative OpenGL backed [vector graphics package](http://hackage.haskell.org/package/gloss)? Or maybe you're an audiophile and like [music packages](http://hackage.haskell.org/package/haskore)? Seriously, where did this rant/troll post come from? What is the motivation?
I'm fully aware of the current industrial applications of Haskell, just as I'm sure Paul Graham was aware of ITA back when he was working on ViaWeb. Doesn't change the fact that the vast majority of employed Lisp programmers until recently on IRC were @itasoftware.com Your link doesn't change anything about what I've said. I'm tired of "I used typeclasses to make a proof about set theory that was originally formulated 100 years ago!" posts. I want to see people making things. Cf. #any_other_bloody_programming_language_subreddit I'm not going to stick my neck out for a language community that can't grow up past grad school. Sort it out, then they come.
http://www.reddit.com/r/haskell/comments/dgwx1/how_to_get_a_professional_programmer_to_use/c104gnw
Because the "work" they're doing isn't doing me any favors. It's arbitrary and frequently useless outside of earning community cred. I've heard enough people whine about how this language is so 'seasoned' and yet unpopular, but nobody's willing to speak up about why.
&gt; Your link doesn't change anything about what I've said Um, yes it does. You said 'make something'. Msottile showed that lots of people have. &gt; I'm not going to stick my neck out for a language community that can't grow up past grad school. I'm sure they'll do fine without you.
Is [writing music](http://programmusic.livejournal.com/) "making something"? Whatever you think about the music, it's even Haskell you shouldn't need a Ph.D. to figure out. 
Me as an individual is pretty besides the point here. Show me what it takes to safely yet statefully memoize and invalidate at will, the results of an arbitrary function in Haskell.
&gt;Is writing music "making something"? Yes. Is it what I meant? No. I mean making products people use when you're not Galois. Xmonad doesn't fucking count either.
I'm not sure what you mean by monitoring tasks but that's a "write C in Haskell" library. I can't monitor servers with it, not to the best of my knowledge :)
Nice blog entry. It sent me off playing with a bunch of nonsense for a some time before I realized just how tricky the problem is. Its nice that there's a solution at all, but it feels really unnatural. I know that if one strays too far afield, then your type system becomes a mess, but it seems like there's got to be a smarter/more general way to unify existential constraints to let some of the more straightforward approaches work. Alternately, there must be an approach that allows the type safety of ST's phantom type without letting that type bleed explicitly into any other class constraints.
I get the impression that you're just trolling and nothing anyone points out would count as a ["Truly"](http://en.wikipedia.org/wiki/No_true_Scotsman) useful piece of software to you. Also, tone down the "fucking"
http://en.wikipedia.org/wiki/MADtv_recurring_characters#Stuart_Larkin **"Look what I can do!"** That's 99% of Haskell posts and 'projects' Your idea is really really really cool, but you've brought your coal to Newcastle.
How to get professional programmers to stop poorly reimplementing the same old shitty software and poorly-thought-out APIs on crappy platform of the day: Get them to talk about math and *think hard* about what they write before (and while) they write it. (I upvoted you, but not because I agree, instead because this is a topic I like to see discussed)
Well, while everyone was making fun of Python for being a "juvenile" language, people were making libraries to do everyday things. Tons of them. While Haskell remains the purview of category theorists and grad students, there is a poor and not very well developed selection of libraries for what programmers need to do on a day to day basis. It's pretty hard to justify hs to a boss if you can't actually save time using it.
It is for hard real-time monitoring of embedded programs in a non-intrusive manner. The term "task" was probably a poor choice on my part as it wasn't being used in the normal operating system theory definition.
I would like to see more forethought and professionalism out of my colleagues but Haskell as a community and a language isn't really doing anything for Joe Coder.
In my world thought and knowledge are tools used to make things. Tetris isn't particularly useful but it would show that you can do graphics, sound, input and more with Haskell. It would also be an opportunity to put the knowledge you feel is to important into practice. I empathise with ninja_band.
Re: 'make something'. What would you like us/them to make? Absolutely serious here, if there's any question.
The great thing about open source is... if you see a need, you can fill it yourself. Lead, follow or get out of the way. There is no whine.
Notice I didn't say that you said making things &lt; learning.
I can think of a few things that aren't doing you any favors :-)
I could. But it might take some math.... http://conal.net/blog/posts/memoizing-higher-order-functions/
And that is because people set up the false dichotomy of "thinking/doing math" vs. "getting shit done". Many people (like you?) see math and instantly write it off as bullshit academic mumbo jumbo, turn off their mind, and thus have no reason to ever change their stance on it. It's sad, but while it would be nice to get more people using the language, if pretending we're just like any other coding community is what it takes, it's not worth it. I love reading the stuff posted to this subreddit, and love the #haskell IRC channel even more. I usually try to avoid this attitude, but honestly, if you don't like it, either start posting your own "real coder gettin' shit done in Haskell" blog posts, or quit reading the subreddit if the other material bothers you too much.
http://groups.google.com/group/clojure/msg/c2a9e8d27a1fff3f?pli=1 "As a side note, years ago, I wanted to write something in Haskell that worked like Clojure's memoize (which is implemented in a half-dozen or so lines of code in Clojure's core), and asked about it on the Haskell mailing list. I was pointed to a PhD dissertation on the topic of how to write memoize in Haskell. All I could think was, "Do I really want to be using a language where memoize is a PhD-level topic?" " Proving my point.
catamorphism: &gt; Why would it be helpful for others to stop doing work they're good at? you: &gt; Because the "work" they're doing isn't doing _me_ any favors. Self-centered much? Most of us enjoy this place just plenty.
General purpose memoization function wrapper that works on arbitrary types. A web framework that uses existing middleware standards like WSGI for deployment and is at least as productive as the 'average' web framework. A template rendering library that doesn't make me cringe and wish for death. (Cf. jinja, haml, erb ) Leksah to not suck and do weird shit like replace plaintext with glyphs by default so that it doesn't remind me of APL and make me reach for my flamethrower. A reasonably mature database ORM that can handle the statefulness. A games development framework that helps you make client-side 3D games fairly quickly (think, C + OGL + SDL hack-ups) For people to stop making the 99th parsing library in haskell and then making a shitty benchmark comparing it to 6 others. (Yes I mean **you**) For the language to not require being a grumpy professor to achieve "Grokhood" of it.
I like haskell because people there are working on new ideas and elegant solutions for the known problems (parsec, xmonad, cmdargs, ...) instead of reimplementation of existing applications in new language as only benefit. I don't see too much value in haskell re-implementation of something already done in C,Java,Python,C++. But I can imagine it can be usefull like a proof of concept or tutorial.
Dandy. But I will mock any posts whining about achieving critical mass mercilessly, using this quote as grist.
I don't know who you mean by "everyone", but the exact same process you were describing for python is happening now for haskell. You can either sit there and complain about it not being done yet, or you can rally people to help get it done.
I am coding a computer simulation of consumer behavior to drive a machine learning system that will optimize messages sent to consumers in real time. So far it's 100% haskell. BTW, it's a commercial effort. For me, coding a project in haskell is like a jigsaw puzzle. Hard at the start, but very easy at the end. Easy to improve an add features as well. The academic origins of haskell seem to disturb you and that's too bad. I like to look at haskell as providing extreme modularity--inherited from all that math. There are lots of smart people working on haskell related projects. And they love answering questions. A lot of academics they must be. Check out CUFP in a couple of weeks. I am sure there will be some people of a more "practical" bent there who may be able to give you comfort about the future of haskell. Cheers and next time please be more specific when you report a compiler error ;-) 
Haskell, avoiding success at all costs
Demonstrating that you're shifting your point to keep up the troll. Initially, you said you liked the language but not the mathy attitudes of people doing research with it. Then you asked for useful things. I showed how all this mathy stuff does yield very interesting and useful results in the end. Now you're just complaining that useful stuff is complicated.
How did the AI people kill Lisp? Seriously, how does one group of people using a language prevent others from using it? What, did the AI people steal all the parentheses for themselves?
How would you write a general purpose memoization wrapper that works on polymorphic higher order functions by the way? (you *did* say all types, right?) Would this wrapper have optimal efficiency on different types, would it rely on their having a potentially decent hash method, would it use binary trees? How would you suggest that Haskell programs adhere to a python standard (i.e. WSGI)? For templates, have you seen hamlet? How would you propose implementing an Object Relational Model in a language that isn't object oriented? Go learn something.
Glad to see that you're leading the charge for professionalism here. If you hadn't clarified that, a less observant reader of this thread might not have picked that up.
it gave the language momentum towards a dead end. When the AI winter came, it also hit Lisp by proxy.
Now I'm just waiting for the big math bust when people decide that teaching computers to calculate was always an impossible dream resulting from a poor analogy :-)
Incurvatus in se - Haskell is like this shining piece of enlightenment that eludes poor mortals (such as I) with promises and glimpses of abstract mathematical beauty ... but the thing is so pure, that when you need to rub it against real word dirt to make ugly, but necessary things, it makes you pay for soiling the Vision.
It also occurs to me that complaining about how people do math and cs research using Haskell is sort of like complaining about how people write in languages I can't understand using the alphabet.
math existed prior to computers. AI could only exist given the existence of computers. Comparing math and AI on the grounds of how they relate to computers can be considered by some a strawman
Perfect. I have nothing more to say or contribute that could possibly be of higher truth than this.
+1 for the jigsaw analogy
Mechanical Turk? The thing that the Amazon thing is named after.
From wikipedia, &gt; The Turk, the Mechanical Turk or Automaton Chess Player was a ***fake chess-playing machine*** constructed in the late 18th century. From 1770 until its destruction by fire in 1854, it was exhibited by various owners as an automaton, though it was explained in the early 1820s as an elaborate ***hoax*** I hope you´re not implying that AI can be achieved by mechanical means,but I´ll rephrase &gt; AI, as it is currently viewed and understood, could only exist given the existence of computers.
If you pay attention to research in security or distributed systems, you'll already know who [David Mazières](http://www.scs.stanford.edu/~dm/) is. This looks like an excellent and very exciting opportunity.
While you may have me for supper, it's clear from the last time that that quote was mentioned here that the context is both wider and deeper than it lets on. But we have established, you're trolling.
Taking the "higher ground" doesn't actually make your points or contradictions more valid, nor mine invalid. The fact that I'm willing to get messy and grab people by the balls merely means I'm an opportunist. Not that I am wrong. Nota bene: 55 comments and counting. How much have you done to bring attention to the state of Haskell?
Disconnected from reality as always.
&gt; Taking the "higher ground" doesn't actually make your points or contradictions more valid, nor mine invalid. Wait, what? Pointing out that you're wrong doesn't make you wrong? Since when?
I'm not wrong, I'm just annoying. Flies are annoying, doesn't mean the corpse ain't stinky.
_hydo_'s addendum: Once past a certain point of knowledge, you see that what you thought was Haskell making you pay for soiling the Vision was actually a beautifully designed separation between what you know to be correct and the unpredictability of the outside world.
That guy really did ask an excellent question; I was just wondering the same thing myself.
Be the change you wish to see.
I might be missing something, but I don't understand why "concatMap" or "concatMapM" are useful. I mean, why prefer "concatMap f" to "concat . map f"? Or "concatMapM f xs" to "concat &lt;$&gt; mapM f xs"? Some other things we might have and I sometimes use, but I wouldn't like in the stdlib: dropTake d t = take t . drop d mapu = map . uncurry enumerate = zip [0..] 
&gt; That's fine for things that are basically portable concepts/features that happen to be implemented differently on different platform I think all programs fall into this category -- as in the worst-case, the entire program is reimplemented for each platform, and there are no conditionals anywhere :-) &gt; In my experience a significant number of such tests amount to isWindows. That is not true in general. Even if it is true now, your program might be sent to the future and compiled there, where other platforms implement these features, or Windows have dropped/added support for that feature.
Michael, this looks interesting. Does it use template Haskell anywhere? That's one of the principal reasons I've not tried your server-side code: too many fancy language features in flight for my comfort.
Dibs!
I should note that my brain is warped by spending lots of time lately thinking about subtype systems, but it increasingly seems to me that subtypes provide a significant degree of expressive power even absent records or the like -- and in particular they offer a way to unify plenty of higher order types that "feel" like they should.
&gt; Once past a certain point of knowledge there lies the problem for derp prone folks such as I
That doesn't make any sense. The only way that could be the case is if the AI people were keeping Lisp alive, and when they died so did Lisp with no one else to support. That hardly counts as AI people killing Lisp.
I'm proud to say that for once I've kicked my TH addiction ;).
I wouldn't really say IntMap was invented by Chris. IntMap is just big-endian Patricia trees. 
Thanks for this. I've often wondered about what other persistent data structures are known. Has nobody ever published a comprehensive survey?
Lock-free/wait-free data structures are also an active research topic.
If you need concatMapM partially applied, it gets a bit more irritating, requiring one of these: fmap concat . mapM f (fmap concat .) . mapM I've used it often enough that I'd like to see it in the base libraries (and swap on pairs, for that matter), but others disagree. 
But data structures with locks are by construction not purely functional.
IANA concurrency expert but IIUC, they're an alternative to the common "purely functional data structure behind a MVar/TVar" approach.
As a side note, "PFDS behind MVar/TVar" have what Herlihy and Shavit call a "sequential bottleneck" and will not perform well under high write contention. For instance, k concurrent writes to a balanced tree will take BigOmega(k * t(n)), where t(n) is the time to make one write to the tree. That is, there will be a speedup of 0%.
Okasaki's book references many other wonderful structures that he doesn't discuss in detail. Haim Kaplan [wrote a survey chapter on persistent data structures](http://www.math.tau.ac.il/~haimk/papers/persistent-survey.ps). Keep in mind that "persistent" is different from "purely functional". **edit**: grammar
Community defined achievement? Half the fun is when the game tells you what you achieved without your asking. That would be fun. 
if you know this guy please force him to write a blog post about not being programmer/cs guy, and learning haskell as the first (and only) language. this is the publicity we need.
Anyone who could tell me why another compiler? 
Dumb question time: Are B+ trees (ala CouchDB) a purely functional data structure? 
Yay for generic deriving!
The main reason is that &gt; it provides a great platform for experimenting with language implementations, language extensions, etc. 
Because competition is good! UHC is used as a teaching and research platform at Utreecht University. I belief currently the only significant extra feature UHC has over GHC is the generic deriving mechanism. I.e. you can write your own 'deriving MyClass' code. But there's room for so much more! Just the simple fact that UHC is a complete separate codebase form GHC means that some things are easier to implement in the one compiler versus some other things that are easier to do in the other. Having multiple sufficent language implementations creates an environment for doing more research on cool language and type system extensions. And besides, for the moment everyone will just continue to use GHC in production environments, so we won't have the webbrowser compatibility problems ;-)
&gt; Because competition is good! This. It's the same reason I want clang to support Boost and Qt (the two most C++ abusive libraries I know of). Once this happens, code I write can be tested under it and more test coverage of my code is good. Granted Haskell doesn't have the issues C++ has, but the competition aspect is still good. Speed, optimization, and dynamic linking are things that compilers can compete with GHC in.
B+ trees can be written in a purely functional way. In fact, Hinze &amp; Paterson's famous finger trees are actually B+ trees! I do not know if CouchDB's B+ trees are purely functional.
That's pretty cool!
To take that to its logical extreme, maybe it sounds like you need to stop writing code altogether. That sounds silly, especially since more terse code isn't even "less code" at all.
 drop 2 . scanl (\(x, y) z -&gt; (y, z)) (undefined, undefined) add `map (uncurry f)` to get the operation version.
Actually it does quite correctly argue for light weight code bases with easier to combine abstractions, because the more glue code you have to write over and over again, the better your chances of failure are at getting it right :-). This is one of those "been there - done that" situations for me though.
Good argument for autocompletion. Maybe that's why java programmers don't mind all the typing. 
Not always the case. It depends on the coder's intelligence. For stupid or lazy coders, copy and paste might be the best choice.
quicksilver says: ``zip`ap`tail`` - the Aztec god of consecutive numbers
Well, if you have a language that executes the desired program using only the empty set of tokens.
this is the same old debate that comes up in any open source ecosystem...why another browser? just contribute to blah...why another db? just contribute to foo... the same replies apply here: 1. competition is good 2. not everyone wants to assist an existing effort 3. established projects tend to be risk-averse 4. you never know, one day ghc could just suck. or the maintainers could go nuts and take it in some strange direction...its nice to have options 
Does it work on OSX?
[Stolen](http://stackoverflow.com/questions/3774247/what-do-we-call-this-new-higher-order-function/3775293#3775293)
Si, senor!
That's a fantastic idea. I'll get to writing it immediately
BTW the hackage build fails with: HsColour: src/System/Plugins/Load.hs: hGetContents: invalid argument (invalid UTF-8 byte sequence)
Stupid and/or lazy developers will get copypaste wrong, too. Discussions of code correctness devolve to downright silly if we are to consider particularly bad developers writing correct code.
Swap space collector?
I'm already done! I made it self bootstrapping.
Experienced programmers will also get copy paste wrong.
They most probably mean semispace as in [Cheney's GC algorithm](http://en.wikipedia.org/wiki/Cheney%27s_algorithm).
"Any verbose and tedious solution is error-prone because programmers get bored." — Bjarne Stroustrup
I've been defining it as zipTailWith (and zipTail) for years: zipTailWith :: (a -&gt; a -&gt; b) -&gt; [a] -&gt; [b] zipTailWith f xs= zipWith f xs (tail xs) zipTail :: [a] -&gt; [(a, a)] zipTail = zipTailWith (,)
since `ap :: (Monad m) =&gt; m (a -&gt; b) -&gt; m a -&gt; m b`, what is the type `m` here?
&gt;To take that to its logical extreme, maybe it sounds like you need to stop writing code altogether. I know you're being facetious but I'll reply anyways. I don't think this is implied. There is an implied minimum functionality you need, but you don't want to write more code than required to meet that, measured on some "expressiveness" metric.
Great quote &gt; Haskell is a rewarding language to work in. It's a little like reading Kant.
in this case the monad `m` is `(e -&gt;)` aka `(-&gt;) e`, which is a reader monad.
Well, if you can solve your problem without writing a single line of code, then why would you write a single line of code?
For that matter, also on Stack Overflow, [Travis Brown](http://stackoverflow.com/users/334519/travis-brown) is apparently a Ph.D. student in literature and linguistics and is solidly on the [Haskell tag high score tables](http://stackoverflow.com/tags/haskell/stats). No idea how much previous programming experience he had before picking up Haskell, though.
You can build a brand new app on top of multiple libraries. Or you can use a framework that does 95% of what you need and write the extension/customization for the remaining 5%. The mistake is trying to build a brand new app on top of various frameworks that each only provide like 5-10% of the functionality that you need. That is a common enterprise mistake that always ends in disaster. Since Haskell is pure FP, I would assume it is much easier to build libs of pure functions than it is to build large frameworks.
What´s the difference between passing a "giant data structure to a rendering library" and creating it inside a framework? Would Haskell´s laziness work differently in these contexts? I think that lazy evaluation and higher-order functions help composability, but not having shared state is really the crux of the matter. I also think that there´s some flawed reasoning when trying to compare libraries and frameworks purely on the grounds of composability. Usually, frameworks have a much larger scope than libraries. A great part of the work done by frameworks is abstracting away the plumbing necessary to make libraries work and behave well together. It´s like saying that bricks are more composable than whole walls ... it is true, but that isn´t really the point. Since this is Haskell, I think that the boundaries between what constitutes a framework and a library are really blurry. In my n00bi3 opinion, map really is a framework, albeit a very simple one.
One of the main problems I have with frameworks is indeed their "large scope". Why do you need _one_ complete web framework that encompasses URL routing, HTML templating and Database connectivity? Why can't those be three different libraries? Each of those can _do just one thing and do it well_. (Hey, UNIX text streams are also lazy evaluation!) I guess there are situations where you don't want to be concerned with picking the best library for those three tasks, and just want stuff to work together. In that case you can use a more higher-level library that wraps around those, something that's pre-configured. It still doesn't need to be a framework that encompasses your entire application. Also, on the semantics of what is and isn't a framework, I don't really care. If you want to call `map` a framework, that's fine with me. In that case, I use dozens of frameworks a day, keep on building and using more of those tiny frameworks I say!
I guess I see your point, but I have dreadful memories of trying to mix and match Perl libraries to build a couple of web apps in the past. I think sometimes "choice" gets in the way of "done"
Yes, I too have horrible memories of C# libraries. But those are strict languages, with no culture of higher-order functions. However, we're in Haskell-land now, everything is better here! Or at least the tools exist to create better libraries. Using laziness and pure, higher-order functions.
The actual problem is prematurely composing loosly coupled elements: If a framework is just a collection of default interfaces, or even an arrangement of libraries with some glue in between, you don't lose anything. A say MVC-controller doesn't need a specific HTTP base layer, database backed or even application interface. Fixing them to anything but a good interface to plug stuff in is nothing else but paternalism.
Actually, I am in the pits of Delphi Hell :(
It's pretty surprising how available and inexpensive the necessary hardware to make an autopilot for an R/C plane is. I haven't built one myself, but it actually seems like something a person could plausibly do in their free time. http://diydrones.com/profiles/blogs/ardupilot-main-page
You're whole section titled "Downsides of a framework" is more like a section on downsides of bad design. I can build an application thats tightly coupled with a specific encryption library. Then your encryption library is just as difficult to replace as that of a poorly written framework. Alternatively I can build a framework that utilizes an encryption adapter and then changing the encryption library is just a matter of writing a new adapter and plugging it in.
Well the fact that a framework designer has to explicitly create an "adapter" is already kind of bugging me. But yes, you're right. It's possible to create a framework that doesn't suffer from any of the problems I mentioned. Just like you can write horrible, uncomposable libraries. I just needed a name to characterize this sort of architecture, and I think "framework" qualifies. Most frameworks I've seen suffer from at least some of the problems. I think once you start writing some code that you call a "framework", your already closer to making some of the mistakes I mentioned. If the thing you're building is "just" a library, you'll hopefully avoid some of the pitfalls. Also, a perhaps more correct title "Why Good Design is better than Bad Design", isn't nearly as catchy.
If they also radically different models of the data, it can be a royal pain. A well designed framework works out of the box for 90% of what you need, and makes supporting the other 10% easy due to good design.
if you don´t mind me asking, what were the Haskell frameworks you were thinking of when you wrote your post?
None in particular. I've not used big things like Happstack, so I can't pass judgment. I'm just suspicious of things that call themselves a "framework". I should try Happstack sometime, maybe its good, I don't know yet. When I hear people talking about how they're going to create some all-encompassing Haskell framework. "Honestly, we've learned from Java this one won't be crappy. It's Haskell so it's gonna be great!". I get a bit scared sometimes...
[somewhat relevant](http://discuss.joelonsoftware.com/default.asp?joel.3.219431)
Yes, that piece is hilarious!
Isn't that a language extension planned for GHC 7.2? Sounds like something SPJ could knock out in a week or so.
The author writes this as if people are actually building monolithic java-style "frameworks" in Haskell. "Inversion of control" is so natural and idiomatic for us that we get it for free, it's not even worth talking about. A couple of examples to illustrate my point, and pretty much the only two I can think of: [criterion](http://hackage.haskell.org/package/criterion) and [test-framework](http://hackage.haskell.org/package/test-framework). Both offer "`defaultMain`" program drivers which handle everything, so by the author's book definition these are both "frameworks". Is it possible to argue that these packages are not absurdly convenient and easy to use? Or that you can't easily scrape off the framework layer to treat these packages like "libraries"? I think the thing that the author is missing is that even when you create a "framework" in Haskell, the chances of it developing the kind of "frameworkiness" he's talking about are pretty much nil, and he's even identified the reason why: we just don't build Haskell programs that way, we use higher-order functions!
...says the inventor of C++!!!!!!!!
&gt; make something. Yup, [nothing to see here](http://hackage.haskell.org/packages/archive/pkg-list.html).
"cmu library and program: Unification in a Commutative Monoid" "calc program: A small compiler for arithmetic expressions." "pesca program: Proof Editor for Sequent Calculus" "probability library: Probabilistic Functional Programming" Indeed.
Yeah, probability never helped anyone do anything practical.
The point is priorities. Either accept that you're a using a niche language and play it up, or stop trying to promote it as a general purpose lang. Wouldn't kill you to have some dignity and not downvote me.
I'm downvoting you because you're an obvious troll. I'm on record in many places describing Haskell as a niche language. At the same time, I think that niche can expand, and I've been uploading practical software to Hackage. What have you contributed?
Everybody wants an excuse to take a crack at Diogenes' children :) I don't give a fuck what you've said or what you think. Get the fuck outta here. Movie critics aren't required to be directors in order to make accurate judgments. You don't like it, hide the fucking link.
&gt; I don't give a fuck what you've said or what you think. Get the fuck outta here. So you're just here to yell at us and then cover your ears? Real mature.
Wow, I see there are java and llvm backends but didn't find any information other that how to enable them. I'm especially interested in calling haskell code from the jvm. Does someone have more information about that?
The benefits and downsides of libraries weren't made explicit to me while reading the article, just a hammering away at frameworks. I usually just start working with a framework rather than read a book. I thought the whole point of frameworks is prevention of rebuilt wheels. Frameworks can be built in multiple varieties for various purposes from the sparse and light to the humongous kitchen sink variety. You choose it like any other tool, for the job, and a framework is just a further refinement of your tool set (e.g. Sinatra versus Rails; two different Ruby approaches both considered frameworks.)
&gt; Yes, I too have horrible memories of C# libraries. But those are strict languages, with no culture of higher-order functions. Not really true of C#. The new exciting features in C#, like LINQ, are largely from FP. And professional C# developers will indeed write lots of anonymous functions, whether they're simple lambdas (whose syntax is actually shorter than Haskell's lambda!) or LINQ expressions (which are pretty close to monad comprehensions).
My horrible memories are from long ago, before .NET 2 with generics, and before LINQ. Lately I've seen a lot of good libraries in .NET. LINQ and Rx are great examples of libraries! They make heavy use of higher-order functions, and the IEnumerable/IObservable interfaces are designed to be lazily evaluated. I think .NET is heading in the right direction, I haven't used .NET 4.0 yet, but I hear it has Lazy&lt;T&gt; built-in, that's great!
http://www.iep.utm.edu/diogsino/ Learn what a cynic is, touchy little shitkicker.
Nice writeup, although I fear that for programmers who are in the grip of the Nameless Continuation Fear, this won't help :-)
Haskell code is more readable though.
If you're running `deepseq` each iteration, instead perhaps you could make the data structure itself strict. Or, if you need laziness in the structure while building it, but not at the end, then can you identify where exactly in the data structure the laziness is required? That should help to reduce (perhaps even down to a single `seq`) the expense (and inelegance) of a deepseq traversal.
Why not just upload your package to hackage? Or alternatively send Don a new implementation of his existing API so he can upload a new release of the dlist package.
Kind of frustrating that you end up writing a new type class which essentially conveys the same information as the old one.
For those folks, I guess the one thing I hope, if anything, they get out of this is that you can use continuations to do folds without the data structures. They're one and the same.
The only differences are function names and the use of newtype. No?
The process the author uses here, turning a data type into a thing that is more a function than data, seems common among experienced Haskellers. Is there some info on the thought process behind it? Another author said "When in doubt, wrap everything in a lambda and try again." This looks like the list version of that. And while I can more or less follow what's happening here, after reading it, sleeping on it, and reading it more, I couldn't begin to do this on my own. 
Pretty much the only reason LINQ expressions *aren't* monad comprehensions is that C#'s type system can't do `return`, otherwise LINQ gives you all the `fmap` and `(&gt;&gt;=)` that you want. One of the C# language guys even had a blog post where he implemented the continuation monad for LINQ.
This kind of conclusion makes me wonder — why do we even bother with laziness?
I'm not a haskell user, and don't know the proper venue for reporting errors, but the cabal install seems to fail on windows too: src\System\Plugins\Consts.hs:38:22: lexical error in string/character literal at character 'd' update: With a little persistence, I figured it out. The config.h file had un-escaped backslashes in the GHC LIB PATH variable, which I removed manually. After that I had to add import System.Environment (getEnv) import System.Plugins.Consts (ghcLibraryPath) to the Env.hs file. After that, everything seemed to work. update2: The lib is installed, and the tests seem to compile okay, but eval fails when run: blarg.exe: N:\dev\haskell-platform\2010.2.0.0\lib\base-4.2.0.2/HSbase-4.2.0.2.o: unknown symbol `_recv' blarg.exe: user error (resolvedObjs failed.) Some kind of link error I'd image?
I'm *so* impressed by your references to Greek antiquity. Being a "cynic" sounds so much better than being a troll.
See for instance this recent [post](http://tom.lokhorst.eu/2010/09/why-libraries-are-better-than-frameworks).
I don't care how you label it. I'm spurring improvement and the ostentatious tactics are garnering attention. Your delicate sensibilities were apparently bruised, but I'm hoping you'll be able to grow a fucking dick and remember that this is the internet. Or maybe not, you can continue to serve my amusement at your own expense as much as you like.
&gt; I'm spurring improvement Keep telling yourself that. &gt; the ostentatious tactics are garnering attention. Well, it's all about attention, isn't it? &gt; you can continue to serve my amusement at your own expense as much as you like. You assume that I'm not equally amused.
This guy's problem isn't really laziness. His problem is not understanding it. Laziness and strictness are meant to work in harmony with each other, and haphazardly shotgunning `seq` and bang patterns throughout your code is usually only going to make things worse and more confusing in the long run, despite any short term benefits.
&gt;You assume that I'm not equally amused. Then let us dance! http://www.youtube.com/watch?v=kwqDa7uaBo4 Dance ya punk bitch, dance!
That show sucks a big one.
You'd know what constitutes big.
I may have misunderstood something, but it seems to me that the solution he settled upon was making all his data structures strict. Is that right? Can you suggest a different solution? Also, can you recommend any reading material specifically for understanding laziness?
Well, the way I see it, the process of turning it into "more function than data" is typically using [Church Encoding](http://en.wikipedia.org/wiki/Church_encoding). This is not what happened in this article, but it addresses your question of how to systematically translate "data type" into "more function than data". One straightforward way to get the church encoding has a funny name: "catamorphism". The "catamorphism" of an ADT would be a function that converts the ADT value to a higher-order function. This function takes an argument for each possible data constructor and returns the one that corresponds to the correct constructor. For example: data Bool = False | True Can be represented, instead, by: false f t = f true f t = t Such that both false and true have the same type: false, true :: r -&gt; r -&gt; r And indeed, this is exactly [the encoding Church used](http://en.wikipedia.org/wiki/Church_encoding#Church_booleans). We can now write a function `bool` to be Bool's catamorphism. A function that takes a normal Haskell boolean and returns its church encoding: bool :: Bool -&gt; r -&gt; r -&gt; r bool True f t = t bool False f t = f Let's try a more complicated example: data Maybe a = Nothing | Just a The catamorphism here now has a constructor "Just" that has a value inside it. So it makes sense to make the argument that corresponds to "Just" be a function, so it can be given the contents of the Just. So if `bool :: Bool -&gt; r -&gt; r -&gt; r`, then `maybe :: Maybe m -&gt; r -&gt; (m -&gt; r) -&gt; r` (Where `m` is the type of the content of Maybe). Indeed, the function "maybe" is: maybe :: r -&gt; (m -&gt; r) -&gt; Maybe m -&gt; r Which is the same (except for flipped-order args). For lists, this is slightly more complicated, because of the recursive reference: data [a] = a : [a] | [] So we would get: list :: [a] -&gt; (a -&gt; [a] -&gt; r) -&gt; r -&gt; r Note that we have `[a]` as an argument type for the cons case in there. To get a fully-functional representation without ADT's, we need to get rid of that too. Fortunately, we have the function "list" that can convert `[a]` to its church encoding. By using it recursively within `list`, we can get the type: list :: [a] -&gt; (a -&gt; r -&gt; r) -&gt; r -&gt; r Note, this is just foldr, with a flipped argument order. So "foldr" is the catamorphism of lists. What happens in this article, however, is a bit different. The performance problem we have with normal lists is that since they're built from immutable components, if we want to "append", we can't just modify the last cons cell -- we have to replace every reference along the way to point to the new path to a new cons cell. To solve this, we can leave the "end of the list" an open argument, so that we can always "modify" it simply by changing the argument. So `[a] -&gt; [a]` can be read, here, as "list that replaces its EmptyList end with given arg". That way, you don't have to duplicate all the cells to "change" the end. The broad perspective that can lead to this kind of solution in more contexts is: * If you have some change which is expensive, try to put your data structure as a result of a function that takes the things that needs to change as an argument. Let's look at another example of this approach. Images are in many languages represented as rectangular arrays of pixels or such. Then, if you want to change the position of the image, or the rotation, you have to do a lot of work. IOW, changing the position or rotation of the image should be made an argument of the image. So if you replace: data Image = Image RectOfPixels With: data Image = Image (Position -&gt; Rotation -&gt; RectOfPixels) then you can easily fix each of the drawing functions to return this new kind of image, by allowing them to rotate the image they're drawing in the first place. Then, rotating/moving becomes an O(1) argument change operation. This is not a really good representation type for images in general, but it's a big leap towards a good one. See Luke Palmer's article about Semantic Design for more information about this example: http://lukepalmer.wordpress.com/2008/07/18/semantic-design/
I wish I had gone. 
I've seen all of those proofs before. :-/
Do you have a link to that video?
Happstack is pretty much not like that. It can be cumbersome, but the individual pieces aren't coupled in any way. The HTTP server/pathing can be easily used separate from the persistence layer. To be honest there isn't even anything gained by using them together. The data versioning bits are even released in a separate library (even though the persistence mechanism relies on it).
He seemed so nervous.
To some extend, I count myself among those afflicted, that's why I prefer my [operational](http://projects.haskell.org/operational/) package for writing monads. It's not high performance, but it's easy to get any semantics I dream of ([examples](http://projects.haskell.org/operational/examples.html)).
I have written some preliminary stuff on the wikibook ([introduction](http://en.wikibooks.org/wiki/Haskell/Performance_Introduction), [graph reduction](http://en.wikibooks.org/wiki/Haskell/Graph_reduction)), but I prefer the explanation in Richard Bird's book. Concerning laziness, it's so pervasive in Haskell that you'll be surprised at how many things don't work anymore when you take it away. The example `zip [1..]` comes to mind, as do parser combinators and the `&gt;&gt;` operator.
Here is an example of some source code in Omega: http://code.google.com/p/omega/source/browse/#svn/trunk/tests
Just came to say that I learned Haskell from a class taught by Tim Sheard (Omega) and Mark Jones (HUGS) at Portland State University. It was a good class.
Not saying it's directly equivalent, but [Harpy](http://hackage.haskell.org/package/harpy) does stuff like this in a nice controlled way, only it doesn't work on x86_64 which is a bummer. :(
[LLVM](http://hackage.haskell.org/package/llvm) isn't equivalent to either, but also can be used for similar effect (and it probably is the most pleasant to use among the three for more complex stuff)
because: % ./test 6 zsh: segmentation fault ./test 6 moder architevture emplyos X^R either in software or in hardware. You should mmap position with PROT_EXEC flag to execute.
Exciting times to be in Baltimore: seems like everyone's going to be there, which makes me doubly disappointed to be missing it.
I think Omega is just an attempt to get DT into Haskell while keeping most Haskellisms. If backwards compatibility isn't necessary, what advantages does it have over Agda or Epigram? 
Me too. Especially seeing as I just moved away from Baltimore a year ago...
I've played with Omega, here's the result: http://hpaste.org/40123/kind_polymorphism_in_omega I defined monoids such that Monoid Hask is a Haskell monoid, Monoid EndHask is a Haskell monad, and Monoid Graph is a category. [Previously](http://monoidal.blogspot.com/2010/07/kind-polymorphism-in-action.html) I've written that for UHC, but it doesn't support GADTs, so the last example was not possible. The result is this: x :: Monoid Hask Int -- instance Monoid (Product Int) y :: Monoid EndHask Maybe -- instance Monad Maybe z :: Monoid Graph (-&gt;) -- instance Category (-&gt;) where definitions must give unit / multiplication, return / join, or id / (.). Annoyances: it seems you cannot display a higher-rank type. Typing ":t Monad" crashes the interpreter. I couldn't understand error messages when confusing levels (kinds with types etc.) so initially I had to bang against a wall a lot.
A bit of background: I am the author of monad-embed. I submitted a [flawed early version](http://www.reddit.com/r/haskell/comments/c1zhz/a_possibly_new_syntactic_sugar_for_monads_in/) of monad-embed to /r/haskell a few months ago; some problems were pointed out and I fixed them. I also wrote a prototype interpreter, documentation, and some toy example programs. I then [submitted](http://www.reddit.com/r/compsci/comments/detfo/monadembed_is_an_experimental_language_that/) it to /r/compsci. I'm now resubmitting it to /r/haskell, even though it isn't strictly Haskell-related, because /r/compsci is so small. (Not that /r/haskell is larger... Maybe I should try /r/programming.) Questions, comments and criticism welcome.
This is about how I see it. It's an attempt to approximate dependent types without moving too far away from Haskell. For instance, a cursory search suggests that GADTs originated with Omega. GHC has since picked them up, but Omega still has some other nice features that GHC doesn't. The obvious advantage is that Haskell has high-performance compilers _now_, and Omega doesn't really do anything that breaks that significantly. There's still a simple distinction between runtime stuff and compile time stuff, the latter of which gets erased. You can do more fancy stuff at compile time, but it all disappears. ATS seems to be similar. But, I don't think there is any fundamental limitation of dependently typed languages that makes them unable to compete with the above. It just hasn't happened yet. There are, keeping with the above, ideas for having static/dynamic divides in dependently typed languages that don't rely on a simple type/kind versus value distinction (see modal type theory, erasure pure type systems, dependent intersection types...), and I think these are likely to be better solutions in the long run than being conservative and Haskell-like. Maybe I'm overlooking something, though. The author talks about (paraphrasing) 'dependent programming without the downsides,' but never specifies what he's talking about. I can think of downsides to current implementations, but quite a few aren't really fundamental (rather, they're issues of priorities and manpower/expertise), and languages like Omega and ATS have their own downsides by comparison.
a) As for code reuse - technically you can define map/filter/... as: map = runIdentity . mapM (Identity . f) mapM _ [] = return [] mapM f (x:xs) = liftM2 (:) (f x) (mapM f xs) vs. map _ [] = [] map f (x:xs) = f x : map f xs So if you are concerned about reuse you can always write monad version and run in Identity. I'm not sure if added complexity &amp; stict evaluation justifies this approach. b) As of FAQ: As of pair of 1 or 2: pairOfOneOrTwo = liftA2 (\x -&gt; (x, x)) [1,2] is not so much more complicated then: pairOfOneOrTwo = unwrap [do { x = either [1] [2]; } then Pair x x]; even if shortened to for example: pairOfOneOrTwo = *[do { x &lt;- [1,2]; } &gt;&gt; (x, x)}] Also example powerset seems to be simpler: powerset = filterM (\_ -&gt; [True,False]) Also - not every applicative functor is monad so there are situations when you CAN use applicative functor but you CANNOT use monad. 
LYAH first on the list ;) nice job BONUS
Hello -- I remember you from my [old post](http://www.reddit.com/r/haskell/comments/c1zhz/a_possibly_new_syntactic_sugar_for_monads_in/). &gt; So if you are concerned about reuse you can always write monad version and run in Identity. The issue is not so much code reuse as that in Haskell, the monadic version of the function is going to be more verbose than the pure version. Even if the added verbosity is not very much, it is enough that people write pure versions by "default". &gt; As of FAQ: As of pair of 1 or 2: &gt; &gt; pairOfOneOrTwo = liftA2 (\x -&gt; (x, x)) [1,2] &gt; &gt; is not so much more complicated then: &gt; &gt; pairOfOneOrTwo = unwrap [do { x = either [1] [2]; } then Pair x x]; (I'm assuming you meant `liftA (\x -&gt; (x, x)) [1,2]`.) I have rewritten that part of the FAQ because it misses the point. The important thing is that monad-embed works with monads, which are more powerful than applicative functors. &gt; Also - not every monad is applicative functor so there are situations when you CAN use applicative functor but you CANNOT use monad. monad-embed and applicative functors are not mutually exclusive. When you need an applicative functor that is not a monad, you can use `&lt;*&gt;` and `pure`, or applicative idiom brackets, or whatever. (The current version of monad-embed doesn't support infix operators, but that's irrelevant.) I also [added this to the FAQ](http://timmaxwell.org/pages/monad-embed/faq.html#applicative-functors-2). &gt; Also example powerset seems to be simpler The powerset example given on the front page is written in such a way that I can explain it to people. I could have written it as powerset = filter (\_ -&gt; either [True] [False]) If we defined `or` as `either` (and added some syntactic sugar that isn't currently present), it could even be: powerset = filter (const ([True] `or` [False])) 
&gt; Also - not every monad is applicative functor so there are situations when you CAN use applicative functor but you CANNOT use monad. I was under the impression that they were, as in mf &lt;*&gt; mx = mf &gt;&gt;= \f -&gt; mx &gt;&gt;= \x -&gt; return (f x) Could you elaborate? Does it have to do with any laws that applicative functors must satisfy?
I think he meant the opposite: not every applicative functor is a monad. This is also consistent with the second half of his sentence.
I thought about being in that class(and dropped by the first two days) but ultimately decided not to unfortunately. I ended up taking the same class with just Professor Sheard a year later (although Professor Jones dropped in for a lecture or two. Compilers with Professor Jones was fun too.
Could someone explain to me what this code is meant to demonstrate?
Correcred.
I believe its meant to demonstrate how to take binary data representing a short machine code program in a variable and run it.
Yeah I really enjoyed the compilers class taught by Jones. Probably my favorite CS class at PSU.
I think a comparison with [Disciple](http://www.haskell.org/haskellwiki/DDC) would be helpful.
&gt; monad-embed takes this to its logical conclusion by parameterizing everything on a monad by default There are various possible generalizations of any arbitrary piece of code. Parametrization for different monads is just one of them. I'm not sure built-in language support for this is a good thing. I think maybe just writing the non-monadic version in terms of the monadic one (and the Identity monad) should suffice.
&gt; There are various possible generalizations of any arbitrary piece of code. Parametrization for different monads is just one of them. That's true, but it's a very useful one. &gt; I think maybe just writing the non-monadic version in terms of the monadic one (and the Identity monad) should suffice. That's the same thing that [mpiechotka suggested](http://www.reddit.com/r/haskell/comments/dixtp/monadembed_is_an_experimental_language_that/c10jqk8). The main problem with this is that in Haskell the monadic version of a piece of code is going to be more verbose, so people don't and won't parameterize their Haskell code on a monad.
Um, if I'm not totally mistaken, then there are no tests in the testsuite tarball. Edit: Has been fixed.
*Yeah!* And dons is even serving cake today...
[Done](http://timmaxwell.org/pages/monad-embed/faq.html#disciple).
Is there a changelog?
http://darcs.haskell.org/ghc/docs/users_guide/7.0.1-notes.xml
If I'm just getting started learning, this doesn't really matter, does it?
I have not seen any large changes in 7.0.1's notes.
Thanks
how do I view that? chromium gives wall of text, firefox displays xml source code - both are unreadable.
Completely unreadable with Firefox. Is it too much to ask to just provide a text file, as opposed to XMLing everything? Why does a changelog need to be in XML format anyway?
&gt; I think maybe just writing the non-monadic version in terms of the monadic one (and the Identity monad) should suffice. This is brutally painful in the general case. For example, monad-embed (minus the kind noise): flip :: {m} (a -&gt; b -&gt; c) -&gt; b -&gt; a -&gt; c flip f a b = f b a Haskell: flip :: Monad m =&gt; m (a -&gt; m (b -&gt; m c)) -&gt; m b -&gt; m a -&gt; m c flip f a b = f &gt;&gt;= \f -&gt; a &gt;&gt;= \a -&gt; b &gt;&gt;= \b -&gt; f b &gt;&gt;= \g -&gt; g a Now, an "effectful" version of 'flip' (if that's what you want to call it) is perhaps silly. In Haskell, you'd just use do notation or bind explicitly to run actions before passing them to flip, explicitly bind the result to some function, et cetera. With the monad-embed approach, you can write a monadic flip easily and just give it a type signature that indicates 'm' is 'Identity' to get the "pure" version. Since everything in the language is appropriately lifted, no explicit 'return' and 'runIdentity' is required as would be required in the Haskell version: flip_pure :: (a -&gt; b -&gt; c) -&gt; b -&gt; a -&gt; c flip_pure f b a = let f' = return (\a -&gt; return (\b -&gt; return (f a b))) a' = return a b' = return b in runIdentity (flip f' b' a') Again, a bit silly. Point being though, writing the most monadically-general version and then deriving non-monadic versions is extremely nasty in the general case. No one is going to do it except in cases where they foresee themselves needing it. We've seen that become a problem with Haskell libraries where monadic versions are unavailable because the author didn't foresee anyone needing them.
Semi-arbitrarily filtered list: &gt; * The full Haskell "import" syntax can now been used to bring modules into scope in GHCi, e.g. Prelude&gt; import Data.List as L Prelude Data.List&gt; L.length "foo" 3 &gt; * GHC now returns memory to the OS, if memory usage peaks and then drops again. This is mainly useful for long running processes which normally use very little memory, but occasionally need a lot of memory for a short period of time. &gt; &gt; * GHCi now understands most linker scripts. In particular, this means that GHCi is able to load the C pthread library. &gt; &gt; * GHCi now understands layout in multi-line commands, so this now works: Prelude&gt; :{ Prelude| let x = 1 Prelude| y = 2 in x + y Prelude| :} 3 &gt; * It is now possible to use a quasi-quoter for types, e.g. f :: [$qq| ... |] &gt; * Control.Monad exports a new function "void :: Functor f =&gt; f a -&gt; f ()". &gt; &gt; * The types provided by the time package now include Data instances. 
Not large for the newcomer, but there are plenty of large changes. Perhaps the main things for newcomers are: * full `import` syntax in GHCi: import Data.Map hiding (insert) import qualified Data.ByteString as B * `--make` is the default, so fewer people should complain about linker errors (and everyone on IRC rejoices) * A Monad instance for Either is in base. Other items I consider of great interest: * a new I/O manager (greatly improves scalability, see Johans work) * an LLVM code generator (some superb performance benefits, see Dons blog) * type checker has been overhauled - if you don't care about the type checker then I don't know what you're doing using Haskell * Haskell 2010 support * Shared libraries are once again supported on Windows. * An `-rtsopts` link-time option to enable the full range of runtime RTS options (which are now disabled by default)
Funny to see what different peoples take are on the important aspects. [--&gt;Mine](http://www.reddit.com/r/haskell/comments/dj5wu/announce_ghc_701_release_candidate_1/c10llyu) for example doesn't include anything about the quasi-quoter or multi-line GHCi as I've never considered those of use in my daily Haskell hacking.
Functor and Applicative (maybe also Category) would work in some cases, as you can lift some pure functions to fancy functions. But I guess that only for Monad you can make a monadic version for every pure function. That means that a function will be Functoric and not Monadic depends on what you do in it. That might be awkward.
&gt; We've seen that become a problem with Haskell libraries where monadic versions are unavailable because the author didn't foresee anyone needing them. Wouldn't an instance of Traversable resolve much of this issue?
&gt; type checker has been overhauled - if you don't care about the type checker then I don't know what you're doing using Haskell I guess many people care that it works, but don't really care about an overhaul..
If you're a newcomer, you probably don't care about the type checking stuff that's changed much. It's nothing Haskell98-related (nor 2010, I think).
Woo! I, for one, am quite happy about the type system overhaul. Sadly it's still too slow for some useful type-level computation. However the overhauled system is *much* more reasonable and therefor, hopefully, easier to optimize.
Seems like a DocBook source to the GHC documentation.
Did higher-ranked types get improved? i.e: Will `runST . forever $ do ...` type check?
&gt; &gt; let ones = 1:ones &gt; It creates a function that, once called, creates an infinite list of number ones. And it’s a perfectly fine piece of code, because of laziness. Here’s what happens when we call ones: Nitpick: This creates an infinite list, or a definition, not a function. There's no function in there (except in the operational semantics of laziness, perhaps).
Yes, if it's possible to write one without accessing private details of the library in question. There are, of course, possible efficiency issues of which to be aware. Luckily, GHC fuses fairly well nowadays I assume. For what it's worth, I don't think the monad-embed approach is worth the cost. The complication of equational reasoning and the order-sensitivity of it removes much of what is great about Haskell. 
In my Haskell projects, I've not really encountered a lack of monadic functions (I once had to write takeWhileM, and once I needed a tail-recursive sequence, I think that's about it :-). Lifting functions to applicative is something I do all the time though. The monadic "flip" seems useless, do you have any example use of it? An applicative-lifted flip is used all the time. I wonder if there can be some convincing examples of the need for this stuff? I don't buy mapM (since it is trivially defined via sequence/map) to be a convincing case. filterM is slightly more convincing, though you'd probably need it in implicitly-effect-typed languages too (For monadic parsers, etc), afaiu. tl;dr: Any convincing examples (more-so than mapM/filterM) that wouldn't be resolved by instances of the Traversable class?
 plusOrMinus = either [plus] [minus] quadraticFormula = \ a b c -&gt; (negate b `plusOrMinus` sqrt ( (b `times` b) `minus` (4 `times` a `times` c) )) `divide` (2 `times` a) I'm not sure this example illustrates the benefit of auto-embedding in monads. plusOrMinus is specifically in the list monad, so just lifting the (/ 2*a) there manually has no significant drawback. 
I meant data structures (such as Data.Map, which I understood was blamed for not exporting a "monadic" interface) should export a Traversable instance, and then this "problem" is solved, afaiu? Yeah, I like Haskell's explicit *lack* of evaluation order, and an implicit monadic effect behind every expression seems to restore it. I think most of Haskell's pure functions are inherently pure and should not be complicated with the intertwined monadic effects. Those functions that traverse a potentially large structure might want to "intertwine" the monadic effects in between the computations -- and that seems to already be solved by Traversable.
Terrible cameraman. He should have focused 90% on the diagrams, and 10% on the speaker, not the other way around. The proofs are interesting, but it seemed like he was trying to take credit for them?
This is an issue with any higher-order function. `Traversable` resolves the issue only for `map` and its equivalents for other data structures, such as `Data.Map.map`. It doesn't help with non-`map` operations, such as `filter` or `sort`, or even with other `map`-like functions, like `Data.Map.mapWithKey`. monad-embed is a general solution. Edit: Actually, `Traversable` doesn't even save any work. Creating an instance of `Traversable` is just another way of defining `mapM`.
Just use the [Haskell Platform](http://haskell.org/platform)
If you have mapM and fmap, you can build filterM, so you don't need a monadic filterM too: filterM p xs = map fst . filter snd . zip xs &lt;$&gt; mapM p xs What do you mean by a monadic sort? What would it do? I doubt mapWithKey can be any better than just fromList . map f . toList, so it might be a bad example. I'm not sure it's a bad idea to embed monads everywhere automatically, but I think I would need a really convincing example to let go of the niceness of "unspecified evaluation order" :-)
Doubtful. In fact, I think it got worse. I've had to fix some stuff that used `($)` which used to work. You used to be able to write: foo :: (forall b. b) -&gt; (forall b. b) foo = id and GHC would instantiate the `a` in `id :: forall a. a -&gt; a` impredicatively (regardless of `ImpredicativeTypes`). Now, it refuses to do that, even _with_ `ImpredicativeTypes` (which still exists, to my surprise, and still enables stuff like `Maybe (forall a. a)`).
Read [this page](http://timmaxwell.org/pages/monad-embed/why.html) if you haven't already. &gt; Any convincing examples (more-so than mapM/filterM) that wouldn't be resolved by instances of the Traversable class? Here are some: `foldrM` and `foldlM`, `sortByM`, `findM`, and almost every higher-order function in `Data.Map`. Besides, `Traversable` doesn't actually help. Defining an instance of `Traversable` for a data type is just another way of defining `mapM`. It doesn't save the programmer any work.
That example is meant to help people see what monad-embed is, not to demonstrate why it is useful.
Looking at the patches on cvs-ghc, it looks like SPJ quietly de-deprecated ImpredicativeTypes. Haven't seen any human-readable text by him on the subject since he last talked about deprecating it, though.
&gt; What do you mean by a monadic sort? What would it do? In Haskell, its type signature would be sortByM :: Monad m =&gt; (a -&gt; a -&gt; m Ordering) -&gt; [a] -&gt; [a] Some uses are: * Print out each comparison as you performed it, or ask the user to perform the comparisons, or ask a remote server to perform the comparisons, using the `IO` monad * Count the number of comparisons, or the number of times a given element is compared, using the `State` monad * Report errors in the comparison process using an `Either` monad. For example, if you were comparing objects based on priorities that the user had assigned to them, and two objects had the same priority, you might want to report that to the user. If you wanted to just warn the user rather than aborting the sort, you could use the `Writer` monad instead. &gt; I doubt mapWithKey can be any better than just fromList . map f . toList, so it might be a bad example. `mapWithKey` is not equivalent to `fromList . map f . toList`. &gt; the niceness of "unspecified evaluation order" I can think of several things that are nice about unspecified evaluation order, and some of them are preserved in monad-embed. What in particular were you thinking of?
&gt; Read this page if you haven't already. Here goes: &gt; monad-embed makes writing code that uses monads as easy as writing pure code. It makes do-notation unnecessary in many cases. It makes sequence, liftM, and other functions unnecessary. This saves labor by making the monadic code that people already write less verbose. Very little of my Haskell code is monadic code, the vast majority is pure. More code is Functor/Applicative than is Monad. &gt; More importantly, this makes it easier to use functions other people have written. Because functions in monad-embed are parameterized on an arbitrary monad by default, functions like mapM and filterM are unnecessary. Haskell offers monadic versions only of common functions such as map, filter, and foldl; when a programmer needs a monadic version of another function, they must write one for themself. More often, they write a manual solution. This reduces code reuse. I think code re-use should be achieved using existing mechanisms, rather than a new mechanism. For example, if filter was implemented in terms of map, maybe with some extra type-class, we could get filterM for free (given that we have mapM, too). I think the [List](http://hackage.haskell.org/package/List) package has the right idea. Did we really exhaust existing mechanisms for polymorphism before inventing a new one? Also, mapM is trivially defined in terms of sequence and map/Traversable. filterM trivially defined in terms of mapM. fold*M seem to be trivially definable as sequence and a normal fold. I've never actually used fold*M myself. &gt; While this works, it is far more verbose than mapWithKey processFun. It would be best if there were a monadic version of mapWithKey, but it is not feasible to define two versions of every function in the standard library. I don't think *every* function would need a composition with sequence or mapM, especially since it's just a minor convenience, and not actually adding power. Some would define this convenience, and it's really not a big deal: mapWithKeysM f = liftM fromList . mapM f . toList Even inlining that, you get: processPair (x, y) = (x,) `liftM` processFun y processFuns = liftM fromList . mapM processPair . toList Which is not so much worse than: processPair (x, y) = (x, processFun y) processFuns = mapWithKeys processPair I think the extra price paid here is making the sequencing explicit, which I am not sure is "verbosity" because it contains actual information (about where sequencing may take place, and which evaluation has unspecified order). &gt; monad-embed solves this problem by making every function work with any monad by default. If Data.Map were ported to monad-embed, the functions it defines would work with any monad without any extra effort on the part of the programmer. I think this creates new problems -- it gets rid of the nice "unspecified evaluation order" stuff. It is yet-another-mechanism for polymorphism, when existing mechanisms may be stretched to cover this ground. &gt; A programmer trying to debug a complicated library might temporarily instantiate their entire library in the IO monad to allow them to write debugging output to a file, much as programmers in imperative languages temporary add print calls when trying to track down bugs. This would work without hacks like using unsafePerformIO or Debug.Trace to write debugging information. What about laziness? Debug.Trace is a hack precisely because it exposes laziness, evaluation order, breaks referential transparency, etc. &gt; We can instrument this to print a message every time we compare two numbers without changing the definition of sort at all; we only need to define our own comparision function: What about referential transparency? &gt; Here are some: foldrM and foldlM, sortByM, findM, and almost every higher-order function in Data.Map. These all don't seem to add any power at all, just convenience wrappers around sequence compositions. &gt; Besides, Traversable doesn't actually help. Defining an instance of Traversable for a data type is just another way of defining mapM. It doesn't save the programmer any work. Are you trying to solve an expressiveness problem (Cannot express this when library forgot to implement mapM), a performance problem (implementing mapWithKeys using fromList/toList is inefficient), or a verbosity problem? I think Conal Elliott was the one who said that solving complexity problems by adding syntactic features is solving the symptom. That the problem is semantic complexity, and we should fix that by simplifying the semantics, not find better syntactic ways to express that complexity. 
Bummer. Higher-rank types sounds like an interesting direction for Haskell to research, but if `runST $ do ...` doesn't type-check, it's probably not very usable...
There isn't any chance ghci will get support for creating adts or declaring types of functions inside ghci?
I wouldn't hold out hope for `runST $ do ...` ever making it back in. The rule that enabled it essentially said that type checking of `f a b c` _had_ to check `a` first, and use it to instantiate things in `f` before checking `b`, and so on. This allowed the higher-rank type in `runST` to instantiate the `a` in `($) :: a -&gt; b` to `(forall s. ST s r)`, which then affected the result of checking the do-block, causing it to be inferred with the `forall s`, which wouldn't normally happen. But, this is of course ad-hoc. If you want something where the right argument affects the left argument in a similar way, well, you're out of luck, it only works left-to-right. Perhaps there are additional technical reasons for it being undesirable, as well.
Exactly why the type checker is of great interest.
You can already declare types of functions inside GHCi. Not sure about the barriers to writing ADTs.
runST $ do { ... } type-checks just fine for me with the new type-checker. runST . forever $ do { ... } does not, however. You can re-parenthesize it to make it work, though.
&gt; For example, if filter was implemented in terms of map, maybe with some extra type-class, we could get filterM for free (given that we have mapM, too). I think the List package has the right idea. I'm don't think that would save any work. Can you elaborate on how you would define `filter`? In particular, I suspect that writing the instances of the "extra type-class" would end up being just as much work as implementing `filterM` manually. (Just to clarify, we're talking about a general `filter` that works on arbitrary data types that are instances of `Traversable`.) &gt; mapWithKeysM f = liftM fromList . mapM f . toList Your definition of `mapWithKeyM` is wrong in the sense that it's not equivalent to a monadic version of `mapWithKey`. `mapWithKeyM` should have the type signature `Monad m =&gt; (k -&gt; a -&gt; m b) -&gt; Map k a -&gt; m (Map k b)`. The correct definition of `mapWithKeyM` is more complicated. Here's the best I could come up with: mapWithKeyM f = liftM fromList . mapM (\(k,v) -&gt; f k v &gt;&gt;= (k,)) . toList That's not trivial, and there are dozens of higher-order functions in `Data.Map`. &gt; `fold*M` seem to be trivially definable as `sequence` and a normal fold. &gt; &gt; Here are some: foldrM and foldlM, sortByM, findM, and almost every higher-order function in Data.Map. &gt; &gt; These all don't seem to add any power at all, just convenience wrappers around sequence compositions. I don't know of any way to define `fold*M` in terms of `sequence` and a normal fold; I note that [Data.Foldable](http://www.haskell.org/ghc/docs/6.12.2/html/libraries/base-4.2.0.1/Data-Foldable.html#v%3AfoldrM) defines it manually rather than in terms of `sequence`. I don't think the others can be defined in terms of composition with `sequence` either. Have you tried actually defining them? &gt; What about laziness? monad-embed basically breaks laziness; functions in monad-embed are strict by default. Lazy functions are available if you specify them explicitly. How much you care about laziness is a matter of personal opinion; I suspect that most Haskell programmers don't use laziness very often. &gt; What about referential transparency? You lose referential transparency, but you keep most of its benefits. You can still see exactly what an expression can do by looking at its type signature. &gt; Are you trying to solve an expressiveness problem (Cannot express this when library forgot to implement mapM), a performance problem (implementing mapWithKeys using fromList/toList is inefficient), or a verbosity problem? I'm trying to solve an expressiveness problem (if I understand how you are using the term). Although, I would argue that expressiveness and verbosity are really the same problem. It is possible to implement monads in C, but they would be very verbose. &gt; I think Conal Elliott was the one who said that solving complexity problems by adding syntactic features is solving the symptom. That the problem is semantic complexity, and we should fix that by simplifying the semantics, not find better syntactic ways to express that complexity. I think that what is important about a language is what it makes cheap, what it makes expensive, and what it makes free. Syntactic sugar can make something expensive into something cheap, and that can be very important. monad-embed is a bit more than a syntactic sugar, but it makes parameterization on an arbitrary monad free, and I think that's very useful. I didn't reply to some parts of your comment; if you think I dodged something important, then point it out.
As an aside, it works in UHC: data ST s a instance Monad (ST s) run :: (forall s. ST s a) -&gt; a run = undefined for :: Monad m =&gt; m a -&gt; m b for = undefined v = for . run UHC doesn't seem to have Control.Monad.ST, at least in the old version I have. Omega crashes given equivalent code.
I've nothing useful to add at this point as my brain is still processing it. Losing some of the equational reasoning properties of Haskell is a shame, but being able to recover them by proving that the "flavor" is the identity monad can be done automatically (so maybe it's not so bad). This seems like something that would've been discovered ages ago. Any related prior work?
&gt; being able to recover them by proving that the "flavor" is the identity monad can be done automatically It might be better to do it by proving that the flavor can be any monad. It might even be possible for the compiler's code generator to optimize away the monad completely if the flavor can provably be any monad. &gt; This seems like something that would've been discovered ages ago. Any related prior work? That's what I thought too, but I haven't been able to find any prior work on this.
Thanks after you told me that i found [this page](http://efreedom.com/Question/1-3093133/Provide-Explicit-Type-Declarations-Functions-Using-GHCi) which showed that you could do it like that let numUniques' :: (Eq a) =&gt; [a] -&gt; Int; numUniques' = length . nub and it also works for pattern matching (let a 1=True;a 2=False), awesome now if they just allowed creating ADT things would be perfect(or am I wrong again and there is a way to declare new adts in ghci?)
The dirty &amp; fast way: import Network.HTTP import qualified Data.Map as M import Text.HTML.TagSoup url = "http://darcs.haskell.org/ghc/docs/users_guide/7.0.1-notes.xml" ls = M.fromList [("para", "p"), ("title", "h1"), ("itemizedlist", "ul"), ("listitem", "li"), ("screen", "pre")] jn (Just x) xs = x:xs jn Nothing xs = xs cv [] = [] cv (x@(TagText _):xs) = x : cv xs cv ((TagOpen s a):xs) = jn (fmap (`TagOpen` a) (M.lookup s ls)) (cv xs) cv ((TagClose s):xs) = jn (fmap TagClose (M.lookup s ls)) (cv xs) cv (_:xs) = cv xs main = do str &lt;- getResponseBody =&lt;&lt; simpleHTTP (getRequest url) putStrLn $ renderTags $ cv $ parseTags str runhaskell x.hs &gt; x.html
I suddenly noticed that the "i"s in the title of this submission are missing their dots. Unicode!
Lowercase of I is ı and lowercase of İ is i in Turkish and it makes quite sense :-) But it causes problems with i18n related software. Fortunately Unicode has solutions for these exceptional cases.
&gt; It is now possible to use a quasi-quoter for types This one's for snoyberg :)
TIL ...
Turkish was designed to fuzz out internationalization software. I'm completely sure it can't be a coincidence.
I agree. He could've called it a piece of data. Arguably, the property of being a CAF is alluded to here but the jargon won't fly in a tutorial. 
Yes, please, please someone elaborate on the downsides of dependent programming.
http://community.haskell.org/~ndm/temp/supero.pdf - you missed my paper! (i haven't put up a version in a final place yet, but I will when I get back)
A better link (to the final version) for "Scrapping your inefficient engine" is http://www.cs.st-andrews.ac.uk/~eb/writings/icfp10.pdf Thanks for this!
Videos! I want videos!
Abstracting abstract machines: http://www.ccs.neu.edu/home/dvanhorn/pubs/vanhorn-might-icfp10.pdf
I'm a Baltimoron who isn't at ICFP but would be happy to provide travel advice or pub/bar/restaurant/museum suggestions for any attendees. Hit me up here, at [@llimllib](http://twitter.com/llimllib) at bill.mill@gmail.com . I love my city, don't be scared by having seen *The Wire* :)
&gt; Baltimoron Is that the real word or self-deprecating humor? (I'd find it funny either way, but I'm curious)
Humor, although the more accepted "Baltimorean" doesn't exactly roll off the tongue.
Looks like binary won't compile for me on my mac with this version...
&gt; Print out each comparison as you performed it, or ask the user to perform the comparisons, or ask a remote server to perform the comparisons, using the IO monad Well, if comparisons are not guaranteed to be transitive/consistent, then it's buggy. If they are, it is safe to unsafePerformIO them, because they really are a pure input. As for printing comparisons as they happen, this seems very problematic with non-sequential evaluation, anyway -- so you might want to distinguish a potentially parallel-evaluation sort (sort) from a sequential-one (sortM). &gt; Count the number of comparisons, or the number of times a given element is compared, using the State monad Not sure why I would ever do that :-) &gt; Report errors in the comparison process using an Either monad. For example, if you were comparing objects based on priorities that the user had assigned to them, and two objects had the same priority, you might want to report that to the user. If you wanted to just warn the user rather than aborting the sort, you could use the Writer monad instead. This could be done by verifying the priorities separately. &gt; mapWithKey is not equivalent to fromList . map f . toList. Oh, oops, sorry. I guess wrapping the composition of sequence with mapWithKey is the reasonable thing to do then. &gt; I can think of several things that are nice about unspecified evaluation order, and some of them are preserved in monad-embed. What in particular were you thinking of? Allowing compilers to optimize, e.g: automatically insert speculative parallelism, alternate between two evaluation orders, etc. When everything is monadic, you have to specify an evaluation order (though I guess you could say it becomes unspecified again in the identity monad?). Also, what about guaranteed referential transparency?
So how can higher rank types and ST ever be first-class citizens in Haskell?
 v = for . run is wrong. Try: v = run . for $ return () 
So, some other posts around here have prompted me to test a little more, and I'm uncertain what all is going on. `runST $ do ...` now works. And if we instead define: ($$) :: a -&gt; (a -&gt; b) -&gt; a x $$ f = f x then `(do ...) $$ runST` blows up. So, it looks like I was wrong, and left-to-right instantiation is back in. Also: f :: ((forall c. c) -&gt; b) -&gt; (forall c. c) -&gt; b f = ($) -- or id works. So it seems I was incorrect about impredicative instantiation being gone. _But_ f :: (a -&gt; (forall b. b)) -&gt; a -&gt; (forall c. c) f = ($) -- or id _does not_ work. The error message leads me to believe that it's rewriting the type to: forall c. (a -&gt; (forall b. b)) -&gt; a -&gt; c (which is admissible) and then trying to match the solo `c` with `forall b. b` and failing. This is kind of weird. However, `forever` has a bigger problem. You have: forever :: Monad m =&gt; m a -&gt; m b and then: runST :: (forall s. ST s r) -&gt; r but, there is no way to instantiate `m b` to yield `forall s. ST s r`; this is doomed to not work. Instead, you need to group the `forever` with the rest of the monadic code, so that `m a -&gt; m b` can be instantiated to `ST s a -&gt; ST s b`, and the whole group implicitly generalized due to the demands of runST's type. So, `infixr $` fans rejoice: runST $ forever $ do ... is a case that is _not equivalent_ to runST . forever $ do ... due to these higher-rank shenanigans.
A direct pdf link for the semantic subtyping paper: http://www.infsec.cs.uni-saarland.de/~hritcu/publications/dminor-icfp2010.pdf
That's really helpful and I've re-read it several times. Can you describe the benefit I'd gain from constructing the catamorphism for one of my ADT's? It certainly looks Haskell-y. Or is it just a fun exercise if you like lambda calculus? 
&gt; Can you describe the benefit I'd gain from constructing the catamorphism for one of my ADT's? * Convenience/Conciseness: In many cases it is much more convenient to use a catamorphism than pattern-match the cases, e.g: case m of Nothing -&gt; 0 Just x -&gt; x + 1 Can be replaced by: maybe 0 (+1) m Recursive catamorphisms save you from having to recurse yourself, there's probably little need to portray the usefulness of "foldr". * Efficiency In some cases representing a data structure by its catamorphism can be more efficient. If one function returns a value of an ADT, and another uses "case" expressions on it, it might generate code that actually goes through an intermediate data structure, and then compares the tag, etc. If you instead return a catamorphism rather than a value of an ADT, the generated code might save the intermediate structure. * Fun As you said, it is fun to play with LC :-) * Other languages which don't have ADT's In C, for example, where ADT's a real pain to represent (An enum + A structure containing a tag + A union of structures), I sometimes use the catamorphism using a bunch of function pointers: struct parsed_data_vtable { void (*parsed_plus)(void *, plus_args); void (*parsed_minus)(void *, minus_args); ... }; void parse(char *expression, struct parsed_data_vtable *, void *arg); Can be used instead of an ADT, such that `parse` calls the appropriate function with the appropriate args to represent the right "data constructor".
I really hope higher-rank can be made to work seamlessly with the rest of Haskell, not breaking basic Haskell equivalence rules like the ($) vs (.) you mention below.
Can't wait to show off an elegant "wc" implementation in Haskell out-performing the one written in C :-)
I don't think any fix for this is going to be very simple. If we put in explicit type application, the `($)` case looks something like: runST $ /\s -&gt; forever@(ST s) $ do ... making the right thing happen with `(.)` is hairier: (runST . forever@?) $ do ... For the thing in parentheses to type, we have to fill in the question mark with: ? = \a -&gt; forall s. ST s a For one, that's a type-level lambda expression. For two, `forever` expects its type argument `m` to be an instance of `Monad`, and we can write instances for neither the limited type-level lambdas that Haskell _does_ allow, nor polymorphic types. So I believe we're out of luck.
This is really cool, even if you only care about optimizing over large but finite sets. I believe that by working with a result type `r = Bounds s` consisting of a lazy list of improving upper bounds on a final value of type `s`, you should be able to use this to implement *branch-and-bound* search with the same kind of separation of concerns between generation `g` and evaluation `f` that you have by writing a brute-force search in the form `maximum . map f $ g`. However, the internals are rather mysterious and the run time is highly sensitive to the choice of how you construct your set `g`. For example, consider the alternate definition cantor' = (cantor' &gt;&gt;= return . (False :)) `union` (cantor' &gt;&gt;= return . (True :)) The queries `supremum cantor' (!! n)` and `supremum cantor' (all . take n)` both run in quadratic time in `n`, whereas `supremum cantor (!! n)` runs in linear time but `supremum cantor (all . take n)` takes exponential time. I would love a way to be able to compute both queries in linear time, but I would be happy with just understanding why these definitions behave in the exact way that they do!
&gt; Oh, oops, sorry. I guess wrapping the composition of `sequence` with `mapWithKey` is the reasonable thing to do then. I don't believe that it is simple to define `mapWithKeyM` in terms of `sequence` and `mapWithKey`. Could you please provide the exact definition you were thinking of? You keep saying that monadic versions of functions can be defined by composing with `sequence`, but you never provide actual definitions, and I think you are mistaken. &gt; Well, if comparisons are not guaranteed to be transitive/consistent, then it's buggy. If they are, it is safe to unsafePerformIO them, because they really are a pure input. That's a good point. Sorting isn't a very good example of where monad-embed is useful. &gt; This could be done by verifying the priorities separately. But it's much nicer to verify them in place. Anything that Haskell can do "could be done" in C, but Haskell is still a better language than C. &gt; Allowing compilers to optimize, e.g: automatically insert speculative parallelism, alternate between two evaluation orders, etc. &gt; &gt; When everything is monadic, you have to specify an evaluation order (though I guess you could say it becomes unspecified again in the identity monad?). I suspect, although I am not certain, that the compiler can often determine that an expression has no side effects and optimize accordingly. For example, if the flavor of a function is a free type variable (so that the function can take on any flavor) then the function cannot have any side effects, so it can be compiled non-monadically with all of the optimization possibilities that allows. &gt; Also, what about guaranteed referential transparency? What specific benefits of referential transparency were you thinking of? Edit: It seems to me that monad-embed makes it much nicer to write and maintain monadic code, but it is useless or harmful when you wouldn't be using a monad anyway. Maybe a hybrid language would be best -- a language that allows monad-embed-style monad usage when you want it, but that allows it to be disabled when you don't. Also, I'm not sure how well monad-embed "reduces to" ordinary Haskell in the case where the monad is `Identity` or side effects are not in use; if it could be made to "reduce to" ordinary Haskell perfectly in that case, that would probably be the ideal.
I find it rather amusing that the PostgreSQL guys fixed the problem immediately, while the PHP guys still won't fix it after five years.
&gt; I don't believe that it is simple to define mapWithKeyM in terms of sequence and mapWithKey. Could you please provide the exact definition you were thinking of? You keep saying that monadic versions of functions can be defined by composing with sequence, but you never provide actual definitions, and I think you are mistaken. mapWithKeyM :: (Monad m) =&gt; (k -&gt; a1 -&gt; m a) -&gt; Map k a1 -&gt; m (Map k a) mapWithKeyM f = Data.Traversable.sequence . mapWithKey f &gt; But it's much nicer to verify them in place. Anything that Haskell can do "could be done" in C, but Haskell is still a better language than C. I am not sure -- because it makes your "sort" a partial function, at least in this case. Haskell's pure sort is a total function. &gt; I suspect, although I am not certain, that the compiler can often determine that an expression has no side effects and optimize accordingly. For example, if the flavor of a function is a free type variable (so that the function can take on any flavor) then the function cannot have any side effects, so it can be compiled non-monadically with all of the optimization possibilities that allows. Well, this is the approach used by most mainstream languages, though they fail miserably at identifying purity. I guess monad-embed could definitely fare better in that regard. I think it might encourage a non-pure/overly-monadic style, whereas currently Haskell encourages more modular/composable code by strongly encouraging pure code. &gt; What specific benefits of referential transparency were you thinking of? * Equational reasoning (at least when partiality is absent) for almost free. This is a really powerful way to reason about programs and it would be at least a bit sad to lose it "by default". * The ability to replace any expression by its defined value anywhere -- for attempts to experiment with other operational semantics. * Pedagogy/Simplicity: Simpler language semantics 
Sorry. Same results for that.
I sometimes wish it were possible too.
&gt; mapWithKeyM :: (Monad m) =&gt; (k -&gt; a1 -&gt; m a) -&gt; Map k a1 -&gt; m (Map k a) &gt; mapWithKeyM f = Data.Traversable.sequence . mapWithKey f You're right -- I wasn't thinking very clearly when I wrote that. However, you've also claimed that `foldrM`, `foldlM`, and `findM` could be defined in terms of `sequence`. I think that the sequence trick only extends to some `map`-like functions, not to higher-order functions in general. For example, I don't think you could use it to define `Data.Map.alterM`. Am I wrong? &gt; I think it might encourage a non-pure/overly-monadic style, whereas currently Haskell encourages more modular/composable code by strongly encouraging pure code. It would definitely encourage a more monadic style. I don't think that's less composable, though. In monad-embed, it is common for a function's "flavor" to be a monad transformer composed with a monad, where the monad is polymorphic. This makes it possible to, in a sense, "mix" code written in one monad (transformer) with code written in another monad. The function doesn't have to "know" what monad it will be embedded in, even though it is already embedded in one or more monad transformers. There aren't very many examples of stacking multiple monad transformers on the website, because it's mostly a technique that's useful in large programs. However, if you go [here](http://timmaxwell.org/pages/monad-embed/tutorial.html) and scroll down to "Parameterizing on a monad" there is an example. &gt; * Equational reasoning (at least when partiality is absent) for almost free. This is a really powerful way to reason about programs and it would be at least a bit sad to lose it "by default". &gt; * The ability to replace any expression by its defined value anywhere -- for attempts to experiment with other operational semantics. &gt; * Pedagogy/Simplicity: Simpler language semantics That's true, and I mostly agree with you there. I suspect I've been over-selling monad-embed; it was meant as an experiment, and I think it succeeds as that. But I've been sounding like I'm trying to replace Haskell completely, which I'm not.
Cool, I wonder how that works. I need to take the red pill and dive into higher-ranked type systems :-)
&gt; you've also claimed that foldrM, foldlM, and findM could be defined in terms of sequence. Gonna assume that you mean: foldrM :: (Monad m) =&gt; (b -&gt; a -&gt; m a) -&gt; a -&gt; [b] -&gt; m a I define it in terms of foldr and (&gt;&gt;=)/return, not in terms of sequence: foldrM :: (Monad m) =&gt; (b -&gt; a -&gt; m a) -&gt; a -&gt; [b] -&gt; m a foldrM cons nil elems = foldr cons' (return nil) elems where cons' x acc = acc &gt;&gt;= cons x Or: foldrM cons nil elems = foldr ((=&lt;&lt;) . cons) (return nil) elems Similarly for foldl. So I don't have to duplicate the code of foldr/foldl, just have the accumulate monadically accumulate rather than normally accumulate. &gt; I don't think you could use it to define Data.Map.alterM. Am I wrong? I think it indeed cannot be done without duplicating some of its logic: alterM :: (Functor f, Ord k) =&gt; (Maybe a -&gt; f (Maybe a)) -&gt; k -&gt; M.Map k a -&gt; f (M.Map k a) alterM f k m = setKey &lt;$&gt; f (M.lookup k m) where setKey v = M.alter (const v) k m So it can apply to any Functor, no need for Monad. Will alter in monad-embed be general to all Functors? &gt; It would definitely encourage a more monadic style. I don't think that's less composable, though. In monad-embed, it is common for a function's "flavor" to be a monad transformer composed with a monad, where the monad is polymorphic. This makes it possible to, in a sense, "mix" code written in one monad (transformer) with code written in another monad. The function doesn't have to "know" what monad it will be embedded in, even though it is already embedded in one or more monad transformers. I think strict code with implicit effects is inherently less composable than pure/lazy code. Consider the examples in "Why FP". &gt; There aren't very many examples of stacking multiple monad transformers on the website, because it's mostly a technique that's useful in large programs. However, if you go here and scroll down to "Parameterizing on a monad" there is an example. I find monad transformers to mainly be useful "in the small", especially in Haskell where various functions (e.g: bracket) become unusable with your actions. &gt; That's true, and I mostly agree with you there. I suspect I've been over-selling monad-embed; it was meant as an experiment, and I think it succeeds as that. But I've been sounding like I'm trying to replace Haskell completely, which I'm not. And I might be under-selling it :-) It sounds like an interesting direction to research (and as others mentioned, very similar to disciple) but I must admit it feels like a regression in some senses from Haskell's philosophy of encouraging purity. 
Too bad it's not available for download anymore on vimeo. Any particular reason?
yesod continues to rock! best feature - the docs! anyone ever benchmark the default yesod server?
SIGPLAN copyright. The fact that you can view it means you can keep a copy of it, even if it is via non-approved means.
I really enjoyed the Typeclassopedia (in issue 13 of the Monad Reader). [link (pdf)](http://www.haskell.org/sitewiki/images/8/85/TMR-Issue13.pdf)
Very interesting, and thank you my good sir. The article is spot on for my current stage of haskell knowledge.
Some highly readable classics/papers/posts from my memory: * **[Fun with type functions](http://research.microsoft.com/~simonpj/papers/assoc-types/fun-with-type-funs/typefun.pdf)** - shows how you can use type system to express complex ideas like matching protocols or taking locks in order. * **[A tutorial on the universality and expressiveness of fold](http://www.cs.nott.ac.uk/~gmh/fold.ps)** - You'll become a master in fold after that - many recursion forms will suddenly become transparent. * **[Haskell monoids and their uses](http://blog.sigfpe.com/2009/01/haskell-monoids-and-their-uses.html)** - guide to the Monoid type class * **[Applicative programming with effects](http://www.soi.city.ac.uk/~ross/papers/Applicative.pdf)** - a paper introducting Applicative * Semantic editor combinators: **[shorter](http://stackoverflow.com/questions/413930/)** and **[longer](http://conal.net/blog/posts/semantic-editor-combinators/)** exposition. Quick, given `x :: [(Bool -&gt; [Integer], Char)]` how to square all integers? * **[The Typeclassopedia](http://www.haskell.org/sitewiki/images/8/85/TMR-Issue13.pdf)**. Great exposition of standard type classes with a lot of references. * **[The zipper](http://www.st.cs.uni-saarland.de/edu/seminare/2005/advanced-fp/docs/huet-zipper.pdf)** - Very short, simple idea with many posible variations. * **[Type safe patterns](http://www.itu.dk/people/mir/typesafepatterns.pdf)**. How to implement pattern matching ad hoc if a language doesn't support it. * **[Monads for functional programming](http://homepages.inf.ed.ac.uk/wadler/papers/marktoberdorf/baastad.pdf)** &amp; **[Tackling the Awkward Squad](http://research.microsoft.com/en-us/um/people/simonpj/papers/marktoberdorf/mark.pdf)** - One of first descriptions of monads in Haskell; how "impure" things like IO and concurrency are achieved using monads * **[Seemingly impossible functional programs](http://math.andrej.com/2007/09/28/seemingly-impossible-functional-programs/)**: Can you write conjunction `and :: (X -&gt; Bool) -&gt; Bool` for some fixed type `X`? If `X` is finite, then it is `and f = f x1 &amp;&amp; f x2 &amp;&amp; .. &amp;&amp; f xn`. But when `X` is infinite, it seems impossible - you have to check infinitely many cases. For naturals that is equivalent to halting problem. It turns out that, using laziness, you can do that for X = Integer -&gt; Bool!
You've demonstrated that it's usually possible to make a non-monadic function monadic without duplicating all of its logic, although often some of the logic must be duplicated. I think this solution is imperfect enough to make monad-embed worth considering, even if it isn't perfect either. &gt; Will alter in monad-embed be general to all Functors? No. &gt; I think strict code with implicit effects is inherently less composable than pure/lazy code. Consider the examples in "Why FP". I'm too busy. We'll have to leave this point undecided. &gt; I find monad transformers to mainly be useful "in the small", especially in Haskell where various functions (e.g: bracket) become unusable with your actions. I haven't compiled this, but I think it would work (assuming a reasonable definition of `try` and `raise`): bracket :: (a :: *, b :: *, c :: *) =&gt; {IO} [a] -&gt; (a -&gt; c) -&gt; (a -&gt; b) -&gt; b; bracket = \ [begin] end fun -&gt; do { foo = begin; resultOrException = try [fun foo]; end foo; } then case resultOrException of { Result x -&gt; x; Exception y -&gt; raise y; }; It could be used like so: contents = bracket [openFile "foo.txt"] closeFile readFile; &gt; It sounds like an interesting direction to research (and as others mentioned, very similar to disciple) but I must admit it feels like a regression in some senses from Haskell's philosophy of encouraging purity. It sounds like we're in agreement.
Glad the docs are useful. I've been distracted from working on the book for the past week, but I intend to get back to it. My goal is by the end of next month to have completely polished the first five or so chapters. Let me know where something could be clarified!
[Planet Haskell](http://planet.haskell.org/) is a good place for blog posts, although it's fairly busy! I like [Edward Yang's blog](http://blog.ezyang.com/), in particular.
if you need a little bit more of haskell love: http://okmij.org/ftp/Haskell/ if you prefer to hate haskell: first chapter of http://www.cse.unsw.edu.au/~benl/papers/thesis/lippmeier-impure-world.pdf
You are hereby awarded one (1) instance of high praise for this wonderful comment. Hadn't read several of those, but they're all great. Thank you so much! 
&gt; &gt; Will alter in monad-embed be general to all Functors? &gt; No. Maybe there's room for improvement in something like monad-embed/disciple -- making things generalize even further. &gt; I'm too busy. We'll have to leave this point undecided. Alright, but http://cs.fit.edu/~ryan/library/functional_programming/whyfp.pdf is definitely worth reading. &gt; I haven't compiled this, but I think it would work (assuming a reasonable definition of try and raise): Will your bracket work in any monad transformer stack or just in IO, as its type says?
Almost a cliché by now, but working through [Project Euler](http://projecteuler.net/) never hurts.
Lots of fun things to play with here, thanks!
Thanks a lot!! 2 down, 7 to go :). 
Ew, non-standard licensing terms. Not just that it's not open source, but the more licenses I have to deal with the less happy I am. Ok, fine, you don't want to share your invention openly. At least choose a fairly standard closed license for libraries. [Here's a few](http://en.wikipedia.org/wiki/Shared_source#Non-Open_Source.2Fnon-Free_licenses)
&gt; Maybe there's room for improvement in something like monad-embed/disciple -- making things generalize even further. I think it would be pretty straightforward to make it work with any applicative functor unless monad-specific constructs (non-lazy `do`-bindings, in particular) were used. I wonder if there's such a thing as an applicative functor transformer. &gt; Will your bracket work in any monad transformer stack or just in IO, as its type says? I was duplicating Haskell's `IO.bracket`. What would the semantics of a non-`IO` `bracket` be? It would probably be possible to define it. I'm not aware of a generalized version of `try` to other monads, but if such a thing existed, then it would work in any monad that defined `try` and `raise`.
You aren't really giving us very much to go on, to help you. 
I'd suggest taking the time to formulate a specific, well-worded question. I often find formulating these questions to be an enlightening experience in and of itself.
I had trouble too, until I did this: Take a three or four line use of the state monad in "do" notation. Manually desugar it. Manually start substituting in the "bind" function. Manually start reducing the resulting code. Basically run through the entire thing with manual execution. Do not cheat. You should probably use paper and pencil, though if you've got the discipline you can do this in a text editor. You can also try coming at it from the other angle by trying to write it yourself and satisfying yourself that you really do need that function type in the type signature. This was a big "Aha!" moment for me, as I started to really grok abstracting over functions in a new way.
It's not. You're probably looking for something that isn't there.
I find the name "state" misleading, because state monad isn't really a state, It's a state consumer. so, newtype State s a = State { runState :: (s -&gt; (a,s)) } (State s) is a function that takes a state, itself isn't the state at all. If this is in C, it would somewhat like this: /* won't compile */ struct result {int a; int s;}; struct State { (struct result) (*runState)(int s);}; struct result bind(int input, struct State s) { return s-&gt;runState(input); } struct result myRunState(int s) { struct result = {0, s+1}; return result; } void test() { struct s1 = s2 = s3 = {&amp;myRunState}; int input = 1; struct result r1 = s1-&gt;runState(input); struct result r2 = bind(r1-&gt;s, s2); struct result r3 = bind(r2-&gt;s, s3); printf("%d\n", r3-&gt;s); } (this is just for demonstrating the idea) edited: okay, I wasn't really thinking 
Hey, the code *I* wrote is BSD licensed. I'm guessing you're talking about Yices itself, but I wanted to clarify.
It's something that irritates me with a lot of Hackage packages. They're BSD licensed, but they're simply wrappers over non-BSD licensed libraries. That's dangerous.
It also proves you aren't just an out-of-practice troll.
All right. I think this is a valid discussion, but could you please start a self-post or something? It irritates me when discussions of technology get drowned out by license politics. And I've seen it happen quite a few times on Reddit.
Just a quick note as to the power. The language {a^n b^n c^n | n &gt; 1} is of course not context free. However, just because you can match it doesn't mean that you have the power of matching all context sensitive languages. In fact, there is an infinite hierarchy, the Control Language Hierarchy, that contains this language but is less powerful than the context sensitive languages. http://en.wikipedia.org/wiki/Mildly_context-sensitive_language#Control_Language_Hierarchy
Suppose you have data Tree a = Leaf a | Node (Tree a) (Tree a) You want to write a function Tree () -&gt; Tree Integer that traversed tree | / \ | | Node (Node (Leaf ()) (Leaf ()) (Leaf ()) / \ () () () and gave: | / \ | | Node (Node (Leaf 1) (Leaf 2)) (Leaf 3). / \ 3 1 2 In C-like language, you might write that as: int n = 0; Tree dfs(Tree tree) { if (tree is Leaf) { n += 1; return Leaf(n); } else { x = dfs(tree.left); y = dfs(tree.right); return Node(x,y); } } To translate that to Haskell, you have to make the variable local - the function will get value of n, and return the new value each call. dfs :: Tree () -&gt; Integer -&gt; (Tree Integer, Integer) dfs tree n = case tree of Leaf () -&gt; (Leaf (n+1), n+1) Node l r -&gt; let (x, n') = dfs l n (y, n'') = dfs r n' in (Node x y, n'') Do you see how it works? You sequence state-changing calls to `dfs l` and `dfs r` - pass n to the first function, use pattern matching to get new value n', pass it to second function, get new value of n'' and return it. This "relay race" pattern is covered by the state monad. dfs2 :: Tree () -&gt; State Integer (Tree Integer) dfs2 tree = case tree of Leaf () -&gt; State $ \n -&gt; (Leaf (n+1), n+1) Node l r -&gt; do x &lt;- dfs2 l y &lt;- dfs2 r return (Node x y) To understand this, take paper, write definition of &gt;&gt;= and translate, step by step, definition of dfs2 to dfs. Sorry but learning requires effort. You'll see nested let... in... corresponding to "relay race" rather fast. There are auxiliary functions `put`, `get`. `put x` is Haskell equivalent of void put(x) { state := x; /* return nothing */ } so it is put x = State $ \s -&gt; ((), x) `get` is Haskell equivalent of statetype get() { return state; } so it is get = State $ \s -&gt; (s,s) [first `s` is the result of the function, second means that the state is unchanged.] `return x` is Haskell equivalent of xtype ret(x) { return x; } so it is return x = State $ \s -&gt; (x,s) Using that you can write: Leaf () -&gt; State $ \n -&gt; (Leaf (n+1), n+1) as Leaf () -&gt; do n &lt;- get put (n+1) return (Leaf (n+1)) Again, please take paper and check how `n` is assigned state, which is later changed and returned.
&gt; I wonder if there's such a thing as an applicative functor transformer. There's no need for that because Applicatives naturally compose. You cannot compose two monads, because the monadic operation "join" cannot be expressed from the two monads' join. That is, you can't get from: m (m a) -&gt; m a and: n (n a) -&gt; n a to: m (n (m (n a))) -&gt; m (n a) You need more operations to do this, hence the need for transformers. With Applicatives, you can get from: f (a -&gt; b) -&gt; f a -&gt; f b and: g (a -&gt; b) -&gt; g a -&gt; g b to: f (g (a -&gt; b)) -&gt; f (g a) -&gt; f (g b) And of course you can get from: `a -&gt; f a` and `a -&gt; g a` to: `a -&gt; f (g a)`. You can just use `liftA2 . liftA2` to apply a binary function within two Applicative wrappers. 
As many others, he too has encountered the big String performance problem. Will we ever be able to make Haskell libraries more polymorphic, so that whether one uses a String, ByteString or Lazy ByteString is not a big deal? Sure, there may be performance repercussions, but it would still be nice to easily switch between these data structures. We do have cheap strictness annotations that don't affect the types for similar reasons: Changing operational semantics cheaply, without affecting semantics (except issues of definedness). Also, maybe we can replace ByteString with packed unboxed polymorphic arrays, and using type-families allow for the specialized Word8 code when it is used? That way, maybe the Array could be a Functor and an instance of other classes, so that the disparity between ByteString and list API's will be smaller (Rather than relying on the ListLike class). 
you could implement ta-lib.org functions in Haskell..
You are right that showing that this specific language can be parsed does not suffice to show that every context sensitive language can be parsed (but nobody claimed that it suffices). The following function shows that there is an infinite regular expression for every computable language with a finite alphabet (here for the alphabet Bool): decide :: ([Bool] -&gt; Bool) -&gt; RegExp Bool decide f | f [] = eps `alt` r | otherwise = r where r = alt (sym False `seq_` decide (f . (False:))) (sym True `seq_` decide (f . (True:))) Nils Anders Danielsson uses a similar construction in [his paper on total parser combinators](http://www.cs.nott.ac.uk/~nad/publications/danielsson-parser-combinators.pdf) 
I don't think you want to treat Strings and ByteStrings the same. They are semantically different: the first is a sequence of Unicode code points and the second is a sequence of bytes. Here's a rule of thumb: for binary data use ByteString and for Unicode data use Text.
The following notes include a fairly detailed introduction to the state monad: http://www.cs.nott.ac.uk/~gmh/monads 
i am nowhere near your league of haskell awesomeness, but i must stridently disagree. the proliferation of string types is ridiculous, particularly with more and more library coders using ByteStrings even for the smallest, most insignificant string cases. do a cursory overview of the web libs for haskell and you'll find ByteStrings everywhere, even for function params that will probably be five chars long or less. you can't dispute that many library writers simply assume ByteStrings are better Strings. doing "pack" for practically every string i use in conjunction with these libs is a real drag, makes my code look like hell, and just makes it look like the haskell folks have totally ignored making the language usable in favor of letting everyone roll out their own 1% improvement in storing text i can't think of any other modern programming language (even those that are strongly typed) that forces users to make choices like [Char], String, ByteString, Text, etc...how is it that everyone else has avoided this problem? Is it just an obsession with type wankery? i'm all for spending 30 minutes in the hackage docs to figure out something cool and unique, but most of the time i spend 30 minutes figuring out some boring but necessary way to push strings from one type to another i'd love for a benevolent dictator to just pick one implementation, make that the implementation for String itself, obliterate the other implementations and let us move on. managing strings is one of the first thing any haskell adopter is going to deal with, and when they discover the mess, there is a good chance they'll utter "WTF?" and go back to D or Java or Scala or Go or whatever other C++ replacement they were considering. once again, all props to tibbe, i watch your stuff on github...i'm not trying to imply my depth of haskell knowledge is equal to yours
&gt; managing strings is one of the first thing any haskell adopter is going to deal with, and when they discover the mess, there is a good chance they'll utter "WTF?" and go back to D or Java or Scala or Go or whatever other C++ replacement they were considering. When they discover that D has two incompatible standard libraries they'll utter "WTF?". When they discover that Java has recently pushed back support for lambda expressions until *late 2012* they'll utter "WTF?". When they discover that Go's solution for generic containers is one that even Java had rejected by 2004 they'll utter "WTF?" I could go on.... The point is, all languages have flaws. I agree with you and the Google paper authors that string handling is currently a weak point in Haskell. But your rant massively overstates the case. I write a ton of Haskell code and I don't spend that much time worrying about string types. Python 2.x also has two string types (`str` and `unicode`) with an awkwardly delineated line between them. It's a big enough problem that they fixed it in Python 3.x, yet it didn't stop Python 2 from taking over the world.
The state monad requires a lot of explanation. [Here's](http://mvanier.livejournal.com/5406.html) my attempt. This is the 7th in a series that started [here](http://mvanier.livejournal.com/3917.html), so if you have the time I'd recommend starting at the first installment.
&gt; Two more weeks of Haskell programming with you guys, and I will be able to &gt; write beautiful Haskell programs. (Disbelief on HAZEL’s and CODY’s faces.) That part made me wonder how many times I've said I understand a complex Haskell syntax and I was wrong (State, Cont, Zippers, Monad Transformers, you name it) 
about the problem from 6.6-&gt;6.8, wouldn't it be possible to implement base-$(prev_version) in terms for base-$(curr_version) and release it on hackage? having base-3.0.0.1 implemented on top of current ghc would make all the old programs work, right? and undefined/error is still a big mistake - it should be forbidden.
because practicing troll has a PHD, has written a bunch of F# books and prefers to troll with blog posts full of wrong results?
I think there's a law against writing monad tutorials in comments. I'm sorry, but you have to start a blog.
Flaws are there to be fixed. Python fixed their str/unicode awkwardness with Python3. Haskell should also fix their problem.
You mean Un-i-code?
You are correct. Strict/Lazy newtype wrapper around ByteString (that internally uses UTF8 or other encoding) could be used then (though that does rule out or make more difficult to have a Functor and other class instances).
&gt; and undefined/error is still a big mistake - it should be forbidden. I don't get this comment. If you think there is a problem, surely `undefined`/`error` is not the cause but a symptom, given that I can define `undefined = undefined ; error _ = undefined`. Are you trying to say that Haskell should be Agda, or did you have something else in mind?
I would think it should be possible to define a new type class for common text operations that has implementations and constructors for various different string back-ends. (A wrapper generalizing over several string types, basically.) A user could switch between back-ends by changing the constructor, and do some simple performance evaluations quickly to choose the best one for the job.
That would be ... ı-code?
Great changes. I'm personally really excited about improved exhaustiveness checking for GADTs, and I'm hoping GADTs will be making their way into more of my own code in the near future. Everyone who worked on this has my sincere thanks.
I'm glad to see this writeup. I was very confused when my programs first started failing to compile after that monster patch to GHC HEAD, and I was even considering writing it up as a bug, but I figured I would wait in case it was either a known bug or had a planned workaround.
granted none of the other tools i mentioned are perfect, but they still managed to keep simple things simple...one string type and other than my griping, it doesn't appear that the haskell world really cares to fix this problem the real fix would have been for a benevolent dictator to come in when dons released bytestrings as a library and simply pulled it in then (ghc 6.10?) as the new String implementation. not a perfect solution, but since most people are just blindly using ByteStrings as better Strings, it would have addressed 95% of the problems while keeping the language usable there are parts of haskell which are somewhat challenging to deal with, but in many regards there is often a good reason and discovering the haskell way is rewarding. this is not the case with string handling 
"undefined = undefined" would be great! if non-termination was the only solution, no one would write the function head of type [a] -&gt; a.
&gt; I write a ton of Haskell code and I don't spend that much time worrying about string types. Because the support is that wonderful, or because you've internalized the quirkiness and instinctively avoid it because you're just used to it? I started trying to port my personal website over to Haskell using one of the web frameworks. I thought as a first step I'd convert my HTML normalization function, which I use all over my website. And I hit problem after problem, unless I'm willing to just give up on correctness or performance, to which I say "absolutely not" and "why bother converting then?" respectively. I can have my HTML in a Data.Text, but I can't run a regular expression on that with any library I could find. I can try to parse it myself and just encode my regexs in the parse rules, but wait, attoparsec only runs on _byte_strings, not Data.Text, so I'd have to write the UTF-8 handling there myself if I want to use that. But the UTF-8 support doesn't seem to expose anything like information about UTF-8 itself or a progressive decode function or anything else, it exposes a bare minimum API for using UTF-8 but has nothing for generating it or reading it when it isn't already in a pretty chunk. I can get Data.Text out of my string, but then we're back to nothing actually taking that type that I could see. Or I can give up on Unicode but even on my English-only website I've been known to use a bit of unicode now and then. I ended up back down at using Parsec with a typeclass specification for Data.Text I found in some email somewhere to feed it into Parsec one char at a time, but by comparison to the Perl/Python (I've got both) code I'm porting this is absurdly verbose and hardly likely to be any faster. (Also, I just noticed, shouldn't Parsec have a regex combinator?) And this is still just a summary of a full night's labor whereby I failed to port a relatively simple function from Perl/Python into Haskell. Technically, I did get to the point where it could work (haven't quite finished it but it would just be polishing now), but I had to compromise to hell to make it happen. Yeah, Haskell can move strings from here to there but if you want to do anything interesting with them I hit enormous problems. Now, let me make my point clear: My complaint is not that Haskell has a weak string story. It is still in many ways a very young language. I can deal with that. I approve of the direction things are moving in and if anything I'm just excited about what is happening and trying to run too far ahead of where we actually are, a crime of passion. __My point is that I don't want people to think that there isn't a very big problem here__, because then it won't get fixed. There is a problem. (I would try to help fix it except that the fix here would require so many steps I don't even know where to start and I don't have enough time.) You can have something in Unicode _xor_ parse it efficiently, run a regular expression on it at all, or do anything much more sophisticated than chop it up and spit it out. (Oh, and yes, I see Yesod just released an HTML normalization library. However, converting mine was a bit of an exercise, and also, my library has a lot more knobs and stuff than that one and is also moderately field and battle tested. That counts for something in an HTML normalization function.)
there is only one solution: SPJ or another meaningful benevolent dictator creating a bug to fix all this, and annotating the bug such that ghc 7.2 will not be released until the bug is closed. 
I believe it would be better if Prof. Hutton drew the picture for ST as follows: .-------. | | .--&gt; | f | .--&gt; | | | | | '-------' | | | | | v | .-------. x | .-------. | s | |---' | |--' -----&gt; | st | s' | f x | | |------&gt; | |------&gt; '-------' '-------'
Anyone know what they used each language for?
It's not a GHC bug, it's a library meta-bug.
my point was that this issue will only be fixed by someone creating a fair degree of contrived drama, and holding up a ghc release is pretty dramatic 
It's just another Monad. And like all monads, it's magical. All jokes aside, you should clarify your question. Are there monad's you understand? Like the Maybe or the List monad? What about the Writer monad and Reader monad? Make sure you understand those first. Also maybe your problem is with the type system. 
" doing "pack" for practically every string i use in conjunction with these libs is a real drag" This at least is easy to solve with a LANGUAGE pragma: {-# LANGUAGE OverloadedStrings #-} http://www.haskell.org/ghc/docs/6.12.2/html/users_guide/type-class-extensions.html#overloaded-strings
The main point of `String` is that it's equal to `[Char]` so that the existing plethora of functions for lists is available. Any abstract data type (like `Text` or `ByteString`) that aims to replace `String` will have to reimplement them. Due to semantic differences (lists are very lazy; runtime of `cons` is O(n) vs `(:)` is O(1)), it's not possible to reuse the implementations for lists. In other words, the problem is this: strings are not lists, but they are so close to them that a painful duplication of interfaces and code is unavoidable. There is no easy solution. The only really clean solution would be heavy compiler magic that replaces `[Char]` by byte strings on the fly, just like deforestation replaces `[Char]` by `(Char -&gt; b -&gt; b) -&gt; b -&gt; b` when applicable. Stream fusion achieves some of this, but GHC is not ready for this kind of magic, yet.
At least here, `undefined = undefined` does terminate. Try it yourself -- define `main = main`, compile, and run! =)
I simply use `String` for [my personal website](http://apfelmus.nfshost.com) which is built with [pandoc](http://johnmacfarlane.net/pandoc/) and a homebrew `make`-like DSL. It's fast enough for me. As I remark in [another comment here](http://www.reddit.com/r/haskell/comments/dkpdj/haskell_at_google_for_the_hard_problems_in_the/c10zp20), the problem may be big, but it's even harder to find a good solution. That said, `String` works for me.
C++ for a simulated annealing-based instance solver, Python for a coordination webapp, Haskell for generating hard instances (the contest involved both solving and creating puzzles). They also said they used a few other languages along the way for smaller stuff.
when compiled - yes. runghc and ghci don't do anything (they loop without eating cpu/memory). knowing that, would you write the head function in a regular way?
It would work for me too. There isn't a (serious) language out there that couldn't handle my website, actually. But half the point of my project is to play with the fast libraries like BlazeHTML and to work with loop fusion and iteratees and such, but all the parts are so immiscible I can't get them together. I'm not sure the problem is all that hard to find a solution. All those libraries I mentioned that don't take Data.Text? Make them take it. [Char] should come to be seen as an anachronism when dealing with text.
&gt; In other words, the problem is this: strings are not lists, but they are so close to them that a painful duplication of interfaces and code is unavoidable. There is no easy solution. At least the semantically equivalent functions could be polymorphic to both types. Different operational semantics should not mean that we have to explicitly choose the function in every place it is used. We should be able to choose by selecting a type in a single place. &gt; The only really clean solution would be heavy compiler magic that replaces [Char] by byte strings on the fly, just like deforestation replaces [Char] by (Char -&gt; b -&gt; b) -&gt; b -&gt; b when applicable. Stream fusion achieves some of this, but GHC is not ready for this kind of magic, yet. Why not typeclass polymorphism?
*All those libraries I mentioned that don't take Data.Text? Make them take it* yes. anyone working with web libraries is going to have to deal with utf8 
Well, take for example the `map` function: map f [] = [] map f (x:xs) = f x : map f xs This implementation is great for lists, but it's awful for `Data.Text` where `cons` is O(n) and the whole thing would take quadratic time. So, `Data.Text` has to reimplement this function. Now, there are many such functions to be reimplemented. If you were to put them in a type class, the signature of this type class would be huge and it's no longer clear whether this is superior to just duplicating the functions in a module (like `Data.Text` currently does). A type class does buy you some polymorphism, but the cost is that all type signatures become clunkier (`IsString s =&gt; s -&gt; Int` instead of `String -&gt; Int`). I'm not sure whether this compares favorably to the clunkiness of having to convert between different string types (`pack`, `unpack`). The type class doesn't quite do what you want either: what you actually want to do is to use `String` in the place of `Text` *and vice versa*; you don't really want to distinguish different string types anyway, a string is a string, after all. The ideal solution would be that the compiler switches representations internally. 
As said, `[Char]` is extremely versatile because you have all list functions available and it's very easy to define new ones. Not so with `Data.Text` which has O(n) `cons` and no pattern matching. The tradeoff is ease of use / expressiveness vs performance. If forced to choose, personally, I would choose the former, not the latter. :-) 
&gt; Now, there are many such functions to be reimplemented. If you were to put them in a type class, the signature of this type class would be huge and it's no longer clear whether this is superior to just duplicating the functions in a module (like Data.Text currently does). If you manage to make Text parameterized on the character type (as in, Text = PackedUnboxedEncoded Char (Where a type family is special-cased to use UTF8 to encode Char array here, but it is otherwise a packed unboxed array). Then you can re-use "fmap" for "map". Monoid already exists for concating. Various other functions could be in various small classes, there's no need for a huge class. I think it would always clearly be superior to duplicating the functions in a module -- because this situation hard-codes every piece of code to be specific to a type. Even a huge type-class would at least allow re-using code with other types. &gt; A type class does buy you some polymorphism, but the cost is that all type signatures become clunkier (IsString s =&gt; s -&gt; Int instead of String -&gt; Int). I think the type-classes, if done right, would be more general than that. If your signature is "length", then we should probably get length from something like "Foldable" (perhaps a refactoring of Foldable or rewrite rules to make "length" efficient). So it might be: Foldable f =&gt; f -&gt; Int &gt; I'm not sure whether this compares favorably to the clunkiness of having to convert between different string types (pack, unpack). The problem is not just clunkiness, it is a lack of power. If a module is written to work with a specific kind of String (e.g: String), its code simply **cannot be used** reasonably. &gt; The type class doesn't quite do what you want either: what you actually want to do is to use String in the place of Text and vice versa; you don't really want to distinguish different string types anyway, a string is a string, after all. The ideal solution would be that the compiler switches representations internally. Well, that may be true if we can make pretty much *all* list-based functions be polymorphic enough to work on such string types.
Other languages, many many _many_ of which have O(n) string appending, seem to manage to solve this problem in practice, and they don't even have loop fusion or all that other fancy stuff that people are constantly going on about on /r/haskell. What's the point of having all this fancy stuff if I can't even parse UTF-8 HTML with it and perform some minimal processing on it? If it's so hard to define new functions on Data.Text, then how about _fixing that_? It's OK for such functions to not be exactly the same as operating on lists of characters; text strings are not lists of characters. The desired O() characteristics of things with strings are not the O() characteristics of strings. This is every bit as silly as insisting that all maps must be represented by lists of tuples and then yelling at people when they complain about how hard it is to work with them and how bad the O() characteristics are. Text strings are not [linked] lists of characters. Text strings are not lists of characters.
Obviously. Did you think I'd disagree?
&gt; As said, `[Char]` is extremely versatile because you have all list functions available and it's very easy to define new ones The other reason that `[Char]` is the default is that it's maximally lazy. For a language that was first and foremost a research platform for lazy functional programming, this is a big deal. The `String`-related pains we're running into now are a consequence of Haskell outgrowing this research niche. Someone else in this thread said that "Haskell is a young language". That's not really true; it's been around since 1990 at least. What's new is people using it for "real" stuff: caring about concurrent I/O performance, Unicode correctness, accessibility to non-PhD's, etc. 
As `jyper` said, it's showing how to load a buffer full of machine code, then execute it as a function. Because Haskell is seen as this incredibly high-level object of pure mathematics, I take perverse joy in porting over some of the terrible low-level ideas from C. It's firmly tongue-in-cheek; there are better (safer, more expressive, more portable) ways to do runtime code generation in Haskell.
Sorry for the terse post. It was the result of frustration induced by staring at state monad tutorials for entire week. But I think I am starting to "get" it a little. The problem was the name "State" when it actually should be "State transformer". It took a long time to understand that the "do" blocks return a state transformer and the actual state is passed to it later. And that initial state will be modified and passed around by the small transformers chained in the big "do" transformer. Still need to spend a few more days on this to internalize the info. 
Being pedantic here... but just so you know there is no difference whatsoever between String and [Char].
Couldn't agree more. Kudos to the team for the new checker and this very helpful write-up. I can't wait to throw away the type signatures that were necessary when using GADTs.
The proliferation of string types is indeed unfortunate. In the beginning there was [Char] and [Word8] for sequences of Unicode code points and bytes, respectively. These turned out to be too slow (due to excessive memory usage and indirections) so ByteString, and later Text, were invented. We're now in a transition phase from the old types to the new types and hence the confusion. To add to the confusion many programmers don't understand Unicode very well and they, like you say, think ByteString is a faster String when it in reality is a faster [Word8]. The reason I gave my rule of thumb is I don't want new user to have to make a choice. Follow it and you'll be mostly OK. The only pain point is that you have to sometimes use Strings because a bunch of libraries still work on Strings. Consider those librares to be in a transition phase, eventually to be rewritten. As for redefining String, it think it's too late. It would simply break too much code. 
Unfortunately [Char] is deceptively simple. The reason is that Unicode code points (i.e. Chars) do not correspond to what we humans think of as characters. For example, this function is wrong stringToUpper = map toUpper as uppercasing a string might produce a string of different length! Same applies to most other element wise operations (e.g. filter, uncons, cons, etc). The list API simply doesn't make sense when used on Strings, in many case. One could write an API over Strings that does the right thing, but why bother when Text already has it and often offers better performance (and less memory usage).
Please see my comment to your earlier comment. The list API rarely ever does the right thing when applied to String.
Take your pick! http://www.haskell.org/haskell-symposium/2010/accepted.html
He's the grand master troll. 
Ah, code points like accents that combine with the following code point to produce a single character. Didn't know that. But looking at the `Data.Text` documentation, it appears to me that this only affects the functions dealing with case conversation, while `filter` treats it as a list of code points? Unfortunately, it's not entirely true that `Text` offers better performance. This is only true for reading strings, not so much for writing them. After all, `cons` is O(n). That said, writing ordinary `String` is not that great either, a difference list is usually the better choice.
Ah, I'm not saying that it's hard to write functions for `Data.Text`, I'm saying that functions for `Text` are often duplicates of functions for lists. This a matter of code duplication. It's true that strings are not lists of characters, but they are so darn similar that an API duplication feels very unsatisfactory. My particular point about the O(n) `cons` is only that it aggravates this code duplication problem: not only do you have to implement `map` twice, you also have to implement two different algorithms; you can't replace`(:)` by `cons` because of the different running times. (Comparing to non-pure languages does not necessarily help because most of them don't use written strings as first class values. Their main mechanism for writing strings is a `print` function, whereas Haskell encourages string concatenation.) Anyway, I'm not happy about the current state of affairs either, I'm just trying to point out that there's an inherent difficulty.
Not sure. `Data.Text` does replicate a substantial part of the list API verbatim.
I'm not the best with academic-y papers, but I *think* that is saying that with Nikola I can compile Haskell code into CUDA (albeit only a subset of Haskell code). Can someone with a better grasp on this explain it in a slightly easier way for me?
I think you mean the Haskell tag on Stack Overflow ;-) I quite like Stack Overflow, but I'm indifferent as to whether we have a separate site or use the Haskell tag. What's the difference between the two options?
&gt; Why settle for a tag, when we can have a whole site of our own? Your question implies that a separate site would have some great advantage, and that using a tag on the larger stackoverflow.com would somehow be inferior. Please elaborate.
By being on Stack Overflow, we can ask questions like "Are Scala's traits similar to Haskell's type classes", tag them with both `scala` and `haskell` and get answers from both communities. Or ask more broader questions and tag them with `functional-programming` and `haskell`. Having access to a bigger base of programmers is great! I see no advantages to splitting off the tag to a separate site.
Eagerly awaiting Part 2. 
I'm not really seeing the benefits of this. On the other hand, I can see some downsides: - Having more sites to watch when answering questions: Looking at the all-time totals for answer upvotes in the Haskell tag, three of the top five users are actually more active in other language tags. - Makes multi-language questions awkward: I've seen more than a few questions that weren't strictly *about* Haskell, but were clearly relevant, e.g., general questions about ML-ish type systems. These wouldn't belong on a pure Haskell site, but would still be of interest to Haskell users. - Organizing a new site is a lot of work vs. just using a tag on SO, so there's pretty much a downside by default. I note this because I'd feel kind of obligated to help out with such a site and I don't think it's worth it, honestly. Not to mention that *keeping* an SE site isn't a given. Activity on the Haskell tag seems to be about 30 questions per week, which would be awfully low for a full site (compare to other specialized SE sites--looks like 50-100 Q/wk is more typical). It'd be really discouraging if a site launched only to fizzle out. And finally, there's [this blog post](http://blog.stackoverflow.com/2010/10/) which basically says they'll close as duplicates any SE proposals that would mostly pull questions away from other SE sites rather than expand coverage and pull in new users. How would you pitch a Haskell site in a way that avoids that? What would be on-topic on such a site that wouldn't already belong elsewhere, e.g. on SO? **EDIT**: And the proposal has indeed been closed on precisely that basis.
I think Haskell should have its own Stock Exchange.
I think Haskell should have its own Stock Cube.
Cabal and Hackage are great. cabal-install ... not so great. I'm glad they are looking into a Nix-style persistent package management, but why go for a Nix-style management when it would be a lot simpler to just use Nix itself? Nix is already written, well thought out, and works today. Haskell Platform ... still a terrible idea that has a good chance of destroying Haskell through stagnation.
&gt; Haskell Platform ... still a terrible idea that has a good chance of destroying Haskell through stagnation. Could you say more, please. That's a very harsh criticism. With e.g. text in the next release, we're already moving faster than the old fptools, while maintaining a buffer for stability in front of GHC's rapid evolution. You can always just live on ghc nightly builds and cabal installs, of course. But the majority shouldn't do that.
I expect HP to effectively become an enlarged Prelude and will have all the problems that the Prelude has except on a larger scale. The Prelude has effectively fixed the Num heirarchy and the Monad hierarchies even though I think most people believe it would be better if both were more fine grained. The Haskell platform will extend these sort of problems, to things like the mtl, which is known to be buggy; it was even stated that it was buggy in the proposed update to the mtl; and the proposal even acknowledged that they are replacing the mtl with a newer mtl with the same bugs. [Network effects](http://en.wikipedia.org/wiki/Network_effect#Lock-in) due to library dependencies will eventually fix the interfaces of the Haskell Platform, just like they have already fixed the Prelude interfaces. AFAIK the "problem" that Haskell Platform is trying to solve should be solved by cabal install. It is a program that automatically fetches required dependencies. Yes, network effects are at work here too, slowly fixing interfaces. But there is no need to accelerate this process. Let people explore and chose their own dependencies. The network effects are more likely to fix on better interfaces if the process is slow and many choices are available. Sorry, I should tone down my rhetoric lest I become like Zeilberger.
&gt; AFAIK the "problem" that Haskell Platform is trying to solve should be solved by cabal install. It is a program that automatically fetches required dependencies. Yes, network effects are at work here too, slowly fixing interfaces. But there is no need to accelerate this process. As long as the HP is conservative in bringing packages into the fold, then it is just reflecting the unavoidable network effects you acknowledge must exist. The point of the HP, as I understand it, is to save new users the trouble of recognizing the implicit fixed interfaces already present on hackage. The real risk for stagnation is with cabal-install. If maintainers of large packages can punt on portable packaging by getting binaries into something like the HP, then there will be less incentive to improve the build environment in which potential competitors will have to exist, and that will stifle competition. The way to avoid that is to ensure that the HP is *not* used as a crutch for the difficulties in cross-platform portability, and is instead something that any user could be expected to build themselves by "cabal install haskell-platform".
The Haskell Platform is simply a list of cabal-installable packages. By construction its members are cabal-installable.
&gt; and undefined/error is still a big mistake - it should be forbidden. can you elaborate? I find `error` very convenient, and `undefined` basically unavoidable, *and* convenient (yeah, technically you could use a loop instead of `undefined`, but how is that any better?) 
Right, which is it's saving grace. On top of roconnor's concern about accelerating interface fixing (which you are mitigating by not moving too quickly), there is the danger of the HP providing any other kind of advantage to the included packages. My concern about a packaged installer is the temptation to short circuit something like the GUI library angst by loading the HP with a bunch of per-platform binaries. I am not actually against such a packaging effort, but I would prefer to see it kept distinct for the reasons mentioned.
using error/undefined is trading your (programmer) convenience for your user/tester's convenience ("god damn it, it crashed again, couldn't the author spend 5 more minutes and do it the right way?"). If you mean programming convenience as in using undefined during development ("I'll put undefined here, because I want to fix other errors first"), this case can be solved with the loop solution. Another solution would be regular undefined, that results in compile warning (without the off switch). I'm willing to accept the error function, but with a slightly different type - instead of String -&gt; a, it should have the type :: Name -&gt; Address -&gt; Insult -&gt; a, so the usage would require providing your name, current address and value of type *data Insult = Moron | ...*. this would prohibit writing library functions with errors (you wouldn't want your name in someone elses' app), but the final application coders still would be able to get all the blame. and loop is a better solution, no sane person would write a function that has the possibility of infinite loop - how would you debug it? there wouldn't even be dummy stack trace like error provides. 
&gt; On top of roconnor's concern about accelerating interface fixing (which you are mitigating by not moving too quickly). By not moving quickly they are causing the acceleration of interface fixing. If they moved quickly, changing interfaces constantly, users would find HP unreliable and this would mitigate interface fixing (and defeat the purpose of HP at the same time).
&gt; If you mean programming convenience as in using undefined during development ("I'll put undefined here, because I want to fix other errors first"), this case can be solved with the loop solution I mean that too, but I use error for that: something like `error "myFunction: not implemented yet"`. I don't see how loop is any better; what do you prefer to see, an error messsage or `&lt;&lt;loop&gt;&gt;` or a crash? `undefined` is used in all kinds of type hackery, when you have a type, you want a member of it, but you don't know about any concrete element. An example is the `Storable` class. With EmptyDataDecls, you can even have types which have no members except the bottom... Again, I don't see how loop is any better than undefined. What I would like to see is that `error`, `undefined` and friends would print which source file and which line they appeared (though that does not help with `head []`...).
&gt; error "myFunction: not implemented yet". this would be even better with a friendly warning from the compiler:) type hacks using undefined aren't really hacks, it's usually statically assured that it's safe. anyway, I think few of those problems can be solved without undefineds - every undefined :: a, can be substituted with Phantom :: Phantom a, and dispatch in a type sig on that a. I'm not totally sure about empty types, but I think there's no problem with using undefined with them, program would only crash if it was evaluated, but the only way to do that is to pattern match on a non-existing constructor (seq breaks that assumption, so maybe empty data types are bad after all). it seems that there are many different "types" of undefined, I only object to those that can crash the program (like head []).
Yes, let's ghettoize ourselves as much as possible.
&gt; It's by far the best place to ask and answer about Haskell questions online. Citation needed. I like IRC...
I think that by moving slowly they are able to limit themselves to the implicit interface fixing that happens on hackage as certain packages become nearly ubiquitous dependencies. That kind of thing can be hard to spot for a newcomer. Moving fast would not only defeat the purpose of the HP, it would add some artificial force to the mix of organically evolving tendencies found on hackage (i.e. certain packages becoming nearly ubiquitous). The way the HP is run now, after manually installing a handful of libraries from hackage on a fresh GHC you end up with a high percentage of what is included in the HP. That makes the HP a useful service that doesn't inject much, if any, of its own personality into the issue of package selection.
Actually, due to a bug linked to a change in the AttoJSON package, it's already 2.0.1. Apologies!
&gt; That makes the HP a useful service that doesn't inject much, if any, of its own personality into the issue of package selection. I disagree. On #haskell newcomers would often ask which monad library they should use. I would always tell them that the mtl is full of bugs and bad interfaces and they should use a modern library such as monadLib. Now the HP has totally thwarted this and newcomers often reach for the mtl since it is installed on their system already **even though their dependencies do not require them to use the mtl**. I.e. these people could start using a modern monad library, allowing us to slowly break free of the mtl, but don't.
I think you have a good argument with mtl, but I don't think the HP has played much of a negative role in the situation. The frustration here is that the mtl becomes the default *just in case* you might want to interoperate with an mtl-using library, and you want to save yourself a potential future headache. This is a problem on hackage irrespective of the HP. Is the HP really making this worse? Are there many programmers savvy enough to figure out they need monad transformers, then determine that they already have a library installed that exports things of the right type, all without consulting some reference? I would really expect most people to try out monad transformers either because they need to use a particular transformer library for interop with another library or because they have read an article or been informed on #haskell to look into it. Such advice can easily include the phrase "cabal install monadLib". The only reasonably accessible data I can think of to inform this debate could come from new packages on hackage that depend on mtl. Authors of such packages can say why they chose mtl. Is it because it is in the platform? Is it because it is what everyone else uses? etc. I really don't know the answer to this, and my own hunch could easily be wrong. My hope is that HP will be part of the solution here, which, apparently, is already underway.
Can someone please write a self:haskell post comparing YHC, GHC, NHC, UHC and Hugs? I admit, I'm lost now. I just default to GHC because it's Hackage-friendly.
I'm not entirely sure what exactly you are asking for, but loading a Haskell source file named "Test.hs" which resides on my desktop always goes like this: computer:~ apfelmus$ cd Desktop computer:Desktop apfelmus$ ghci Test.hs GHCi, version 6.12.3: http://www.haskell.org/ghc/ :? for help Loading package ghc-prim ... linking ... done. Loading package integer-gmp ... linking ... done. Loading package base ... linking ... done. Loading package ffi-1.0 ... linking ... done. [1 of 1] Compiling Main ( Test.hs, interpreted ) Ok, modules loaded: Main. *Main&gt; In any case, MacOS X follows the [UNIX conventions][1] for paths and directories. [1]: http://www.washington.edu/computing/unix/startdoc/directories.html
**WHY IS THIS IN COMIC SANS?**
this is what it says "Now navigate to where it's saved and run ghci from there." Its an hs script (baby.hs) Your load instruction does not seem to have any slash, nor rootfolder called c:// ? computer:~ apfelmus$ cd Desktop computer:Desktop apfelmus$ ghci Test.hs Also, you use two instead of just one line?
I'm not entirely sure what you're trying to say, but I'll try to be more clear :). The point of the guide is to say that you need to be on the same directory as the Haskell file. The C:\ was just an example, and you are supposed to change directories how it's done in your operating system. He was using two lines to clarify the point. He could have also called the script directly with the path. The following invocations all work. cd $HOME/Desktop ghci Test.hs # Test.hs is the file to load in this case ghci $HOME/Desktop/Test.hs # Test.hs is the file to load 
The root of Mac OS X (and any Unix-like systems I've seen) is /. You could say this corresponds to C:, although other drives that you use (that would be D:, E:, etc in Windows) reside in /Volumes/volume_name instead. When you start Terminal.app, you end up in your user folder, i.e. /Users/yourusername. You can see this by typing pwd and hit enter (print working directory). Use cd to move around in the folder tree, in the same way as on Windows (main difference is that / is used instead of \). Use ls to see which folders and files are within the folder you are right now. So, for the baby script, you would use cd to get to the folder for baby.hs, then run ghci and then type :l baby to load baby.hs. Hope this helps. :) 
Thx for the help :) thought my system acts a bit strange, when I type "cd" I get "&lt;interactive&gt;:1:0: Not in scope: `cd'" I cant even get to my home directory?? 
THX for the link, it explained a lot.
Wait I got it, first go to the root, THEN type ghci Of course my instructions didn't made any sense!! Sorry for all the noobish irritation folks ;)
The cd command is for the command line, not for ghci. You were in your interactive ghci
Afaik, you're best sticking with GHC as YHC,NHC are not maintained as well as GHC. UHC is under development but isn't quite there yet for everyday use. Hugs is just an interpreter.
Aaaah, working smootly now, thanx to u guys conclusion; haskell means that you must know your UNIX commands too... Which is a bit of a surprise, Basic and Java dont work that way
You need to open a Terminal program, and use that to call `ghci` from the appropriate directory. Invoking `ghci` directly will not work. IIRC, the Terminal program is usually available under the `X` symbol. Be warned that it can be quite difficult to locate the files you interested in in the file system because MacOS X displays rewritten paths to the user which cannot be used at the UNIX layer.
If you just need to compile Haskell and you don't want to hack on compilers, GHC is really the only contender on your list.
Don't forget Jhc.
This question can't be dodged forever. One day, our children will have to pay for our font abuse, and it won't be pretty.
Let's get this done: http://www.kickstarter.com/projects/662700668/comic-sans-or-the-most-hated-font-in-the-world
&gt; I'm lost now. I just default to GHC because it's Hackage-friendly. If you're lost, you should use what the Haskell Platform provides for you. That is its purpose. http://haskell.org/platform
Damn it! Another package with a 'Crypto' dependency. It doesn't even build if you have a recent quickcheck installed. ARRGGHHH. EDIT: Is randomly generating the pad bytes standard in SSH? Is there a reason not to use a good RNG? EDIT2: If you use Crypto for Data.LargeWord please move over to crypto-api. I intend to nuke Crypto by uploading an incompatible Crypto-5.0 some day to ensure this insanity ends.
By the way, I wouldn't quite call this "pure Haskell": it's using HsOpenSSL, which is a binding to the OpenSSL C library. Now using the [tls package](http://hackage.haskell.org/package/tls) would be pure-Haskell.
It appears he only uses openssl for DSA, which isn't exported by the tls package (if it's even in there).
I think Mr. Peyton Jones likes Comic Sans - at least most of his presentations that I've seen, seem to be in it.
Why am I being downvoted? Of all the obscure places that Comic Sans might still have a foothold in, I did *not* expect the Haskell or Reddit communities.
It's up: http://docs.yesodweb.com/blog/enumerators-tutorial-part-2/
&gt; why go for a Nix-style management when it would be a lot simpler to just use Nix itself? Nix is already written, well thought out, and works today. Haskell is not used only on unixes, although a lot of people seem to make this unfortunate assumption. I haven't personally used a language that makes it easier than cabal-install to install a library on my windows machine. There are some kinks (eg. Gtk2Hs), but they're getting ironed out as it matures.
&gt; For the rendering of the 2D-graphics (as well as for some other things) we use Qt, more precisely the hardware accelerated OpenGL® backend. I'm wondering if they mean through qtHaskell or if they used Qt directly?
You *can* write them efficiently using a Builder (like for Binary). See http://hackage.haskell.org/packages/archive/text/0.9.0.0/doc/html/Data-Text-Lazy-Builder.html Note that the main API feature is the Monoid instance. It probably deserves a mention in an example. It's just not as simple as using lists (although, with sufficiently prominently available examples it should only be marginally harder.)
What about LHC? And HBC!
I suspect that not-whining-about-fonts has a foothold in the Haskell community.
never heard of the haskell motto "Avoid success at all costs!" ? 
Because SPJ is the only man on whom Comic Sans is not ill-placed. I mean, he's still got that jumper...
Iavor (author of monadLib) always has an interesting way of coding and I'm not sure I like it. For example: bugs me (though I know it shouldn't matter) when the module hierarchy is "backward" and the deeper modules depend on the higher ones (`MonadLib.Monads` depends on the `MonadLib` module). I avoid the mtl/transformers/monadLib divide when possible (ex: crypto-api doesn't have any monad library dependency) but otherwise stick with monads-fd as it's slated to replace mtl when mtl-2.0 comes out. On a similar line of thought - we should have a 'Prelude Report' that compares the currently dominant Prelude with alternatives ([prelude-plus](http://hackage.haskell.org/package/prelude-plus), [utf8-prelude](http://hackage.haskell.org/package/utf8-prelude), [gofer-prelude](http://hackage.haskell.org/package/gofer-prelude), [numeric-prelude](http://hackage.haskell.org/package/numeric-prelude)). These all seem to be single-issue parties ("numeric hierarchy sucks", "gofer nostalgia", "utf8 support sucks") but the Prelude SHOULD NOT be as static as it is/has been. Perhaps we just need a good proposal.
Simon Peyton-Jones does _all_ his presentations in comic sans. It's the official font of Haskell :)
[http://www.youtube.com/watch?v=i3k5oY9AHHM](http://www.youtube.com/watch?v=i3k5oY9AHHM)
Big thanks to Michael for writing these; besides the obvious benefits of having excellent tutorials available, he's also found some lacking docs or missing features in the library itself. Because no reddit post is complete without typo nitpicking: &gt; can feed Ints to any **Iteraee** accepting
Great tutorials! A few questions: * Why is Chunks used? Why not allow a parametric type and if it happens to be a list of chunks, that would work too (And have the same performance, if chunks are in fact used there)? * Isn't SomeException specific to IO? Maybe parameterize the exception type? * Maybe instead of an EOF data constructor, the Continue constructor could have another continuation in it to handle EOF -- because the type it returns in that case should be different (Must be Yield or Error)? * "Yield" can have a "left-over" Stream, which seems error-prone too. But if we get rid of Chunks, it doesn't make sense to have it anymore. In which case, implementing Chunked iteratees on top of simple (unchunked) iteratees could stick the "rest of chunk" inside the Yield result type. Then, the "Yield" and "Error" constructor could be unified, and the type sum could instead happen inside the yielded result type, rather than complicating Step, the core of iteratee. Seems a bit more modular that way.
(e: reddit's mangling the markdown. sorry) 1. Because then you couldn't compose chunked and unchunked iteratees. 2. `SomeException` isn't specific to IO; pure code can easily generate exceptions. In early versions of the package the error type was parameterized, but in practice this just caused problems when composing iteratees/enumerators from different libraries. Any type can be packed into a `SomeException`, so using it allows every library to share error handling. 3. This was an idea when I last revved the major version number, something like: newtype Iteratee a m b = Iteratee { runIteratee :: m (Step a m b) } data Step a m b = Continue ([a] -&gt; Iteratee a m b) (m (Result a b)) | Finish (Result a b) data Result a b = Yield b [a] | Error SomeException But it diverges too far from Oleg's initial design for my comfort. It's possible that at some future point, when the correctness and performance of left-fold enumerators are better understood, this design will be implemented. 4. Yield must contain 0 or more input items, since sometimes multiple input items must be consumed before the iteratee can determine what output to generate. Unifying `Yield` and `Error` will just force everybody to type their iteratees like `Iteratee a (Either SomeException b)`, which is not an improvement over the current design.
Not that I downvoted, but I am a bit down-vote happy on reddit when I see ALL comments (out of 10+ comments) are on things like fonts when the topic is GHC 7, or 70+ comments on whitespace when the topic is Haskell 2010. I mean WOW! No decent conversation on reddit at all for many of these topics, its really a shame as reddit _could_ be good but is currently a ghetto.
&gt; Iavor (author of monadLib) always has an interesting way of coding and I'm not sure I like it. For example: bugs me (though I know it shouldn't matter) when the module hierarchy is "backward" and the deeper modules depend on the higher ones (MonadLib.Monads depends on the MonadLib module). I'm also not a big fan of how all the monads and related stuff are in a couple large modules. Maybe it's silly, but I rather like how the mtl and its derivatives put each one in its own namespace. [edit] I should note: aside from the above complaints, I think monadLib equal to or better than the mtl-alike designs.
1. Couldn't you "lift" an unchunked iteratee to be a chunked one? 2. Giving up type-safety of exceptions and essentially using Dynamic for this sounds like giving up. What kinds of problems was it causing? Couldn't error types be converted/coerced to common sum types? 3. Why is diverging from Oleg a big deal? 4. I don't quite understand your point here, can you clarify? (EDIT: also, it will only force people to use (Either SomeException ..) if their code can indeed throw exceptions. Using a Dynamic exception type rather than explicitly-typed exceptions/lack-thereof seems "wrong" for the same reasons we have purity in Haskell in general.) Here's an example I toyed around with, that uses my approach (and names I find nicer): http://gist.github.com/608187 
Basic and Java can be used from the Unix command line as well, but you probably used an IDE for them. There are IDEs available for haskell as well, like this one: http://www.leksah.org/ , but I'm guessing most people learning haskell are already comfortable with and more productive using the command line.
Yes. What I mean is that it's not advisable to concatenate `Text` itself, you have to use a different data structure for that. `String` is more lenient in that regard.
Just a small remark: I find it a bit confusing that all the functions are called `sumN`, especially when half of them calls `sum6`, and the other half is composed in some form with `sum6`. So you have `(sum11 $$ sum10 $$ sum6)`, but clearly the first two are completely different beasts from the third one.
It's a good point, I considered renaming them to something else, and I probably should have. I'll keep it in mind while writing part 3.
Response from Simon PJ: he likes Comic Sans and he doesn't care what you all think :)
Can someone give a rundown on the essential differences between this and [Accelerate](http://hackage.haskell.org/package/accelerate)? It seems like we've had duplication of effort here. Sadface.
This is correct. The only two modules it uses in HsOpenSSL are DSA and BN - I'd love to remove that dependency, but when I was writing this I was time-constrained.
Huh, wasn't aware of the Crypto dependency hell. I'll get on that, thanks. Yes, randomly generating the padding is [standard](http://tools.ietf.org/html/rfc4253#section-6). I suppose a good RNG could be used - initially they were just null bytes.
(sorry for the long reply delay; reddit didn't give me an orangered!) 1. For some simple cases, but anything more difficult (like nested enumeratees) requires being able to convert both ways -- you'd end up packing/unpacking the input type many times. 2. It's not giving up type-safety; using `Dynamic` for casting requires the value to be the same type as its being cast to. If error types are parameterized, it's not possible to compose enumeratees using different error types. Conversion is not possible unless the sum type can be converted to/from every possible error type (aka, is `SomeException`). 3. Because every existing enumerator-based library uses his types and notation. Changing it would break these libraries, as well as make porting from other implementations (such as `iteratee`) much more difficult. 4. Errors can come from any part of the stack -- Iteratees, Enumerators, and Enumeratees. It's not possible to generically compose these types unless the error types are identical, or at least mutually convertable. For this reason, every standard implementation in the `enumerator` package would have to use `(Either SomeException ...)`, and so anybody who wanted to use the standard library would *also* have to use an `Either` return value. You're welcome to try re-implementing enumerators to fix these problems; as a test of your design, build a composition of (enumerator, enumeratee, iteratee), each using a different error type.
i. Maybe using some type-class magic, you could easily write unchunked iteratees and have them lifted automatically (in all cases, that way you don't need to unchunk them). Maybe that would mean having a different type for chunked/unchunked iteratees. Maybe a newtype around Iteratee that wrapped the input type in a list-type, and have the various combinators use that, and the "simple" iteratee only as a simple building block. ii. Runtime type safety isn't really "type safety" (in the sense we know&amp;love in Haskell). I don't mean a generic sum type, I mean a sum type the user chooses on a case-by-case basis. iii. I see. Maybe API adapters can be used? iv. What if errors from Enumerators went into the "input" type of the Iteratee and errors from Iteratees when into the output type of the iteratee? What about enumerator/iteratee combinators that automatically use sum or product types to compose the different types? &gt; You're welcome to try re-implementing enumerators to fix these problems; as a test of your design, build a composition of (enumerator, enumeratee, iteratee), each using a different error type. I'll try. Do you have a good example that involves some interesting error types?
i. I don't want to introduce type-class magic, implicit conversions, and separate iteratee types just so somebody doesn't have to type `Chunks`. ii. The problem is that error sum type has to be support by every single element in a composition, from the initial enumerator to the final iteratee. If any element uses a different error type, then it won't type-check and you're stuck. iii. You're welcome to write one -- let me know if you can get it to work. iv. Then every iteratee has to be written `Iteratee (Either SomeException a) m (Either SomeException b)`. &gt; What about enumerator/iteratee combinators that automatically use sum or product types to compose the different types? It's not possible to generically compose error types, because they have to be mutually convertable. That is, you'd need two functions `:: (e1 -&gt; e2)` and `:: (e2 -&gt; e1)`, and every single combinator would need to take them as parameters. &gt; I'll try. Do you have a good example that involves some interesting error types? The case which caused me to remove parameterized error types was trying to combine a file enumerator (error: `IOError`), XML parser (error: `String`), and JSON serializer (error: [`GeneratorError`](http://hackage.haskell.org/packages/archive/yajl/0.3.0.2/doc/html/Text-JSON-YAJL.html#t:GeneratorError)). You don't need to use real error types for testing, just three different, mutually exclusive types.
D'oh! I fixed the publication date, which made the above URL wrong. The correct one is: [Haddock Revamp](http://mtnviewmark.wordpress.com/2010/10/01/haddock-revamp/)
They *are* very close. Still, it seems that Accelerate is a bit lower level in terms of GPU memory allocation and transfer. - Accelerate makes you explicitly transfer an array to the GPU by calling "use" whereas Nikola (I think) handles the array wrapping/unwrapping behind the scenes. As a consequence, your Nikola functions will have ordinary Haskell types, which you can then plug into any other piece of Haskell code. - Since they're handling GPU memory for you, Nikola needs to implement size inference to figure out the size of any inputs/temporaries your code may need. 
&gt; but I think that is saying that with Nikola I can compile Haskell code into CUDA (albeit only a subset of Haskell code). That's pretty much it. Most of the paper focuses on *how* they embed a CUDA-friendly language into Haskell. 
Good for him! I think it adds to the quirky charm of the Haskell community myself...
The question is if it is interoperable. It wasn't for TLS (despite being encouraged in the RFC), and you suffered quite a bit if your implementation did it.
&gt; Giving up type-safety of exceptions and essentially using Dynamic for this sounds like giving up. What kinds of problems was it causing? Couldn't error types be converted/coerced to common sum types? Dynamic is essentially a generalized sum type. data Either a b = Left a | Right b this is a value of one of two types, tagged at runtime with information about which of the two it is. data Dynamic = forall a. Wrap a (TypeRep a) this is a value of one of one of any of the types that `TypeRep` is capable of representing, tagged at runtime with information about which type it is. If you don't mind a closed universe of types, you can even use GADTs for the type rep, and then there'd be absolutely no basis for saying that it isn't "type safe". Or you could have a JHC-like built-in open GADT for type reps, and retain the ability to work for every monotype in the language. GHC's implementation is certainly more hackish. It is merely safe by construction, and that construction is easy to defeat. But it is in concept no less type safe, or more dynamically typed, than using sum types, as it is just an infinitary version thereof (existential types are, after all, related to dependent _sums_).
Nice fish! And well-done presentation.
Why are we all not using agda? edit: I guess the video answered my question to some extent, and I have no real experience with dependent types (just reading about agda), but it does seem that Haskell exhibits agda envy.
Well, `Either String Int` is less type-safe than `Int`, when all the values you're actually going to use are of type `Int`, and your code will not handle the `String` case. Any sum type larger than what you're actually going to have is less precise, more partial and thus less type-safe. So of course an infinitary type sum is less type-safe than a small closed type sum, where you can actually handle all of the options properly. 
I'm sad an ACM membership is still required for some of these publications.
My first contact with basic was with the commodore 128, an insanely primitive computer by todays standarts, It was still great fun thought. So modern IDE's are Terra Inconita to me, but learning them is a real joy, just as teaching myself to program the (much) more advanced languages.
Actually, this sounds like being near the sweet spot between the practical applicativity of Haskell and the metatype features provided by dependent typing.
We are not all using Agda because (1) using Agda is hard and (2) it is not (yet) ready for prime time as a practical tool for actually getting things done. I don't think it's Agda envy so much as just knowing a good thing when you see it. Haskell is borrowing features from Agda in the same way that other languages borrow features from Haskell.
&gt; MonadLib.Monads depends on the MonadLib module Control.Monad.Cont in monads-fd depends on Control.Monad. Doesn't that bug you as well?
I do wish the other languages would borrow *purity*. ;-)
No. First you should consider the package hierarchy then the module namespace. `Control.Monad.Cont` is in `mtl` and `monads-{fd,tf}` and they depend on `Control.Monad`, which is in `base`. It seems rather obvious everything involving new monads and monad transformers will need the definition of `Monad`.
not everyone is convinced I/O should be typechecked
&gt; The Haskell platform will extend these sort of problems, to things like the mtl, which is known to be buggy; it was even stated that it was buggy in the proposed update to the mtl; and the proposal even acknowledged that they are replacing the mtl with a newer mtl with the same bugs. Its interesting that you mention the mtl example because I think it shows how we are able to make progress quicker by coordinating a community-wide transition. I'm sure a transition would be much slower and more messy if we did not have the platform.
Aside from non-terminating compilation, what are the weaknesses of Agda? I'm embarrassed to admit that in my own coding I still don't take advantage of most of GHC's type extensions. Haskell98 is a comfortable rut for me to be in, but the next logical level of type system power for me to jump to seems to be Agda. It seems to be more semantically concise than GHC with all its extensions, and in that way easier to use. If anything, I'll probably learn how to do things in Adga, and then try and see what pieces of knowledge I can import back to GHC. I'm quite enthusiastic about the work you're doing, nonetheless.
Well the converse is you can only write terminating programs, right? You don't just get to encode the proofs you want, but also are forced to encode a number that might not concern you, some of which could be somewhat painful, or at least besides the point.
Throughput and latencies would be very nice to know also. Thanks for asking the question, it was nice meeting you at CUFP.
What do you mean by "long-running"? It's standard practice to restart Perl processes once per night (or even once per hour). Unless you're doing very specialized things (or have HA requirements), this isn't a problem once you've figured out that you can and should do that. The nightly scheduled reboot of Windows servers is also legendary. This might be a little bit heretic. All I want to say is that it is quite likely that you can work around issues which only arise in long-running processes. But of course, it's nevertheless an interesting and important question.
We, Galois, have several things that run weeks or months at a time. There's probably some processes we've not shut down for &gt; 12 months. The main thing: hammer it for 24 hours with heap profiling on. If you survive that, you're good to go. PS. A flat line heap profile after a day or a week is good evidence for your boss et al.
But restarts are just a workaround, and a weak one at that. Let's say during normal load, your process (due to memory leaks, or other resource issues) needs a restart every 30 hours. For safety, you plan to restart every 24h (nightly). However, during non-standard conditions, e.g. end-of-quarter processing, or during a self-inflicted DOS due to some badly written script that interacts with it, or anthing that is not "normal load", this will can become not 30 hours, but 30 minutes (it's just a factor of 60 after all). At this point, you'll need to restart so often that you can't keep up. Relative time in computer systems is not the same as for humans, and the rate of "perceived flow" can change very much due to external factors, not under your influence. So… I personally wouldn't use in production anything that needs a periodic restart for "cleaning up".
Bikeshed :-(
I've got a long running Haskell process, actually a couple of them, using forkIO concurrency for handling different STM TChans of data. One of the threads is responsible for talking to a data conversion program that's written in C currently for some raw socket data processing. Another thread deals with stdin to the Haskell process for command processing, and there's others for logging and stdout handling as well. The challenge with long running processes in Haskell typically run around data leaks and too much laziness to do anything long running in the language in it's default mode. You really need to read Real World Haskell's profiling chapter to understand the tools that you have to help you solve this problem you will most likely run into if you don't think of strictness up front. My program runs in flat data space now for months easily, and if it dies, it can be restarted as well. Whether you think that's weak or not, it's important to have such watchdogs unless you like getting service calls from Japan at 2:00 AM when stuff doesn't work. The worst error committed is thinking you didn't commit any errors. Lastly you should think about performance. For me, I'm VERY I/O bound (9600 baud ports), so I could have written this program in nearly any language and not worried about the performance much. 
I have a public-facing HappStack web-app [1] I wrote for my own amusement that hasn't been restarted since May 3rd. It's got a virtual memory footprint a little less than 14 MB. Before I figured out how to use nohup, I had to restart it whenever my ssh connection to the remote server died. I can't remember it ever crashing due to an internal bug. [1] http://lists.bootlegether.net/
It's great to see someone as obviously passionate and excited about what they do as Simon in this presentation. Really gets you excited to be in such a great community. Go Haskell!
We have numbers for a web server app at work. I'll see if I can dig those up. Also, check the recent epoll paper, for particular low level latency benchmarks, http://www.serpentine.com/bos/files/ghc-event-manager.pdf
I used to run the hmp3 player for months. I left it on one server in a loop playing mp3s into /dev/null for upwards of 6 months. Interestingly, this was in 2004, and I used the compacting GC to shrink the footprint even further. Oh, and I forgot: xmonad uptime is ridiculous.
I am running haskell to capture live broadcasting over multicast UDP to store it to the disk in circular buffer, multiple streams. Another process, also long-running, streams it over RTSP on demand. System is in testing phase, but not at all idle. I'm yet to see it to leak. Of course, I memory-profiled it, but it was not hard to do, I had to change couple of places which I wrote with later profiling in mind. I also have one HAppS server which crashes regularly due to buggy mysql driver from hackage, but I don't keep much state, so restart is painless (and quick, compared to java, hehe). When hackage will compile with ghc7, i expect to gain benefits from its new i/o...
My xmonad and xmobar once ran for about 3 months total uptime (which doesn't include downtime due to suspend to disk), until I managed to crash X.
I wrote a web-based booking system that has been running live since september 2009. I delivered the last bugfix sometime in January and since then the guys have had "a couple of times" when they had to restart everything. Unfortunately the causes of these downtimes are unknown, it could be my server software, the server machine(running windows), network problems or even someone accidentally terminating the server process. I don't know about the load on the software, the system has about 600 potential users(that's how many members are registered), but I suspect fewer actually use it.
I have a happstack website. August 12 is the last time its process was restarted. I typically restart it periodically because a maintenance task happens on shutdown, but that's because I haven't bothered to write code to trigger it some other way. Right now "top" shows that this process is using 516 megs of RAM.
Compilation will always terminate in Agda! Well, except if the termination checker marks one of your functions (correctly) as non-terminating, and then you try to use it in a type... The main "disadvantage", compared to what everyone seems to interpret dependent types as: you can _specify_ arbitrary properties in types, such as a function `div` that takes a non-zero second parameter, but then when you call that function, you need to provide an actual proof (usually written by you!) of your arbitrary property. These proofs can be monstrously difficult, and will often depend intimately on the inner structure of the code you're proving about, so that breaks abstraction (unless you care to fully specify the behavior in terms of a property of your algorithm and then prevent people from looking inside afterwards). It's also not always clear what proofs are _relevant_ to computation and which ones aren't. Agda currently supports a new annotation mechanism for this, but there are still lots of kinks that need to be ironed out of it. Furthermore, while everyone agrees that dependent types can lead to awesomely efficient compiled code, most dependently typed languages don't compile very well. Agda compiles by using Haskell as a code generator (_not_ a typechecker, since Agda's types are mostly more expressive than Haskell's), which means that every single expression in your Agda code is wrapped in an `unsafeCoerce` before getting passed to GHC. This prevents many handy optimizations that can't see past all the `unsafeCoerce`s. Having said all that, I love Agda, and write stuff in it almost daily. It's definitely worth learning, but don't go into it expecting a "more powerful Haskell" or you'll be disappointed :) Also, no typeclasses or type families!
stop your perl FUD. indeed if you are running web serving processes, afaik the current crop of haskell frameworks require a recompile and restart any time you change the content of a page, so the chances of you restarting your haskell server/framework are much higher than a perl server...presuming content changes happen more frequently than core dumps
yawn....i've had server processes whose uptime was measured in YEARS. yes YEARS, and thats under high load. i don't see whats even moderately interesting about a haskell process staying up for weeks
You can write non-terminating programs in Agda. You just get a warning. Of course, you can't use non-terminating expressions in type checking unless you are willing to deal with the consequences.
This isn't a server bragging thread. The poster wants evidence that Haskell is being used successfully for server processes, to give confidence to companies considering Haskell for projects in this area. Haskell can do this, so we should document that fact. Are your multi-YEAR processes Haskell? If not what do they have to do with this.
As I asked on the blog (though comments seem moderated), why are those packages listed as "unmirrorable"?
I'm not sure about type families, but aren't type classes fairly easy to emulate in Agda? I feel like quitting my job and just hacking around with Agda for the indefinite future.
my point is that long-running processes are not exceptional, and no one should be patting themselves on the back for a haskell process that has been up for two months
Right. The point is to tell the people who for whatever reason think that this is something that current Haskell implementations can't handle that such processes are not only not exceptional in general, but *not exceptional in Haskell*.
First, content of a *template*, not content of a page. Big difference. A blog post does not require a restart with anything. Second, [Yesod recently grew a fix for that](http://docs.yesodweb.com/blog/wai-handler-devel/) so it's not a fundamental problem _per se_.
as usual, suggestions appreciated, if you find any errors let me know!
We run (and wrote) angel at Bump Technologies ( http://bu.mp ) to act as a process supervisor for all our backend systems: http://github.com/jamwt/Angel It's been running for a couple of months with constant memory and no hiccups.
Mirroring is an operation that shouldn't ever fail for a valid package. It shouldn't be any more restrictive than Hackage *ever* was. But the process fails on these: &gt; network-info-0.1.0.1/network-info.cabal: This package requires Cabal version: &gt;=1.8.0.4 The Cabal 1.8.0.4 line either a parser warning or a cabal-check error. The behavior of both hackage.haskell.org and sparky.haskell.org is to reject on the basis of parser warnings and cabal-check errors but not cabal-check warnings. I'm not sure how it was uploaded, but perhaps Cabal has changed since then. I'll see what I can do. &gt; Bad file type in package tarball: ./PaxHeaders.14903/binembed-example-0.1. For portability, package tarballs should use the 'ustar' format and only contain normal files, directories and file links. Your tar program may be using non-standard extensions. For example with GNU tar, use --format=ustar to get the portable format. Repackaging it server-side might help. &gt; old-time-1.0.0.1 has an entry in the source [log](http://hackage.haskell.org/packages/archive/log) but doesn't [actually exist](http://hackage.haskell.org/package/old-time-1.0.0.1). I don't think there's much that can be done about this.
You can simulate type families on closed "sets" of types with custom universes. But not in general, no. A dependent type family would essentially give you typecase, and that's bad for parametricity :)
Really? I guess that's a pretty simple statement to prove, if you can find one such person :)
Nice! I didn't know about angel. It seems very handy. Have you thought about a hackage release?
And you can load plugins.
i love yesod and michael is one of my favorite haskell hackers...but it must be said, openID is utterly dead and no one cares about it
Thanks for all the great responses so far! If anyone remembers the uptime stories from hpaste and hackage2 in any detail that would be helpful as well.
i've done haskell for 5 years and remain unconvinced
Care to elaborate? Also, total time since "starting" isn't a very good measure for informedness. _Edit_: sorry, that sounds like I'm suggesting you aren't informed. I just meant that telling us how many years you've done it is irrelevant, and that I'd rather see a reasoned argument than an (implicit) reduction to authority.
Really? I've been noticing it on more and more sites recently. All the *overflow sites use it, for example.
Just a quick question, on one line for DiffLists you define mempty like so: mempty = DiffList (\xs -&gt; xs ++ []). Shouldn't it be the other way around? Like: mempty = DiffList (\xs -&gt; [] ++ xs) Which is what you do say earlier?
Decided to dive into Haskell a few weeks ago as a free-time type of project and I found your site. It's been a great place to start so far. Thanks, glad to see you got a book out of it. I'll be buying one straightaway.
it should just be `id`, no?
i've written a decent amount of Haskell and run a couple of workshops on the language. I'm not saying it's a bad way to ensure correctness for IO, just that it's probably not going to see any kind of widespread adoption. in fact, it's probably the biggest reason Haskell isn't more widely used. everyone who tries it loves the purity, pretty type expression etc etc etc but when it comes to writing actual software and they're told IO is handled by some fancy thing called monads, most people stop expecting to write serious pieces of code in Haskell. why? because the average software engineer will never want to have his or her programs machine checked, no do they care about the curry-howard stuff IMHO, typed IO should be a compiler option that is off by default. monadic blocks should be sugared, either by template Haskell or just some compiler hand waving to return either void, a primitive or some user defined type. tl;dr: java has a more useful type system than Haskell edit: i still use Haskell enthusiastically, but I try to be realistic about it's lack of uptake 
Do you keep a current PDF version of the book around? The only one I ever found was missing a few chapters. It would be nice to read this off-line on my iPad. 
How would an untyped IO interact with your pure code and its types? I can see you taking the types of "pure" values and "forgetting" them, when throwing them into IO, but you often want to do the opposite too: pass IO computations around in pure (typed) code.
It would be typed like pure code, but actually have side effects doesn't SML do that? I guess I'm arguing for a weaker static typing
Oh, so you're saying we should be able to do IO from within any pure function? That'd kill Haskell. Edit: just to elaborate, pure code doesn't have a specified evaluation order or a notion of sequencing statements. It can be reordered and restructured in crazy ways by the compiler. This gives amazing benefits. Throwing side-effects into it breaks all of that. It also makes the types a lie (as they are in the *ML languages).
&gt; but when it comes to writing actual software and they're told IO is handled by some fancy thing called monads, most people stop expecting to write serious pieces of code in Haskell. Yes, there's a lot of FUD about monads. It's a simple (and optional) generic API for doing something natural — in this case, gluing together descriptions of IO actions. What you describe is a problem with how Haskell is taught / hyped / misunderstood, and not a problem with the language. &gt; why? because the average software engineer will never want to have his or her programs machine checked, no do they care about the curry-howard stuff &gt; IMHO, typed IO should be a compiler option that is off by default. monadic blocks should be sugared, either by template Haskell or just some compiler hand waving to return either void, a primitive or some user defined type. This makes no sense. How is Haskell's A -&gt; IO B any more cumbersome than C++ or Java's B f(A x); Please provide some code examples so we have some idea what you're talking about. &gt; tl;dr: java has a more useful type system than Haskell Pure troll. Good show.
&gt; tl;dr: java has a more useful type system than Haskell Just out of curiosity, what do you consider to be the purpose of a type system?
type systems in general? or static typing
thats nice, but i'm pretty sure its dead a little thing called facebook connect came along and has 500 million (potential) users. how many people actually use openID? i'm not even entering into a discussion of its usability and security problems
I don't want no summary, I want videos!
No I don't think that would be any better (though it would still work) for the sole reason that we are trying to make appending work a certain way. But I have just realised that since this is only one case in the pattern it does not matter much in terms of efficiency. The real bottleneck would be if the mappend function was slow I think; somebody please correct me though if I am wrong.
If there is no PDF avaliable then why not follow OptimusYPrime and offer him money for the PDF version for your sole use? I am sure that he will atleast consider it. Though I guess that depends on how much you really want it on your iPad. :)
Are you the author? This is such a great tutorial that I find myself rereading it as I work though Real World Haskell + Project Euler problems. It really helps me put the big picture together. I can't thank you enough for this text. (Although maybe buying the book would be a good thanks come January...) --- Typo in: instance (Error e) =&gt; Monad (Eihter e) where 
maybe? doing io is still pretty elaborate compared to just write blocks in whatever imperative language you care to name. isn't that the whole point of do? to express blocks and side effects sort of like imperative langauges? maybe all i really want is a version of do that does the unsafe IO automatically? in fact, yes. yes i do.
Firstly, thanks for the props ;). As far as OpenID: even if most people won't get what OpenID is, there's a few other things to consider: * We're targeting programmers here, not the average joe. I think OpenID awareness is much higher in that audience. * Most people have either Google or Yahoo! accounts, and don't even need to think about OpenID when logging in. I think it shouldn't be too difficult to add Facebook Connect support, which should grab anyone that doesn't have an OpenID account.
I *have* thought about it, and I faithfully followed the instructions here: http://hackage.haskell.org/packages/accounts.html .. to obtain a user account. I never got a response though, and that was several weeks ago. Is there a better/more direct way to get a hackagedb account I don't know about?
Some corrections while I'm reading: * s/beans that cost **25** cents/beans that cost **10** cents/ * s/we still had to make sure that the context **is** taken care of/we still had to make sure that the context **was** taken care of/ * s/ghci&gt; stackManip [5,8,2,1]/ghci&gt;**runState** stackManip [5,8,2,1]/ * s/newtype State s a = { runState :: s -&gt; (a, s) }/newtype State s a = **State** { runState :: s -&gt; (a, s) }/ * s/which takes a error/which takes a**n** error/ Incomplete/incorrect sentences: * "With it, we will be able to take stateful computations like these and operate**..**" * "Let's **look** take a gander at return first." * "once we had **the extracted just** the result from the monadic value" * ",**the &gt;&gt;=** the context of the monad"
 (\xs -&gt; xs ++ []) = (\xs -&gt; [] ++ xs) = (\xs -&gt; xs) = id All four expressions denote one and the same value, and the last one is the fastest.
I'm going to buy the dead tree version when he finishes. I didn't realise he was selling unfinished copies. 
Right, because something owned by a private corporation that not everyone is comfortable handing their personal data to doesn't have any usability or security problems. OpenID certainly has problems, and the fact that you have to set up your own server for it if you want any control over your identity is extremely annoying (and the reason I've never signed up for any *overflow site), but there is no way in hell I would ever use Facebook Connect to sign up for anything.
snoyberg: how the hell are you so productive? I'd be very interested in a comment/post that explains the way you work.
Again, great work! Hackage 2.0 will be made of awesomesauce and win.
I'd really like to hear what you all have to say about lightweight text-level preprocessing for Haskell.
typo: "mathemathics". I know it objects to lots of function names, etc, but you may want to consider running a spellcheck over your chapters, as it will save you grief in the long run. 
It appears to me that this example can also be expressed as main :: IO () main = do putStrLn vshow("2 + 3") putStrLn vshow("product [1..5]") vshow = id There is no point in printing the source of an expression if that's the only thing you do with it. It only makes sense if you both print the expression and use its value: veval (product [1..5]) = (product [1..5], "product [1..5]") EDIT: Oops, I didn't read properly. The OP does exactly that by putting the value of the expression in the string as well. For semantic reasons, `veval` cannot be a function, of course, only a template splice or similar.
&gt; There is no point in printing the source of an expression if that's the only thing you do with it. It's not. We write vshow(product [1..5]) and we get the string "product [1..5] = 120" I didn't write `120` anywhere. It's the value of the expression. `id` won't do that.
LLVM is not a replacement for C--, we're still using C-- as our internal intermediate language. LVM is an alternative to the native code generator (NCG) and the C backends. We have no plans to drop the NCG, but we do plan to deprecate the C backend, except for porting purposes - that is, we're deprecating the "evil mangler" that is used to optimise the C backend route. So we're not forcing people to use the LLVM backend at all. We could possibly make it the default for -O2 in the future, but of course that would have to fall back to using the NCG if LLVM isn't available.
Oh, and the problem with using C-- as a target is that, well, there's no C-- compiler.
There have been some C-- compilers, but they've all been abandoned. Why is that?
Fixed it!
Does LLVM not work for you? What problems do you have with C++? You might be interested to know that a large part of the LLVM and Clang codebase does not use OO with inheritance and indirection. Much of it is parametrically polymorphic for data using enum/switch, and type polymorphic via templates. The LLVM and Clang codebases are cleaner than GCC, and have a more commercially-friendly license. Clang's static analyzer is also lightyears ahead of GCC. (Not that I hate GCC or anything, I think it's a fine compiler.) If you don't believe C++ should be part of your life in any way, you should stop using all of the major browsers (FireFox, WebKit, IE, Opera), Windows (kernel and userland are C++), OS X (kernel extensions and some userland are C++), Linux (GCC is a C++ compiler!), and most video games.
for lists, ([]++) is id, but I do (\xs -&gt; [] ++ xs) so that it's visually similar to (\xs -&gt; "blah" ++ xs) and thus obvious that it's an empty difference list
no one's selling anything right now :) there is a pdf version put it's not up to date. the book will be out in irl form in january and i'm pretty sure it will also be available in electronic form then. anyway, thanks for reading!
thanks! fixed that!
yeah, spellcheck is cool if you're writing a book about vampires or something but for this kind of stuff it turns out you have to teach it like half of the words. i'll do that one of these days though
What do you think about the idea of building a website where everyone can put project proposals or work on existing ones. With ratings and scores ala StackOverflow/reddit/etc.
wow, thanks a lot for these corrections! they should all be fixed now.
I am trying as hard as I can to not use any software that is built on C++ except for browsers. If there were a browser that did not use C++ I would probably use it(I'd like to see a browser written in Go or Haskell) We do not need to start a war over this. People who like C++ and wish to see it be a part of their future should continue using it, but simply letting it be a part of the future for me out of convenience is not part of my ideology. To put what you're saying into perspective, to me it sounds like you are saying "What's that? You don't want Aids? Everyone has Aids now, what are you going to do? Stop fucking? Ludicrous!" or something akin to that.
To be really pedantic, it should be specified that GHC uses Cmm as its intermediate language, which is not the same as C--, but is mostly a subset of it.
Oops, sorry, my fault. I didn't notice the equal sign and the part after it.
Thanks ;). The honest truth (not just saying this cause she might read it) is that my wife is incredibly supportive and helpful. She does most of the graphic design on sites we put together, and let's me talk out some of the more complicated problems I'm trying to crack. In fact, I remember a particular night turning off the computer at 1am, talking to her about a template haskell-induced bug until 1:30, then suddenly getting the solution, running to the computer and hacking again until 3:30. I guess the other main point is to have solid goals in mind. At first, I was just playing with Yesod, and progress went fairly slowly. Once I had a commercial project relying on it, features starting churning out *very* quickly. But even with non-commercial projects like Haskellers.com, just decide what you want (eg, OpenID 2 support) and keep hacking until it works.
almost done, hold on: * "we see that we used **out** moveKnight three times " * s/because multiplication is **distributive**/because multiplication is **associative**/
no magic bullets like GTD? all I need is haskell related job and a wife that speaks TH? that seems... impossible!
I didn't say she speaks TH. I've tried teaching her functors/monads once or twice, but she's been scarred by C++ ;). You just need a wife who doesn't mind listening when nothing you say makes sense.
In § The Writer type, left to the hatted-mustached-cowboy head :"This gives us gives us a Writer w a value ..."
fixed fixed fixed! you rock!
nice catch, fixed!
Actually, one other trick I've been using: lists. I keep a list of open things to do (documentation to write, bugs to fix, new projects to start, *everything*). When I'm just started out in the morning and I've got plenty of energy, I like to attack big things. When I need a break from that, I'll answer emails and the like. But having that list never gives me a chance to say I don't have any work to do.
yes but the people you are targeting are more aware of openID's flaws than its benefits....its exactly the uber-savvy who were the first to drop openID upon learnings of its weaknesses...the non-savvy have never heard of it and never will
In § Stacks and stones, s/we do push 1 stack/we do push 3 stack/ In § Making monads, s/we usually make a type that whose purpose/we usually make a type whose purpose/ that's all from me, thanks for this good page (again)!
EDSLs in Haskell rock! I really enjoyed this talk. It seems [slides are available](http://www.scribd.com/doc/38559736/kansaslava-hiw10) too. Looking forward to playing around with it.
Weird. So GHC's CPP replaces tokens within strings, while the real CPP doesn't. That's surprising, to me at least. I guess that explains why it doesn't support stringify...
ocaml macros (using camlpl4 or 5) work just fine in ocaml toplevel - without any boilerplate at all
I actually anticipated the overall structure of the solution, and thought that was fairly obvious. However I clearly don't understand all the race conditions; the devil really is in the details in this one!
Glad to hear ThreadScope is still being developed/used. Soon I will return to helping. :)
Can't you do that using template haskell? I think CPP is useful mostly for #ifdefs and the __LINE__ macro for "debug prints".
[A similar thread in SO](http://stackoverflow.com/questions/2769487/mtl-transformers-monads-fd-monadlib-and-the-paradox-of-choice/2769664#2769664)
1. Anybody with a Google, Yahoo, LiveJournal, MySpace, or AOL account already has an OpenID. There's easily more OpenID accounts than Facebook accounts. 2. I will bet money that the typical Haskell developer is more likely to have an OpenID-enabled account somewhere than a Facebook account.
We hope to release it soon, including a user manual.
Did you read the article? When I speak of "lightweight" and "text-level" I'm directly contrasting with Template Haskell. TH is a powerful, complex, cumbersome tool. It's best suited for bona fide AST generators, not simple textual substitution. I've no doubt it's technically capable of solving this problem, but I seriously doubt the solution will be anywhere near as clear as this one small line of CPP. If you do write the TH code, I'd love to see it. What I'm looking for is something simple, squarely in the domain of text processing, not abstract syntax trees, yet with a little more flexibility and Haskell-ish syntax than CPP provides. Anyway I'll shut up now because I'm just repeating what I said in the article.
&gt; the fact that you have to set up your own server for it if you want any control over your identity is extremely annoying (and the reason I've never signed up for any *overflow site) Setting up your own server is helpful for maintaining control, but not mandatory. If you don't control your OpenID, you can still change providers, you just have to do it one site at a time instead of all at once.
Ah... The functional programmers ultimate data structure: lists :) 
I'm working on the LLVM backend for DDC and from what I can see, the main problem with LLVM is that it supports x86, x86_64 and arm resonably well, but that other architectures like PowerPC or Sparc are rather poorly supported. 
These kinds of huge awesome improvements, along with the fact GHC is already doing pretty well in benchmarks, makes one think GHC still has far more awesomeness to grasp at :-)
Well played sir ;)
Chicken and Egg problem. The C-- compilers have never got to the stage that they're good enough to use in a serious compile like GHC. The C-- compiler creators didn't want to keep working on a compiler that no one was using, wanted some commitment to it. Other issue is that creating a good backend compiler is a huge amount of work. LLVM has been in continual development for about 8 - 10 years now. Only in the last 3 years or so has it really taken off. It managed to get industry backing from Apple, C-- was always very research/academic driven and so the people involved didn't have much interest in investing 10 years to build a solid compiler when there wasn't that as much research value in it anymore (design having been done).
Why do people like me want to see GHC using LLVM? Because it produces fantastic code, it is already better than the NGC for many cases, especially those performance critical bits of code that are very loop intensive, and i want my programs to run quickly. Having the LLVM backend means the GHC developers can spend less time trying to implement low level optimisations, that are already being implemented elsewhere (such as in the LLVM code generators), and spend more time implementing higher level transformations that will allow the lower lever optimisations to fire, and spend more time implementing more of the features you'd like to see. From what I understand, the developers of LLVM use C++ in a very clear, well structured way, the way it should be used. C++ has a bad wrap because it is often abused, and the LLVM developers work very hard to make sure this doesn't happen. As far as C++ projects go, LLVM should be looked upon as an example of how it should be used, and not just thought of as being crap because it's C++. Edit: I'd also like to add that you are well and truly in the minority here, most people are looking forward to LLVM being incorporated, so that GHC can progress more quickly, as can their compiled programs. You are the first person I've seen who believes that the LLVM backend is anything but excellent.
While TH might be too heavy for simple macros, implementing a *macro system* in TH may be the way to go. Your example reminds me a lot of the printf function that appears in the TH examples. ---- If you want to avoid TH entirely My biggest concern is that it needs to be pretty apparent that we are stepping out of Haskell-land and into somewhere else; Haskell is enough of a challenge without adding a minefield. The C macros blend in too well. I've worked with [ERB](http://ruby-doc.org/stdlib/libdoc/erb/rdoc/classes/ERB.html) and like the syntax. Replace &lt;% with {-% and you end up with something a little easier to see. Of course, PHP would do just as well. Then you add a few hooks into Cabal that looks for these files (let's give them a .ehs extension). It takes them, runs them through a Haskell script that echos everything outside the {-% %-} blocks using putStr and processes everything inside them as if they were code. Implementation is beyond me at this time (I'd probably look at WriterT IO or something like that and beat on it with stick until it works, I give up, or something better comes my way). This gets output to a temporary .hs file stuffed somewhere in the dist directory (e.g., dist/Temp). Finally, the temporary file is added to the list of source directories and the program is, otherwise, built normally.
Just for the record, Facebook's authentication is based on the OAuth 2.0 proposal AFAIK. Implementing support for that was infinitely easier than OpenID. On the other hand, OAuth requires the relying party to set up credentials, so it's not really a proper replacement for OpenID.
Just FYI, text is the first proposal. It is carving the path.
Thanks for this insight. Many aspects around the HP seem opaque and/or tedious for an outsider.
Perl doesn't compact the heap, so depending on your allocation profile, there is a tendency for what are essentially space leaks (even if you don't create data structures with cycles.) As I said, I don't consider such issues a major point when choosing a particular technology, so I wouldn't say this is FUD.
We can't keep up with a sixty-fold load increase anyway, so that's not really a concern for us. The self-DoS scenario sounds all too familiar, but dealing with that by massive overprovisioning seems rather complicated and not financially attractive. (Our applications generally result in creation of an artifact which is archived, and it seems cheaper to clean up manually than to archive all that junk, too.)
I wasn't following the discussion, so I can't say I'm familiar with the entire process or the details here. But that said, in my mind this merely falls into the category of stuff that's not fun to do, but critical to the future of Haskell as a now-successful language. Believe me, if a few million people write Haskell code in the next few years, their collective annoyance at some naming inconsistencies will be greater than the annoyance at having to resolve those inconsistencies, and argue about consensus on how to do so, right now. It's simply a different trade-off when a library is to be incorporated into the platform, versus just being available to whoever decides to use it. To put it another way, by incorporating a package into the platform, there's a large opportunity cost in that you're automatically raising the bar for any other library that might want to do the same thing but with a simpler / more consistent / cleaner API. The community as a whole has a *right* to insist that some collective decisions are made to try to iron out whatever we can. Whereas I have creative license to choose the style of my own packages and just tell people to deal with it, that becomes a lot less true as those packages are given a more official status. That's all said with utmost respect to Bryan, whose time is, of course, valuable, and for whom this process doesn't look very fun. But even a trackless mire can have value.
Did Bryan submit his package to the Haskell Platform, or is this dons' doing and he is being dragged into it?
The process for adding packages is actually very clearly defined: http://trac.haskell.org/haskell-platform/wiki/AddingPackages
They submitted the proposal with his knowledge and approval. Haskell Platform proposals must be backed by the package maintainer, otherwise it must be rejected.
It sounds to me like you are falling for the fallacy that a poor language necessarily leads to crappy software.
I love this book. I almost get monads now. And, BTW, it is going to be a book: http://www.amazon.com/Learn-You-Haskell-Great-Good/dp/1593272839/
Nice job BONUS ;)
This is a very good idea. However, I wish the website looked a little bit more professional. The logo clashes terribly with the tagline, and the goofy avatars really have to go.
I'm sure the msnoyman would have no problem with any contributions of design work -- we've got a few really talented designers now in the Haskell community.
&gt; the goofy avatars really have to go. Users pick their own avatars (via [Gravatar](http://gravatar.com/))
I meant the default, random-generated cartoon faces.
A hearty upvote here. This book was integral in my Haskell education. I am glad to see it in bound form! I hope it does well. 
Indeed. From Amazon: About the Author : Miran Lipovaca is a computer science student in Ljubljana, Slovenia. His online tutorial, "Learn You a Haskell for Great Good!," is widely regarded as the best way to learn Haskell.
what about custom, non-random cartoon faces? are those professional enough for you?
&gt; a function div that takes a non-zero second parameter, but then when you call that function, you need to provide an actual proof (usually written by you!) of your arbitrary property. Indeed, and integrating something typestate-like local reasoning for these properties would probably help automating this tremendously. Non-zero is a pretty simple example that shouldn't be hard to satisfy at all: either you propagate the requirement up to the caller, or you handle the zero case yourself locally.
I figure now would be a good time to post.
Awesome! Will be getting it and recommending it to my coworkers!
Miran? You are a star!
Gravatar has different [default styles](http://blog.gravatar.com/2008/04/22/identicons-monsterids-and-wavatars-oh-my/). These are "wavatar" faces, which are particularly goofy.
No, no, I'm not Miran! I just discovered his site a few days ago, and then I ended up making a Reddit account within the next day or so.
That didn't make any sense to me until I came back here the second time.
Aw, damn. I thought we had a celebrity in our midst.
Be sure I will recommend that book to every beginner Bonus!
Good techniques like this one that I'll have to remember to steal is why I subscribe to `/r/haskell`. 
http://www.reddit.com/user/BONUS_/
ah... of course.
I'm thinking he should make some improvements and/or extensions to the material to make the book more attractive. Maybe some people would like a book, but an online reference is good enough most of the time.
I now understand this as well. I'll get my coat...
Already preordered.
400 pages? Is it a lot more extensive than the website?
Nice work! I can't wait to buy a copy!
Your coat? [Ah.](http://www.youtube.com/results?search_query=%22I%27ll+Get+Me+Coat%22&amp;aq=f) There's even [a website by the name.](http://www.illgetmycoat.co.uk/) Fake explanations [here](http://www.google.com/url?sa=t&amp;source=web&amp;cd=2&amp;ved=0CCAQFjAB&amp;url=http%3A%2F%2Fwww.mudcat.org%2Fthread.cfm%3Fthreadid%3D56653&amp;ei=K2ywTLqFMoaBlAf5mOjkDw&amp;usg=AFQjCNHTnYS6D8lRQFgeyOAB2sMOODQFbQ&amp;sig2=F6NbfGpPYL2J12cBE5zz5g). Book by same title [here](http://www.bookworks.org.uk/asp/detail.asp?uid=book_B19047C7-573E-4B1A-90D2-F89116B96DD1&amp;sub=new).
Edit: 9 down votes, this is a sad day in the annals of improbable research.
haskell isn't "easy"
Yes, it is :-)
I am an old guru of many languages. Haskell's behavior surprises me and I am not immediately familiar with it from my other languages. My "Haskell guru friend" tells me that magic is going on. I'm too old for this language!
I am really coming to loathe the something x = (glimph . fargh) nuegen $ x where glimph = somethingcomplicated fargh = something else complicated nuegen = something else complicated style, vs something x = let glimph = somethingcomplicated fargh = something else complicated nuegen = something else complicated in (glimph . fargh) nuegen $ x Computers eat that binding order for lunch, humans end up bouncing all around the definition to understand what's going on and if you're new to Haksell, _you don't know where to look_. Not instinctively yet, anyhow. Besides, the former feels so top-down while the latter feels more bottom-up, and I think functional's strength lies with the latter, not the former, small pieces composed together to make something larger, not the "something larger, oh, by the way, here's how you make it I guess...".
Frankly, most of the work so far has gone into making OpenID 2 a reality; I think I've spent only 10% of my time on this site working on the site itself :(. And of that 10%, most of it went into coding, not designing. As sclv said below, I'll happily take an design contributions, plus give appropriate credit in the footer. And regarding the avatars: you're right, I just grabbed the code from another site I had done and forgot to choose a better default avatar. Edit: the default gravatar is now using identicons; I could have gone with plain silhouettes but I like having something unique per user. Also, the tagline is gold instead of blue. Did you have any other concrete problems with the design? I think overall it's fine, but I'm not a designer by trade. The only other major change I'm planning is pagination/filtering for the homepage.
I'd take a look at blueprint.css just for some basic layout help and sane typography defaults. Give it a try, I think you'll find that adding it will provide a lot of polish right out of the box.
thanks! :D
well i wouldnt say star but hopefully a blockbuster hollywood movie will be made based on LYAH soon
i think it will end up being just below 400 pages, but it's not anymore extensive than the site. it's surprising that it actually is that much. but then again, there are a lot of code examples and pictures
thanks, i can't wait for you to buy it as well!
cool account :)
Actually it's 4-7 now. I wanted to go on Thursday, so I got into their systems and changed it.
This can be arranged...
This is a great site and a great resource, thanks! A few suggestions: * Report how many private profiles are in the system in addition to those shown publicly on the homepage. * In the profile, indicate the industries for which one has programmed, using Haskell and not using Haskell * In the profile, indicate technologies known other than those specific to Haskell, both those for which one has programmed in Haskell and those for which one has not * Allow leaving the question about whether one is searching for employment blank * Allow search by any of the types of information in the profile * Profiles that might not be serious should not be displayed on the home page. E.g., people who have not supplied their full name. Not sure about people who supply a picture of a chihuahua as their avatar. * Profiles whose avatar is even slightly objectionable should not be shown on the homepage. E.g., someone shown sticking their finger in their nose. It should be made clear that this is completely at the discretion of the site owner. 
* There's two different types of "private" profiles: users who have not verified an email address, and users who have not selected public. I assume you're referring to the second type, right? * Good idea * I would imagine this would go under the skills section, right? * You can already leave employment option blank. Yes, I thought about that too ;) * Search is *definitely* a coming feature, I just want to give things a chance to settle down to see what we'll be dealing with. * I'm considering adding badges in the future for "real Haskeller" or things like that, and those people will get priority. * I saw that one too... When asking for gravatars, Haskellers only allows G-rated ones through, so in theory objectionable content should be filtered by them. Of course, in reality, I believe you are correct: we'll need to add a "flag this user" link unfortunately.
I think the where-form puts the primal computation up front, which helps readability if the variables are named meaningfully as needed. I rarely read code linearly - too much to keep in memory - I look for the primal computation and then the parts of it I don't understand. The where-form makes that much easier.
This is a nice post. There is a typo in the final code example of the "Enter type operators!" section - spurious right paren.
I have a lot of respect for someone who can write an interpreter for such an elegant language in the piece of shit language known as javascript.
I have no idea why you were downvoted for your opinion. At the very best, it's a non-inspiring mediocre utility language.
JavaScript is actually a really nice language. The problem is that when you code on the web, you don’t code in JavaScript. You code in a polyglot of 10 different mutually incompatible implementations of JavaScript.
Documentation quality appears par for the course for academic projects: one-liner boilerplate everywhere, with a master’s thesis on the side.
I hate coding in JS as least as much as you do, but it's still Turing-complete.
I love you. You got me into Haskell. My exam question for Haskell was your example of functional programming to determine the shortest route. They even used the same distances. Thanks for the good work. I can't use Amazon (they don't take paypal) or I would buy the book.
Thanks.
Now that I think about it, most of my hatred of javascript comes from different browsers handling the DOM manipulation differently.
You can PayPal me and I'll have it sent to you, if you'd like. We can do it over eBay so it's more secure.
Brainfuck is also Turing-complete. That doesn't necessarily make it a good language to program in.
Any noob could create a turing complete language given enough time; they actually seem to evolve rather naturally but it takes real skill to create a beautiful language like Haskell. My vote is somewhere in between for Javascript though; there are some really awkward behaviors that you need to be aware of. Maybe this will tip me in favour. :) 
I'd say that JavaScript a nice language, I dunno about "really nice". Proper tail call implementations would certainly make JS much more bearable, as it would greatly simplify generating JS code and avoid the common situation of needing to write form validation code twice, once for the client side and once for the server side. Also, it's approach to dynamic typing leaves a lot to be desired; really you shouldn't support both automatic conversions (like converting ints to strings) *and* operator overloading (like using + for both int addition and string concatenation). Python, for example, avoids automatic conversions and supports overloading, while Lua avoids overloading and supports automatic conversions. JavaScript, on the other hand, prefers to sacrifice the associative property of +. But yes, the fact that JavaScript is in fact 10 subtly different languages is probably the most frustrating thing of all.
It's the same old stunted C family syntax with a prototype object model and aggravating scoping. There's nothing necessarily wrong with being mediocre, that's probably why it's so accessible. I'm appalled that anyone would claim it's a great or even good language though.
No, it's not piece of shit. It's [misunderstood](http://www.crockford.com/javascript/javascript.html). And most of the time, people hate DOM and blame JavaScript :)
&gt; which helps readability if the variables are named meaningfully as needed. A very, very big "if" in Haskell, the Land of the One Character Variables. If there was less "glimph" and "c" and "z" flying around and the 'primal computations" less frequently looked like someone stuttering I might agree with you.
But I don't see how that is relevant to let vs where? There are conventional patterns to the one character names: * ``a`` is some type * ``b`` is ``a`` when ``a`` is taken * ``f`` is a function * ``m`` is a monad * ``x`` is anything, specifics irrelevant Me not being a Haskell programmer, this list is obviously not exhaustive. Any else you've seen in use?
That's the Turing tarpit and we don't want to be stuck there.
And first class functions! Which make almost anything else tolerable.
&gt;But I don't see how that is relevant to let vs where? I'm afraid I don't see how you said "if" in your first post, then suddenly don't get what your own "if" statement means in the next post. You say it makes sense if good names are used, I point out good names are frequently not used. There are indeed conventional patterns. I do not have a problem with conventional patterns; every language has conventional patterns ("'i' is the for loop counter"). There's also a lot of single-characters or little snippets of two or three characters that show up all over the place in real Haskell code that are not any of the conventional patterns. Haskell is the only modern user community where I've actually seen people go to bat _against_ descriptive names in their code. I'd rather see them explained first than last. Here's an example, the [Yesod enumerators tutorial part 1](http://docs.yesodweb.com/blog/enumerators-tutorial-part-1/), definition of sum5. sum5 = Continue $ go 0 where [extended definition of go] The point of this definition is the "go", it ought to come first in the prominent position. In fact I'd pull it out into a separate function entirely and then simply define sum5 here as `sum5 = Continue $ summator 0` immediately after the definition of summator. (Note I'm not complaining about "sum5", it makes sense in context.) Just because the language _lets_ you snarl the code into a wildly random-access-hodgepodge of definitions doesn't mean it's a good idea. It should be considered good style to impose some order for the sake of readability into your code, and I'd contend it's more functional to have lots of little useful pieces and more sensible to clearly build from the bottom to the top than to just have a snarl.
Turing complete is easy. The real trick is creating a useable language that *isn't* Turing complete.
.
Wow, you have low standards.
Nice! ((* -&gt; *) -&gt; * -&gt; *) -&gt; ((* -&gt; *) -&gt; *) -&gt; (* -&gt; *) -&gt; * is a pretty epic kind.
&gt; while Lua avoids overloading and supports automatic conversions Lua [supports overloading](http://lua-users.org/wiki/MetamethodsTutorial). I do agree with your sentiment, though.
&gt; JavaScript is actually a really nice language. No, it's not: * No block scope. * Hoisting. * No bound methods and flaky `this` semantics. * Byzantine implicit conversion rules. * Crappy function syntax. * Semicolon insertion. There's a nice little language hiding in there. Single-parent protos and first-class functions are a neat core to build a language on. But, in his unfortunate haste, Eich didn't get the chance to really polish the rough edges off of it.
I was bored so threw this logo together tonight http://james-sanders.com/d/haskellers.png If you like it I could clean it up a bit.
Yes, I was using slightly muddled language, probably a consequence of slightly muddled thinking. ;-) I used the vocabulary of language mechanisms, but I was really talking about what makes up an appropriate policy. One of the real strengths of Haskell is the policy; for example the typeclass mechanism allows for JavaScript-ish shenanigans if abused, but e.g. instances of Monoid are expected to follow the identity and associative properties.
lexical closures, to be precise. c can pass around functions via function pointers, but without lexical closures it's not nearly as useful.
&gt;&gt; Report how many private profiles are in the system in addition to those shown publicly on the homepage. &gt; Users who have not selected public, I assume you're referring to. Right. Or perhaps better, just report the total number of Haskellers registered. &gt;&gt; In the profile, indicate technologies known other than those specific to Haskell, both those for which one has programmed in Haskell and those for which one has not &gt; I would imagine this would go under the skills section, right? Well, it could. But all users so far have understood that by "skill" you meant specifically "Haskell skill." That indicates that there is room to expand the structure of this field a bit. &gt;&gt; Allow leaving the question about whether one is searching for employment blank &gt; You can already leave employment option blank. Yes, I thought about that too ;) Oh, right! That blank option didn't catch my eye. Perhaps write "&lt;leave blank&gt;" in the drop-down, then show it as blank when people view it? &gt;&gt; Profiles that might not be serious should not be displayed on the home page. &gt; I'm considering adding badges in the future for "real Haskeller" or things like that, and those people will get priority. OK. My main concern is keeping them off the front page. Perhaps only people who have earned a certain badge would be allowed to appear on the front page. 
When logging in today, I mis-typed my openID. The resulting error message was quite cryptic.
Absolutely. Unlambda and LazyK are clearly preferable.