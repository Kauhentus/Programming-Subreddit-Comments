This is a silly argument! If the new compiler pass takes a long time then it'll negatively impact compilation times. One can write slow code even with the best optimiser in the world.
I agree. It's totally within scope to define "distribution" data types and functions to manipulate or draw from them. My goal is to have a set of (community) packages that can be imported and extended with own modules by the users. All the data types and functions can then be used in the column formulas. Then there would be a probabilistic-models package that defines a bunch of distributions together with combinators, functions that draw samples, and a monadic interface to random number generators. The programmers would then work on these packages, and if necessary define own custom modules, and the analysts would just do the data modeling and use the functions that are available.
I have opened a ticket already. https://github.com/raaz-crypto/raaz/issues/300 
By the way tom-md do yo have any comment on the performance numbers. Do the look good/bad/ugly ?
Yes, CSV import/export is a must have and pretty high on the todo list :)
I've become a fan of default signatures. class Monad m =&gt; MonadMetrics m where getMetrics :: m Metrics default getMetrics :: (m ~ t m', MonadTrans t, MonadMetrics m') =&gt; m Metrics getMetrics = lift getMetrics Now the boilerplate associated with providing a variety of instances is just a single line per instance instance MonadMetrics m =&gt; MonadMetrics (StateT s m) instance MonadMetrics m =&gt; MonadMetrics (WriterT w m) ... But I can't say I *totally* disagree with just an overlappable instance instance {-# OVERLAPPABLE #-} (MonadMetrics m, MonadTrans t, Monad (t m)) =&gt; MonadMetrics (t m) where getMetrics = lift getMetrics But I don't really agree with it either
If OverlappableInstances are only problematic in presence of orphan instances then I don't really see what the problem is in general. We avoid orphans anyways
Nix does not strip dependency bounds unless you explicitly jailbreak the package. Nix is like Stackage in that it picks a default version for every package and if that default version falls outside your specified version range then your build fails. Then you either have to expand your version bounds or override the default version that Nix selects.
Hello, u/dmwit! I taught myself Haskell last year, and I'm a senior, dual majoring comp sci and math, looking for job after graduating this semester. Would you describe the Software Engineering position to be entry level? I noticed you mentioned you were a crypto n00b, as I am, which is why I ask!
I've been doing the latter recently. Generally, my MTL classes have a single `lift` operation that lifts some algebra, often a `Free` monad over some syntax functor. With this, my type class just becomes a single homomorphism (a `lift` operation), and that composes naturally with `lift`. Concrete interpreters then overlap with this instance (as they themselves are monad transformers). As to why - I simply don't see any other way that `mtl` can compose without orphans. All it takes is two different packages. Package `a` exports `MonadA` and `AT`. Package `b` exports `MonadB` and `BT`. Neither of these packages have anything to do with each other, but - without this overlapping instance - they don't compose. `AT` is not a `MonadB`, and `BT` is not a `MonadA`. There is no way to use the effects together without either introducing an orphan instance, or writing a new monad transformer. However, the overlapping instance just says "if you give me an honest monad transformer, then I can give you a law-abiding away to lift my effect syntax".
&gt; but now I have `UndecidableInstances` and a multiparam type class, and I'm not sure how well this actually plays out in the wild. I wouldn't worry about the former, but that multiparemeter type class is suspect without a functional dependency. It should probably be class (MonadReader r m, MonadIO m, HasMetrics r) =&gt; MonadMetrics r m | m -&gt; r if you decide to go down that path, mirroring `MonadReader`.
Yup that's what I've been doing as well. I don't think it's a blocker, but I want to do it right, ya know? The thing is I used `ByteString.zipWith` which returns `[Word8]` instead of `ByteString`...
This is really what connection pools are for, which `postgresql-simple` provides support for. Asking whether or not a particular connection is being used feels an awful lot like an [XY problem](http://mywiki.wooledge.org/XyProblem) to me. What are you trying to accomplish?
It does! I'm just trying to understand which type is a better representation of the underlying bits. I don't really understand the relationship between `ByteString` and `Word8`.
I feel kind of dumb, but I don't understand what this means :-/
Seems to be that this means it should take the same approach `Monoid Int` takes. Newtype instances for each bit order!
Actually Styx is a very small shell around nix. So think of it as a "vote" in favour of nix.
Once you master nix, Styx is a very small step.
Honestly, I don't mind being that guy. What's ironic is that not providing an overlappable instace actually forces the users of your library to define orphan instances in case their transformer stack includes a transformer not compatible with your class. Which is probably all of them except for the ones in `transformers`.
In addition in your styx.yaml you can easily: - fix the version of nix that you'll be using (so no surprise breakage) - replace any given package by a version of your choice
Thanks for the reference! I'll investigate that as well. We're investigating alternatives to using the current server, which is a PHP/Lumen Passport app. Alternatives include upgrading the app to Laravel, using an off-the-shelf Node/PHP/Ruby/Java/etc. server. We're not particularly looking to roll our own, as we've had some bad experiences with that in the past, and no one on our current team has the requisite understanding of OAuth to do it right.
`Word8` is an unsigned 8-bit integer (an `unsigned char` in POSIX C). That is also an unsigned "byte". A "Byte" string is a packed array of bytes (or in the case of a lazy bytestring, a lazy list of packed arrays representing the concatenation). So each item in a ByteString is a Word8.
Which endianness? But far more importantly, there's just too much stuff in `Bits` for `ByteString` to be a legal instance. You wind up with annoying edge cases that don't pass laws. What do you do when you go to set a bit beyond the end of the bytestring? extend it? Do we renormalize to collapse if we shrink the result? Leave it alone? The zero bytestring would have to be empty. Complementing it would not result in an infinite bytestring of 1s, because we can't do that... If you want to make a newtype around fixed length bytestrings then you can easily implement `Bits`. It'll just be slow.
Another option is to not provide any instances for outside libraries. Most Haskell applications will wrap their monad transformer stack in a newtype and they can provide a non-orphan instance of your class for their newtype.
The best example is Data.Map.Map where the consistency of the Ord instance is a prerequisite for the correctness of the data structure. If you you build up some map using one Ord instance, then try and query/update it with a different Ord instance, you can get incorrect results. HashMap is similar. If instances are first-class you can work around it. You have to carry an instance with the map explicitly though, and it feels a bit non-Haskelly. Other languages have chosen this approach (or one very similar), though.
If you take a look in https://hackage.haskell.org/package/postgresql-simple-0.5.2.1/docs/Database-PostgreSQL-Simple-Internal.html#t:Connection, there is an MVar which gets locked down whenever a query is being run. If you *really* wanted to know if a query was being run you could check with https://hackage.haskell.org/package/base-4.9.1.0/docs/Control-Concurrent-MVar.html#v:isEmptyMVar. I'm not sure that's the best idea in general since it opens you up to the possibility of race conditions, but it may be appropriate to your use case.
It's also referred to as endianness. Big endian means the most significant digit is at the end, little endian means the least significant digit. With big endian, 10100000 means 3, with little endian it means 160.
Cool! I guess that means I was wrong in my original post about `Word8` being "higher-level". If it's a representation of a byte, then why is `Word8` associated with an encoding? From the [docs](https://hackage.haskell.org/package/word8-0.1.2/docs/Data-Word8.html) &gt; All function [sic] assumes that Word8 is encoded in Latin-1 (ISO-8859-1).
Interesting - I'll take a look. Thanks!
I'm not convinced this is meaningfully better than just using nix by itself. I just added `haskellPackages.callCabal2nix` to nixpkgs which means anyone can easily pull in GitHub packages and hackage packages that aren't in nixpkgs. I can also switch compilers with ease. It's trivial to pin down nixpkgs within nix too. I haven't yet found a stack feature that I miss....except for Windows support.
this is less framgentation and more an abstraction over nix and cabal. 
ticket here: https://github.com/NixOS/nixpkgs/pull/22191 Can you give an example call?
Sure, here's a random package set: Notice that I'm defining the GHC version myself, pulling from Hackage, and from GitHub. This is only slightly more verbose than styx/stack yaml file but much more flexible. haskellPackages = pkgs.haskell.packages.ghc802.override { overrides = self: super: { turtle = self.callHackage "turtle" "1.3.1" {}; rawr = self.callCabal2nix (pkgs.fetchFromGitHub { owner = "PkmX"; repo = "rawr"; rev = "669a66819b92a7066f91f744f3a8372e614d6189"; sha256 = "1gdgndmfpv2iszx0wx091m7ffrxkwy2xmm3cb8185ivyqj8p8zyq"; }) {}; }; }; **Edit:** And just for good measure, I can even pinpoint a version of nixpkgs like this: pkgs = import ((import &lt;nixpkgs&gt; {}).fetchzip { url = "https://d3g5gsiof5omrk.cloudfront.net/nixpkgs/nixpkgs-17.03pre99971.05ee547/nixexprs.tar.xz"; sha256 = "0snpqlrp1s6bqq7lgv7y0zajd9mag28sb7irk853vgz9iml3y0zi"; }); 
Very clear explanation, thank you!
`ByteString` is equivalent to `[Word8]`
Do you have a code example of this approach?
I've tried it, it works; it's still a prototype but the dev team is motivated and they will extend it re. features and usability in the upcoming weeks. 
That would be great if you can support ADT. I used to work on a BI project in which the customers asked for operations on numbers with two zeros! One of which was specified with `-9` in the excel sheets! Custom Read/Show classes is also required then.
This only works at the very top-level. I introduce and eliminate effects *inside* my application all the time. For example, I might wrap a small chunk of code in `ExceptT` to add an error case to a small block of code. But now I can't use my logging library, unless that also happens to have an instance for `ExceptT`. The only way this possibly works well is if your application `newtype` handles *all* effects you might ever need, and then you can stack them multiple times to locally scope effects. But this is too powerful, and generally a mess :(
Styx definitely has a simpler syntax because it's designed to do exactly one thing: manage Haskell packages. My point is that if you know Nix decently well then Styx really offers very little over what you get out of the box with Nix.
I recommend Atom if you are not comfortable with emacs/vi. Visual Studio Code seems also to have a lot of traction lately. Plugins for Haskell for both VS Code and Atom are pretty easy to set up. Here is an example for Atom https://atom.io/packages/ide-haskell
I agree that this pattern discourages restricting effects to the bare minimum you need, unless you're willing to create a new `newtype` for each set of effects you are interested in
Thank you so much for the detailed explanation! It sounds like depending on the defaulting rules is a bad idea for this case. What would I need to do to not need it?
Here's a snippet of the posting that I think is relevant to your question: &gt; * Education – Minimum of a Bachelor’s degree in computer science or equivalent. MS or PhD in CS or a related field desirable but optional, depending on specific role. &gt; * Required Technical Expertise – Must have hands-on experience developing software and/or performing computer science research. Demonstrated expertise in aspects of software development mentioned above. I know you specifically asked whether it's an entry level position, but I don't think labeling it "entry level" or "not entry level" is going to make it any more precise than the above description.
I recommend the [brick package](http://hackage.haskell.org/package/brick), which I find quite simple to use. You could use a Brick.Widgets.List.List for displaying a list of titles and you don't even need to implement the up-down scrolling logic yourself. The text of the selected item could be displayed in another widget next to it. You could implement left-right page-switching by explicitly handling the left and right arrow keys with updates to the list. In brick, you explicitly provide your application state by specifying the initial state and how the state changes when handling events. The List would be part of the application state.
Programming against type class constraints instead of concrete stacks is imo the way forward. A function: interpret :: (forall m. AllMyAppEffects m =&gt; m a) -&gt; IO a where `AllMyAppEffects` is some constraint kind synonym that packages up all the things your app can do allows you to run your stuff at the top level. Then your smaller functions operate in `JustOneEffect m =&gt; m a` etc.
&gt; I recommend the brick package, which I find quite simple to use. Thank you, from the first glance, this looks exactly like what I need -they also provide a collection of demo programms. &gt; You could use a List for displaying a list of titles and you don't even need to implement the up-down scrolling logic yourself. The text of the selected item could be displayed in another widget next to it. You could implement left-right page-switching by explicitly handling the left and right arrow keys with updates to the list. Thank you, this should cover all my use cases. This looks like it contains all the state I need - enough information to get some feeds from a local database or get a list of options: data List n e = List { listElements :: !(V.Vector e) , listSelected :: !(Maybe Int) , listName :: n , listItemHeight :: Int } deriving (Functor, Foldable, Traversable, Show) 
I have wrapped styx as the first example of my new project tool generalizing project 'np': https://www.reddit.com/r/NixOS/comments/5rxlnm/new_tool_np_abstracts_over_project_tools/ np is capable of calling any build tool, as specified by the project configuration. Edit: explain
This makes it _better_ (even though fundeps again kill it), but you still can't handle some of the effects in the middle of your app. If there's some deep function that's throwing some kind of an exception, whoops, now your main function needs to know about it. (this is assuming that there isn't a fundep forcing you to only have a single type of exception anyways, hah)
Yuppp. Though I tend to be in the `MonadThrow` camp wrt exceptions.
Get. That. Polished. *You may have something...*
Thanks, I meant more the lifted Free approach.
Just restating this excellent answer in a way that may be easy to remember: `mypackage.cabal` controls how your project is built. *If you weren't using `stack` you would still need this file.* `stack.yaml` controls the environment for `stack` itself.
The words you're looking for are "extensible effects." =P There is no solution in the land of monad transformers that satisfies all of the usability requirements. But extensible effects do. They just either cost you performance in Haskell, or require magic syntax in PureScript. There is probably a nice way to do extensible effects in a dependently typed languages, but you'll still have performance issues without PureScript's approach.
`zipWith` loses any extra values. Extras should be treated as 0s, and `xor x 0 ==x`.
&gt; [Intero] has type info for locally scoped variables Somehow my ghc-mod setup on Atom has got those too?
It does? I never had that working. I'm probably mistaken though =P
May be I need to clarify: 1. Rate is Gbps (Giga = 1000^3 not 1024^3) bits (not bytes) per sec. I used this unit because most of the time network providers use this as the internet speed so it is easy to compare. Some figures out there are in the wild are GigaBytes/Mega Bytes (not clear whether it is 1000^3 or 1024^3). See for example https://calomel.org/aesni_ssl_performance.html 2. The performance is for raw buffer operations (size matching the l1 chache and aligned appropriately). When we look at performance for some thing bytestring there is going to be some overheads due to copying and say smaller buffer sizes. What I found is that alignment is a really big deal when it comes to performance. I was aware of this but thanks to hvr who early on drove this point (See https://github.com/raaz-crypto/raaz/issues/256) For example, if the buffer is not aligned to 32 bytes for Chacha20-vector256 the performance is significantly bad worse than the portable implementation. That is the reason in this release I have payed a lot of attention to alignment. See: https://github.com/raaz-crypto/raaz/commit/d01af1 
Ah, good to know, thanks!
It supports higher order functions. 
You can try to use Intellij plugin: https://github.com/rikvdkleij/intellij-haskell
Doesn't matter for purity
Does postgresql-simple have connection pools? I checked a while ago and didn't see anything, so I followed this blog http://codeundreamedof.blogspot.com/2015/01/a-connection-pool-for-postgresql-in.html and used the resource-pool package to create one
Thanks for your input! In addition to the input working "just like Excel", I also think it would be great to get forms that are easy to work with for free (imagine a form for adding a new row). Given that we have the built-in language its also within reach to make validation easily configurable (just write a function `Row -&gt; Bool` or `Cell -&gt; Bool`). Regarding the interface to the rest of the system, I totally agree. The backend can fetch or subscribe to the data, which comes well-defined according to the column types. If needed one can of course first transform the collected data inside Herculus until it suits the needs of the backend.
If you know ASM you can try to read one commit of me. Following the conversation here: https://github.com/mikeizbicki/HLearn/issues/83 i hacked together https://github.com/mikeizbicki/subhask/pull/57 with the explanatory commit https://github.com/Drezil/subhask/commit/619baeeee974b74f59a4fa342e6714b4571aa67a (above &amp; below the diff) It is a basic implementation of the paper "recycle your array" linked in the source.
You have to install both hlint and ghc-mod. You can do that with stack in the project your are currently working on. `stack install hlint &amp;&amp; stack install ghc-mod` 
Posting here because I suspect a lot of r/haskell folks will be interested in the applicability of a long awaited and long anticipated language extension. More context further upthread: https://github.com/ghc-proposals/ghc-proposals/pull/6#issuecomment-276150270 If we have to choose (and in IMHO no choice is strictly worse than either choice), should OverloadedRecordsField have first class support for plain record selectors (i.e. functions `T -&gt; a` for some record `T`), or for lenses?
That's true. The PR is titled that way though.
Leksah repo uses different resolver: https://github.com/leksah/leksah/blob/master/stack.yaml Probably you should try to build with specified in repo resolver. I don't know why leksah developers don't use stable LTS for project and why nightly is outdated.
When you wrote the blog post on that style of type class, I was at first pretty interested, but I've since come to realize that the "free laws" I get from this approach are just those of MonadFree in disguise, which actually makes the laws seem pretty constraining and uninteresting to me. You must feel differently, so I'm wondering if you care to comment on the kinds of properties you end up with from your greater-than-my experience with this approach and whether you have actually found them useful or mainly just kind of "comfortable."
Can I PM you or email you questions about the role, even if I'm not currently on the market yet?
I am so strongly in favor of generated lenses that I'm flipping out right now
[removed]
Check out fpcompletes stack tool, spacemacs, and the intero emacs package. The build tools for Haskell aren't bad, largely because of fpcomplete, but it isn't documented as well as it could be. Have patience, putting this stuff together takes a small bit of time. 
If you are on VS Code check out Haskelly! (I'm one of the developers)
Sounds like a slightly different use case, and it seems these are both businesses who are presumably in competition 
For packages, cabal is closest to pip and stack is closest to conda (which if you haven't used for python yet you should). I'd recommend using stack if you're just getting started as it's way more likely to "just work". For ide's, I use spacemacs with haskell-mode and love it. I hear good things about atom's haskell integration too, which will be closest to pycharm. Pycharm does a lot more for you than any haskell ide does; part of that is that haskell's tooling support is less developed, part of that is that you just need less. For unit tests/TDD/interactive debugger -- this is the dominant workflow I've witnessed. It's also possible (which I never could do in python without hating the exp) to code for days without touching the repl; instead you lay out a sketch of what the types looks like, make a ton of little functions that build out the features, iterate on the types to get them to support what you need, and once it compiles/typechecks usually you're done + bug free. I've heard this called type driven development, which I guess fits. Once you get more used to haskell this becomes an attractive development approach. For python virtual envs -- refer to conda for an alternative approach, which is pretty close to what stack does in haskell. You can also use cabal's sandbox feature, which is much closer to python venvs, but I'd still recommend the approach that stack and conda take over that of cabal and pip. NB: you're going to write way, way fewer tests in haskell than you did in python. They just aren't as useful/necessary -- you can shove things into the typesystem as either's or maybe's that you'd usually have to throw exceptions for. This is the ideal approach as well. At first (i.e. for the first year or two) using exceptions for control flow should be looked on with large amounts of suspicion (unlike in python where `try... except` is normal). With exceptions banned, you end up encoding them into the type system. With them in the type system + using small functions, you end up using equational reasoning (i.e. does this function make sense) to guard against bugs. This cuts down the surface area where unit tests are useful vastly, effectively to the value-level (i.e. does `2 + 2 = 4`) and even then equational reasoning trims that area down extensively. 
But effects in them do not. `bracket`, for example, or `listen`
Which is exactly my point. This stuff doesn't compose nicely. It's a fundamental problem to the monadic approach.
I'm not convinced that other effect systems solve the above problem I described either
That's true
I like this idea of lenses. What would the error messages look like?
No, it at least very much *did*. I specifically said "I must have a version of directory newer then ..." and it selected a version 2 versions older which specifically had the implementation error I was avoiding. There was a bug filed, and it was a WILLNOTFIX. They were unwilling to have multiple copies of packages even when the currently available packages were broken and they did not fail to build in that cases. It was extremely explicitly done in the Haskell modules configuration file.
I'll look for it, this was over a year ago though. The core problem seemed to be they believed because 'directory' was distributed with GHC it would satisfy for any package that would build with that GHC and there was no reason to package directory from Hackage. This was very specifically an issue with how they viewed GHC distributed packages but sadly GHC distributed packages have issues also.
Oh that may be a special case. Regardless, I don't think it's fair to say "Nix doesn't respect version bounds," when that's only true for corner cases. Anyway, I may experiment and see what's up with that...
Navigate to the package page on Hackage, e.g. https://hackage.haskell.org/package/pipes. From there, you can click into the module documentation under "Modules". Each heading in the module should have a corresponding "Source" link on the right of the page that takes you to the HTML-formatted source. Alternatively, from the main package page there should be a "Cabal source package" link under "Downloads". Click the [browse] link to see the full package structure and browse the raw source. 
&gt; but it isn't documented as well as it could be. If you have any suggestions on how to improve stack's documentation, could you please [open an issue](https://github.com/commercialhaskell/stack/issues/new)? For developers it's often hard to notice problems in your own product's documentation because you've never approached it from the outside.
Alphasheets is more like a traditional spreadsheet, except that you can use Pyhon or R instead of VB script. Herculus is more about handling relational data, but only has a spreadsheet-ish UI. I wrote some more about the differences here: https://www.reddit.com/r/haskell/comments/56ailh/herculus_purely_functional_spreadsheet_tools/d8i028l/
What do you mean by "numbers with two zeros"?
I give my reddit-based vote to lenses too, as far as that counts. 
That all sounds great. One thing I would add is that vim + Haskell plugins is generally fantastic. IMO modal editing is superior for speed and low hand strain. 
... and Spacemacs is a Vim emulation layer of sorts for Emacs (besides many other things). ;) I've long been hesitant to switch from Vim, but Agda forced me to, and it's more pleasant than I had imagined.
Vim and Emacs are fine and all, but if you want a modern editor with modern concepts of how editing files works, VSCode, Atom, even simply Notepad++ with a plugin or two are all just fine for editing Haskell code. Don't let anyone tell you that you need to learn Vim or Emacs, any editor that can just match the previous line's indentation when you press enter and use spaces instead of tabs is probably smart enough to edit Haskell with. (Which basically means that any editor that you'd be able to use Python with, you can probably write Haskell with.) 
Why not? Most features can be removed from GHC and then embedded in Haskell (like recursive bindings via fix). But, imo, language support is worth the huge gains in convenience and debugging. 
iiuc, when they share both the same field name and base type, an overlapping instance. 
In that case would something like `AsciiChar` or `Char8` or something make sense? Because `Word8` to me does sound like an encoding-less number.
I'm disappointed that the article doesn't even _mention_ shrinking, which isn't particularly complicated, and which lends a lot to coming up with _understandable_ counterexamples.
Ah, the holy war. I used tmux + vim + haskell plugins for a while, and they were nice. Spacemacs is like tmux + vim + emacsOS for running emacs tools... I like it, but still use vim for text editing outside of it. However, for a self-proclaimed newb who uses pycharm for dev... would you really recommend vim/emacs/any advanced editor to learn concurrently with haskell? I'm sorry, but that's a recipe for learning neither. Haskell is more than enough of a mindbending level-up on its own. If you're used to pycharm, with no emacs/vim plugins/keymaps, then focus on learning haskell and just use atom. 
&gt; You only need one of them Worth noting that the `stack solver` command requires that `cabal-install` is present. Stack is a higher-level tool but didn't set out to reinvent every wheel, so it sometimes falls back on Cabal.
Honestly I think it isn't the worst idea. I remember doing it when I was learning how to properly use C, I didn't let myself use any other than editor for C, but still used other editors for other languages for my existing projects so that I didn't slow down on them.
I'm pretty sure that won't work but I'll try. I'm thinking it'll return a monad not an Integer. 
It's running in the IO monad, that's why you can call `readFile`. The `x &lt;- (expression :: IO Int)` binds `x` to an `Int` though. You can think of `&lt;-` as stripping off the `IO`. Just define `readFileAndTitleCount = titleCount &lt;$&gt; readFile "nrs.html"`
Sure!
&gt; Is there a better way to do that which does not require `Data`? Sure, use `Typeable` and heterogeneous equality: {-# LANGUAGE ExistentialQuantification #-} import Data.Typeable data F = forall a. (Typeable a, Eq a) =&gt; Foo a -- | -- &gt;&gt;&gt; print $ Foo "foo" == Foo "foo" -- True -- &gt;&gt;&gt; print $ Foo "foo" == Foo "bar" -- False -- &gt;&gt;&gt; print $ Foo "foo" == Foo () -- False instance Eq F where Foo x == Foo y = case cast y of Nothing -&gt; False Just x' -&gt; x == x' 
&gt; There's also the r/reflex sub, which we all should start contributing to. You mean r/reflexfrp.
&gt; Just define `readFileAndTitleCount = titleCount &lt;$&gt; readFile "nrs.html"` The type of this expression, as you pointed out yourself, is `IO Int`. It's still explicitly in `IO`, and that explicitness is the thing being "enforced." Meanwhile, `titleCount` can still be pure because we have the `fmap` abstraction. You seem to be implying, in your first post, that the separation of pure and impure code is somehow tied to `do` notation, but that's just a convenience. It's only the types of individual (sub-)expressions that matter here. For instance, the entire original test could be rewritten as: it "finds the correct number of titles" $ flip shouldBe 59 =&lt;&lt; titleCount &lt;$&gt; readFile "nrs.html"
http://spacemacs.org
omg, I am currently working on a project using Servant and https://github.com/dhess/hpio to automate some physical tasks in my house. Definitely gonna check this out!
Idiom brackets could be added with QuasiQuotes, couldn't they? [appl| readInt + readInt |] Same with what he refers to as 'bang things', no? [bang| putStrLn (sum !(readInts !readNat)) |] &amp;nbsp; Edit: thinking further about applicative: I don't think I've ever had much issue with writing/reading: (+) &lt;$&gt; Just 3 &lt;*&gt; Just 2 Although I do agree infix might be nice sometimes, e.g.: Just 3 `liftA2 (+)` Just 2 Which we can still approach with `let`: let (|+|) = liftA2 (+) in Just 3 |+| Just 2 I do wonder, however if we could maybe get closer to ``Just 3 `liftA2 (+)` Just 2`` in a more general way by Just 3 [op| liftA2 (+) |] Just 2 If anyone knows whether this is possible, please let me know.
You can compile your program with profiling enabled and automatic cost centers for better traces: `-prof -fprof-auto` and run it with `+RTS -xc`, then you will get the sequence of calls that leads to your error. I learned this from the HaskellWiki on [Debugging](https://wiki.haskell.org/Debugging).
 Warning: 'ghc-options: -prof' is not necessary and will lead to problems when used on a library. Use the configure flag --enable-library-profiling and/or --enable-profiling. (-)-enable-profiling was not a recognized flag either. +RTS -xc complained that -prof was not enabled even though it's there. How is the cabal file supposed to look?
You _compile it_ with prof, you _run it_ with +RTS. GHC adds magical command-line arguments to your program
By searching for "[idiom brackets haskell](https://www.google.com/search?q=Idiom+brackets+haskell&amp;oq=Idiom+brackets+haskell&amp;aqs=chrome..69i57j69i60l3&amp;sourceid=chrome&amp;ie=UTF-8)" this is what came up for me: https://hackage.haskell.org/package/applicative-quoters-0.1.0.8/docs/Control-Applicative-QQ-Idiom.html I'm not sure what to search for the second one, anyone knows the proper name for the syntax? 
The parameter you invoke stack with is `--profile`, for example: stack build --profile (In case you're using stack)
There is a series on YouTube called "code deconstructed". It features xmonad, yampa and pandoc. 
By using those snapshots, you are using the collective vetting that this set of version referred to by "LTS 7.18". That is, nice people have collectively cleared issues behind your back _before_ you even started your project. If you allow some additional, non-vetted, packages via `extra-deps` in `stack.yaml` it's not guaranteed that it will work out, but you still have 99% chances that it will, and if it does not at least you know where it comes from
It works! I don't need to worry about +RTS -xc when using --profile? It only worked when building an exe and then running the exe. Is there any way of getting it to work with the REPL?
So ghc-options in the cabal file specifies arguments passed to the compiler, and -prof should be specified there, but it didn't work because either I ran it through stack? Would it be possible to run the REPL with +RTS?
Could one avoid the incoherence induced by orphan instances by disallowing orphan instances for the class using a hypothetical language extension? So you could have something like this: class {-# NOORPHANS #-} Monad m =&gt; MonadMetrics m instance {-# OVERLAPPABLE #-} (MonadMetrics m, MonadTrans t, Monad (t m)) =&gt; MonadMetrics (t m)
There's something to that. Atom's OK, and it will keep the focus. But vim and/or emacs are really worth learning too, for far more than just Haskell. So it's really a matter of knowing your own best way of learning things.
I'm thinking this is something that shouldn't be done. E.g., &gt; Haskell makes your functions pure by default. You have to change the type signature if you want to do IO, and mark every function between your function and main() as tainted with IO. This forces you to be conscious of IO. It encourages you to keep functions that do IO as high up the stack, close to main, as possible. http://dubhrosa.blogspot.co.uk/2012/12/lessons-learning-haskell.html?m=1
Great question! I actually don't know! Can you not run the binary from the stack build folder?
The binary did work and it pointed out the line number causing the exception. Having the same stack trace in the REPL would be handy because it allows for a faster and simpler workflow, using :reload and all.
You can put this code in your test suite, and then run stack test --trace . And it will run the test suite with profiling enabled and pass `+RTS -cx` to the test suite executable, and that will print out the stack trace when you get an exception.
Is there any advantage to doing that compared to compiling with --profile then executing? 
VS Code with intero or ghc-mod plugins. Faster than atom and more accessible than vim or emacs.
I never said the current api was a _good_ idea. ;)
QQ isn't capable of adding syntactic sugar since it is, in itself, syntactically bitter.
Overwhelming support, thank you all. I got what I needed. It's worth mentioning that there's a bug when running stack trace options by setting them live in the REPL (https://s29.postimg.org/ekrn30n5x/repl1.png) and in order to pass the options through Stack they need to be quoted: stack ghci --ghci-options "-fexternal-interpreter -prof"
Was watching some CppCon talks and my ears perked up when I heard that Battle.Net does property based testing (Starts at 23:15) - It seems that Haskell concepts (namely property based testing in this case) are making their way into other projects written in other languages. While I know this is not a new thing, it did get me wondering - how many of you have been using ideas you gained from Haskell and functional programming in your work outside of Haskell? What ideas/concepts ported well and what didnt? Did you get any resistance from your colleagues in doing this?
It's very important to find a way to support both. If it's not possible out of the box - which seems to be the case from the discussion so far in the linked github issue - then there needs to be a trivially easy way to turn one or the other style on or off using a pragma or a simple import. EDIT: Clarified wording.
This is probably due to the new version of `error` which prints the stack trace anyway, as suggested by its type: error :: String -&gt; a -- base &lt; 4.9 error :: HasCallStack =&gt; String -&gt; a -- base == 4.9 However the "call stack" is not updated by default, you can either annotate definitions that use error (directly or indirectly) with a `HasCallStack` constraint (which will be specially handled by GHC), or compile with profiling. This makes `+RTS -xc` redundant for `error` (you will see the trace twice), but it is still useful for other kinds of exceptions (e.g., non-exhaustive patterns, missing class methods, deferred type errors, IO errors).
Ah, you are right. It seems it always prints stack traces when compiled with `-prof`. With `+RTS -xc` it just prints a different stack trace. $ stack clean &amp;&amp; stack build 2&gt;/dev/null $ stack exec -- demo-exe demo-exe: &lt;error message&gt; CallStack (from HasCallStack): error, called at src/Lib.hs:6:12 in demo-0.1.0.0-7OSynWc3gNX420JGpTrCJd:Lib $ stack clean &amp;&amp; stack build --profile 2&gt;/dev/null $ stack exec -- demo-exe demo-exe: &lt;error message&gt; CallStack (from HasCallStack): error, called at src/Lib.hs:6:12 in demo-0.1.0.0-7OSynWc3gNX420JGpTrCJd:Lib CallStack (from -prof): Lib.someFunc (src/Lib.hs:6:1-23) Lib.CAF:someFunc (src/Lib.hs:6:1-8) $ stack exec -- demo-exe +RTS -xc *** Exception (reporting due to +RTS -xc): (THUNK_2_0), stack trace: Lib.someFunc, called from Lib.CAF:someFunc --&gt; evaluated by: Main.CAF:main demo-exe: &lt;error message&gt; CallStack (from HasCallStack): error, called at src/Lib.hs:6:12 in demo-0.1.0.0-7OSynWc3gNX420JGpTrCJd:Lib CallStack (from -prof): Lib.someFunc (src/Lib.hs:6:1-23) Lib.CAF:someFunc (src/Lib.hs:6:1-8) edit: The difference is, that the `HasCallStack` stack traces don't show for all errors. For example this `1/0 :: Rational` will only give you a stack trace with `+RTS -xc`. As far as I see you only get `CallStack` stack traces with functions that explicitly support them like `error` or `undefined`.
Okay, but here the idea is not to remove memory constraints so all memory is used, but to set it in a smart way around cache memory limits as a good default behavior.
I have written a small ray-tracer. I have done some experiments with different values and it was about 30% faster because of setting first generation GC sizes to L3 cache level size of my machine.
Not exactly an influence, but having a previous intuition for monads helped me to understand [Javascript promises](http://bluebirdjs.com/docs/coming-from-other-languages.html#haskell) quicker that it would have been the case otherwise. (Among other things, [Promise.all](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Promise/all) is basically `sequence`.)
What do you mean?
total does not mean pure. It means defined for every value. f :: Int -&gt; Int f x = f x Is not total (it is not defined for *any* values), but the Haskell compiler can not prevent you from writing this type of programs. It type checks just fine. If it could Haskell would not be Turing complete.
Sometimes if you add enough sugar to something bitter it can become sweet though. If maybe somewhat bittersweet.
Watch the video. They explain it in the first minute.
I watched the xmonad videos; it's a really instructive series, will watch the other videos. Thank you.
The problem here is that `catchError` doesn't belong in `MonadError` at all! `catchError` is an *implementation* of `MonadError`. The real type of `example` is simply `MonadState Bool m =&gt; m Bool`. The exception details are entirely internal.
I believe that `throwError` and `catchError` belong in the same class, because they satisfy very natural laws analogous to monad laws: throwError x `catchError` f = f x m `catchError` throwError = m (m `catchError` f) `catchError` g = m `catchError` (\x -&gt; f x `catchError` g)
AFAIU [`vcache`](https://hackage.haskell.org/package/vcache) is supposed to be able to do just that. It's very similar to `acid-state`, except `vcache` doesn't require everything to be kept in RAM. 
So, I followed the instructions and managed to build Leksah. Then I ran it and clicked ok on the first window to continue. Then I see all kinds of messages on the terminal and the application crashes.
&gt;Worth noting that the `stack solver` command requires that `cabal-install` is present. Worth noting, indeed. If `stack solver` fails for that reason, `stack install cabal-install` should solve it. (IIRC the relevant error message of the `stack solver` command suggests that.)
No. Fuzzing is the idea of throwing adversarial inputs at your executable and seeing if it breaks. It's usually more of a black-box test and focused on detecting undefined and exploitable behavior. Property testing is about defining invariants/specifications/properties of sections of your code (functions, interfaces, etc.) and checking them, in a mechanized, but not necessarily in a randomized fashion. A good experience report is [this](http://www.cs.tufts.edu/~nr/cs257/archive/john-hughes/quviq-testing.pdf) paper (I haven't watched the associated [video](https://www.youtube.com/watch?v=zi0rHwfiX1Q)).
oh, that's not mine :) EDIT: it belongs to /u/dhess
Classes already control syntax: Monad for do-notation, Num for numeric literals, Enum for ellipses, OverloadedStrings for string literals, and OverloadedList for lists. 
Here's a good example of using afl to fuzz (the c-bits of) haskell code: https://github.com/ndmitchell/hexml/issues/6
An example of a property-based test is: &gt;&gt;&gt; import Test.QuickCheck &gt;&gt;&gt; quickCheck (\xs -&gt; reverse (reverse xs) == xs) +++ OK, passed 100 tests. &gt;&gt;&gt; quickCheck (\x -&gt; x * 2 == x) *** Failed! Falsifiable (after 2 tests and 1 shrink): 1
While nice laws, I'm not convinced they are all that useful (are they anything more than free theorems given the parametricity of throwing an error anyway?) - non help us reason about the problematic code above.
Hi, I'm on a similar path as you two, it seems -- wrapping GPIO controllers with `servant`. That's why I wrote `hpio`. I currently have 2 projects which combine `hpio` with `servant`: https://github.com/dhess/mellon (for electric strikes) and https://github.com/dhess/pinpon (an Internet-enabled doorbell, basically). Mellon is fairly well-developed at this point, and I've been using it in production for about 7 months. Pinpon is still a bit rough but does work. I'm actively working on it at the moment to make it a bit more robust, and possibly to generalize it to function more as a GPIO notification hub, rather than just a single "doorbell." As for `hpio` itself, though it presents a generalized GPIO programming model, currently the only interpreter it provides is based on the Linux `sysfs` GPIO programming model, which is clumsy and a bit slow. Eventually I would like to support other GPIO systems, such as something based on http://leventerkok.github.io/hArduino/, or even other OSes like FreeBSD. I haven't had time to work on those as they're not a personal priority, but I would welcome submissions.
Great idea. I hope more in similar situations see this and post bounties in a similar fashion.
But you can't use your own class. Which can be annoying, as `Num` for example is very far from perfect. Also `Enum` could definitely be rethought and could actually be put into an `Ord` hierarchy based on order theory. Also you might want to use something different than just a plain monad, perhaps some sort of indexed monad or something more general like Applicative or Functor. And there is no 'OverloadedChars` which is a bit annoying sometimes, and `OverloadedTuples` could be nice, say if you didn't like titles instances and wanted to change them, or if you wanted a strict pair. The current situation is no where near perfect. It's better than most languages but it is not perfect.
`f x = f x` is semantically equivalent to `f x = undefined` https://wiki.haskell.org/Bottom
There was a time when `do` notation was sugar for a new and unfamiliar abstraction.
I wrote this article last xmas, maybe you can find it useful http://allocinit.io/haskell/haskell-on-raspberry-pi-3/
&gt; but Elm is certainly the easiest to start with and get something working. Btw. this is one of the biggest reasons why Golang became so popular among enterprises. It may be a good decision to keep Elm as lean as possible for now.
Thanks for letting me know, I'll do that.
But then you are still ultimately using `Bool`. Not `MyBool`. Like if you wanted different / additional class instances for your custom Bool then pattern synonyms don't help you. 
Just as a note: any project that involves complex build systems / dependencies, especially relating to Haskell, can probably benefit from being written in the Nix language and built through the Nix package manager (which can run on Mac OS).
Yeah it's not really a solution, just wanted to point out that the extension exists.
Thanks a lot for the link! Denotational design looks _very_ interesting! Do you know where I can learn more about how he implements his design?
Yes, I've attempted to use cabal-macosx and spoken to the maintainer. If I'm doing something wrong let me know but I think it's been broken by the latest OS and XCode, it also does not work well with stack.
For "all the built-ins", I also keep this tab open: https://downloads.haskell.org/ghc/latest/docs/html/libraries/ Same principles as above at work here.
You're going to get a lot of varying opinions on this. My personal rule of thumb is to make spines strict, and leaves lazy, unless I'm looking to make infinite data structures. Lists with this rule would look like this: data [a] = [] | a : ![a] Notice that the contents (leaves) are lazy, but the structure (spine) is strict. For simple record types, I advocate treating fields as spine, not leaves, so that you can't accidentally pass a record around with an undefined field. In practice, I'm far too lazy or forgetful to actually do this most of the time =P
IIRC, we didn't implement row-polymorphism record like the one in Purescript or trex in hugs because of the complexity. But now this new proposal is becoming quite alike anyway, so maybe we can borrow some design from them?
The real value you get from Haskell here is that you can actually mark functions as being pure or impure in the type system. If `titleCount` is pure, then you can write `titleCount &lt;$&gt; readFile "nrs.html` without making `titleCount` itself impure. The beauty here is that even when reading that expression, it's still obvious which part is pure and which part is not. (Note that `&lt;$&gt;` is just infix form of `fmap` which is from `Functor`.) This, by itself, is not damaging in the slightest. However, what *would* be unwise (likely) is to define a new function: titleCountFile :: String -&gt; IO Int titleCountFile file = titleCount &lt;$&gt; readFile file and use that everywhere instead of `titleCount`. This is where your concern becomes valid. `titleCountFile` does two things instead of one, forces the caller to use IO, and completely erases the helpful line between the pure logic and the IO. So the question is primarily: *where* should I draw the distinction between pure code and IO? The answer is *usually* that your top-level functions should do as few things as possible and make as few assumptions as possible. Thus you'd want to keep `titleCount` as the primary top-level API. However, when you go to use it, or write tests, then combining the pure code and the impure code is inevitable and harmless.
&gt; ... and also allows any generated code to skip evaluation checks on data. JFYI, that doesn't apply. Strictness in some argument or data component doesn't mean GHC can eliminate checks for evaluated-ness. It does mean it can eliminate some allocations. Consider `f :: Int -&gt; Int; f x = case x of ...` -- obviously, `f` is strict in its first argument. That means when you *generate code to call* `f`, you do not need to allocate a thunk. But when you *generate code for* `f`, you must still see if `x` is a thunk. That is because some call sites might, in fact, provide thunks (because perhaps the call to `f` is not direct). In a case like `map f [fibonacci 10, 1234]`, one application is a thunk and the other isn't. You can make it work in this simple case, but it is fragile because other optimizations will break the invariant that `f` does not have to check anything. e.g. you could expand `map f ... ==&gt; map (\x -&gt; case x of y -&gt; f y) ...` to ensure every argument is strict. But GHC will also want to optimize `case v of z -&gt; k z` *back* to `k[z/v]` (i.e. eliminate the spurious case, through beta substitution). This breaks the invariant again because the `case` no longer exists to ensure-evaluatedness. This obviously gets more complex for higher-order examples. You can see this example in the paper [Types are Calling Conventions](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/08/tacc-hs09.pdf). What you actually want is a stronger guarantee, that the argument to `f` is not only strict, but *unlifted*, i.e. it can possibly represent a heap object, but it cannot ever be bottom.
I interpret him as meaning, sure you could do that, but its better not to. 
I like how you broke it down and agree completely. 
Things that work for me: 1. Package Management: Stack. (it includes Cabal as the package manager, but go with Stack and think of it as the package manager) 2. Haskell IDE: Atom + https://atom.io/packages/ide-haskell 3. Unit test: I use HSpec (http://hspec.github.io/) 4. REPL: use Stack, and type `stack ghci` interactive debugger: Not sure about this. virtual env: Not sure, but I think Stack handled it for you. Disclaimer: I'm also a newbie Haskeller.
Related question: how do people handle this problem on production servers? Do you run one server with profiling enabled to get stacktraces? Go without? Errors like this are thankfully rare in Haskell but I just imagine that you'd eventually run into them.
With GHC 8.0.1 and better, just compile with the `-g` flag. No need for profiling or cost centers.
Reading Okasaki really drills this home 
You can safely use postgresql-simple connections from multiple threads; if the connection is busy, the query you send will simply get in line and wait until all of the queries ahead of it finish. There is a big caveat, in that this sort of concurrency safety rarely extends to transactions. If you issue two explicit transactions in two different threads using the same connection, then you can get interleavings of queries that make no sense. The best approach for this higher-level concurrency control I think is probably application dependent; for example, `snaplet-postgresql-simple` adds connection pooling with correct support for transactions to `postgresql-simple`, in a form that's convenient to use with the `snap` ecosystem.
I don't think any such good rule of thumb is justifiable. You just have to try to understand whether you want each field to be strict or not. Here are some patterns that I think are *common*, but don't take this as a prescription: * Polymorphic fields are often lazy. * Monomorphic fields are often strict. * When you are often looking at just the head of a recursive data structure, it will often be spine lazy. * When you are often traversing a recursive data structure to the leaves, it will often be spine strict.
Who?
Chris Okasaki, who wrote the seminal 'Purely Functional Data Structures'. * https://en.wikipedia.org/wiki/Chris_Okasaki * https://www.cs.cmu.edu/~rwh/theses/okasaki.pdf
&gt; It just makes the language heavier and more complex. It's just a GHC extension. 
I mean we already have a shit ton, and if it becomes commonly used in libraries it is sort of de-facto in Haskell. And eventually I assume many popular GHC extensions are expected to move into Haskell itself. Also I'd like an answer on the `fix` thing. Because I'm pretty sure you can't just throw away recursive bindings and I don't think fix alone will solve it. I remember something about `let x = f x in x` being the implementation that gets you the sharing you want. And `fix f = f (fix f)` is itself a recursive binding, unless you are exempting recursive functions from your removal proposal, which makes no sense as functions and other values are both first class objects that are treated pretty much the same in Haskell. So if anything removing recursive bindings only from values and not from functions makes the language more complex.
&gt; However, for a self-proclaimed newb who uses pycharm for dev... would you really recommend vim/emacs/any advanced editor to learn concurrently with haskell? I'm sorry, but that's a recipe for learning neither. Haskell is more than enough of a mindbending level-up on its own. I wouldn't necessarily say that. Haskell is only a mindbending level-up if you already are an advanced programmer in another language. Plus one of the things I like about vim (and emacs) is that it works in multiple domains - your time invested is not time invested in Haskell. 
I guess that's just wasn't a use case, so currently RTS doesn't expose an API for tweaking settings once it started. I think currently the only exposed RTS setting is [`setNumCapabilities`](http://hackage.haskell.org/package/base-4.9.1.0/docs/GHC-Conc.html#v:setNumCapabilities). I've been recently experimenting with a live profiler (a GUI that shows cost center tick changes live) and I had to provide some extra hooks in the GHC library. It's a lot of work because RTS wasn't design with this use case in mind.
Why would python and Haskell be in a head-to-head? They're both good in their own domains (and Haskell is better for parsers not least of which because of its speed). &gt;Anyone who’s done Haskell might be able to guess at the problem I was about to have: The very same test, written in Haskell, wouldn’t compile! You wrote code that wasn't correct. It wasn't "the same" code. 
Most of the evolution has been about reducing complexity, rather than increasing it. ;-) Regardless of the question about the `IsLabel x (r -&gt; a)` instance, this is still substantially simpler than a hypothetical row-polymorphism extension, at least as a change relative to where we are now. If we were designing a language from scratch, row polymorphism might be a good approach, but extending Haskell is a rather different matter.
First of all, thank you for pushing this forward so far! I followed the discussion on github closely, I can understand the situation is quite complex since everyone have their own preference, for example the poly-kinded `HasField` discussion(which i personal think quite meaningless since type level ADT is not extensible anyway). I wonder if we can avoid all these problem by just using a more formal row-polymorphism desgin as a frontend language construction. That is, no magic class, no overloaded label, just a clean opaque syntax which give us extensible record, it can be based on typeclass or anything you have to use to make it work, but we don't exposed these internal things to user at all, we hide all these abstractions so that user don't have to concern it at all.
I think that's a little questionable personally. Mostly because of the performance implications (both time and space, surprisingly enough space leeks can come from both too much and too little lazyness) of completely losing guarded recursion: I wrote two data structures, one of which like yours above, one of which was lazy instead: data ListL a = NilL | a :- ListL a data ListS a = NilS | a :# !(ListS a) And implemented `Functor` and `Foldable` the standard (and the same) way for both. Then I wrote the following code: listToNL :: Int -&gt; ListL Int listToNL n = go 0 where go x | x &lt; n = x :- go (x + 1) | otherwise = x :- NilL listToNS :: Int -&gt; ListS Int listToNS n = go 0 where go x | x &lt; n = x :# go (x + 1) | otherwise = x :# NilS main :: IO () main = print . sum . fmap (+ 1) . listToN(L|S) $ 5 * 10 ^ 7 Lazy main only reached a few hundred megs of memory usage, and took 3s. Strict main reached 7.5GB of memory usage, and took 20s. EDIT: main = do let a = fmap (+ 1) . listToN(L|S) $ 5 * 10 ^ 7 print $ sum a print $ product a Also has a similar but admittedly somewhat smaller divide. About 9s vs 21s. And about 2.5GB vs 7.5GB. So it's about strict fmap just being bad, not about streaming. 
&gt; Some of the haskell game engines do this in their initialization function. Why do they?
It's `3 - 2`. The whole reason for the subtract function is because you can't write `(- 2)` since it's ambiguous if that's an operator section of simply a negative literal.
As performance is usually the thing you consider once you've got the thing working, you can then look at fields on a case-by-case basis. In practice most people are too busy creating features than analyzing and benchmarking every single data type in their project, so a rule of thumb for a good default _is_ useful. Strict-by-default data structures is a good _rule of thumb_ for newbies for three reasons: 1. **Correctness**: Because it gives you a better static check: being a compile error for you _and your users_ to fill in strict fields, rather than leaving them as lazy exceptions as in the case of omitted lazy fields. 2. **Performance**: The effects of laziness are simply harder to predict, when evaluation and allocation actually occurs is by design "when needed", which depends completely on how it's used. On a complex program the answer is a distant vagueness. Strict fields set a boundary on that question: if my constructor is forced, then so are all its fields. 3. **Exceptions**: Exceptions/error conditions tend to escape their original context and bubble up their ugly head further up (and then back down) the call chain in strange ways. If a strict data structure throws an exception when one of its fields is an exception, that's, again, setting a good boundary. But I wouldn't go opening PR's to strictify anyone's data structures, unless you had some data and benchmarks to contribute, because that's just rude. 
`ghc-options` in the cabal file specifies compile-flags, right? Adding `-g` there does not work (on GHC 8.0.1 when built with Stack)
Ok, thanks for that. I'm not sure the differences but my system always returns: &gt; open dist/build/crux-exe.app LSOpenURLsWithRole() failed with error -10810 for the file /Software/crux/dist/build/crux-exe.app. ` Not sure why that is yet.
To run faster? You can't always trust the user to set the right RTSOpts, and I struggle to imagine a situation where the program must use a single thread regardless of the system it runs on. 
Definition: subtract = (\b -&gt; (\a -&gt; (a - b))) Substitution: ((subtract 2) 3) ===&gt; (((\b -&gt; (\a -&gt; (a - b))) 2) 3) First application (b/2): ((\a -&gt; (a - 2)) 3) Second application (a/3): (3 - 2) Primitive operation: 1
I don't think I have seen that error. What happens if you run: dist/build/crux-exe.app/Contents/MacOS/crux-exe Does it work if you double click on crux-exe.app in finder? 
TBH I don't even know what they are :D I just learned what lax monoidal functors are today. Hopefully it would be harder to understand that a blog post written by another one wouldn't subsume mine.
For `data X = X {x::!Int,y::!Bar}` the expression `X {x=1}` is a compile error.
Ah. I never use that syntax, but I would have expected a compiler warning, at least, regardless of strictness. Correction: I do use that syntax, but never to intentionally leave out fields. I'm pretty sure there is a warning, though not an error without -Werror, without strictness. I'm not looking it up right now because phone.
Then you need to quadruple the bounty. It seems that the reward isn't worth the effort or someone would bite. Also, it will help your company make money, so they can afford it. Or it will help get you a promotion/ make you look good, so you can afford it. Edit: Software bounties are actually a bit antithetical to the open source movement, though. A better strategy would be for you to present your failed attempt at getting this to work and then let people pick it apart. Then you'd get your answer for free *and* you would learn how to implement it, which makes the community stronger because at least one more dev (you) is now more capable. Some quick google searches suggest that: 1. "Bounty programs where ***users who*** want a particular bug fix or feature, ***offer bounties*** to anyone (which may then be taken up by the original developers, by third parties, or by no-one), ***have not been as successful***. Many programs have been created over the years to try to address this perceived need - but few have succeeded. " [[Link]](https://www.quora.com/What-are-some-bounty-programs-for-software-development) 2. [The Problem With Bounty](http://www.dit.upm.es/~jantonio/documentos/articulos/bounty.html) includes factors such as encouraging people to steal code for personal gain, which is antithetical to the entire open source ethos, the potential for artificially driving open source projects in a direction that the market does not need, disincentivizing collaboration because of having to split profits, etc. It is easy to see how such behaviors are threats to the entire open source model. 
You don't use record construction syntax?
&gt; and usually indicates that what you really want is a streaming abstraction, not laziness. Could one not say that laziness itself is a kind of streaming abstraction? I.e. it's perfectly acceptable to use it unless you require a more elaborate streaming abstraction? I wouldn't want to have to use Pipes/Conduit/Machines to do the above.
I would get into the habit of using `Maybe` for that type of thing, especially if the library is intended to be distributed and used by other people. I would be taken aback if a library I was using threw an error due to an undefined value!
If laziness is a streaming abstraction, it's a fairly bad one. You probably shouldn't be pretending to store stuff in a list if you aren't prepared to allocate that list.
&gt; One of those values is guaranteed to never be accessed unless it has a value, and its starting value is undefined. There are arguments for whether such things should be undefined or `Maybe`; I think if you're going to leave a value undefined, then an expression like `error "Field foo accessed before it has a value"` would be better than a bare `undefined`. Firstly, it tells the reader what guarantee your code satisfies; secondly, it tells the victim of a crash which of your guarantees got broken ;)
That is actually a very good monad tutorial. Explaining monads in terms of category laws return &gt;=&gt; f = f f &gt;=&gt; return = f (f &gt;=&gt; g) &gt;=&gt; h = f &gt;=&gt; (g &gt;=&gt; h) is probably the most comprehensible way to understanding them. 
What is the argument for not having the lazy equivalent also be a compile error? If someone wants the unspecified fields to be `undefined` they should have to say that explicitly. My typical usage with this syntax is for making changes to some existing record (e.g. `def { logging = True }`).
It's the Kleisli category. The credit for introducing Kleisli categories at the very beginning in the tutorial should go to Bartosz Milewski :D
&gt; I've heard people implore never to use undefined, because it can blow up your program in difficult-to-determine ways, and you wouldn't ever want undefined behavior. `undefined` is *very* useful, when you're in the process of writing a program. It makes it much easier to interact with a half-baked program with ghci, since you can use it to easily get half-baked code compiling if you want to test something quickly. In a production program, `undefined` is almost certainly the wrong thing.
This is more nitpicking than disagreement, but there is an argument in favor of laziness for each of your points. 1. Correctness: Non-strict evaluation is more likely to find the normal form of an expression than strict evaluation. 2. Performance: Non-strict evaluation is the asympotically fastest way to evaluate any expression. 3. Exceptions: When using the `Either` type for error handling, points 1. and 2. should apply, so those "exceptions" could be faster and more correct, at least in theory. For applications in "*the real world*" you are technically right, of course.
Nice find. The links to dons at galois.com are 404. Maybe someone from Galois or able to reach dons could update https://wiki.haskell.org/Ghc-gc-tune.
It's not the implementation I'm calling rare. It's the use case of forming an enormous list and expecting laziness to elide most of the allocations that is rare.
You might not like this response but I would consider saying good bye to arch. To me it's a distro for tinkering and learning about unix/linux. The only unixes i accept now are Ubuntu and OS X. Source: used arch for 5 years, got fed up with breakage and constant tinkering required.
Oh. It's not about "enormous lists". It's all about "inplace tail rewrites", you see: https://github.com/ocaml-batteries-team/batteries-included/blob/master/src/batList.mlv#L67 https://github.com/fsharp/fsharp/blob/master/src/fsharp/FSharp.Core/local.fs#L99 you don't need to jump through all this hoops and unsafe coersions with laziness
You can also use `-fdefer-type-error`. Note that some custom preludes define an `undefined` which raises a compilation warning, this way you cannot forget one by mistake in your code. (edit: or `load!` in ghci)
Sorry, I do, but I never intentionally leave out fields. Also I was right that there is a warning even with lazy fields.
I want to emphasize that the value in question is not accessible to the end user.
&gt; Let's say I have nested records whose fields are not accessible to the user. One of those values is guaranteed to never be accessed unless it has a value, and its starting value is undefined. In theory I could use a Maybe, but since I can guarantee that my library will never access the value, why not use undefined? Some people are suggesting `Maybe`, but it might be better to just have two separate data types. From what I'm reading (without seeing code), you'd have other parts of your application that *know* the value is `Just` - in which case you'd probably have lots of `case myField myRecord of Nothing -&gt; error "Impossible!"`. Types should only contain values that you expect. If a type has states that you don't anticipate, it's too big, and introducing a different type is usually a good solution.
If performance is a concern it makes more sense to produce some benchmarks and see where the bottleneck is. Only then, might it start making sense to avoid `Maybe`. I know you know this, but I wanted to re-iterate this (I don't think it can be stated enough!) to avoid people thinking that `Maybe` holds some hideous performance implications (it's probably just going to be the cost of an extra single pointer indirection - aka inconsequential).
I hope you would be disappointed.
The biggest problem is that it blocks unpacking (until we can unpack sum types), but I think this also is something you can afford to think about later when start optimizing.
I like `error "NYI: description"`, because it's easier to track down the source. 
Using `error` appropriately, like /u/chriswarbo and /u/ocharles have suggested is also a good approach here, in truly exceptional circumstances!
It's a good question. Maybe (... and almost all other types) is opaque in the sense that once you set the value the compiler loses this information. What I usually do is parameterize the type data R a = R { x :: Int, y :: a } -- start out with: r :: R () r = R { x = 0, y = () } -- later when you want to set the value: r2 :: R Bool r2 = r { y = True } The parts of your program that don't care whether the value is set can take arguments of type `R a`, where the value is needed you use `R Bool`. This prevents you from hiding what you know from the compiler. Whenever I think "The type says X but I know that it's Y" I try to figure out a way to give the compiler the information its missing. Don't outsmart the compiler :) 
I would but cannot afford that. If I had that specialization I would do it myself unfortunately MacOS internals are far outside my focus. I have been looking for a solution to this problem for several months, asked a lot of questions, tried to motivate others and attempted to modify multiple existing packages myself. I don't see how compensating someone for prioritizing this problem and sharing the result with the entire community is a threat to anything.
Ok I think I must have explained it poorly, because that was not my point at all. My point is that spine strict `fmap` just straight up sucks, not that I wanted a streaming type: main = do let a = fmap (+ 1) . listToN(L|S) $ 5 * 10 ^ 7 print $ sum a print $ product a Gives me 9s for lazy, and 21s for strict. And about 2.5GB for lazy and 7.5GB for strict. This is not about streaming, although you are correct to point that out as my example was measuring two advantages of laziness when I just wanted to focus on one.
Hmm. It worked here to get stack traces. :/
I wouldn't know, I'm not exactly experienced. But isn't either of them sufficient on its own?
Well, I think I could, but definitely need work... I guess the priority now is to get `let` working properly (it's not), add support for `where`, etc.
`complement 0 = -1`, which is an infinitely long string of 1s, abstractly. There is a less well founded instance of `Bits` for `Natural` that just has `complement` undefined. You could in theory copy that behavior, but it'd take a little tricky maneuvering to always keep the shortest bitstring with the active set of 1s -- and even then I'm not terribly partial to the er.. partial instance we have for `Natural`.
&gt; One of those values is guaranteed to never be accessed unless it has a value, and its starting value is undefined It sounds like you need two separate types or data constructors... one for specifying the starting state and another for specifying the in-progress state. Why not just use a sum type? data MyType = MyTypeInitial field1 field2 field3 | MyTypeInProgress field1 field2 field3 inProgressField1 Now you can derive lenses, use accessor functions as normal, but you can use pattern matching to determine if the fields are accessible or not.
Hi. I am amazed by VS Code experience. Actually, when I am launching Atom with all haskell ide plugins, my not so old mac starts turning into a heater. With VS Code it is much cooler )) I have a question about Haskero. How much it is integrated with ghc-mod VSCode extension? Actually are there duplicating features? Should I install both? Or one is enough?
The other comments are saying the right thing, but let me put it a different way. If something with the value `undefined` is evaluated, your program crashes--always. If that's the behavior you're looking for, then maybe `undefined` is ok. There are some uses of phantom types where can use `undefined` without your program crashing because you know it will never be evaluated. You can use `Proxy` and the [tagged](http://hackage.haskell.org/package/tagged) library to avoid these kinds of `undefined`s.
There's [no shame in using regexes](https://hackage.haskell.org/package/pcre-heavy). `scan [re|.*?sometext(beg.*?End)other.*|]` is a regex that returns something like what you're expecting.
Yep, I totally agree. Just like the CSV interface this is very much needed and high on the to do list. Similar with simple things like copy/paste which would also help populating the tables with data.
These days I also use holes often in places I would have used undefined in the past. One advantage -- you can't forget to remove them :-)
There's a case where you want to parameterize the quasiquote interpreter with some additional data at compile-time of the module. In that case, mixing the two makes sense -- TH for that ability and QQ for the use of completely different syntax. But typically you only need one or the other.
I mean the fact that the lazy version has the chance to get all these optimizations and the strict one can never get them by itself is a pretty massive incentive to use prioritize lazy lists. While I do think you have a point that it is dangerous to rely on lazy lists to be streamed and have `O(1)` space, and you shouldn't use lists that are going to be consumed far enough to the point where storing them in memory would cause problems. I do still think that having these optimizations fire quite frequently and making your application just that much faster is really nice, and a strict list is never going to give you any of those optimizations, making even the use of small lists that you can afford to store in memory substantially slower. I can definitely see plenty of situations (like seriously a huge amount, probably most programs have at least a few of these somewhere) where I have intermediate list that I would love to be optimized out, but where I am still using the list type appropriately (not streaming the final resulting list, or if I am it is small enough that I can afford the memory). main = do let a = listToNS $ 3 * 10 ^ 7 b = fmap (+ 1) a c = fmap (+ 2) a print $ sum b print $ product b print $ sum c print $ product c Still gives me 13s vs 21s. And 5GB vs 7.5GB. So I am still correct (although admittedly not as dramatically) in saying that strict fmap kind of sucks. And that difference should probably even be a little bigger but the four calls at the bottom that both have to take the same non-negligible time for dilute it a bit. I think those extra 2.5GB are most likely the thunk / stack frame buildup that is necessary for the strict list (as you don't have guarded or tail recursion, you have the naive slow recursion that is often hated on). Lists are pretty much the prime example of when lazyness is good, trying to go for strict lists instead seems very silly to me. Strictness has its uses, but I would say this is absolutely not one of them and will basically never benefit you.
Wouldn't work, you'd need your own parser for Haskell expressions.
It's my blog. I would consider translating the posts about lambda calculus. I didn't read through CTAGI but quit to try Awodey's book instead. I wouldn't recommend CTAGI as a good book now so I won't translate it.
I saw it, but it's counterintuitive if someone misuses it as a normal function.
This is an good post, thanks! I've had similar adventures in optimizing my own ChaCha20 code using a portable C implementation. But I think the introduction is wrong: this post precisely illustrates why it's challenging to rely on the C compiler, and relying on "the beast" is a dual-edged sword. Things like auto vectorization/vector types often are non-improvements or even pessimizations (I suggest you just ignore it completely, as you've learned :). The speed of the generated code is also often going to greatly rely on the compiler, its version, and its target architecture; when I was doing this ~2yrs ago on my old Sandy Bridge, there were good 10% differences between various modern versions of GCC, Clang, etc. That's substantial performance variance, even for a well-worn microarchitecture that was years old at that point. (What GCC version are you using for the examples, BTW?) This was on x86_64, which is, without a doubt, the most well maintained architecture for any modern compiler. Of course, sometimes they just simply regress even with new compiler versions. I've even seen cases where old versions did better and new ones did worse, precisely because the new compiler pass was *more correct* (e.g. it fixed a bug that allowed some optimization to make an assumption, causing it to kick in -- but that didn't *always* hold, it just happened to luck out for me and not destroy everything). Implementations like the Krovetz' ChaCha20 get around this by using C, and they are in the same ballpark as your reported numbers (actually a little faster still, I think) -- but it carefully vectorizes manually with raw intrinsics for separate architectures at core hot spots, unrolls and interleaves some loops manually with careful testing, etc. This gives it a very consistent performance profile across targets. There are less 'rough outliers', even without the most modern GCC. I didn't get around to testing it, but I wouldn't be surprised if you saw much different results on e.g. Raspbian RPi3, using Debian Stable's GCC for armv7 (4.9.x). Variance on these platforms is often much greater. I could get around to this perhaps. However, I am also not a fan of writing things in pure assembly. ChaCha20 is easy enough actually (very simple to implement safely), and actually a perfect algorithm for 'embarassingly wide' SIMD implementation (you can break up the state block and compute over multiple entries in a single instruction, and this should probably get excellent speedup, the wider your units are). But it gets substantially tricker for almost anything else. I'd like to see a return to typed assembly languages or something, maybe. I have a small DSL in Haskell that turns Haskell into a macro assembler, and it works well enough, but I haven't used it for any crypto yet. Either way, this was really interesting! Thank you. I'll keep my eye on Raaz.
 -- -- hoursWastedOutsmartingTheCompiler = 42 --
You can make it strict and put `unsafeCoerce ()` in instead of `undefined`. It doesn't work for all the types though.
Are you aware of https://docs.haskellstack.org/en/stable/travis_ci/? You can probably get what you need by manipulating the "complex" config referenced there a bit. [Here](https://github.com/commercialhaskell/stack/blob/master/.travis.yml)'s a largish battle-tested .travis.yml in case you want to see more…
I just log everything, including exceptions. Haven't needed `CallStack` in prod.
What would be the exact opposite of that? Something like being able to build a small library with `stack` or even only with `cabal`, using the latest stable GHC release?
Wadler's "Monads for Functional Programming" is the best "monad tutorial" I've read. And there's no mention of burritos.
That is rather evil. I think I would go with [this option](https://www.reddit.com/r/haskell/comments/5se6cg/is_using_undefined_always_bad/ddeiyxh/) first, and throw in some strictness annotations.
&gt; The truth that both of them can define one another is left as an exercise to the reader. &gt; A monad is just a monoid in the category of endofunctors, what’s the problem? Erm.. whoever you call "laymen" in your world has already advanced well beyond the "laymen" in mine.. so do enjoy a moment basking in that fact ;)
Yep, you can always use aliases to help you out with the verbosity of that. IMO keeping track of which fields you have defined on a type level is worth it, if you ever find yourself needing to not define all your fields at once, which I don't think is super often, particularly with laziness allowing you to define them without actually having to pay for them until you use them.
Great series /u/deque-blog promote :: (a -&gt; Gen b) -&gt; Gen (a -&gt; b) promote f = Gen $ \rand a -&gt; runGen (f a) rand and from [`Test.QuickCheck.Gen.Unsafe`](https://www.stackage.org/haddock/lts-7.14/QuickCheck-2.8.2/Test-QuickCheck-Gen-Unsafe.html) promote :: Monad m =&gt; m (Gen a) -&gt; Gen (m a) seems to less general versions of [`distribute @Gen`](https://hackage.haskell.org/package/distributive-0.5.2/docs/Data-Distributive.html) distribute :: (Distributive g, Functor f) =&gt; f (g a) -&gt; g (f a) distribute @Gen :: Functor f =&gt; f (Gen a) -&gt; Gen (f a) distribute @Gen @((-&gt;) _) :: (t -&gt; Gen a) -&gt; Gen (t -&gt; a) Does that make the `Distribute` and [`Representable`](https://hackage.haskell.org/package/adjunctions-4.3/docs/Data-Functor-Rep.html) instances unsafe
Yeah that does seem like it would work. By the evil tricks thing do you mean that you kind of have to get it right in an internal module with no help from the compiler, since the compiler can't easily prove your assertion that `exists in list` implies `is defined`?
Thank you for your feedback! I really appreciate hearing your story and hope to hear more. Your situation is remarkably similar to mine (and I'm sure I'm not the only one). My first "real" program that did more than calculate some stuff for Project Euler is here: https://github.com/5outh/fhck If you look way back in the commit history, that's exactly how I started out, too! I got about [here](https://github.com/5outh/fhck/blob/ee6f0e91e0dc9ee338359e6e53af715f3dbd80ca/src/fhck.hs) without using anything fancy whatsoever (and I used _tabs_...shudder). Eventually I got some really good advice (from this subreddit) and cleaned it up using fancier machinery. Anyway, the main benefits of structuring programs around `Reader` and `State` to start with is that: - You don't have to explicitly pass configuration around - You don't have to explicitly pass internal application state around - You can separate what can and can't be modified in your program It's not like you can't _do_ anything without the `Reader`/`State` stack, but eventually you might find yourself asking "ugh, do I _really_ need to pass around these global variables to _every function_?" and this basic application structure helps alleviate that pain.
Hint: the merge step of mergesort is a pure, single threaded computation which processes two lists in tandem.
`-fdefer-type-error` is per-file - `undefined` or `error` is per term.
I'm not sure there's ever any reason to use `undefined` rather than `error` in production. During development, the reason is "I don't have to come up with a meaningful string for the error message." Note that `undefined` does *not* produce "undefined behavior" in the C sense.
If I recall correctly, 0 was just like a normal 0 and -9 was called the special zero that behaved differently in averaging and such accumulation operations but normally in some others, e.g., the average of (-9) and 10 is just 10. There was also a need for null that was different from zeros, but I cannot recall the details correctly. I am pretty sure one can make a proper algebra out of it in Haskell.
Despite ongoing discussions about how to make this sort of effort easier and cleaner, I'm glad it was this easy to begin with. A couple years ago there were some bugs that prevented the right gcc flags from flowing through that negated any attempt to leverage AVX-amenable C code via cabal. Mirroring some of the thoughts here on reddit, I consider the below to be possible ways to make such c-code and optimizations easier: * Allow per-file C flags. For example, `-mavx` on one object and not on another (without using the `make` build-type). * Properly track the C compiler (a `-with-[g]cc` option that actually functions). * Automated detection and cabal-flag setting for every supported intrinsic of the underlying toolchain (-maes? -mavx? -mfoobar?). * Perhaps too niche, but automatic generation of jump tables, which would be populated based on a check cpu features via cpuid, and have the effect of executing the most optimal routines for the current platform.
Yeah, the compiler takes a vacation as soon as you write `unsafeCoerce` :D The problems start when your program crashes without you touching the fields. In my case it turned out that the GC didn't like what it found in the fields and segfaulted. Great fun debugging that.
Oh right for some reason my mind blanked on the entire context of our conversation. The `unsafeCoerce` stuff fits with the rest of what you have said just fine. &gt; The problems start when your program crashes without you touching the fields. In my case it turned out that the GC didn't like what it found in the fields and segfaulted. Great fun debugging that. As cool as your idea sounds (not being sarcastic, it does seem cool), I think I'm gonna stick to my type variable thing just because you mentioned the possibility of that.
Huh. I have had some experience with python and for the most part I found it a pleasure to work with for the most part - provided it was used for scripting. I've used it on the RPi largely because it has libraries that are light-years better. Haskell is faster, sharper learning curve, etc. I think it's worth it but I also wouldn't recommend it to people in various domains/circumstances. And likewise for python though I don't like it as much. 
Shouldn't the goal be to not break guarantees? 
I've done a lot of coding with monad stacks, but I tend to avoid it for small utilities and or scripts. I'm not sure it's a good idea to tell beginners to immediately reach for them because they play a role similar to classes and objects in object oriented programming. They can make your code more modular if done well, but they often aren't easy to do well on your first go and end up ossifying your design some. I think it's better for people to throw a lot of hacky ad-hoc IO code together and refactor afterwards when they're more certain how responsibilities should be divided. In many ways, Haskell is opposite of C++. In C++, deciding objects up front is more reasonable and the adage about "premature optimization" is more true. In Haskell, i think it pays off to [think more about optimization from the start](https://www.reddit.com/r/programming/comments/5nid64/fast_haskell_competing_with_c_at_parsing_xml/) as your naive first draft is not likely to perform very well relative to what you'd do in C++. With regards to factoring your responsibilities, C++ takes a more modular component-based philosophy, and your choices are more arbitrary and a matter of taste, so it makes sense to front-load that. In Haskell, your code will be more *layered* and if you layer it wrong, you'll pay for it, so it's better to hack it together and then look at usage and judge what belongs in which layer.
I think one can look at fuzzing as a special case of property based testing (where the property is generally "doesn't segfault"), although in practice the focus (and thus tradeoffs) can differ.
There seems to be something oddly unsatisfactory about this class, but I can't really put my finger on it. On the one hand, it's clearly more versatile than `MonadMask`, but on the other... it feels very ad hoc. But maybe that's not such a bad thing, given it's use... Hmm.
If we used / instead of |, then we could avoid parenthesis more often and probably avoid unclear error messages when the compiler confuses a guard with an or-pattern. It might prove easier on the programmer reading the syntax as well.
I think the more generic "solution" would be reworking/improving how the GHC typechecker deals with type synonyms. However, that's clearly quite difficult from both a design and an implementation standpoint. It's unclear what the behavior *should* be, and treating type synonyms specially probably requires significantly more bookkeeping in the typechecker.
Overwhelming response , thank you .
I use or-patterns in Rust (sometimes) and they're really nice. I don't feel like I need them often but it's a nice feature in terms of programming language ergonomics.
There's a reason why assert throws in most other languages: a broken guarantee should pretty much require that you bail out no matter what.
Wow. I know everyone says Rust is the same as Haskell so this won't help but I'm still happy to see this :)
A profunctor is a bifunctor which is contravariant in its first term, in Haskell terms the function arrow `(-&gt;)` is a profunctor. (.) :: (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c fmap :: Functor f =&gt; (b -&gt; c) -&gt; f b -&gt; f c -- f a ~ (a -&gt;) rmap :: Profunctor p =&gt; (b -&gt; c) -&gt; p a b -&gt; p a c -- p ~ (-&gt;) So, specialising to `(-&gt;)`, we have that `rmap == fmap == (.)`. On the left, we see: lmap :: Profunctor p =&gt; (a -&gt; b) -&gt; p b c -&gt; p a c -- p ~ (-&gt;) lmap_fn :: (a -&gt; b) -&gt; (b -&gt; c) -&gt; (a -&gt; c) -- specialised to (-&gt;) `rmap` is covariant, `lmap` is contravariant. And then you make it a strong lax profunctor, form a monoid of them, and you have arrows, which is left as an exercise for the reader. (Or see https://arxiv.org/pdf/1406.4823.pdf, if you can get through the math.)
What about `||`
Exception handling has always been an extremely hairy area for me in Haskell. I have never been able to wrap my head around the different concepts related to exceptions - sync, async, mask, uninterruptible mask, bracket, finally, etc. I always wonder if the authors of the libraries that I depend on - eg Opaleye, pg-simple, Gogol, OAuth, wreq, etc - have understood these concepts and have used them correctly!
In my own code, I tend to use parenthesis in situations like this: f x | (x == 2) = True | otherwise = False They aren't necessary syntactically, but I put them there anyway because it reads better to me than having the == and = run together. Using || duplicates the exact same problem in other cases and I'd end up using parenthesis for stylistic reasons again which defeats the purpose of avoiding | in the first place. The / symbol is often used informally in English as a kind of "or" operation, such as when people write "he/she". I think it's a natural syntax to use here.
Read at least as far as 1990 in http://www.cvaieee.org/html/humor/programming_history.html for the inside joke on the "what's the problem" line. The good news: if you followed the whole article, the other than the minor fuss about "endo" snuck in there (all functors in Haskell are endofunctors, so just ignore it) the sentence should at least parse as something with meaning, if not intuition. A Haskell "layman" might be able to define `&gt;=&gt;` in terms of `&gt;&gt;=` and vice versa simply by following the types (hint: you need to introduce (lambda) or eliminate (application) the argument to the first function of `&gt;=&gt;` to get the types to line up). Proving that if one is lawful, the other will be, is not so straightforward, I'll grant you, so perhaps "layman" here means "mathematically inclined and practiced" ?
May be I was a bit unclear. By hand optimisation I mean sticking to using C with vector types not going all the way down. I was assuming that given the hints that certain variables can be updated parallely the C comipler should be able to do much more, but may be it is wishful thinking. I am not that surprised that the C compiler was able to better than the 128-bit implementation, i.e. if I give only the options -ftree-vectorise (without native flag) it still performs better than my 128 bit implementation. It gave a performance better than the portable implementation without vectorisation but lesser than the 256-bit implementation. I can attribute this to better instruction selection and things like that. What I am surprised is that the compiler was able to gauge (from the portable implementation i.e.) one can do 2-blocks at a time and get better (I am not very sure that this is the case but looking at the assembly it seems to be the case). Infact ones it figures this out (may be by unrolling or whaterver), then I think the fact that it does a better instruction selection should give it an advantage. 
&gt;How can I rewrite this search using any parsing library, or should I use regex again? If you already know the regex use that! Parsers are great in Haskell but they also kind of depend on monads so they take longer to get used to. 
I'm a Haskell noob. I think I understand the gist of the proposal, but there's one bit of syntax here that I haven't seen before. Could somebody explain what the `{}` means in this example? stringOfT :: T -&gt; Maybe String stringOfT (T1 s) = Just s stringOfT (T2{} | T3{}) = Nothing I would have guessed that last line would have been written `stringOfT (T2 | T3) = Nothing`.
It essentially puts in as many _ as necessary for it to typecheck. The purpose is to allow you to match based on record name instead of argument order: data Foo = Foo { bar :: String , baz :: Bool } foo :: Foo -&gt; String foo (Foo {baz = False}) = "" foo (Foo {bar = a, baz = True}) = s
And a really beautiful place in the western ghats of India https://www.keralatourism.org/pictures/wayanad-district/ds373
Oh my, this looks amazing! I'd love to see further work on this, e.g. making some optimizations more reliable.
Don't let anyone tell You "Enter + autoindentation" is enough. All the goodies are there for a reason. So please check Your CURRENT text editor for Haskell plugins. Autoindentation (on copy paste too!!), syntax highlighting, autocompletion, equivalents to ghci :type :info are all welcome. For learning Haskell :type and :info are important, though one can relay on ghci there to some extend. I advised already used editor cause that's massive investment and big efficiency gains if one can master it's text &amp; code editing capabilities, Your already have leg up with current one, so...
For the first bounty, I have cabal-macosx working and building an SDL2 app, and I believe I have solutions for the LSOpenURLsWithRole and Rez errors you posted. For the second bounty, I also have experience writing Swift-Haskell Mac apps. I don't have bitcoin set up, but do have PayPal. I'm [nanotech](https://github.com/nanotech) on GitHub.
It depends on how you use it - I find putting all the TH into files which have as few dependencies as possible works quite well, because they're rarely recompiled. That only works for things like lenses and Persistent though - if you start using TH for things other than data definition you'll suffer with compile times.
Here's a quick (untested) cut-down version: language: generic sudo: false cache: directories: - $HOME/.ghc - $HOME/.cabal - $HOME/.stack matrix: include: - env: BUILD=stack GHCVER=8.0.1 STACK_YAML=stack-8.0.yaml compiler: ": #stack 8.0.1" addons: {apt: {packages: [ghc-8.0.1], sources: [hvr-ghc]}} - env: BUILD=stack GHCVER=8.0.1 STACK_YAML=stack-8.0.yaml compiler: ": #stack 8.0.1 osx" os: osx # Note: the distinction between `before_install` and `install` is not important. # - in before_install we install build tools # - in install we install (haskell) dependencies before_install: - unset CC - export PATH=$HOME/.local/bin:/opt/ghc/$GHCVER/bin:$PATH;; - ./.travis-setup.sh install: - echo "$(ghc --version) [$(ghc --print-project-git-commit-id 2&gt; /dev/null || echo '?')]" - stack --no-terminal test --only-dependencies;; script: - set -e; stack --no-terminal test --ghc-options="-Werror" That's the bare minimum you'd want - compiling &amp; running tests with Stack under Linux and OSX. No docs, no Cabal, no multiple GHC versions. For actual use I'd recommend keeping a build around a build that compiles against the latest GHC/resolver, so that you get advance warning if there are breaking changes in a dependency.
Cool! We can use this to test if GHC optimizes a piece of code in the way we expect without looking at the generated Core, right?
Yeah, I guess that would be more backwards compatible. I'd accept that too :-) Personally I'm not a big fan of that style however because I don't think it's a good idea to have non-unit values silently thrown away. After all, that's why ghc warns when you do do foo bar if `foo` returns anything other than unit. 
An instance `Distributive Gen` would leak into `Test.QuickCheck`, and users might then mistakenly use `distribute`, which breaks the abstraction of `Gen` as probability distributions, instead of `traverse`.
Be careful of the consequences of combining or-patterns and guards. If you allow guard expressions to mention variables that are bound in an or-pattern subject to the guard, then you are faced with the question of how to pick which sub-patterns provide the values of the variables. Since the values of variables affects whether the guard will succeed, backtracking is required in order to get the "obvious" semantics. On the other hand, if you pick the left-most matching sub-pattern and don't backtrack if the guard fails, the implementation is straightforward and efficient but programmers might be surprised by possible matches being missed. OCaml takes this approach, and the missing match problem was bad enough for a warning to be added.
ah, sorry, it would need to be `(forall b. a -&gt; SomeException -&gt; m b)`. `throwM` is polymorphic in the result type too, so it will be a hint. *EDIT*: you could of course return `Left` kind of value, but I guess you cannot prevent that. I'm actually thinking that how you would implement `catch` using your `bracket`, and then you'd need `(a -&gt; SomeException -&gt; m r')`... So many choices.
Specialized functions are created that deal with unboxed values. This isn't always done because, as you say, module boundaries and such. But the unboxed sum types are actually something we're getting https://ghc.haskell.org/trac/ghc/wiki/UnpackedSumTypes
As put by Syrak, my understanding is that the "unsafe" only comes from the specifics of Gen: i.e. not being a real Monad unless we interpret it at the probability distribution level. This is the main reason why the post does not go the Monad way (in an attempt to avoid this topic). Functions such as "promote" have indeed some potential usage that leak and show whether two Gen are really equal. My understanding is that it is however quite safe to use "promote" to convert our function (a -&gt; Gen b) to a Gen (a -&gt; b) if we do not keep a reference to the converted function. The type classes themselves are not unsafe per say. Making use of them in the context of Gen would be.
`catch` is not typically implemented in terms of `bracket`.
Anything that handles exceptions as soon as possible and drop them out from sight is good. So any kind of bracket is better than anything else. But this is not ever possible. Exceptions destroy monadic/applicative composability; A catch or a bracket is like a strange beast that interfere in your smooth algebraic-monadic composition. Exception handling don't work well with multiple threads. Exceptions, at last, are like gotos and introduce additional complexity since the exceptional code has to be maintained and may jump in unexpected ways out of the carefully crafted flow of the application. Dangerous. [I favour the use of continuations to deal with exceptions](https://github.com/transient-haskell/transient/wiki/Transient-tutorial#finalization-better-than-exceptions). Instead of setting exception handlers with `catch` along the code, they can be stored in the state of the monad so the stack of these handlers can be called by a single `catch` in the `liftIO` definition. Of course you can declare handlers for closing resources next to the place where they are opened: takeResource res `onFinish` const (freeResource res) rest of the code... Note that unlike `catch` and thanks to the use of state, `onFinish` does not disrupt the monadic sequence, and the declaration is located where the resource is opened. The handler is transported by the state to the place where the IO computation produces the exception, while with `catch` or `bracket`, the code has to be broken in small pieces to have opening and closing of resources together. Well, bracket does it better but the disruption of the monadic sequence still happens. takeResouce res rest of the code -- maybe a dozen lines? `catch` (e... freeResource res &gt;&gt; what to do next? .... If you also store the continuations of these handlers, you can do interesting things; You can fix a problem resulting from an exception and resume the execution at this point, and you don't need to write catch handling code: The continuation is the handling code. Well, of course you must write something, but the execution is not diverted to a new branch (it can't), since it returns to the main branch. This is an invaluable guarantee and may give peace of mind for developers and maintainers. do onException $ \ e -&gt; when (e is connection error) (restart node &gt;&gt; continue) connect node ... Edited heavily for more details, sorry
No, I'm not questioning why have more threads (I assume that's what you're talking about). What I don't get is why change it at runtime with a function rather than compile with `--with-rtsopts=-N`.
First of all, you should do away with all the typeclasses, see [this for reference](https://lukepalmer.wordpress.com/2010/01/24/haskell-antipattern-existential-typeclass/). You can work with composition in case you later need something that has a BookMessage but is not a ButfinexBookMessage. As for your error, the `getExchange` function promises it can give the caller *any e that is also an Exchange*. But you only give it a `BitfinexExchange`. 
&gt; The former only works with Emacs and maybe Vim currently, while ghc-mod is more widely supported (including Atom). Intero has support for Neovim via [intero-neovim](http://github.com/parsonsmatt/intero-neovim), which IMO is more than enough to justify the switch from Vim.
&gt; In fact, my belief is that a type system like that of Agda might be more understandable than the plethora of extensions like type families, Data kinds, Gadts, existential types etc. I kind of agree. Advanced type level craziness in Haskell is indeed very crazy, because the solutions we have are often obscure workarounds for the lack of dependent types. As much as I look forward to `DependentHaskell`, I don't think it will solve many of these problems.
It doesn't seem surprising to me that if you restrict the definition of Applicative to functors that point at Set, you can remove the need for strength. Have I misinterpreted something?
You're right. Maybe what a category is is more obvious than you'd expected?
One points to the GitHub page, the other to the blog post, I think it's ok.
Regarding resistance/adoption, it depends. Some languages support functional programming ideas much better than others - for example, the success of Ramda JS doesn't surprise me much, since functions have always been first-class. You can do the same stuff in php, but the failures of such libraries also doesn't surprise me, because it's just not idiomatic php. Some ideas port over fine, particularly the higher level ones like "keep pure and effectful code separated." Others, like optional values a la Maybe, or applicative style, or Van Laarhoven lenses, will absolutely not suit some programming environments.
Probably because a fixed number of threads for the binary is not ideal for an arbitrary system. While common approximations (like number of cores plus one) aren't universally ideal either, they are decent approximations in many situations. 
`stack install purescript`, as you probably know, will only take packages from some specific snapshot. The PureScript compiler's dependencies don't quite line up with stackage snapshots, so this doesn't work, as you've found. The solution is to use `stack unpack purescript-x.y.z &amp;&amp; cd purescript-x.y.z &amp;&amp; stack build` - this way Stack can use the extra-deps defined in stack.yaml. This is described in more detail on the download page on purescript.org. Last I heard Stack was planning to make `stack install` use the package's stack.yaml if one exists - once Stack does this then `stack install purescript` will Just Work and we won't need to worry about any of this.
So this is just about sharing the right-hand-side? You can already do (warning, a little rusty so syntax might be a little off): stringOfT (T1 s) = Just s stringOfT | T2{} = Nothing | T3{} = Nothing and you can use a let (or maybe a where?) to define the RHS if you want to avoid rewriting it. Is there any precedent of other languages doing this? Seems like a very minor gain to justify additional syntax to memorize.
That's my hope too. Of course, these are *equations* rather than reduction rules. A proof of `elegantExpr === optimalExpr` wouldn't imply that GHC is optimising `elegantExpr`; it could be pessimising `optimalExpr`! ;)
Thanks for replying, Let me just be clear that the threshold for #1 is that a user with a fresh installed OS and just using stack, even if they don't have ghc and cabal globally installed, should be able to compile a working app bundle. Does that seem reasonable?
For first-party APIs this is probably nice; for third-party APIs like Twitter which often return insane amounts of unwanted data, I'm quite content with the current functionality.
What is the definition of `Succs` /u/nomeata? Interested
Same pros and cons that wildcard patterns (`_`) have, along a different axis (arguments of constructors *vs* constructors). Same caveat applies, GHC will complain about a pattern `Foo _` if you add a second argument to the `Foo` constructor but won't complain if the pattern has an empty record update `Foo {}`.
dang those relative paths, hyperlinking succs
I learned about idiom brackets from a helpful youtube comment of one line: the HaskellWiki's entry on idiom brackets is ... utterly unintelligible.
I need to look closer at the HERMIT. I only saw it in conference talks so far, and my uninformed initial impression was that it something large and external to GHC. But it might just be the right tool for this purpose. Also, chris, do you have the `Consumer` code somewhere? It might make a nice addition to my set of example modules with proofs.
Thank you! Those are useful comments indeed. I'll try to rewrite this using `Generic`s threading the `HashMap` through as you suggest. Will update the blog post once I'm successful. The verbose `let` is actually to help future me read the code -- being a Haskell newbie and someone who spends most of their time writing C, I tend to try to keep my code as simple as possible to read (possibly to a fault) so when I come back to it months later, it's easy to decode. Also a good catch on me missing the `fieldLabelModifier` -- I've updated my post to fix that.
Can `[]` be ripped out data Succs f a = Succs a (f a) deriving Functor instance Alternative f =&gt; Applicative (Succs f) where pure x = Succs x empty Succs f fs &lt;*&gt; Succs x xs = Succs (f x) ((fs ?? x) &lt;|&gt; fmap f xs) instance Alternative f =&gt; Monad (Succs f) where Succs x xs &gt;&gt;= f = Succs y (fmap (getCurrent . f) xs &lt;|&gt; ys) where Succs y ys = f x given [`xs ?? x = map ($ x) xs`](https://hackage.haskell.org/package/lens-4.15.1/docs/Control-Lens-Lens.html#v:-63--63-).
Correct. But if you trust GHC to not pessimize code in unreasonable ways, you cold still achieve this. But keep in mind that while proving, GHC.Proof inlines functions more aggressively, so it is not quite the guarantee you are looking for.
I think what is being pointed out in multiple responses is that somehow the JSON has to include the information of which actual type is being encoded. In the Sum type, it's just there. Once you existentially quantify, that information is gone--you explicitly removed it! You can put it back, by adding some other typeclass that your quantified instance must satisfy, one that maps each quantified type to a unique tag or some such. I've gone down this path. I've even written a small library to simplify it. But eventually I gave up. It complicates things, particularly in the decoding step (I ended wrapping the Aeson Parser in a ReaderT to give it access to map of type tags to types) and, for me, it led to more issues down the road, anytime I wanted to be able to do more things with the types I had wrapped. It ties your use of the types behind the wrapper to the wrapper in a painful way. I ended up back with the Sum types. For me the pain point there was the ability to extend the set of types--I wanted the equivalent of open sum types--so I've had to work around that. The record solution is good as well but doesn't lend itself as well to the use of the generics for Aeson and Serialize. Obviously, YMMV.
Thank you!
I don't think laziness would necessarily be lost, you would only need to change that code to monadic that already threads the g. You could generate an infinite list [Int -&gt; (Int, Int)], each of which is only used once, but breaking that invariant would make your code insecure so that probably smells. I wonder if there's some way to ensure that each is only used once.
N often equals the number of virtual cores rather than the number of physical cores. Hyperthreaded cores aren't great at running Haskell I think.
Yes, that would also fit the purpose of the library, I believe.
Both Applicative instances have crossings between the left and right side, while product acts in parallel, so I don't think so.
I see! Makes sense, thanks ;-)
Don't be afraid of monads. They're very light weight.
I think this type signature has the same downside as the one I put in the blog post, namely that it can't work for `ExceptT` or `MaybeT` due to the potentially non-present `r` value to pass to the "Finalize" handler. (I'd mention `ListT` with its multiple-return-values, but I don't think anyone pays much attention to it.)
Is there any tool that would allow you to jump from a type you have defined / imported to all locations where you pattern match on that type? That seems like it would help a fair amount with the major issue that this solves. Or potentially even better jump to all `_` matches on that type. I guess you could temporarily make the type uninhabited. And try and compile. I don't have a particularly strong opinion on the proposal. I can definitely see how it would be useful but it also is a pretty substantial addition to the grammar. So I want to make sure it is worth it. 
Out of curiosity, what do you mean by this? A number of us are working quite hard on a variety of related efforts in the Haskell ecosystem.
Regarding the error: I made BitfinexExchange an instance of Exchange just before: `instance Exchange BitfinexExchange where...`. Why doesn't it consider BitfinexExchange an instance of Exchange?
It's more that I'm afraid of losing laziness, parallelism and clarity. I'm new to this so I don't know what length it is normal to go to in order to avoid monads. It seems to me that if it's necessary to use monads for the vast majority of the non-trivial code then why use Haskell in the first place
From my understanding of the MonadRandom library your code would just be: randIndices len = do i_left &lt;- getRandomR (0, len - 1) i_right &lt;- getRandomR (i_left + 1, len) return (i_left, i_right) You wouldn't lose readability IMHO.
But it isn't better its identical, and this part "You have to change the type signature if you want to do IO, and mark every function between your function and main() as tainted with IO" is just flat out wrong. (Also contradicts " It encourages you to keep functions that do IO as high up the stack, close to main, as possible." but eh.) it "finds the correct number of titles" $ titleCount `shouldReturn` 59 Is the simple way to do what he says is impossible. Or (`shouldBe` 59) &lt;$&gt; titleCount Or shouldBe &lt;$&gt; titleCount $ 59
Could this same trick be used to implement instance Eq (a -&gt; b) ?
We were all under the impression you were only allowed to manage things and not write code. Great stuff, welcome back! :)
This makes sense in a monolith app. Maybe I should look at doing just this at first, and then try to generalize. The starting point was that I'm looking to build a library. Apps using the library will provide concrete data constructors, e.g. for various exchanges - Nasdaq, NYSE, Bitcoin exchange, etc. The library will provide abstract interfaces. What is the common design pattern for this sort of problem in Haskell?
Split, at least as implemented by `tf-random` (noting that [`StdGen` does not split correctly](https://github.com/haskell/random/issues/25)) is quite cheap, most of the time it only does simple bitwise operations, and needs one iteration of a hash function every 64 splits (based on [the paper behind `tf-random`](http://publications.lib.chalmers.se/publication/183348-splittable-pseudorandom-number-generators-using-cryptographic-hashing)). You'll already be spending much more time actually getting numbers out of the generator. Turning the generator into a list is somewhat of a waste because it splits more than necessary. -- With splitting, no need to return a new generator. -- To call this, split your generator, keep one half, -- give the other half to `randIndices`. randIndices :: RandomGen g =&gt; Int -&gt; g -&gt; (Int, Int) randIndices len g = (i_left, i_right) where i_left = randomR' (0, len-1) g1 i_right = randomR' (i_left+1, len) g2 (g1, g2) = split g With [`splitn`](http://hackage.haskell.org/package/tf-random-0.5/docs/System-Random-TF-Gen.html) you can easily and efficiently split into a large number of generators to distribute in your function body. [g0,g1,g2,g3] = [splitn 2 i | i &lt;- [0 .. 3]] I've never seen this method being actually used as an alternative to monadic code (I mean, QuickCheck relies on split a lot, but this is abstracted away in the `Gen` monad), but I think it is a nice fit for purely functional programming.
Saints be praised! bos is writing Haskell again!
I think Applicatives are sufficient for using regular expressions, but I guess learning applicatives is similar to learning monads. Edit: Use regexes :)
Generally you want an `Alternative`, but yeah I know I personally do not use the typically overkill powers of `Monad` for the parsers I write.
It's a bit annoying that OCaml make the 'when' clause common to all parts of an or-pattern. I would often like to write something like match x with | Some i when i &lt; 0 | None -&gt; None | Some i -&gt; sqrt i But I guess that would open another can of worms.
&gt; Start with the simplest thing, identify pain points and repeated code I would just like to say that this is a software design principle that needs to be shouted from the heavens over and over again. I'm writing a new project right now and it's something I have to keep reminding myself before I overthink my abstractions.
You mean it takes either a plain Message, an ExchangeMessage or a BookMessage and behaves differently depending on the type of message?
Just `Constraint`, I would also prefer the recent [`Type`](https://hackage.haskell.org/package/base-4.9.1.0/docs/Data-Kind.html#t:Type) to `*`.
+1 on lenses here. I did read all of the discussion. But lenses are progress and selectors are not. That said, if it's neither for the time being, I can't argue.
Haskell's roadmap has not moved nearly as quickly as Rust's. It's understandable, considering the circumstances. I'm personally envious of the Rust community's ability to progress their language very quickly. It's just an advantage that newer languages have
Sounds like it might be a problem with the `/Software/crux/dist/build/crux-exe.app/Info.plist` file. What does it look like?
I guess perhaps the CFBundleInfoDictionaryVersion or CFBundleIdentifier are the issue? plist here: http://pastebin.com/8APsefej
Message and ExchangeMessage are abstract types. And instance of ExchangeMessage is BitfinexBookMessage or BitfinexTradeMessage. An instance of Message could be UsdFundsRateMessage. And yes this function could ignore the UsdFundsRateMessage messages, and only process BitfinexBookMessage messages. Another implementation of the function would do something else.
 state = error $ "unpossible! " ... lol. Good to see a `Data.ByteString.Lazy` interface for this one. I had difficulties the other day combining existing compression libs with lazy bytestrings as many only supported strict bytestrings. This is super important if you intend to use the `binary` package and something like `binary-vector-instances`, so you'd rather not swap to something like `binary-strict` and lose all your convenient library instances.
This is another point in favor of the / syntax. It could make per-branch guards less awkward.
Rust has a small number of people driving things forward and making most decisions (edit: with a quick and efficient process). Things don't get locked by indecision amongst many people nearly as often as they do with Haskell.
You don't lose laziness to monadic code, only parallelism. But you already don't have that. The `g` argument is already always dependent on the output of the previous generation event, so your semantics as far as that's concerned are unchanged by a monad. And MonadRandom doesn't affect anything else. It's literally just a shortcut for what you're already doing. And once you're used to it, it's not a loss of clarity either. Generally a gain IMO. :)
I very appreciate you and other guy 's hard work on ghc and haskell ecosystem, and what I envy is Rust has plenty of resource and active contributors to work on its compiler, toolchains, etc. For example, [Rust Language Server](https://redd.it/5ole1x), Haskell has the equivalent: [Haskell IDE Engine](https://github.com/haskell/haskell-ide-engine), but the development is stalled
Yes, I have this working in a fresh VM using only stack and its locally (~/.stack) installed ghc and cabal.
It means Haskell is such a lovely language but stupid bosses do not get it. 
If there were a Haskell propaganda minister he would disagree.
You pretty much should always prefer `error` to `undefined`. But if we're looking for cases to tempt people to use undefined, `fmap (const undefined)` might be useful to convert between different kinds of nullity, like `[]` or `Nothing`. But even then, still better to use `error` with some message you never expect to see.
I thought "guilty as charged" when I read the piece.
&gt; If anyone has done this before or has any insights on the right way to do it, I would appreciate hearing about it. I've done this kind of thing [before](https://twitter.com/haskell_cat/status/761379632734212100), and it just so happens that I had a collaborator so there are a few github issues ([free bifunctorial category](https://github.com/dimecon/free-cats/issues/8), [free semigroupoidal category](https://github.com/dimecon/free-cats/issues/9)) documenting our discussions at the time. I started by defining a type with too many values, just like you did, and then what really helped me was to draw example values of that type. This allowed me to visualize the laws which this type did not yet satisfy as rules explaining how parts of the drawing (in my case, boxes) are allowed to move freely along certain axes but not others. This in turn allowed me to visualize a way to normalize those drawings, by pushing all the boxes to one side. The correct GADT is then the one whose values are those normalized drawings.
Well, Stack has support for referencing specific commits in git repos. So if you wanted to do this you could write a tool that modified `stack.yaml` to point to the versions of the packages you were interested in from some Non-Hackage set of packages. But my actual opinion is that while I definitely agree this feature is a good fit for some languages, there's also value in standardization, and I would be personally happy with just a more-correct Hackage.
If people asked their questions on stackoverflow instead of r/haskell the numbers would be much higher :)
&gt; I don't know off the top of my head a way to get a free profunctor. Wouldn't it just be an `f` with a function in both directions? data FreeProfunctor f a b where FreeProfunctor :: (a -&gt; b) -&gt; f b c -&gt; (c -&gt; d) -&gt; FreeProfunctor f a d instance Profunctor (FreeProfunctor f) where dimap f g (FreeProfunctor f' k g') = FreeProfunctor (f' . f) k (g . g') 
I think it's more the slight at Haskell. Lots of good things come out of rigorous thought/academia, though obviously they aren't packaged products. Also python is on the list, so I don't know it's that meaningful for some use cases. 
Looks law abiding to me!
Could also possibly be embedded or, less likely, perf projects. 
I'm allowed conjugal visits with Emacs and a terminal window once a month :-)
I don't like the idea of using `/` as it isn't a reserved symbol. I think `|` works just fine.
 -- Please come back and update this after you're done
Haskell isn't suitable for real-world software. The most important factor in choosing a programming language for real-world software is how many people know it (both in terms of job market, on a particular team, third party library support, community mindshare, etc). Everything else is basically icing on the cake. Haskell completely fails on this point and thus is not suitable for real-world software. It's that simple.
I have indeed -- in my case, I expect the client and service to be tightly coupled. So if the set of fields isn't exactly the same, I assume something is wrong with the deployment. Of course, if this is not the case, and you want more relaxed requirements the standard mechanism might be okay. Or, as you say, you can do something in between by adding API versioning in the JSON itself.
Might be worth noting in the Haddocks that you'll also get `Skip`s in the case of invalid data, and not always `Error`s. It currently says: &gt; Either the compressed frame was empty, or it was compressed in streaming mode and so its size is not known. So I would have assumed it was for empty bytestrings or people using the streaming interface, but it seems like all the garbage data I feed it gets a `Skip`: &gt; import Codec.Compression.Zstd &gt; :set -XOverloadedStrings &gt; decompress "foo" Skip Also wanted to add: awesome to have this library, only had these comments because I was trying to write example code for it but couldn't figure out how to get an `Error` case Edit: Oh I thought of something: &gt; decompress $ (compress 1 "foo" &lt;&gt; "bar") Error "Src size incorrect"
&gt; If something with the value undefined is evaluated, your program crashes--always. This is only true if you don't catch exceptions. The effect of `undefined` is to throw an exception of the type `ErrorCall`, which can be caught and handled in IO. Good reasons to do this are rare, though.
Wait, so who is the Rust propaganda minister?
That might be a stupid question, but why keep the `f b c` at all in the constructor? Why not "apply" it when you extract the functions? If the essence of profunctor is dimap, it seems that it's the only thing that is required.
Yes it's fair. I was more sadden by the fact that instead of highlighting the good this is repeatedly the first thing that gets said. I mean there are 5 lines about Haskell and that's all the article says :(
Yeah, although you often end up having to bind variables in order for those guards to be useful. Then you have a variable in one part of the or-pattern but not another, which is an error. It's probably possible to come up with a design for pattern-local bindings that would allow it, but whether that would be worth the modest gain is unclear. Apart from the variable problem, I think nested guards are a reasonable idea.
Yup. Tooling, tutorial documentation, reference documentation, editor integration, interop with other languages, familiarity, performance, maintainability, high-quality libraries… Sure, the Haskell community could do better at some of these, but there are plenty of valid reasons to use Haskell in production. I’ve done it for several projects at different companies, and I’d gladly do it again.
A problem people hiring Haskellers face though, is that a typical Haskeller is probably quite proficient in some other language(s) as well, so the Haskell shops are competing for talent even though Haskellers greatly outnumber available positions.
It does not exist yet. I have seen a talk by a student Gert-Jan Bottu who is researching this (more generally) and hopefully can implement it. This talk was on the Duth Functional Programming day. I don't think there are any slides online... He told me that the first who describe such an extension (quantified class constraints) were Hinze and Jones. See section 7.2+ of "Derivable Type Classes" by Hinze and Jones.
Exactly. Just like PHP, as you can see in the other chart. Wait what?
This was just the smallest example from my code; most of which utilize randomly sorted vectors. As I understand it; that would mean that all code using `randIndices` would have to be monadic as well, and lose some readability. It might be possible to separate out as much code as possible that does not rely on these vectors in order to avoid monadic computation. How much importance is given to separate functional and monadic code in real world Haskell programs? It seems to me that after a while nearly everything would be monadic.
&gt; I think that's one way to parse any CFG: you start enumerating strings, at some point you either come across the input string (parse successful) This sounds like something PHP would do :)
&gt; As I understand it; that would mean that all code using randIndices would have to be monadic as well No. Only when using an "impure" monad like IO you have to propagate that all the way up. Because IO can't be "escaped" but we know how to extract values from other monads. If you wouldn't see the definition and type of `f` in the following examples you wouldn't even know that a monad instance is used internally: &gt; f a b = do { x &lt;- a; y &lt;- b; return (x+y) } &gt; :t f f :: (Num b, Monad m) =&gt; m b -&gt; m b -&gt; m b &gt; :t f (Just 1) (Just 2) f (Just 1) (Just 2) :: Num b =&gt; Maybe b &gt; :t f [1,2,3] [2,3,4] f [1,2,3] [2,3,4] :: Num b =&gt; [b] &gt; It might be possible to separate out as much code as possible that does not rely on these vectors in order to avoid monadic computation There always is a way, just maybe not worth it code complexity wise. &gt; How much importance is given to separate functional and monadic code in real world Haskell programs? Monadic code is functional code, the fact that we just have the syntactic `do` notation in the language is just there to simplify the code. What there is to emphasis is the separation of impure code from pure, maybe that's what you wanted to say as well but led wrongly to believe that monadic code is impure. It is not. IO is impure. 
http://lmgtfy.com/?q=where+haskell It isn't that hard.. 
[removed]
I loved x86 programming during my college days. These days though, nasm seems to be closest to the tasm/masm programming that I used to do, so I hack a bit on that for nostalgia! :-)
I know you're getting downvoted like doomsday, but there is a very large grain of truth in what you're saying. Despite what we may or may not like in a language, as far as the job is concerned, the industry chooses it on the bases that you've mentioned. 
depends on how good your knowledge of haskell is - usually you mutate some nodes by labelling them, you can use `Data.Map` for that but nevertheless, I recommend writing a bunch of test cases to make sure you get the algorithm right - there are some slides by spj online where explains the algorithm and quite a bit more https://www.ukuug.org/events/agm2010/ShortestPath.pdf
Ah great, that's exactly what I need.
Thanks! The drawings you use are pretty crazy, and it's a little difficult for me to develop an intuition around them. But, I think I get the gist of the train of thought: making sure that the GADT has exactly one inhabitant per collection of operation should are considered equal. Like, making sure that `id . id` and `id` normalize to the same thing. I'll try to post here if I make some progress.
Nice design! Way better style than if I had rolled my own. Is it compatible with bindings in other langs? I do like how you expose the dictionary training API nicely too :)
I think there are 2 major reasons a language would be used more on the weekend. You have fallen languages that are no longer useful in industry (assembly) and rising languages that are not yet useful in industry (Haskell).
Though now that I think about it, this wouldn't be entirely useful. [Here's how](https://hackage.haskell.org/package/profunctors-5.2/docs/Data-Profunctor-Strong.html#t:Tambara) you make any profunctor strong: newtype Tambara p a b = Tambara { runTambara :: forall s. p (a, s) (b, s) } instance Profunctor p =&gt; Profunctor (Tambara p) where dimap l r (Tambara f) = Tambara $ dimap (first' l) (first' r) f instance Profunctor p =&gt; Strong (Tambara p) where first' (Tambara f) = Tambara $ dimap (\((a, c), s) -&gt; (a, (c, s))) (\(a, (c, s)) -&gt; ((a, c), s)) f The point is, you need to be able to provide `forall s. p (a, s) (b, s)`. With your free profunctor, this is at the very least nontrivial
Constraints for small test case in Code Jam are often chosen small enough so that one can easily use brute force solution. Haskell is incredibly well suited here. There's a nice blog about typical problem of such a kind: https://bartoszmilewski.com/2015/05/11/using-monads-in-c-to-solve-constraints-1-the-list-monad/ Haskell solutions for Code Jam problems: https://www.go-hero.net/jam/16/languages/Haskell For practising I suggest to give [HackerRank](https://www.hackerrank.com/) a try. Problems are good, and they support for Haskell. You probably want to take algorithms track, but there are more, even functional programming track. EDIT: Overall the most important thing when choosing language is how familiar with it you are. This is a very fast paced competition, once you solve the problem, you should better be able to bang out code with no need to lookup the docs.
So, Haskell is *really* good at representing trees, but graphs are a bit trickier - you can do it with impure data structures, like /u/andrewthad, or you can hack something together with `Data.Map`, which will cost you a lot of performance because of the *O(*log *n)* lookup time. Once you have the graph representation down, I don't expect that the algorithm itself will be too difficult to implement. Like /u/epsilonhalbe says, the conventional way to mark visited nodes is with mutable state, but you can get around that by just keeping a list or map of visited nodes. It might also be worth looking into `Data.Array`, if you know how many nodes are in your graph - it's immutable and fixed-length with *O(1)* lookups.
I don't have any idea what that "cofreely" comment means. I'm not as well versed in the category theory of profunctors =P I would *guess* it has more to do with the duality between tuples and `Either` with respect to strength / costrength. But that's just a guess. EDIT: The `Cotambara` type a little bit down the page seems to confirm that guess.
I think the problem here is having a high end IDE. If Haskell had the equivalent of Visual Studio+Resharper I bet writing that 10 lines without a compile error would be a lot easier. Some work is being done in the area, thankfully, but there is a long way to go yet.
That page sure contains a lot of crazy types! The one named "Pastro" (how do they come up with those names??) says it "freely makes any Profunctor Strong", but it looks like it doesn't really require `p` to already be a profunctor, so I think Pastro is a free Strong Profunctor: data Pastro p a b where Pastro :: ((y, z) -&gt; b) -&gt; p x y -&gt; (a -&gt; (x, z)) -&gt; Pastro p a b instance Profunctor (Pastro p) where dimap f g (Pastro g' p f') = Pastro (\(y,z) -&gt; g (g' (y,z))) p (\x -&gt; f' (f x)) instance Strong (Pastro p) where first' (Pastro g p f) = Pastro (\(y,(z,c)) -&gt; let b = g (y,z) in (b,c)) p (\(a,c) -&gt; let (x,z) = f a in (x,(z,c))) second' (Pastro g p f) = Pastro (\(y,(c,z)) -&gt; let b = g (y,z) in (c,b)) p (\(c,a) -&gt; let (x,z) = f a in (x,(c,z))) 
Thank you for all your work on this library, I think it's really great!
One very good use of monadic code is to abstract away a common calling pattern. `Reader` (common input), `Writer` (secondary, accumulated output) and `State` fall into this pattern. In the loosest sense, you are _already_ using the `State` monad, just manually, without having Haskell help you. &gt; What I'm looking for, is a way to generate that Random Generator way up top, hence containing the gooey stuff, the way that it's done now, and passing the `g` implicitly. Is there such a way, if so, how? This is _literally_ asking for a `State g` (subject to `RandomGen g`): randIndices :: RandomGen g =&gt; Int -&gt; State g (Int, Int) randIndices len = do i_left &lt;- getRandomR (0, len - 1) i_right &lt;- getRandomR (i_left + 1, len) return (i_left, i_right) where getRandomR range = do g &lt;- get (ret, g') &lt;- randomR range g put g' return ret `getRandomR` would need to be defined at top level to avoid duplicating it everywhere.
&gt; you could match that way, but you cannot parse (i.e. produce the parse tree). Do you mean it's impossible to parse this way? I think with some bookkeeping it should be possible to convert a recognizer into a parser. To be able to generate strings you need to keep a form with non-terminals and terminals, and generate new sentences by using different productions in non-terminals in turns. I think with each sentence if I keep a history of derivations I can get the full parse tree when the input string is recognized. Sorry if this is a bit too hand-wavy :-)
If you want to look at code written in Haskell for competitive programming contests, you can look at the Codeforces submissions of this guy http://codeforces.com/submissions/utkarshl/page/1
But working in Haskell seems to be considered a perk by the developer. All else being equal, I'd rather work in Haskell than in Python.
It was the second option, yes, thanks!
If I recall, you can do it very cleanly via tying the knot, but there are tradeoffs - you have to be able to partially inspect distance (I think I used unary, there were performance concerns but it passed the challenge time limits so I did not optimise further) and if there's no solution it diverges.
Can we make a zipper around the traditional heap, and perform all 'mutable' calcs inside of "State HeapZipper"?
The ones have done, the medium or hard problems didn't work with brute force but can usually be simplified by doing some "math" to a few lines. What do you mean by data-memocombinators ?
Haskell is not a bad choice, but you have to familiarize yourself with writing fast Haskell code. It's useful to get used to the profiling tools and actually profile the code you have to see if you can improve it. Solve problems from competitive sites like HackerRank, ProjectEuler, etc. and compare your solutions to the best solutions out there. A competitive advantage over a language like C++ is Haskell code is much more terse, and hence faster to prototype solutions in. There's a lot of bad ways of writing Haskell code, which is okay for smallish programs where performance and memory usage isn't a concern, but that's definitely not okay in code jam. And obviously, avoid unnecessary abstractions (like codensity) which are elegant, but over-hyped; you can always find ways to solve the problem efficiently without such abstractions. And never hesitate to use external libraries - things like text, elemOrd, vector, etc. are crucial to having a competitive edge. Lastly, be aware of fancy tricks like stream fusion and compiler optimizations.
I had a good experience in a time-constraint competition: http://www.joachim-breitner.de/blog/677-Fifth_place_in_Godingame_World_Cup
&gt; It might also be worth looking into `Data.Array` My understanding is that, while not part of the Haskell2010 Report (Libraries), `Data.Vector` is generally both more flexible and better performing than `Data.Array` (which is in the 2010 report). 
You can't use a string enumerator by itself to parse, but it's just as easy to make an enumerator that gives a parse tree along with each string. Which appears to be what Earley's generator does.
Everywhere you need to use dynamic programming, you can usually change a brute-forcing solution into a dynamic optimized one just by adding a memoization.
And this one too! https://github.com/moajohansson/IsaHipster
You can already do this by just running a mirror yourself and putting the repository in `~/.cabal/config`? What else do you need? Am I missing something? I even think FPCo has code to mirror stuff onto S3, so you could probably even avoid running server/storage if you wanted.
&gt; Get your descriptive statistics right. But, it's hard. *whine* And the pretty chart says what I want it to now, so I'm done.
Have you used Visual Studio? I haven't used Intero, so I can't compare.
what do you think about this [master](http://www.vub.ac.be/en/study/applied-sciences-and-engineering-computer-science/default) 
Hard to say, I'd look more at professors and their research areas, and classes offered, than at a particular program.
I have no clue how can I evaluate this master by checking the professors profile. however what makes me interested that it has a really good connection(exchange program) with universities such as ETH, EPFL , Chalmers university of technology... do you you see this a strong point?
thanks, just sent them an email.
&gt; It seems clear that this is the wrong behavior. Deprecation should affect the solver. Why? A deprecated library doesn't mean anything is wrong, it just means there's no maintainer. The semantics of "This package is broken" and "This package is unmaintained" are not the same. I don't even know of any precedent where it *does* change the behavior of any package resolution, in any language. It's extremely unintuitive for the solver to abandon choices on this principle, IMO (especially if it starts doing so out of nowhere, so that ship has likely sailed). &gt; Someone could also release a new package, and that would affect the solver as well. This happens all the time. Builds that depend on the solver will never be reproducible. Trying to save solver reproducibility (which doesn't exist in the first place) at the expense of making package name+version a shaky reference is a bad tradeoff. The point *isn't* whether any change can affect the solver. In fact, it's completely OK to me if a new package release changes my install plan (providing it abides by the PVP of course, so I don't get surprising behavior). But like I said, changing it in the face of a deprecation does not conceptually make sense to me, because it does not indicate the package is *broken*, which is what revisions are for, aside from keeping hackage healthy by performing minor bumps -- there's no precedent for it anywhere, and it's definitely going to end up as a user visible facing change (vs just the ordinary solver heuristics) if you change it now. Of course, I don't run what happens in Cabal, so everyone there may disagree. It could go either way, but needless to say I don't think it's necessarily wrong. Second, the amount of scope allowed in revision changes are small, and revision changes are only carefully done by 3rd parties or the maintainer, and can always be reverted (because there's always a log and the scope is so small). The scope for errors can be large, I guess, if someone just randomly like, broke every version of `lens` or something I guess -- but in practice I don't think we've seen particular major failures for it (because they can always be undone too, so failures seem unlikely to spread dramatically or be persistent). Finally, empirically -- install plans from Hackage have gotten *much* better thanks to the response from careful curators including people like Herbert, because many invalid or wrong plans have been removed, and packages that were busted themselves have been fixed, and ones that were deprecated or unmaintained still got a few updates (like some minor bounds bumps for e.g. base) that they needed. I consider it hand-wringing because the people who do substantial work to monitor failures and fix them in-line with the trustee principle see good results. (Aside from trusting them on the matter, if they didn't, I figure they wouldn't do it.) Simply banning packages isn't enough, you sometimes actually have to manipulate constraints. If simply "bump version and new upload" worked, we wouldn't be doing this. &gt; I really don't consider this hand wringing. Preemptive upper bounds waste my personal time, which is valuable at least to me. And I only maintain two packages! I can't imagine how hard it is for people that are more active library maintainers. I'm not going to get into this (IMO, unbelievably played out and tired) argument and waste my time on it. You'll just have to live with the fact I use them (except in some limited cases) if I publish anything, I guess.
I'd prefer otherwise but it's acceptable.
A HN comment mentions that "used on weekends" doesn't only reflect "used for fun hobby", but also "needed for class assignments".
Sounds like a good example!
This comment made me smile :) On a more serious note, while I do do a lot of advocacy because I love Rust, my aim isn't to produce propaganda; if I'm ever being wrong, I prefer to be called out on it.
Is there some way to get a REPL with ghcid? The automatic reloading is great but I've found myself wanting some way to interact with the new code after that. 
Ok this is great. I'll be able to go over it and test it out later tonight.
Well, it's not just instantly showing errors. VS+Resharper has context aware auto completion. To give you a concrete example, in C# we use camel case everywhere. So if I have a method called `BuildAccessPlan(SomeSource src)` I can just type `BAP(` and the method will be filled in (with the cursor in position to fill in the parameter and help text above showing the name of that parameter `SomeSource` and any help text associated with it), because in context that's the only possible method that fits that pattern (actually, usually the `B` would be enough already so I'd be typing `B(`). Of course you can type a new name of a method that doesn't exist, then you can press a simple key sequence to automatically generate that method (including the parameter types it deduced from the parameters you passed and names for those parameters based on your defined naming conventions). For Haskell I could imagine this working similarly: if I type `fromIntegral` the IDE can already narrow down what variable this can be applied to based on the variables visible in scope and which ones conform to the appropriate type class. So it's not just that the IDE showing you a red underline when you've done something wrong (that's probably the uncommon case, once you get used to working this way): you immediately see it won't provide the function/variable auto complete that you expect to be there so you ask yourself why the IDE doesn't see that as a valid completion. Once you get used to working this way development becomes shockingly fast.
Yes, for this to work, the variables that are only bound in some parts of the or-pattern but not all, would have to be prevented from being used in the expression the pattern is guarding. Still make them scoped over that expression, but make it an error to use them outside of the per-pattern guard. The scoping is important, in case there's some shadowing. 
I'm not the expert here but what you want appears to need meta-programming. Meta-programming in Haskell is accomplished, generally, with template haskell. Hopefully that will get you started. You're second issue should be solvable by type class families.
You can use `genericLength` that is basically `fromIntegral . length`.
No I was asking if there was a way to get around that annoying `fromIntegral` we have to use each time we convert from integers to reals. I mean, something like implicit type conversion.
Yeah, it's annoying that `3`, `length [1,2,3]` and `6/2` all have different types in Haskell, even though the things you might want to do with them are exactly the same. In most other languages (even functional ones like OCaml) they have the same type. I guess using conversions or one-off helpers like `genericLength` is the best you can do.
"generics-sop" might work well in this case.
That's the second time I've read about the "Haskell &amp; graph algorithms" problem today!
Only length is problematic here. The other two are polymorphic.
What I had in mind saying brute force is generating a list of all candidate solutions and checking all of them. Think of eight queens puzzle or "send more money" puzzle. In Haskell, simply use list monad and you're done. Very simple, clear and there's little room for mistakes. In C++ I usually ended up with an ad-hoc solution, involving recursion and passing partially initialized candidate back and forth, some parts of it being overwritten and some reused. That was before I knew Haskell, now I no longer attempt such stunts. Also note than focusing on small test case makes sense only when the time is running out, otherwise you would attack the hard problem. Thus time pressure is immense and you need to cut every corner you can. No time to engineer solid solution.
Occasionally I'll bitch about the annoying verbosity of 'fromIntegral', then I remember the three times I've spent 2+ weeks debugging implicit conversions in C and decide it's not so bad... 
On the other hand, often automatic conversion between some set of types would be very useful. I think the natural solution for this conflict is to make auto-conversions (including numeric literals) configurable on the module-level (or even more locally), similarly as we now can configure default strictness on the module level.
[removed]
Automatic numeric type conversions are a nightmare in C. Even when you don't care about losing information due to results smaller than the operands, you do never know it a = 1; b = 2; c = a/b; will make c = 0 or c = 0.5 until you look around on the rest of the code. Python gets away with most of the problems by having only 2 easy to use numeric types, the equivalents of Haskell's Double and Integer. It's not so good for speed, so there is a huge load of runtime optimizations to make them viable, and they still don't manage to make calculations fast. I do think Haskell got the best solution possbile. Yes, it's boring to keep putting fromIntegral everywhere, and once in a while you also have to specify a type, but you will never make a calculation that you don't know the result, and it will be as fast as possible.
Type aware completion sounds nice, so I looked it up and it is actually advertised on the [intero page](https://commercialhaskell.github.io/intero/) as a planned feature. Currently 9 times out of 10 I find autocompletion just really annoying and end up disabling it (the popup obstructs other code and I haven't gotten around to remapping the complete key from ENTER to something else, so often I accidentally auto-complete). I also think it pays of less for Haskell, because Haskell identifiers tend to be a lot shorter than in C#, Java, etc.. An advantage of text editors like Vim and Emacs, is that most of your editing skills are language independent. So I may not have fancy code generation, but I can easily write it myself some snippets. But the generating a new method command sounds cool.
Type-aware completion would help _tremendously_. 
In addition to Ed's points, it isn't just Haskell that objects to this. Even a language as different from Haskell as Go won't do this conversion for you automatically. There's also a unit error of a sort going on here, in the physics sense of units perhaps moreso than Haskell types. If `length [1, 2, 3]` is returning "the number of elements in a list", what does `length [1, 2, 3] * 1.5` really _mean_? What is 4.5 "elements in a list"? You could have some fun defining some mathematical meaning of that (mathematicians _love_ [taking things that seem like they could only be integers and making them reals](https://www.youtube.com/watch?v=gB9n2gHsHN4)), but back in the real world with your real RAM and your real CPU it doesn't have meaning. There really fundamentally is a type error here, and part of what you're doing with that `fromIntegral` is explaining to Haskell what you want, clearly, because all of "round up to int", "round down to int", "round to int some other way", and "do some sort of erroring out" are also viable options that you may want, and you need to be clear which you're asking for.
&gt; I also think it pays of less for Haskell, because Haskell identifiers tend to be a lot shorter than in C#, Java, etc.. Sure, it's obviously more critical in "noisy" languages like C#/Java but that wasn't what I was going for. The key is that auto complete won't show anything that isn't valid so after a while it's like having the type checker in your brain. As soon as you start typing and the auto complete won't offer what you expect your brain already starts working on why that is. &gt;But the generating a new method command sounds cool. Yea, that's just one of the awesome refactoring things you can do. The refactoring Resharper can do for C# is already quite amazing (e.g. turning many nested loops into a single Linq statement) but there is a limit because it's C#. I would expect much more advanced refactoring to be possible in Haskell because the IDE can know so much more about the code. At some point I would expect to e.g. switch between MTL and Free with a key sequence (and this includes changing all code in the project that uses the interfaces).
As Haskell is pretty much the only one I use know, it is the only one I can use without needing the doc (except of course for external libraries). Written on a napkin it wouldn't probably compile the first time but neither with any of other languages I used to use.
Very nice! I scrolled through the video with sound off, but I wonder how deployment works. I see that the FP Complete docker images are provisioned using propellor, and this creates a nice 1-layer image with no cruft. The command line used, and how the docker container is created is somewhat impenetrable though.
Doubles do not subsume Int's. Things like order, ring structure etc of Doubles are just half truths where as Int's they are real truths. There for auto conversion from Int to Double is a really bad idea.
Also if you think of OverloadedString, which has a similar purpose (I want to write HTML within source code), might be fine for Text or similar types but is definitely not good for types where the conversion is not an injection. 
I think it is the `Int` that is the half-truth here. `(maxBound :: Int) + 1` is a lie on my computer. `fromIntegral (maxBound :: Int) + 1` is perfectly correct. What rule does `Double` break for the subset of values that are actually Ints? A compiler could replace all use of `Int` with `Double` and be perfectly compliant, except that there would be no wrap-around (if that's required semantics). 
If you're going to be converting ints to doubles with `fromIntegral` in any case, where is the really bad idea?
Airthmetic operations of Double do not satisfy the laws that they are expected to satisfy (commutativity associativity etc - the ring laws to be precise). On types like Int, Word they hold (modulo 2^Wordsize of course) . 
I don't really understand everything here, but do I see a flame war? :)
Of course, if you can solve hard problem, you get easy one for free. But sometimes you can't, because, well, it is too hard, or there's not enough time left. Solving small test case only will still earn you a few points.
&gt; What rule does Double break for the subset of values that are actually Ints? One of the obvious examples: 1 + 1 + 2^53 \= 1 + (1 + 2^53)
Note that Int in Haskell is 64 bits so this is not true. Doubles only give 53 bits of precision 
We have the equivalent of `OverloadedStrings` for numbers by default. This is more like being unable to concatenate a `String` and a `Text`—behavior, I think, everyone finds reasonable.
That's great! The main takeaway I would want to give is that these path algorithms become nice and easy to express in a purely functional setting once you got the right combinator going. (For efficiency reasons, you might then want to go and implement the combinator using mutation behind the scenes (and hidden away in a ST Monad or so, so that the mutation doesn't escape).) Implementating Djikstra's​ algorithm and similar directly in Haskell without a nice combinator is otherwise a bit of a disappointment. Even using something like the state Monad, the code wouldn't look nicer nor easier to reason about than the imperative equivalent. 
OCaml actually makes more distinctions than Haskell, including separate operators for integer and floating point arithmetic (ie `*` vs `*.`) and different literals (ie `1` vs `1.`). The only reason `6 / 2` has the same type as the other two examples is that `/` is *integer division*—same as Haskell's `div` function. So `5 / 2` is `2`. `5 /. 2`, on the other hand, is a type error.
You could also define some operators to do this for you. It's still explicit so you get all the benefits of not automatically converting numeric types, but the code is much more readable: i *. f = fromIntegral i * f f .* i = f * fromIntegral i I've found this is the best tradeoff for code that does a fair amount of multiplication like this.
At this point I should mention that `div 6 2` in Haskell has another, fourth separate type :-) I think the most sane way to build a number system for a non-systems language is to have only bigints and doubles, with distinct literals and a unified set of arithmetic operators accepting a mix of both. (That said, I agree that `div` should be separate from `/`.) Anything beyond that seems like overengineering to me.
oke, So something like this : data IdPainting = Painting { id : String } data DataPainting = Painting { description : String title : String year : Date } data ImagePainting = Painting { url : String } class (DataSourceName, StateKey req, ShowP req) =&gt; DataSource u req where fetch -&gt; call to the api to get the ids -&gt; call to the api to get the data -&gt; call to the api to get the image 
&gt; The parallelism will be taken care of for you. Your `DataSource` will be called with several requests at once for you. You still need to make the actual requests work concurrently: https://github.com/facebook/Haxl/blob/master/example/facebook/FB/DataSource.hs#L83-L88
That s what I do as well except I only use one operator which do both x *. y = fromIntegral x * fromIntegral y 
I think I know what you mean. I used visual studio 15 years ago so unless things have changed since I think I have an idea of what they do. I'm not saying IDEs are useless but only that I don't think they give an advantage in fast paced competition like GCJ. Put it in other words, when I did it, I didn't feel I was missing out by having to type `ap` instead of tab after having typed `m` ... It's not like you are using complex API that you are discovering, you pretty muchbonly use an "extended" prelude.
A *lot* has changed in 15 years. Again, I'm not talking about Visual Studio alone. As far as I know, it's not all that great even now. But when you include the 3rd party plugin "Resharper" (from JetBrains) it becomes something amazing. &gt;type `ap` instead of tab after having typed `m` Well, you wouldn't be typing tab, you would type `m`, `map` would fill in, you would type space, then you probably get a template for a lambda, followed by functions in scope that could be used in map in the auto completion drop down (so long as it's not more than 5 or so).
You're right; the pending-request-batching is taken care of for you. I misspoke.
All product types have corresponding coproduct types: data Foo = MkFoo { foo :: Int, bar :: Int } data CoFoo = Foo | Bar If `Foo` type is a *record*, then `CoFoo` is a *corecord*. A corecord, loosely speaking, is a datatype which represents a field of a record. We can use a value of that corecord to project a value from the record: access :: CoFoo -&gt; Foo -&gt; Int access Foo = foo access Bar = bar If the record has varying types, then that function becomes a good bit trickier to write! We need to make it a GADT: data Foo = MkFoo { foo :: Int, bar :: Char } data CoFoo a where Foo :: CoFoo Int Bar :: CoFoo Char access :: CoFoo a -&gt; Foo -&gt; a access Foo = foo access Bar = bar You can use Template Haskell to get some compile time verification that `Foo` and `CoFoo` are in sync. I have a function that takes a simple sum type and generates the corresponding record type. Now that you've got a reified data that represents your indexes, you can just write a function `present :: Language -&gt; CoFoo -&gt; Text`.
The Haskell answer to automatic type conversions is type classes. Instead of automatically converting values from one type to another you create a class that describes the operations you were going to do with the converted values, and then create instances of the class for all the types you were going to convert.
I would parametrize your data types and use generics. type family ($?) (a :: Maybe (* -&gt; *)) b where ($?) Nothing x = x ($?) (Just f) x = f x newtype FieldName x = FieldName { getFieldName :: Text } deriving IsString data GenericRecord f = GenericRecord { firstName :: f $? Text, age :: f $? Int } type Record = GeneticRecord Nothing type RecordFieldNames = GenericRecord (Just FieldName) englishNames, frenchNames :: RecordFieldNames englishNames = GenericRecord "First Name" "Age" frenchNames = GenericRecord "Premier nom" "Âge" ....
It doesn't look like I'm the only one who would love some additional clarifying context. My first impression is that there might be a better way to structure your code than multiplying list lengths by fractional numbers. I know I'm scratching my head a little trying to figure out what that would mean from a [dimensional analysis](https://en.wikipedia.org/wiki/Dimensional_analysis) standpoint.
Thanks, I can also do there the json part so the part where I take out the json that I need or only the wreg request to the api
How can you do static analysis over mtl applicatives? Say you have three Web requests which are each IO, in mtl they're sequential, but in Free you could interpret them into a parallel IO.
The OP's problem is that `3.5 * length [1,2,3]` unexpectedly doesn't work. The reason is that Haskell uses a halfway solution with overloaded literals, which enables `3.5 * 3` but not `3.5 * length [1,2,3]`, throwing people off. (Another problematic case is `3.5 * div 6 2`.) One alternative is automatic promotion, which would've solved all these cases while being easier to understand. And even if you don't like that idea, I think it would've been better to somehow make arithmetic work as expected before making it extensible. Though admittedly that's a very high standard!
[removed]
[removed]
Use Common Lisp! ( ͡° ͜ʖ ͡°)
If you're adventurous, it might be interesting to see some kind of abstract number type using ExistentialQualification and building conversion rules into the Num instance. You'd still need to promote the return values of things like `length` to your new type. As other's have pointed out, there's a cost, but it might make a decent /r/haskell post for us to see somebody explore that cost.
/u/cmov
&gt; &gt; &gt; News to me. Where did you find that? https://www.fpcomplete.com/blog/2015/08/stack-docker 
&gt; Haskell uses a halfway solution `genericLength` works. It's the base library that exhibits this particular "problem", not the language.
It's true that it would be more natural if there was a dedicated command for this sort of thing. But what would the semantics be? I can't think of any substantive difference between the semantics of this command and repl. I'd be quite interested to hear if you know of some. Re tools, aren't these just executables? Cabal (the underlying library) has been slowly growing support for declaring these more precisely; for example, see https://github.com/haskell/cabal/issues/3708
Rickrolling, still? O L D
Who cares? `(maxBound :: Int) + 1 &lt; (maxBound :: Int)` And remember that `maxBound` is some 2^n *where you do not know what n is*!
No `Int` in haskell is something that wraps around at some 2^n where n is in the range [29...]. You have no clue when it wraps around without explicitly bringing `maxBound` into all of your calculations.
And yet, I'm still waiting for something like ocsigen in Haskell. 
So what you are saying is that they are still correct that it is not safe to just throw `Double` in whenever there is an `Int`.
https://hackage.haskell.org/package/base-4.9.1.0/docs/Data-Int.html `Int` is guaranteed to be safe for the range `[-2 ^ 29 .. 2 ^ 29 - 1]` which is a pretty darn big range.
If we're talking about `Double` as "whatever else", why? If you use Int in vehicle dynamics simulations, I'm pretty sure that you don't rely on modular arithmetic. My guess is that if you ever wrap around with an Int, your program will misbehave. Thus, if you get close to the range where Int wraps around, and this is where the semantics change, you wouldn't be able to see a semantic change in the behavior of your program if there was an implicit `Int -&gt; Double` conversion happening. 
Are you seriously trying to argue that `Double` is an all around better type and a drop in replacement for `Int`? Because that is honestly ridiculous, I mean first of all what about extremely useful functions like `div` and `mod`? Then you have the fact that floating points in general are just quirky, `==` is not reflexive, addition and multiplication are not associative etc. Now you are correct that if you are very careful to never use operations that actually deviate from the integers then you should get the correct results, but why throw away that safety. Most of the time I actually do want exact integers and don't need the extra power of floating points, and a lot of the time I care about correctness and will use a rational or similar instead. I would be very pissed if I was randomly getting doubles thrown at me for no reason than "flexibility".
&gt; News to me. Where did you find that? By manual inspection. You see stuff like this which is interesting in an archeological sense, but not easy to interpret. `bash -c ex port DEBIAN_FRONTEND=noninteractive;if ! dpkg -l libgmp10 &gt;/dev/null;then apt-get update; apt-get in stall -y --no-install-recommends libgmp10;fi;/builder/.stack-work/install/x86_64-linux/lts-5.14/7.10.3/b in/propellor-config "$@" bash (BuildConfig {bcProgName = "stack-image-builder", bcCwd = "/home/manny/sta ck-image-builder", bcLtsSnapshots = fromList [(LtsSlug {unLtsSlug = "lts"},LtsSlug {unLtsSlug = "lts-7.0 "}),(LtsSlug {unLtsSlug = "lts-0"},LtsSlug {unLtsSlug = "lts-0.7"}),(LtsSlug {unLtsSlug = "lts-1"},LtsSl ug {unLtsSlug = "lts-1.15"}),(LtsSlug {unLtsSlug = "lts-2"},LtsSlug {unLtsSlug = "lts-2.22"}),(LtsSlug { unLtsSlug = "lts-3"},LtsSlug {unLtsSlug = "lts-3.22"}),(LtsSlug {unLtsSlug = "lts-4"},LtsSlug {unLtsSlug = "lts-4.2"}),(LtsSlug {unLtsSlug = "lts-5"},LtsSlug {unLtsSlug = "lts-5.18"}),(LtsSlug {unLtsSlug = "l ts-6"},LtsSlug {unLtsSlug = "lts-6.17"}),(LtsSlug {unLtsSlug = "lts-7"},LtsSlug {unLtsSlug = "lts-7.0"}) ,(LtsSlug {unLtsSlug = "nightly"},LtsSlug {unLtsSlug = "nightly-2016-09-16"})], bcLayer = MetaLayer, bcR ebuildParents = False, bcPushMode = NoPush, bcPushTagSuffix = "20160916_2251", bcShowPhonyTargets = Fals e, bcJobs = 4, bcLocalInstallRoot = ".stack-work/install/x86_64-linux/lts-5.14/7.10.3", bcReleasesPublic ToolsDistDir = Nothing},Target {targetVariant = BasicVariant, targetRepo = MinRunRepo, targetLts = LtsSn apshot {ltsMajor = Lts7, ltsMinor = 0}, targetLayer = MajorLayer})`
I mean `3` is fine, it is as general as possible `Num a =&gt; a`. It is sort of necessary to have `6/2` have a non integral type, because `/` is supposed to be true division, not truncated division, so unless you either have implicit conversions (ruins type inference, and in this case would lead to loss of information for say `5/2`) or you have to prove at compile time that the two numbers divide exactly (undecidable in general case). You could also make `/` on integer types truncated division and regular division on non-integer types. But then you get different laws for `/` depending on the underlying type which a huge amount of people would have a problem with. `length [1, 2, 3]` is the only one I would argue could be improved, I think `length` returning a `Num a =&gt; a` would be perfectly reasonable.
`tool-depends` sounds great and at least very close to what I was trying to describe. I will have to play with it. One aspect which I don't think this covers is say I always use ghcid, perhaps it is required for my editor Haskell support. That tool doesn't belong in the project cabal file but in my local config (`~/.cabal/`?). I won't speculate any further without trying it.
Do you not see how disastrous anything like that would be for type inference? `Add` wouldn't even have any amount of injectivity, as `Add a b = c` with `c = Double` and `b = Double` could still `a = Int` or `a = Double`. You either are asking the compiler to just try its best and let the types lie to you, and hardcode some `Double` and `Int` a bunch (no one in a million years will approve that change). Or you are suggesting using non-injective type families and multi param type classes for basically all arithmetic. Which is IMO not completely awful, but also should not be the default, as a few `fromIntegral`'s is relatively painless. Now I do support making length return a `Num a =&gt; a`, but I think that is plenty to help with this issue. Now if you wanted to do the type family thing you could do it all yourself in a library, and I would welcome you to go ahead, it is definitely doable, I just personally would not use it.
As far as I’m concerned, it’s a bug that `explicit` isn’t the default for single-parameter constructors. For new code, it could easily be fixed by adding an `implicit` modifier (like C++11 `override` and `final`) with a warning for use of implicit constructors not marked `implicit`, but that would break old code, which is Not Allowed. :(
I have never personally run into that issue, as generally when using `div` I am not really messing around with floating points. However with that said if I did want that to typecheck, it would be by expanding `div` and `mod` to work on non integral types. as ``3.5 `div` 1.2`` could reasonably return `2.0` and ``3.5 `mod` 1.2`` could reasonably return `1.1`, now the specific laws and such would need a bit of thinking, but probably wouldn't be too hard. Really `toInteger` is the only thing in the `Integral` class that truly needs to not be given a floating point argument. Now I would personally if I could start over I would redefine a ton of the numeric tower, largely based on various group-like and ring-like structures, but my complaints with the current one are not the lack of implicit conversions. A non injective type family based library as an alternative to the more standard way of dealing with numbers would be perfectly OK with me, and is doable right now, but I personally wouldn't use it.
I was actually going to use this trick for ghc-mod when I rewrote the Cabal support but it turned out we needed more fine grained control over which GHC flags to retrieve. The overhead of re-reading in the then relatively large setup-config file didn't help either :/ 
How do `div` and `mod` work fine? They currently don't work at all `div 3.5 1.2` does not typecheck. And if you did want to implement them for floats they would be SLOW, whereas integer based `div` and `mod` are pretty damn fast. &gt; Doubleis the default implementation for all numbers is javascript, there is no integer type, and no javascript developer I know of are afraid of randomly getting non-integer numbers. I'm not sure how much time you have been involved with the Haskell community, but pro-tip, saying "JS devs don't complain about it", is just about the single worst argument to use in favor of something. And arguing that `length` should return `Num a =&gt; a` is reasonable, and I would support it, but only if you actually stop there and do not even consider implementing the type family based implicit conversion stuff. Because IMO it is not worth it, losing backwards type inference and implicit conversions and potential loss of information are all pretty annoying. Also in case this is what you meant, arguing that `length` should return `Double` is very unreasonable, and should absolutely not be done. Some people do want to work with `Int` (I still think most do, it performs well and supports all the operations you want efficiently, and overflow is rarely a problem in practice)
&gt; allows you to defer thinking about semantics until later Can you elaborate? (Can't MTL-style do the same, and why isn't the decoupling an advantage?)
Nice. We were going to add something like this to the `lens` package at one point. I'm not sure why it didn't make it in.
The point is that I haven't found any formulation of Applicative that would require strength. This requirement seems to have come from nowhere.
&gt; How do div and mod work fine? They currently don't work at all div 3.5 1.2 does not typecheck. &gt; &gt; And if you did want to implement them for floats they would be SLOW, whereas integer based div and mod are pretty damn fast. 3.5 % 1.2 in javascript is defined as `let x = (3.5 / 1.2) in (x - (floor x)) * 1.2` (maybe I misrepresent it a bit for negative numbers). Works fine. And talking about *SLOW*, you should really measure the performance of `div` because it's one of the worst punishments you can give your CPU. It takes *42-95* cycles on Skylake and completely stalls it - it can only do 1 such operation at a time, while floating point division takes *13-14* cycles, but the CPU can do *3 in parallel* bringing the throughput down to *4* cycles. `Double` `div` is up to *20x* faster than 64-bit integer division. That's not all, as `Double` division is supported by vector instructions, making it possible to do 4 of these at the same time, making the average throughput *1* cycle per division. All in all, almost 100x faster than 64-bit integer division worst case.
Nobody who uses my code need to care about the promotion of types that happen in the code I write. So shouldn't I be able to say that I like this particular tower that I've built, and it works for me? 
Additionally, to specifically answer your conjugate question: `conjugate` is inherently a complex number operation. The reason the example you gave doesn't work is this: [Complex numbers (even conceptually) don't have a total ordering on them](http://math.stackexchange.com/questions/487997/total-ordering-on-complex-numbers). Finding the maximum means there *must* be an ordering. Though Haskell's type system detects this inconsistency and reports it, this particular thing is independent of Haskell: it is a result of how the math works out. A language might have implicit conversion, but I've given reasons why you might not want that in my other post (and there are other resources I can pull up to expand that further). I would imagine things could get confusing if you allow an implicit conversion between something that doesn't have a certain operation and one that does have that operation. You'd certainly be poised get more runtime errors.
I left comments on github.
Sorry, was and am again on my phone, so we'll see how this goes... Yes, MTL style can do the same things, but uses different features of the language. Free has you put the syntax in a type definition and the semantics in its interpreters. MTL style has you put the syntax in a type class and the semantics in its instances. So they are related, for sure. However, Free satisfies no properties other than the monad laws. When you are using a free monad, you are usually assuming more about the behavior of the interpreter than just the monad laws. It's annoying because there is nothing sensible to "attach" laws to other than the interpreter, which you encounter only after you have defined everything you are about to interpret. It's not even guaranteed that the interpreter you are expecting is the one that will be used. Contrast with MTL style: you have a type class to attach additional laws to, and all code using it may, by convention, assume those laws hold, because by convention the instances are supposed to be carefully written to satisfy those laws. "But hold on," one might say, "if the free monad's type definition is analogous to the MTL style type class, why don't we just 'attach' the laws to the type definition and require that all interpreters satisfy the laws?" Unfortunately, it's a lot less clear what an "interpreter" is than what an instance is. There is no syntactic distinction between an interpreter and any other function, nothing to remind implementors, who may not even be thinking of it as an "interpreter", that satisfying laws may be required. So in practice most users of Free monads either have to dangerously assume their code is going to be used in specific ways or else they must be literally thinking of their code as a sequence of instructions to be interpreted in whatever arbitrary way some other code wants instead of attributing any sort of semantics to it, which is pretty crazy. Edit: Of course, the mtl library itself *doesnt* give many (any?) of its type classes laws. I think this is just because it's sometimes difficult to come up with good laws, not because it's not desirable. 
I think your points about representation are correct, but a bit off topic. It's certainly true that members of Z, R and C need different representations in the machine. But all of my threes have static types that guarantee they will fit in Z, the most restrictive of the bunch. They're not different from each other in that regard. The question is why they should be different w.r.t operations involving other values from R and C. In other words, I'd be fine with any stance on implicit conversion as long as it's consistent. For example, if Haskell says that `3.5 + 3` is okay, then `3.5 + div 6 2` should also be okay. There's just no math in existence that says otherwise. The same goes for the complex stuff, if `3 + conjugate 1` is deemed useful for engineering, then `maximum [3] + conjugate 1` should be deemed useful for the same reasons.
You're welcome to go build whatever ad hoc system you want if you only care about a closed world of a handful of types and n is manageable. The key word there is 'closed world'. That it doesn't play well with others or scale as you extend it, so it doesn't make sense to build the language around it. With the way the numeric tower works in Haskell today someone can take an expression type from a library that someone wrote and use it with the AD type from a library I wrote and use the result to compute symbolic derivatives of code someone else wrote. Nobody needed to know about all the different types involved, except maybe the person at the end 3 libraries later whom imports all those things and writes one line of code, but 99% of the code that I write along the way isn't written by the "final consumer" who knows exactly what types are in this little closed world. If you are only writing concrete code for working with concrete types and never need to accept code that works on class constraints like `Num` or in the most general possible manner then there is all sorts of stuff you could do. In fact if you turn on `RebindableSyntax` you can make a little C-like world in which you can do most of the things. Knock yourself out. On the other hand, the moment you need it to work with more general types you're looking at rebuilding it all, since such an approach doesn't play nice with functions that are just constrained by type classes. I for one don't find a lot of utility in the approach as I find it actively limiting to the things that _do_ interest me about this language, and if I want a numeric tower that works that way, I have pretty much every other language to choose from.
I'm not so sure about that: http://stackoverflow.com/questions/2550281/floating-point-vs-integer-calculations-on-modern-hardware Seems to say there isn't all that much difference between float division and integer division. Also are you sure that you will get exact integer values for floating point div/mod at minimum within [-2^29 to 2^(29)-1]? Also I would much rather get overflow (because it is usually much more obvious and explosive) than some slight errors that may or may not cause problems far from the source.
This is just the type system at work providing us with flexibility that doesn't require a sacrifice of safety. 3 :: Num t =&gt; t maximum :: (Foldable t, Ord a) =&gt; t a -&gt; a conjugate :: Num a =&gt; Complex a -&gt; Complex a div :: Integral a =&gt; a -&gt; a -&gt; a length :: Foldable t =&gt; t a -&gt; Int &gt; Note that I'm only interested in those threes that are statically known to be integers… … then you must disregard _all_ of your examples, since none of them are known to be of type `Integer`, though all will satisfy an `Integral` constraint (at least, not exclusively). The `Num` and friends constraints might be a source of confusion for beginners at Haskell, but IMO not for long, and I find this solution far more aesthetically appealing than, say F#'s approach of requiring a suffix for any non-`int` literal, or Ruby's approach of allowing addition between the literal `3` and an HTML document.
Summary: it's complicated!
But not for all ranges of `Int`, so it's an unsafe assumption to make.
Do any languages keep their bootstrapping chain so it can be reproduced easily?
&gt; I have a function that takes a simple sum type and generates the corresponding record type. Got a link?
Just... wow. That is perseverance right there. Respect :)
what an odd thing to be blocking on. what is timer_create() being used for in GHC anyway?
Well, to be honest I used it once a while ago and I assumed it worked also on all Num using id . I probably didn't have a use case when it didn't work. Anyway it might be worth to have a typeclass which could do the equivalent of fromIntegral on all Num type (when it makes sense obviously)
I haven't looked into "what" but here is a quick grep: rts/posix/Itimer.c:24: * timer_create doesn't exist and setitimer doesn't fire on iOS, so we're using rts/posix/Itimer.c:58: assume usage of timer_create function. The problem is that if we rts/posix/Itimer.c:62:ghc-stage2: timer_create: Not owner rts/posix/itimer/Setitimer.c:86: // option if timer_create() is not available. rts/posix/itimer/TimerCreate.c:40: if (timer_create(CLOCK_ID, &amp;ev, &amp;timer) != 0) { rts/posix/itimer/TimerCreate.c:41: sysErrorBelch("timer_create"); 
It is used in the runtime at startup, if I remember correctly. So many applications written in Haskell wouldn't work (ghc, pandoc, ...) I worked around it by working in a VM. 
I would love type aware autocompletion in haskell
This is awesome. I guess the scenario it should enable is developing on Windows but targeting Linux without having to run docker or a vm. Does anybody have that scenario? I guess you'd run an editor - emacs/vim/atom - on Windows and invoke the ubuntu stack/cabal? Does anyone have their editor doing that? Or does it make more sense to build on Windows from the ide/editor and run a separate Linux build from the commandline?
In a nutshell, no. Many of the other posters have made excellent arguments about things like type safety, so I'm going to provide a different point: conversions between reals and integers are *lossy*, in **both** directions. The Integer type can hold integers of arbitrary size, without loss of precision. Floats &amp; Doubles follow IEEE754, which means they are inherently imprecise. (And then there's Ints, which have limited size and therefore can't represent values of the same magnitude as either of these types at all.) Given that conversion between these types is lossy, you want to do it as infrequently as possible, so having syntactic overhead to discourage it is a good thing.
It's so funny to see Windows trying to catch up to Linux, after so many years of Wine unsuccessfully trying to emulate Windows...
SBCL (Common Lisp) started out as a bootstrappable CMUCL, and is now arguably _the_ CL implementation. It requires a fairly simple Common Lisp implementation. https://www.doc.gold.ac.uk/~mas01cr/papers/s32008/sbcl.pdf The commentary in the source code for boot.lisp is very interesting: https://github.com/sbcl/sbcl/blob/master/src/pcl/boot.lisp#L24 They implement "Portable Common Loops" which is a teeny subset of Common Lisp that they need, and this is able to interpret itself.
I meant more like in a naive language they all would have type Z, but Haskell makes them non-interchangeable. There's no argument why that's a good thing, but /u/cdsmith gave a historical explanation that I'm happy with.
It is well that you note that really existing `mtl` doesn't state any laws. Note further though that as soon as we try to replicate the sort of stuff de Goes is talking about we get, as [here](http://www.parsonsmatt.org/2016/07/14/rank_n_classy_limited_effects.html), stuff like: class Monad m =&gt; MonadHttp m where get :: Url -&gt; m ByteString post :: ToJSON a =&gt; Url -&gt; a -&gt; m ByteString type Url = String and, as [here](https://www.reddit.com/r/haskell/comments/3yuoky/how_can_mtl_truly_compete_with_various_extensible/), stuff like: class Monad m =&gt; MonadTwitter m where tweet :: String -&gt; m () I think you will have trouble with laws here too. 
To be fair, I always considered that conversion explicit. It's not written down, but comes from some very simple rules that are exactly like most other languages (AKA C) except where the other languages get it wrong (basically the integral and floating point (/) overloading). There's a lack of Double -&gt; Int conversion that I'd naively expect from the C inheritance, but it comes naturally from the syntax, so it's not really a problem. Yet, Python's optimizations aren't even completely transparent.
In the case of commutatitivity of addition it's not really a problem but I suspect that in other cases the presence of potentially infinite structures makes things pretty complicated. You can't reason by induction on them so the "proof" would not necessarily be valid. Edit : multiplication defined by matching on the first argument is not commutative for instance. `0 * m` returns `0` immediately but if `m` is infinite then `m * 0` never returns.
TBF, 3+HTML is definitely going to throw `TypeError` in Ruby... you may be thinking of PHP?
Probably. But... Can we write it in Haskell? Because doing that kind of thing in C would be very annoying.
We need productivity checking not totality checking because most Haskell data types are coinductive.
That didn't error with exception &lt;loop&gt;???
I've been wanting to use this rather badly, but am still stuck using Ansible because of silly policies against root ssh login, and unfortunately there was no support for going via sudo last I checked.
Something like this? {-# LANGUAGE MultiParamTypeClasses #-} {-# LANGUAGE FlexibleInstances #-} {-# LANGUAGE FunctionalDependencies #-} class Addable a b c | a b -&gt; c where add :: a -&gt; b -&gt; c instance Addable Integer Double Double where add a b = fromInteger a + b instance Addable Double Integer Double where add a b = a + fromInteger b instance (Num a) =&gt; Addable a a a where add a b = a + b It seems to work as expected with monomorphic literals: foo = foldl add (1 :: Integer) ([2,3,4] :: [Integer]) bar = add (1 :: Integer) (2.0 :: Double) So far it seems nice and explicit. Are there any subtleties I'm missing? What makes it worse than the default, except the requirement for monomorphic literals (which I'd consider a feature)?
Really, a language (GHC) extension for that would be interesting!
&gt;There's no argument why that's a good thing All in all, I believe it's a good thing that scenarios like the following one don't happen... n = 16 -- This is a promotable integer. q = div n 3 -- This value will be used somewhere else. -- ... r = q / 2 -- Thinko: I meant to use div here. z = 4.5 * r -- Oops. ... even if it means I have to slip in a `fromInteger` for the final multiplication. (Also, I wouldn't necessarily describe the point about user-defined types you and cdsmith are discussing in the parallel subthread as "historical". It's nice that the custom number types work under the same rules than the standard ones.)
Yes, I was clarifying what he seems actually to be up to. He thinks of the constructors of the functor as the main thing and already has in place a machinery for using them in two ways. If I understand what he's up to, if he was writing Haskell he would probably have a bit of TH to derive actions in `Free f`, `Free (FreeAp f)` and `FreeAp f`. (Sometimes it seems like what he's after is something like `Free (Coproduct f (FreeAp f))` - to use his terms.) Of course the free applicative do-blocks have to be lifted into free actions to be sequenced in `Free (FreeAp f)` blocks. But given this, everything is in place. In cases where he has an `optimize :: FreeAp f -&gt; FreeAp f`, like his filesystem example, then given a `sequentialize :: FreeAp f -&gt; Free f` he'll use `sequentialize . hoistAp optimize` before appyling `retract` and doing whatever he does to interpret. Or something like that. In cases where he wants concurrency given `act :: f r -&gt; IO r`, he can use things like ioInterpret :: Functor f =&gt; (forall r. f r -&gt; IO r) -&gt; Free (Ap f) a -&gt; IO a ioInterpret act = foldFree (runConcurrently . runAp (Concurrently . act)) Everything is in place, mad as it seems. I would like to see some complex code where he makes this look practical. It seems attractive if it can be made practical, I don't really see the point of comparing it with `mtl`. 
I may be misreading your point, but I fail to see the part that can't be done with mtl?
well, you could write a self-hosting Haskell compiler with an ANSI-C backend, and always include the compiled-to-C versions in the releases.
What do you mean? You don't need an extension. they already did it with existing extensions. This is the kind of thing that can be defined in a library. 
&gt; Furthermore, many useful rings violate this law such as those formed by modular arithmetic Which is even something everyone uses on a daily basis, since clocks are Z mod 12 or 24...
`Int` is modular arithmetic with negatives. Word32 is `ℤ/fromIntegral (maxBound :: Word32)`. Also, with dependent types modular arithmetic becomes particularly useful. For example, (!) :: ∀ (size :: ℕ) α. Vector size α → ℤ/size → α This makes it impossible to pass an index that is out of bounds of the `Vector`, which would allow Haskell to elide run time bounds checking. Haskell should stick to the best computational interpretation of mathematical abstractions as much as possible as those have centuries of research behind them rather invent our own ad hoc ones. 
here's a great [blog post](http://blog.sigfpe.com/2007/07/data-and-codata.html) about it
&gt; `10^1000 + 0.5` compiles without complaint, though. `10` isn't an integer literal, though. It's a *Num* literal.
Coinduction is not lazy data structures. It's call by name. Certain patterns can safely be lazy but this idea has to stop. 
There is exactly one embedding of the integers into the complex numbers that's homomorphic with respect to addition and multiplication. This makes for a very obvious choice; and indeed that's the choice that Haskell made, it's how `fromInteger` is defined for complex numbers.
&gt; What I really want is inductiveness polymorphism I thought about this recently when that paper on levity polymorphism was posted and had a chat with /u/edwardkmett about it over on [Facebook](https://www.facebook.com/groups/programming.haskell/permalink/1479033078774355/)
Interesting. Ok good so it seems I'm not getting anything wrong in my understanding than it seems. We could add this to Haskell with productivity checking and totality checking. Folds would simply require their input be "data". `fmap` etc would be polymorphic over it and `repeat` would return coinductive lists. I propose that every type be given this property and then have a syntax to explicitly specify it. Otherwise, just bidirectionally infer it from the code back into the type signature. It would be nice to have name optional type parameters for types like Agda as often I need to specify only one of the latter type arguments and type inference can fill in the rest for me. This would allow current list code to automatically gain the annotations and then we could have a strict mode that enforces them for transitional purposes. Being able to reason about totality should allow a lower risk to doing more optimistic evaluation while maintaining non strict semantics. Also, it would allow automatic unboxing in more cases.
It applies equally to both. Hence why I think type families would be a better solution. As far as I know everything that can be done with functional dependencies can be done with type families (but the converse is very much not true, even with things like undecidable and flexible instances to help out), and there are numerous advantages of type families, the one downside being that they can be slightly more verbose sometimes. Basically non-injectivity just means that you have to know the exact monomorphic type for both the inputs. Whereas with an injective type family knowing the output can tell you stuff about one or more of the inputs. For example all constructors are equivalent to injective type families. No more inference and no less inference: type family Tuple a b = c | c -&gt; a b where Tuple a b = (a, b) This will give you equal type inference to the `(,)` type constructor. Which is not to say they are equivalent, for example you can't make `Tuple` an instance of anything, as it isn't a type constructor, it's a type level function. 
That certainly is true. But WINE project never targeted to *finish* it. They keep making WINE "good enough" so that most software can run "good enough".
&gt;What I really want is inductiveness polymorphism. I hear that, in NuPRL, there is a subtyping relationship between corresponding algebraic and coalgebraic datatypes.
It goes into a fair amount of details. I liked it a lot.
&gt; Likewise I could ask - what is a 263 length list? No such thing exists. The type can hold values that would never be relevant as a return value for length. Sure it can. `[1..2^65] :: [Integer]` is a perfectly cromulent list with more elements than that. `length` on it will wrap around to 0 after a _very_ long time. If you are _only_ calling `length` and nobody else is holding on to it, it will even be produced lazily and not exhaust all your memory. Your assumptions about what fit in memory need to be reworked when you are working in a lazy setting. But remember, `length` works on anything `Foldable`, and its very easy to produce containers that fold just fine that are longer than that, and the calculation can even be O(1) for many containers. We have `genericLength` for producing arbitrary numeric types, but its pretty awful, as it uses that numeric type during the length calculation not just at the end. (That handles the [1..2^64] case properly, but `fromIntegral . length` is faster for anything of sane lengths.) Now the real argument to court is that `length` should be `Natural`. ;) But we didn't have a `Natural` type in the language spec and working with unsigned lengths tends to be a recipe for disaster when doing length comparisons. Folks aren't careful enough about which side of the &lt; they do their additions/subtractions on and tend to underflow unsigned calculations. This is why `Int` rather than `Word` is used despite the fact that we can't use half the output range. &gt; Doubles subsumes Ints up to 56-bit long, so it is a more general type Right up until you need to store a 64 bit pointer in one on a 64 bit platform. They each can hold values the other cannot.
That's a great idea. The only thing still missing would be an ANSI-C compiler in GHC Haskell. But in a more serious aside, that's indeed a great idea.
While I agree that productivity checking could be tremendously *more* useful, I'm not sure totality checking couldn't be valuable itself.
[removed]
That doesn't entirely avoid the Reflections on Trusting Trust problem. A smart enough subverted compiler could hide its evil parts in obfuscated C code. Which is why the article notes: &gt; Some GHC re­leases in­clude files con­tain­ing gen­er­ated ANSI C code, which re­quire only a C com­piler to build. For most pur­poses, gen­er­ated code does not qual­ify as source code. 
&gt; I think using 3 as a value of user-defined type is overrated I'm not going to argue your opinion, but I can tell you why I like polymorphic literals. The definition of haskell's literals in terms of`Num`, `IsString` (with `-XOverloadedStrings`) and `IsList` (with `-XOverloadedLists`) make them more useful, and constructing values from outside the base libraries easier. This helps level the playing field between libraries and the core language. Don't like the performance of `String`? Switch to `Text`. No need to decorate all your constants with `fromString`, just add a `LANGUAGE` pragma at the top of the file. Plus, they appeal to my sense of fun. :) I like that I can define an instance of `Num` for `Num a =&gt; b -&gt; a`, so that I can do λ loeb [ (!!1) + (!!2) + (!!3), 1, 2, 3 ] [6,1,2,3] ([loeb](https://github.com/quchen/articles/blob/master/loeb-moeb.md) - beware nerdsniping)
You have a point. It's possible to start from endofunctors in an arbitrary monoidal category C. In that case, a lax monoidal endofunctor would not give you a natural transformation pure::a-&gt;f a. You need strength for that. I will edit the post to make this clear. On the other hand, you can start with a lax monoidal endofunctor [C, C] and port its definition to Haskell. You end up in Set (modulo bottoms) and there you get pure for free. One could argue that it's because of canonical strength, or because of self-enrichment of Set. So it's pretty arbitrary how you back-port a Haskell construct to category theory. Starting with endofunctors won't give you the equivalence with Day convolution though, which I consider crucial. My comment about endofunctors was related to the fact that strength can only be defined for endofunctors. Closed functors can indeed go between closed categories.
Say what you will about PHP, you can't overload operators in it. You can in Ruby. At runtime. &gt; 3+HTML is definitely going to throw TypeError in Ruby... Unless someone's tinkered with `+`. Or `method_missing`.
thanks for the note. The first is correct -- that path is not for where the _platform_ is installed, but is the default target for cabal to install binaries that are configured `--global`. The second is also correct -- that path is not for where _cabal_ is installed, but rather for where cabal run under the installing user's account would install local binaries. The third is one that we should probably look at -- there's a bit of a mismatch because the platform installer as a whole is a systemwide installer, but it calls out to the stack installer the stack team bundles, and that installer is intended to be run as a single user. If you want to file an issue over this, you may want to go to the tracker: https://github.com/haskell/haskell-platform/issues 
It would be entirely possible to introduce anonymous records as a proper language feature, using row polymorphism. However, it would be a lot of work to design and implement, and I think it would lead to more awkward design questions, rather than fewer, because one would have to figure out how the new records interact with Haskell's existing records. I'd love to see a proposal for proper anonymous records in Haskell. But I've spent too much time in the records swamp and have gone native, so it needs someone else who is foolhardy enough to actually do it... ;-)
Actually - are you sure it sets stack\_root to the admin directory? I checked on my system and its "C:\sr" and I don't _think_ i set that manually, though I haven't yet checked. Additionally, it appears that the stack documentation itself recommends setting it manually? https://docs.haskellstack.org/en/stable/install_and_upgrade/#windows (also, in the latest release, doesn't it let you pick the install location for stack?)
Yes, because the 10 is being inferred to be a Double. Try `(10 :: Integer) ^ 1000 + 0.5`
Okay, I see the first one. That's probably why it was recommended that installer be run with Admin. But the second one suffers the same problem as the stack installer, as I ran installer as Admin but will run the toolchain as another normal user, when I don't have access to the PATH of the Admin. 
Ah, if your primary issue is lack of trust, then it's a different question, yes. Even though the compiler I had in my mind would emit clean C code, not obfuscated; also I don't really think going through the motions described in the blog post (which does not explicitly mentions trust issues) would give you much trust either. If you aim for trustlessness, I would start with clean state, with a very simple (and relatively high level) interpreted language, something a like a really simple lisp or forth dialect (for which anybody mildly competent can write their own interpreter), and then gradually build up a series of more complex interpreters/compilers from that. Each generation could be both interpreted and ideally compiled to some simple (common?) bytecode VM (and maybe C) by the previous generation. If you are really paranoid, make multiple compilers for each generation of these languages and compare the outputs, etc. But it's a lot of work, and of course you cannot trust commodity hardware (or any third-party hardware, really) anyway...
This is still technically incorrect. This clearly depends on the particular denotational semantics. In fact, it is often valuable to have a denotational semantics whose congruence relation is equivalent to the congruence relation induced by an operational semantics.
Now I see what you and Edward Kmett have been saying all along, basically that Integer + Double = Double is bad for type inference because it's supposed to be bidirectional. But just to make sure I get it, can you give a simple example where type inference fails for `Addable` but succeeds for `Num`, without using polymorphic literals? I thought something like `foldl add` could be such an example, but it typechecks and the type makes sense.
For that I would say that instead of a conservative totality checker, but a conservative non-termination checker. I can definitely see the purpose of totality checking on my entire program, but most of my bugs tend to not lead to non-termination, and it sounds a little restrictive, but I would totally support an infinite loop checker.
&gt; That's one of the major points of monads. I disagree. I see the major point of monads as wrapping a computation in the first place; enabling the neat separation of IO. But the syntactic sugar you're talking about is IMO just a way to make poor architecture feel less painful to the coder.
You can save a few characters and avoid importing a module for `runIdentity` like this: f x = ($ ()) $ do x &lt;- pure (g x) pure (x + x) Not that I think this is so great, but... just sharing anyway.
http://stackoverflow.com/questions/11921683/understanding-stg https://ghc.haskell.org/trac/ghc/wiki/Commentary/Compiler/GeneratedCode http://blog.ezyang.com/2011/04/tracing-the-compilation-of-hello-factorial/
Will check it out and play with it, thanks!
The decision is made by the core but the participation is vast. The Rust RFC process is inspired by Python's PEP, and the concept has proved success in both communities. 
&gt; … you may be thinking … You give me too much credit, sir. &gt; … throw `TypeError` in Ruby. However, that implies a _runtime_ fault, which I would expect, but which doesn't help me if that circumstance happens to be part of the state space I didn't test. :-) More precisely, "the Ruby compiler will accept as a valid program" ?
Hi, haskero has the same goal then ghc-mod. But it's backed up by another tool called intero. Thanks to this tool, I can't get ride of most ghc-mod issues. So you just have to install haskero alone. There are still missing features, but we are closing the gap :-)
Wowsers, haven't seen *that* site before! This is probably the neatest ("modern" aka non-native-desktop-app) way I've seen that could easily and "as-if-accidentally" turn non-coders (or dem young uns) into *functional* functional-programmers! Brilliant work.
Instead of using the GHC API, and the StgSyn module directly, you can also opt for the route we've taken with our Clash (Haskell-&gt;Hardware) compiler. That is just use the GHC API to get the initial `GenStgExpr` for the expressions you are interested in, and convert them to your own data structure. The benefits for us were: * We could use a bound name library of our own choosing (http://hackage.haskell.org/package/unbound-generics in our case), and not deal with how GHC has chosen to do its free variable calculation, capture-free substitution, etc. * We have a stable API; because, even though the data types in the GHC API stay the same over many versions, functions over those data types change/disappear between minor releases (especially if you're dealing with types). We work with Core instead of STG, but you can take a look at our code that converts GHC's Core to our own Core here: https://github.com/clash-lang/clash-compiler/blob/master/clash-ghc/src-ghc/CLaSH/GHC/GHC2Core.hs#L232 
It would be quite cool to code the build steps up in nix and thereby capture this bootstrapping build in a reproducible form. 
You know what, I'd rather watch a talk with informed heckles and well-considered responses than something more in the style of a lecture. On the other hand, I feel bad for John, the heckles were certainly quite robustly thrown. I'm certainly not convinced by either side (yet) but I feel more informed and I appreciate John's taking the flak in such a constructive way.
There's no single "right" answer for this. It all depends on how strong the relationship is. If it's *essential* and part of the core of your application itself, then it should probably be a field.[1] If not, it's probably OK to just give every Person a UUID and use lists/sets of those to indicate followership. [1] Do note, however, that recursive structures like this can be a tad tricky in Haskell if you define them in situ. Look up: "Tie the knot".
&gt; the way you described with 'following' excludes the possibility of mutual followership. The second way repeats data. Does it? When I want to make single followership possible (I follow you, you don't follow me) isn't this the most simple way? Of course I could use some kind of polymorphism, but this should be ok too
Most of the flak he is getting is coming from one of his co-workers.
And another one https://github.com/dmvianna/haskellbook
So you have full forward type inference. Just no sideways or backwards inference. So if you only ever start with monomorphic values (and only ever use functions that return monomorphic values) then you should be ok. The issue is that such a restriction is well... restrictive. Polymorphic values / functions that give polymorphic outputs that aren't constrained by the input type are very common in Haskell: mempty pure read maxBound minBound Are a few examples within just the prelude. And then of course there are basically all conversion functions. And as I was talking about earlier, having functions like `length` or `sizeOf` return a polymorphic value and having functions like `!!` and `plusPtr` accept a polymorphic value can be very nice. As it is generally more convenient (IMO) and it can give performance benefits where doing the whole computation with a different type is cheaper than last minute casting, and you can always design the polymorphic function to do a last minute cast when there won't be a performance benefit. One other downside of your approach is it tightly couples several of the number types, and is a fairly opinionated approach. So if someone cares about precision more than performance and wants to use a rational instead of a float when dividing two integers then they are kind of screwed. As you already defined division on ints to return a float. You would basically have to define your own type families and typeclasses and operators when you wanted to disagree with the opinion of the default type family. Which might not be uncommon at all. 
If that is the limit of your concerns then you can get away with a simple ToDouble class and just run with it. class ToDouble a where dbl :: a -&gt; Double default dbl :: Integral a =&gt; a -&gt; Double dbl = fromIntegral instance ToDouble Int instance ToDouble Integer instance ToDouble Float where dbl = realToFrac instance ToDouble Double where dbl = id I do agree the length of the name `fromIntegral` and complete unrememberability of `realToFrac` are annoying (shouldn't it be `fromReal` anyways?).
It's also called mutual recursion
You seem to be trying to implement everything at once - how are you going to test any part of your code, when the file won't compile at all because of all the half-defined functions? You'll want to comment out all of the unfinished parts until you get to them, so the only thing *not* working is the part you're working on right now. For the parsing: `parseBoard` is supposed to return something of type `Board`, but right now, it returns `[[String]]`, as far as I can tell. When implement it, follow the types. Your input is of type `String`. When you split on the "/"s, you get something of type `[String]`. You then want to further split on the ","s (using `map`), which gives you something of type `[[String]]`. Now, to get a `Board`, you need to turn those `String`s into `Square`s, to get something of type `[[Square]]`, which you can then use your `Board`-constructor on. You seem to already know how to turn a "w" or "b" into a `White` or `Black` - both `parsePlayer` and `parseColor` do this, but they use the wrong type names (maybe you changed the type from `Color` or `Player` to `Figure` at some point). What happens if the `String` is empty, though, if there's no "w" or "b"? Can you implement a function, `parseSquare :: String -&gt; Square`, which does the job? Once you have that function, you can use `map (map parseSquare)` to turn your `[[String]]` into a `[[Square]]`. I hope this was helpful - the trick of following the types is pretty useful in Haskell in general. Good luck with the project! :)
Haskell (and GHC) are particularly good at allowing you to do this. (+), (*), etc. are all "normal functions" that you can shadow or otherwise redefine, not built in syntax. You can even invert precedence if that make your code easier. With RebindableSyntax, you even get to substitute in your own `fromInteger` and `fromRational` for processing literals. One nice advantage of `(+) :: a -&gt; a -&gt; a` instead of `(+) :: (PromotesTo audend add, PromotesTo addend add, Additive add) =&gt; augend -&gt; addend -&gt; Sum add` (where Sum is an associated type family of the Additive type class) is that type inference is trivially bidirectional in the former, but is difficult to achieve in the latter.
&gt; It takes 42-95 cycles on Skylake and completely stalls it - it can only do 1 such operation at a time, while floating point division takes 13-14 cycles, but the CPU can do 3 in parallel bringing the throughput down to 4 cycles. I had no idea it was still this bad. I mean I knew I could still surprise people with benchmark results by just taking `%` out of their C code, but I expected it to only be 3-5 as slow as multiplication, and certainly not to stall *every* integer unit. :(
I think GP meant when you are dealing with the [-2^53, 2^(53)-1] intersection of `Int` and `Double` all the expected properties apply to either type.
I use [quickbench](http://hackage.haskell.org/package/quickbench) to compare performance more easily. It generates tabular reports, which you could graph with something else.
Using `unsafeCoerce` to prove a point about higher level things such as the semantics of the language is downright silly: ghci&gt; data Foo = Foo1 | Foo2 deriving Show ghci&gt; data Bar = Bar1 | Bar2 deriving Show ghci&gt; unsafeCoerce Foo1 :: Bar Bar1 ghci&gt; unsafeCoerce Foo2 :: Bar Bar2 Therefore by your ridiculous logic `Foo1` is the same value as `Bar1`... [] :: String and [] :: [Integer] have the same **binary representation**, but they are NOT semantically the same value, AT ALL.
I used unsafe coerce because I don't have a way in Haskell to do heterogenous equality. Also, how do you distinguish values from their runtime representation?
&gt; I used unsafe coerce because I don't have a way in Haskell to do heterogenous equality. Maybe that is because Haskell does not have such a thing as heterogenous equality, because two things of different types are semantically NOT THE SAME VALUE. &gt; Also, how do you distinguish values from their runtime representation? By their type... and the fact that you can't even compare them without using `unsafeCoerce`.
You distinguish them fairly trivially by the fact that any attempt to compare them without **UNSAFE**coerce does not compile. Are you saying that unless you slow down the performance of the entire Haskell language by refusing to erase types just to "double check" that things are different at runtime, that somehow things of different types are the same if they happen to have the same bitwise representation? Notice that (under that definition): 42 :: Double is equal to 4631107791820423168 :: Int because `unsafeCoerce` plus `==` says so: ghci&gt; (42 :: Double) == unsafeCoerce (4631107791820423168 :: Int) True
I'm trying to get you to define you terms in a way that doesn't require assuming the Haskell way is the only way. I think the term "value" is used much more broadly that just the Haskell community so it deserves a definition that doesn't assume "the Haskell way".
But we are talking about Haskell, on /r/Haskell, and discussing potential changes to Haskell. So it is pretty thoroughly implied that we are talking about *haskell* values. Now that we have established that, `3 :: Int` and `3 :: Double` are different values according to Haskell. And you didn't really answer my question, are `42 :: Double` and `4631107791820423168 :: Int` the same value?
Okay, so if you want "value" to mean in-memory representation, then you are correct. You have also missed the conversation, though. This thread was not asking about having too many in-memory representations for values that mean 3. It was about semantically distinct values. In this context, it's difficult for me to imagine which non-Haskell meaning makes sense.
I have a [quick hack](https://github.com/bgamari/criterion-compare) that I use to compare CSV output from Criterion runs.
If there is any package that you'd like added to the `lts-8.x` series, open an issue on https://github.com/fpco/lts-haskell/issues. Now that lts-8.0 has released, I've disabled a lot of packages on the _nightly_ build plan in order to accommodate the latest versions of `vector`, `servant`, `blaze-html`, and `aeson`. Affected packages with these as direct dependencies were notified, but a lot of packages with these as transitive dependencies were not. See [the commits](https://github.com/fpco/stackage/commits/master), or the diff once the next nightly is published.
`add` can't infer the types of `a` and `b` from `c`, which prevents type inference working in reverse
I just wanted to say that I am super excited about Hercules. I have thought about implementing such a beast more than once myself. But then I remember that I already maintain too many projects.
&gt; it's necessary for the category Hask to make any sense! I actually think this is more an issue with how morphisms in Hask are inherently monomorphic -- because categories only allow morphisms from a single source object to an signgle destination object, and objects in Hask are types -- and that is just (yet another) way that Hask doesn't quite match up with Haskell.
I didn't mean that all integer units are stalled, but there is no pipelining for integer division. 
Nice, thanks for sharing this!
A very nice project, but to me, so disappointing. There is *still* a huge need for Haskell, not a "Haskell-like language", compiled to the JVM. EDIT: That is not correct. It's Haskell. See below.
Thanks! I was mainly responding to /u/drb226's comment that &gt; Affected packages with these as direct dependencies were notified, but a lot of packages with these as transitive dependencies were not. My suggestion is that the affected packages that transitively depend on `aeson` etc. should be notified too. --- &gt; I'm pretty sure the curators do their best to add back the transitively dropped packages, but it's a semi-manual processes so it's possible some get missed sometimes. It's good to hear that the Stackage maintainers are already taking care of this. Can we help automate these processes though? Is it mostly the lack of a way to automatically manipulate YAML files while preserving formatting and comments that's holding up further automatization of the Stackage maintenance processes?
I second you on that. However there is a easy(-ish) way of replicate the OP pattern just using either IORef or STRef. The problems is you end up having IO everywehe. Strangely IO everywehe doesn't seem to bother OO aficionados, but bother Fp programmers which claims OO is better in that particular case ...
You have basically 3 ways of dealing with this : Tying the knot. Works well for creating static structure but doesn't work at all when you need to update values. So even though it is often the recommended way, it doesn't work in practice. The second way is to have a concept of Id and somewhere a Map Id to Person. It works but it is somehow equivalent to rolling down your own version of pointers which all the maintenance problems which comes with it. You need to make sure each Id have an entry in the Map and that each Id in the Map correspond to one Person. However it has the benefit to be compatible with some graph libraries ... If you really want pointers then STRef is the way to go. It has all the benefits of traditional pointers without the null pointer problem. However the types (and the doc) are a bit complex. Sometimes, you also realise that actually don't need the relationship to be embedded in the data and realise that having `Person` and `[(Person, [Person])]` is actlually enough. Obviously it doesn't work if the same person have to appear multiple times in the graph so it works better the key and the values have different types.
&gt; What I meant is that it's a never ending game of cat and mouse. Welcome to tech. Or perhaps, welcome to human nature =) it's not something that just began with Wine/Linux/Windows.. or even with cats and mice!
All packages in stackage have a maintainer, which can be separate from the maintainer which is mentioned in the cabal file or on hackage. This is so that people other than the official maintainers can add pacakges to stackage *if* they are willing to update dependencies (eg. by making pull requests in the original repository and/or nudging the original maintainer to update dependency bounds on hackage). So stackage can not spam people who are not interested in about stackage issues. In other words, stackage is opt-in rather than opt-out.
True. I'm not trying to bash Haskell though. I'm trying to figure out the implications of a different design, which no language has AFAIK. There's something fishy with type classes, as evidenced by defaulting rules and `show . read`. The non-interchangeability of different [threes](https://www.reddit.com/r/haskell/comments/5t3wwn/types_of_threes/) seems like a related problem. Maybe there's a simple way to make all these go away, the question is how much important stuff would be lost compared to Haskell. For example, what if we banned typeclass polymorphism from happening only in the positive position? (E.g. `show` would be okay but `read` wouldn't, and overloaded literals and `fromInteger` would go away.)
Well, Monad was nice while it lasted: return :: Monad m =&gt; a -&gt; m a This is the kind of thing that gives these 'forward-only inference' languages fits. `do` notation gets a lot of mileage out of flowing constraints up to the different statements. e.g. Monad transformers die a swift death under your regime as well.
Yeah, polymorphic code would have to be explicit: mconcat :: Monoid a =&gt; [a] -&gt; a mconcat = foldr mappend (mempty :: a) -- Funny syntax but you get the idea And yeah, I can see how it will get hairy with monad transformers. Thank you!
&gt; Obviously it doesn't work if the same person have to appear multiple times in the graph so it works better the key and the values have different types. What do you mean by this? 
One of the things I love about haskell is the brevity and lack of repetition. Also the `mconcat` example you just gave won't work without `ScopedTypeVariables` and an explicit forall.
Thanks! I'm excited about it too. One nice feature we're hoping to implement is a standalone cli program which will build your application exactly as the CI server will, no more long edit-&gt;push-&gt;wait loop while trying to get CI set up.
Do you mean data A = A | B | C vs. let a = (B, C) ?
Well, this is just an example. You can imagine another effect instead of Writer and my question would still be valid. Ideally I should be able to switch between an efficiently logging implementation and a `Writer`-style implementation.
[removed]
No, because data A = A | B | C is more like an Enum (it carries no information, just a data constructor), whereas data A = A B C -- or replace it with something sensible data A = A String Int does not utilize the polymorphism of sum types, but is basically identical to type A = (String, Int)
Ok, I was simply a bit confused because the syntax in your original question is a bit funky. I think you've got it right. Typical sum types like `Bool` simply have several nullary constructors while tuples are typical product types. I think you can argue that `Either` and `(,)` are the fundamental sum and product types constructors because you can construct an isomorphism of any classical Haskell type with these two building blocks – at least when you also have [`Void`](http://hackage.haskell.org/package/base-4.9.0.0/docs/Data-Void.html#t:Void).
If you want to read further on algebraic data types you can look at one of these articles: * http://chris-taylor.github.io/blog/2013/02/10/the-algebra-of-algebraic-data-types/ * https://bartoszmilewski.com/2015/01/13/simple-algebraic-data-types/
I *think* you're misusing the term "sum type." `data A = A B C` is not a sum type. That is a product type. A sum type would be `data A = A | B | C`, which yes, looks like an enum. All product types can be encoded with tuples, and all sum types can be encoded with `Either`. data Product = A B C -- Same as: type Product = (B, C) data Sum = A | B | C -- Same as type Sum = Either () (Either () ()) data ProductSum = A B C | D E F -- Same as: type ProductSum = Either (B, C) (E, F)
The difference is pattern matching, `data A = A String Int` would be equivalent to `("A", String, Int)`.
Why isn't the `snap` package included (`snap-core` and `snap-server` are in)?
Yeah, I guess that's what /u/edwardkmett said. Probably makes sense - I don't know enough about the limits of type inference to argue.
I've tried to use this for image rotation as an alternative to Imagemagick. Unfortunately it was too slow for me. I tested it on x86-64, but eventually would want to run it under GHCJS. I would love to see a fast image rotation routine. The image rotation code I looked at seemed to use a lot of dynamic dispatch, and possibly slow data access. Alternatively a GHCJS backend using the canvas, or even WebGL. Probably not the target for your library, but that's my use-case anyways.
According to https://github.com/fpco/stackage/issues/1766, snap was already missing from nightly in August 2016 and added back in October, [only to be dropped again with the move to GHC-8.0.2](https://www.stackage.org/diff/nightly-2017-01-17/nightly-2017-01-25). From https://github.com/fpco/stackage/issues/2203 it looks like heist hadn't been updated to be compatible with directory-1.3 sufficiently quickly so it was dropped together with its reverse dependencies. And no one has added it back yet…
Fair enough. I'll edit the article a little bit and repost it later.
If the "sufficiently smart" compiler is free to arbitrarily chop up and rearrange the fused loop body into precisely as many loops as it wants to though, that will be ideal, so your premise is not entirely wrong but it's more complicated than what you presented. :)
This sounds very promising; it feels intuitively right that the expression of these relationships should have an elegant type-level formulation, but I have neither found nor been able to approximate one. Let's hope this is it!
I'm not saying that the popular implementations of ORM in today's popular languages are good (my Django scars to bear witness); it is more that I can see a way to bridge OO and RDB which respects the intent of both systems more clearly than in Haskell. More bothersome is the fact that Haskell has superlative means to express complex and nuanced relationships; given this, and having seen win after win in other fields, I was chagrined to see an apparent lack of accomplishment in this (for me) common case. But it looks like there is indeed work afoot, and given that this is possibly a much harder problem to do right than first appears, I wholeheartedly support waiting until we have a correct approach.
This was [asked four days ago](https://www.reddit.com/r/haskell/comments/5sovad/is_there_such_a_thing_as_higherranked_constraints/). See the responses there for the answer.
This is exactly the desugaring of typeclasses so aside from being able to explicitly provide implementations to functions (intead of instance resolution doing it for you) you don't really have other advantages. When you eventually have to make a `Query m` value it's exactly the same as implementing an instance. So for your advantages: * mtl classes can also work for any monad, just provide an instance for it * implementing instances and making `Query m` values is the same thing * you can just provide test instances * you wouldn't need lift with mtl classes
Right, that's what I thought. But I don't see the advantage of going the mtl route. And I think I still have to use `lift`? Let's say my `Query m` needs to carry a Connection: data Query m = Query { getThings' :: m [Thing] , sleepSome' :: Int -&gt; m () } makeQueryIO :: Connection -&gt; Query IO makeQueryIO conn = Query (myGetThings conn) threadDelay vs {-# LANGUAGE FlexibleInstances #-} import Control.Monad.Trans.Reader import Control.Monad.Trans.Class class Monad m =&gt; Query m where getThings' :: m [Thing] sleepSome' :: Int -&gt; m () instance Query (ReaderT Connection IO) where getThings' = ask &gt;&gt;= lift . myGetThings sleepSome' = lift . threadDelay The first approach looks way simpler and more flexible to boot. It's also easier to make `Query` values for specific test cases since you can do it inline.
Yeah sure. You can make "instances" that capture some existing context which you don't really get with typeclasses, but now you have to pass this implementation around manually. Not to mention the order-of-parameters hell when you start having more than, say, 4 effects in your functions. Then you probably want to start using implicit parameters, of which I'm personally not a fan of. I still think the best approach to this problem is an extensible effect system.
&gt; Not to mention the order-of-parameters hell when you start having more than, say, 4 effects in your functions. Then you probably want to start using implicit parameters, of which I'm personally not a fan of. Sorry I don't follow, can you explain? &gt; I still think the best approach to this problem is an extensible effect system. I'm reading about this now.
When you call other functions you're gonna have to give them your implementation values. If they require mutliple effects then you need to give a value for each effect, in the correct order. Nothing forces you to order your effects in the same way in the same function so you're almost guaranteed to have to look it up every time.
I made the lisp version more lispy (and also more haskelly): (defun onerights (n) (when (&gt; n 0) (let ((b (+ a 1)) (sq (+ (expt a 2) (expt b 2))) (c (isqrt sq))) (if (= (expt c 2) sq) (cons (list a b c) (onerights (- n 1))) (onerights (- n 1)))))) 
Hm, what's the point of this? Homework? Comparison? oneright :: Int -&gt; [(Int, Int, Int)] oneright n = [ (a, b, c) | a &lt;- [1..n], let b = a + 1, Just c &lt;- [isqrt (a*a + b*b)] ] isqrt :: Int -&gt; Maybe Int isqrt n | sq_n * sq_n == n = Just sq_n | otherwise = Nothing where sq_n = floor . sqrt . fromIntegral $ n N.b. reddit can format code — indent it by 4 spaces. --edit-- Oh yeah, this `isqrt` is not right. Description "why" is [here](https://wiki.haskell.org/Generic_number_type#squareRoot). Fixing it is left as the homework for the reader ;)
Check out [this article and its Reddit comment](https://www.reddit.com/r/haskell/comments/4std0v/rank_n_classy_limited_effects/) from several months ago. It touches on some of the same ideas
Right. So, going all the way down the rabbithole this probably doesn't scale that well. `ImplicitParams` could solve the problem of passing the context around in effects but doesn't look particularly great. {-# LANGUAGE ImplicitParams #-} needsData :: (s :: App m) =&gt; m () logs :: (?s :: a m, Logs a m) =&gt; Text -&gt; m () 
Naive quasi-literal translation (the substitute for `isqrt ` here is [accident-prone](http://stackoverflow.com/a/10865580/2751851), as kgadek also notes): import Control.Monad (guard) oneRights :: Integral t =&gt; t -&gt; [(t, t, t)] oneRights n = do a &lt;- [1..n] let b = a + 1 sq = a^2 + b^2 c = (floor . sqrt . fromIntegral) sq guard (sq == c^2) return (a, b, c) A sound solution, using [*arithmoi*](http://hackage.haskell.org/package/arithmoi-0.4.1.3/candidate/docs/Math-NumberTheory-Powers-Squares.html) for calculating the square root without round-tripping through `Double`: import Data.Maybe (maybeToList) import Math.NumberTheory.Powers.Squares (exactSquareRoot) oneRights' :: Integral t =&gt; t -&gt; [(t, t, t)] oneRights' n = do a &lt;- [1..n] let b = a + 1 c &lt;- (maybeToList . exactSquareRoot) (a^2 + b^2) return (a, b, c) If the range of `Int` is adequate for your purposes, you can get a significant performance boost by specialising the signatures to `Int`.
Yeah, this looks like the best way to do it; basically make a `runIOToMonadStack :: Member MyMonadStack r =&gt; Eff (IO ': r) a -&gt; Eff r a` to "smoosh" the IO into MyMonadStack. (assumes `MonadIO MyMonadStack`) The real question is: can you write `compressMonadStack :: (MonadTrans t, Monad m, Member m r) =&gt; Eff (t m ': r) a -&gt; Eff r a` to compress *any* component of a transformer stack into another one. I'll try to do this sometime soon, but something tells me that [layers](https://hackage.haskell.org/package/layers-0.1/docs/Documentation-Layers-Overview.html) might allow this, or some other mtl-like library that uses associated types for the inner monad.
In my old hask code you can curry constraints in the category of constraints and use natural transformations/ends over them to handle universal quantification. It needs some updating to work post ghc 7.8 though. For this particular example `Lifted Monad t` -- would work using the Constraints package, but that weaker version requires manual instances.
Warning: You need a better definition of `Category` for this to work out right.
Thanks for the reply, is there something I could do to get `snap` in lts8 now? I don't like being stuck in lts6
While we do *try* to add back packages, it is a manual process and very much best effort. Pull requests are still a more timely and preferred way to get your package back in. After a new major LTS release (when we tend to do major version update pruning) is probably a good time to check your package is still in Nightly. In normal circumstances we don't remove packages from a LTS release. (edited) The Distributions field in Hackage for a package may also help. I agree that we need better notifications around Stackage package removals, but it is not trivial to do.
&gt; probably with Snap May I ask why?
Yesod looks a bit too monolithic and the other options look a little too immature/light. I'm open to considering other options though.
&gt; Nobody ever got fired for choosing PG.
Take a look at Spock. As a WAI app you can host your websockets and site app together on the same server for some juicy realtime connectivity.
There's a system call in the Haskell runtime that is missing an implementation. https://github.com/Microsoft/BashOnWindows/issues/307
That recently has been fixed.
&gt; While we do try to add back packages, it is a manual process and very much best effort. Pull requests are still a more timely and preferred way to get your package back in. After a new major LTS release (when we tend to do major version update pruning) is probably a good time to check your package is still in Nightly. Thanks for clarifying! &gt; The distribution field in Hackage may also help. I don't understand what you mean here. Can you expand a bit on this? BTW thanks a lot to you and the other Stackage maintainers for providing this service to the ecosystem! I don't think everyone realizes this but my impression is that Stackage helps _a lot_ in detecting dependency conflicts and coordinating their resolution. Pretty much every Haskell user benefits from this, it's not limited to stack users.
I guess the first step would be to get `heist` and any other missing dependencies of `snap` back in. [Here](https://www.reddit.com/r/haskell/comments/5plnlc/stackage_design_choices_making_haskell_curated/dcsqp39/)'s a recent thread where someone explains how they help keep some packages on Stackage that they didn't author themselves. Maybe you can work out a similar arrangement with the `snap` maintainers.
Wait, what do you mean with the "neural networks" bit, related to image processing? Are you just referring to discrete convolution?
Yea, this applies except when you want to call fuctions in IO, then you need liftIO.
&gt; What we really need is... I'm not sure we really want this, from a software engineering perspective. Functors that aren't endofunctors on Hask are rarely ever needed, and can still be represented in Haskell just in case. Having them all unified under one type class destroys type inference and error messages; a cost that I think vastly outweighs the benefits. But, as I said, this is only from a software engineering perspective. From a Category Theory perspective, it would be quite nice =P
Btw, if you want to use cpphs as a library, take note that it is licensed under LGPL and you should either provide your source or use dynamic linking.
Guessing what you're likely to want to do, I'd suggest the following: data Person = Person { name :: String, age :: Int } -- The phantom type here lets you represent say, Key Person and Key Tweet, as Integers without mixing them up. -- You can just have type Key = Integer if you prefer. newtype Key a = Key Integer followership :: Data.Map.Map (Key Person) Person On first glance, this looks more complicated but it's not that much more complicated. But as soon as you hit a database or start talking to the outside world, it's actually simpler. 
Are they hiring interns?
&gt; Functors that aren't endofunctors on Hask are rarely ever needed They are only as necessary as the ability there is to express them! Many monad transformers are [functors in the base monad argument](https://hackage.haskell.org/package/mmorph-1.0.9/docs/Control-Monad-Morph.html). Tuples and `Either`s are bifunctors. This type class captures that nicely. There are various notions of [indexed monads](https://personal.cis.strath.ac.uk/conor.mcbride/Kleisli.pdf) that can provide greater type safety and flexibility (polymorphic state monad). `Sets` are functors in the category of functions that respect substitution for a the element type's `Eq` instance. Functors from `Kleisli Maybe → Hask` allow us to talk about a notion of filtering. The type class I mentioned above doesn't even let us talk about that notion, so even it isn't flexible enough. If linear/affine types are ever added to Haskell, there might need to be a way to talk about endofunctors in the category of linear functions. There are plenty of uses. &gt;can still be represented in Haskell just in case With code duplication :( &gt;Having them all unified under one type class destroys type inference The definition above doesn't do much to damage type inference because both the functor's `Domain` and `Codomain` are determined from it. Even if it did harm type inference, that might not be too bad because type inference is a coincidence anyways. &gt;error messages There should be a way to customize the type error messages. If we are going to be pushing more of our programming errors to compile time, there should be tools for debugging 'm and throwing type errors with custom messages. 
&gt; Not to mention that acid-state requires all your state to fit in memory We recently switched to SQLite (using `persistent` as the interface) from acid-state because of this. Do not use acid-state if your data will grow in time. Migrations are harder too, because usually `acid-state`-like libraries couple internal types with storage-layer types so you have to migrate much often, or you won't be able to change your internal types.
[GHC works now](https://blogs.msdn.microsoft.com/commandline/2017/02/09/haskell-on-bashwsl/) but you need the latest dev build of windows. Also, stack installs MSYS2 so no need for cygwin. You can drop into shell by ``` stack exec mintty ```
I use this in an application I'm working on. I like it more than using mtl-style typeclasses or using a free monad DSL. I'm not sure if it's actually as expressive as these other two options (it might be), and I don't know how one would go about demonstrating that, but it's been able to do everything I've asked of it so far.
How do you do logging? What happens when you have effects that depend on other effects? Can you give an example?
Interesting. Can you provide links to some Domain Driven Design materials that illustrate this approach?
There are many firms that are still WIndows-only. Trying to get a Linux or Mac box in there equals death by red tape.
The following errors are from using let instead of let* and from using the variable, a, without defining it. Warning: Syntactic warning for form A: A assumed special. Warning: Syntactic warning for form B: B assumed special. Warning: Syntactic warning for form SQ: SQ assumed special. 
Yes, it's a language comparison. In Common Lisp, isqrt has no such problems. Therefore, for it to be a fair comparison, the solution really should be in solid Haskell, without relying on any code that has any such problems. Thanks for the note about formatting code. I will try to remember that.
See [Morte](http://www.haskellforall.com/2014/09/morte-intermediate-language-for-super.html?m=1) and how to deal with recursive functions in a a language which does not support recursion. Very insightful. Gabriel had a nice talk about this as well. EDIT: talk [Internet of code](https://begriffs.com/posts/2015-10-16-internet-of-code.html). 
I can't run this from inside bash (in the bridge), though :'(
There are some changes. In particular, evaluation is strict by default. That already means that you can't hope to take existing Haskell code, compile it against standard libraries, and get the same or similar result. I think there are other changes. EDIT: That is not correct. See below.
Not just room - a significant need.
You could also just manually merge the effects into one. makeApp :: Log m -&gt; Query m -&gt; Debug m -&gt; App m Unfortunately if you have code that needs only two of the three effects you would either have to make the type signature too general with `f :: App m -&gt; m` or create a combined effect `logQuery :: Log m -&gt; Query m -&gt; LogQuery m`. I think the MTL style has an advantage here.
&gt; In particular, evaluation is strict by default. It's lazy by default (according to the home page, I haven't tried it yet).
You can also use "snd" instead of "($ ())"
This version uses Yitz's formula. I renamed it to lyrights, with ly meaning the Lisp version using Yitz's formula. (defun lyrights (n) (loop with sr2 = (sqrt 2) as a from 1 to n as b = (+ a 1) as c = (ceiling (* a sr2)) when (= (expt c 2) (+ (expt a 2) (expt b 2))) collect (list a b c))) (lyrights 160000000) took 2 minutes, 49 seconds, on my PC. ((3 4 5) (20 21 29) (119 120 169) (696 697 985) (4059 4060 5741) (23660 23661 33461) (137903 137904 195025) (803760 803761 1136689) (4684659 4684660 6625109) (27304196 27304197 38613965) (159140519 159140520 225058681))
When I say CNN what I really am trying to say is: a library feature that includes training of a neural network over a (dynamically sized) window and efficient application of that network to an image, obtaining a new image with pixel values of the resulting NN processing. Since I'm elaborating, when I say stroke width transformation (SWT) I mean: http://digital.cs.usu.edu/~vkulyukin/vkweb/teaching/cs7900/Paper1.pdf
Admittedly, CL has a *huge* stdlib. There is no built-in `isqrt`. The correct implementation is linked, so I'm leaving copy-pasting that stuff and wrapping into `Maybe` as the homework for the reader ;) --edit-- full solution with my minor changes: oneright :: Int -&gt; [(Int, Int, Int)] oneright n = [ (a, b, c) | a &lt;- [1..n], let b = a + 1, Just c &lt;- [isqrt (a*a + b*b)] ] squareRoot :: Int -&gt; Int squareRoot 0 = 0 squareRoot 1 = 1 squareRoot n = let twopows = iterate (^2) 2 (lowerRoot, lowerN) = last $ takeWhile ((n&gt;=) . snd) $ zip (1:twopows) twopows newtonStep x = div (x + div n x) 2 iters = iterate newtonStep (squareRoot (div n lowerN) * lowerRoot) isRoot r = r^2 &lt;= n &amp;&amp; n &lt; (r+1)^2 in head $ dropWhile (not . isRoot) iters isqrt :: Int -&gt; Maybe Int isqrt n | sq * sq == n = Just sq | otherwise = Nothing where sq = squareRoot n
To my understanding, this includes the floating point problem that I linked to in my previous comment.
If using stack: stack install arithmoi stack repl If using cabal: cabal install arithmoi ghci
Well, `TemplateHaskell` exists if you wan't to work with huge haskell AST. And `TH` is already some kind of macro system in Haskell though rather ugly. But I thought this work on extending GADT's can help simplify things and bring nice new modern replacement to `TH`: https://www.reddit.com/r/haskell/comments/5iovx9/trees_that_grow_interesting_paper_about_extending/ But I can't find ghc proposal with that issue...
Ah, nice, I keep forgetting we finally added that instance. 
Thanks.. I was looking at your code and saw weird constraints like `r ~ (s ': s')`. I'm currently writing `Eff` code and so far I've never had to add such constraints. Why did you need to add those?
Tell GHCi to load object code and turn on the optimisations (`-O2`): ghci -fobject-code -O2 GHCi&gt; :l aor.hs You can also build a stand-alone executable -- just make sure your `main` actually forces the whole relevant segment of the list to be calculated. $ ghc aor.hs --make -O2 It is also worth noting that, for some of the examples above, you can get a significant performance improvement by switching to more limited integer types -- that is, from the polymorphic `Integer t =&gt; t` (as in my own take) to "bignum" `Integer`, and from that to the fixed size `Int`.
You're right, I was wrong. My comments were based on [this reddit thread](https://www.reddit.com/r/haskell/comments/5cdgxa/ghcvm_the_ghc_haskell_to_jvm_compiler_was_renamed/) from 3 months ago. But much of what is in that thread was from comments taken out of context. In fact, the author is not transitioning to a different "Haskell-like language"; it's Haskell, but a subset of what GHC supports, favoring features that are widely used in practical applications. I'm very happy to find this out, and I thank you for correcting me.
Yes, there's one on the Part 2 linked here: https://vaughnvernon.co/?p=838 Look for where it says: &gt; Rule: Reference Other Aggregates by Identity I think I posted the wrong link elsewhere in this thread - I'll try to fix that. This isn't the only place where this advice appears online but for some reason it's the only one I can find today - my Googling skills seem to be having a bad day. Elsewhere in this thread, I link to a discussion about Entity Component Systems in game programming, http://t-machine.org/index.php/2007/09/03/entity-systems-are-the-future-of-mmog-development-part-1/. Now DDD and ECS aren't normally considered in the same breath but personally I see both of them as having a view that's more like Haskell's or relational algebra's than the conventional OO/ORM view of the world. I'd also include event sourcing as heading in the same general direction - again things needs to have IDs. 
Now I understand what you meant. I agree that this approach cannot really handle that situation very well. However, I have not worked with a DSL where pieces of the interpreter needed to refer to other actions provided by the DSL, so I haven't really needed the ability to express this.
No, it doesn't -- and your version is is also fine. The root of the problem kgadek mentioned is that `floor` and `ceiling` in Haskell's *base* library ~~return~~ take floating point numbers, and not integers (according to [this](http://www.lispworks.com/documentation/HyperSpec/Body/f_floorc.htm), their closest match in Common Lisp are `ffloor` and `fceiling`). The Common Lisp `ceiling` you use does return an integer, and yitz didn't actually use `ceiling` from *base*.
Thank you! Yes, the source got a little messy because there are several steps of encryption involved, and they are all contained in a single function. I could have split the function into smaller parts, and maybe I'll try to do that some day to write tests, but at the moment it seemed sufficient to isolate parts of code into blocks within the function.
Here's why: &gt; The "monomorphism restriction" is a counter-intuitive rule in Haskell type inference. If you **forget to provide a type signature**, sometimes this rule will fill the free type variables with specific types using "type defaulting" rules. The resulting type signature is always less polymorphic than you'd expect, so often this results in the compiler throwing type errors at you in situations where you expected it to infer a perfectly sane type for a polymorphic expression. https://wiki.haskell.org/Monomorphism_restriction 
People comparing languages don't know anything about modules, compiling, or anything. If they do the above, they'll probably get something like: The IO action ‘main’ is not defined in module ‘Main’ People comparing languages probably don't want to spend time learning the mundane details of each language's modules, compiling protocols, etc. They probably just want to copy and paste the code from a forum such as this, and follow the instructions they see with that code. E.g. "copy and paste this code into a file such as filename.hs then enter these commands: (list of commands). If they have to get more involved than that, there is a risk that they might never get around to including that language in their comparison, because there are a lot of languages to compare, such as Scala, Lua, F#, ML, Racket, Clojure, and hundreds of others. 
This is kind of sad though.
I don't believe that using `cpphs` to preprocess your source code makes your source code a derivative work of `cpphs`. Just like how using `gcc` to compile your code does not make it a derivative work of `gcc`.
You (and I) would think that, hence the "counter-intuitive rule" note in docs. All I can say as to why, without copy-pasting the section devoted to the topic in the Haskell2010 language definition, is "because it is how Haskell is defined". See the section for more info: https://www.haskell.org/onlinereport/haskell2010/haskellch4.html#x10-930004.5.5 Thanks for bringing this up, I've been curious but never bothered to look into exactly why. 
Sadly I'm just not smart enough to write lisp without getting runtime type errors. I actually prefer the "look and feel" of scheme to Haskell though, frankly I find Haskell "ugly" in a way (maybe mostly because of camelCase which I find aesthetically hideous... but obviously that's a trivial thing).
Seems you could just do isqrt :: Int -&gt; Maybe Int isqrt n | sq_n * sq_n == n = Just sq_n | (sq_n+1) * (sq_n+1) == n = Just (sq_n+1) | otherwise = Nothing where sq_n = floor . sqrt . fromIntegral $ n 
It's still not clear to me what functionality you have in mind, but CNNs require a whole host of dedicated machinery for training (feature manipulation, optimization etc.), something which I wouldn't expect a generic image processing library to provide. 
Mine [here](https://www.reddit.com/r/haskell/comments/5tpu5x/rewrite_short_common_lisp_code_in_haskell/ddoztve/) is at 0.08s for up to ten million. $ ghc -O2 test.hs &amp;&amp; time ./test [(3,4,5),(20,21,29),(119,120,169),(696,697,985),(4059,4060,5741),(23660,23661,33461),(137903,137904,195025),(803760,803761,1136689),(4684659,4684660,6625109)] real0m0.080s user0m0.080s sys0m0.000s 
Honestly, I think I'd do a simple monad transformer for this. {-# LANGUAGE RankNTypes #-} newtype QueryT m a = QueryT { unQueryT :: forall r. (a -&gt; m r) -&gt; m [Thing] -&gt; (Int -&gt; m ()) -&gt; m r } instance Functor (QueryT m) where fmap f (QueryT m) = QueryT $ \c -&gt; m $ c . f instance Applicative (QueryT m) where pure a = QueryT $ \c _ _ -&gt; c a QueryT mf &lt;*&gt; QueryT ma = QueryT $ \c g s -&gt; mf (\f -&gt; ma (c . f) g s) g s QueryT ma *&gt; QueryT mb = QueryT $ \c g s -&gt; ma (const $ mb c g s) g s instance Monad (QueryT m) where return = pure (&gt;&gt;) = (*&gt;) QueryT m &gt;&gt;= f = QueryT $ \c g s -&gt; m (\a -&gt; unQueryT (f a) c g s) g s instance MonadTrans QueryT where lift m = QueryT $ \c _ _ -&gt; m &gt;&gt;= c getThings :: QueryT m [Thing] getThings = QueryT $ \c g _ -&gt; g &gt;&gt;= c sleepSome :: Int -&gt; QueryT m () sleepSome t = QueryT $ \c _ s -&gt; s t &gt;&gt;= c runQueryT :: (Monad m) =&gt; QueryT m a -&gt; Query m -&gt; m a runQueryT (QueryT m) (Query g s) = m return g s It feels cleaner to me because you aren't explicitly passing around a manual dictionary.
I looked at the GHC Trac proposal. To me, supplying `cpphs` with source available online is the best option. I hope we can get this issue included in the GHC future discussions so we can reach a consensus and remove this long-standing annoyance.
Its a good point youre making. 
Silly thinko in my previous reply here: Haskell's `ceiling` *takes*, and not returns, floating point numbers exclusively (which is still a problem in this case). 
No it didn't. Mostly because it's not particularly interesting and doesn't really give you any extra tools, so you might as well just do the thing manually in your application.
Thanks - this is very helpful! 
Here is how I would translate that "word for word" so to speak: oneRights :: Integer -&gt; [(Integer, Integer, Integer)] oneRights n = reverse $ flip execState [] $ do for [1..n] $ \a -&gt; do let b = a+1 sq = a^2 + b^2 for (isqrt sq) $ \c -&gt; modify ((a,b,c):) Note: isqrt :: Integer -&gt; Maybe Integer 
Post your output, and info about your PC.
The author gave the explanation somewhere else (can't remember where) that Haskell is a very loaded word. It has a reputation of being very hard to learn, and to be unsuitable for real world applications (which is demonstrably wrong, but yet, this is widely believed).
I mean...that all might be true, but I still don't see how it justifies basically taking credit for developing a language that has been around for a few decades and worked on by hundreds, if not thousands, of language theorists. And I mean, really, like simply not calling it "Haskell" is going to somehow fool people into learning it? I don't buy it. To further increase puzzlement, in the actual documentation it makes it clear that Eta *is* Haskell, so in the end, if you want to actually learn Eta, the author directs you to go off and read up on Haskell. Sigh.
Yes. Start from [here](https://en.wikipedia.org/wiki/Pell_number#Pythagorean_triples) and do some napkin math.
While writing code for this you suddenly realized those could be used for this? As in, writing code helps you think math better? Did you ever have any use for them before? Or did you encounter them in school or something?
This works as a one-off, but I'd like to integrate stack into the rest of my toolchain/ecosystem that is built on the bridge :|
I have a math background and sometimes do mathy programming for fun. After taking a first stab at your problem in C++, I decided to explore the sequence numerically a little bit, and saw that the ratio between successive values of `a` approaches 5.828427. A quick Google search told me that's 3+2*sqrt(2). That kind of thing is a telltale sign of a linear recurrence, the same thing happens for Fibonacci. At that point I remembered vaguely about Pell numbers, that they are related to Pythagorean triples and also have some kind of recurrence. (I'd ran into that stuff long ago on Project Euler, but forgot everything.) Then I went checking on Wikipedia and OEIS and quickly found some formulas from which I could work out the right code. It's all quite simple really. I guess a pro would've googled for 159140519 and found the right formula on OEIS right away, finishing the problem in five minutes, but oh well.
Well I thought the listed ones are only those "not considered 'standard'" by Stack, but now that you mention it, `text` is in there too! That's of course pretty standard. Yeah it's (meant as) a long-term library for me so I'm "vigilant". But sooner or later I'll bite the deps-bullet *anyway*, realistically.. but not for line-wrapping `Show` outputs :D
I've written a basic tool for doing something similar: [format-hs](https://github.com/rdnetto/format-hs). It's not particularly lightweight (uses a Haskell parsing library), but since it's just an executable it doesn't require you to change your project's dependencies.
&gt; "Stack makes handling dependencies a non-issue" I hope no one has been saying this, because it's silly. Handling dependencies is a hard problem. What stack does is make the solution you come up with reproducible, which is a still a big deal. (By the way, cabal-install can also do this with `cabal freeze` and by recording the date for use with `index-state` later: http://cabal.readthedocs.io/en/latest/nix-local-build.html#cfg-field-index-state) Even then you still have system dependencies to deal with, but that's why we have Nixpkgs=)
It's strange that you had to add e.g. `text` to your `extra-deps`. Did you use a GHC resolver like `ghc-8.0.1`? Another possibility is that the library you installed had some dependency bounds issue that made it incompatible with the LTS or nightly resolver that you used.
I did! 8.0.2 in fact.. kinda abusing stack's `resolver` to keep my GHC updated
Nice! AFAIK stack currently doesn't guarantee reproducibility on the Hackage revision level. (There's some discussion on this [here](https://github.com/commercialhaskell/stack/issues/2217).)
Yikes, thanks for pointing that out! To clarify for anyone who's casually following along, the Stackage snapshots refer to a specific revisions of a package by its hash, so those are reproducible, but the references in `extra-deps` are not because they just take the form `foo-1.0.0.0` without specifying the revision used:/
One thing that is *really* important about Persistent's `SqlPersistT` monad is that it operates in a single transaction by default, which gets rolled back if any exceptions are thrown. This means if you have some long running code like foo :: SqlPersistT IO a foo = forever $ do liftIO . doSomeWork =&lt;&lt; doSomeQuery threadDelay 10000 It'll never commit, and will eventually error. You can use `transactionSave` to fix this, starting a new transaction. I feel like `foo = ask &gt;&gt;= \ctx -&gt; ...` is a bit of a smell. If you immediately `ask` for the entire context, then you're not really operating in the spirit of `MonadReader`, and might as well just pass it as a parameter manually. createGame game conn = do execute conn "..." game boardGameId &lt;- fromIntegral &lt;$&gt; lastInsertRowId conn game' &lt;- fromJust &lt;$&gt; readGame boardGameId conn pure (boardGameId, game') Alternatively, you can define helper functions for the API you're wrapping, like executeR :: (MonadReader Conn m, ToRow q, MonadIO m) =&gt; Query -&gt; q -&gt; m () executeR query args = ask &gt;&gt;= \conn -&gt; liftIO $ execute conn query args lastInsertRowIdR = ask &gt;&gt;= liftIO . lastInsertRowId which lets you simplify the above to createGame game = do executeR "..." game boardGameId &lt;- fromIntegral &lt;$&gt; lastInsertRowIdR -- ... 
That answers the question I was going to ask. With a snapshot resolver, you only need to list packages not in the snapshot in the `extra-deps` (in your case, you wouldn't need to put anything there, as your would-be dependency is in the latest Stackage snapshots, and consequently the same goes for all of its dependencies). That being so, I'd say the specific annoyance in your case of having to manage the huge `extra-deps` list has more to do with Stack being a snapshot-centric tool than with what the dependencies are. (By the way, if you have a list of `extra-deps` to manage, you can let Stack update it for you with `stack solver --update-config`).
Is it the same with SBCL and Lispworks?
Thanks for clarifying! I'll probably run through and update the code; if not, I'll certainly point out the point about transactions so as not to confuse.
If the function is total and the decreasing argument is known at compile time then you know that the invocation can be replaced by a finite unfolding of the function. Also, once the unrolling is done the body of the function is now instantiated with concrete values too which may make it possible to fire some additional optimisations. In an imperative setting, the typical example of this sort of thing is [loop unrolling](https://en.wikipedia.org/wiki/Loop_unrolling).
It seems strange that you only get 8 triangles with both Lispworks and SBCL, but with my old version of Lispworks I get 11. What do you get if you enter in the repl this expression? (= (+ (expt 4684659 2) (expt 4684660 2)) (expt 6625109 2)) Also see if this gives you 6625109: (ceiling (* 4684659 (sqrt 2))) 
A little more trivia about those dependencies: * With a `ghc-*` resolver, you only get out of the box the libraries bundled with GHC (i.e. *base*, *array* and the handful of others that GHC itself also requires). That doesn't include *text*, as you noted, as well as other similarly ubiquitous packages, such as *mtl*, *unordered-containers* and *vector*. * A good chunk of your long list of transitive dependencies is made of the abstract machinery that *lens* depends on -- *comonad*, *profunctors*, *semigroupoids*, *kan-extensions*, and so forth. (Some of them are also direct dependencies of *trifecta*, which is also an Edward Kmett package.) Odds are that before long you will be spotting the pattern as soon as these start to pop up on your screen after a `stack build` :) 
Care to link to the incoherence without orphans thing? Because to me that sounds like it might be a bug in the implementation of overlapping instances. As I assumed that they were considered sound and coherent when orphans are not allowed. I know that personally I would prefer a coherent overlapping instance pragma combined with no orphan instances to the opposite. I would also then want to look into approaches for defining instances in a way that doesn't incur dependencies / tightly couple libraries but that doesn't allow for any orphan issues. I still think having a library that is designated to be the "linker" between two libraries that defines instances that span the two isn't the worst idea. The developers for both libraries would have to agree on the linking package, and it would be enforced by the compiler. So if you had both libraries you would not be allowed to define orphan instances, but you could optionally import the linking library, and the instances would be brought into scope when either the relevant class or the relevant data type was brought into scope.
Care to elaborate on the fundep issue?
You could use a type family instead of multi parameter typeclasses and functional dependencies. That would allow you to hide the `r` as well. Might need mtl-tf to make it work though. Not 100% sure. I could look at it more closely later. 
Hmm. Well I guess I know what I am doing this evening. What are your thoughts on the rest of my comment?
Ways to learn about inlining: - Read about [INLINE and NOINLINE pragmas in the GHC manual](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#inline-and-noinline-pragmas) - Read about the [SPECIALIZE pragma in the GHC manual](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#specialize-pragma) The main reasons that I'm interested in getting functions to inline are when machine integers are in play. If I've got a bunch of functions that work on `Int`s, I want to make sure that inlining happens. I cannot read GHC Core. People who are really good at profiling GHC haskell programs can. But, with `criterion`, I've put together benchmarks where I was able to figure out when certain things were inlining. In my situations, the evaluation time of an expression would drop from 100ns down to 35ns, and then I would know that it was working. You don't really need to worry about inlining for the most part. And by not marking your functions with any kind of `INLINE` pragma, you're letting GHC make the decision for you, which is usually the right thing to do. What I find more interesting from a performance perspective is taking advantage of GHC's [evaluation strategy for top-level bindings and let bindings](http://stackoverflow.com/a/3951092/1405768).
Adding any dependency has a cost, and so there's always a tradeoff to be considered between code reuse and bundling your own minimal implementation. I find myself considering these tradeoffs rather frequently when writing haskell code, but OTOH I also use a lot more libraries in haskell code than I used to in C/perl. It does seem like stack could make this simpler somehow, so reducing overall cost of dependencies. What is the minimal information needed in stack.yaml for stack to always pick the same versions of the transitive dependencies of a new dependency? Could that be done without an explicit list of every transitive dep? Well, let's see, stack.yaml contains the resolver and the version of the dependency. Stack stores the packages list in a git repository. So, if stack.yaml also included the current HEAD rev of that git repository at the time it was constructed, all information used to pick versions of transitive dependencies would be known, and the same ones could be reproducibly picked without being listed. However, it looks like the all-cabal-hashes repository does not currently include historical versions, only the latest one. That would need to change somehow, which would use more disk space.
Eh. I think it's fair to say that Stack and Nixpkgs make dependency handling a non issue. It doesn't mean it's not an issue, it's just one that most people no longer have to concern themselves with. Stackage and Nixpkgs not only make your build reproducible, but also give you a painless and tested upgrade route, unlike cabal freezing.
Tail-call recursion is easily replaced with a loop, and a loop is easily inlined. General recursion _could_ be expanded as if it were a loop being unrolled, save that at the start of each iteration, context must be preserved, and at the corresponding end of that iteration, context must be restored - in other words, a stack is maintained. And if the recursion level is non-deterministic, each unfolded iteration would need to perform all the conditional computation and branching it would have done anyway. Performance would be unlikely to improve.
I have installed Haskell Syntax Highlighting 1.4.4 and Haskero, and highlighting is working fine. Perhaps you have a conflict with some of your other extensions - try disabling everything except the syntax highlighting extension. You should have "haskell" indicated in the bottom right of your status bar showing the language, too. If it's something else, it's overriding the Haskell language detection.
What's sad? 
The need for this.
The current openings are for permanent positions. We are looking into the possibility of also adding an internship position.
Is it just me or is the site almost unreadable on mobile?
&gt;If the function is total and the decreasing argument is known at compile time then you know that the invocation can be replaced by a finite unfolding of the function. My first approach was to check whether there was any way to show GHC that the function is total. I was so damn sure that there was a compile time flag, but that was GCC. Type level natural numbers where an approach, but I kind of forgot how to do them and really wanted to try Template Haskell. That being said, I really like mpickering's answer, since it doesn't need another module.
It's not theoretically possible to implement `isInt` correctly. (By the time the conversion to `Double` has been performed, information has been lost. You need more than a `Double`.)
The problem of unrolling ackermann's function is still an open one. 
Agreed but the example in OP's package could've literally used like `print' = print . replace "\n" "\n "`
Everyone draws a different line, mine is "not 40+ for something that's one-off &lt;20 lines in 5 minutes and not going in production" ;) but you're onto something, NIH is real and I need to be careful there always
It has something to do with trust, but more with keeping complexity down. You have to ask "Is it worth it to add this dependency?", which is not always an wasy question to answer. And of course these libraries sits upon this huge mountain that is GHC, Linux, libc and so on. I actually spent a fair ammount of time removing "environmental dependencies"(apache configuration, postfix configuration, start-up scripts etc) of my projects and moving them into the project itself. This made more code in my project but less complexity overall.
Well, there's still an unroll-plugin, but it's not documented. I've tried it on the question's code, and I got the same times as with the tail-recursive worker-wrapper, which was still a plus. [The generated core](https://github.com/thoughtpolice/unroll-ghc-plugin/blob/master/tests/Unrolling.hs) is rather large, and it seems like it cannot get optimized any more (by GHC). /u/aseipp worked on the plugin for some time, he probably knows more than I do about the capabilties of GHC regarding this topic.
Don't worry, GHC will get slower and then you will get a new machine.
What are the differences between cpphs and hpp? (Apart from the license, of course.) If I switch to hpp, should I expect everything to Just Work? Also, since hpp exists now, will GHC actually ship it at some point?
The difference is caused by `(sqrt 2)`. In ANSI CL this usually will be a single-float. Your implementation probably returns a double-float with higher precision than a single-float. If you want to return a double-float in ANSI Common Lisp, you need to write: (sqrt 2.0d0) See the effects of the various number formats - 6625108 vs. 6625109: CL-USER 47 &gt; (ceiling (* 4684659 (sqrt 2))) 6625108 0.0 CL-USER 48 &gt; (ceiling (* 4684659 (sqrt 2.0))) 6625108 0.0 CL-USER 49 &gt; (ceiling (* 4684659 (sqrt 2.0s0))) 6625108 0.0 CL-USER 50 &gt; (ceiling (* 4684659 (sqrt 2.0d0))) 6625109 -0.7071068184450269D0 CL-USER 51 &gt; (setf *read-default-float-format* 'double-float) DOUBLE-FLOAT CL-USER 52 &gt; (ceiling (* 4684659 (sqrt 2.0))) 6625109 -0.7071068184450269 
Thanks a lot 
Also, consider [this thread](https://github.com/aelve/microlens/issues/79#issuecomment-276599304). Any pretty-printing library using `haskell-src-exts` to parse the text has this issue, since `haskell-src-exts` is a _source parser_, and data types with non-lawful `Show` instances (such as `UTCTime` and `UUID`), which don't produce valid Haskell code, will give dodgy results. This includes the `groom` package. `pretty-show` does not rely on `haskell-src-exts`'s parsing, and does indeed not screw up timestamps and UUIDs. Essentially, we're conflating debugging output, pretty output, and parseable Haskell code. `Show`, ultimately, should be for producing valid Haskell code, ``` read . show == id ``` , and the other two uses should have their own -- separate -- typeclasses. It is widely acknowledged that UTCTime's `Show` instance is basically broken, as the designers decided to make it _pretty_ instead of _parseable_.
I'm not howling in agony over this because it's easy enough to construct a `Functor` given any `Bifunctor`. newtype WrappedBifunctor f a b = WrappedBifunctor { unwrapBifunctor :: f a b } instance Bifunctor f =&gt; Functor (WrappedBifunctor f a) where fmap f = WrappedBifunctor . second f . unwrapBifunctor What's more, a `Bifunctor` gives rise to at least one other functor - the one which maps over the other argument. You can combine `Flip` and `WrappedBifunctor` to get hold of this functor. newtype Flip f a b = Flip { unFlip :: f b a } instance Bifunctor f =&gt; Bifunctor (Flip f) where bimap f g = Flip . bimap g f . unFlip Your proposal doesn't include a nice way to specify that second `Functor` instance as a superclass.
Many things in Haskell come in two versions, one for values and one for types. For example, you can apply a function to a value, like in `head [1..]`, and you can also apply a type constructor to a type, like in `Maybe Int`. In that example you could say that the first is application in the term level and the latter is application in the type level.
what is sense behind apply type constructor to a type? 
I'll look into fixing that 😱 What device are you using?
I see. This is kind of nice, I'm going to keep it in mind.
you get a new type as a result of the application, in perfect analogy to function application to a value f a = b -- the function `f` takes the value `a` to the value `b` F A = B -- the type constructor `F` takes the type `A` to the type `B` think of `Maybe Int`, here `Maybe` creates a "new" type called `Maybe Int` from `Int`, what is that type? well it is kind of a "set" of all the values in `Int` (wrapped in `Just`) and one special value `Nothing` as a bonus
I have hacked a solution together, adding a `RequiredHeader` alongside `Header` and altering the `HasServer` instance accordingly, in servant and servant-server source code. Surely I'm missing something? I will open an issue later if nothing comes up.
I'm using Windows Phone 8.1. The problem is not so much the actual text as the massive scrolling needed to read the text!
I feel like there is a problem with a lot of Haskell packages being far too granular. In some cases there are packages that are literally no value what so ever without some other package (which itself is questionable value without the dependent package). Is there some cost to having functions in a library that I don't happen to need that I'm not aware of? I mean, I understand not wanting to create an additional dependency on your library to support 2 or 3 rarely used functions but I think strong consideration must be given to what can the individual packages actually do on their own.
You aren't. There is long-standing unsolved issue: https://github.com/haskell-servant/servant/issues/241 ATM we (servant maintainers) have a good understanding on how we want to solve this, but I guess I have to write it down, so someone could implement it (if we don't get to it any time soon).
Yes, that's correct (I also work at Myrtle).
But this points out that even with a double float there will eventually be numbers too big for it to work. But it no longer matters, because want_to_want's "mathematical hackery" version beats all of them.
Ah, missed that! I think it would be a very useful feature to have. For anyone else who shares my requirements, see this gist: https://gist.github.com/tomsmalley/770310914c2d84413fc1a011a6cc0e07
Which in Lisp could be written in some imperative fashion like this: (defun foo (n) (loop for (ax ay az) = '(3 20 119) then `(,ay ,az ,(+ (* 7 (- az ay)) ax)) for bx = (1+ ax) for (cx cy) = '(5 29) then `(,cy ,(- (* 6 cy) cx)) repeat n collect (list ax bx cx))) Edit: changed 199 to 119.
Link to the talk? 
thanks a lot, awesome explanation.
I'm sure this can be fixed relatively easily soon; thanks for letting me know!
added the link 
I've been building Docker images with these for a while. A simple hello world application results in an approximately 8MB image file. The smallest Debian based version I've built so far is around 125MB but that's not nearly as bad as the fpco/stack-run images which are around 1.5GB. I'd recommend taking the time to read through and personalize these scripts before using them but here's how I built/used [GHC on Alpine with Docker](https://github.com/mgreenly/dockerimages/tree/master/alpine-stack)
But that requires a bunch of annoying newtype shenanigans. Which I would prefer to avoid. And my proposal actually could incorporate that quite easily: class Functor f where fmap :: (a -&gt; b) -&gt; f a -&gt; f b class Functor2 f where fmap2 :: (a -&gt; b) -&gt; f a c -&gt; f b c type Bifunctor f = (Functor2 f, Functor (forall a. (f a)) bimap :: Bifunctor f =&gt; (a -&gt; b) -&gt; (c -&gt; d) -&gt; f a c -&gt; f b d bimap f g = fmap2 f . fmap g Which avoids any of that annoying newtype stuff.
So without them could you have two states with the same underlying state type? Or would it just allow you to have two different typed state transformers?
no love for [opaleye?](https://hackage.haskell.org/package/opaleye)
I think there are 3 "levels" where we might want to use macros: Text substitution. This is what CPP provides, although it's awkward since: - It's designed to work within C's syntax - It does much more than necessary. I think something like [lips](https://rbryan.github.io/posts/guile-lips-scheme-as-a-generic-macro-language.html) would be nicer for this use-case: it works by evaluating expressions; provides a complete programming language; is easy to escape (only `~` needs escaping, escaped version is `~~`); macros are easy to strip for tool makers (look for `~FOO` where `FOO` is a single s-expression). Parse tree manipulation. This would basically be an s-expression representation of the Haskell code: mixfix keywords, precedence, etc. has all been replaced by parentheses, but no further processing has been applied. For example, `f x y = x + y` might be parsed to `(= (f x y) (+ x y))`. This would allow tree-based manipulation, like a quoted s-expression in Lisp. I don't know anything that provides this, but I'd like to see e.g. a GHC flag to dump it. AST manipulation. Tree-based, but the tokens have undergone some amount of semantic interpretation, e.g. `f x y = x + y` might be parsed to: (Definition (Function (Name f) (Args (Cons (Pattern (Variable (Name x) (Type nil))) (Cons (Pattern (Variable (Name y) (Type nil))) Nil) (Body (Apply (Variable (Name +) (Arity 2)) (Args (Cons (Variable (Name x)) (Cons (Variable (Name y)) Nil))))))) This is what TemplateHaskell does, from my understanding.
Glad you like it. If it helps you learn or do anything useful, I'd love to see it.
Interesting idea. Perhaps the API could even allow some rate of non-PoW requests from a given IP, and then ask for PoW to get a rate beyond the base rate. (edit: that's kind of what you suggest in your edit) And the threshold for when PoW is required could take into account the global server usage, so that if a ddos hits it, then the threshold drops and more clients are required to provide PoW. I have no idea how to build this but it sounds awesome! 
have you seen this yet? https://functionalprogramming.slack.com/messages/haskell/ or http://webchat.freenode.net/?channels=%23haskell
If you're not already aware of [memory mapped files](https://en.wikipedia.org/wiki/Memory-mapped_file), you might want to read up on them. There's a haskell package [mmap](http://hackage.haskell.org/package/mmap) that wraps the system calls.
Are you maybe hiring interns?
The second one, although it's easy to get the first with something like `Tagged`
The reasoning for it is to avoid unexpected duplication of work For example: foo = expensiveMathThing 30 59 24 If the above is polymorphic, then using it twice might require two executions of the expensiveMathThing function. Even if the type of both of the places you use it is the same. Unless GHC optimizer can save you. Now I personally think that the monomorphism restriction is more annoying than the above. So I don't think it is worth it. But I do understand where they are coming from. 
In that case I personally think the functional dependency (or preferably type family) based solution is better. Using the type of the state to dispatch to the correct StateT is IMO a little questionable. I would personally prefer it if defining your own transformer was easier and didn't require defining pass through instances for literally every other transformer. Perhaps with overlapping instances. Then I would just newtype StateT to get say `GameStateT` and `WorldStateT` etc. then compose those. 
Yes, absolutely!
I was discussing this exact idea earlier this week. I wanted to apply it to all content-creation requests. The main concern is that mobile SOCs have so little power, and burning a hole through someone's chip and battery so they can make a POST request to your server seems like abuse of your user's device. [And we wouldn't want to do that.](https://www.reddit.com/r/apple/comments/3orsqc/facebook_drains_battery_in_background_even_though/).
Personally, I disagree with your last paragraph: using `Show`/`Read` for lightweight serialization/deserialization is awkward and, as long as we're using `Show` in GHCi and other interpreters, pretty is *important*. In practice, this means that lots of `Show` instances are broken so you can't—and people don't—reliably use it to serialize data. The "correct" solution would be to have separate classes for serialization and readable display in the REPL but, as long as we conflate the two, I think the latter is a much higher priority if only from a usability point of view. If you want to serialize and deserialize data, do it in some other format: JSON, s-expressions, protobufs... whatever. 
My main worry is that the more dependencies you have, the harder it will be to build—and the more likely it will be to fail in the future. Version bounds, orphan instances, changes in compiler version and platform-specific hacks all make having too many dependencies a minefield. This is especially true if you want to build on other compilers (GHCJS) or niche platforms (OS X, Windows, ARM). In practice, I'm happy to have lots of dependencies in application code—worst case, I can just fork a library that breaks—but want to minimize dependencies in any libraries I publish. Not that I've been publishing many libraries, but that's the idea anyhow...
But if I were an attacker, and your NP problem was finding a hash, I could just spam "0000000000000000000000000000000000000000000000000000000000000000". By "bogus proof," I just meant giving something obviously wrong and letting the server waste time verifying that it's wrong.
It doesn't. It's less code to define getInt :: MonadState Int m =&gt; m Int getInt = get than a whole new class and you get the exact same type inference.
Well for me this is not DDOS protection (intended to overwhelm your IO/resources), but rather protection against injecting crap into the database. It's a defense against someone who intends to make money by spamming your platform with their content, not against someone who has cash they're willing to burn simply to disrupt your service and shut you down (DDOS).
The issue is that when you DON'T have multiple of the same transformer, then you still have to pay for your strategy in terms of type inference, whereas mine is completely free. Yes it is more expensive per instance of a duplicated transformer, but the base cost is nothing, and the common case (at least for me) rarely involves too many if any repeated transformers. And even if you do have a duplicated transformer, if you have a lot of unique ones alongside it, then you probably end up with similar amounts of boilerplate.
Perhaps yeah, I feel like this could also work just fine without backpack. But if backpack is what gets it implemented then I am happy with that. You might also want some way to do: links: unrelated-library ==1.1.*: - Or similar, which basically just says "no working linkage library". In case say your update broke the linkage library and you need to wait on an update but want to limit the number of people affected by this breakage for the time being. As now only people who actually need the instances get affected, rather than everyone that auto-installs the linkage library.
Unfortunately storing the source byte positions doesn't necessarily make sense because the utility pulls data from multiple sources, not just files. I would have to cache an abstracted 'reconstruct from source' operation, which seems possible, but also like it could get messy really fast ("Hey, IronGremlin, why the hell is our historical messaging archive getting like 20,000 individual requests for messages per second from like 2 dozen different hosts on our intranet?") So for sanity's sake I'd probably be looking at data duplication, and storing some kind of index type structure of the destination cache in memory. Which, at that point, sort of seems like SQLLite's MO, I'm just wondering what cost I'm paying by pulling in a whole DB to make what is essentially a grab bag for a bunch of strings.
Ah okay. That makes sense. 
Agreed. Type applications help a lot though.
How are you supposed to get the challenge?
Author here. Tried all the other prettyprinters I could find for my derived Show instance, none worked. Had I had the timd I would have rewritten the (purposefully) simple parser using ReadP, but alas, there are many other thingy to do, and my testsuite gives me good errors now. 
We can also apply terms to types head :: [a] -&gt; a Applying the **term**-level function `head` to the **type** `Int` using `TypeApplications` head @Int :: [Int] -&gt; Int And then applying it to the infinite **term**-level list head @Int [1..] :: Int
 isPalindrome :: [Int -&gt; Char] -&gt; Bool would not work since the type `Int -&gt; Char` cannot be compared for equality.
It's worth noting that this arises from the fact that `head :: [a] -&gt; a` is really `head :: forall a. [a] -&gt; a`, which is kind of like (and actually represented as) `head :: \(a :: *) -&gt; ((List :: * -&gt; *) (a :: *) -&gt; (a :: *))`. In type theory, this is often represented as an uppercase lambda: Λ. The TypeApplications extension arises as a way to explicitly provide an argument to this "polymorphism lambda." So the type of `head @Int` example is really `(\a -&gt; ([a] -&gt; a)) Int`, which beta-reduces to `[Int] -&gt; Int`
patches that make sense are welcome ;) 
In its current state, there's admittedly not much of a benefit; it just removes an argument. In a "real" application or whatever, establishing a connection pool and putting it into a `Reader` environment along with other read-only variables would be a nice pattern. Here, it might be a bit much.
But the point is that it does match up. `id :: Int -&gt; Int` and `id :: String -&gt; String` are morally different, and I would absolutely consider them different values.
It's beyond difficult to achieve, it is impossible to achieve, because you inherently DO NOT have injectivity. But I otherwise agree with your point.
I mean this isn't just one of those "hard to solve, but we can maybe do it" things, type inference mathematically CANNOT happen bidirectionally on the `+` mentioned above, because you inherently do not have injectivity. So there is no coherent way to go backwards, or sideways for that matter, you MUST have monomorphic inputs ALWAYS.
I think even accounting for overflow the laws still hold. If someone has a counterexample I would love to hear it.
The point is that 99% of the time you won't do that.
I'm honestly a little curious what you needed such large finger-trees for. And I guess that means you had to not use `Data.Sequence`, as I know that leads to undefined behavior for them.
I would say that for certain things like `length` it might be worth it to rewrite it as `fromIntegral . length` and use that instead. IMO `length` should not be forced to return just an `Int` anyway. `genericLength` is IMO best avoided, as it only works on lists not on foldables, and also apparently has performance issues due to excessive lazyness / support for lazy number types: see [here](http://stackoverflow.com/questions/36753155/actually-generic-length-function-in-haskell)
I was talking about Ints. I thought that was somewhat obvious seeing as Doubles can't overflow.
Int addition are modular addition 2^64 so the ring laws actually holds. That is the sense in which it is better to distinguish between Int and Int encoded as Doubles.
Nice! You should publish it on Hackage!
the zero bit vector is how long? empty i'd guess. So setBits zeroBits 1 = zeroBits. okay I guess. Or if you grow the bit vector to be the smallest bitvector containing the bit you want to set. But then complement (setBit zeroBits n) != clearBit (complement zeroBits) n But if you don't then you can't copy a FiniteBits number by starting from zeroBits and using setBit clearBit andcountTrailinngZeros. If you do grow then you should also shrink though, otherwise other equations get wonky. 
Cool! I might use that for stack!
Wow, I hadn't heard about that extension at all, TIL!
It would help to get a sense of the mathematical nature of the "backtesting" problem first; I believe this boils down to assigning a value and/or a risk (probability of a loss larger than some value) to a certain investment, in the face of assumed market dynamics (historical data, in this case). Trading firms also do various forms of sensitivity analysis , the so-called "Greeks": http://www.investopedia.com/terms/g/greeks.asp 
You are perfectly right, thank you for pointing this out. I will correct it quickly.
Tnx!
They'll keep their jobs. Leaving the EU will only affect new hires after that point.
Ah ok. If you just go to quantopian and click the start coding button you will get the same thing as I had at that link. I'm not sure about web requests. I could check that out. 
Keep in mind that strategy is going to be much slower than doing everything in their sandbox, so it's probably fine for back testing your model or longer term trading, but if you're trying to beat other bots in real time, you're probably best off just writing in Python.
Very cool! 
Quantopian is a pretty large value proposition: they provide technical data *and* fundamental data (via morningstar) to you for free for your backtesting (afaik). If you do something on your own you would have to pay for this data and the pricing just isn't realistic for a single investor playing around (unless you're already very rich and can afford to toss a few thousand away for fun). If you live in the US and would restrict yourself to Forex, Oanda has a REST interface, including historical data. In the US I think there are other provides that have REST interfaces even outside of Forex. Regardless of where you live, you could go with Interactive Brokers: they have a proprietary interface but they provide a C library you could interface with. If you just want to play, the only options I know of are Quantopian for Python and Quantconnect for C# (and some others but no Haskell).
Could the various Visual Studio plugin teams please join up and collaborate?
Just found https://marketplace.visualstudio.com/items?itemName=Vans.haskero and it works great ! No bug so far, nicely integrated. edit: as /u/alien_at_work said previously, it would be great if the various VScode plugin teams could teamp up or share some code. For instance, Haskelly's integrated ghci session, or current-file-QuickCheck-tests runner are interesting features. they would be great to have in haskero (or the opposite)
I'm not an organizer of this event. I just stumbled across it and noticed that it had not been announced on this subreddit.
My impression from the previous thread about 5 days ago was that this is a university project, so they have to be separate for the time being.
You are quite right, and I completely failed to see this. Thank you for the nice ideas. I will correct both these examples in a very near future.
Hi there, you are right, it is a university project so we cannot just merge our code with others. But we are allowed to accept merge requests, which is why we created https://github.com/haskelly-dev So every merge request is welcomed :)
We do this actually (though not in wai), and based on how much you trust the client based on IP, country etc, you can even adjust the difficulty of the challenge. It's pretty cool!
I did a couple of contracts for Myrtle last year. I can confirm it's a great work environment with smart colleagues and challenging and interesting problems to solve! Highly recommended. 
If you want to look at this with more abstract structure: Think of `||` and `&amp;&amp;` as being the operators in two different monoids that both use the set of booleans as the values: - given x and y, we can combine them with some operation - we have an identity value that if we combine it with another value x, we just end up with x. That's allowed and common: for example, `*` is a monoid over numbers, with 1 being the identity; and `+` is another monoid over numbers with 0 being the identity. So folding some monoid operation over `""` (which is an empty list `[] :: [Char]`) we should get the identity value of that monoid. So what are the identities in the two monoids above? The identity is something that we can accumulate in the monoid without having any effect. With the and-monoid, accumulating `&amp;&amp; True` won't change the result: `x &amp;&amp; True == x` With the or-monoid, accumulating `|| False` won't change the result: `x || False == x` So folding in the and-monoid over that empty list should give us the identity value `True`; and folding in the or-monoid over that empty list should give us the identity value `False`. 
Are the site maintainers aware that the "sponsor" &amp; "schedule" links all 404?
While the `&amp;&amp;` and `||` perspectives are correct, another (equivalent) way to look at it is with `there exists` and `for all` saying `for all x in "", x is uppercase` is _trivially true_ because there is no `x` (no character) in "" to check! saying `there exists an x in "", x is uppercase` is blatantly false, since there is no character to even check the first place! This motivates the monoidal definitions of `&amp;&amp;` and `||`
In addition to the references to monoid and identities, this will also make decent sense just thinking about it intuitively. "All square balls are red" is actually a true statement (see [Vacuous truth](https://en.wikipedia.org/wiki/Vacuous_truth).) "Are any square balls red?" actually wants to see a "witness" or an example of a red square ball. But you cannot find one.
Funny story, I remember a discussion with /u/quchen about me adding a dependency on [either](https://hackage.haskell.org/package/either) to a project in order to get [leftToMaybe](https://hackage.haskell.org/package/either/docs/Data-Either-Combinators.html#v:leftToMaybe), inadvertedly adding the entire Kmett universe as dependencies :-D
I don't know about Stack. But with Nix, I can generate hoogle for my project and all its dependencies. Makes sure I'm looking at the right version of docs. Unfortunately, I don't know how to view any of it through anything other than a browser. But I haven't tried to so there may be a way.
Is this a Language Server Protocol implementation?
We like functions to have nice properties like this: (all isUpper x) &amp;&amp; (all isUpper y) == (all isUpper (x ++ y)) For that to work in all cases, `all isUpper ""` has to be true. There's a similar explanation for `any`. Here's a fun exercise for thinking about properties of code. Let's say you need to write a function that splits strings, e.g. `split "," "a,b,c" == ["a","b","c"]`. What should these return, and why? split "," "" split "" "abc" split "" ""
If this were the case, could servers end up using client computers to solve arbitrary tasks for them, such as BitCoin mining, or SETI? (kind of how google captcha uses clients to id images)
If you try to code a naive implementation in pseudo code with &amp;&amp; and || you can also understand why bool any(string str) bool ret = False for c in str ret = ret || isUpper c return ret bool all(string str) bool ret = true for c in str ret = ret &amp;&amp; isUpper ret return ret 
Alternatively all = foldr (&amp;&amp;) True any = foldr (||) False Which makes it pretty clear why True/False are the defaults - they are the identity elements for &amp;&amp;/||.
stupid me. i'll delete my comment, no value in it. :)
Also some admirable efforts, just arriving, to make Stack 'offline only mode' feasible: https://github.com/ndmitchell/offline-stack#readme
Oh, interesting. That makes a lot more sense than what I wrote lol.
[removed]
Yes and no, the property of the problems that can be delegated like this is that they have to have a quick verification process. I've been thinking about this more recently, as its really kind of awful how proof of work usually does nothing productive...
It's still counter-intuitive. I mean, if you and I are medical doctors and nurses and there's been a disaster and we've been operating on explosion victims for 14 hours and we've run out of units of blood and I ask you to go check the waiting room for patients, and you find the waiting room finally empty and you report back to me that all the patients in the waiting room are bleeding profusely - I may make you bleed!
Of course, all of the patients in the waiting room are also perfectly healthy. :P
Not sure I follow. How would the Maybe monad replicate this?
I had seen this. This thread describes itself as an "unofficial feeler thread about organizing BayHac 2017" and was in no way a guarentee that BayHac was happening.
Emacs + intero provides you the following information for types (I've got an expression highlighted) (the type information is shown on the bottom status bar thing). Intellij has some haskell plugins with similar functionality as well. [intero](http://i.imgur.com/CkQ9ayS.png) [intero 2](http://i.imgur.com/kLJGrIa.png) What editor are you using?
If all your "partial" functions return `Maybe`, you can just use `do` notation to achieve this. Granted, you might have to use `return` or `Just `at the end, but that's a pretty minimal syntactic overhead.
Is there any support for international speakers? Might be that info is on copy no posted to the site yet.
At least not without the ill-advised: instance Eq (Int -&gt; Char) where f == g = all (\ x -&gt; f x == g x) [minBound .. maxBound]
Still not sure I follow. I can see how `fail` for Maybe can give the default `Nothing` if a case doesn't match, but that's not really case-matching like how PartialFunctions in Scala allow. Do note that I am aware that Maybe can be used to do what Scala does with PartialFunctions (in fact, `mapMaybe` already uses that, which is similar to Scala's `List.collect`). The syntax I'm talking about would be something like: mapMaybe :: (a ~&gt; b) -&gt; [a] -&gt; [b] -- new defintion mapMaybe go [1,2,3,4,5] where go 1 = 1 go 2 = 2 When ran, this wouldn't throw a match error, but return `[1,2]`. EDIT: Added "new" definition of `mapMaybe`. I think I wasn't clear that this is a new type. 
Oh, I see. So basically it would automatically convert partial functions into ones returning a maybe?
What about `mapMaybe`? http://hackage.haskell.org/package/base-4.9.1.0/docs/Data-Maybe.html#v:mapMaybe [Edit] Oh... you mentioned `mapMaybe`. I guess I don't get what you're looking for?
Oanda seems to have no information I can find about their fees/commissions for live account trading. Do you know where it is?
Is that a problem preventing you from working or just an observation?
Not really, it's sporadic discussion that will be somewhere in logs. The short answer is, use `ReaderT[Option, ? ?]` and you now have no reason to ever use `PartialFunction`.
No, nothing automatics happens, the gist I posted show this. Essentially it is a different type, not `(-&gt;)`. In the gist I used `(~&gt;)`. This would not be the same as normal function application. In this world, `mapMaybe` would take a `~&gt;`, not a `-&gt;`. So there is no implicit conversion or anything magical happening. 
OANDA is a market maker, not an ECN. Here is a short summary between the two: http://www.investopedia.com/articles/forex/06/ecnmarketmaker.asp As far as I know, they make money on spreads. Their spreads are a little wider than others', so they are essentially skimming off the top of each trade. As a result of this, they don't need to charge commissions or fees. There are more complexities to deciding to use a market maker vs ECN, but this is basically how they make money. I personally like this while I'm testing algos because: 1. I can trade with real money but with very small amounts, and the "commission" from their large spreads is correspondingly small. 2. If my algo is successful on OANDA with their larger spreads, then I'm more confident it can do well with smaller spreads on another broker like Interactive Brokers.
I think you misunderstand me. In this world the type definition of `mapMaybe` would be: mapMaybe :: (a ~&gt; b) -&gt; [a] -&gt; [b] Note that it isn't a normal function, but a different type. So there isn't any implicit or magic happening (and type safety is maintained). 
Well, to be honest, on some of my project even ghc-mod decides to eat gigabytes of memory. Windows is pretty good at swapping so it's not usually a problem but it is depressing because I can't really imagine why a tool like this would need to use this much memory. Even more so when it sometimes drops to a third of what it was, meaning some GC managed to clear a load of junk. Why did it persist for so long?
I still don't see how `a ~&gt; b` is different from `a -&gt; Maybe b`. All the function you defined have an analog for this Kleisli arrow.
Basically GHC isn't designed not to use heap loads of memory so it does ;) I've actually been investigating this [a while back](https://github.com/DanielG/ghc-mod/issues/834#issuecomment-271056632) and couldn't pinpoint what exactly was causing the problem so I filed a [GHC bug](https://ghc.haskell.org/trac/ghc/ticket/13110) which seems to not have gotten any attention yet.
I didn't (explicitly) install the dependency but after one of the recent updates it started working for me :)
It isn't, besides the utility functions it is the same. It is literally a syntax change. 
Thanks! Also am I correct in understanding that Oanda only serves Forex &amp; CFD Markets, so they don't have access to stocks/options for exchanges such as NYSE &amp; NASDAQ in the U.S. (though to be fair, OP didn't specify what kind of trading)
`Ctrl+F Morgan`... no... so [De Morgan's laws](https://en.wikipedia.org/wiki/De_Morgan's_laws) are worth getting a handle on, in general, if you're a computer programmer. There's a version for `all` and `any` (actually ∀ "for all" and ∃ "exists"). The relationship is this: all :: (a -&gt; Bool) -&gt; [a] -&gt; Bool all pred = not . any (not . pred) any :: (a -&gt; Bool) -&gt; [a] -&gt; Bool any pred = not . all (not . pred) Obviously the Haskell version here is circular here so you have to define one of them. De Morgan's laws come up in a number of other places in Haskell, for example in the type system, you need them in order to get existential (∃) types when you're only allowed to use universal (∀) quantifiers in your code. However, the laws come up plenty in programming in any language.
I've heard the term "[vacuously true](https://en.m.wikipedia.org/wiki/Vacuous_truth)" instead of trivially true. I like it because it hints to the absence of something to be actually verified. 
Possibly. It's TBD. Send in a submission, mention your travel constraints, and we'll see what we can do.
Thanks a lot for the help man, that was super useful! I will look into all the things you have said. I was originally planning on doing stock trading as opposed to Forex, but I don't have a specific reason for doing so, so I will look into Forex for sure. Awesome library! If I do end up doing Forex trading I would also be open to helping out with the library.
Let me check it again and reply in the issue
Or the [*slightly*-*better*-*advised*](https://hackage.haskell.org/package/enumerable-0.0.3/docs/Data-Enumerable-FunctionEquality.html) instance (FinitelyEnumerable a, Eq b) =&gt; Eq (a -&gt; b) where (==) :: (a -&gt; b) -&gt; (a -&gt; b) -&gt; Bool f == g = all (liftA2 (==) f g) enumerate (/=) :: (a -&gt; b) -&gt; (a -&gt; b) -&gt; Bool f /= g = any (liftA2 (/=) f g) enumerate There are also other interesting approaches, one is allowing other logics than `Bool`. [*subhask*](https://github.com/mikeizbicki/subhask#comparison-hierarchy) (better [code](https://ghc.haskell.org/trac/ghc/ticket/10592#comment:12)) associates each type with a logic. class Boolean (Logic a) =&gt; Eq a where type Logic a :: Type type Logic a = Bool (==) :: a -&gt; a -&gt; Logic a Most types use a simple `Bool` logic instance Eq Bool where type Logic Bool = Bool (==) :: Bool -&gt; Bool -&gt; Bool False == False = True True == True = True _ == _ = False But for functions this gives a more natural semantics instance Eq b =&gt; Eq (a -&gt; b) where type Logic (a -&gt; b) = a -&gt; Logic b (==) :: (a -&gt; b) -&gt; (a -&gt; b) -&gt; (a -&gt; Logic b) (f == g) a = f a == g a
Why would it be good to _NOT_ throw a match error on unhandled cases? What's the real world usecase here?
That is some wise advice! I will probably decide either Quantopian or Oanda, depending on how my roommates I am working with feel. And that is a good point, to be honest I am not actually super familiar with how the intricate details of trading work, I just am pretty good at coding / mathematics type stuff so I figured I should try and make some money. So learning that stuff ASAP is definitely a good idea. I also do hope Robin Hood gets a nice API fairly soon, as being able to do this with no commission / fees / spread etc. would be really nice.
although i am not a Freebsd user, I found haskell stuff to work quite well on a freebsd system I used a few years ago. For example, my window manager there was xmonad, I was using darcs quite well. I have not done too much development though
Most of the size is the profiled libraries, nothings been stripped. Debian separates out ghc/ghc-prof, the latter being profiled libraries. Aka 300ish meg for ghc another 400 for ghc-prof. Stripping the entire thing brings it to ~700MiB. I didn't bother with reducing the size mostly due to not caring until it was upstreamed, which it is now. Ref: https://anonscm.debian.org/git/pkg-haskell/DHG_packages.git/tree/p/ghc/debian/rules?h=ghc_v8.0.1-17#n126
Should it not be mapMaybe :: (a ~&gt;b) -&gt; [a] ~&gt; [b]
As I hinted I use emacs a lot. Is intero a fork of haskell-mode (or vice versa)? They seem to have much overlapping functionality. TBH I very rarely wonder what the type of a function is (or it's readily available to me and I don't notice). The more usual questions are "what was that function with xyz type called?" (hoogle), "what was the name of that typeclass?" (basically google, haddoc -&gt; info would be pretty nice for this I guess) and "what exactly does that language ext do again?"
There's an example included in [the haskell-lang.org tutorial for optparse-applicative](https://haskell-lang.org/library/optparse-applicative).
If everything runs fine, developing shouldn't be an issue unless there are unforeseen differences. While not an emacs person, I've done most of my haskell in emacs over the years, since it gets the tabbing sorted out for me best (and I had to for a course I was teaching). Development generally comes down to text editors and IDEs that aren't an issue.
I made good use of PartialFunctions (and its syntax support) when I wrote Scala for a couple years. `collect` with PFs over a list of ADTs is a really useful abstraction. However, I don't think adding it to Haskell would work. For one, the implementation of `isDefinedAt` and `apply` often requires mutability/memoization to not do a computation twice. And I don't think what PFs add justify additional syntax for them. I do think this thread is full of misunderstandings though. PFs fit fine in a purely functional world. It's just that in modern Haskell their power:cost ratio isn't up to par.
I see, I wasn't aware of their cost. Thanks for the insight. &gt; I do think this thread is full of misunderstandings though Yeah, I find it odd. My very first sentence was how I was not talking about partial functions as defined in Haskell, yet it seems a lot of people seem to think that's what I'm talking about. I guess it is hard to envision them unless you've seen them (I don't know of any other language that allows them, for better or worse). 
This sounds bad: &gt; It is the responsibility of the caller to call `isDefinedAt` before calling `apply`, because if `isDefinedAt` is false, it is not guaranteed `apply` will throw an exception to indicate an error condition. If an exception is not thrown, evaluation may result in an arbitrary value. (from https://github.com/scala/scala/blob/v2.12.0/src/library/scala/PartialFunction.scala#L1) That is not in the spirit of Haskell.
I like your explanation. I wanted to point out that `Nothing` does not 'take a value and construct a new value'. Only `Just` does that.
But the thing is that you can simply shrink the list instead of generating an element of type `b`. I think we both agree that `a ~&gt; b` is equivalent to `a -&gt; Maybe b`. At which point `(a ~&gt; b) -&gt; [a] -&gt; [b]` is equivalent to `(a -&gt; Maybe b) -&gt; [a] -&gt; [b]`. Which is a perfectly safe and total function: mapPartial :: (a -&gt; Maybe b) -&gt; [a] -&gt; [b] mapPartial = Data.Maybe.mapMaybe Honestly I'm not even sure what you would even want to do with `(a -&gt; Maybe b) -&gt; [a] -&gt; Maybe [b]`. Return `Nothing` if even a single one fails?
That is not how it would work on Haskell (nor should it work on Scala). That's an (idiotic, I think) choice by Scala to call 'unsafeApply' by default instead of the safe one that returns an option. The little implementation that I made (gist in OP) would expect `f a` to return a `Maybe`.
FWIW, `traverse` is the same as what you're describing. Specializing to `[]` and `Maybe`, you get a "partial" function on lists. traverse :: (a -&gt; Maybe b) -&gt; [a] -&gt; Maybe [b]
Maybe https://github.com/realli/chatqy would be helpful to study.
But that gets into linguistic maxims. We ought to only use "All the X" when X is present by sheer relevance.
Which presents us with a nice property: asking "is the waiting room empty?" is equivalent to asking "are all patients in the waiting room bleeding profusely and are all patients in the waiting room perfectly healthy?" 
https://hackage.haskell.org/package/pipes-concurrency-2.0.7/docs/Pipes-Concurrent-Tutorial.html
I for one can't imagine it being very useful, considering `mappend :: (Monoid m) =&gt; m -&gt; m -&gt; m`, which means that you can't mappend two things with different result types...
But it does have `Alternative` and `MonadPlus` which have the exact same restriction. And both of those are extremely useful for parsers, you basically can't live without them. I mean it's pretty much perfect for parsers. Since the whole point of `&lt;&gt;` or `&lt;|&gt;` is to model the notion of choice (for parsers that is). Where if the first parser fails you use the second one, so of course the result types have to be the same. 
If the required PoW difficulty is not overly high, a 7700k would allow spammy behavior but a mobile SoC would be perfectly enough for normal posting. The "PoW for a token that lasts some time" approach sounds very good!
Yeah, it's not helpful against network-level DDoS, but it can help against app-level DDoS (where the goal is to make tons of requests to expensive app endpoints).
In HTML, you can put the challenge in the form like people do with CSRF protection tokens. With a REST API, an extra requests indeed sounds like the best possible option.
Hi! We are sorry to hear that autocomplete and type suggestions don't work for you, could you please make an issue on our github explaining what is going wrong? We would love to improve everyone's experience. Link to github: https://github.com/haskelly-dev/Haskelly
&gt; saying for all x in "", x is uppercase is trivially true because there is no x (no character) in "" to check! An added check for lowercase would give the same result leading to a statement where a string has all uppercase AND all lowercase characters which is nonsense. This is bad, right?
Fair enough, I didn't think that through before posting...
There's ample precedent for the claim that, as per consensus at least, the following identities should hold for a well-behaved `Show` instance: `read . show == id` `show . read == id` * [GHC #9240: `read . show ≡ id` not satisfied by `Data.Fixed`](https://ghc.haskell.org/trac/ghc/ticket/9240) * [A sticky stringy quandry](http://www.stephendiehl.com/posts/strings.html): "Together with the Read class we get a textual **serializer and deserializer for free with the laws** governing that relation being: `read . show = id`[...]" * [The source for `GHC.Show` in `base`](https://hackage.haskell.org/package/base-4.9.1.0/docs/src/GHC.Show.html): "Derived instances of `Show` have the following properties, which are compatible with derived instances of `Text.Read.Read`: The result of `show` **is a syntactically correct Haskell expression**[...]" And so on. 
[removed]
a -&gt; Maybe b has the benefit that in the process of determining if the result is defined or not, you can do much or all of the work of computing b. Scala's partial functions are equivalent to saying `(a -&gt; b, a -&gt; Bool)` where the second function can't help the first do any work. This pattern leads to code duplication and performance penalties. On top of this, the inheritance relationship is annoyingly backwards. If `Function[A,B]` came equipped with an always true `isDefinedAt` then you could use a function as a partial function. But as it is the a PartialFunction is a subtype of function not the other way around, because the inheritance relationship is about having access to the isDefinedAt function, not the utility it provides.
So for me, the real question is why don't we have a nice library which provides these sorts of TH helpers. Just yesterday I was writing code which looks like let !s0 = s + t !s1 = mul_pwr2_d s0 2 + square s0 !s2 = mul_pwr2_d s1 2 + square s1 !s3 = mul_pwr2_d s2 2 + square s2 !s4 = mul_pwr2_d s3 2 + square s3 !s5 = mul_pwr2_d s4 2 + square s4 !s6 = mul_pwr2_d s5 2 + square s5 !s7 = mul_pwr2_d s6 2 + square s6 !s8 = mul_pwr2_d s7 2 + square s7 !s9 = mul_pwr2_d s8 2 + square s8 !s10 = s9 + 1 in ... because I knew the recursive version wouldn't fire a lot of the inlining that would be necessary for this (this particular code might be rewritten with explicit continuations at some point, but that's besides the point). If I could've written let go x = mul_pwr2_d x 2 + square x in $(repeat 9) go x I would've been a lot happier (and furthermore, this is a function that needs to be defined generically for several types, where the number of repeat applications can be computed based on the type it's being used to, so it would be fantastic to write this once and get good performance from clean code for all of them).
Not really sure, all I know is Intero comes with Spacemacs and seems to work quite well. 
I hope you've been able to see that this can be done in a completely safe way without these awful ideas from Scala. It might look like this is a useful feature, but to me it looks completely inconsistent and needs special casing all over the place (the example where a function from 1 -&gt; 1, 2 -&gt; 2 is mapped over [1..5] and returns [1,2] is completely bonkers).
What do you mean by typed? This is definitely well typed if it compiles.
it's bonkers because it's inconsistent with the rest of the generally very well thought out Haskell ecosystem. what should be the returned value when given `[1,2,3,1]`? Should it be `[1,2]` because we stop as soon as we find failure? (this isn't at all how any map like function operates)? Should it be `[1,2,1]`? Well then we just have `catMaybes` and why bother with the magic if `(a ~&gt; b)` when we could be explicit by using `(a -&gt; Maybe b)` and not have code that understanding it rests on being able to tell the difference between `~` and `-` in the type. This reinvents several wheels we've had for 20 years without any benefit. I hate to say it because it makes me sound like a language snob, but generally if there's a feature in Scala that isn't widely used in Haskell, it's often because it's just a bad idea; people have thought about these things and already learnt what The Right Way™ to do things is. This is not a universal truth, but it's true often enough that when you think about how do do X in Haskell like Scala does, you should be asking what is the Haskell way to do it, not how to emulate the Scala way. (this is true of all languages by the way, and many other domains too - I can't count the number of times I've had people come into #macosx after using linux and having to tell them that Mac OS X is not linux, things are done differently; if you embrace the difference rather than trying to turn it into your distro of choice you'll have a much better time). &lt;/rant&gt;
Well we wouldn't be copying PF from Scala directly. I mentioned this in the post, our version of PF would be equivalent to `a -&gt; Maybe b`. Inheretance and all that mess would not be in Haskell, ofc. 
One thing that I just realized that's bad with PFs syntax (as defined in Scala) is that you can't fail on a case. mapMaybe g [1..5] go 1 = Nothing go n = someFunc n ... With PF syntax you would need to do something like `if n /= 1`, which is simple in this case but can grow unwieldy specially if you are trying to do multiple "good" cases (you would need to do multiple case match layers or copy the `if` over and over again). 
Can't you do it just with Git? http://stackoverflow.com/questions/16524225/how-can-i-populate-the-git-commit-id-into-a-file-when-i-commit
The problem is that successful variable lookup is guaranteed by a property that is not reflected in the data types involved, a correspondence between term and environment usually termed 'scope discipline'. You could transform `LamN` to a GADT that enforces scope discipline at the type level, and that would give you totality for lookups. From there you should be able to get to PHAOS. Dunno that doing this would be at all helpful in practice, except perhaps as a fun exercise.
[saltation](http://www.saltation.de/) is a software company in Bielefeld, Germany. We use Haskell for Yesod and Scotty web and some other projects (parsing related among others).
&gt; Sorry for the delay, the floating format problem is definitely not as simple as i thought Happens to the best of us
TH is untyped AFAIK.
Oh god, inotify/epoll/kpoll is just an endless source of annoyance in general. Don't expect haskell is help with that. I haven't used stack much beyond building projects that use, but it is amazing how every project that uses stack just builds no matter where I try to build it.
 all isUpper "" == not (any (not . isUpper) "") This somewhat more intuitive property (think about "forall" and "exists") necessitates the difference you are asking about.
Sorry I meant for parsers. Not in general.
Has anyone tried a pure haskell solution? Seems to be a lot of C code, but nowadays Haskell is quite good at generating tight loops with a little bit of hand written trickery. With FFI code you lose the inliner (important for i.e. constants) and you have FFI overhead. Edit: Actually, the Haskell code that is doing the invoking is really verbose compared to the size of the C functions. Could a vet chime in on why we don't just port the C to Haskell?
If you make a tqueue that holds values of type a it's easy enough to merge as many streams that write to that queue as you want. The your user function just sits atop a gal like go = read tqueue &gt;&gt;= concurrently_ user go
Seems interesting! Could you upload the documentation to Hackage?
&gt; without resorting to checking the names first and then using partial functions. The point of PHOAS (and HOAS) in this context is to use the host language to handle variable binding; as such, you need to "[resort] to checking the names", but you shouldn't need to resort to a (host language) partial function if your guest language can handle unbound variables. Note that you can't use PHOAS to embed a language with anything other than straight-up lexical scoping in Haskell.
Just appeared here, great!
&gt;Silent fatal crash if you save a file with an error [Pft. Only silent fatal crash? Tooling is getting soft on us](https://twitter.com/bos31337/status/116372971509121025)
Great point. I hope he opened a PR for this specific case. :D **EDIT**: Michael didn't but someone else did: https://github.com/haskell/network/pull/232
Grisu3 is already well implemented and tested, and used by many people. There's very little need to reimplement it for what amounts to aesthetic reasons. It's also unclear to me if a Haskell version wouldn't actually look and perform much worse (just look at all the conversions/casts in the C that'd have to go through something like `fromIntegral`). GHC is also less adept than perfectly folding constants than you may think (it's not smart enough to do things like any kind of advanced narrowing analysis for example which could thwart some things). In general the overhead of an unsafe FFI call is really pretty low. Second, the Haskell code and the C code are doing different things, which is why it's all a bit verbose. The Haskell code has to both use Grisu3 and *also* fall back to Dragon4 (AKA the existing `floatToDigits` function, written in Haskell), because Grisu3 is not complete: it may detect that its output is not "the shortest possible" for some small percent of inputs, and thus it will fail. Furthermore, despite the somewhat opaque naming -- these functions are NOT functions of type `Float -&gt; String` -- they are effectively implementations of the `floatsToDigits` function which only gives you the list of digits and exponent as a pair of `([Int],Int)`. That means you still have to *actually* convert this to a string in the proper representation. The code inside the patch, aside from adding Grisu, is effectively just a copy of the source code from [Float's Show instance inside GHC.Float, here](https://hackage.haskell.org/package/base-4.4.1.0/docs/src/GHC-Float.html), but instead using the Grisu/Dragon combination, and specialized to `Builder` from `ByteString`. That's why the patch replaces `floatDec` and `doubleDec` which were previously just implemented as `string7 . show` (because `show` for `Float` and `Double` in fact was literally defined as (`show x = formatRealFloat ...`), and it replaces them with a copy of our already well-used formatting code. So, everything put together, I think this patch is quite good as it is, though reformulating the IEEE754 printer inside GHC.Float to be more reusable would be great. And using this same Grisu/Dragon combination-approach would likely speed up the ordinary instance of `Show` inside GHC.Float as well, ignoring `bytestring` and its `Builder` entirely.
That PR doesn’t do what Michael proposes here: It only adds the module name but Michael proposes to add the host and the port that the connection should be established to.
I mean, just fundamentally, there are (infinitely many) non-integer values that are too close to an integer for `Double` to distinguish them from an integer. It follows solely from the fact that `Double` has limited precision.
In which components (i.e. executable or library) of your package does your code live? Is it listed in the `other-modules` section of the respective component? Maybe give us the rough structure of your .cabal file.
Yes, but AFAICT as of right now you still need to specially opt-in to the fix. It's not rolled out widely yet. I on one of the insider tracks and `apt-get install ghc` is still failing for me. Soon...
Thanks for the info. At this point I'm a long way from dealing with libraries, and hope to get to the point where if run into libraries with FreeBSD portability issues that I'll be able to contribute to get them working smoothly on my favorite platform.
(If you'd like reddit to retain the formatting of your input file, you probably need to indent each line with 4 spaces.) main = withFile ".txt" ReadMode $ \h -&gt; do rcLine &lt;- hGetLine h let rows:_cols:_ = map read $ words inputLine matrix &lt;- replicateM rows $ do row &lt;- read &lt;$&gt; hGetLine h :: IO [Int] print matrix -- or whatever Should work, once you add the necessary imports.
The results are type checked like all Haskell code is, it's not a simple macro system, it's a typed AST. 
I like static checks better than dynamic ones.
This sounds really interessting, but a quick googling turned up nothing. I would like to know more about that.
I would also recommend: * Suggest causes/solutions to the problem (e.g. "Couldn't connect to Port 4000. Is something else using the port?") * Point to relevant documentation if necessary. For example, CSRF protection is complicated, so the Yesod CSRF error message points to the `Yesod.Core.Handler` Haddocks: "A valid CSRF token wasn't present in HTTP headers or POST parameters. Because the request could have been forged, it's been rejected altogether. Check the Yesod.Core.Handler docs of the yesod-core package for details on CSRF protection." Personally, I'm happy to have a ridiculously long error messages to convey more information. You'll save your users a lot of time, and will help them in a time that is sure to be frustrating.
Doing puzzles like the [Advent of Code](http://adventofcode.com/2016) helps me improve - parsing, computation, graph algorithms, and the opportunity to experiment with things like lenses, monad transformer stacks, and free monads on small throw-away bodies of code.
You can do [this type of things](https://github.com/gallais/potpourri/blob/master/haskell/stlc/Bidirectional.hs#L117) and even recover [PHOAS-style combinators](https://github.com/gallais/potpourri/blob/master/haskell/stlc/Bidirectional.hs#L294) by using higher rank polymorphism and some typeclasses. Here the language is typed too but you can drop that invariant and focus on well-scoped.
That kind of a breaking semantic change would be pretty brutal. FTP was pretty controversial and it was a strictly generalizing change.
Thanks for your reply. Having read pipes-concurrency docs, it does seem to address my need. I noticed you also edited your comment to remove a link to your library. I'd love to try it out, but unfortunately I don't understand how to use it (newbie here :) ). If not taking your time much, it would be great if you can show an example by solving a case similar to mine :)
I mean `Alternative` is literally just `Monoid` but more restricted (in terms of kind / superclasses / laws). And since a lot of them agree IMO they should just all agree for simplicity. That way things like `fold` and stuff can all be used somewhat interchangeably.
In case you don't know where to start, pick a sample project of yours, run `stack list-dependencies` and pick a package that looks interesting to you.
I see this an argument for returning `Either` (with a proper error data type in the `Left`) instead of throwing exceptions and using exhaustive pattern matching.
Wait what? Every `Alternative` gives rise to a `Monoid` without fail. If you have an `Alternative` that is law abiding then you can trivially obtain a `Monoid` that is law abiding. 
Yes, though IO (Either Err a) is fairly annoying because you kind of want this to be a proper monad...
You just create a new list each time... this is less inefficient than you might imagine, especially for small datasets. This is especially so since haskell has a copying collector, so a lot of transient allocation doesn't impact GC as much as one might fear. Note that lists in haskell are built as singly linked. So if you add to a list it "shares" the tail, and if you swap one head of a list for another, both also share the tail. Other structures such as available in Data.Map (say, indexed by pairs of ints for coordinates) can share in a more complex manner. If you _do_ need in-place mutation for large datasets, mutable arrays can be handled in `IO`. There's the standards `array` package but also the much more efficient `vector` package that also has a great `vector-algorithms` package with a lot of tools one might want. But my advice for someone _just_ starting out on the concepts is try learning how to write without worrying about space concerns at first, then learn to reason about the space usage of your code and common patterns to minimize it, and only _then_ really start to consider more exotic structures or optimizations.
The idea of pure functional programming is to stop thinking in terms of destructive updates and start thinking in terms of *evolution* of a piece of data. In an imperative, object-oriented program, you might model a game of checkers with a `Board` class, which would encapsulate some program state and expose methods like `void makeMove(Point, Point)`. These might defer to other private methods like `void movePiece(Point, Point)` and `void capturePiece(Point)`, each of which alter the board state. In a pure program, the idea is not too dissimilar, but instead of having a special, encapsulated piece of mutable state, you would simply write functions that represent *evolutions* of your state. You would have functions like `makeMove :: Point -&gt; Point -&gt; Board -&gt; Board`. The definition of `makeMove` would describe how to *evolve* the old state with a new piece of information to make an entirely new state. This new state will have pieces in different positions, and some pieces might not be on the board at all. So how could this possibly be efficient? Well, in a game of checkers, there aren’t actually very many moves, and the board is small. A tiny bit of time spent garbage collecting the old data will not have much of an impact on program runtime. As /u/sclv points out, there are lots of ways to get destructive updates in Haskell if you really need them, but this is often unnecessary with the right choice of data structure. Persistent maps and vectors can share much of the data between “generations”, so you can write in a functional style without paying much cost. You will not need to deep copy data on every iteration, and in fact, this is one of the benefits of immutability: sharing is always okay because the data will never change! You never have to deep copy anything if the underlying data does not change.
You just create a new list? updateBoard :: Move -&gt; [(Location, Piece)] -&gt; [(Location, Piece)] updateBoard (Move from to) pieces = map (\(location, piece) -&gt; if from == location then (to, piece) else (location, piece)) pieces updateBoard (Kill killLocation) pieces = filter (\(pieceLocation, _) -&gt; pieceLocation == killLocation) pieces Now in your main loop somewhere where you collect input from the client: main :: IO () main = playGame initialCheckersBoard initialCheckersBoard = [ ... ] playGame board = do displayBoard board move &lt;- promptForMove playGame (updateBoard move board) Of course this ignores most of the actual game logic of playing checkers, but this kind of skeleton is what you should expect. A few notes: 1. Data is never updated. In the `playGame` function, the old board was displayed to the user, a move read in, and then a new board constructed from the old board. GHC's implementation of Haskell performs these updates efficiently. 2. Recursion is used instead of a loop. Imperative languages often speak of an 'event loop'. There is no such thing in Haskell (unless you're using `IORef`s and such and writing in an imperative style). Instead, we recurse on the `playGame` function. This recursion currently goes on forever, but in the a real game, you would have some sort of quit condition, where you just yield `return ()` or something. Technically this kind of structure is a *fold* or, more generally, a *catamorphism*. In fact, even imperative event loops are technically event folds or event catamorphisms because there is assumedly some accumulated state being kept around. 3. The 'storage' of globals that you would traditionally expect in an imperative language is instead accomplished by passing arguments and recursing infinitely. While you wouldn't typically loop infinitely in a game, a Haskell server may be written to continue indefinitely. In Haskell, we think about what state we want to expose to a function, and simply encode that as arguments to the function. When we want to 'update' those values, we simply construct a new value and then recurse when necessary. Number 3 is very important. Note that there is never a condition in an imperative language where mutability of data structures actually matters *unless* loops or recursion are involved. What do I mean by that? Consider any imperative sequence of commands, including variable assignments. If there are no loops, then you can trivially just rename all variable reassignments to be slightly different (say replace ever `x` with `x'`) and then modify all subsequent references to those variables. In fact, most imperative languages (like C or C++) do this transformation during compilation -- this is known as Static Single Assignment Form. When you introduce loops, that is when you have to start introducing complexity and true mutability in the form of 'phi nodes'. Haskell avoids all of this. In 'imperative' looking Haskell code, you can simply do something like reallyComplicatedMultiSteppedFunction :: BigDataStructure -&gt; BigDataStructure reallyComplicatedMultiSteppedFunction reallyBig = let step1 = doStep1 reallyBig step2 = doStep2 step1 step3 = doStep3 step2 in performFinalAnalysis step3 Haskell compilers could even potentially be capable of compiling something like this into a mutable form because `BigDataStructure` is used *linearly* in this example, but we don't need to worry about this. In general, humans don't really think in terms of mutability (even though most programmers do), so it's best to just avoid it and let the compiler figure it out.
I don't think scope is discussed in terms of totality all that often. It does come up for some subjects like typed compilation, though. Anyway, you proceed by defining environment, variable and term types that have a 'depth' argument. With these you can ensure that a variable (de Bruijn index) of depth `d` can always be found in an environment of depth `d` or greater. This is the total lookup that you are after. Add to that a term type that expresses the way that environment depth changes with scope, and you're good to go. You can even have free variables, as long as you can provide an environment of the necessary depth when required. Here's an implementation: https://gist.github.com/gsg/24af05bc09ec5d53f674e89734ec1d83. It's in OCaml because I'm not all that fluent in the more fun parts of Haskell - a translation should not be too difficult. (The only strange bit is `| Nil, _ -&gt; .`, which is a refutation clause indicating a match that is impossible because of the type structure of a GADT. Dunno if Haskell has this.)
I never looked into this before. Oddly, the paper cites the source of this construction as the separated conjunction from Reynolds' separation logic. (Reynolds actually called it "separating conjunction"). See page 5 of Reynolds' paper for the laws and inference rules regarding it: https://www.cs.cmu.edu/~jcr/seplogic.pdf I think the inference rules hold, but I don't immediately see how to write the adjunctive separating implication. Furthermore, the analogy seems to break down as the rules of separation logic are that the propositions refer to _distinct_ regions, while in our case this is not "necessarily separated" so much, I think, as "possibly separated" as compared to the typical product of functors that is "certainly not separated." I'm probably missing something, but `:**:` itself doesn't seem of much interest to me -- its just a purely formal bookkeeping device that reshuffles some type variables to keep our indicies in place. The best analogy I can think of for it is to a traditional hlist. While an hlist gives us a type-indexed hlist of values, this sort of gives us a kind-indexed hlist of types (and their attendant values, which in the hasochism case are mere witnesses, I think).
I mean one law you get out of `Alternative` that you don't get out of `Monoid` is the parametricity aspect that the inner element won't be inspected. So you could use it when you wanted a `Monoid` but wanted to be sure the `Monoid` did not inspect its inner type. I do definitely see your point that having multiple operators doing essentially a similar task is not always that useful. Honestly I would even cosnider having `&lt;&gt;` and `empty` be the only operators, but have the other classes with superclass constraints and no operators of their own that you can optionally require in order to require types with more laws.
Thanks for digging around, checking it out ---- This follows data (:++:) :: (a -&gt; Type) -&gt; (b -&gt; Type) -&gt; (Either a b -&gt; Type) where InLL :: f a -&gt; (f :++: g) (Left a) InRR :: g b -&gt; (f :++: g) (Right b) where they remind me of [product](https://github.com/ekmett/hask/blob/cd4d30e7911dd7cc2da78383fd833272b1ff9303/src/Hask/Category/Polynomial.hs#L26) and [coproduct categories](https://github.com/ekmett/hask/blob/cd4d30e7911dd7cc2da78383fd833272b1ff9303/src/Hask/Category/Polynomial.hs#L62) type Cat k = k -&gt; k -&gt; Type Product :: Cat i -&gt; Cat j -&gt; Cat (i, j) Coproduct :: Cat i -&gt; Cat j -&gt; Cat (Either i j)
And, you could definitely make an `f` so that `f a &lt;&gt; f b == f ( a &lt;&gt; b )` but which would not have `f a &lt;|&gt; f b == f ( a &lt;|&gt; b )` say, `f = length`. I definitely have been convinced that you could define a trivial Monoid for every Alternative. I am excited to see if there's a sensible reason to use that trivial Monoid, and it would be nice to use Alternative as a sort of `uncons` for arbitrary Monoids. 
Make a small game server or something for a simple game you like. Servers are a good match for Haskell and introduce you to state, concurrency and transformer stacks.
&gt; I mean one law you get out of Alternative that you don't get out of Monoid is the parametricity aspect that the inner element won't be inspected. So you could use it when you wanted a Monoid but wanted to be sure the Monoid did not inspect its inner type. `ForallF Monoid f` already gives you this. The `Alternative` class is pointless with your suggested laws. &gt;I do definitely see your point that having multiple operators doing essentially a similar task is not always that useful. Honestly I would even cosnider having &lt;&gt; and empty be the only operators, but have the other classes with superclass constraints and no operators of their own that you can optionally require in order to require types with more laws. All a monoid is an associative law and an identity. If one is going to create type classes that provide extra laws on top of that why not just provide the operator and identity itself and gain the flexibility to talk about multiple notions of monoid for one type. Monoids are just too general a notion to restrict ourselves to one monoid instance per type. Keeping `Alternative` separate gives us a way to talk about monoids with respect to different hierarchies. The fact that the most useful monoid instance happens to line up for `[]` and a few others is not convincing enough to place the restriction on all cases. 
Yep, that's my PR. While I don't think /u/snoyberg is specifically saying the `network` package should be changed, I think the exception messages there could be greatly improved. That PR is a reasonable start, and I'm sure more context could be added to the messages. I'm still waiting for some sort of response from the maintainers. Seems like there are a number of PRs and issues that have not had a response in a while.
Could you recommend a good book that explains parsers? The subject just confuses me. While learning Haskell and the subject of parsers comes up, I'm completely lost. I'm coming from an electrical background so functional programming is challenging but also rewarding once something clicks...