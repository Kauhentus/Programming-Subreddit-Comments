To do what, develop an OS? Probably not. To develop a static analysis tool? You betcha! The question seems underspecified.
right, and also, how mature it is to take on a very big project.
Yeah, but that's a code generator. You're spitting out C code in the end. I mean... I guess that fits the requirement? But the system that runs the code is not going to use Haskell. Maybe I misunderstood the question. If we're saying that Haskell can serve as the compiler/origination language for the code that runs, sure, yeah.
Yes. For $100 000 000 the project can pay to hire skilled haskell hackers to help clean up any warts that would otherwise make me wary of using the language. It's smaller sums of money with smaller margins I'd be more worried about.
I do real-time development. real-time is a lot about speed and responsiveness. I'm very worried about timing and instruction sequences which is what haskell really abstracts away from.
Would you borrow me $100,000,000 to bet on Haskell?
You're brave, paying developers out of pocket like that. You know you don't get paid if you don't deliver, right.
That's why Atom is a domain specific languages that compiles to C. Haskell is used as a powerful macro language.
Fair enough, but I think there's a semantic difference that you're glossing over to make a point.
He's not. C is not a bad target for compilation. It's been called portable assembly.
Where did you report it did you go to David's webpage request a email and then email the report to him or was there a better method that I missed? This is what i was going todo today.
That `[g] -&gt; IO [g]` type looks an awful lot like `StateT [g] IO ()`! Why not actually use that? Then `replicateM` looks like just the ticket.
Yes, I just reported it by email; I was very pleasantly surprised by the quick response. (Unfortunately it seems that the HsOpenSSL versions with the changed type signature, that iterIO now depends on, don't work in the GHC bytecode interpreter on this system, so not only does it not load in GHCi, but my Template Haskell code doesn't work...)
It would be portable assembly if it wasn't for the undefined behavior.
Good to know should have just done it last night apparently.
LISP?
It seems like one of the issues with the package is that it should be broken up into multiple packages to make dependencies optional. I thought I saw it required unix.
Hmm. Okay... I'll do it. How do I sign up?
Profiling rocks! 
Set cost centers: walk surface seen count = walk new seen' count' where poss = {-# SCC "poss" #-} nub . concat . map reachableCoords $ surface new = {-# SCC "new" #-} filter (\p -&gt; isNew p &amp;&amp; isAccessible p) $ poss seen' = {-# SCC "seen" #-} foldr (\k m -&gt; Map.insert k () m) seen poss count' = {-# SCC "count" #-} count + length new isNew p = {-# SCC "isnew" #-} not . Map.member p $ seen Run with profiling: &gt; ghc --make -prof -auto-all ants.hs -O2 -rtsopts &gt; ./ants +RTS -p &gt; less ants.prof includes the line : poss Main 274 2796 69.2 10.0 70.6 21.0 which means 70% of the time is in poss. Replacing nub by a better nub (e.g. this one: http://hackage.haskell.org/trac/ghc/ticket/2717) knocks that down to 35%, 30% of which is accounted for by even the new nub alone. So yes, nub is *that* slow -- remember the problem isn't the constant cost, but rather that its complexity is n^2 rather than nlogn. Another 35% of the cost is in "seen", where I notice that for no good reason you're using Data.Map rather than Data.Set, which could help with allocation a tiny bit. Edit: a few more tweaks (strict fold, cleaning out some duplicated logic, using sets uniformly) brings me to the following, which I'm sure could still be much improved: walk3 :: [Coord] -&gt; S.Set Coord -&gt; Int -&gt; Int walk3 [] _ count = count walk3 surface seen count = walk3 new seen' count' where poss = {-# SCC "poss" #-} S.fromList $ concatMap reachableCoords $ surface new = {-# SCC "new" #-} filter (\p -&gt; isNew p &amp;&amp; isAccessible p) $ S.toList $ poss seen' = {-# SCC "seen" #-} foldl' (\s k -&gt; S.insert k s) seen new count' = {-# SCC "count" #-} count + length new isNew p = {-# SCC "isnew" #-} not . S.member p $ seen main = print $ walk3 [start] seen 1 where start = Coord 1000 1000 seen = S.singleton start 
He is glossing over the difference: the difference is that the semantics being used are not Haskell's anymore, but of the atom DSL. I think that qualifies as "not Haskell". It's not about the compilation target, it's about the compilation source (semantics). If this atom language happened to have a proper parser for it, I don't think we'd be calling it Haskell still.
Here's how I just did it: http://hpaste.org/51578 You can't go to 999 or 1799, so the range is restricted to 1000-1798. The digitsum function can be based on a UArray Int Int. You probably meant to use Set Coord instead of Map Coord (), but I don't see that as making a huge performance difference. I think I made more idiomatic use of lists and sets. Also I only keep track of the current generation and previous generation visited sets instead of tracking all ever visited. Also, I believe a Set.fromList on a list of size n can be much faster than n set inserts. Edit: That "= 25" in digitsum should really be something like "= 100". I don't know how it works as it is.
How is it not a 'proper parser' because it's written in Haskell? One of the main features constantly touted by Haskell advocates is the ease of creating parsers for DSLs.
nub is O(n^2), but can be done in O(n log n) with a HashSet or (Int)Set. You may get even better results by using IntMap IntSet or HashMap Coord ()
Well, nub is _really_ slow (n^2 in the length of the list) and your list has lots of duplicates. Also, converting an integer to a string is a small inefficiency.
If I'm not mistaken the range is 1000-1599. I also looked into the fact that the square (1000-1029, 1000-1029) is valid. using Set sounds like a great idea, as well as using Sets for "nub"ing.
thanks for the fantastic effort and input! Well, there will always be a lot left to learn (for me ;) this was really helpful!
Haskell does not make reasoning about performance easy. It makes it harder than, say, C.
Dashed hopes of fully combining code and documentation. We ended up using a wiki for documentation.
yes, I have profiled the code before and was just surprised by the sheer difference between walk and walk2. Both use basically the same infrastructure ("show", etc.) but with a huge difference. With sclv's changes the difference is now down to 1.1 seconds vs. 1.5 seconds, which still seems quite costly for the list-based version...
I'm pretty sure modern processors have some UB too in some of the more nuanced behaviors (e.g: Cache coherency protocols between multiples cores, chips, etc).
Yeah, since each coordinate has to have a digitsum of at least 1.
&gt; It isn't terribly difficult to reason about performance in Haskell (there are quite a few people who know how to) once you're taught a few basic concepts and techniques, but we do a poor job of teaching people ~ http://blog.johantibell.com/2011/08/results-from-state-of-haskell-2011.html
~~This has nothing to do with Haskell, walk is simply visiting a lot more cells than walk2~~ Wrong.
I agree it is not terribly difficult, but it is still more difficult than in C.
There are somewhere around 1000 people listed on http://www.haskellers.com So give me all of those people, as well as (not sure who among these are on the website) SPJ, Dons, Oleg, Wadler, you know, all them gurus. I'll pay these ~1000 people each $100k for 1 year (more for the very experienced people, slightly less for everyone else) to work full time; many of them would work exclusively on Haskell toolchain improvement. Holy crap, do you know how much that Haskell uber-dream team could accomplish in 1 short year? Yes, sir, I would bet $100,000,000 on Haskell, and oh, the miracles you would see my 1k Haskellers perform.
I'm throwing a link to this question on /r/haskell because I really want a good answer to this.
are you sure? I would have thought that they'd visit almost the same number of cells (i.e. available cells + a border of one + some duplication). maybe I can add a little code to compare how many cells they visit... walk2 visits 595393 cells. walk visits 595392 cells (+ the starting cell). The two algorithms are equivalent in this regard, as I had thought they would be.
I meant that I didn't see a parser for it. Not that a parser written in Haskell isn't proper. More-over, what I meant was that if you could write a text-file (non-Haskell) that was the atom language, you wouldn't claim it was Haskell just because the compiler was written in Haskell.
Yes, and in imaginary land, you will find your $100,000,000. :)
Reading the replies here so far, the question should really be would you mortgage all you have to get a project that would pay $100MM if you succeed but nothing if you don't.
Also, perhaps this is a better place to ask the tangential question: is "strictness point" a technical term? Is there another word for it? Is it a misleading/bad term? Why or why not? Depending on the weather, sometime later I might try to argue on behalf of the term "strictness point".
I agree; I've no need for the OpenSSL, HTTP or attoparsec code, so it'd be nice to avoid those dependencies ([especially with the issue I'm having](http://www.reddit.com/r/haskell/comments/kjem1/iteratees_at_tsuru_capital_various_contributions/c2l9p01)). But I think this kind of thing is a trade-off for the maintainer; it becomes much harder to maintain and work on the package as a single entity.
I had the same question once and it is possible to find the answer on the Haskell wiki. I can't remember where at the moment. Also one of Johan Tibell's performance talks covers this when he covers Core Haskell. I gave an answer from memory over on SO.
This page may be of interest: http://www.haskell.org/haskellwiki/Category:Idioms
I believe the word you're looking for is lend.
I think the best part of the blog post is the question at the end: &gt;if a hundred million dollars changes your approach to getting things done in a quick and reliable fashion, then why isn't it your standard approach? I never work with "$10M-scale" problems, so the drawbacks he lists are not relevant, but even for smaller problems it's often the case that using Haskell is more work than some other language (e.g., GHC unicode support is still a bit spotty). I guess the answer for me is that I'm not only interested in solving a particular problem, but also improving the programming language landscape in general. By using Haskell and getting to know the pain points, I hope to one day be in a better position to fix some of them.
Talk is cheap. I might *say* I'd use Haskell, but to be sure, send me the money and we'll find out.
This might help: http://stackoverflow.com/questions/6003728
[I always link this. Read the second chapter called typeclassopedia](http://www.haskell.org/wikiupload/8/85/TMR-Issue13.pdf) 
You shouldn't sign a contract no matter what language you are choosing under the conditions presented.
&gt;So give me all of those people, as well as (not sure who among these are on the website) SPJ, Dons, Oleg, Wadler, you know, all them gurus. I'll pay these ~1000 people each $100k for 1 year (more for the very experienced people, slightly less for everyone else) to work full time; many of them would work exclusively on Haskell toolchain improvement. This excites me. Edit: To say something slightly more useful - how do we go about, as a community, increasing industry supported toolchain work (as opposed to straight industry use). Is this something we can galvanise at all?
We are pretty happy breaking the Yesod project up into many smaller packages. The key is to have them all sitting side-by-side in the same repo. On the whole, it may be a little more work, but there are the normal upsides to having better isolated code.
Bit smug. But very interesting. 
I didn't get "smug" from that at all. Seemed like nice, thorough, easy-to-follow set of slides. I'm sure the talk was even better.
Awesome. BUT, whats the greatest compiler bug ever? I've been preparing to write something very similar about my startup, Hachicode. Of course, we're in the Yesod world so we'll be talking a lot about having a radically, statically typechecked webapp..
You know, you seem to have a knack for concocting [interesting](http://www.reddit.com/r/haskell/comments/k4yvs/positive_rationals/) [algorithms](http://stackoverflow.com/q/6273762/157360) of various kinds...
Switching to unordered-containers will give you another speedup also -- it's much faster than Data.Set.
Thanks, this looks perfect for my needs.
It does, thank you.
This is good, thanks.
everybody's got to have a hobby?
I actually tried to move to more operations taking advantage of set, and using `S.difference` to calculate new in particular, and found that it resulted in a net slowdown. Unordered-containers might make those sorts of algorithmic improvements into optimizations as well. Also, given that our coords are in a small range of ints, we could project them directly to ints and use an `IntSet`, which I would imagine *should* be able to beat any plain old set -- although, actually, we get the same effect (with a slightly better structure?) by giving them an efficient hash function along the same lines and using unordered-containers.
I think the order of magnitude of the budget does matter. If my startup has less than 100k of funding, I'd probably not choose Haskell for the one, very important project. If I'm in charge of a $1M project, then I'd think about using Haskell. If it's U$100M then hell yes. Since the $100M version is such a no-brainer, the $1m version is probably more interesting.
Well, it's a good hobby!
Looks interesting, but highly inefficient, unless I'm missing something.
Not really, actually. Using this [as a core for the ant puzzle](https://gist.github.com/1231390) I was able to solve it in ~0.11s.
Can't we ask the GHC HQ to fix this and do a 7.0.5 release?
Oh right, took me a little while to figure out what was actually happening. I hadn't realised that you needed to calculate the sum of digits of consecutive numbers, which is obviously much more efficiently done your way.
&gt; BUT, whats the greatest compiler bug ever? http://twitter.com/#!/bos31337/status/116372971509121025
I don't think that there is a clear cut-off between must-know and good-to-know stuff in Haskell. Also, it can never hurt to learn too much since learning becomes exponentially faster the more you already know. That said, I like the following curriculum, which I think is essential and well-rounded: (EDIT: added lazy evaluation) * Memorize the whole `Prelude` by heart * Understand one Functional Pearl by Richard Bird (for example the Sudoku one) * Implement parser combinators (with `s -&gt; [(a,s)]`). * Memorize the standard monads by heart (`Maybe`, `State`, `[]`, etc.). * Understand lazy evaluation, in particular the famous "rock-paper-scissors" example (which can be found in the book by Bird &amp; Wadler or in the [recent **3rd** edition of Thompson's book][2]). * Understand Paterson's &amp; Conor's original paper about applicative functors. * Understand functional lenses; `Lens a b = Lens { get :: a -&gt; b, put :: b -&gt; a -&gt; a}`. That's pretty much it. Once you know this stuff, learning the rest should be easy and can be deferred until you need it. [2]: http://www.haskellcraft.com/craft3e/Home.html
Very interesting. &gt; The desktop piece is in C#, Cloud components are in Haskell So it's another vote for "Haskell on the server, something else on the client" like expressed in http://lachlanrambling.blogspot.com/2010/01/cocoa-vs-haskell.html ?
Somewhere he wrote "we'll hire soon", so i don't think they can tell much about this yet.
Hahaha! Arguably, it's the right thing to do, though. :D
There won't be any [reactive bananas][1] for Objective-C programmers. (Though CoreAnimation comes close. Ironically, it doesn't integrate very well into Objective-C.) [1]: http://haskell.org/haskellwiki/Reactive-banana
[Google image search for "lambdacats" if you don't know what the hell I'm talking about](http://www.google.com/search?q=lambdacats&amp;hl=en&amp;biw=1280&amp;bih=712&amp;um=1&amp;ie=UTF-8&amp;tbm=isch&amp;source=og&amp;sa=N&amp;tab=wi)
Ouch. That is hard to beat.
I am not sure what you mean with "reactive bananas for Objective-C programmers".
It's a pun. I wrote a library for functional reactive programming with the name "reactive banana". It allows you to write GUIs without all the imperative state mess. [Examples here][1]. [1]: http://haskell.org/haskellwiki/Reactive-banana/Examples
Tres bien! (It seems to overstrain the google translator, though ....)
Also for some reason all the Haskell code was translated into OCaml... (I kid, I kid)
http://www.reddit.com/r/haskell/comments/km2b8/running_a_startup_on_haskell/ See slide 6.
I've read about this, but i am unsure how writing GUIs in Haskell relates to ObjC-programmers.
***bos31337***: &gt;[2011/09/21][04:47:33] &gt;[[Translate]](http://translate.google.com/#auto|auto|So glad you asked! The best ghc bug ever involved a dev version of the compiler deleting your source file if it contained a type error. 'google translate this tweet'): So glad you asked&amp;#33; The best ghc bug ever involved a dev version of the compiler deleting your source file if it contained a type error. [[This comment was posted by a bot][FAQ]](http://www.reddit.com/help/faqs/tweet_poster 'tweet_poster FAQ')[[Did I get it wrong?]](http://www.reddit.com/message/compose/?to=tweet_poster&amp;subject=Error%20Report&amp;message=[Oops!](http://reddit.com/r/haskell/comments/kmk6q\) 'report an error')
GHC had a quite radical way of ensuring no program ever suffers from type errors, didn't it?
Looks like a quantum monad.
link doesnt work
Functional reactive programming is - in my opinion - a simpler way to write GUIs, but you can only use it in Haskell, not in Objective-C, that's the point I want to make. The main reason for that is, of course, Haskell's expressiveness. FRP might be possible in Objective-C, too, but it's really awkward to use compared to a Haskell version. In fact, CoreAnimation is similar to FRP and it's quite clear that it's not native to Obj-C.
Just did a little archeology: http://hackage.haskell.org/trac/ghc/changeset/434ef2b14b37df405602a74838b8b38d0f5b4375
Google? This is translated by hand. Some of the wording is a bit awkward, but translating these kinds of fun-informal technical texts is always a challenge.
&gt; Google? This is translated by hand. Of course, but my browser tried to convert it to english and on some sub-page stopped responding ......
&gt; So, um, yeah &gt; C# is pretty decent. &gt; Except for the times when it launches the chainsaw. &gt; From the basket of mewling kittens. &gt; Straight into your jugular. This made me laugh pretty hard. :-)
It's not a bug, it's a feature to filter out lesser programmers. If you cannot do type inference in your head you shouldn't be programming in the first place. 
thanks for the good explanation! looks quite elegant to me.
What troubles me a bit is, that without context -- used to fish out the preprocess output files for the purposes -- of cleaning up. ppFilesFromSummaries summaries = [ fn | Just fn &lt;- map (ml_hspp_file . ms_location) summaries ] does not look like an error might lead to the removal of source files.
Let's hope there is no type-error in GHC code.
Smug from a C# developers point of view maybe?
It might be the case that it's nicer to program GUIs in FRP or even in non-FRP Haskell, but currently not many people outside of academia seem to do it. I don's see this is ObjC-specific since MailRank is using C# for the frontend. What is your experience? Do you have to provide support for a cross-platform product you've written in FRP? How do you manage to setup/compile your product for Linux, Mac, Windows? What GUI features were easy to provide and which were hard? 
Now *that*'s extreme programming!
Libraries like reactive-banana are designed to add act as a second layer over a GUI toolkit binding of your choice, but that's all moot if that binding itself isn't up to par. There are fairly complete Haskell bindings to GTK and wxWidgets but I'm not sure I'd bet my business on them. Besides if you look at [mailrank.com](http://www.mailrank.com) you'll see that their product is integrated with Outlook, so it makes sense they'd want the easiest possible access to the Windows APIs. Haskell doesn't offer much in the way of bindings to platform-specific APIs, and that's often the first thing you want in a client-side application.
Yet another reason to use source control and aggressively commit.
That's what i feel, but it's somewhat sad to realize that Haskell is more of a server-side language when it comes to practicability.
And if your code wasn't in some form of version control, you deserve to lose it.
said in response to a question yesterday at the StrangeLoop conference during his talk on [Running a startup on Haskell](https://thestrangeloop.com/sessions/running-a-startup-on-haskell)
[direct link](http://bos.github.com/strange-loop-2011/talk/talk.html#%286%29)
GHC is keeping the bar high for Haskell programmers. We're not playing limbo here.
Haha, no :) I'm the translator, and I'm currently working under the creator of OCaml. Kinda anti-corporate, but eh... irony!
You approach is interesting. In these cases I usually do something like sum . map Char . digitToInt . show which is conceptually so wrong (shame! shame! shame!). But I had this somehow related discussion on SO http://stackoverflow.com/questions/4841078/why-map-digittoint-show-is-so-fast (conclusion: never evaluate performance in the interpreter)
Does this work: https://twitter.com/bos31337/status/116372971509121025
Sure, because most people check code in before they even try to compile it.
Yes, I can't say the translation is near perfect. For instance, someone mentioned me that I left a lot of "cool" while sometimes, it doesn't sound cool at all in French! :) And it might also be the case that I left some sentences in passive style while they would read better in active style in French, or this kind of things. Feel free to shout me an email if you encounter things that really bug you, I'd be happy to improve the text!
True that. Personally, I would always bet on Haskell, though; either by hiring someone to improve wxHaskell or writing an "only eat what you need" binding to the platform specific frameworks (I've done this years ago with Haskell and Cocoa.)
This bot is the best thing that has ever happened to me.
Wait... I thought that was C++. /s
Only works if you commit before you compile, which is not exactly best practice.
Which creator? I'm pretty sure I can figure out who you are :)
At one point I proposed a similar feature for the #haskell IRC channel--that users should be kicked from the channel for trying to evaluate code with type errors. If memory serves me, the objections to this (obviously completely sensible) idea mostly involved the implementation detail of making lambdabot an op, rather than the idea itself. ;]
I should benchmark it getting the sum of the digits this way for a batch of ints against using `map digitToInt . show`
Use git, commit frequently, squash all local commits when finished. Problem solved :)
I meant beyond the revert-to-last-commit slap on the wrist for failing to mentally type check it first. I was actually hit by a similar problem with Visual Studio 6 back in the day. It would sometimes require me to save the file twice in a row to actually save it. I think it was because of a virus scanner. If you only saved it once, it would actually get deleted. Of course, one day VS 6 crashed between my two saves. Fortunately, I was able to save most of my code by using a debugger to recover pieces of it still left in memory. I wish I had learned about version control before that :)
You should try [RES](http://reddit.com/r/enhancement). It makes twitter posts act kind of like self posts. It also does a million other things.
This is good, thanks. I'd forgotten Bird's Sudoku - I filed it away for a time when I might have a chance of understanding it, I should try again.
what are the 3 books mentioned on slide 39? I know of real world haskell and that its available free online but I didn't know there was another that is free online..
I forgot something important, namely understanding lazy evaluation. Edited my original post.
http://learnyouahaskell.com/ I am not sure what the 3rd one should be. Maybe Hutton or Hudak.
Could even set something up to do that automatically every time a file is saved, or just commit periodically at regular intervals. Based on the IRC bot announcements in #haskell, I'm pretty sure Edward Kmett uses something like that, except that instead of committing it just uploads directly to Hackage. ;]
To paraphrase Patton Oswald: programmers: they're all about coulda, not shoulda.
Yeah, I just thought it was amusing that this had been posted on the same day...
You beat me to it.
Harsh but fair.
Actually it does -- not the code itself, but the comment. If it "fishes out" all the files thinking they are preprocess output files and hands them over to the cleanup routine, I can easily see how it could cause the bug.
I like your thinking. I ask because I don't always know _what's_ idiomatic in a language.
IO is the only one I really understand, so I shy away from the other monads. What happens if you fork my code and implement `evolve` with a StateT? Does the program use less memory or run faster? The makefile generates a graph of the program's resource usage.
Why is it inefficient? A stab in the dark: One side of `&gt;&gt;=` remains a thunk until the whole thing is evaluated, resulting in stack overflow?
[YO DAWG](http://images.cheezburger.com/completestore/2011/9/21/d939eb4a-0f06-4945-9dd3-22fb6bdf8f5d.jpg)
Great presentation! &gt; I must hunt down and learn to use a QuickCheck-like library for my language. If I can't find one, I should write one. Port all the QuickChecks! If anyone wants to improve my libraries, just send a merge request. * Common Lisp: https://github.com/mcandre/cl-quickcheck * Smalltalk: https://github.com/mcandre/quicksmash * Node.js: https://github.com/mcandre/node-quickcheck * Factor: https://github.com/mcandre/factcheck
Obligatory example: 12:32:31 a@link ~ ~/ghc-head/bin/ghci GHCi, version 7.3.20110921: http://www.haskell.org/ghc/ :? for help Loading package ghc-prim ... linking ... done. Loading package integer-gmp ... linking ... done. Loading package base ... linking ... done. Loading package ffi-1.0 ... linking ... done. Prelude&gt; :set -XConstraintKinds Prelude&gt; type Stringy a = (Read a, Show a) Prelude&gt; :i Stringy type Stringy a = (Read a, Show a) -- Defined at &lt;interactive&gt;:1:6 Prelude&gt; :k Stringy Stringy :: * -&gt; Constraint Prelude&gt; :{ Prelude| data Bit = T Prelude| | F Prelude| deriving (Show, Eq) Prelude| :} Prelude&gt; :t T T :: Bit Prelude&gt; :k Bit Bit :: * Prelude&gt; :i Bit data Bit = T | F -- Defined at &lt;interactive&gt;:1:6 instance Eq Bit -- Defined at &lt;interactive&gt;:3:26 instance Show Bit -- Defined at &lt;interactive&gt;:3:20 Prelude&gt; :show bindings type Stringy a = (Read a, Show a) data Bit = T | F instance Eq Bit instance Show Bit Prelude&gt; 
Nice! Does the Haskell syntax makes it impossible to use REPL like in LISP (i.e. can cut and paste the content of the file into the REPL and it works)?
Probably not. The module header would be invalid syntax, and I don't think it would like out-of-order declarations.
I tried :load /dev/stdin but it doesn't work: target `/dev/stdin' is not a module name or a source file 
Apparently it was an intuitive implementation, so thanks for providing the intuition! 
You're right I should say "one of the creators". Xavier Leroy is the one. And you can find my name at the bottom of the linked page if you need confirmation ;) Conversely, I do not guess who you are.
Trying to be clever doesn't work either: $ ln -s /dev/stdin Paste.hs $ ghci &gt; :load Paste.hs *** Exception: Paste.hs: hFileSize: inappropriate type (not a regular file) However, I guess it shouldn't be too hard to write a `:cmd` binding to paste the clipboard to a temporary file and then load that. 
Ha, I thought that was intentional! I just took the obvious implementation in Haskell, did the pointless translation in my head, and ended up with exactly what you wrote without even intending to.
there's another interesting ticket on ghc trac to make it possible to evaluate a do block, command at a time: http://hackage.haskell.org/trac/ghc/ticket/4316 Control.Monad.State&gt; flip evalStateT 10 $ do Control.Monad.State| i &lt;- get Control.Monad.State| lift $ print i 10 Control.Monad.State| put 11 Control.Monad.State| j &lt;- get Control.Monad.State| lift $ print (i+j) 21 however this seems much more involved to allow, requiring: http://hackage.haskell.org/trac/ghc/wiki/PolymorphicDynamic http://hackage.haskell.org/trac/ghc/ticket/4459 
Thanks, I hadn't even known that something like type families existed. Understanding laziness is certainly something I need to work on.
Awesome, this has been sorely needed!
The country I am in now doesn't play well with Twitter :)
The first five issues have never been available in PDF form. It was Gwern Branwen who converted them from MoinMoinWiki to MediaWiki, I believe.
I just did a happy dance.
It's unclear what that even means for arbitrary do blocks, or even arbitrary instances of `MonadIO`.
So I remember that somehow we were misclassifying source files as temporaries and removing them, but the bit I don't remember is how it came to be dependent on whether there was a compilation error.
Frankly emacs with haskell-mode solves this issue in a much better way. 
It is nice, but I think it's pretty unlikely they can/should switch to Gitit. What they want sounds like it'd take extensive changes throughout Gitit and also Filestore; Macfarlane doesn't have time to code it all up for them (he's the one who understands the Gitit codebase), and I have no real interest in helping them out - too much work and I have my own projects.
Ah, thanks for the info. I guess I'll have to try to compile them by hand.
Just from the paste above, it looks like it's gathering a list of all the files mentioned in the error summaries.
Can you give an example of how to do the stuff from 1nine8four's comment using haskell-mode? I find it pretty impressive, and would be shocked if you can accomplish it there.
Really ? Ctrl-c-l loads the current file to ghci session in another buffer. So create a file test.hs, open it in emacs, type in whatever definitions you want. Hit Ctrl-c-l, and you have them loaded in your ghci. 
Ah, well of course you can put this stuff in a file and then edit the file, and reload everything from scratch and start over if you want to add any new types or instances. This feature is, of course, about saving you the need to do that.
How so? Give me an example
Consider how you'd implement it for ContT. It's certainly not clear to me how that would work.
Could you please explain me how typing in your definitions in one window (ghci) is different from typing them in another window (emacs buffer) I know of course that in emacs buffer i can save what i type whereas in ghci i loose anything i type, and that in emacs window i can fix my error, whereas in ghci i have to retype the entire multiline definition if i made a mistake. But aside from that what is it that i am missing ?
You mean aside from the obvious difference where you are constantly switching between two windows or buffers? Of course, right now I do that all the time, as I imagine most of us do; but it's certainly not the most convenient thing in the world when you don't really *want* to save the definitions.
Wait, you mean like swapping view of one window with another like Alt-Tab ? It does not work like that in emacs. Emacs shows both windows side by side (tiling). So not much switching involved. You work on one panel and see results in the other panel. And switching focus between them is one shortcut away. 
Yay! Yay!
Whether it's alt-tab or a different key, there is still a need to change focus between two areas, and it remains true that if you define anything on the GHCi side, it's lost when you reload the source file. And you still end up with Test.hs files scattered all over (I've found about six of them in different directories looking around just now). So, yeah, of course it's been possible to cope, but being able to do this without temporary files and swapping between two places is nice. I also don't mean to imply there's no reason to ever load a source file into GHCi via emacs (or cabal repl, or other similar ideas). But this is useful when you're tinkering and not interested in creating a file separate from the interactive environment.
&gt; I know of course that in emacs buffer i can save what i type whereas in ghci i loose anything i type I've wanted a nice way of recording a GHCi session and extracting usable .hs code from the result. I'm not sure exactly how that should work. I mean, usually I'd want the definitions I used to be part of the .hs file, but on occasion I'd prefer the results to be the definition. 
Delivered -&gt; http://db.tt/JNcnnujP
Also in spanish (http://aprendehaskell.es) (Almost 1 year old)
Oh, I did not know this! Cool.
No, I just took S = B (B W) (B B C) from Wikipedia.
I disagree, because it only solves it for emacs users.
Try to hit this http://themonadreader.wordpress.com/previous-issues/ cheers jakub Oboza
Are there supposed to be any graphics on these slides? Viewing in Chrome on Ubuntu, all slides are plain white with plain black text. The first few slides are totally blank. Seems like something is wrong here.
Ah OK, must have been some temporary problem with the site. Not blank now. Compared with my Mac - indeed, no graphics.
&gt;And you still end up with Test.hs files scattered all over I can't agree that this is a problem. The majority of snippets hefty enough to warrant a Test.hs are usually worth keeping. I have a snippets/haskell/ folder for this purpose and it's turned out to be quite useful. The other value is when your temporary ghci session turns into something of value and you want to retain it somewhere. Ideally it would be possible for ghci to create a .hs file with all it's current bindings - but using temporary files alleviates this for now.
Agreed - it's disappointing how much better Haskell mode is in emacs than vim (at last with respect to having a inferior ghci buffer). I've tried SHIM and one other ghci plugin for vim, but neither worked well enough.
I would be interested to learn more about what your hopes were, what you tried, and why it didn't work out, should you have the time to elaborate.
This is a pretty good question. But I think it's interesting that it got (so far) a lot more upvotes on SO than here. The reason might be that this question is much more interesting to people who are just starting in Haskell than to people who are more experienced. Not because experience gives you an answer. Rather, because you soon begin to realize that usually it just isn't important to know in which order things will be evaluated. That is a very liberating realization. It marks the moment when you begin to feel the power of Haskell.
Fair point.
Congrats on 200 issues.
I have a snippets folder, but sometimes I just want to check my work when writing a mail or something. If that requires a new type, I previously needed a file, and many times that wasn't a file that was really worth saving.
I absolutely agree that this is a valid usecase. I'm just worried that there's now a danger of creating useful chunks of work in GHCi and then losing them. The more I think about it, the more I'd love the option to spit out current GHCi bindings to a source file. 
Yeah, that'd be handy, too.
How about (in the future) GHCi accepting "offside rule" syntax, instead of the {;} syntax? 
the ticket just says "to run commands under StateT monad based on IO or maybe even any other monad based on IO", so perhaps its not supposed to do anything for arbitrary monads - in any case, I don't get it but 'vivian' seems to believe it doable if we had polymorphic Data.Dynamic (which is however nontrivial). Apparently, Clean has the latter. "By introducing bind at the interpreter level we can invoke side-effects on a line-by-line basis within any monad construct. " unless something changed in the proposed design in the process..
It already does. You just have to go in multiline mode using `:{`, type your expression using indentation as normal, then type `:}` to evaluate it.
&gt; I have a snippets/haskell/ folder for this purpose and it's turned out to be quite useful. Similarly, I have a directory (under source control, even) full of all the bits and scraps of code written for testing or demonstrating stuff on Stack Overflow. A few times I've wanted to refer back to something in there and was very glad I'd started doing that instead of having a bunch of disorganized `Test.hs` files or just defining things in a GHCi session and then abandoning them.
&gt; This is a pretty good question. But I think it's interesting that it got (so far) a lot more upvotes on SO than here. &gt; &gt; The reason might be that this question is much more interesting to people who are just starting in Haskell than to people who are more experienced. Well, keep in mind that "this is a pretty good question" is roughly what upvotes on SO are supposed to mean, as opposed to "this interests me personally" or such. I upvote rather a lot of Haskell questions on SO and it's usually not because I'm personally interested in an answer, except insofar as I then proceed to write one. In contrast, I rarely bother to vote on anything here.
Out of curiosity, what were you hoping `:load /dev/stdin` would do?
A +200 bounty has been posted! Now go amaze me.
Is it too late to make a slight addition to the newsletter? I added a +200 bounty to my question on [Haskell's strictness points](http://stackoverflow.com/questions/7490768/what-are-haskells-strictness-points), and I'd like this to get as much exposure as possible, given how high I'm trying to set the bar for the answer.
No, GHC has a type `ModSummary` which contains the module header information including imports. It's used to do the dependency analysis in `--make` and GHCi.
I really would not like to see /r/haskell overrun with imgur links. They seem to be common lately.
Chrome has some interesting ideas about [this page](http://hackage.haskell.org/packages/archive/algebra/2.0.2/doc/html/Numeric-Decidable-Zero.html) as well, at least for me. I expect it's just some heuristic getting really, really confused by all the weird terms and abbreviations that show up in Haskell code.
Yes it is. Data is written the same way in Danish and English. Nat means night.
And "Ord" means "word". "Show" is commonly used in Danish as well, with its English meaning.
My chrome shows it correctly. What was the translation, btw?
Of course not, Haskell is (barely) [more intelligible](http://www.youtube.com/watch?v=s-mOy8VUEBk) :)
Seconded! Good work!
No, Chrome, Haskell is not Danish.
Add -&gt; eww 
I missed those ;)
I need to keep my secret identity :)
?
Danish has a soft "D" sound at the end of words, and if it's repeated it makes the soft D longer. The soft D sound like a mix of L and the "TH" in "That" or "the" to English speakers. "Add" in Danish is an informal way to write a particular expression of disgust, similar to the English "eww".
This is beautiful...
What's in the nikki.zip?
Read from the console input, allowing me to paste a whole file on the console without having to create a temporary file. Of course, this can't work as GHCi has already open the standard input. Maybe what we need is a special instruction ":load-from-console" that allows you to paste anything in the console (you terminate the input by entering EOF=CTRL-D). The problem would then be "what should we do on :reload since no path is associated the last load?"
Probably [Nikki and the Robots](http://joyridelabs.de/game/download/), a platform game written in Haskell.
Not sure if awesome, nerdy, or both...
While I agree, I don't see a need to be concerned about this just yet. From a quick scan backwards through /new, this is the only imgur link in the last two months. (The cat animation was on tumblr). The occasional bit of humor is fine, as long as it does not obscure the real content.
This is more interesting than it seems from the title: "This package provides the region monad transformer. Scarce resources like files, memory pointers or USB devices for example can be opened in a region. When the region terminates, all opened resources will be automatically closed. The main advantage of regions is that the opened resources can not be returned from the region which ensures no I/O with closed resources is possible."
3rd one was Hutton
Really? I only come to /r/haskell for the funny cat pictures.
Everyone should come!
I happen to know both of you. How much money can I make out of that? :) 
&gt; Cost: $1,600. Location: Portland, Oregon Sigh. Theoretically, someday I won't be a poor college student...
&gt; Duration: 2 days Haskell in 2 days? &gt; This will include concepts related to designing with high-level abstractions, reasoning about and exploiting powerful type systems, and connecting these to issues and patterns that arise in building production-grade commercial software systems. And next weekend: The Tao of Aviation.
The translation only collapsed the whitespace. The words stayed the same.
So much mystery! My curiosity has been awakened...
2 days does seem way too short. I've spent a good bit of my career teaching training classes along these lines, and you'd have to have a remarkably talented group of students to have any hope of leaving them with any practical skills after 2 days of a new programming language. Maybe Galois is betting on the topic filtering for very smart students... but I'm afraid they are underestimating the extent to which teaching in a traditional classroom setting, and charging so much money that everyone in the class was sent by their boss, will act as a filter to ensure they get students with below-average motivation and initiative.
The Tao of Quantum Physics.
I was wondering, what would be the right amount of time for "'Haskell for Kids' for Adults"?
Hmm... depends on what your goals are. In my class, I'm teaching brand new programmers, and our objectives include a lot of organizational skills, creative expression, algebraic methods, attitudes toward technology, and so on... so you wouldn't teach that class to adults. Galois, even in two days, will almost surely teach more of the language itself, too... teaching Haskell is ironically not really one of my goals. So this isn't really comparable to what Galois is doing, or what you'd do with most adults.
Wow, that is a dense nugget of code there. And that's *with* comments! Maybe I'll get the implementation with another hour of study... maybe. I love the Haskell community. :)
By the "atomically $ save Madoka" team. The ICFP winners were announced: http://www.icfpcontest.org/2011/09/results.html
Quantum Physics is magic. There's nothing extraordinary to it.
i've got another version that uses my fifo monad that I find much easier to follow. I'll post it with an explanation when I get home
No chrome, pastry!
To turn it around, why is it a bad choice for this kind of app?
Here, hopefully this is a bit clearer: {-# LANGUAGE BangPatterns #-} module Main where import Data.Function (fix) import Control.Monad (when, liftM) -- local import Fifo -- For convenience, let us define the `digsum` of a number as the -- sum of the digits of the base 10 representation of the number. -- and `coordsum` of a point as the sum of the digsums of its coordinates. -- `spread` creates a fractal pattern out of a function and a finite list spread :: (a -&gt; a) -&gt; [a] -&gt; [a] spread f as = fix $ concat . (as:) . map (\a -&gt; f a : as) -- enumerate the digsums of [0..] digsums :: [Int] digsums = scanl (+) 0 listOfDiffs where -- enumerate the differences between the digsums of [0..] -- * generally we just digsum(x+1) - digsum(x) is one -- * except we subtract 9 from that when we change a digit in the 10s place, -- and another if we change a digit in the 100s place, etc. listOfDiffs = subtract 9 `spread` replicate 9 1 -- a point in our X-Y plane, along with the information we need to get -- its coordsum data Point = MkPoint { _x :: Int , _y :: Int , _xDigsum :: [Int] , _yDigsum :: [Int] } instance Eq Point where MkPoint x y _ _ == MkPoint x' y' _ _ = x == x' &amp;&amp; y == y' -- move to an adjacent point goUp,goRight :: Point -&gt; Point goUp p@(MkPoint _ y _ yDigsum) = p { _y = y + 1, _yDigsum = tail yDigsum } goRight p@(MkPoint x _ xDigsum _) = p { _x = x + 1, _xDigsum = tail xDigsum } -- find the coordsum of a point coordsum :: Point -&gt; Int coordsum p = head (_xDigsum p) + head (_yDigsum p) -- Count all the points that have a path to the point (1000,1000) where -- * all the points in the path's coordsums are 25 or less (including the endpoints) -- * each point in the path is vertically or horizontally adjacent to the next -- point in the path count :: Int count = evalFifo $ do -- do a BFS heading up and right from (1000,1000) for -- all reachable points with low enough coordsums -- (claim: this is sufficient) push $ MkPoint 1000 1000 at1000 at1000 bfs 0 Nothing where at1000 = drop 1000 digsums -- start by shifting a point off the fifo bfs !n overlap = shift &gt;&gt;= bfs' n overlap -- given a point from the fifo, -- decide whether this point is valid, now that we've reached it bfs' !n overlap (Just p) | coordsum p &gt; 25 = -- skip to the next point in the fifo bfs n Nothing | otherwise = do let up = goUp p right = goRight p -- don't go up if the last guy hit that point going right -- (no need to traverse the same point twice) when (overlap /= Just up) $ push up -- always enqueue the point to the right push right -- count this point as valid and reachable -- and continue to the next point in the fifo bfs (n+1) (Just right) -- when we reach the end of the fifo, stop searching bfs' !n _ Nothing = return n -- show the number of points reached main :: IO () main = print count And [here's the Fifo monad](https://gist.github.com/1233293) I'm using to implement my queue for the [BFS](http://en.wikipedia.org/wiki/Breadth_first_search). 
You forgot type system features like GADTs, Type Families etc. Learning a dependently typed language like Agda is a great way to become familiar with these things.
Yes, but they are usually easy to learn once you grasp the basic nature of Haskell.
[Not much I'm afraid](http://www.youtube.com/watch?v=1z6o1GIEsQE). But you should post more!
A clue : I'm pretty sure you dont know each other.
Did you see the advanced topics =&gt; • Combinator library design • Parallelism and concurrency • Server-side programming • Domain specific language design • Performance analysis How would they cover that in two days including ramping up on FP in general, and Haskell syntax. 
...is there already or will there be a video of this talk? :-)
Those two types are equivalent as far as the compiler is concerned. However, using a,a1 rather than a,b might be used to hint to a human reader that a and a1 are related to each other.
From toying around a bit, it seems GHCi uses the following rules to choose type variables: 1) When a value has been given given a type signature, GHCi will prefer to use the same variables as in that signature. For example, the Prelude gives const and id the type signatures "a -&gt; b -&gt; a" and "a -&gt; a". When you ask for the type of "const id", ghci&gt; :t const id const id :: b -&gt; a -&gt; a the variable "b" is from the type of const, and "a" is from the type of id. 2) If the programmer-supplied type signatures in play use the same variable in incompatible ways, its different versions will be numbered. For example, in ghci&gt; :t const const const const :: b -&gt; a -&gt; b1 -&gt; a "b" is the "b" from the first const, and "b1" is the "b" from the second const. 3) If there's no type signature to guide it, GHCi will use the variable "t" (and, by the second rule, "t1", "t2", etc.). ghci&gt; :t \x y -&gt; x \x y -&gt; x :: t -&gt; t1 -&gt; t
GHC seems to attempt to preserve the names given in explicit signatures. So if you write: foo :: (a -&gt; b) -&gt; a it will display the type of foo as such. When two explicitly given variables have to be unified, it picks one of the names to preserve, probably biased by how the unification works out, but I can't say for sure. This sometimes means you end up with more than one variable of the same letter (or string). But, it still wants to use the explicitly given names, so it disambiguates using numbers. When it's a generated type, it seems to use t as a default.
Thanks. I was hoping there was some extra information I could rely on for understanding more complicated types.
&gt; Being able to employ quality developers to work full time on tools like cabal would go a long way. The developers to do that are available, what is a bit harder is finding people/companies that want to pay for it. If you want to fund this then contact either [Well-Typed](http://www.well-typed.com) or the [Industrial Haskell Group](http://industry.haskell.org/).
Sure, I was claiming in this hypothetical some of the $100,000,000 pays for that work.
Unfortunately, it does not volumize your hair until you need it, so it can be hard to predict how much space your hair will take up.
Ruby: rantly
I wonder what 1316741167734.jpg is.
Beautiful...
LOL! This is Maru! :) http://www.youtube.com/watch?v=QZR_6K03gWk#t=01m02s
 ghci&gt; length ['a','b'..] 1114015 ... that's not an infinite list! ;)
Yikes. Good catch! I better add a note about that. :)
Yeah, large finite lists are also pretty nice to be able to handle -- a lot of searches for things naturally give rise to combinatorially large (but still finite) lists that it would still be stupid to compute entirely if you had to do it in an all-or-nothing fashion.
That's great! Fast development, (soft)Real Time, and efficient? Was the talk recorded? Or the slides will be available?
I really wish CUFP would record and publish all talks, and this one looks particularly interesting. Is there a reason why they're restricting the dissemination of knowledge?
That is it? No recording and no information at all?
Given that this talk was yesterday afternoon and that last year's conference videos weren't available on the site until January, we have some time to wait.
I had almost the same code in my course. plus instead of add and recursing on the second variable instead of the first.
I don't think all talks were made available from last year's conference. For example, [all Haskell videos](http://cufp.org/videos/keyword/55) are from the 2009 conference in Edinburgh, and do not include the 2010 talk on [High-performance Haskell](http://cufp.org/conference/sessions/2010/high-performance-haskell). All I can find is [the slides](http://blog.johantibell.com/2010/07/high-performance-haskell-tutorial-at.html).
Since these presentations are by people from companies there are reasons why some of the talks never end up online. 
Oh, I know you.
Ruby (rushcheck): https://github.com/IKEGAMIDaisuke/rushcheck Erlang (triq): http://www.javalimit.com/2010/05/triq-the-free-quickcheck-for-erlang.html
If people don't want the public seeing their talks, why do they give talks at all? How do they know that someone who they don't want to be present, won't be present?
It is not about "don't want to let know" it is about corporate bureaucracy.
For what its worth, that was a tutorial session, not a talk.
Tak! I left Denmark when I was 6...
Perhaps the slides could be made available earlier?
Hash consing is still along the line of memoization. For explicit sharing, wouldn't it be better to avoid it in the first place? Can't help but to put a shameless self-plug here: http://haskell.cs.yale.edu/?post_type=publication&amp;p=263
As heisenbug says, it's about the bureaucracy. It's much easier to get permission to give a talk that has no permanent records than to get permission for a talk that is recorded, or has a paper, or has published slides.
There is something like that, it is called git-wip.
I have nothing to contribute, but I must ask: why would you try to do something so complicated for your first Haskell program ever? Why wouldn't you try something like a small prefix calculator or something?
Sorry, should've clarified - I meant that it was my first *nontrivial* Haskell program.
Do you have an example program for which you interpreter doesn't work right? I've fed it the Hello World and rot13 examples from Wikipedia and the factor example from the [esoteric.sange.fi](http://esoteric.sange.fi/brainfuck/bf-source/prog/) archive without seeing any problems. I can't find any obvious bugs from looking at the code either.
[Lost Kingdom](http://jonripley.com/i-fiction/games/LostKingdomBF.html) doesn't seem to work at all, and neither does [this Life program](http://www.linusakesson.net/programming/brainfuck/index.php). I've tried at least one program that had no input instructions but didn't generate any output until I pressed enter - and it crashed immediately afterward. Also, when I run [this game](http://www.49-6-dev.net/ftotwen.htm), it expects input about one line earlier than it should. It's relatively minor bug, but I have no idea why it does this.
Ah, that's a buffering problem. GHC-compiled programs use line buffering for input and output by default, so if you're not going to flush them manually you want to add hSetBuffering stdin NoBuffering hSetBuffering stdout NoBuffering to the beginning of main. With this all the examples you've given seem to work. They're still depressingly slow, so you want to run them with a CPU gauge handy so you can tell when they're churning and when they're expecting input.
&gt; With this all the examples you've given seem to work. They're still depressingly slow, so you want to run them with a CPU gauge handy so you can tell when they're churning and when they're expecting input. This formulation is simply great for its perspective on the computer as if it were a mechanical device.
Yay, it works now! Thanks!
Newsflash: program written in esoteric language is depressingly slow ;) The question is: is it slow compared to other interpreters?
Would assembly be considered esoteric?
Yes, it is. You can download brainfuck interpreters that are orders of magnitude faster. But at least it works.
The tutorials are highly interactive, so it doesn't make sense to record it. You would just get 3 hours of someone wandering around the room chatting with people, and not much audio.
You mean, like all the videos here? [http://cufp.org/videos](http://cufp.org/videos) Please do a bit of basic Googling before you accuse us of "restricting the dissemination of knowledge". As for this year's videos, we got all the speakers to sign video release forms, so the local Japanese staff (all volunteers) will process them and make them available to us in due course, and I'll upload them to the cufp.org website as soon as that happens.
&gt; If you wanted to use type safe URLs with Snap or Happstack you could certainly do so. In fact, the web-routes package that I believe Yesod uses for it's type safe routing was originally developed for Happstack. Indeed, I'm using Snap happily with web-routes (specifically web-routes-boomerang) and having no problems.
I imagine it might be somehow possible. Let's suppose for a second that in the first day they could bring out all the basic concepts of functional languages, and their implementation in haskell in particular. Surely monads would be a most important point. With that done, on the second day they could simply list code and walk the students through the code; and if at some point questions are asked, pull out the "mysterious" code and break it down even further until it reaches chunks easily digestible. Just a thought.
A brainfuck interpreter isn't much more complicated than a prefix calculator.
This is actually last week's. I got caught up in other things, and am late posting it. Sorry!
Apologies if I've caused offense. I note you've also responded to my [other comment](http://www.reddit.com/r/haskell/comments/kqoe5/fourteen_days_of_haskell_a_real_time_programming/c2mgy81), so you know I have actually googled around in search for a specific recording. You're saying it doesn't make sense to record tutorials — this wasn't obvious to me, at all.
Hi. Just wondering if you plan to publish some of the kids programs? The keyboard looks great :)
I'm pretty sure you could get order-of-magnitude improvements out of that. Normally dropping into the ST monad and just slamming around mutable arrays is considered more-or-less cheating, but I'd argue in this case that that is simply the most natural representation of the underlying problem here; it's what a Brainfuck interpreter _is_ and trying to be all "functional" about it at the interpreter layer really doesn't buy you anything. Remember, Haskell is the best imperative language around! (Not that I'm suggesting you "should" do this change. Just an idea if the fancy strikes you.)
Did the kids license their programs under some kind of open-source license? :)
In one of the early weeks, there was a link to blogs that the kids are writing. If they want to share code, I'll encourage them to do it there. I have no doubt that their final games will be shared toward the end of the year. But understand that school and children have a shaky relationship, and even in the cool subjects, varying levels of effort go into everyday homework stuff like this. I just don't want to embarrass or scare anyone.
Will do. https://github.com/mcandre/resume
btw, was the session video-recorded?
No worries, I'm just cranky with jet lag :-) We've gone back-and-forth on the issue tutorial recordings over the years, but I can't think of how to make it work online. The speakers will often have the material ready from CUFP and can run the tutorial at your local FP group if they're nearby, so you might want to organise a local event. Or, just come along to the next one and drink in the atmosphere in person! 
Awesome!
It fails with IE: Sorry, there were problems with your program: tmp/hSTelj8HIB5cCGgw03B5fw==.hs:1:1: parse error on input `Loading...' I see that you are using a polyfill for EventSource, but it doesn't seem to help :-(
Yes, and I've got permission from all speakers to put the videos on YouTube - I hope to get the copies in the near future.
I'm still collecting all the slides from the other HIW talks, but most should be online this weekend.
priceless: &gt;"Haskell is the best of the obsolete programming languages!" he pronounced, with a mischievous look. Now, I know when I’m being trolled, so I said nothing and waited a moment, whereupon he continued, "but don’t take it the wrong way – I think they’re all obsolete!" By the way the tutorial slides are great.
From [Johan Tibell](http://blog.johantibell.com/2010/09/slides-from-my-high-performance-haskell.html)'s [High Performance Haskell tutorial slides](http://www.slideshare.net/tibbe/highperformance-haskell/37) &gt; ### Reasoning about laziness &gt; &gt; A function application is only evaluated if its result is needed, therefore: &gt; &gt; * One of the function’s right-hand sides will be evaluated. &gt; * Any expression whose value is required to decide which RHS to evaluate, must be evaluated. &gt; &gt; By using this “backward-to-front” analysis we can figure which arguments a function is strict in. 
Hmm... works with IE9. Maybe there's a problem with older IE versions? "Loading..." is what gets put into the source code text field before it gets replaced by actual source code using JavaScript. 
"doubtless after several decades of overexposure to parentheses". heh...
And so soon after I posted week 6... but only because I was so late with week 6.
Typo in title, should be "Finding rectangles in an image"
This reminds me a google jam problem [link](http://code.google.com/codejam/contest/dashboard?c=837485#s=p2) for which I don't think an O(n)^2 is acceptable. 
Hmmm, yet another Web framework. Colour me skeptical, but I can't see the point of this one. &gt; Type safe routes, specify url-handler mapping in one place. Great, `web-routes` already does this. &gt; We can reverse this mapping from handler value automatically, thus don't need to construct url string manually in code, avoiding url errors. Great, `web-routes-boomerang` already does this. &gt; Simple yet elegant handler via type class. Looks trivial to write that for at least Snap, and I imagine you can create this abstraction in any of the big frameworks. I really don't see what new ideas this brings to the table.
Err. http://www.reddit.com/r/haskell/duplicates/kv610/the_evolution_of_a_haskell_programmer/
Ouchie. I searched just this reddit for the link. I'll get rid of it if this gets too many downvotes. :)
Isn't using `nub` in every bind really slow?
In the google jam problem n is at most 50, so O( n^2 ) should be fine.
I never realised until today that the Boy Scout Haskell Programmer didn't actually tie his knot. It should be y f = let x = f x in x.
The Haskell community is always forgiving. 
I've been thinking for several years that fibonacci numbers would have been a better subject than factorial. Maybe it's time for an updated variant. Plus, you could include spiffy new GHC extensions like GADTs and Type Families, and newer idioms such as Applicative Functors.
I'm missing the GHC hacker: *Sends in patch…* {-# LANGUAGE FactorialOperator #-} fac n = n!
I'd like to have a local mirror of Hackage, much like http://search.cpan.org/~rjbs/CPAN-Mini-1.111007/lib/CPAN/Mini.pm for Perl. Should I respond to this questionaire, or is this more about people providing public global mirrors?
That's the most indexed monad I've ever seen!
I don't see a big problem, those duplicates are years old. I'm sure there's been a lot of new Haskell'ers who'd get a kick out of this since then. I only found this a few weeks ago from a SO thread, for example.
 (!) = fac where fac 0 = 1 fac (n+1) = (n+1) * (n!)
I am currently advertising a PhD Studentship in Functional Programming at the University of Nottingham in the UK. Due to the nature of the funding, the position is only to UK/EU students who have resided in the UK for the last three years. If you are interested in applying, please drop me a note to &lt;graham.hutton@nottingham.ac.uk&gt;. Many thanks, Graham.
Always good to read this once in a while. 
looks as if it gets reposted about once a year... thus it was already overdue =)
This is Haskell. Beauty before efficiency.
I'm missing the 'Oleg' category, i.e. doing the equivalent of `product [1..n]` on the type level.
Beautiful efficiency. No compromises.
I like it.
The nice thing about Hadoop is that they have done a lot of the plumbing of taking the compiled code and automatically distribute it to the entire cluster (i.e. moving the code to the data, not the other way around). Also, there is a very big eco-system around it. Therefore, it would be awesome if you could find someway to combine the two: build this on top of Hadoop.
The nice thing about Hadoop is that they have done a lot of the plumbing of taking the compiled code and automatically distribute it to the entire cluster (i.e. moving the code to the data, not the other way around). Also, there is a very big eco-system around it. Therefore, it would be awesome if you could find someway to combine the two: build this on top of Hadoop.
Have you looked into some of the code on Hackage? I know in theory Haskell can generate very optimize code, but in practice most people (at least the code I've seen on hackage) do not care that much about efficiency. If you can't be objective about it and just want to remain a fanboy, then I have nothing to add.
Yes I've seen those libraries. I curse their names, uninstall them and look for a library that isn't terrible.
I'd hardly expect it to bring new ideas to the table at version 0.1 - give it some time.
Will there be any videos of the lectures? Maybe on iTunes U? That would be awesome!
Why? To me, if I'm going to start a new project and release it, I want to be pretty sure I have an idea it solves a problem that no one else has solved, or do something uniquely different. Otherwise, my time is probably better spent helping the other projects.
Well done Max! David did a great job getting the LLVM backend into GHC, but I think there's still a lot of work to be done to maximize the benefits.
Nice post, and TIL about _strength reduction_ as a general concept.
OCaml also permit this, and it's really annoying to have it missing in Haskell. In situation like this, I often has to duplicate some code, any better way to handle this kind of cases? (beside the obvious refactoring into a function)
Rather than extending the case syntax, I think introducing alternatives to the pattern grammar is better. We can't use ',' though as it's already used in tuple patterns. I like '|' with the proviso that it must occur inside parenthesis so as to prevent conflict with the guard syntax. (It also matches the syntax for sum types.) data X = A Int | B Int | C Int f :: X -&gt; Int f (A x | B x | C x) = g x My two cents.
Thanks a lot for dia-base. I use your module for producing output easily for some haskell experimentation. A well done and simple module!
pretty good! 
A new class called MonadFail?
Interesting. I once made a clone of a TwiML evaluator in Haskell.
I suggested on the StackOverflow thread that some of these cases may be covered by record syntax; perhaps it's so in your case? For example data Foo a b = Bar { x :: a, y :: b } | Baz { x :: a } | Quux { y :: b } funcThatSharesBarAndBaz (Quux {}) = 3 funcThatSharesBarAndBaz v = x v funcThatSharesBarAndQuux (Baz {}) = 3 funcThatSharesBarAndQuux v = y v
grep for "Static Haskell Programmer" =)
That's okay. Every time I revisit this I find I can understand a few more of them. Scary.
Gray on white :(
Very nice writeup though
don't know if you ran across this but the slides are online http://www.scs.stanford.edu/11au-cs240h/
Sadly outdated. Where's the shim with polymorphic recursion to let you call the type level program at runtime?
Very nice! A few more of those and we're all set! :)
I don't think many Haskell programmers turn to books as much as experience and common sense. Haskell isn't based on too many conventions. It's simple enough that, if you think your way of doing things is simple, reasonable and concise, everyone else will tend to find that just dandy.
There's not too much info of this kind out there. Probably the best is the video about the design and implementation of the XMonad window manager. 
I was blown away by all the things that were **NOT** there in 2005: * ghci * debugging * any parallelism/concurrency * ghc -O * ffi And many others. Some things still seem to be hanging around * Records * The Simon^2 bus factor But knowing how far GHC has come, I have little doubt in the abilities of the community. (I'm even cloning the GHC repo to poke around a bit!)
You're misreading the survey. Ghci, GHC -O, FFI, at least, were listed as killer features already existing, not as items on a future wishlist. Likewise, parallelism/concurrency were there to some degree, but there were complaints about how powerful they were(n't).
The architecture of many projects is *approximately synonymous* with how their monad or monad stack works (or how they use arrows, or how they can manage with just an applicative...). This captures the *how* of how they work. As Erikd says there isn't much other info out there. 
One great thing about Haskell is that the type system makes it easy to change your mind. If you realise you made a poor design decision in the beginning just go back and change it, the compiler will then tell you where you need to make adjustments in the code.
If anyone has an example of a tight loop in Haskell that isn't going as fast as they expect with the LLVM backend (compared to e.g. a C equivalent), send it to me. I'm interested in diagnosing+fixing more of these issues.
I do think it's hard to follow along for people unknown to Haskell or functional programming. For me, as a beginning Haskeller, it was a great exercise. I hope you don't mind, but I have put the HTML online so you can see the presentation as it was, instead of the markdown file. http://f.cl.ly/items/0B0K371h0z0L1c3k3a1N/haskell-amuse-bouche.html#(1)
This seems like it could be extremely interesting, but the fact that we can't see the slides is kind of a bummer.
 (!) = fac where fac 0 = 1 fac n = n * ((n - 1) !)
I think you still don't understand currying. Both of the following two functions are curried. foo x y = z foo' x = \y -&gt; z They are merely notational variations of the same thing. The uncurried variant is foo'' (x,y) = z Currying takes a function of type *(a,b) -&gt; c* and gives you a function of type *a -&gt; b -&gt; c*. The uncurried function takes as its *single* argument a *pair* and gives you back some value, while the curried function takes as its argument the value that *would have been* the first element of the pair argument to the uncurried function, and gives you back a function that takes as its argument the value that *would have been* the second element of the pair, and gives you back finally the value that you would have gotten by using the uncurried function.
Thank you for your explanation. I agree that I probably don't understand the full theory behind currying. The way I understood it was that these are both the same thing, but the first one in Haskell, the second one mathematical: ``foo x y = z`` ``foo' (x,y) = z`` Do you have any pointers on where to explore the topic further so I can improve my post? Thanks again and I hope you at least enjoyed my effort at an explanation :)
This was a good talk.
Visual Basic for Applications.
no, they're both in Haskell (and both mathematical!). The first one is just a higher order function, namely, *foo = \x -&gt; \y -&gt; z* while the second is *foo' = \(x,y) -&gt; z*. The first takes a value and gives you a function that takes a value and gives you a value, the second takes the two values as a pair and gives you the third. I'm not sure how you could improve the post, to be honest. It doesn't touch on currying *at all*. Essentially, this is currying: curry f = \x -&gt; \y -&gt; f (x,y) All currying is is taking a function with pair arguments, and turning it into a higher order function that collects up the elements of the pair one at a time. As for where you could explore it more, I don't know. Currying isn't a very deep thing, so people don't really have much to say about it. You just have to remember that if you have a function that requires a pair argument, you can always get some values one at a time and stick them in a pair before giving it to that function (and similarly, you can always take a pair and feed its elements into something one at a time, ie uncurrying).
Here's [part1](http://www.youtube.com/watch?v=EP0UgtZ9EDQ) [part2](http://www.youtube.com/watch?v=zeNcgnmBPv8) [slides](http://xmonad.wordpress.com/2009/09/09/the-design-and-implementation-of-xmonad/)
There are two ways that "currying" comes up. One is literally within Haskell, where, for example, the Prelude functions `curry` and `uncurry` convert between functions on tuples and their curried variants. So, for example, both `foo` and `foo'` in your post above are valid Haskell, and it turns out that `curry foo'` is the same function as `foo`, and conversely, `uncurry foo` is the same as `foo'`. Prelude's `curry` and `uncurry` only work with functions on two parameters, but you can extend the idea, and write your own combinators, to more parameters. A similar thing happens, though, with your mental *model* of a function, and you can think about currying there, too. I think this is where you're coming from when you wrote this. According to Haskell, the function types f :: a -&gt; b -&gt; c f :: a -&gt; (b -&gt; c) are the same. But a typical Haskell programmer, upon seeing the first, thinks "`f` is a function that takes two parameters", while upon seeing the second, thinks "`f` is a function of one parameter, and returns a function." Haskell doesn't distinguish between these ideas at *all*, but the two type signatures communicate different things to human readers. The second explicitly suggests that you should *think* of `f` as a higher-order function, and not as a function of two variables. It's also perfectly okay to think of the *mental* change between those two perspectives on the meaning of `f` as a kind of currying. You'll get a lot of people arguing with you in the Haskell community, but that's because a lot of the Haskell community has a particular philosophical bent toward being very literal and formal about everything. The idea is still the same in both cases: currying is the change from looking at a function as taking multiple parameters to looking at it as a higher order function; and uncurrying is looking at a higher-order function (specifically, one that returns a function) as a function of multiple parameters. Getting comfortable with making that transition of perspectives smoothly will be worthwhile in the long run. 
Thank you for your explanation. I think I was indeed hang up on the mental model for currying, trying to distill it from LYAH, posts and wikipedia and not from actual usage. Your explanation helped me a lot and thanks for taking your time to write it down.
Ough, you are being harsh for a beginning Haskell programmer! I appreciate the honesty though! I do understand that for you it must seem as an idiots approach to currying, but for me, and I think many beginning Haskell users, writing it down as I did makes it easier to start using "partial function application". Thank you for taking the time to show me how a more experienced Haskeller looks at the subject. Although you don't seem have faith in the post, I will try to improve it so it reflects better on the material.
Yes, I saw those. I was just hoping that the lecture would also be video-taped and released. Found this to be brilliant with the "Developing Apps for iOS" [1]. Also a Stanford course. [1]: http://itunes.apple.com/us/podcast/cs193p-student-final-projects/id395605774?i=90218598
Blast! You're right of course. My humblest apologies.
I'll talk to Wren and see if we can get a copy of the slides.
I don't really think psygnisfive is being harsh; it's hard to offer ways to improve a post if it is indeed based on a misunderstanding.
Sorry for the harshness, it's just that the post is simply *wrong*, and so there's not much can be done to fix it without scrapping it. :\
The slides for this and my other talks are up on Christopher's webpage: http://www.cas.mcmaster.ca/~anand/DSL2011.html
A newer version of the slides that I presented at AMMCS later in the summer are available here: http://llama.freegeek.org/~wren/pubs/smoothing_ammcs2011.pdf
When you are right, you are right. I'm indeed chucking this post away. Currently in the process of collecting all the comments and with the help of those write a new post which _does_ explain currying.
[Part I](http://alfredodinapoli.wordpress.com/2011/09/12/lets-make-an-elemental-type-system-in-haskell-part-i/) [Part II](http://alfredodinapoli.wordpress.com/2011/09/19/lets-make-an-elemental-type-system-in-haskell-part-ii/)
Great Serie! I found it very fun... It has a "wizard" charm!
What's the motivation? Access to CPAN? Check the Perl bridges on Hackage.
My motivation is that I will have to, in about 6 months, have to write code that interfaces with a pre-established set of libraries written in perl. Looking at my options, currently. I just looked into parrot, and the possibility of writing a haskell-to-pir compiler seems promising. But I'm not sure at this point if that will achieve my goal. There's more reading to do. I'll check out those Perl bridges.
Parrot / PIR may not be a great fit for what you need, then. AFAIK Perl 5 on Parrot isn't really a thing beyond some experiments.
If you want to interface perl with Haskell I think using the FFI to interface with SWIG would work best. http://www.swig.org/papers/Perl98/swigperl.htm I don't know enough to be sure it's possible though.
Thanks everyone :)
Such a shame that Haskerl was not finished, and then the name was stolen for an Erlang bridge. http://groups.google.com/group/comp.lang.perl/browse_thread/thread/7f7246a58bb26471/bfe0b267404eeb81
&gt; It comes with a price, though: a humongous learning curve. Going from PHP, Ruby or Python and into the world of Haskell is not trivial. There's the challenge: To negate this statement. I believe it need not be true. I'm out to find a way to teach these programmers Haskell in a way that'll get the going as quickly as it would take them to learn node.js!
Is Haskell really memoizing the results for the Fibonacci sequence? I thought you had to write it like so: fibi n = fibiList !! n fibiList = 1:1: zipWith (+) fibiList $ drop 1 fibiList So that the results were stored in a global list in order to get menonization. 
While you're right, fib 40, as in the OP's code, actually only gets executed once, it'd be a different kind of story if the 40 was an actual parameter specified in the HTTP request: The code is going to serve the exact same ByteString every single time. But then, I doubt memoising that list you have there (that is, having it be a [CAF](http://stackoverflow.com/questions/5032750/how-to-tell-whether-haskell-will-cache-a-result-or-recompute-it/5032920#5032920)) is beneficial for performance in that case, just adding up numbers is faster than traversing kilometres of linked list, trashing the cache, or worse, in the process. That's still no excuse to use a quadratic fibs implementation, of course. Oh, and the fibonacci sequence starts with 0, and you can use `tail`, there.
You could have isWeakTo :: TargetableUnit -&gt; Maybe Element -&gt; Bool m `isWeakTo` elem = case elem of Just e -&gt; case checkProperty m ((==e) . succ) of Just b -&gt; b Nothing -&gt; False _ -&gt; False Or, in do notation and using fromMaybe :: a -&gt; Maybe a -&gt; a isWeakTo :: TargetableUnit -&gt; Maybe Element -&gt;Bool m `isWeakTo` elem = fromMaybe false $ do e &lt;- elem checkProperty m ((==e) . succ) because if you have no element, than the question "Are you weak to {x}" where x is any element is False. This cleans up the rest of the code somewhat.
You seem to vastly underestimate the amount of effort it takes to write a Haskell compiler. I suppose a GHC backend shouldn't be so hard, though.
Well, that function is a lot quicker than the quadratic implementation, that's for sure. What you really want is a better memoisation primitive; [uglymemo](http://hackage.haskell.org/package/uglymemo) seems to be the fastest in most cases, but [data-memocombinators](http://hackage.haskell.org/package/data-memocombinators) and [MemoTrie](http://hackage.haskell.org/package/MemoTrie) are basically more efficient expressions of list-based memoisation.
I'm currently working on a project where we've been prototyping the front-end in pure javascript, pulling data from a very simple collection of backend feeds using JSOP. We're ready to start creating some middleware (to proxy some non-JSONP friendly services, cache some of the fancy data analysis we've been doing in the browser, etc). I love Haskell, and I'd love to write my middleware in it, but... I'm not the only developer on this project. The other guy's come to javascript from a pure Java background, and he's come pretty much 0-60 on javascript during the course of this project. So I'm proposing Node.js for our middleware. It means we'll be able to migrate some of code back from our prototype front-end almost directly into the middleware, and my partner doesn't have to learn yet another language (and I don't have to gouge my eyes out and do it in Java). The recent Node.js bashing has got me a little nervous, but it has also provoked me into some defensive reading.
One (side issue) this article doesn't deal with is this original statement: &gt; Conceptually, this is how any web application architecture that's not cancer still works today: you have a web server program that's job is to accept incoming requests, parse them, and figure out the appropriate action to take. That can be either serving a static file, running a CGI script, proxying the connection somewhere else, whatever. The point is that the HTTP server isn't the same entity doing the application work. Developers who have been around the block call this separation of responsibility, and it exists for a reason: loosely coupled architectures are very easy to maintain. &gt; And yet, Node seems oblivious to this. Node has (and don't laugh, I am not making this shit up) its own HTTP server, and that's what you're supposed use to serve production traffic Now I know this is Snap specific, but it does bother me that the de facto way to deploy Snap at the moment is reverse proxying to your Snap executable. While it is blazingly fast, and it's not the only way to deploy, there is definitely something nagging me as to way this needs to be the case. Anyone have any thoughts on that?
An equally interesting question is how to design a new language with Haskell-like features that is easier to learn. What would you change? Would you sacrifice anything? (What are the low-hanging fruit; is there some aspect of Haskell that is obviously confusing to new users yet not greatly contributing to the language's power?) Could you do it without sacrificing _any_ power?
Snap actually is quite loosely coupled. The reason to roll everything into one executable is simple: Inter-process communication doesn't come for free, and iteratees do a way better job chaining and logically separating stuff, while providing tight-loop performance, than even zero-copy unix pipes/sockets. From an application programmer's POV, working with a raw iteratee interface is *way* more high-level than CGI, which is, after all, slightly castrated HTTP over non-tcp transports: CGI means you parse virtually everything twice. That is, do note that Snap (or warp, or any other modern Haskell web server) is faster on its own than in combination with apache, ngnix or what have you, allthewhile providing a vastly superiour application interface.
&gt; That's still no excuse to use a quadratic fibs implementation, of course. The point of the original post (the one this post was responding to) was to have a slow computation in the handler. A quadratic fib implementation was just an easy way to do this. It makes the point more clearly than a sleep() IMO.
Hehe, don't worry, I'm reading it right now and you did a pretty good job - but I'm only at the beginning (As an Xmonad user I was pretty interested in learning more about Haskell, so I was reading the original english one when I stumbled upon this link - it made my day!). /french awful gibberish Merci les gars! Bon boulot, même si de temps en temps certaines expressions peuvent sembler bizarres (genre, "sucre syntaxique", mais au final ça passe tout seul, je dirais même que c'est rafraîchissant ;) ) Si je vois des fautes d'orthographe ou de grammaire au cours de ma lecture je vous le ferai savoir :) /no more frog-eater stuff ^^
I giggled at uglymemo's "pure" memoization function: memo f = let f' = unsafePerformIO (memoIO f) in \ x -&gt; unsafePerformIO (f' x) ಠ_ಠ
That function is pure.
Nice write up :)
Yes I'm aware that `unsafePerformIO` has type `IO a -&gt; a`. Sure it's pure. Pure evil. ಠ_ಠ
Your nagging suspicion is well-founded. The WAI interface was built to solve this problem. Currently, in practice, most people want to deploy with the WAI warp server (or the snap server if they are a Snap user) anyways, so WAI is more about different web frameworks re-using warp rather than any one framework having the option to deploy with different servers. But that may change in the future, which is also the point of WAI. Two new frameworks were recently released that use WAI, and one of them (webwire) uses an entirely different paradigm (FRP - continutation-based).
One can achieve loose coupling with the server and still have a single executable- that is how most Yesod users are using WAI.
I only think this is possible for someone know to programming or with a different background. If you are already programming PHP, Ruby, or Python, node is going to be easier to learn. That being said, we should seek to lower the barrier to entry as much as possible, and we can target being easier to learn from other languages (perhaps C++).
Haskell is only a "humongous learning curve" because it is significantly different than other languages, and Haskell is rarely the first language a programmer learns. If you learn Haskell first, then it would be a "humongous learning curve" to learn PHP. (How do I foldr in php?) If you learn PHP first, then you have probably built up a humongous unlearning curve you will need to deal with. Fun fact: `array_reduce(array, func, initval)` can be used to perform a fold in PHP, but "Prior to PHP 5.3, `initval` can only be integer." [Wikipedia: fold](http://en.wikipedia.org/wiki/Foldl) o_O
`unsafePerformIO` is not pure. `memo` is pure. BTW, `unsafePerformIO` is even I think used inside the `base` library, so if you're going to ಠ_ಠ everything that uses it you have a lot of work cut out for you. It's meant as a tool to implement functionality that wouldn't be possible otherwise. As long as the code using it is pure as a whole, it's perfectly acceptable.
i think ocaml (and sml) are already significantly easier to learn than haskell, simply by sacrificing immutability and laziness-by-default.
there also isn't the problem of so many type extensions to sort through. This cuts both ways of course, since it is often nice to have these options, but it does increase the learning curve.
Wow, that is some really nice looking web design.
unsafePerformIO exists for benign effects, like this one. It's perfectly safe to use unsafePerformIO so long as you have a proof that the IO you perform will not change based off of when you execute it relative to the main branch of IO. They put it in the language for a reason. While there are many invalid uses of it, this one is perfectly legitimate. 
It's only meant to be used in a context where your program can't be mucked up by compiler transformantions like inlining and common sub-expression elimination. None of the uses in the base package satisfy this requirement, nor for that matter does the allegedly pure memo function AFAICS. What happens if it gets inlined? Well I guess this is probably harmless in the denotational sense, but not in the operational sense (you'll lose the memoisation you were hoping for). These are the sad but true facts.
I agree this is a legitimate -- you might even say "safe" -- use of unsafePerformIO (though you have to wonder if turning on ghc optimizations are going to mess it up). But I tend to agree with Luke Palmer's [whole program fallacy](http://lukepalmer.wordpress.com/2011/05/20/the-whole-program-fallacy/) train of thought; in my opinion it would be better to just use `memoIO` while you're inside the IO monad, and then pass the memoized function wherever it needs to go.
That seems very very useful. Have my upvote.
Wait, do you want to actually transliterate (compile) Haskell code into Perl code? Or do you want access to code written in Perl whilst coding in Haskell? If the former, you may want to check out the [Awesome Prelude](https://github.com/tomlokhorst/AwesomePrelude) and how it is used to compile Haskell to JavaScript.
The type of `($-)` there is overly specific: ($-) ∷ (d → a → d) → (d → k) → d → a → k ($-) f g d a = g $ f d a Whereas after normalizing the type variable names, ghci infers: ($-) :: (d -&gt; a -&gt; e) -&gt; (e -&gt; k) -&gt; d -&gt; a -&gt; k Though I find: ($-) :: (a -&gt; b -&gt; c) -&gt; (c -&gt; r) -&gt; a -&gt; b -&gt; r more readable. I think the more general type makes it clearer what argument the second argument takes (with the first type signature it may either be the given `d` or the result of the first arg). When written like this, it becomes visible that it is just `flip $ (.) . (.)`.
Yeah, I have my poor man's implementation of `any` and `all` in php :)
When in Rome, do as Romans please! The poor bastard who has to follow up your code.
My colleagues like functional programming slightly more every day. Also they agreed on the code when they reviewed it. Plain an simple, they have seen the practically of these two functions in the context I've used them :)
Post it here, and see if the equivalent "php-way" is simpler/more-readable or not.
The link for the slides in pdf is dead...
Nice catch. I have not tried using it like that, but it does seem a lot more powerful. It allows to do things like $- ($)
I don't know of any reason Node.js would be any worse than other event based libraries, and with V8 it's probably at least faster than ones in other scripting languages. The generic downside of explicit event-loop plus callback designs is that much of the ease of programming (such as is left after manually cps-transforming your code) comes from only running on a single core. That means spans of code between "blocking" calls run atomically. Even if it is technically feasible to dispatch callback execution over many worker threads, that pseudo-STM property would be lost. To the extent you'll need to deal with IPC to scale out (if only to two boxes for a modicum of fault tolerance) it might not be so bad to run additional copies on each node if you want full CPU utilization.
&gt;Oh, and the fibonacci sequence starts with 0 [http://www.google.com/search?q=fibonacci+rabbits](http://www.google.com/search?q=fibonacci+rabbits) 
&gt; After all, ADA language is used into mission critial software, and Haskell has been influenced by ADA: There will be a reason, after all, don’t you believe? What?
I find it interesting that your formulation is more readable to me, in that the original one is type letter salad to me that I didn't bother decoding, and rewriting it with a, b, c, and r made it immediately understandable to me, even though it actually has one more distinct type in it. I knew there was convention in the letters used for types, but I did not realize how powerful they were until you showed me.
I think Italian is his native language.
Memoisation of a pure function is simply an optimisation; plenty of compiler optimisations can be broken by other optimisations a compiler does. But it works in practice, which is what matters when optimisations are the topic.
But it *is* safe, and pure. A top-level IORef exposes possible side-effects and the like that would not otherwise be available; there is literally no way to exploit the `memo` function in this way. The fact that `memoIO` is used to implement it is just that: an implementation detail.
But both immutability and lazy-by-default are core to what makes Haskell be Haskell. Laziness definitely takes some unlearning to get over what they beat into your head in other languages. But without quarantining mutability many of the nice things simply wouldn't be possible, or if possible then wouldn't be worth the cost. E.g., many people have tried STM; the reason it actually works nicely in Haskell has a whole lot to do with the lack of mutability.
Actually, there are a few people who are using FastCGI in production.
"any" and "all" aren't that big a deal. They're straightforward to implement with for loops over array indices, if nothing else. Getting funky with "fold" would be a different story.
The big usability problem with that approach is that the returned function is `(a -&gt; IO b)` which forces you into IO to use it. Personally I'd say that `unsafePerformIO` should only be used on that inner IO. It's bad to erase the outer IO because you're loosing the fact that this involves allocating a memo table, and that you really want to do that only once and share it everywhere. Whereas, provided the implementation is threadsafe, the inner IO is truly just an implementation detail.
&gt; As long as the code using it is pure as a whole, it's perfectly acceptable. Not necessarily. There are cases where two independent uses of `unsafePerformIO`, which are independently valid, can be combined to result in something which is invalid. So, while generally true in spirit, you need to be careful of the details.
Haskell was pretty much my second language I really programmed in (after teaching myself perl) and I find JavaScript to be just an inscrutable mess, although I've invested many hours of my life trying to become competent with it.
Well, I wouldn't call those uses independently valid, then. :) But yes, there are obvious subtleties. I don't think uglymemo falls prey to any of them, because memoisation is purely an optimisation.
What exactly is that title supposed to mean? Sounds like complete gibberish to me. Could someone translate that sentence to (conventional) English? thx.
Wow, you are awesome, bos. I'd just like to say thank you for all your contributions to the Haskell community. You didn't have to open source all that code as a startup, but you did, and for that: maximum respect.
Ops, sorry for my English :P yes, Italian is my native so don't be rude I'm still learning :D I just wanted to say that Haskell syntax is similar to ADA one (watch the type declaration, or ranges). So, ADA : imperatives languages = Haskell : Functionals languages That's all :D
The title of the post is the original title of the book :) http://learnyouahaskell.com/
The slides are great... makes me curious whether there'll be a video to accompany them eventually... :-)
 myArray::any( function($item) { return $item-&gt;isDeleted(); }, $collection, );
I don't think it's an acronym, so just write it as "Ada" and people are more likely to know what you mean.
I've got a question regarding the bootstrapping process, as I'm curious how this works in criterion: 1. You repeatedly measure the evaluation time of a function *f*, and get a series of time-lengths. 2. Then you resample by using random subsets of the values collected in 1., and then based on those subsets you estimate statistical parameters (do you assume a standard-distribution here, and estimate based on that assumption the mean &amp; std.-deviation parameters?) 3. you use the estimated parameters from 2. to categorize the outliers of 1. (by defining a confidence-interval for std-deviation based on the estimated parameters?) did I get this right?
Remove significant whitespace, use words not symbols for things. Both of these things annoy a hell of a lot of programmers.
Concerning the `Prelude: head: empty list` message, I have to say that it has never been an issue for me. That's probably because I only use it when internal invariants guarantee safety. (Also, I give each use of `head` a different error message). If the exception still happens, then I know that my invariants are screwed up completely and I have to rethink and simplify everything anyway; so there is not much use in trying to find the original bug as the code is going to the trash bin anyway. 
&gt; A top-level IORef exposes possible side-effects This is an untrue fact. Only unsafePerformIO does that. 
Given Haskell supports explicit braces as well as significant whitespace, and virtually all choose to use the latter, I think removing significant whitespace would annoy a whole lot more programmers.
Did you know that the function (+) uses `unsafePerformIO` under the hood? Underneath that pure interface, it actually mutates and clobbers CPU registers!
On my (bad contrast) CRT I can barely read some of the text...
I don't know if the Ada language appeals to many here :-)
There is the [Snap](http://snapframework.com/) framework but I haven't used it yet.
Luke is right to attack the idiom used by the OP, which really is "evil", not because it makes use of a top level MVar, but because uses unsafePerformIO to read that MVar from a (not) pure function. But even if doSomething was in the IO Monad, this would still be sloppy (but safe) practice IMO. But I can't help suspecting that this is just another "straw man" attack on top level state in general. It's unfortunate the OP in Stackoverflow implied that this idiom was commonly used. I've never seen it used, but there are other perfectly reasonable (in fact unavoidable for anyone concerned about safety) uses of top level mutable state in IO libs.
Yes, there are a few workarounds. But none of these work if the error occurs in an external library.
happs is now happstack: http://happstack.com/index.html This thread has a good (if dated) comparison of snap and yesod: http://stackoverflow.com/questions/5645168/comparing-haskells-snap-and-yesod-web-frameworks
I don't think that's a valid argument, given that most people programming haskell are haskell programmers, not most programmers. But come to think of it I don't think it's so much the whitespace, as the confusion as to where block constructs start. Too many different ways to begin one maybe? =, do, where, in, let, any others? Then again I guess ruby has def, do, class, module, {, where, if, unless. I dunno.
&gt; happs is now happstack That explains why my "happs vs yesod" Google searches were in vain!
Bryan, great library smith - thanks you rock!
Heh, I generally moved to laptops and LCD TV, and only recently came back to use my oooold desktop setup :-)
Snap is well regarded, well implemented, and of the "big three" most like the base structure of web frameworks in other languages. If you've coded web apps in other frameworks before, start with Snap. It will be the easiest to get into.
The Haskell report spells it out quite clearly. The following start a new block: let, where, of, do. Not that many to learn.
Right, I know of it - but I still don't understand what its saying, or why it is saying it so strangely. "Learn you a haskell" - Is this just "learn Haskell", with 'you' and 'a' being intentionally wrong (but why?). "for great good"? I don't get it, for good what? EDIT: hm, apparently this has been asked on stack exchange http://english.stackexchange.com/questions/165/what-does-great-good-mean-in-tutorial-title-learn-you-a-haskell-for-great-good so, according to answers, perhaps its from Borat? if this is true and someone has an idea where exactly it comes from, I'd love to see the relevant clip.
I'll try it as soon as possible. I'm looking for an alternative to PHP and I'm currently using node.js because it has very simple support for socket.io.
You are wellcome, I am happy that it was useful for you!
I tried to decide some times ago. In the end my preference goes to Yesod. The advantages are tiny and most subjectives, here are the reasons: 1. Hamlet is almost Haml, Cassius is almost SASS, Lucius is almost SCSS and Julius is javascript with type safety added. And type safe is good. 2. Yesod is almost at its 1.0, from now, yesod 0.9.2 is more a realease candidate for the 1.0 3. Contrary to the examples given in the documentation, most of the time you don't use quasiquotation. 4. Greg Weber (one of the main Yesod contributor) was the first to give a way to deploy yesod (and more generally Haskell) to heroku. 5. Widget is a very clever idea I never saw anywhere before. From what I understand you can use many part of the yesod web framework inside snap and conversely. Also, from some benchmark, it seems the standard way of deploying yesod is a bit faster than the standard way of deploying snap. But, I also believe you can use each method with snap and yesod. I didn't looked at snap neiver at happstack from some time now. But to be short: - I believe yesod has more feature. - Yesod might use a strange syntax _but_ this is for good (Type safety) You should read the introduction of the yesod book. Also here is an example that proove yesod rocks. Recently, somebody posted a troll article "node.js is cancer". He gave an example of with a fibonnacci function. Somebody answer that haskell might be the cure and used snap to demonstrate its point (http://mathias-biilmann.net/posts/2011/10/is-haskell-the-cure). Here is the equivalent solution using yesod: http://gist.github.com/1261882 Its behaviour is perfect as expected. Only the first access is long. Once the fibonnacci value is calculated, the answer is cached and served extremely fast. As the code is minimal I used quasi quotes.
I highly recommend happstack, which I use for my own services.
Now that someone has written down what needs to be explained, how about someone post a followup actually explaining it? So, "::" is "has type"? What about "-&gt;" and "=&gt;"? How should one read a complicated type for a function? What about other weird operators? are "&gt;&gt;" and "&gt;&gt;=" both "bind"? Should I not be calling "++" "plus-plus"? What do I call "&lt;-"?
Can you explain what makes Snap closer to web frameworks in other languages than Yesod? I would have said Yesod is closer, simply because it provides more integrated tools (ORM, for instance) and a more sophisticated scafolding.
websocket support is just now being implemented in Haskell. There is code already for server-sent events. 
http://www.haskell.org/haskellwiki/Pronunciation (looks like some stray code tags on that page now -- could use a little work, if anyone feels up to it!) Sometimes, depending on context, certain words can be omitted, just like we don't say "period" or "question mark" when we read sentences out loud. For example show :: forall a. Show a =&gt; a -&gt; String I'd pronounce that as "Show has type for all a, show a, a to String." do a &lt;- show &lt;$&gt; (doIt =&lt;&lt; getIt) return $ a + 5 I'd pronounce that as: "Do. A from show fmaped over doIt from getIt. return applied to (or simply 'dollar') a plus 5."
How else do you create a top-level IORef?
Some people say that Haskell is the best imperative language; I sometimes think that ML is the worst functional language. In C and C-like languages, the effects mostly occur in statements, which are explicitly sequenced with semicolons. It is at least plausible to trace invariants of variables by sliding propositions in between each statement (this makes the {proposition}statement{prosition} sequence around each statement into a Hoare triple). Sadly, you can write expressions with side effects in C-like languages which disrupts this process (I like to think of BASIC (or some form of BASIC) as a purely imperative language where effects can only occur during statement execution). In ML, there are no statements, and all effects are side-effects of evaluation. I imagine this makes tracing invariants of mutable variables and other effects next to impossible. I suspect the only reason ML programs work at all is that ML programers fake the sequential structure of C programs by using sequences of let statements. Edit: After reconsidering, I think my argument shows that ML is the worst imperative language rather than the worst functional language.
For lowest entry barrier i would recommend Snap. I myself use Yesod. Reason: hamlet. You can use hamlet in Snap.
Fair enough. Of course, debugging external libraries was never particularly pleasant in the first place.
I guess the words I say in my mind for `Ord a =&gt; a -&gt; String` are roughly: "a to String, given that a is an Ord"
You could also just say "is a" for "::" as in: "`name` is a String" or "`length` is a list-to-Int"
I meant in the sense that in Snap it is easy to see the basic, common pattern of "my handler is just a function from http request http response" which while low level, is pretty universal. Seems to me that Yesod immediately tries to impress you with several DSLs, an ORM, etc. I realize that you can use Yesod ignoring those things, but it isn't the face it presents. Those things, while powerful, I think make the learning curve too steep for some coming from popular web environments.
Happstack has a pretty thorough tutorial with lots of little examples that can be downloaded, run, and modified: http://happstack.com/docs/crashcourse/index.html It's a pretty solid framework and offers a lot of flexible options for templating, routing, etc. 
&gt; There is code already for server-sent events. Where? (I did try Google.)
Can you explain some things that make Snap easier than Happstack?
I've been using Happstack for almost 3 years now, and in fact my first "bigger" application ever written in Haskell was a Happstack webside. It's definitely a fun way to get more familiar with Haskells type system, learning to read haddock pages and get actual results by doing that. I have also used Snap before, but (at least at that time, it's been a while) I found it to be very limited in functionality compared to Happstack. I also had a quick look at Yesod, but as you said the template haskell part put me off pretty soon again (and I still don't like TH too much, even though I got reasonable good at it). It seems to be quite popular though.
First, not all Haskell code is written by yourself. You use people's libraries, and others' code all the time. And when it happens there, debugging it is pretty bad. As for the part about invariants, that sounds like pure rationalization to me. We are shaped by our tools, and learn not to want the things we can't have. Of course right now if you get a "Prelude: head: empty list" error, you're going to do deep re-thinking, simplification, etc. You have little choice. If you had a crash context, instead, you would be able to figure out what broke your invariant much more easily, and maybe you could choose to do something that is more important than rethinking that piece of software *right now*. I think we should avoid making excuses for the sorry state of debugging in Haskell. Stack traces are taken for granted in every other language, and there's no reason that Haskell couldn't have something equivalent (even if not a call stack per se).
have you tried Yesod yet?
Chris Smith is using it in his Snap project now. It ports directly to WAI (yesod), not sure if anyone has done that yet.
You're right, we shouldn't be making excuses. And yet, I too have never had issues with that error message once I knew Haskell well enough to start doing projects in it. Just as we shouldn't make excuses, we also shouldn't spread FUD. Just pick a different error message, one which more people actually do run into in production.
Your concern about metaprogramming may be overstated, but it isn't completely groundless. In particular, when specifying Models in Yesod I find the TH to make my models a bit opaque to me with regard to exactly what is doing what, and I've produced both a couple of bizarre error messages, and done some things that should have been errors but weren't. That said, I think it all balances out in Yesod's favor, particularly in the documentation arena. It's got legs.
Yes, I have reddit eyes for you too. ಠ_ಠ
Let's be honest: just pick one that looks *interesting to you* and run with it. Yesod, Snap, Happstack are all good options, and unless you have a very specific approach that you want to take, any will suit you just fine.
I don't think that "runtime debugging in Haskell is terrible" is FUD. Haskell has plenty enough good qualities to compensate for that shortcoming, too.
I'll have to respectfully disagree then. We have a very large number of Yesod users, many of whom are relatively new to Haskell, and they seem to be picking it up just fine. A lot of our users come from a Rails background. __Edit__: Also, I think you're drawing a false dichotomy. Snap and Happstack also have special systems for routing and templates which are outside the realm of simple request -&gt; response, and they have a special monad to run handlers in. You can make arguments about why Snap might be preferable to Yesod, but I don't think this is one of them.
I would say the concern with Template Haskell is overstated in general. I wouldn't really categorize TH as *metaprogramming*, so much as *code generation*. The difference is significant: there are no funny dynamic tricks occurring in TH. Once the code is generated, you get all the same static guarantees as normal Haskell code. And in the case of Yesod, that means *more* guarantees (type-safe URLs, compile-time checked data marshaling, etc). That said, let me just point out where TH is used: * Route declarations. This could easily be avoided by writing a few instances and data types by hand. It also takes all of 2 minutes to learn the syntax, so I don't think there's a significant learning curve. * Templates. This is a place where I think TH is the perfect solution. The alternatives are: * Combinator libraries like blaze-html. This is great for some situations (I use it myself), but is not designer-friendly, and mixes logic and view code. * A non-typesafe template language, like Heist or HStringTemplate. Some people like this, but I think just about any comparison will show that they require more code to use and don't catch as many errors as Hamlet does. * Persistent. I can definitely appreciate why people don't like this, but: * Persistent isn't part of Yesod, so it's really an orthogonal question to framework choice. * You can use Persistent without TH. Some people are doing so in production now (an iPhone app, since GHC for the iPhone doesn't support TH). When I have some free time, I intend to write a tutorial on doing so. And remember, when you're using Yesod, you never have to *write* TH code, you're only ever calling it.
Kaya!
 &gt; As for the part about invariants, that sounds like pure rationalization to me. &gt; If you had a crash context, instead, you would be able to figure out what broke your invariant much more easily, and maybe you could choose to do something that is more important than rethinking that piece of software right now. When developing my [reactive-banana][1] library, I've completely thrown away two internal implementations before settling on the current one; that was the simplest way to fix their problems. I expect the current implementation to bite the dust as well and make room for a simpler and yet more powerful one. I like my code to be obviously correct. A debugger only helps me to make not obviously correct code live a bit longer. Sure, I've recently had two bugs in the current implementation as well. One involved a program crashing when you build it with `cabal` but working fine when you use `ghc --make`. The other was an unexpected freeze. Sounds nasty, but they were extremely easy to pin down; in fact, I had already anticipated them to some extend. For the first one, I just had to complete a change that I was too lazy to finish; for the second one, it will be automatically be solved by the next functionality in my roadmap. [1]: http://haskell.org/haskellwiki/Reactive-banana &gt; We are shaped by our tools, and learn not to want the things we can't have. The same could be said about debuggers ("and learned to want the things we always had.").
I think there is a clear parallel of the differences between Snap and Yesod in the Ruby world: Sinatra and Rails. Sinatra is simple, easy to understand fully, but you often need to add functionality from other projects (mix and match). Rails is far more complex and extended, but more coherent/integrated. I think there is room for both. For some web applications, you want a simple framework or one that lets you pick all the components, for other web applications you want something that is well-integrated and provides everything that is necessary.
Whether you mix logic and view code with Blaze is up to the programmer, just like with any programming. The Shakespearean templates provide quasi-quoters that could similarly be used to mix logic into views. It's a structural issue that more or less solves itself in Haskell thanks to the "functions are cheap" design.
I like Happstack because it feels more Haskellian. Snap and Yesod seem to be more about implementing the standard conventions for "modern web frameworks" in Haskell - which might make them easier to relate to and learn if you come from another language, but is also less interesting and perhaps benefiting less from Haskell.
&gt; I like Happstack because it feels more Haskellian This is exactly the kind of comparison I'm interested in hearing more about. I understand it might seem arbitrary, but it's the kind of aesthetic that appeals to me. Unfortunately, it's also difficult to judge without a background in Haskell. (It's like trying to choose between Django and Pyramid without knowing what "Pythonic" means.)
I don't think *having* an extra tool that you can ignore is in any way worse than being required to anticipate every bug or rewriting every software to the most obviously-correct form at all times. reactive-banana has stable dependencies. What happens when you work on a large project with dozens of other developers, who are constantly pushing new semi-tested code? When their code crashes and burns -- you have very little in the way of finding out what the problem is. 
Hello, there, likely friend of one named Georgeson.
Fair enough. I'm only saying that personally, I don't feel much need for a debugger and detailed my reasons.
I love how the top comment suggests that java and C++ etc will always be more popular because they are "copy/paste" languages. Functional languages are even better copy/paste languages, arguably, because they heavily emphasize composition as a design principle. If copy/paste facility is the benchmark for "regular people" language usage, why not use [XY 2](http://www.nsl.com/k/xy/xy.htm), which is completely flat? There is a serious point here, though, in that your average programmer just sees functional languages as disconnected from what they are doing, and the functional rhetoric seems to come across as condescending, to them. And, I have to admit, I think often my "real programmer" friends have a point, often the best choice for a large business is the mediocre one from the expert's point of view. Experts can't be catered to because they are rare. 
&gt; Functional languages are even better copy/paste languages, arguably, because they heavily emphasize composition as a design principle Well no, because that requires an ability to understand abstractions and composition. I think the "copy and paste Java programmers" don't even know that there are levels of abstractions in simply copying code into another function, and that is the big problem. My gut feeling is that these folks believe that they don't need/grok function composition and the like, while not actually realizing it's almost how they are building their programmers. Sadly, I'm not sure I expressed my view very elegantly, but perhaps someone can expand on my ramblings :)
That's why for every medical doctor there are thousands nurses. It's not that hospitals do not want more doctors, it's just prohibitively expensive. And same with programmers. Functional programming will never be for the masses.
I agree with you, of course, but the copy/paste programmers are at least building an intuitive, messy model of the ad-hoc composition semantics of chunks of Java (or whatever) code. They may as well do it for the _much simpler_ semantics of something like Haskell or Lisp.
I'm not entirely sure this is true, but I don't think it effects your point. My sister is a medical doctor, and she suggests that medical schools purposefully suppress enrollment to ensure high wages for doctors. Guild tactics. We should do this for functional programming. 
It takes more than 10 years (!!!) to finish medical college and get a medical doctor license. 99.99% of people simply do not have that kind of money/time/effort. Functional programming is in the same league for the same reason. You won't find a book "learn haskell in 7 days" :) 
Yeah, I am pretty sure that the 'this' keyword is more confusing than monads. The 'infamous javascript loop problem' is pretty nasty too. (http://robertnyman.com/2008/10/09/explaining-javascript-scope-and-closures/)
However, it's easier to "grok" pure functional code than imperative code because there are no side-effects to worry about. What you see is what you get. Even imperative code in Haskell very easy to reason about because they reuse the same control functions over and over again to compose imperative code. Another real advantage of strongly-typed languages is that it's very easy to reason about what a function does just by looking at its type signature.
I don't have any specific beef with FastCGI, I just don't understand how it is in any way preferable to reverse-proxying HTTP. It's still a protocol you have to parse.
But you can learn Erlang in 7 days. I know several people who employ decent numbers of Erlang programmers without prior knowledge of Erlang, and they report that those people start writing useful production code after a week.
What are their hourly pay ? What are they tasked to create ? Somehow i do not think it's the GUI CRUD applications :) The observation still stands. FP is way too expensive to demand/pay for it en masse. 
The term copy/paste language is perjorative, if correct. Can't we all agree they are "invoking" a computation. Or maybe the term should be "evoking". Even Harry Potter reuses the same spells. I think the real difference is that FPers want to see the connections between all the spells based on a faith and delight in factoring, while "real programmers" just want say *Engorgio*^* and be done with it. \*Causes objects to swell in size.
Exactly that kind of thing. All nonsense.
I am old enough to remember all these "mainstream programmers will never need/understand/use" arguments from when Object Oriented Programming first arrived on the scene. And ditto when structured programming arrived. When y'all make it to the other side of the Monad, I'll be there with open arms!
I wonder if medical doctors are just as inept at solving real-world problems as functional programmers. "It appears you have cancer. Now, if this cancer was referentially transparent, I could tell you all sorts of solutions for removing it safely from your body. But, unfortunately, it's already begun to spread pretty rapidly. It's worse than some of the spaghetti code I saw while I was working at Oracle! Well, take these pills to apply some heuristic refactorings to the cancer cells. Hopefully it doesn't break the build.... I mean... kill you."
That was not helpful, neither it was funny. &gt;inept at solving real-world problems as functional programmers. I haven't seen an inept functional programmer in my life. All of them are highly productive and effective programmers. But i've seen a lot of inept programmers, all of them with imperative background. 
I'm not sure of all the use cases, but I think it would be most prevalent in a shared hosting environment running either Apache or Lighttpd. Those two servers will automatically spawn FastCGI processes, while spawning your own long-running process that's listening on a port would likely be impossible in that environment. Also, I think it's more convenient from a maintenance perspective to use named sockets instead of non-descriptive numeric ports, but I've heard that Nginx can actually reverse proxy to named sockets as well.
Actually, I'm pretty sure "Learn You a Haskell" can be comfortably read and completed in 7 days by just about any programmer.
This line of reasoning doesn't hold up anymore. C++ with boost is just as complex and has just as many difficult abstractions as Haskell. I think it is vastly more abstract and "difficult" to the average programmer in fact. What corporations do to deal with this is to define the subset of C++ that one is willing to use. An issue with Haskell wrt *analyzing its difficulty level* is that since everyone is a Ph.D student, the willingness to use all sorts of "difficult to understand" abstractions is there. Industrial use of Haskell would probably limit usage of very abstract, non-trivial abstractions. Given that the industry moving to peer-review of code, limiting abstractions in a powerful language is a good option, better than making everyone use a "limited" language like Java. This removes this problem, and C++ use in industry is proof that it works.
I don't really care what you find funny, nor do I care about your personal anecdotes :D
It's not the GUI CRUD applications indeed. It's supercomputer management software, streaming media serving software, and a large and very high-loaded public web project. I'm not sure what this implies about GUI CRUD development with FP;
But functional programming languages have been here for ages (lisp). How did OOP skip the line, so to speak?
But with OOP that objection was right. They don't need, understand, or use OOP, just something with the same name.
There is a lot of history on this subject just go read. My take is that performance of functional languages was not very good early on (has vastly improved since), and this was only exemplified by the low power of computers back then. Imperative programming was much closer to the machine, and a lot more practical at the time. But of course it is not that simple and the history is very interesting.
Lisp itself also suffered heavily by being tied (in the perceptions of non-lisp-users) to AI. When the AI Winter of the late 80s hit, Lisp was hit with it.
None of this code, ~~the underlying websockets package,~~ or the new settingsIntercept in Warp, are yet released, but thought I'd give everyone a little heads-up. Jasper made an incredibly nice interface that fits right into Warp. Basic overview of how things work: * A websockets request starts off like a normal HTTP request, but with a few extra headers. * Warp receives the request, then asks the settingsIntercept if it wants special handling. * If special handling begins, then Warp gives the interceptor the request value and the socket. * The socket is only used for sending data to the client. * The interceptor lives in its own Iteratee, which is where it should read data from the client. * The WebSockets package expects the request headers to be in the stream, so the interceptor pushes that information back into the buffer (via yield). * iterSocket turns the socket into an Iteratee. * runWebSockets (from Jasper) takes care of all the magic. Note that this settingsIntercept could be generally useful for things beyond websockets as well.
The [example app, which is literate Haskell](https://github.com/yesodweb/wai/blob/master/wai-websockets/server.lhs) needs a very recent Chrome (version 14 at least) or Firefox to run. To run it, get the source: git clone https://github.com/yesodweb/wai cd wai ./scripts/install run the application: wai-websockets-example browse to http://localhost:9160/client.html
&gt; There is a serious point here, though, in that your average programmer just sees functional languages as disconnected from what they are doing, and the functional rhetoric seems to come across as condescending, to them. And, I have to admit, I think often my "real programmer" friends have a point, often the best choice for a large business is the mediocre one from the expert's point of view. Experts can't be catered to because they are rare. I’m trying to figure out whether you were being deliberately ironic when you wrote that. In any case, I don’t think it’s true that most programmers see functional languages as disconnected any more. Whether or not they are aware of the underlying academic theories or even the term “functional programming”, they are starting to employ widely useful ideas like lambda expressions and higher order functions in mainstream languages, many of which have been introducing or expanding support for such features for quite a while now. The jury is still out on whether functional programming in its current state could be a better general purpose programming style than the various tools that dominate industrial practice today. I think Simon Peyton-Jones pretty much hit the nail on the head a couple of years back with his “nirvana” slide: an ideal programming language would be both safe and useful, and we haven’t yet come up with one that does both of those things well.
You can _learn_ Java in 7 days without prior experience in programming? I really can't believe it. Also, it doesn't take years to learn enough Haskell to write useful code in it. Yes, it probably takes a bit more time to adjust from PHP to Haskell than from PHP to Java. At least for writing some code - I doubt that learning to write proper, well-designed Java takes shorter time than learning to write nice Haskell (using Java OO without creating a mess is amazingly tricky business, and even most long-time Java programmers seem to get it wrong too often).
Programmers are not tasked to create beautiful code. They are required to produce a working program. Spaghetti code is fine. And a lot of them actually manage to do that if you don't look under the hood :) But the same is not true for haskell. It simply makes it very hard or impossible to create a working program if you do not follow a modular, composable design. So "learn in 7 days" book will not help with haskell, but will help with practically any imperative language as long as you have prior programming experience. 
I could go either way. I'm trying to ascertain what the path of least resistance is.
ha, I'd just started building an app based on 0.3.0.0. Looks like Jasper's changed everything :) still, move fast and break stuff, right? Looking forward to putting this through its paces.
Lambda expressions and higher order functions are just the very tip of the iceberg, though. 
New Haskell user here. Yesod is working out for me just fine. The questions I have are just part of being new to Haskell.
&gt; Functional programming has its downsides. It tends to result in heavily nested code. It's hard to fan out results, so programs tend to be trees with a single result. Huh?
That arguably depends on whether you’re considering cost or benefit. Both of those features are simple but very flexible. Moreover, you can integrate them very naturally into the existing framework of most imperative languages. In other words, they offer a substantial jump in expressive power for very little extra complexity in syntax or semantics. I suspect one of these days algebraic data types and pattern matching are going to hit the mainstream for similar reasons. I’m a little surprised that they haven’t already. The picture changes significantly when you look at more specifically functional concepts, though. I see a lot of potential over the long term in ideas like lazy evaluation, more expressive type/effect systems, and formal proof methods. However, in the short term, the implementations in today’s functional languages aren’t even close to usable enough to justify adoption in most industrial situations.
"However, in the short term, the implementations in today’s functional languages aren’t even close to usable enough to justify adoption in most industrial situations." Is this really true by virtue of the features themselves, or by virtue of the fact that most programmers are relatively far away from understanding those features? If you replaces every programmer in the world with seasoned Haskeller, would Haskell still be unsuited for industrial strength development? I can imagine that the answer is no, but it is hard to see how the features are the reason. What is wrong with Haskell's type system, for instance, that makes it inappropriate for corporate use? I could buy the fact that library support is the problem, but that is solvable. 
And you come out the other side of it knowing Haskell syntax with nfi how to actually do something with it. At least that was my experience. Also only a fairly shallow understanding of the whole Monads thing, which is apparently quite important.
I taught a coworker the basics of Haskell in a few hours, and she continued from web resources and was productive in it less than 2 weeks later.
I really don't understand what is so functional about Lisp. Python isn't considered functional, and I don't think Lisp is any more functional than Python. Lisp also has quite a few unappealing traits of its own, so even if it were functional, its rejection doesn't tell you much about FP's acceptance.
I assume the French title is just as grammatically questionable as the English one? I hope French-speakers don't see that and think it's because of a bad translation :)
&gt; The importance of concision is clear: other things being equal, shorter code is easier to read, easier to write, and easier to maintain. There is an important point here, but I don't think this statement is correct. If it were correct then he'd advocate writing in J. There are some correct parts, however. Not all programming languages are equally concise, but not all verbosity is bad. Some of it is cruft and boilerplate that doesn't serve any purpose. Or rather, it doesn't serve any purpose to me, the engineer. Presumably it helps someone (or something). Then there is the verbosity that actually helps me understand what is going on. An example of the latter, perhaps, are comments. Comments are not necessary for the correct working of code, but (good) comments are often necessary for understanding code. Another example would be Haskell type annotations for functions. I like me some strong typing, but I'm not always in favor of inferred types for a couple of reasons. One of those reasons is that I'm not as smart as a compiler and I can't always figure out what types a function takes and if you can't figure that out then you are in trouble. A type annotation, while not strictly necessary, can help me understand the code. So, how much verbosity is good? Beats me. I'm still trying to figure that out.
I think there's two "functional"s, old and new. Old-style functional is where functions are first-class objects, and you have some sort of closure. Recall that none of C, C++, or Java have it (or don't have it to speak of), so while it may be old hat to everyone here we've only recently gotten to the point where this is simply a part of programming. New-style functional is, well, Haskell. Strong typing, immutability, and a syntax and culture that move beyond merely tossing around a few "map" statements with closures but using functions and types to build yet higher level constructs, such as the ones in the Typeclassopedia. Lisp and Python are old-style functional. Old-style functional can be bolted on to an otherwise OO language, and at the risk of being shot in this crowd, OO + old-style functional isn't a _terrible_ paradigm to program in. It's a bit of a semantic mess in theory, but in practice there's usually a concise way to do whatever it is you need to do. New style functional can't just be bolted on, because OO as commonly conceived is fundamentally built around mutation. A weaker form of OO can be bolted on to the side of a new functional language, but so far nobody other than recent OO refugees seem to think it adds much to the functional language.
Well, I think when people talk about FP having elegance/beauty, they're talking about denotational programming in particular. map, reduce, lambdas and closures are nice "imperative power tools", but they don't really make a language functional. And that's almost all of the "functionalness" that Lisp brings to the table. And even that part *did* catch on, all newer languages have it, so asking "Why didn't functional languages like Lisp that has been here forever catch on?" is misguided -- the functional concepts from them have caught on. Denotational programming, however, has not yet caught on, but that's understandable, given that we don't have denotational solutions to so many problems yet.
There's this: http://stackoverflow.com/questions/3870088/a-monad-is-just-a-monoid-in-the-category-of-endofunctors-whats-the-problem
I think the point is that you *couldn’t* replace every programmer in the world with a seasoned Haskeller. Some of these features, as they are implemented today, are simply too difficult for us to produce that many people capable of using them effectively within a reasonable timeframe and budget. In ten years’ time, when things that can be automatic are automatic, and when things that can be inferred are inferred, and when we have practical tools for concurrency semantics and controlled side effects and distributed systems that match the elegance of the underlying mathematical models to a trained eye today, *then* maybe these features will be ready for prime time, and perhaps we’ll see a quantum shift in productivity and quality in software development. But first the controls have to match up to the engine, and right now, I doubt we even know what those controls might look like.
That's perfect. This isn't going to be a haskell tutorial, as I'm not qualified to do that. Rather, it's an experience report. I could just breeze over the subject of monads with that little joke.
Can you elaborate on that?
I thought he was talking about the compiler memoizing the output if it doesn't rely on any external input. The recent LLVM backend will actually do this. This, of course, is the **entire point** of functional programming. If a function is pure, then it should always return the same result given the same input (or no input, in this case). You can't make that kind of guarantee in a language with side-effects.
I recommend the following very informative [Stack Overflow](http://stackoverflow.com/questions/5645168/comparing-haskells-snap-and-yesod-web-frameworks) post. If you want an all-in-one solution, then use either Happstack, Yesod, or Snap. Note that these solutions are interchangeable and you can mix and match solutions from each one if you are unsatisfied with how one of them handles a particular area. However, there is nothing preventing you from also mixing Haskell with other solutions. For example, you can have Apache as a web server and use Haskell to write fastcgi scripts to dynamically generate web pages. Such a script can even take advantage of many packages derived from the above frameworks. My experience is that Happstack is by far the most simple and straightforward framework to use. It has a really excellent tutorial at: http://happstack.com/docs/crashcourse/index.html
I'm a relative Haskell ignoramus, but here goes. (Ahem) A monad who covered a set Tried to pick up an IORef "get" But not Hindley nor Milner Could rescue this swindler For the bloke hadn't lifted it yet. I was going to write something about Arrows, but it quickly became too abstract and hard to follow. ;)
I'm stealing this. Unless you have a url I can use to credit you with. My final slide will be people and projects that helped me put this thing together.
&gt; I suspect one of these days algebraic data types and pattern matching are going to hit the mainstream for similar reasons. I’m a little surprised that they haven’t already. I am completely mystified by this, and don't understand why salaried programmers don't seem to bump into these limitations. It is a consistent feature of common OOP languages that basic conditions such missing, not found, failured to instantiate, are all encoded using null pointers/references - failing to recognize that it breaks class abstraction everywhere. A simple option type as the most basic algebraic possibility, would formalize and put tagged names to these. Additionally the use of enums in c/c++/java/c# thing, when in 99% of cases, what is actually desired is the functionality of a discriminated union. Returning more than one alternative from a function? Use a visitor pattern or enums - pick your poison. With such low-hanging fruit, the difficult to shake the feeling, is that common programming practices are really in the dark-ages.
&gt; The term copy/paste language is perjorative, if correct. At my last Java job, when I was tasked to create functionality similar to that which we already had, I was encouraged to copy and paste from the old code, and then tweak until it provided the desired new functionality. I got in trouble once or twice for *not* copy/pasting. This disturbed me. Most programmers are OK with this style of programming, and encourage it because it causes a lot of the code to follow the same format and patterns. But the FP-er inside me just can't stand copy/paste programming.
Allow me to suggest that spaghetti code is not fine, due to technical debt. Slap it together quickly now, pay for it later in maintenance. That's what I'm finding to be true first hand, and is the topic I bring up when justifying my implementation language choices. Let the true reason, "Haskell is really cool.", be our little secret. 
I would never suggest that spaghetti code is fine :) Just that it's not what programmers in millions of companies are paid for. As long as the program works, in 99% of cases no customer even have a necessary level of understanding to care about the code. 
Let me just say that there's a me in a parallel universe entirely unrelated to this one and the people in it, that is shocked, shocked! at what certain senior engineers are allowed to get away with. But in this universe everyone uses best practices all the time, and I would never suggest otherwise. ;)
Yeah, looks like someone forgot either about tuples (and datastructures in general) or about the "let" expression, depending on my interpretatino of their words.
The objections to Haskell in the comments are interesting. Common themes seem to be: * I can't believe anyone advocating Haskell has actually used it for a nontrivial program. They probably just read about it last week on Reddit and jumped on the bandwagon. * People seem to be drawn to Haskell based on some sort of aesthetic attraction to its theoretical elegance (which has no practical application), like a sailor drawn to the rocks by a beautify siren's song. * Haskell was made by academics, who obviously don't know anything about real systems. * At any moment, one of my coworkers may become a Haskell convert and propose that we throw away the system we've been working on for the last four years and replace it with a Haskell prototype he wrote last weekend that doesn't actually work. We must present a united front of opposition to this threat to our way of life. It seems that the best way to counter these sorts of claims is to build powerful, widely useful software in Haskell. It would be nice if I could point to some ubiquitous tool and say: "Look, you really can use Haskell for large, complicated projects." The best candidates so far seem to be ghc, darcs, and xmonad. I wish I could point to something like mysql, or apt, or firefox, or blender, or mediawiki, or even wesnoth or povray. Of course, if you just set out on a big project just to prove that Haskell is good enough for real-world problems and not because you actually want to solve a real problem, you're probably doomed to failure. Similarly, it's not enough to just clone an existing program, you have to make something that's better. Here's a thought experiment: of the software you use every day, which programs do you most wish were written in Haskell, and could those programs be made more powerful?
This sentiment is replicated in several comments. &gt; Functional languages require the developer to approach problems with an entirely different mindset. There is a steep learning curve to really understand how they work. And I'm not talking about just the syntax. Functional languages are fundamentally different than procedural languages. Truly understanding how they work requires a lot more brainpower than procedural languages. Also, I'd like to note that Finnish is a lot easier to understand and use than English, since there's a steep learning curve to really understand how English works. I'm not talking about just the different words, it's fundamentally different than Finnish. Truly understanding how English works requires a lot more brainpower than just continuing to communicate using my native language.
And 0.4.0.0 will break stuff again, I'm afraid, because we want to be able to support multiple versions of the protocol. This turns out to be insanely tricky, and we can't do it with the current API.
"Spaghetti code is fine." - vagif, 12 hours ago "I would never suggest that spaghetti code is fine" - vagif, 7 hours ago You wouldn't!
Exactly - it seems that Haskell, inexpertly used, might still be better than Java, modulo libraries. 
Not Haskell specific per se: I used to think I was keener, For using C, as it was leaner, But with pointers abounding I found it confounding. Now I use a language much cleaner.
Woosh.
I see some work is being described in ghc trac wiki (so I presume some code is being written too) about using the SIMD instructions via this backed too for the DPH code. Is this supposed to happen in 7.4 or later?
To be fair, `(d -&gt; a -&gt; d) -&gt; (d -&gt; k) -&gt; d -&gt; a -&gt; k` is a smaller type than `(a -&gt; b -&gt; c) -&gt; (c -&gt; r) -&gt; a -&gt; b -&gt; r` -- in particular, it only has one total implementation. However, the point about type variable naming is valid. I often find the type variable names that GHC sometimes generates -- things like "t1" from "t" -- rather confusing, and would much prefer if it came up with new unused single-letter names instead where possible.
This was such a nicely done write-up. Thanks to Eric Kow for doing it.
this is by far the best resource for learning haskell i stumbled upon. you can check out the content here at http://learnyouahaskell.com/chapters. the author writes in a good didactic and funny way and it is easy to follow. and yes, he drew all these gorgeous pictures himself!
Excellent!
 There once was a proper Haskell fanatic Whose functions were transparent and typing was static Till he fell for a lady Foreign, impure, and shady And they unsafeperformioed until the RTS went into panic. Also, these: http://www.haskell.org/haskellwiki/Humor/Limericks
Looks like there is overlap with my marxup package... http://hackage.haskell.org/package/marxup (look in the examples directory) Questions: * Would you welcome metapost support? * Why do you have so many modules? :) I could not find how you handle references.
The students were instructed to find partners for the next HW assignment. One posted this to the message boards.
Hey, thanks! No blog, but I just PM'd you my name. Good luck with the talk.
I would love something mixing HaTeX and lhs2TeX!
I didn't know your package. Anyway, it seems to have a slighty different purpose. About your questions: * I don't know MetaPost. Well, I know it, but I have never used it. So, if you want MetaPost support, and you are familiar with it, why don't you implement it yourself? I will receive the patch gladly. * Each LaTeX package has its own module in HaTeX. I thought this is more organized, and allows you to import those things you really want to import easily. Also, every module with LaTeX commands has an analogous module with monadic interface. Yes, there are a lot of modules, but they are arranged with sense (or I tried it!). Regardless, any suggestion is welcome!
I think it would be better to redefine SpellEffect and ItemEffect as TargetableUnit -&gt; TargetableUnit (i.e. make them all custom). Then you'd make Damage, Inflict, Cure, and Restore into functions by moving the parts of cast and use that come after pattern matching into their own functions. A good way to think about this kind of refactoring is that your types are too strong. They protect a property (that the effects are **only** those in the list) that you don't want. Types are too strong when there are values you want to represent in them that do not exist (so you need to keep adding them) and too weak when there are extra values you don't need (so you need to keep throwing them out via error checking). It seems like anywhere you need to check for strengths, you'd also need to check for weaknesses, so you might want to combine these into a single function that returns both. This would also give you an opportunity to define a type that asserts that it is impossible to be both strong and weak to the same element. That way, you'll be able to see how having stronger types can also simplify your code. Another refactoring I'd suggest is giving normal damage its own element (and perhaps also for mana damage). Obviously, you won't be able to use Enum to give you the strengths and weaknesses if you do this, so I suggest using break and cycle to define those.
&gt; Anyway, it seems to have a slighty different purpose. Correct; I hope it can build upon yours in the long run. &gt; I will receive the patch gladly. That's what I wanted to know :) &gt; Yes, there are a lot of modules, but they are arranged with sense (or I tried it!). I should really not have mentioned that, as it is really a matter of taste -- not a big issue. I am really more concerned about the following: * You don't seem to have a system to handle references at the haskell level. To see what I mean, look in my example how "section" is used. The corresponding combinators are in MarXup.Tex; constructors "Refer" and "Label". * You seem to use a monad transformer (LaTeXT_), but I don't see why that is useful. BTW the 1st link on http://www.haskell.org/haskellwiki/HaTeX is broken. 
Just as a heads-up, LSV (the Little School on Vermijo) is having their inter-quadrant break next week, meaning no school. Also, I'll be in Las Vegas. So, week 9 won't be there until the following week.
&gt; You seem to use a monad transformer (LaTeXT_), but I don't see why that is useful. Well, this is a very simple example, but take a look: lift (readFile "foo") &gt;&gt;= verbatim . fromString You can insert text from a file very easily. Well, that's a very simple thing, but intercalate IO computations with LaTeX functions is a good possibility. At least, I use it very often. &gt; BTW the 1st link on http://www.haskell.org/haskellwiki/HaTeX is broken. Yes. And all that page is now obsolete. Indeed, it refers to HaTeX 2 version. I will edit it right now. Thanks for notify.
I wish there is a Haskell course in my school...
In which use are you thinking exactly? Speak and your dreams might become true! (:
&gt; Intercalate IO computations with LaTeX functions is a good possibility I guess I am too much a follower of the purity dogma to really appreciate that :) But I see your point. Anything to say about references? Overall I think I might have use for HaTeX in the future... so thanks in any case!
Well, there is a cost to remembering all the applications of a function to different arguments in case it one gets reused. It's not that it can return different results, it that it might not be worth the space/time to cache it every time.
'sok, it's short enough I don't mind rewriting too much. Trying to get it running on heroku - it may not work, just because they're a bit funny about opening new ports. fun hack though.
I'd like to be able to write an article with haskell code inside using HaTeX instead of raw latex in the comments.
Not that I expect the students will ask this, but when you showed how to graph functions, it kind of makes you wonder: can you write a higher-order function that takes a function from numbers to numbers, and returns a picture which is the graph of that function? :)
(the context is that we just received a pair programming assignment and a number of students had posted on the forum looking for partners)
Sure, if you want to do it poorly: graph f = line [ (x, f x) | x &lt;- [ -250, -240 .. 250 ] ] Better graphing requires analyzing the syntactic structure of the function to find asymptotes and undefined points and such.
which versions do you support currently, btw?
Aww. A rejection would feel like a kick in the monads though. 
I knew it would be possible, but for some reason I thought it would be harder than that. Nice.
That's very creative and cute! 
original thread starter seems to be the following link: http://haskell.1045720.n5.nabble.com/TypeFamilies-vs-FunctionalDependencies-amp-type-level-recursion-tt4437587.html#a4489484
He's talking at a higher level of abstraction than you. He's talking about the effects of what happens, which if you're talking about Haskell for example do have to bubble all the way to the top.
&gt; I guess I am too much a follower of the purity dogma to really appreciate that. Then your friend is the Identity monad. type PureLaTeX = LaTeXT Identity &gt; Anything to say about references? I was seeing your package. Currently, references in HaTeX work like this: refExample = do section "The Forest" "There is a label" let lab1 = createLabel "one" label lab1 " in the labeled" let lab2 = createLabel "two" label lab2 " forest." section "The Sky" "And there is a reference (" ref lab1 ") in the referenced (" ref lab2 ") sky." Perhaps, you search for something like this? refExample = do section "The Forest" "There is a label" lab1 &lt;- label "one" " in the labeled" lab2 &lt;- label "two" " forest." section "The Sky" "And there is a reference (" ref lab1 ") in the referenced (" ref lab2 ") sky."
Do you support forward references?
Hey, thanks for the feedback! Writing this is a great way to get a feel for what the community is up to :-)
That was just an hypothetical code. But, yes, I can do it with the option of forward references.
Is it possible to use different latex packages in HaTeX. For example, is it possible to generate code which makes use of the *algorithm* LaTeX environment?
This is very VERY well-done tutorial. The OchaCaml extension (shift/reset) make this very simple to understand. At last, a continuation tutorial that shows you everything you can do with continuations. In the Haskell world, the Monad.CC makes delimited continuations harder to use in practice (partially because of the monad transformer layer). Is CC-delcont the only package for delimited continuations in Haskell?
Your second question first: Yes, I think so. The first question: I think it would be pretty hard to do that in Haskell. Since dependent types "require" that all programs are total and terminate for type checking to be decidable, we could not do this in Haskell since not all programs are guaranteed to be total or terminte. I put "required" in quotes because this assumes we want decidable type checking. While decidable type checking is definitely nice and possible (see Agda, Coq, Epigram) maybe we don't really need it to be productive (Cayenne, for example). It's often just easier to start over with dependent types as a goal from the beginning. 
Closed type families would be beautiful for type level programming and the top-to-bottom match order would make them a lot less subtle to reason about than overlapping instances.
Anybody know if video of this talk exists?
You'll probably get a better response on StackOverflow.
I just can't help it. Every time I see this package I wonder "What does this guy have against X?"
You need to use a parsing library, such as parsec or uu-parsinglib. For a quick (but slightly outdated) parsec tutorial, look at http://legacy.cs.uu.nl/daan/download/parsec/parsec.html
I will take it a look.
Ja! It took me a while to understand it! Well... I don't have nothing against any letter. My apologies to the alphabet!
The only reason we would want total terminating programs is if we want to use the type system for theorem proving. "undefined" already breaks all that, so, meh. We don't lose anything by adding dependent types, certainly totality is not required for dependent types, it's just required for the curry-howard isomorphism to give strong guarantees.
Runtime debugging is one thing. "Oh noes, head of empty list is *everywhere*!" is FUD.
Try to choose a representation that fits your problem domain. For example, something like this: type VariableName = String type FunctionName = String data Expression = Constant Integer | Variable VariableName | Application FunctionName [Expression] This is isomorphic to the structure you're proposing, but far easier to work with and reason about.
For very simple languages like this one, writing the parser yourself is also maybe a good idea. Parsec is simple and powerful but you will need to get acquainted with some haskell "advanced" features to use it.
That's sort of like looking at regexes (or CFGs, or mildly context-sensitive grammars) and saying "Can we just implement a Turing machine?" On the one hand, sure, we could. (Modulo whatsheon's comments about practicality of doing so for Haskell.) On the other hand, are we really sure that's what we want? While weaker formalisms can't express everything we'd like to express, they generally have numerous other benefits in things like having efficient implementations, enabling specific optimizations, allowing the decidability of important questions about uses of the formalism (or improving the complexity of their decision procedures), etc. I'm not being flippant here, it's an honest question: do we really want full dependent types in Haskell? If we're just after having a language with dependent types, then why not use Agda or Coq? If we just want to extend Haskell, then are we sure dependent types are the direction we want to go in? If we just want to write better programs, then do we believe dependent types in Haskell will help with that, and if so then why?
Typeclasses, monads or applicative functors. I think something like [this](http://hpaste.org/52291) is much easier to grasp when you're starting. Don't get me wrong: abstractions are beautiful, easier to reason about etc. But given the fact that the OP doesn't seem to be acquainted with algebraic datatypes, I seriously doubt diving into Parsec (whose lexer setup looks like magical incantations for someone new to the language) right away is a good idea.
come to UPenn!
&gt; [this](http://hpaste.org/52291) This is pretty much what I expected. Some way of recongnicing a very simple grammar. This is the first step in interpreting the language itself. Thanks ^_^
Makes sense. As you can notice, I'm starting to learn Haskell :P A Tree structure was my first option, most probably because that is what I would use in any other language. Your option is also a tree but looks more ~~pythonic~~ haskell-ish.
Almost - as whatsheon pointed out, normalization also gives you decidable type checking. Roughly, the simplest way to compare two types is to just run them and see if they reduce to the same value. But if types aren't guaranteed to terminate, this algorithm won't work. It isn't quite true that we _need_ normalization for decidable type checking, though. It's enough for every term to have a head normal form, which is a little weaker. In fact, coq admits some terms that don't terminate but have head normal forms! Still, in haskell not every term has a head normal form (think undefined). So we'd need some other way to check equivalence at the type level, and that's a hard problem.
Sure, but I'm saying that if you're willing to sacrifice decidability of type-checking you lose two advantages. One is the correspondence to (consistent) logic, which we already lack. The other is guaranteed termination of the compiler - which, seeing as type checking in GHC is likely PSPACE-complete anyway, is effectively no worse than crashing GHC on pathologically large types that make it run out of memory and go boom or take a very long time to complete. So - the compiler being guaranteed to terminate isn't such a big deal, in my opinion. Hence, seeing as we have no logical correspondence, and the compiler can already take a very very long time to finish, losing the decidability of type checking is not so bad for Haskell, in my view.
I've never been really bought this argument, which if I understand correctly is that "type checking ML/Haskell is already really inefficient in the worst case, so who cares if we make it undecidable". The thing is that I've never run into these pathological inefficiencies (except when explicitly constructing them), and it's a lot easier to accidentally write down programs that sometimes don't terminate. I've definitely made GHC and Agda's typechecker loop several times (when using some unsafe extensions). Still, there have been systems that explored this (notably cayenne), so I'm willing to be convinced.
also, for those who are interested I found(NOT MINE) this definition of the FibSeries discussed in the above article: fibs = fix $ (1 :) . scanl (+) 1 Pretty cool. 
The definitions of `curry` and `uncurry` are curry f = \x y -&gt; f (x, y) uncurry f = \(x, y) -&gt; f x y 1. Simplify `curry id` curry id = \x y -&gt; id (x, y) -- definition of curry = \x y -&gt; (x, y) -- definition of id = \x y -&gt; (,) x y -- desugar = (,) -- eta reduce 2. Simplify `uncurry const` uncurry const = \(x, y) -&gt; const x y -- definition of uncurry = \(x, y) -&gt; x -- definition of const = fst -- definition of fst 3. Express `snd` using `curry` or `uncurry` and other basic Prelude functions and without lambdas. uncurry (flip const) = \(x, y) -&gt; flip const x y -- definition of uncurry = \(x, y) -&gt; const y x -- definition of flip = \(x, y) -&gt; y -- definition of const = snd -- definition of snd Farily simple after solving 2. 4. Write the function `\(x,y) -&gt; (y,x)` without lambda and with only Prelude functions. uncurry (flip (,)) = \(x, y) -&gt; flip (,) x y -- definition of uncurry = \(x, y) -&gt; (,) y x -- definition of flip = \(x, y) -&gt; (y, x) -- syntactic sugar 
As the guy that posted that definition on the Hacker News discussion of this post, I must say that I find the following definition even more elegant: fibs = 1 : scanl (+) 1 fibs Granted, it doesn't use the fixed point combinator, but it gets rid of some of the syntactic clutter.
Is anyone to be preferred over the other, disregarding syntax clutter?
Hey if anyone has something about type safety that would be great. Here's what I got but you could do better. There once was a hacker named Hailey Who cared not for any type safety His first try was dreck It didn't type check So back to the drawing board, Baby
Sweet. Thanks! :)
Aside from syntax they're identical: fix f = f (fix f) so fix $ (1 :) . scanl (+) 1 = (1 :) . scanl (+) 1 $ fix ((1 :) . scanl (+) 1) and since fibs = fix $ (1 :) . scanl (+) 1 we can simplify the second equation to fibs = (1 :) . scanl (+) $ fibs or, getting rid of the parentheses: fibs = 1 : scanl (+) 1 fibs So pick whichever syntax you prefer, since they are both the exact same thing. 
I find the [wikibook article](http://en.wikibooks.org/wiki/Haskell/Fix_and_recursion) much easier to understand.
Excellent, thanks.
Hal6 = Haskell in Leipzig (http://portal.imn.htwk-leipzig.de/events/hal6-haskell-workshop), almost 60 participants
Thank you for posting your very well thought out explanations, they have been super helpful! Because of your post on HN, I ended up spending about 2 hours yesterday reading about the fixed point combinator and infix functions.
&gt; and applicative is not necessary for parsec, it has its own choice &lt;|&gt; operator That's just &lt;|&gt; from Alternative. The friendly documentation: &gt; This combinator is defined equal to the mplus member of the MonadPlus class and the (Control.Applicative.&lt;|&gt;) member of Control.Applicative.Alternative.
I disagree that the price is the learning curve. It just isn't that hard to learn the language. The price is the toolset, or more correctly, the lack thereof. Crappy errors, non-existent debuggers, profilers nobody uses, even the basic print debugging is hard. There is some work to fix this but it is slow in coming.
Something like [instance chains](http://web.cecs.pdx.edu/~mpj/pubs/instancechains.html) would probably give the same benefits as closed type classes/type families, but more flexibility, I think.
I'll take a look to your refactoring suggestions really carefully, I promise :)
I thought the following from the OP's article was very valuable (and not found in the wikibook article): &gt; You need to embed some extra information into `foo` to ensure that, given the value you’re supposed to return, you also do some identity preserving operation on it, *assuming that it is the same value you’re supposed to return*.
A new package has to be implemented containing functions like **algorithm**, **algorithmic** etc.?
I don't doubt that Monad and typeclasses are fundamental parts of Haskell, but you cannot deny they are more advanced than algebraic datatypes and (higher-order) functions, which I assume the OP wants to master first. If parsec was your first monad and you had no trouble with it, fine, but did you start right away with parsec or did you learn the basics first? My assumption is that the OP didn't want to fiddle with any parsing code at all but focus on the interpreter. Learning cabal (even if it is simple) to import parsec and then understanding parsec's boilerplate is really not what you want when you learn to program. You just want a single file with a few functions that you can play with. Regarding "applicative is not necessary for parsec", what I meant is that you can almost always (except for context dependant grammars) avoid Monads and stick to Applicative when writing parsec parsers. It is conceptually simpler so what I meant was : using parsec requires the usage of monads or, at least, applicative functors.
Nice; I never thought of fst as uncurried const before, but it makes perfect sense.
Correct. If we want to add the *algorithms* package to HaTeX, we have to add a module called *Text.LaTeX.Packages.Algorithms* with an implementation of its commands. Like, for example, with the *beamer* package in *Text.LaTeX.Packages.Beamer*.
Types generated by GHC can also be useful because they reuse the types names from the source. (.) ∷ (b → c) → (a → b) → a → c (,) ∷ a → b → (a, b) Prelude&gt; :t (.) (,) (.) (,) ∷ (a → b) → a → b1 → (b, b1) a and b come from the type of (.) and b1 is b from the type of (,)
Dependently typed languages aren't just like Haskell but with more expressive types. They fundamentally change the way you program.
Just that there are tons of things that are referred to by "object-oriented programming" and they often have little to do with each other, or even conflict. &gt; Actually I made up the term "object-oriented", and I can tell you I did not have C++ in mind. -- Alan Kay in The Computer Revolution hasn't happend yet -- Keynote, OOPSLA 1997 
Yeah, I was about to say this. I thought the name was a joke at first until I realised a knob is a type of handle.
TIL. Maybe we should have an international review committee to confirm that names we choose for packages don't have inappropriate meanings.
Hmm it's not hard. Just type any potential name into urban dictionary before finalising it.
Any news on progress?
The name is semi-intentional. I was originally going to call it "memory-handle" or something, then I typed "handle" into a thesaurus and got back "knob". My inherent immaturity kicked in, and I decided it was the best name ever. I am not ashamed to admit that the process of writing that release announcement was often interrupted by fits of giggling. The final count of innuendos I was able to cram in comes to...7. Hey, that's pretty good.
&gt; Also does your library have the ability to handle streams? Not currently. The idea is loosely based on Python's `StringIO` module, and it just does fixed buffers. If you'd like an easy way to handle streaming data, I recommend [enumerator](http://hackage.haskell.org/package/enumerator).
(⊥) actually looks like a bottom. ... I'm sorry. I'll leave now.
I know a lot of people here have been playing with gloss-web, so this is your public service announcement. I just added a parameter and made `initial` into a function, for both simulation and game mode, so it's possible to use random numbers in a game. Code will need to be updated. The other change I made was to add a permalink feature, so you can share links directly to your programs. Just click "permalink" beneath the program when it's running.
Important detail missing: what timezone are we using to sync our efforts on? I'm on GMT +10 so when should we all try to be online? Because I want to hang with everybody if I do this; that's half the fun. So we all need to be there at the same time. 
Thanks for the initiative! Great idea and looking forward to try this. Hope plenty of people around our big sphere join to keep momentum in all timezones!
Thank you!
You do not shoot yourself in the foot in Haskell. You construct a new foot that has a bullet lodged in it, then construct a new version of yourself that shares all of your original body parts except for said foot.
"Immutable Feet" is the name of my new band.
The last one with 'click' was definitely the funniest.
You shoot the gun but nothing happens (Haskell is pure, after all) 
Ah brilliant, I never even noticed! "I spent a day or so polishing it up before posting it to the internet." :)
Ooh, good point. I suppose people should put their time zones on the signups. I'll edit it to ask for that. Sorry!
Use a zipper to construct a version of yourself with a shot foot.
I guess this event isn't for those new to programming in Haskell?
I don't suppose there's anything stopping you from hacking!
Yes but are Haskell and FRP being used to solve *real world problems*?
But it has the semantics of Alternative's &lt;|&gt; and obeys the laws of Alternative, does it not?
Yes. Next!
By all means! Feel free to sign up, and when we get started we'll have a section for hacking sessions in progress. You don't have to have a project to propose; but if you do, that's fine too. You can jump in, join Google+ hangouts and talk over plans with people, and contribute with documentation or code as you're able. Some familiarity with distributed version control (darcs and/or git) will probably help.
Hey thanks! I'll do my best to do what I can that weekend, and participate however!
Well then, that's sorted!
Here are a few things worth reading on the subject: * http://broadcast.oreilly.com/2009/01/why-you-should-learn-haskell.html * http://scienceblogs.com/goodmath/2006/11/why_haskell.php * http://www.inf.ed.ac.uk/teaching/courses/inf1/fp/2009/lectures/lect00.pdf * http://cdsmith.wordpress.com/2011/03/13/haskells-niche-hard-problems/ 
Building on cdsmith's general posts, you'd probably be looking for something like [Atom](http://hackage.haskell.org/package/atom). While the world you're living in may never directly run Haskell programs, it is possible to leverage the guarantees given by type systems into other environments with DSL embeds like that. Speaking as a software guy, I would have to say that in general, EEs and hardware guys on average work with downright suicidal software engineering practices, which hurt them immensely in ways they are hardly even capable of perceiving because they don't even know that there are entire classes of advances in software engineering that they don't know about. (Yes, I phrased it that way on purpose, they are "unknown unknowns".) Learning about the concepts in Haskell and learning how to leverage them could give you a powerful leg up on tackling the hardest sorts of problems in the field. Never miss the opportunity to let your development environment eliminate as many wrong answers as possible, as early as possible.
If anyone in this subreddit would actually try to be honest, the answer would be the intellectual challenge. There is nothing Haskell can do that other languages cannot do [on a practical level, people!].
No, but it can do lots of things *better*. Or do you take worship of Turing-equivalence to the levels of writing all your code in brainfuck?
Better in what sense? If [UNIX philosophy](http://en.wikipedia.org/wiki/Worse_is_better) has taught us anything, better is not always better. ;)
Well that may be true of a program at run time, but at compile time there is a LOT more you can do with Haskell than many other languages. But I do agree with a good reason being the intellectual challenge.
&gt; There is nothing Haskell can do that other languages cannot do. If you are referring to Turing Completeness, that's a remarkably silly thing to say, because 1. Not all language implementations are capable of the same interactions with hardware, among other practical differences. You cannot write a device driver in Wirth's original, vanilla Pascal for example. 2. And, even in the purely computational realm, removing a language feature may require a *global transformation* of a program that uses that feature. call/cc and proper tail calls are classic examples. You should read Matthias Felleisen's [On the Expressive Power of Programming Languages](http://www.ccs.neu.edu/scheme/pubs/scp91-felleisen.ps.gz), and study the notion of macro-expressibility. So yes, in a practical sense, there are things a given language can do that other languages can't, even in a purely computational sense.
No, I'm not referring to Turing completeness, which only non-practical people care about. I'm talking about it at a practical level. For the majority of the problems you want to solve, you can accomplish with similar efforts in C, C++, Java, all the major languages out there.
Things that matter! If I'm writing an optimization program to predict viewer's next movie choice on Netflix, what does Haskell give me? If I'm writing the next Go-playing engine, what does Haskell give me? If I'm writing the next first-person shooter, what does Haskell give me? The problem I have with language enthusiasts is that most of them have a very skew view of programming---narrowly focusing on the tool rather than the problem. Does a builder just admire the hammer that builds the house, or should he put more admiration on the house he built?
Well, are you going to define better, or are you just going to vote people down who disagree with you?
Here is a worthwhile essay on the subject. It's about lisp, but I think most of the points apply also to Haskell (though the original author would probably dispute that). http://www.paulgraham.com/avg.html Basically, you should learn the most powerful language you can find and use that, because you'll be able to solve problems much easier than in a weaker language. If it's a powerful language that only a few people use, then that's even better, because it means you have a significant advantage over everyone who uses more mainstream, mediocre languages. Of course, the assertion that Haskell is the most powerful language available is very controversial -- it's the most powerful that I know, but that doesn't necessarily mean there isn't some better language that I don't know. In the end, you have to make the judgment yourself what the most powerful language is, and in order to do that you can either a) hope to get lucky on your first guess or b) learn many powerful languages and choose for yourself. The opinions of others are usually suspect, because any given programmer can only recognize less powerful languages -- more powerful languages just look like equally powerful languages with some weird, unnecessary feature bolted on. You might want to look at "the right tool" ( http://therighttool.hammerprinciple.com/ ) to see what the general trends are.
One of the arguments for why to learn Haskell listed in the lecture slides is &gt; Puts experienced and inexperienced programmers on a more equal footing Why is that so?
Ah - but you've weakened this to the 'majority of the problems you want to solve' now. Not that I'm really a Haskell programmer, but fwiw, my first experience with Haskell was solving a combinatorial puzzle. Small enough to brute-force, but a complicated enough enumeration that it would have been seriously nasty in C, C++, etc. Even factoring in learning Haskell on the fly, it only took an afternoon to code up, no bugs once it compiled, and only took overnight to run. Probably something in the Prolog vein would have been just as good for this problem, but isn't as general a purpose problem solver. I'm not sure if OCAML has quite as much of the useful syntactic sugar pre-made to have been as useful, but probably wouldn't have been too much worse. 
Are you going to give any examples of things you believe some languages can do and others can not - or do you want to claim the only reason to learn any programming language after your first is intellectual challenge?
Upvote for reading paul grahams essays. The man has something to say! As for haskell beeing the most powerful language: In terms of the turing completeness it obviously isn't. But i found myself getting really excited when i learned about the declarative approach in programming and how easy it made some problems, e.g. lists in haskell. I can't imagine a faster and more elegant way to handle sets and lists than in haskell! As great haskell may be for such tasks, i wouldn't use it to write a webserver, because the problem appears to me much clearer in an imperative manner. I think it all boils down to using the appropriate tools for a given problem.
If you can write perfect code every time, sure Haskell doesn't give you much. If you want to do everything that you can to prevent runtime errors, then Haskell might be of some use since you can encode many kinds of bad behaviours as type errors. One example that jumps to mind is statically enforcing the separation between user generated input and computer generated output (say to prevent XSS attacks for a website). A builder may be able to build a safer house with better tools.
Nope. I need 10x less effort to solve the same problems in Haskell; also because I spend almost no time debugging.
Because Haskell has many fundamental differences from languages your programmers have learnt before: * Lazy evaluation as the default strategy. This one's huge - it means that you need a whole new way of thinking about space and time usage by a program, as the language does not evaluate everything you offer it. * Static typing with type inference. You don't need to be explicit about types everywhere (like you do in most mainstream languages with static typing), so it looks a bit like dynamic typing. Except that tricks that work in dynamically typed languages give you type errors. * Pure functional. This one gives you two gotchas for a mainstream programmer; one is that functions are now effectively data (you can pass a function around, you can manipulate it into a different function etc). The second is the need to be explicit about side-effects; in C, a function that just prints a word to the screen is defined as neither taking nor returning anything. In Haskell, it's defined as a value of type IO (). Again, this changes the way you think - state is something you try and avoid unless it's needed (so you use things like map to apply a function to each element of a list, or a fold to summarise a list, rather than writing a for loop with state). There's more, but those were the three that really got to me when I learnt Haskell the first time, and were still an issue when I went back to it 5 years after I'd last used it.
&gt; Never miss the opportunity to let your development environment eliminate as many wrong answers as possible, as early as possible. Yes!
I find your analogies flawed, and I am no language enthusiast. The majority of my time is actually spent writing code in Matlab unfortunately. Every day I struggle with the problems that plague that environment - mainly the lack of static typing. What arguments does this function take? Oh crap, it takes a row vector not a column vector? Oh, the shape of the array is off, now a 30 minute computation just failed on a runtime error. Not to mention I worked a few years in Embedded Systems on both micros and SoC Linux processors. A lot of the same problems there also - it would of been so nice not to have to carefully go over every single line in that pre-emptable kernel driver just to convince ourselves that our concurrent pointer handling and data structure manipulation was correct, only to find out in the field that indeed it was not. I could go on for a long time but the point is made, and it comes from pure pragmatism.
It's a first CS class. By using something different from most other languages there's less of a gap between students who already know a bit of C or Python or something and the students who are starting totally from scratch.
&gt; Things that matter! Ruling out classes of bugs with static checking doesn't matter? The things I learn on Reddit.
Other languages also have static type checking. He is asking what Haskell gives over other commonly used languages. The answer here could be the elimination of classes of bugs that are introduced if there are side effects.
IMO you're damning Haskell with faint praise, and not doing justice to its actual brilliance in the process. Haskell is very generally applicable indeed. In my experience, it's pretty difficult to find things that Haskell isn't better at than other languages, but they do exist, e.g. other languages can sometimes beat Haskell at short, specialized tasks. Imperative tasks are among those that Haskell excels at.
&gt; Other languages also have static type checking. How do you massage C's type system to provide the same guarantees as something like the [monadic regions package](http://hackage.haskell.org/package/regions)? &gt; The answer here could be the elimination of classes of bugs that are introduced if there are side effects. Isn't that just a subset of what I said?
After you succeed in shooting yourself in the foot you realize your foot has become your bottom.
I can't speak to Haskell specifics. You might want to look at NEO4J.
Learn Haskell because it is beautiful, a joy to program in, and you can use to build practical programs. Learning Haskell isn't as hard as popular opinion holds and doesn't require knowing higher maths. It is quite different than the standard imperative languages, but learning it will improve your code in those languages.
Neat! I like using the shellish package better than doing normal shell scripting. I believe it works on windows also. shellish lends itself to re-using commands after binding them to Haskell names. Binding them to Haskell names gives some additional safety (particularly after having used them once). I suppose you could still try something similar with shqq. The ideal would be for the qq to warn you at compile time if the shell command was valid.
except that unix is awesome :D
Just a guess, but perhaps it references the [perl qq operator](http://perldoc.perl.org/perlop.html#Quote-Like-Operators)? Shell commands are historically one of perl's domains so maybe it makes sense.
We have an in-memory Haskell graph database using STM at Silk. The graph store was rather easy to write, but now appears very hard to maintain when the amount of data stored per-process grows beyond several gigabytes. There are lots of design choices that affect the expressiveness, performance, memory consumption, transactional overhead etc of such a store. What is it exactly you want to achieve?
Probably the qq is about quasiquoting, which is a technique not restricted to perl.
Bitcoin? =)
I could, but I think it'd be wasted on you; feel free to keep making false downvote accusations, though. But seriously, why do you read /r/haskell if you really don't think it has any advantages?
I use Haskell at work for rules reductions, i.e. given a set of rules derived from observations, find an equivalent set of rules that has minimum number of rules. For this type of application, Haskell's syntax is amazingly succinct. But some "smart" manager here decides the GUI should also be written in Haskell (instead of using the generated C code and just embed it in some other framework better suited for GUI, say Visual Basic). Suddenly, the Haskell code looks disgustingly littered with IO, do-blocks, and MVar everywhere. The resulting GUI looks sick and feels dumb to the users. So you see, I've been there and Haskell is no holy grail.
I use Haskell at work for rules reductions, i.e. given a set of rules derived from observations, find an equivalent set of rules that has minimum number of rules. For this type of application, Haskell's syntax is amazingly succinct. But some "smart" manager here decides the GUI should also be written in Haskell (instead of using the generated C code and just embed it in some other framework better suited for GUI, say Visual Basic). Suddenly, the Haskell code looks disgustingly littered with IO, do-blocks, and MVar everywhere. The resulting GUI looks sick and feels dumb to the users. So you see, I've been there and Haskell is no holy grail.
When did I claim "any examples of things you believe some languages can do and others can not?" No, beside intellectual challenge, there is also the reason because that language offers you the library that could get your work done sooner. Does that answer your questions?
You don't understand what I said. Do people talk about MongoDB or CouchDB because of the language they were written in or because of the functionalities that they offer?
good for you; set aside the chance for Haskell preachiness to tell the real reason: that article was explaining *Why Haskell for an introductory course*, rather than simply *Why Haskell*. IMHO Haskell as an introductory course puts the imperative crowd at a disadvantage, since they have more unlearning to do.
I do agree that Haskell code, once compiled, usually works. And I have much higher confidence releasing code in Haskell than in C++. But come on, you can't tell me any Haskell programmer write good code. Programming languages do not write good code; good programmers write good code. Even good C++ programmers can write good code, believe it or not.
That's just a consequence of functional GUI programming being a research topic. Anyway, you can write a GUI in Haskell easier than you could in C or C++ with Gtk or Qt -- you can use glade with gtk2hs, you know, to decouple the UI specification from the code -- and yes, it will be imperative code, but *so is code in other languages*. But even if Haskell *was* somehow inherently bad at GUI programming, what does that matter? You don't compare languages based on their weakest points.
This was not a very popular answer, but honestly, the value of the "intellectual challenge" aspect of Haskell should not be underestimated. Most answers to this question are something like "Haskell forces you to learn a new paradigm, and learning this will help you code better in other languages". That's basically saying the same thing as "Haskell is an intellectual challenge".
I agree with statically typed languages. I hate Perl and Php for that lack of it. But my analogy is that Haskell fanboys worship their damned hammer so much they never seem to think about what they are building with the hammer. CouchDB was popular not because it was written in Erlang, but because of the different data-store paradigm it exemplifies.
Bullshit. Have you tried debugging in Haskell? Or do you mean your programs are so trivial that compiling correctly would already means it works? If so, then why the need for QuickCheck?
Finally a voice of reason!
I wasn't comparing languages! I merely stated that learning Haskell is great for intellectual challenges, but for practical tasks at hand any programming language would work as well. What is that so hard to agree?
In Haskell, many nontrivial programs are bug free once they compile correctly; Hindley-Milner and friends be praised. I recommend that you try it yourself as well.
When in Rome, do as Romans. On Windows platform, Visual Basic can build much better looking, much usable GUI than gtk2hs. And Visual Studio allows you do it in no time. Don't you agree?
It's so hard to agree because it does not reflect my experience: Haskell is definitely more expressive and reliable for a wide range of tasks I've put it to than languages I've used in the past.
So, no need to ever write unit tests in Haskell?
No. If anything, we test algebraic properties with randomly generated tests.
But why test at all if compiling correctly means it works already?
&gt;On Windows platform, Visual Basic can build much better looking, much usable GUI than gtk2hs. Well, I don't care about Windows, but fine -- there is also a nice wxWidgets binding (higher level than the Gtk one, in fact) for Haskell, and if you think wxWidgets doesn't produce acceptable Windows UIs then you're disagreeing with a lot of successful uses of it. If you can accomplish your GUI task easily in Visual Basic, then it was trivial. Fine, I accept that there may be advantages to writing a GUI in this way, but it can plug in to existing Haskell code using the FFI. You haven't presented any argument for Visual Basic being better than Haskell (a ridiculous statement, for sure); you've just said that Haskell isn't particularly good at GUIs. Which is a completely different statement from Haskell just not being particularly better than [insert language here].
Did you catch any haddocks yet?
Haddock. It's like 'sheep'.
Or at least syntactically valid, but a problem here is that there are many different shells that aren't completely compatible with each other. There are probably practices for how to write portable shell scripts, but not sure how trivial it would be to automate validation of that. Perhaps different quoters could be provided bound to specific shells: [bash| $(oops |] -- does `bash -nc '$(oops'` at compile-time, always uses bash at run-time
&gt; Hindley-Milner and friends be praised That put a big smile on my face. :)
Dude, end users care about the user experience. This is why Apple has been so successful with ipod, iphone, even though there are technically better products. You can't say you don't care. For the GUI app, you don't even need FFI, although it is arguably more portable and resilient for maintenance. You could just embed the generated C code. Dude, no sane mind would argue VB &gt; Haskell as a language. But the IDE support + the VB framework for GUI development is quite amazingly fast, easy, and produces very beautiful looking GUI on Windows.
&gt; I use Haskell at work There are so many people on this subreddit that would die to be able to say that. Jealousy &gt;,&lt;
:) Don't get me wrong. As far as fanboys go, I'm a pretty big Haskell fanboy. I'll try to be less reasonable next time ;) &gt; There is nothing Haskell can do that other languages cannot do [on a practical level, people!]. I was also going to say something scathing about this, but I think others have more than sufficiently scathed. Suffice it to say: even though the basic interpretation of that statement is clearly true, the connotation/insinuation attached to it practically begs for a flamewar. :)
Embedding the generated C would not work; not only is GHC's C backend deprecated, but it'll only give you a main() function, which will hardly be enough to plug into the VB code; that's why you use Haskell's FFI to expose functions as a library. Anyway, of course end users care about the functionality. Are you going to reply to the points I've raised, or are you just going to keep insisting that "VB produces a better GUI on Windows than Haskell" implies "Haskell does not give any advantages in practice"?
You use the `OverloadedStrings` extension, and someone's implementation of `fromString` fires at your foot at runtime.
True, true. And mucking around generated code is never a good idea, which is why I do agree the FFI is a safer approach. When did I make the assertion you claim? I already said I like Haskell in some tasks (rules reduction), but hate it in others (GUI).
You are putting words in my mouth that I did not say.
Well, you said in your first comment that the only honest answer to "why learn Haskell?" would be for the intellectual challenge, rather than the advantages it gives in practice.
In honesty, I code in Haskell because it flexes my mental muscles. That's the primary reason. And I don't recall saying, "only," in the original comment.
I was playing with the haxr library on Sunday and was half-awestruck/shocked by the recursive polymorphic variadic class type instance cool "black magic" of its 'remote' call function. I thought I knew some Haskell, but didn't know you could that!
Well, you definitely implied that people answering otherwise would be dishonest.
Dude, in math, a topic which Haskell builds its foundation on, the two terms "if' and "only if" convey different meanings and neither one implies the other. There is no implication of "only" without explicitly stating it.
If anyone in this subreddit would actually try to be honest, the answer would be the intellectual challenge. → If the answer isn't intellectual challenge, they aren't actually trying to be honest. If you don't think this implication holds, I'm afraid you're mistaken. Also, you're talking in English, not formal logic.
If you reread my comment, I did not disagree with the deduction that not answering "intellectual challenge" implies "dishonesty." Rather, I disagree with your prior claim that I had implied "intellectual challenge" is the *only* answer for learning Haskell, which I certainly did not claim. I was using [formal logic](http://en.wikipedia.org/wiki/If_and_only_if).
&gt; In my experience, it's pretty difficult to find things that Haskell isn't better at than other languages * Adding logging to your application. * Defining many repetitive methods that do similar things (ruby equiv map {|meth| define_method })
http://en.wikipedia.org/wiki/Contraposition
Dude, reread what I wrote. I was not disagreeing on the contrapositive statement but something else.
So you're not retracting the claim we're all dishonest?
Sigh... How did you derive "all" from what I said?
Ripple?
&gt; If anyone in this subreddit would actually try to be honest, the answer would be the intellectual challenge. Colloquially, the first part is in the subjunctive, implying that it's not actually the case that anyone in this subreddit tries to be honest. You wouldn't start a sentence "If wheels were round, we would ...", would you? Formally, you can at least take the contrapositive. There's some choice in the interpretation of quantifiers, but I get something like "If the answer isn't (doesn't include?) intellectual challenge, [the one responding] is not actually trying to be honest". I suppose drb226 and perhaps neitz don't fall afoul of that, so "all" is too strong. How about "most"?
I concur with skew's comment, but really, the content and tone of your post do not make any sense at all with whatever strictly formal interpretation with unnaturally-placed quantifiers you are trying to insist on; if you're not strongly implying that Haskell has no real benefits beyond the intellectual challenge, then your post is stating nothing.
There's an HTTP interface, but it's hideously verbose and slow. Embedding it in Haskell is not likely to work, since Neo4j runs on the JVM. Personally, my preferred method is to write a little server in Java that embeds Neo4j and gives me the API that I want, then communicate with it using some lightweight protocol like [MessagePack RPC](http://msgpack.org/). For me, this has worked pretty well.
In my opinion, Lisp (I assume you mean Common Lisp) is not at all close enough to Haskell to be much help. In fact, I know of no Lisp dialect that is. There is no really great transition, I think. Just go for it head first!
Lisp is no closer to C/C++ than Haskell is, so while I think learning Lisp is also a great idea, I think the order isn't important.
Interestingly enough Haskell had a very large influence on the new C++11 standard. Too bad concepts (pretty much a C++'d up version of Haskell's type classes) didn't make the cut. I would say just dive in and start working with Haskell. Haskell and C++ are pretty much the only two languages I work with these days, and learning Haskell has greatly influenced the way I use C++.
I would guess most Haskellers agree about the intellectual challenge, but didn't like being called dishonest.
"Most" is still too vague. I wasn't being vague. I said: if you are honest, then you would say the reason to learn Haskell is for intellectual challenge. Hence, the contrapositive is if *you do not agree that learning Haskell is for the intellectual challenge*, then you are being dishonest. Note the qualifier is *one who do not agree that learning Haskell is for the intellectual challenge*.
I think skew understood it better than you did, and you certainly did not understand what he said. Reread his comment and my reply to his.
Are you unfamiliar with colloquial English?
Clojure is as similar to Haskell as any language that isn't actually a typed functional language. It encourages pure functional programming, and uses lazy lists for a lot of things. Protocols and multimethods resemble type classes, and destructuring binding is at least closer to pattern matching than you usually see. No type system, though.
English is my second language. Why?
&gt; But even if Haskell was somehow inherently bad at GUI programming, what does that matter? You don't compare languages based on their weakest points. Err, that seems like one of the most useful ways to compare languages. Weak/Strong basically gives you fit for purpose.
This whole thread displays some of the worst reddiquette around imo.
This place used to be so much more friendly... Give him a break! It cannot harm to have one more framework, just don't use it.
Well, I'm trying to say that you can't take "Haskell is bad at GUIs" as evidence that Haskell is bad, in general; "Haskell is bad at GUIs but Visual Basic is bad at high-performance network servers" says something about Haskell and VB but very little about the relation between the two.
I understand what skew said perfectly, and my post does not contradict theirs in the slightest.
Yes, and Hackage used to be a lot smaller. While I agree, the solution to sorting out Hackage is more within a better cataloging system, right now adding more web frameworks *is* a problem. We already have new people coming along saying "Should I use Snap? Should I use Yesod?" - and now they have even more uncertainty. It also seems development has already halted for a month, and it's not even past 0.1.
Skew asked me if I was retracting my statement that all folks here are dishonest. I said no, I had a qualifier in my statement: for those who does not think learning Haskell is for the intellectual challenge. Skew then realized indeed there are exception to "all." So, he suggested "most" instead. But I went further to point out to him the more precise qualifier. Now, what did you understand?
You seem not to intend to imply some of the things your sentences do imply, so I was wondering if English was not your first language.
I agree with ehird that &gt; learning Haskell is for the intellectual challenge says that the set of reasons for learning Haskell and the intellectual challenges of using Haskell are (nearly) identical. In other words, your original post says the only reason to use Haskell is for the intellectual challenge, and anyone claiming Haskell has any other good points like "For this type of application, Haskell's syntax is amazingly succinct." or "I have much higher confidence releasing code in Haskell than in C++." is just being dishonest.
&gt; your original post says the *only* reason to use Haskell Again, I never used the word "only." And that word can never be implicitly implied according the standard verbal usage in formal logic. 
Again, I never used the word "only," and that word can never be implicitly implied according the standard verbal usage in formal logic. Am I not using standard English in the sentence above that is causing you trouble in understanding?
To answer the other side of the question, if you are already planning to learn a language with first class functions, that might help for learning Haskell, but probably not enough to make it worth learning a language you wouldn't have otherwise considered. The other thing you might hope to learn elsewhere is the type system, but I don't know of any other languages with similar type systems besides other fairly similar functional languages like O'Caml, SML, maybe F#.
another day another string implementation i have an announcement along these lines - Textiratees. unicode 128 strings are lazily consumed before they are even produced, with an interface identical to booleans. all operations are done in O(1) time but it will take O(n^n^m) time to integrate it in with the other three string classes produced this morning
BTW, you never define "better" yet. Do you have any Haskell code in production use? I mean, someone paying you to write Haskell that gets run in production system that is actually used by others. Just trying to understand if your experience with Haskell has been purely academic or as a hobby only.
I understand your point. However this library actually unifies two popular vector-like libraries: *bytestring* and *vector*. Also, as described, this library is not intended for production code just yet. It first has to become as fast as *bytestring*. Ideally, when this happens, we can replace *bytestring* with *vector-bytestring* (which basically means renaming *vector-bytestring* to *bytestring* and renaming the modules accordingly). But first we need to be able to play with this a bit.
i was just trolling. i'm sure your code is awesome
It is ;-) since most functions are either just copied from `bytestring` (and slightly adapted to vector) or just use the corresponding function from vector directly. There's not much code of my own.
The awesomest code is the code that somebody else already wrote.
For non-german readers: It's about an iPad-based patient-file system for hospitals with some of the components written in Haskell. Beyond the usual praise about Haskell's strengths, there's some imho interesting criticism raised w.r.t. where potential for improvement is perceived (translated/paraphrased slides): page 12: **Laziness** * hard to balance laziness vs. strictness * strictness not represented at type-level * strictness is a low-level/operational matter * heretical question: wouldn't a strict-by-default language with laziness opt-in via annotation be better? page 13: **Runtime Monitoring** * what's the server currently doing? * why does a STM transaction cause so many retries? * which threads are currently running? * which constructors are currently being allocated? * ... page 14: **GHC Profiler** * offline analysis (online profiling might be desirable, like available for JVM) * doesn't work with multi-core(?) * significant performance-overhead page 15: **Libraries, cabal-install** * Many libraries on hackage inappropriate (don't scale, don't perform, not correct, experience needed to select the proper library, causing one to write own libraries) * multiple versions of same library cause problems with cabal-install * solution: private hackage repository with (hand-picked) stable subset from Hackage page 16: **Barrier to entry** * only few programmers are proficient in Haskell * huge step from occasional Haskell programmer to professional Haskell programmer * other languages (e.g. Scala) offer a smooth transition 
&gt; hard to balance laziness vs. strictness I hope we'll be able to encode more strictness properties in library design in the future. For example, we're splitting Data.Map (and the rest) into Data.Map.{Strict,Lazy} to make it easier to state up front what kind of behavior you want (instead of having to remember to use the right version of a function at every call site). &gt; what's the server currently doing? &gt; which constructors are currently being allocated? Edward Z. Yang recently added some runtime GC monitoring (available through an API under GHC.*) which is a start. We should be able to expose more heap information during runtime. This is something I want to see happen too as it's important for long running servers. &gt; significant performance-overhead [when profiling] Simon M's recent work on stack traces (which will use the same infrastructure as profiling) should improve this *a lot* (I don't quite remember the numbers he gave but they were very encouraging).
Cool. I'm glad details are being paid to the benchmarking -- that's really whole point of bytestring after all. Any work on memory footprints? The lack of pinning should be good for this.
"Powerful" in this context doesn't really mean powerful in the sense of turing completeness. In that sense, almost all languages are equivalent. However, just because it's possible to write a program with the same behavior in two turing-complete languages, they are different from the standpoint of what's easy or convenient. If the languages are different enough that you tend to solve problems in different ways, they are, to the user, very much not equivalent. I'm not sure Haskell is powerful in quite the same sense that Paul Graham meant. He tends to be rather disinterested in safety -- a language should get out of you way and let you do things your own way, in his view. My experience is that I tend to be more ambitious when I know I can trust the type system to catch my worst mistakes. As for web serving, I've had good luck using HappStack, and Snap and Yesod look to be quite promising, so it isn't a ridiculous thing to do in Haskell. Granted, writing a web server is probably quite a bit harder in Haskell than what I have done, which is to write a specific web app using an existing framework, but clearly it is doable.
But if those methods are that similar, why not abstract away the similar parts, and make them higher-order functions that take the customization part as simple arguments?
But it's written *for* Haskell programmers, for whom this is a perfectly well-understood and unambiguous statement.
Because some things are too simple for abstraction and just plain repetitive. Think say choosing 20 functions you want to proxy directly to 20 other functions in a different module. I'm not talking about having to write 6-7 lines over and over here, talking about having to write 2-3.
Very nice. I will definitely have to steal these.
The first slide says Haskell is uniquely suited to help build programs from composable parts. How are classes in OOP languages not similar? And please, it's not a troll question. Please discuss with constructive arguments.
&gt; the answer would be the intellectual challenge. http://www.merriam-webster.com/dictionary/be Identical to, coextensive with, have the same meaning as. "the intellectual challenge" doesn't make sense as a property, so 1d or 1e of the definition do not fit. If you didn't want to imply only, you could say "the answer would include intellectual challenge". Also, "If anyone in this subreddit would actually try to be honest" is only good phrasing if you intend to give offense.
Are you one of those people who upon seeing a black goat in Scotland would declare that the observation only proves there exists at least one goat in Scotland who is at least half black but no more?
if object A has got some state, and object B gets ahold of A and starts invoking state-altering methods (even through public interfaces), then to predict A's behavior, you also have to look at how B affects it. if C affects B, then you're looking at C to understand A, etc. now scale to 1000 different classes. *edit:* note, i would not make the claim the slides are making. i'm just addressing why oop isn't a composability silver bullet.
So, how does Haskell decompose (and recompose) states?
You seem to be, and I'm trying to tell you goats are usually the same color on both sides.
I could pull up any non-trivial, say, C++ article and it would be full of OO jargon. I honestly don't see your point. 
Do you agree this conversation has diverged away from what the post is about? Right now, we are arguing the meaning of "be," "only," and rather goats have the same color on both sides. :) Time to move on with our lives?
Kliesli composition is the same as normal function composition, but for monadic functions (`(Monad m) =&gt; a -&gt; m b`).
"a monad is like a burrito"
Hoping some Haskellers can shed some light on this. :)
These are actually great. I want to try pandoc!
http://en.wikipedia.org/wiki/Purely_functional
&gt; why does a STM transaction cause so many retries? this seems to have been addressed partly by the authors themselves: http://factisresearch.blogspot.com/2011/10/stm-stats-retry-statistics-for-stm.html
in general it is related to not having side effects and same input having same output. so function fo argument "a" will always return same thing and it will not change states out side of it. so if you will have foo a = a eg. for 1 it will return 1, for 2 it will return 2…. and you can call it any number of times with 1 and it will return 1. eg. in C you can have unpure function like this int a = 5 ; int foo(int a){ a = 6; return time(0); } which will with each call return different result (time) and set a global variable to different state. This can be cause of bugs, problems and it is a sample of "dirty" (unpure) function. 
The first thing I think of when hearing the words "composability" and "Haskell" together has nothing to do with OOP: Lazy evaluation. One example from these slides is: minimum = head . sort In this example let's assume sort is implemented like [here](http://www.haskell.org/haskellwiki/Introduction#Quicksort_in_Haskell). Now until the end only the "lesser" part will evaluate until we get the first element. The complexity will be: n + (1/2)n + (1/4)n + ... = 2n = O(n), instead of O(n*log n), which means sorting the whole array and then take only the first element. So, in Haskell composing two functions can reduce the amount of calculations you do, to get only what you need. There are probably some better examples of that but I am, er... to lazy to find one :P.
State is stored in spacesuits.
Functions are a concept from mathematics, and consist of a mapping from values in a domain to values in a codomain. Procedures are a concept from computation, and consist of a set of instructions. One use of them is to generate a mapping.
As I understand it, Data.Vector is always pinned too :-(
This is a good question, and John Hughes' classic paper, [Why Functional Programming Matters](http://www.cse.chalmers.se/~rjmh/Papers/whyfp.html), is a great answer to it if you have the time. Do try to read it; it's written in an extremely friendly and accessible style. The TL;DR version is that higher-order functions and lazy evaluation are "two new, very important kinds of glue" that let you write much more modular programs. &gt; Functional languages allow functions that are indivisible in conventional programming languages to be expressed as a combinations of parts — a general higher-order function and some particular specializing functions. Once defined, such higher-order functions allow many operations to be programmed very easily. &gt; [Lazy evaluation] makes it practical to modularize a program as a generator [f] that constructs a large number of possible answers, and a selector [g] that chooses the appropriate one. Program f can even be a nonterminating program, producing an infinite amount of output, since it will be terminated forcibly as soon as g is finished. This allows termination conditions to be separated from loop bodies — a powerful modularization. Lazy evaluation is perhaps the most powerful tool for modularization in the functional programmer’s repertoire. 
So the extensive collection of packages found on hackage is a reason to learn Haskell? Good. Sorted. Moving on.
Why post this to r/haskell? Everyone here already knows haskell, right?
`ByteStrings` (from the `bytestring` package) and *storable* `Vectors` are represented in basically the same way: data ByteString = PS {-# UNPACK #-} !(ForeignPtr Word8) -- payload {-# UNPACK #-} !Int -- offset {-# UNPACK #-} !Int -- length data Vector a = Vector {-# UNPACK #-} !Int -- length {-# UNPACK #-} !(ForeignPtr a) -- payload The only difference is that vectors don't store the offset. They will advance the `Ptr` in the `ForeignPtr` instead. So that will safe some but not much memory. It would be interesting to extend `vector-bytestring` with a `Data.Vector.Unboxed.ByteString` module which exports: type ByteString = Data.Vector.Unboxed.Vector Word8 Note that a `Data.Vector.Unboxed.Vector` is ultimately defined as a `ByteArray#`. (That one is unpinned right?) 
some ppl here might be interested in giving talks about "why learn haskell", and might benefit from having access to existing slides on that topic...
They can be pinned or unpinned depending on how they were created, but as far as vector's concerned, I'm pretty sure it creates unpinned ones. Edit: yeah, http://hackage.haskell.org/packages/archive/vector/0.9/doc/html/src/Data-Vector-Primitive-Mutable.html#MVector shows it using `newByteArray` rather than `newPinnedByteArray` from the primitive package.
Two of the answers do not even address the question. Why do these people even post answers? Why are people _here_ doing the same thing?
I know what it *means*, the question is why we use the word "pure" instead of some other word.
I know what it *means*, this doesn't answer the question at all.
An interesting question. I'll ask around and see if anyone remembers when the word "pure" first started appearing.
Ah. Good point.
&gt; The only difference is that vectors don't store the offset. They will advance the Ptr in the ForeignPtr instead. How do they avoid problems with GC freeing things out from under you?
Very good example. Although the O(n) complexity is a nice result of laziness. I think the main win really is the terse syntax for function composition. This is only possible if you language supports higher order functions and currying.
Someone who knows Haskell better than me can go on about the State monad and such, if they like. But one answer is that state is often avoided. When you would add an element to a list, thus changing its state, the more Haskellish option is often to instead return a new list, identical to the old but with one extra element. Keeping things immutable whenever possible is an idea that's gaining popularity even in non-functional languages like Java.
Some of us (raises my hand) are here to learn from others as we begin to learn Haskell for ourselves.
In principle I like the idea of returning a new list/tree/graph/etc, but at a practical level how performing is creating a new list/tree/graph/etc when there are lots of changes with each change involving only a small number of elements in a large list/tree/graph/etc? 
&gt; How do they avoid problems with GC freeing things out from under you? Storable vectors are never "freed out from under you" because the GC only executes the associated finalizer of a `ForeignPtr` (like `free ptr`) when all references to the foreign pointer (in the Haskell heap and stack) are gone. Being able to change the internal pointer of a `ForeignPtr` does not change the previous semantics. So for example it's safe to do something like this: (you should not advance the pointer outside its allocated space of course!) basicUnsafeSlice i n (Vector _ fp) = Vector n (updPtr (`advancePtr` i) fp) where `updPtr` is simply: updPtr f (ForeignPtr p c) = case f (Ptr p) of { Ptr q -&gt; ForeignPtr q c }
the state monad and lists aren't necessarily related. for what you're asking about there, as opposed to state and monads, see [Persistent data structures](http://en.wikipedia.org/wiki/Persistent_data_structure). if you do it right, then under certain circumstances, they're more efficient than mutable data structures. say you've got a tree, and you want to both change a few nodes, as well as keep the original. with mutable structures, you'll copy the whole damn thing, and then do your updates on the copy. bam, 2x memory allocation. persistent data structure, you just allocate new nodes for the affected portions, and then keep using the parts of the original that weren't changed. memory allocation: reduced! now, you could potentially do that in other languages, but immutability guarantees that it's safe, and haskell is designed with this use case in mind. (garbage collector is probably tuned for it, that kind of thing.)
I think you'll find it dificult to get a definitive answer on this. I am of the opinion that it arises from the mathematical jargon of: f is a pure function of a This carries a different meaning from the programatical one, where it means that the f is just a normal function of a, _and nothing else_. It's a phraseology that is less commonly used today, but back when a lot of this was done by 'Natural Philosophers', it was used to indicate that that it and been demonstrated that there was nothing else complicated the case under question. It can be seen that the use of the word 'pure' holds the same meaning in both the natural philosophy sense, and in the computational sense. To pre-empt the next obvious question in, I think the natural philosophers [0] used that prior to the key developments in set theory and functions that resulted in the modern definition of a function; so it was not always clear that the relationships were being described in a total fashion. Any other word that could have implied that 'lack of other confounding aspects' got appropriated to mean something else in mathematics (e.g. total, simple, real, natural). Hence it was pure that stuck. [0] And, of course, many others. In modern mathamtics a 'function' is taken to be pure, so the term is less used, of course.
&gt; heretical question: wouldn't a strict-by-default language with laziness opt-in via annotation be better? I wouldn't say this is "heretical". I'm pretty sure one of the Haskell "higher-ups" (one of the Simons?) has written a paper considering the benefits of strict-by-default, though I can't find it right now.
I would recommend a `newtype` with an exported constructor and eliminator rather than a synonym; I find the `show` instance for ByteStrings valuable, and since a full interface is exported, the slightly increased verbosity for using a ByteString as a Vector shouldn't be much trouble. Is there any advantage to using storable Vectors over unboxed Vectors? The latter have the advantages mentioned in other comments, but I'm curious if there's any reason to use storable Vectors at all here.
I'm convinced that a certain percentage of commenters don't read what they are responding to, or they are so stupid they come out with the first thing that enters their head *anyway*.
Perhaps, but you just wind up having to repeat yourself (in admittedly very terse ways) if you don't have laziness at your beck and call making it okay to just implement the most general thing once and let it prune off the portions of the structure you don't use.
Peter Henderson refers to "purely functional" languages and defines them in the standard sense in his text book "Functional Programming Application and Interpretation" from 1980. His remarks seem to suggest Landin was using "applicative languages" as a synonym before this (as I believe Backus was). Maybe David Turner was using the term "purely functional", as I no longer have a college affiliation I've never seen any of his early papers on SASL etc.
&gt; ddarius: Well when people say you need a PhD to program Haskell, you can point out that SPJ doesn't have a PhD Interesting. I wonder how he managed to get a teaching position at Cambridge.
This whole thing is only a problem because GHC can't dynamically pin and unpin ByteArrays. We discussed a design which would support this at one point but it never really went anywhere.
Those kind of lazy, vote-grabbing answers are annoying. StackOverflow really is getting poorer for them. I felt like commenting on the first one posted but feel like that would be a bit curmudgeonly. A single user isn't responsible for the whole problem.
Yeah, I'd love for that to be possible. Is the design discussion public anywhere?
I'm with you for the `newtype` suggestion. &gt; Is there any advantage to using storable Vectors over unboxed Vectors? The latter have the advantages mentioned in other comments, but I'm curious if there's any reason to use storable Vectors at all here. Because one of the design goals of `ByteString` was for it to be easy/efficient to use with foreign libraries.
Honestly, I'll take consistency over availability. It's fairly easy to maintain uptime on computers, and I'd rather have minor occasional delays than errors.
I have a feeling we're wasting our time.
Not by the definition you give, but I find that definition of "production use" suspect.
&gt; I find the show instance for ByteStrings valuable Can you explain why you find it valuable? Do you use the show instance purely as a debugging aid or do you actually use it in production code? Of course, I can always provide a newtype with the desired show instance *besides* the `ByteString` type synonym (it can even be defined in a separate package).
Alas, no, it was an email thread which I don't seem to have any longer. IIRC, the basic idea was to have an operation like: pin# :: ByteArray# -&gt; ByteArray# Then you could say: let y = pin# x and y would have the same contents as x but be pinned. Whether it is the same memory block as x would be implementation-dependent but with GHC, it would be. When GHC would garbage collect y, it would unpin x (if x is still alive). You would also have unpin# so if you said: let z = unpin# y then z will become unpinned when y is garbage collected.
Wow, that'd be nice. Would you be opposed to (me, or someone else) making a bug on the GHC trac to propose that, to at least revive discussion on the matter?
[Here's my answer](http://stackoverflow.com/questions/7750533/why-are-pure-functions-called-pure/7761129#7761129); it's nothing mind-blowing but those are my thoughts on the issue.
Oh, please do that! It would also be good to have an example where this would actually be useful if you can think of one.
Re: *state is often avoided* Agreed. It's a different take on "worse is better." It's a poor programming habit to have mutable state strung throughout all your code. Haskell takes the approach of making it harder to write code with mutable state than to write code without it. By providing very poor support for that bad programming habit, Haskell pushes you to come up with a clean design. 
&gt;Can you explain why you find it valuable? Do you use the show instance purely as a debugging aid or do you actually use it in production code? As a debugging aid, but considering that GHCi is basically the one and only debugging tool for Haskell, and is also incredibly useful for development, that would seem to be a rather major feature :) More important, I think, is whether there is any reason to use a type synonym; IMHO newtypes should be the default solution we go to.
...and [forget about anything requiring atomic transactions](http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html#comment-334097393). I'm not saying that isn't a valid design tradeoff: I find it very appealing. But the hard consistency vs. availability problems are still there. If you're Ticketmaster running sales for a hot concert with this scheme, you'll have to send out "confirmation" emails that say: "Your order for four tickets has been received. It may or may not be fulfilled." Sure, your system is "available", but not in the sense that users are receiving useful answers to queries like, "Did I get tickets?" 
One suggestion I've heard is to have an 'isPinned' primop, and then just copy the byte arrays which aren't pinned into pinned memory (the big ones are already pinned anyway). I worked on this once, but I'm not sure where I put the branch. I'm pretty Cmm impaired.
The end goal of the 'isPinned' primop being a function like: asPinned :: ByteArray -&gt; (ByteArray -&gt; IO a) -&gt; IO a or the like.
&gt; My point is that OO jargon and category theory are on two entirely different levels of conceptual difficulty. Only if your understanding of OO is the paper thin sort of OO that one might get away with in Perl or something. Can you quickly and clearly explain to a college freshman what contravariance and covariance are and why they have sometimes made implementation of generic types tricky? I would observe the words "category theory" end up showing up on the [Wikipedia page](http://en.wikipedia.org/wiki/Covariance_and_contravariance_%28computer_science%29). &gt; if C++... Wost. Possible. Example. C++ used at its fullest is ___much___ harder to understand than Haskell. If you find it easier than Haskell, it's probably not even that you don't understand Haskell yet, it's probably that you don't understand C++ anywhere near as well as you think you do, and you don't even realize how much about C++ you don't know!
&gt; How are classes in OOP languages not similar? Goals aren't results. OOP had the _goal_ of making programs something you could build from composable parts, but it has failed to convert that _goal_ into a _result_ in the promised manner. Many people have forgotten this because the promises have been walked back since the early days of OO, but in terms of the original promises made by OO advocates, OO is simply a failure. (I think it is actually useful and some value was salvaged from it. I'm not trying to bury OO here, I still use it when appropriate.) Composability remains a goal with FP as well, but I'm not sure I'm ready to declare it a "success" yet. There's some interesting results, though, and some signs that it may ultimately have more legs than OO, but we're still all exploring that possibility together. My experience has been distinctly mixed.
I created a ticket at http://hackage.haskell.org/trac/ghc/ticket/5556 :) Not sure I have a terribly compelling use case, but it's pretty compelling for me :P
Uh, I've never taken any formal instruction in category theory and I understand monads (as they relate to Haskell) just fine. It's a pretty similar level of complexity to understanding objects in an OO language. I've had about equal amounts of difficulty explaining objects as I have monads to other people, it's just that people would feel foolish saying they didn't understand something pertaining to a common word (in English) like "objects" while they have an excuse in that they've never heard the world "monad" before.
From the discussion on Hacker News: This looks like Event Sourcing / CQRS. Is that correct? nathanmarz: There are similarities. It's really closer to functional programming than anything else.
Well, it's atomic if you wait until the next batch pass, right? ;-)
&gt; Can you explain to me what all these things are: applicative, functor, monad, monoid, morphism (and all the subtypes)? Because these terms pop up in Haskell conversations all the time. They pop up in Haskell conversations all of the time because a lot of category theorists are interested in Haskell, not because they are necessary to understand Haskell. I've come to understand them with time just by being exposed to them, but I've never explicitly used a morphism or monoid in my code. &gt; To grok the most basic Haskell, you need to understand monads. Complete BS. Haskell implements IO using monads. C implements IO using unknown feature X. Saying that you have to understand monads to do the most basic Haskell programming is like saying you have to understand unknown feature X to do C programming. Hell, I can't even remember what a functor or category is, and yet according to you I can't have written the thousands of lines of Haskell code that I have without knowing what those two things are. With this evidence of my lack of knowledge, your entire argument about Haskell's inscrutability falls apart. &gt; say, I/O, Haskell is hard. Here's a [post](http://www.reddit.com/r/programming/comments/bh7ky/how_to_and_not_to_give_a_talk_on_f_or_any_other/c0mratv) I made a year ago address this. I give the same IO-heavy code in C# and Haskell and the one-to-one correspondence between the two is obvious. Notice how the word "Monad" does not show up in the Haskell example, nor does evidence of their existence. This is just the same as the C# example in which "unknown feature X used to implement IO" does not appear, nor does evidence of it's existence. Now if you want a language that is really fucking hard to learn, try C++ (which you mentioned). Even though I learned Java and C# as my first two languages, I still can't come close to understanding C++. Even the wikipedia [article](https://secure.wikimedia.org/wikipedia/en/wiki/C%2B%2B#Criticism) agrees with me.
See Phil Trinder's PhD thesis under Wadler for the design of a purely functional database in a "Haskell-like" language.
Because they were born on a full moon and bathed in the blood of virgins. Also they don't alter the environment around them.
If this gives up consistency, how exactly is it "beating the CAP theorem"? That one actually says "CAP - pick any two" and in that sense this is just one of many points in the CAP solution space ...
The compiler can share parts of the data structure that do not change between the old and new versions.
Is that not why stackoverflow, like reddit, has a down vote?
The point is that this depends on your problem domain. If you're, say, Facebook, you don't really need consistency. It doesn't matter if some people sometimes see out of date data, or similar 'errors'. What matters is that your site is always up. The same goes for Google: they'd rather have a fast query that might be missing some inputs, than the same results for everyone, but slow.
Maybe this a dumb question and not the right place for it but: is the CAP theorem formally proven somewhere?
I [proposed and implemented](http://trac.haskell.org/vector/ticket/64) specific `Show` and `Read` instances for vectors of `Chars` and `Word8s` that show and read like `Strings`. Hopefully this is a satisfactory solution.
I think if I wanted a strict-by-default functional language I... wouldn't choose Haskell?
[Brewer’s Conjecture and the Feasibility of Consistent, Available, Partition-Tolerant Web](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.20.1495&amp;rank=1)
Common Lisp is. Scheme isn't.
If it's "unique", it's for the combination of features. Another part of the answer is parametric polymorphism, to explain what's flexible in a function as well as what is required. You can get some of that with generics in some OOP languages, but those are usually annoying to work with.
Neat. Will certainly make this assumption and try out how it works out. It does sound like the runtime system does a lot of work to make this efficiency happens. Hopefully, most bugs in such complex system are flushed out already.
Curious to hear how OOP has been a failure in achieving composable software. And even more curious why you think FP is still trying to get there. Seems like you have in mind what composable software should be like and you see big gaps in OOP and at least some gaps in FP as well. Can you elaborate on what properties of the language would achieve the goal of composable software? 
In principle that can be done. Does anyone know if in practice Haskell's runtime does this? I really like this idea, but I'm a little worry that such runtime will require some sophisticated coding to support this in a safe way, especially when dealing with parallelism in Haskell.
Over the course of a normally skilled (as opposed to a wizard) person's creation of some larger program to meet some need, a language and paradigm that encourages composability ought to have two properties: First, many of the tools used ought to be re-used chunks of code from other projects or libraries, and second, much of what is produced for this project ought to be clearly either one of 1. a reusable component or 2. specialization of reusable components for the local problem. OO has had some success in the large and in the small. In the large, you get the huge frameworks like web frameworks or ORMs or similar things. They are massive, and generally if not designed for each other often have a hard time being combined in an unexpected way. (Hard != impossible, BTW.) In the small, you have things like containers or simple bindings to libraries that do some well-defined task, like convert an image from PNG to JPG or something. But there's a vast wasteland in the middle. Way back in the day, OO promised us a rich ecosystem in the middle, where people could just snap components together to make business software, but it doesn't really work that way. You end up with too binding to the local environment for any of this to work, or if you do try to build a component that has this property it ends up so virtual that nobody can understand it, and the syntax hoops to use it are insanely complicated. FP has some hope that this middle ground might be covered, but one thing that I think has already been proved is that simply waving laziness and monads at the problem isn't sufficient. But we're still learning a lot. One interesting thing that I've observed for instance is that there's some good sharing in the parsers; Aeson, for instance, reuses and provides some generic stuff for handling JSON parsing, built on and extending Attoparsec. Another interesting field of middle-ground coverage that OO can't do at all is the STM library, which provides a sophisticated approach to dealing with concurrency while composing reasonably well with any normal functional data that you have. But it's not perfect, and I still think there's some unlearning to do before FP reaches its potential. Haskell inherited the OO idea of encapsulation by hiding lots of things from users, so the consequence is that even if you download a library that useful bits in it, the useful bits are often walled away from you by "encapsulation". I don't think you need all of strong encapsulation by hiding symbols, very strong typing, and complete immutability preventing you from changing anything (like monkeypatching). Perhaps it won't work either. It's not out of the question that the sort of composibility that OO advocates first promised us is simply impossible, and we are simply doomed to forever live in a mire of endless specialized code that can't be extracted. But at least FP gives us another shot at the dream, now that we're pretty sure OO can't do it.
It's called structural sharing, and from everything I can gather GHC does this, as well as Clojure's compiler. Certainly, I can't imagine how GHC couldn't do it and Haskell programs not be slowed to a crawl.
It's not a characteristic of the compiler/runtime! individual datastructures do this (even in other languages), and most Haskell datastructures do it where possible. All common non-array, non-hashtable datastructures in Haskell do it. It's a lot easier to share than not to share, and it's not a problem to do so because things are immutable.
Back when I was learning OOP in the early 90s there were major problems with object hierarchies being designed at the time. Although I wasn't aware of it at the time, the problem stemmed from inheritance failing to follow the Liskov substitution principle. The LSP seems is an extremely strong constraint on inheritance, especially when objects hold mutable state (which is practically what objects were designed to do). See the circle-ellipse problem and definitely read [Subtyping, Subclassing, and Trouble with OOP](http://okmij.org/ftp/Computation/Subtyping/) which goes on to show that even immutability is insufficent. If I understand correctly, if hierarchies fail to follow the LSP, then program correctness becomes a global property rather than a local property. Clearly correctness being a global property is anti-modular and prevents programs from being composable. To make matters worse, the LSP is a semantic property and cannot be checked syntactically, so the compiler cannot enforce it (Actually this isn't entirely true; There is [a set of syntactic rules you can follow to ensure the LSP is satisfied](http://okmij.org/ftp/Computation/Subtyping/Preventing-Trouble.html), but these rules include no mutable publicly accessible encapsulated state). I'm told things have changed, and object hierarchies are better built today, but since I haven't done any serious OOP in a decade I cannot confirm this.
Just for clarity's sake, the real CAP theorem says in the unavoidable presence of the danger of partitions, you can't have both 100% consistency and 100% availability. You can't say "I'm picking CA and ignoring the P."
It's pretty easy in Haskell to throw everything in IO and keep mutable state in IORefs and such. It's just frowned upon, and Haskellers are typically Haskellers by choice, so they don't do that. I suspect if Haskell gets an influx of unwilling programmers, you will see plenty of that kind of thing.
Sure, fair enough.
If you consider inheritance to be part of OOP, then it is distinctly anti-modular. Consider an example. Many OOP unit test frameworks use a TestBase class that you inherit to implement "setup" and "teardown" methods for your setup environment. Then you're supposed to inherit that again and implement "run". Example in Python: class Setup1(TestBase): def setup(self): ... def teardown(self): ... class Setup2(TestBase): def setup(self): ... def teardown(self): ... class Test1(Setup1): def run(self): ... Now, consider what happens when Test2 needs to use (compose) both Setup1 and Setup2. If it tries to multiply inherit them, then you're going to get conflicted implementations of setup and teardown. You may write a disambiguator of all methods -- but then you have to repeat the calls to all classes in all methods, in the right order. In short, these do not compose. Consider an alternative, a higher-order function: def setup1(func): def new_func(*a,**kw): ... setup func() ... teardown return new_func This higher-order function composes perfectly: setup1(setup2(my_test)). It also makes the interface/functionality exposed by each component clearer.
If you write `-- stmt 1` instead of `*//stmt 1*` then people can copy and paste directly into a text editor and run your code. You should also indent all the lines after `main = do`. Also you might get more answers on stackoverflow than here.
Here you don't assign things, you name them. Your x here isn't something you'll be able to update here and there in your code. You have given a name to the result of the first call of "fmap readInt getLine". Then you've given a name to x+3, namely y. Your second call actually kind of "shadows" your first x, it doesn't *modify* it. So y is still the name given to x+3, x being the *first one*. It's not like in the equivalent C code where you'd have a variable evolving together with another.
If I understand you, you are curious why the second `print y` does not change when x changes in the line preceding it. Haskell is a single assignment programming language. You can only set x once. If you want to use a new value of x, call the variable `x'`, ("ex prime"). Then calculate a new y, called `y'` ("why prime"), and print `y'`.
"x" from stmt1 and stmt2 are two different uncorrelated objects. The operator "&lt;-" is not assignment as in C which changes value of variable. It defines a new name, hiding the previous binding. The same is with let x = 5; y = x in let x = 6 in y which is 5. Second "x" is a different object which only by coincidence has the same name. It's like C code: { int x = 5; int y = x; { int x = 6; printf("%d¨, y); } } which also prints 5. In Haskell variables once set don't change, if somewhere you say "let y = x + 3" then since x cannot change, y won't either. If you want to have expression "x+3" for varying x, use a function: main = do x1 &lt;- fmap readInt getLine *//stmt 1* let y x = x + 3 print (y x1) x2 &lt;- fmap readInt getLine *//stmt 2* print (y x2) You could write that with x instead of x1 and x2, but this is only confusing. You can have "assignment" behaviour by using State or ST monad, but for now try accustoming to functions which are more often used and more flexible. It turns out during programming you don't need assignment often. The usual situation is when you update a variable during a loop: x = 0; for i in a: x += i; print(i); In Haskell this is done using functions like fold, map, filter, sum, sequence etc. They are larger "blocks" and say more about situation on first look than a code which updates a variable multiple times.
@g_: kk..i get you that the value of x can't be changed in haskell. So, now tell me one thing when i write print y (what i think is :- y here is equivalent to x+3) so the thing am not still getting is why the different o/p when i write print y and print (x+3) so why in x+3 the updated value of x is used..??
kk....so what u mean't to say is that if the value of x is set once it can't be changed....so why the different o/p in:- print (x+3)
"y" is equivalent to "x+3" but the name "x" refers here to old variable, whose name was shadowed by another. It's like you are on a party. People give George a nickname "Donkey". Later George leaves, but another person whose name is also George comes. The name "George" now refers to the new guy. But if you say something about Donkey, you will still refer to the old guy who is now absent. When you defined "Donkey" you defined it to be "George" where meaning of "George" was determined at time of *definition*, not at the time of *reference*. If you talk about "Donkey" later you're talking about the person who was George in the past, not neccessarily to the person who is now called George. When define "y" using "x", compiler looks once where "x" was defined. If you define new "x" later, compiler won't update "y" since it was defined using 'old' "x". &gt;the updated value of x It is not "updated"! The value cannot change. Look at this: let x = 5; y = x+1 in let x = True in y If you allowed "y" to use "new" x, you would have to add True+1 which is a type error. Old "x" and new "x" have different types and are in different places in memory. Calling both of them "x" causes confusion just like you can confuse two people who are called George.
When they designed Haskell, they could have completely disallowed this type of name shadowing (indeed, GHC warns you about it if you use `-Wall`) but they didn't. What are the legitimate use cases for name shadowing?
`y` isn't a set value but an expression which evaluates to `x + 3`, whatever value x is. By printing the new value of x, you force Haskel to reconsider its calculation of `x + 3`, and thereby reconsider the value of `y`. That's why you use single assignment. If you want to change a value, create a new variable (`x2`, `x'`, `z`, etc).
thanks. A very nice explanation.
It's worth pointing out that if you compile the above code with warnings on (-W or -Wall), you will get the warning: Warning: This binding for `x' shadows the existing binding bound at temp.hs:8:11 (the stmt 2 line) This is generally because binding a variable twice in Haskell is rarely a good idea. Both x's are different variables, which makes it confusing to reason about. You can enable warnings in ghci with the command `:set -W` or `:set -Wall`. `-Wall` gives more warnings, while `-W` just displays the most important ones.
ya...its a nice thing
&gt; ddarius: isJust :: Maybe a -&gt; Bool; isJust = unsafeCoerce Indeed... λ&gt; quickCheck (\s -&gt; isJust s == unsafeCoerce s) +++ OK, passed 100 tests. It's funny but I can't explain it.
I don't know any really legitimate ones, but it helps me to be lazy and don't think about new names. For example I want to call a variable "map", I can write: f = .... where map = .... This shadows Prelude's map, but I don't care about it. In a large program, many useful names could be taken, so you'd have to worry if a name was defined previously and continuously come up with new names. Of course the better way is to use "import" selectively and break the program into small modules, but the practice isn't so simple. It's normal names are not unique, for example "pi" can mean [a lot of things](http://en.wikipedia.org/wiki/Pi_%28disambiguation%29). You have to make the names longer which isn't Haskellish. Perhaps there are also some theoretical reasons as lambda calculus allows it too.
Well, you would get the same result from a naive translation of this code into Python, even though what is going on is conceptually different: from sys import stdin x = int(stdin.readline()) y = x + 3 print y x = int(stdin.readline()) print y You get the same result even though Python mutates a single variable x, whereas in Haskell you have two variables named x, the second of which shadows the first at statement two. So this isn't a very good example. It seems you intended to compare and contrast these two examples, first in Python: from sys import stdin x = int(stdin.readline()) y = lambda: x + 3 print y() x = int(stdin.readline()) print y() And the naive translation back into Haskell: readInt :: String -&gt; Int readInt = read main = do x &lt;- fmap readInt getLine --stmt 1 let y () = x + 3 print (y ()) x &lt;- fmap readInt getLine --stmt 2 print (y ()) 
You can simulate the desired behaviour with IORefs (or STRefs or the state monad) like this: import Data.IORef readInt :: IO Int readInt = fmap read getLine main = do xref &lt;- newIORef 0 let y = fmap (+ 3) (readIORef xref) writeIORef xref =&lt;&lt; readInt print =&lt;&lt; y writeIORef xref =&lt;&lt; readInt print =&lt;&lt; y Though, I wouldn't consider this good practice. 
Shadowing `map` sounds like a really bad idea, since it's incredibly common. `exp`, `ord`, sure, but I don't think you should *ever* shadow something like `map`.
I don't know almost any Category Theory.. I know what Kleisli arrows and Kleisli composition is purely from Haskell'ing...
If you use qualified/closed-list imports, then the size of the program doesn't affect the size of your typical namespace at all.
That's not true, it's just that the x in "let y = x+3" and the x in "print (x+3)" refer to different variables which both happen to be called x. If you printed y again at the end you would still get the same answer as when you first printed it.
Aha, I see. So you're keeping a reference to the finalizer, instead of the root address, and that's sufficient to keep it alive. I've messed around with `Ptr`s before, but never with `ForeignPtr`s except in trivial ways. One of the things I like about the ByteString-style representation is that since you know the offset (as well as the length) you can use heuristics for when it's better to copy the content rather than just share the old version, since the heuristic tells you that the leading offset is garbage and shouldn't be kept alive because you need a few bytes from the end of the buffer. Or you can use heuristics to maximize the amount of the offset being shared, as Data.Trie does. Is there any way to be explicit about doing these things with the Vector representation?
&gt; Is there any way to be explicit about doing these things with the Vector representation? No I don't think there is. Good point BTW! We should keep it in mind.
Thank you, my mistake.
[Control.Concurrent.Future](http://hackage.haskell.org/packages/archive/future/2.0.0/doc/html/Control-Concurrent-Future.html) -- I'm not qualified to discuss the differences. :P
http://hackage.haskell.org/packages/archive/orc/1.2.1.1/doc/html/Orc-Monad.html#v:eagerly
http://hackage.haskell.org/packages/archive/spawn/latest/doc/html/Control-Concurrent-Spawn.html
&gt; instance Fork IO where &gt; fork x = do &gt; r &lt;- newEmptyTMVarIO &gt; forkIO $ x &gt;&gt;= atomically . putTMVar r &gt; return $ atomically $ takeTMVar r Note this code is unsafe: if `x` throws an exception or an exception is asynchronously thrown to it, executing the `takeTMVar` will deadlock! And what happens when a user tries to execute the returned computation multiple times. Again deadlock! Also using `MVars` instead of `TVars` is easier (no `atomically`) and probably more efficient (haven't benchmarked it though). The following version fixes these issues: instance Fork IO where fork x = do mv &lt;- newEmptyMVar _ &lt;- mask $ \restore -&gt; forkIO $ try (restore x) &gt;&gt;= putMVar mv return $ readMVar mv &gt;&gt;= either (\e -&gt; throwIO (e :: SomeException)) return This is how I implemented it in my [threads](http://hackage.haskell.org/package/threads) package. Note that the [monad-parallel](http://hackage.haskell.org/package/monad-parallel) package exports a similar class. It suffered from the same deadlock issues but I send in a patch to fix those.
Exactly what I thought. You are on a good way, OP. Futures are the future!
You could use `finally` instead of `try`, which will save the hassle with `Either`. Your solution rethrows the excetion in a thread that doesn't even have anything to do with the exception itself. OP, if you want to step up your game a bit and maybe use advanced stuff like reader/write or state monads in your concurrent application without the need for T/MVars, take a look at my [mstate-package](http://hackage.haskell.org/package/mstate). It offers a simple and secure way to fork new threads in stateful applications, makes it possible to catch exceptions outside of pure IO functions (via the [monad-peel-package](http://hackage.haskell.org/package/monad-peel)) and works well together with all kind of different monad transformers (see all those [instances](http://hackage.haskell.org/packages/archive/mstate/0.2.3/doc/html/Control-Concurrent-MState.html#t:MState)). It offers threadsafe modification of the state (using STM) and allows for easy thread control (killing and waiting for threads). Give it a try! :)
&gt;OP, if you want to step up your game a bit and maybe use advanced stuff like reader/write or state monads in your concurrent application without the need for T/MVars Sounds like a step backwards to me.
Care to explain?
`State` has absolutely nothing to say about concurrency at all; to replace concurrent use of TVars/MVars with it barely makes any sense, let alone being an improvement. They are completely orthogonal, and shoe-horning a use of concurrent mutable references into `State` will inevitably be a step backwards, if it's even possible.
Did you even take a look at my library? Why should it be impossible to write an application that shares the same state over multiple threads? Every application with a database works that way, and the most common use of MVars/TVars is exactly for that purpose. I don't understand why those two techniques should be orthogonal to each other?
Thanks. This is exactly what I was looking for.
Well, the way I was handling monad transformers was to borrow the design from the paper ["A Language-Based Approach to Unifying Events and Threads"](http://www.cis.upenn.edu/~stevez/papers/LZ06b.pdf) and refactoring it to work across any monad. My current implementation looks something like this: data Thread f m = Done | Lift (m (Thread f m)) | LiftIO (IO (Thread f m)) | Branch (f (Thread f m)) | Exit done = cont $ \c -&gt; Done lift' x = cont $ \c -&gt; Lift $ liftM c x liftIO' x = cont $ \c -&gt; LiftIO $ liftM c x branch x = cont $ \c -&gt; Branch $ fmap c x exit = cont $ \c -&gt; Exit fork x = join $ branch [return (), x &gt;&gt; done] run x = do q &lt;- liftIO $ newTChanIO enqueue q $ runCont x $ \_ -&gt; Done loop q where loop q = do t &lt;- liftIO $ atomically $ readTChan q case t of Exit -&gt; return () Done -&gt; loop q Branch ft -&gt; mapM_ (enqueue q) ft &gt;&gt; loop q Lift mt -&gt; (mt &gt;&gt;= enqueue q) &gt;&gt; loop q LiftIO it -&gt; (liftIO $ forkIO $ it &gt;&gt;= enqueue q) &gt;&gt; loop q enqueue q = liftIO . atomically . writeTChan q There are some obvious problems with it (like ensuring that all threads are closed once you exit). My original goal in adapting the design was that I wanted to thread state/reader/writer information between threads, but then I realized that STM would do that better. What motivated my post was trying to figure out if there was a way to generalize STM to any instance of MonadIO.
Good catch. I didn't think about handling the case of exceptions. Thanks for the great tips and I'll also check out your threads package.
All these examples are great. Now I have so much note comparing to do.
Thanks. I can't believe I didn't notice that before.
&gt; My original goal in adapting the design was that I wanted to thread state/reader/writer information between threads That was exactly my intention when I wrote that mstate library. You might enjoy it. :)
You should also check out [monad-par](http://hackage.haskell.org/packages/archive/monad-par/0.1.0.1/doc/html/Control-Monad-Par.html), which adds some flexibility and performance to what you've started here. Specifically, it uses first-class IVars (a write-once, read-many structure similar to your MVars) to describe the information flow of your parallel application. For performance, it has a pluggable scheduler that implements work stealing a la Cilk. [The paper](http://community.haskell.org/~simonmar/papers/monad-par.pdf) has more details.
Well, the solution I came up with works for any monad and special-cases the IO monad so that it can be done in a non-blocking way. It's also based off the very elegant design from the paper I referenced that lets you write your own scheduler (the "run" function in my example's case). However, the first thing I noticed is that there was no good way to implement concurrency primitives on top of the state monad without implementing poll-waiting. That's why I prefer the STM approach to shared state. What I'm interested in is figuring out is if there is a way to implement STM purely in the language without relying on runtime concurrency primitives.
Your do-notation is sugar for: main = fmap readInt getLine &gt;&gt;= (\x -&gt; let y = x + 3 in print y &gt;&gt;= (\x -&gt; fmap readInt getLine &gt;&gt; print y ) ) I think it's a lot more obvious what is going on when you look at it like that. The first x is in scope for y's definition but the second isn't. 
Have you seen the Par monad ([slides](http://community.haskell.org/~simonmar/slides/CUFP.pdf), [paper](http://research.microsoft.com/en-us/um/people/simonpj/papers/parallel/monad-par.pdf), [hackage](http://hackage.haskell.org/package/monad-par))?
Sounds interesting. I'll give it a look.
This is fascinating. Thanks.
&gt; You could use finally instead of try, which will save the hassle with Either. I don't see how to do that correctly. For example, in your version of `fork` what would be the value of `x` in: main = do m &lt;- fork $ throwIO DivideByZero x &lt;- m print x And please don't say `undefined` ! &gt; Your solution rethrows the excetion in a thread that doesn't even have anything to do with the exception itself. I think that's the only sensible thing to do. What's the alternative?
Do note that `monad-par` cannot be used for speeding up *IO computations* (which I believe the OP needs) it is for speeding up *pure* computations using parallel processors. 
Very true; the core determinism guarantee would go out the window with IO, so monad-par doesn't attempt to address it.
Isn't the Monad/Functor calamity enough motivation?
Submit it!
how about the mess with number classes and containers! 
I get the impression he's looking for use cases that don't involve changes to the standard libraries.
I don't see how the alternatives they listed are alternatives to Prelude.head.
`listToMaybe` gives you either `Just head` or `Nothing`. `take 1` gives you either `[head]` or `[]`. `foldr` lets you process a list in an essentially arbitrary manner. `List` is a bad name and implementation for the common `data NonEmpty = NonEmpty a [a]` type.
&gt;This function is used for catching and dealing with exceptions from a block of code. Let's say that instead of a standard IO action, you have a ReaderT IO action: myAction :: ReaderT ReaderValue IO SomeResult &gt;There's no way to pass this directly to the try function, and therefore there appears to be no way to catch the exceptions from myAction. This is where the monad-control packages comes into play. It essentially inverts myAction, turning it into something like `myActionInverted :: IO (ReaderValue -&gt; SomeResult)`. I don't know where the lie is here, but it's very confusing, since you definitely can't transform the former action into the latter.