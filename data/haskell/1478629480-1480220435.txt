Ah, the old GHCi+GLUT issue still exist? I meant that GLUT compiled is mostly reliable. GLFW is probably also worth a try. It wasn't suitable to all my purposes when I last looked into it, but maybe it's better now, and it definitely should be more modern (than GLUT, which is *really* old).
Yup. Going to spend my vacation / part of the next month or two getting this all in shape. Still important to get this all in good shape. But it all takes time and effort and such. :) It's my fault things have lain fallow. 
One important thing that it allows for is the separation of commands from queries so that a sequence of commands (punctuated by a query, if there is one) can be packaged together into one request automatically. The `remote-monad` package implements most of the functionality, so you mostly just need to define a type that represents your commands, a type that represents your queries and an interpretation of your EDSL and the package takes care of the bundling of commands, etc. There's an example [on the package page](http://hackage.haskell.org/package/remote-monad) of what that looks like.
This is great! Looks like exactly what I need. I will have to read up on those extensions to fully understand what's going on though. But on the surface it looks very similar to the semantics I imagined.
I'm experiencing the same issue with `double-conversion` on OSX. But it looks like I reported the bug in the wrong place: https://github.com/NixOS/nixpkgs/issues/19497
thanks! I think I am using ghc 7.10.3 running through stack
I've had similar issues with GLFW. It might have worked with GHCi but there were definitely problems with the repl exiting when you close the window and things like that. Is there a particular reason not to recommend SDL?
SDL does a lot more than just windowing and input (ex: threading, sound, file i/o), that would be the only reason to recommend against it.
I was considering your library, but value binding seems kind of messy. What I like about mustache is that I can pass it some kind of json like adhoc object. I can also just use generics and derive JSON instance of my data types and insert them easily into this adhoc object which I pass to template. Something like this : data User = User {userName :: Text, userPassword :: Text} deriving Generic instance ToJSON User result :: Template -&gt; Text result template = render template $ object [ "someRandomVar" ~&gt; "Hello", "myUserObject" ~= user]
See also [this intro to recursion](https://www.reddit.com/r/haskell/comments/5bvoth/intro_to_recursion/). **Edit:** To get a bit more serious, the author *defines* tail recursion and motivates why tail recursion is so good, but doesn't show how to write tail-recursive loops. I like to talk about "itero-recursive algorithms," iterative algorithms converted into recursive ones, as a way to give you an idea of how these are written. The basic idea is "I am going to teach you how to write a for loop in Haskell. I know that you used these a lot in your other languages, so I'm going to teach you how to write one here." So start by writing your favorite for-loop in your favorite imperative language: function fib(n) { var curr = 0, next = 1; for (var i = 0; i &lt; n; i++) { [curr, next] = [next, curr + next]; } return curr; } If the algorithm does some stuff before the loop you may need to split that initialization out into its own function; you usually have a little more freedom (two locations' worth) to postprocess the params after the loop. So now I want to say, create a function in Haskell which takes all of the new variables as arguments: fib n = undefined where loop curr next i = undefined and then just like the above loop, we're going to use the first space to initialize the variables and inside we'll perform the logic first: fib n = loop 0 1 0 where loop curr next i | i &gt;= n = undefined | otherwise = undefined Note that if we wanted to postprocess we could say `where postprocess value = ...` and then write `postprocess $ loop 0 1 0` up top. Also note that one of these branches corresponds to "what do I do if the loop is over?" and in the above case we see: return `curr`. Otherwise? We loop again until the loop is over, but now `i` needs to get `i+1`, `curr` needs to get `next`, and `next` needs to get `curr + next`. *But we can just do that, now that these are arguments.* fib n = loop 0 1 0 where loop curr next i | i &gt;= n = curr | otherwise = loop next (curr + next) (i + 1) A slightly more advanced example might be a breadth-first search where the iterative algorithm is: function bfs(tree, value) { var queue = [{cell: tree, path: []}], node = null; while (queue.length &gt; 0) { node = queue.shift(); if (node.cell.value == value) { return node; } else { for (let c = 0; c &lt; node.cell.children.length; c++) { queue.push({cell: node.cell.children[c], path: node.path.concat([c])}); } } } return null; } The only thing I like less about this example is that it has a lot of incidental complexity with the subloop and the data structure and the queue. You have to teach someone first, "hey, here's a simple FIFO queue:" data FIFO x = FIFO [x] [x] push :: FIFO x -&gt; x -&gt; FIFO x push (FIFO xs ys) x = FIFO xs (x : ys) shift :: FIFO x -&gt; Maybe (x, FIFO x) shift (FIFO (x:xs) ys) = Just (x, FIFO xs ys) shift (FIFO [] ys) = case reverse ys of [] -&gt; Nothing (first : rest) -&gt; Just (first, FIFO rest []) And combined with a `data RoseTree x = RoseTree {value :: x, children :: [RoseTree x]}` and a `data BFSResult x = NotFound | Found {cell :: RoseTree x, path :: [Int]}` you can begin to attack that algorithm itero-recursively; it's not substantially harder.
arg, ok i have changed it to this but still nothing happens when i type `keyboardm :: KeyboardMouseCallback` `keyboardm key Down m pos = print "ji"` `keyboardm key _ m pos = return ()` `main :: IO ()` `main = do` `(_progName, _args) &lt;- getArgsAndInitialize` ` _window &lt;- createWindow "Hello"` ` keyboardMouseCallback $= Just (keyboardm)` ` displayCallback $= display ` ` reshapeCallback $= Just reshape ` ` windowSize $= Size 1000 700 `s ` mainLoop ` i do see this warning Warning: build failed, but optimistically launching GHCi anyway The following GHC options are incompatible with GHCi and have not been passed to it: threaded maybe thats what is wrong?? but why would it work in linux?
It does make some sense in this specific use case. However, I still believe the syntactical convenience is not worth living with the loss of readability and increased potential for typos not being caught by the type checker. P.S.: One possibility your original, non-realistic example hints at is using these overloaded bindings instead of partial record accessors in sum types. It is not all that helpful, as you still need to pattern match the appropriate constructor, but it is a neat trick.
FWIW, recent versions have a few helper functions that might ease the pain a little, and I am not at all opposed to spending some time on providing an easier interface. The machinery for the whole marshalling is in place already: there is `dict :: [Pair] -&gt; GVal m`, the GVal equivalent of Aeon's `object` function, there's `(~&gt;)` to construct pairs (similar to Aeson's `(.=)`), there's a `ToGVal` instance for `Value` (which means you can trivially marshal anything that has a `ToJSON` instance to Ginger using `toGVal . toJSON`); all that would be needed is a set of helper functions similar to the `makeContext` family that take a plain value instead of a lookup function. If you want, I can probably come up with something in no time. As far as the output side of things is concerned, `runGinger` pretty much boils down to `Context -&gt; Template -&gt; Text` or `Context -&gt; Template -&gt; Html` for concrete use cases, even though the type signature looks a bit more complex - that's just the result of making it generic over the output type (including output type specific encoding functions). And the existence of `runGingerT` is strictly only so that Ginger templates can run over arbitrary monads, accepting actions in that monad through their context - you don't normally need this, but there are a few use cases for this that I've encountered in the wild.
I didn't fully check, but does ginger caches and re interprets template files when changes are made?
[removed]
you were right!! that was the issue -- 30 mins of figuring out how to actually build via stack later...its working. Thanks so much for all your help!!
It does not; I decided to keep this functionality outside its scope, for various reasons. It is possible, and not very difficult, to add caching yourself, at various levels; the template loader mechanism, for example, is pluggable (see http://hackage.haskell.org/package/ginger-0.3.5.0/docs/Text-Ginger-Parse.html#t:IncludeResolver), it's up to you to decide how and when you want to cache/preload. I can totally imagine providing such functionality as an add-on package though.
Thanks for filling in this detail! I considered including this, but this talk is already an hour long and that's *after* editing.
I would suggest you "zip" the two together so the things are actually available together and use the above approach. It's a more functional and pure way of doing it. Alternatively, you could do something like this: &lt;forIndices&gt; &lt;li&gt;&lt;dataset1 forId="${ind}"/&gt; : &lt;dataset2 forId="${ind}"/&gt;&lt;/li&gt; &lt;/forIndices&gt;
I see a lot of debate and disagreement. If only we were stuck with a language that had only one option, then things would be so much more peaceful...
My issue with `ExceptT a IO` is that you can't write a sensible instance of `MonadMask` for it. That's only true because `ExceptT a IO` is general, i.e. you have no idea what kinds of exceptions are being captured by `a`. Not being able to write a sensible instance for `MonadMask` really just means that there's no sensible, *generic* way to write a `bracket` function that can apply to all `ExceptT a IO` stacks equally. This makes resource management a huge pain in this stack.
I can't predict all possible usecases. Because of that, i want to allow users to manipulate data in various ways. Also, I don't see what's pure about that approach.
Problem is that this isn't THE case, it's just one of many.
What is the rationale behind the name `dimap`? Is it because, if you turn the first part of `bimap` (i.e. the `b`) , you get `dimap`?
Well, in the OO world they have classes and instances. In Haskell we have first class functions and type level classes. What mocking is trying to do is provide a "fake" API provider, the API normally being the public methods of the class or interface of the dependency (i.e. your B/C). Concretely, in C# I might have: class A { public A(IBProvider bp, ICProvider cp) { _bp = bp; _cp = cp; } public void A() { if(WantB) bp.B(); else cp.C(); } ... In Haskell this might be done like: afun :: Bool -&gt; (() -&gt; IO ()) -&gt; (() -&gt; IO ()) -&gt; IO () afun wantB bfun cfun = if wantB then bfun () else cfun () so in your tests you can just pass a suitable `bfun` and `cfun` to `afun`. Using type classes, you can do something closer to what you do in OO land. EDIT: Oh sorry, I saw you wanted to call B and C in any case. Also, if what you are looking for is the bit where you can do this: _mock.Record(() =&gt; { _mockB.B(); _mockC.C(); }); _mockA.A(); _mock.VerifyAll(); I've never personally tried to do anything like this but others have mentioned `test-fixture` which seems to do this mocking and recording. I didn't read enough to see if it also supports the functional variant I showed or only handles type class based mocking.
So you're saying it's only the people who have the power to choose which languages are used in commercial code who fail to recognise the severe limitations of these indexes. Well, that's OK then. ;-)
Memoization is referentially transparent as well.
Ah, not constant time, I misspoke - just clarifying that the author was referring to reflection without remorse and not something else, since he didn't actually mention the technique by name. Sounds like he was?
Yeah, I guess my use of the words "functional and pure" was not the right way to say it. "Idiomatic" probably would have been better. Typically in Haskell it's more common to `fmap` over a container and avoid dealing with indexes altogether. Something like this: map (\(item01, item02) -&gt; makeListItem item01 item02) $ zip dataset01 dataset02 as opposed to this: map (\item01 -&gt; makeListItem item01 (dataset02 !! itemInd item01)) dataset01 ...because the latter case has more ways it can fail. But as my two examples demonstrate, Heist can support both models.
This usage of the prefix "di-" is [well-established in category theory](https://ncatlab.org/nlab/show/dinatural+transformation). I don't know its historical origins, though. In any case, your suggestion is a very nice mnemonic :)
Any mutation that happens only inside the function, but externally the function is still a pure function like ST and IORef types. import Control.Monad.ST import Data.STRef import Control.Monad &gt; :{ product xs = runST $ do acc &lt;- newSTRef 1 forM_ xs $ \x -&gt; modifySTRef acc (*x) readSTRef acc :} product :: (Foldable t, Num a) =&gt; t a -&gt; a &gt; &gt; &gt; product [1, 2, 5, 6] 60 it :: Num a =&gt; a &gt; 
Yup, thinkining about it from beginner perspective: stumbling upon a post that's supposed to show good practices for the hard (not only in Haskell) problem of exception handling. Then look into the comment section and see that nothing is clear...
FWIW, this is actually a limitation of the `exceptions` package itself in not putting a function like `bracket` into the `MonadMask` typeclass. `lifted-base` can, for instance, provide `bracket` for `ExceptT`. Yes, I am well aware I'm providing an argument in favor of an approach I call out as a bad practice :)
Well `exceptions` *does* provide a `bracket` function that's polymorphic on `MonadMask`. But the problem is that I don't know how to write a sensible instance for `MonadMask` for `ExceptT a IO` *in general*. From what I can tell, `lifted-base`'s treatment of `ExceptT a IO` is a common source of surprise to people because it takes a particular stance on how the exception handling should be ordered...which is not intuitive for all kinds of exceptions.
Alternately, you can skip the `FIFO` data structure and discuss corecursion: bfs :: Eq a =&gt; RoseTree a -&gt; a -&gt; Maybe (RoseTree a, [Int]) bfs tree a = (\(node, indexes) -&gt; (node, reverse indexes)) &lt;$&gt; found where found = List.find (\(cell, indexes) -&gt; value cell == a) queue queue = (tree, []) : walk 1 queue walk 0 _ = [] walk n ((RoseTree _ nodes, indexes) : queue) = annotate indexes nodes ++ walk (n - 1 + length nodes) queue annotate indexes nodes = zipWith (\index node -&gt; (node, index:indexes)) [0..] nodes 
Is this "vertical"/"horizontal" nomenclature well-established? I have been thinking of stream processing lately, about the facts that it's possible to compose streams across completely different axes, and now I'm worried that the directions in which I've been drawing things might be clashing with some existing nomenclature. First, if we have a stream which produces 10 elements, it's possible to prepend it to a stream which produces 5 elements. If the stream terminates with a value, then this type of composition can be monadic, so I like to draw this vertically using a do block: we assign the result of the 10 element stream via `&lt;-`, and then we run the 5 element stream. The other way in which we can compose streams is by transforming the entire stream into a different stream, for example using `map`, `filter`, or using a more complicated algorithm which pulls elements from its input stream and pushes elements onto its output stream. Since the vertical direction is already taken by monadic composition, I like to draw "transformative composition" horizontally. Now that I'm thinking about it further, both directions involve taking some input and producing some output, it's just that vertically it's a distinguished return value which is received and returned, while horizontally it is a whole stream of elements which are received and returned. So I don't know which axis, if any, should be the one which "connects inputs to outputs" and which one should correspond to a natural transformation...
My Category Theory is a bit lacking, so, bear with me. From what I understand, a Functor preserves structure between two categories, or sub-categories in this case; my types. So vertical composition then would be the natural transformation that takes a `Username -&gt; FaveNum`, and turns it into a `Username -&gt; Password -&gt; FaveNum`. AFAICT that has structure preservation between two functions, which could be seen as functors. Since it helps me to have an analogy to hang new knowledge off of, Is there some sense in which my toy example *could* demonstrate vertical composition?
Maybe profunctor should be difunctor, or dimap be promap.
Is this an error? &gt; meaning that Callback is covariant on a I can understand that CallbackRunner is covariant on a, but Callback is contravariant on a as stated here: &gt; Suppose we're just trying to deal with a -&gt; IO (). As we've established many times above: this function is contravariant on a
If you're familiar with any languages which make a distinction between "value types" and "reference types" (maybe also known as "object types"): Mutation of value types in local bindings (arguments or local variables) is still pure. You don't even need linear types for this. (You *might* need strict evaluation.)
Functions are usually considered morphisms, not functors. Similarly, types are usually objects, not categories. The reason is that categories have an internal structure, a bunch of objects and some morphisms between them, which types don't have. Type constructors, on the other hand, can be considered functors from the category of types to this same category of types. The objects of this category are types and the morphisms are functions between them. So if your usernames and favorite numbers were parameterized by a type representing, say, the data center in which those users and numbers were stored: favoritenum :: Username a -&gt; FaveNum a We need a third type constructor, so let's say that each number is represented by an image which is stored in the same data center as the number: image :: FaveNum a -&gt; Image a Now if you want each user to use the image of their favorite number as their profile picture, you can compose the two functions: profile :: Username a -&gt; Image a profile = image . favoritenum And that composition would be vertical composition. According to my understanding of the definition from the other comments, at least! It's the first time I hear the term.
We need to clear up a few things here. &gt; Functor preserves structure between two categories, between two categories, or sub-categories in this case; my types. Types are not categories. [**Hask**](https://wiki.haskell.org/Hask) is the category with Haskell types as objects and Haskell functions as morphisms. If you want a subcategory of **Hask** with a single object (e.g. one with only `FaveNum`) you also have to restrict the morphisms accordingly (e.g. to `FaveNum -&gt; FaveNum` functions only). &gt;that has structure preservation between two functions, which could be seen as functors. If types are not categories, a function is not a functor (at least not in the sense you meant here). Functors are mappings between categories that preserve the *category structure* (the functor laws express this preservation). For instance, an instance of `Functor` class is a functor from **Hask** to **Hask**, in which the type constructor maps objects to objects and `fmap` maps morphisms to morphisms. That said, we can have a second look at the types of your functions: favoriteNum :: Username -&gt; FaveNum authedFavoriteNum :: Username -&gt; Password -&gt; Maybe FaveNum Both of their types amount to applying (the object mapping of) a `Functor` to `FaveNum`. That becomes obvious if we rewrite the types as... (-&gt;) Username FaveNum (-&gt;) Username ((-&gt;) Password (Maybe FaveNum)) ... and then, tidying things up by using `C` as an operator for function compostition: ((-&gt;) Username) FaveNum ((-&gt;) Username `C` (-&gt;) Password `C` Maybe) FaveNum (This `C` is the same as `HC` in echatav's comment, and does the same that `Compose` from `Data.Functor.Compose` does. Using `Compose` in actual code, of course, requires some newtype wrapping boilerplate. Such boilerplate doesn't affect at all what we are doing here, so I will just pretend it doesn't exist.) Since for what we are about to do it doesn't matter whether we are working with **Hask** or with the subcategory with only `FaveNum -&gt; FaveNum` as morphisms, I will generalise the types to: forall a. ((-&gt;) Username) a forall a. ((-&gt;) Username `C` (-&gt;) Password `C` Maybe) a &gt;So vertical composition then would be the natural transformation that takes a `Username -&gt; FaveNum`, and turns it into a `Username -&gt; Password -&gt; FaveNum` A natural transformation is a mapping between functors that preserves the functor structure. In the particular case of a natural transformation between two `Functor`s `F` and `G`, that amounts to a function of type `forall a. F a -&gt; G a` (cf. echatav's comment). The structure preservation demand for some natural transformation `r`, then, becomes `r . fmap f = fmap f . r`, which thanks to parametricity is guaranteed to hold regardless of what `r` is. Now, if you have a function... requireAuth :: (Username -&gt; a) -&gt; Username -&gt; Password -&gt; Maybe a ... or, equivalently... requireAuth :: ((-&gt;) Username) a -&gt; ((-&gt;) Username `C` (-&gt;) Password `C` Maybe) a ... it is a natural transformation from the `Functor` `(-&gt;) Username` to the `Functor` ``(-&gt;) Username `C` (-&gt;) Password `C` Maybe``. The naturality condition (which, as I mentioned before, is guaranteed to hold thanks to parametricity) boils down to: requireAuth (g . f) user pwd = fmap g (requireAuth f user pwd) That is, it doesn't matter whether you compose `g` with `f` and then transform it with `requireAuth` or transform `f` alone and then map `g` over the `Maybe`-result the transformed function ultimately gives out. It is, I would hazard, a rather boring result. Given the extremely wide range of polymorphic functions that can be seen as natural transformations in this manner (a range that becomes even wider if you express a bare `a` as `Identity a` and a non-parametric `Foo` as `Const Foo a`), it probably would be asking too much to expect an exciting result. Note that over the last few paragraphs I didn't mention "vertical composition" anywhere. Vertical composition, in its category theory meaning, is a way of composing natural transformations, and in the example above we were only considering a single natural transformation. The tl;dr, then, is that you can find a natural transformation in your example, if you twist it hard enough. My gut feeling is that this fact has little practical relevance, though I would be happy to be proved wrong.
You might like my monad ste package which has a nontrivial prim monad that can only be a monad throw but can't be a monad catch or mask
Excellent talk - thanks!
I think the approaches are orthogonal. At work I use type classes to define operations ("interfaces"), but the main implementation is based on `Haxl`, which seems to be familiar to `remote-monad` in a sense it provides lots of operational goodies (bundling, caching). *EDIT* to elaborate: In `haxl`, it's possible to arrange things so the same "requests" are executed differently, by having configurable enough data sources. OTOH for testing/mocking purposes it's might be easier to switch out of haxl completely.
but I've added the empty list case, in the SO answer. perms [] = [ [] ] yes, it won't do anything until it traverses all of `as`.
Could you substantiate your claim that a fixed evaluation strategy makes a language less pure ? From a definitional point of view, I am not sure I buy into it, as long as it does not break referential transparency.
Sure, but I wouldn't be sure I'd call such thing a templating system anymore. It would then be specific to one output language and would be much more strict (good!) than what we usually get. For example, the system would need to be smart enough to interpolate values differently for `&lt;div style='%s'&gt;..&lt;/div&gt;`, `&lt;img src='%s'/&gt;` and `&lt;span&gt;Hello %s&lt;/span&gt;`. I really can't see how you'd enable this without turning the template into an AST first.
Evaluation strategy can change whether a function converges or diverges. In a fully pure language, functions are functions in the mathematical sense, not just in the sense of referential transparency. Transformations that preserve the denotational semantics of a mathematical function, such as swapping the order of the arguments, preserve the semantics of functions in the programming language as well. That may sound theoretical, but as an application programmer I can assure you that the practical benefits are huge. It makes the meanings of programs precise and extremely clear to the reader. And that makes programs easier to debug, maintain, and safely refactor.
&gt; Evaluation strategy can change whether a function converges or diverges. Yes, I am fully aware of that. &gt; In a fully pure language, functions are functions in the mathematical sense, not just in the sense of referential transparency. I think you conflate purity and totalness. &gt; Transformations that preserve the denotational semantics of a mathematical function, such as swapping the order of the arguments, preserve the semantics of functions in the programming language as well. Your example transformation does not affect the semantics of the language, be it strict or pure. Lazy evaluation, in particular, enables a style of compositional programming that allows you to swap things around and compose functions on infinite structures without breaking anything nor causing non-termination. In other words, I still fail to see your point.
&gt; Invariance will usually (always?) occur when a type parameter is used multiple times in the data structure Not always! If all uses of the type variable have the same variance then you don't get invariance.
Well, I guess we agree on the matter. I was talking in context of mustache which really does not cut it. I can certainly admit that shakespeare does what you say, but I had it firmly pinned in my mind as special html generation dsl :) 
I see now that the phrasing was ambiguous, but what I meant was that in all cases where invariance occurs, a type parameter is used multiple time in the data structure. Is that assertion correct?
Lets hear the example. I'm curious and comfortable with templates.
So - can we expect the flaw to be fixed while keeping the api?
There are now fairly thorough TensorFlow bindings for Haskell
One thing I'd like to see more widely talked about is the trick of avoiding invariance by adding different type parameters for the co- and contra-variant uses. Edward Kmett mentions it briefly [here](https://www.schoolofhaskell.com/user/edwardk/phoas#p-is-for-profunctor).
While the wrapper itself will never be allocated, the underlying value will! Given the name `AnyVal`, I assume that they're meant to be the equivalent of Java's primitive types `int`, `double`, etc., which are always allocated on the stack. In Haskell, `Int`, `Double`, etc. are allocated on the heap unless the compiler can optimize those allocations away, so `Int` and a newtype'd `Int` are just as likely to be allocated on the heap as on the stack.
`newtype` never allocates. In fact, you can just use `Data.Coerce.coerce` to coerce your value to a newtype wrapper for it: import Data.Coerce (coerce) newtype NotInt = NotInt Int deriving (Show) main = print ((coerce (1 :: Int)) :: NotInt) Clearly that doesn't do any allocations and it's also safe. (This is what a newtype wrapper is in Core -- it's just a coercion)
Can't find the reference at all, but IIRC there are cases where using a `newtype` uses more memory than not using it, and that's why there is (was ?) a lot of coerce in `lens` ...
Another good point. Now that there are two exceptions to the rule, I'm worried there's another out there too, and I should probably just go with the "usually" landing with perhaps a link to this very thread :)
Newtype wrappers never allocate, but if you pass a newtype wrapper to a higher-order function that allocates you may end up doing redundant allocations. Example: import Data.Coerce (coerce) newtype NotInt = NotInt Int deriving (Show) intLst :: [Int] intLst = [ 1 .. 10 ] {-# NOINLINE map' #-} map' _ [] = [] map' f (x : xs) = f x : map' f xs main = do print (map' NotInt intLst) print (coerce intLst :: [NotInt]) First line in `main` allocates a new list, second line doesn't. I'm not sure which part of your link is about newtype-related allocations, but I suspect it's a case like this. 
To be fair, that is `(.)` allocating, not a newtype constructor. I agree it is a subtle failure mode, though, since `(.)` would probably not be used if the newtype constructor wasn't needed.
&gt; I'm not sure which part of your link is about newtype-related allocations, but I suspect it's a case like this. It's the third paragraph, and it seems to be about eta-expansions wrappers. Unfortunately, I have no idea what those are :)
Yeah, this is really useful.
Haskell more explicitly has unboxed primitive types that represent values held in registers and not wrapped in a thunk of any kind. They can only be used in certain contexts where it is clear no thunks are involved. Newtypes exist purely in the mind of the compiler and never result in the allocation of a wrapper object like a java.lang.Integer can. 
It might be worth also pointing out that calls to `coerce` are always zero-cost and they can be used "over" other type constructors containing `newtype`s: notAnIntFn :: [NotInt] -&gt; [NotInt] ... g :: [Int] -&gt; String ... xs :: [Int] ... result :: String result = g ((coerce notAnIntFn) xs) (Some extra parens there for clarity.) This coerces over two "levels" of type constructors (the function type and the list type) to get to the `newtype` at no cost while if you instead did this by `map`ing conversion functions over the values you would have the cost of the list traversals.
Am I the only one that clicked the link thinking they wanted to code a backend for an ML compiler (e.g., SML/NJ) in haskell? Glad I was wrong LOL
Lens uses a bunch of coerces because internally, once the compiler gets done with a newtype constructor Foo, Foo . f ~~ id . f ~~ \x -&gt; f x rather than f. It is to avoid this eta expansion that I `coerce f` into the type I want rather than post-compose the newtype constructor. Foo #. f ~~ f This much is a pure win. As I understand it, the other situation where newtypes can affect things is in arity analysis. GHC is good at batch applying all the arguments to a function. But if that is a function returning a newtype around a function (e.g. a Kleisli State monad action) then it may instead return a closure that has to then be called again, yielding a small hitch in efficiency. That part is a lot more temperamental as it relies on when things happen, what gets inlined, etc.
Whole point here is that I can't predict all possible scenarios, and that I don't want to have to modify my view model every time new case arises if all required data is already in the context.
Seems nice. Are sessions supported?
`AnyVal` is Scala is the parent type of all the "primitives" (int, double, etc). So yeah, they are meant to be represented as `int` and `double` (when they can be. If boxing is needed, they will be represented as their `Integer` and `Double` counterparts, which go on the heap). However, in Scala (because Scala loves overloading syntaxes) it also means `newtype`. As in: `case class Wrapper(x: SomeThing) extends AnyVal` The idea is that, barring the list of exceptions, `Wrapper` will only exists at compile-time. 
It would be nice to have a dumb streaming method without streaming library dependencias, like wreq's "foldGet". Sometimes that's enough.
The README says that "proposing entirely new issues" is only open to CLC members. Is "proposing new issues" the same as the **proposal process** described further down in the README? Is "the process is open for everyone to contribute" primarily an invitation for non-CLC members to comment on pre-existing pull request comment threads? Just trying to clarify the guidelines. Thanks!
One of the authors recently gave this [https://www.youtube.com/watch?v=PtXwF6w0V88] talk. There are other Simons Institute talks which he mentions which talk about the theory behind it.
Get it now. The state it is in now (or for that matter a year ago) is still better than any other learning resource for Haskell. If you're new to Haskell, approach it with an open mind and you will get a lot from it. To put a value on it - it will accelerate ~ 2-3 years of trial and error FP maturation into a month or two, so the cost is worth your time.
Haskell is considered by many to be a fundamental (if not iconic) functional programming language. I would go so far as to consider it the C of functional programming languages. Other languages, like OCaml, F#, etc all heavily have been influenced and have influenced Haskell. The biggest thing you learn with Haskell, to me, is how to think functionally. You gain this with Haskell very easily since it is fully immutable and fully lazy (when possible) so you have to think very strongly in the functional realm. The way of thinking and reasoning about languages should be what translates over into the fp-family (and your general way of programming) more so than any of Haskell's syntax or particular design choices.
Learn you a haskell is a bit of a curse in sheep's clothing. It succeeds in making Haskell appear accessible, and giving the illusion of learning ("I typed it and stuff prints out"), but the monkey-see-monkey-do pedagogical style is deceiving and when you sit down to actually create something you get stuck because you don't know how to combine the concepts to solve a problem. This is what haskellbook.com addresses - you start from the beginning and learn the foundations. Then you can solve problems. The exercises are great - just the right small bite-sized chunks but also engaging enough to learn from. I don't have experience with OCaml or F#, but generally yes. It's even transferrable to weakly typed functional languages (people are trying to get abstractions like monoids and functors into javascript even I'm told). It's a great foundation for FP concepts in any language. Warning, you will feel the pain of not being able to express concepts and invariants that you can in Haskell. It's been called "the wire" of programming languages in that (although not perfect) it tends to ruin other languages for you.
&gt; But eventually though, learn the ideas behind haskell semantics, don't just transplant 1-1 operations from other languages. Yes, yes. Agreed. That's why I'm asking! &gt; As has been bought up in other threads here, haskellbook.com is recommended for picking up the haskell way of reasoning. I have it!
Honestly, this is such a minute detail that I wouldn't worry about what some community thinks about it. If the expression is simple (`map f . filter p`), I'd use functions, if not, comprehensions, but go with whatever you feel is more readable.
The book accelerated my Haskell learning by months. The initial chapters might be a bit slow, but don't skip them. Three things stood out for me in the book. 1. Loads of exercises -&gt; Don't skip them. Conceptually, Learn You a Haskell is good. But exercises are missing. And at the end of it, all my Haskell knowledge was just smoke. I couldn't write a decent Haskell program to save my life. 2. Further reading after every chapter -&gt; I highly recommend going through them. They filled in a lot of gaps in my understanding of concepts, and in many cases enhanced them. 3. It teaches you FP, and not just Haskell syntax. These ideas can be inculcated into languages that you are programming in right now. The book is worth every cent, and I say this when the exchange rate for my currency is bad! I cannot speak for the authors, but I have seen them give discounts on the book for people who cannot afford it. IMO, it is definitely the right way to go. Edit: Formatting.
If you want a full (at least for types; I'm not going to think too hard about higher kinded parameters), precise (albeit a _little_ unsatisfying) definition, it's not _too_ hard: * A type constructor has _polarities_ (positive and/or negative) for each of its arguments. * The function arrow's first position is negative; its second position is positive: `- -&gt; +` * Reference types' position is both positive and negative: `IORef +/-` * Product and sum types' parameters are positive: `(+, +)`, `Either + +` * Data definitions have type parameters; these parameters _occur_ in the body of the definition * An occurrence of a parameter is said to be positive or negative based on the following: * If it occurs directly in a positive (resp. negative) position, it is positive (resp. negative). * If it occurs as a parameter to another type constructor, its polarity depends on the polarity of the type constructor's occurrence: * if the tycon occurs positively, the tyvar's polarity is the polarity of its position in the tycon * if the tycon occurs negatively, the tyvar's polarity is the opposite of its polarity in the tycon * Induct appropriately. * A parameter is said to be: * covariant if it occurs only positively * contravariant if it occurs only negatively * invariant if it occurs both positively and negatively * bivariant if it occurs neither positively or negatively
What's the definition of `merge`?
Ha yeah I realized that would be kind of necessary after the fact. &amp; now I'm no longer at work and can't access it anyway (and I'm too wiped to try to reconstruct it ATM). Sorry, the whole post is kind of premature :/
What about if you do: map NotInt [0..] Does that compile to the same code as: coerce [0..] :: [NotInt]
I like the current type more. It forces the user to handle the failure case, whereas your new version does not. Because that's one of my biggest issues with the (native) Promise API design: You never know if a Promise is going to fail or not, or if you already have a catch() somewhere in the chain and don't need to deal with it anymore.
Spock with Tensorflow bindings to start with. Is that viable ?
I don't have experience with other Haskell learning resources but I feel I'm progressing well using this book with the good balance of read/practice. I will say that as someone who has done FP before and has significant programming experience the first chapters are still interesting but no need to do all the intermediate exercises after the sub sections. I just did the ones at the end of the chapters as the others were just tedious.
It's not Rust that I find really bad, it's the TIOBE index.
What you're looking for is an open union. data Union :: [* -&gt; *] -&gt; * -&gt; * where UNow :: f a -&gt; Union (f ': fs) a UNext :: Union fs a -&gt; Union (f ': fs) a instance Functor (Union '[]) where fmap f _ = error "Not possible" instance (Functor f, Functor (Union fs)) =&gt; Functor (Union (f ': fs)) where fmap f (UNow a) = UNow (fmap f a) fmap f (UNext a) = UNext (fmap f a) uncons :: Union (f ': fs) a -&gt; Either (f a) (Union fs a) uncons (UNow a) = Left a uncons (UNext a) = Right a unwrap :: Union (f ': []) a -&gt; f a unwrap (UNow a) = a unwrap _ = error "Not possible" With a lot of typeclass and type family magic, you can create functions with signatures like these inj :: Elem f fs =&gt; f a -&gt; Union fs a prj :: Elem f fs =&gt; Proxy f -&gt; Union fs a -&gt; Maybe (f a) For implementation, checkout the `extensible-effects` library. It does this exact kind of thing to enable an arbitrarily effectful monad. 
Not a fan. The reason I was excited about GHCVM was that it was literally GHC on the JVM. We already have plenty of not-Haskell languages like Frege and Scala on the JVM. I was hoping for a compiler that could seamlessly use every library on Hackage and keep up to date with the latest GHC features.
Buy it. Now. I have been dabbling with Haskell every now and then for years and only after reading this book I feel that I'm able to really write Haskell programs.
Likewise.
A tip that saved me a huge amount of time, you don't have to exit the ghci and execute it to reload your code changes, all you need to do is type `:r` and then if you want to run your code again just run the main function again: `main`. Does anyone know how to combine the above into one command? **Edit: I just read up on ghcid which appears to do the above automatically! That is awesome.**
not familiar with the streaming library , but could it be related to https://www.well-typed.com/blog/2016/09/sharing-conduit/ ?
Estimated Time of Arrival ? 
Might as well call it DNF...
With ghci + :reload it compiles instantly. I feel like depending on one's setup, Haskell can be either extremely productive or quite unproductive. That instateneous feedback makes a lot of difference. There are several good habits to pick up as well, like using type holes and undefined to incrementally prototype your logic, instead of banging your head against the wall of type errors. If every small change gets you an immediate feedback, you know exactly which change gave you this feedback, in contrast to making big monolithic changes. I've tried ghc-mod, but did not like it as much as simple ghci setup for rapid development. In my vim setup, each time I press "mu", it saves the file and executes function named "c" inside env.hs. The rationalle is that "m" and "u" are under index fingers on Drovak , so it's both ergonomic and fast. Also, I don't want to get just type errors, I want it to be interactive, like printing data and doing something. Also, I want to leave window management for a good tiling window managers, not have windows in a terminal. So, my current vim setup is pretty simple: function! Hacompile() write call system('xdotool type --window $(xdotool search --class hcompile | head -n 1) ":load env.hs"; xdotool key --window $(xdotool search --class hcompile | head -n 1) KP_Enter c KP_Enter') endfunction nmap mu :call Hacompile()&lt;CR&gt; So, each time "mu" is pressed, without unfocusing the current window (so you can continue writing the next line), it finds the window with class "hcompile" and types ":load env.hs" &lt;ENTER&gt; "c" &lt;ENTER&gt;. It does it instantly. Also the window can be anywhere (it does not even need to be on the current workplace). Now, you just need to choose which window is going to be your ghci window, and run this alias: alias set-ghci-class='sleep 2; xdotool getwindowfocus set_window --class hcompile' which will wait for 2 seconds and assign the class "hcompile" to the currently focused window, so that xdotool knows which window is your current ghci window that you use to get feedback. It's so simple, but I like it much more than anything else I've tried. So, this "c" is the current cell that you're working on, like iPython cell or a line in Mathematica, and "env.hs" is like your notebook. When you're done with it you just rename the function to something else besides "c" and move to something else, if you want to go back and work on the old cell, then you just rename it back to "c". You normally don't care about reading many cells at the same time anyway, but even if you need this, then you can just move them all to "c" and now you can get feedback about several things at the same time. I like KISS approach so far. 
Exercise: in your own words, can you explain what the `.` does in `f.(pow (n-1) f)`?
Buy it. It's great. If you can't afford, contact the authors. They seem amenable to working something out.
If you don't believe that verbosity equals clarity, read Graham Hutton's book instead.
It is pretty expensive. It's worth it, but can be hard to justify. It's worth sending the creators an email, they have helped some students get it for cheaper. 
What brynser said. The only reason I finished CIS194 one time through, was I was very determined to learn Haskell. I had to do a lot of asking around on IRC in #haskell-beginners to get through some of the exercises. Getting the book will cover everything CIS194 and NICTA does and then some. It also covers everything at a pace you can keep up with.
Yeah, it looks like they might have plans to add new features to make it work better with the JVM (hence the term "dialect" in the README) but still fully compatible with GHC Haskell.
Either that or I will just have to wait until the swedish currency gets a better exchange rate to the US dollar, its really bad atm :P I dont want them to make less money, I assume a lot work has been put into the book :) 
It's faithful to the Haskell 2010 language standard. But that includes very little compared to GHC. Frege has a completely different prelude, isn't designed to work with Hackage, and doesn't have any GHC language extensions.
Here's an unrelated tip if you're new to posting code on reddit. Format your code. Using markdown is pretty easy, and reddit has a nice "formatting help" dropdown link when you're typing a comment. That way, you can write you're previous comment like this: smallestElement :: [[Int]] -&gt; [Int] smallestElement x = map minimum x Especially when your code gets longer, people are much more likely to help you if its formatted.
In Haskell, I find that `map` and `filter` are syntactically much nicer than in Python, especially with point-free style; they're also more natural for me to think about, so as a rule of thumb, I'd say, "Prefer map and filter to list comprehensions when possible." The major reason to use list comprehensions in Haskell is when things resemble cartesian products. For example -- Defines an infinite lazy list of pythagorean triples -- More like Python generators than straight-up Python lists pythagorean = [(a,b,c) | a &lt;- [1..], b &lt;- [1..a], c &lt;- [1..b], c^2 + b^2 == a^2] Even that, though, I might consider writing using a `filter`: isPythagorean (a,b,c) = a^2 == b^2 + c^2 triples = [(a,b,c) | a &lt;- [1..], b &lt;- [1..a], c &lt;- [1..b]] pythagorean = filter isPythagorean triples
I worked through all the CIS 194 class exercises and understood them. I thought everything up to Monad was great and very thorough, but then for the Monad exercise mostly all we did was work with a random number generator. Would this haskellbook.com expand a lot on exercises for really understanding monads?
your requirement "but if that list doesn't contain a smallest element i.e [1,1,1] then don't print anything" - that requirement is somewhat arbitrary, the requirement could possibly, in your example, be 1. So break that out into a separate function. Write lots of little functions that get at aspects of the problem. Your input is a list, right? So import Data.List and familiarize yourself with the functions in that module. Exercise each of these, write a few functions using one, then move on to the next. Eventually your solution will become obvious; you'll find it amongst the small bits you've created.
Yeah too bad that I use a well-designed text editor
I understand what they're going for, and for its purpose, it's great. I would just prefer more real life applicability. I also realized that given my preference, Haskell may not be for me.
With `do`-notation pythagorean :: [(Integer, Integer, Integer)] pythagorean = do a &lt;- [1..] b &lt;- [1..a] c &lt;- [1..b] let isPythagorean = a^2 == b^2 + c^2 guard isPythagorean pure (a, b, c) 
~~Based on what I've seen, sirinath is part of the new [typelead org](https://github.com/typelead)~~. They seem to be starting a company around this.
Link please ? When their website had more info it didn't list that fellow and there are seemingly zero commits by him. 
I'm a friend of Rahul and as far as I know, sirinath is not a part of typelead org. Also he was planning to post a blogpost clarifying the relationship between eta and ghc by this weekend.
Do you know meaning of your type signature `smallestElement :: [[Int]] -&gt; [Int]`?
Julie and I both have upcoming, but separate, projects. Fret not :)
It's great. I've read LYAH front to back along with about half of Real World Haskell, and First Principles the best out of all of those. Well worth the money. It's really the exercises that make the difference. They pose the question and don't give you the answer, the difficulty of the questions is just right.
Hi Cody. We generally started with much shorter, terse chapters earlier in development. There was one exception that got chopped up a lot. From there, material only got added based on reader feedback and testing. I'd rather a book be long because it needed to be, based on actual testing and review. I did not want to write a book this long or spend this much time on it, but we did what was necessary. Have you worked through the book? Do you understand why others in the thread are enthused about it?
I have a vim autoexec that writes the currently edited .hs file to ~/.vim/current_hs, and a ghci macro :L that loads from that file. Then I use :L to switch to the edited module, or :r to stay in the current one. Usually I stay in the test module and run test_whatever by hand, while modifying either the test or the tested function.
Soon™
You can use the datatypes a-la-carte machinery to reduce some boilerplate: class f :&lt;: g where inj :: f a -&gt; g a instance f :&lt;: f where inj = id instance {-# OVERLAPPING #-} f :&lt;: Sum f g where inj = InL instance f :&lt;: g =&gt; f :&lt;: Sum h g where inj = InR . inj send :: (Functor f, Functor g, f :&lt;: g) =&gt; f a -&gt; Free g a send = liftF . inj This works as long as your `:+:` things are right-associative and you never manually parenthesize a type like `Foo :+: (Bar :+: Baz) :+: Qux` /cc /u/ElvishJerricco, penny for your thoughts about this approach vs. going full `Union`?
I really disagree with efforts to make Haskell "easier". GHC Haskell has those "wacky" extensions because we need them. Why do people assume that removing features makes project more accessible? 
It's not always removing features that breaks compatibility. PureScript is a great example of regularizing things (e.g. `forall` all the type variables!). Such changes probably do make the language easier to learn, at the expense of no longer being GHC-Haskell. We all benefit by having both efforts to do better than GHC-Haskell (e.g. PureScript and perhaps Eta) as well as efforts to expand the reach of GHC-Haskell (e.g. GHCJS).
Well they're pretty much the same thing =P Just different representations.
The downvote button has the hover-text "Not constructive," so now is the perfect time to use it. GNU Emacs is based off of some unusual input decisions compared to more recently created editors, but it's a very powerful development tool for many languages.
Why do you write edited file to another location rather than just updating the file that you're editing? For myself, I found that switching to ghci to manually type something is not good for several reasons: 1) I found myself switching back and forth between ghci and vim, and using arrow + enter constantly to execute something again 2) it's hard to see what was previously typed, because you have to use arrows to browse history, if you keep it in a separate file, then all your tests and experiments can be easily viewed and navigated, not to mention that you can use normal version control tools like git 3) it's harder to edit in ghci in general compared to editing something in a proper editor like vim. This is why I went away from using ghci and started to use a file for this: import TestModule -- c = map show [1..10] -- c = previousTest -- c = test_whatever _ -- e.g. compiler says found hole with type Int, no need to type :t c = test_whatever 5 With vim-commentary, comment / uncomments a line is two-three key presses, so it's very fast by default even without any macros (and by automating {commenting or renaming -&gt; next line -&gt; typing c = }, it can be even faster). It sounds ridiculously primitive, but so far it's been quite productive for me, a lot better than just typing stuff in ghci. 
Can you tell us what the entire question is? It's not obvious to me why `[1,1,1]` doesn't have a smallest element, for example. (I would say that the smallest element of `[1,1,1]` is the number `1` which is also the second-smallest and the third-smallest element.) Well, maybe you want to get the smallest element but only if it is unique. That would be a function: smallestUnique :: (Eq x, Ord x) =&gt; [x] -&gt; Maybe x smallestUnique = undefined with the `Nothing` case indicating that there is no smallest unique element and the `Just x` case indicating, "there is! It's `x`!". Now it's obvious that this doesn't directly have a recursive structure because if you match on `(x : xs)` and call `smallestUnique xs` and get back `Nothing` you don't know what the smallestDuplicate element was that caused this to be Nothing, if there even was one. So instead you need to pass around some `(x, Bool)` where the boolean flag indicates whether this thing has duplicates or not. So our wishful thinking suggests that we're looking for: smallestUnique :: Ord x =&gt; [x] -&gt; Maybe x smallestUnique [] = Nothing smallestUnique (x : xs) = process $ findMinAndDupe (x, False) xs where process (x, dupe) = if dupe then Nothing else Just x findMinAndDupe = undefined so now we've got to fill in that new function which finds a minimum element and also whether it's duplicated. But we do that by pattern matching: findMinAndDupe tpl [] = tpl findMinAndDupe (curr_min, curr_dupe) (x : xs) = case compare curr_min x of LT -&gt; findMinAndDupe (curr_min, curr_dupe) xs EQ -&gt; findMinAndDupe (curr_min, True) xs GT -&gt; findMinAndDupe (x, False) xs and that's that. Replace that `undefined` above and you should be able to detect whether the minimal element is a duplicate or not.
I got really tired of installing nginx and gssapi module just to achieve single-sign-on functionality. It seems to work for me :) Suggestions welcome before I push it to hackage. 
I think an open union is not as ambigous and has other nice properties such not requirering overlapping instances, is that correct?
Not just compared to recent editors. What about vim?
I remember that there was a reported 10x slowdown using do or monad comprehensions over plain list comprehensions, even if the monad was `[]`; it was a while ago, though. Do you know if it's still true?
This is intermediate stuff for those interested. There are more general ways of implementing functions like `catMaybes` without list comprehensions. # [`Foldable`](https://hackage.haskell.org/package/base-4.9.0.0/docs/Data-Foldable.html#t:Foldable) Type Class This is how we can define a more general `catMaybes` using only `Foldable`, I change the name since it doesn't operate on `Maybe` anymore. foldFold :: (Foldable f, Foldable g) =&gt; f (g a) -&gt; [a] foldFold = foldMap toList We recover the original `catMaybes` with `foldFold @[] @Maybe`. # [`lens`](https://hackage.haskell.org/package/lens) Library The `lens` library provides many ways of generalizing `catMaybes`, here is the same type as above: foldFold :: (Foldable f, Foldable g) =&gt; f (g a) -&gt; [a] foldFold = toListOf (folded.folded) the nice thing about it is that it is easy to add layers foldFoldFold :: (Foldable f, Foldable g, Foldable h) =&gt; f (g (h a)) -&gt; [a] foldFoldFold = toListOf (folded.folded.folded) and one with a more general type foldEachJust :: Each s s (Maybe a) (Maybe a) =&gt; s -&gt; [a] foldEachJust = toListOf (each._Just) and now it works on tuples too: foldEachJust @(_, _) :: (Maybe a, Maybe a) -&gt; [a] foldEachJust @(_, _, _) :: (Maybe a, Maybe a, Maybe a) -&gt; [a] foldEachJust @(_, _, _, _) :: (Maybe a, Maybe a, Maybe a, Maybe a) -&gt; [a] &gt;&gt;&gt; foldEachJust (Just 'A', Nothing, Just 'C', Just 'D', Nothing) "ACD" This can be effortlessly over-generalized, ¡Ay, caramba! toListOf (each.folded) :: (Each s s (f a) (f a), Foldable f) =&gt; s -&gt; [a]
You can use [Vim](http://www.vim.org) for vim controls.
But I can't integrate GHCi with Vim, which is why I do my Haskell development in Emacs. To each their own.
Can you run ghci in the way you want with a shell command?
I've been looking for something like this - fantastic to see. I will try it out in a few weeks, but much easier to do that once in stackage. 
What are the packages that no other editor has?
I don't that often, but after discovering some cool subreddits I'll probably be on here more regularly :] Greatly enjoyed the podcast, again.
Tracked down the library after watching this Computerphile video: https://www.youtube.com/watch?v=BYx04e35Xso
Apperantly there's already a paper about translating STG to Java. [Compiling Lazy Functional Programs Based on the Spineless Tagless G-machine for the Java Virtual Machine](http://www.jaist.ac.jp/~khchoi/doc/flops2001.pdf) What ever happened to this? Why not take this approach as well? 
As a self taught beginner, the way I see it, learning Haskell require you to advance your understanding on multiple fronts simultaneously. And advancing along some of these fronts not only takes mere understanding, but also fluency and intuition. I am not sure any book can do anything about it. In short, if you try to learn Haskell from one book, you are doing it wrong. What I mean is, there is only a very small part of 'Learning Haskell' that could be served by a book. The huge share of investment must come in the form of an effort from you. It is like body building. You can take an expensive membership is a premium gym. But at the end of the day, you have got to lift the weights yourself. Anything that promises otherwise is selling you a lie. Want to learn Haskell? Try to do stuff. Ask in IRC when you get stuck. That is the best and only way. And best of all, it's free. But books like these might be useful to fill in the gaps of self taught beginners like me.... Downvotes welcome..
What's the point? You can't make a quantum algorithm work on a classical computer.
Is it impossible to do or is it impossible to do efficiently. Because my admittedly noobish understanding was that you could run quantum algorithms on classical ones but just not efficiently. 
I think the point of the library is to simulate quantum algorithms. It's just that the benefits you get from being quantum are thrown out the window. At least, that's what was implied in the video OP linked.
You can [generate circuits though](https://www.youtube.com/watch?v=59frzb__Eqo) if you pick a different underlying representation for the monad.
"Not efficiently" is a very mild way to phrase it. You get an exponential slowdown, not to mention complicated matrix algebra. It would be much worse than any classical algorithm.
I'll be honest: I was afraid of reading Datatypes à la carte because I thought it would be very difficult to understand and would have a lot of type theory, specially those type expressions (I'm a physicist, not a computer scientist, I have no training in type theory, formal or informal). But I decided to try and was very surprised to learn it's a very well written and clear, and very readable for anyone whose only contact with type theory is Haskell's own type system. This article truly deserves to be called a pearl. 
It looks like they're going for retaining compatibility with Haskell, depending on compile options. Just coming up with their own diverging language would be rather silly, since the [Frege](https://github.com/Frege/frege) project is considerably more developed.
&gt; PureScript is a great example of regularizing things (e.g. forall all the type variables!). Such changes probably do make the language easier to learn, at the expense of no longer being GHC-Haskell. You could have language extensions for these things. There is no reason why a more "regularized" language should not be also be supported under 'GHC-Haskell'.
I too am almost done meditatively reading LYAH (been carrying it about in my bag for over a year -- I'm about to complete the Monoids chapter) and I kind of get the language so far but I can''t yet write code. Its my first pass through the language and I meant to read RWH and or do the 99 problems or write you a scheme next for practical exercises. Now I think I''m going to read the Haskell book next instead and solve the problems as I go. I'm waiting for the hardcopy though. I prefer the feel of an actual book especially the way often flip back and forth through it. So I'm waiting for the print version. And if its not available by the time I'm done studying LYAH (and watch a ton of monad videos) I will proceed to RWH and or do the 99 problems or write you a scheme as originally planned. Based on the great feedback about this book I suspect, or rather hope, that it help Haskell gain mindshare in the developer community. The other thing that's needed tho' is a killer application (e.g. what Rails did for Ruy). 
Presumably you could write code that could both run slowly in an emulator, or output code that could run on some futuristic quantum computer. Running the code in an emulator might be enough to test it works for a small, known problem space before booking time on the quantum computer for the full data.
I'm not a fan of nesting your libraries deep under Data, Network, System or whatever. It doesn't even prevent naming conflicts like Java's somewhat similar practice. Just pick some unique-ish name and use that as your top level module. If you insist on something generic like "TcpServer", wrap it in a distinctive module, say parts of your real/company name, and you're set.
Imagine a video game emulator, except instead of emulating ancient architectures from your childhood on your maxed out i7, you're emulating a PS4 on your i386. (I pulled these numbers out of /dev/null, in reality I'd guess the difference is exponentially larger, but you get the idea). 
The explanations here miss a very important point. It depends what you mean with efficiency, and what you want out of the calculation. Quantum algorithms rely on the stochastic nature of evaluation on a quantum computer. If you want to analyze the probability distribution of the possible outcomes you NEED a classical computer and it can be calculated very efficiently. It is impossible to do this on a quantum computer. You could only ever get an approximation of the distribution. However, if you're only interested in one possible result, then getting it directly through quantum mechanics will of course be faster. 
FYI Chrome warns me that my connection is not private if I try to go to datahaskell.org .
if you are a beginner i would start with doing some simpler backend computations first - e.g. some simple modeling with the statistics package or wrapping some R routines w/ inline-r before tackling the complexity of tensorflow. once you're confident in you can hook up the end-to-end data flow, then take it on.
This isn't correct. You can always integrate schroedinger's equation numerically.
We THINK it's impossible to do efficiently. It's always possible to integrate schrodinger's equation numerically, but the state space blows up like 2^n rather than just n where n is the number of (qu)bits.
What sets Emacs apart from other editors is that it can run subprocesses and repls with history and completion and also a compilation process. Vim cannot do it, you need to open a terminal and run ghci or runhaskell. Emacs is also one of the few programmable editors available. Vim can be extended with VIM script but it is not scalable like Emacs Lisp and Vim doesn't have a package manager. If you miss Vim there is Evil mode package that allows Emacs to emulate Vim. Emacs is also useful as terminal emulator replacement for the horrible cmd.exe on MS-Windows. Its built-in shell eshell gives a multi platform unix-like shell implemented in Elisp that can run on Windows, Mac OSX and Linux. The biggest selling point of Emacs is that it can be integrated to any tool. I always try new editors, but I always go back to Emacs. Emacs is addictive! Emacs rocks!
&gt; Quantum algorithms rely on the stochastic nature of evaluation on a quantum computer this is a common misconception. The output of at least some quantum algorithms is deterministic. Quantum mechanics without measurement is completely deterministic.
I agree. If your hackage package name is unique on hackage, then it makes a great top-level module name.
IIRC it's for bosons it's possible to solve QM problems efficiently using (classical) Monte Carlo algorithms, but for [fermions](https://en.wikipedia.org/wiki/Numerical_sign_problem) it is NP-complete in the worst case.
I meant that the vast majority of the time I (and I'd assume most people) hit enter on the terminal it's to get some human readable output rendered in such a way to make it easy to read for people. Not that you couldn't have some other format with a nice `Show` instance, just I wouldn't use the other instances that much. Moreover, my `String` interpretation was incorrect as it's really a `ByteString` which makes way more sense. I think piping around a sane version of `JSON` could be pretty cool -- keep the bytestring, but serialize the data -- but if the blob doesn't have what I want readily accessible then I'll be doing a similar, though more constrained and bounded amount of work to get it in the right shape during consumption.
As a downstream library of conduit, I think you have it right. Conduit should eventually get out from under Data. Until then, you should just follow the mothership hierarchy. Calling it a pure brand name feels like it would lower library discoverability. It works well for marquee libraries, or when a library crosses multiple domains, but I'm guessing here a potential user will already have conduit in mind.
I found [this](https://www.reddit.com/r/haskell/comments/4cr4kz/typeclasses_that_most_everything_should_support/) by searching for `site:reddit.com/r/haskell Eq Ord Show Read`.
&gt; Conduit should eventually get out from under Data. Why? 
The set of "transpilers" is a subset of the set of "compilers." It's useful to know about logical subsets. Transpilers are compilers, but you know more based on the word "transpiler" than you do based on the word "compiler." For one thing, it indicates that the source is mapped almost one to one, with no added runtime system. This, to me, is the defining feature, and anything that violates it is not a transpiler. But that's not a formal definition. Also, look at parser generators. They take BNF and produce some other language. That's a compiler. And yet the term "parser generator" is still preferred to the more general term "compiler," because it's more accurate. Point is, the term is perfectly reasonable and there's no reason for the stigma against it.
Hey Chris what's up. I do understand why people are enthused about it, different people learn differently. I prefer concision. I haven't worked through the book, only the samples that have been available at various stages. I do think that's sufficient to point out the difference in writing styles; you are covering more material than Hutton, but not 4 times as much. That being said, I'll probably buy a copy soon to support you and Julie (would have already if it was available in a reflowable text format for ereaders).
List comprehensions are just syntactic sugar for do notion, specialized to the list monad (then re-generalized by the monad comprehension extension, but that is a different story). The point of the sugar is to make do-notation for the list monad easier to read by analogy with standard mathematical notation for specifying sets. I find that list comprehension notation is indeed more readable than do notation for the list monad, especially when the set analogy matches what I am trying to express in my code. So I usually prefer list comprehensions whenever I am tempted to reach for do notation in the list monad.
I try to mold my *data types* to fit *type classes*. ### `Functor`, `Foldable`, `Traversable` A fine representation of color is data RGB = RGB !Word8 !Word8 !Word8 but **parameterizing** it lets us derive all sorts of instances data RGB a = RGB !a !a !a deriving (..., Show, Functor, Foldable, Traversable, ...) that give you a whole host of functions {-# Language PatternSynonyms #-} pattern Red :: RGB Word8 pattern Red = RGB 255 0 0 &gt;&gt;&gt; fmap (`div` 2) Red RGB 127 0 0 &gt;&gt;&gt; elem 255 Red True &gt;&gt;&gt; find (&gt; 127) (RGB 40 20 30) Nothing &gt;&gt;&gt; find (&gt; 127) Red Just 255 &gt;&gt;&gt; for_ Red print 255 0 0 ### `Representable` And Its Ilk As the gravy train comes to a screeching halt we realize we have to buckle up and actually write code. Luckily there is a trick — a data type (`RGB a`) is [`Representable`](https://hackage.haskell.org/package/adjunctions-4.3/docs/Data-Functor-Rep.html#t:Representable) if it can be represented by a function `ABC -&gt; a` {-# Language InstanceSigs #-} import Data.Functor.Rep data ABC = A | B | C instance Representable RGB where type Rep RGB = ABC index :: RGB a -&gt; (ABC -&gt; a) index (RGB a _ _) A = a index (RGB _ b _) B = b index (RGB _ _ c) C = c tabulate :: (ABC -&gt; a) -&gt; RGB a tabulate f = RGB (f A) (f B) (f C) and this peculiar type class provides default definitions of for a lot of other instances (you *do* have to write some boilerplate code) {-# Language InstanceSigs, TypeApplications #-} import Data.Functor.Rep import Data.Distributive import Control.Monad.Fix instance Distributive RGB where distribute :: Functor f =&gt; f (RGB a) -&gt; RGB (f a) distribute = distributeRep @RGB instance Applicative RGB where pure :: a -&gt; RGB a pure = pureRep @RGB (&lt;*&gt;) :: RGB (a -&gt; b) -&gt; (RGB a -&gt; RGB b) (&lt;*&gt;) = apRep @RGB instance Monad RGB where return :: a -&gt; RGB a return = pureRep @RGB (&gt;&gt;=) :: RGB a -&gt; (a -&gt; RGB b) -&gt; RGB b (&gt;&gt;=) = bindRep @RGB instance MonadReader ABC RGB where ask :: RGB ABC ask = askRep @RGB local :: (ABC -&gt; ABC) -&gt; (RGB a -&gt; RGB a) local = localRep @RGB instance MonadFix RGB where mfix :: (a -&gt; RGB a) -&gt; RGB a mfix = mfixRep @RGB ### Buy One, Get Two Free If we define [`TraversableWithIndex`](https://hackage.haskell.org/package/lens-4.15.1/docs/Control-Lens-Indexed.html#t:TraversableWithIndex) we get [`FunctorWithIndex`](https://hackage.haskell.org/package/lens-4.15.1/docs/Control-Lens-Indexed.html#t:FunctorWithIndex), [`FoldableWithIndex`](https://hackage.haskell.org/package/lens-4.15.1/docs/Control-Lens-Indexed.html#t:FoldableWithIndex) instance FunctorWithIndex ABC RGB instance FoldableWithIndex ABC RGB instance TraversableWithIndex ABC RGB where itraverse :: Applicative f =&gt; (ABC -&gt; a -&gt; f b) -&gt; (RGB a -&gt; f (RGB b)) itraverse f (RGB r g b) = RGB &lt;$&gt; f A r &lt;*&gt; f B g &lt;*&gt; f C b it provides us with indexed versions of common functions like [`imap :: FunctorWithIndex i f =&gt; (i -&gt; a -&gt; b) -&gt; (f a -&gt; f b)`](https://hackage.haskell.org/package/lens-4.15.1/docs/Control-Lens-Indexed.html#v:imap) imap @_ @RGB :: (ABC -&gt; a -&gt; b) -&gt; (RGB a -&gt; RGB b) &gt;&gt;&gt; imap (,) Red RGB (A,255) (B,0) (C,0) &gt;&gt;&gt; ifor_ Red $ \a b -&gt; unless (a == B) $ print (a, b) (A,255) (C,0)
I would bound it with a typeclass with an instance for instances of `MonadIO` so that you can reuse the code in single-threaded (or, for that matter, `par`-based) scenarios. If people want to enforce IO capabilities in their code they can do that themselves. Adding an IO bound at the library level just makes it harder to mock.
Thanks for releasing. I was a little bit expecting a "hello world" that spans both client and server-side. But currently the projects are completely separate from each other.
Boson sampling is at least #P-hard. Quite hard.
Ah, it was just because you gave https:// in the link. It came up with a big scary-looking screen like [this one](https://storage.googleapis.com/cdn.vlogg.com/uploads/2014/09/chrome_certificate_error_updated_2.png), but if you just try to go to http://datahaskell.org it's fine.
If the system is in a state corresponding to one of the set of orthogonally chosen projectors used in the measurement, the measurement outcome is deterministic. If I prepare |g&gt;+|e&gt; and then measure along X, I will measure +1 with probability 1.
...with errors. Really, you can do anything with any tool if you allow errors. Then again, we're talking about probabilistically measured quantum computers here.
It's a horrible neologism for a source-to-source compiler. First we already had better-sounding words for it (transcompiler), second it just looks yucky (transpill your coffee), and lastly it reflects the dominance of the world's worst programming language JavaScript, which is so bad a million transCOMpilers were written to stop the bleeding. But don't take it from me. Babel is brought up as the quintessential example. Go to their [website](https://babeljs.io/) and look what you see: "Babel is a JavaScript compiler" in huge bold letters.
&gt; You wouldn't call clang a "transpiler" when it emits LLVM source No I wouldn't. The definition of "transpiling" that I imagine only applies to compilation that is nearly one-to-one between sources. That is, near-zero manipulation of the logical structure of the language of the program. Languages like TypeScript and CoffeeScript achieve this pretty obviously. Under that defintion, the project that spawned this discussion certainly isn't a transpiler, and neither are the other examples you pointed out. That doesn't, however, mean the term is useless.
This looks great! You should publish it somewhere a little more publicly. 
Ah that's helpful because at first I didn't understand why you were involving stack at all. Now it seems more useful to me as I also prefer to use nix for everything. Stack can *use nix* pretty well, but nix cannot *use stack* nearly as well (i.e. making derivations that use stack for compilation is not well supported). Since I'm using `nixops`, I need everything to be fully derived in nix thus including stack into the mix is more trouble than its worth.
Right, GSoC dropped haskell. Like I said, I already downloaded it. edit: and, I've donated &gt;$100 to haskell.org 
You will probably be interested in Quipper then: http://www.mathstat.dal.ca/~selinger/quipper/ The caveat is that it needs a bit of love to bring it up to 8.0. I tried bringing it up to 7.10 last year, but there were some bugs in 7.10 that prevented me. I think in 8.0 everything should be fine though.
Agreed. Even if (like me) you are ultimately skeptical about the value of pushing Haskell toward dependent types, this remains a bad choice. Betting that GHC 7.10.3 will remain relevant in the long term seems like an obvious mistake.
If you do this there will be (hopefully) a fork that is actually a GHC backend for the JVM, since there is a huge amount of demand for that. Why not start out with that fork to begin with in an organized and official way, so that development on the two branches can share code and ideas fruitfully?
AWESOME
Do some of those quantum algorithms with deterministic output nonetheless have better time complexity than the classical algorithm?
`NFData` is important - I have been bitten hard by library authors forgetting to provide it. It's not a niche. `Read` instances are also important. When you need quick and dirty serialization, e.g. when debugging multiple components of a large system, you can just use `Show` and `Read` - unless even a single upstream library author didn't bother to derive `Read`, in which case you are stuck building the entire monster from scratch. Or using generics or something. OTOH, deriving a `Read` instance can be costly in compile time. So I guess `Read` should be considered "very nice to have", try very hard to provide it if you can. Personally I never need `Typeable` or `Generic`, but I think a lot of people do need them. The suggestions of /u/Iceland_jack about `Foldable`, etc., are great where appropriate. But they don't always make sense. So those are more type design good practices than required instances.
I'm a beginning non-programmer. I bought the second edition of Programming in Haskell, was completely fine for the first two chapters, and then completely hit the wall on chapter 3. I couldn't understand what on earth was happening with all of the type and function-related terms Hutton was introducing / building / talking about in the chapter. The problem I have with Hutton's book is that it is too concise. This is not a good thing for a beginning non-programmer like me (it is advertised as being targeted for beginning non-programmers along with programmers). I need a gradual and progressive approach that builds concepts slowly, with ease, and in detail, along with a lot of examples. I didn't get this in chapter 3. It's possible that if I work through chapter 3 hard enough, and especially, if I start asking questions about it via the internet, I will end up understanding it, but I instead decided to buy Haskell Programming from First Principles a couple of days ago. I finished the Lambda Calculus chapter (which is the first chapter), and I am now doing chapter 2. The Lambda Calculus chapter was not easy, and in fact I also got stuck in several parts. What saved me were the detailed answers to the chapter exercises, which allowed me to probably understand the chapter. I say probably as I've sent a few questions about the chapter to the authors, along with feedback on how the chapter was written. Hopefully they will answer me, which will allow me to ease my doubts about my understanding of the chapter. I have not yet reached the topics covered in Hutton's chapter 3, but I am hoping that when I do the Haskell Book's approach will be a better fit for a beginning non-programmer like me. I am still planning on studying Hutton's book after I finish the Haskell Book, and that is because if I have understood correctly, Hutton talks about "Equational Reasoning", which is a way of reasoning about algorithms using Haskell, and the "Compositional Style", which is due to the Lazy Evaluation of Haskell. From what I have understood, Haskell Programming from First Principles doesn't talk about either of them (but I could be wrong).
I have no idea how to help solve the problem, but have you looked at the source for the [Emacs plugin](https://github.com/commercialhaskell/intero/blob/master/elisp/intero.el)? I'd suggest looking there and asking questions about things from that which you don't understand.
Thanks for the advice. I have already tried to understand the source code for the plugin, but I have difficulty reading elisp. I'll try again!
Is that useful for anything? Is there anything special about deterministic quantum algorithms regarding analysis or their capabilities? 
I've merged your intero update into the reflex-platform mainline. Thanks!
Did you do the exercises for Hutton chapter 3, and look at the appendix A for answers to the exercises when you got stuck? If you did, I'd be very curious to know exactly at which point you got stuck. But like I said, different people learn differently, it's good to have options.
The measurements are not probabilistic and quantum computing can be done exactly with integer math. http://thegreves.com/david/QDD/qdd.html maps quantum algorithms onto BDDs. I also have to emphasize, as I did in another post, that quantum measurement needn't be probabilistic. At all. It usually is. The set of states the system can be in for a deterministic outcome is measure zero. BUT, if the system is in a state corresponding to one of the set of orthogonally chosen projectors used in the measurement, the measurement outcome is deterministic. Deutch-Jozsa is a fully deterministic quantum algorithm.
Are there plans to merge the reflex-platform nix configuration with nixpkgs itself? It seems it would be a win-win situation.
I see. Thanks for summing this up. It's been a while for me. But yes, all deterministic algorithms rely on classical parts or they don't do anything interesting. In Deutch-Jozsa's case the algorithm isn't well defined for most of the possible inputs, so it doesn't count as is. Not a problem of course cause it's a huge starting point for other algorithms. I got used to not calling algorithms deterministic if that property isn't used in any way. For instance, the class of deterministic Turing machines is a subset of non-det TMs. It's confusing terminology but that's normal in math (see what I did there?). AFAIK for quantum algorithms you can't immediately determine that even through types or from construction, so any analysis would have to assume non-determinism. 
"But yes, all deterministic algorithms rely on classical parts or they don't do anything interesting." what do you mean? Deutch-Jozsa has no classical computation.
I'd love to move as much upstream as possible! There are a few bits and pieces that wouldn't make sense to move upstream, but right now the main limiting factor is that I don't have the time to shepherd all the PRs. If there's any particular piece you'd like to see upstream ASAP, let me know, and I'll see what I can do.
This looks really cool. From the white paper, it appears to represent quite a bit of research and R&amp;D. But what does it do exactly? Is there some introduction to this somewhere? The [smart contract wikipedia article](https://en.wikipedia.org/wiki/Smart_contract) is useless for the uninitiated.
https://github.com/vaibhavsagar/duffer
yeah, I prefer shorter import hierarchies (like `import Pipes`). 
I actually find the temptation to anthropomorphize (or in this case "cytomorphize") perhaps the biggest problem with OO as it is commonly used. Something about objects, fields and methods sends programmers off to the land of "this guy here takes care of these things before he sends it over to this fella who's in charge of ...". It usually just obscures the actual problem being solved. Objects are just A(bstract)DTs with a magic function syntax, visibility sugar, and largely problematic subtyping. I guess I don't think a metaphor is going to save programmers from having to simply design and factor solutions.
In OOP, state isn't really "completely hidden", objects are stateful by their very nature, resulting in a combinatorial explosion of possible states your system could be in. This may be what you want in some cases, it certainly benefits biological systems. But many systems need to be deterministic, reliable and secure. This is best achieved by limiting the number of possible states a system can be in, so that it is easier to reason about (whether formally or otherwise). Static-typing, while hugely beneficial, does have a closed-world assumption, but something more dynamic and OOP-like could still of course be used at larger scales (e.g processes, services, actors).
If you want to run it as a separate language server, you can use https://github.com/alanz/haskell-lsp to manage the communication. And an example server at https://github.com/alanz/haskell-ide-engine/blob/lsp/src/Haskell/Ide/Engine/Transport/LspStdio.hs 
i'm not an expert nor experienced in either, but i think intero.el uses company mode which provides the completion, and i think you can help it out a bit; with vscode the company-mode like thing is "intellisense" where it effectively does what company mode does. So I'm suggesting you should look into that in [vscode](https://code.visualstudio.com/docs/editor/intellisense) to be able to port intero to vscode. I don't believe either of these solutions instrument compilation... oh and edit: I'd check out [RustyCode](https://github.com/saviorisdead/RustyCode) they do these types of things in vscode 
The post itself is fine. And as a title of the blog post, that's prolly fine too. It's just the title of the reddit link that's suboptimal
I'm going to second the author here: this is useful *exactly* because it defeats that part of Stack, while still enabling Intero. In a perfect world there would be some way to do this more directly (ie just tell Stack to use Nix's Haskell packages), but we work with what we have.
Why is that a good thing? In terms of compiling Haskell packages, the only real thing Nix has over Stack is remote caches. But other than that they're fairly equivalent. Stack and its resolvers are just enormously easier to use, and probably more up to date and stable.
tagsoup is _great_ for just crawling. It doesn't extract data into a fully well-formed page structure -- instead, it gives you just enough additional lexical structure to parse out data well from all sorts of weird html and xmlish things even if they're not well formed. So I prefer it to something stricter and more structured for that purpose. But if you want to actually treat the html more properly, then indeed other approaches are better.
What is "lowering"? Compiling?
Wait, why the `Monad m` constraint? You're not thinking of using `fail`, are you? If you're only using `Monad` for the `fail` method, then you really don't lose anything by just monomorphizing to `Maybe`. In fact `Data.Map.lookup` underwent exactly such a change a while ago. And the `fail` method is going away anyway: https://prime.haskell.org/wiki/Libraries/Proposals/MonadFail . So concretely I would propose to just replace `m` by `Maybe` everywhere, and get rid of the type parameter from `LeapSecondMap`. Unless there is some other purpose for parameterizing over an arbitrary `Monad` that I am not seeing.
I actually do use that, but I think your change would help me. timezone-unix would especially, if it actually worked well enough. I currently track the partiality of the table externally.
Compiling, but referring to the generation of some intermediate representation on the way to interpretation (for interpreted langs, or on the way to object code for compiled). For instance the guile compiler actually goes through 4 representations (Tree-IL, GLIL, Assembly, Bytecode) before object code. (https://www.gnu.org/software/guile/manual/html_node/Compiler-Tower.html#Compiler-Tower)
The way it works is that you have a session, and it keeps the state of the last successful load. So if you have a new file, load an empty (or skeleton) file to give the completion system something to start with.
It's the best textbook i ever read, all subjects considered. It's even better than C++ Primer, and i wish all my math textbooks were like that. They struck the perfect balance between explaining enough but not too much, going fast enough but not too much, exercises interesting but not too hard etc. It's really, really great.
Congrats, that's my christmas present sorted!
I already parse Olson timezone files in the [timezone-olson](http://hackage.haskell.org/package/timezone-olson) package. It would be easy to add leap second tables to that, and I plan to. It would be really cool if there were already a standard type for leap seconds in time. I was going to release a separate library for that.
Awesome! After the [Ethereum mishap](https://en.wikipedia.org/wiki/Ethereum#The_DAO_and_the_blockchain_fork), there's no question we need a safe language for smart contracts. Has the language been deployed in production for any commercial projects yet, or is this solely a research project so far?
Ah, I see. That makes sense, though it still seems a bit overengineered to me. I don't forsee myself personally using this API though, so take my opinion with a grain of salt. At the very least it certainly would make sense to use `Applicative` or `Functor` or whatever the least restrictive thing is you can get away with.
Really interesting - I am a fan of Ethereum but I think that being Turing-complete is not an advantage when it comes to smart contracts. If you don't mind me asking, what do you envisage private blockchains will be used for? Apart from the obvious inter-bank settlement systems etc.
Most packages on hackage already have links to stackage in the Distributions section anyway, seems like a lot of effort to save a few seconds...and you could accomplish the same thing easily in Greasemonkey: window.location = document.querySelector('a[href*="stackage.org"]').href;
I think this sort of split is corrosive to a community. It won't help newcomers who desperately need a reliable source of packages and information and are likely to trust the official site instead of a third-party solution. Unity people! What's the cause of the out of date stuff on hackage and can we fix it?
The basic idea behind a smart contract is two-fold: 1. A "contract language", which enables programmatic contracts, thus avoiding the need for a (human) third party (eg. a judge) 2. A medium in which to irreversably embed these contracts, such that the parties to the contract cannot later deny that they entered into it in the first place In the case of Bitcoin, the contract language is called "Bitcoin script", and the medium which these contracts are stored is called the "Bitcoin blockchain". So, in the Bitcoin blockchain, scripts (smart contracts) are embedded, and associated with some number of bitcoins (from 1e-8 bitcoin and up). The Bitcoin script defines the requirements needed to move bitcoins from the given script to a new script. So when you send 1 bitcoin to bitcoin address `12wdBApoi77BYCSdB3CNVW3GPcwQcjgnLv`, 1 bitcoin becomes associated with a script, which requires the redeemer (he who wishes to send/move the bitcoins to a new address/script) to provide a digital signature over the redeeming transaction, which verifies against a public key that hashes to `12wdBApoi77BYCSdB3CNVW3GPcwQcjgnLv` (160 bit hash encoded using "base58 check" encoding). That's an example of a simple script. More complicated script are possible, like this script: IF &lt;service pubkey&gt; CHECKSIGVERIFY ELSE &lt;expiry time&gt; CHECKLOCKTIMEVERIFY DROP ENDIF &lt;user pubkey&gt; CHECKSIG which requires *either* 1) two signatures: one that verifies against "service" pubkey and one that verifies against "user" pubkey *or* 2) when `&lt;expiry time&gt;` has passed: only "user" pubkey 
I wanted to do something like this for my master thesis! I was wondering if dependently typed languages like idris and agda would be useful to submit "proofs" about smart contracts to improve mechanical trust in them. Really looking forward digging into this for inspiration.
OO is optimized for building programs out of communicating stateful parts. Many programs can be written in this way very naturally, like UIs or games. If you try to enforce purity, you'll be using stateless things to simulate stateful things (like text boxes or game monsters), so your program's internals won't correspond as nicely to the problem domain. It's better to use FP for problems that don't look like a bunch of stateful objects, but more like a mathematical function, like compilers or numerical programming.
Visit www.java.com and tell me how in the world that language could possibly succeed :-)
Be aware that tagsoup consumes a lot of memory and is often a bottleneck when the html page is big (not so uncommon). Also the librairy has its own isString mechanism, so you cannot use OverloadedString extension with it (which is kind of a pain)
Doomed even if it does, IMO. At best you basically end up with Frege or Scala, i.e. not Haskell.
[dom-selector](https://hackage.haskell.org/package/dom-selector-0.2.0.1) if you want something more high level 
Haskell has an open world assumption actually
I wasn't aware of that, thanks!
I just use chrome search engines... `hk` for hackage and `sk` for stackage
The `.|` syntax is relatively new, so old content still refers to the old operators. But it's a pretty straight-forward translation: x $= y = x .| y x =$ y = x .| y x =$= y = x .| y x $$ y = runConduit (x .| y) If the old operators seem needlessly confusing/redundant... well, that's why we have new operators :). You should be able to use these libraries fairly easily from what you've already learnt, but here's a kick-starter example: #!/usr/bin/env stack {- stack --install-ghc --resolver lts-6.23 runghc --package http-conduit --package html-conduit -} {-# LANGUAGE OverloadedStrings #-} import qualified Data.Text.IO as T import Network.HTTP.Simple (httpSink) import Text.HTML.DOM (sinkDoc) import Text.XML.Cursor (attributeIs, content, element, fromDocument, ($//), (&amp;/), (&amp;//)) main :: IO () main = do doc &lt;- httpSink "http://www.yesodweb.com/book" $ const sinkDoc let cursor = fromDocument doc T.putStrLn "Chapters in the Yesod book:\n" mapM_ T.putStrLn $ cursor $// attributeIs "class" "main-listing" &amp;// element "li" &amp;/ element "a" &amp;/ content
The only thing holding back LibClang from moving to a more recent LTS is the version of Cabal. LibClang has a non-trivial Setup.hs which does not compile with version 1.24. I have recently [moved it to lts-6.22](https://github.com/chetant/LibClang/commit/126cb4e8243d2118146aa919a250e787016646b1) so you can at least get GHC 7.10.3 and some newer dependencies.
Thanks! We're in POCs with financial institutions in use cases in collateral management and international payments, and starting to look at pension risk-transfer use-cases too. Mainly financial-institution stuff so far.
You can also cf [hopper](https://github.com/hopper-lang/hopper-v0) which we were involved in before. Regarding Pact, would love to hear ideas about manageable ways to submit contracts for formal verification, given that Pact is dynamically-typed and interpreted.
Thanks! Private (or permissioned) blockchain can also be seen as a controlled-membership deterministic BFT consensus (as opposed to Bitcoin/Ethereum being a open-membership probabilistic one). Therefore its easy to imagine "public" deployments with a governance structure for running the nodes. This has some advantages over the compute-power arms race that public blockchain encourages. Otherwise, it's any transactional system that would benefit from perfect replication and messaging. Outside of financial, applications are in records transfer, supply chain, etc. The whole point of a non-use-case specific smart contract language like Pact is it allows you to model pretty much anything (that operates in a transactional context). 
I've ended up just doing custom Storable instances based on testing in my projects. Not recommending this, but it was the most direct path for me to something working.
You're looking for an IDE. * [Leksah](http://leksah.org/) is one that some people have success with. I haven't, though I haven't tried using it in a long time. * You can get a lot of what you're looking for with [Atom](https://atom.io/) and the various [ide-haskell](https://atom.io/packages/ide-haskell) plugins (that page will list other dependencies) A lot of people have good setups with Vim or Emacs (or Spacemacs), but I haven't used them in a Windows environment (or really Haskell in general, since it was a bit of a headache to use on Windows back when I was trying. Since then, things like [stack](https://docs.haskellstack.org/en/stable/README/) have come along that greatly simplify a lot of things. I'm guessing you're using the Haskell Platform from your mention of WinGHCi, which a lot of people will tell you to scrap in favor of stack. Sorry I can't be of more specific help, but Leksah or Atom are good starting points, I think, though they may require some work to get setup.
But, as it turns out, that "natural" approach to build UIs (maybe even games!) result in code that's very tedious to write, complicated and people started seeking other abstractions anyway. In this area FP oppers FRP, which, in my opinion, beats the OO in every aspect;) And I totally don't agree with: &gt; If you try to enforce purity, you'll be using stateless things to simulate stateful things (like text boxes or game monsters), so your program's internals won't correspond as nicely to the problem domain. When you think of an UI, the first-class events and varying values (aka behaviours) are really your problem domain, there's no mismatch at all! 
I don't have any experience with Windows, but on Linux, using `stack` made everything just work with minimal frustration. It is not a GUI but it is still pretty simple. If you want an editor that has good Haskell integration see after the break. I installed only stack, then while I was a noob, I would just `stack new projectname simple` anytime I wanted to make a new file until I took the time to actually learn how stack and cabal worked. stack will install ghc and everything else needed to compile your programs. You can also do most anything you would 'normally' with a non-stack project by just appending `stack` to the command. e.g. instead of typing `runhaskell file.hs`, you type `stack runhaskell file.hs`. Instead of `ghci`, you type `stack ghci`. If you want to get up and running quickly with stack, just type `stack --help` for a quick overview of all commands. Otherwise you can get a guide to it here: https://docs.haskellstack.org/en/stable/GUIDE/ Or a quick TL;DR below for common commands: `stack new projectname simple` - Setup a new project. Important files are `src/Main.hs` which is your main code file and `projectname.cabal` which is where you add dependencies. `stack setup` - setup the build environment in a new install. This downloads GHC etc. `stack build` - this compiles your application. If you added dependencies, this will also download them. `stack ghci` - Go into a ghci session with access only to the same libraries and ghc version your Main.hs file would have access to. ----- There are probably more editors available, but the only two I have used is Atom and Spacemacs. Atom I last tried a few months ago (so it may have improved) but when I used it, it would mostly work with suggestions and error highlighting etc. but it was also super fragile and would randomly break all of the time. It was also kinda slow. Spacemacs is where I am at currently. It has 'layers' (similar to plugins) and one of them is a haskell layer. It has [intero](https://commercialhaskell.github.io/intero/) integrated into it which is sort of like a mini IDE. You can additionally open a terminal in it (`SPACE '`) and use stack directly. Only thing that is a downside for some is that spacemacs is either emacs or vim keybindings. There is no 'normal' option for keybindings (by normal, I mean the type of keybindings that notepad++ and most other GUI editors use). That said, I think putting in the time to learn vim keybindings is definitely worth it to be able to use spacemacs. EDIT: Typo in command
Interesting. I would love to understand more generally what kinds of logic (ie not just the bytecodes, but the intent) you can run on Bitcoin and what you can't. What can you *not* do with pay-to-script-hash?
&gt; If the old operators seem needlessly confusing/redundant... well, that's why we have new operators :). https://xkcd.com/927/ :)
This is a very fair objection to the reskin to be honest. FWIW, here are the relevant links about this: * http://www.snoyman.com/blog/2016/09/proposed-conduit-reskin * https://www.reddit.com/r/haskell/comments/55domr/proposed_conduit_reskin_michael_snoymans_blog/ * https://twitter.com/snoyberg/status/779244960600162304
Well, I thought OP intended to use this tool to discover the layout at runtime, but it's also possible that they would like to code-generate the serializer in advance.
I'm not entirely sure, but I think [this commit](https://github.com/bitcoin/bitcoin/pull/4365) enables most/all script ops in P2SH redeemScripts, as long as they have, at most, 15 signature operations. But to be sure, you're probably better off asking this in #bitcoin-wizards on freenode, since I'm uncertain about the more exotic opcodes like string concatenation etc. (I *think* they're still disabled).
&gt; What's the simplest way of having single program to edit haskell code and run it? * Install Haskell stack from fp-complete. It is faster to do it with chocolately package manager. See: - https://chocolatey.org/packages/haskell-stack - https://chocolatey.org/packages/emacs64 &gt; choco install haskell-stack * Install Emacs. &gt; chco install emacs64 With Emacs install Haskell mode and use M-x compile that will run the compilation command inside Emacs. Example: M-x compile stack ghc myapp.hs Run **M-x async-shell-command stack gchi** to run the Haskell ghci REPL. I've create a gif example about it available [here](http://caiorss.github.io/Emacs-Elisp-Programming/Utilities.html#sec-1-4). 
Ah, I see. I assumed OP was trying to generate bindings ahead of time. If this is happening at runtime `LibClang` would indeed be a heck of a dependency.
I'm about to finish the latest version of the book, and plan to write a review in a few weeks. Until then, I can easily recommend this book to you, so please consider this a super-brief review. To put things in perspective: this book is one of the reasons I decided to tackle learning Haskell after a few failed attempts throughout the years. Combined with the current maturity of the ecosystem, as well as tooling (e.g. Stack, Emacs integration, etc.), this book is very well positioned to get you started, and bring your understanding up to a high level. The amount of effort spent on its pedagogical approach is worth a lot of praise. I have personally witnessed how seriously the authors took reader feedback into account on many occasions. No book can be the "one and only" when it comes to such a wide and deep topic, but nevertheless this book managed to become my go-to reference for Haskell as well as functional programming. A funny note before finishing: I had already started another good book some time ago, "Functional Programming in Scala" by Paul Chiusano and Rúnar Bjarnason, went on to do the exercises of the first few chapters, but then decided it would be easier and more focused to try to understand many of the fundamental concepts using Haskell (I mean relatively easier, but don't misunderstand me, some of the exercises in "Haskell Programming: From First Principles" turned out to be pretty difficult for me, I could feel my neurons firing more than usual). The timing of "Haskell Programming: From First Principles" turned out to be perfect. One thing for sure: this book set the bar very high, not only for future Haskell books, but for any book that claims to teach a programming language starting from its fundamental concepts up to an advanced level. And I consider this to be a very good thing.
I've added support for this to `clang-pure` with an [example](https://github.com/chpatrick/clang-pure/blob/master/examples/ListStructs.hs). `test_structs.h`: #pragma pack struct Foo { long long foo; int bar[42]; const char* baz; } Invocation: &gt; stack exec list-structs test_structs.h [CStruct {cStructName = "Foo", cStructFields = [CField {cFieldName = "foo", cFieldOffset = 0, cFieldType = Type { typeKind = LongLong, typeSpelling = "long long"}},CField {cFieldName = "bar", cFieldOffset = 64, cFieldType = Type { typeKind = ConstantArray, typeSpelling = "int [42]"}},CField {cFieldName = "baz", cFieldOffset = 1408, cFieldType = Type { typeKind = Pointer, typeSpelling = "const char *"}}]}] Mapping to Haskell types is left as an exercise to the reader.
&gt; if the original site can be fixed -- let's just fix? My point is that one thing does not exclude the other. Regardless of any reliability issues with Hackage, it still makes sense for Stackage to host separate documentation builds -- for instance, so that the docs can be conveniently grouped according to snapshots, something that isn't, and shouldn't be, a concern for Hackage. Whether it is worth fixing the issues with Hackage (yes, it is!) is an entirely independent question from whether alternative sources of documentation should exist (and how should these alternatives be promoted is yet another independent question).
So you use `parseTAIUTCDATFile`? I was planning on removing that, since `tai-utc.dat` doesn't include an expiration date.
 I too found the articles / tutorials lacking (might be due to my inexperience though). I've just finished-ish a small project that does some basic scraping and outputs the results as json. I've basically used xml-conduit. I've built some 'error reporting' functionality around it too. The hardest thing to grasp was the Cursor and those dam operators. But once I did it was smooth sailing. And in the end landed with a really concise way to express the scraping 'rules' for example: parseArchDoc :: String -&gt; (Document -&gt; Either ([Cursor], String) [Cursor]) parseArchDoc alias = (\doc-&gt; Right [fromDocument doc] &gt;&gt;= extract "1" (($/ element "body")) &gt;&gt;= extract "2" (($/ element "div")) &gt;&gt;= extract "3" (attributeIs "id" "content") &gt;&gt;= extract "5" (($/ element "div")) &gt;&gt;= extract "6" (($/ element "table")) &gt;&gt;= extract "9" (($/ element "tbody")) &gt;&gt;= extract "10" (($/ element "tr")) &gt;&gt;= extract "11b" ($/ element "th") &gt;&gt;= extract "11" (contentIs alias) &gt;&gt;= extract "12" (parent) &gt;&gt;= extract "13" ($/ element "td") &gt;&gt;= extract "14" ($/ element "div") &gt;&gt;= extract "15" ($/ element "table") &gt;&gt;= extract "16" ($/ element "tbody") &gt;&gt;= extract "16" ($/ element "tr") &gt;&gt;= extract "16" ($/ element "td")) Not sure how helpful it could be but the project is over here: https://github.com/chrissound/ArchLinuxPkgStatsScraper It's actually my very first 'real world' Haskell Program, and mostly just a learning project, so I'm sure it has a lot of rough edges, feel free to ask any questions! 
One question that is probably frequent and go on the FAQ is: what about efficiency of generated code? How does it compare with GHC's backends today? What is the target for future versions? In the "minor nitpick" category, I would describe eta-reduction as `(\x -&gt; f x) =&gt; f` rather than `f x = g x =&gt; f = g`, which looks more like an eta-equality principle.
Will Eta be lazy? Also I wonder if the coming module system in GHC could be useful in these kinds of language "ports". Keep the core logic of your program the same, but swap language-specific low-level modules.
The hardest part of learning programming for me was always getting to a meaningful hello world. Once you can start iterating on that, everything else is pretty easy. Languages are a lot easier to learn than build environment nonsense. Stack defaulting to creating a binary + library, and successfully building and running by default is a huge win. I don't think that many people have some huge intrinsic motivation to learn haskell before they're able to get even a small taste of using it. Monads being hard is fun. Build environments are tedious. They're not similar types of difficult.
The alternative is to specialise `m ~ Maybe`: type LeapSecondMap = Day -&gt; Maybe Integer utcDayLength :: LeapSecondMap -&gt; Day -&gt; Maybe DiffTime utcToTAITime :: LeapSecondMap -&gt; UTCTime -&gt; Maybe AbsoluteTime taiToUTCTime :: LeapSecondMap -&gt; AbsoluteTime -&gt; Maybe UTCTime From the user's point of view, there's little difference, except that in the case that someone has constructed a total leap-second map, they'll need to use `fromJust` to extract total conversion functions. I have no idea how common this case will be.
How successful are JVM languages anyway? By success, I mean something more than giving you a nicer way to write Java, and in battle-tested production environments. Take Scala; how long can you go before you have to integrate with an Oracle jdbc library, or a messaging lib, or some solution that requires you to drop back into a Java mindset? Are there heavy-duty deployments out there that never or rarely touch "normal" Java? I won't even ask about clojure ;) The reason is this para from TFA: &gt; Moreover, using the JVM as a platform will allow Eta to take advantage of a well-engineered and battle-tested garbage collector, a whole host of Just-In-Time compiler optimizations at runtime, **and a vast ecosystem of libraries for almost any task.** I'm a longtime Java developer, and one of my favorite parts of leaving Java behind is returning to the land of decent C FFIs. Java is the worst when it comes to integrations outside of Java, and while Sun did a great job convincing the world to re-write everything in "pure Java" (what a phrase), my feeling is those days are kind of over. Java is in serious danger of becoming strictly legacy at this point, when you can simply use Go and pkgconfig and just get work done. Meanwhile, it seems to me that JVM langs sound great, and then you go to do something with industrial focus, and boom you're back in Java land, maybe with less curly braces. As an industrial Haskell programmer, I just don't buy that JVM integration is the big thing missing. So therefore, I also don't agree with this para: &gt; But we are also big fans of Haskell and we are disappointed that it's used so little in industry. We feel that Haskell can solve many big problems in software development, but there's no concerted effort in solving the infrastructure/tooling problems that are hard requirements for large-scale, industrial use. We hope that we can solve these with Eta. We have met one of the biggest industry requirements with Eta: interoperability with the Java ecosystem. What's also hilarious is Java tooling .... actually kinda sucks. Maven is a serious shoot-me-now situation. Let's just say that when the left-pad controversy hit JavaScript, I knew their pain. Production java is jar dependency HELL. Yes the IDEs are a work-of-art, but that is because without an IDE you seriously can't get anything done in Java. Does Go have an IDE?
If `f` is an expression that computes a function, `(\x -&gt; f x)` will recompute it on each application. I understand the FAQ wording as "a bit simpler and a bit more efficient", as in it doesn't really matter most of the time.
&gt; But my opinion is that those who gave up because of caball-install, or other peripheral reasons, never had the motivation needed to learn Haskell in the first place. it's not binary. the assumption that "they wouldn't learn monads anyway" (paraphrasing) is incorrect, I think, and misses some basic ideas from teaching/UX/accessability/marketing. say you're interested in software engineering, you've followed patterns of purity or type safety, but want more automated/reliable verification (like language support). you have a list of tools trying out, Haskell's one. "Peripheral reasons" are demotivating, but they're not a confession of being unmotivated. if you can't even run your code, it exceeds demotivation, it becomes legitimate skepticism: "huh, i heard good things about Haskell, but I've heard good things about 10 other things. trust but verify. i'll spend some time on writing simple programs, but if I can't even get it to run, how can I verify any of the claims? like most claims, it's likely false. i'll try the next."; "does the haskell community not care about tooling? i'd rather write a good programming language in notepad, then a bad programming language in an IDE. but, this is too much."; "if the setup pain is too great, I might just try Idris, which isn't 'ready for industry/production' either, but seems cooler"; etc. these are questions I am absolutely certain that many stillborn Haskell have asked themselves. and you know what? even though their answers to those questions are wrong (type errors are awesome, legible or not; tooling is good and sometimes very good, like stack's integration with nix; the many parsing combinator libraries like parsec and Earley are alone worth learning haskell to parse some log files for; etc), their questioning is right. I heard the same claims about Scala I did about Haskell, which I even tried learning first since I wrote Java at work. And I did in fact spend a while putting in a good faith effort to learn Scala before giving up, and then had to do the same for Haskell. Many extremely motivated programmers are not quitting because "monads are too hard", but because there's just too many tools that have been hyped. By "motivated", I mean "motivated to become a better programmer", not necessarily "motivated to learn Haskell", which is only a proxy, because they're not sure it will make them a better programmer. Personally, I was so profoundly disgusted with way I was told to do things, that I "had faith" (lol) and sunk a lot of time into Haskell. Learning the abstractions was hard but fun, other things like weren't fun but my hope brushed them away. Like me, it sounds like you had a good time learning Haskell, but there are other people on this sub, people who contribute to the community, who picked up Haskell and dropped it down many times over several years (iirc from an older thread called "why did you learn haskell" or something). And, since I did, i'm happy people are spending their efforts, almost entirely unpaid, on things like making the compiler faster or adding a new language extension. But you can't pick one alternative over another, without verifying it yourself. And you need basic stuff like installing and running to work to verify. So, of course that matters too. (btw, didn't mean to rant at you, just a thing I've heard a few times, and had to reply somewhere. as far as this thread is concerned, i agree. i doubt ETA won't find get more users due to IntelliJ integration or whatever, and people should put up with some crap to learn Haskell because it is worth it. or so I claim...) edit: also, would you have stuck with Haskell without Cabal? Before the ecosystem was cabalized, packages were built with makefiles. https://www.gwern.net/Resilient%20Haskell%20Software 
A few questions: * Does Eta conform to haskell2010? https://www.haskell.org/onlinereport/haskell2010/ * Will new extensions (e.g. row-type polymorphism, anonymous record types) be introduced as language pragmas? (e.g. `{-# LANGUAGE RowTypePolymorphism #-}`) This seems like the "right way" to introduce such extensions imo until they are incorporated into the Haskell report. * Can you give some examples of the ghc8 type system changes that have caused issues? * Will efforts be made to integrate eta with stack and/or cabal-install? There's an interesting opportunity here for hackage packages to be compiled to `.jar` files and distributed a la maven, so perhaps integrating with JVM-centric build tools might make sense for Eta.
The old operators are a holdover from when Source, Sink, and Conduit were entirely separate types, right?
&gt;You can compile the vast majority of programs that GHC Haskell 7.10.3 can also compile, with the restriction that TemplateHaskell ... &gt;In fact, it's almost identical other than the Foreign Function Interface. Sorry to be negative, but Haste has taught us that lack of these are huge issues that effectively mean very large and popular parts of Hackage are useless. 
Yes, plus showing conduit's roots in enumerator.
I see where your coming from and agree that the build/package environment can be especially frustrating. From the OP: &gt; But we are also big fans of Haskell and we are disappointed that it's used so little in industry. We feel that Haskell can solve many big problems in software development, but there's no concerted effort in solving the infrastructure/tooling problems that are hard requirements for large-scale, industrial use. I see that a lot of effort will go into ETA, and that trade offs will have to be made. Part of my point is just to remember that tooling alone will not guarantee great success for Haskell, and that this should be kept in mind when making trade offs. Personally I don't think our tooling is that bad, and I'm wondering if efforts might be better spent on improving the tooling we already have? Those doing the work are free to make their own judgements though; they know more than me. I hope they succeed as that will be best for everyone.
Haskell is an amazing language for understanding high level computer concept. It really bends your mind because functional programming is so different from standard imperative languages like C. Some more recent languages seem to be adopting concepts from Haskell so might be nice to learn in general if you have interest. I'm an ex-physicist because I really got into mathematics once I learned about computer science applications. What I like about Haskell is that functional programming is very close to how I mentally think of problems since its so closely based in logic and math. I think you will enjoy it. There's a book called Haskell data science or something like that that gives a good introduction to numerical work with Haskell, though it's meant more for statistics and machine learning but probably still useful. A general book like Learn You A Haskell For Great Good might also be helpful. This is likely not an overnight project, it will take some time to learn the language and get practice, and the numerical community is probably not as developed. If you're in a rush, you might want to use python and scipy and numpy. But if you have time it's fun to learn and one of those life changing learning experiences (at least it was for me! Opened up whole new world to me). Good luck.
I think ghcjs is the right model to follow here -- the success over things like Haste has been because its tried to be so faithful to so much and really let people tap into a wide base of packages. Note that of course ghcjs has a special javascript ffi as well, and given how java has objects so baked into the calling convention, i'd imagine that even more work may be necessary to have a proper java ffi. But that said -- nonetheless, support for TH (and ghcjs showed the way how here) feels very important, and other superficial differences in the name of "usability" will probably hurt more than help, even if in the _absence_ of a large base of haskell packages, on their own, there would have been a good case for them...
It depends on what you want to do with it. Haskell is lacking a real equivalent to numpy / scipy / pandas, and a number of domain-specific libraries are missing as well. I'm a graduate student in astrophysics, and I have started using Haskell as a replacement for shell scripts (using the excellent [turtle](https://hackage.haskell.org/package/turtle-1.2.8) library). I've also used it for plotting and scraping log files. I find it much nicer to use than python for those use cases, because the type system saves me from a bunch of debugging. But there are some astronomy-specific file formats that I can't work with in Haskell, and there are some very-high-performance things which I still need to write in C++.
Java objects have the advantage they there are two calling conventions available for them: the native one and one in C. http://hackage.haskell.org/package/jvm#readme leverages the C calling convention to making calling Java objects pretty seamless, and with reasonably low overhead.
&gt; There's an interesting opportunity here for hackage packages to be compiled to .jar files and distributed a la maven Indeed. [sparkle](https://github.com/tweag/sparkle) does just that: provide a command-line tool to create .jar files from Haskell programs. So it is possible. In that sense one can see .jar files as some kind of universal format for apps on the JVM - no matter whether your application is pure bytecode or in fact a native blob generated from Haskell source. I guess one could integrate that into the build system as you say, via the Setup.hs. I think that would be a good idea.
The wheeler talk I linked (the vimeo link) describes what sounds like a similar situation -- a gnarly ODE that needs a combination of symbolic and numeric methods to bring to heel. On the other hand, if you're at the learning phase at this point, and if the way they want to teach you is in fortran, then it makes sense to learn from the people that know how, in the argot that they speak. Once you grasp the approaches they take, you'll be more easily able to see how they translate. There's nothing magic about fortran here -- its just a setting where people are comfortable doing a bunch of matrix manipulation.
Out of the hat, here some advises: * **Do not use list if you want performances**, but use vectors instead. List in haskell are linked list, so if you traverse them a lot you will lose a lot, (and it will be not obvious to see with profiling) * Use [profiling](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/profiling.html) to look at the **cost center**. As you use stack, you can enable profiling with --executable-profiling and --library-profiling. When you have your cost center, if your function is too big try to split it, to pin point the culprit. Once you have the culprit, you can try to [INLINE](https://downloads.haskell.org/~ghc/7.0.3/docs/html/users_guide/pragmas.html) the function to speed thing up. Try to add some strictness with bang patterns. For strictness, a quick win is to use the GHC 8 extensions -XStrict when compiling your project to see if you have a major difference in perf. Lazyness can hit you when you **share the computation instead of the value** (and so you re-compute too often) or you spend **more time building a computation instead computing the value** at each step. * Use [profiling](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/profiling.html) again to look at your **memory allocation pattern**. Allocating memory is not cheap, especially during a tight loop of computation. If you are in this situation, try to pre-allocate your objects Use vector here instead of list voronoi :: [Point'] -&gt; [Edge'] points n gen is not strict here, so you are computing it inside voronoi voronoi $ points n gen Use criterion to accuratly mesure voronoi
Right -- that works great for calling from GHC proper. But does it make sense for invocations happening within a haskell-on-the-jvm like Eta? 
&gt; Take Scala; how long can you go before you have to integrate with an Oracle jdbc library, or a messaging lib, or some solution that requires you to drop back into a Java mindset? Maybe you'll have to do it right away, maybe never. It's not like there are no idiomatic Scala database libs and such. 
This piece of code is a little bit convoluted ys = fmap ((id &amp;&amp;&amp; id) . snd) vertices ... (minY, maxY) = foldl1 (\(a,x) (b,y) -&gt; (min a b, max x y)) ys it can be written as ys = fmap snd vertices minY = minimum ys maxY = maximum ys -- or if you like arrows (minY, maxY) = (minimum &amp;&amp;&amp; maximum) (fmap snd vertices) In following part of your code, you are both filtering list and mapping function over it concatMap (\(Edge' _ _ l r) -&gt; if l /= (0,0) &amp;&amp; r/=(0,0) then [line [l, r]] else []) edges it can be written as fmap (\(Edge' _ _ l r) -&gt; line [l, r]) . filter (\(Edge' _ _ l r) -&gt; l /= (0,0) &amp;&amp; r /= (0,0)) $ edges -- or with list comprehension [ line [l, r] | (Edge' _ _ l r) &lt;- edges, l /= (0,0), r /= (0,0) ] That way it is more clear that you are picking some Edges and then converting them to lines. I think that package Diagrams depends on package Linear internally so you would be better off with `V2 a` instead of `(a, a)`. For example (centerx, centery) = (/n) *** (/n) $ foldl1 (\(x, y) (a, b) -&gt; (x+a, y+b)) vertices -- would become V2 centerx centery = foldl1 (+) vertices / n Main function contains some redundancies like `colors' n = colors n` or calling `read n` twice instead of using `read &lt;$&gt; readLine :: IO Int`.
I think for haskell-on-the-jvm they can do better. As you say, GHCJS-style FFI. Makes a lot of sense to me.
Granted, my experience is somewhat limited, but using Java libraries in Clojure was not at all as painful as you probably imagine it to be. Even if Clojure didn't have any libraries of its own, writing Java code with Clojure idioms would be vastly preferable to most other languages you can come up with. It's not to be understated. I imagine Haskell-on-JVM isn't much worse. Especially considering GHC with a JVM backend already gives you access to most Haskell libraries. Remember Clojure and Scala are new languages which had no libraries on their own when they were created. Haskell has a wealth already! 
I think they should choose PureScript and rebranded it as Eta instead. PureScript better matches the JVM runtime model (strict) and laziness is a tough sell not just for performance reasons but for correctness reasoning. PS already has anonymous record types and row polymorphism. It doesn't have the cruft of Haskell, defaulting to more efficient data-types for strings and arrays from the start. It also has a compiler that is small and simple. It is also notable for reaching Scala developers in a way that Haskell has not been able to. It has been successfully [compiled to C++](https://github.com/andyarvanitis/purescript-native). The only thing wrong with PureScript, as a thing that you could sell, is its name. Like "JavaScript", it sounds like a "throw away" language, or one which is going to target JS only. Really, if I was TypeLead, I would have gone for PureScript. But it would probably be hard to abandon their GHCVM work.
I'm using it for my PhD, but the ecosystem for numeric programming is not the best. There is a severe need for library bindings. Also, given the fact that every single numerical application has type class restrictions (namely `Storable`), you lose all the type classes. Something like a `Prelude`-compatible `SubHask` would be great. But I still prefer using it than other options, I think the future will be bright.
It's not really a problem of the language, but of the underlying libraries. There is a lack of a good common numeric base library, like in Julia or Python. There are a few libraries, but they are mostly incompatible with each other. Also relevant xkcd: https://xkcd.com/927/
In my experience Haskell is really great for pure-mathematics type symbolic / combinatorial computations (and thus probably also for theoretical / mathematical physics), with the obvious caveat that on the ecosystem side Haskell is (so far) missing most of the capabilities of a big a symbolic algebra system like Mathematica. While this is "only" a question of libraries, big CAS systems have many thousands man-years of work put into them, thus we cannot expect this gap to be eliminated on short-medium term, except for some specific subjects. However working in a typed environment and with good syntax is so much better, often I feel it's even worth to do what you can do in Haskell, and then generate say Mathematica code for the final step(s) which Haskell cannot do yet. 
What's wrong with this? Prelude Data.Aeson Data.Text.Encoding&gt; :t decodeStrict . encodeUtf8 decodeStrict . encodeUtf8 :: FromJSON a =&gt; Text -&gt; Maybe a 
Where are you getting this JSON as text from? Most sources (e.g. network sockets and files) will quite happily give you a ByteString. Unless you are transcoding this from something other than UTF8 there wouldn't be any point in going via Text.
I am using Reflex FRP and it like to pass things around as Text
&gt; But does this mean that Stackage only contains a (stable) subset of packages and it might happen that some of them can't be found there? That's right. The latest [Stackage nightly](https://www.stackage.org/nightly-2016-11-13) contains 1931 packages. Hackage contains over 10,000 (according to [this site](http://www.modulecounts.com)). There's also a difference between the Stackage LTS and the Stackage nightly. For example, I'm currently using the nightly because I want to use [Aeson 1.0](https://hackage.haskell.org/package/aeson), but the latest Stackage LTS is still on Aeson 0.x.
All in one conversions for anything you may need (ass long as you are using UTF-8): https://hackage.haskell.org/package/string-conv-0.1.2/docs/Data-String-Conv.html 
&gt; Stackage contains only packages that build together without issues. This is technically correct, but might give the impression to someone reading too quickly that non-Stackage packages have build issues. "Stackage contains a subset of packages known to build together without issues" would perhaps be clearer.
&gt; Stackage contains only packages that build together without issues More specifically, stackage contains only a small fraction of the currently 73941 releases (10492 packages) you can find on Hackage. And [we](https://github.com/haskell-infra/hackage-trustees) try to make sure that Hackage packages work together out-of-the-box without having to rely on efforts like Stackage. Generally there's very little such issues nowadays, but for the event that you are able to run into packages having issues building together, you ought to file a bug report at the package's issue tracker, and/or [here](https://github.com/haskell-infra/hackage-trustees).
Good to know, maybe every hackage package page should contain a link to the hackage-trustee's github page, so that users know that they can report package build issues there.
You should consider `text-conversions`. It does an honest job when decoding UTF-8.
&gt; What's also hilarious is Java tooling .... actually kinda sucks. I think your knowledge of Java tooling is about 10 years out of date. 
&gt; But does this mean that Stackage only contains a (stable) subset of packages and it might happen that some of them can't be found there? Yes, but note that it isn't an either-or situation. For instance, if you are using Stackage through [Stack](https://docs.haskellstack.org/en/stable/README/) it is easy to use a Stackage snapshot and supplement it with per project installations of non-Stackage packages, so that you can still use pretty much anything you want and, in spite of that, any eventual build issues with the non-Stackage packages will not affect your whole Haskell environment. &gt;In conclusion: new packages should only be distributed over Stackage, right? No. Beyond what others have said, it is worth emphasising that there are many entirely non-problematic packages that aren't on Stackage merely for circumstantial reasons. Packages also sometimes get temporarily dropped from the latest Stackage snapshots until build issues are ironed out -- issues that don't necessarily affect all users (for instance, Hakyll, a very popular package, will only make it to the 7.x LTS snapshots now, after more than two months, due to what I would describe as unlucky timing of some version bumps). In both of these cases, it can be very useful that packages are available on Hackage. 
&gt; In conclusion: new packages should only be distributed over Stackage, right? No -- I am not even sure that is possible. I have packages in stackage. When they need updates to fix build issues, I patch them and upload them to Hackage. Then stackage pulls them from there. The most important aspect of stackage is that it is a social contract. By putting packages in stackage, you agree to update them in a timely manner. Fortunately, those updates go back to hackage, so every benefits even if they do not use stackage directly. Another benefit of stackage is providing guidance on *when* to update your package. Let's say a commonly used library that my library depends on updates with a breaking change. Do I update my library right away? If I update too soon then people will be able to install my library, but might not be able to install other libraries they need that have not yet been updated. If I wait too long, then I might be the one holding up the process. With stackage, this effort is coordinated so that everyone switches at approximately the same time. Once again -- everyone benefits, since the changes are uploaded to hackage. The decision to include things in stackage has little to do with packages being new vs old and everything to do with how much you care about keeping them updated. I put all my web development libraries on stackage because it is important that they be up to date, so I am willing to put in the extra effort. The library I wrote for generating minecraft maps that nobody uses? That is only on hackage. I should note that I don't even use stack or stackage myself. I think the effort to coordinate developers and notify them when their packages break is worthwhile and useful. But I am not really convinced that the rest of the technology is the right solution. 
From the docs, [cpure](https://hackage.haskell.org/package/distributed-closure-0.3.3.0/docs/Control-Distributed-Closure.html#v:cpure) looks promising. Try this: intDict :: Dict (Serializable Int) intDict = Dict closureIntDict :: Closure (Dict (Serializable Int)) closureIntDict = closure (static intDict) closureInt = Closure Int closureInt = cpure closureIntDict 42 
&gt; using Java libraries in Clojure was not at all as painful as you probably imagine it to be. Oh no, that wasn't my point: coding Java in Clojure is way better/faster than Java itself, and I would imagine Scala too. My question was how much of production JVM coding is precisely this coffeescript-like use-case, and how much actually leveraged mature toolsets in the "native" Clojure/Scala/Kotlin JVM lang. 
Best to submit this question on the project's issue tracker. The authors (among which me) might not spot it on reddit. Fortunately, /u/andrewthad gave exactly the right answer. :) /u/andrewthad I think your example code would be a great addition to the project's Haddock's, if you're up for submitting a PR. Just to clarify, the code in the README isn't meant as a standalone example, just an illustration of a use case assuming a Cloud Haskell like communication framework (but distributed-closure isn't CH specific in any way).
Well, that's kind of my point -- Java forced me to leave Emacs circa 2005 -- it was really the refactoring tools that did it, "Create Method" and "Extract Variable" were simply too useful. I did attempt to shoehorn the Eclipse incremental compiler into my elisp environment before I gave up the ghost. Coming to Haskell and returning to Emacs has been a joy.
Sure, I was doing immutable this, functional that all over the place. But that's a style/library. I assume with Java 8 things are far easier now. 
Not really. Java has a tradition of taking well-developed computational concepts, removing all the useful bits and implementing what's left. Java 8 was no exception.
Looks great! Nice and readable code. One small personal gripe: There's a lot of files in your /library folder. Maybe you could move some things into a topical namespace. Just from the name I wouldn't know what a RattleTrap.Mark or RattleTrap.Attribute is. I'm not good at naming but perhaps something like RattleTrap.ReplayData.Mark or something. edit: Also, maybe those datastructures should be in a whole different package, exposing a generic namespace like RocketLeague.ReplayData or even caries Network.RocketLeague.Replay. I get the feeling that's sort of what many Haskell packages do. Obviously it's not really consequential, just interesting thinking about it..
&gt; edit: Also, maybe those datastructures should be in a whole different package, exposing a generic namespace like RocketLeague.ReplayData or even caries Network.RocketLeague.Replay. I get the feeling that's sort of what many Haskell packages do. Obviously it's not really consequential, just interesting thinking about it.. [*pandoc*](https://hackage.haskell.org/package/pandoc) is a high-profile counterexample. Also, there are [varying opinions](https://www.reddit.com/r/haskell/comments/557gwl/numeric_numerical_math_data_etc_or_how_to_browse/) on how useful taxonomy such as `Network.*` is.
Yep. There are no promises of backwards compatibility between `lts-6.*` and lts-`7.*`, only between `lts-6.x`and `lts-6.(x+n)`.
No, my impression of Hackage trustees is they get involved to fix minor issues or as a last resort; first contact the package maintainer, not the trustees. 
What a great thread. Learned a lot!
I too found it unsatisfactory until some time ago, but eventually learned to use it. Datastructures under `Data`, functions under `Network`, or `Numeric`, or `Control` or whatever, according to their purpose. I think that committing to a standard that just prescribes root names is not a big burden.
&gt; Even if Stackage is the obvious solution now, it might not be in 2-3 years. This might be hard to tell but how will it look like in the next years? Will Hackage and Stackage co-exist like today or will Stackage become a successor platform that is independet from Hackage?
In Haskell, indentation is significant if explicit braces and semicolons are not present. Do you have a specific question you're trying to answer by getting the Java translation?
In my experience, the overhead is minimal. I am not sure I have ever had to make any changes specifically for stackage. Supporting stackage mostly comes down to a matter of timeliness. You have to be willing to update your packages within a few days of them breaking. Occasionally you have to submit a small github pull request. For packages that I plan to maintain for the longterm anyway, it is no big deal. I put my packages in stackage as a signal that I am willing to be a responsible maintainer for those packages. For silly things like my minecraft package -- not so much. If you write too much Haskell code, eventually you spend all your time updating your packages and not enough time writing new code :(
There was a post with comments about what this means (and what TIOBE *doesn't* mean) a week ago ([*TIOBE: November Headline: Is Haskell finally going to hit the top 20?*](https://www.reddit.com/r/haskell/comments/5bmanm/tiobe_november_headline_is_haskell_finally_going/)). It would be nice if an increase in popularity led to some humble ecosystem improvements, as long as we don't lose the magic.
&gt;TIOBE is not the end all be all That is a very generous assessment. See also: [*Haskell not in Tiobe top 50 for the first time in many months :-(*](https://www.reddit.com/r/haskell/comments/24qc14/haskell_not_in_tiobe_top_50_for_the_first_time_in/?ref=share&amp;ref_source=link) (May 2014); [*TIOBE: November Headline: Is Haskell finally going to hit the top 20?*](https://www.reddit.com/r/haskell/comments/5bmanm/tiobe_november_headline_is_haskell_finally_going/?ref=share&amp;ref_source=link) (November 2016).
Keep it under wraps, we want to “[avoid success](https://www.reddit.com/r/haskell/comments/39qx15/is_this_the_right_way_to_understand_haskells/) ^((*at all cost*)^) ” donchaknow
Hackage is a community-based social contract, and not subject to the same level of open competition as Stackage. Hackage would have to break badly, or community sentiment would have to radically move for this to happen.
| and then generate ... code The holy grail for numerical haskell is to aim for the same api for both native computation and non-haskell code generation. Design and test in haskell, knowing we can type-safely drop down into high performance. I'm not aware of any other language that this can even be expressed in. It's our natural advantage that can put us ahead of the mega-year CASs. 
I use https://github.com/crufter/haquery, it's the most simple and wraps around tagsoup.
Yeah, it's the removals that are the main problem, but I didn't know of a better way to visualize that off-hand. But I'm surprised by the 302 removals number. That seems like an uncomfortably high number for a supposedly stable package set.
Every time I try to learn a new language I start by trying to get it to compile in any IDE. I always waste hours on it every time. It feels like the hardest part about learning a new language is to get it to compile anywhere. Is this usually any easier on Linux? Sorry to blow off some steam but I'm mentally exhausted...
I don't think it's quite as cut and dry as you make it sound. In this hell you have to manually list the exact version of every package you want that is different from what is in stackage. With cabal's dependency solver you don't have to do that. Dependency hell is a real thing whether you're using stack or cabal or anything else.
I was looking for a way to find the size of an object in GHC a couple years ago, and I couldn't find a good one. One reason is that Haskell uses `thunks`: you need to evaluate all the thunks (i.e., deepseq) to make the whole object inside memory. I don't remember the details, but even if you do deep-seq, there are other technical difficulties.
I'm aware of two query-able data on this front, but neither is variable-specific. 1. The `getGCFlags` function: https://downloads.haskell.org/~ghc/latest/docs/html/libraries/base-4.9.0.0/GHC-RTS-Flags.html#v:getGCFlags You could use that to query limits that were set via the `RTS` flags. 2. The `getGCStats` function: https://downloads.haskell.org/~ghc/latest/docs/html/libraries/base-4.9.0.0/GHC-Stats.html#v:getGCStats You could query that to access general stats about the current heap. There are `C`-level hooks for handling OOM situations: https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/runtime_control.html#hooks-to-change-rts-behaviour --- but I don't see how those could be used for recovery. Lastly, there's the async exceptions GHC throws when OOM. https://downloads.haskell.org/~ghc/latest/docs/html/libraries/base-4.9.0.0/Control-Exception.html#t:AsyncException *BUT*, the docs say GHC currently doesn't through HeapOverflow. So, that doesn't seem promising for you to use right now either... I suppose the only immediately actionable idea based on the above is: you can use the queries to determine if you're nearly-OOM, so --- if you already know which of your data structures are most likely to be the large ones --- you could then could act on the fact that you're nearly-OOM. HTH.
Object-oriented, imperative are the two dominant paradigms now. Both have been very well-developed. There's also aspect-oriented programming, which so far has not gone very far. Probabilistic programming is really cool and there is actually some interesting stuff going on in Haskell with that. Quantum computing has its own paradigms but then that's kind of different. 
There's [`weigh`](https://hackage.haskell.org/package/weigh). There's [`unsafeClosure#`](https://hackage.haskell.org/package/ghc-prim-0.5.0.0/docs/GHC-Prim.html), but its power is exceeded only by its mystery. Nothing as high level or convenient as you are after, I'm afraid.
Arguably, concatenative (aka stack-based) is meaningfully distinct from functional. Also arguably, dependently typed programming is different enough from untyped functional programming that they should be considered different paradigms.
https://www.info.ucl.ac.be/~pvr/paradigms.html
in spite of the name, the scope of probabilistic programming seems narrow enough to be regarded as a "DSL/EDSL paradigm". the application space is large, but what you need to express is fairly narrow.
Category theory certainly has a higher level of abstraction.
Great example of Java removing the useful bit :) Abstract on the flatMap and map call to remove infinite code repetition. Woopsie you can't in Java. 
Sorry, unpack* closure
it seems like no one read more than the title. :\ unfortunately, I know of nothing more abstract, except perhaps something like automatic program synthesis from specifications, which hardly counts as a paradigm, really
Not for calls between Eta &lt;-&gt; Java, but it could provide an alternative route for existing packages that rely on C FFI along with possibly a fragment written in C. This is not ideal on the JVM, since it ruins byte-code portability, but it's better to have alternatives nonetheless. 
I guess the "get it done" approach is to use a mutable `Unboxed` or `Storable` vector as an underlying memory space and resize that appropriately whenever necessary. This works especially well if you are dealing with lots of identical "objects". This kind of manual memory management is not too uncommon in other languages (eg Java on Android).
Probabilistic programming is quite high-level and different: http://mc-stan.org/
This is really interesting. Particularly this graphic: https://www.info.ucl.ac.be/~pvr/paradigmsDIAGRAMeng108.pdf I often like to explain Haskell concepts using Excel, and I see they're in the same column. Describing Haskell and OCaml as, respectively, *ADT functional programming* and *ADT imperative programming* is also interesting.
There's an issue with the certificate of the website. I hesitate opening it. 
Some businesses like to stick to what they know. They may have very profitable applications running on what we would consider legacy platforms. Heck, they may still be running on Java 6. Being able to integrate with the existing platform against low risk can make all the difference.
I disagree
Numbers 1 and 6 seem more like features of the languages than pitfalls. And either way, they're not specific to physics or math. As for 2, the parallelism support is pretty good but it could be a lot better, especially given the nature of the languages. 
It is about all pitfalls I have faced when dealing with Haskell. Laziness can be a problem because it is hard to spot space leak and reason about performance if you don't be careful like in the foldl case. Some Physics and engineering problems requires high performance calculations like fininite element methods. IDE and tools are a pitfall too. I forgot to say low IDE support, but Emacs solves it for me and dependency management is solved by stack tool. It is not only about the language. IDE, ecosystem, tools, ffi and libraries matters a lot. The monad curse is only a joke.
Looks like it's missing the root certificate, number 2 here: https://pki.cesnet.cz/en/ch-tcs-ssl-ca-2-crt-crl.html. If you add that, it should work, I believe. This should have been bundled with the site's certificate, but isn't. There's nothing nefarious about this as far as I can see (and Chrome accepts it because it has the relevant root certificate, I presume).
As a fresh newcomer to haskell i proudly pass the famous monad barrier 8] Now i'm leading to master lazy evaluation and many will follow me, Haha! :D
&gt; Maybe I was impatient but I couldn't find anything quite like this on Hackage and, lo, here we are. I've been using [authenticate-oauth](https://hackage.haskell.org/package/authenticate-oauth) with [http-conduit](https://hackage.haskell.org/package/http-conduit) to do two-legged oauth: {-# LANGUAGE OverloadedStrings #-} import Data.ByteString.Lazy (ByteString) import Network.HTTP.Simple import Web.Authenticate.OAuth get :: IO (Response ByteString) get = httpLBS =&lt;&lt; sign "http://httpbin.org/get" where sign = signOAuth myOAuth emptyCredential myOAuth = newOAuth { oauthConsumerKey = "john" , oauthConsumerSecret = "blab" } 
concatenative languages is an interesting programming language family featuring a different coding "paradigm" . Factor (http://factorcode.org/) is such an example of concatenative languages. 
This is partially why a lot of people advocate becoming familiar with a "real" text editor (Vim or Emacs, for example, or Atom), because often you can find plugins for working with the language you want (it's a bit more flexible), but you don't have to start from scratch with an IDE that may not have features you want, or has some learning curve of its own. I don't think that is quite what you're looking for, but that's how a lot of people here do their work, just with varying degrees of complexity. Some people literally have two windows open like you mention, one for editing, one for compiling to get any warnings, etc. Some people get rid of the extra window by using plugins that will automatically typecheck or compile their code by piping it to a separate process and then printing results in the editor window. It may seem clunky, but especially when you're first learning the language, having something with syntax highlighting (I think notepad++ has Haskell syntax) and a separate command line open is probably fine to get your feet wet, and you can worry about a better setup afterwards, rather than trying to tackle two issues at once. If you do give Leksah a whirl, I think one of the maintainers occasionally browses this subreddit, so you might be able to get answers if you run into any specific issues, but I can't guarantee that, obviously. I feel your frustration, lack of an IDE that's quick and easy for newcomers to setup is something often discussed, and a lot of progress has been made over the years in that direction, but it's just not quite to that "download this app, install, have fun programming in Haskell" experience yet. I don't know that it's necessarily easier on Linux, but I'm fairly certain (no hard numbers) that at least in the Haskell community, Linux is probably the OS used by a majority of developers, and so a lot of tutorials and whatnot is geared towards Linux users. If you're not already familiar with Linux, then it is an extra thing you'd be trying to learn at once, not that it isn't worth it.
Glad to see the continued effort, yes. But very sorry to see it forking away from being a GHC backend for standard Haskell. I hope someone will pick up the lead on that.
Yes.
What would a "category theoretical" programming language look like? Has any been made?
Maybe this is a failure of imagination, but I don't imagine there could be anything more abstract than logic programming; you literally just specify what you're looking for and you obtain it.
Haskell is undeniably increasing its share in enterprise. More and more companies are using it in production. I think the breakthrough can be attributed to stack, stackage and mature web ecosystem. Having said that TIOBE is too unscientific to use it as a reputable source.
A good measure for business usage in the UK is IT JobsWatch. http://www.itjobswatch.co.uk/jobs/uk/haskell.do 
You didn't really say which part you needed help with. Maybe reading about my adventures doing the same might help? http://gelisam.blogspot.ca/2014/07/homemade-frp-study-in-following-types.html
The Hackage trustees are volunteers who work without pay. There's no point in creating work for them if the package maintainer can be reached and fixes the problem.
I hope no one will use the evil function `fromJust`. Users of `utcDayLength` will probably either use `fromMaybe normalDay` if they don't care, or keep the `Maybe` if they do care. Users of the other two will be quite rare, and those who do use them probably do care and want the `Maybe`.
I prefer the following variation: data LeapSecondMap = LeapSecondMap { lsmStarts :: Maybe Day -- Nothing means since forever , lsmExpires :: Maybe Day -- Nothing means forever , lsmLeapSeconds :: Day -&gt; Integer } utcDayLengthMaybe :: LeapSecondMap -&gt; Day -&gt; Maybe DiffTime utcDayLength :: LeapSecondMap -&gt; Day -&gt; DiffTime utcDayLength lsm = fromMaybe normalDay . utcDayLengthMaybe lsm utcToTAITime :: LeapSecondMap -&gt; UTCTime -&gt; Maybe AbsoluteTime taiToUTCTime :: LeapSecondMap -&gt; AbsoluteTime -&gt; Maybe UTCTime I also propose that whatever we do, we include plenty of human language documentation and do not rely on people figuring it all out by looking at the types. EDIT2: Removed EDIT about supporting UT1 and pre-1972 UTC. If their relation to UTC is linear and not stepped, then forget it. And I *really* don't understand why we would want to give `UTCTime` any semantics other than proleptic extension of the current UTC both forwards and backwards in time.
Got it, thanks.
Well the delete method is basically deleting a node from the binary tree. Splay makes x become the new root, (ANCHOR.left == root). I have to return true if deleted, and false if it is not. While join joins two lists together, thats why I did u.left and u.right, because its joining two lists without the node (well, thats my thinking).
If you want to do functional-esque things in Java, you should get used to returning a new version instead of mutating. Even better, you could have delete() return a builder (quasi-equivalent to laziness), so that multiple delete operations can be fused together efficiently. 
The assignment was to return true if deleted, false if not. If this wasnt a requirement, Im sure there are numerous ways to doing it. However, is my logic behind my method correct/where did it go wrong because it is not properly deleting:L
Yeah the google thing is annoying. Can you provide a version that takes you to the latest *hackage* docs, for those of us who prefer that?
Oh I agree that IDE is a problem, but it's more haskell in general. And the ecosystem is weird, because it's hard to keep the pace of progress that Haskell libraries have - other languages don't have that problem. 
If you are using this fairly standard definition of a BTree: data BTree a = Empty | Node (BTree a) a (BTree a) then I don't understand what you mean be `join`. You can't create a `Node` without specifying a value. And if your code for `splay`, using the above `BTree` type, is: splay :: a -&gt; BTree a -&gt; BTree a splay x t = Node t x Empty then it's hard to understand your use of splay, both in Haskell and in Java. For example, your code where t'@(Node tl y tr) = splay x t is the same thing as writing where t' = Node t x Empty tl = t y = x tr = Empty so you can eliminate all that, and your code for `remove` boils down to just remove _ Empty = Empty remove _ t = join t Empty Could that be what you want? You never even examine the value `x`. Bottom line - please show us the code for `BTree`, `splay`, and `join`. If you don't have those in Haskell but you do have them in Java, then paste those. If you don't have those and need to write them, then let's go back a step, give us the exact specs and we'll work on those first.
In Haskell we don't use mutation, so a function `:: Ord a =&gt; a -&gt; Btree a -&gt; Bool` would do nothing at all to its second argument. If your Java assignment asks you to modify your argument then there will not be an easy Haskell analogue.
I'm a bit confused about your expectations for `printStrLn`. What were you expecting? The output you report looks just like what I would expect. Different threads sending data to the same fd simultaneously - you get a jumble. If you want to impose ordering, do that with `Chan` or a `TQueue` or some other ordering abstraction. 
I don't think I have ever in my life relied on the thread safety of data IO functions. It just feels wrong to share an IO channel between multiple threads. I can't imagine a situation where doing this wouldn't make it all too easy to shoot your foot down the line, when you decide to use another policy for caching data, or simply wants to add an "if" somewhere where there wasn't one.
&gt; It just feels wrong to share an IO channel between multiple threads. Isn't that the exact purpose of channels?
As /u/Saturday9 already suggested, *term rewriting* (which means applying rewrite rules not only at compile time - like you can do with GHC, - but also in runtime). Wolfram Mathematica has at least elements of rewriting and it's a primary paradigm for [Pure](https://purelang.bitbucket.org) language.
Except the premise is flat out wrong. This has nothing to do with thread safety. It's all about the default buffering mode. Even without buffering it is still perfectly thread safe, it just doesn't output each line atomically. Calling putStrLn from multiple threads will not cause an error (as he demonstrates for us). The title of the section should be "I dislike the default buffering mode for putStrLn", not "putStrLn is not thread safe". It implies that putStrLn is non-reentrant or something. EDIT: After thinking about this more it has nothing to do with the buffering mode. But I still do not agree there is any way sensible to make putStrLn atomic since it doesn't own the "stdout" resource. It should be the responsibility of the developer as in the comment above.
Well absolutely, I'm under no illusions, but the choice of language and platform is not entirely at my discretion!
&gt; Except the premise is flat out wrong. This has nothing to do with thread safety. Thread safe means that you don't have race conditions. Garbled output due to two threads trying to print some text at the same time is the _perfect_ example of a race condition. What's so valuable about multiple threads writing single characters at a time? How is garbled nonsense in any way not an error?
My first thoughts on the putStrLn rant: 1. I've seen this myself, it surprised me a little for a few seconds, then I realized that this was actually good thing, because it is *exactly* what I should have been expecting. I find it pretty amazing that we can do this - send strings to stdout from multiple threads and have them come out with single-character granularity. 2. This has nothing to do with thread safety. `putStrLn` is threadsafe; it does exactly what we're asking it to do, there is no state bleeding between threads, there are no deadlocks, no output is eaten, it doesn't crash or throw, it is completely reentrant. 3. The solution to this problem is not finding the right output buffering mode, or fiddling with obscure details of the buffering implementations of various IO functions; the elegant solution is to consolidate all stdout writing into one thread, and use something like a Chan to have other threads send messages there. This is actually much better than line buffering, because you get to group output arbitrarily, rather than relying on newlines for grouping, which is really really useful for logging, because now you can safely write multi-line log messages and have them come out in one piece.
The use of the word "channel" is a bit misleading here, because when you say "channel", most Haskellers will immediately think of `Chan` or `TChan`, but you seem to be talking about file handles / file descriptors.
I have not experienced the aforementioned `Chan` issue, but I'm not surprised. I've had this same issue with sockets. This situation is endemic when utilizing shared resources. Michael's solution works nicely, but I think I'm okay with the currently unsafe default, just as the `network` library defers to its unsafe roots for performance.
What's so great about writing single characters at a time from threads? How often does *that* happen? And MVar locking _per printed character_?! This has to do with thread safety because two print function calls from two threads depend on timing to achieve a correct result. That's the definition of a race condition. The bleeding state is the current letter of the string that you're printing. The solution remains to be discussed, but start from something sensible, please. For what it's worth, [hPutBuf](https://hackage.haskell.org/package/base-4.9.0.0/docs/src/GHC.IO.Handle.Text.html#hPutBuf%27) is perfectly capable of writing, blockingly, wrapped in a takeMVar per handle via wantWritablHandle (preventing any other thread from writing to the handle), the complete string provided until completed, regardless whether it's line or no buffering. So there's that.
It should do what `hPutStrLn` does which is to write in block based on the buffer size: https://hackage.haskell.org/package/base-4.9.0.0/docs/src/GHC.IO.Handle.Text.html#writeBlocks [This is how stdout is supposed to work.](http://stackoverflow.com/a/10904459/334632)
&gt; What if the strings passed to printStrLn are infinite? I think that lazy IO is a worse mistake than what we're all discussing at present. The answer should be: if you want to print infinitely, run putStr a lot. 
We can't know what the underlying block size is. So if we don't support LineBuffering exactly the result remains similarly defined. Both lead to some unknown interleaving of the outputs. So you can't really rely on the result thus you shouldn't do it this way.
In the case of [bytestring's hPutStrLn](https://hackage.haskell.org/package/bytestring-0.10.8.1/docs/src/Data-ByteString-Char8.html#hPutStrLn) it's hardcoded to 1024, which I think is reasonable.
I think this is just a question of defaults. Having granularity and options is good, but Michael seems to think (and I somewhat agree) that the default buffering isn't what you would expect when you write the first, simple example he gave.
I can see splitting read/write in two Threads for one socket making sense but thats about it I guess?
That won't work reliably. Even all POSIX systems or Linux systems. You're going to have to do FAR better to get a reliable definition. POSIX uses PIPE_BUF for the size that can be 'write'ed atomically. My point, and I'm going to end here, is you really have to consider these things with care you don't seem to be taking. If you're going to write reliable multithreaded systems the edge cases aren't "ridiculous" they're the core of what you're doing.
Not just an `MVar` locking per printed character... a *syscall* per printed character: $ cat test.hs; strace -f runhaskell test.hs 2&gt;&amp;1 1&gt;/dev/null | grep 'write(1, .*, 1)' main = putStrLn "hello, world" [pid 6984] write(1, "h", 1) = 1 [pid 6984] write(1, "e", 1) = 1 [pid 6984] write(1, "l", 1) = 1 [pid 6984] write(1, "l", 1) = 1 [pid 6984] write(1, "o", 1) = 1 [pid 6984] write(1, ",", 1) = 1 [pid 6984] write(1, " ", 1) = 1 [pid 6984] write(1, "w", 1) = 1 [pid 6984] write(1, "o", 1) = 1 [pid 6984] write(1, "r", 1) = 1 [pid 6984] write(1, "l", 1) = 1 [pid 6984] write(1, "d", 1) = 1 [pid 6984] write(1, "\n", 1) = 1 
Conal Elliott has written several papers on how he has implemented Fran: * http://conal.net/papers/icfp97/ * http://conal.net/papers/new-fran-draft.pdf * http://conal.net/papers
This might work for you: http://hackage.haskell.org/package/sqlite-simple
&gt; [leave] putStr fast. A syscall per character is not fast!
The single-char writes only normally happens when the handle is set to unbuffered. With line or block buffering, the ghc runtime manages much longer writes with no changes to laziness behavior. Line buffering tends to be the default, but ghci (and thus also runhaskell) uses unbuffered by default. (Line buffering does tend to prevent a mess when multiple threads are adding data to the buffer concurrently.) Single-char writes can be surprisingly expensive. Over a ssh connection, such writes often result in many separate packets, which increases bandwidth use by some large multiple. I mostly see this when a remote ghc (which uses unbuffered output for some reason) prints a large type error and it *crawls* over a slow connection.
One common case is that either thread can fail, and print an error message. If two fails manage to fail at the same time, a scrambled mess is printed. Easy to accidentially end up there in my experience.
Michael's articles tend to have an inflammatory element to them; often in the titles. This post concerns two rather specific and unimportant details, yet the title implies considerable weight, and the message is wrongly inflated to the order of Haskell's concurrency as a whole. As a result, the comments in this thread debate these trivial matters in quite a heated fashion. I'd urge everyone to take the article for what it's worth (a valid criticism on minor details) rather than for what it claims (which is IMO greater scope than the evidence supports).
I pretty explicitly call out in the post that I'm looking at strict `Text` values, and acknowledge implicitly that `String` is a lost cause.
Why can't it? `stdout` contains an `MVar`, it's entirely possible for `putStrLn` to hold that `MVar` until it's completed its work.
Again, it doesn't control stdout. You could use a lock around stdout (like an MVar) but even then any IO function (which could potentially call into C code) could ignore that MVar and write to stdout. It just can't be controlled. At the end of the day if you want to serialize output to putStrLn it's in your hands as a developer. I'd personally rather it didn't pay the performance penalty of an MVar automatically and let me make that choice (especially since it doesn't buy you any guarantees).
Now I don't know what we're arguing about: the file descriptor with the number 1, or the `stdout` value made available by the Haskell libraries. For the former, you're correct, but it's not really anything I'm arguing about. For the latter: we already have an `MVar`, and there's no reason why we can't expand the scope of the lock we're already taking so that the entire payload is written to the buffer instead of only one character at a time (or the payload separate from the newline).
Thanks for adding this information! I learned something. So there are a few surprising things here: * GHCi and GHC differ in their default behavior for I/O * GHCi, in particular, chooses a pretty surprising default! I could argue for GHCi's behavior in isolation. I'm sure there are some pretty clever demos that you can do with laziness there, and buffering would get in the way. But the fact that it adds a gratuitious difference between GHC and GHCi makes the decision questionable...
Java, for example, does not guarantee that an entire string printed to stdout is sent atomically. In practice, it tends to be true for strings of reasonable length just because the runtime buffers stdout by default, and also because Java tends to swap threads in its runtime far less often than Haskell. The converse is guaranteed, though: writing a newline character forces a flush, so a println will never return until its contents have been written (but print might...). You're right that this behavior is problematic for logging, which is why important logging, especially if it will be parsed by machine, is usually done using a logging library such as log4j that provides stronger guarantees (as well as more flexible options for output).
try [intero](https://commercialhaskell.github.io/intero/) for emacs. it locks you into stack, but it's the only IDE that builds and then "just works", and I've tried them all.
I dug a little deeper into the source and I see what you mean, an MVar is already being used. So I now see what you mean by expanding the scope of the lock. I appreciate your reply. BTW - I thought the article was great I just thought the title was a little off. putStrLn is not thread-safe threw me off because to me usually not being thread safe implies non-reentrant code. In fact there are cases where you might desire the current behavior.
NoBuffering makes sense for ghci. Seems like runghc should behave as similarly to a compiled program as possible though. ghci uses it to get an incremental display, eg: ghci&gt; 1: sort [2..10000000] [1
No one is opposed to fixing the original! It just appears to be non-trivial to keep it up and running. People are working on it, and have fixed this latest snafu already.
The `bool` function is fine. Some of our developers use it all the time. No one has ever gotten shot in the foot.
Wouldn't the "reasonable size" have to be a rather arbitrarily chosen magic value? Would it be platform independent? Wouldn't it just shift the problem from single chars to chunks of arbitrary size?
Can't promise as I'm currently swamped up with work. But I can accept a PR.
Sure, but the notion of "reasonably sized" is unspecified, and Haskell's behavior here is also perfectly legal (though unlikely) behavior in Java. (By "Java", I mean the specification; OpenJDK in particular will never exhibit this behavior, because it looks like it always sets up a buffer of 128 bytes for System.out.) That's what I meant by saying that this would hide bugs. Nothing is any more correct; just less likely to do the surprising thing. I'm not saying that's *bad* per se. In fact, I find the inconsistency of ghci/runghc versus compiled code to be a bit troublesome. But changing the buffering to be more like Java certainly wouldn't make it any more "thread-safe", nor would it make the code that relies on buffering any more correct. *Edit*: I had originally (and incorrectly) said 128 characters, when the buffer is actually 128 bytes.
I love Haskell's syntax, but one of the few things I've always missed was a ternary operator. `if then else` is just so much more verbose, and I really like how terse Haskell usually is. I think it would be great to have ? in the Prelude.
That lack of associated evaluation strategy etc is due to his generality and abstraction. If it had a particular evaluation it would not be so general. Although it is not the case that graph reduction is THE associated evaluation strategy for functional programming, since there may be others. In the other side, Haskell is a good example where category theory has been implemented. monad classes (applicatives, alternative classes) in particular may be considered as the receptacle for the definitions of different execution strategies (by chaining endofunctors). They have been used to execute imperative and logic programming (and constraint programming). Denotational semantics of imperative execution has been formally defined using monads. That's why I think that it qualifies Category theoretical as a paradigm beyond imperative, logic and functional...
I guess we'll have to agree to disagree then. I would say that changing the behavior so that, for the vast majority of common use cases (single lines under 1024 or 2048 characters), having the more friendly output is well worth it. I'd say don't throw the baby out with the bathwater: just because the current behavior is "legal" and perfect behavior would be much more expensive, doesn't mean achieving good behavior the vast majority of the time isn't worthwhile.
Stability across major LTS releases is not the point of the project. The LTS 6 -&gt; LTS 7 transition is meant to contain breaking changes. 
Hi! I wasn't even aware of the *parsers* library. I am aware of *megaparsec* and *attoparsec*, which promise higher performance than the *parsec* library I'm currently using. Having a cheap way to switch between them would be highly useful. IIRC, *attoparsec* focuses less on error reporting, trading that off with higher performance. Given that only one of my 3 RDF parsers is 100% compliant with the W3C unit tests (the *NTriples* parser), that's currently the only one that's probably ready to have error reporting traded for better performance. A PR offer is very kind, thank you. Because it's bug free, and the implementation so straight forward, maybe the [NTriples.hs](https://github.com/robstewart57/rdf4h/blob/master/src/Text/RDF/RDF4H/NTriplesParser.hs) would be a good start for migration to *parsers*? Is there a well understood "highest (runtime) performer" of the instances backed by *parsers* ?
He's not arguing for `Prelude.putStrLn` to change, but for `Data.Text.IO.putStrLn` to behave as `Data.ByteString.putStrLn` does. While the argument given is a little overstated, I have no problem with the idea. It's handy for demos, tests and debugging to do putStr output, and convenient if it's reasonably atomic.
&gt; Even though there are some really valide reasons for it Which are?
&gt; However, `bool` is the other way round, ie ifElseThen More accurately, it is elseThenIf. The "if" being last makes that order more convenient than the usual one in some scenarios. As for "else" first, it is not such a serious issue, given that there are common analogues in Haskell for that order, and that `bool` is visually quite different to `if`/`ifThenElse`/etc.
I feel like if then else is made purposefully verbose and ugly in order to be syntactic salt, to discourage the use of booleans and help people avoid unconscious boolean blindness. 
the current order if arguments makes a parallel to `either` and `maybe`, or the natural continuation of the either/maybe progression. it's also in the order that the constructors are defined, like in either/maybe. might be worth noting that `foldr` is also in the same vein as either/maybe/bool, but it's arguments are flipped. 
`bool` has the order it does because it makes a better eliminator this way. You enumerate the cases and then can choose to pass it the last member of not. eta reduction is a fairly common thing parseNumber &gt;&gt;= bool byLand bySea with this combinator and it fits in with the other eliminators it was created to mimic. `ifThenElse` on the other hand does not really lend itself to partial application.
I don't know how you get the count of six, but even if you were right, that's clearly about 5 to many ;-)
Good question! I put it to the public-rdf-comments@w3.org list this morning: https://lists.w3.org/Archives/Public/public-rdf-comments/2016Nov/0000.html Andy Seaborne's response: https://lists.w3.org/Archives/Public/public-rdf-comments/2016Nov/0001.html In short, RDF says that language tags must be syntactically correct but does not require them to be registered. Here's the list of valid language tags: https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes So, "en" is a valid tag for English, whereas "zz" is not in the list. However, according to Andy's response, both: "Foo"@en "Foo"@zz Are both valid RDF language literal nodes. So, whilst having data LangLit = EN | FR | ... Would've been nice in this Haskell library, it seems that for language literals at least, stringly types make sense. 
Oh for row types: if :: Bool -&gt; &lt;then :: a, else :: a&gt; -&gt; a if condition &lt; then = doThis ... , else = doThat ... &gt; 
Well, i upload this package with (?) in less than 10 mins, let's see if there's someone wants it. http://hackage.haskell.org/package/if
Wow, awesome thanks!
Ah OK. Yeah, then it makes sense for strict text to all at once, and lazy text by chunks.
I am not saying that the fact that "no one has ever gotten shot in the foot" is empirical evidence that it will never happen. I am saying that `bool` makes just as much logical sense as `ifThenElse`. So there is no reason to think that any more problems would occur for `bool` than for `ifThenElse`. That agrees with our own experience in practice. In fact, in the context of Haskell, `bool` makes even more sense than `ifThenElse`. See the sibling post of /u/edwardkmett.
Fair enough. But then call a spade a spade - 'unexpected defaults' and 'not threadsafe' are rather different degrees of breakage.
&gt;I don't know how you get the count of six I miscounted it. It is only nine characters, actually -- and I still insist on the "only". if foo then bar else baz foo ? bar $ baz -- as the OP suggests
I didn't realise `bool` put the test at the end, which indeed reduces the chances of mixing the arguments with an `ifte` function.
Honesty, I don't get what is "verbose and ugly" in `if cond then val1 else val2`.
Nice! I liked jinja2 when I was doing python. :-)
Why would you take the lock *before* forcing the portion of the string you're writing?
Maybe because `False &lt; True`, so it makes sense to list the False case first? &gt;&gt;&gt; Nothing &lt; Just () True &gt;&gt;&gt; maybe Nothing Just &lt;$&gt; [Nothing,Just ()] [Nothing,Just ()] &gt;&gt;&gt; False &lt; True True &gt;&gt;&gt; bool False True &lt;$&gt; [False,True] [False,True] 
`bool false true cond` exists as a more convenient version of `cond ? true : false`.
Looks nice! How does this compare to https://github.com/brendanhay/ede ?
The difference (including whitespace) is 9.
GHC Haskell has a highly optimized I/O manager and lightweight thread scheduler. I can't find an article about it, but I believe that it should be able to handle on the order of 100,000 concurrent connections, all with their own threads.
imo, the title seems to imply that haskell (including its ecosystem) totally lacks certain features necessary for concurrency to function at all while the post does clarify this (in the first paragraph), the title seems passive-aggressive relative to the rest of the content which is "`putStrLn`acts unexpectedly when you're using it from multiple threads" and "there are no closeable channels available by default". i would have expected "missing concurrency basics" to include "you can't fork a thread" or something equally critical
Learn the JVM internals and be awed. If your only experience with the JVM is through Java, you won't see the magic. The JVM is this magic machine that can dynamically optimise your code very, very well - a fact that they don't throw around much as they should. I was an anti-Java person for all my life until last year when I started digging in.
Thanks for the support! I'll reveal more details in the coming months as the compiler/rts become more stable.
I feel a bit confused by the apparent removal of my post. It didn't appear on the actual subreddit and the text seems to have been removed. Has the post been removed by a moderator? I don't understand what rule my post would have violated. Or is my post awaiting review? I don't understand what's going on.
Thanks for the feedback.
It's been a long time since Robert Harper wrote about Haskell... wait a minute...
I know it's meant to contain breaking changes. I wasn't trying to suggest otherwise. I was responding specifically to this statement in the parent: &gt; TL;DR maintainers are working together to keep packages on Stackage free of dependency hell issues Using stackage doesn't make all dependency hell magically go away.
Interesting idea! Thanks for your input. Besides being hard to abandon the work, I lose out on the GHC optimisations, which in practice have shown to be very valuable. I don't think any haskell-on-the-jvm is viable for production use without said optimizations. Moreover, I actually like a lot of the GHC extensions and I use many of them freely in certain features of the Java FFI, like support for generics. I'm betting on the fact that there's a very structured and organized way to teach full GHC Haskell with its extension soup and besides working on the compiler/rts, this will take up the most of my time. 
I've designed the FFI in a way that you can essentially embed Java inside of Haskell using a monad + combinators on that monad. Check out [eta-2048](https://github.com/rahulmutt/eta-2048/blob/master/src/JavaFX/Main.hs) for an example of how this embedding looks. The fancy optimisations from GHC will then optimise out the monad to give you code that resembles normal, sequential Java code with a bit of cruft which will be optimised out in future versions of Eta. Inline-java, as to my knowledge, doesn't support Java language-level features like generics to make it easy to interface with all sorts of Java APIs, while maintaining type-safety. Moreover, it goes through JNI which has a slight overhead. The benefit in Eta, is that the JVM's JIT compiler can optimise between the Eta &lt;-&gt; Java layer because at the bytecode-level, they're compatible. Having said that, I'm very impressed with the work the amazing work they've done in making the Java interop work for native Haskell.
Most likely we'll add specialised Java agents that augment Eta-specific runtime data onto the JVM runtime data to give you a clear picture on what's going on. Memory dumps and thread stack dumps will still be relevant since Eta's implementation uses the Java call stack. This has made it easy to debug the runtime. See [Debugging Stack Traces](https://github.com/typelead/eta/blob/master/docs/Debugging-Stack-Traces.md) in the docs. 
ping /u/vincenthz
&gt; Learn the JVM internals and be awed Do you have some learning material you recommend?
First of all, this looks nice. Thank you for making it! However, while this is obviously not your fault at all, I think the fact that a user’s guide needs to exist *separate* from the main API documentation is a real failing on the part of Haddock. Haddock is okay for API documentation, but it is atrocious for anything more prosaic. The typesetting tools at one’s disposal are limited, and the default stylesheet is not conducive to reading longform text at all. The font size is small, and the line widths are *huge*, especially on a large monitor. I often find that I need to resize my browser to make the viewport width smaller if I want to comfortably read longform Hackage documentation. Contrast this with the Racket documentation tooling, which is designed to allow [a tutorial-style user guide](http://docs.racket-lang.org/guide/index.html) right alongside [the full API reference](http://docs.racket-lang.org/reference/index.html), and the two are carefully crosslinked. Because this tooling is so good, other user-defined packages include the same guide/reference format as the core docs, such as [the Racket lens package](http://docs.racket-lang.org/lens/index.html), [the Pollen digital publishing tool](http://docs.racket-lang.org/pollen/index.html), and my very own [parser combinator library](http://docs.racket-lang.org/megaparsack/index.html). It would be awesome if your nice user guide could be integrated with the rest of the documentation, too, instead of needing to live in a separate place. I imagine there are many packages that would benefit from it, since developing an entirely separate system for authoring and publishing *versioned* documentation is far too much for many packages.
It's not about the number of character but about the noise. Normally I read `a b c d e f` as a call to a function with 5 arguments, not 3.
That seems less convenient, I wouldn't want it backwards like that.
FWIW, the current ginger site already pulls documentation content directly from github, adding just some templating, markdown expabsion, and styling (and yes, I am dog fooding on this one). I believe the system I have in place could be extended to also pull in Haddock from Hackage, with the caveat that I would have to somehow modify the cross-links. I don't think Hackage API docs are all that bad though, and for most libraries, writing the user guide right into the Haddock is perfectly appropriate. The reason I took a different path here is because yhe target audience is not just developers, but also non-programmers - content editors, web designers, etc.
I'd agree if this were about something more important. But stdout is typically a bit of a throw-away thing anyway. So, no harm in making it work 95% of the time to avoid being a pain. The implementation is also pretty costly performance-wise. I'm convinced that this should be "fixed" everywhere except GHCi, where the surprising behavior is a feature.
Great to see, thanks for sharing. Off-hand, I wonder how it [compares](https://nuetzlich.net/gocryptfs/comparison/) by various measures (confidentiality of file contents, names, sizes, integrity concerns, disk use and read/write performance). In addition there's always the security arguments that are the meat of the interesting material (mount security mechanism, key scheme, encryption and file format, code quality, etc). I glanced and didn't see a list up-front on the core issues but instead a reference to the underlying crypto library. I'll dig if I find time.
fwiw Data.List has `genericLength = fromIntegral . length` don't know what prompt2 is. the sql3 driver missing might mean you haven't installed any necessary external dependencies, which happens in any language as their package managers only track "internal" dependencies. though `stack --nix` might help you build Haskell packages that depend on C libraries. If you just want to map over each item in a set, Data.Set should have its own `map` function. Sets aren't Functors because `fmap :: (a -&gt; b) -&gt; Set a -&gt; Set b` doesn't make sense. I think this is a good place to learn more about what Functor means (google "why isn't Set a Functor"). the issue is that fmap must be able to take any function between any two types (a and b), but Set restricts those types to be equatable/comparable. e.g. for lists, we can map some numbers to incrementing functions fmap (+) [1,2,3] = [(+1), (+2), (+3)] but we can't put functions like `(+1)` into a Set, since we can't compare them for a quality. 
Dropping most of this, because I'm not sure what to say... The oddity with newlines and Data.Text.putStrLn is unique to that package. If you try your very first example from the blog post, but just compile it instead of using runghc, then it does exactly what you expect. So I suspect the bug for the latter issue should be filed for `text`. The problem seems to be that hPutStr contains an implicit flush, and hPutStrLn picks it up by accident.
This might be slightly off topic, but the one thing that really bugs me about haskell is the everlasting desire to save small amounts of keystrokes. Haskell gets short not because of short function names, but because of superior ability to abstract things away. Haskells code is already pretty tight, i personally don't need to save a few keystrokes by avoiding `if then else`. Also i like `if then else`. Its reasonably short, but also really concise (i also avoid the ternary operator in other languages, i think it tends to make code unreadable). I support dealing with symbols when one is thinking about more abstract concepts and as always there are exceptions to the rule ($).
&gt; aka stack-based okay what is that? Google gives me conflicting answers. Totally agree with the second part.
Can you elaborate? I'm thinking about using this and I'm a little concerned about it blowing up somewhere.
I believe I've found a typo: in &gt; From f:X→P(Y) we can take R={(x,y)∈X×Y|y=f(x)} and from R we can construct f(x)={y∈Y|(x,y)∈R}. it should be &gt; R={(x,y)∈X×Y|y∈f(x)}.
Does not refute my point, which was that the issues appear to be smaller scope than was implied by the title.
Thanks for pointing that! I guess there should be many package provide this though.
Describe what you are trying to do in more detail, and we can provide more detailed responses.
http://concatenative.org/wiki/view/Concatenative%20language I never tried to delve into it, but this should give a fair amount of informations.
This is brutal, nice work. We plan to provide limits on call-stack depth, list/object sizes, and overall stack-data size -- this amply demonstrates their importance. The "safe" in the thread title is indeed inaccurate. Pact is not masquerading as a "safe" language but one that has embraced a number of constraints toward the end of safe computing: single-assignment, turing-incomplete, no nulls or `NULL`s, etc. It's imperative, uses a schema-less KV metaphor for the database, and is dynamically-typed, in an effort to balance utility with safety. Pact being SSA and Turing-incomplete is speeding our efforts to make Pact programs verifiable so hopefully soon we'll be getting direct feedback about weaknesses in the language.
I assume something technical went wrong. I have re-posted it. Please ignore this entire thread. Don't 'publish' it or whatever.
&gt; The documentation tells us that empty should be an identity for &lt;|&gt;, and that &lt;|&gt; is a binary associative operator (huh, sounds like a monoid, right?) In fact this is exactly the monoid instance of [`First`](http://hackage.haskell.org/package/base-4.9.0.0/docs/Data-Monoid.html#t:First), a newtype wrapper around `Maybe`.
So you suggest this instead: first :: Foldable t =&gt; t (Maybe a) -&gt; Maybe a first = getFirst . foldMap First someOtherFunc :: Int -&gt; Maybe String someOtherFunc i = first [foo i, bar i, wat i] It seems pretty OK.
I regularly have to `grep` through lots of files. Problem is, the data I’m looking for is not at the regex I enter, but in its surroundings. `grep` has options for that (-ABC), but those yield long and hard to read through output. vgrep solves that problem by basically giving the user a nice view for `grep -C∞`. Controls are inspired by Vim so many should feel right at home.
This is the meetup page: https://www.meetup.com/Software-Circus/events/235459993/
This is great! You are right that it does feel very much at home for a vim user.
This is why I run `grep` (actually I run `ag`) in vim sometimes instead of in shell. Useful tool regardless.
I see. That kind of "quasi-atomicity" is great. In fact, if you take the specifics from the article's message, it just wants Text.putStrLn to include the linebreak on the same operation as the text. I have no argument against this, looks like a good thing to do. I only have a problem with the general message that printing should be atomic.
[removed]
I take a lot of inspiration from your posts, so I'm glad to see that you like them. Thanks!
I have been thinking about this for some time. Short answer: Add it to the [parser](https://github.com/fmthoma/vgrep/blob/master/src/Vgrep/Parser.hs). Long answer: `grep` output colors come in ANSI escape codes. You would have to parse those, and AFAIK there is no Haskell library for parsing ANSI yet. Furthermore, `grep` usually only enables coloring when printing to a terminal, not when writing to a pipe; so this would require forcing `grep` to colorize output (`--color=always`). And last, keep in mind that different implementations of `grep` (e.g. GNU grep, Git grep) use different color schemes.
&gt; I really .. ?
We'd certainly like to eliminate as many attack vectors as possible. Regarding long-running programs, we've also considered a simple function-application limit, which should protect against that. The language is designed around deployment in a permissioned blockchain, which almost always will also involve some kind of legal or regulatory framework, and offers confidentiality (private transactions that don't run on every node). In this model, we're not overly concerned about arbitrary malicious behavior as it will either be impossible to hide the originator of it (in a non-confidential/universal-execution case), or its impact limited (in the confidential case). Nonetheless, we're also thinking about how a permissioned blockchain could run in a public context. Just because membership isn't a free-for-all doesn't mean that there can't be a public, transparent mechanism for node operation. In this case we would be *very* concerned about malicious actors. Do you see some other vector for long-running programs that isn't tripped by an funapp limit or other similar limit?
I like this post a lot, but I do want to raise a concern about MaybeT's Monad instance. Whenever I see MaybeT I first ask the question, "what failure cases are being hidden?" Nothing encodes failure, but failure without context. More often than not our Transformers have complex failures modes, this makes exceptions or Either more appropriate when monadically representing failure in a transformer context. It isn't always the case, but monadic MaybeT is a code smell. The Alternative instance however is fantastic. Thanks Matt for bringing it to my attention. When utilizing Alternative all we care about tracking is the lack of success, not modes of failure. MaybeT is a great fit for this.
I have just recently started trying to use Alternative, so this post reinforces my thinking about it. Thanks for the approachable tone and useful examples!
I completely agree. I would not recommend making `MaybeT` part of a custom monad stack, or even using it as the return of a function. I do occasionally do things like: foobar = runMaybeT $ do someThing &lt;- MaybeT $ someAction lawl &lt;- lift . pure . someMaybeFunction $ someThing MaybeT $ otherMaybeAction lawl or similar, when it can get rid of a lot of boilerplate in a case where I only care about the whole thing returning `Maybe` or not.
have no idea how that got posted. 
 cond ? val1 $ val2 looks much nicer IMO
Hi Mark, We had a brief chat about Servant and functional web dev on HN a few weeks ago. Congrats for publishing your book; and thanks for making it available under CC! I just bought me a copy. Probably a bit too soon, but also looking forward to volume II.
Just bought my copy, very awesome.
&gt; attoparsec I think that would be interesting.
Is there any tutorial on `Frames`? The package seems to have no example usage.
Fantastic! The real intent of this post was not to endorse message pack, but to motivate and work through the process of evaluation. I'm overjoyed you were empowered to take this action. As mentioned, a truly robust evaluation requires a more in depth benchmark. I'd love to see the results.
An additional consideration: JSON and message pack provide a platform agnostic serialization format. `Data.Serialize` is Haskell specific. `Accept` allows you to support this format through REST, yet it has less universal utility since services from other platforms cannot share this serialization format. Checks and balances, yada yada.
You can export individual functions as callbacks. https://github.com/ghcjs/ghcjs-base#passing-haskell-callbacks-to-javascript But, I don't know how to export a haskell module to javascript. PureScript is better at interaction with javascript.
Great! Almost everything was already done for me, so I couldn't help it. It was too easy.
Relevant video: https://youtu.be/QMPum88xluEg *EDIT:* Explanation starts at 10:00
Saw natural language processing and haskell and bought at 8 usd. Thanks for the book. Have not finished lesrn you a haskell yet (at 50%), but soon. Also liked the honesty. At a bout a year with Haskell now and loving it. When it clicks it is the most simple language next to python.
Thanks for the kind comments! I should have mentioned that there is also a read online link on the publisher's web page.
Just bought the book! Also still currently working through "Learn you a Haskell" but this looks great and really want to support anyone who makes the effort to bring Haskell to more people. Thank you for all the hard work on this. 
This one has been sitting on my harddrive for ages, I stumbled upon it and gave it some last polishing before releasing it. :-)
Thanks for the reply. Now, I *really* do not mean to be antagonistic, the following is just for clarity so that no mistaken ideas are being spread. &gt; it allows me to make some revenue from my writing 100% of CC licenses allow that. In fact, there is nothing you are not allowed to do as the copyright holder. The "NC" clause is really "commercial right reserved", i.e. you have commercial rights no matter what, you are just blocking anyone else from having commercial rights also. There's no strong evidence that letting others have commercial rights hurts your revenue. The only case where it clearly does is those rare situations where someone else wants to use your work commercially whatever it takes and thus will pay for a special license from you if needed. The other effect of NC is to create incompatibility with free licenses. So, if someone wanted to do valuable creative combination of your work with that from the Haskell Wikibook, it's blocked even if there's no commercial use ever. It's just incompatible terms. But the ND restriction blocks all creative derivations anyway. That certainly does nothing to help your revenue or encourage distribution or anything else, it just blocks creative potential. Yes, you have legal right to use these restrictive terms, and you don't owe me or anyone a justification, but people in general should not just accept without question the premise that these restrictions are positive or helpful, certainly not without strong evidence. There's very good reasons why comparable NC and ND type restrictions are never accepted in the free software / open source worlds, and the same reasons apply equally to works like instructional texts.
Logging is a use-case.
In the blog post, he's specifically talking about lazy `Text`. As for lazily defined strings, if they're finite, printing one character at a time is still slow. 
&gt; But I still do not agree there is any way sensible to make putStrLn atomic since it doesn't own the "stdout" resource. It should be the responsibility of the developer as in the comment above. But there *is* a way to make it atomic for strict `Text`.
Never heard of broken locales even on crappy 10 years old Linux systems. If you somehow have a broken locale anyway, fix it. Haskell is not responsible for this. But I suspect it's not broken, it's just not what you expect. The root account normally has its locale set up to C (or POSIX), while normal users' accounts are set to whatever.UTF-8. There are reasons for that. A Haskell program would die if you try to write a non-ASCII character in a POSIX locale simply because it has no idea how to do that. This is not unique to Haskell and Linux by the way, I've seen C programs do the same on Windows if you look at them at a wrong angle. Worse still, they just silently stop all output after the first invalid character! Back to Haskell, you can always force UTF-8 encoding on any stream you want: import System.IO main = do hSetEncoding stdout utf8 ... and merrily output whatever character you want in whatever locale you want.
For reference, `asum` is in `Data.Foldable` in the base library. It is traditionally called `choice` in parser libraries, and that is also a much better name for it in the context of `Maybe`.
Yes. I used `First` much more in the past. Now I realize that `Alternative` usually results in cleaner and more readable code.
[removed]
`MaybeT` is great in a custom monad stack, and as the return type of a function. Provided that you are using it to allow a choice of alternatives with effects, not to track errors.
I'd like to have type providers for (Postgre-)SQL schemas similar to those shown in this talk: [Type Providers and Error Reflection in Idris](https://youtu.be/dP2imvL92sY). Is this feasible? Furthermore, we could have some kind of type-checked embedded SQL, including all Postgres functions. I mean writing SQL as a Haskell EDSL with GHC pointing out type mismatches, wrong column names, etc.
I think you'd have a point if you interpret the video as saying "we should stop teaching these two sorting algorithms and just teach one of the two." Granted, there are some parts in the video that overstate the case. But you are also understating the case: the interesting bit here isn't just that the two techniques perform the same number of comparisons; it's that (in a sort of naive implementation, anyway), they perform *exactly* *the* *same* comparisons. The addition of "in a naive implementation" is necessary here, because actually the algorithms discussed here aren't really the way anyone would write insertion or selection sort. Rather, they are the way you'd write them if given only min and max functions, and otherwise were prohibited from making control flow depend on data. Really, for insertion sort, you'd stop comparing once you found where the element belongs; and for selection sort, you'd go search for one element and swap it into place rather than a cascading sequence of swaps of adjacent elements. That's probably a far more legitimate critique of the video, and it's one that's explored in more detail in the paper linked by /u/trust_is_an_opinion earlier. But once you have accepted that these aren't really the canonical insertion and selection sorts, you're still left with something pretty cool. In any case, this is fun to play with. Here's a function that relies on memoization, and can be either of the two: mySort :: [Int] -&gt; [Int] mySort values = [mins ! (n, i) | i &lt;- [1 .. n]] where n = length values input = listArray (1, n) values mbound = ((1, 1), (n, n)) mins = listArray mbound [ if i == j then maxs ! (i, j) else min (mins ! (i - 1, j)) (maxs ! (i, j)) | (i, j) &lt;- range mbound ] maxs = listArray mbound [ if j == 1 then input ! i else max (mins ! (i - 1, j - 1)) (maxs ! (i, j - 1)) | (i, j) &lt;- range mbound ] 
Interesting perspective, though I get the feeling that a List transformer is likely more suited to that?
Just testing this I see nothing like what you refer to. Do you have any idea how the locale is broken? Do you mean perhaps setting a LANG that the system doesn't have? That's the only type of "broken" locale I can even imagine beyond a completely corrupt file which should give all sorts of issues. I'll admit I've never even heard of a broken locale before or how one might possibly have that happen.
It's so funny, you've never heard of a broken locale, I don't even know how one gets a properly working one ;-) So, I don't really know how this entire 'locale' business works. I'm quite confused by it. But basically, install a fresh Ubuntu or Raspbian or I guess any of the Debian based distros and just type 'man' or 'locale' and you get various errors. That to me is already strange, like why would it be like this out of the box? If your OS by default installs in a way that leaves 'man' complaining about a broken environment, something isn't right. Then, many StackOverflow answers suggest to do things like `sudo raspi-config` and select &amp; rebuild a locale, or do `sudo dpkg-reconfigure locales`, all spitting out more errors, none of which fix anything. On my RPi, manually adding the relevant env exports to `/etc/default/locale` did the trick, my desktop Ubuntu needed some other fix (was a while ago, sorry). That still doesn't seem to fix the environment for programs running under daemontools, though.
I haven't been ae to make it work either
Is there something similar to this but with type classes?
I have things like: trySomething &lt;|&gt; trySomethingElse &lt;|&gt; reportError all over the place. I don't think idiom brackets could be any more clear than that.
 liftA2 (&lt;|&gt;) Or here are some alternate spellings of `liftA2 (&lt;|&gt;) f g`: (&lt;|&gt;) &lt;$&gt; f &lt;*&gt; g pure (&lt;|&gt;) &lt;*&gt; f &lt;*&gt; g
You can always interchange `Maybe` with single-element lists, via `listToMaybe` and `maybeToList`. But why would you want to do that, unless you already happen to be in a list context?
Cool, thanks for the write up! I'll give this a shot in my next project :-)
turns out that autocomplete stops working is there is an error in the file! I had unresolved imports!! 2 days I spent trying to work that out!! haha well I know now
Apples and oranges? Julia is dynamic.
But everything is type annotated... Maybe I don't know what dynamic language means... 
Under the hood, conduit and pipes are folds. See the Oleg papers linked above. They also allow transforming streams. I'm not convinced they are orthogonal; I suspect this library is yet another flavor of Oleg iteratee folds.
&gt; There are more functions than just map and filter in prefolds that allow to transform a stream before consuming it, so you can construct large pipes without actually using pipes or conduit. I would classify these complex transformations on Folds (like your "takeWhile" function) as some kind of "transducers". I like to think of transducers as stream transformations seen from the consumer side. I must say that fiddling around with fold-like types is great fun! They are simple to define and yet have lots of interesting instances and behaviour. I wrote two fold-related packages myself: [foldl-transduce](http://hackage.haskell.org/package/foldl-transduce) (which works on top of the folds from the "foldl" package) and [streaming-eversion](http://hackage.haskell.org/package/streaming-eversion) (which transforms producer-side transformations into consumer-side ones). 
&gt; the interesting bit here isn't just that the two techniques perform the same number of comparisons; it's that (in a sort of naive implementation, anyway), they perform exactly the same comparisons. This would be more interesting if there weren't exactly n choose 2 meaningful comparisons you can do. &gt; Really, for insertion sort, you'd stop comparing once you found where the element belongs; and for selection sort, you'd go search for one element and swap it into place rather than a cascading sequence of swaps of adjacent elements. That's probably a far more legitimate critique of the video, and it's one that's explored in more detail in the paper linked by /u/trust_is_an_opinion I sort of thought that was implied. That's the main reason that they perform differently. What I see in this video is not a comparison of insertion sort and selection sort at all. It's just a demonstration that you can do (fairly inefficient) sorting by comparing every pair of elements, and that you have options as to which order you compare them in. But I bet not many people would bother to watch a video about how you can sort a list by comparing every pair of elements, even if the video pointed out the cool fact that the order you compare them in can be flexible.
I'm trying to wrap my head around the entire thing. The type-level magic is still rather hard for me. After getting it to compile and few simple examples I've came up with a similar idea. As services grow bigger, writing your logic in the Application monad is becoming increasingly pointless. Have you tried to implement your it yet maybe?
They aren't folds, they are coroutine based instead.
Difference between static and dynamic type system is whether you know type of every expression during compilation or not.
As a contrast to sorting within a ram model, you may be interested in a sorting network: https://mitpress.mit.edu/sites/default/files/Chapter%2027.pdf 
Published today, even. Wow!
It's working for me. [Here's my .spacemacs](https://gist.github.com/saurabhnanda/9e4489ac190b3861728b85daf2a615c3) if it helps.
A stream like a `pipes` `Producer a m r` or `ListT m `, or a `conduit` `Source m a`, or an `io-streams` `InputStream a`, or a `streaming` `Stream (Of a) m r` are typical things to feed to such folds as you are constructing ... alongside lists, vectors etc. Beautiful folding is not at all 'orthogonal' from the point of view of `pipes` or `streaming`, but in both cases the standard way of executing a left fold over a stream. Or put otherwise, consumption by a beautiful fold is a typical purpose for generating a producer/stream. `streaming` in particular completely dispenses with "consumer" types, as an unnecessary complication, and makes do with beautiful folds like yours or else dedicated folding functions, like `stdoutLn`. (One of the things the `streaming-eversion` library shows is that it is possible to write any "consumer" in the sense of the streaming io libraries, as a beautiful fold. Any way of disposing of a stream at all can be written as a beautiful fold.) Here are functions for feeding a streaming `Stream (Of a) m r` to the sort of fold you are defining http://sprunge.us/fOYH I think they're right. By the way, the `Applicative` instance seems to work differently from what's usual in beautiful folding libraries. Thus &gt;&gt;&gt; exec (liftA2 (,) Fold.sum (Fold.any even)) [1..3] (3,True) We might have expected `(6,True)` but maybe you intend this to be done in another way. 
This approach is actually not comparing every possible pair of elements. Most of the comparisons are between results of other comparisons, and in general, some pairs of input values will end up being compared several times, while others not at all. In fact, the question of how one would sort after comparing all possible pairs is not any easier than how one would sort an array out-of-place to begin with. It would remain to describe the branching structure of how you intend to look at the results and choose an output; and once you've done that, you may as well replace "look at the results" with "do the comparison on the fly", and you're back to just describing a plain sorting algorithm.
Yes, it's different, because `f &lt;*&gt; g` stops as soon as either `f` or `g` is stopped, so the whole thing evaluates only the [1,2] part, since `2` is even. If you want to stop when both folds are stopped, you can use `(&lt;+&gt;)`: `exec ((,) &lt;$&gt; sum &lt;+&gt; any even) [1..3]` evaluates to `(6,True)`.
thanks - at what point in my haskell evolution do I see this by reflex? :)
&gt; I must say that fiddling around with fold-like types is great fun! They are simple to define Types are simple, but folds themselves are not so simple for me. For each of the "complex" folds (`groupBy`, `chunks` and such) in `prefolds` I spent at least a couple of hours implementing them. &gt; foldl-transduce Oh, I'm gonna steel some of your transducers. Nice thing about `prefolds` is that you can implement `chunksOf` in terms of more general `chunks`: `chunksOf n = chunks . take n`. &gt; streaming-eversion Do you use it for something?
&gt; It should be possible to write an equivalent of these for your folds. Yes, but I haven't yet grasped those `handles` and `handlesM`. There are other things missing like e.g. a comonad instance for `Fold a m`. Regarding your `foldD_`, I think it shouldn't be defined manually like this, but instead you provide a usual left fold in some particular form and I provide a function that handles folds in this form. I.e. like `pipes` and `Control.Foldl` are designed to be compatible. But I haven't tried this yet. &gt; Beautiful folding is not at all 'orthogonal' from the point of view of pipes or streaming, but in both cases the standard way of executing a left fold over a stream. When I say, "A is orthogonal to B" I mean that A and B can be used independently, but it also makes sense to use them together.
Actually, what I had in mind was code like this, where having an actual operator is pretty nice. fizzbuzz = fb &gt;|&gt; fz &gt;|&gt; bz &gt;|&gt; pure.show 
Yes, right, I did it by hand; you could have something like `purely` and `impurely`. Then libraries like `pipes` etc. could avoid depending on your library if they write special fold functions with whatever the right type is, e.g. prefoldM' :: Monad m =&gt; (x -&gt; m b) -&gt; (x -&gt; a -&gt; m (Either x x)) -&gt; Either x x -&gt; Producer a m r -&gt; m (b,r) or whatever. Folds in your sense can't be used independently, they need to be fed *something*, whether a vector or a stream or list or whatever. Similarly, consumption by a left fold is a typical way of eliminating anything that is sequential whether a vector or an effectful stream. Or anyway that's all I meant, I guess it's consistent with what you are thinking.
Well my point was that `ListT` models alternatives better than `MaybeT`. Though I suppose if you know you only want one result, the difference doesn't much matter.
It sounds like you've accidentally run the main function of intero (which uses the GHC API, which is producing the error) rather than the main function of your actual module. You can get some visual cues about this by looking at the prompt which should give you the name of the module that is loaded and in scope.
One can't write `groupBy` with `handles`, either. Or any non-trivial decoder.
[removed]
&gt; using map with a list of ints will trigger a compilation of map for ints. Because Julia's map uses parametric polymorphism and thus instantiates `map{A}` to `map{Int}`? Or are you saying that even though my example specified the type `List{Any}`, Julia will strip the type annotations and replace them with `List{Int}`?
Oh cool. One question through: if its based on diagrams is it fast? Chart has a diagrams Backend and its considerably slower than the Cairo Backend. Thats No Problem normally but if you try to Plot stuff with a few thousand points it takes longer to render...
Haskell... are you ok?
I did not believe my eyes first!
Here http://sprunge.us/EYXF is foldlify :: Monad m =&gt; Fold a m b -&gt; L.FoldM (ExceptT b m) a b But I'm not sure how to get closest to `impurely` It will be a bit more complicated.
I agree. I did not mean to say that Julia has weak typing.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/julia] [Julia vs Haskell type system.](https://np.reddit.com/r/Julia/comments/5e1yxr/julia_vs_haskell_type_system/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
Oh, it is Mark Watson the guy who continues to passionately learn despite age! Congrats. Looking forward to reading it ...
Tried &gt;:main, and still get the same error. However, other functions I've defined in the file are runnable in the repl.
Thanks for all the advice. I'm just trying to work out whether intero is a good choice for working with Haskell in emacs. With a bit more investigation, I found that I can't run main from gcfi either, though that may just be because I need to do something else to load the project. That said, I noticed the following idiosyncracy of intero: In gcfi, I can define the following function: `doubleMe x = x + x` The definition is accepted and I can run the function in the repl as accepted. However, in intero, defining the function as above produces the following error message: &lt;interactive&gt;:36:12: error: parse error on input ‘=’ Perhaps you need a 'let' in a 'do' block? e.g. 'let x = 5' instead of 'x = 5' I can define the function as follows in the intero repl and it works as expected: `let doubleMe x = x + x` Seems like maybe a bug in intero???
In my view, static languages are those where semantic analysis such as type checking is performed ahead of time, before any code is run. Emphasis on **semantic** analysis, which means more than just an AST transformation. Dynamic languages aren't just languages that aren't compiled. That would be *interpreted*. Dynamic languages are just languages that lack static semantic analysis.
&gt; ok, what is the type of `undefined :: a`?. That type would be `forall a. a` and it's not a *hack* in Haskells type system in any way. But if you have a value of that type, there's nothing you can do with it. You can pass it as an argument but no function can extract any information it unless you bypass the type system entirely and use some unsafe type coercion. Which is the easiest way to introduce segfaults into your program. Julias `Any` is closer to Haskells `Dynamic` from the [base](https://hackage.haskell.org/package/base-4.9.0.0/docs/Data-Dynamic.html) library. And there are functions that can actually work on it, e.g. `fromDynamic :: Typeable a =&gt; Dynamic -&gt; Maybe a`. Another way to emulate dynamic typing in Haskell would be the `Value` type from [aeson](https://hackage.haskell.org/package/aeson-1.0.2.1/docs/Data-Aeson-Types.html) or any other suitable ADT. Dynamic objects are really just heterogeneous dictionaries together with some runtime tag. Every feature of dynamically typed languages can be implemented that way.
/u/Woofer14123 thanks for the friendly "hi", and welcome to our community. This channel is about the Haskell programming language. The "upvote/downvote" in the channel is used to share how helpful each post is for learning about Haskell and related technologies. Regardless of downvotes, we all welcome you to the community and hope to have some great discussions with you about Haskell.
Any demos of examples?
Yeah, I wrote [`impurely`](https://github.com/effectfully/prefolds/blob/master/src/Core.hs), but it requires generalizing your monadic folds ([for example](https://github.com/effectfully/prefolds/blob/master/test/Main.hs#L297)). That `Absorb` [doesn't seem](http://stackoverflow.com/questions/40716239/what-is-this-thing-similar-to-kleislifunctor) to have some nice categorical structure, but the laws at the link look OK and the whole concept kinda makes sense.
IIRC from Haskell (I'm here from /r/Julia) `id :: a -&gt; a` is not `(id::Any)::Any` it is `(id::T)::T` in that you get the same type out as you call with. (Although, of course, T could be Any but that's tangential) 
&gt; NLP using Haskell Do you have the link for that book?
I've put quite a few examples in the haddock documentation for simple usage (a lot of them are hidden under a `[+] Example` heading). There's also some more complex examples in the [examples folder](https://github.com/cchalmers/plots/tree/master/examples). Things like plotting stocks from yahoo or plotting a csv from criterion.
It will defeat the purpose if the library that wants to interoperate with `prefolds` has to use the `Absorb` class, no? I don't think there is too great a problem if there are dedicated `foldPrefold` functions in other libraries each perhaps matching a dedicated `impurelyForXYZ` inside `prefolds`. But I don't know. 
There's a few differences between the two. Here's a few major ones: - Building a plot is quite similar to Chart's Easy API in that all plots and options are done statefully, usually with `do` notation. - Plots is much more class based for setting various options. This makes the types more complicated but (IMO) the code for generating plots is nicer (but you'll get horrible messages if you make a mistake) - Since plots is based on diagrams you can put any diagram in as a plot marker, or even the plot itself. It also uses a diagram's native backend text you can use the [pgf-backend](http://hackage.haskell.org/package/diagrams-pgf) for Latex typeset labels, for example. This also means I don't have bounding boxes for text, which can be a problem when drawing things like the legend. I have some ideas on how to fix this but it's not an easy problem to solve.
Hi, I'm sorry if this reply seems inappropriate, but I'm wondering if the company you're working are looking right now and accepting remote. I have a lot of experience in Ruby and PHP, but I'm learning Haskell with Servant and Persistent, thanks in advance.
Either you mean it's `forsome a. a`, or any function can legitimately do anything with it even while doing perfectly "safe" things.
&gt; It will defeat the purpose if the library that wants to interoperate with prefolds has to use the Absorb class, no? Yeah. But `Absorb` can live in a tiny separate package. Anyways, `Absorb` is a one-method type class, we can inline it. Here is a new [`impurely`](https://github.com/effectfully/prefolds/blob/f4c5ba0a2714e8ad3464e73e43f3b6c419c06bef/src/Core.hs#L249) and an [example](https://github.com/effectfully/prefolds/blob/f4c5ba0a2714e8ad3464e73e43f3b6c419c06bef/test/Main.hs#L298). Does it look OK now?
We're not currently hiring unfortunately, though we do remote and definitely would be looking for mixed-FP/OO experience sets.
Yup. stack.yaml has resolver: 7.9. Tried changing to 8.0 and 8.0.1, but intero wouldn't start then.
&gt; What's also fun is to differ the types of values stored in Drive so as to let the fold finalize in knowledge of whether it saturated or didn't. That's how I [originally](https://github.com/effectfully/prefolds/blob/d7725a2b04a3c7dbf77522ada7dd732ee23c9b3b/src/Fold.hs#L9) wrote it. Then I removed it, because complicated folds become even more complicated and sometimes you also need to trace more things in order to find out what is saturated and what is not. Then I tried to add it again, but decided it's too complex.
Right, this is faster than my `impurely'` though it requires a special function in the interoperating library. Compare "d" "e" and "f" in this benchmark http://sprunge.us/ZCVI
That is right, thank you.
[removed]
Here is a quick-and-dirty `Display` instance for HyperHaskell: instance Display [Double] where display fs = display . dia . renderAxis $ axis where axis = r2Axis &amp;~ do linePlot (Prelude.zip [0 ..] fs) $ plotColor .= limegreen Performance is definitely satisfactory for a few hundred points. It would certainly be interesting to see what other people come up with...
At the repl, you cannot write bare definitions the same way that you can at the top level of a .hs file. You have to use `let` as recommended. Intero presumably just uses ghci under the hood, and this is ghci working as designed. It's one of those "it's not a bug it's a feature" things and I'm sure there's some reason it was designed this way, it's just unfortunately unintuitive.
Sounds like an intero bug to me. One dumb but simple way to work around it might be to define: main = theRealMain And then just run theRealMain when using the repl.
&gt;Haskell projects are built with the Stack tool, which guarantees reproducible builds using a stable set of packages [...] There is an unsatisfactory lack of nuance in this sentence. It is perhaps worth noting the passage might have been just as easily phrased as "The Stack tool guarantees reproducible builds of Haskell projects using a stable set of packages [...]". *Edit: removing a misinterpretable sentence in the first period. As discussed below, with it I had just meant to associate the lack of nuance in the wording to the general B2B marketing tone of the post as a whole, and not to imply that the post somehow consists in inappropriate promotion of Stack.* 
But a constraint kind is _precisely_ what you are asking for -- which is necessarily more machinery. On that count, I suppose you're asking for a different syntax for them. But I think such notation would be more confusing than not, as `a :: Ord` has a perfectly well defined meaning already -- a value of `a` assigned the _type_ `Ord`. This sort of overloading looks like it would create more problems than it solves. The other thing you ask for is that somehow typeclasses be able to automatically respect such constraint kinds, without treating them as additional data. But there's no one way for them to do so -- and when you add those constraint parameterizations you end up with a fundamentally different class than you might be expecting, in counter-intuitive ways. For example, `Set` isn't a monad, or even a functor, even with the `Ord` constraint. Its only a monad/functor with regards to the category of types with equality preserving morphisms. Even the `Ord` constraint doesn't really suffice. In particular, imagine you have a datatype `newtype BoxInt = BoxInt {unBox :: Int}` but whose `Eq` and `Ord` instance equate all values. Now, you have `fmap (unBox . BoxInt) /= fmap unBox . fmap BoxInt`.
Yeah, good point about Set. I knew it but forgot :-(
&gt; The point here is that is not a concrete type, as you said you can do nothing with it. It is a concrete type and it is even inhabited by a value. Most importantly it is very useful in Haskell programming for several things. You can use it as a sort of wildcard expression to be written later or use it to throw errors when testing if your code is as strict (or as lazy) as you want it to be. &gt; Unfortunately, it does not allow you to do `(1::Int) + (2::Float)` because it is very strict about typing which is usually good. You can do this with multi parameter typeclasses and [Functional dependencies](https://wiki.haskell.org/Functional_dependencies). Then it will infer the result type from the first two types, e.g. adding an integer to a float will result in a float. Here is a runnable example: {-# language MultiParamTypeClasses, FunctionalDependencies #-} module Main where import Prelude hiding (Num (..)) import qualified Prelude as Prelude class Num a b c | a b -&gt; c where (+) :: a -&gt; b -&gt; c instance Num Int Float Float where a + b = fromIntegral a Prelude.+ b instance Num Float Int Float where a + b = a Prelude.+ fromIntegral b main :: IO () main = do let a = 2 :: Int let b = 3.1 :: Float print (a + b, b + a) -- prints (5.1,5.1) 
Set (at least with how Eq is defined) would still not be a Monad interestingly. It wouldn't even be a Functor. Here is an example of the issue. data AllEqual a = AllEqual { unAllEqual :: a } instance Eq (AllEqual a) where _ == _ = True instance Ord (AllEqual a) where compare _ _ = EQ set = fromList [1,2,3] (map unAllEqual . map AllEqual) set = fromList [3] map (unAllEqual . AllEqual) set = fromList [1,2,3] So the Functor Law `fmap f . fmap g = fmap (f . g)` is broken by the above example. It is because the `Eq` can't reasonably have the property `if a == b then forall f . f a == f b`. Having this would mean things like Map and Set would have no Eq instance which is less than ideal.
&gt; but then again ads are not known for nuance There is an unsatisfactory lack of nuance here.
PHP to Haskell. My eyes read, but my mind won't believe. BTW, servant It's an amazing piece of typed software &lt;3
&gt; Edit: No, that would require restricting Monad to only "nullary" kinds, which would require adding sorts to the language. I guess that answers my original question, thank you! Can you expand on this? Also, sorts probably aren't necessary with TypeInType
Well, to make Monad work for sets (strawman syntax): data Set (a :: Ord) = ... instance Monad Set where ... We would need to make Monad polymorphic over kinds, like it's already done for Category: {-# LANGUAGE PolyKinds #-} class MyMonad (m :: k -&gt; *) where myReturn :: a -&gt; m a myBind :: m a -&gt; (a -&gt; m b) -&gt; m b But that doesn't work for all `k`, because the type `a -&gt; m a` makes no sense if `a` isn't a type. So `k` must be a kind that contains types, it can't be something like `* -&gt; *`. But there's no way to specify that restriction without sorts or something like that. I don't understand TypeInType, does it help with this problem?
# `exference` There is a also [`exference`](https://www.reddit.com/r/haskell/comments/3ad41d/ann_exference_a_different_djinn/) which a more recent version of `djinn` (it generates Haskell expressions from types) with support for polymorphic functions and type class hierarchies. [D] -&gt; (D -&gt; EitherT e (State s) [FB]) -&gt; (FB -&gt; FB) -&gt; State s [Either e [FB]] ----&gt; \ds g f -&gt; traverse (runEitherT . (fmap (fmap f) . g)) ds # `MagicHaskeller` The program [`MagicHaskeller`](https://hackage.haskell.org/package/MagicHaskeller) (interactive [website](http://nautilus.cs.miyazaki-u.ac.jp/~skata/MagicHaskeller.html), extended [abstract](http://nautilus.cs.miyazaki-u.ac.jp/~skata/Haskell2013.pdf)) takes an incomplete specification and synthesizes a program that fits it. f "abcde" 2 == "aabbccddee" ----&gt; f = (\a b -&gt; concat (transpose (replicate b a))) and f [(+ 3), (4 -)] 5 == [8, -1] ----&gt; f = (\a b -&gt; map (\c -&gt; c b) a)
&gt; So k must be a kind that contains types Is there any kind other than `*` which contains (inhabited) types? `#`, perhaps? 
[`&lt;$&gt;`](https://hackage.haskell.org/package/base-4.9.0.0/docs/Data-Functor.html#v:-60--36--62-) is exactly [`fmap`](https://hackage.haskell.org/package/base-4.9.0.0/docs/Data-Functor.html#v:fmap). fmap :: Functor f =&gt; (a -&gt; b) -&gt; (f a -&gt; f b) (&lt;$&gt;) :: Functor f =&gt; (a -&gt; b) -&gt; (f a -&gt; f b) (&lt;$&gt;) = fmap just infix. &gt;&gt;&gt; toUpper &lt;$&gt; "Hello" "HELLO" You can use [Hoogle](https://www.haskell.org/hoogle/) (or [Stackage Hoogle](https://www.stackage.org/lts-7.9/hoogle)) to search by operator, function name or even type signature!
To add a bit of advice, I suggest you add hoogle (if using cabal) or the stackage hoogle clone (if using stack) as a custom search provider in your browser as it is better than google when trying to understand functions like this. I have mine set to the keyword `h` so if I needed to lookup this function, I just type `h &lt;$&gt;` in the URL bar and I am instantly dropped into the search results on stackage's site which in this case, tells me that it is a synonym of fmap. https://www.stackage.org/lts-7.9/hoogle?q=%3C%24%3E https://wiki.haskell.org/Hoogle#Hoogle_Integration_Modes https://www.haskell.org/hoogle/ https://www.stackage.org/lts-7.9/hoogle 
Oh thank so much! I didn't realize this "Hoogle" existed.
Thank you!
When first learning Haskell it seemed like every abstraction brought its own Alphabetti-Spaghetti operator(s) so it's worth documenting the ones you will encounter often: * **Semigroup** + [`&lt;&gt;`](https://hackage.haskell.org/package/base/docs/Data-Semigroup.html#v:-60--62-) (from [`Data.Semigroup`](https://hackage.haskell.org/package/base/docs/Data-Semigroup.html), pronounced **mappend** or **em-append**), originally located at [`Data.Monoid.mappend`](https://hackage.haskell.org/package/base/docs/Data-Monoid.html) but is being [moved](https://prime.haskell.org/wiki/Libraries/Proposals/SemigroupMonoid). * **Functor** + [`&lt;$&gt;`](https://hackage.haskell.org/package/base/docs/Data-Functor.html#v:-60--36--62-) (from [`Data.Functor`](https://hackage.haskell.org/package/base/docs/Data-Functor.html), pronounced **fmap** or **map**), infix version of [`fmap`](https://hackage.haskell.org/package/base/docs/Data-Functor.html#v:fmap) (&lt;$&gt;) :: Functor f =&gt; (a -&gt; b) -&gt; (f a -&gt; f b) (&lt;$&gt;) = fmap + [`.`](https://hackage.haskell.org/package/base/docs/Data-Function.html#v:.) (from [`Data.Function`](https://hackage.haskell.org/package/base/docs/Data-Function.html#v:.), pronounced **dot**, **compose**, **of** or **after**) is an instance of `fmap` believe it or not (.) :: (b -&gt; c) -&gt; (a -&gt; b) -&gt; (a -&gt; c) (f . g) x = f (g x) Check out the [instance](https://hackage.haskell.org/package/base-4.9.0.0/docs/src/GHC.Base.html#line-638) instance Functor ((-&gt;) r) where fmap :: (a -&gt; b) -&gt; ((r -&gt; a) -&gt; (r -&gt; b)) fmap = (.) If you are in GHCi, try the following &gt;&gt;&gt; [fmap, (.)] [fmap, (.)] :: [(b -&gt; c) -&gt; (a -&gt; b) -&gt; (a -&gt; c)] &gt;&gt;&gt; :set -XTypeApplications &gt;&gt;&gt; :t fmap @((-&gt;) _) fmap @((-&gt;) _) :: (b -&gt; c) -&gt; (a -&gt; b) -&gt; (a -&gt; c) * **Applicative** + [`&lt;*&gt;`](https://hackage.haskell.org/package/base/docs/Control-Applicative.html#v:-60--42--62-) (from [`Control.Applicative`](https://hackage.haskell.org/package/base/docs/Control-Applicative.html), pronounced **ap**) + [`*&gt;`](https://hackage.haskell.org/package/base/docs/Control-Applicative.html#v:-42--62-) (from [`Control.Applicative`](https://hackage.haskell.org/package/base/docs/Control-Applicative.html), pronounced **then**) * **Monad** + [`&gt;&gt;=`](https://hackage.haskell.org/package/base/docs/Control-Monad.html#v:-62--62--61-) (from [`Control.Monad`](https://hackage.haskell.org/package/base/docs/Control-Monad.html), pronounced **bind**) + [`&gt;&gt;`](https://hackage.haskell.org/package/base/docs/Control-Monad.html#v:-62--62-) (from [`Control.Monad`](https://hackage.haskell.org/package/base/docs/Control-Monad.html), pronounced **then**), this is an older name for `*&gt;` made redundant by [this proposal](https://github.com/quchen/articles/blob/master/applicative_monad.md) * **Function application** + [`$`](https://hackage.haskell.org/package/base/docs/Data-Function.html#v:-36-) (from [`Data.Function`](https://hackage.haskell.org/package/base/docs/Data-Function.html), pronounced **apply**) is function application. It is very common and has an extremely simple definition ($) :: (a -&gt; b) -&gt; (a -&gt; b) f $ x = f x it can be defined in terms of `id` ($) :: (a -&gt; b) -&gt; (a -&gt; b) ($) = id + [`&amp;`](https://hackage.haskell.org/package/base/docs/Data-Function.html#v:-38-) (from [`Data.Function`](https://hackage.haskell.org/package/base/docs/Data-Function.html)) is flipped function application (&amp;) :: a -&gt; (a -&gt; b) -&gt; b x &amp; f = f x it can be defined in terms of `$` (&amp;) :: a -&gt; (a -&gt; b) -&gt; b (&amp;) = flip ($) or `id` (&amp;) :: a -&gt; (a -&gt; b) -&gt; b (&amp;) = flip id
Do you want it to only contain a `String` and a `Char`? In that case you can remove the `x` variable and do **not** need the pipe character (`|`). data NotTuple = NT String Char deriving Show `NT` is the one and only constructor for the `NotTuple` type &gt;&gt;&gt; :type NT NT :: String -&gt; Char -&gt; NotTuple that you use as such &gt;&gt;&gt; NT "Aloha" '!' NT "Aloha" '!"
I'm not convinced heavy type-level programming of the kind you often get into once you add `GADTs` and `DataKinds` to the mix constitutes good software engineering. Too much mucking about, too little payoff.
Thank you :) in that case you need *two* type variables `a` and `b` data NotTuple a b = NT a b deriving Show which gives you a constructor of type NT :: a -&gt; b -&gt; NotTuple a b where `a`, `b` can be any types you want (they can also be the same type). Compare this to the type of the built-in Haskell tuple (,) :: a -&gt; b -&gt; (a, b) In keeping with the original example, let's pick `String`, `Char` &gt;&gt;&gt; NT "Matane" '?' NT "Matane" '?' &gt;&gt;&gt; :t NT "Matane" '?' NT "Matane" '?' :: NotTupe [Char] Char (recall that `type String = [Char]`)
Agreed! Simplifying suggestions are even better but when GADTs fit they fit, this type defaultConfig :: XConfig (Choose Tall (Choose (Mirror Tall) Full)) could be written with type-level lists as defaultConfig :: XConfig [Tall, Mirror Tall, Full]
Awesome! Thank you so much this all makes so much more sense now.
It kind of does. That's what typeclasses are for. The mechanism doesn't support multiple "arities" directly; Haskell functions don't really have arity anyway, they're all unary (`a -&gt; a -&gt; a` being a higher-order function of type `a -&gt; (a -&gt; a)`, that is, it takes an `a` and returns a function of `a` to `a`). We can however make typeclasses that support returning either a value or a function by making both `a` and `MyTypeclass a =&gt; b -&gt; a` instances of `MyTypeclass`. This is what [`printf`](http://hackage.haskell.org/package/base-4.9.0.0/docs/Text-Printf.html#v:printf) does in order to support an arbitrary number of format arguments.
The latter is much less flexible, though; e.g. [LayoutCombinators](http://hackage.haskell.org/package/xmonad-contrib-0.12/docs/XMonad-Layout-LayoutCombinators.html) would have had to be part of the original design rather than a third-party library that somebody can just tack on.
Well, I'm aware that typeclasses accomplish something similar, I'm curious about specifically overloading function names without having to resort to type hackery like `printf`. Specifying a new typeclass for every overloaded function you want to introduce isn't a very satisfying solution in many cases.
One more resource to the list: [Operator Glossary](https://haskell-lang.org/tutorial/operators) :)
&gt; We don't charge for this stuff -- it's open-source -- and we get accused of inappropriately advertising? I didn't accuse you of "inappropriately" advertising. Perhaps I might have chosen a less loaded word than "ads", but in any case there undeniably is a marketing pitch to the post as a whole. That is not a problem. I merely objected to how a specific passage of the post was worded. That is all there is to it -- I have no dog in this fight. In fact, I use Stack myself, and find FPComplete's contributions to the ecosystem valuable.
&gt;Many problems are solved more easily with concurrency (networking, file I/O, video/image/document processing, database access, etc.) simply because the programming model is easier to understand. Wondering why you would say that. Could you elaborate? I find thinking concurrently massively difficult. With all the edge cases and locks and races. A linear program is infinitely more natural to understand. We get stuff like equational reasoning then. People don't use concurrency because the model is easier to understand, I cannot believe that is the case. People use concurrency for speed ups (in the use cases you mentioned). Nothing about making a path tracer concurrent is easy for example. But it's sure the way to go for raw speed. I think the argument would hold better if you'd say. Concurrent programming is more natural for problems that are concurrent by nature. Writing a server with an event loop is a pain in the ass, but firing up a thread per connection is easy. 
Why should the first version be picked in your first example? I can write a Show instance for functions. Not to mention that the result is, `Num a =&gt; a` which is already ambiguous without type defaulting.
I sympathize with your frustration, but have you considered that just as the perceived accusation may appear demotivating to you, the same applies to cabal developers who give their unpaid life-time to work on cabal and then see Stack marketed as if was *the* official tool for Haskell and perceive this as if their effort wasn't valued?
To avoid misunderstandings, I have edited my post and added a clarifying remark.
At first it was something that really bugged me in Haskell, coming from C++ and Python. However now I'm not too fond of function overloading and I rather prefer function with different names. This is the same problem for default optional arguments on functions. I prefer two well named functions than one ambiguous named one with two different prototypes. (Edit: english)
We might say that, in Haskell, any kind of overloading is expected to be reflected somehow in the types. That being so, some amount of type hackery is inevitable.
The alternative to concurrency in these cases is programming models like nodejs, based on nonblocking IO and callbacks. I think Chris's point - and I'd agree with it - is that proper thread based concurrency is much easier to reason about.
What about https://hackage.haskell.org/package/ssh ? (no idea on what it's worth, but it was easy to find) Otherwise, there is still the option of using a C library (like libssh).
I think the most convincing argument is first class functions, including overloaded ones. I don't know an OOP language that lets you seamlessly pass overloaded function to another function. In c++ in order to take a pointer to overloaded function, one has to spell whole type of the function. Too cumbersome for function name reuse. Java doesn't allow to pass methods, functions have to be wrapped into a class first. But you can't overload a class! Cute thing about Haskell type classes is you can overload on return type alone. For example, &gt;map read ["1", "2", "3"] :: [Int] [1,2,3] &gt;map read ["1", "2", "3"] :: [Float] [1.0,2.0,3.0] `read` is overloaded: &gt;:t read read :: Read a =&gt; String -&gt; a 
Maybe just by the number of reverse dependencies?
Well, one problem would be type signatures and type interference in general, I would think. For example, what should the type of ``` foo = map add ``` be? I could specilize it into ``` foo :: Num a =&gt; [a] -&gt; [a -&gt; a] ``` or into ``` foo :: Num a =&gt; [a] -&gt; [a] ```, but what if I don't want to specialize it and instead want to leave it generic? Typeclasses already do this sort of thing anyway and also make it explicit inside the type that said function is overloaded, so do we really need another way to do the same?
My original question was pretty much "why can't we have more fine-grained kinds that contain types?" I still think it's an interesting idea, but very hard to add to Haskell as is.
Yes! That's a much better metric.
Surely, this depends on what kind of software we're talking about. In aerospace or banking, the payoff you get is probably worth it, at least some of the time. Of course, the spectrum from imperative code to just a bunch of types, like Coq can produce, is immense, and the trade off should probably be weighed for each case. To be clear, I don't think the greatest advantage of this type level stuff is the actual program. The greatest value is that it forces the programmer to specify the operation of his program to such a level of detail that it's impossible not to fully understand the thing you're building. It's easy to "get something to work" by writing a bunch of imperative code, but to produce rock solid software, we really need to understand exactly what it is we're building, if we want to avoid all its pitfalls. I don't think there's any way around that. Also, there is huge value in interfaces that cannot represent invalid operations. It allows amateur programmers to use advanced functionality, without the risk of shooting themselves in the foot.
Wow, thank you so much! This is very helpful!!
&gt; or `id` &gt; &gt; (&amp;) :: a -&gt; (a -&gt; b) -&gt; b &gt; (&amp;) = flip id Uhhh, what?
Ohhhh, I see.
I want to thank the FPcomplete people for their effort to create resources for experienced programmers in other languages who are considering using Haskell as a real and effective alternative for their work. I took the liberty to translate the last snippet to transient, since it is a good canonical example: https://github.com/transient-haskell/transient/wiki/Concurrency-example-with-worker-threads
I think the code has a subtle bug Consider the line writeTBMChan responseChan (workerId, int, int * int) Last time I checked, this would only place a _thunk_ into the chan, which would only be forced once the value is printed, meaning all the work is actually done in the printResults thread, thereby destroying the parallelism you have worked so hard for. 
The thing that struck me about this article is how easy it is to jump in and play. curl -sSL https://get.haskellstack.org/ | sh 
I'm trying to remember when we decided to *stop* avoiding success at all costs...
Well, that's an effect of purity. If the code were heavily `IO` bound (avoiding lazy `IO` oddities), then the parallelism is useful. In the example, this is exactly the case, as the "work" is a `threadDelay` in `IO` =P If you want parallelism with pure code, traditional `IO` style concurrency is the *wrong* approach. In that case, you want to be taking advantage of the `Eval` monad or the `Par` monad. The `Eval` monad is pretty magical, using the spark pool to get concurrency at a cost even lower than Haskell's green threads.
I've tried using curl for sftp and first of all, the curl implementation is very slow. This is true for just the c library not Haskell itself. Also I simply could not get it to work. I tried to debug the bindings and couldn't find anything wrong as far as API compatibility goes, but your results may vary. https://github.com/GaloisInc/curl/issues/15 This also exists, but it's not finished : https://github.com/noteed/sftp-streams
&gt; And is there any alternative to stack for real-world development out there? Yes. `cabal` is getting better. But Nix is already as good or better than Stack in terms of capability; it's just not as straightforward and easy to use.
Another option is to use [process](http://hackage.haskell.org/package/process) (for a `String` interface) or [process-extras](http://hackage.haskell.org/package/process-extras) (for a `Text` or `ByteString` interface). The stability is listed as "Unknown" for both of these packages, but they are commonly used, and the `process` package in particular is a [core library](https://wiki.haskell.org/Library_submissions#The_Libraries). (FWIW, I have used both in production for years without issue.) I would use one of the above packages to implement a module that calls the `sftp` command on the system (provided by [openssh-client](https://packages.debian.org/jessie/openssh-client) on my system). I would probably put it into a module named something like `Network.SFTP.Process` and use the exported API via a qualified import as follows: import qualified Network.SFTP.Process as SFTP Then, if you want/need to implement a different version in the future, you can put it in a different module (such as `Network.SFTP.Curl`) and use the same interface, so that you only have to change the qualified imports in your application code. Note that the trickiest part with using the `process` package is, as with any `IO`, the error handling.
I believe it was sometime around 9 March 2012
`&gt;&gt;&gt;` is a spelling of `&amp;` which is actually more general: (&gt;&gt;&gt;) :: Category cat =&gt; cat a b -&gt; cat b c -&gt; cat a c Some other common ones you didn't mention: * **Applicative**: `&lt;*` (why not, if you are already mentioning `*&gt;`?) * **Functor**: `&lt;$` (why not, if we are already mentioning `&lt;*` and `*&gt;`?) * **Monad**: `&gt;=&gt;`
Right. The logger shouldn't be concerned about evaluation strategies since it shouldn't be doing the work to begin with. 
Not sure who is the *we* here, but my take is that you can read it two ways. First way, the user base should be kept small so that the language could be changed easily. The user base isn't small anymore, and it's been a while now that some changes, such as those in base, are handled very carefully. Backward-compatible changes (such as those enabled by compiler extensions) are still added regularly. The other way is that you should not compromise for the sake of popularity. I don't see how spreading the good word is hurting anyone, and it will benefit those who want to work in Haskell.
It is. I try hard to avoid making inflammatory comments, and to spread a message of inclusion and rationalism. It pains me to make comments like that one, because I want this to be an inclusive community that isn't riddled with flamewars. But we keep seeing these posts that seem almost like they're designed to divide people, and I just want that to stop.
This is about concurrency, not parallelism. The two are related but not the same. In concurrency, you want to service simultaneous needs without delay. The underlying calculations don't necessarily have to happen in parallel, as long as you can service the needs in time. Haskell's non-strict semantics will often handle that correctly by default, but you do need to pay attention to it. In this (contrived) case, I don't need the squares of the numbers until they are to be printed. If in a real application those calculations are expensive and may not always be needed, putting them into the chan as a thunk could very well be exactly the behavior you want. In other cases that could be a memory leak, so you would override the default behavior by forcing the calculation before it goes into the chan.
&gt; Broken things with easy fix are not fixed &gt; No documentation. "follow the types". These are both just a matter of time. Lots of people in the Haskell community are trying to do what they can, in a limited time budget. Getting all the details right takes a huge investment, and while we might all wish we could always make that investment, the truth is we can't. I'd love for more people to have more time. No one is opposed to that! But that's a case where wanting it doesn't make it happen. &gt; A programmer ask for how to print something and he is told about comonadic metafunctors Okay, this is a legitimate concern. But if people are here to think about "comonadic endofunctors" (which... wouldn't that just be comonads? A comonad is always an endofunctor), then more power to them! Perhaps there's a better way, as a community, to distinguish between what's practical to use in general code, and what's part of the research world. On the other hand, what draws a lot of people to Haskell is that there isn't an iron wall between practical programming and the research world. But in the end, I agree that a lot of people who have become fluent in manipulating categorical abstractions don't understand exactly how unusual that is, or how much it limits their audience. &gt; The wheel is reinvented again and again To be honest, this is one of my favorite things about Haskell, and something that would be very disappointing to give up. And I think it's the essence of the motto "avoid success at all costs". In that motto, "success" means letting ourselves be stopped from trying things and doing things better, or be pressured to crystallize and decide things before we know the right answer. Of course, as the Haskell community grows larger, we're getting more sensitive to the needs of people with established code bases, and things like the AMP had a years-long migration path to minimize disruption. I hope we always take that with the caveat, though, that it should always remain possible to try new things, iterate, be patient as we find the right answer, and to go back and do something right when those new things work out.
Yes, I know. I did not expressed what I waned to say very well. What I wanted to say is that function calls are resolved at runtime depending on the type of the first argument, just like most OO languages. Haskell matches all arguments at compile time, but evaluation is lazy, with currying. They are very much opposite.
There are already two commercial haskell 'groups' that I know of: the [Industrial Haskell Group](http://industry.haskell.org/), which funds specific projects improving the Haskell ecosystem, and [commercial Haskell group](https://github.com/commercialhaskell/commercialhaskell), which is mostly a discussion and collaboration group.
If I understand you correctly, you want the compiler to automatically choose the right type based on context. Extending your second example I suppose you want let x = [add 1 2, add 2 3] print (fmap ($ 5) x) print x to print `[8, 10]` followed by `[3, 5]`, right? So basically, `x` has two different types with different implementations, and the compiler has to choose the one that matches. This can be easily extended to expressions having an arbitrary number of types. This is not unthinkable, but very quickly leads to situations, where type inference is either very inperformant (the compiler has to check every combination), ambigous, or even undecidable. That is why we don't have this. Instead we have type classes, which provide a subset of these possibilities, but on a much more theoretically grounded level. A different thing would be disambiguating ambigous identifiers (not expressions) locally based on types. This would not work in your example, because `add 1` is still ambigous, but could be used for example to disambiguate between `Prelude.map` and `Data.Map.map` without explicit quantification based on the type of the argument given. This is implemented in e.g. Idris and also commonly proposed as an extension for Haskell.
In addition to all the other resources, I recommend reading [this](https://blog.jle.im/entry/inside-my-world-ode-to-functor-and-monad) for a basic intro to functors and monads, and [this](http://blog.sigfpe.com/2006/08/you-could-have-invented-monads-and.html) when you want a deeper understanding of monads.
There's no reason the two schools of thought need to be disjoint. Commercial interest and academic interest are perfectly compatible. There is just a little less effort being put into the commercial needs than there should be right now.
Check out [the paper](http://www.cs.nott.ac.uk/~pszgmh/monparsing.pdf)! Does a good job of explaining what parser combinators are and how monads are a good fit. This paper actually really helped me solidify my understanding of monads.
&gt; Missing IDE tooling. Off today, willing to do some hobbyst haskell activity and play with https://www.fpcomplete.com/blog/2016/11/comparative-concurrency-with-haskell Booting emacs (spacemacs) knowing that a new version of `intero` has been released a couple of weeks ago. Eager to see all the nice new goodies while playing with concurrency bits. Guess what ... I still get the old `intero` version. It looks like this PR has been ignored for 17 days: https://github.com/commercialhaskell/intero/pull/304 Well never mind, I guess I will give it another try another day on my next day off. Isn't this little story a bit ironic ?
This is the rosy view that an accommodated theorist like you have of the situation. You have Haskell the way your want it to stay. As an academic toy with some prestige in industry, but not used practically, surrounded by a aura of mystery and a community of people that want to be like you. Do you mean that is exciting to look at people talking endlessly about doing traversals of lists, fibonacci numbers and factorials when more interesting and real problems are ignored? Have you seen the advances in Scala or F# in scientific calculus, machine learning or distributed computing??? Seriously, that Haskell is not widely used in Industry after 20 years when it is the best language means that there have been a great effort in making it fail. I'm sorry but I can not contemplate neither the situation neither the haskell community with good mood.
Plus. The academic efforts put into Haskell are a large part of its commercial value. Advanced features like STM would be considered academic and immature by many, but we've had a mature implementation for a decade in Haskell.
It took me far too long to realize "vector-length function" meant a mathematical vector, not an array... Need to grab some coffee!
It is indeed inclusive. if that is your only desire and you are not bored and upset like me by his self masturbating inanity, then it is what you want it to be.
Just that one is colluding with haskell.org while the other is independent and that shows in the outcome.
Indeed even if the computation is IO bound it can have pure unevaluated thunks. [My recoding using transient](https://github.com/transient-haskell/transient/wiki/Concurrency-example-with-worker-threads) does not have this problem since the next computations in the flow (`printResult`) run multihreaded, within the threads spawned by the workers. That is a problem of libraries like async that can not chain computations in a monad an keep multiple threads running. Details in the link.
Seems like the only reasonable way to go. Thanks for you input.
:D why that specific date?
&gt; Presumably at some point in the next 5 years we'll need WaylandMonad. IMO the best timing is now. Given that there aren't many mature Wayland compositors yet, `WMonad` would have a decent chance of obtaining a good portion of the market share.
Let's be clear, Stack and Stackage wouldn't have existed if they weren't predicated by Hackage and Cabal.
I read it a while back, but IIRC it's not really Haskell specific. Code is in another ML dialect (can't remember which?) in addition to Haskell. The book is really useful in describing ways to think about performance in an immutable (and lazy) setting. I don't think it is really useful in learning Haskell, so I would complete the Haskell book first.
I learned to mistrust overloading in other languages, and wonder if there's an actual case where overloading is really useful in a language like Haskell, with all its tools for slicing and dicing functions and their arguments. It seems like OOP overloading is a short cut that ropes you off from more interesting and useful things. For example, your second example is a little odd. In real life, I'd expect to be handed structures like this: ``` let x = [(1, 2), (1, 2), (1, 3), (5, 4)] ``` So, you're more likely to actually write a function like this: -- Pick one of your add functions add :: Num a =&gt; a -&gt; a -&gt; a add a b = a + b processX x = fmap (add 5) $ fmap (uncurry add) x Thinking about this leads one to think "hey, why do I need to hardcode the add function and the global argument?", which leads to this: processAnything :: (a -&gt; a -&gt; a) -&gt; a-&gt; [(a, a]) -&gt; a processAnything f x xs = fmap (f x) $ fmap (uncurry f) xs processX = processAnything add 5 Now I can do your second example, but with any operation, not just add. This seems a lot more useful to me than what I get with OOP-style overloading. I know this is a contrived example made to illustrate the point, so I don't want to be too hard on it. But I'm having a hard time understanding what problem OOP overloading would solve in Haskell that isn't solved in some better way. 
Thank you!
I'll concede that. But can we retire them now?
No. Nix is an equally powerful solution to Stack, and relies on Cabal and Hackage. Plus, Stackage is only a place for stable and maintained packages. There's no reason would should expire other code.
Anyway, why wasting the keystrokes here, in a comment thread that's gonna get forgotten soon anyway - you have any idea where one could raise this issue? Or maybe stackage guys are already aware of it? Pinging u/snoyberg maybe (with all due respect to all stack/stackage contributors, that's the name that comes to my mind first).
can you explain? I just set up xmonad, and read through the docs, last week. do you mean how message/somemeasage is like exception/someexception? 
Consider me pinged :) The relevant repo/issue tracker is https://github.com/fpco/stackage-server. If someone's interested in taking a stab at a PR, that would definitely get this moving even faster. I agree that the ordering of search results leaves something to be desired. Fun fact in case it's useful: you can add `+package-name` to your query to search a specific package, or simply use the (newly added) Hoogle search on a package page like https://www.stackage.org/package/aeson.
This was a great article. I think the main obstacle for Haskell becoming mainstream is the absence of a unified story around just about any use case. It's all "some assembly required; use your best judgment". The prototypical example of this is the community's collective inability to pick a winner between `pipes` and `conduit`; newbies will come in and ask for a recommendation, and the community will give them anything but that. Beginners don't want this. They want a straight path to results, they'll pick up new skills and better judgment along the way. A nice serving of analysis paralysis is *not* a good a good way to start your Haskell journey.
&gt; Methinks this is going to result in a Trump-style uprising in Haskell-land. Ouch... comparing commercially competent community members who care about moving Haskell forward to Trump is quite below the belt.
The parallels are there, though. To a first approximation, the Trump camp does not know or care about policy; while (again, to a first approximation) the commercial Haskell camp does not know or care about higher theory. In both cases though, what people *do* know and care about is how their respective ecosystems' [Brahmins](http://unqualified-reservations.blogspot.ca/2007/05/castes-of-united-states.html) are running things in a way contrary to their interests.
I haven't read the book myself, are you referring to this book: http://haskellbook.com? I've heard very good things about it, so I imagine you'd be best served finishing that up. Otherwise, here are some good resources that others have found useful for learning *haskell* (not necessarily functional programming in general): * https://github.com/bitemyapp/learnhaskell * http://dev.stephendiehl.com/hask/ Some may argue that SICP is still a valuable text to read through, though I haven't, myself: https://mitpress.mit.edu/sicp/full-text/book/book.html
Yeah, there are three places where upStr &lt;- upCase s seems to have been cut off. Maybe a Wordpress bug? Edit: It's fixed now!
I think the article is best read if you ignore the *inflammatory* bits ;-)
Listing reasons why you should or should not read something is common practice. It allows you to quickly discriminate information that is or is not useful for you.
The moment I figured out monad transformers, I became a fucking zealot
At this point, I believe the Haskell community has made a lot of efforts to attract beginners. Nice beginner friendly tutorials, books, ... I even wonder if the quality of such materials is not one of the best out there. Unfortunately that does not make Haskell an easy language to learn. And intermediate to advance documentation is still lacking. I don't know of one single book that target intermediate Haskellers. I am afraid that to make Haskell shines more, difficult problems needs to be tacked. Here are some examples: - Text/String/ByteString collusion (hopefully solved with `backpack` adoption in a distant future) - Much more popular Haskell applications used/known in the wide (Golang is quite popular these days thanks to Consul, Docker, ...) - First class IDE support (hopefully solved in a distant future) - Some (stylistic/practice) convergence (`traverse` == `mapM`, `for_`, import, effects, ...). Whatever you do in Haskell there are dozen of ways to do it (I am afraid this is both a curse and a blessing and will never be 'solved'; I am personally ok with it and see that side as part of what Haskell is) 
&gt; Of course, as the Haskell community grows larger, we're getting more sensitive to the needs of people with established code bases, and things like the AMP had a years-long migration path to minimize disruption. Finally someone understands the reasoning of Haskell's unofficial motto! Haskell is wonderful *because* adoption is low which has facilitated decades of rapid^†, principled refinement. † subjective to other highly adopted languages with prominent stability concerns.
Well it's easy to understand how to translate Purely Functional Data Structures to Haskell. And I'd be surprised if the Haskell examples in the appendix didn't compile today. If you were interested in the book for its content, I'd say it's still a good read.
Would you consider remote applicants?
No, it wasn't. It's very conceited to think you know why I wrote something, and the fact that you've on multiple occasions attributed meaning to my words which isn't there has created controversy where none need exist. I'm trying to recruit people in the community to work on this outreach with me. If someone thinks cabal-install isn't a problem, they're not going to agree with my recommendations. I wanted to weed out complaints of "how dare you say that Stack is the only solution" by telling them to turn back early. If I want to make snide remarks at people, I'll make snide remarks at people. If I want to tell people that a blog post isn't for them, then I'll tell them it isn't for them. Period. By the way: inflammatory _was_ definitely relating to your comments. I was hoping people would realize that some people on /r/haskell tend to get upset, and that's why I requested the blog post not appear here.
Why couldn't it just be left as unknown until its used? I imagine that allowing such a thing would require a significant amount of changes to the way the compiler operates, but I was wondering whether there was a more fundamental reason for this.
Well, we've had a mature interface for a decade. https://www.youtube.com/watch?v=c0E9iSYGNtU
I didn't intend to write that Stack is the only build tool; rather I most likely got carried away with selling Haskell that I was making a much sillier claim that "all/most Haskell projects have reproducible build [plans]". I've reworded it to "Projects built with stack will be reproducible". For us at FP Complete, reproducibility is the big driving force in our work. Cabal-install and Nix other alternatives.
May I remind you that something very similiar happened [not too long ago and the same comments apply here as well](https://www.reddit.com/r/haskell/comments/504de5/follow_up_haskellorg_and_the_evil_cabal/d7126bm/)? There is undeniably a noticeable correlation between your blogposts and the kind of discussions that result from those postings. Would it really have detracted from your blogpost if you had left out the inflammatory parts?
Framing this as an "inclusivity" argument is probably not correct. I believe it's addressing the wrong problem. The question that one should be asking is whether we really need multiple tools solving the same narrow set of problems in the first place? Is diversity really required here? Havent we given diverse ideas enough time to be tested? is it time to pick the best ideas and unify them into a single tool and make it the best package/environment/build management tool out there? At what point does "survival of the fittest" begin to kick-in?
The delayed release of intero has been frustrating (I wrote the qualified import completion and still run into trouble with it wanting to install the old version!) but I'm happy to extend substantial patience [in this case](https://github.com/commercialhaskell/intero/issues/295). 
I was pretty clear in my previous comment that I had reason to say what I said. Just about everything I say these days is interpreted by someone on this subreddit as inflammatory. For example, my [previous blog post](http://www.snoyman.com/blog/2016/11/haskells-missing-concurrency-basics) was apparently also inflammatory. I've given up trying to anticipate what this subreddit will interpret one way or another, and just write things as I want to say them. And seriously: stop it with this "noticeable correlation." I've been attacked for every action I've taken in the Haskell community for years. I have gotten massively popular feedback from dozens (if not hundreds) of users because of initiatives like Stackage and Stack, and yet I'm attacked here regularly for it. I'm not going to stop trying to improve Haskell because some people get upset about it.
Thank you. Almost every action I've taken in the Haskell community has been seen as divisive by _someone_. I get attacked for creating Stackage (it's going to encourage PVP violations), for creating Yesod (who knows why now), or get called out for FUD because I write an article on exception handling. I'm not going to bother trying to guess what is divisive and what isn't. There was absolutely 0 part of me that ever considered the possibility that my previous blog post would bring down the ire of this subreddit on me, and yet it did. I've given up dealing with this place. I'm going to write what I want, and mostly avoid reading the discussions here. Maybe the rhetoric can tone down at some point. Maybe you can be part of helping that. And heads up: there's an inflammatorily titled blog post coming out tomorrow. It's not targeted at this subreddit, I hope it doesn't get posted here at all. It's intended to get attention outside of the Haskell community and dispel some misbeliefs.
&gt; Nix is a different beast though. Nix is not a competitor to Cabal or Stack. Absolutely. For 99.9% of people (perhaps super Nix experts excepted) Nix is the wrong solution to manage Haskell dependencies. Use stack or cabal-install for that and use Nix to manage the non-Haskell dependencies of your project. The Haskell-specific package managers have a much, much better UI for working with Haskell projects. This isn't surprising, and doesn't say anything bad about Nix.
Serialisation of `Int` isn't architecture-dependent – if it was, it would break serialisation of *any* structure containing an `Int` somewhere in it. See https://github.com/GaloisInc/cereal/blob/master/src/Data/Serialize.hs#L227: -- Ints are are written as Int64s, that is, 8 bytes in big endian format instance Serialize Int where put i = put (fromIntegral i :: Int64) get = liftM fromIntegral (get :: Get Int64)
how long has nix been in development? is it a Haskell specific tool at all? or is a completely different approach to general package management?
You should also have a look Idris’ `Eff`ects, they’re a really nice solution for a similar problem. Unfortunately, they just don’t work as well for Haskell’s simpler type system.
I really don't complain about the delay of the release. I just don't understand why something that is released (or tagged as one) is not yet available after 26 days (or so it seems to be) because of a detail (or so it seems to be) while a PR (that seems to solve it) has been ignored (no comment) for such a long time. As an outsider, it is difficult not to interpret this as either "I must be totally dumb" or "this is pure arrogance" which in both case it not a happy feeling ;-)
I agree. There needs to be some mechanism of picking the best ideas, making them better, and driving adoption. 5 different ways of solving the same problem, in perpetuity will not help anyone. There's a time to try new ideas, and there's a time to consolidate. No one seems to be consolidating. 
Great write up! It cements some intuition that's been building in my mind for a while. Thanks!
Any reason why stackage can't be extended to have an uncurated resolver, which gives the same effect as Hackage?
that tech stack &lt;3
Well to a certain degree, you can actually do this. It just depends on Hackage =P Specifically, you can use `ghc-xxx` as your resolver, which doesn't use any resolver and therefore gives you no 3rd party packages. Then it's up to you to put all your dependencies and versions (including indirect dependencies) in `extra-deps`. But the thing is, using `extra-deps` this way will probably go out and depend on Hackage instead of Stackage. It will definitely do that when `extra-deps` depends on something not on Stackage, such as a brand new version of a package. So yes, Stack can do this. It just does so through Hackage so you can't get rid of Hackage. Though the `nightly` resolvers keep things somewhat up to date. And you can always use `extra-deps` to get arbitrary Hackage packages regardless of resolver.
I like this comment. &gt; I understand the Text vs. String issue, but what does ByteString have to do with it? Text and String are character containers, while ByteString is a byte container. Totally right. &gt; I think all the controversy caused by AMP has shown that it's not popular to break old stuff, so it's not likely that return, mapM, etc. will disappear anytime soon. I'll continue my theme that this kind of conservatism is what's holding us back from being principled, not the haskell-lang.org people (who can be rude but who don't suggest compromising the language). Keeping things like `head` in the Prelude is a compromise for convenience. As long as `head`'s in there though we won't have a principled Prelude. I think "avoid success at all costs" suggests we move fast towards the more principled Prelude, we can make an `OldPrelude` or something for those who want the convenience.
Came here to say this. They are "dual" in some sense, but the order of the ops definitely matters for implementation.
[This blog post on categories](http://sordina.github.io/blog/2016/03/05/1457137265-categories-for-greedy-bastards.html) opened a window in my mind to the generality of composition. Here's a short example: import Control.Category ((&gt;&gt;&gt;)) comp0 = times3 &gt;&gt;&gt; plus2 where times3 = (*3) plus2 = (+2) comp1 = times3 &gt;&gt;&gt; plus2 where -- same line of code as comp0 times3 = logging (*3) plus2 = logging (+2) logging f m = m &gt;&gt;= \x -&gt; print x &gt;&gt; pure (f x)
`&gt;&gt;&gt;` is actually [`.` from `Control.Category`](https://hackage.haskell.org/package/base-4.9.0.0/docs/Control-Category.html#v:.), flipped Thanks for adding to the list, `&gt;=&gt;` is useful but for beginners the best use of it is to [express the](https://ghc.haskell.org/trac/ghc/ticket/12672#comment:4) [`Monad` laws](https://ghc.haskell.org/trac/ghc/ticket/12683) (a &gt;=&gt; b) &gt;=&gt; c = a &gt;=&gt; (b &gt;=&gt; c) return &gt;=&gt; a = a a &gt;=&gt; return = a compared to (a + b) + c = a + (b + c) 0 + a = a a + 0 = a
&gt; I'm really hoping he does what he says, and writes whatever he wants, however it comes out. [...and whatever the cost?](https://www.reddit.com/r/haskell/comments/50389g/resignation/)
Thanks for writing this up! I was thinking "span" would be helpful in some way but wasn't sure the right approach. Very cool
Yep. I was referring to Obsidian Systems, the group behind reflex. I benefit a lot from their open source libraries and their helpfulness in #reflex-frp and awesome workshop at Compose Conf last year - OS is just real class act. Not to exclude Will Fancher, whom I've never met but indeed has some really interesting things on github :)
I can see why you're thinking that. The way I see it, many people just grew tired of his knee-jerk reactions, pushy attitude, obvious lack of patience and self-righteousness. That's a pity, because he's obviously a very talented person with lots of things to say from a pragmatic perspective - we definitely need more of his kind. I don't want to be unfair or abuse anyone, I just think his message is obscured by all these silly polemics. I am not telling him to shut up. I'm suggesting that he focus on what he has to say.
In GHCi you need to write `@(_ -&gt; _)` &gt;&gt;&gt; :set -XTypeApplications &gt;&gt;&gt; :t id @(_ -&gt; _) id @(_ -&gt; _) :: (a -&gt; b) -&gt; a -&gt; b **Edit**: Unless of course you write the *slightly* awkward (**very** awkward for longer, nested expressions or expressions with contexts) &gt;&gt;&gt; :t id @(a -&gt; b) :: forall a b. _ id @(a -&gt; b) :: forall a b. _ :: (a -&gt; b) -&gt; a -&gt; b With ticket [`#11350`](https://ghc.haskell.org/trac/ghc/ticket/11350) &gt;&gt;&gt; :t \@a @b -&gt; id @(a -&gt; b) \@a @b -&gt; id @(a -&gt; b) :: (a -&gt; b) -&gt; (a -&gt; b)
Especially when speaking his truth to power. And a cheap shot to link the personal decision of an individual, who maintained a nuanced view throughout, and imply blame. "I hope that by stepping back I can continue to retain or perhaps regain some of those friendships that recent events have strained." Let's respect that request. 
I remember that whole debacle, and I remember which post of Snoyman's was the *catalyst* so to speak. When I read that post, I agreed with Snoyman whole-heartedly. In my view, it was very clear that the Haskell committee wasn't acting "as it should". The fact that Snoyman brought that to light, and that eventually lead to Kmett resigning from it, shouldn't be blamed on Snoyman. Especially since that(at least to me) Kmett's resignation basically reads like "I signed up for GSoC and some stuff, and all this drama wasn't supposed to be a part of it." which isn't the same as "I feel like I being personally attacked." But maybe I didn't understand Kmett's resignation as he meant it, and he felt like he was more central to the conflict than it appears to me. I can't read minds, obviously. IMO, if the committee had been "behaving", then there would have been no reason for a blog post, and there would have been no drama. PS: I quote the words where I lack a better word. Please don't think I'm berating the people in the Haskell committee. 
&gt; At what point does "survival of the fittest" begin to kick-in? When you have something that is so much better that everyone switches to it... that's sorta the definition in this context. This process involves cross-pollination as well, hence `cabal new-build` which is better than stack in some respects. Anyway, doesn't stack use cabal under the hood? https://docs.haskellstack.org/en/stable/faq/#what-is-the-relationship-between-stack-and-cabal
Something [type-indexed `Typeable`](https://github.com/ghc-proposals/ghc-proposals/pull/16) could help with?
Although we aren't doing this at work right now, I've been thinking about using Reflex on the backend in WebSocket apps to help with CQRS design. The idea being that a WebSocket server is a function `Event t IncomingMessage -&gt; m (Event t OutgoingMessage)`. With this you can set up dynamic, event based systems. So on top of it, you can build the query part of CQRS as a function `Dynamic t Query -&gt; m (Dynamic t Response)`, with the command part being some arbitrary `Event t _`, which is useful for a realtime app. And I even think such abstract functions of dynamics could be used to get a free REST API out of the WebSocket API in `servant`-style. I've got a small [proof of concept](https://github.com/ElvishJerricco/reflex-websockets/) which is in sore need of an update to the reflex it depends on (blocked on some changes to reflex I want to make). It's nothing substantial, and it only hits the first mark, but I'm hoping to build on it.
There is no doubt that Snoyman is one of the most active and the most vocal supporters of Haskell in this community. Currently, I don't think he has an equal, not in terms of how much he is doing to get Haskell more widespread. To me, it can sometimes appear that people take other people's opinions too seriously in this subreddit. Even though Snoyman is a "a big guy" in Haskell, when he writes blog posts on his own blog, those are just his opinions. If people disagree, they can just disagree, and not take the fact that his opinions differ from their own as some personal insult. An example would be the list at the beginning, of people that don't need to read this blog post. It seems that some in this thread are trying hard to defend some hypothetical group of people that *may* have those opinions. Point being: he isn't attacking anyone in particular, and he isn't attacking a group of people. He's simply stating: If you have opinion X, Y or Z, then the contents of this blog post will probably not be for you. One part of the problem may be the fact that this is still a rather small community. It's nice in so many ways, but that inevitably also means that if someone speaks out against some opinion that some, not entirely negligible subset of the community might have, that subset may still be rather small and those people may feel targeted as a result. I also think there may be some bad blood, and that people should let it be water under the bridge. At any rate, whenever I run into blog posts from Snoyman on this subreddit, I always read the entire post first, and then the comments. Inevitably, on matters like this, there are always more comments than there are upvotes. And inevitably, I can't see how so many people can be on the defensive, because I never really feel like there is anything to be defensive about in the blog post. I just wish we could use all the energy we waste on drama like this and put it towards improving Haskell adoption in one way or another. 
Well, that is entirely different from what I got from Kmett's resignation on reddit. I guess things were much more complicated.
I think there was a little bit of debate about what should be on Haskell.org and then A LOT of debate about who said what. Nobody talks about the actual issues and tradeoffs, only who said what, who was inappropriate or not, who has hidden agendas or not, etc. Just like I'm doing now. Someone once told me that once a debate has degraded into "who said what" it's time to stop.
Wow. I just read that post and that just blew my mind. Of course it makes sense in hindsight. I should've realized that composition is "more general" than the way one uses it normally, but I didn't. I think I'll be using `(&gt;&gt;&gt;)` and thinking in categories more often now. It is also very appealing to be able to just lift the constituents into `Kleisli` and get monadic composition for free.
We are on it ;)
A shorthand for `putStrLn . show` is [`print`](https://www.stackage.org/haddock/lts-7.10/base-4.9.0.0/Prelude.html#v:print)
This fails if the number doesn't parse, for that [`readMaybe`](https://hackage.haskell.org/package/base-4.9.0.0/docs/Text-Read.html#v:readMaybe) is a start readMaybe :: Read a =&gt; String -&gt; Maybe a
Thank you for bringing my attention to MagicHaskeller. I've never heard of stuff like it before, but I've been interested in working on projects like it for a long time.
It's the tech stack endgame.
I don't feel the "abuser mentality" line of enquiry will lead to anything good, so let me suggest a different perspective. Here in /r/haskell, discussions about the substantive issues relevant to this discussion have featured uncomfortable levels of trolling, done in the name of all sides (skimming the comments to this post reveals a number of examples). That seems to be getting worse lately (at least it seems so to me). A significant chunk of this trolling has consisted of attacks against Snoyman. At the same time, many of those who read and comment here, being committed in varying degrees to all sorts of stances, aren't at all trolls. Somewhere along the way, wires are getting crossed. Consider, for instance, the dispute around the word "inflammatory" in this thread. What I would qualify as a minor stylistic flaw in the title of an otherwise entirely normal blog post has led, through back-and-forth escalation between good-faith participants, to half of the subreddit taking offence in the name of stances that hardly anyone actually supports, as [pointed out](https://www.reddit.com/r/haskell/comments/5eac5i/spreading_the_gospel_of_haskell/dabof3k/) by /u/saevarb . Meanwhile, you, I, Snoyman, and everyone else waste energy in the ensuing debacle. Everyone else, that is, except for the trolls... Perhaps there is something going wrong in this place, after all.
I think it is best to use a `Map String Int` for each of the things you are keeping track of, let's say you have two counts [ ("aaa", 1), ("bbb", 1) ] [ ("aaa", 2) ] Is `[ ("aaa", 3), ("bbb", 1) ]` the answer you'd expect after combining? If so, you should use [`unionWith (+) :: Map String Int -&gt; Map String Int -&gt; Map String Int`](http://hackage.haskell.org/package/containers-0.5.8.1/docs/Data-Map-Lazy.html#v:unionWith) import qualified Data.Map as M a, b :: M.Map String Int a = M.fromList [ ("aaa", 1), ("bbb", 1) ] b = M.fromList [ ("aaa", 2) ] combining them gives &gt;&gt;&gt; M.unionWith (+) a b fromList [("aaa",3),("bbb",1)] Does keeping track of 3 separate maps sound reasonable for your problem M.fromList [ ("toto@aaa.com", 1), … ] M.fromList [ ("org", 3), … ] M.fromList [ ("aaa.org", 2), … ]
&gt; Haskell’s simpler type system Do you mean dependent types, row types, other? I read through http://docs.idris-lang.org/en/latest/effects/index.html but I don't grok it enough to say what it's built on. 
I'm not suggesting that overloading in this fashion would or could be used to create a `printf` function. The parent poster was suggesting that I look at how `printf` works to support functions with multiple arities in my code, which I thought was unnecessarily complex for most use cases. Imagine having to resort to `printf` style type hackery to create overloaded smart constructors. Pretty unwieldy. My question is merely, why doesn't or why can't Haskell support function overloading similar to what is found in Java, C++, C#, etc. Is it something fundamentally at odds with the type system, or a design choice?
/u/snoyberg I just wanted to point out that the phrase is [chock-full](https://www.google.com/search?q=chock%20full) not 'chalk-full' :)
I agree and disagree at the same time. Sure people can do whatever they want. You cant control what people do in their time. But you can have a central body/process that blesses some of these projects and commits to their continued growth and success. And we already have that in the form of a Haskell.org committee. And I'm sure you agree to the previous paragraph otherwise you shouldnt really be rooting for a central package repo as well. Central and de-centralization are both necessary and a balance needs to be struck between them. You cannot root for one extreme. So, the only thing that we're debating out is which tools/libs need to be centralised and to what extent, at this point in time. 
[removed]
Chris Okasaki's book is THE BOOK for functional data structures. The language in the book is ML augmented to apply for lazy evaluation. You can dig up articles in Functional Pearls for most (if not all) data structures from Okasaki in haskell (pretty version agnostic versions). If you can follow the augmented ML, implementing all the stuff in that book in haskell would be a great learning experience. I'm sure a bit of googling will produce a bunch of haskell implementations of everything in the book. 
In the example of `data FormData a = FormData String` it would be returning the `String` invalidate :: FormData a -&gt; String invalidate (FormData str) = str You can use an existential type if you don't care if it's validated or not data SomeData where SomeData :: FormData a -&gt; SomeData
&gt; It certainly has some UX and documentation issues, but they can be overcome Compared to other system package managers, Nixpkgs' weakness on documentation is more that offset by its massively more principled approach to things. I'm on NixOS. &gt; being easy to learn isn't the only measure of good UX Totally agree. &gt; I think it would pay off in the medium or long term for anyone working on non-trivial packages with non-Haskell dependencies I agree that managing non-Haskell dependencies with Nixpkgs is great. My issue is entirely with managing Haskell dependencies. Nix proponents keep suggesting it for that and I don't thing it's a good suggestion. There's just no gain in correctness, and there's a big loss of convenience. Between `cabal freeze` and Stackage snapshots both the other package managers have the same correctness story as Nixpkgs. And the convenience loss is big. No more just writing `foo-1.2` in my `stack.yml` to get a different version of `foo` from my current snapshot. Instead I'd be downloading `foo`, checking out `1.2`, running `cabal2nix` on it, and hoping for the best. 0.1% may be an exaggeration, but I think most Haskellers should be sticking to a Haskell-specific package manger for their Haskell dependencies. EDIT: Oh hey, you're the Tikhon I've emailed with (I'm Ian Jeffries)! Hopefully I'm not coming across as too much of an ass here.
EDITING MY POST TO SHOW COMPILER ERROR
for me, that's /r/haskell (but yeah, it's not always recent news). 
I know that perhaps this is not possible since haskell is both immutable and statically typed, but is there any way to modify the type of the existing validated string to make it invalid without generating a new invalid instance of form data? Or any kind of workaround that has the intended effect of invalidating form data in place?
There is probably research into that, maybe someone else can answer. What kind of an event would invalidate data? If it changes because some file or online access control list changed then that check is already effectful so a mutable reference wouldn't be so bad. Information flow control (IFC) may be something to look into
Unindent `main` and your specific error will go away, but you'll then get type errors related to the other responses. module Main(main) where import Data.Complex main :: IO () main = do a &lt;- getLine b &lt;- getLine putStrLn $ show (a :+ b) This compiles and runs: `main` is at the left column, and there's a `$` between `putStrLn` and the thing you want to put, because Haskell is treating your original code as `putStrLn (show) (a :+ b)` and not `putStrLn (show (a :+ b))`.
Maybe `vector magnitude` would have been a better choice of words.
Meh, I would advocate using the fish operators before using `Kleisli m a b`. `Kleisli` is nice when you need to use it with functions that take arbitrary `Arrow`, `Category`, or `Profunctor` instances. But in general code that doesn't need this, it's cleaner just to use `(&lt;=&lt;)`.
That sounds like resource tracking, something I'm not overly familiar with but look into [linear types](https://ghc.haskell.org/trac/ghc/wiki/LinearTypes) ([Embedding a Full Linear Lambda Calculus in Haskell](http://functorial.com/Embedding-a-Full-Linear-Lambda-Calculus-in-Haskell/linearlam.pdf)), affine types, /r/rust etc.
This looks really useful. One of the most important properties of shell scripts is portability, and stack solves this. I didn't know I could use stack like that. I definitely think I will be writing some Haskell script in the future that use fsnotify. I tried in BASH, but it just gets too complex too quickly.
What you're talking about is sometimes referred to as "type state". This is related to "linear types" (and "affine types", which is what Rust has). The short answer is that, you can encode much of it in Haskell, using "indexed monads" or "parameterized monads" (I'm not actually clear on the difference between the two), but it's not especially convenient. Dependently typed languages like Idris have a better time with this; one of the leads on Idris recently [announced](https://edwinb.wordpress.com/2016/10/21/state-machines-all-the-way-down/) a paper talking about how to do it.
Idris always makes me want [alternative bindings](http://docs.idris-lang.org/en/latest/tutorial/interfaces.html#pattern-matching-bind) readNumbers : IO (Maybe (Nat, Nat)) readNumbers = do Just x_ok &lt;- readNumber | Nothing =&gt; pure Nothing Just y_ok &lt;- readNumber | Nothing =&gt; pure Nothing pure (Just (x_ok, y_ok)) — (yes ;) I know about [`ExceptT`](https://hackage.haskell.org/package/mtl/docs/Control-Monad-Except.html)) — idiom brackets and `!`-notation too...
Haha, no worries :). I've been using Nix to manage Haskell packages at work, and haven't had that experience. I just wrote a couple of helper functions to get packages from git and from Hackage (at specific versions), so adding dependencies not in the default channel is easy, whether they're in Hackage or not. It's just as easy to fetch from other version control systems or even just to download a tar ball or zip file. You have to be a bit more explicit by including a sha256 hash, but that feels like the responsible thing to do anyhow. It's still awkward, but I think it's really close to being great: it just needs a few library functions and some examples. When I'm more confident about my current setup, I'll try to release something like that. Longer term, I'm just going to run my own Nix channel to manage all our dependencies including our in-house packages—infrastructure that works just as well for non-Haskell development (ie Python and R). A CI system can both test our code *and* build it, giving people binary downloads of both internal and external packages while also giving us a lot of control over everything.
"Composing", "Higher form" not to mention "Kleisli arrows"!! These words will put some one coming from a programming background to sleep in no time. If you want to explain Haskell stuff to imperative programmers, you have to use the terms they are familier with. So when an imperative programmer is reading a Monad tutorial like this, and they see these kind of stuff, their mind is going to shutdown and congratulations, you have just made Monads a bit more elusive in the minds of the reader... &gt; The "how" is pretty easy for most people... But these tutorials never show the "how", and assumes that people already know how do notation de sugarers and how a typeclass definition is just like an Interface definition that they are familier with. Only after they understand it, they can start to consume stuff like these. I think, 90% perceived magic with Monad's reside in how the 'do notation' desugars? Instead of that they go like Moands are awesomablahgoblabalblablabla....and the reader does not move one bit closer to having a good understanding of the thing. I speak from my experience as a beginner.
Thank you so much for your answers. I'll definitely use Data.Map, the insertWithKey is also a nice option for what I want. Regarding the "parts" of the emails I want to keep, it's actually more like: "keep as many characters - backwards - that match in sequence in all emails in the given list". I don't plan to split them at specific locations.
&gt; But these tutorials never show the "how", and assumes that people already know how do notation de sugarers and how a typeclass definition is just like an Interface definition that they are familier with. The blog post in question shows the "how" quite directly. Anyway, I agree that the theoretical mumbo jumbo that I'm talking about is not directly a good approach to beginners. That wasn't really my point. My point was that monads aren't as simple as understanding the type signature of `&gt;&gt;=`.
Haskell '98? Where we're going, we don't need Haskell '98 ;) 
/r/purescript is over there =P (in all seriousness, I do actually wish Haskell required explicit forall and had scoped type variables by default. It's just more awkward that it *isn't* this way)
Nice, I'll try that. 
Agreed on scoped type variables (+ something like like [`#12540`](https://ghc.haskell.org/trac/ghc/ticket/12540))
 (&amp;) :: forall a b. a -&gt; (a -&gt; b) -&gt; b (&amp;) = flip @(a -&gt; b) @a @b (id @(a -&gt; b)) !
Keep in mind Rust (the community and ecosystem itself) has a very powerful sugardaddy capable of funding all sorts of cool stuff. Haskell does have the talent supply to be like Rust, but it doesn't have the money. Not even close.
OP for one.
bencoding is just one of many serialisation formats which exist, and isn't particularly good or bad for any particular task. The next iteration of the binary package is likely to use the CBOR format, because it has excellent speed and space tradeoffs which defining a more structured format than Binary currently does.
I'm very pleased to hear there's work going on to make diagrams faster! It's a great lib, but rendering speed can become a big problem.
Well, even Emacs doesn't have a powerful sugardaddy.
Well Mozilla is in the extremely rare position of being a non-profit that actually has some power (as opposed to the "please sir i want some more" that would result if you or I tried to start one) since they get to choose the default search engine for 300 million users. This singularly accounts for [90% of their money and more than $300 million annually](https://en.wikipedia.org/wiki/Mozilla_Foundation#Financing).
&gt; dependent types Probably this. It allows the combinators that work with Eff to do type-level append/union and remove/subtract of the list/set of effects. I'm personally not a fan of `Eff`-like approaches because most of them don't respect the fact that monad transformers do not commute add so it's difficult to distinguish `StateT s Maybe r` from `MaybeT (State s) r` in them; and I've needed both of those before, separately.
One thing that quite annoyed me last time I used it was handling input/output processing of sub processes. Although there are quite some functions for that, I felt that I always wanted to do things I couldn't easily. Also, IIRC, sub processes aren't killed cleanly when the parent process is killed.
I wrote a [small downloader script](https://github.com/danidiaz/oplss-downloader/tree/master/src) and wrote about it [here](https://medium.com/@danidiaz/pet-project-report-a-small-downloader-utility-38d4367263ad#.cb75neqho).
I’d be interested in hearing more about the product you’re building, since I haven’t heard of Wrinkle yet. The website is still empty, and “next generation communication platform” is also just sales talk. What’s next generation, what are the issues with the current one? What is communication, is it chat, is it voicechat, is it both, what’s the difference to established communication platforms? What’s a SaaS communication platform, is it like Skype? If yes, what makes it *not* Skype?
That’s an issue of the particular Eff implementation. Lists are just as non-commutative as monad stacks are. (Also, I’m constantly confused about which order my ExceptT and StateT should go, so I regularly ask Lambdabot to ?unmtl it for me)
Just to be clear, I didn't suggest that things shouldn't change because of outdated documentation, I just pointed out that it prevents convergence. It's quite optimistic to say that outdated learning material is doomed. LYAH is still being recommended to newbies, and it's not updated after AMP - it even explicitly points out why Applicative isn't a superclass of Monad.
 Alternative f =&gt; (t -&gt; f a) -&gt; (t -&gt; f a) -&gt; t -&gt; f a by adding redundant parentheses you can see that it's really a binary operation on functions, with a polymorphic source Alternative f =&gt; (t -&gt; f a) -&gt; (t -&gt; f a) -&gt; (t -&gt; f a) This should make you think of the `((-&gt;) t)` instances (**nota bene**: I will use operator sections as proposed in [`#12477`](https://ghc.haskell.org/trac/ghc/ticket/12477) and write `(t -&gt;)` instead), for example the `Functor (t -&gt;)` instance lifts `a -&gt; b` to `(t -&gt; a) -&gt; (t -&gt; b)` (this is just function composition: `.`) fmap @(_ -&gt;) :: (a -&gt; b) -&gt; ((t -&gt; a) -&gt; (t -&gt; b)) The `Applicative (_ -&gt;)` instance lets us lift a single value (this is `const x _ = x`) pure @(_ -&gt;) :: a -&gt; (t -&gt; a) and `liftA2 @(_ -&gt;)`, `liftA3 @(_ -&gt;)` let us lift arbitrary functions liftA2 @(_ -&gt;) :: (a -&gt; b -&gt; c) -&gt; ((t -&gt; a) -&gt; (t -&gt; b) -&gt; (t -&gt; c)) liftA3 @(_ -&gt;) :: (a -&gt; b -&gt; c -&gt; d) -&gt; ((t -&gt; a) -&gt; (t -&gt; b) -&gt; (t -&gt; c) -&gt; (t -&gt; d)) The `(_ -&gt;)` instances *always* trip people up, it took me a while to get it, especially stuff like sequenceA @[] @((-&gt;) _) :: [t -&gt; a] -&gt; (t -&gt; [a]) but it becomes a very natural way of thinking, for example if you ever see aORb :: Char -&gt; Bool aORb ch = 'a' == ch || 'b' == ch you may notice that these are actually two functions ('a' ==) :: Char -&gt; Bool ('b' ==) :: Char -&gt; Bool with `||` lifted over them aORb :: Char -&gt; Bool aORb = liftA2 (||) ('a' ==) ('b' ==) Using “idiom brackets” which aren't built into Haskell but are available as a [library](https://hackage.haskell.org/package/applicative-quoters-0.1.0.8/docs/Control-Applicative-QQ-Idiom.html) we can write aORb :: Char -&gt; Bool aORb = [i| is 'a' || is 'b' |] where is = (==)
It was a list of *people* who should and shouldn't read the article, which is more pointed.
Thanks, I'll give it a try next time. Really looks like a great API! I have been using `process-conduit` for stuff where it's critical to quit.
It's all clear then. The plan is as follows. 1. Write an open source browser in Haskell 2. Collect $300 million 3. Fund useful/entertaining Haskell stuff Seriously though, a rendering engine written in Haskell would be truly awesome. And an immense task to write. Perhaps we could use some bits from the Mozilla Servo engine, written in Rust, since it's a bit more Haskellish than what all the other popular engines are written in?
Wow cabal really is improving a lot.
Since you're using stack, you basically already know which versions of packages you're getting because you choose a snapshot. So in your cabal file, you can simply specify the packages you are using without any version bounds, and then upgrade the snapshot in your `stack.yaml` file, and run `stack build` again. PS: If this is a bad idea for some reason, someone let me know.
That's a first step. The issue of pushing security holes information to the user was also raised here https://www.reddit.com/r/haskell/comments/4uta1e/proposal_tracking_security_holes_and_major_bugs/
&gt; function calls are resolved at runtime depending on the type of the first argument, just like most OO languages. No, multiple dispatch resolves which method to call depending on the types of all arguments. And if the dataflow inference (which is an optimization-only heuristic and allowed to give up and say Any for code that's too complicated for it to handle) can determine the types of all inputs at compile time, then the method call can get inlined and optimized without any runtime dispatch cost.
Perhaps I need to understand the snapshots better but presently I'm just using: resolver: lts-5.17 So if I keep that to using a recent snapshot from stackage then I'd be alright? Each stack install of build would update accordingly? I have a hangover from maven days where depending on snapshots are a bad thing if you're in production. ( ͡° ͜ʖ ͡°). P.S. Thanks!
Oh, you can use burritos to glue things together? Great, I'm going to go try that now.
In what way does `ListT` model alternatives better than `MaybeT`? Especially since `Maybe`, when viewed through its `Alternative/Monoid/MonadPlus` instance, is the *canonical* way to model alternatives. In fact, I'm certain that there is some way to express that by showing that `Maybe` is a universal object in some category, but I will leave that as an exercise.
If you keep that snapshot as-is, then the packages will not update, because the versions of those packages are frozen in that snapshot. If you change the snapshot to a newer snapshot, and then rebuild, the packages will be updated. The snapshots are basically a collection of packages that are frozen at certain versions that are known to build together with those versions. There is a problem with that approach, however. It isn't guaranteed that all the packages in your current snapshot have made it into a newer snapshot. I think all packages are migrated to a newer snapshot automatically, if that is possible. Naturally this can't be done every time, since it may happen that packages X depends on Y, Y is updated, but X isn't, and Y's change breaks compatibility with X. You can diff snapshots [here](https://www.stackage.org/diff/lts-7.8/lts-7.8). I also think that the `stack solver` command may be able to figure out the newest snapshot that works with your combination of packages, but I'm not entirely sure. It doesn't say so in the documentation, but I know I've seen it happen when migrating a project from just using cabal to using stack. Perhaps just backing up your `stack.yaml` and then running `stack init` will do it for you.
My book has more emphasis on 'pure' Haskell programming and covers IO, network programming, and using databases lightly. Real World Haskell is a great resource for handling IO, etc. I think I own almost every Haskell book ever written. My goal in writing this book is to provide a gentle introduction and still cover interesting examples.
ListT allows you evaluate all the alternatives, letting you pick from them. While MaybeT limits you to one. Worth noting, Alternatives are just a special kind of monoid. So naturally `[]` is still the universal object (ignoring bottoms of course).
Right, lists aren't *about* alternatives. But they still offer the most information. Can you prove your last statement? Alternatives are monoids in the category of endofunctors under a cartesian product of functors. That is, the product is: newtype Product f g a = Product (f a, g a) A monoid under this product is a functor `f` that supports empty :: Const () ~&gt; f (&lt;|&gt;) :: Product f f ~&gt; f -- where ~&gt; is natural transformations type f ~&gt; g = forall a. f a -&gt; g a Given the similarity to products in Hask, I'd be surprised if the free monoid for this were not he same as the free monoid in Hask, aka lists
`!`-notation, I envy... I don't really see the advantage of alternative bindings from that example; in Haskell, `fail` would be `mzero` would be `Nothing`, wouldn't it?
You can bump the LTS version and you should get any backwards compatible updates, but that does require a change to your `stack.yaml`. If you don't want to change your metadata and still get updates, that can be accomplished with Hackage and Cabal. Cabal allows you to specify a range of version bounds that your package can work with. This allows you to specify something like `aeson &gt;= 1.0 &amp;&amp; &lt; 1.1` which will allow any backwards compatible security updates / bug fixes through when you do `cabal update`. Stack is designed to give you reproducibility, i.e. pinning everything to one specific version. So it's a tradeoff...you can't pin something and at the same time allow it to change automatically with no action on your part. With Cabal you can also pin things if you choose (i.e. `cabal freeze` or manually set the version with `aeson == 1.0.2.1`), so Stack is strictly less powerful than Cabal's model in this regard.
Yes it is a less common construction, but this construction has also existed in typeclassopedia for some time.
This is in both `IO` and `Maybe`, and short-circuits the `IO` to return immediately if the first `readNumber :: IO (Maybe Nat)` returns `Nothing`. It's like using `MaybeT`.
Stack does all that for you. Just follow https://haskell-lang.org/get-started and don't let anyone tell you otherwise.
If you want that for any types x and y, you can just do: data NotTuple x y = NotTuple x y Then you could have values like: NotTuple 3 "a" :: NotTuple Int String NotTuple True 3.4 :: NotTuple Bool Float NotTuple (NotTuple 3 "a") True :: NotTuple (NotTuple Int String) Bool etc. Not sure if that's what you're looking for.
First of all, changing the type of an existing variable *is* possible in some systems, see the research on [strong updates](http://www.iro.umontreal.ca/~monnier/tr.pdf) for example. It's not possible in Haskell though, because variables are immutable so neither their value nor their type can be changed. Second, there's nothing wrong about creating new variables to view the same object with a different type. You shouldn't think of it as "generating a new object", but merely as creating a new view via which to look at the same object. Consider the following example. import Data.IORef -- | -- &gt;&gt;&gt; example1 -- about to add one -- Left 43 example1 :: IO () example1 = do ref &lt;- newIORef (Right "foo") writeIORef ref (Left 42) putStrLn "about to add one" r &lt;- readIORef ref case r of Left x -&gt; writeIORef ref (Left (x + 1)) Right _ -&gt; error "how can it be a Right?" r &lt;- readIORef ref print r Here, I have a mutable reference of type `IORef (Either Int String)`, which means it can either hold a value of the form `Left someInt` or a value of the form `Right someString`. With the normal way to use such a reference, with `readIORef`, we have to handle both forms. This can be annoying if we know that we've just put a value of a particular form in it just a moment ago. Wouldn't it be nice if we could specify that within a particular block of code, we want to consider the reference to have type `IORef Int` instead of `IORef (Either Int String)`? -- | (fake syntax, doesn't actually work) -- -- &gt;&gt;&gt; example2 -- about to add one -- Left 43 example2 :: IO () example2 = do ref &lt;- newIORef (Right "foo") writeIORef ref (Left 42) magicallyChangeTheTypeOfRef $ do -- now ref has type (IORef Int) putStrLn "about to add one" x &lt;- readIORef ref writeIORef ref (x + 1) -- now ref has type IORef (Either Int String) again r &lt;- readIORef ref print r But this can't really work. What happen if other blocks of code have already type-checked with the previous type? -- | (here's why it doesn't work) -- -- &gt;&gt;&gt; example3 -- about to add one -- type error: (Right "adding one") + 1 example3 :: IO () example3 = do ref &lt;- newIORef (Right "foo") let logMessage :: String -&gt; IO () logMessage msg = do writeIORef ref (Right msg) putStrLn msg writeIORef ref (Left 42) magicallyChangeTheTypeOfRef $ do logMessage "about to add one" x &lt;- readIORef ref writeIORef ref (x + 1) r &lt;- readIORef ref print r So if we could change the type of `ref` within a block, it would be unsound to do so, in the sense that it would introduce type errors. So instead of trying to change `ref`, let's try to prevent `ref` from being used within the block, while still allowing it to be modified via a specialized `View Int` which behaves like an `IORef Int`: data View a = View { readView :: IO a , writeView :: a -&gt; IO () } fromLeft :: Either a b -&gt; a fromLeft (Left x) = x withLeft :: IORef (Either a b) -&gt; a -&gt; (View a -&gt; IO r) -&gt; IO r withLeft ref x body = do writeIORef ref (Left x) body $ View { readView = fromLeft &lt;$&gt; readIORef ref , writeView = writeIORef ref . Left } -- | -- &gt;&gt;&gt; example4 -- about to add one -- Left 43 example4 :: IO () example4 = do ref &lt;- newIORef (Right "foo") withLeft ref 42 $ \view -&gt; do putStrLn "about to add one" x &lt;- readView view writeView view (x + 1) r &lt;- readIORef ref print r This works, and we did not need to allocate a new `IORef`, but we still need to prevent the original `ref` from being used within the block. We cannot do that while also allowing arbitrary IO such as `putStrLn`, but there are techniques to define a custom set of allowed effects, with free monads for example you could allow `putStrLn`, `readView` and `writeView` but disallow `readIORef` and `writeIORef`. This post is long enough though, so I'll let you look it up :)
Hey there! Would you consider junior Haskell developers, or is it more of a senior role? Thanks.
Username doesn't check out¯\\\_(ツ)\_/¯ 
No matter which path you take, you won't need to uninstall -- ghc installations live side by side. If you want a tool that manages various installations for you and downloads them automatically (when the long-term-support release for them is available) then that's stack. If you want to download a more traditional installer that puts the tools in your path in the standard way and then gets out of the way -- then that's the minimal platform. With the latter, when you want to upgrade to a new release, you download and install the new release -- but many releases can live side by side. (Also, note that the minimal platform includes a stack binary, and that the traditional installed ghc and stack-managed ones can coexist just fine together).
Does someone know if there's a stack command to change the `resolver` in a `stack.yml` file to the latest Stackage snapshot?
That doesn't make sense. How can there be an isomorphism between those two types? And if `Alternative`s are monoids under cartesian functor products, and if `Maybe (f _)` is the universal one of these, then why isn't `First a` the universal monoid under cartesian Hask products? The universal monoid should be the free monoid, right? Well the free monoid under cartesian functor products is still the list functor, by the same theorem that proves the free monoid in Hask, and the free monad.
It's nice to use some bounds in your .cabal file if you know what they should be. I try to get my projects to be buildable in both stack and cabal. But I only add version bounds lazily as I discover problems.
I don't think it is fair to lecture him like this for a blog post that had anyone else made it no one would have cared or objected. It can very well be read as Michael trying to speak his mind without stirring up controversy. Seeing 95 comments (mostly meta) on this rather harmless blog post is like seeing an infected wound poked. I can understand that some people have had it (which is probably why he asked for this not be to posted here, never mind if realistic or not). Those people however should try to be graceful and avoid a fight over each single thing that can be taken the wrong way. I find the moralist approach taken by some people here at times quite disturbing. Getting defensive when you're approached and accused like this is a natural (and probably healthy) psychological coping mechanism. There is more than the direct way to call someone evil. Being told off and shamed for things blown out of proportion is at least as damaging especially if multiplied over occasions and persons. We owe it to ourselves to try to hit the reset button on relationships that went downhill from time to time and if we can't do that at least not to keep digging.
&gt; Java doesn't allow to pass methods [It does, actually](http://stackoverflow.com/questions/20001427/double-colon-operator-in-java-8). 
Just to be fair to all parties, portability over time is solved by stack, but portability over OS is solved by Haskell itself.
You may be interested in https://github.com/well-typed/binary-serialise-cbor 
better yet, use something that's specialized on managing system tool installation like NixOs or traditional system packagers (see also http://downloads.haskell.org/debian/ btw) which provide binary distributions configured &amp; optimized specifically for your current OS release, allow for automatic updating, and also take care of installing any additional system dependencies (c compiler environment, c development libs &amp; headers and so on). 
I do a similar thing, except I use a custom prefix and avoid sudo (for GHC). Something like: ... ./configure --prefix="${HOME}/.tools/ghc/${GHC_VERSION}" make install ... And I have the following in my bash_profile to switch between GHC versions. function ghc_version { ghc -V echo "changing ghc version to $1" export PATH="$HOME/.tools/ghc/$1/bin":$PATH ghc -V } Life is good.
How does it compare speed wise to 1) other scripting languages and 2) vanilla Haskell. 
Nix is great for advanced users, but it's pretty heavyweight and takes some investment to get up and running. I wouldn't recommend it for someone who is new to Haskell like the OP.
i've always wondered what benefits nix provides or what it even is? it looks like a linux distribution? or is it something i install on top of my current os?
You're not the only one who is baffled about all that irrational Stack-hating but maybe [this](http://www.snoyman.com/blog/2016/08/haskell-org-evil-cabal) can provide you with some context. 
Let me try to give a complete answer to the broader question. I'll likely get some stuff wrong, though, particularly since I don't have much experience with stack. Corrections are welcome. If you know that there is a security bug in a package, you should always first add a version bound to your cabal file. * If you use stack, then assuming the version bound wasn't already satisfied, it will refuse to build the package. You would first try to switch to a more recent snapshot; and if that isn't possible, add the package to `extra-deps` in stack.yaml. * If you use cabal-install, then it will try to solve the new version bound automatically to obey your new constraint on the next build. But, being cabal-install, it will pay little attention to leaving everything else the way it is, and may break things. That leaves the question of what to do about security bugs that you're not aware of. What you want here is to try to use recent versions of all packages, as a policy. * When using stack, you should try to update your snapshot regularly to avoid this, which gives a pretty reliable upper bound on age of the packages you use. * When using cabal-install, in theory, this should again happen automatically. However, that property is fragile, as it can be broken by any package version upper bound - whether on your own package, OR in any of its (transitive) dependencies. None of these answers is ideal, for reasons that are hashed over again and again. Ironically (and this is something that often gets missed in these discussions), Stackage makes the last point about cabal-install considerably better, since it does a fair job of making sure that a lot of packages are buildable together at recent versions. So even in a hypothetical world where `cabal new-build` takes over as the consensus best way to build Haskell projects, we'd still want Stackage around. There are a few things I think we could do better: 1. Implement an advisory system in Hackage for versions of packages that are known to be "bad", so build tools could warn about using them. But this requires some kind of rough consensus on what "bad" means. 2. Distinguishing between "untested" and "known bad" versions of dependencies in Cabal files. This has been proposed a number of times, particularly during discussions on package upper bounds.
I'd think most of it. Though, I don't know if it would be possible to use the existing `Monad` type class, so you might need `RebindableSyntax` to make things look good.
Why would you want a Complex String?
Doesn't sound like you did anything wrong. We probably need to see code, including build configs.
Or /r/haskellquestions
&gt; Pretty much everyone loves stack Exhibit A: https://www.reddit.com/r/haskell/comments/3lax6y/discussion_thread_about_stack/
Well that was over a year ago, only shortly after Stack was released. Community has mostly converged since then
Figured it out, it just doesn't automatically link, I made a Makefile that pulled out the generated js and html and I'm all set. For such a powerful tool, this is extremely poorly documented.
Probably
I'd love to believe that... but then again... why is it then that nit-pickers are still getting hung up on whether Stack is Haskell's build tool vs Stack being a build tool for Haskell? And why is haskell.org still promoting cabal and the HP rather than only stack?
Wait what? That doesn't sound right.
If you install stack, you'll want to ignore `~/.stack` in your backup systems. It'll dump gigs of stuff in your home directly.
The big thing the nix package manager provides is isolation between packages. It lets you install several different versions or builds of packages if you need to, and drop into a shells with particular combinations available on your PATH. NixOS is a linux distribution which manages just about everything with nix isolation, but you can also install the nix package manager onto your current OS. Nix won't interfere with normal system packages at normal places, and I use it to manage a handful of programs at more recent versions than my main distribution provides.
&gt; Just wondering, how does one set upper bounds with confidence, not knowing the future? For executables it doesn't matter, cabal-install users should just do whatever's convenient. If a particular package releases a lot of breaking changes and you don't want to deal with them right away go ahead an put an upper bound on it. More importantly you should always use `cabal freeze`, and you should commit this to the repo. This ensures your builds are reproducible. Library authors should follow the PVP. EDIT: This is just my opinion. I think it's right though=)
I'll use a self-deprecating example to clumsily mask my stereotypical adherence to FP elitism. I'm in Russia right now, and I was recently in a cafe with some guys from our company. The waiter took our order, and in doing so, used the incorrect declension of tea. As soon as he left, there were immediate remarks about what a total dunce he must be to make such a stupid mistake. Yet these same people tell me I speak amazing Russian. The bottom line is I speak Russian *so badly* it's not even worth mocking or correcting; I'm literally in an entirely separate category of badness that isn't even worth criticizing. So when you see the top thread in /r/haskell is full of complaints about Haskell, the actual reason is because it's good enough to complain about. (And the "good" here is by FP ideals; Haskell fails quite miserably at being good when judged by, for example, Rust's ideal of "good").
Would it get new packages automatically (on each `stack build`), or only after running some command (eg `stack check-stackage` or something). If the former you can get the same effect using the command @sjakobi points out below: `stack config set resolver nightly`. If the latter this goes against the principled approach of stack, that each package's stack.yml file should specify its exact Haskell dependencies exhaustively.
Is there a best practice for cleaning out old LTS versions, stack-work, old ghc versions, etc?
&gt; If you think that programming languages should succeed on purely technical merits, and silly "marketing activities" like writing newbie-oriented tutorials and making engaging screencasts is unfair competition, you shouldn't read this blog post. While I don't think they *should*... imagine the merits of one that *can*. I think Haskell can be that one.
If you want a function `forgetValidation :: FormData Validated -&gt; FormData Unvalidated` that never allocates new memory you can use `Data.Coerce`, which is basically like `unsafeCoerce`, except it checks that the types are actually compatible. In particular, with data FormData a = FormData String you can use coerce as `FormData Validated -&gt; FormData Unvalidated`, or even `Maybe (FormData Valiated) -&gt; Maybe (FormData Unvalidated)`, and it just returns the same pointer under a different type (unlike removing and reapplying the constructor, which might allocate a new value that shares just the string). One restriction on coerce is that you can only use it if the constructors of the type you are changing are accessible, so not exporting the constructor is still enough to keep clients from violating encapsulation, even if you use coerce to efficiently implement some type-changing functions. For this particular example, you could also make FormData a newtype, which means values of any type `FormData a` are actually implemented as just a `String` at runtime, and adding or removing the constructor doesn't actually generate any code.
hey thank you for that description i'm going to investigate using it to manage packages, i didn't realize i could use with my current system like that.
Yes indeed, almost the same, except I suggest both for `lts` and `nightly`. I would not complicate them with additional options, it is obvious that they are not for reliable rebuild. The documentation could say: not for package release or production build, use them only during development at your own risk. Giving the information what version currently is used and a warning when version is changed, as it was suggested in that discussion, is a good idea.
Unfortunately, interns are a relatively low priority at the moment. But we could make it work for the right candidate.
hey thanks that sheds some light on previous confusions i had when first learning how to get up and going with haskell i've always felt like there are these different entities telling me to get started in different ways.. and turns out there are!
Thanks for the info; definitely enjoy stack right now, was just curious what the drama was all about :D
I'm really impressed with the effort people are making in Haskell tooling, specifically w.r.t. Stack and Intero, but there's still a *long* way to go. If Haskell ever wants to see wider adoption, it desperately needs to fix the IDE situation. Emacs is not an answer. Yeah, Emacs support is great. But how many developers have I worked with, in the last 6 years of working as a developer, who use Emacs or Vim? Only one. Me. Most developers do not use these text editors, not because they're not smart enough, but they either don't want to, or don't have the time. Or both. And they're paid to work, not to configure emacs.
I bought the book and got through a little bit two thirds of it, and it is pure gold. From the get-go I have been able to write actual code, that are not mere exercises, for my own projects. I'm still going through it, yes, but my haskell related happiness is sky high (Though I must admit, discovering Intero and Stack have also helped a lot). So many things just clicked after reading them from the book. I have a C++ background, and was soon going to start to look for related jobs, but after finally *getting* haskell, I can't go back. The price of the book is, IMO, lower than its worth. Also, the puns! Getting a laugh out of you every now and then makes you relax, helping a lot with understanding.
We should form a committee to investigate.
Ah! That makes more sense! Thank you.
[removed]
Is it really a problem to have two different "Haskell language" websites? Aren't users perfectly capable of comprehending the current situation, where two different groups of actors each have their own view of what Haskell is or should be? So, rather than try to contain all Haskell language-related information in one, monolithic site, why not explain to users that there are two such sites, due to a disagreement, and leave it at that? I'm not saying that two distinct Haskell language sites is a good thing, so if this incentive is received well by both parties, I'm all for it. I'm just saying that I find it perfectly understandable if someone wants to have their "own" Haskell language site. I don't see any reason that all Haskell language-related information needs to exist in one place, nor do I think users demand it either. I think this whole situation is simply a symptom of Haskell growing more popular. FP Complete is making money off Haskell, and pouring it back into sites like haskell-lang.org, and tooling, and I think it's wonderful. Let's no ruin this gift by assuming that all such companies, which may want to spread the good word of Haskell, are all willing -- or even capable -- of fitting their content into the haskell.org site. There is no such site as cpp-lang.org simply because C++ is way too popular, and I personally think that's where Haskell is going too, and that we're just beginning to see the symptoms of this.
I tried googling "haskell". haskell.org was result 1. I couldn't find haskell-lang in any of the 18 result pages. Same goes for "haskell language" which somehow listed haste-lang &amp; clash-lang though. So haskell-lang.org effectively doesn't exist for a new comer, but the majority of people visiting a web site about a programming language are new comers. It is a problem until a new comer friendly site is on page 1 of a google search for "haskell".
I hope this is the start of a process of healing the rift in our community.
If you are doing development in a team setting, using Debian's packages will always be inferior to letting stack do it. As mentioned elsewhere, this is manual dependency management. 
[This is how I imagine committees operating](https://www.youtube.com/watch?v=55fqjw2J1vI)
Ok. Thanks!
Email theme directly to tell them you want to help. 
I am ok with sharing data as a JSON, but want to make sure all the various parts agree on the shape of the data. 
&gt; You might wonder: won't this be inefficient to marshal the data through this intermediate Data.Vector.Vector type and then to marshal the data into a separate Data.Vector.Storable.Vector type before mmaping the data? Actually, the vector library should be smart and remove the intermediate representation and go straight to the final Data.Vector.Storable.Vector representation if inlining and rewrite rules all fire correctly. Whoa. That is awesome! Thanks for the detailed post and advice here. 
谢谢！
That helped me finish this: https://github.com/codygman/csv-mmap-caching
I hope this download-page committee will be able to resolve the downloadgate crisis! Seriously though, I'm glad that almost all members are known for their "get stuff done" attitude. So maybe haskell.org will become a hub for all things haskell for newbies and professional haskelleres, wherever they are interested in academic or commercial side of haskell. Of course we don't have a "community manager" on payroll and HWWG members have a day job and families, so content can only come from haskellers who are interested in getting it on haskell.org. So I think primary concern for HWWG should be the removal of barriers to get your content to show on haskell.org. So that people who want to announce conferences or meetups or any news important for significant portion of haskellers are posting it to haskell.org first, and reddit or mailing lists second.
"Just point"? Did Google Translate get this wrong? 
This very attitude of insisting on the purity of the one true way is what caused all the drama. We have more to gain by embracing diversity than fight it by partitioning Haskell into isolated walled sub-ecosystem which promote pick-a-side mentality.
I agree we have too many committees - and wanted to lead this as a solo effort with no committee. But people felt for inclusiveness a committee was better.
Once we have a single English site, then having subsites in different human languages seems reasonable. What do other language communities do in this space? If the HWWG lives longer term, having someone represent the i18n side of things sounds like an excellent idea.
"点 " has tons and tons of meanings. In this case, it means "ordered" :)
That outcome is not inconsistent with the committee. If that's what we as a community want, that's what we'll get. But I don't think there needs to be such a sharp dichotomy. Stack has some really nice beginner-friendly features. If it's easy and well documented for non-beginner developers to switch between build tools (i.e., between the two methods of managing projects and package repositories), and between curated and dependency-based default package selection (both methods supported by both tools), that would be the best of all worlds.
That's the behavior for the default `Post` combinator. You can however write: type Post' = Verb 'POST 200 and use `Post'` in your API instead of `Post`, if you want your POST endpoints to return 200 on success. On failure, any status code can be returned and you don't know about which one just by looking at the API type. Note that there's been some discussion about making API types explicitly list all the possible status codes, but things become more verbose and we don't have a good story for that yet. Some discussion [here](https://github.com/haskell-servant/servant/issues/349) and [here](https://gist.github.com/jkarni/0d23e20ba695cad9a620).
If cassava used the interface of `Data.Vector.Generic` there would be no need for expecting the rules to fire correctly. Is there any reason it is boxed-vector specific?
Renah says, "Function composition for monads". It's actually flipped composition, like `&gt;&gt;&gt;`. Regular function composition would be '&lt;=&lt;'. So if you like a combinator style where you express a sequence of operation as pipeline of functions using `"."` or `"&gt;&gt;&gt;"`, you can do the same thing in a monadic context using the "fish" operators `&lt;=&lt;` or `&gt;=&gt;`. Another member of our team, Vlatko, likes to use flipped bind `=&lt;&lt;` as "function application for monads". Putting those two together, where you might write foo . bar . baz $ boo in a non-monadic context, you would write (mfoo &lt;=&lt; mbar &lt;=&lt; mbaz) =&lt;&lt; mboo in a monadic context. (Unfortunately the fixities are different, so you need parens.) EDIT: For that last expression you can use only `=&lt;&lt;`: mfoo =&lt;&lt; mbar =&lt;&lt; mbaz =&lt;&lt; mboo just like you can use only `$` in a non-monadic context, with the same trade-offs.
This sounds reminiscent of the the Ig Nobel 2012 winner in the "Literature" category: &gt; The US Government General Accountability Office, for issuing a report about reports about reports that recommends the preparation of a report about the report about reports about reports.
I suppose different community use different kind of sub hostname, there's no standard though, for example both `www.haskell.org/zh` and `cn.haskell.org` looks fine to me. I guess someone with multiple language abilities is more suitable for representing i18n process, sadly english and chinese is the only two language i'm good at. So i'm here for chinese related work. BTW, i'm leading the translation of ghc user guide on github [here](https://github.com/haskell-chinese-working-group), it's slowly progressing, i hope we can make it before GHC 8.2 release, but no promise can be given at this point though.
Actually it solves both problem, thank you. I'll complain for one thing: the stack line is damn impressive on top of any script, see for yourself : -- stack runghc --resolver lts-7.10 --package text --package base --package process --package containers --install-ghc -- -hide-all-packages It may be interesting to provide a simplified version for most use case. Usually we want to select a resolver and a list of package (including base), so something like that may be more friendly: -- stack lts-7.10 text process containers 
Very good answer, I have a tiny addition: &gt; When using cabal-install, in theory, this should again happen automatically. This is still pull-based, right? To trigger an upgrade you have to: &gt; cabal update &gt; cabal sandbox delete &gt; cabal sandbox init &gt; cabal install This will give you all minor updates that have been released in the meantime.
This looks like a power play by the haskell.org people. I'm skeptical.
I just noticed that haskell.org has no real "get started" link anywhere. Nor does the haskell-lang.org. If you go to that isocpp.org link, you'll see both instructions on how to get a working environment, and pointers for information on how to program in C++. On both sites the first one is on Downloads - a pretty goo name, and the second one is on Documentation - a name that is too vague, there's no hint that it has basic tutorials too. 
Please FPcomplete, don't fall in the hands of the researchers of the fibonacci list
&gt; I had a look again and I think http://haskell.org seems pretty much equivalent to iso-cpp.org, if the Download page was called "Getting Started" and had a link to https://haskell-lang.org/get-started under the Stack section. I this sounds like a good solution, except that it's likely users won't notice they've been redirected to a different domain. Perhaps it would be better to just wait for haskell-lang.org's response. 
I wanted to do an Android app with Frege for the last few months, but didn't find the time...
Or a symlink 
I've built a custom Data Definition EDSL to support data exchange between functional languages. So the killer feature here is ADT support :) Currently it generates Haskell, Purescript, C++, Java and C# data types with their JSON serialisation code. * EDSL DDL [example](https://github.com/lambdacube3d/lambdacube-ir/blob/master/ddl/Definitions.hs#L254-L265) * generated [Haskell](https://github.com/lambdacube3d/lambdacube-ir/blob/master/ddl/out/haskell/LambdaCube/IR.hs#L286-L299) * generated [Purescript](https://github.com/lambdacube3d/lambdacube-ir/blob/master/ddl/out/purescript/LambdaCube/IR.purs#L268-L279) * generated [C++](https://github.com/lambdacube3d/lambdacube-ir/blob/master/ddl/out/cpp/LambdaCube.IR.hpp#L547-L572) * [others](https://github.com/lambdacube3d/lambdacube-ir/tree/master/ddl/out) Is this something what you need also? 
This is a good question. The `stack config set resolver lts` command only accomplishes the desired goal for packages in stackage. It won't get updates for any of your non-stackage dependencies because those dependencies will still be pinned to a specific version.
You're welcome!
&gt; The tools do not support this very well -- for example, I once accidentally almost uploaded a private package to hackage because cabal upload chose that as the default. Btw, the next major cabal version is making it harder to accidentally upload package releases to Hackage, as it defaults to uploading as package candidates (which is generally desirable for intended uploads as well). That way you have an additional safe-guard to detect any mistakes. Moreover, there's also safe-guards planned on the server side to reduce the risk of accidental uploads. &gt; I think (3) is by far the easiest to set up, and at least with stack it scales very, very well. This works equally well with `cabal` (see [Cabal User Guide | 5.1.1. Developing multiple packages](http://cabal.readthedocs.io/en/latest/nix-local-build.html#developing-multiple-packages)) which I make frequent use of. PS: There's a little known feature in cabal that allows to use HTTP authentication credentials in combination with the `wget` transport mode that allows to run a private non-public hackage server without a VPN. There's also support for adding hackage repositories in your per-project `cabal.project` file (this was already possible via per-project `cabal.config` files) if you prefer them per-project rather than user-global.
Wayanad is a real place btw.
A sensible reply! So much more than my comment deserved ☺ 
If you already setup Hackage, you could (if it isn't inconvenient), block outside access to it and have it reachable for the users you want via an SSH tunnel.
Inclusiveness mixed with commitees is a guarantee that tribal politics instead of technical outcome will be more important. https://twitter.com/rpaulwilson/status/678695029913841665 
Has a couple quirks, but: https://github.com/bitemyapp/ghcjs-starter-project https://github.com/bitemyapp/ghcjs-starter-project/blob/master/Makefile#L8
&gt; This very attitude of insisting on the purity of the one true way This sounds very uncharacteristic of Haskellers.
You can't put arbitrary sized data types in a Storable Vector, the Storable class has a size-of method in which you have to report the size of the type without a value to inspect. Therefore ByteStrings can't be put in a storable vector without a custom instance reporting a hard-coded size. Cassava yields the CSV columns as undetermined-length strings, so it naturally only supports boxed Vectors.
That's true for the generic record, but when you are decoding to a specific record `a` which obeys `Vector v a`, I see no problem.
Better please make the separation explicit and rename haskell-lang.org to eta-lang.org
Nice work! Good to see a general purpose parsing library being actively maintained. Its often mentioned as one of Haskell's strengths, before this lib being around it was kind of hard to price that claim.
`dbg` looks awesome. So glad megaparsec exists. 
I recently have started programming, and you should just start with some language. There are so many languages, it's easy to get choice paralysis with figuring out what to start with. At the end of the day, what you start with matter less than whether you've started. To answer your question, I started with haskell, and I feel it was a good choice. I dont have experience with python though. Other people will likely have better opinions. Just remember to start, don't get caught up in all the choices
My suggestion for being productive throughout the learning process: Elm -&gt; Purescript/GHCJS/Haskell basics -&gt; Haskell advanced concepts I see Elm as a nice beginner language that gets people used to the syntax and simplest concepts (functions, defining types, compiler errors, using Maybe/Result), which will translate pretty smoothly into Haskell once you feel comfortable in Elm. Once you get to Haskell land, then you can pick how fast, steep a learning curve you want to attempt.
Are you actively coding to this day? Was it a challenging but rewarding road?
If you like that property then roll with haskell. The heart of the language is really simple.
What happened to parsec and attoparsec? 
I'm kind of surprised about the claim that many people don't know about hspec-megaparsec. One of the first things I did when writing my first megaparsec parser was google "megaparsec testing" and it showed up right away. 
That I am hoping is the case. There will probably be more to do. So far the haskell community is also very good.
&gt; I'd rather be a good programmer first By (my) definition, this means you must become proficient with Haskell.
I'm assuming that "modulo some unfortunate technical stuff" is about overflow? :P
Even with two's complement theres still the possibility of "bottom" hiding in there somewhere.
With respect to stream fusion vs foldr/build, some code would be compiled more optimally using one while other code would be more optimally compiled using the other. No promise that's the issue to which they refer.
I'm not saying that anyone is obligated to write a library for anyone else. However, if you believe that the right thing to respond to someone is "go write the library yourself," then this blog post isn't for you. It's an off-putting comment, and frankly I'd rather that attitude never be shown to new users. There are lots of other potential responses which are fine, such as: * Unfortunately no one has written that yet, it's a hole in our library ecosystem * If you write to *some list*, I'm sure someone would be willing to mentor you on writing it * There are companies in the Haskell world that do paid work, if your company needs this, perhaps it's worth talking to them? Again, no one is responsible to do _anything_. I'm talking about how to respond to people asking questions. Let's not scare away new users by implying that any limitations they find in the Haskell ecosystem will be their obligation to solve unilaterally.
Ahh heh. Full circle 😂 I don't think you're being obstructive. Stack currently does what I want with the commands you listed, so why make it more complex? Simple is better IMO
I went from Python to Haskell, but I wouldn't recommend anyone start with Python for anything but scripting. In other words, fine for small hobby programs, not useful when either correctness or performance is required. And to be honest, I prefer BASH when it comes to scripting. Although this may simply be because it's always at hand, in my terminal. I just feel like, if I'm throwing away the usefulness of types anyway, in order to get something done quick, why not just use BASH? So, it may sound harsh, but my verdict on Python is that its type system is too weak to get much out of it for real software, and too much in the way when you just want to get something done quick. I prefer piping stuff in BASH rather than loading it into variables in Python. Variables just get in the way really. 
Your question isn't very clear... Perhaps you could give more detail, or an example of what you're trying to do?
&gt; What happened to parsec Nothing. That was the problem. &gt; and attoparsec? Attoparsec is for a different use case: Fast parsing for limited input types (Text, ByteString) and no real "error handling" to speak of (other than just failing the parse).
If you want to become a good programmer first you probably want to start with Java, simply because of the huge amount of literature available using it. Part of being a good programmer means knowing about for example data structures and algorithms and Java is pretty much the standard language for books about that. Python could also work as it's become a starting language at many universities by now, which also brings a lot of teaching material with it. 
Great to see `megaparsec` addressing some of `parsec`'s papercuts and improving beyond. Keep up the good work!
I am pretty sure SPJ explains it here: https://www.youtube.com/watch?v=uR_VzYxvbxg
There seem to be a few projects in this space. Here is my own: https://github.com/timbod7/adl My goals are: * Cross language Algebraic Data Types * A small language for defining data types rather than an ESDL (to be useful for non haskell developers) * Support for generics * custom type bindings * literal values It currently has backends for c++, java, and haskell, with a javascript backend in the pipeline. 
Thanks, I missed this ping earlier, but fortunately a coworker corrected me. Silly typo is usually because I'm dumb (reference to following blog post...)
I think what the author meant was how to write a program that continually echoes "foo" on the console, but can be stopped/restarted with a POST request to `localhost:8000/foo`. Something like this (using servant, servant-server, warp, async): {-# language NumDecimals #-} {-# language TypeOperators #-} {-# language DataKinds #-} module Main where import Data.Bool (bool) import Control.Monad import Control.Monad.IO.Class import Control.Concurrent (threadDelay) import Control.Concurrent.MVar import Control.Concurrent.Async (concurrently_) import Servant (Server,serve,Proxy(..)) import Servant.API import Network.Wai.Handler.Warp (run) type FooAPI = "foo" :&gt; Post '[JSON] () makeHandlers :: MVar () -&gt; MVar Bool -&gt; Server FooAPI makeHandlers latch switch = let changeIt isOn = not isOn &lt;$ bool (flip putMVar ()) takeMVar isOn latch in liftIO (modifyMVar_ switch changeIt) loop :: MVar () -&gt; IO () loop latch = forever (do swapMVar latch () threadDelay 1e6 putStrLn "foo") main :: IO () main = do switch &lt;- newMVar True latch &lt;- newMVar () concurrently_ (loop latch) (run 8000 (serve (Proxy :: Proxy FooAPI) (makeHandlers latch switch))) (I wrote this in a hurry, maybe there's a simpler/more correct way.)
In the paper it says that it was a surprisingly lightweight change.
I would actually also recommend going through Elm first, to get the hang of how to model things using types. As for learning resources, I can really recommend https://guide.elm-lang.org (!!) which I just went through this weekend. It is super beginner friendly and probably has one of the nicest/friendliest ways of explaining why types are useful in the "Types" section. You can just stick with the online editor while going through, since all examples throughout can be put into one file. Elm feels a bit like a simpler subset of Haskell, so once you've gone through the guide (which is like a fairly small book, not that long), then I'd recommend picking up Haskell via http://haskellbook.com. So in short, Elm will be perfect to get your toes wet, and then Haskell for the full dive later on. __EDIT:__ To add further (because I just commented without reading your full post first.. &lt;.&lt;), one advantage of also picking up Elm, is that you now have a frontend language/framework that you can use to communicate with your APIs that you end up making in Haskell later on (I personally like Yesod, but Servant also has a lot of momentum atm). I'm currently using Elm in Electron to make a desktop application, which with amazingly minimal changes also work directly in the browser if I'd wanna go that route later on. You can also run Haskell in Electron [0], and I plan on doing a post later on with Elm + Electron and finally Elm + Haskell + Electron, just to get some resources out on those topics :) Finally, with regards to Python, it is much simpler than Haskell, and you would probably very quickly be able to pick it up once you've learned Haskell later on, so I wouldn't worry about that. [0] https://codetalk.io/posts/2016-05-11-using-electron-with-haskell.html
I think it does not matter. I believe that to be a good programmer you need to eventually master several languages of different paradigms. More programmers should definitely learn Haskell, I think. But knowing only Haskell would make you a poor programmer. Haskell teaches you a lot about purity. C/C++/assembly will teach you about the actual low-level workings of the computer. Java will teach you about what the industry likes. Python, in my personal experience, is often the tool to get stuff done in the least amount of time. PHP and JavaScript teaches you about why web apps are so often crappy and how a programming language should not be designed. Mastering Perl will enable you to do text processing programs in no-time. And Prolog ... well ... I don't know what to say, but it sure is an interesting language. :-)
Do unlifted ADTs make sense? For example, type Unlifted = ’[UPtrRep] Bool' :: TYPE Unlifted `Bool'` would be a pointer to data constructor, not a thunk.
I think it's a good thing, in any case I'll be maintaining it just the same. The only other plugin I use is deoplete with neco-ghc. There's a new set of tools coming: http://haskelltools.org/ which will be ready next year, and it's likely that I integrate some of them with neovim, so watch the space!
They just added a flag to let bindings and a few Core Lint checks, so it was surprisingly little. 
Only a 0.3% reduction in allocation apart from the atypical shootout programs. 
`WriterT` is unfortunately bad and should be avoided. You can avoid doing `lift` for state and writer functions by using the MTL classes. In your case this basically just means that you drop `Trans.` from your `Control.Monad.Trans.State.Lazy` and `Control.Monad.Trans.Writer.Lazy` imports. As for IO actions, you can use the `MonadIO` typeclass that gives you the `liftIO` function. This basically takes an IO action and lifts it as much as needed.
For your team, you need the person using OS x, and the Ubuntu bearded guy to both get repeatable builds. So either you distribute a container with the environment, or the build tool does it. Uninstalling is incompatible with immutable infrastructure we don't want it care about it (neither does nix). Security upgrades, yes they should be done by stack downloading new fixed versions.
&gt; WriterT is unfortunately bad Whoa, I didn't know that. What's going on with that? ~~Can the simple-minded reordering of the monads fix this?~~ I'm dumb, IO is the base monad, you can't s/IO/Writer, duh.
In my experience Java simply takes the fun out of programming, so I'd never recommend it as a first language.
&gt; Levity Polymorphism Many ways to laugh?
6 people on the committee also isn't good, too many ways to get deadlock. Perhaps consider adding another member.
It's still a welcome change. &gt; Remember, **the baseline compiler already recognises join points** in the back end and compiles them efficiently (Sec. 2); the performance changes here come from preserving and exploiting join points during optimization. and &gt; Moreover, **the transformation pipeline becomes more robust**. In GHC today, if a “join point” is inlined we get good fusion behavior, but if its size grows to exceed the (arbitrary) inlining threshold, suddenly behavior becomes much worse. An innocuous change in the source program can lead to a big change in execution time. That step-change problem disappears when we formally add join points.
We can just implement `WriterT` as a newtype on `StateT`, and then lifting remains unambiguous for `mtl`. I think someone already did this but I can't find the library on short notice.
There is also [ether](https://int-index.github.io/ether/) which allows you to have multiple different transformers of the same type.
If I remember correctly, the problem with the `Writer` monad had to do with lazyness and space leaks. See for example [this](http://stackoverflow.com/questions/7720929/space-leaks-and-writers-and-sums-oh-my) stackoverflow thread.
I use both and think Python is very easy after Haskell. Haskell is good but GHC isn't that good. GHC's typeclasses are sophiscated with too many extensions and produces enormous warning and error messages which are misleading and horrible. I usually spent hours to overcome such misleading messages. I doubt that production usages and learning could be benefited by such things. Also, many extensions are always experienmental in GHC. There are several Haskell-like languages makes things much simpler and produces much leaner messages. [Purescript-native](https://github.com/andyarvanitis/purescript-native) [Idris](http://www.idris-lang.org/) Here's even a logic/functional language, the only one which comes with logic paradigm for production. [Mercury](http://mercurylang.org/)
Piping only works when your entire program has only one type of data. If you're doing anything that isn't a one line sed/awk magic trick then you're going to be hopefully using variables anyway. Variables in bash are practically *designed* to be accidentally misused
Awesome paper. It's great to see a more thorough write up of this. Also, found a typo ("should to be compiled" used instead of "should be compiled").
&gt; I did a quick rewriting of your example here. That's beautiful! I must note, though, that I left most of the code as it was originally. Your rewriting is a sight to behold. I'll look into lenses. Time to take the plunge!
I wonder how this relates to Swift [SIL](http://llvm.org/devmtg/2015-10/slides/GroffLattner-SILHighLevelIR.pdf)'s "basic block arguments", and for that matter LLVM's phi nodes? Is it something more specific?
You won't be able to make the `Person` type `Storable` since it has a bytestring field, and bytestrings don't have a fixed size. By contrast the `Star` type in the other module is made of 12 components of fixed size, so it has a fixed size and can be `Storable`. 
Oh okay, I just haven't gotten far enough yet then.
Could I use some sort of fixed size bytestring perhaps? Do you know if anything that exists?
The paper isn't opposing stream fusion to build/foldr fusion, but stream fusion to what they are calling unfoldr/destroy fusion. The latter is just the 'natural' co-church encoding of the list or vector, in terms of its corresponding `unfoldr` data Stream a where Stream :: (s -&gt; Maybe (a,s)) -&gt; s -&gt; Stream a (omitting here a `Step` type). Important functions like `filter` are still recursive with this approach, so the `Skip` constructor is introduced to write a fusible nonrecursive `filter`. (This means that, under the hood, one is implementing lists or vectors in terms of the co-church encoding of `[Maybe a]` rather than `[a]`; with this, something like `filter` becomes a map.) The claim is that since with the current approach 'recursive stepper functions are fusible', the detour via the `Skip` constructor becomes unnecessary. 
For libraries it looks like the PVP just wants a upper bound (see the "Dependencies in Cabal" section) but doesn't require any more than that. So it's up to you whether to disallow the next A, B, or C of foo-A.B.C.D. I'm a little surprised at this. I won't give advice here -- if the PVP document doesn't specify what to do it should be fixed there, not by me guessing on Reddit. For executables it really is totally up to you. Don't want to upgrade a particular dependency a lot? Specify it narrowly (this all assumes you're using cabal-install and not stack). Don't care if it upgrades? Use wide bounds or none at all. Since you're testing and `cabal freezing` after every `cabal install --upgrade-dependencies` the repo can't get in a broken state, either for you or for someone else using the executable.
Okay, having now read to the end of paper, here is the relevant part from the related work section: &gt; **SSA** The majority of current commercial and open-source compilers &gt; &gt; (including, for example, GCC, LLVM, Mozilla JavaScript) &gt; &gt; and compiler frameworks use the Static Single Assignment (SSA) &gt; &gt; form [7], which imposes on an assembly-like language the invariant &gt; &gt; that variables are assigned only once. If a variable might have &gt; &gt; different values, it is defined by a *φ*-node, which chooses a value &gt; &gt; depending on control flow. This makes data flow explicit, which in &gt; &gt; turn helps to simplify some optimizations. &gt; &gt; As it happens, SSA is inter-derivable with CPS [2] or ANF [5]. &gt; &gt; Code blocks in SSA become mutually-recursive continuations in &gt; &gt; CPS or functions in ANF, and *φ*-nodes indicate the parameters at &gt; &gt; the different call sites. In fact, in ANF, the functions representing &gt; &gt; blocks are always tail-called, so adding join points to ANF gives &gt; &gt; a closer correspondence to SSA code—functions correspond to &gt; &gt; functions and join points correspond to blocks. Indeed the Swift &gt; &gt; Intermediate Language SIL appears to have adopted the idea of &gt; &gt; “basic blocks with arguments” instead of *φ*-nodes [14]. This is a bit implicit for someone (such as myself) who's not already closely familiar with the relationships between these concepts. If I want to make it explicit, is the statement that: * GHC's Core and SIL are both ANF, and * adding join points to ANF, which this work does, and as SIL already has, makes it exactly correspond to SSA with *φ*-nodes, * which is capable of expressing more optimizations than ANF *without* join points, * and all of the same optimizations as CPS?
About `vector`: - `vector` and `pipes`/`conduit` are very different. `vector` is a data structure while the others are streaming mechanisms, so it's more about memory usage. - If you are just folding thing, there is not much difference. If you need random access, `vector` is the choice, if you need concatenation, `DList` is the best, if you need a mix of access and concatenation, `Seq` is good. It really depends on situation. In general, if you are using `!!` you should probably be using something else than list.
&gt; vector and pipes/conduit are very different. vector is a data structure while the others are streaming mechanisms, so it's more about memory usage. The `vector` library performs stream fusion, written correctly the same task in each of those libraries should use about the same amount of memory, my question is if you suffer a performance penalty for using `pipes`/`conduit`.
the overlap of things the vector library is used for and pipes/conduit is used for is virtually zero... here is a flow chart: 1. do you need to store and access data in constant time? - yes: use vector 2. do you need to do streaming IO with complex memory and resource management and error handling in constant space? - yes: use pipes/conduit I honestly can't imagine a situation where the decision would be not extremely extremely straightforward haha. it's like asking when you should use a hammer over an apple.
First, let's start with the types: For all of the monad transformer types (i.e., all the ones ending in T: StateT, ReaderT, IdentityT, etc.), the "m" and "a" in the type signature have a standard meaning: "m" is the monad you're wrapping with the transformer (so it's usually either IO or another transformer applied to another monad), and "a" is the original underlying value (usually thought of as the value that a function operating within the monad will eventually "return"). Figuring out `a` is usually pretty easy. For your `doRound` function, the value it returns is `()`, so if you wanted to make `doRound` monadic, the `a` would be `()`. If you wanted to make `newScores` monadic, then `a` would be `GameScore` and so on. Figuring out what `m` should be requires understanding a pretty important distinction. In the type `IO a`, the monad is `IO`, not `IO a`. Similarly, in the type `StateT s m a`, the monad is `StateT s m`, without the `a`. In particular, because `IO` is a monad, `StateT s IO` is also monad. When applied to a type (`StateT s IO a`), it's not a monad anymore, instead it's what we sometimes call a "monadic action". Why is this important? Because when you're deciding what to use for `m`, it's a *monad* you want (the part without the underlying type `a`), not the full monadic action. As an example, if you want a monad that incorporates a state, a reader, and IO, you build it from `StateT`, `ReaderT`, and `IO` as follows. Assume you want to start with `IO`, transform it with `StateT`, and transform that with `ReaderT`. The signature for `StateT` is `StateT s m a`, but only the `StateT s m` part is the monad, so let's stick with that. To wrap the monad `IO` in a `StateT`, we just substitute `IO` in for the `m` to get the monad `StateT s IO`. To wrap that monad with the `ReaderT` transformer (which has full signature `ReaderT r m a` where the monad part is `ReaderT r m`, we just substitute our monad so far into `m` to get the new monad `ReaderT r (StateT s IO)`. Note that the parentheses are needed because the whole type `StateT s IO` is the monad `m` that `ReaderT` needs to transform. So, our final monad is `ReaderT r (StateT s IO)`, and every function we want to evaluate within this monad should return a value of type `ReaderT r (StateT s IO) a` for some underlying, "real" return value `a`. Since writing that monad over and over again is a pain, it's very common to define a type: type MyMonad a = ReaderT r (StateT s IO) a (though this won't work as-is -- we need to give some concrete types for `r` and `s` as noted below) and then all the functions running under that monad will return values of type `MyMonad ()` or `MyMonad GameScore` or whatever. Since you wanted a `StateT` on top of your `IO` but didn't need a `ReaderT`, you can just write the simpler version: type MyMonad a = StateT s IO a where `m` is `IO`, the underlying monad your `StateT` is wrapping, and `a` is the underlying, "real" return value of every function you're making monadic. (Again, this won't actually work as-is -- it needs a concrete type for `s`.) Is that all clear? The only part I left out is the meaning of `s`: it's `StateT` specific, and it gives the type of the state you want to carry around. In your game, you wanted to use the `StateT` monad to carry around the current score, so your `s` can simply be `GameScore`, so you'll have: type MyMonad a = StateT GameScore IO a (By the way, if you had a bunch of different things to keep track of, you'd probably define a custom data type: data MyState = MyState { score :: Int, gameClock :: Int, highScores :: [Int] } and use that for `s`.) Note that everything in the `MyMonad` type is fully determined except `a`, but that's okay. We typically want to use our monad for multiple functions, and the `a` will vary from function to function depending on the underlying "real" value. Second, let's talk about refactoring your code to use `StateT`. You can think of `StateT` as making available a value of type `s` (here, `GameScore`) that can be read and written with the monadic actions `get` and `put` (and others documented in `Control.Monad.State`). Because these are monadic actions, they will actually have types: get :: MyMonad GameScore -- your monad, with a=GameScore put :: GameScore -&gt; MyMonad () -- again, your monad, with a=() and they can be used in a do-block as follows: add10Points :: MyMonad () add10Points = do score &lt;- get put (score + 10) Any function in your program that takes a `GameScore` argument that's intended to be the current score or that returns a `GameScore` argument that's intended to be the new score is a potential candidate for being rewritten using `MyMonad`. Let's start with `doRound`, which takes a `GameScore` argument and runs in the `IO` monad. Let's getting it running in the new monad (ignoring the state for now). Because `MyMonad` includes `IO`, you can just swap it into the type: doRound :: GameScore -&gt; MyMonad () and you're done! Ha, ha, not really. If you try to compile this, you'll find multiple errors. Let's fix them up one-by-one: 1. The IO actions in `doRound` all have the wrong type. For example, `putStrLn "..."` has type `IO ()`, but it needs to be `MyMonad ()`. The solution for this is to use `liftIO` which "lifts" any `IO` action into an IO-capable monad, like `MyMonad`. Basically, you should take any expression that has an `IO a` value (e.g., `getLine` by itself, `randomRIO (1, 2)`, etc.) and wrap it in a `liftIO` call until everything typechecks. 2. The `doRound gs` call in `main` has the wrong type: `main` runs in the `IO` monad, but `doRound gs` needs to run in the `MyMonad` monad. Here, the solution is to "run" the state transformer, which unwraps the `StateT` off of the `MyMonad` to reveal the `IO` action underneath. The functions to do this are in `Control.Monad.State` with names like `runStateT`, `evalStateT`, and `execStateT`. They all take a `StateT`-wrapped monadic action (i.e., `doRound gs`, in your partially refactored program), a starting state (the initial score `gs`), and they return a new monadic action for the underlying monad (in your case `IO`). They differ in whether they return the underlying value (for `doRound`, it's `()`), the final value of the state, or a tuple of both. You just need `evalStateT` which will return an `IO ()`, the perfect action for including at the end of main's do-block: `evalStateT (doround gs) gs`. Note that `gs` is duplicated here only because we aren't finished yet. At this point, your program should compile and run `doRound` in the new monad, though it's not actually *using* the state you've provided. To use the state, rewrite `doRound`, eliminating the `GameScore` argument and using `get` and `put` monadic actions to manipulate it. As a first crack, all you need to do after removing the GameScore argument from the function definition and its call in `main` is to stick the following statements in the correct places: gs &lt;- get put newGameScore Once that's compiling and running, here are some other things to refactor: 1. Eliminate the unnecessary `gs` variable by using the `modify` function from `Control.Monad.State` to apply your `newScores` function to change the state to reflect the new score. (You'll want to add in a `newGameScore &lt;- get` after the `modify` call to avoid breaking everything else.) 2. (Tougher) Now, eliminate `newGameScore` entirely by replacing it with `get`. (Hint: Where `newGameScore` is used to create a monadic action, you'll want to use `get` together with the bind operator `&gt;&gt;=` to replace it.) 3. Try making `newScores` monadic, too. You might not do this in a real program -- the pure `newScores` function works fine as is -- but it'll be good practice for working with StateT. Hint: first, rename `newScores` to `updateScores` to reflect what it's really doing, and then change it's signature to: updateScore :: Int -&gt; Int -&gt; MyMonad () Note that it's now a monadic action that changes the score but neither accepts the current score as a current argument nor returns the new score as a value. Anyway, hope that helps, and have fun. 
It is a modification to core, but it isn't a modification to that much code. Join points are a special case of ordinary let bindings. The difference being that they can be compiled to just a jump and no allocation. The modification to core is just a flag. And then there is a small amount of change that has to happen for compiler passes to not mess up join points, but it turns out that appears to just be a win anyways.
I think you have the right basic picture. Two things I would add * GHC Core with join points is *higher order* while most intermediate languages are *first order*. That is, it is still a closure conversion + lambda lifting away from LLVM. Thus, it can express optimizations which couldn't easily be expressed in either the old GHC core or a lower level SSA format. * Join points allow for expressing all the obvious CPS optimizations, or maybe, all the CPS optimizations we know of. Someone might invent new optimizations which change that picture. There is still a fundamental difference between the two: GHC core with join points has a type system a intuitionistic logic with only local control while CPS is a double negation translation and so allows for implementing global control effects.
The two tools definitely have plenty in common. They're solving a similar problem after all. The difference I'm talking about is that cabal files have a way to specify a range of versions you're willing to use while the `stack.yml` file to my knowledge does not.
Vector will not always outperform a list. Folding a list can likely perform as well or better than a vector in many cases. The question is whether you materialize the list before folding. If it is materialized then an unboxed vector will always win.
`fork` is `(&amp;&amp;&amp;)` for [`Kleisli`](http://hackage.haskell.org/package/base-4.9.0.0/docs/Control-Arrow.html#t:Kleisli) arrows. It is also `liftA2 (,)` for [`Star`](http://hackage.haskell.org/package/profunctors-5.2/docs/Data-Profunctor.html#t:Star) from profunctors. The problem is that a lot of newtype wrapping-unwrapping is required. (I sometimes bite that bullet, though.) `tee` is `(&lt;*)` for `Star`.
Do you have an exemple of a use case?
I use vscode sometimes when I'm bored with Emacs. There is not a lot of choice for Haskell in vscode. * Haskell ghc-mod * Haskell Syntax Highlighting * haskell-linter * Hindent format I use these settings too for a better experience. ... "editor.fontFamily": "Hasklig", "editor.fontLigatures": true, "editor.formatOnType": true, "files.autoSave": "onWindowChange", "haskell.symbols.provider": "hasktags" ... Of course you will need to install the tools (ghc-mod, hlint, hindent and hasktags)
Right
&gt; what the author meant was how to write a program that continually echoes "foo" on the console, but can be stopped/restarted with a POST request How did you deduce *that*? :D
I have used &amp;&amp;&amp; ("fork") a few times. It always boiled down to getting different things out of the same Value. Although that was not embedded in a Monad iirc. Eg: Getting different bytes out of a dword. I don't really recognise the other two immediately though so I probably never needed them.
it's indeed `traverse ($ 1) [f, g] `, except f and g have to have the same type in this case
If I understand correctly
I'm not sure I understand all the hostility you seem to be exhibiting in this thread. You seem to have quite a bit of experience of with haskell based on your posting history and I think less aggressive conduct from people like you is always a good thing. Forgive me if I have misunderstood your tone.
One thing I will say is that join points have a really cool relationship to a whole bunch of ideas related to linear logic. For direct style computation they kinda capture what the "right hand side of a case" is in a way that allows nice sharing. There's also some fun "at most once" properties that let you avoid needing doing needless closure application that's kinda awesome. 
That's a fair point, and something I had considered when it first dawned on me that I might want something like these functions. To be fair though, let me point out a few things: 1) Your alternative definition of &gt;&gt;=/ doesn't provide the other two functions which I have also found to be useful 2) You mention that "It's great when it points out that something is a relatively simple combination of well-known operations." -- Isn't that sort of what is being shown here with &gt;&gt;=/? That it can be written in terms of tee, and that tee, in turn can be written in terms of fork? I will concede that to someone unfamiliar with the significance of (fmap . fmap . fmap) In the context of function composition the definition of tee looks rather complicated, but I am quite familiar with that idiom so to me, this reads more like (fmap fst) `compose2` fork which for me is quite straightforward. With all of that said, its definitely true that point free style can, at times, be quite confusing. I find it to be pretty satisfying to write things in that way, but I will concede that it is probably of limited utility to do so. 
It's a difficult choice. Given your use case (financial app backend with strong data handling capabilities, speed and strong support for parallelism), and of course other factors like availability of libraries for your domain and a community to collaborate with, I would have to say F# seems like a better fit. F# has a crazy-cool feature called 'type providers' that let it connect to various data sources in a type-safe way. I'm also influenced here by the availability of the 'Programming Languages, Part A' MOOC on Coursera, which teaches syntax, semantics, idioms and key abstraction techniques using SML, a precursor language of both Haskell and F#. This course is what I consider to be the gold standard in intro programming courses, and the SML part of it, which is most relevant for F# (minor syntactic differences), is just five weeks long. A new session starts this Monday btw. Another benefit of F# is that you get the Visual Studio tooling (IntelliSense, autocomplete, etc.) as well as the NuGet library ecosystem. This is no joke: in Haskell land we still have to jump through quite a few hoops with various tools to set up a usable development environment.
I feel that `do` notation would be much more readable than these utilities, but since you asked: fork = liftA2 (liftA2 (,)) tee = liftA2 (liftA2 const) m &gt;&gt;=/ f = do a &lt;- m f a return a
I'm not sure why you connect function composition with `&amp;`. `&amp;` is not flipped composition but flipped identity.
`&amp;&amp;&amp;` is handy. For example, I recently used it to generate a quick histogram of letters in a string: sortBy (flip (comparing snd)) . map (head &amp;&amp;&amp; length) . group . sort . map toUpper . filter isLetter And to get multiple values at once out of a `State` environment: (foo, bar) &lt;- gets (envFoo &amp;&amp;&amp; envBar) 
A simpler option for concurrency is the [Concurrently](http://hackage.haskell.org/package/async-2.1.1/docs/Control-Concurrent-Async.html#g:9) applicative from async, and its related functions. I often throttle the concurrency level with an auxiliary function like -- |Perform an action concurrently across a container, with a limit on the -- maximum number of simultaneous actions. traverseThrottled :: Traversable t =&gt; Int -&gt; (a -&gt; IO b) -&gt; t a -&gt; IO (t b) traverseThrottled concLevel action taskContainer = do sem &lt;- newQSem concLevel let throttledAction = bracket_ (waitQSem sem) (signalQSem sem) . action runConcurrently (traverse (Concurrently . throttledAction) taskContainer)
Sorry if I came across as hostile! That wasn't my intention. I was indeed rather curt, so I'm sorry that it was easy to misinterpret. /u/Tekmo's answer was a much better way of saying what I wanted to say with my first comment: https://www.reddit.com/r/haskell/comments/5ew2bi/do_these_functions_already_exist/dag3pab/ I really don't think these functions would lead to more readable code. Regarding "Only on /r/haskell", I just thought it was funny to see a comment "Sounds like `traverse $ ($ i) &lt;$&gt; [f, g]`". That really is the kind of thing you only see here! 
`tee = liftA2 (&lt;*)` as mentioned by /u/dramforever, since (&lt;*) = liftA2 const
I have to disagree. F# and C# are prime examples for how research can feed into the production compiler without causing major friction. This is also true for OCaml. The trick is in incorporating the fruits of research without breaking existing code. Java does the same albeit at a very slow pace and with mostly disappointing results.
Oh, I did not take it as hostility! I am well known of unleashing the unfiltered results of my neural net on the outside world :-) The "only on /r/&lt;meme&gt;" is also pretty much a geeky thing, so we are in the same club.
I agree that existing code should just work with a new release and breakage should be limited to special cases like deprecated APIs. I cannot say I agree on cabal hell though. I never ran into issues like that with sandboxes and even less so since the new-build introduction, but that could just be the projects I've worked on. In fact, I'm one of those users who always ran into issues with stack, which is mostly because of its insistence to automate things. I know this is the preferred way to develop for many, but it's not the only or even major way, so disregarding those who are perfectly happy with cabal sandboxes and new-build is part of the reason the rift appeared, not the sole reason tbc. Even the Golang community has alternatives to the built-in tools for various reasons, so their ideal to have everything consistent, opinionated and in one toolchain doesn't work in the world. So why should there be a single way to develop with GHC? I see the stack differentiators as optional features of GHC and Cabal more than a separate ecosystem and even less a separate community, just like TH or DPH or the LLVM backend. Haskell's community is already small, so drawing lines between research and production code, and classifying developers and code that way, does a huge disservice to Haskell.
Hi, let me answer you with a question, if you don't mind. Suppose that in your Elixir webapp you have a map of urls indexed by some identifier. How would you fetch the contents of each url concurrently, and then construct the corresponing map of responses? (I know very little about Elixir. I just like to use this particular question to compare the solutions in different languages.) 
Basically Haskell positions itself as a powerful purely functional language with strong static typing. I don't know much about Elixir so I'm not sure how the two compare.
The obligatory github page: https://github.com/Gabriel439/post-rfc/blob/master/sotu.md
what is the best http server for haskell?
`warp` afaict, although worth pointing out that server != framework.
and webframework?
As one of the authors of the paper you linked I can speak to that particular case. We certainly want our work to benefit others and we will get there at some point. If we could add an STM implementation as a library, we would have this today. There are implementation details that make this hard to do right now while keeping performance. Bolting on modularity just to get multiple STM implementations to work would likely be a performance penalty that everyone would have to pay for, not just STM. Perhaps the biggest hurdle would be at least partially addressed by the [mutable fields](https://github.com/simonmar/ghc-proposals/blob/mutable-fields/proposals/0000-mutable-fields.rst) proposal as it requires the ability to teach GHC about new mutable structures. We do not want to replace the existing STM directly for a several of reasons: * We are still working on making the existing STM better and some of the same improvements from the paper will apply to that implementation. * We only know how well the two implementations perform on benchmarks, not real world applications. We would love application derived benchmarks where STM is critical to performance! * All of this may only matter with larger core counts anyway and the existing STM implementation has no global bottleneck while TL2 does have a global bottleneck. * STM may not matter in the long run and leveraging HTM may be the more important factor. An implementation that best supports HTM may be the right choice. Edit: To answer the "How can I be sure?" question. Emailing the authors is the easiest way I know :D.
Vector is a C-array like data structure with constant access time for each element. Haskell lists are linked lists with linear access time. Vectors are better for numerical computations. Use vectors when you need high performance nunerical computing. Conduits are used to solve the problems of lazy IO like resource leak. Performance tip: To avoid lazy evaluation memory leaks in recursive functions write them in tail-recursive form with strict annotation in the accumulator.
type system
how about actor model?
Can you explain what you mean by 'span process inside enum'. It would be great if you can provide us with a small sample code that does this?
in Haskell you can convert a sequential operation like this: map calculate list to a parallel one using combinators from the module `Control.Parallel.Strategies`: map calculate list `using` parList rseq you can also define it as an expression somewhere and evaluate it in parallel when you want: myCalculation = map calculate list main = print (myCalculation `using` parList rseq) so you don't need to change your sequential code at all.
Yes, sorry for not expanding that. Hardware transactional memory.
You are performing a series of transformations on data coming from some `Foldable` type (probably a `List` since that seems to be what most libraries emit). Each transformation may or may not consume multiple elements for each element it emits (or it may emit more elements than it consumes), but all of them are single-pass and so you can avoid creating any intermediate structures *as long as they emit each element as soon as they finish computing it* (which can be difficult to do with the `fold` family of functions). Both `vector` and `pipes`/`conduit` can do this, which is faster?
Thanks. Watching some of the videos now :)
Is it the "can only be called in tail position" restriction which corresponds to the linear "only once"?
&gt; So the vector library has more in common with pipes/conduit/ListT-libraries than people think. I had thought this was common knowledge, actually.
I think your intuitions are correct. Haskell is usually 'harder' for programmers to learn because they already have pre-existing intuitions when they reason about code. This gets in the way of them picking up a new paradigm. If you're starting from nothing, I think Haskell is a fine choice and not any more difficult to start off with than any other language.
Yes. I use that. But it will be tedious to update eventually, as you can only specify shas currently.
I hadn't heard of that before. Is there a particular aspect of join points which reminds you of sea of nodes or vice versa?
So you'd like to be able to specify e.g. packages: - location: git: https://github.com/user/mypkg tag: 1.2.3 extra-dep: true Is that right?
The paper says that ($) had a special case in the type system for levity polymorphism, and it can be now dropped. Will this also cover the special casing allowing `runST $ ...`?
A comparison to related work I would have liked to see is to [intensional type analysis](http://www.cs.cmu.edu/~rwh/papers/intensional/popl95.pdf), which is a way of compiling code polymorphic over unboxed types without requiring a JIT -- basically by passing the required size/alignment information as runtime values.
You're correct that there has to be two directions to the proof. What it boils down to is: Direction 1: Given `Functor m` with functions `eta :: a -&gt; m a` and `mu :: m (m a) -&gt; m a` and properties `mu . eta = id`, `mu . fmap eta = id` and `mu . mu = mu . fmap mu`, you get functions `return = eta` and `(&gt;&gt;=) = \ m f -&gt; mu (fmap f m)` with properties `return a &gt;&gt;= f = f a`, `m &gt;&gt;= return = m` and `(m &gt;&gt;= f) &gt;&gt;= g = m &gt;&gt;= (\x -&gt; f x &gt;&gt;= g)`. Proof left to reader. Direction 2: Given `Functor m` with functions `return :: a -&gt; m a` and `(&gt;&gt;=) :: m a -&gt; (a -&gt; m b) -&gt; m b`, you get functions `eta = return` and `mu = (&gt;&gt;= id)` with properties `mu . eta = id`, `mu . fmap eta = id` and `mu . mu = mu . fmap mu`. Proof left to reader. Besides the proofs of properties, you can see the bidirectionality directly in the fact that you may define `(&gt;&gt;=)` in terms of `mu` or vice versa.
I didn't want a committee - I wanted to do it all on my own. Expect this committee to function more like FP. I hate committees. 
If we get to a vote, we've already failed. If there is a tie, as chair I can resign and break the tie :)
Haskell is not hard to learn for writing useful programs. However, the process is so time consuming due to the "size" of the language and a good bunch of GHC extensions which are regularly used in others' code. 
The representation is based on the idea that once you have your program in SSA form the ordering of instructions withing a basic block does not matter any more: the dataflow dependencies are enough to order the instructions. This makes optimisation a lot easier because you can now treat instructions as pure functions of their arguments. The sea of nodes IR even goes one step further because the only nodes for which it matters in which basic block they are in are phi nodes. So instead of having basic blocks contain instructions you have the phi nodes point to the basic block they are in. You also invert the pointers between the basic blocks: instead of having each basic block point to its successors, you have each basic block point to its predecessor. This setup allows you to express most compiler optimisations including control flow transformations as local equational rewrites. In that way it is *more* functional than ANF/CPS. Here is the original paper: http://grothoff.org/christian/teaching/2007/3353/papers/click95simple.pdf
Good questions, glad you asked. &gt; ... isn't F# basically a Microsoft lock-in version of OCAML? No. F# is a completely open source project with an open development model, _but_ with _significant_ contributions and tooling support from Microsoft, _and_ significant tooling outside of the Microsoft toolset (see http://fsharp.org/use/linux/). The F# Software Foundation is at fsharp.org and the F# open source projects are at https://github.com/fsharp . Anyone can get involved, make suggestions, and submit patches to the language, libraries, tooling, etc. &gt; ... it seems that OCAML doesn't handle parallelism well due to it's GC design. Something about a Global Interpreter Lock (GIL). F# does not share OCaml's limitations (and to be honest, some of OCaml's strengths). F# runs on the .Net platform (including Mono), doesn't have any global interpreter lock, and has a very strong asynchrony and parallelism story, just like Haskell. &gt; ... would it not be more prudent to start with the language I actually want to learn instead or would the principles learned via that course speed up the learning process. With the caveat that I haven't read the Haskell from First Principles book, I personally feel that the MOOC I mentioned has some _excellent_ educational value for teaching functional programming, differentiating syntax from semantics, functional idioms, how to understand typechecking, and perhaps most importantly how to implement abstraction and information hiding. &gt; ... my understanding of programming is that we break up problems into pieces and create solutions for solving them step by step aka an algorithm. You are close but the last and perhaps most important step is to _hide_ implementation details and _enforce invariants_ in your internal code, and only expose tightly controlled operations to the user of your code (which may include 'future-you'). This is something Prof. Dan Grossman covers very well in the last week of the course. &gt; ... and also in the financial software arena To be honest, F# has made a bit of a name for itself in financial software. Check out this intro tutorial on several common activities in finance: http://www.tryfsharp.org/Learn/financial-computing# . Since you're on Linux, you won't be able to run the examples in the browser (they use Silverlight), but you can follow along the tutorial and see that it solves common real-world financial domain tasks in an elegant, functional style.
This depends a bit on your needs, but for server side API, and also for writing http client code, Servant is amazing.
The MOOC I mentioned to you earlier is an honest-to-goodness intro programming course that I genuinely believe starts you off with functional programming in the right way. After doing it, you'll be able to go to almost any language and look under the surface to see deeper meaning. There is also a MOOC offered by Microsoft specifically for F#: https://www.edx.org/course/programming-f-microsoft-dev207-1x , paced at 4 weeks. Finally, the most famous F# guide is Scott Wlaschin's 'F# for Fun and Profit' at https://fsharpforfunandprofit.com/ . It's written as a series of blog posts which can be read like a book.
Just to nitpick, “monoid in the category of endofunctors” isn’t incomplete — the definition of a monoid already includes the identity laws and so on.
It might not be ideal for reproducibility, but I would also like to ask for the tip of a branch, like master
It is somewhat incomplete, but for a different reason. The choice of tensor to equip the category of endofunctors over hask with to make it a monoidal category is ambiguous. data (:*:) f g a = f a :*: g a newtype Compose f g a = Compose (f (g a)) data (:+:) f g a = InL (f a) | InR (g a) data Day f g a where Day :: f (a -&gt; b) -&gt; g a -&gt; Day f g b are all perfectly cromulent choices of bifunctors on [Hask,Hask] that admit monoidal structure. The "monads are monoids in the category of endofunctors, what's the probem" definition assumes you pick that to refer to `([Hask,Hask],Compose,Identity)` -- and even then things fall apart just a bit because everything isn't quite as 'on the nose' as mathematicians would prefer.
*Applicatives* are monoids in the category of endofunctors too So the the "monoids in the category of endofunctors" it isn't a sufficient definition of Monad, it's just a property that other things can have as well.
Did you test if you can simply specify `commit: master`? If that works, I'd still suspect that `stack` will fall behind the upstream branch, and not pull updates automatically. If you need that feature I suggest you open an issue. You may have to submit a patch yourself though…
&gt; but haven't found any decent resources for learning F# Some resources for F#: - https://fsharpforfunandprofit.com/ - http://fsharp.org - https://learnxinyminutes.com/docs/fsharp/ - www.tryfsharp.org
Sadly no, you might be able to find some in the references of the papers.