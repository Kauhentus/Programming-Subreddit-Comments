Yeah but this new array, you can do with it anything you want. Seems like you'd need to convert all your functions that transform arrays into this negative form, like `write :: Int -&gt; a -&gt; Array a -o (Array a -o k) -&gt; k`. This seems super cumbersome to use.
Why the negative vote? 
You could start by reading Wadler's 1990 paper "Linear types can change the world". It gives a relatively concise introduction to the idea and goes over some possible applications.
But I suppose that for some code you could at least introduce heuristics. Optimizations are almost always heuristic in nature anyhow, and they might not always fire. But having them fire sometimes is better than not having them at all.
How does linear typing play with lenses? It feels like deep nested record updating should be trivial. 
Well, I guess fusion laws are sometimes free theorems that are used for optimisation?
I actually prefer the first version. I have some legacy code that does IO and database access and so on inside the form, but find this confusing. I would prefer to have the form do just pure validation of the input, and do stuff like adding a timestamp in another step. 
Will talks be recorded? A lot of them look really awesome, but I don't think I'll be able to make it.
You don't actually need a monad to do what you're doing, you can just do this: blogpostForm :: (MonadIO m) =&gt; Form Text m Blogpost blogpostForm = Blogpost &lt;$&gt; "title" .: nonEmptyText "title" &lt;*&gt; "content" .: nonEmptyText "content" &lt;*&gt; monadic (pure &lt;$&gt; getCurrentTime) We have `monadic :: m (Form v m a) -&gt; Form v m a`. `getCurrentTime :: IO UTCTime`, so `pure &lt;$&gt; getCurrentTime :: IO (f UTCTime)` for any `Applicative f`. `Form` is `Applicative`, so we have `f ~ Form v IO` for any `v`, and voila - we can use `monadic`.
Precisely, so if the problem you're trying to solve requires it, you might as well use some other easier to use protocol other than a blockchain. Because you can't achieve the main value proposition of a blockchain i.e. trustlessness. Edit: I'm all for using blockchain like ledgers as proof-of-existence records. What I'm arguing against is the limitation of self executing smart contracts as a trustless third party. i.e. replacing intermediaries. It is simply impossible for a trustless blockchain "entity" to know enough about the world or the internet to handle most use cases. Edit 2: It's important to note the "ontological problem" isn't too much of a demand for some use cases. Where the thing being managed IS the digital representation on the blockchain. Such as a currency, or in Slockit's case a house or a car, because the physical device forces its behaviour to "copy" the digital token. So what you're really buying or renting is a digital token, and it just so happens that a thing in the world copies the behaviour of that token. But it can't happen the other way around. You can't have some smart contract that gets triggered by some event in the world. Because it can't ever know trustlessly about that event, and suddenly your smart contract isn't trustless, but reduced to what ever level of trust its oracle mechanism works by. So you now immediately create an incentive to lie to the smart contract in some way, to create a "matrix" for it. Something like [Oraclize](http://www.oraclize.it/) reduces the trust model from trustless to the weaker of either the integrity of the person running the service or the security of the private key behind the SSL cert which by definition has to sit on an internet connected server. So cold storage isn't going to help you. I guess what I'm trying to get at is the reason Bitcoin has been so successful is because of its trustlessness. There were plenty of attempts at a digital currency before it that didn't take off. Therefore I believe that any blockchain like smart contracting platform that compromises on the trust angle probably won't take off either, because it won't be enough of a value add over existing solutions that are easier to use, and more scalable.
[We now have `Coyoneda`]( https://github.com/ekmett/profunctors/blob/master/src/Data/Profunctor/Yoneda.hs) in `profunctors`, which is your `FreeProfunctor` type. Also, `runPro` to run `Pastro` has been added as `pastro`.
&gt; Because you can't achieve the main value proposition of a blockchain i.e. trustlessness. Hm. That's perhaps the main value proposition of _bitcoin_. For other contexts, the main value proposition is an immutable public ledger: it can be OK to require trust in a small number of authorities, if the value you're deriving is that those authorities are held to "show their working." (But bitcoin isn't trustless, it transfers trust from actors to algorithms. Since algorithms tend to behave in a more predictable and verifiable fashion, this isn't insignificant.)
No offense to the OP, but I think this question is about as meaningful and interesting as random musings on the topic of what's going to happen when an unstoppable force meets an immovable object etc. etc. Because all of that is necessarily qualified by "well, in the abstract" or "in the perfect world". There are more interesting questions along these lines, like, "Is it possible to improve software development processes and methodology to greatly reduce the impact of the inevitable errors in the resulting products, as compared with commonly practiced processes and methodologies?" The answer to that one is, "Yes, definitely, formal program verification helps there for one thing." Or, "But is possible to achieve the same result without greatly increasing expenses and TTM?" This being a Haskell subreddit, I'd expect the commonly heard answer to be along the lines of, "Yes, it's still possible, though the gains would be lower than in case of going with full-blown formal verification," or something similar. These questions - and answers - actually *matter*. But let's say the definite answer to your question is, "Yes, it is possible to write a 'large' and 'perfectly correct' piece of software by sheer coincidence." (Which sounds "plausible" to me in that in *could* happen over an astronomically large span of time... and number of observed universes.) Why would it matter to me as a practitioner? My reaction to that is, "Oh, okay. Probably not going to happen to me. I'll get on with my life." What if the answer is "no"? Well, that's even better! Since it's impossible, I'll just stop bothering and get on with my life. You're talking about absolutes in an applied field, which is by necessity all about clever trade-offs and compromises. I honestly don't believe this is in any way productive, even as an intellectual exercise.
Anything is possible? At this point I'm not sure what you are asking anymore.
Be careful of the word "possible". It's also possible that our Sun will disappear at 17:37 UCT today.
`type (-&lt;&gt;) = (-&gt;); infixr 0 -&lt;&gt;`
Maybe if we can calculate exactly how improbable that event is, we can make it happen...?
I recently switched from vim -&gt; atom to spacemacs and I am very happy with my workflow
Yes, an example such "other case" would be a form to create a new user on a website. If the username entered already exists you'd like to give back a nice indication of this. As I see it, there are two stages of validation. The first is making sure the input is well formed regardless of context. The second is to make sure the input makes sense in some context(current state of the database).
Thanks, glad it's still useful!
In my view, the blockchain is a base layer that allows many different trust relations. I can choose to trust, say, an official Nasdaq hourly stock price quote contract, or only trust a median between quotes from competing exchanges, etc. If my bank would offer financial services on the blockchain, they already have my trust, and I'm not ideologically opposed to trusted institutions, so that opens up nice blockchain opportunities for me. The "maximalist" ideals of total trustless decentralization will probably fade into a more complex view of trust. This is already the case in realistic projects.
Blockchain hype is tacky like all hype, so we could just talk about "peer-to-peer networks of digitally signed transactions with economic incentives to maintain Byzantine fault tolerance without central authority," and ask whether that's a useful idea. Apparently Wadler thinks there's something to it...
This subreddit is about the other kind of Haskell programs.
Yes but Bitcoin (blockchains in general) solve the problems of scarcity and sequencing. Digital signatures are useful for public authentication, but they do not establish authentication through time (i.e. the sequence of events). Because blockchains do establish a sequence of events, they also allow the creation of scarcity. With only digital signatures you have the problem of double spend attacks.
If you're compromising on trust do you still need the Byzantine fault tolerance?
I would probably write the `Maybe` version and use it to implement the `newtype` version. Both of those are reasonable and idiomatic approaches. The default value style feels a bit old-fashioned to me, but it's perfectly valid. I greatly dislike the type synonym version and would strongly advise against it.
&gt; Returning maybes just forces you to handle cases you know can't happen. If you know that the `Nothing` case genuinely can't happen, you can simply strip the Maybe by using the unsafe `fromJust` function, and add a comment to that effect.
Wonderful!
There is a fifth possible solution, but whether it is relevant in this situation depends on what your data type `A` actually is. You could define another type (let's call it `SubA`) that is this restricted subset of `A` you are interested in. Again, this is not always possible/practical. If the `A` type is actually `Int`, then you cannot really do this. If you could provide the definition your types and a verbal description of your function, I can tell you if this is possible in your case.
More specific examples are needed on this. I find this kind of crucial here: &gt; I never intend to apply f to any argument outside of X Compare it to, for example &gt; I never intend to apply f to any argument of the wrong type Why is the former clumsy and the latter nice? (Since you're using Haskell, I assume you like strong types) The keyword is 'intend'. You don't say you 'intend' to do something, and have it right automagically. If you *ever* find yourself checking that the argument is inside of X for calling f, you're likely falling into Boolean Blindness. And if you *ever* find yourself relying on your remembering to check the argument to make sure the program doesn't crash, you should stop. Similarly, a 'promise' is too weak to be true for Haskellers. It's, I dare to say, *impossible* to maintain promises as you'd expect, if the 'promise' is in human language and not injected into the code. You intend. You promise. But two things *do* happen, and they happen too easily: - I happen to change some other code and break the promise - I happen to re-use a function, and forget to propagate up the promise When I see a function, I want to look at it locally and get each part and the joints right, and get it self-contained. Haskell makes it easy. Please don't reverse this effort by promising. **If X is a reasonable, static subset of Foo, and you can explain what it is by giving relatively simple criteria, e.g. `Foo -&gt; Bool`, or f actually returns something that is sure to be in X, then do this**: Make a `newtype`. Put it in a separate module and don't export the constructor, or in an `Internal` module and give it scary names and documentation, explaining the requirements and implications of violation. newtype X = UnsafeX Foo Make a 'smart constructor'. makeX :: Foo -&gt; Maybe X Then you can happily follow your `f :: Foo -&gt; b` one without the burden and unreliability of the 'promise'. For a simple example, see [nf the package](https://hackage.haskell.org/package/nf). **If the criteria are specific to `f`** Write `f` as f :: a -&gt; Maybe b TL;DR I'm a stupid developer. I forget about some code after I wrote it, unless I saw and was forced to understand it again.
I think the default style works out when you can frame it like a fold. That scales to N constructors.
I've been thinking the same thing for a while now. There seems to be a lot of unsustainable stuff being worked on in the "blockchain" space. Probably much in the smart contract space, as you mention. Another example is "more controlled and centralized" blockchain, completely missing the point of blockchain (you might as well use a MySQL database).
Smart contracts don't need to encode all that stuff to be useful. The idea that trustless mechanisms will entirely replace law is a maximalist position and not the only reason to be interested in blockchain smart contracts.
Then what I have suggested is not a good fit. Both of these are pretty hard to represent correctly with haskell's data types (in a practical way).
&gt; After the DAO debacle, I assume people realized the importance of carefully restricting the expressivity of the contract language. I'm not sure the security flaws that lead to the DAO hack were just the fault of solidity's expressive power. When you deposit money into an account with a smart contract in Ethereum, the VM executes some callback code. This doesn't have anything to do with Solidity; it's a byte-code thing. Other smart contracting language for Ethereum, such as *Serpent* and *lll* compile to the same byte-code. It's possible Contract A to deposit to Contract B, and for Contract B to have in its callback a call to Contract A. It is security critical that all the code be reentrant, just like you might worry if you had a `static` buffer in a function in C. See [the Ethereum Wiki entry on Safety](https://github.com/ethereum/wiki/wiki/Safety#reentrancy). One obvious solution would be to have Solidity emit byte-code where, by default, any attempts to reenter contract code fail while it is already executing. Solidity could do this today. I'm not sure why it doesn't. Another solution would be to ban reentrancy at the VM level. Last time I chatted with them, this is the approach the [Ethereum WASM](https://github.com/ewasm) VM team was taking. To be honest I can't see how higher order types or linear types would avoid the security bugs that lead to the DAO, but I would be interested in learning. On the frontier, there is sadly pressure to require even more language/VM expressivity. Here are some examples: 1. It would be nice to have high level convenience facilities in a smart contracting language for interacting with side chains. 2. Another challenge is to provide ways for dealing with payment channels and lightning-network type systems. I know that (1) is being talked about for Ethereum's next release, while (2) is what [Aeternity](http://www.aeternity.com/) is trying to do. It would be nice if this stuff was secure from day one, but I suspect this stuff will probably come with a fresh crop of footguns.
&gt; Unpacking via a package identifier (e.g. stack --resolver lts-7.14 unpack mtl-2.2.1) will ignore any settings in the snapshot and take the most recent revision. I think this is worded incorrectly, it sounds like both the snapshot _and_ the requested version are ignored. Only the snapshot is ignored.
At least we know about the other other kind now...
I can always `fromMaybe` and it's clear...
I'd like to add that there's another way to think about it. Whether it's different depends on how you interpret 'failure' `Nothing` is a completely valid, successful result. For example, `lookup` returning `Nothing` does *not* mean it failed, it just means 'no the key is not in the map'.
Then yes, writing a large program with no bugs is possible and somewhere between unrealistic and extremely unrealistic.
Sure, but aren't unparameterized ones too?
Thanks for the response! Some follow-ups: &gt; `map :: (a -o b) -&gt; [a] -o [b]` The paper points out that this type is incomparable with `map :: (a -&gt; b) -&gt; [a] -&gt; [b]`. I'm having difficulty getting my head around the difference in the context of promotion. Can the map typed linearly accept an unrestricted function as its argument? Otherwise, this inferred type is not backwards compatible, right? &gt; We'd need to give a type to `seq` which indicates that it's using 0 times its argument. This would be compiler magic like the implementation of `seq`, right? This means it would be impossible to give the same type to `deepseq`? It feels to me like `seq` can more accurately be thought to be "putting the value back when it's done"; is there a meaningful distinction between that "using a value zero times" like `const`?
Please excuse my ignorance, I'm coming from the Ethereum smart contract model. In Ethereum, smart contracts are objects with public and private methods. They also have a balance of ether, and can control the ether in their balance. There is a namespace service written in Ethereum itself called [ENS](http://ens.readthedocs.io/en/latest/introduction.html) for resolving the names of smart contracts. This is all part of the EVM, which is what Solidity compiles to. Are Pact smart contracts objects too? If not, could you demo what a subcurrency contract might look like? I am pretty curious if you made any design decisions to avoid the sort of security flaws which ruined the DAO. Also, does Pact handle namespaces using a smart contract like ENS, or does it do something else? Finally, is there a VM or do you just have a runtime for pact? The EVM has some serious problems - for one **all** arithmetic is 256 bit, and there is no bitwise shift (only multiply, divide and modulus). So needless to say Solidity's implementation of `Uint8Arrays` is pretty obfuscated. I am rather curious how your VM works if you have one. If you aren't using a VM, how are you metering exactly? I have some other questions regarding metering if you have the patience...
(I should note that I'm pretty unsure of this answer.) I think that, in the interest of backwards compatibility, `f :: T -o ()` imposes no restrictions on callers of `f`, so they can use the output non-linearly and they can even pass a non-linear `T` in, if they want. the two things that this stronger type does (over `f :: T -&gt; ()`) are allow the use of `f` in a linear context, and require that the implementation of `f` actually use `T`. Another useful bit is that you can consume a unit by pattern matching on it. The purpose of `free` is to make it possible to write code using `alloc`; without `free`, it's not possible to consume the abstract type `T`, making it impossible to write a function of the type `alloc` requires.
I think of returning maybe as pushing the decision to the caller. That way you can still use it as a partial function but all information is in the same place and you don't have to keep as much implicit information in mind. If the Nothing part shouldn't ever happen then a partial function might be the right way to encode that, though an explicit error probably would be clearer.
1. Define a new data type `x` that exactly represents *X*. ("Make invalid states unrepresentable.") 1. Define `f` as an `x -&gt; b` instead. (Since the *intended* domain is *X*, use the type that matches your intent.) 1. (Optional) Expose a `Prism' a x`. `f' :: a -&gt; Maybe b` can be defined in terms of `f` and this new prism, if needed.
Author here. I actually spend my days coding in C++ and Python at work. This slide was a joke to lighten up in the otherwise equations filled slide deck
While it might be confusing, I think the term "value type" would be quite apropos for what you are calling a "nullary type". If you have a value `v` with type `t` (i.e. `v :: t`), then `t` has kind `*` (i.e. `t :: *`). So, the only types with values are nullary types. Unfortunately, "value type" wouldn't cover *all* nullary types, since `Void` (or whatever you want to call the type with no constructors) has kind `*`, but no values.
Thanks, I don't think it's really comparable to anything in Haskell right now. There's definitely some parallels to Purescript Pux though :)
I don't think restricting type constructors to mean _non-nullary_ types is particularly useful. Similarly, I consider both `Just` and `Nothing` value (/data) constructors.
And what would be the term to contrast it with? Also value type is already a term, which is the opposite of a reference type
Instead of adding a comment, consider using [`fromJustNote`](https://www.stackage.org/haddock/lts-8.5/safe-0.3.14/Safe.html#v:fromJustNote). 
That terminology is useful ~~in Java~~ on the JVM but I think it a bit troubling because *in general* references are values, too.
With template haskell you can reify the type signature of a particular value, and then reflect on it. So in a module defined _importing_ the one that uses `f` you can get access to those constraints to programmatically manipulate them. You could also do this directly through the GHC api, but that seems like a much more difficult path...
The advice to avoid partial functions is "partial" at best. If you're in control of the entirety of your code, such that calling your function with a value outside of the expected domain is always _your_ bug, then writing a function that on failure throws an informative error is often ok, assuming you exercise this with quickcheck or the like to ensure you've got it right. Furthermore, with our new partial-call-stack stuff, you can annotate appropriately and put a partial stack trace with your errors, which is much nicer! https://hackage.haskell.org/package/base-4.9.1.0/docs/GHC-Stack.html The compile-time-enforced runtime-cost-free restriction of a datatype to a subrange is also the sort of thing that LiquidHaskell is good for, though that is still in a "researchy" state and not for beginner usage I'd think.
&gt; No offense to the OP, but I think this question is about as meaningful and interesting as random musings on the topic of what's going to happen when an unstoppable force meets an immovable object etc. etc Maybe to you, but not to me. I think it's been an interesting discussion for a few reasons, like: 1. Devs should to be used to being really explicit and accurate (computers do what we say and not what we want), yet many are using the word "impossible" here when they probably don't mean it. 2. There's a significant difference between impossible and not. If something is possible but REALLY hard then we might be able to make it easier and ultimately achieve it. If it's impossible, then that's not going to happen. I like to think that the [poor state of software](https://twitter.com/SoftwareFailed) is something that will improve but frankly with the attitudes of many devs I see online lately, I'm not holding my breath :( &gt; Why would it matter to me as a practitioner? I never said it would. I'm just curious about the opinions and thought I would ask. I think it's quite interesting that the split between possible/not possible is almost exactly 50/50 and has been all the way from the first 10 votes on my poll to 160. I'm sorry if you feel you've wasted your time here, but I've found it interesting so the effort here hasn't been entirely wasted :-)
I don't see why that means I should be careful, that's a perfectly valid point (though luckily it turned out that it didn't happen :-))
Hello World is an example of a "large software" You are asking about? 
The only prima-facie solutions to the oracle problem are constructs like schelling contracts or other "voting" mechanisms (e.g. prediction markets). I think BreakfastBob's point still stands, that the trust is "reduced to what ever level of trust its oracle mechanism works by", in this case, a voting mechanism. Which of course can be subverted if the majority decides it's in their interest to vote differently. Bitcoin miners enjoy a global interest in the bitcoin currency, but Smart Contracts may only have a much smaller pool to draw from. For example, employees may form the majority of a company, but don't like how a merger is playing out, and so may out-vote the lesser number of executives to lie to the oracle/schelling contract/prediction market. There are ways to produce oracles of different degrees of satisfaction, but I don't see a way to solve the problem in a way that delivers on the supposed advertised benefits (e.g. the 'revolution') of Smart Contracts. 
&gt; Let's say, for the sake of argument, that you're using formal verification methods and have proved your code correct, for some definition of correct. I wasn't proposing we could ever figure out if a program is bug-free; only asking whether it's possible it could be. I was refuting the idea that it's *impossible* to have bug-free software but not whether we could ever ensure/measure/verify it. &gt; do you assume the CPU faithfully executes your opcodes? Well that depends. For the huge majority of people this is a valid assumption and if their program crashes because it doesn't then it's fair to say that's not a bug in *their* program. Obviously if part of the brief is tolerating this, then not doing so is a bug.
All great questions, and u/spopejoy can talk more to them though I'll try to answer a few. Also, see our whitepaper(s) for more info at http://www.kadena.io. There's a number of differences in philosophy between EVM and Pact. The biggest one is probably what smart contracts (in a private blockchain context) should be used for. We think they are for capturing business events and executing business rules (logic) based off of those events (i.e. closer to SQL than Java). EVM is more about leveraging a cryptocurrency to create a "world computer" platform (i.e. closer to Java than SQL). For example, we don't think that you should be folding proteins or pricing options on-chain. That's not to say that you couldn't do that in Pact, just that the language isn't designed to make that easy. What it makes easy is what you'd use a transactional database for generally: capturing and enforcing rules (by running logic) based on events, integrating with a front end application, populating a downstream application (Pact is generic over the DB it uses for persistence). That isn't to say it's a philosophical XOR. I think that for private blockchains our approach makes more sense while for public chains EVM's may be valuable. ### Metering Right now, Pact doesn't meter. Metering matters more for public chain infrastructure (which we're not targeting at the moment) because you need to have some sort of currency to pay the meter with. Remember that for private chains, the notion of a core cryptocurrency is rather meaningless; private chain users want the features that the chain itself provides. Instead, you want memory/step limits (mostly to protect against buggy contracts). Metering limits/calcs are on the roadmap but aren't OS yet. ### Subcurrency &amp; Safety Pact is designed to be a much safer language to work with. To that end, recursion and loops are out, making DAO's problem N/A. One of our motivations is: what type of language would I trust to protect my bank account? IMO I want a simple &amp; obvious language that avoids as many bugs as possible by construction, can't catch errors (e.g. if it isn't doing exactly what I expected it to do then stop), and can compile to z3/be proven just in case. The "token" example becomes pretty simple when you have tables and auth as first class citizens. All you really need is a table of [account:balance] and some auth. The web editor's "payments" demo is a tiny example and here's an one with some more bells and whistles: https://github.com/kadena-io/pact/tree/master/examples/accounts ### Objects &amp; Interpreted (so no VM) I'm not sure what you'd call Lisp-like modules but I'm pretty sure they aren't objects. There's no inheritance if that's what you're asking. When a module is put onto the blockchain, it get's fully resolved/inlined (using the modules that are already on-chain, if any are imported) at the Term level. When you call a contract's function via the API, you're calling that module's term-inlined function. This means that, when loading code onto the chain, you send over its source and that's what gets committed to the blockchain itself. We see on-chain smart contracts being human readable as a feature. 
Unless being able to understand the human language is the goal of your software I don't think that's a requirement to make a bug-free program. I'm pretty sure everyone here can write a bug-free Hello World?
&gt; It is impossible to build bug-free system for a very simple reason. No, but you said: &gt; It is impossible to build bug-free system for a very simple reason. So I'm posing the question. If you think it's possible to write a bug-free Hello World, why do you think it cannot be scaled up? At what point does a program go from being possible-to-be-bug-free to impossible?
Interesting! Why is Nothing a value constructor when it takes no arguments, if Int isn't a type constructor because it takes none? :)
But a program must satisfy requirements defined by humans. How do we define whether it is bug free? If it does "what it's supposed to do" correctly. This is the case of using `&gt;` instead of `&lt;`. Both are programs that run and compile, but one will be considered a bug or a mistake, because it still does not perform the right computation. How does the program know what the right computation is? How does the program know when something is syntactically correct, but semantically wrong? This is where interpreting requirements comes into play and there is where you'll always have bugs.
It's really simple. What we consider to be a bug depends on requirements defined by humans. The challenge in creating bug free software is, how do you translate fuzzy human requirements into a program without bugs? I think your software will need to be psychic in order to do this. Perhaps a better statement would be, how does your "compiler" or "tests" or "bug checker" or "bug-free prover" know that the right computation is?
Why do flammable and inflammable mean the same thing, but effective and ineffective are opposites? Usage, history, and general lack of consistency in human communication.
I was hoping we could do better... :-P
Ah, good tip! I could hack something together with that method. My needs are quite specific so I don't really need a beautiful formal framework at this point.
Cute name, but 1) it's totally un-Googleable, and 2) it doesn't mean anything.
lol, that's hilarious. And an excellent answer to the question. I am reminded of statistical mechanics, and the fact that air pressure is even throughout my room merely as a consequence of random chance and very large numbers. It is "unrealistic to extremely unrealistic", but possible, that all the oxygen atoms might suddenly find themselves in the upper top left square foot of my room.....
I had the [same question](https://github.com/aelve/microlens/issues/89#issuecomment-286089054) two days ago when trying to write a `Zoom` instance for a newtype-wrapped `StateT` in microlens... Thanks for asking this question here, and thanks /u/ekmett for the elaborate answer!
Our *best* estimates of the cost difference between formally verified software and well-tested release-quality software probably come from the SMACCM ("smack 'em") project. During that project a hack-proof drone was put together; a "red team" with access to the source was unable to find an exploit in 6 months. IIRC, they informally estimated cost of formally verified code at 3x to 5x the cost of normal software development. For software that needs to last a half-decade or more with a *static* purpose, you might be able to break-even based on having lower support / bugfix costs. Maybe. There's another problem with formal verification: when your requirements change, your verification has to restart. There might be pieces that can be reused, but everything tends toward the old "waterfall" method of development. In many environments, that's a non-starter. Now, verification can be done modularly, but the methods of doing so are the results for research that's even newer than SMACCM, and the latest I results were along the lines of taking a large problem and producing smaller sub-problems, not focusing on the development styles that are currently in favor, where small requirements are generated and resolved then later integration requirements come through later where you need to take small sub-solutions and produce a large solution. When requirements are very dynamic, and the focus is on this quarter's new features, formal verification as it currently works seems ill-fit, and there's no real other way to bug-free code. I think we can improve formal verification techniques and integrate them into an agile development process; but for now, for most development, formal methods are too costly. That said, I'm writing my next "for fun" project in Idris. I think formal verification is a better way forward than increased testing. Quickcheck and Smallcheck and American Fuzzy Lop are all great, but there's nothing quite like a proof term in a consistent logic. Maybe along the way I'll develop an intuition I can use to do "agile verficiation".
No particular order; mostly US groups; off the top of my head. * CMU's [POP group](http://www.cs.cmu.edu/Groups/pop/) * UPenn's [PLClub](http://www.cis.upenn.edu/~plclub/) * Cornell's [programming languages](https://www.cs.cornell.edu/research/lang) * [PLT](http://racket-lang.org/people.html) at Northeastern/Utah/Indiana/Brown/Northwestern/WPI/etc * Maryland's [PLum](http://www.cs.umd.edu/projects/PL/) * UCSD's [Programming Systems](http://cseweb.ucsd.edu/groups/progsys/) * Washington [Programming Languages and Systems](https://www.cs.washington.edu/research/plse/people) Edit: if you're specifically interested in category theory, not all of these apply. But PL research is a broader field than just Haskell or just category theory. Edit2: added a few more. More generally, a good way to find out about researchers is to look at recent publications in premiere academic conferences for the field. For PL research with a theoretical bent, I'd look at [POPL](http://popl17.sigplan.org/track/POPL-2017-papers#program), [ICFP](http://icfp16.sigplan.org/track/icfp-2016-papers#program) and [PLDI](http://conf.researchr.org/track/pldi-2017/pldi-2017-papers)
Great, thank you.
Tbh Haskellcast isn't particularly theoretical, but it's a really nice Haskell podcast. The Type Theory Podcast may be more what you're looking for, though it has only a few episodes. Welcome to Haskell, by the way :)
Thank you for the extended response. I still have some questions, but I think I should read your white papers and look at your examples before asking you anything further. Anyway, it's great to see you guys operating in the space. Keep up the good work!
This question wasn't about the *verifying* code as bug free; but whether it was possible to have created it.
&gt; Can the map typed linearly accept an unrestricted function as its argument? No. &gt; This inferred type is not backwards compatible, right? Right. We may want to infer the 'multiplicity-polymorphic' type written in the paper instead. As I wrote the inference is still in flux. &gt; This would be compiler magic like the implementation of seq, right? Yes. &gt; This means it would be impossible to give the same type to deepseq? Unclear at this point. It could be imagined either way. &gt; It feels to me like seq can more accurately be thought to be "putting the value back when it's done That's an alternative. But seq would need to have another type. 
&gt; My guess is that the people, who claim it's impossible to write but-free software, are the ones who write relatively buggy software. Mistakes happen, sure, but claiming that the absence of bugs is an impossibility sounds more like someone wanting to protect their ego rather than capture a truth. In some ways I agree with this; but there are also lots of devs I really respect that are saying it's impossible. That said, many of my colleagues said it was impossible but after just a few minutes of discussion (or thinking about it overnight) changed their opinions.
&gt; lol, that's hilarious. And an excellent answer to the question. I wouldn't say it's hilarious, I think it's actually one of the most accurate comments in this thread! It's exactly what I'm certain is the real answer - which is that it's not impossible. Impossible has a very well defined definition, and "extremely unlikely" is not it.
Lol. I actually appreciate HN being so critical about names, at least it means if you accidentally name your project the same as something else you'll find out about it.
Why are we still asking this kind of questions :D
[removed]
Doesn't the "smart constructor" end up being a partial function? So in some sense the problem is just moved onto another function. Is this an improvement? (I'm perfectly willing to believe it's an improvement, but wanted to ask.)
Only if you have a improbability drive.
Some tips that will hopefully help with readability and working out the issue: 1. Abuse pattern matching to make your code neat orGate :: (Int,Int) -&gt; Int orGate (x,y) | ((x,y) == (0,0)) = 0 | otherwise = 1 Can be written as orGate :: (Int,Int) -&gt; Int orGate (0,0) = 0 orGate _ = 1 2. Abuse constant functions to save typing and use the right types. Also it might be worth switching to Bools just for real type safety. You can have functions t :: Bool t=True f::Book f=False So that you don't have to use True and False everywhere. 3. Install the library Debug.Trace. That will let you see what your program is doing as it runs. You can use it by adding the following line as the first definition of your function (you can use it in other ways but this is easy to comment out) foo bar | trace ("running foo: "++show bar) False = undefined I hope you solve this one yourself using the tools that Haskell gives you, if you're still stuck feel free to PM me :) Edit: It seems editing Markdown on mobile isn't my thing. Hope this is readable enough 
&gt; When I'm browsing the web, if my internet connection drops while loading a page, my browser will show me a message that tells me the connection was lost. This is designed behaviour, but by your definition it is a bug in the browser. That's not a bug, that's a feature! :D It is entirely wanted and expected behavior when an internet connection drops.
I really like the `script` addition. Thanks! How big of a deal would it to just infer the `--packages ...` part based on the imports in the file? Also another thing I'd really love is to be able to transitively download all the sources right down to `base` for offline lookup. I'm actually much more interested in the latter and if the Stack team is open to it I'd like to help out with it.
We all agree that writing bug-free software is possible. We just need enough multiple lifetimes of the universe in order to finally get it right. :)
is `/Users/jon/.local/bin/stack` in your path before `/usr/local/bin/` ? 
Hehe, to me it's ego going the other way around: can some programmers really not stomach the thought that no software is perfect — not even their own?
The former is not in my PATH, these two are though (in the order shown): * /Users/jon/Library/Haskell/bin * /usr/local/bin Do I need to add `/Users/jon/.local/bin/stack`? Why is this required, I would expect the upgrade to apply for all users - should I have `sudo`'ed `stack upgrade`?
What benefits of setoids outweigh the value of maps though?
I've always thought there is another useful distinction to be made between "constants" and "functions", but the jargon for it wrt GHC Haskell is at the very least not well-signposted! For example, assuming `type family Id x where Id x = x`, both `Maybe` and `Id` inhabit the kind `Type -&gt; Type`. But the declaration of `Maybe` *generates* a *fresh* type *constant*, whereas the definition of `Id` does not. `Id` is called a "type family" in Haskell-ese because "type function" implies a type-level lambda a la System F-Omega, but in the terminology of term algebras ( https://en.wikipedia.org/wiki/Term_algebra ), I think `Maybe` is a constant whereas this `Id` is a function. This distinction explains why `Monad Maybe` is obviously well-formed while `Monad Id` is a more complicated beast. (This has to do with how GHC judges types to be equal and/or equivalent.) (Note that you can have nullary type functions such as `type Age = Int` or even the more exotic https://github.com/ghc/ghc/blob/e71068617d15b0fea65fe24e20c0ab0db9fc660f/compiler/prelude/TysWiredIn.hs#L307 `Any` type.) I think of this spectrum as: irreducible type families (e.g. `Any`) then reducible type families (e.g. type synonyms) then injective type families (e.g. `Id`) then type constants (e.g. `Maybe`). But I don't know of established jargon to tersely differentiate different points on it. Advice? Final observation: it seems misleading to compare "type constructor" and "data constructor". The key difference is that all data constructors are term *constants* but some type constructors are type functions (as I described earlier in this comment). On the other hand, I think "term constructor" (a term I've never seen used in this context!) is comparable to "type constructor". What is a "term constructor"? It's essentially a piece of (abstract) syntax, according to term algebra (https://en.wikipedia.org/wiki/Term_algebra ). Thus, wrapping up, an "X constructor" is "a thing that becomes an X after being applied to zero or more arguments." I hope this helps anyone as much as it's helped me just to write it.
Just advertising Chalmers. Most high-profile people in the academic FP community have at one point or another been at Chalmers. Current research interests include automated theorem proving, languge based security, and the development of Agda.
Typofix thread: s/practial/practical/ in https://hackage.haskell.org/package/setoid-0.1.0.0/docs/Data-Setoid.html
Why not simply `` $ rm `which stack` `` `` $ curl -sSL https://get.haskellstack.org/ | sh `` There's no point building when there is a binary available. 
Oleg Kiselyov has also written on the subject. See: http://okmij.org/ftp/Haskell/TypeClass.html#undecidable-inst-defense
I think that in general there may be collisions in the file imports, so inferring `--packages ...` might be impossible. That aside, I too would so very much like to have sources downloaded for offline lookup.
&gt; I wasn't proposing we could ever figure out if a program is bug-free; only asking whether it's possible it could be. I was refuting the idea that it's impossible to have bug-free software but not whether we could ever ensure/measure/verify it. For an appropriate definition of correct (ie, conforms to spec) there's proof by example in [seL4](https://ts.data61.csiro.au/projects/TS/l4.verified/). Whether that's the only way to achieve it is not relevant, it's clearly possible. By extension, it's possible for _large_ systems, if you're willing to spend the money on it. &gt; Obviously if part of the brief is tolerating this, then not doing so is a bug. The informality of the term "bug" is a bug? :-)
Throw a trace statement into the addition function and see where it goes wrong then. See my comment below on some more style tips, using Ints as Bools is asking for trouble
why hasql? I don't get the allure of writing sql in QQ. And is it composable?
Jah bless hpack.
Oh wow, had no idea. If non-Hackage sources, `Github` etc are stored here as well then the feature already exists!
No, packages from git sources are stored in the `.stack-work` directory where they are used. It should still be possible to `--prefetch` them and access them e.g. via `stack haddock`.
Will the unification of TypeFamilies and FunctionalDependencies (in terms of core) improve the undecidable instances situation at all? IIRC type checking is done before desugaring so perhaps you still aren't in a great spot. And in regards to the overlapping `MonadIO` stuff, is there any way to avoid the whole `n^2` issue with transformers with only safe / no usage of overlapping instances. It is pretty annoying IMO.
Is there a specific module or set of modules you think would particularly benefit from review?
I'd say there are two modules that come to mind: DbStructure and DbRequestBuilder. DbStructure has some necessary complexity in its embedded SQL queries but it also uses gnarly pattern matching in some of the functions that could be improved. DbRequestBuilder is short, but very dense. It interprets web requests and translates them into a tree of resources that should be requested from the database. For instance somebody might request a list of movies from the API, but ask that those movies' actors should be embedded in the response. This module translates that into a SQL query with the right JOINs etc. Decomposing the big functions in this module into simpler building blocks would be pretty nice.
`--show-iface` option on ghc will give you the type signatures in your code. 
Important question: how good are the tests? Can I make refactorings and assume they're valid if the tests pass?
Then I wouldn't really say it's proof that you have a "**lot**" of experience in this area.
The tests are quite good. I make sure each pull request that adds or changes any functionality includes a test. All the tests are at a high level, testing that HTTP requests against the server behave as they should. There is also CI integration that runs the test suite, looks for hlint suggestions, does cabal check, uses packdeps to find overly constricting dependency bounds, as well as some other things (see circle.yml). What could be improved about the tests is that they set up a huge database fixture at the start that gets shared among all the tests, so it can seem kind of magical when certain tests somehow "know" that certain endpoints are available to them. I would prefer if each test (or at least group of related tests) would set up the database for itself and clean up afterward.
Good point. I think at the time I did the tuple thing to interface with the way the Hasql db library reads columns and rows. But maybe there's a more natural way, possibly with a custom data type as suggested. Also like you say some lines with two operators some with one, totally weird. :)
IMO stack should disable the `stack upgrade` suggestion if it was installed via a package manager (could be a build-time option). It only causes confusion.
&gt; Fill a system's memory, disk, CPU, etc. with a set of bits that hasn't been tried before, and run in production. I don't think you need to go that far. If your application crashes because of some random bits on disk/memory etc. that are not related to nor access directly by the program then I'd argue that's not a bug with your program. It might be a bug with the OS/runtime/framework/libraries/whatever.
&gt; How do you like this? Would you change or add something? I am currently using `HashMap.fromListWith` to do the kind of things that are in your example, and just in the process of giving `discrimination` a spin. Are there performance advantages to setoids?
Chalmers is great
&gt; Will the unification of TypeFamilies and FunctionalDependencies (in terms of core) improve the undecidable instances situation at all? It really all depends on when and how the check happens. &gt; is there any way to avoid the whole n^2 issue with transformers with only safe / no usage of overlapping instances. You're stuck writing n^2 instances. But to be fair, n is about 6, and not all of them commute, e.g. MonadWriter and MonadCont, so the full n by n chart has _holes_ in it that matter. Worse, some runs of holes run horizontally and some vertically. Lying to yourself by trying to fill it in and spackling over the holes means you silently start getting instances that don't pass your laws. This is one issue with most (er.. all) other effect systems. Either they are too crippled to handle Cont as an effect because they limit themselves to handling purely algebraic effects (and then things like pass/listen are also missing as they are "handler"-like) or having a "handler" is a lie and you need to check your laws based on how you handle the result.
Sorry, I have no need for the library right now, but I'm sure one day I will, so am happy to see stuff like this being created. Well done on publishing your first package; that's a great milestone to have reached :)
On (imagined) behalf of the community: please provide an ASCII version of `⊸`. People in this thread are already using `-o` (illegal according to current parsing rules, I believe), `-.` and `-&lt;&gt;`, and I see no ASCII alternative in your fork. Linear typing would be far less convenient if it could not be easily typed (no pun intended).
Both HashMap and `discrimination` are probably more performant than `setoid`, although I have not tested `discrimination` yet. Those libraries don't have to maintain "uniqueness" of members after transformations. Our implementation is actually a wrapper around Data.Map and I should probably benchmark it with HashMap to see if it would make it faster. Thanks for the suggestion.
Here's an alternative design of the Category class that is more like what you are suggesting: https://hackage.haskell.org/package/data-category-0.6.2/docs/Data-Category.html It needs no `u` methods because it uses identity arrows to represent the objects.
Alternatively, `fromMaybe (error "comment")`. 
Category isn't as general as it could be. e.g. you can't use it in its current form to properly model product and sum categories. A middle ground is to use some kind of constraint to represent the set of objects. data Dict p where Dict :: p =&gt; Dict p class Category (p :: k -&gt; k -&gt; * where type Ob p :: k -&gt; Constraint id :: Ob p a =&gt; p a a (.) :: p b c -&gt; p a b -&gt; p a c src :: p a b -&gt; Dict (Ob p a) tgt :: p a b -&gt; Dict (Ob p b) class Trivial a instance Trivial a instance Category (-&gt;) where type Ob (-&gt;) = Trivial id x = x (.) f g x = f (g x) src _ = Dict tgt _ = Dict This is enough to allow product and sum categories and a lot of other interesting examples, such as a proper category of natural transformations over functors between any two categories. It isn't sufficient for things like slice categories or anything where you have interesting equalities to deal with. You can go further, like Sjoerd Visscher does in [`data-category`](http://hackage.haskell.org/package/data-category) but the result gets pretty far outside of the usual Haskell style.
Aha, I think I understand now. Thanks for fixing my misconception. Would you help picking a better name for it then? Why is it a subset then? Why not the complete quotient set?
Why do people keep pointing out `NaN == NaN`? It is just as false in any of the mentioned languages; in fact it is false in any standard complying language.
&gt; Is hoogle what I'm looking for? Does any editor have this integration already? Yes. I use spacemacs+intero and I didn't have to do anything special in order to get hoogle results about my codebase.
Great. Could you help me with the keybinding, please? And what do you do to discover new editor features?
&gt; even after forcing subexpressions, I found that the debugger still descends every subexpression. That's odd, it's working for me. Here is a simple two-step expression: &gt; :step f (g 42) _result :: Int = _ x :: Int = _ --- f :: Int -&gt; Int f x = x + x ^^^^^ &gt; :step _result :: Int = _ x :: Int = 42 g :: Int -&gt; Int --- g x = x + 1 ^^^^^ &gt; :step 86 And when I force the subexpression, it terminates in one step instead: &gt; :step f (g 42) _result :: Int = _ x :: Int = _ --- f :: Int -&gt; Int f x = x + x ^^^^^ &gt; :force x x = 43 &gt; :step 86 &gt; it would be nicer if you could determine yourself which parts of the execution you want to track. You can use [`:steplocal`](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/ghci.html#ghci-cmd-:steplocal) to skip the steps from the subexpressions. It will still evaluate the subexpressions, but it won't stop and wait for commands during their evaluation. For example, if I use `:steplocal`, the evaluation terminates in one step again: &gt; :step f (g 42) _result :: Int = _ x :: Int = _ --- f :: Int -&gt; Int f x = x + x ^^^^^ &gt; :steplocal 86 &gt; I wonder if the ghci debugger could more convenient to use. You can build [more advanced stepping commands](http://gelisam.blogspot.ca/2015/05/strongly-typed-ghci-commands.html) by combining a few of the current ghci commands, but I agree that there is still a lot of room for improvement in terms of usability. &gt; I get the impression the ghci debugger is hardly ever used. Indeed, many of us prefer to think of our Haskell programs more in terms of the mathematical relationship between the inputs and the outputs of our functions, and less in terms of the step-by-step execution which the debugger will expose (we prefer the "denotational semantics" rather than the "operational semantics"). I think I saw a /u/Tekmo tweet recently which said something like "only debug as the ultimate last resort" :)
&gt; So are the things we can define with Category a subset (not sure if I'm using the right word here) of categories, only the ones which have Haskell types as objects? Yes.
Okay thanks for the link, I'll have a look at that.
Just skip past the first ~5 minutes to get to the meat. Even ignoring the above there's countless good reasons to look into, evaluate and compare "higher-level"(-than-JS) source languages ..
What hoogle support do you have in the spacemacs wrappers for intero?
What I find easiest in formulas like these, is to use a list comprehension: &gt; let f m n = sum [(m+i)^n | i &lt;- [m..n]] in f 0 3 36 
i meant n&lt;m =0 , the sum from the number m until it reaches n, eg inputs are m=3 n=5, i want to compute the sum of the above expression from 3 to 5. the first guard checks that m and n are not negatives. thanks for the observation, any other suggestions?
Works here: http://i.imgur.com/qGUG0YR.png Did your module type check before you wrote `CM.`? Intero needs a successful type check before it's able to get data about the current module.
Typos are embarrassing :) Fixed it on github already.
My mistake, it wasn't spacemacs+intero, it was simply stack: &gt; cat Foo.hs module Foo where data Foo = Foo Int mkFoo :: Int -&gt; Foo mkFoo = Foo getInt :: Foo -&gt; Int getInt (Foo i) = i &gt; stack hoogle 'Foo +mylib' module Foo Foo data Foo Foo Foo :: Int -&gt; Foo Foo mkFoo :: Int -&gt; Foo &gt; what do you do to discover new editor features? The haskell keybindings are [here](http://spacemacs.org/layers/+lang/haskell/README.html#key-bindings). I'm still new to both spacemacs and intero, so my strategy is: I google and google and give up, then I look at the source code in `~/.emacs.d`, then I give up for real :(
&gt; However, linearity isn't useless. Oh, I agree. I read the comment I was replying to as saying that *the undecidability* "would seem to suggest[...]". It's totally the case that there are *other* things that suggest that here pretty clearly. I just wanted to get at the parent's reasoning (or figure out how I misread them).
I certainly wouldn't say "without any issues", no. I haven't learned how completion works yet, those and "go to definition" (`g d` and `SPC p g`) both work unpredictably. What I use the most so far is the repl and `SPC m h t` to get the type of the selected expression. That doesn't work all the time either, but here's what I've figured out so far: I have to build the project ("compile-time") with `:!stack build`. Then, I type `C-C C-L` ("load-time") and intero will use the compiled modules for everything except the current file. In the repl, I have access to the definitions from the file at load-time, and if those definitions use definitions from other modules, it's the compile-time version which will be used. If I switch to a different file and type `C-C C-L` again, I will have the load-time version of this new file but the compile-time version of the previous file. The only way to see both changes at once (for example, to use the functions in one file to test the changes in the other) is to recompile the project and to run `SPC SPC intero-restart` to get a new compile-time copy of both modules.
Leader h l will give you a helm buffer for all possible layers, which opens a layer's documentation upon selection. 
Thanks. At it least C-c C-t which shows the type at point works.
I think you should be getting `[1,1,0,1,1,0,0,0,1]`, assuming the leftmost bit in each list is the least significant. Note that I've got 9 bits in my answer rather than 8.
It's not Haskell specific, but SPLASH/OOPSLA is in Vancouver this year. If you're a student, you could volunteer to get in for free.
https://www.youtube.com/watch?v=_eRRab36XLI
Well, take for example this program: f x = g x + x g x = x + 1 Even when I `:force` the result, it still steps into it: λ&gt; :step f 42 Stopped in Main.f, /tmp/test2.hs:1:7-13 _result :: a = _ x :: a = _ 1 f x = g x + x ^^^^^^^ 2 g x = x + 1 λ&gt; :force _result _result = 85 λ&gt; :step Stopped in Main.f, /tmp/test2.hs:1:7-9 _result :: Integer = _ x :: Integer = 42 1 f x = g x + x ^^^ 2 g x = x + 1 λ&gt; :force _result _result = 43 λ&gt; :step Stopped in Main.g, /tmp/test2.hs:2:7-11 _result :: Integer = _ x :: Integer = 42 1 f x = g x + x 2 g x = x + 1 ^^^^^ 3 
Congrats on your first Hackage library! I love that it is split out into a client part and a core part, so that ghcjs, purescript-bridge and the like can make use of the types and the API for frontend work.
I would say it's the second best option. In terms of information, it tells you that you need to handle error (or explicitly forego it). Most of the time, even more useful is a type that tells the producer of `a`s about the constraints on the values they're producing. The `newtype Foo` approach does this, with a smart constructor - though types that structurally can't represent an invalid value are better where that's easy.
You *could* write a `NonNan` type in Haskell that, like `NonEmpty`, actually prevented you from even performing such equalities. ;)
That logo is just ... bad.
Thanks for the Hac φ love, from one of the organizers! :-) It will be happening again this year, never fear!
I'd really appreciate Haskell-specific feedback, for any of you that like to cringe at beginner Haskell code. I know there are more efficient ways to compute the GoL, but I'm probably unaware of where I should be sharing a top level function, adding a bang pattern, etc.. For example I've got a trivial [test suite](https://github.com/samtay/conway/blob/master/test/Spec.hs), but it takes a full ~ 15 seconds to complete. I imagine this is due to some naivety with evaluation or non-strictness. Thanks in advance.
Ah, yes that's true. In that regard it was extremely well done and very helpful. I doubt many people have tried as many different solutions as you have!
That does appear to have worked. Note, I wasn't, as far as I know, building anything.
I believe Reflex has some pre-render stuff. I haven't used it myself yet, but if you view source on the [Obsidian Systems website](https://obsidian.systems/) (which is generated by Reflex) you can see that it is pre-rendered.
I don't think that's prerendered from Reflex-DOM itself. Most of it is static HTML and CSS.
I think most people using PureScript/Elm/GHCJS are making fat clients. For fat clients you don't really care about server-side rendering. That might explain why it's not mentioned as a priority in this talk. Certainly _I_ don't care at all about code sharing, it feels a bit overplayed to me, but that's explained by the kinds of apps I write. 
How would you do that without dependent types?
It's not going to be effective to use an app to make apps anyway. Better decouple your UI from the rest and develop/test on the desktop. Hopefully you won't need the emulator too much. But of course, it would still work, just a bit complicated with the Android toolchain. Anyway, here is an example of a full Android app in Frege (check out [mchav's blog](http://mchav.github.io)): https://github.com/mchav/try-frege-android/blob/master/app/src/main/frege/io/github/mchav/tryfrege/MainActivity.fr
Yes: [mchav/froid](https://github.com/mchav/froid) It's not an actual app itself, but I don't know why you'd want that.
Hi! [I'm a second-year PhD student](http://very.science/) in the [programming languages group](http://www.cis.upenn.edu/~plclub/) at UPenn. I'm advised by [Stephanie Weirich](http://www.cis.upenn.edu/~sweirich/), who does [some](http://www.seas.upenn.edu/~sweirich/papers/esop2016-typeapp.pdf) [fair](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/gadt-pldi.pdf) [bit](http://www.seas.upenn.edu/~sweirich/papers/coercible.pdf) [of](http://www.seas.upenn.edu/~sweirich/papers/aritygen.pdf) [functional](http://www.seas.upenn.edu/~sweirich/papers/tldi12.pdf) [programming](http://www.seas.upenn.edu/~sweirich/papers/popl163af-weirich.pdf), as do both of the other faculty in our research group ([Steve Zdancewic](http://www.cis.upenn.edu/~stevez/) and [Benjamin Pierce](http://www.cis.upenn.edu/~bcpierce/)). We annually host the "Haskell exchange" Hac Phi (I'm a co-organizer), [recently reviewed](https://www.reddit.com/r/haskell/comments/5zr2on/upcoming_uscan_conferenceshackathons/) as "awesome" by /u/HaskellOpenBSD, and there's a ton of Haskell-related research—and, of course, some not-as-Haskell-related-but-still-awesome research—going on here. If you (or anyone else) wants to chat about what it's like at Penn, my virtual door is open; just shoot me an email at the address on my personal page linked above.
Great talk! How do API calls/updating the backend database state fit into the flux pattern? It's not obvious to me how this would work if a user updates state in the view, the application needs to send the update to the backend, present an animation in the view while waiting for a response, and finally update the view upon receiving a successful response. Have you released the GHCJS + GHC implementation? I'd be interested in looking through the code. 
It would have made sense to use n as your iteration variable if you wanted to implement something like [ Sum(i) from i=0 to n ], but in your [ Sum(m+i)^n from i=m to n ], the m and the n appear in the (m+i)^n term, so you can't discard them. If you use n as your i, you'll be computing [ Sum(m+i)^i from i=m to n ] instead!
Interesting! I notice that if I write f x = let gx = g x in gx + x or f x = let r = g x + x in r instead, then I can get the single-step behaviour by either forcing `gx`, `r` or `_result`. Maybe there's something special about nested function applications?
Thanks =P
Purescript-servant and purescript-bridge have done a good job of sharing types between the front and back end for me, although I've only used it with a relatively simple hobby site
It is pre-rendered from Reflex-DOM code. There is no static HTML.
i think it's perfect
the first time i debugged the "missing symbols" i.e. unlisted module took me hours... i felt so dumb. if haskell weren't overwhelmingly superior to other languages i might've abandoned it right there, and my project would suffer through some impure functional language alternative. 
The command is `:l` (lowercase L, for "load"), not :1 (the number 1) 
For the first one, can you provide the exact usage and the full error message given?
unfortunately, it's not cross-platform. if nix worked on windows, then all programming languages could actually implement their build tool and package manager on it, rather than reinventing dependency-tracking/caching/etc. and haskell would probably be the first to do so.
Hopefully the linux subsystem on Windows gets Nix working. Then I could justify doing 100% Nix if I ever get around to making a language.
You seem to be slightly confused :) Firstly, if you want to define a function (e.g. `doubleMe`) in GHCi, you need to prefix it with `let`. So, to define `doubleMe` in GHCi you would write: let doubleMe x = x + x Optionally, you can supply its type inline like this d :: Integer -&gt; Integer; d x = x + x or using `:{` and `:}` for multiline input: :{ d :: Integer -&gt; Integer d x = x + x :} Lastly, the command for loading a file is `:l` (lowercase L), short for `:load`. See the [GHCi Guide](https://downloads.haskell.org/~ghc/8.0.1/docs/html/users_guide/ghci.html) for more info.
same. for me, my thoughts are less about a personal programming language (there is only one programming language, and it's name is Haskell With EDSLs... lol), but rather implementing an application-level package manager (like Emacs does) via Nix. rather than either restricting users to editing textual configs (no dependencies) or manually installing (haskell, system, other) dependencies via the command line. idk enough about either windows or nix, but windows has so many incompatibilities/idiocies (like filename size??), something radical (ie WSL) is probably the only realistic solution. and then there are probably other lesser incompatibilities, a poverty of developers, and the fact that this feature might be withdrawn for whatever reason. still, i'll just hope and wait. 
While I can't speak for /u/l-d-s, one possible argument I can think of for undecidability suggesting optimizations is that a) humans have a tendency to focus our interest on "general" problems/properties, immediately disregarding properties like "this program takes a prime number of steps to halt", b) [Rice's theorem](https://en.wikipedia.org/wiki/Rice's_theorem) states that all semantic properties of programs are undecidable, and c) "general" semantic properties of programs are often good opportunities for optimization. It's not the world's best argument; 'a)' is a bit weasel-ly, and I imagine you might disagree with 'c)' for the same reason you disagreed with the original comment. Still, I think it's reasonable enough, to the point where I'd find it more likely than not that you'd be able to optimize a program if you could prove *any* well enough studied property of it.
Sharing types indeed is the one thing I do use. Good point. ☝
There's probably a fancier way, but I prefer the conceptually simple approach. checkList :: [Int] -&gt; Bool checkList xs = all zeroOr1 xs || all zeroOrNeg1 xs where zeroOr1 x = x == 0 || x == 1 zeroOrNeg1 = x == 0 || x == -1 
A (maybe less readable) way that leverages applicative nature of function application: f = (||) &lt;$&gt; all (zeroOr 1) &lt;*&gt; all (zeroOr (-1)) where zeroOr x = (||) &lt;$&gt; (==x) &lt;*&gt; (==0)
This looks cool! I can't look at it right now but I starred it. 
This is a convoluted journey to explore Haskell functions. &gt; [ (n `elem` [0,1], n `elem` [-1,0]) | n &lt;- [0,1,0,1] ] [(True, True), (True, False), (True, True), (True, False)] This list of pairs tells us something about the corresponding value of our input list `[0,1,0,1]`. The first component of each pair tells us if a value is 0 or 1, the second component tells us it is -1 or 0. We then use [`unzip`](https://hackage.haskell.org/package/base-4.9.0.0/docs/Data-List.html#v:unzip) unzip :: [(a, b)] -&gt; ([a], [b]) Now the first component `[True,True,True,True]` tells us that each element of `[0,1,0,1]` is 0 or 1: &gt; unzip [ (n `elem` [0,1], n `elem` [-1,0]) | n &lt;- [0,1,0,1] ] ([True,True,True,True], [True,False,True,False]) We want to check that this holds for all elements, for that we use [`and`](https://hackage.haskell.org/package/base-4.9.0.0/docs/Data-List.html#v:and) which returns `True` exactly when if every element in the list is `True` &gt; :type and and :: Foldable t =&gt; t Bool -&gt; Bool &gt; :set -XTypeApplications &gt; :t and @[] and @[] :: [Bool] -&gt; Bool &gt; and [True,False,True] False We want to map `and @[]` over both our pairs, so we use [`bimap`](https://hackage.haskell.org/package/base-4.9.1.0/docs/Data-Bifunctor.html) &gt; import Data.Bifunctor &gt; :t bimap bimap :: Bifunctor p =&gt; (a -&gt; a') -&gt; (b -&gt; b') -&gt; (p a b -&gt; p a' b') &gt; :t bimap @(,) bimap @(,) :: (a -&gt; a') -&gt; (b -&gt; b') -&gt; ((a, b) -&gt; (a', b')) &gt; :t bimap @(,) (and @[]) (and @[]) bimap @(,) (and @[]) (and @[]) :: ([Bool], [Bool]) -&gt; (Bool, Bool) We can see that the first component is `True` (because `[0,1,0,1]` only contains 0 or 1) and the second component is `False` (because it contains a 1): &gt; bimap and and $ unzip [ (n `elem` [0,1], n `elem` [-1,0]) | n &lt;- [0,1,0,1] ] (True,False) We want to see if at least *one* of those condition holds, so we use [`(||)`](https://hackage.haskell.org/package/base-4.9.0.0/docs/Data-Bool.html#v:-124--124-) &gt; :t (||) (||) :: Bool -&gt; Bool -&gt; Bool But we want to apply it to the pair `(True,False)`, we can use [`uncurry`](https://hackage.haskell.org/package/base-4.9.1.0/docs/Data-Tuple.html#v:uncurry) to transform a function `a -&gt; b -&gt; c` into `(a, b) -&gt; c` &gt; :t uncurry (||) uncurry (||) :: (Bool, Bool) -&gt; Bool &gt; uncurry (||) $ bimap and and $ unzip [ (n `elem` [0,1], n `elem` [-1,0]) | n &lt;- [0,1,0,1] ] True Now we can define the complete function check :: (Num a, Eq a) =&gt; [a] -&gt; Bool check ns = uncurry (||) $ bimap and and $ unzip [ (n `elem` [0,1], n `elem` [-1,0]) | n &lt;- ns ] We can get rid of the list comprehension nicely with [`(&amp;&amp;&amp;)`](https://hackage.haskell.org/package/base-4.9.0.0/docs/Control-Arrow.html#v:-38--38--38-) &gt; import Control.Arrow &gt; :t (&amp;&amp;&amp;) (&amp;&amp;&amp;) :: Arrow arr =&gt; arr a b -&gt; arr a b' -&gt; arr a (b, b') &gt; :t (&amp;&amp;&amp;) @(-&gt;) (&amp;&amp;&amp;) @(-&gt;) :: (a -&gt; b) -&gt; (a -&gt; b') -&gt; (a -&gt; (b, b')) &gt; :t (`elem` [0,1]) &amp;&amp;&amp; (`elem` [-1,0::Int]) (`elem` [0,1]) &amp;&amp;&amp; (`elem` [-1,0::Int]) :: Int -&gt; (Bool, Bool) Now we can write our function pointfree check :: (Num a, Eq a) =&gt; [a] -&gt; Bool check = uncurry (||) . bimap and and . unzip . map ((`elem` [0,1]) &amp;&amp;&amp; (`elem` [-1,0]))
I would tend to avoid ever using the monad/applicative instances of functions. It makes code that's incredibly hard to read. First you have to notice that a parameter is missing to realize that this is all in the function monad, then you have decipher where exactly that parameter is being threaded through.
Is there another more idiomatic way you would construct a function like: \ f g -&gt; (,) &lt;$&gt; f &lt;*&gt; g :: (i -&gt; a) -&gt; (i -&gt; b) -&gt; i -&gt; (a,b) That's a nice pattern to use in practice. 
 \f g i -&gt; (f i, g i) This is infinitely clearer to anyone reading the code. Your version may be satisfying to write, but it takes me an order of magnitude longer to recognize what that's doing when I stumble upon it within a project.
(jaw drop) Well that settles it! Awesome.
Yes, `(&amp;&amp;&amp;)`
Or you can see straight away that `all (zeroOr 1)` and `all (zeroOr (-1))` are `(||)`'d together. The entire point of that is to abstract away mudane details of passing the argument in question. Of course, if there's something non-obvious in argument passing, you want to avoid forcing it through applicative. Example o bad usage: `(==) &lt;*&gt; f`.
People can create Github issues on the project for things they spot, or even pull requests if they're feeling ambitious. The chat room is a good place to hear general development thoughts - https://gitter.im/begriffs/postgrest
This isn't correct - take a look how let desugars in do: https://www.haskell.org/onlinereport/haskell2010/haskellch3.html#x8-470003.14 do {e} = e do {e;stmts} = e &gt;&gt; do {stmts} do {p &lt;- e; stmts} = let ok p = do {stmts} ok _ = fail "..." in e &gt;&gt;= ok do {let decls; stmts} = let decls in do {stmts} So, following these rules, it desugars directly to the let .. in version. The weird part is that (&gt;&gt;) / (&gt;&gt;=) never occur in the desugaring, so there isn't much point in using do notation, and so is misleading. It is perfectly reasonable to use do notation for pure code. For example, the Maybe monad: f a = do b &lt;- (functionThatMightFail a :: Maybe Int) c &lt;- (anotherFunctionThatMightFail a :: Maybe Int) return (b + c)
This article is very difficult to read on a phone. A larger font or mobile-friendly theme would be great.
Because it’s confusing. The *whole purpose* of `do` (at least in standard Haskell) is to provide a succinct, readable notation for monadic expressions. Frankly, I would always expect a use of `do` to involve a monadic computation, and I would assume the final expression in a `do` block to produce some value with a type of class `Monad`. In another timeline, it’s wholly possible that the designers of Haskell could have collapsed `do` and `let` into a single notation, replacing `let...in` with `do...let` and idiomatically using `do` for both monadic and non-monadic code. It didn’t work out that way, though, so uses of `do` that don’t involve monads are decidedly unidiomatic, and unidiomatic code is often much harder to read than idiomatic code.
Your blog post is good! I didn’t mean to dogpile on the technical details—it was only after I wrote my first comment that I discovered two other people had written very similar things while I was typing (and I decided to leave mine around since it mentioned something the others didn’t). That said, I’m not sure how to avoid that. Honestly, I have never personally seen anyone make that mistake, but you mentioned you saw two other people do it, so maybe I’m just not interacting enough with learners. My favorite resource to help people understand `do` notation better (once they’ve *already* grokked monads, at least at a basic level) is probably [Do notation considered harmful](https://wiki.haskell.org/Do_notation_considered_harmful). I don’t actually consider `do` notation harmful at all—I think it’s quite a good thing, actually—but that article covers some of the common pitfalls people fall into because they don’t really understand what `do` notation is doing (and how simple it is).
iirc, /u/ryantrinkle has collaborated with Conal? 
There's some cool stuff with Node that you can do with Elm, although it's not officially supported. 
"Not officially supported" sounds like a recurring theme. ;) But then again, I'm not even sure what "official" means in the OSS world.
Yes, I don’t think there is a strong technical reason to avoid it, but there is a strong social one. Programs are meant to be read and all that. I think your quote from /u/Tekmo is precisely right: there’s nothing fundamentally wrong with it, but people will probably give you funny looks.
This seems to be wrong (but I don't accuse you for assuming that :) ), this: do let a = if True then 4 else c c = a a + c produces `8` with ghci 8.0, so you can define recursive symbols in a `let` statement of a `do` block.
That makes sense. I was actually referring to the fact that *because* `let` is recursive, then merging them with `do` would require that `do` is also recursive, but that's a non-trivial when it comes to monadic code. Either that or make `let`/`do` non-recursive (like other ML langs) but add `do rec` to cover both recursive let and recursive do.
I'm wondering if you've looked at reactive-banana or reflex for doing this? I've found them both to be really great in this space. 
When we embarked on this project there was some folklore that linear types fit better with strict languages. We found this to be mostly incorrect. Most difficulties that you get with thunks you also get with closures. My personal hope is that linear types will make laziness more useful, mitigating SPJ's statement, precisely because it can be controlled better by the programmer using linearity. See this wiki page: https://ghc.haskell.org/trac/ghc/wiki/LinearTypes/Examples (especially the last part). 
You actually don't need `let ` in ghci definitions since 8.0.
[removed]
From a quick browse it looks like reflex came and went within 3 commits, and I'm struggling to find any code in those commits that actually used it. It can understand that though - it can be a bit daunting to get started with, especially in the non-web space :)
Eve is also a programming language, but I believe it will remain vaporware, so don't let that stop you.
This might give you some list fusion magic, but I think I'd prefer to use a library like `foldl` for this, just to get this optimised into one pass: import Control.Foldl checkList :: [Int] -&gt; Bool checkList = fold $ liftA2 (||) (all zeroOr1) (all zeroOrNeg1 xs) where ...
While this is a lovely post exploring Haskell functions, I honestly think you've just ended up with something unreadable. If a colleague tried to commit that `check` code in at work, I'd have to reject that in a code review - that is seriously opaque.
They are happy slaves, for some definition of happiness. I do not doubt the sincerity of some Chinese that says that his country is more free in some aspects. There is no such amount of informers and amateur policemen enforcing the state ideology than in some Western countries
 (`elem` [[],[1],[-1]]) . nub . filter (/=0) `nub` is quadratic if you use the whole output but will be fine here because we only need it to produce one element and check whether or not there is a second element. If it worries you, `group` also works but needs an extra layer of `[]`. edit: thought about it some more and although the `nub` version is linear time I need to convince myself that it's constant space. The `group` version isn't quite as pretty but I'm more confident about it: (`elem` [[],[1],[-1]]) . map head . group . filter (/=0)
You misunderstand the Bitcoin protocol. Burning off energy is not a bug, it's what sets Bitcoin apart from all other digital currencies before it. The special thing about a Bitcoin is that *a proof* is calculated, by the chips that burn the energy, such that everyone can easily verify that the energy has been burned off, thus preventing the transaction history from being altered. If this feature were not present in Bitcoin, it would be possible for everyone to agree to give each other 10x as many currency units right before you buy one bitcoin for $1200. And there would be no way to prove that it hadn't happened, because history can be rewritten so easily without proof-of-work, which isn't acceptable for digital money.
It's worth remembering that Haskell allows arbitrary recursion, which has exactly the same effect as a runtime error (except that it uses a bit more power). Using linear types, we can restrict recursion, thus increasing safety.
There's a lot of folklore around evaluation order. Unfortunately, much of it is wrong or misleading. Evaluation order is best formulated as a property of types, rather than a whole-language property. The fact that we think of ML and Haskell as "strict" and "lazy" respectively is IMO mostly an accident of history. Under a linear type discipline, every argument to a function will be used. As a result, you can think of the function type as being inherently strict. However, pairs come in both strict and lazy variants, and linear type systems need to offer both (the strict version is the "tensor product" A ⊗ B and the lazy version is the Cartesian product A &amp; B). The sum type A ⊕ B is naturally strict. However, *don't* make the mistake of thinking datatypes are strict in general -- in fact, inductive types *also* come in strict and lazy versions. The Haskell design lore that you can often gain performance by making datatypes spine-strict is a consequence of this insufficiently-appreciated fact. Likewise, basically the entirety of John Hughes's famous paper *Why Functional Programming Matters* can be read as an advertisement for the benefits of supporting lazy inductive types. Coinductive datatypes are lazy, but there is no strict coinductive type *unless* the language also has support for first-class continuations -- i.e., you are working with classical linear logic. (With first-class control, you also get a lazy sum A ⅋ B, pronounced "A par B".) When I say a type constructor "is strict" or "is lazy", what I mean is that this is the evaluation order which ensures the full set of beta/eta laws will hold, even when the language is effectful. The basic framework is best explained in Noam Zeilberger's paper [*On the Unity of Duality*](noamz.org/papers/unity-duality.pdf), and the story about (co)inductive types was to my knowledge first explained in David Baelde's paper [*Least and Greatest Fixed Points in Linear Logic*](http://www.lsv.ens-cachan.fr/Publis/PAPERS/PDF/baelde12tocl.pdf). If you like denotational semantics, Paul Levy's work on [*call by push value*](http://www.cs.bham.ac.uk/~pbl/cbpv.html) offers another useful way of understanding these issues. (Paul told me he worked out how linear cbpv works, but unfortunately he hasn't written it up for publication yet.) 
So I came back to the thread to see that the person you replied to actually said higher ranked types. I thought they said higher kinded types and that you were the confused one. Turns out I'm dumb. Sorry. ...That being said, there's a pretty decent chance they did mean HKT. People usually say rank-n types, not higher rank types.
May be but there is also a lot of idealism behind many of the concepts that make the foundation for cryptocurrency
Actually, [I have something for that](https://github.com/dalaing/reflex-host-examples) - although I've been distracted by other things for the last few months so I'm not even sure if it still compiles against the master branch of `reflex`.
Not directly, although we've had some discussions over the years.
Double the newsletters, double the awesome!
It's not elegant, but it seems to work: foo :: Num a =&gt; [a] -&gt; Bool foo xs = foo' $ filter (/= 0) xs where foo' [] = True foo' (1:ys) = and $ map (== 1) ys foo' (-1:ys) = and $ map (== -1) ys foo' _ = False
In St. Louis, MO the [Strange Loop](http://www.thestrangeloop.com/) conference is a good one. It is on for September 28-30 this year.
I say who cares. The other day I was studying for an exam and I was searching for the answer to a certain question. All results had some comment like "seems like homework", and I couldn't find a good answer. Instead of assuming that people are lazy, assume they have good intentions. If a programmer gets through school using nothing but answers they find online they will will probably regret it down the line anyway.
I just got burnt by this too :) https://github.com/commercialhaskell/stack/issues/3062#issuecomment-287177739
Frankly many Haskell devs could use a hand with these two
I agree that asking for help is legitimate. But then phrase the question to make that clear. The approach that [cdsmith](https://www.reddit.com/user/cdsmith) took in answering was geared toward facilitating learning, rather than just giving an answer.
Also the "import" keyword should be lowercased :)
With plenty of comments, to boot! Nicely done.
`and . map p === all p`
Damn, that didn't work [when I tried it](https://www.reddit.com/r/haskell/comments/5xpb1f/is_it_much_harder_to_find_haskell_developers_than/deju6vq/).
In general, when hunting for an error, try and produce the smallest file that still causes the same error. (Version control is your friend here: make a throw-away commit, and then hack away at your file to remove almost everything.)
I'd been looking for some examples for a long time. Thanks!
You can get `runConcurrently` by relating `Free (Ap g)` with something that it is isomorphic to. newtype ConcurrentFree f a = ConcurrentFree { runConcurrentFree :: Free (Ap f) a } deriving Functor instance Applicative (ConcurrentFree f) where -- Do the law breaking instance here. -- Because this type will not have a monad instance, there will be no law to break! instance Concurrently (ConcurrentFree f) (Free (Ap f)) where runSequentially = runConcurrentFree runConcurrently = ConcurrentFree
Does killing your program even with `-xc` not print anything? Compiling with `-prof -fprof-auto` should make your program maintain such a stack which should be displayed with `-xc` whenever an exception is raised.
Ah, I now see that the `Free (Ap f)` definition comes from your post, not the OP. I thought I had understood your post, but clearly I should read it again.
Sadly no, just that the process has been terminated (sending SIGTERM) But I can see all the exceptions happening while the process is not stuck
Not necessarily: I think that there are several possible choices. Yet the example makes most sense with a special type for linear streaming.
Sorry to let you down. I'm actually writing real honest to goodness ghc now, so I musta missed it :-)
That's how sorry he is....!
Haha, the story here is pretty funny actually, "reflex" was a name I was considering for the project, then I found that it existed already and changed the name ; I didn't ever use reflex code in Eve :)
I originally tried to use Yampa for my text editor, but I personally didn't find FRP to be intuitive for something discrete like a text editor, I'm sure others could figure it out, but I wanted something that fit my mental model well, I'm sure that others think like me too, even though it may be "less functional". The FRP libs are great, but they're not the best solution to every problem, in particular I couldn't imagine how to build the extension system which eve supports into existing FRP systems, it's probably possible, but I'm happy with the modularity I achieved.
Ok, I have re-read your post, and I don't understand how you can have a "Concurrent, law-breaking `Applicative` instance" for `Free`. Since it's a Free Monad, the result of `ff &lt;*&gt; fx` is a piece of data describing a sequential computation, not an IO computation which can spawn some threads. So while you might be able to write a "Concurrent, law-breaking `Applicative` instance" which constructs this piece of data in parallel, that won't cause the computation represented by this piece of data to run in parallel. To do that, you'd need `Free` to have an [extra constructor](https://www.stackage.org/haddock/lts-8.2/haxl-0.5.0.0/src/Haxl.Core.Monad.html#line-202) representing parallel computations, and then the interpreter for this Free computation would have to recognize this constructor and run its arguments in parallel. Woudn't it?
Nope. Take a look at [these](https://elvishjerricco.github.io/2016/04/08/applicative-effects-in-free-monads.html) two [posts](https://elvishjerricco.github.io/2016/04/13/more-on-applicative-effects-in-free-monads.html). You can just have the law breaking instance use an underlying Applicative, and later use that Applicative to figure out the concurrency. Note that I didn't acknowledge the law breakage of the Applicative instance in the first post. This is addressed in the second one.
Although that is the standard implementation, I wanted something without data or newtype (ADT's).
Little known fact: If you are using a modern version of GHC (preferably 8.2 which has some improvements in this area), have compiled your code with `-g`, and GHC has `libdw` support for backtraces, you can send a `SIGUSR2` signal to your application at runtime to have it dump a trace on `stderr`. [See the code here](https://github.com/ghc/ghc/blob/428e152be6bb0fd3867e41cee82a6d5968a11a26/rts/posix/Signals.c#L533). This does exist in 8.0.x as well. This is all still rough though.
Thank you!
I meant compiled programs, like more funding into research on understanding lifetimes and origination of space leaks and maybe more knowledge on the upcoming linear types extension being worked on. I'd imagine compiler speed would be better too though...
How does this differ from the standard implementation of a LinkedList in Haskell?
&gt; I'm not sure a whole IDE is valuable but that's probably just me Such a thing is not to be underrated. Time and again I come back for some project/use-case to an ecosystem/environment driven by a "powerful corporate IDE" (OK let's just say VS or JetBrains are the only winners in the space really) and every single time this so much more evolved experience (compared to my up-to-now "default"/"home" editor Sublime with its (dis)array of semi-slapdash hit-or-miss pythonic plugins some for v2 some for v3) blows me away. In fairness, most of the long-time vim/emacs users probably have so many extensions and config tweaks present that they half-unwittingly plumbed together their own haphazard IDE experience that delivers the core benefits just-as-well. Ie if you see the Intero-in-Emacs gifs, that's pretty close to "a whole IDE" in terms of what actually helps productivity (semantic not lexical autocomplete, symbol info/signature, go-to-definition/type/implementation-of-symbol, find-references-to-symbol, insert signatures/imports, remove unuseds etc etc) --- it all adds up over the course of the day when it's all at your fingertips so you can keep track of your current thought without getting distracted by inanities All the tools for a great Haskell IDE experience exist on the command-line. It's "just" a matter of writing robust, solid, all-bases-covered editor plugins that fall-back-gracefully when individual sub-tools go missing or won't install locally. Which actually presents numerous challenges for ponderation once you get pumped on the idea. Use an existing "Haskell IDE middleware" tool? Or library? Which one? Or roll your own. Just bundle up whatever popular *individual* tooling is there (for just-lint, just-check etc)? Oh this one looks most feature-complete. Oops abandoned 2 years ago, won't build with 8.0.2. Etc etc...
I think it would be cool to see that and how they compare!
Imagine that `(==)`, instead of returning `False`, threw an exception when they 'failed to equate two arguments'. That's not what I want. What about the map?
Whoops, just realized I'm logged into my old account haha
In the right fold example, a "list" has type `(a -&gt; b -&gt; b) -&gt; b -&gt; b`; it's simply a higher-order function. The "list" isn't tangible; it doesn't support pattern matching. Instead, it takes a combinator represented by `(a -&gt; b -&gt; b)`, and additionally takes the recursive base case `b`, and yields a single result. The responsibility of the "list" is to receive the combinator and apply it. For example, the list `[1, 2, 3]` would be written as `\c n -&gt; c 1 (c 2 (c 3 n))`. If you provide `(+)` as the combinator, and `0` as the recursive base case `n`, your result will be the sum. Ultimately with Church Encoding, you may only extract information from the "list/function" by applying it. EDIT: My initial explanation was incoherent.
Scalacheck?
HotSpot has multiple pluggable GC strategies as well that fit different workloads. GHC has one GC.
so someone on the irc showed me his version of a church encoded list. (apparently using pairs was not the ideal which i got the idea from misreading the wiki probably). but I understand his version but very confused on how to get the tail. http://lpaste.net/353663 
IIRC someone claimed a while back that the garbage collector would be an obvious target - it's good, but it's not competitive with the kinds of collectors in Java etc mainly due to the lack of massive-scale investments. AFAICT (and certainly it's commonly claimed), for code generation GHC is already at least state-of-the-art and arguably ahead of the curve for optimization already. After all, one back-end is LLVM - the same one used by Clang. But there's extra optimizations - an extra layer of back-end (core), including optimization. In particular, there's strictness analysis - mainstream compilers don't have that because mainstream languages aren't lazy. I'm still right on the fence on laziness because of &lt;insert tediously predictable but still valid reasons here&gt;, but taking the high-level view, strictness analysis allows a class of abstraction overheads to be mostly eliminated for a powerful abstraction mechanism that most languages don't even provide. 
I'd imagine the IDE tooling would be seriously impressive. Some features off the top of my head: - Quick abstraction. You highlight a bit of code, press a hotkey, type in a function name and it splits out into its own function. - An inliner. Similar to the above, it could replace a call to a function with an inlined version if you wanted to get rid of some abstraction with the option to inline it via a lambda. - Some 'quality of life' things like converting between let and where wish the push of a hotkey. - Automatic converter to pointfree similar to http://pointfree.io - Autocomplete that takes types into account. - Function search of libraries integrated via hoogle. - type 'coercer' such as you type in a function and it needs a text, but you gave it a string, it could add the corresponding conversion function. - Convert between sugared and non sugared code, such as do syntax. 
Don't worry, `tail` is understandably difficult using the right fold method. :) Without changing the core signature of a list from `(a -&gt; b -&gt; b) -&gt; b -&gt; b`, the `tail` function must be `O(n)`. I'll translate the right fold `tail` function listed on the Wikipedia article to Haskell and describe it. let tail l = \c n -&gt; l (\h t -&gt; \g -&gt; g h (t c)) (\t -&gt; n) (\h t -&gt; t) -- ^^^^^^^^^^^^^^^^^^^^^^^^^ ^^^^^^^^^ ^^^^^^^^^^^ -- | the "combinator" | | base | | extract | -- | | | case | | tail | Within the combinator, I explicitly separated the bindings for `h` and `t` from `g` for clarity. The list `l` is provided a combinator that ***itself*** yields a function over `g`. *We have higher-order functions within higher-order functions; I'm having a tough time keeping this straight.* The combinator is applied "bottom up", so the parameter `t` within the combinator is a resulting function over `g`. This is why the complexity cost for `tail` is `O(n)`; it builds the tail "bottom up". The resulting application of `l` is the top-level function over `g`. This is why you see three arguments provided to `l`, when lists usually have only the combinator and the base case. This third argument, `(\h t -&gt; t)`, is substituted for `g` in the top-level result of the combinator, extracting out the tail. Additionally, if you'd like this function to fail on empty lists, you should replace `(\t -&gt; n)` with `(\t -&gt; error "empty list")`. EDIT: Reddit doesn't support Github's triple back-tick formatting.
lmao... when you noobs pick a fucking string type then you can dream about massive adoption... smh #cantmakethisshitup
#{-# LANGUAGE OverloadedStrings #-} edit: that was meant to be a hashtag, didn't realize it bolded
Gabriel has a nice talk on beautiful folds: https://www.youtube.com/watch?v=6a5Ti0r8Q2s This goes into a bit more depth on using monoids for folds.
Uhm...Scala with Scalaz?
When you noobs realize that different string types mean different things and have different performance characteristics.... Even c++ has char*, wchar*, std::string, and std::wstring, and many other different implementations with different characteristics. Advanced languages have lots of options....
[Char] is pretty bad. It's a more convenient representation of single characters, sometimes, in some scenarios, but largely it's just useless. Text is what you want 90% of the time if you are dealing with significant amounts of data. Bytestring is just a convenient way to think about raw data. It is generally not a string. You can use it as a string if you really need to, for some reason, if you know how encoding works and you know for sure you're never going to need multibyte chars. Forget String exists, use -XOverloadedStrings, and use the Bytestring and Text libs for what they are designed to do, and you shouldn't have any real problems.
I think Haskell's *current* IDE situation is already better than most "popular" languages. Sure, it doesn't have a lot of the bells and whistles, but when it comes to actually writing code that is works, Haskell is up there. I've used Emacs, Atom, and now I use VS Code with Haskero (Intero underneath). It works with very little fuss. That said, with wide adoption think we'd start to see a more standardized language (as is the plan for Haskell 2020) and more competition for compiler. I would expect GHCJS to become mature much faster and for companies to start adopting certain features to push them through (this is already happening with GHCJS and GHC is getting some commercial push [c.f. recent discussions about linear types]). This would probably (hopefully) include JVM and CLR backends. The story for mobile would develop much more quickly as well. Last, and perhaps most important of all, Haskell would lose its stigma of being impractical and academic only. Monads would be prized and treasured by all. Instead of mocking and dismissing, the programming culture at large would be eager to hear and learn about new concepts like Profunctor and Free Applicatives. And with the added knowledge, more code would be reusable and less code would be written along with the bugs that go with it. And programmers at large would be able to work their way into even tougher problems that most have never dreamed to solve with the ways of thinking that are common today.
So, to summarise, it doesn't support pattern matching? It seems to me that this is a mere syntactic difference and the implementation will end up being extremely similar.
I suspect strongly that the perception of difficulty and the general lack of utilitarian modules is really what would / needs to change. Haskell doesn't yet have the 'critical mass' of adoption that helps keep docs and libs up to date, and the constantly moving bleeding edge of abstraction gets much more focused attention than the day to day​ workhorse stuff. This is historically what makes or breaks a language as a tool to get work done. Nobody uses PHP because it's a good language. It's a pile of garbage. People use PHP because it's a giant pile of garbage that has a wide selection of support for simple usecases. Since 90% of what you spend your time on professionally is the simple crap, this is what "matters" the most for getting apps built. I think Haskell definitely has the potential to hit that critical mass sometime in the near-ish future. Industy adoption seems to be growing steadily, and as more people start learning the basics, more beginner​ blog posts etc. spring up.
In some instances, you're correct. However, if you use Church Encoded lists, you're unable to (quickly) extract it's tail. I've explained how it can be done in `O(n)` time elsewhere in this thread, but that simply isn't acceptable outside of frivolous examples. :/
Minus the point free converter, all of these features are standard in Java IDEs (and by extension Scala). Curious what other things Haskell's type system would allow for that isn't possible or easy in other mainstream industry wide languages.
Here's an equivalent one-pass solution using my `foldl` library: {-# LANGUAGE ApplicativeDo #-} import Control.Foldl (Fold) import qualified Control.Foldl example :: (Eq a, Num a) =&gt; Fold a Bool example = do a &lt;- Control.Foldl.all (\x -&gt; x == 0 || x == 1) b &lt;- Control.Foldl.all (\x -&gt; x == 0 || x == -1) return (a || b) main :: IO () main = print (Control.Foldl.fold example [0,1,0,1])
&gt; I'm not sure a whole IDE is valuable but that's probably just me I mind it a lot less just because Haskell has less boilerplate than I'd write in e.g. Java. 
It's more than that - they build an IDE for their specific use cases, which lets them implement features that would be far too niche in a mass-market IDE.
Nope, I expect you were downvoted for the needlessly inflammatory manner in which you expressed yourself. A number of responses even agreed with your point. Why not try an experiment: try being more civil, and see if your valid points continue to be downvoted. Just a thought...
Just so you know, there's a subreddit devoted to that book and any questions you have on it: [r/haskellbook](http://reddit.com/r/haskellbook). If you want to loop, take a look at the Control.Monad.forever function. Might be what you are looking for.
That sounds like it could easily be gamed by spamming hackage. A more robust approach would be to simply require the package to be specified explicitly if a collision was detected, and use the automatic detection if one isn't. That way any ambiguity is resolved explicitly, which is a good thing IMO.
I replaced **p &lt;- liftIO getChar** with **p &lt;- liftIO getLine** and made a few other minor changes to allow for the fact that p is now a String rather than a Char. Now it works. Seems that it has something to do with Windows as it works using getChar on linux. This is the final code: module Morra where import Control.Monad.Trans.State.Lazy import Control.Monad.IO.Class import Data.Char (isDigit, digitToInt) import System.Random (randomRIO) import Control.Monad (when) morra :: StateT (Int, Int) IO () morra = do p &lt;- liftIO getLine let p1 = head p when (isDigit p1) $ do let p' = digitToInt p1 c &lt;- liftIO $ randomRIO (1, 2) liftIO $ putStrLn ("P: " ++ p) liftIO $ putStrLn ("C: " ++ show c) (pt, ct) &lt;- get if even (c + p') then do liftIO $ putStrLn "Computer Wins" put (pt, ct + 1) else do liftIO $ putStrLn "Player Wins" put (pt + 1, ct) morra main :: IO () main = do putStrLn "-- p is Player" putStrLn "-- c is Computer" putStrLn "-- Player is odds, Computer is evens." (personS,compS) &lt;- execStateT morra (0,0) putStrLn ("Person Score: " ++ show personS) putStrLn ("Computer Score: " ++ show compS) if personS == compS then putStrLn "No Winner" else if personS &gt; compS then putStrLn "Winner is Person" else putStrLn "Winner is Computer" 
You are right. That's a better approach. Particularly because the number of collisions will be very small (I'm personally not aware of any). Alternatively: if no collisions currently exist in any Stackage snapshots, then just make it a rule that for a new package to be accepted, it must only export not-already-taken module names. I think that's a sensible rule as a library developer anyway: don't release something with an already-taken module name.
&gt;IIRC someone claimed a while back that the garbage collector would be an obvious target - it's good, but it's not competitive with the kinds of collectors in Java etc mainly due to the lack of massive-scale investments. If that is indeed the case, then that's a massive shame. Immutability and value semantics should make garbage collection much easier, no?
Great points and totally agree. Nit: &gt; Nobody uses PHP because it's a good language. It's a pile of garbage. People use PHP because it's a giant pile of garbage that has a wide selection of support for simple usecases. We might use Node or Python as modern examples here. PHP *today* is used out of habit or to avoid migrating legacy codebases. And back in its hey-day, when Java was in println(helloWorld) stage, the alternatives where ASP (pricy MS licenses on the server? yuck), ColdFusion (similar), or Perl/CGI (cryptic). And by using PHP we're missing the fact that other rapid-mass-adoption ecosystems such as Ruby Python Node all started out with *very little* in the way of "selection of support for simple usecases", but somehow implicitly presented a super-appealing mutually-encouraging environment for tinkerers and community contributions to not only quickly flourish and iterate but also to be found out about by everyone instead of lingering on some lone forgotten repo, or "paper". And most of it, sure, "just C-bindings" to powerful libraries and OS features, each of which made "our shiny trendy beloved scripting language" just yet-again substantially more powerful and "expressive" (perceivedly), every bit as what made PHP so appealing back then (data access, sockets access, image processing, xml sax/dom, you name it, the whole 9 yards). Because as you say, most of the daily to-dos are about wiring up ever more plumbing between different existing external components and your tiny bit of domain logic, which frankly can usually be expressed quickly and easily in any sort of language because most of the time is anyway spent fighting the plumbing (data format quirks, compatibility-breaking version changes, migrating your plumbing code from old lib to new lib, old db to new db, sockets to streams or vice-versa, xml to json, etc etc you name it --- *never* about "refining our custom eDSL to allow for a more abstract/generalized/type-safe/type-level expression of our domain models" and such stuff --- a common FP assumption is that this is because their languages don't *enable* them to, but that's just an assumption and possibility, not the only logically-possible explanation)
Operationally, list elements are probably stored in closures instead of product constructors. 
I really don't know. On one level, immutable data means immutable pointers which presumably means it's easier to determine what is garbage, even with the collector running concurrently with the program. But the trouble is there's a strict evaluation order assumption in there - the implementation of laziness has "thunks" which include mutable pointers to structures representing unevaluated expressions (with closures etc) and to result values. Those mutable pointers only mutate in very limited ways, though, so maybe that's enough to make the collectors job easier and more efficient - I don't know. But don't forget - Haskell has mutable variables as part of `IO`, `ST`, `STM` and probably more, and they're garbage collected too. Also, Haskell has mutable variables that aren't garbage collected as part of the C foreign function interface (which can be used even without interfacing to any C code). But some of those C data structures can reference Haskell data structures, so I assume that means there's the potential for lots of GC root nodes. I believe the GHC collector is better optimized for immutables than mutables. I have no idea how using the FFI for non-GC C-style data structures that reference Haskell data structures impacts on the costs. One concern is that even if the GHC collector had perfect efficiency for immutables, functional programs tend to "allocate like crazy" - a bigger graph to traverse is always going to cost more. There's both language features and optimizer features to control that, but again, I'm not really the person to ask about that. 
/u/edwardkmett or SPJ would stop doing fun stuff and would only write "get off my lawn" blog posts as they morph into Uncle Bob.
Seems to have some sortof lazy evaluation problem :-) 
Why wouldn't it?
Thanks a lot for this gem. I will try to grab ghc 8.2 ! Thanks again !
It would probably be taught as first programming language in most higher education. Which is a big deal given that this spot is now mostly taken by Java, Python, C#.
 I think /u/ocharles beat you to the Foldl version. =) https://www.reddit.com/r/haskell/comments/5zu68a/whatd_be_the_best_way_to_check_if_a_list_is_all/df1mlur/
That's also quite possible. I am being optimistic.
Agreed. But my point is that having powerful type-checkcing *as you type* is, IMO, 5x more valuable than things like "extract function/variable." I would consider that a "bell" or "whistle." Certainly it's useful, but of a categorically less useful kind than what Haskell's types give you. In fact, you can even "find usage" by creating a type error! :P
`ghci` provides a `:script` command to run any commands within a file as if you had run them within your current `ghci` session. So you could create `script.hs` file whose contents are: var &lt;- ioAction ... and then run `:script script.hs` to re-run the `ioAction` command and store the result as `var` within your current session Is that what you had in mind?
&gt; If I define this action directly on the file, every time I use `var` it will be executed, and it will always have the type `IO whatever`. No, if your `ioAction` has type `IO Int` and you write `var &lt;- ioAction`, the IO action will only be be executed once, when you type `var &lt;- ioAction`, it will not be executed again when you just type `var`: $ cat Hello.hs ioAction :: IO Int ioAction = do putStrLn "IO action is happening!!" return 42 $ ghci Hello.hs λ var &lt;- ioAction IO action is happening!! λ var 42 λ var 42 If you do want the IO action to be executed each time you type `var`, you can simply use `=` instead of `&lt;-`: $ ghci Hello.hs λ var = ioAction λ var IO action is happening!! 42 λ var IO action is happening!! 42 If you need the `ioAction` part to perform an expensive IO computation and the `var` to retrieve the result using a cheaper IO action, another option is to have your IO action return *another* IO action, like this: $ cat Hello.hs import Data.IORef ioAction :: IO (IO Int) ioAction = do putStrLn "expensive IO action is happening!!" ref &lt;- newIORef 42 return $ do putStrLn "cheaper IO action is happening." readIORef ref $ ghci Hello.hs λ var &lt;- ioAction expensive IO action is happening!! λ var cheaper IO action is happening. 42 λ var cheaper IO action is happening. 42 &gt; I would like to just have a command like `reload` (defined by me) that executes the actions and give them the appropriate names. If you define a bunch of those `var`s and you want to re-define them every time you `:reload`, you can define a custom ghci command which will both reload and define your `var`s: $ ghci Hello.hs λ :def myreload \_ -&gt; return ":reload\nvar &lt;- ioAction" λ :myreload expensive IO action is happening!! λ var cheaper IO action is happening. 42 λ :myreload expensive IO action is happening!! λ var cheaper IO action is happening. 42
There would probably be a new std lib without all the stuff people replace most times anyway (no `String`).
There's a couple of ways to accomplish adding back in the laziness, without using a product algebra. In particular note that the domain of your `(&lt;&gt;)` contains invalid, where the domain of my `(&lt;&gt;)` does not contain nothing. Just with that modification we can get back our laziness. &gt; f = isJust . fold alg &gt; where &gt; alg Nil = Just 0 -- Valid, no commit &gt; alg (Cons x y) = toCommit x &lt;&gt; y -- Refine commit &gt; toCommit 0 = Just 0 &gt; toCommit 1 = Just 1 &gt; toCommit (-1) = Just (-1) &gt; toCommit _ = Nothing -- This function makes it strict enough in the head. &gt; Nothing &lt;&gt; _ = Nothing -- This line makes it lazy enough in the tail. &gt; _ &lt;&gt; Nothing = Nothing &gt; (Just 0 ) &lt;&gt; y = y &gt; x &lt;&gt; (Just 0 ) = x &gt; (Just (-1)) &lt;&gt; (Just (-1)) = Just (-1) &gt; (Just 1 ) &lt;&gt; (Just 1 ) = Just 1 &gt; _ &lt;&gt; _ = Nothing
Thanks responded to that directly
I worked on static analysis of Java programs that use frameworks. Absolute nightmare. It's pretty much hopeless in general. However, we had to do this to check whether the program was following API contracts in order to improve the IDE support. Guess what, in Haskell you can design the API in such a way that the compiler will check it for you fully by constructing a proof (GADTS, phantom types, etc. are used for it, linear types in the future). No special tooling needed. And I see this as the major advantage of Haskell: so many things can be done within the language instead of having to resort to external tools/analyzers. In Haskell, the community seems to reject half-ass methods and hence the focus on papers and doing things provably correct in general. Not always possible but that seems to be the direction. So, yes I see great future for Haskell, it has a potential to eliminate many problems that plague the industry today. 
I don't understand what you think Haskell has here in IDE's that other languages don't. Every other language has as-you-type type-checking, error checking, style linting, in IDEs... Haskell just has a much better type system. I've tried Emacs and VSCode for Haskell using ghc-mod and Intero, and honestly, while I really love Haskell, it still absolutely sucks here. If you only write Haskell you won't notice it, but if you're constantly switching from other languages to Haskell, you will feel this pain every day. Type-checking is slow. Emacs + Intero is so sluggish, seriously it feels ten times slower than IntelliJ. And it stops showing type information when there's an error in the file. Writing Javascript or Scala code in IntelliJ, you can have a hundred errors in your file and the IDE will still type-check your code as you type, allow you to go to definitions, etc. Being able to quickly navigate through a code-base, ctrl+clicking on functions to dive into their definitions, even when the definitions are in external libraries, is worth so much.
Does this really need to be advertised here every month?
Sorry if this doesn't completely fit here, but I found /u/jwiegley's talk very useful. Repo: https://github.com/jwiegley/nix-config Event page: https://www.meetup.com/Bay-Area-Nix-NixOS-User-Group/events/237430925/ Originally submitted on /r/NixOS: https://www.reddit.com/r/NixOS/comments/5xo9so/talk_how_i_use_nix_for_haskell_development/
Sure, the type system is stronger, and that's definitely one of Haskell's strengths, but in terms of developer experience, the tooling isn't on par yet. For example, hacking together find usage by causing compilation failures requires changing the state of the entire project, and also won't be exhaustive, since dependencies failing to compile will stop GHC from even attempting to compile other modules. I strongly believe the developer experience is a huge part of a language's ecosystem - it makes it much more attractive to new developers, and can increase the productivity of experienced ones tenfold. I also think we shouldn't be blind to the deficiencies in our tooling, but instead, focus on them so that we can understand how to improve it. e.g. it should be fairly straightforward to extend hoogle to be capable of find usages... (famous last words!)
Oops! *** Exception: /home/mitchell/java-apache-lucene/java-platform/src/Java/Lang/Iterable.hs:24:17: error: • Couldn't match expected type ‘Dict (JReference (JIterable a))’ with actual type ‘Char’ • In the first argument of ‘Sub’, namely ‘'a'’ In the expression: Sub 'a' In an equation for ‘lifting’: lifting = Sub 'a' • Relevant bindings include lifting :: JReference a :- JReference (JIterable a) (bound at /home/mitchell/java-apache-lucene/java-platform/src/Java/Lang/Iterable.hs:24:3) (deferred type error) 
Also, possible autocomplete based on types, like with `djinn`.
Is it maybe set in a global .cabal config or something?
* Records would be way better than their current state * Exception and error handling would be standardised * exhaustive checking and partiality checking would be way better * String/Text situation would be solved. 
I don't personally care for his methods with Nix, but I absolutely love that Nix makes it possible to have such diverse strategies.
Perhaps it's just me, but when all three arguments are supplied I find `f &lt;$&gt; a &lt;*&gt; b` more readable than `liftA2 f a b`, as there are more visual cues abour what is going on. With the function applicative that is specially important -- for instance, with `(&lt;$&gt;)` and `(&lt;*&gt;)` if you are going to apply the resulting function *in situ* you have to add parentheses, in a way that makes it obvious that the applicative expression amounts to a function: (f &lt;$&gt; a &lt;*&gt; b) x With `liftA2`, on the other hand, given that you can drop the parentheses it is tempting to write it like this... liftA2 f a b x ... which will make a larger share of readers do a double take.
I might be able to help diagnose this, but sadly, I can't even build the code you're referring to due to its very difficult-to-install dependencies. Can you minimize the example to a self-contained file (with preferably zero or few dependencies)?
Yes sir, I'm putting together a single file that exhibits the bug. In fact, I've already got it! One moment...
Personally I often get stuck when reading code with operators in them, and it mostly stems from being unable to see how the operands will be grouped. I think probably newbies have the same problem. If people like this, we could probably push to get it added to mainline GHCi.
Lots of people are all jazzed about IDE's and such but there will be some negative as well, eg. I'd guess: * adding nice things like the Linear Types proposal that was posted a few days ago would quite challenging. * closely related, fixing past mistakes (FTP, BBP) will be far more difficult if not impossible. * GHC would fork into "GHC-academia" and "GHC-industry" or academics will just move to Agda or something. If you're in the industry camp this might not sound like a bad thing but IMO it would be a loss. GHC and Haskell has benefitted greatly due to contributions by researchers. You get the idea ...
For our enlightenment, could you summarise what you do differently and why? 
I'm using 8.0.2
Oh it's nothing serious. I just avoid using the global nix package management as much as possible. I confine things to individual projects and pin those projects to specific versions of `nixpkgs` so that I don't have to do any of the things he does with "rebuilding 2000 packages" all the time.
This is cool!!!
Yes please to mainline GHCi. I'd just about offer to do it myself.
&gt; Example [of] bad usage: `(==) &lt;*&gt; f` It is worth mentioning that this can be ungolfed while still using applicative style: `(==) &lt;$&gt; id &lt;*&gt; f`.
No one would write video games. Only parsers now. 
Awesome. I remember [a Stack Overflow question](http://stackoverflow.com/q/40331179/2751851) from a while ago in which the OP was looking for exactly this. 
&gt; Sorry if this doesn't completely fit here It fits wonderfully!
Is there something like the eclipse debugger for haskell? Where you can set breakpoints anywhere and just edit and call any code and have the program run with the changes? I understand that you can use ghci to debug, but I sometimes have a hard time getting the application state into ghci.
Cool stuff! Thanks for going through the effort, I'll definitely take some time to study this! At a first glance it seems a bit tougher to follow than Eve; but that's of course to be expected, (not only since I wrote Eve, but also because Reactive-Banana is more general and abstract). I'll study it a bit more when I have some time (and watch your video), but I'm curious how you'd handle a situation in which the types and quantity of your event [`Sources`](https://github.com/dalaing/eve-rb-compare/blob/master/src/Main7.hs#L26) aren't known, as is the case in extensible systems like `Rasa`; we don't know all of the types of events which people may dispatch or choose to listen for. (Side Note; Just watched your talk on Cofun and Cofree Comonads, cool stuff! I admit I got lost at the section on Pairing, but I'll come back to it later).
The 'killer app' for Haskell may be any app that is critical to its organization. Many years ago I worked for Federal Express just as it was preparing for its initial public offering. As part of 'due diligence' an auditor asked to see a copy of the payroll program; a week later he told them that the payroll program didn't seem to have any sneaky things like roundoff values being added to the programmers check. Then he said: "Prove to me that the program you gave me to examine was used for last weeks payroll!" Fed Ex had to create a configuration management system where programs were passed by the programmer to a database and a different group recompiled the program and put it into production. Timestamps in the system log made it possible to say "Last weeks payroll was created by a program compiled on &lt;timestamp&gt; using a compiler created on &lt;timestamp&gt;. The time is coming when an auditor, commissioned by the board of directors (who have a fiduciary responsibility for the integrity of their companies data), will command: "Prove to me that the &lt;latest computer virus&gt; can not happen here!" It is possible that in future 'due diligence' will demand tightly typed code proved correct by the compiler. Haskell/GHC is 90% there. Edit: formating, wording
You've got a few options there. I've used classy lenses before when I was trying to limit the scope of some code to just using the sources it needs to, but I'm not sure that will work for you. You can do things like your status system - just have components expose a status event. I think I got most of the way to an equivalent to that system in final part. With this kind of frp you can also do a lot of dynamic event graph modification - including creating new sources and spinning up event loops for them - in response to various events. Part of that functionality allows you to work with events carrying other events or behaviors. Between that and the applicative for behaviors, you have a lot of scope to do fancy things in-system. 
I dislike unnecessary special cases on principle 
You're welcome. There's possibly an alternative implementation of lists with `O(1)` tail, I'm simply not aware of it.
What about regular functions in the expression? Do they get annotated with their library too? That could also be handy.
I just remembered [this](https://github.com/HeinrichApfelmus/threepenny-gui/blob/master/doc/design-widgets.md), which might help when it comes to thinking about API design with events and behaviors. It can also be worth mining reflex-dom for examples / ideas / idioms.
Yes, some standard types and classes use special syntax. List brackets and commas and quotation marks around strings are among these. Other special syntax includes: single quotes around character literals, numeric literals (that privilege the `Num`, `Fractional`, and `Eq` classes), `do` notation (that privileges the `Monad` class), guard syntax and `if/then/else` (for the `Bool` type), list comprehension syntax (which has special support for lists and `Bool`), and tuple notation (for built-in tuple types). The `deriving` mechanism also privileges standard classes and certain others known to the compiler. GHC offers several language extensions that relax some of these to one extent or another, but there are limits.
We're working on getting some of those features into HaskellDO actually. I like the idea of having code transformations (do notation, pointfree converter) utilized by an IDE because it could help someone learn about those things.
Some things have special syntax, but very few things aren't semantically re-implementable. Lists and tuples, for example. We aren't allowed to use this syntax, but this is how it would look if we were: data [a] = [] | a:[a] data (a, b) = (a, b) foo :: [a] -&gt; Maybe (a, [a]) foo [] = Nothing foo (a:as) = Just (a, as) We *can* however, write this: data List a = Nil | Cons a (List a) data Tuple a b = Tuple a b foo :: List a -&gt; Maybe (Tuple a (List a)) foo Nil = Nothing foo (Cons a as) = Just (Tuple a as) It's not just *basically* equivalent. It's *exactly* equivalent, except for the magical syntax. Strings are another example of magical syntax with reproducible functionality. Somewhere in `Prelude` is this type synonym: type String = [Char] Building on our previous example, we can reproduce this with this: type String = List Char The only problem is that we can't use `"hello, world"` with this type (and of course we can't use `[1, 2, 3]` with our `List` type). The quotes and list brackets are a magical syntax that desugar to normal Haskell. Another example of syntax being translated to ordinary Haskell is numeric literals. The difference is that we *can* overload the implementation of numeric literals, using the `Num` and `Fractional` classes. These classes provide methods that get called by Haskell when you use numeric literals. class Num a where ... fromIntegral :: Integer -&gt; a class Fractional a where ... fromRational :: Rational -&gt; a foo :: Num a =&gt; a foo = 3 -- Equivalent to 'fromIntegral 3' bar :: Fractional a =&gt; a bar = 3.14 -- Equivalent to 'fromRational 3.14' Finally, strings actually *do* let us overload the magic syntax, if we enable a language extension. {-# LANGUAGE OverloadedStrings #-} import Data.String data MyString = MyString (List Char) instance IsString MyString where fromString str = MyString (foldr Cons Nil str) Similarly, the `OverloadedLists` extension allows you do overload the list syntax, but it's more complicated so I won't go over it here.
Thanks!
Thanks!
 &gt; - Autocomplete that takes types into account. This will soon be easy to implement in IDEs: https://www.reddit.com/r/haskell/comments/606uj6/show_valid_substitutions_for_typed_holes/
Would be great to have an option not too show qualified names - they add quite a bit of noise if you are just trying to see preciedence/associativity. EDIT: Looking at `Ppr` I can't see a way.
Non-Mobile link: https://en.wikipedia.org/wiki/Doner_kebab *** ^HelperBot ^v1.1 ^/r/HelperBot_ ^I ^am ^a ^bot. ^Please ^message ^/u/swim1929 ^with ^any ^feedback ^and/or ^hate. ^Counter: ^45308
This is a cute idea. The only downside is that it becomes less clear by direct inspection what the type of the `case` statement is supposed to be. It's not unprecedented; we already have something similar for pattern matching in function definitions. But to flesh out this proposal, you would need to specify the exact algorithm for type checking.
Not really well versed in this, but a really early paper I know of is [Can Programming Be Liberated From The Von Neumann Style?](http://www.thocp.net/biographies/papers/backus_turingaward_lecture.pdf) 
Just watched this talk. My understanding is that he advocates for baking the Applicative effects directly in the `Free` datatype (or, better yet, build your own `Free` through coproduct) rather than nesting them as `Free (Ap f)`. That would simplify my question into: interpret :: (PairingM f g m) =&gt; (a -&gt; b -&gt; m r) -&gt; Cofree f a -&gt; FreeT g m b -&gt; m r interpret p eval program = do a &lt;- runFreeT program case a of Pure x -&gt; p (extract eval) x Free gs -&gt; pairM (interpret p) (unwrap eval) gs Parallel f -&gt; _ -- ?
while `($)` is usually considered to be simply implemented as `f $ x = f x`, there is some special handling for it in GHC to allow writing `runST $ do ...`. I don't know about the exact details though :/
~~I don't think $ has special handling. It just has very low precedence. You could replace the $ with parentheses like this:~~ ~~(runST) (do ...)~~ I stand corrected.
Sounds very cool. What backends are supported? Browser? Native UI on one or more platforms? Mobile devices?
No, [it has special hacks to make it work](http://stackoverflow.com/a/9469782/1651941). Also, it's type is not true as shown in the REPL. There was a discussion about that when Levity polymorphism got into GHC 8.
See here: http://www.mail-archive.com/glasgow-haskell-users@haskell.org/msg18923.html &gt; However, people so often write runST $ do ... that in GHC 7 I implemented a special typing rule, just for infix uses of ($). Just think of (f $ x) as a new syntactic form, with the obvious typing rule, and away you go. &gt; It's very ad hoc, because it's absolutely specific to ($), and I'll take it out if you all hate it, but it's in GHC 7 for now.
"non-IO built-in types can still be special in two ways: syntax and performance." This part helped with understanding all the other replies a lot. Thanks man! Edit: Can you provide some examples of FFI types?
Interesting idea, could see adding this as a `case` variant (as with `LambdaCase`) and I definitely have wanted to eta-reduce a clause (usually not *all* of the clauses, especially in a big case expression). Since patterns must be fully saturated I suppose it could be mixed (not saying this is a good idea): either :: (a -&gt; b) -&gt; (a' -&gt; b) -&gt; (Either a a' -&gt; b) either left right = \case Left a -&gt; left a Right b -&gt; right b either :: (a -&gt; b) -&gt; (a' -&gt; b) -&gt; (Either a a' -&gt; b) either left right = \case Left -&gt; left Right b -&gt; right b either :: (a -&gt; b) -&gt; (a' -&gt; b) -&gt; (Either a a' -&gt; b) either left right = \case Left -&gt; left Right -&gt; right I would like to see more use cases 
This is very close to what I was looking for - kudos! Now, for example, where was the Maybe or Either type first introduced for handling with errors?
I've only tested it on the browser, but since it uses React, it's very likely that it'll be able to be run on mobile devices using react-native. This is something I'm wanting to try to do, but it'll be great if someone from the community can help me with this :)
some musings, this * Could allow writing `\(a, s') -&gt; (f a, s')` as \((,) a) -&gt; (f a, ) \(a, ) -&gt; (f a, ) \(,) -&gt; (,) . f * `runState (StateT state) = runIdentity . state` becomes `runState StateT = (runIdentity .)` * `mapStateT f m = StateT $ f . runStateT m` and `withStateT f m = StateT $ runStateT m . f` get a nice symmetry mapStateT f StateT = StateT . (f .) withStateT f StateT = StateT . (. f) There could be interesting uses (and problems) for functions of multiple arguments. * `widthAlg` from Gibbon's [Folding Domain-Specific Languages: Deep and Shallow Embeddings](http://www.cs.ox.ac.uk/jeremy.gibbons/publications/embedding.pdf) can be written widthAlg = \case IdentityF -&gt; id FanF -&gt; id AboveF -&gt; const BesideF -&gt; (+) StretchF -&gt; const . sum * Simple expression language data Exp = Val Int | Add Exp Exp | Mul Exp Exp | Div Exp Exp incr :: Exp -&gt; Exp incr = \case Val -&gt; succ Add -&gt; on Add incr Mul -&gt; on Mul incr Div -&gt; on Div incr
To start with, look at the `Storable` class, in `Foreign.Storable`, and the function alloca :: Storable a =&gt; (Ptr a -&gt; IO b) -&gt; IO b in `Foreign.Marshal.Alloc`. The docs say: "alloca f executes the computation f, passing as argument a pointer to a temporarily allocated block of memory sufficient to hold values of type a. The memory is freed when f terminates (either normally or via an exception), so the pointer passed to f must not be used after this." For the full details, look through the modules in the [base](http://hackage.haskell.org/package/base) library under `Foreign`.
Huh? `B` may not have an inverse.
I'd rather make it unnecessary to use `($)` just to apply a function to a `do` block.
It makes me sad to see Haskell was abandoned for a project because of the GC. Would it be possible to have several GC strategies built into Haskell, and then chosen via a flag? I seem to remember that Java has a few different GC strategies*. I understand this would be a mountain of work, but if this is something industry players want then maybe it's something that could be supported by industry. \* Edit: search "GC" at http://jvm-options.tech.xebia.fr/ to see JVM GC options.
&gt;Strict evaluation is typically better for performance than lazy evaluation (thunks cause allocation, so you’re gambling that the computation saved offsets the memory cost), but it does make things less composable. There have been a couple of times where I’ve gone to split up a function, only to realise that doing so would require allocating a data structure in memory which before was not needed. As someone who's never really grasped the composability benefits of laziness, is there a *Tao of Laziness* blog post somewhere that explains this? Ideally with at least a couple non-trivial examples.
On the other hand, Pusher has somewhat specialized requirements. On their front page they call out realtime operation twice: https://pusher.com
Can we just get argument do pls
I personally find it really hard to read : there is far too much magic going on. As usual (with the type checker : computer or human) everything is fine when b where is no error. How does the compiler knows that you have intentionally forgot the argument of B ? What will happen when you add an extra arguments to B, should the compiler warns you that constraint has too many arguments or complains that B hasn't enough? We already can do half of it using the wildcard extension. Had you given name to B fields (let's say x and y) you could do B {..} -&gt; const x y The use of this extension is already prone to lots of problems but at least it is clear that we are not caring about the arguments. Maybe something similar could be done with non record where it will mean : take or use the arguments in order so you could write something like B {*} -&gt; const {*} 
Inspired by /u/cartazio's copatterns, this probably doesn't do what I want but I'll revisit it type Op = Exp -&gt; Exp -&gt; Exp data ExpTag a where ValTag :: Int -&gt; ExpTag Int AddTag :: ExpTag Op MulTag :: ExpTag Op DivTag :: ExpTag Op data Exp where Exp :: (forall xx. ExpTag xx -&gt; xx) -&gt; Exp pattern MkExp :: Op -&gt; Op -&gt; Op -&gt; Exp pattern MkExp {add, mul, div} &lt;- (getOps -&gt; (add, mul, div)) where MkExp add mul div = Exp $ \case ValTag i -&gt; i AddTag -&gt; add MulTag -&gt; mul DivTag -&gt; div incr :: Exp -&gt; Exp incr MkExp{..} = Exp $ \case ValTag i -&gt; 1 + i AddTag -&gt; add `on` incr MulTag -&gt; mul `on` incr DivTag -&gt; div `on` incr
It's a little weird seeing a blog post I wrote months ago suddenly appear on a bunch of sites, I wonder what the trigger was.
That's what [Why Functional Programming Matters][why] is mostly about. [why]: https://www.cs.kent.ac.uk/people/staff/dat/miranda/whyfp90.pdf
Could you provide some information why this approach is better than the existing solutions? I haven't really looked into it, but at first glance it looks a bit over-engineered. Comparing line counts of the TodoMVC: - Reflex: 235 - Typescript + React: 558 - Glazier: 1061 Of course, that alone tells nothing about scalability, performance or type safety, but good abstractions usually make code shorter not longer, and so you should at least provide a tutorial explaining why it is useful and necessary to be that verbose.
Sure, in fact that's how you go from an interface to a concrete type: you switch on the actual type. I don't find open sums as useful as closed sums, though.
I am not entirely sure but isn't this basically the exact use case linear types are proposed for? Messages can be safely freed once they are popped from the queue since they won't be shared so you can just store them on a non gc heap.
The RealWorldOCaml cert just expired. The page no longer loads on Chrome.
Thanks for the post :) &gt; There have been a couple of times where I’ve gone to split up a function, only to realise that doing so would require allocating a data structure in memory which before was not needed. Could you please elaborate with an example? Where do you need *more* intermediate allocations than in Haskell when you split a function in two?
This syntax should use a symbol other than the arrow because that is consistently used when introducing new symbols into the environment. Perhaps borrowing then from if would be more appropriate.
Even the 'coercer'? I have been working on such a system, and it can be pretty complicated. In fact the system I originally envisioned turned out to run into problems with the Church-Turing thesis.
When is that expected to land?
I don't think so. However, if you write good tests, using quickcheck, it is trivial to get your app state to debug. Quickcheck gives you the app state that fails. So you don't need to have all your app to debug it. Since haskell is purr, you just need the piece of data that is failing. Instead of doing conventional debug, the best thing is to write/generate test cases that fail for your bug. Writing good tests requires you to write testable code and this is hard, really hard. But it is a skill that carries you for life. 
My problem is, afterall, `Just` is a value, therefore I understand `case x of ; Just -&gt; g` as : if `x` is equal to `Just` then return `g`. Were this extension available I'll be able to write case Just of Just -&gt; someExpression This will be syntacticly valid but wouldn't type check because the `Just` is of type `a -&gt; Maybe a`, but the second one of type `Maybe a`. This feel a bit inconsistent to me.
Hi Anfelor, thank you for your comment. You bring up a good point. You might be right that TodoMVC is over engineered, it's my testing ground to explore new ideas. Unfortunately, Glazier.React apps will probably never win in the line count department, for the following reasons: * I use about 90 chars/line in my editor * I like to use one line per data constructor. * I like to import every module explicitly, instead of using reexported modules * 8 lines of GHC extensions per file * Every widget exports the same set of 15 names, to provide a consistent way to use them. This can be cut down by just exporting a record of functions; at the moment I'm doing both. In total, there is about 50+ lines of extensions and imports * There is about 20 lines of common boilerplate (generic instances, TH, type synomyns) per widget. Finally, because each widget is modularized, some application gadget structure is repeated. For example, the Todo.App.gadget "overrides" the generic "List of &lt;Todo&gt;" gadget ToggleCompletedAction by additionally forcing a re-render for the list (in order to cull the list due to the selected filter). This will always be more verbose than moving all the widget code into the one file. But the point of Glazier is to be able to build larger widgets out of smaller widgets. However, you bring up a good point - maybe there is something I can do to reduce the amount of lines and noise. As for advantages: * I think Glazier is simpler to understand as it's just "WindowT == ReaderT" and "GadgetT == ReaderT + StateT", and can be modifed using standard Lens's magnify and zoom. * Smaller IO surface area. All IO is either in event handlers (where you should only use IO to read javascript values, and dispatch Actions), and interpreters of the Command result of Gadget processing (where arbitrarty IO is allowed). * The binding to React is very simple. The same React.PureComponent is used throughout the app; only ref, render, and setState and componentDidUpdate react methods are used. * The bindings to React is very efficient: Only the react components that have changed are re-rendered; and the state exposed to React is just a sequence number (so it's fast to compare). 
On that theme, I would have thought that Rust would be a better choice than Go for realtime - especially given that Pusher has a Haskell background. I'm not really familiar with linear types; but my vague understanding is that Rust's lifetime system is related, at least in purpose. Is there support for some sort of delimited linear-type code in Haskell programs? Edit: removed edit that was supposed to go on a different comment.
In the interest of fairness, maybe one could simulate lazy evaluation for a case like this using channels according to patterns like the ones described here: https://appliedgo.net/flow/ On the other hand, a channel-based implementation would be more complicated than what could be done with simple list manipulation in Haskell. Edit: using channels would also mix concurrency concerns with algorithm logic. I was thinking that would not have to be the case, because sending on Go channels is non-blocking if the channel was initialized with a message queue, so channel producers and consumers would not necessarily have to run in separate goroutines. But then I realized that to properly simulate laziness you need backpressure, which means small message queues. I think that when code sends on a channel that has a full queue, the send will block. So that means that if producers and consumers are not running in separate goroutines you are likely to get deadlock.
That's a different thing you're thinking about. witheve.com or something.
Thinking of it in terms of do { } binding tight is quite compelling. I hadn't thought of it in those terms before, just as a convenient syntactic hack that makes long term haskellers' eyes bleed. ;)
There are some dragons in this design space though. unsafeInterleaveST is less safe than unsafeInterleaveIO as the lack of multiple execution guarantee matters when the interleaved bit can be potentially forced by multiple source threads.
This is great! Thanks for making it. While not strictly about the bindings themselves I think the [typeclass trick](https://github.com/llvm-hs/llvm-hs/blob/llvm-4/llvm-hs/Setup.hs#L45) used to make the build script compatible with older versions of Cabal is really neat.
That's awesome!
That makes perfect sense. Thanks. (Might I suggest clarifying that in the blog post?)
Isn't this how the internet works? I'd say a good portion of programming blog posts are posted and reposted to reddit a significant amount of time after they were originally published.
I think they responded to the question specifically by saying that Rust did not have a mature Raft implementation at the time.
That looks pretty nice, though I find the massive spacing in the example a little odd: briDoc' &lt;- MultiRWSS.withMultiStateS BDEmpty $ do
I agree with a lot of what is said in the article. Wonderfully written. I only have one gripe... "The tooling is really bad" is sort of misleading because it is just not. * gofmt/goimports on file save * Auto-completion using go-autocomplete (with the auto-completion layer) * Source analysis using go-guru * Linting with flycheck’s built-in checkers or flycheck-gometalinter * Coverage profile visualization And that is just what I copied from the Spacemacs layer page. Not having something akin to Haskell specific profiling tools doesn't make the tooling bad. 
I'd be quite interested to see if you could, though I seriously doubt it, as you could imagine a programming language in which you could match on types and have a nonuniform, unnatural version of the universal where this wouldn't actually be true.. I'm no expert though.
What do you expect `f 1` to do?
I may have misread, but I thought the author was referring mostly to a lack of good dependency management. Haskell really shines in this area these days, between Stack and Nix.
Yah I get it. I've used other langs and other IDEs. I mainly think that Haskell's type system makes up for much of its lack in IDEs. That said, I long for those features as much as the next person.
Golang trades throughput for latency. It's a different set of tradeoffs.
From the article: &gt; In Go, you import packages by URL. If the URL points to, say, GitHub, then go get downloads HEAD of master and uses that. There is no way to specify a version, unless you have separate URLs for each version of your library. &gt; This is just insane. Insane indeed. I wouldn't call a dependency manager any good if it gives you no control over versions. I'm pretty adamant about needing to have exact version pinning of the entire dependency tree. Though I now realize that that quote was not from the "tooling" section of the article.
While valid points, the go get command that he is criticizing was just a quick way to get dependencies, and not really a dependency manager. The in-development dep tool aims at fixing that.
You should promote that more. I've been unhappy with the lack of options of Haskell formatters, and this is the first I'm hearing about it.
&gt; I'd rather we doubled down on it and made it syntax, so it could work on types as well. ;) What is wrong with this? type family ($) f x where f $ x = f x infixr 0 $ Seems to work just fine. I would much rather we find an elegant way to extend the type checker that isn't a hack.
Maybe try /r/agda ?
He probably just compiled his Agda post to Haskell.
Agda's auto proof search is hardwired into the compiler. That makes it fast, but limits the amount of customization you can do. One alternative approach would be to implement a similar proof search procedure using Agda's reflection mechanism -- [Pepijn Kokke and I tried something along these lines](http://www.staff.science.uu.nl/~swier004/publications/2015-mpc.pdf). With the recent beefed up version of reflection using the TC monad, you no longer need to implement your own unification procedure. Carlos Tome's been working on reimplementing these ideas (check out his code [here](https://github.com/carlostome/AutoInAgda)). He's been working on several versions that try to use information from the context, print debugging info, etc. Hope this helps!
I think that we would get a lot of single-person developed, not maintained websites, with: - most of the code in IO, even if this code is mostly pure, with 'putStrLn "here 123"' deep inside call stack - no user-defined data types, Bool, Int, String, lists and tuples for everything - cascades of if-then-else instead of pattern matching - roll-your-own recursion instead of fold/map - no higher-order functions - you name it...
I wrote a patch for this years ago ([`#8751`](https://ghc.haskell.org/trac/ghc/ticket/8751)) but it must have fallen by the wayside .. 
This subreddit is about the programming language Haskell, please check before you post. 
Seems like that's a partial solution for pattern matching, then
I don't know the answer to your question but one thing you have to keep in mind is that the amounts themselves have to be converted, not just their format. So it's not as simple a problem as it might seem. Especially since exchange rates change frequently. Once the amounts are converted, using the right format is really just a matter of selecting a format from a list of formats. And, knowing how to convert, implies your program already knows which format to use.
I see your point, and I agree to some extent. I just don't think that the problem is as big as you make it out to be. At the very least, mentioning nerds talking about programming languages and then racism and terrorism in the same sentence is outrageous. These are just good-natured jabs at programming languages and their designs. It's not like this community is going around harassing people who don't like Haskell. 
If he (or anyone else, for that matter) is interested, [Agda could really use someone interested in working on the builtin Auto functionality](https://github.com/agda/agda/issues/1397#issuecomment-279382641).
Currency conversions does not happen for now. So it is just display.
Haskell ruined my career as a programmer!
Yes, it's been a pain point for a little while now. There's [an AIM](http://wiki.portal.chalmers.se/agda/pmwiki.php?n=Main.AIMXXV) coming up and I am planning to devote time to try to understand Auto. The more the merrier!
I like the package, but I don't see any l10n support at all. I think l10n is what the OP wants. For example, you'd want to display 4.20 USD as "$4.20" in the US, but "4.20 USD" in CA. Vice-versa you'd want to display 3.50 CAD as "3.50 CAD" in the US (or ignore this case because it doesn't happen), but "$3.50" in CA.
Do you think it would have been better if the class hierarchy had been more granular to account for stuff like this?
It becomes complicated. e.g. if you deconstruct Applicative into Pointed and Apply and then make an Applicative class that just says their laws interact, you still wind up needing separate names for the Applicative-supplied combinators or inference will only pick out Apply and Pointed, and not the Applicative constraint necessary for legal interaction. If we were better in Haskell at dealing with deep class hierarchies it'd be an option, but we don't have a good way to do superclass defaulting, so finer grained hierarchies mean a lot more work for everyone involved. In general I favor fine-grained class hierarchies (as evidenced by my code!) but you don't get to take advantage of laws that simplify the set of operations you must supply when the operation and the law aren't specified in the same "step" of refining the class hierarchy, meaning a lot more work for library authors/maintainers to supply abstractions that may not pull their weight.
I think the fact that do notation has a different failure mode for patterns than every other part of the language is a bad design. So I don't really care for making use of `fail` at all, especially not implicitly. In these cases, I'd much rather just establish a convenience function and use it. justRight :: Either e a -&gt; Maybe a justRight (Right a) = Just a justRight _ = Nothing The overhead in the do block will be about the same compared to sprinkling returns and patterns everywhere, and it's much clearer what exactly is happening.
They're not in H2010. They're not turned on by default. They are however not deprecated as a _ghc flag_. https://prime.haskell.org/wiki/NoNPlusKPatterns
Well, maybe the existence of a naming collision now means that using only one's first name was not a very good idea in the first place. This applies equally to all people implied in the conflict, but as to the question of how to resolve it, it seems reasonable to consider history of established usage to decide preference, because that is how people's memory often work -- which corresponds to giving out the name space on a first-come first-served basis, which is rather usual and not "weird". &gt; You could always browse the site briefly to find "Oleg Grenrus" if you care. Making it confusing for one's readers is never very good. I think there it is also unfortunate that there is a risk of misattribution: someone could read content from "Oleg" and just assume that it comes from Oleg Kiselyov -- without "browsing the site briefly". I wouldn't want to make this more of an issue than it is (not very much), but my remark and clarification proposal seem to be common sense to me.
I agree that if avoiding *as much potential for confusion as possible* is a goal of the author, then your feedback would certainly be valuable. The part I had difficulty with is that I got the impression that you were implying that the author *should* take steps to clarify, as well as potentially accusing (and I apologize if this was not your intent) the author of *intentionally misleading* readers, which I would consider ridiculous in this context. Your proposal regarding naming conflicts I think is reasonable in some domains, but when it comes to someone's first name I suppose we would have to disagree. In my opinion it is "common sense" to avoid making presumptions based solely on a first name. As an example, I work in a department of ~20 people and we happen to have 3 "Karen"s. Just a quick data point I found interesting: searching Google for "oleg haskell", in the first 5 results the name "Oleg Kiselyov" appears twice and the author, "Oleg Grenrus" appears once.
It probably should be done for `unsafeInterleaveST`. I don't remember if I've filed a ticket for it. **Edit: I didn't but David Feuer [did](https://ghc.haskell.org/trac/ghc/ticket/13457).** I only noticed it when I went to go use `unsafeInterleaveST` in some code for `discrimination` in a situation where the result was sufficiently complicated for this situation to actually arise.
Automatic inlining and extraction: Finds names that are only used once and inlines them, and finds patterns that are used multiple times and extracts them. Perhaps inline only if the type needs not specialize, and extract even from used-once patterns if the type generalizes.
We have too many Simons, and now a surplus of Olegs. =) There is now an `edk` on the #haskell IRC channel. So even `edwardk` isn't safe from name collision. ;) I'd use an SHA1 of my birth certificate or something for a nick but [eh...](https://shattered.io/static/shattered.pdf)
Sure but they're proposed to be eliminated as well
I am talking about the section entitled "The tooling is bad". That section doesn't mention Nix or dependency management. On dep management know that gopkg and the godep tool solve many problems. There is also a simple dependency locking tool called `godeps` https://github.com/rogpeppe/godeps There is really little need for something like Nix (even thought Nix is quite nice now). 
good point about interaction, I never thought about it!
Great article! Just a followup question: It is mentioned that recursive definitions are definitely loop-breakers. Are there other "categories" of functions/definitions which fall under this category?
No, it is only self-recursive definitions which are definitely going to be loop-breakers. The function which decides is `OccurAnal.nodeScore` and it quite well commented if you wanted to look.
**Here's a sneak peek of [/r/agda](https://np.reddit.com/r/agda) using the [top posts](https://np.reddit.com/r/agda/top/?sort=top&amp;t=year) of the year!** \#1: [Agda 2.5.1 released](https://github.com/agda/agda/issues/1933) | [0 comments](https://np.reddit.com/r/agda/comments/4f66nz/agda_251_released/) \#2: ["This module contains an optimised implementation of the reduction algorithm (...) It runs roughly an order of magnitude faster than the original implementation."](https://github.com/agda/agda/commit/08f69ccfeb455478d71a48c797eef11cfc8104c1) | [0 comments](https://np.reddit.com/r/agda/comments/50bno0/this_module_contains_an_optimised_implementation/) \#3: [Irrelevance + Rewriting = Extensionality](https://np.reddit.com/r/agda/comments/4veb5a/irrelevance_rewriting_extensionality/) ---- ^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| [^^Contact ^^me](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| [^^Info](https://np.reddit.com/r/sneakpeekbot/) ^^| [^^Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/5lveo6/blacklist/)
Indeed. &gt; justRight :: Either e a -&gt; Maybe a If you don't want to roll you own convenience function, *errors* [provides it as `hush`](https://hackage.haskell.org/package/errors-2.2.0/docs/Control-Error-Util.html#v:hush). --- (Re: your comment in the parallel subthread.) &gt; pureRight = either (const empty) pure Alternative spellings for that include `getAlt . foldMap (Alt . pure)` and, if you are using *lens*, `alaf Alt foldMap pure`.
This is a great summary. One thing that could probably use mentioning is GHC oniy inlines functions which are fully-applied syntactically speaking, which is not really what you'd expect. This is why you see library functions written in the tortured foo a = \b -&gt; go b where ... ...style. Also as someone who can be accused of "abusing the `INLINE` sledgehammer" in the libraries I write, I always bristle a little at criticism of the practice in the name of compile times. Optimizing a haskell library is difficult, and I don't have a ton of time to check every function to see whether changing an `INLINE` to an `INLINABLE` would be a regression, or to check whether a function I suspect would benefit from an `INLINE` actually doesn't require it. I feel it's about 85% Not My Problem. https://ghc.haskell.org/trac/ghc/ticket/13376 My pet theory is that much of the issue is that GHC devs (god bless 'em) aren't dogfooding to the extent that changes aren't guided by microbenchmarks that might lead to more `INLINE` pragmas which would also, I imagine, make plain ways that GHC performs poorly on the sort of code that library writers have learned to write to make stuff go fast. But i may be wrong, and I don't mean to suggest there are any spare cycles there either though, or that dev time would be better spent differently (and I wouldn't presume to think I should have a say in the matter).
I changed the *by* annotations to have my full name.
My impression (rather superficial as it is, being built on a quick glance at the TOC and the free sample) is similar to yours. In particular, the two first allusions to `IO` make me a bit wary. In pages 2-3 (section 0.2.1), there is a Hello World: &gt;#### Listing 0.1 hello.hs a Hello World program --hello.hs my first Haskell file! ❶ main = do ❷ print "Hello World!" ❸ &gt;❶ a commented line with the name of our file &gt;❷ the start of our 'main' function &gt;❸ the main function simply prints out "Hello World" &gt;It might come as a surprise but to really understand Haskell's "hello world" requires [sic]. At this point in the book don't worry too much about what is happening in any of the code in this section. Our real aim here is to learn the tools we need so that they don't get in the way while we're learning Haskell. Then, a couple pages later (section 0.2.4), a section with advice on breaking down definitions into small functions begins with this: &gt;One of the most frustrating issues for new comers to Haskell is that "basic" IO in Haskell is a fairly advanced topic. This sounds like a self-fulfilling prophecy. Now contrast with Allen &amp; Moronuki. In pages 4-5 of the free sample (cf. http://haskellbook.com/), we find: &gt; Then enter the following code into the file and save it: &gt; sayHello :: String -&gt; IO () sayHello x = putStrLn ("Hello, " ++ x ++ "!") &gt;Here, `::` is a way to write down a type signature. You can think of it as saying, “has the type.” So, `sayHello` has the type `String -&gt; IO ()`. These first chapters are focused on syntax, so if you don’t understand what types or type signatures are, that’s OK — we will explain them soon. For now, keep going. Then, shortly after the promised tour of basic syntax, `IO` shows up for the second time in page 47: &gt;As you can see, `main` has the type `IO ()`. `IO` stands for input/output but has a specialized meaning in Haskell. It is a special type used when the result of running the program involves effects in addition to being a function or expression. Printing to the screen is an effect, so printing the output of a module must be wrapped in this `IO` type. When you enter functions directly into the REPL, GHCi implicitly understands and implements `IO` without you having to specify that. Since the `main` action is the default executable, you will see it in a lot of source files that we build from here on out. We will explain its meaning in more detail in a later chapter. They don't fall for what I regard as a strategical misstep -- there is no "fairly advanced topic" talk, and no mystery.
I got it working as I wanted with your last suggestion plus Tekmo's one, thank you!
FWIW I clicked the link thinking "I didn't know Oleg [Kiselyov] was interested in lenses. It's only when I saw nicely syntax highlighted code that I thought twice.
Point well taken, though obviously the scale is completely different. Truthfully I'll have to give some thought to reconciling this. I suppose one aspect would be a matter of branding/promotion/trademark. This whole exchange makes me think of: https://en.wikipedia.org/wiki/Microsoft_vs._MikeRoweSoft
Here's a thing I meant to reply with earlier! [Yet another comonadic game of life implementation in Haskell](https://gist.github.com/gatlin/21d669321c617836a317693aef63a3c3)
When I was learning Haskell, this feature of Maybe and [] specifically was sold as one of the main features of the language.
&gt; I used fail for Either String x in a library and Simon Jakobi pointed out that it just calls error and doesn't return Left str, which is startling. [Once upon a time](http://blog.ezyang.com/2011/08/8-ways-to-report-errors-in-haskell-revisited/) `fail x == Left x` was the law: -- Prior to base-4.3 Prelude Control.Monad.Error&gt; fail "foo" :: Either String a Loading package mtl-1.1.0.2 ... linking ... done. Left "foo" -- After base-4.3 Prelude Control.Monad.Instances&gt; fail "foo" :: Either String a *** Exception: foo
We generally don't put laws in place that require 'cross-hierarchy' reasoning like that. Nothing says that the same author will supply both instances, or that they even know about each other. When you move down a hierarchy you know the author of the subclass had to know about the superclass instance and consciously choose to embrace the law. Amusingly, we actually saw breakage way back in the day in scalaz, because they tried to use just this reasoning scheme originally. The lies you start to tell yourself (or at least, obligations you start to incur) get bigger and bigger as your class hierarchy gets deeper and deeper. Also in the process you lose the ability to talk about the sort of weird no-identity-law construction that is `(Pointed f, Apply f)` in case you actually did mean it.
Seeing this bot's respond with the agda sub's top posts (all having zero comments) just made my day.
&gt; not even Haskell 2010 anymore Fixed. BTW, haskell is not haskell2010 for quite some time.
Ultimately anything that represents a Traversal with profunctors will be isomorphic to Wander/Traversing. No extra instances can or will be gained. You can try to concoct something but then you just get stuck in a little typed ghetto of your own design that doesn't work with anything else without wrapping it up so you can do exactly what wander or traverse' does but through messier means, so we might as well just encode it that way directly. On a related "principled" note, I went through and documented laws for all the profunctor subclasses in HEAD the other day.
I find Wander/Traversing unsatisfying. If an affine traversal is a 0 or 1 target, then a traversal should terminate on 0 and then provide another traversal on 1, somehow recursing. I tried to work it out once but I wasn't smart enough. My elegance detector strongly suggests there must be a way though. Cool beans on laws!
I have some pretty strong concerns regarding finite vs. infinite cases, and the placement of infinities when you deal with infinite traversals with such a naive inductive encoding. Just as `[a]` isn't the free `Monoid` in haskell, because we're in a world where you can have all sorts of fixed points like foo x = foo x `mappend` x `mappend` foo x that prevent you from re-associating into a normal form. Unlike Set, in Hask such functions can produce meaningful results. The ability to reassociate once with the laws doesn't let you constructively reassociate an infinite number of times. Similarly in Lens we use the `Bazaar` type, which feels like the "unsatisfying" Wander in many ways, rather than twanvl's `FunList`, which are isomorphic for the finite cases, because the former handles infinite cases and the latter gets stuck.
or `preview traverse` or `preview folded` or `preview _Right` or `^?_Right` or ... =)
Unfortunately this meant that you couldn't use `Either e` at all without an `Error` constraint on `e` and the instance had to be supplied as an orphan out in the `mtl`. Note that with `MonadFail` taking over `fail`, by ghc 8.6 this will once again "just work", but without compromising the `Monad`.
or `asum . fmap pure`. Sadly `sum`, `asum`, etc. don't take a function to pick what to "sum".
I'm reading the OP's question as asking for per request middleware, such that example.com/dashboard and example.com/api can be part of the same app, but use different middleware stacks.
I'm the author of this library. This library provides channels that behave almost identically to those seen in the Go programming language (thus the name). I suppose I could have called this package something like `csp` (for "Communicating Sequential Processes"), but given my goals, `gochan` seemed more appropriate. To clarify for those unfamiliar with Go's channels: this library provides a bounded channel implementation where the user can specify the channel's buffer size. You can, of course, send and receive messages on a given channel. The core feature of this library that sets it apart from the existing channel libraries is the `chanSend` function, which -- like Go's `select` and occam's `ALT` keyword -- gives you a way to wait on multiple channel operations (either sending or receiving), firing a callback when one of the operations is finished. Starvation is avoided by randomizing the order in which each operation is attempted. I wrote this (over a crazy three day weekend about 5 months ago) for a couple reasons: 1. I wanted to make sure I understood Go's channel implementation 2. I wanted to make it easier to port Go code to Haskell 3. I wanted to end these kinds of Go vs Haskell arguments (we can now say that Haskell's channel support is a strict superset of what Go provides): https://plus.google.com/109566665911385859313/posts/FAmNTExSLtz 4. It seemed like fun I'm posting this here because I'd like feedback on the user facing API. Something that I think I'd like to do is provide a type level means of making channels either readable, writable, or both. That way consumers and producers can only do their part, which will be immediately obvious from their type signatures. However, I'm not really sure what the best way would be to do that -- I've kicked around a couple ideas, but I'm not really happy with any of them. I'd be happy to receive any suggestions/proposals. I'm fully aware that the internals could use a lot more comments and some clean up. There's a whole lot of unsafe stuff going on, some of which I think I can remove, though probably not all (at least not without a big performance hit from stuff like Data.Dynamic). I'll try to clean up and better document the internals over the next week. Interestingly, the performance of this library would be greatly improved with something like the ["Mutable constructor fields"](https://github.com/ghc-proposals/ghc-proposals/pull/8) proposal (I keep a mutable, doubly linked list of each thread blocked on a given channel which gives me O(1) removal and append). I'm hoping I can pin down any last big API changes and release a stable version 1.0.0 soon. 
As an example of `(Pointed f, Apply f)` is e.g. `Default k =&gt; Map k` atm
We have long had ["hamtmap"](http://hackage.haskell.org/package/hamtmap), but unfortunately due to zero promotion the community never caught up with the project. Later ["unordered-containers"](https://hackage.haskell.org/package/unordered-containers) has moved to the HAMT-based implementation. On a side-note, if you're ever looking for AMT-based vectors, we have them too with [the "persistent-vector" package](https://hackage.haskell.org/package/persistent-vector). But again, the community seems to be pretty unaware.
I am in a "Enterprise Software Architectures" course right now - Java EE 7 - yuck! I'd love to see *anything* else than Java! 
Very interesting, thank you for starting this discussion. I have defined select :: [STM a] -&gt; STM a select = msum and taken a stab at the examples [here](https://gobyexample.com/select) and [here](https://tour.golang.org/concurrency/5). You can see the result in [this gist](https://gist.github.com/phischu/4c66fe8667457316c23476ee32620d1a). Two things are not so nice in Haskell: You can't do IO from STM and you can't break a loop with return.
&gt; You can't do IO from STM That's a good thing! `atomically` blocks use "optimistic concurrency control", meaning that the transaction is aborted and restarted until it succeeds. If you were allowed to perform IO inside of those transactions, these IO actions would sometimes be mysteriously executed more than once. In other languages which implement this idea, the documentation would tell you that the transaction can be executed more than once, most of the time they wouldn't so you'd accidentally write stuff which should only be executed once, and then you'd encounter a mysterious sporadic bug in production much later. So you should be glad that the language prevents you from making a hard-to-debug mistake! If you want "pessimistic concurrency control", in which critical regions are protected by a lock, you can use [`withMVar`](http://hackage.haskell.org/package/base-4.9.1.0/docs/Control-Concurrent-MVar.html#v:withMVar). Or you can simply write your IO actions outside of your `atomically` blocks. &gt; you can't break a loop with return That's certainly true: [`for`](http://hackage.haskell.org/package/base-4.9.1.0/docs/Data-Traversable.html#v:for) and `return` behave completely differently in Haskell than in an imperative language. In particular, they are not builtin keywords with a fixed meaning, they are regular functions you could implement yourself. And if you want to write a version you can break out of, you can: import Prelude hiding (break) import Control.Monad (when) import Control.Monad.IO.Class (liftIO) import Control.Monad.Trans.Maybe import Data.Foldable (for_) type BreakableIO a = MaybeT IO a breakableFor_ :: [a] -&gt; (a -&gt; BreakableIO ()) -&gt; IO () breakableFor_ xs body = do r &lt;- runMaybeT $ for_ xs body case r of Nothing -&gt; return () Just () -&gt; return () break :: BreakableIO a break = MaybeT $ return Nothing -- | -- &gt;&gt;&gt; main -- 1 -- 2 -- 3 -- 4 -- 5 main :: IO () main = do breakableFor_ [1::Int ..] $ \i -&gt; do liftIO $ print i when (i == 5) $ do break Here I have defined `breakableFor_`, a variant of [`for_`](http://hackage.haskell.org/package/base-4.9.1.0/docs/Data-Foldable.html#v:for-95-) which you can break out of using the `break` action. It's an extra action which is not normally available in IO, so the body of `breakableFor_` is not an `a -&gt; IO ()`, but an `a -&gt; BreakableIO ()`, indicating the extra side effects you can perform in there. It just so happens that there is already a monad transformer `MaybeT` which adds a short-circuiting side-effect to an existing monad, so I can easily define `BreakableIO` and `break` in terms of it. And tada! I can now break out from for loops.
STM has `orElse` which is `&lt;|&gt;` for the STM monad and is a more powerful way of composition than select, since it can compose STM computations, not mere selection among a set of STM variables. 
"Picking up from a belt" is a behaviour that must be coordinated between actors and therefore transactioned.
I'm afraid you've got the wrong subreddit; this one is for the programming language Haskell. 
I'm not sure if I should lol on this one
bad taste
How does this compare to https://hackage.haskell.org/package/chp ? 
You can get early exit with `ExceptT` for loop breaking
&gt; Virtually all code I've ever looked at could have Haskell's type system stapled to it [...] Have you looked at much Python at the API layer? There's something to be said about the convenience of a function which (for example) has a default parameter, or accepts a single function callback, or accepts an iterable of callbacks. It's the same function, and the documentation is all in one place, so readers don't have to muck around looking for the super general variant defined only internally. They also don't have to deal with awkward default value handling by default records. Would I give up Haskell's type safety to write Python? Of course not! But I don't think we (the type system research community) should give up on exploring type systems that try to capture more of the flexibility of untyped languages.
I've always said "good riddance" to overloading like that though, especially with default parameters. Changing behavior based on unchecked types is a recipe for disaster, and it's always caused me more pain than it would to be explicit about it.
I've never found any good ideas in Python (many years of use) that couldn't be somehow captured in a type system like Haskell's. Haskell's type system specifically doesn't lend itself to certain patterns in Python (like named arguments with defaults) but there *are* type systems which do capture this, and there are even Haskell libraries that can do it. Everything I've found that would be very hard to capture in a type system is because it wasn't good idea in the first place (cough metaclasses whose methods modify the call stack cough).
&gt; If you want to dispatch on which transaction was selected you have to tag the results. Surprise! foo :: TMVar String -&gt; TMVar String -&gt; IO () foo c1 c2 = join $ atomically $ select [ do msg1 &lt;- takeTMVar c1 return $ putStrLn $ "received " ++ msg1 , do msg2 &lt;- takeTMVar c2 return $ putStrLn $ "received " ++ msg2 ] Instead of returning a tagged result and then pattern-matching on that tag to decide which IO action to run next, I am returning the IO action directly, without executing it. I then execute the IO action returned by the `atomically` block: do ioAction &lt;- atomically ... ioAction Which simplifies to `join $ atomically ...`. &gt; for an imperative programmer not already sold on Haskell this reads: &gt; &gt;&gt; If you want a version you can break out of, you have to write your own. Would it sell better as "our breaking-out construct isn't tied to loops, so you can also break out of `if`s, nested functions, etc."?
&gt; Something that I think I'd like to do is provide a type level means of making channels either readable, writable, or both. That way consumers and producers can only do their part, which will be immediately obvious from their type signatures. However, I'm not really sure what the best way would be to do that -- I've kicked around a couple ideas, but I'm not really happy with any of them. I'd be happy to receive any suggestions/proposals. What about https://gist.github.com/NicolasT/ebddeb1031ecfb6890b4819b953bd449
&gt; As an exercise in implementation, implementing Go channels in Haskell might be fun, but for serious programming my advice would be to be very sure you've explored all the Haskell options before you import Go channels. That sounds fair to me. The goal, aside from the exercise itself, was to provide yet another option -- one that would probably most benefit Go programmers picking up Haskell, or people who need to quickly port something from Go to Haskell.
the only limitation is when iam running the .hs file (using haskell platform on windows) iam only allowed to type the length of the array, also i have been studying c,c++,c#,java and got dropped into haskell, it has a weird syntaxing which drives me nuts
[hoogle](http://hoogle.haskell.org/) and [hayoo](http://hayoo.fh-wedel.de/) are your friends. The trick with searching either is that sometimes it pays to be a little more generic - [searching for `a -&gt; [a]`](https://www.haskell.org/hoogle/?hoogle=a+-%3E+%5Ba%5D) rather than `Int -&gt; [Int]` may be more instructive.
helper n current = 1 + helper current (n-1) ..umm something like that?
&gt; I don't think that rule would invalidate or change the meaning of any currently legal Haskell program. Except, of course, for all those Haskell (and other) programs which have to parse Haskell ;)
Not quite, you have to return a list, not an `Int`. You will have to use the cons operator `:`. The point of using `current` is to put it in front of the current list, so it would be like this: `current : ...`. Don't change `n` otherwise you won't know when to stop. With this helper function I had in mind a left to right building of the list. You can however do it without an explicit `current` by decrementing `n` like you seem to be doing. In this case you need to prepend the list built by `n-1` and stop when you reach `1`. listgenerate :: Int -&gt; [Int] listgenerate 0 = ... -- Base case listgenerate n = ... ++ [n] -- Recurcive case
Could you tell me what kind of magic FRP does to abstract away the infamous GUI's callback and avoid the callback hell.
&gt; Probably the biggest difference is that there's a lot of applications where it's just not transactional, so why would you use STM pervasively? `STM`'s `TQueue` is a lock free queue and performs faster than the non-STM `Queue`. So it may not be transactional, but by being transactional, you get lock-free performance characteristics. (I think, I may be wrong, it's been a while since I read Marlow's book on this)
Just use a text editor?
&gt; The one type which cannot be defined by normal users is IO. It's possible to make a "world-passing" state type, and encapsulate it inside a module. In which case, the tricky parts seem to be running `main`, implementing `unsafePerformIO` and defining a library of effectful operations (`putStr`, etc.). `unsafePerformIO` would require a type coercion. Haskell does allow these, e.g. we can use `loop = loop` as a value of `a -&gt; b`, although such things are usually divergent/unproductive. That might not be an issue if we're relying on some separate mechanism/run-time-system to "spot" such occurrences and act on them. For example, the `ghc-dup` package includes some C-- code for spotting and replacing calls to a particular function symbol (unfortunately this is broken since the switch from package names to package keys). If we think about an `IO a` value as being a program for a separate, effectful machine then one possible implementation would be to store literal strings of source code for some other language, e.g. Scheme. To define an operator like `putStr`, we can just wrap up the equivalent Scheme code. Building up a `main` value via `do` notation, `&gt;&gt;=`, `return`, etc. would cause these snippets to get spliced together. One potential problem is we'd need some way to serialise/deserialise data across the Haskell&lt;-&gt;effectful-language barrier; this probably isn't an issue if our underlying operators only use a small set of concrete types, e.g. `String -&gt; IO String`, since that guarantees that all other values/calculations will take place on the Haskell side. The final issue is how to make `main` execute, or in the above implementation: how can we invoke the effectful Scheme program? GHC's run time system does this for `main :: IO a`, and I can't think of a pure Haskell way to implement an alternative. One interesting possibility is to compile *very aggressively*, such that the value of `main` which appears in the output (ELF file, or whatever) is a single, fully-formed Scheme program; i.e. all of the Haskell evaluation has been inlined/simplified away, and we're just left with a string of Scheme. A post-processing step can extract that string, and we've got ourselves a Haskell-to-Scheme compiler! Unfortunately, this would only work for programs whose Haskell component is relatively trivial, since we'd need to be capable of evaluating/fusing it completely at compile time. This means *either* writing a pretty trivial program, *or* doing all the hard work in Scheme and using Haskell as nothing more than a fancy macro system. Have I made any glaring mistakes in the above description?
We're usually more interested in productive corecursion than tail calls per se. https://hal.inria.fr/inria-00322331/document Laziness still applies to enums as until you look at the enumeration to determine its value, we still don't do any work. The same holds for Int and other "simple" types. const True (let x = 1 + x in x :: Int) = True (let x = 1 + x in x :: Int) would spin forever if forced, but const never looks at its second argument at all. Laziness lets this program terminate despite the fact that something with "all the information stored in its constructor" shows up in the code.
Thanks. That spells out the differences pretty clearly. 
Just pick the file extension to be .hs. it is just a text file with a given file extension.
I've found a way. Thanks for your help!
listgenerate :: Int -&gt; [Int] listgenerate n = [1..n-1] ++ [n] that one seems to be working, if i am correct
Right that would work but not really what I was trying to guide you to. Here are the 3 options I alluded to in my previous messages: listgenerate :: Int -&gt; [Int] listgenerate n = [1..n] Or: listgenerate :: Int -&gt; [Int] listgenerate 0 = [] listgenerate n = listgenerate (n - 1) ++ [n] Or: listgenerate :: Int -&gt; [Int] listgenerate n = helper n 1 helper :: Int -&gt; Int -&gt; [Int] helper n current | n &lt; current = [] | otherwise = current : helper n (current + 1)
WYAS was my copy/paste experience to be honest. Coming from years and years of such vastly different languages Haskell just seems... weird. Can't understand a thing but I've wanted to learn it for SO LONG.
Since `msum` uses `&lt;|&gt;` for `STM`, then it's just the difference between using a List vs not
and if i wanted to spice that and replace certain numbers with a string (making another variable "m") listgenerate :: Int -&gt; Int -&gt; [Char] listgenerate m n= if (mod m n) == 0 then "Boom" ++ [m] else [1..m-1] ++ [m] doesn't seem to work, what am i doing wrong here?
Are you trying to write a FizzBuzz program? Are you sure the return type is `[Char]` and not `[String]`? Be careful not to mix types, in the list you return you are mixing `String` and `Int`. If you want to convert an `Int` into a `String` use the function `show`
This thread didn't get a whole bunch of attention, but either way... This community is like, super informative, and it shares a LOT of characteristics with the Rust community (except I actually can do stuff in Rust...) Rust is literally the only other language that seems to have so much.. just... "Stuff" to do and read about, it's insane and I love it, but I can't make any sense of it. Another big parallel, everyone who knows Haskell seems to REALLY KNOW HASKELL, and that's how it was with rust too. I'm pretty mediocre there, but it feels like in both these languages people either know nothing at all because they feel so alienated and confused, or they get neck deep and fall in love. Hopefully I'll keep working with Haskell, the other book that was posted here I haven't seen before, and I've been working along through it having tons of fun. Thanks again guys, I'll probably be back for more in the IRC, or maybe back here after I read some books and get smart.
I can't easily guess what you are trying to do, but are you trying to output "Boom" every `n`?: listgenerate :: Int -&gt; Int -&gt; [String] listgenerate 0 _ = [] listgenerate m n | m `mod` n == 0 = listgenerate (m - 1) n ++ ["Boom"] | otherwise = listgenerate (m - 1) n ++ [show m]
For what it's worth, I think it's great to have options. The more concurrency options the better.
I disagree. I have started to look into ML and i only spend my time in jupyter. I think this is what default-parameters etc. are made for and python really shines when all you do is experiment in a repl-like environment. The code complexity is not that high, 90% of the code you write is useless and will never be read by another person, because your idea didn't work (which is NOT true in nearly every other scenario). I wouldn't want to build real software using those features, but that doesn't mean they are totally useless. But in the context of normal software development, i agree.
There is also [try haskell](https://tryhaskell.org) if you just want to fiddle with the language in the browser.
Yeah that classic example is pretty weak. You can always replace that use-case with either some sort of ADT or if you want to be more ad-hoc, HList/Coproducts.
Yeah, I mean that `&lt;|&gt;` and `msum` are more than select, since the latter can not compose computations in which channels are read. It just compose reads of channels
If we're designing an api familiar to Go programmers, then perhaps something like this to hide the cruft in helper functions: select = join . atomically . msum scase ma k = do a &lt;- (takeTMVar ma); return (k a) foo c1 c2 = select [ scase c1 $ \msg1 -&gt; putStrLn $ "received " ++ msg1 , scase c2 $ \msg2 -&gt; putStrLn $ "received " ++ msg2 ] 
This!
Do you plan to support :t?
i've both used and written ridiculously overloaded python functions. among my friends who are professional "dynamic programmers" (ruby, js), they all hate that idiom. if you want to expose a primary symbol for some functionality, you just parametrize it: post PostDefault post (PostCallback print) post (PostCallbacks [print, (\\x -&gt; modifyMVar ...)]) etc. it's only slightly less convenient and much more readable. 
At his point I have no idea how /u/jwiegley can possibly so productive. This guy has his fingers in nearly all projects I am interested in and plays an important role in half of them.
 f n = [1..n] or f = go 1 where go m n | m &gt; n = [] | otherwise = m : go (m + 1) n
The Durability part of ACID is about data integrity surviving crashes, power loss, etc. so I believe the idea is that it's not directly applicable to STM itself because it's only concerned with managing state in a transactional way. To get the Durability in that sense, you probably want to go past STM, maybe maintain and restore the entire program state between failures, like the way some Smalltalk implementations are image-based and can be stopped and resumed later. I suppose you could write a program in a way that dumps memory to disk after transactions, and restores it on next startup, but it seems like there's a lot of potential for edge cases doing it like that, compared to maintaining and restoring an image. 
&gt; The inferencer need not be sound If it's not sound, why am I using it? I use tools to catch my mistakes, not make them for me. As a real world example, this is exactly the reason I didn't use the 2to3 python tool to convert Python 2 code to Python 3 code, and instead did so by hand. In addition to gaining familiarity with whatever library I was porting, the value of not having to debug generated output cannot be understated. If I did something wrong while manually porting, I knew where. If an inferencer is allowed to return potentially incorrect types, then having inferred the types, I'd not know where to begin to fix them. Either I manually change a few types on a hunch and am led further astray, or I change enough types that the inferencer is happy, but am not guaranteed to get a result that I expected. If an inference algorithm is sound, I suspect that even if types that are more constrained than I would have wished are generated, I'll have a better clue about how to fix them. I might be missing the point, but I'm not buying what they're selling.
I will be messaging you on [**2027-03-21 23:39:23 UTC**](http://www.wolframalpha.com/input/?i=2027-03-21 23:39:23 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/haskell/comments/600x9i/what_would_haskell_look_like_with_massive_adoption/df8mj5q) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/haskell/comments/600x9i/what_would_haskell_look_like_with_massive_adoption/df8mj5q]%0A%0ARemindMe! 10 years) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! df8mjph) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
OP, no offense but you strike me as very young (16-18), or just emotionally immature. Your post has multiple fuck/shit swear words in it. Sounds lame to point out, but people don't really swear around here. There's just no need, it comes off as overly edge and compensating for something... which is quite obvious from the rest of your post - serious feelings of self-doubt and imposter syndrome. My advice is to just slow down and enjoy the learning process. Also, ask many questions on IRC and /r/haskell_questions, but please be respectful! 
I'm never a fan of adding weird library level special cases to the compiler (see: `$`). To be completely honest, I just don't think this is a *real* issue for anyone. Generating thousands of warnings across Hackage will generate several orders of magnitude more work than has been ever spent tracking down issues of this kind. I have personally run into this problem *zero* times, and I'm sure that's true for a good majority of Haskell users. And for those that *do* run into it, it takes a few minutes to ask someone or to figure it out. It's just not a meaningful barrier to anyone, and any further changes will generate way more work than it's worth.
That's a good point. But I wouldn't say it "works better" in dynamic languages, it just works easier. GHC generics *work* fine, and give you type safety, optimization, and sum types.
I personally think having functions generalized to `Foldable` in `Data.List` is definitely not great. IMO they should be deprecated and moved to `Data.Foldable` if they aren't already there. Then perhaps once they are fully out of `Data.List` we could consider introducing back some select monomorphic versions of those functions that people want. I personally wouldn't use them or want them that much, but if others do I think this would be much better than the current situation. I just wish people didn't import `Data.List` unqualified so much.
It's just because we use List a lot in haskell. I just wish we can bring the monomorphic functions back to Data.List. I know it's for compatibility reason we generalise them to Foldable. But still the price is higher than the profit, ask user to qualify their list import is a good thing after all. Now when i look at the type of functions in Data.List, i just feel inconsistent and weird.
I thought "D" is almost always implemented with a write-ahead log, and at least the log (but not necessarily the log's "projections") must be persisted in durable storage before anything else in the system sees those txns. So if STM was somehow able to implement that, it could be durable.
My hobbyhorse of what is difficult in Haskell where the equivalent isn't bad in dynamic languages is nanopass compiler frameworks. Most of the reason I'm not satisfied with the existing solutions is because I demand more from a typed language. What I want is to be able to express type-safe passes, with as many similar but not identical ADTs as warranted, without massive code duplication. Haskell can do this using any one of its extensible variant solutions, but I wouldn't want to use any of them in production. There is good research into type system features that would help significantly, but some of it is actively rejected by the Haskell community/developers (Anything to do with subtyping) and some of it isn't judged enough of an improvement to justify integrating it (row types).
I've just used `llvm-hs` with some pieces of code from [Implementing a JIT Compiled Language with Haskell and LLVM](http://www.stephendiehl.com/llvm/), and I've been able to use that and `QuickCheck` to show that the compiler for a little toy language does the same thing as an interpreter built from the small-step semantic rules. I wouldn't have thought of trying it if `llvm-general` / `llvm-hs` / Stephen's series hadn't made it so approachable.
The important thing here isn't that `Traversable` has laws. Rather, the difference is that the `Traversable` functions (with the obvious exception of `foldMapDefault`) give results with types parameterised by the element and container types, while `Foldable` includes things such as `length :: Foldable t =&gt; t a -&gt; Int` and `null :: Foldable t =&gt; t a -&gt; Bool`. It is the same issue that /u//ezyang discusses in the article using `Functor` as an example. 
But what good would that do with regard to what the program is holding in memory? If only part of the program needs transactions and the program crashes and you re-run it, most of the program state will have to be built back up again anyway. That puts the Durability concern back on whatever database or storage you're using. STM's just about verifying that variables are updated synchronously, not what you do with them after (like database writes). Mixing STM with some kind of program snapshot system, like I mentioned above, would be pretty cool, though. Transactions to guarantee things that need to happen at the same time combined with a way to kill and relaunch the program and have it chug along like it never stopped. 
I think you're right that STM and Durability are different concerns, as you say (good arguments!)... it's probably a "good thing"™ that it doesn't make sense, i agree.
Sure, you could say generics "work better" in haskell because type safety makes everything work better. But on the other hand, "works harder", when it crosses your personal brainpower threshold, becomes "doesn't work at all." Brainpower doesn't increase unless challenged though, so maybe, like so many things, it's a bitter pill that's good for us in the long run. There's a trade-off in there though. I was going to say I still miss dir(), but come to think of it, :i and :browse do very well.
I completely agree. I wish all the `Foldable` functions in `Data.List` were deprecated, and then eventually removed. Then at some point in the future once all the packages catch up and stop importing them, we can start introducing a select few back into `Data.List` monomorphically. People importing `Data.List` unqualified doesn't seem like a deal breaker, we can start searching stackage and hackage and cleaning up such imports pretty mechanically right now. Then we can have a long deprecation period as well. Although Prelude should use the generalized versions and not the monomorphic ones. 
One thing to note is that the key part is the recursive part not being inspected until after everything else is put into a data structure. So: incrementList (x : xs) = [x + 1] ++ incrementList xs incrementList [] = [] is just fine, even though you aren't explicitly using `(:)`.
This is probably the best proposed solution I've seen thus far. Being able to attach the suggestion to particular unpopular instances would be particularly nice, but probably never entirely reliable. Doing it in the class should be very reliable, although it may produce more warnings than some would prefer.
I've just tried https://github.com/mr/ftp-client and it works. I can login, list, and download using it. Do give it a shot. 
I've just tried https://github.com/mr/ftp-client and it works. I can login, list, and download using it. Do give it a shot. 
Wouldn't have happened in a language with totality checking.
if its not well typed its not meaningful anyway
The former isn't necessarily dynamically typed. Its just an expression. Depending on the language its in, it may well be statically typed.
I think the opt-in pragma, if done right, could be just something tossed in teaching or beginner or other custom preludes (like for house style at a large shop). As such it would be a generally useful tool that can be used by people who really hate the `length` changes to just sidestep the issue, and the rest of us can happily ignore it.
I think something like the following works: let a = 3 :: Int let b = [1, 2, 3] :: [Int] let c = "Foo" :: String
&gt; But backwards compatibility is much less of a concern with Haskell. I disagree, strongly. Breaking BC will not cause silent errors to creep in in Haskell, but it will cause compilation to break even if you weren't using that particular function. It will cause problems of one sort or another to all your downstream users. &gt; haskells syntax is so much more regular than Python This is a tangential point, but I disagree with this assertion. Haskell's syntax is in flux, and extensions regularly break previous parsers. If you've ever used something that depends on `haskell-src-exts` and a feature introduced in the latest GHC, you will have experienced the pain. For example, HLint currently can't handle files with type applications in them. &gt; Ultimately, Api backwards compatibility is only a _major_ problem when code is going to run that can't be verified to have been updated to the newest Api Imagine if every major GHC release renamed the `Monad` type class. Now imagine you're a package author who wants to support multiple versions of GHC. True, this is an extreme case of backwards incompatibility, but I think it's a major problem and thus it server as a counterexample to the claim. &gt; As for the second concern, I'm not sure how it's a concern. In this scenario, you do not have control of the `Handler` type or the `register` function, but you get to express an API preference to upstream: either they take lists and to the composition internally or they expect the arguments to be precomposed. 
I can agree with that. I do think that, in general, type systems could be said to *enforce* understanding. This is great...unless you don't have a good understanding of something. In Python, you're free to not understand things and move forward effortlessly (ignoring the long-term cost of that). In Haskell, not understanding might mean you need to find a different way of doing it...a way you do understand. This is great for many things, but does have downsides. I think your case probably fits into this general pattern.
Is `5 :: Integer` not good enough? For lists you could use type holes I believe: `[1] :: [_]`.
&gt; In this scenario, you do not have control of the Handler type or the register function, but you get to express an API preference to upstream: either they take lists and to the composition internally or they expect the arguments to be precomposed. This seems like an arbitrary restriction, no? I have control over the type of the function, but not the type of the representation? That seems odd. &gt; If you've ever used something that depends on haskell-src-exts and a feature introduced in the latest GHC, you will have experienced the pain. I have done so. I understand your point on this one -- still I feel that things like renaming functions out to be an easy fix in Haskell. It would be nice if we were able to import particular versions of packages specified in the source file and have modules explicitly marked for compatibility. If cabal could substitute in an API adapter module for things that expected the older API that would be nice too. &gt; Breaking BC will not cause silent errors to creep in in Haskell, but it will cause compilation to break even if you weren't using that particular function. Again, I cannot see the issue -- you're upgrading to a new version of a library. If the version number has changed indicating binary breakage, don't upgrade until you're ready to change. In the meantime, it would be nice if security fixes were backported, but I'm not sure why we need to preserve BC at all costs -- that leads to software like the win32 api. If you've ever contributed to WINE, you'll know what a mess that is. &gt; Imagine if every major GHC release renamed the Monad type class. Now imagine you're a package author who wants to support multiple versions of GHC. True, this is an extreme case of backwards incompatibility, but I think it's a major problem and thus it server as a counterexample to the claim. This is a real easy one... just have multiple modules that redefined the types for each GHC version and use CPP? I guess, ultimately, in my experience preserving backwards compatibility is not always possible and attempting to do so can cause more problems than just forcing users to make changes. Especially if the change is something like passing in a default value, it's pretty straightforward, and will save so much pain in the future. 
What's the usecase for overloaded lists? Honestly asking, never heard of this before
You seem to be overcompensating a bit too with all your accusations of "edge", but you have a point. OP, this is a hard journey for almost everyone, since almost all of us come from an imperative background. Breathe. You'll make it, Haskell or Rust or whatever it is that you end up wanting to learn. Maybe swing by #haskell on IRC whenever you feel something goes from easy to wtf all of a sudden? 
I mean it's not like a big issue in a specific piece of code. Just from time to time I do some arithmetic within a function without using a function like `length` that forces a specific numeric type, and I have `-Wall` / want to avoid type defaulting (in its current state). Other times I define a list within a function and consume it within that same function, such as with `fold` or `asum` or something. Although to be fair with those I can usually use `TypeApplications` on the consuming function. It also would be really nice for messing about in GHCi, now that I think about it that is a case that pops up a ton, as I obviously don't use explicit type annotations basically ever in GHCi, so ambiguous type errors actually pop up a lot.
Defining e.g. `Map` values as `[(0,"foo"),(1,"bar")]` without the `M.fromList` boilerplate.
I mean may as well allow overloading of all the built in literals. There is no downside as long as it is opt-in, and I am not proposing opt-out overloads for those literals anytime soon.
The documentation with some simple examples is here: https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#overloaded-lists
If you are using `Vector`, `Seq`, `Set`, `Map` and other data types a lot, and don't want to have to put down lots of `Map.fromList` functions. I mean: `[(1, 2), (3, 4)] @(Map _ _)` is more concise than `Map.fromList [(1, 2), (3, 4)]`, so even when type inference doesn't help you out, you still end up with more concise code. And when type inference does kick in you obviously need way less code.
[this](https://wiki.haskell.org/Foldable_Traversable_In_Prelude) It's a pretty neat change that generalized a lot of functions. But it unfortunately did some slightly weird and unintuitive stuff. Specifically `Data.List` contains a lot of functions that operates on all `Foldable`s instead of just `[]`.
I think there should be a more specific abstraction for "collection length". For example class Sizable a where size :: a -&gt; Int This class should be used only with some sort of collections. And when `size` is used it means you want collection size and not just length in any mean. Maybe there could be a Either instance size (Left _) = 0 size (Right x) = size x But I am not sure. Is there any type class with Either instance similar to length (Left _) = 0 length (Right x) = length x Does it break any laws?
One option would be to put this in HLint, where custom warnings on specific named functions is the default, not a special case. 
It's difficult for hlint though, because hlint doesn't have typ einformation.
… and it's done! I was able to successfully eliminate the issue in the new API. https://hackage.haskell.org/package/rawfilepath-0.1.1/docs/System-Process-RawFilePath.html Again, thanks for pointing this out!
Very true - would be hard to have the "apart from on List" exemption.
The distinction is useful when you are matching on a singleton GADT. Doing the match with do notation gives you a dependent match exactly like a case expression would, but doing the match with an irrefutable match in a let expression does not. For example: Refl &lt;- sameNat (Proxy @a) (Proxy @b) -- a ~ b is now part of the typing context. Vs: let Just Refl = sameNat (Proxy @a) (Proxy @b) -- We know nothing because of laziness, -- but even using a result from inside the -- match wouldn't bring it's context into -- scope.
A few points: (1) it shouldn't be hardcoded; users should be able to program it themselves. (2) it probalby shouldn't be in `-Wall`. (3) It's not "how often" the problem occurs, but how difficult it is to diagnose when it occurs. This seems like one of those "very hard to track down" problems.
&gt; I think there should be a more specific abstraction for "collection length". `StableFactorialMonoid` from [monoid-subclasses](http://hackage.haskell.org/package/monoid-subclasses-0.4.3.1/docs/Data-Monoid-Factorial.html) works well for that. **Edit:** Actually, scratch that, because there are a couple of intances that people might consider risky, like FactorialMonoid () (FactorialMonoid a, FactorialMonoid b) =&gt; FactorialMonoid (a, b) Still, `StableFactorialMonoid`is useful as a constraint for functions that want to work with a monoid that has a well-defined notion of "length", but don't care about the particular monoid. Imagine, for streaming libraries, a version of `take` that works over the accumulated length of the yielded elements. 
&gt; That's it! No BC-breaking flag days, no poisoning functions, no getting rid of FTP, no dropping instances: just a new pragma, and an opt-in warning that will let people who want to avoid these bugs. How is depending on a new pragma more backward-compatible than simply using a custom prelude which replaces `length` with a list specialised version? With a new pragma you'd still have to clutter your code with CPP to support pre-typeapp GHCs
Having to annotate all our show calls with the type of what we are show'ing seems like a bridge too far, although by the logic of my proposal, this seems to be what I'm suggesting! (Perhaps the difference here is that picking up the wrong instance here is "wholly unsurprising". It's obvious that you printed the wrong thing.)
What does brittany bring over hindent? I thought the reason for mentioning `gofmt` as a Good Thing was that it was officially sanctioned as the one true way. If so, Haskell loses one point in that battle for every new formatter it gets ;-)
Tangential note: if your goal were merely to obtain the `length` of a container within a `Right` (as opposed to addressing the usual concerns about `length @(Either _)`), you might do it with: GHCi&gt; length . Compose $ Right [1..10] 10 The [relevant instance](http://hackage.haskell.org/package/base-4.9.1.0/docs/Data-Functor-Compose.html#t:Compose) is `(Foldable f, Foldable g) =&gt; Foldable (Compose f g)`. A corollary is that this use case depends in an essential way on `Foldable` for `Either` being what it is. (In particular, if `xys` is a nested foldable with the inner containers having equal lengths, `length (Compose xys)` results in the product of the lengths of the two `Foldable` layers.) *(Edit: in the first half of this reply there used to be a few remaks on design decisions about laws, but I realised it was built around a slightly flawed example.)*
To be clear, there is exactly one pre-typeapp, FTP GHC, and that's 7.10. So if you need to support 7.10, yes, don't use type applications, and suppress the warning. This doesn't seem too bad to me?
Having s type aware linter sounds valuable, and sounds like a good place to put this idea. ghc-mod or intero could take up the role of type aware linting.
&gt;they are basically acting as tuples. But you cannot map over tuples. For javascript arrays it would be better to use something like anonymous union type. x :: List (Int | Double | String) list = [1, "foo", 1.0] y = map list (\case x : Int -&gt; "int", x : Doulbe -&gt; "double", x : String -&gt; "string")
That's a nice trick!
&gt; Haskell can't give me a Value with any arbitrary type in it because sum types are closed. You can create a type in Haskell that contains pretty much every value you can think of. It doesn't matter that the sum is closed. data Any = Primitive Prim | Object (Hashtable Any) | Method ([Any] -&gt; IO Any) | Anything (ForeignPtr ()) Everything you can do with a Python value you can also do with a value of `Any`.
This isn't really any different to just adding `lengthList`, `lengthMap` and so on, is it? The previous ("overly") general `length` still exists, but you have the choice of using a type-refined version too now. It does mean that everyone else providing `Foldable` instances also has to provide these functions, I guess. I also wonder if the trouble people have with `length` on `Either a` and `(a, )` are really that these are instances on `Representable`* functors. It seems we really want `length :: !Representable f =&gt; f a -&gt; Int`, but of course we can't actually say that :( *`Representable` essentially says that a functor contains exactly `n` elements. `(a,)` contains one value, `Either e` contains one value, etc. `data Two a = Two a a` is also Representable, and it's questionable that you'd want to call `length` on this, though `sum` is kinda useful (`sum . fmap square` giving you the L2 norm of a vector).
&gt; It also would be really nice for messing about in GHCi Using an older GHC at work I deeply miss `TypeApplication` when exploring in GHCi. I avoid using `::` wherever I can since it usually means having to add parentheses in annoying places. I will also forget that my expression had a `::` and having to redo it, which doesn't seem like much but in REPL-time it is: &gt;&gt;&gt; foo :: Foo ... &gt;&gt;&gt; foo :: Foo &amp; pprFoo bzzzz you fail &gt;&gt;&gt; (foo :: Foo) &amp; pprFoo ... With `@` it's one less thing to think about.
I think the problem is that `Data.List` (essentially a "data type" module) exports generic functions (odd idea in the first place), which prevents people from simply being able to write `List.length` consistently as with `Vector.length`, `Seq.length`, etc. when they want a monomorphic type.
But `Either e` can contain one or zero values, isn't it so?
I think that's a matter of priorities. If you don't care about how code looks as long as it's consistent then one style is perfect. If you care more about expressing your own style than being consistent with everyone else then you need a tool that'll cater to your needs without infringing on your tastes.
But a tool that caters to my needs will *also* make the style consistent! It's only inconsistent across project boundaries, which I really don't think matters, especially since the tools automates it so that you have to do zero work to style for another project. Not to mention, a good tool will be used with default settings or with a few specific changes by most people, so it's still going to be fairly consistent.
It's good you never faced this issue. In our production code we experienced such problems after one developer replaced type from `[a]` to `Maybe [a]`. And later in code `length` was called on the value of this type. That really hurt us a lot, because we're developing distributed system and finding errors is really a pain. Especially such errors. Well, it's not robust enough in current situation to force everybody to use explicit type applications, because we all humans, someone can always forget. So in order to prevent such issues in future we came up in our custom prelude with different type class hierarchy , fully compatible with `Foldable`, but we are forbidding usages of `length` and company over `Maybe`, `Either`, `(,)`, etc. using GHC 8 type errors feature.
Which definitions are you talking about specifically?
I'm not sure, but I *believe* it's just for performance's sake. Polymorphic recursive functions are hard for GHC to specialize. EDIT: ~~Yes, just did a benchmark. Even if you explicitly tell GHC `{-# SPECIALIZE length :: [a] -&gt; Int #-}` for the fully generically defined function, it's still over three times slower.~~ EDIT: Turns out I did it wrong. The performance was the same when I did it right.
Somewhat randomly chosen: A lot of the functions in `GHC.List`, `Data.Bits`, functions like `mapM` and `sequence`. It looks like it is actually less common in `base` than I thought so I think I’ve just repeatedly looked at specific definitions where it is used. It is also used quite heavily in `vector` and `bytestring` which tbf are not directly part of GHC iirc but are still core libraries that I (and probably others) turn to when looking for guidance on how to write fast Haskell code.
...and having attempted to use the type system in Python 3, even the tools that are there are poorly designed.
Seconded! Very admirable, thanks for doing this!
But imagine if everyone used Brittany. For the most part, everyone would be consistent. There would just be a few configs that some people might change. I'd call that pretty consistent. Of course the same applies for a configurable Hindent, or Hindent as is. I actually like the idea of having one style that's almost universally agreed upon; but I do think it's silly to *require* that style to use the formatter, because the point of the formatter is to allow people to not think formatting. Besides, the real main reason that I prefer Brittany to Hindent is that I just like the output better. I feel like Hindent often produces code that reads like a long and thin diagonal, whereas Brittany tries to keep more on one line. Regardless, I don't think this is a matter of "consistency vs customization," because Hindent isn't nearly popular enough to be a consistency in the community. If it were, I'd be very likely to bite the bullet. But as it stands, I just don't prefer its output, and it's not a real convention.
Response to you edit: Wow, why is that? It seems like that should be completely solvable...
It would also be useful for `MonadComprehensions` f :: (Alternative m, Monad m) =&gt; m () -&gt; m () f xs = [ x | x &lt;- xs, x == () ] &gt;&gt;&gt; :t \xs -&gt; [ x | x &lt;- xs, x == () ] @[] \xs -&gt; [ x | x &lt;- xs, x == () ] @[] :: [()] -&gt; [()] &gt;&gt;&gt; :t \xs -&gt; [ x | x &lt;- xs, x == () ] @Maybe \xs -&gt; [ x | x &lt;- xs, x == () ] @Maybe :: Maybe () -&gt; Maybe () Maybe even `do`-notation barring problems with parsing foobar mx ys = do @[] x &lt;- mx y &lt;- my pure (x + y)
Ah that’s interesting, thanks!
Note you may not be seeing the whole story here. For example, it's plausible you had to explicitly provide the type annotation for this to fire properly (so it doesn't default to `Integer` anywhere..) or perhaps the specialize rule doesn't matter because the function was inlined and fused away, but the fused version still has the typeclass dispatch. These can be a little tricky like that.
Link gives 403 for me. *Wait, 403???*
Well yes. If you are using mixed data types in JavaScript array you either need to know on which position is what data type (which is essentially a touple) or you need to have a function that checks what kind of object it is currently working on which essentially boils down to sum types in haskell.
http://hackage.haskell.org/package/prelude-compat-0.0.0.1/docs/Data-List2010.html is a third-party packaging of the monomorphic `Data.List` module.
tl;dr: They implement an image processing algorithm with Repa and Accelerate, obtain good speedups with the former but performance degradations with the latter. They then do some analysis and conclude that the quality of the code generated by Accelerate is poor and that pre-processing actually takes more time than GPU processing. They do not seem to have researched the library very thoroughly, as they make suppositions on its internals that should easily verified by looking at the code: &gt; This result suggested that in the preprocessing, the Accelerate runtime performed scheduling of CUDA kernel execution &gt; (and maybe functions as units). It also looks like the Accelerate code has been more-or-less directly ported from the Repa code. However, GPUs being very different from multi-cores, it's unlikely that such an approach will lead to best performance (but I may be wrong). I'd love to read what the Accelerate people think about it.
Forgive my ignorance, but what would the purpose of this be? Just avoiding the need to explicitly convert from Int?
imo it should return Word or Natural for semantics sake. 
I use length polymorphically and would find this annoying. Here's a neat trick: for any decent sized project, use your own Prelude. Having your own prelude is a great way to get around virtually every problem with Haskell's standard lib, and it's real easy to just monomorphize `length` right there. Your own prelude can have modules like `Our.Data.List` which exports monomorphic list functions.
&gt; It also looks like the Accelerate code has been more-or-less directly ported from the Repa code. However, GPUs being very different from multi-cores, it's unlikely that such an approach will lead to best performance (but I may be wrong). Accelerate isn't supposed to be *too* hardware-specific, though. They even have non-GPU backends these days (through LLVM) that are supposed to perform pretty well. I'd expect high-quality Repa code to be fairly similar to high-quality Accelerate code.
Ya indeed, clearly I was far too pre-coffee with that comment. &gt; [a] isn't Representable either! Yes, this was kind of my point. In this case `length` *does* make sense, because it actually calculates something.
This comes at the low low cost of preventing your code from compiling on anything older than GHC 8, requiring an extension to use a Prelude function at all, and requiring another ScopedTypeVariables extension if you work polymorphically with Foldable. =( I confess I'm not a fan.
Honestly, I've never found a use for `Foldable` on 0-1 containers like `Maybe` and `Either`. If I want to inspect them I'll pattern match or use the eliminator functions. So for me, I think poisoned instances is in the goldilocks zone, but maybe it's just me :)
Excellent, I was planning to write up a proposal for this. This labor model is the core of my company, and extremely effective for certain goals. You'll need a bit of incentive/communication tweaking to make it profitable but I'm sure what you propose will be valuable to some. I'm happy to discuss details or put you in contact with bounty seekers if desired
Great idea.
According to [Data.Int](http://hackage.haskell.org/package/base-4.9.1.0/docs/Data-Int.html#t:Int) an `Int` is guaranteed to be in the range of [-2^29 .. 2^29 - 1]. The [implementation](http://hackage.haskell.org/package/base-4.9.1.0/docs/src/Data.OldList.html#genericLength) of `genericLength` is not just `fromIntegral . length`, but a reimplementation of `length` using the given `Integral` type for counting. I guess this allows the functions of some containers to work with more than 2^29 elements by choosing something bigger than `Int` as index type.
GHC does many things you should not do, for good reason. Should you, as a user, vendor every single dependency of yours like GHC does? Probably not, unless you really need it. It's true that the use of `INLINE` in GHC itself might give way to a "perception" that it's OK to use all over the place, but even then it's flawed logic. The reason people pay attention to "ghc people use `INLINE`" but not &lt;the million things GHC does that you should not do&gt; is because people want to quickly make code faster, with little effort. That appeals to everyone and it seems so "easy". That said `INLINE` is a necessary evil as far as I'm concerned, and it's very useful for users sometimes, too (much more than vendoring is). I'd prefer if GHC had better cost models for all this stuff. Unfortunately, that is a ridiculously hard problem to solve and will require huge amounts of work, probably.
It's also not exactly noob friendly that length xs / 2 doesn't type check though. EDIT: I'm no longer so sure this was such a great example. I can't think of why I'd want to do floating-point division on the length of a list. And of course, length xs \`div\` 2 *does* type check, so that example kinda backfired. But I do remember being confused by something like this when I was learning Haskell, so there must be better examples! I guess this indicates why this doesn't have an obvious answer.
Luckily in practice we get a 64-bit Int. Otherwise a lot of my code wouldn't work. Sadly, that just means I'm writing unsafe code, but its really hard not to given one often can't write a good 'length' one's self. I wish it was different.
You don't even need GADTs or DataKinds for this: newtype Chan k a = Chan (GoChan a) data Read data Write make :: Int -&gt; IO (Chan Read a, Chan Write a) send :: Chan Write a -&gt; a -&gt; IO () recv :: Chan Read a -&gt; IO (Result a) close :: Chan k a -&gt; IO()
Yeah, I was having some trouble making Notepad go beyond .txt I've been using Sublime all day today and it really helps me out Thank you! 
Thank you, It's really worth the "bookmark"
Yes, I also think that's the correct type (and at the very least we should have genericLength in Prelude). But how often do you need to use the length of a list in a floating-point expression? In this case, the current type suggests that maybe you meant to use div instead, which is probably more likely. This is why I think my example was bad. I have needed genericLength before though; I just can't think of a compelling example off hand. And we also need to motivate why Num is the correct typeclass and not Integral.
Is `Vector n a` Representable with the input being `Fin n`? 
Instance resolution only looks at the head (the part after =&gt;). After an instance is chosen, only then is the context looked at. Your instances are the same as far as the instance search is concerned. The reason why this is is probably (as in: I don't know) analog to why you get exhaustiveness checking for patterns, but not for guards. Instance heads are just checked for structural compatibility while the context can have almost arbitrary expressions.
Isn't it possible to have two generic lengths, `length` and `length'`. Or `length` and `lazyLength`. Since IIRC the reason why `genericLength` is so slow is because it supports lazy numbers like lazy naturals, but it seems like you can have something of type `(Foldable f, Num b) =&gt; f a -&gt; b` that is strict and not too slow.
&gt; Polymorphic recursive functions By that do you just mean recursive functions that happen to be polymorphic? Because polymorphic recursion itself (where the type changes on recurse) is a whole different beast, that also happens to be very hard to optimize.
This paper goes waaay over my head, but I would love to see some "lay" explanations of: - What would be an example of "terms whose types are provably equivalent, but not definitionally equivalent"? - What is the difference between definitional and propositional equality? - What does it mean to "strenghten" the definition of equality? As in "The assumption rule strengthens this definition of equality considerably compared to intensional type theory" - What is "syntactic" type theory, how it differs from homotopy type theory?
How do you *know* that the generic diff *actually* worked for all cases? My point is that it's quite possible that you actually **couldn't** figure it out in Python, it's just that you (after enough tests) *thought* that you had.
&gt; Haskell can do this using any one of its extensible variant solutions, but I wouldn't want to use any of them in production. Why? Just curious. AFAICT many of them basically subsume row types... and with Dependent Types coming along they'll probably even have nice internals at some point. (FWIW, I think Haskell people are right to be extremely wary of subtyping given the inference problems it presents. Just look at how pitiful and ad-hoc Scala type inference is. There's a recent paper/PhD Thesis which claims to have solved type sub-typing+HM type system inference problem, but Haskell's type system is a lot more complicated.)
Okay thank you! I guess it makes sense in terms of implementation because you'd need to discriminate on superclasses otherwise.. I wish I could do it :P
Makes most sense to me. Let the library maintainer flag which instance methods of overly polymorphic types are non intuitive, most likely from feedback through users.
Hi, I've looked at this before. Both the project and tutorial are excellent: The tutorial covers not only how to use the project in detail, but also doubles as a really great tutorial on sound synthesis. And because it's in code you can actually see how common effects like [reverb](https://github.com/spell-music/csound-expression/blob/6a32a12b7b1427c0275f53d4dca4963c3c65ecc5/src/Csound/Air/Fx/FxBox.hs#L1060) are put together.
Thanks. One thing I find difficult when reading about type theory is differentiating "judgements" from things that are "in the types themselves", so to speak. Equality is one thing that can exist as a judgement (definitional equality I guess) but also as a type in the language. And "entailment" seems to exist as the level of judgements, but also seems related to implication/function types in a way.
 class TmShow gen where tmshow :: Tm gen -&gt; String instance TmShow Char where tmshow (Elem gen) = [gen] tmshow (Inv gen) = [gen, '\''] instance TmShow String where tmshow (Elem gen) = gen tmshow (Inv gen) = gen ++ "\'" instance (TmShow gen) =&gt; Show (Tm gen) where show = tmshow
Sweet! Thank you :D
&gt; Nothing says that the same author will supply both instances As long as `Pointed` and `Apply` are in the same package isn't it basically required that the same author provide them unless you use orphans? And to me it seems like orphans in general are way more of a problem than the risk of an unlawful interaction. I still think some sort of "blessed-package" system would work to avoid orphans completely. So either the instance is defined in the package that defines the class, or in the package that defines the type, or in a package that both packages agree is the blessed one, otherwise the instance is explicitly forbidden.
Which tech would you use for your server? `servant` or `yesod`? 
Most of them are unsatisfactory in some way, e.g. * Relying heavily on TH or other code generation * Breaking inference and usability * Very slow with large types * Hard to wrap my head around (finally tagless, I'm looking at you) I'm looking forward to the extensible data that's in the works, but it still doesn't feel right. Regarding subtyping, I think it's a bad fit for Haskell, but I don't think that Scala is a good example of the way forward, and I think there's a lot of research directions that the PL community shies away from because of what amounts to "ewww! subtyping" I was doing research on building a language with a type system entirely based on completely inferred structural subtyping with polymorphism, and I was never adequately convinced that it was a bad idea by any of the arguments against subtyping.
It is a typo indeed.
I like this idea of irrelevant quantification of types being a core part of the design. The amount of times I've actually pulled a type-level computation down to term level in dependently-typed programming is... probably zero. Yet languages like Agda retain all of that type-level computation clutter in their output in order to give me this power which I never use. I'm quite content to have type-level computations irrelevant (and thus absent from generated code) by default!
Lucid presentation by one of the masters. Can't recommend it highly enough. 
[removed]
Were these libraries evaluated in parallel?
&gt; Especially when only income would become more of a competition with others, instead of getting paid to work in Haskell on projects that I'm interested in. So, the idea is to NOT have multiple people compete for the same bounty. I was thinking of being transparent about the following information regarding the problems &amp; bounties: * open problem OR being worked upon by someone * bounty amount * will become open again, if the bounty is not claimed till such-and-such date The last point is important because it forces the intern to focus and commit to some sort of timeline. My experience has been that interns (especially students) go off on multiple tangents and never end-up completing. And then life/course-work catches-up and stuff is left incomplete.
I see what you did there
But often you need `Vector.length`, or `Map.length`, or `Set.length`. In our company prelude we re-export `Vector`, `Map` etc. types, so `length @Map` would be more ergonomic (until we can re-export quantified "sub-modules"). Yet I won't do `length @[]` until compiler can help me to spot where I have to add such annotation.
That is really cool! &gt; sortTraversal (each . traverse) ( [11,-4,3], [9,6], [7 ,5] ) ([-4,3,5],[6,7],[9,11]) 
Off-topic: Please share your experience of using Purescript. We are, as of now, undecided about the technology to use in the frontend (with Haskell in the backend).
Oh interesting. That's definitely equivalent. But of course mine works with anything using applicatives, not just traversals.
This is so cool. Can we put this in Prelude? EDIT: so given Discrimination, we can get a normal form for any traversable in linear time?
I'm using purescript to front a Haskell application. If you use servant on the backend especially, it's a no brainer. servant-purescript reflects any types you want to the front end, and will even build client functions for accessing your servant endpoints. I've changed types on the backend, and the url for servant endpoints, and the frontend has been just a case of fixing the type errors. Purescript also has a really nice development experience in emacs, better in some ways than haskell. AMA :-)
You got here before I could even ask you to respond on our behalf! You're doing Gods work. ❤️
Neither can I! However I thought that the trick of using `State` with `traverse` was just a stunt, but I found it useful a few times, so perhaps this will be the same here.
Yea using `State` with `traverse` to modify the contents of a collection unsafely is pretty gross. But it's usually easier to do things that way; rewriting algorithms to work on `Ap (Mono x y) a` is a little nontrivial, since you have to be careful to leave the `Pure` terminator untouched.
&gt; EDIT: so given Discrimination, we can get a normal form for any traversable in linear time? Are you talking about [this library](https://hackage.haskell.org/package/discrimination-0.2.1)? Looks like I've got a bunch of reading to do =P
The machine behind`partsOf`, the [`Bazaar`](http://hackage.haskell.org/package/lens-4.15.1/docs/Control-Lens-Internal-Bazaar.html) is an `Applicative`, so you can plug it in about everything too.
Yeah, watch [the video explaining it](https://www.youtube.com/watch?v=cB8DapKQz-I) for details. If we can combine that with support for sorting arbitrary traversals and then abandon the type/term distinction (which the current top post gives imminent hope for!), then I think [I'm all set](https://www.reddit.com/r/haskell/comments/5vz9sw/maplike_data_structure_with_normal_form/)!
You may find this interesting - http://hackage.haskell.org/package/wai-extra-3.0.19.1/docs/Network-Wai-Middleware-Routed.html#v:routedMiddleware In general, wai doesn't have a native routing solution, so applying different middleware to different routes depends on what you are using for routing. You can usually just do something like - selectiveAuthMiddleware app req | isAuthRoute req = authMiddleware req app | otherwise = app 
Only the reader state and writer parts are a little out of date
&gt; Purescript also has a really nice development experience in emacs, better in some ways than haskell. Oh, that's interesting! Do you have any pointers to packages/tutorials etc? I do use PureScript on the frontend as well and I mostly do love it, although my experience has been a bit spoiled mainly by: * Bower &amp; co : Definitely the lack of a lock file (but apparently somebody recently is trying [to fix this](https://github.com/infusionsoft/bower-locker)) means that reproducible builds are a no-go. Hopefully things like Chris Done's `purify` can help in that regard. * The language and the ecosystem are a bit of a moving part: this is totally justifiable considering the compiler itself didn't even reach 1.x, but nevertheless upgrading from 0.9 -&gt; 0.10 was such a big job that I'm now running in production with an outdated stack &amp; compiler. Sigh. * On Emacs, I have found the whole experience totally unsatisfactory. I saw there were a couple of packages out there, but nothing seemed to be as intuitive and hassle-free like, say, `intero`. I think that these days I'm using [this one](https://github.com/emacs-pe/purescript-mode), which gives me pretty much only syntax highlighting. Hopefully these days things changed for the better! I would be very curious to hear you experience! 
It's not a legal [operator](https://www.haskell.org/onlinereport/haskell2010/haskellch2.html#x7-180002.4). There may be other rules at the type level, I'm not 100% certain.
Yep! Sadly `Fin n` is a bear to use practically in Haskell. =/ This leads to things like the `linear` library lying and using Int as the "representation" of `V n a`. This makes me sad. =(
As you continue to push in this direction you wind up deciding things like MonadPlus aren't necessary. e.g. You could always require that Alternative + Monad = MonadPlus, etc. But can you prove that the MonadPlus laws are implied by the Alternative laws? I can't. Comonad + Apply = ComonadApply is tempting, heck look at the names, but the interaction between &lt;*&gt; and extract is non-trivial to prove and doesn't hold for many instances of Applicative that happen to be able to be Comonads. Also the symmetry laws required for ComonadApply are stronger. Do we rule out making instances of Applicative (and hence Apply) and Comonad for the same type just so we can have magic laws come into scope when these two classes are applied to the same type? Then you run into the dilemma of "where do you define the operations that such new laws imply that you can have?"
I recently gave a talk comparing GHCJS, Elm, PureScript and TypeScript: https://www.youtube.com/watch?v=BZfvoW8wixU
We are maintaining custom prelude not just because of `length` error, don't get me wrong :) But I'm totally agree that having some hlinting for such cases or even type-directed hlint would be awesome.
this looks exciting! What's the roadmap? When can we expect to play with these features? EDIT: i am aware that it might take some time.
So it is. I need to train my Emacs to NEVER use tabs...
Also took Substructural Logics. (Link: http://www.cs.cmu.edu/~fp/courses/15816-f16/schedule.html)
Hmm, that's a fair point. I guess you should probably only do that kind of thing in situations where the laws of the subclass are implied by the laws of the superclasses. Such as `JoinSemiLattice` + `MeetSemiLattice` =&gt; `Lattice`. So something like: class (JoinSemiLattice a, MeetSemiLattice a) =&gt; Lattice a instance (JoinSemiLattice a, MeetSemiLattice a) =&gt; Lattice a is very reasonable IMO. But you are right that often new laws are desired that are not free. It seems like the three main options are: 1) Have a fairly course set of classes, which I personally really don't like. I would be 1000x happier if `Semigroup` was a superclass of `Monoid`, and if `Num` was broken up into `Ring`, `Semiring`, some nearrings and seminearrings. Having things like `(+) = (++)` and `(*) = liftA2 (&lt;&gt;)` as well as allowing parsers and regexes and matrices to be manipulated using `+`, `*`, `one`, `zero` and for some a bit of `/` and `-` sounds amazing. I mean who doesn't want to be able to have `"x" ^ 10` be equivalent to the regex `x{10}`. You could also get things like integer multiplication to generalize `replicate`, although I suppose that is already possible with `&lt;&gt;`. But still this all sounds awesome, and I personally want to go as far as possible with it, `(+) = (||)` and `(*) = (&amp;&amp;)` for booleans sounds fun as well. 2) Have different operators for different parts on the chain: mempty :: Pointed a =&gt; a (&lt;&gt;) :: Semigroup a =&gt; a -&gt; a -&gt; a mappend :: Monoid a =&gt; a -&gt; a -&gt; a although I would not have them defined in the class itself, you should be explicitly prohibited from overriding them: class (Semigroup a, Pointed a) =&gt; Monoid a mappend :: Monoid a =&gt; a -&gt; a -&gt; a mappend = (&lt;&gt;) 3) Just accept the fact that inference might give you something more general than you intended, and just tell users that if their explicit type signature has `(Semigroup a, Pointed a) =&gt;` in the type signature, that they probably want to fix it. Any types used within the function that aren't exposed in the type signature you probably don't have to worry about, as in order for type checking to work they have to be unambiguous (modulo the very limited defaulting rules), and if you know the exact type you are using you should know what `&lt;&gt;` and `mempty` and such are going to do. I guess maybe some weird type family or functional dependency stuff could maybe cause issues in an edge case where you miss out on laws you expected, but I highly doubt it will come up more than once in a blue moon. Honestly I think this route would probably work fine. You could even have some linter tools to look for type signatures (and maybe inferred types as well) that contain something like `(Pointed a, Semigroup a)` and warn you about the lack of guaranteed agreement.
Glad it was helpful! Also, I see what you did there :-)
Good to hear!
By "text editor" we mean vi (the only visual editor required by the POSIX specification; or a modern variant like vim or neovim) or emacs (the GNU-approved editor of choice; or spacemacs). All others are pretenders. :P
Been reading about Transient today, how does it handle client-server communication? 
You've made some good points for reflex, I'm considering trying it out along with GHCJS. How does it handle client-server communication? My main motivation for using Haskell on both front-end and back-end is to avoid writing repetitive glue code for the two to communicate. 
Well, it's "suggest" rather than "require", so you don't *have* to do it. But you're right, if someone uses this in their library, then all downstream code requires GHC 8
You might want to post that here to: https://www.meetup.com/berlinhug/
I've used it with both websockets and AJAX requests. My favorite is websockets because you can get strongly typed communication with very little overhead like this: https://github.com/mightybyte/hsnippet/blob/master/shared/src/HSnippet/Shared/WsApi.hs With the AJAX approach you can also use [servant-reflex](https://github.com/imalsogreg/servant-reflex) to eliminate a lot of the boilerplate. It's not as nice as with websockets, but it's still pretty good.
&gt; Wait a couple of GHC releases. This one is coming. =) Awesome! Do you know if we will also get anything like `SemiAlternative`? As that is what I am currently wishing I had for my compiler's parser. The way I am doing error handling in my parser is by keeping track of the maximum (in terms of line and column count) error encountered at any point in time, and if the whole parse fails I return that. But since I am using `&lt;&gt;` and `&lt;|&gt;` I need to define an empty error, which I currently have as just `UnknownError`, which I don't like as it should really never be returned, all errors should have locations and it would be nice to statically enforce that. Admittedly using a `SemiAlternative` it might be a bit more complex than my current solution as I will need something like `Either3 Error (Error, Program) Program` instead of `(Error, Maybe Program)` which is simple with `Writer` and `MaybeT`. But I think it would probably be worth it to make sure I don't screw over a user with a completely useless error message. &gt; Fixing Num is a "big freaking deal" we've really been unable to make any headway on. Hmm, we really really need a better system for pulling things into new superclasses, as we have talked about earlier. Because a first step that seems like a no brainer is making a more disciplined superclass that doesn't contain `fromInteger`, `abs` and `signum`. We could call it `Ring`, although I suppose due to people using it for things that don't form a `Ring`, we could perhaps downgrade it to some sort of near-ring or even cheat and call it `Ringlike`. One annoying thing is basically every formal name for a ring-like structure needs an additive identity, so we would probably want `zero :: a`, which might make migration even harder. But something like: class Ring a where (+) :: a -&gt; a -&gt; a (-) :: a -&gt; a -&gt; a (*) :: a -&gt; a -&gt; a negate :: a -&gt; a zero :: a class Ring a =&gt; Num a where abs :: a -&gt; a signum :: a -&gt; a fromInteger :: Integer -&gt; a &gt; It seems to me the danger with #2 (not being able to override them) is that you can miss optimization opportunities only exposed by knowing about the combined laws. I thought about this, and I am unconvinced. As far as I know the main ways you can do law based optimizations are either: 1) in the implementation of the function for a specific instance: But since you are dealing with a concrete type in this situation, you know exactly which optimizations will give you the same semantics. So you know for example whether `&lt;&gt;` is commutative, even before you implemented some sort of `Group` typeclass. I guess as a sort of proof by contradiction: if you implement the best `&lt;&gt;` you can, and then use additional laws to make an even faster `&lt;#&gt;` (for groups), then you can just copy that implementation back to `&lt;#&gt;` with no type errors, violating our assertion that our `&lt;&gt;` was the best we could do. 2) rewrites based on rules: Those seem unaffected by whether the functions are defined within the typeclass or entirely separately, either way you can define all the rules you want, such as `mappend mempty =&gt; id` but not `(&lt;&gt;) mempty =&gt; id`. Although I will admit this is a good argument for having two separate operators instead of just telling users to not always use the inferred type. 3) I don't think there is a (3), as far as I know GHC doesn't do any optimizations based on asserted or derived typeclass laws. Now I kind of wish it did, as for example if you knew that `+` was a commutative monoid. Then you could change (if you wanted to, a.k.a if you know `+` is strict): sum [] = 0 sum (x : xs) = x + sum xs to: sum = go 0 where go n (x : xs) = go (n + x) xs go n [] = n and IMO the first definition looks much nicer and is a fun way to define things. Now if GHC did do typeclass based optimizations, then hopefully it would be smart enough to fully optimize situations where the type is known based on all laws related to that type, although that may not be trivial, hopefully GHC can sort of go backwards to find laws, so since `(&lt;#&gt;) = (&lt;&gt;)`, then any `&lt;#&gt;` laws are `&lt;&gt;` laws too for that type. I would also say that in situations where the type is not known and the function type is `(Pointed a, Semigroup a)` then it is fine that `Monoid a` based optimizations aren't applied, as the user explicitly put that they wanted a pointed semigroup, and again linters should warn users about that plenty. &gt; ... Profunctor ... I guess I should weaken my statements from "functions that can be derived from superclasses should not be in the subclass" to "functions that are defined exactly in one superclass should not be in the subclass". So `&lt;#&gt;` should definitely not be in `Group`, but `dimap` is fine to be in `Profunctor`. I guess the above is just an extension of the very general issue that functions composed cleanly and generically in terms of some mathematical rigorous primitives are often quite a bit slower than functions optimized for specific types. Hence why `Foldable` is so damn massive. One possible idea I have for that is to have a closed section of each typeclass (so for example `Foldable` would just have `foldMap` and `foldr` or something like that), and an open section of each typeclass where users can add arbitrary pseudo-members of the typeclass that can be defined purely in terms of primitives of the typeclass but that can be overloaded for whichever types you want for optimization. Now you would probably have to put every pseudo-member in the dictionary at compile time, as otherwise polymorphic recursion and friends will cause big problems. The bigger issue is probably that if not done safely the above is basically a form of overlapping instances, so you would want to disable any orphan-like behavior, which has all the downsides that the current orphan situation has. I really wish we had blessed package system for orphans, I still think such a thing would work very well. But without it then any pseudo-members defined in one package cannot be given optimized implementation by other packages. Now even with these issues I would say that if implemented properly it is no worse than the current situation (as defining all the pseudo-members alongside the class would be no different than putting them in the class), and sometimes it will be better. Then `dimap` would be a pseudo-member of `Profunctor`, which still means you would want to define it on top of `lmap` and `rmap`. So it doesn't fix that (probably unfixable in the general case) issue, but the situation would seem a bit cleaner to me. Honestly I got a bit distracted and was focusing largely on how to make `Foldable` not so huge. But yeah you are right that `dimap` must be associated in some way with `Profunctor`.
`partsOf` here basically does the "evil" partial thing you bemoaned at the start of the post: Draining things out into a list and then putting them back. It can be made "impartial" ok, er.. total, by saying if the list runs out to use the present value and not checking if the list is emptied. This is actually the difference between `partsOf` and `unsafePartsOf` in that the latter will fill in bottoms if you run out of entries and the former leaves the data you had alone (but then can't change types) if it runs out early. This does call to mind that `lens` really should have a `sortOf` combinator in its repertoire, though.
I suppose that you mean browser-server communications, because two servers (nodes) can also communicate. The JS program compiled with GHCJS is sent to the browser, then it opens a websockets connection. Then the most useful primitive is `atRemote` wich execute his argument in the server and return the result back to the client (or viceversa, see below). The communication transport the variables necessary for executing the computation remotely. There is no explicit serialization neither communication. All is done implicitly. `atRemote` can be executed inside itself, so a computation can jump from server to client and back. So a browser can be controlled by the server or the other way aroud. But the execution starts in the browser. There is very little data to transport to execute remotely. In the other side, streaming and reactivity in both directions is included with no additional considerations with `atRemote` and other primitives. Isn't awesome? In the other side, there is an experimental template editor to generate static HTML templates. The server can execute a rest route and bring the corresponding page template and the JS code to the browser, so web crawlers can find something to read. Mi goal is to make it work in the browser or the server transparently. currently all rendering is in the browser, and different templates can be created for different page routes in the server.
Eggs + sausage + rye toast
No. I'm an undergrad and AFAIK it's mostly in the realm of research right now. But there are applications for designing languages for concurrent programming as well as typed assembly languages. 
How many Haskellers are there in your company?
What a wonderfull time it is! It is so exciting that we are able to see lectures given by patriarchs of a scientific field we are doing. I wish we could see David Hilbert or Henri Poincare or Leonard Euler talking about their ideas!
You may find the function [`tails` of type `[a] -&gt; [[a]]`](https://hackage.haskell.org/package/base-4.9.0.0/docs/Data-List.html#v:tails) interesting, this is one way for a *‘high-level’ solution* &gt;&gt;&gt; tails ['a'..'g'] ["abcdefg","bcdefg","cdefg","defg","efg","fg","g",""] ---- First of all let's revisit how list comprehensions work, a list comprehension that doesn't do anything is `[ a | a &lt;- as ]` which equals `as` &gt;&gt;&gt; [ n | n &lt;- [1,2,3] ] [1,2,3] and same works with pairs &gt;&gt;&gt; [ (ch, n) | (ch, n) &lt;- [('a', 0),('b', 1)] ] [('a',0),('b',1)] If you want to check if the values are element-wise equal, you can &gt;&gt;&gt; [ a == b | (a, b) &lt;- [(1, 1), (2, 2), (3, 3)] ] [True,True,True] And then you can use [`and @[] :: [Bool] -&gt; Bool`](https://hackage.haskell.org/package/base-4.9.0.0/docs/Data-List.html#v:and) to check if all elements are `True` &gt;&gt;&gt; and [ a == b | (a, b) &lt;- [(1, 1), (2, 2), (3, 3)] ] True so this can be implemented as doPairsMatch :: Eq a =&gt; [(a, a)] -&gt; Bool doPairsMatch xs = and [ a == b | (a, b) &lt;- xs ] We can make `doPairsMatch` perform the [`zip :: [a] -&gt; [b] -&gt; [(a, b)]`](https://hackage.haskell.org/package/base-4.9.0.0/docs/Data-List.html#v:zip) doPairsMatch :: Eq a =&gt; [a] -&gt; [a] -&gt; Bool doPairsMatch xs ys = and [ a == b | (a, b) &lt;- zip xs ys ] With the [`zipWith`](https://hackage.haskell.org/package/base-4.9.0.0/docs/Data-List.html#v:zipWith) function zipWith :: (a -&gt; b -&gt; c) -&gt; ([a] -&gt; [b] -&gt; [c]) zipWith :: (a -&gt; a -&gt; Bool) -&gt; ([a] -&gt; [a] -&gt; [Bool]) zipWith (==) :: Eq a =&gt; ([a] -&gt; [a] -&gt; [Bool]) doPairsMatch :: Eq a =&gt; [a] -&gt; [a] -&gt; Bool doPairsMatch xs ys = and (zipWith (==) xs ys)
Really depends on the maintainers. I've worked on a fairly large Haskell code base and a lot of it was incredibly clean. But there were deprecated functions everywhere. The issues were that the code that was being written was slowly fixing issues by replacing old code but the developers didn't have the time (or emotional investment) to track down all the uses of the old versions of the functions they were rewriting (which were just different enough to break things). This meant that until all call sites were updated they couldn't really be removed. So, while the code was readable and useable and fairly easy to work on, there would often be conversations about the benefits of each of the three different internal functions for generating a biased float. 
"Haskell", no. But see https://hackage.haskell.org/package/ClassLaws, https://hackage.haskell.org/package/quickcheck-properties-0.1.
Agreed. From the linked GitHub page: &gt; Python 96.8% Shell 1.5% Other 1.7% From the linked StackOverflow page: &gt; Tech Stack python distributed-computing mongodb blockchain database kubernetes amazon-web-services azure docker bigchaindb A conspicuous absence of Haskell.
You might also check lambdacube3d's [getting started](http://lambdacube3d.com/getting-started) page. After you grasped the basics check the second example the OBJ viewer: [HelloOBJ.hs](https://github.com/lambdacube3d/lambdacube-gl/blob/master/examples/HelloOBJ.hs) to install: `cabal install lambdacube-gl -fexample` For hard-core level you might check [lambdacube-quake3](https://github.com/lambdacube3d/lambdacube-quake3) also. There is a related presentation: [The innards of a purely functional 3D shooter game engine (influenced by Quake 3) ](https://skillsmatter.com/skillscasts/8733-the-innards-of-a-purely-functional-3d-shooter-game-engine-influenced-by-quake-3)
Totally agree. The only thing to check was the logic (which we could [should]) have been doing with quick check. It was really nice to be able to do. Most of the 'errors' were spec changes which needed to be adjusted so even that was a very reasonable cost to pay.
Thanks, I'm saving this (last time I wrote a Functor I ended by checking them mostly by hand).
Well, the idea here is to start from the beginning, and implement a protocol which does exactly what the app needs. The app is built in functional language, so the most natural way would be just to call remote functions and pass them some typed data. Like RPC, but RPC usually does not takes into account network latency and other side effects. We want to explicitly control network issues and other effects, so we added a little layer over simple function calls, which handles network errors, does serialization/deserialization, error checking, security checking, enveloping etc. Then we pass around some expression language AST, which encodes protocol semantics, using this layer. Data types can express everything the app needs - queries, commands (differentiated mostly by side effects they cause), compositions of them, versioning, anything else. Also, this protocol can be made fully duplex - there is no difference whether the client calls the server, or vice versa. In fact, it can be used for communication between (micro)services just on top of TCP. Or using websockets, HTTP, and so on. Underlying protocol does not matter. Transport encoding does not matter. Another example of encapsulation FTW :-) REST just does not intersects with all of this, it forces one to squash app needs into some ad hoc artificial form - why? Actually, I think REST does not intersects with requirements most (web)apps have, and I don't understand why it is being used. Must be thought inertia or what...
&gt; Really, subtraction and decrement should not be defined for unsigned types. I agree, perhaps a separate operator for subtracting unsigned values, that way you know to be careful and that you are dealing with a very partial function. If we did generalize `Num` to `Ring` and such, then `Word` not having `-` would actually come for free. As `Word` is not a group under addition, and thus `Word` would not be an instance of `Ring`, it would only be a `Semiring`. Back to thread at hand, I personally think it makes the most sense to return a `Num`, just like how `genericLength` does. For performance reasons we could perhaps have `length` and `lazyLength` (or some other name), with the following default implementations: length = foldl' (const . (+ 1)) 0 lazyLength = foldr (const . (1 +)) 0 As IIRC the reason `genericLength` is so slow is because it supports lazy addition, such as with lazy naturals. Which you almost never want, so unless we can basically guarantee that `lazyLength` is transformed into `length` whenever we have a strict `+` operator, then we should have `length` be strict.
The compile warnings if anything makes the solution better, as now you will always be warned about potential inefficiencies. You can always stick in an `@Int` when necessary. IMO it is absolutely a worthwhile tradeoff. Particularly since I usually care about correctness over efficiency (so `Integer` is often pretty nice, in practice it is rarely the bottleneck unless you are doing some serious math).
From what I know, React bubble up events to the top level where there is a giant event handler that try to render everithing when an event happens. Only the virtual DOM makes this feasible. That is why the React components "compose": they do not care for their own events, they add actions to that massive all encompassing event handler on top. Or alternatively said, the event handlers are put on top and everything is redrawn. It's a cheap way to avoid event handlers here and there and also allows to aggregate, not compose, elements. Since for me, composition means some form of computation flow. React, like al MVC framewoks, is only rendering, not computation.
What is unsafe about it? I have done that kind of thing a decent amount, such as updating names of things within a structure (primarily for compiling and optimization passes on the compiler IR) and using `State` to avoid duplicates. I also remember that writing `foreach :: (Traversable t, Applicative f, Num i) =&gt; t a -&gt; (i -&gt; a -&gt; f b) -&gt; f (t b)` required such a thing. Which is handy for things like: foreach myList $ \i x -&gt; do putStrLn $ "Index: " &lt;&gt; show i &lt;&gt; ", Value: " &lt;&gt; show x putStrLn "New value?" read . getLine Obviously a toy example, but I have found `traverse` and `State` very useful together. EDIT: Ignore this, I should have read the post first before guessing what ElvishJerricco meant.
Wait what now, can you give an example? Are we both just talking about the `State` from transformers and the `traverse` in the `Prelude`. To be honest I am unsure what the context of a lot of this is, I just saw this comment and reacted, is just using `traverse (foo :: a -&gt; State s b) (bar :: Traversable t =&gt; t a)` unsafe? EDIT: Oh wait, I'm an idiot, I should have read the post before glancing at the comments and reacting. The above seems fine.
Yea it's just getting elements of a list that you don't know is long enough that's unsafe.
That sounds awesome - I am totally with you on how unsuitable REST is to most web apps, and I am also baffled by how standard it is. What you've described is pretty much the reason why I want to switch to GHCJS and build isomorphic apps - I've read your blog posts on it earlier today and am sold on it! I've not had much experience with writing protocols before - would you be able to point me to some example code I could try out? (Or a tutorial?) Especially something that inplements the duplex functionality you've mentioned, that would be really useful for me to have a look at!
Nice. Since you started using Unicode for ∀ and ∃, you might as well go all the way and use →, ∧, ∨, ≡, ×, ≥ :)
That's not a bad call. I don't like unicode in actual code (at least for now, I could maybe be convinced that a small subset of unicode wouldn't be the worst idea) but for stuff like this it is pretty nice. EDIT: Done
I wish! Haskell's (at least base / the default prelude) numeric tower is not so great, I would love it if breaking up hierarchies of classes was easier, that way we could start bending the numeric tower to look more like this. This is the actual mathematical hierarchy of things (or a subset of it). Haskell's numeric hierarchy is more about float vs real vs rational and so on. Which don't get me wrong is good to have, and even in my ideal world things like `pi` `sin` and friends would probably (I think anyway) have similar types. But IMO `+`, `-`, `*`, `/`, and probably things like `abs`, `signum`, `mod`, `rem` and more can be separated and made into more mathematical and law based typeclasses.
Oh _that's_ where those semicolons and braces all come from!
How are you handling the large output size from GHCJS?
I've been working on an Accelerate computation and I was curious to see if I could achieve better speeds with Repa. It looks like it's not the case: benchmarking Accelerate/Vector/1024 time 4.367 ms (3.823 ms .. 5.058 ms) 0.833 R² (0.753 R² .. 0.906 R²) mean 4.881 ms (4.519 ms .. 5.426 ms) std dev 1.335 ms (1.026 ms .. 1.728 ms) variance introduced by outliers: 92% (severely inflated) benchmarking Repa/Vector/1024 time 163.7 ms (139.5 ms .. 208.8 ms) 0.945 R² (0.844 R² .. 0.998 R²) mean 148.9 ms (132.4 ms .. 166.3 ms) std dev 23.92 ms (14.80 ms .. 35.37 ms) variance introduced by outliers: 41% (moderately inflated) 
Proportional to the amount of time the developers put into keeping it clean. Haskell's purity and strong types make it easier to maintain programs no matter how clean they are. But in terms of cleanliness, the second law of thermodynamics applies to code just as well as to the physical world.
Finding them is easy, understanding and accounting for the slight differences in the logic over a large code base is harder. We each got rid of a few calls each week or so (while writing and updating the code to add features and fix actual bugs). They will probably find better ways of solving the problem this or next year. The team I was on is pretty innovative. 
I don't think you can blame that one on decay. Code isn't haphazardly becoming worse due to natural forces. Humans actually *make* it that way! ;)
&gt; It's safe if you promise to preserve the length, but we don't tend to like preconditions in Haskell Can't you just make `unsafeReifyContents` a `where` clause in `sortTraversable` so it's not possible for anyone to misuse it? I don't think anyone would contend that having preconditions in a where-clause is in bad taste.
Of course. I just thought it was valuable to prove that we can extract **and** reify the contents of a collection safely. Not to mention, there's the potential for smarter sorting if anyone figures out that mergesort FIXME.
`Integer` has pretty bad performance comparatively, even once everything gets unboxed, and originally `length` was only defined for `[]`, so scanning for more than 64 bits on current hardware of list length was going to take a long time. For most applications using `length` you won't be working with gigantic containers of a whole 2^1000 entries (like you'd obtain by self-concatenation or WordNumbers tricks, etc.), so the `Int` is an appropriate scaled result and is highly efficient. For crazy scenarios you always have `genericLength` but it can easily be a good order of magnitude slower. On the other hand, the other issue that folks raise is that they want it unsigned, as it returns a range where half the possibilities are not valid outputs. For that, working with positive only numbers for sizes is very very hazard prone. If you ever subtract a quantity from another quantity you better be damn sure you don't go less than 0 lest it wrap all the way back around to maxBound. `Natural`, now that it exists, would have all the problems of `Integer` performance, mixed with the fact that it blows up if you ever temporarily wrap below 0, so knowing i &gt;= j you can't even do foo - j + i like you can with unsigned ints and allow the + on the end to bring things back into the positive range for a comparison against another value, but rather have to manually reassociate as you have to always work with positive quantities and - is dangerous. In practical usage this is even worse than the other 2 colors the bikeshed didn't get painted, despite being the theoretically tightest fit around the range of the output.
&gt; in practice it is rarely the bottleneck unless you are doing some serious math Like updating it a few billion times as you fly through a huge container to compute a length? =)
If an algorithm is suitable for vectorised execution, then Accelerate ought to be faster than Repa (all other things equal). I think the problem the authors of the paper encountered is that Accelerate has several subtle performance gotcha s. You easily end up with Accelerate generating a large amounts of specialised GPU kernels with inlined constants corresponding to variables in the Haskell program, for example. In general, when benchmarking Accelerate, you also have to be careful not to count kernel generation and compilation time, which can be a bit subtle.
I mean with `length :: (Foldable f, Num b) =&gt; f a -&gt; b` you should still be ok, since: If the data structure is something like `[]` where you actually might be updating an integer a massive amount of times, then you can simply have `[]` use `Int` internally, and call `fromIntegral` at the end. Since it is impractical to increment enough times to overflow `Int` (I guess with `2^29` it isn't completely impossible, but in practice we have a much larger range than that, and we aren't going to touch `2^63`). If the data structure is not going to be huge, then you can just use integer literal type things like `0`, and `+ 1` directly and not worrry about it. If the data structure stores its own length such as `Seq`, then just return that value (via `fromIntegral` potentially) I just personally don't think so many things should be monomorphic to `Int`, I get that it is fast, but it is less safe and correct than `Integer`, and IMO correctness &gt; performance. So if it were up to me `!!`, `length` and friends would be more polymorphic. Admittedly such a generalization won't play well with my desires to generalize `!!` to use type families to get the index type, since as far as I know you can't return `Num a =&gt; a` from a type family. On a side note is there a workaround for that? Because I actually do really really want that.
FYI, should have Rumpus up and running on Linux very soon, have been working in there a lot lately : )
Using halogen for UI framework. Love it! Early version had much noisier types the recent upgrade is a huge improvement I've built some interop with moment.js. Pretty painless. Slamdata's stuff on the whole has taken care of everything I'd need. I'm using their ace and echarts packages which are both excellent. At some point in the future I'm considering building a binding to vega-lite at which point I'll report back Haven't needed to do any low level stuff yet, but there's a story for it in some of the halogen packages that wrap up stateful stuff. e.g. ace and echarts Just checked the js, it's 4.8M. This is for an internal dashboard and is completely un-optimised. I'm sure closure compiling and minifying could bring that down? Haven't looked at all at the last couple of things. Performance has been acceptable so far.
Yup, it's like Bitcoin, you need a proof of work. One published paper gets you fifty semi-colons and a hundred braces.
&gt; If—when—I realize my design was wrong, The 'when' part is just so real...
I'm not going to say our codebase at work is pretty, it's far from it. Good engineering practices still apply - code reviews and consist style are important, and the compiler will not enforce them upon you. However, we all acknowledge our code could be cleaner but ultimately - it hasn't been a huge issue, and here's where Haskell is different. We have *three* different ways to access data from our database, and all sorts of old approaches to doing things - but it all just works. Not "just works" in the sense that we have rigorously checked all behavior with tests - we don't have any tests. The types have been sufficient, and when combined with a terrific compiler that does a good job of removing the cost of abstractions, we don't mind stacking two approaches on top of each other. We're currently working on replacing `persistent` + `esqueleto` with `opaleye`, so we type classed all our function calls, and now we have a monad transformer that is capable of running a single database transaction for both of these wildly different APIs. The flip side of things - and perhaps the reason we are fairly "careless" when it comes to code cleanliness - is that it's so easy to tidy up. It might be time consuming, but it's rarely a difficult job: rip something out, follow the type errors until it compiles, and with a little bit of testing you're probably done!
Functor instances are unique. From the typeclassopedia: &gt; Unlike some other type classes we will encounter, a given type has at most one valid instance of Functor. This can be proven via the free theorem for the type of fmap.
A theory I have is that Haskell codebases are by-and-large clean because Haskell is optimised for good code. If you're writing code well Haskell makes it easier (e.g. the `Either`/`Maybe` monads for error handling), but if you're writing bad code Haskell makes it harder (e.g. static typing with no coercions). There's a kind of natural selection going on where the path of least resistance for programmers writing bad code is to jump ship to a less rigorous language. Having said that, I've seen some gorgeous JavaScript and some heinous Haskell.
For the highest awesome-to-effort ratio, you need to try hylogen/hylide: https://github.com/sleexyz/hylogen Very easy/lightweight to get started. First time I tried livecoding shaders in Haskell was a "wow I do live in the future" moment. Some older resources (ok for starting out, i wouldn't build anything too ambitious based on these): a realtime raytracer with gloss (the code is included in gloss examples): https://www.youtube.com/watch?v=jBd9c1gAqWs (nice, gloss performance isn't great last I checked though) https://hackage.haskell.org/package/not-gloss (one of the earlier &amp; accessible 3D efforts, but as with gloss, probably not particularly scalable) There's also this tutorial on youtube by jekor https://www.youtube.com/watch?v=-IpE0CyHK7Q The code is out of date but the tutorial is beginner-friendly.
This comment reminded me of /u/Tekmo's [blog post](http://www.haskellforall.com/2016/04/worst-practices-should-be-hard.html?m=1) about bad practices being hard.
If you want to see and example of how to use GLFW and OpenGL in Haskell, I can offer a small toy project in [voxel meshing](https://github.com/phischu/voxel-populi).
&gt; At some point you get tired of or outgrow a job even if it is Haskell you are working on. Being forced to use a monomorphically-typed language with a gopher mascot but getting to work on a team you're excited about? It's better than working on Haskell with a job you've outgrown. Having only recently managed to get a Haskell job after all those years, I find it hard to believe. I guess we'll see!
Haskell aging is not a bitroting issue so much as getting out of fashion. - Cool kid: How? don't you use zoom and magnify? How you don't?. It is the last, it is pretty and it is Lens!!!! - Worker: Because I made it five years ago. And it works pretty well; It had no bu... - Cool kid: This code can not continue running as part of our critical applications. Let's redo it, and by the way, we can use this other framework xyz that I saw last week in the Lambda-Strut conference.... Another problem is over-engineering. I would write about it if I have enough votes
&gt; because genericLength has absolutely abysmal performance Well, it doesn't seem like a convincing point. If you're calling `length` on big lists a lot of times, then you're probably doing something wrong and you should use better data-structure (like `Seq` or `Vector` which has O(1) `length`). And if you're calling it only on small lists then you probably won't notice performance issues.
I think the simplest solution is this: import Data.List findList x y = findIndex (isPrefixOf x) (tails y) That's O(m*n) though. For O(m+n), use Knuth-Morris-Pratt: import qualified Data.Algorithms.KMP import Data.Maybe findList x y = listToMaybe (KMP.match (KMP.build x) y) For a good time, try removing the `Data.List` import from the first solution, and reimplement the three utility functions on your own. Here's some hints for you, just fix all instances of "undefined": tails :: [a] -&gt; [[a]] tails [] = undefined tails (x:xs) = undefined -- Recurse here! isPrefixOf :: Eq a =&gt; [a] -&gt; [a] -&gt; Bool isPrefixOf [] _ = undefined isPrefixOf _ [] = undefined isPrefixOf (x:xs) (y:ys) | x == y = undefined -- Recurse here! isPrefixOf (x:xs) (y:ys) | otherwise = undefined findIndex :: (a -&gt; Bool) -&gt; [a] -&gt; Maybe Int findIndex f = go 0 where go _ [] = undefined go n (x:xs) | f x = undefined go n (x:xs) | otherwise = undefined -- Recurse here!
You might be interested in /u/winterkoninkje's many maps: * [a map of individual binary operations](http://winterkoninkje.dreamwidth.org/79868.html) * [a map of ring theory](http://winterkoninkje.dreamwidth.org/80018.html) * [a map of common normal modal logics](http://winterkoninkje.dreamwidth.org/87291.html)
Out of curiosity, why are you replacing persistent + esqueleto with opaleye? I haven't had the opportunity to use either stack in a serious production environment, but I was thinking about using the former soon. 
Long compilation times on `persistent` definitions, not as type safe as it could be (joins and on conditions have to be specified in a precise order), and I find `opaleye`s model to be more composable.
is rumpus usable w/o a VR headset? if so, would like to try it on OSX too.
My one issue is that I need to find a version of Hoogle to run on the command line. Sometimes I write functions but have a hard time finding them by text search and end up writing them again.
If you get rumpus running on linux, you would be a hero in my eyes! &amp;nbsp; Please create a detailed tutorial / description on how you did it. The first thing is that one probably needs a ssh key on github to clone all the submodules recursively. I changed all the git://github URLs to https://github.com/. &amp;nbsp; The next problem is in the sound system. Just to be sure I installed: "sudo apt-get libopenal1 libopenal-dev libopenal1-dev libopenal1-dev libopenal-dev libalut-dev". Then I edited "submodules/pd-hs/cbits/libpd_openal.c": #if defined(_WIN32) #include "AL/al.h" #include "AL/alc.h" #else #include &lt;AL/al.h&gt; #include &lt;AL/alc.h&gt; //#include &lt;nAL/MacOSX_OALExtensions.h&gt; #endif Since on Linux the headers are named "AL/alc.h" and NOT "OpenAL/alc.h". Here one should introduce a new ifdef for linux, like "#if defined(_LINUX)". &amp;nbsp; ~~If it's more complicated than you thought, please make a github / bitbucket repository with your changes and we can work together (e.g., so I can send you pull requests too).~~ (I didn't understand that you are the author of Rumpus when I wrote this, sorry.)
I am quite interesting. I wonder, should you change "base" requirement to newer versions? Error: rejecting: base-4.9.1.0/installed-4.9... (conflict: voxel-populi =&gt; base&gt;=4.8.2 &amp;&amp; &lt;4.9) If I change the requirements of base, transformers and lens to higher numbers, all works.
Superbly stated!
I was involved in the re-coding of all the internal applications of a company (mostly simple CRUD apps) made in a 4GL language -that worked perfectly- to Java with no new functionality whatsoever. Simply because Java was cool at that time. The new applications were terribly over-engineered. There were more people working in the architecture department -creating libraries for layers upon layers upon layers- than application developers. The application don't even run with machines with 10x more power that the previous ones. That was because Java was cool and everyone wanted to be "architect". That was not only because the bloated in-house developement: the Java frameworks were bloated enough: OSF, ICefaces, OQL ... Later people told me that the "chief architect" moved to other company before the disaster was evident. I know that he has reproduced the same disaster in the other company and moved to a third. Cool Kids. I'm sure that this is beginning to happen with Haskell right now.
You can derive the `Functor` class using the `DeriveFunctor` extension. The derived instance is guaranteed to obey the functor laws.
I mean I know they aren't structurally associative, but the errors you get due to lack of associativity seem no worse to me than the errors you just naturally obtain from them rounding poorly. Now I know there are some edge cases where weird rounding can make a difference, such as summing an ascending list being more accurate than a descending list generally. But I still think it is worth not completely ruling them out. Out of interest would you consider unioning maps or sets in a way that isn't structurally associative (insert one into the other for example) to still be monoidal? Now I realize that doubles are quite a bit worse than structural variation between maps, but I am curious where you draw the line. I personally am on the fence between drawing it between map and double, and drawing it after double.
&gt; It was tempting over time to build overgrown lawless typeclasses. This is usually born out of a desire to emulate the interface keyword that OOP languages have – something that lets you quickly introduce overloaded functions. These usually got uglier and uglier over time as more and more instances would run into special cases that required extending the typeclass. I would hazard that most of these are shadows of some bigger architectural problem. At what point do you create a new typeclass? Being fairly new to Haskell I've never created a typeclass of my own, but I don't expect I ever would unless I had written very similar functions for 3 or more different types first. Once the underlying pattern was clear I'd make a typeclass. Is this how it works "in the wild", or do programmers reach for typeclasses right away and violate the YAGNI (you aren't going to need it) principle?
&gt; Out of interest would you consider unioning maps or sets in a way that isn't structurally associative (insert one into the other for example) to still be monoidal? Honestly, I'm not sure. I suppose it really depends on how the rest of the interface is designed. If the hash table or search tree (or whatever) implementation is proudly exposed, and structural/implementation equivalence is preferred/prioritized/privileged over other equivalence, then, no I wouldn't claim them as monoidal, since just claiming them as "magmal" is sufficient in practice and more true. If the implementation is treated as a detail, and only exposed in order to prevent unnecessary performance penalties, and the primary equivalence relation on them is set-wise (or map-wise) instead of structural, then I think it's fair to claim they are monoidal, even if the structure does not match exactly. The ideal would always be to depend on definitional equality, and require the monoid / group / whatever equalities to hold definitionally, but I think that's not practical. I think judgmental equality and higher inductive types might be the right level to expect monoid (etc.) properties to apply, but I'm not sure.
I'll give it a go, thanks!
So it seems as though for Data.Map and Data.Set you would be ok with it, and would you perhaps support flagging or documenting that `showTree` is not really lawful, e.g `a == b =&gt; f a == f b` does not hold, and it allows you to see unlawful behavior that would otherwise be an implementation detail? I honestly probably would support such a thing, IMO `Eq` should mean `a == b =&gt; f a == f b`. &gt; The ideal would always be to depend on definitional equality, and require the monoid / group / whatever equalities to hold definitionally, but I think that's not practical. Is that the stuff I am talking about where you can't observe any differences? If so I agree that it is the ideal, and I think we can get an almost as good outcome where all non-flagged functions obey such a thing.
The classes look like they have a lot of overlap. What's the difference? Should I only look at the later one?
&gt; for Data.Map and Data.Set you would be ok with it, and would you perhaps support flagging or documenting that `showTree` is not really lawful, Yep. &gt;&gt; The ideal would always be to depend on definitional equality &gt; Is that the stuff I am talking about where you can't observe any differences? Yes. Two definitionally equal terms would evaluate to the same value.
i heard there was an issue with the non-determinism of haskell compilation (i.e. same .hs and same .nix, but their binaries can differ). haven't run into it myself. 
Lecture 27 about only allowing programs of a certain complexity is quite interesting. There's also a [course](https://scs.hosted.panopto.com/Panopto/Pages/Sessions/List.aspx#folderID="07756bb0-b872-4a4a-95b1-b77ad206dab3") on "Advanced topics in programming languages" by Rober Harper.
https://github.com/tysonzero/algebraic-structures/tree/master
A logic is defined by a collection of [inference rules](https://en.wikipedia.org/wiki/Rule_of_inference) (at least, every logic we considered was). These inference rules give meaning to the "logical connectives" under study - examples of familiar connectives include "AND", "OR", "IMPLIES", etc. Each of these have linear analogues. The Rust type system uses a variation of affine types, which are similar to linear types (the theories of which is given by affine and linear logic respectively). I believe the [Clean](https://en.wikipedia.org/wiki/Clean_(programming_language) language uses a variation of linear types, but it might actually be more similar to affine types, not sure. Broadly, the logic defines the type system almost directly, but even so developing the computational interpretation can be challenging (in the case of homotopy type theory it is an ongoing problem). A logic is a calculus of propositions. For example, I might define a logic which includes a typical "AND" connective. "AND" is defined by three rules (given propositions A, B): A B -------- A AND B A AND B -------- A A AND B -------- B The propositions above the vertical bar denote premises and the ones below the bar denote conclusions you can draw from them. So, for example, if I know A, and I know B, I can prove A AND B. The thing to notice here is that the rules for AND encode the properties of the type pair. If I have an a of type A, and a b of type B, I can construct (a, b) of type (A, B). If I have an (A, B), I can extract an A, or I can extract a B. Each of these behaviors is encoded by one of the rules; in essence (A, B) is really the same thing as A AND B. In fact, the same goes for OR, which encodes sum types, and IMPLIES, which encodes function types. I think there are a couple of reasons we don't see more of these programming languages. One is PR and complexity budget - it's hard to attract people to a language that have features that are exotic to programmer's experience, even if the apparent complexity is worse than the reality. Haskellers and MLers are still contending with this, and those are built on top of much more conventional logics which use connectives that everyone is familar with. Another is usability. Working in a language that only permits one use of each variable (as is the case with affine types) can feel cumbersome; working in a language that both mandates one use and only permits one use of each variable (linear types) doubly so. There are ways to mitigate this problem and make these languages work more like the ones we are used to, but the solutions aren't always totally clean. With that said, while you might not see many languages built on top of these theories coming out on r/programming, languages of this sort are all over the research literature.
lazyLength is better implemented as a foldMap so that for large containers with explicit sharing it can potentially reduce in sublinear time. foldr and foldl' always pay full price, with the former sometimes getting time off for being lazy, but for the example lazy nat case sums are just as lazy as succ's, and there are other containers for which the monoid can win by astronomical amounts.
 stack hoogle 'a -&gt; Bool'
&gt; Is this how it works "in the wild", or do programmers reach for typeclasses right away and violate the YAGNI (you aren't going to need it) principle? Once you get the hang of it, you can recognize the kinds of problems which would be solved using a type class, and immediately write both the type class and many instances for it. You'd never write a type class with only one instance "just in case".
I mean that's understandable, but I think that speaks to a sorry state of software engineering. In any other engineering discipline, we would use the best tools available. I'm not sure why we see software as something where the developer's interests overshadow those of the end users. Even worse, it seems in both your cases, the monetary interests of a for-profit company are being put ahead of quality software, but alas, c'est la vie.
&gt; To be frank, none of these things can live in a language standard right now. MPTCs, etc. all take you outside of what we have a standard for, and so just don't become viable solutions for the Prelude, not if we want to continue to have the polite fiction that ghc is just a bleeding edge implementation of Haskell, and that unlike perl, haskell isn't entirely defined by one implementation's behavior. :'( You aren't wrong, but I wish you were. We need a new language spec sometime soon. &gt; As for the Length class you gave, that 'i' in the Length class starts to leak out in all sorts of uncomfortable places. We found something similar when offering up all the various AsWhateverException classes in lens that were parameterized by the p and f choices. If you ever wanted to work parametrically with such an instance the choices of those functors would leak out into your surrounding code. Can you give me a bit of an example of that? I think I can see why that might happen, but I can't really think of any examples where I couldn't make some minor changes to fix it, but I am probably just not thinking hard enough. &gt; I currently personally choose not to live in that world for the most part. I find type application quite painful to use or reason about. Which type is being applied at any given moment is often all but a crapshoot. You're welcome to write your own libraries however you choose, of course. I personally wish foralls were as lightweight as lambdas, then we could require them to be explicit to make type application work, and have `:i` and such show them and have it be an actual part of the functions spec. &gt; The code I write now follows a pretty simple rule that every typeclass I build has a sort of "polarity" to it. I never make up a class where you have combinators where a type parameter mentioned in the class occurs in strictly positive position in some members and strictly negative positions in others. By following that one rule, I find that, when writing the library code other people consume I just never find myself in situations where I need to use manual type applications to get around show . read problems. Since you say "type parameters mentioned in the class", then technically my suggestion to return `Num a =&gt; a` is technically not against your rule. &gt; On the topic of type applications, any solution involving type applications requires me to enable AllowAmbiguousTypes to make any real use out it, and that spackles over real bugs with abandon. &gt; Proxies don't cause me that pain. That seems like a flaw in GHC or something, I see no reason why type applications should not be able to safely do everything proxies can do. &gt; Beyond a few specialized problem domains, I've yet to be convinced they are worth the user experience. I guess my reasoning is that `foo @[] bar` is a more enjoyable user experience than `fooList bar` or `List.foo bar`. As they are all similarly verbose, but only the first also tells you some information about the type that is enforced by the compiler, `fooList` could return a `Set`. And of course when dealing with a combination of function the situation only gets better: `bazList . barList $ fooList foobarbaz` vs `baz . bar $ foo @[] foobarbaz`. As I do still want as much parametric polymorphism as I can get, I just think we can do away with a lot of monomorphism.
&gt; Some people actually still think they can write more reliable code in Python than in Haskell. And I would believe them! On projects where we've used Python, the majority of the time is spent implementing a highly customized type checker in the form of unit tests and asserting `isinstanceof` or `type(x) == &lt;blah&gt;`.
Maybe you are interested in listening how it sounds. You can find some tracks on my soundcloud: https://soundcloud.com/anton-kho By the way, I've just made a new ambient track called River spirits, it also features some tricks implemented in the new version of the library: https://soundcloud.com/anton-kho/river-spirits
Overall I think you make a good case. I don't really disagree with what you have said. On a side note, what are your thoughts on `Double` being an instance of `Eq` but having `NaN == NaN` be false? Because `x == x` is IMO pretty damn fundamental to `Eq`, and this bothers me much more than `Double` pretending to be a `Monoid` does. Regardless of IEEE, if I were to develop my own language I would have `NaN == NaN` be true, and then has something like `eqIEEE` or `==.` or whatever for IEEE semantics. And then of course I would have an easy to use `isNaN` function. Interestingly enough Haskell already has Double lie: `Sum 0.1 &lt;&gt; Sum 0.2 &lt;&gt; Sum 0.3` varies depending on order of operations. I think the newtype solution is probably the best one, I was actually thinking a bit about that before I read your comment, that way people who really care about the IEEE behavior (or that just want very stable semantics and don't want the risk of weird compiler or human optimizations that change the semantics of the code) are safe, but people who just want "reals that are close enough and are fast" can use the newtype for more flexibility and optimization.
Thanks. I just ran your example. Very impressive! It was trivial to install the dependencies. Is this Open Source?? Now I compile it for my quadcore with (correct?): ghc -O3 -threaded -rtsopts -with-rtsopts=-N4 Main.hs &amp;&amp; ./Main One last thing. The code was not very readable for a 3D beginner but with decent haskell background like me. How would the code need to be adapted to run just 5 white circle in random directions? I guess from that it would be easy to backtrack to your own example. 
Yes, 'a b c d' are slices of state. They would be consumed by widgets, parameterise form inputs, hook up to websockets, etc. I'm primarily interested in the frontend boundary since that's where most of the unarticulated expectations suddenly become articulated; especially with regard to UX, the "product" one makes is basically the interface with the user. 
&gt; I don't actually see Eq as representing an equivalence relation. I see it as just ad-hoc overloading for (==) and (/=). That doesn't mean I don't sometimes incorrectly treat it as an equivalence relation, just that I don't always expect that guarantee. Well in a hypothetical language where we actually use the ordering tower I have partially built up above, would you then consider that `Eq` an equivalence relation? Honestly I already consider Haskell's `Eq` an equivalence relation, I think typeclasses with laws are much better than ones without. &gt; This is mainly because symmetry, reflexivity, and transitivity aren't things that are usable at the type OR term level in Haskell I am pretty confused by what you mean here, I don't see the difference between those laws and laws like associativity. Both help prevent surprising behavior, help you reason about your code, and can lead to optimization opportunities. &gt; That said, if Eq really is supposed to represent an equivalence class, I'd actually fall on the side of just not having an Eq instance for whatever types have IEEE semantics. I tend to trust that IEEE got floating-point semantics "right". But then simple things like `if x &gt; 5 then ...` will break, IMO a special case for `NaN` where we diverge from IEEE isn't the worst idea. You can always have `==.` or something for IEEE behavior. &gt; I'd hesitate to have an equivalence relation instance for a type with the semantics of the reals, too. Because it's not computably decidable. I thought we agreed that doubles weren't the reals, hence why we aren't making them a `Monoid`. They are "fixed precision rounding number" or something, where equality is decidable. &gt; Yes. It's not the only lying instance in base, but the community seems to value some combination of attributes over pure honesty in type class instances. I don't like that, IMO laws and law violations should be taken seriously. Things like `showTree` should be documented, and otherwise laws should be obeyed within observation. 
&gt; I am pretty confused by what you mean here, I don't see the difference between those laws and laws like associativity. Like I said, it's probably just a bias. But, I have actually written functions to re-associate a free structure, when I've never written a function that applied symmetry, reflexivity, or transitivity.
This [thread](https://github.com/commercialhaskell/stack/issues/472) has solutions to trigger commands after stack successfully builds a project. You could have it trigger a command which generates your image, dumps it to a file, and reloads your browser. That should let you close your loop sufficiently.
It's definitely an interesting problem of how to represent the interface as a Type, it's something I think about but I don't have a solution to yet. What have you tried so far? What were the limitations?
&gt; `x &gt; 5` Well, see now you are asking more than an equivalence relation, you are either asking for a partial order or (`Ord` =) total order. Where do you want to stick `NaN`? Above `Infinity`, below `NegInfinity`, between one of the largest magnitude finite values and the matching infinity, between 0 and (-0), between one of the smallest magnitude non-zero values and the matching, signed zero? `NaN` is a sticky beast. If you have a (real) floating point type without NaN, what is `sqrt (-1)` or other complex results? --- For fixed precision floating point numbers wihout NaN, yes we should provide an equivalence relation and a total ordering. For fixed precision floating point number with NaN, we should not provide a total ordering. If we want IEEE semantics we should not provide an equivalence relation. I'm not 100% sold that we have to tie the operators `[(==), (/=), (&lt;), (&lt;=), (&gt;=), (&gt;), compare` to an equivalence relation / (partial or total) order. They seem fine having the fuzzy meaning that Algol 68 gave them. Over in Idris, they use `=` (syntax not definable) for the equivalence relation (`DecEq` if it is decidable), and `LTE` for the total ordering (at least on `Nat`).
Maybe yesod's development mode paired with generating a HTTP page with a Meta-Refresh header or something? I'm pretty sure yesod can still re-compile based on file update.
I mean that kind of thing is IMO one of the least important aspects of type class laws. To me its all about ability to reason about your code and do certain kinds of restructuring without worrying about changing the result, and it would be nice if the compiler used this stuff to speed things up. I bet you have refactored `==` before in ways that are only acceptable because of those laws you take for granted. And I guarantee you would also be very surprised if some type (besides the well known Double / NaN situation) happened to disobey them. I mean imagine if `a == b &amp;&amp; b == c` was different than `a == b &amp;&amp; a == c`, or `a == b` was different than `b == a`. I'd go nuts.
&gt; Well, see now you are asking more than an equivalence relation Exactly my point, `&gt;` is higher up on the chain than `==`, so we need to at least start with `==`. Even partial orders obey reflectivity of `&gt;=`. And yeah I know about the whole NaN issue, so either I'd have `sqrt (-1)` be an error (hell: `div 1 0` is an error), or I'd just find a place to put `NaN`. Or yes make `Double` a partial order instead. But no matter what all that requires from the start having a reflexive `==`, because the partial order, equivalence relation, total order, and so on laws ALL require it. &gt; I'm not 100% sold that we have to tie the operators [(==), (/=), (&lt;), (&lt;=), (&gt;=), (&gt;), compare to an equivalence relation / (partial or total) order. They seem fine having the fuzzy meaning that Algol 68 gave them. I mean you do you man, but I sure as fuck don't want to work in a language where typeclasses have no laws and operators are just "whatever seems right", like `&gt;&gt;` being both "stream into" and "bitshift", fuck that noise. Laws are fantastic for `Functor`, `Applicative` and `Monad` and are often used in refactoring, I see no reason why we should ignore them and build some ad-hoc crap instead.
What do you need that Rational can't provide? They should provide a field. It sounds like IEEE floating point values are really just not well suited to being manipulated algebraically. And, it seems like you'd have to work *really hard* to make ANY fixed-precision, floating-point type be even `RingLike`.
Well I mean rational doesn't represent the reals, only the rationals. So it breaks on a decent amount of operations as well. Also performance. But I would definitely have rational be a field, and I wouldn't define irrational operations on it, or require some extra effort to call them (why use a rational if you are just going to call `sin` on it). &gt; It sounds like IEEE floating point values are really just not well suited to being manipulated algebraically. And, it seems like you'd have to work really hard to make ANY fixed-precision, floating-point type be even RingLike. I mean if you care about the structural value and slight rounding errors are a concern then yes, but if that is the case `Double` is probably going to be a garbage type to begin with. And if you ignore edge cases like `NaN` and rounding errors then they obey a large amount of laws.
&gt; I mean you do you man, but I sure as $_(% don't want to work in a language where typeclasses have no laws I feel you, but type classes were originally proposed in an *entriely* lawless manner as simply a way to do ad-hoc overloading of our favorite operators. &gt; Laws are fantastic for `Functor`, `Applicative` and `Monad` and are often used in refactoring. I agree here as well, though I think you'll find a few lies in some `Applicative` and `Monad` instances. Type classes can be used for both. For example, your algebraic number tree can be all about laws and verified instances, while the existing `Eq` and `Ord` could remain just ad-hoc name overloading. I'm not 100% sure type classes are the *best* solution for either. I think type-directed name resolution might actually be a better implementation of ad-hoc overloading. I think groups and rings are their own objects (of a dependent record type), rather than being unique properties of a type, and it might be best to implement them that way. --- Under the assumption we are using typeclasses for both, I'm fine with `Eq` and `Ord` being purely ad-hoc, because I want them for completely non-mathematical, brand-new types of my own devising. I'd prefer `Magma` through `Group` and `RingLike` through `Field` to be ties directly to their algebraic inspiration, even if that means evicting `Double`. I also think that new implementations of these will either be on the representation of some mathematical type, or point-wise lifted into something else. `Functor` makes things too easy, since with the laws it is truely unique. `Applicative` and `Monad` though have a special bit of syntax dedicated to them. So, it's probably worth it to allow developers to overload that syntax even when the laws don't *quite* apply. So, I'm a bit of a mixed bag. :P
&gt; So it breaks on a decent amount of operations as well. How often are you taking least-upper-bounds of infinte sequences and expecting a computable result? It's got all the field operations. :)
&gt; we don't have any tests I'm pretty surprised that this works --- at least in the haskell codebases I had, while we didn't have many "unit" tests (ie, of low-level very simple behavior), we often caught bugs with high-level tests that captured important workflows, etc...
Your python code example is a complete strawman. Nobody does that in Python. What people *do* do is have lists that would be typed like this in Haskell: myList :: [forall x. Class x] where `Class` is some *ad-hoc* protocols, like the 'buffer protocol', the 'iterator protocol', etc. 
Types don't prove any non-trivial properties of code.
&gt;2) Nobody's willing to spend the kind of money necessary to ensure quality. Because they aren't required to by law, whereas you are in other engineering disciplines. Software companies get away with basically having no warranty whatsoever, commonly.
&gt;For 2, a large swath of Haskell code can be written without a single type annotation. But they aren't, and shouldn't be.
That's not really true. The `x` doesn't need to be declared `Int`, you already declared that in the function signature. The `3` doesn't need to be declared `Int`, as integer literals are obviously always integers in any sane language. `+` is already declared `Int -&gt; Int -&gt; Int` where it's defined. So really, for any sane language this is what's required: (\x -&gt; 3 + x) :: Int -&gt; Int which is essentially what every standard imperative typed language looks like, except written like this: function f(int x) int { return 3 + x; } or int f(int x) { 3 + x } or [] (int x) -&gt; int { return 3 + x; } 
But there's no need to know anything about monoids? Just know that there are things you can combine. It's the equivalent of using the bitwise-or to combine flags in C -- no need to know about monoids -- we can still learn the pattern.
But there's no need to know anything about monoids? Just know that there are things you can combine. It's the equivalent of using the bitwise-or to combine flags in C -- no need to know about monoids -- we can still learn the pattern.
Here’s what I would try: * Add a GHCi command that renders a `Diagram` to a known SVG file path * Start a Warp server that just serves that file * Write a small wrapper page that sends an AJAX request and redisplays the result in a browser every few seconds * On Mac, in iTerm2: render to a PNG instead and add a GHCi command that displays the PNG inline using iTerm’s inline image support 
In personal projects, I have sometimes used case statements and sum types to mock up state interactions prior to implementation, just as a method of reasoning. If everything is a token and there is no actual computation, I see no reason why a few typical Haskell sum type declarations and a chain of case statements wouldn't go over just as well as a flow chart in a power point presentation. data LightBulb = On|Off lightSwitch :: LightBulb -&gt; LightBulb lightSwitch bulb = case bulb of On -&gt; Off Off -&gt; On I mean, sure, there'd be some technophobia to overcome, but honestly I think the learning curve on the average excel spreadsheet is far more intense.
Note that if you pass the -l option to the generated executable, it will watch for changes and recompile and rerun itself.
Don't forget that we have _extremely_ lightweight threads in GHC Haskell, so "context switching" is very lightweight. Look for the Architecture of Open Source Applications chapter on Warp, the most popular WAI implementing server, to see how things are actually done, but a naive one thread per request model should be able to perform very well. Remember we have only one OS thread per CPU core, but each of those can handle tens or hundreds of thousands of threads happily, without any need for OS context switching. All IO is built on event libraries like libev or kqueue IIRC so handling thousands of network connections is not difficult. You should also read Simon Marlow's book, parallel and concurrent programming in Haskell to better understand the mechanism being used (and that you have at your disposal).
The webpack thing sounds interesting... how do you do that's?
We could definitely benefit from some
Plainly speaking, if you try to use only a number of threads equal to the number of cores and try to schedule requests with these threads, probably you will not match the efficiency of having one thread per request, leaving the scheduling to the green threads runtime. Less work for you.
&gt; how does servant/wai handle it? Pretty good.
What makes developing a computational interpretation of a logic challenging? How do you know if the computational interpretation that you have in mind is the correct one? Is there a way to test/prove it?
this is the best IDE for haskell. I tried haskforce, vim-haskell-now, atom, sublime-text, lekash...this is the one!
&gt; "context switching" is very lightweight. I assumed as well, but I'm seeing the time taken to serve a request (as measured by WAI middleware) increase by 10x as concurrency increases. Hence trying to understand the mechanics
I'm seeing the time taken to serve a request (as measured by WAI middleware) increase by 10x as concurrency increases. Is this normal? 
Without knowing more about your app, it's neigh on impossible to tell.
Its a pretty standard webapp taking a JSON request, doing some DB work, and responding with JSON. 
Thanks for the detailed explanation. Which queueing philosophy does Warp use?
Tha'ts interesting since a pure one-thread-per-request with no limit of them, which is a form of pre-emptive round-robin can potentially degrade the latency response time for all too. A limit in the number of threads and a queue is probably the best option 
Pretty sure it just shunts threads to the GHC scheduler which processes them FIFO. Snap is the same. This is a great strategy to use if the main thing you're doing in your response handler is blocking on I/O, but it becomes terrible once you're CPU bound. Nobody ships this kind of thing out of the box, because it's easily implementable in user code via combinator and it's policy. 
It seems you're measuring latency. GHC's RTS is optimized for throughput. So what's happening is as the workload increases the RTS tries to handle everyone fairly. If you don't want to be fair, you can implement some sort of prioritized queue and handle fewer requests at a time while allowing lower priority to suffer.
I think the notes for the second one are significantly broader and more interesting....but that might also be because I kind of understood linear types/logic before reading either.
If you have N sub-services running LIFO, then the variance in response time will be maximized, as LIFO is the queuing strategy that maximizes variance. Unless the N sub-services are synchronized, then the overall latency will be maximized. If you're talking about the first entry-point to an API then we also have two issues - either it doesn't compose, or if the request is coming from an end-user, we can't apply backpressure. You can also see that LIFO is never used in large scale systems like those at Google. See this talk by Jeff Dean on this exact topic: Jeff Dean: "Achieving Rapid Response Times in Large Online Services" Keynote - Velocity 2014 https://www.youtube.com/watch?v=1-3Ahy7Fxsc If you have very low fanout and replicate requests to multiple backends, maybe it could fly, but 95th percentile latency is typically *not* what you want to optimize for, it's the 99% or 99.99% percentile that is what people complain about.
The CPP symbols you are using are defined by cabal - any recent version of cabal should suffice and your version of GHC is certainly new enough since the CPP extension dates back a long ways. Notice since the defs are from cabal you can not invoke ghc directly an expect to use `MIN_VERSION_*`.
You are right, the way I built the function is that the first two point should be the lower ones, so I need to subtract 1 not add Thanks :)
Thanks for the link! The paper simulates network delays, not fan-out latency. The difference is that they calculate the latency through a set of hops, while fan-out is about the max latency for a set of requests. But as you say fig 8 demonstrates the principle. Note that the simulated network in fig.8 is a 5-hop network and thus exaggerates the advantage of LIFO compared to FIFO. An N-way fan-out can be seen as sampling 5*N red dots from fig.8, and then taking the max of these and see if the delay is &gt; than the blue line / 5. You can see that there's a trade-off where at certain fan-outs FIFO will win. But this was very interesting - I'd like to see an analysis that looks at both fan-out and hop depth.
This is usually how I begin to carve the domain into algebras; most recently I went as far as mocking 'View device bar bas' types. And talking through them as specs. The problem was that it was too abstract or too poorly specified. 'View web bar' and 'View web baz' as separate nodes 'typechecked' just fine but didn't force us to realize that what we would really end up doing something convoluted to get the view we really needed. Which involved a much deeper architectural issue between baz and bar. 
Bookkeeper is very cool,but has less of an ecosystem than vinyl. I'm experimenting with it for easy generation and validation of update parameters in a servant API client at work. I haven't figured out how to do rtraverse or parameterize over a functor yet. 
Isn't that expected in any system? If you do ten things on one core instead of one thing on one core, those things will each take ten times longer?
A fantastic talk! I'm surprised it hasn't been posted here yet.
There's a PR [0] with many functions like `map` and `traverse` lifted to work with books (as well as big performance improvements), but I haven't cleaned up or merged it yet. [0] https://github.com/turingjump/bookkeeper/pull/27
1. Done, thanks for the heads up. 2. It doesn't compile. The compiler seems to make sense but I'm not sure where to go from there: [1 of 1] Compiling Lib ( src/Lib.hs, .stack-work/dist/x86_64-osx/Cabal-1.24.2.0/build/Lib.o ) /Users/octplane/src/ouiche/ouiche/src/Lib.hs:18:11: error: • Couldn't match type ‘RepositoryFactory n0 m0’ with ‘IO’ Expected type: IO a0 Actual type: RepositoryFactory n0 m0 a0 • In a stmt of a 'do' block: repo &lt;- RepositoryFactory {openRepository = repoOpts} In the expression: do { let repoOpts = ...; repo &lt;- RepositoryFactory {openRepository = repoOpts}; putStrLn "Hello World" } In an equation for ‘someFunc’: someFunc = do { let repoOpts = ...; repo &lt;- RepositoryFactory {openRepository = repoOpts}; putStrLn "Hello World" } /Users/octplane/src/ouiche/ouiche/src/Lib.hs:18:48: error: • Couldn't match expected type ‘RepositoryOptions -&gt; m0 a0’ with actual type ‘RepositoryOptions’ • In the ‘openRepository’ field of a record In a stmt of a 'do' block: repo &lt;- RepositoryFactory {openRepository = repoOpts} In the expression: do { let repoOpts = ...; repo &lt;- RepositoryFactory {openRepository = repoOpts}; putStrLn "Hello World" } Source file is a mere : module Lib ( someFunc ) where {-# Language OverloadedStrings #-} import Git someFunc :: IO () someFunc = do let repoOpts = RepositoryOptions { repoPath = "." , repoWorkingDir = Nothing , repoIsBare = False , repoAutoCreate = False } repo &lt;- RepositoryFactory { openRepository = repoOpts } putStrLn "Hello World" 
The `&lt;-` arrow expects the right hand side to be some `IO` function, but you're using a pure value (`RepositoryFactory { ... }`). Also, it seems the type of the `openRepository` field has a different type than the value you're giving it.
Thanks!
*(Edited to be less aggressive, as suggested by /u/bss03)* May I suggest you to LTFL (learn the language)? &gt; As usual, when I try a new programming language Please give Haskell concepts more time to sink in. As you may have seen, Haskell doesn't really share a common basis with other mainstream imperative languages like C, Python, Java, etc. Known as a 'purely functional language', Haskell isn't based on 'control structures' like loops, `if` statements, etc., but rather functions and their composition. By directly porting what you've learned from imperative languages Haskell and try to 'get started with real-world apps quick', you turn your attention away from Haskell's best parts, and towards the ugly, difficult-to-use emulated imperative constructions. If you insist on pursuing, you might finally get miserable and decide to join the 'Haskell Hard Club', which I don't want you to, and I suppose the Haskell community doesn't want either. Still remember how you struggled trying to write your first ever real-world program before learning enough of the basics? Haskell is going to be like that experience all over again. Try this free resource list https://github.com/bitemyapp/learnhaskell . When you see Haskell worth it, this price of this book will be worth it http://haskellbook.com
Could you explain it in detail?
Would you say handler which is accepting a JSON request, doing some DB queries, and generating a JSON response would be CPU-bound? It would better fit the "blocking on IO" definition, right? In such a case, is it normal to see an increase in processing time of individual requests (as measured by middleware - which I'm assuming it actual processing time only; not wait-time in a request queue)
It's too hard to say without understanding more about your program. There are a lot of things that can be bottlenecks here. All we're saying in this comment thread is that it's unlikely that your bottleneck is due to RTS scheduling. 
oh wow, this is great -- `sum-error` in particular looks fantastic.
Under what conditions would it make sense to use cabal by itself? The only times I can think of would be when you're using something else (Docker, Nix) to provide sandboxing instead.
&gt; Dependencies increase compiletime. For small nonessential utility packages with simple dependencies I would recommend doing what programmers did back in the bad old days: cloning the library's source code into your repository and using stack's multiple-package feature. I would expect this to increase compile-time, not decrease it. If the library is a separate package, then Stack will still compile it the same as if it had downloaded it itself. The real difference depends on whether it's marked as an extra-dep - if it's not, then it'll be interpreted whenever you start GHCi instead of reusing the compiled binary.
&gt; Under what conditions would it make sense to use cabal by itself? Whenever you are doing something that uses minimal libraries beyond the set shipped with GHC. When I'm using lots of libraries, I use [Mafia](https://github.com/ambiata/mafia/) which is a thin wrapper around cabal, which provides shared sandboxes and simpler command line. 
The idea of this blog post was trying to be informative about advanced type system feature on the theme of having some cstruct-like stuff done that was efficient. This wasn't about mapping every single C dama..features. For padding and alignment, I think most of the rules could be encoded with couple of extra types and such, but would have been far too noisy for a blog post. And unless you want extreme portability, which in the big picture would be weird in Haskell anyway, most padding could be handled manually or with a really small conditional compilation. No pointers, because it wasn't useful for me right now (while writing this I was mapping network structure). But again the extension to the blog post code should be mostly trivial. Flexible array are not magic and can just be handled by a "bogus" Array with cstruct, which would allow you to calculate the beginning offset. or you can just see a flexible array in a structure as what it is: a structure without the array at the end, plus an arbitrary amount of data afterwards. Same things with anonymous struct and union member; you can just give it a random name for the definition in cstruct. there's no need to simulate anonymous fields.
Pretty much any? You're using cabal under stack, only now you have to go through stack to deal with cabal. In the end you have to deal with cabal though. Why complicate things?
Nice, I think most are pretty accurate &gt; A monad is composed of three functions and encodes control flow which allows pure functions to be strung together. I think it's pretty hard to come up with a good definition for Monad. I'd keep it vague: A Monad is an abstraction that makes handling side effects, state passing, error handling, etc, more convenient. &gt; Fold applies a function between elements of a list. A bit vague. And a fold isn't limited to list. I'd say A fold reduces many values to one using a binary function
If you want to say 'of course they do', say it. Don't hide behind 'citation needed'. In my experience, outside of a few languages like Haskell, types really don't prove any nontrivial properties of code. They certainly don't prove anything about termination in Haskell, for example.
you forgot to define fmap in one sentence.
One could say that functors are an abstraction over Lists which allow to apply a function on each element of an arbitrary collection.
I can't say I've ever had to deal with cabal directly when using Stack, unless you count modifying the .cabal file. In my experience, it's been dead simple to use. The reason why is pretty obvious - I can compile a given project on a different computer (even one with a different OS) and know the result will be the identical. In comparison, I remember using Cabal directly in the days before Stack, where updating a package needed by one project could completely break another. I didn't dare updating anything Haskell-related on my laptop for the entirety of my honours' thesis, in case it broke something I needed. Now, maybe that was partly my fault for knowing about cabal sandboxes at the time, but IMO tools should make the right option easy / the default.
&gt; sum-error in particular looks fantastic. What do you find fantastic about it? I've briefly looked at the PR diff, haven't been able to see from a glance what it does.
Most web servers/frameworks/libraries are probably agnostic about what db layer or xml library to use. Try Warp, Scotty, Happstack or Snap for a simple webserver-as-a-library and you'll probably be fine. My experience is there all fine and the exact choice doesn't really matter for at least small projects. Have fun!
Yes, you indeed may suggest this. That's exactly what I am trying to do. I have bought a book "learn Haskell", Manning editions and working slowly my way through it. After a while, I wanted to see if it was possible to quickly switch from using lambda to build strings about Mary, its lamb and its relative size to some real life application. Indeed it's too early and I shall resume my apprenticeship. Thanks for the link, too. Googling for "learn Haskell" yields a lot of links (including the aforementioned link) and it's always difficult to find the right one... If later I'm too stupid to understand the actual fundamentals of programming then I'll revert back to the sad old realm of other languages. Cheers!
Seems like a lossy compression algorithm.
&gt; The only downside I can see is that it doesn't provide a way to require a specific GHC version, What you see as a downside, I see as an upside. I have a couple of different versions of GHC installed and I have a shell function `ghcVersionSwitch` that modifies/updates the `PATH` environment. &gt; it feels more like a replacement for Stack than the absence of it. An alternative maybe. Mafia is a thin wrapper around Cabal. Stack is a much more heavy weight approach to the same problem. 
Will do, and yes, you're right about that; it's as you described in your original post. I just realized that a proofy approach would likely be more direct, and more amenable to optimization, than a nested type one.
Shouldn't monad be more like &gt; Nothing goes in, nothing comes out, can't explain that.
I'd have to disagree. Most of these statements are handwavy and (probably) targetted to beginners.
It does sound like the _most important element_ of a Monoid was missing from that sentence. Otherwise it's just a Semigroup...
traverse applies an action to each part, then returns the updated whole as an action. traverse isn't limited to list either.
/s? In case not, it only really does if you have an understand of what monoid, category, and endofunctor mean, *and* an ability to stitch together mathematical concepts like these.
It'd be interesting to find something to the tune of codensity that reassociates an Applicative to a tree shape (instead of right associating like codensity). That would get us merge sort on any data structure.
&gt; actual fundamentals of programming Do note that I did not mean to imply that Haskell is more 'fundamental'. It's just that Haskell is on a different basis. But you do seem to be joking there so I suppose it's fine...
Right. For example, at one point for some customer we had similar behavior that turned out to be caused by having more concurrent connections than supported by the PostgreSQL server for that site. That caused requests to pile up on the Haskell side. There are many things like that which can cause superlinear behavior. You need to look at the whole system.
Nothing terribly magical: https://github.com/diagrams/diagrams-lib/blob/master/src/Diagrams/Backend/CmdLine.hs#L567 
You know you have the right computational interpretation if your interpretation respects type safety. Type safety is a property that relates the static semantics (typing rules) of a program to its dynamic semantics (runtime behavior/computational interpretation). In a type safe language you have that: 1. (Preservation) If a term e of type t steps to an expression e', then e' has type t 2. (Progress) If a term e of type t is not a value, then e steps to an expression e' These might seem like trivial things but almost every language gets this wrong (including Haskell). At any rate, homotopy type theory is completely over my head, but my understanding is that it is not yet known how to interpret the "univalence axiom". 
See comment above. In terms of actually coming up with something like an API for specification, nothing other than the insufficient hand waving mentioned. Ergo, my question. 
I've heard there are periods when it's impossible to take a walk in Singapore because of extreme heat and humidity... So are you open to remote candidates?
so many downvotes for just saying what is the most pragmatic solution for a newbie at this level? Just look at what he's struggling with! And you want me to talk about the subtleties of which package manager to use?!
Hmmm, it's hard for me to give concrete advice because your descriptions are missing any of the actual specifics of what you're dealing with and what the API's need to contain. Could you write down maybe six (or as many as you need) natural-language descriptions of the kind of sentences about the product that you want to represent, and I'll have a think if I know of any formal system that matches that.
We're trying to put together a more scientific benchmark. Will publish once ready.
I don't think we will be competitive with America salaries. We're getting a lot of interest from US, but don't have the ability to match USD salaries. Unless someone wants to move to Goa :)
We've gone with Servant for now. If we hit some issues, we're depending on the Haskell refactoring story to be able to switch with confidence :)
For me there is three different concepts involved, all of them serve different purpose. - Parametric polymorphism. - Ad-hoc polymorphism, i.e. typeclass. - Genericity, i.e. GHC.Generics I'll give you examples of what you can do with them, and their limitations and why we need the others. Imagine you want to write a function which swap the two values of a tuple, something like: swap :: (a, b) -&gt; (b, a) swap (v0, v1) = (v1, v0) You want this function to work on any type `a` and `b`, but you really don't care about `a` and `b`. They can be anything, it will work. This is called *Parametric polymorphism* and you use it when you are able to write a function which is generic enough to work on any type with no limitations. This is what is known in Java as Generics, or in C++ as templates. Now, suppose you want to write a function which compute the area of something. You cannot write a function which works on any type and returns its area, for two reasons: - Not all types have an area, what is the area of a `String`? - I don't know any mathematical function able to compute the area of all kind of existing shapes So we need a way to write an unique `area` function which accepts any type for which computing an area is possible and we need to provide a specific implementation for each case. This is *ad-hoc* polymorphism and *typeclass* are doing this job in Haskell. For example, suppose we have theses types: data Square = Square Float data Rectangle = Rectangle Float Float We can define a typeclass `Area` which, for any `t` which is an instance of the class, is able to compute the area: class Area t where area :: t -&gt; Float Actually, the type of `area` is in fact `area :: Area t =&gt; t -&gt; Float`, which say that the function accept any `t` as long as `t` is an instance of `Area`. And now, for each instance of the class, we can provide an implementation: instance Area Square where area (Square s) = s * s instance Area Rectangle where area (Rectangle a b) = a * b This is `ad-hoc` polymorphism. In C++ this is implemented using template specialization or function overloading. It is usually a good idea to ensure that your instances follows some rules. For example, an area is supposed to be positive, so it is usually a good idea to ensure that all your instances returns positives value. You can do pretty much everything with theses two kind of polymorphism. Two rules : - If you can write a function which does not care about the input type and does not need any knowledge about them, then you can use parametric polymorphism and write one generic implementation for any type. - Else, use ad-hoc polymorphism, but you'll need one implementation for each type. Sometime, you'll meet a weird polymorphism problem. You have a function which works for any type, but with a different implementation, but you can write an algorithm to define these implementation. You don't want to write all theses instances by hand, there is a solution: `GHC.Generics` and friends. You may already have seen generic polymorphism, the `Show`, `Ord`, `Eq` typeclasses that you can "inherit" for free: Prelude&gt; data Square = Square Float deriving (Show, Ord, Eq) Prelude&gt; show (Square 5) "Square 5.0" Prelude&gt; Square 5 &lt; Square 2 False Prelude&gt; Square 5 == Square 5 True For all these typeclass you can write an algorithm: `Show` writes the constructor name followed by the different values. `Ord` compare arguments of the constructor in a lexicographic order. `Eq` checks if all arguments of the constructor are equals. So, to summarize: - If you can write a function which does not depends on any knowledge of the input types, use parametric polymorphism. - If you need to write a specific implementation for each type, use ad-hoc polymorphism (typeclass) - If you know a way to automatize the specific implementation writing for your ad-hoc polymorphism, you want to use `GHC.Generics` (or any other generic tool).
&gt; I am mostly discussing whether or not double "morally" obeys the laws. Just like how `bimap id id == id` is only "morally" obeyed by tuples, since `bimap id id undefined = (undefined, undefined)`. If you are using morally in the sense of "fast and loose reasoning is morally correct", that's specifically about behavior violations involving bottom / undefined / \_|\_. `Double` does not satisfy the requirements of monoid even when you remove all bottom / undefined / \_|\_ values from your reasoning. So, in that case, no, `Double` is not "morally" a Monoid. 
Is there just one server out there? Why wouldn't​ the load balancer be able to hide this?
No, Python 3 does not include a system for doing optional typing. Python 3 lets you annotate function parameters and functions with any Python object. You can use this for typing, or you can use it for something *completely* unrelated to typing. &gt;Worse, perhaps, is that the typing tools have runtime costs: user-defined types, aliases, and so on are instantiated objects. That's entirely intentional, and not a flaw.
Do you not find Reflex rather convoluted and unreadable? I mean look at the examples: e.g. {-# LANGUAGE OverloadedStrings #-} {-# LANGUAGE RecursiveDo #-} import Reflex import Reflex.Dom import qualified Data.Map as Map import Safe (readMay) import Data.Text (pack, unpack, Text) import Control.Applicative ((&lt;*&gt;), (&lt;$&gt;)) main = mainWidget $ el "div" $ do nx &lt;- numberInput d &lt;- dropdown "*" (constDyn ops) def ny &lt;- numberInput let values = zipDynWith (,) nx ny result = zipDynWith (\o (x,y) -&gt; textToOp o &lt;$&gt; x &lt;*&gt; y) (_dropdown_value d) values resultText = fmap (pack . show) result text " = " dynText resultText numberInput :: (MonadWidget t m) =&gt; m (Dynamic t (Maybe Double)) numberInput = do let errorState = "style" =: "border-color: red" validState = "style" =: "border-color: green" rec n &lt;- textInput $ def &amp; textInputConfig_inputType .~ "number" &amp; textInputConfig_initialValue .~ "0" &amp; textInputConfig_attributes .~ attrs let result = fmap (readMay . unpack) $ _textInput_value n attrs = fmap (maybe errorState (const validState)) result return result ops = Map.fromList [("+", "+"), ("-", "-"), ("*", "*"), ("/", "/")] textToOp :: (Fractional a) =&gt; Text -&gt; a -&gt; a -&gt; a textToOp s = case s of "-" -&gt; (-) "*" -&gt; (*) "/" -&gt; (/) _ -&gt; (+) Like... what? What? None of that is clear. Underscores in functions? Random unexplained operators like `.~`? `_dropdown_value`? This feels like some sort of autogenerated code that comes out at the end after some template haskell nonsense, not something that someone would actually write. I think the idea behind FRP is excellent, but every actual example of its use I've seen is hindered by an extremely convoluted presentation and poorly designed interfaces. The control flow here is unclear. The dependencies of each part on each other part is unclear. The idea of writing HTML-generating code with `el`, all of which is completely stringly typed (`el "div`??).. that sucks. This is like PHP. 
Except that it doesn't tell you what kind of chaining your doing. Applicatives are also monoids in the category of endofunctors, just with a different kind of chaining. The behavior of these kinds of chaining is the whole reason monads and applicatives are useful, so you can't just gloss over the word "chaining."
That only works if one knows category theory beforehand, which in the vast majority of times isn't the case. In general, "good intuition" is not good in abstract, but good *to someone*. 
&gt;There's not really any one-sentence definition of a monad. There is a very famous one sentence definition of monad...
Minor code/feature updates.
There's one origin server. There are mirrors and a CDN for spreading the load on the read side, so read-only tasks like users and CI systems building things are not interrupted during maintenance. Why just one host for holding and updating the state? It considerably simplifies the design and operations (which is really important for volunteer-maintained projects). ~3min downtime (for writes only) on a weekend every now and then is not a big disruption as far as we are aware. This could be improved if it is actually disruptive.
It's worrying to me that this post is meant to be about extensible records but it seems like all these libraries for doing it are *super* complicated. I hope the Haskell ecosystem doesn't become even more complicated. 
If you want to have functions with the same signature, but different implementations in different contexts, you might be tempted to make a type class. Generally, you shouldn't need to, unless there are also laws you expect implementations to adhere to. You can instead use a record type: data Hash = Hash { runHash :: ByteString -&gt; ByteString } sha256 = Hash $ … blowfish = Hash $ … 
Haha fair point. I meant to point out the circularity of explaining functor in terms of `fmap`, a functor's operation.
Tail is constant time. It's actually `length` that's slow; it's linear time with the length of the list. So using `length` just to see if there's more than one element is really wasteful. Not to mention, it's strongly recommended that you never use `head` or `tail` since they fail for empty inputs. `!!` is discouraged for similar reasons. I know it's "safe" here since you checked the length, but it's generally considered good practice to use pattern matching instead, which incidentally solves our `length` problem in this case as well. sieveStep [] = [] sieveStep (x:xs) = x : sieveStep [y | y &lt;- xs, y `mod` x /= 0] This is basically just a clean way to ask the list whether it's empty or whether it has a head `x` with tail `xs` so that we can write two different function bodies depending on that. I've found that 90% of the times where I'd use an `if` statement in other languages, I'd rather do it with a pattern match in Haskell.
`tail` shouldn't be the thing slowing this code down. For a start, try pattern matching on the argument to `sieveStep`: sieveStep (x:xs) = x : sieveStep [y | y &lt;- xs, y `mod` x /= 0] sieveStep [] = [] This is much more idiomatic Haskell than using `length` and an `if` statement. This also removes calls to `head` and `tail`, using constructor matching instead, which should help convince you that those functions are not the problem. To understand more about this code, you could look into list comprehensions and what they desugar to. To find a faster solution, you may have to look into different sieves. Be aware that finding primes gets slow much sooner than might be immediately obvious. Think about what the list comprehension in the `sieveStep` function has to compute when `xs` is very long, for instance, the length of `[2..2000000]`. 
There is a cyclic dependency in that code - `n` depends on `attrs`, `attrs` depends on `result`, `result` depends on `n` - which leads to the use of `RecursiveDo`. It's important to be able to have cyclic dependencies like this when you're working with this style of FRP. From my perspective, these kinds of things allows me to use reflex to put together some powerful / expressive / reusable code, quickly and easily, with pretty high confidence. I had to get comfortable with a few new concepts along the way, but for me it was worth it. If it doesn't seem worth it for you, then maybe don't use it? Or, if you think it's worth it but clunky, there's a pretty active IRC channel for reflex and they're pretty good with pull requests... I've found that with reactive-banana / with reflex but without reflex-dom, I have a huge amount of scope with respect to moving back and forth between being fiendishly concise and very explicit about how everything ties together well. I haven't spent much time trying to do that with reflex-dom in the mix, because I haven't been doing much front end stuff. It does seem there might be less scope to move around in that set of tradeoffs when the html generation is interleaved, or that there might be something else in the design space that might allow for that, but so far I haven't wanted it enough to poke around and try to figure out what an alternative might look like.
That's all kind of standard Haskell... Lens is common, if controversial. Underscores in fields is a common side effect of the records problem. Sounds like you have more of a problem with common conventions than with Reflex. Also, this really isn't helping OP in any way.
This article is really interesting. --- Semi-related: Personally I think (and this is controversial) that arithmetic overflow and loss of precision are *incorrect* behaviour for the default integer type of any high level programming language. Fixed-width types should be *explicitly* fixed-width (e.g. `Int64`), floating-point types should be *explicitly* floating-point (e.g. `Float64`). The default integers in any high level programming language should be arbitrary-precision naturals and integers (`Natural`, `Integer`), arbitrary-precision rationals (`Rational { numerator :: Natural, denominator :: Natural }`), and arbitrary-precision reals (`Real { atPrecision :: Natural -&gt; Rational }`). Then we'd have **real** justification for not having `Eq` for all numbers. If you want to go super hyper fast, you can use `Int32` and `Nat32` (much better name than `UInt32` if you ask me) and `Float32`. But 99% of the time you don't want that. And actually, this isn't slow. 
There are some more examples of it in the "Recursion" section [here](http://hackage.haskell.org/package/reactive-banana-1.1.0.1/docs/Reactive-Banana-Combinators.html). I think once you're playing with event-and-behavior FRP that does dynamic modification of the event graph, you're at the point where that kind of recursion is somewhere between pervasive and fundamental. With that said, it wasn't until I'd written a bit of this kind of FRP code that I started to really understand what I was getting in return for diving into the RecursiveDo rabbit hole. From the other side of that rabbit hole, I think it's totally worth it, but to each their own I guess.
Jesus christ reactive-banana is *much* better documented than reflex.
The guys developing Reflex are pretty busy, but documentation is near the top of their list of things to fix. It's definitely lacking right now.
Looks like your rationals are always non-negative. I.e. the numerator or denominator should be an Integer. Or add an explicit sign.
&gt; It's actually length that's slow; it's linear time with the length of the list Very interesting, I did not realize this. Thank you, I really appreciate the guidance; looks like I have more reading to do on patterns! 
&gt; And actually, this isn't slow. I’d be curious to see benchmarks on real programs where `Int` was replaced everywhere with `Integer`, including for things like `length`. (I say this seriously—I really have no idea what kind of effect I would expect, but I am a little wary of this argument without seeing some numbers.)
does this mean that the outage only applies to those pushing package changes and that all read operations (including cabal/stack) are uninterrupted? Or am i misinterpreting what's meant by "read-only" tasks?
what's the use case for using both servant and snap? Why not servant-server?
Sounds like an Android app changelog :)
That's fair enough.
If you aren't actually using integers that are too big, it should be very good, assuming that `Integer` is well-implemented, because it's actually not very expensive to check overflow on x86. https://danluu.com/integer-overflow/
Well, this time it was indeed a rather boring update. It was mostly recompiling with GHC 8.0.2 and newer package deps and restaring the hackage-server process so the dyn linker picks up newer C libraries that `apt {update,full-upgrade}` may have dragged in since the process was restarted last time. For the future I'd like to establish a regularly scheduled weekly maintenance window of about 5-10 minutes on sundays at a time where it affects the least amount of people to deploy minor low-risk changes/updates/fixes (if there are any waiting to be deployed).
I think you just need to read it right-to-left: "If you're looking for a `B (D a)`, then you need to go find an `A a`." This `A a` may then bring other subgoals into scope, search for those, etc.. Eventually you should hit a point with no remaining subgoals, and then you have a valid candidate!
Instance resolution in Haskell: - Only looks at the instance head for a match, not the context. The context is checked after the match. - Never backtracks when it finds an unsatisfied constraint after a match. It just fails. - Doesn't assume it knows about all the instances of a given typeclass (open world assumption). This means it cannot reason like this: "well, there's only one instance declared for MyTypeclass, so I guess that must be the type".
Well, there's the influential paper: "Qualified Types: Theory and Practise" by Mark Jones. As for instances: `instance A a =&gt; B (D a) where ...` says "For all `a`, if you have an `A a` then you also have a `B (D a)`"
Yeah just search for the title in Google scholar.
Nope sorry, I tried a lot of combination to call openRepository correctly to no avail. I have now given up trying to use the library (which I can't help but feel might be somehow broken) and I am now reading the Haskell documentation from the start. Thanks for the support though!
That's overflow detection, not a bignum implementation. For bignums you need tagging which is more expensive.
http://www.cs.uu.nl/docs/vakken/afp/slides/slides06.pdf is a good short introduction to Haskell98 instance resolution
&gt; 4 to be able to convert from and to normal Haskell record and generate record type from real Haskell record (find that in package but can't remember which one). `extensible` has [deriveIsRecord](https://hackage.haskell.org/package/extensible-0.4.1/docs/Data-Extensible-Record.html#v:deriveIsRecord) which allows you to use toRecord :: IsRecord a =&gt; a -&gt; Record (RecFields a) fromRecord :: IsRecord a =&gt; Record (RecFields a) -&gt; a
I actually have this on and it's a good book just for the overview of type theory.
I haven't done enough with macros to be sure about what they offer, but I think (some) macros are actually related more to comonads. Something in one of Robert Harper's OPLSS videos jumped out at me and lead me to that that quote / eval is comonadic at some level.
I don't know about macros, but in Haskell, the debate is usually between free monads and mtl-style classes. There is nothing you can do with a free monad that can't be done with a traditional mtl-style type class for a regular monad transformer. The main draw to free monads is their ease of use, and the potential to reduce the "n^2 instances problem." Basically, mtl-style classes require n^2 instances in order to have all the necessary instances of n classes over n transformers. Doing this recovers ease of use since you can use any effect from any level of the transformer stack, but it adds a ton of boilerplate. The alternative in mtl is to expect users to write newtype wrappers or call `lift` manually whenever they want to compose two transformers, but this destroys composability and ease of use. However, with free monads, you can use [Data types a` la carte](http://www.cs.ru.nl/~W.Swierstra/Publications/DataTypesALaCarte.pdf) in order to use any kind of effect freely without n^2 instances. Basically, you just need to constrain that the type level list of effects in the free monad contains the specific effects you need for your function. This is really the only tangible benefit in my opinion. It's the only thing that isn't cleanly represented with mtl-style classes. Free monads come with plenty of downsides. They have terrible performance, and there are monads that can't be expressed with a free monad at all. In general, I use free monads to prototype stuff, since it's really easy to write whatever monad you need this way. But eventually I'll try to go and figure out how to represent that monad more traditionally. I don't mind the n^2 instances problem because in reality, most applications have a newtype wrapper anyway. This isn't a catchall for the problem, but it's been close enough most of the time.
Impulse purchase. Hope it's good and teaches my how to use types more effectively in Haskell, as well.
This is my own ans, perhaps it makes sense, basically it seems to me that both ADTs and Typeclasses do the same thing from different angles - Typeclasses: collections of types which support the same operations ADTs: A function with an ADT as input can operate on a collection of Types So there is overlap, but, obviously they both have their niches. Perhaps there's some nice math-y analogies and connections between the two though...
Which is another known controversy (also, that case should really be using a `{-# OVERLAPPABLE #-}` pragma instead of `{-# LANGUAGE OverlappingInstances #-}`). See [this old thread](https://www.reddit.com/r/haskell/comments/4mrgeb/how_do_you_avoid_repeating_mtlstyle_instances/?st=j0rz4ch4&amp;sh=daeb52dc) for more detailed debate (in particular, [Ed Kmett's](https://www.reddit.com/r/haskell/comments/4mrgeb/how_do_you_avoid_repeating_mtlstyle_instances/d3y58to/) comment on how to raise problems without orphan instances, and another earlier [Ed Kmett thread](https://www.reddit.com/r/haskell/comments/4lvo34/delude_superset_of_prelude_that_allows_for_more/d3s6jog/?st=j0rzciei&amp;sh=84bee371)). The simplest argument you can take away from that thread is that the overlappable instance approach breaks in the presence of orphan instances. I know orphan instances are rare, but they do happen (actually quite often in test suites). I'd really rather not be the guy who generates a real head scratcher by doing something that's incoherent with the open world assumption. EDIT: It also doesn't always work, since there are plenty of overlappable instances you wish you could write that can't be done just in terms of `MonadTrans`, such as `listen` from `MonadWriter`. Sometimes you can get away with it using something like `MonadBaseControl` or something, but these things tend to get really complicated and impose obscure constraints.
This answer in another similar thread may help: https://www.reddit.com/r/haskell/comments/61mh3p/generics_vs_polymorphism/dffrdgp/
I noticed that - a very good read - but doesn't make reference to ADTs. I think this comment on the first example, "This is called Parametric polymorphism and you use it when you are able to write a function which is generic enough to work on any type with no limitations." Is a little off-the-mark wrt to ADTs... But as I said, great comment, and I learned a lot from it.
In short, the body of a parametric polymorphic is written only once for everything, example f x = x + 1 Is the same for any type where `+` makes sense. `+` on the other use typeclasses and can be overridden for each instance. The body of `+` for `Integer` is different from the body of let's say complex number. So with adhoc polymorphism, you only reuse the name (not the code) and need to implement a new function for each instance. 
Your Real is a "constructive Real", it isn't er... real. ;)
Thanks for the heads up - grabbed a copy.
It certainly is a real. A constructive real and a real are the same thing, because constructive mathematics is all of mathematics. 
&gt; A common way to accidentally call show on a String The other, way is when you write generic code using TH, heterogeneous list or even just polymorphic and don't want to know the type. I could write for example a function which display a formated table `displayTable :: Show a :: [[a]] -&gt; String`. But it doesn't work for String. So in a way, String is showable but I can never use it's instance in don't debugging purpose. &gt; There isn't actually an instance of Show String, but the instance of Show Char defines showList Well, that's actually a trick so that String can have a it's own instance without any language extension. 
This sort of works, but it cannot reach down inside of data structures with strings: {-# LANGUAGE TypeSynonymInstances #-} {-# LANGUAGE FlexibleInstances #-} {-# LANGUAGE UndecidableInstances #-} main :: IO () main = do putStrLn (altShow "done") putStrLn (altShow (5 :: Int)) putStrLn (altShow ("foo",6 :: Int)) class AltShow a where altShow :: a -&gt; String instance {-# OVERLAPPABLE #-} Show a =&gt; AltShow a where altShow = show instance {-# OVERLAPS #-} AltShow String where altShow = id Also, `show` is really only intended to be used in a debugging situation. By chance, the `Int` instance and the instances for simple enumerable types are human readable, but check out the `Show` instance for this: &gt;&gt;&gt; data Foo = Foo {fooAge :: Int, fooName :: String} deriving Show &gt;&gt;&gt; Foo 12 "hey" Foo {fooAge = 12, fooName = "hey"} You don't want to show that to end users either. Even the instance for `Double` is kind of suspect. Do end users really want to see scientific notation like this: &gt;&gt;&gt; print (555555555 :: Double) 5.55555555e8 I think the problem goes beyond `String`'s instance. I would just write my own typeclass complete with it's own default implementation using generics.
I'm sorry that this reply seems overly aggressive to you. That was not my intention, and I will try to reword my comment. I was speculating that OP already learned quite a few programming languages and got used to the feeling, where you can just pick up the language, learn the syntax, open some lib's docs and start working on some real-world web app or CLI utility. If you haven't learned a few functional languages with such a strong type system (I know OCaml counts as one), you're sure going to struggle much, much more with Haskell. This, quoting the reply from OP, is *exactly* what I was trying to discourage. &gt; After a while, I wanted to see if it was possible to quickly switch from using lambda to build strings about Mary, its lamb and its relative size to some real life application. I suppose I got too excited trying to emphasize that point, and the fact that I'm not native English may have limited my kindness of expression.
One of my favorite quotes from the Tim Sweeny POPL'06 slides that got me interested in Haskell in the first place: &gt;C# exposes more than 10 integer-like data types, none of which are those defined by (Pythagoras, 500BC). &gt; In the future, can we get integers right? [from ltu](http://lambda-the-ultimate.org/node/1277)
The issue that I found was that type hints were essentially not usable for any code that had to deal with library functions. Sure, my own pure calculations and manipulations could be typed, but it warns on anything that involves a dynamically created method -- which many libraries rely upon. I also dislike needing to instantiate objects in order to do type annotations when those type annotations aren't even enforced at runtime.
`let show' = let p = (== '\"') in dropWhile p . dropWhileEnd p . show`
&gt; That makes sense but it's really annoying because most of the time, you don't need those extra quotes (I'm sure pretty much all of us have been bitten by it). On the contrary, I like that behaviour much better! It's in other languages that I get bitten, I println a variable in an unfamiliar codebase, find out that it's a `Foo(4, 5)`, and only after much hair-pulling trying to understand why none of the `Foo` methods work on it do I realize that it's actually the string `"Foo(4, 5)"`. Since you're trying to construct a readable string for your users, not creating a debug string for yourself, I'd use `printf` (or one of its many type-safe variants) instead of `show`: import Text.Printf -- | -- &gt;&gt;&gt; helloworld1 "answer" 42 -- "the answer is 42" helloworld1 :: String -&gt; Int -&gt; String helloworld1 = printf "the %s is %d" `printf` only supports a few primitive types, so you'll have to call `show` explicitly if you want to format values of other types: data Foo = Foo Int deriving Show -- | -- &gt;&gt;&gt; helloworld2 "answer" (Foo 42) -- "the answer is Foo 42" helloworld2 :: String -&gt; Foo -&gt; String helloworld2 s foo = printf "the %s is %s" s (show foo) That is, it's the almost the same behaviour you get with `++`: with `printf`, you need to call `show` on values which aren't printable, and with `++`, you need to call `show` on values which aren't already Strings: -- | -- &gt;&gt;&gt; helloworld3 "answer" (Foo 42) -- "the answer is Foo 42" helloworld3 :: String -&gt; Foo -&gt; String helloworld3 s foo = "the " ++ s ++ " is " ++ show foo So if your problem with `show` is that you sometimes accidentally call it when it's not necessary, then `printf` won't help you there: -- | -- &gt;&gt;&gt; helloworld4 "answer" (Foo 42) -- "the \"answer\" is Foo 42" helloworld4 :: String -&gt; Foo -&gt; String helloworld4 s foo = printf "the %s is %s" (show s) (show foo) -- | -- &gt;&gt;&gt; helloworld5 "answer" (Foo 42) -- "the \"answer\" is Foo 42" helloworld5 :: String -&gt; Foo -&gt; String helloworld5 s foo = "the " ++ show s ++ " is " ++ show foo There are [other interpolation libraries](https://www.reddit.com/r/haskell/comments/3w4d9z/24_days_of_hackage_2015_day_9_template_haskell/) which, to my dismay but to your benefit, automatically call `show` on everything except Strings: {-# LANGUAGE QuasiQuotes #-} import Data.String.Here.Interpolated -- | -- &gt;&gt;&gt; helloworld6 "answer" (Foo 42) -- "the answer is Foo 42" helloworld6 :: String -&gt; Foo -&gt; String helloworld6 s foo = [i|the ${s} is ${foo}|] 
`printf` is actually a really good solution (but you still need to know the type of what your are printing).
&gt; A limit on the number of threads and a queue is probably the best option. Of course, this is also what the Haskell green thread runtime scheduler does :-) How can the runtime do that in the case of a web server if the programmer is the one that has to read the socket and assign the threads?. The green trhreads runtime does the scheduling of green threads. Is the programer the one who must assign threads to requests. One-request-one-thread strategy would overload the queue of the green thread scheduler and degrade latency. That may be OK for most tasks, but not for web applications, as you say in the second paragraph. So is _the_programmer_ the one that has to limit the threads, and perform a second level scheduling of web requests with a queue. That second level is the one you describe. So in practice, the green threads scheduler is not good for latency critical applications and should be overcomed with an scheduler in the programmer space that take into account the latency requirements.
I had looked at liquid haskell briefly and am interested in using it in production code (See for example https://github.com/raaz-crypto/raaz/issues/227). However, last time I looked at it, there was now way to use it within a cabal package. Is it possible to do this now?
In Python, integers are automatically converted to big (arbitrary precision) on overflow… Of course Python allocates everything on the heap :D But [PyPy can do tagged pointers for small ints](http://doc.pypy.org/en/latest/interpreter-optimizations.html#integers-as-tagged-pointers). Also [OCaml Zarith](https://github.com/ocaml/Zarith/blob/master/README.md) seems to do that: "small integers are represented as unboxed Caml integers" (63-bit). Does GHC `Integer` do this kind of thing? Also PyPy probably doesn't do this but, since it's a JIT, I think it could compile integer operations to normal unboxed 64-bit ones, and on overflow, recompile to use a stack allocated bigint.
I tried to make `extensible` the most powerful - but yeah, I'm also lazy to write documents, sorry. Which feature do you want more documentation for?
Does it not help to show them what $ and . translate into? For example: f x y $ g z becomes f x y (g z) and (f . g) x becomes f (g x) Maybe describe them as ways of moving inconvenient parentheses around?
By blank slate I meant teaching without any prior knowledge.. I was thinking that teaching haskell to a javascript programmer would be quite different, partly because you have a grounding, but also because some 'unlearning' has to be done. For example I think a fresh programmer would find automatic currying easier to swallow than an experience OOP programmer. That said, you make a good point. I need to find a way to keep the workshop about music, rather than CS though, while still covering some fundamentals.
Yes perhaps it's best to get them started without . and $, get them bogged down in counting brackets and then show them the way out :)
The book focuses more on real-world use cases than theory. Recommended for people interested in dependently-typed programming, Haskeller or not. This is definitely the best book of this kind; the syntax of Idris should be easier to read than Haskell when doing complicated type-level trickery (and of course is also more expressive).
Here is a [slide deck](https://speakerdeck.com/anler/uu-haskell-summer-school) from a participant of last year's installment describing the most interesting content according to him. He also talked about the course in the website [Katade](http://katade.com/2016/08/22/utrecht-haskell-summer-school/) (sorry, this second link is in Spanish).
&gt; i am trying to keep from a list given only the numbers that are greater or equal of i and less or equal of j &gt; &gt; &gt;fromTo (-4) 6 [0, 25..] &gt; [0,25,50,75,100,125] Hmm, 25 is not less-than-or-equal to 6. Do you mean that you want to keep the elements whose position in the array is between i and j, or do you really want the elements themselves to be between i and j? &gt; filter2::Int-&gt;Int-&gt;[u]-&gt;[u] &gt; filter2 y x filteredI = (filter (\e2 -&gt; e2&lt;= x) filteredI) What is `e2` here, is it an element or a position?
You might also consider checking out the [LinkedIn group]( https://www.linkedin.com/groups/8554287) that some of last year's participants started -- I'd be happy to reach out to former members if you'd like to discuss their experience.
How often do you find yourself reaching for non-constructive reals in programming?
Well I've been playing with `extensible` this weekend and you can take record, generate from TH it's conversion to an extensible record and then do something like hfoldMap (:[] . show) (toRecord myNonExtensibleRecord) By doing that you get a list of String with the value of each fields without any boiler plate. That's pretty handy for example to write an html table from an sql table or a csv etc ... However, this doesn't work because of the of the String-quotes problems ... (Also this doesn't actually compiles because of the Show constraints not being propagated properly, but there is a workaround). &gt; On the contrary, I like that behaviour much better! It's in other languages that I get bitten, I totally agree, and in the general case I'm happy with the actual Show. The problem is there is no built-in alternative for when the extra quotes are not needed (and you want some polymorphic function).
&gt; What is the easiest way to test TH codes ? While the `$(...)` syntax allows you to insert generated code into your program, the `...` bit is just a pure expression, so you can just check that the expression is equal to what you expect, or whatever else you usually do to test pure expressions. In particular, I like to use doctest, so [I test my TH using doctest](https://github.com/gelisam/category-syntax/blob/5f13d8c47c904070888c4c3a68270cebaf6be803/src/Tests.hs#L24).
&gt; How can the runtime do that in the case of a web server if the programmer is the one that has to read the socket and assign the threads?. The green trhreads runtime does the scheduling of green threads. Is the programer the one who must assign threads to requests. Yes, I was being cheeky here. The GHC scheduler obviously doesn't do pushback. &gt; One-request-one-thread strategy would overload the queue of the green thread scheduler and degrade latency. That may be OK for most tasks, but not for web applications, as you say in the second paragraph. So is the_programmer the one that has to limit the threads, and perform a second level scheduling of web requests with a queue. That second level is the one you describe. It depends, if you're not CPU-taxed then you can shove huge numbers of green threads at the GHC scheduler and be ok (example application: huge number of clients long-polling a pubsub topic), because *in theory* you only pay a scheduling cost for the green threads that get marked runnable by `epoll()`. What you want to control is the number of user threads that are simultaneously "runnable". &gt; So in practice, the green threads scheduler is not good for latency critical applications and should be overcomed with an scheduler in the programmer space that take into account the latency requirements. Again, it really depends on what the workload looks like. But yes, for all of the existing web/rpc frameworks that I'm aware of, if you want sophisticated policy re: load shedding / pushback / queueing semantics, you have to implement it yourself.
why is it slow? Can't you just use the first bit to reveal whether it's a pointer to an Integer on the Heap or the integer itself? I mean you would halve the range, but i don't think the upper/lower limit is that important. May be wrong though, just a quick thought.
I believe any full time student studying on campus is de facto affiliated with the university he/she is studying in.
also `MemberShip` and co is not clear either ;-)
It's so much effort to do good Haskell training because you cover hard CS ground with such a powerful tool. It makes a lot of sense to have the best, like Utrecht folks, to do this. Would be great to have longer training, but I see why it's hard : because instructors are top notch their time is precious. That's why it is so important to have common learning curriculum, with different trails : Not everyone will need lax 2-functors stuff.. but some do.
They are the bests, so if you need it, go for it
Having a goal is the best way to learn, you will get the concepts over time. I say go for it!
https://github.com/spinda/liquidhaskell-cabal#readme Personally, I never used it though. It was easy enough for my projects to just run `liquid` manually (small deps so this is easy).
Had been planning on buying the print book so this is very handy, thanks for the heads up OP :)
&gt; It's not possible. People have learnt FP as their first programming paradigm in the past.
A graph is just a list of nodes and edges with natural numbers mapping between them, and lists and natural numbers are both just lambda abstractions in disguise, and lambda calculus is just pompous SKI calculus, and SKI calculus is a mistaken search for the *ι* combinator, and I think ((ιι)(ιι)) and (ι(ι(ιι))) is just a lazy syntactic sugar for writing `0011011` and `0101011`. 
yeap thats great,i didnt thought it with the list positions. any chance that this could work on infinite lists? like the &gt;fromTo 1600 1600 [0..] [1599] or with any type of input like a list of chars? fromTo 20 30 [’a’..’z’] "tuvwxyz" EDIT: the second one works. 
Thank you @el-seed. I'll check out Scotty and Spock. (Likely to go with one of the two given your comments on Yesod).
Why wouldn't it? The zip function works fine with infinite lists. Are you getting an error?
Types in functional programming languages should play the role that interfaces and classes have in OOP languages. For example the type `type Parser a = Tokens -&gt; Either String (a, Tokens)` would be an "interface" for a parser as it defines what a parser is in this context. Expressions that use this type correspond to the *implementations* and functions that work on them. For example a parser `digit :: Parser Int` would be an implementation for that "interface". You can think of them as "constructors" in OOP lingo. Type classes have nothing to do with all that and unfortunately a large part of the Haskell community still gets this wrong. In fact many other popular FP languages don't even have type classes and do just fine without them. Type classes *are not* interfaces. They solve a very different problem. Suppose you want a type representing sets, which needs a notion of ordering for its elements. How do you include that? 1. In the type: `data Set a = Set { elements :: [a], compares :: a -&gt; a -&gt; Ordering }`. Now how would you implement a `show` function? How do you compare two sets for equality? You can't trivially compare functions with each other. And efficient union methods can't be implemented since the compare methods might differ for two sets. 2. As extra argument, for example `union :: (a -&gt; a -&gt; Ordering) -&gt; Set a -&gt; Set a -&gt; Set a`. This would solve the issues above but is cumbersome and therefore error prone. 3. Type classes: This is basically the same as 2. but the functions are supplied implicitly and, this is the important part, *coherently*. The latter means that all functions would receive *the same* function. This guarantee allows you to implement more efficient union algorithms as the equality of the comparison function is now implicitly the same. So to answer your question, use type classes to give your types properties that you want to be globally coherent and use types for everything else. Or, in OOP lingo, use type classes for non-virtual functions.
&gt; Any reason you are not using `ppr` (which I belive convert TH data to is Haskell text representation ) I am using it. The doctest I linked to documents the `showExp`, which is implemented in terms of `pprint`, which is a wrapper around `ppr`. The doctest compares the output of `show`, `pprint` and `showExp` in order to demonstrate the latter's usefulness.
These benchmarks are helpful, but the reason I said “real programs” specifically is that the argument appears to be that integral values crop up rarely enough in application-level code that the performance hit is negligible. Obviously this assertion varies depending on the program and its domain, but it doesn’t seem entirely unreasonable to suggest that many applications are not going to be bottlenecked by `Integer` addition. I think the big question is how much code in *libraries* could stay `Int` under the hood for speed and how much would need to be changed to `Integer` in order for applications to never be exposed to potential overflow problems.
I love using `$` in my code—but I actively avoid it when I'm teaching complete beginners or people who won't be primarily Haskell programmers. It's not that the concept is *hard* but that it takes a while to fully internalize. Before you do, reading code with `$` is confusing even if you know what it means; after, you don't even notice it any more. If you're just doing a disconnected workshop or two with an eye towards music—especially if people are just going to use Tidal to perform rather than for large projects—I'd punt on talking about `$` at all. It's not hard to reorganize smaller examples to not have egregious nested parentheses even without `$`, and it makes for one less thing to learn before getting started.
PM incoming.
Im not getting an error but after i run the function with these inputs i get [1599 instead of [1599] and the program doesnt stop running.
I don't know, that's why I'm asking :) I even forgot about laziness when writing that comment. Haskell types can be strict though…?
I've never used Idris, I've touched Haskell a bit and done a fair deal of ATS, but I'm still a low level guy. Is this worthwhile for me?
You can do that (for strict computations), but it's quite a few more instructions than the overflow checking mentioned in Luu's blog post. The instructions aren't horribly expensive, but even apart from execution time there are various machine resources expended on them that are excluded from more productive use - branch table entries, cache lines, pages, ITLB entries, etc. Affordable? Perhaps. Free? Nope. (Vectorisation is also made more difficult/impossible by both bignum tagging and overflow checking.)
I didn't realize. Where is the `baz` coming from in line 27 ?
Oh i see. Yes your filter has no proof that it won't find another sequence of indices in the future that match your condition, because it doesn't realize that the second part of the tuple is increasing. It's probably best to explicitly recurse fromTo from to list = go 0 list where go i [] = [] go i (x:xs) | i &gt;to = [] | i &gt;= from = x:go (i + 1) xs | otherwise = go (i + 1) xs You can also use foldr, which has a notion of 'quitting early': fromTo from to xs = foldr (\(i, x) a -&gt; if i &gt;= from &amp;&amp; i &lt;= to then x:a else if i &gt; to then [] else a) [] (zip [0..] xs) So basically, you annotate each element with the index in the array. If you are currently viewing an element in between `from` and `to`, you add the element to the rest of the accumulator. If you are viewing an element past `to`, just return `[]`. Because you don't return `a` in this case, the computation terminates once `i &gt; to`. Lastly, if you are viewing an element before `from`, simply return whatever's coming up.
The expression problem doesn't have much to do with type classes per se. It's a problem of types in general. Consider a type for people: data Person = John | Annie | Frank name :: Person -&gt; String age :: Person -&gt; Int The type is a closed union. You can add as many properties in the shape of functions on `Person` as you want but you can't trivially add more people. On the other hand the type data Person = Person { name :: String, age :: Int } john, annie, frank :: Person is "open". You can add more people without changing the type but you can't add properties.
I didn't say they couldn't. My point is that there is no easy way to teach programming. You have to go all in or settle for a superficial understanding.
now i see, so i can use foldr with my own function in combination with zip to index the list im having and also terminate it when i is going out of range, this is a very neat explanation, thanks for the time you took explaining this.
I gave `showExp` a slightly longer expression: -- &gt;&gt;&gt; showExp [|foo &gt;&gt;&gt; bar &gt;&gt;&gt; baz|] -- foo -- bar -- baz I used `[|foo &gt;&gt;&gt; bar|]` in the other two examples because the output of `show` and `pprint` was too verbose.
Sometimes a superficial understanding is all that is needed. Furthermore, it is perfectly possible to attain a superficial understanding at first and then, in a later moment, refine it.
It is! But I've moved away from using it in my slides and blog post examples and I'm happy with the result. I figure, if you have an example that results in more than a couple nested parentheses the example should be simplified anyway, $ or no $. And simple, well-factored examples read as well with $ as they do without $.
Sorry for being unclear! It means that you should either be enrolled at a university as a student, or employed as a staff member. Basically, as long as you're not working in industry, we can offer a discount.
&gt; Is it not picked by doctest (which happend to me all the time) Must be that, oops!
I've spent a fair chunk of time computing with arbitrary precision arithmetic. I'm just objecting to the name `Real`. I've used Lennart's `CReal`, which takes the representation offered here, but I find it is a rather poor fit for Haskell. If you need one more bit of precision you have to compute a whole new answer. It demands ever growing precision out of each of the terms pessimistically, always paying the worst case cost. It is a fundamentally "overly strict" algorithm. Given constructive reals there is also the consideration that you lose things like trichotomy. It isn't safe to even compare them, so they don't fit well with the rest of the numeric ecosystem. Representing constructive reals as continued fractions has the property that it is much lazier. You can demand drastically smaller portions of your arguments, but still wastes a lot of work doing redundant calculations. It has the benefit of a nice normal form as well for what you can represent this way. mvr_'s work on using continued fractions of continued fractions is quite clever and can represent more things, but loses the nice normal form property. My own work represents constructive reals as graphs of matrices that represent contractive linear fractional transformations. The graph structure improves sharing of work and in the end you can view the individual linear fractional transformations and their tensor like analogues as little dedicated stream processors with infinite buffering capacity and interesting fusion rules. All of these considerations make me think that there is still some work to be done on constructive reals before anybody really chooses "one true representation" for something like Haskell and bakes it into the language, but also to appreciate the subtleties and care not to over-sell what it is that we do have.
ok, i'm probably coming for the second time :)
Thanks for the clarification. That makes the whole thing really attractive for me. Is there a maximum number of participants and if yes is it first come first served?
Seconded.
Also, the kindle version is provided directly by Manning. If it doesn't look right, give me a poke, and I'll see what I can do!
Good point, I'll update the verbiage when I get a chance. Is there an easy way to get the exact peak consumption instead of eye balling the graph?
The problem is mainly that syntactic tokens (`do`, `\`, `case`, and others) cannot be given as a direct argument to functions. `$` is a workaround to this weird restriction, and the main problem is code like: forever (do lots of lines here) Much nicer without the parenthesis here.
The hack is terrible though :) A nicer thing to do is to simply allow `runST do ...` to work in the only way imaginable.
[iota I imagine](https://en.wikipedia.org/wiki/Iota_and_Jot)
Oh lol
That's facetious. Nobody is asking for a way to teach people F.P. without the learner practicing.
That's awful.
`hosum f n = sum $ fmap f [-n..n]`
This is easy to implement with a list comprehension: hosum :: (Int -&gt; Int) -&gt; Int -&gt; Int hosum f n = sum [f x | x &lt;- [-n..n]] Or as a map: hosum :: (Int -&gt; Int) -&gt; Int -&gt; Int hosum f n = sum (map f [-n..n])
Maybe I’m becoming a grumpy old PL designer, but as far as I’m concerned, a currency symbol doesn’t belong in a programming language unless it refers to, well, currency. `$` is an alright mnemonic for things that begin with S, like $calars in Perl and $trings in some varieties of BASIC, but the Haskell use is totally arbitrary.
yeap i was trying to compute it with a comprehension but i cant figure out how to include negatives as well, if you try it with a negative e.g. hosum (\x-&gt;1) (-8) the sum always returns zero.
Then why not take the absolute value, as in your original definition? hosum f n = sum [f x | let n' = abs n, x &lt;- [-n' .. n']] hosum f n = sum $ map f [-n' .. n'] where n' = abs n 
`filter` doesn't "work" with infinite lists. Try `filter (&lt;3) [0..]` sometime. You'll notice that it never actually gets the final `[]` / `Nil`.
I'd use: fromTo s e = take (succ $ e - s) . drop (pred s) But, that doesn't do quite what you want when the indexes are less than 0.
I'm a fan of the "pronunciation" school. `.` becomes "of" and `$` becomes "applied to" (or even "of" again). So you get `f . g $ x` becomes "f of g of x" or "f of g as applied to x". This already illustrates the difficulty though -- `.` and `$` can often both be used, but obviously are used somewhat differently. Maybe its better to _just_ introduce one or the other at first -- in which case i'd introduce `.` first and just not use `$` at all until you just want to show an easier way to write `(f . g . h) x`.
Isn't this just ap (&lt;*&gt;)? 
Idris is closer as a language to Haskell than ATS with respect to how the syntax works and overall "feel". But as a language, it's targeted closer to the domain of ATS than a dependently-typed language like Agda, and the book certainly focuses on how to use dependent types for verifying properties of practical programs rather than abstract theorem-proving. The "real world" chapters toward the end apply the general principles from the first half towards things like state machine representations of physical systems, network communication protocols, and maintaining security properties of systems like ATMs. I think it's got a reasonably good chance of being relevant to you, although I don't know enough ATS (or what you mean by "low level guy") to be sure. For what it's worth, my day job mostly entails device driver and network stack work in C, and I enjoyed the book. It may not be directly applicable to the language I work in now, but it definitely speaks to the general problem space I work in. It provides an interesting way to make and verify models/prototypes now, and maybe actual implementations in the future.
A nuance of your situation is the `(#)` in your DSL, which is not `(&amp;)` but has the same fixity of that operator. In the spirit of [sclv's suggestion](https://www.reddit.com/r/haskell/comments/61s5wr/how_to_explain_haskell_syntax_to_nonprogrammers/dfhecvf/), perhaps it would help to focus on `(.)` at the very beginning and then introduce `($)` as a way to specify the reach of a `(#)`. To pick a couple arbitrary examples from your docs, these snippets... d1 $ (spread' slow "1%4 2 1 3" $ spread (striate) [2,3,4,1] $ sound "sn:2 sid:3 cp sid:4") # speed "[1 2 1 1]/2 d1 $ every 5 (|+| speed "0.5") $ every 4 (0.25 &lt;~) $ every 3 (rev) $ sound "bd sn arpy*2 cp" # speed "[1 1.25 0.75 -1.5]/3" ... might be phrased as: d1 $ (spread' slow "1%4 2 1 3" . spread striate [2,3,4,1] . sound) "sn:2 sid:3 cp sid:4" # speed "[1 2 1 1]/2 d1 . every 5 (|+| speed "0.5") . every 4 (0.25 &lt;~) . every 3 rev $ sound "bd sn arpy*2 cp" # speed "[1 1.25 0.75 -1.5]/3" (It also might make sense to always use a `$` after `d1` in examples, for consistency and perhaps to emphasise that `d1` is what "plays" a finished pattern. In that way, `d1 $ -- etc.` is comparable to e.g. `liftIO $ -- etc.` or `for things $ \x -&gt; do -- etc.`.) 
is this something related to monads?
You're welcome. I think this blog series I wrote might be helpful for you. It walks through how to make a web server from a practical perspective (it's very light on theory). http://seanhess.github.io/2015/08/04/practical-haskell-getting-started.html
Don't worry about your tone, which was what I mostly expected when I posted here. As you point, I have already learned quite a few languages, and except for XSLT (it this even a language?), my approach of "let's rewrite an IRC bot" has always worked (I'm an actual cargo cultist). I knew that Haskell would probably be much more difficult to learn (after all, I've thought about learning this language for more than 20 years), but not that much. I probably overestimated my ability to cope with a functional strongly typed language, when I've already done production code in both functional and strongly typed languages. Your answer was straight in line with what I had expected, including a very good doc. As @bss03 mentions, my question is not really smart, but, hey, that's also a ~~nice~~ way to judge a community. Also, I'm sure that I should have started with something simpler of with a more popular library (one which has for example, actual working examples). I wouldn't have bothered anyone in that case. Nevertheless, the overall quality of the answers is very very good, and except for the trolling part around package managers (but everybody deserves to troll around package managers), I'm all but inclined to keep on digging. Have a nice day! :) 
Many programmers lack the intellectual empathy necessary to understand how code looks like to a beginner. I wholeheartedly agree with you, I use `$` all the time in my code, but would not use it in a one-shot teaching setting.
Make sure you're ordering the second edition. The first edition was a bit of a mess: http://i.imgur.com/CuFnWTX.png
Besides the recommendable Scotty &amp; Spock which have been mentioned in other comments there's also http://snapframework.com/ worth looking into. Oh, and btw, I don't like Yesod either.
IO operations call the OS and that uses FFI, and FFI stop the execution of any other thread running in the same core until the IO call is finished. That is a great problem of the green threads scheduler in Haskell, if that has not been fixed lately. Although this is is a very [complex issue](https://downloads.haskell.org/~ghc/7.0.2/docs/html/users_guide/ffi-ghc.html) Then you have a bottleneck. The solution is using in-program data caching, so the threads do not perform IO.
The comprehension I gave in my answer, `[f x | f &lt;- fs, x &lt;- xs]` is desugared using the list monad: do f &lt;- fs x &lt;- xs return (f x) And this turns out to be a common pattern, so you can also express it with the applicative `&lt;*&gt;` “ap” operator: fs &lt;*&gt; xs `&lt;*&gt;` has the type `Applicative f =&gt; f (a -&gt; b) -&gt; f a -&gt; f b`; specialised to lists, that’s `[a -&gt; b] -&gt; [a] -&gt; [b]`. So in this case, it takes a list of functions and a list of arguments, and applies every function to every argument. You will often see `&lt;$&gt;` “fmap” and `&lt;*&gt;` used together, to apply a pure function to the elements of containers (e.g., lists) or the results of actions (e.g., `IO`) without naming the intermediate results. For example, here’s a multiplication table: (*) &lt;$&gt; [1..3] &lt;*&gt; [1..3] == [(1 *), (2 *), (3 *)] &lt;*&gt; [1..3] == [ 1, 2, 3 , 2, 4, 6 , 3, 6, 9 ] == [x * y | x &lt;- [1..3], y &lt;- [1..3]] 
There's no maximum. We typically expect about 20-30 participants each year, so no need to rush your application. As long as you register before August 1st you should be fine.
I mean "morally" is much more broad than just specifically talking about bottoms. So no I am not using it in that sense. I am using it in the "close enough, and feels right" kind of way. 
Identifying and taking advantage of structural commonality like that is what a lot of higher math is all about, so you might enjoy a class on something like abstract algebra. Beyond simple examples, a lot of material you might find is going to head in that direction or towards Category Theory anyway.
I suppose my point is that superficial understanding is not understanding at all. To take an example, I don't think it is possible to understand superposition without understanding the maths behind it. You could aim for a superficial explanation but the concept is so alien to normal human experience that anything superficial will fall short of what superposition actually is. Another example is trying to get a superficial understanding of gravity as the curvature of space. Its such an alien concept that I just can't understand it at any trivial level. My overall point is that advanced concepts are totally different to nonadvanved ones. I guess that is why they are advanced. They cannot be learnt unless they are learnt in their own terms.
I suspect one way would probably be to build up a (length-indexed) difference list, run it to produce a vector, then sort the vector. Another way that might be simpler would be to use heapsort instead, with a size-indexed heap. If you use the right sort of heap implementation, it won't matter much which order you merge them in.
I do like this idea, and will play around with it thanks. It's a bit of a shame that | isn't very easy to find on a lot of keyboards!
Awesome. Just bought it! Hopefully I also read it...
Sorry. `&lt;*&gt;` is the infix version of `ap` with a Applicative constraint instead of a Monad Constraint. `&lt;*&gt;` is in Prelude (for recent version of GHC) and `ap` is in Control.Monad. Applicative is sufficient. All of your examples work as stated except for those where you're expecting unique results. The third required me to enable FlexibleContexts (don't know why, I just followed the compilers instructions :-)). If I move wrap the third and last examples with nub composed with sort, I get your results. So all together: Prelude&gt; :set -XFlexibleContexts Prelude&gt; import Data.List (nub, sort) Prelude Data.List&gt; apply = (&lt;*&gt;) Prelude Data.List&gt; apply [abs] [-1] [1] Prelude Data.List&gt; apply [(^2)] [1..5] [1,4,9,16,25] Prelude Data.List&gt; sort . nub $ apply [(^0),(0^),(\x-&gt;div x x),(\x-&gt;mod x x)] [1..1000] [0,1] Prelude Data.List&gt; sort . nub $ apply [head.tail,last.init] ["abc","aaaa","cbbc","cbbca"] "abc" Prelude Data.List&gt; apply [(^2),(^3),(^4),(2^)] [10] Prelude Data.List&gt; nub $ apply [(*5)] (apply [(`div`5)] [1..100]) [0,5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100] Note that the (sort . nub) isn't the most efficient way to do this, but was the most straight forward I could think of.
Again, it doesn't matter what your stupid book says. You can't just take over the term 'typed' and use it however you like. Python is typed. Dynamically typed. Haskell is typed. Statically typed. If you want to refer to static typing, call it static typing.
Macros are the thing that let the *user* define do notation or arrow notation or idiom brackets, in languages with macros like Lisp. Haskell doesn't have macros. It has quasiquotes and reader macros via template Haskell. Macros are compile time, free monads are a runtime interpretation of the structured DSL tree. They're for defining limited DSLs that look a bit like monads. Do notation is basically a macro that works for monads, so you don't have to write a new macro for it. Haskellers are mostly averse or ignorant of syntactic abstraction found in Lisps so don't expect anyone to know what you're talking about.
Dot is pretty easy to explain to musicians: it's chaining two effects boxes. Dollar's harder, but you could describe it as the stuff on the right is the signal and the stuff on the left the processor.
Indeed. haskell has a set of predefined macros called type classes, do and so on that allows some limited kind of programs in the hope that these limitations apply to bad programs and not to good programs.
I mean, all things considered, not using that name is for the best.
Is the book also available for free? Somewhere online as PDF?
We have just defined a couple classes, `Text.PrettyShow`, `ByteString.prettyShow`, and `String.prettyShow`, which have the semantics that you don't need `prettyShow x` to be valid Haskell. The number of times you need that behavior is low enough that the extra package isn't prohibitive.
Yeah, it's optimized for a VR headset for sure but it does work just fine on OS X with basic WASD flying and mouse interaction. Shouldn't be too much work to make it fully usable.
I don't understand why ppl don't like yesod. scotty is good for super simple stuffs, but once you need session, authentication, resource pool, then you might as well use spock, because spock has these built in (and provides a very nice "funblog" example, which covers everything). But then you still need db, template, form solutions. Yesod provides everything, and the scaffolding template is top notch. The documentation is second to none. I think yesod is much easier to understand for beginners (the hvect stuffs in spock gives me headache, and I have no idea about type operations so servant looks like magic to me).
Yes, it should be fine for you. In fact, that was my only problem with this book: it doesn't escalate quickly enough and definitely not far enough. 
It helped me a lot. I keep coming back to this series time to time. thanks
Thank you - this is a great resource and one that I will be referencing.
I left out `into` because I was sticking to left-flowing operators, but yep. :) `over` and `across` both work for `&lt;$&gt;` (I've both used and heard "map over" and "map across" in real conversation) but I feel like `over` works for `&lt;*&gt;`. Definitely a bit arbitrary, but better to have the mnemonic IMO.
Oh! I just remembered another one I came up with for `&lt;*&gt;` -- `against`. Slipped my mind since haven't been working with Haskell enough recently to have actually used it.
&gt;Who needs HashLife when you have comonads? I was hoping you'd benchmark against HashLife!
Now that you mention it, I recall seeing that behaviour while keeping an eye on the task manager, in advance of having to interrupt a potentially leaky function in GHCi. No clue about what is going on either.
AFAIK, the header is not used to compile your code or determine the interface of the function: GHC entirely relies on your declaration. You can hence safely remove the header name, but it serves a useful documentation purpose.
&gt; Much nicer without the parenthesis here. /u/chrisdoner and I would disagree with you :)
They aren't, unless you mean the given example injections are isomorphic to their image (duh).
GHC seems to just *reserve* a contiguous address space region instead of having to move everything everytime the current region can't be grown any further. That has nothing to do with the actual memory allocated, it's rather a book-keeping thing to tell the OS which virtual addresses are already blocked (so they can't be occupied by some `malloc` call in a C dependency). At least on Windows, you can get that by passing `MEM_RESERVE` to [`VirtualAlloc`](https://msdn.microsoft.com/en-us/library/windows/desktop/aa366887\(v=vs.85\).aspx) as allocation type.
&gt; run it to produce a vector, then sort the vector. This is effectively what the sort in the article does. If you're deferring sorting until after the effect is done, then the easiest way to do that is just with `Ap (Mono x y)`.
Yeah perhaps you are right actually. My initial claim was probably too strong. Perhaps it should have been: *superficial understanding is not correct understanding*. The switch from Newtonian mechanics to General Relativity is a good example of how understanding something that is a genuine novelty in yesterday's terms would let us down. Even the vocabulary was radically changed (space went from being something that was absolute to something that was itself relative). In situations like this superficial understanding only gets you so far. I do think the same is true with programming. Either what you say is incorrect (because its too shallow) or it is hard (because it is not shallow enough). But, as you say, maybe we all start with an incorrect understanding of something until we can go into more depth. 
There are many examples of this, if you have ever defined a binary tree data Tree a = Tip a | Bin (Tree a) (Tree a) you are really using [`Free Pair`](https://hackage.haskell.org/package/free-4.12.4/docs/Control-Monad-Free.html) data Free f a = Pure a | Free (f (Free f a)) at `data Pair a = Pair a a` (try substituting `f` with `Pair`). How about rose trees? data Rose a = a :&lt; [Rose a] Well they are [`Cofree []`](https://hackage.haskell.org/package/free-4.12.4/docs/Control-Comonad-Cofree.html) data Cofree f a = a :&lt; f (Cofree f a) this gives you loads of instances for free, import Control.Comonad.Cofree type Rose = Cofree [] rose :: Rose String rose = "AB" :&lt; ["ab" :&lt; []] &gt;&gt;&gt; sequenceA rose ['A' :&lt; ['a' :&lt; []],'A' :&lt; ['b' :&lt; []],'B' :&lt; ['a' :&lt; []],'B' :&lt; ['b' :&lt; []]] Same with [`data NonEmpty a = a :| [a]`](https://hackage.haskell.org/package/semigroups-0.16/docs/Data-List-NonEmpty.html) which is `Cofree Maybe`.
Is there a two line summary describing *what* this talk is about?
Ah thanks. I was getting my hopes up. I thought it was how to get a web UI and multiple mobile UIs from the same codebase :)