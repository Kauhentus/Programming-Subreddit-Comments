That wouldn't solve the problem described in the article. At least until the internal/external deps distinction is implemented (see my other comment).
Please be more specific, I'm afraid I don't follow you.
OK, so what is your proposed replacement?
How do you do it manually?
There's an easier way to do exclusions: -- This can be a top-level declaration -- There is no point putting it in your `do` block exclusions = ["the", "be", ... "at"] main = do ... let chop6 = filter (`notElem` exclusions) chop5 Also, if you find yourself using a bunch of intermediate names (i.e. `XXX1`, `XXX2`, ...) that are meaningless, consider using function composition instead: let top20 = show -- put comments for what each stage does here . take 20 -- get just the 20 best . filter (`notElem` exclusions) -- remove common words . reverse -- order descending . sort . map (\x -&gt; (length x, head x)) . group -- group those words! . sort $ chop
Thanks, much neater than what I came up with. I'll try to tidy up my code :) Peter
&gt; A responsible maintainer should always keep their packages compile with the latest versions of everything. One problem with that, is that the hopefully responsible maintainer (who is typically the same as the original author) does not have access to the latest versions of everything (or even don't want to have the latest versions of everythings). Example #1: OSX 10.5 is not supported by GHC HQ. Example #2: I prefer the OpenGL bindings from 5 years ago compared to the most recent. In any case, often a single maintainer, whose job is not to maintain packages he wrote as a hobby, has a single machine, with a single version of everything, possibly with the impossibility of having more than a single version of everything (cc cabal hell). tl;dr: It's pretty unreasonable to require the authors (who donate their work to the community) to be up-to-date with everything. EDIT: and I personally, while I'm very happy to publish and polish code which I think would be useful for others, really don't want to spend my time and patience to be up-to-date with whatever recent fucking changes was made with the "standard" libraries etc, especially (as I pointed above) that I cannot just simply install the "latest-and-greatest" stuff
Yes, it does require a lot of effort, but communities can do it. Debian (and others) have been operating similar things at a much larger scale for quite some time.
I'm not very familiar with Leiningen, but this doesn't sound much different from the sandboxing implementation we will have in the next release, with the exception that compiler is not installed into the sandbox, but specified on the command line with `-w`.
&gt; Unless it is my day job, I'm just donating code. Oh, certainly. I meant the group to be opt-in for maintainers; you can still drop code on hackage without being required to sign a life-long maintenance contract.
&gt; I love Haskell Wait until you find out about codata and the halting problem.
I think he'll learn about lack of simple dependent types and forget about Haskell forever even sooner
I agree with this article very much. Since having multiple social version of hackage needs some time to realize, I suggest the most lightweight solution for fast-fixing some problem in accordance with a part of the same philosophy. How about just having snapshots of the hackage everyday? A newbie does not always want to use the newest package of X. Just want to see working X. At one point, there must be a moment when package X (say yesod) and package Y(say Fay) were compilable against each other. So when a newbie asks about failing to install, somebody just tell 'use 20121110 version of hackage'. This addressability to some working state should be the first step towards solving this. We already have the tools. snapshot of hackage can be just defined by versions of all packages and cabal-nirvana. So just maintaining a website for everyday version status and polishing existing tool are needed for this. I think that time-parameterized hackage is also compatible with functional language paradigm (like FRP) ;-P
I think your first problem _is_ more or less solvable. When a user wants to install an experimental package, they should be urged to set up a sandboxed build directory that can reference already-installed packages that are compatible, and use a shadowing database in the sandbox to install a set of packages that are mutually compatible. The good news is that we almost have this technology already! We still need some UI work to walk a user through the procedure. When the user tries to install an experimental package, cabal-install can provide two options: go ahead and install the package in the default package database, or initialize a new sandbox in the current directory to let the user work with the experimental package. Maybe an install plan for the experimental package won't be found, but if a plan can be found, it won't clobber previously installed packages.
Can you explain what Leiningen does differently? Preferably with an example.
It already works (and has worked for a long time) this way — you can install both 0.4 and 0.5. The problem is that you can't use both at the same time. If a package indirectly uses both 0.4 and 0.5, it may potentially happen that it uses a type from version 0.4 where the same type from version 0.5 is required. But the implementation of the type in different versions may be completely different, so it's problematic.
I'm scared of the idea of making changes by anonymous voting. This way no-one is personally responsible for the results.
Can you explain in more detail how it'll work in case if I want to make a non-compatible change to a "stable" package?
You'd make the change, and release it to Hackage. Then you'd ask library authors to verify and update their libraries. When you suspect they've all done so, you'd submit the version to the autobuilder, and either it would succeed, or you'd get an email back detailing which build or test failed. This does enforce that only one package changes at a time... so if you need several packages to change simultaneously, then things get more complicated. You probably need some kind of preprocessor or cabal flags magic in the dependent packages... but, the truth is, you needed that anyway if the dependent packages are "in the wild". Perhaps if it's just a small set of closely related packages (hamlet and cassius, for example), you should be able to make an atomic submission to update all or none.
So, a new major release of, say, `bytestring` can't be made until all the "stable" packages depending on `bytestring` — even ones that are long forgotten by their authors — are upgraded? I.e. never?
Who said it has to be anonymous?
I'm joining the cabal discussion very late, and I'm not totally familiar with all the issues, but would it be fair to say that a lot of issues would go away if incremental versions of individual packages were not allowed to break forwards compatibility? It seems the type system might be able to enforce something like this, so that a history of package versions becomes a chain under the "subtyping"-relation lifted up to the module level, where by subtyping I mean refining a type by introducing e.g. a type class or new type parameter. Removing, renaming or changing the type of a definition would require you to fork the package. Since each package history is totally ordered, one could then also determine version constraints by examining types.
This has ALWAYS been ugly in most languages, and there is no simple fix. 
Sorry, perhaps "anonymous" is a wrong word. I mean, you're proposing the changes to be accepted by majority of votes, regardless of whose those votes are. Given that anyone can get a hackage account, it doesn't help much if you can see names of who has voted if those names don't tell you anything. And again, no-one is responsible for the changes.
Awesome, thanks. What do you think of Scala?
Well, perhaps voting privileges (distinct from package upload privileges) might require some form of identification to avoid this kind of abuse.
Sounds like there's a chicken-and-egg problem, too. If we make a breaking change to `bytestring`, it can't be blessed until it works with the existing blessed packages. Let's say `bytestring-21.0` introduces this breaking change, `dependentA-5.0` is a blessed dependent of `bytestring-20.0`, and `dependentA-5.1` is a non-blessed, Hackage'd dependent of `bytestring-21.0`. Who gets blessed first, `dependentA-5.1` or `bytestring-21.0`? I'm just talking out loud here, and now that I've done so the answer is obvious enough: they must be blessed as a unit. This does still leave /u/roche's question, though: how to ensure that `dependentA-5.1` exists in the first place? **Edit:** Basically, this sounds like it would lead to Debian's much loved "mega-freeze" situation.
Maybe a stupid idea because there's way too many combinations and too much data would be generated, but couldn't we get cabal to automatically report to some "home" every result of builds attempted on everybody's machine, so we could keep track "this combination works" or "this combination fails"? So when somebody runs into an incompatibility it would be uploaded, and other persons trying the same combination of packages could be warned by cabal first? OK, ok, it's late...
I have no connection with 10gen or with this position -- other than that I use both MongoDB and Haskell on a regular basis -- but it's the kind of job I would submit my resume to in a heartbeat if I were living in New York. I thought you folks would appreciate knowing about this.
Right... so ultimately, either those packages need to get updated, or they need to no longer be considered stable. There really are no other options. (Okay, when the private dependency problem is fixed, it would be okay for a stable library to internally use an obsolete version of bytestring with an upper bound... just not to expose it in the API; but that's true across the board for all versions). So how do we fix this? In essence, what you want is to start over, adding the packages that are compatible with the new bytestring to a new generation... and ultimately kill off the old generation and switch everything over to the new generation, losing any packages that never made the jump. There are some interesting policy questions as to when a new generation gets created and who can do it and when you flip that switch, but the mechanism seems simple enough. That's the best we can do. It's self-contradictory to want to continue to include packages forever in the stable list but also upgrade base packages without waiting on them to be fixed. So either you have to make the bytestring maintainer fix all the dependencies (not feasible) or you need a process for removing packages from the stable list until they are updated.
This works until you add the `yesod` package, which then locks down half of the Haskell ecosystem. :)
What's wrong with nix?
Sounds like a good idea. Some thoughts: * It might be a good idea to make this all depend on Haskell Platform, either a specific version or test building against a number of Platform versions. By this I mean that your proposed package set would never include a package from Platform with a different version. * As for the testing of this all, sounds close to the goals of the Scoutess project. It's not there yet, but would be great to use it eventually and if people could get interested in Scoutess and contribute to it so it can get there faster. * What you are proposing is basically what Linux distributions are already doing, for themselves, when packaging for Haskell. Cooperation and coordination with Linux distributions on this would be great. It would be great for beginners if they could simply apt-get or yum install a recent version of this package set and be golden. No dependency hell, no waiting for compilation. A problem with this is if their Linux distro later ships updates to these packages (i.e. a new version of your package set) it will break their own user-installed packages. The solution then is to have the Linux distros not do this except for critical security updates, and to document how to deal with this situation when it arises (aka. `rm ~/.ghc` or `cabal install world`). * When you talk about GitHub, are you saying that every package in this package set should be on GitHub? There are many Cabal packages in darcs, even if they're in the minority. Certainly the package set specification itself and the related tools could be on GitHub, but it shouldn't be a requirement for included packages.
Huh. I've never seen that style of layout for a long chain of function composition. It makes a lot of sense. Nice tip.
It's the ancestral cousin to `do` notation.
I suggest: 1. Write your own instance of Storable PktHdr. 2. Submit a patch to the pcap library 3. Use Stack Overflow for well-formed coding questions.
I imagine this would require: * Cabal support, to allow the user to specify that some deps are internal * compiler support, to check that a dependency is internal * possibly linker support
&gt; or they need to no longer be considered stable Quite ironic, given that in the described situation it's them which are truly "stable" (i.e. non-changing). &gt; That's the best we can do. I don't think so. Contrast this with [my proposal](http://www.reddit.com/r/haskell/comments/1306wn/solving_cabal_hell_vetted_packages_multiple/c6znynm). I don't think that a library author should take responsibility for the users of the library, for the reason described above. That's just unreasonable, since you don't have any control over it. On the other hand, it is reasonable to expect a package author to respond to changes in the package's dependencies, since he/she has control over which dependencies to use. If he/she is unhappy with a particular dependency or with their number, he/she can change the package to use other or less dependencies (or remove it from the 'maintained' list).
what exactly makes sandboxing a hard problem? or do people just object to the wastefulness of recompiling/reinstalling the world every time you install a new package?
I do it like [that](http://www.reddit.com/r/haskell/comments/12alw1/why_inbreeding_is_bad_for_your_community_cabal/c6trojp) . Just add extra package-db before last one. The last one is where cabal installs packages.
That's kind of nice.
What subtyping does O'Caml have? Objects and variants are handled by row-variable polymorphism.
Why not simply create a haskell distribution much like a linux distribution. On some schedule, say every 6 months. Get a group to build up this set of packages and their versions. If a package wants to introduce a breaking change, it goes into the next release. Then library authors can check out this version of the platform and ensure that their library compiles against it, and insert their package for inclusion into the release. Developers can pull the release as it stands at any point to ensure that their library builds locally...no need for a build-bot for that, just use the developers machines themselves! Isn't this what other big things like linux distros do? Just different 'branches' on a source tree! To be honest, just getting a working environment, is pretty much impossible. Forget about trying to do it on Arch, GHC 7.6.1 doesn't even work with darcs! I'm trying to get a working system on Gentoo now...also seems hard. Strictly using emerge, but xmonad wont compile. So I've tried out THE two premiere distros for haskell and neither 'works' I'm not going to use Windows or OS/X. There should be someway for me to get ghc, darcs, xmonad, xmobar, yessod all on one distribution that can all be described on one wiki-page, that is guaranteed to work. Newbies have NO PROBLEM with being restricted to certain versions of software. Having the latest bleeding edge has no value if you can't even get *anything*! I think if people focused on at least getting a nominal 'distribution' working, that other things can certify against...it'd be 99% of the problem solved. Honestly, people mostly just want to get on with coding in Haskell! 
I second the point that it is unreasonable to have library authors be responsible for dependency maintenance. There's a reason that Linux/BSD distributions work the way they work.
Hackage2 will at some point probably have a scoutess daemon running. It will spot a *lot* of problems (version bounds compatibility issues, usual build issues, etc). Hackage2 will also let you (the maintainer) edit the cabal file on Hackage directly, without having to upload a new version. I think more efforts should go in that rather than in auxiliary tools and whatnot. Also, a Nix-style branch of cabal is being worked on. Feel free to help.
They may want to leverage the power of the watercooler: haskell evangelism may need to take place both within and without!
&gt; ... but my trackpad is broken j/k for up/down, a for upvote.
I agree with you, but the problem is complicated by packages that fall into the large gray area between experimental and production ready. As a novice haskell programmer, the impression is that most packages continue to sit in between those two extremes for a very long time. It doesn't help that there are many, many package version numbers that still start with `0.__`, so telling the "expert/experimental" packages from the vetted ones isn't terribly easy unless you're already a part of the ecosystem.
That's true. Fortunately, the focus of the vetting process here is quite narrow, its only aim is to ensure that you can actually install the package. Whether the package itself is useful or bug free is an entirely different question.
The evangelists are the ones who are turning people away from Haskell. Intrest in Haskell will grow more healthily if people take inspiration from it instead of making overly broad and elitist statements about functional programming. Then people would flock to the source of inspiration and want to learn more. Evangelists are, by definition, close-minded individuals who preach that one way is better than another and ignores all evidence to the contrary. People hate evangelists.
I recommend Agda. Epigram isn't in development anymore. Coq is an ugly language but the most mature. I love Agda, but I also quite like Idris.
I agree with you, but I think it's possible to advocate for Haskell without "making overly broad and elitist statements about functional programming." Haskell has benefited from the "avoid success at all costs" mindset (aka, let's be uncompromisingly correct), but history is littered with great technologies whose communities assumed that others would see the light and fall in line (c.f., the history of Lisp Machines).
I don't agree. A good evangelist (though the term may be badly chosen) knows the good about a language as well as the bad, and doesn't hide either. For Haskell, I would call Simon PJ a good evangelist.
I think the correct term for what they are looking for is be [Apologist](https://www.google.com/search?q=define+apologist), but this word is not in common usage and would probably just lead to confusion.
I saw this before, but decided to actually apply this time. Wish me luck! :3 Here's hoping that my resume gets past the drones.
This is good information! What I am looking for is exactly "Haskell with dependent types". So I would go for Idris. 
I looked on the site and it looks like you are correct in terms of the use of Haskell. They are actually trying to promote the use of MongoDB which means they will be ignoring any plausibly better solutions, and for that, they are evangelists.
Is any of these languages can be used for things that a normal software developer would do? Actually, even less than that, solving common algorithmic problems. I don't see anyone using Coq or Agda to solve problems on project euler.
I sooo agree with you.
There is the [Leroy Compcert C Compiler](http://compcert.inria.fr/doc/index.html) that was written with the assistance of Coq. I'm not sure about Agda though. The domain that they operate in are typically in academia and not industry.
I'm surprised that nobody have mentioned [Twelf](http://twelf.org/) yet. Based on the rather simple dependent type theory of LF (see [Harper et al - A Framework for Defining Logics](http://www.era.lib.ed.ac.uk/retrieve/273/Framework_Def_Log.pdf)), it's a simple, declarative language (think prolog with dependent types) that is quite easy to work with once you get the hang of it, and the totality and uniqueness checking makes it quite powerful for theorem proving. It isn't terribly used though, and I'm not sure if it is actively developed (outside of routine maintenance).
Some folks also find very convenient and affordable housing in, e.g. Queens, or New Jersey. It's not Manhattan/Brooklyn waterfront or nothing, you know :-)
Unfortunately, Twelf doesn't have the greatest potential to be a programming language. It doesn't have an FFI or linking mechanisms. If what you need is to prove properties of systems with a notion of alpha substitution and some context, then definitely use Twelf. 
Those places are still twice as much as the cost of living here. Nice 2BRs are $800/ month and nice houses are $200,000. Also all of my friends and family are here. Why do employers think their bullshit meetings and face time are more important than our families?
http://www.forbes.com/places/oh/columbus/
`mapM` with `readFile` is giving me `openFile: resource exhausted (Too many open files)` -- there are about 5000 `product_details` pages I've cached and am trying to parse, which is what the `skus` identify. I'm assuming this is one of the big problems that using `pipes` addresses, but I'm having trouble figuring out how to get `readFile`/`openURIString` into a `Producer`/`Server`. Are there any libraries that offer this stock? ---- For reference, here is my last functioning commit (incorporating `Control.Error`, and successfully recovering from malformed urls): [Github](https://github.com/gadtfly/BCL-data-hs/blob/develop/BCL.Data.hs) ---- Sorry to keep bothering you after the fact. I'll probably figure this out on my own, but if it wouldn't be much trouble I would appreciate a push in the right direction. Iteratees/Pipes/etc are a lot to wrap my mind around in parallel with a lot of other new concepts I'm being exposed to in close succession, in the process of writing my first serious Haskell program. (all my prior Haskell experience has been privately exploring new concepts individually, and without a time frame; but this is going to see the outside world, in providing data for the projects I'm doing for a pair of classes)
No, it's a longstanding nickname :) http://614columbus.com/article/columbus-200-cow-town-live-with-it-5504/
I don't know much about it but ghc does have an RTS: http://hackage.haskell.org/trac/ghc/wiki/Commentary/Rts Did you mean a VM instead?
Awesome, thanks again. I would also love those things, and enjoy teaching(/preaching) Haskell in what limited scope I can be confident I know well enough to speak about. I dream of an opportunity to take a real course teaching Yesod or Snap, beginning to end. Similar to something like [Bloc.io](http://bloc.io/), which I'm currently taking to learn web development in Rails -- which is pretty much the only option, and is taught in [every one of the several of these courses currently being offered](http://www.quora.com/Programming-Bootcamps/What-are-the-short-term-3-12mo-development-bootcamps-academies-around-the-world)
Perhaps I am guilty of using terms imprecisely, and perhaps VM is the more precise term, but yes, I know Haskell has an RTS. I was asking about a run-time system *like the .NET CLR or JVM*.
Except almost all of 10gen's positions are New York / Palo Alto, not just this one. I knew a guy here that'd be a great fit for the job, and he moved to Portland. Last I heard he's really happy there. Why does 10gen expect these people to move to NYC or SF, when they're quite well established in Portland or Cambridge or wherever? In this case, beggars can't be choosers, and 10gen are the beggars.
Ironically, I have not been able to get cabal-dev installed on this machine (running Windows). I'll spare you the details, but the problem actually lies with the network package, upon which cabal-dev depends. My problems notwithstanding, I agree that cabal-dev is a nice solution to a lot of otherwise vexing package problems. But does it solve everything? For example, it would not allow me to reference two different versions of the same package in the same application, would it?
Coq is very verbose and hides a lot of things. That's just a consequence of tactics tho, I suspect.
Verbose, ML-like syntax, also not happy about the untyped tactics language.
Well said. 
I personally don't care too much for syntax but I understand some people may not be happy about it. I'll complain about syntax when that's the bottleneck in my understanding :) The tactics language, albeit untyped, exists at least. For big developements it's really a killer feature. I saved me hours when dealing with locally nameless representations for instance, because I could use "domain specific" tactics.
A .NET assembly isn't that much different than a dynamically linked Haskell package. You have: * Metadata about the package: assembly manifest ≈ GHC's package.conf + header files * The code. In a .NET assembly this can be combined with the manifest into a single file. GHC always uses a separate files. Sticking the package version into the exported symbol name would probably allow multiple versions of a package to be used in a single process. There may be symbol name restrictions or some other legacy/historical reason to leave this out. Peter Wortmann's DWARF branch of GHC attaches the package and module names to each symbol, so at least it can be available for debugging even if it's not used by the linker and loader... I figure if private dependencies are ever added to GHC/Cabal that these problems will be rectified. Binding redirects and other Fusion/SxS policies available on the CLR/Windows (they work for native SxS dlls as well) would be very difficult to make work in GHC-land. GHC aggressively inlines code between modules at compile time, in fact it runs the inliner multiple times during optimization, well before any policy could be applied by the loader. So cross package inlining would need to be disabled or performed at runtime (think of GHC as an optimizing JIT compiler). Static linking throws this all out the window though.
My comment was no doubt excessive; my irritation was caused, and I think justified, by the bit where you slipped in the FUD about 'prov[ing] false statements unintenti[on]ally', as if were not the duty of an experimental type system to bump into this, and in any case a demonstrably inevitable risk of the development of mathematical thought. The objection to 'autos. simpl . apply ... destruct. ...' -- or 'autos! simpl! apply ... !destruct! ...' as it ought to be -- can only seem feeble to one cocooned deep in the system. The activity of mathematical proof is the construction of a communicable validation of a proposition by a priori means. Agda may be completely impractical for complex such proofs -- I accept all arguments of that type -- but the user is indeed engaged in that activity, and the text can actually be read and understood; it is plain that the method will not 'scale' and that entirely new ideas are needed for the construction and presentation of this sort of proof. But this doesn't really speak against Agda which clearly has a more limited purpose of preparing the way. The user of Coq is engaged in a *completely* out of date act of irredeemably imperative programming on a '60s or '70s model ... programming with the unusual result that the existence of a proof is declared. The proof itself can actually be found if you care to look; it is a bit like -- actually *quite* a bit like -- inspecting GHC's core. For the user, the result of the succession of his commands is basically occult, in a black box. (Though valuable views are given him or her in medias res, this is what will survive of Coq). I guess we are to believe that there is an implementation-independent way of constructing the genuine term from the series of such commands, but the practice of making this derivation oneself is no where explained, and both of these are surely required for the feeblest claim to the title of a language in which one expresses thought. (I'm not convinced that anything but the implementation knows the connection, but perhaps it is so) Understanding and comprehension are simply *not* advanced in this way, and to compare the succession of imperatives with mathematical activity is absurd on its face. From the point of view of present understanding of the possibilities it is plainly a deep regression. An entirely different approach is needed, and my own feeling is that every speech on behalf of Coq stands in the way of it. Experiments aiming at little bits and pieces of the big picture like Agda and Idris are the only thing that can interest a serious person however feeble and limited the results may so far be. 
&gt; My comment was no doubt excessive; my irritation was caused, and I think justified, by the bit where you slipped in the FUD about 'prov[ing] false statements unintenti[on]ally', as if were not the duty of an experimental type system to bump into this, and in any case a demonstrably inevitable risk of the development of mathematical thought. Sure, but you have to admit that this makes Agda a bleeding-edge tool which may or may not be what the OP wants. There is nothing at all wrong with being a bleeding-edge tool, but sometimes one would rather use a clunky but mature tool than an elegant but experimental tool.
I've worked with locally nameless representations in Agda without pain. Have you seen NaPa?
they heard haskell was web-scale.
OK, locally nameless was probably a bad example. Are you arguing that tactics are always useless though or were you commenting on my example only? I still think they are useful.
Society: now correct by construction! 
That's the kind of stuffs scoutess will be for. Thus my repeated efforts to try to get people helping on the cabal/hackage/scoutess front.
You seem to be purposefully ignoring the fact that Coq proofs are a recording of an interactive proving process. Few people claim that their Coq proof are readable as a program; indeed, they are meta-programs that build a derivation proof piecewise. If you wish to understand a Coq proof script, you should not only read it, but execute it interactively to reproduce the experience of the author. The granularity of understanding and formalization of Coq proofs is the Lemma, not the single proof step. Most people do not actually look into the details of the proof script of individual lemmas once they're proven (they only care about the existence of the script), but demand that the structure of the definition and result be alone enough to understand the "interesting" part of the proof as a whole. That is in fact little different from the non-mechanized proofs mathematicians write *today*, that are full of "by a similar argument ...", "an easy computation leads to .." or "by a direct application of the theorem...". If mathematicians deem it reasonable to suspend the details at this point, there is no reason to reject the idea of not reading the corresponding proof script -- with the assurance of their correctness, plus the possibility to reproduce the interactive experience if it appears needed. Finally, I'm rather offended by your statement that "... Agda and Idris are the only thing that can interest a serious person". I do agree that Agda and Idris (and the very serious research being done in the Coq world about how to design better tactics) are interesting exploratory venues and that the current tactics language of Coq will not have a large place in Plato's world of eternal ideas. Yet tactics (in Coq or Isabelle, by the way) are often a practical approach to getting things done (combined with Coq rather solid extraction mechanism). You have to accept that not every effort is targeted at "research in theorem prover", but some people want instead to do "research *with* theorem provers" and focus their energy on their own formalization problem, instead of which marvelous theories will turn into usable tools in ten or twenty years. That people actually use any of these tools, rather than write papers about how to formalize them better, is a demonstration of the achievements that have been accomplished, and not, as you despisedly describe, only a fool's errand.
Thanks for correcting that. I'm no perl magician but wanted to provide at least a runnable solution.
לפי זה, היו צריכים לכתוב את המידע בשפה ידועה אחרת.
I'd never heard of falling sand games before. I'd say you owe me 3 hours of my day but honestly I don't regret it. As for your package I hit issues pretty quick trying to build it with cabal-dev, although it might have been llvm issues on my osx and not you.
Well, I think it's perhaps the most promising approach to programming out there currently, but none of the languages we have today got what it takes to do it properly. They are all good at their niches, but why isn't there an effort to make a language which is as good as Idris for systems programming and efficient compilation, as good as Coq for proving and as good as Agda for interactive programming at the same time? Sure it's a huge effort, but there doesn't even seem to be talk about it -- which makes me a bit sad.
They avoid success at any costs.
Proofs as found in math papers are also full of imperative statements: "Consider an arbitrary x", "pick δ = ε/2". It is true that it is very difficult to read a Coq proof script without interactively stepping through it. But I don't think that means "understanding and comprehension" is not advanced. The series of insights that a user needs to have to carry out e.g. an inductive proof in Coq are exactly the same as they would need to prove the same theorem on paper, with the additional benefit that each step is checked as it happens. Coq makes a nice vehicle for teaching a student what a (formal) proof _is_, even if the resulting proofs are not very readable. I do not think Coq users generally regard proof terms as black boxes. My impression is that advanced users are well aware of the connection between tactics and proof terms, and switch between them as appropriate. That is for example a common theme in Adam Chlipala's textbook Certified Programming with Dependent Types.
Thanks for trying to help me with this, tibbe. Oddly, I don't get build errors installing the network package. The problem comes when I try to build something *against* the network package. The error I get installing cabal-dev is: &gt; Loading package network-2.3.2.0 ... linking ... ghc.exe: C:\...\HSnetwork-2.3.2.0.o: unknown symbol `_acceptNewSock' I originally tried building the network package under Cygwin, but building it under MSYS produces the same result.
&gt; "You seem to be purposefully ignoring the fact that .." On the contrary, I emphasized it and even praised it. Note that "the possibility to reproduce the interactive experience" depends on the existence of the actual Coq implementation. I don't see that I could write an independent parser of .v files on the strength of which I could figure out the terms, or, what would be more useful for comprehension, set out the tree of dependencies in the style of Gentzen. For example, it is frequently unclear which of the things I have put in a database of lemmas have been employed to yield the new goal and how they are applied. If that is right, the user is hostage to an opaque binary, and is thus left with a radically uninterpretable script. That is, he is not formulating a proof. An .agda file is by contrast a proof. That it is a proof in mind-numbing detail is indeed an objection. But I don't see that a regression to an inscrutable imperative scripting language is a solution at all, much less a stable proven technology for getting things done, as even Java is. It isn't a matter of 'suspending details'; at least as it seems to me, there simply are no details independent of the binary that existed on the day you wrote the script. A mathematician, by contrast, can labor the bits I didn't understand; there is no similarity to that case at all; that you admit no one is interested in the script or even the term related to it makes the distance basically infinite. 
As I alluded to in [this post](http://softwaresimply.blogspot.com/2012/11/why-cabal-has-problems.html), I think there are a lot of differences between the .NET and Haskell ecosystems. First, I think the Haskell ecosystem is moving much more rapidly than .NET. Second, I suspect that Haskell packages tend to be more fine-grained and a lot more of the "core" Haskell functionality exists as standalone packages rather than as a part of the language. Case in point: lenses. They would generally have to be built into the C#/VB language, but in Haskell they can be a library. And as a result they get updated more frequently and we have competing implementations. Other examples are things like text, bytestring, mtl, transformers, etc. They are ubiquitous in real world Haskell code. I think the things in .NET that are that ubiquitous are usually built into the language. In Haskell these are separate packages and they're also under active development. My day job uses Haskell, so I'm not in the small-scale experimental mode. The projects I work on routinely depend on a total of 100-200 other packages. How many packages do your .NET projects usually depend on?
I think there are significant differences between the Haskell and .NET ecosystems. As I alluded to in [this post](http://softwaresimply.blogspot.com/2012/11/why-cabal-has-problems.html), I think the Cabal problems are probably caused by Haskell's fast moving ecosystem and tendency to use a lot more fine-grained packages and abstractions. The core Haskell language and API also tends to be smaller than that of C#/VB/etc and its powerful abstractive capability means that more everyday libraries will be distributed as standalone packages and be more frequently updated. Case in point: lenses. You couldn't get equivalently powerful functionality shipped as a library in C# or VB. They'd have to be built into the language. In Haskell, not only are they libraries, but there are competing implementations and there has recently been a lot of innovation. Another example is packages like bytestring, text, mtl, transformers, etc. Some combination of these packages are practically universally used in real world applications of significant complexity. I doubt .NET languages have this much of their core API shipped as standalone packages. To your point about experimental mode, I do full-time Haskell development so that is not a factor in my experience. I have encountered package conflicts in the past, but recently I really haven't had too much trouble. The applications I work on typically depend on a total of 100-200 packages (transitively). How many package dependencies do your .NET projects typically have? 
30 direct dependencies? How many indirect dependencies are installed in total counting transitive dependencies?
I don't think .NET assemblies being distributed pre-compiled makes much of a difference. On the other hand, &gt;Having two or more versions of a single assembly loaded in an application process is not prevented, nor is it inherently problematic. &gt;Because types are identified with their containing assemblies, assembly version conflicts become type errors (but only in cases where there is an actual conflict) These two points result in a very useful property: even if two packages depend on different versions of a package *p*, you can use them together without any problem as long as you don't try to pass things defined by *p* from one API to the other. While GHC Haskell currently lacks this property, it's precisely what the current work on private dependencies is aiming to achieve, so hopefully we'll see progress on this front soon. The other disadvantage which GHC has against almost every other toolchain (I'm going to stop singling .NET at this point because I don't know it that well) is that it's currently impossible to have more than one instance of the same package/version combination installed at the same time. Combined with cross-module inlining, this means that even if a package specifies very lax version bounds on a dependency once it's been built against a particular version it can only be used with that exact one and no other. This, too, should be fixed soon. Once that work is done, we'll be left with the final, hardest problem, which is what happens when two packages *p1* and *p2* depend on incompatible versions of a third package *p3* for an actual, unavoidable reason (such as because *p1* depends on a new feature in the latest version of *p3* but that version breaks the API and *p2* hasn't been updated yet), *and* you want to use both *p1* and *p2* at the same time, *and* both *p1* and *p2* expose types from *p3* as part of their public interface. In a way, *this* problem really doesn't have a solution; either *p1* or *p2* needs to be modified to work with more permissive bounds. On the other hand, a good toolchain can make that process easier by allowing a user to do something about the problem without having to wait for *p1* or *p2*'s maintainer to make a move. In particular * helping users easily work with local versions of their dependencies allows them to fix the problem for themselves, * permitting users to upload a new version of a package without its maintainer's approval allows them to fix the problem for themselves and others. In the GHC world, the former is supported by cabal-dev, and soon to be merged into cabal-install. The latter is still far-off, while some other language communities (at the very least, all those where github is the primary distribution system) are once again already there. Finally, this last problem is more frequent with GHC/Haskell then with, say, CPAN because things tend to change very quickly on Hackage, and we have a *lot* of projects that are still at version 0.* with substantial API changes every two months, yet already useful enough that people want to build upon them. So to sum it all up, it *is* true that GHC, as a platform, suffers from a particularly high rate of dependency problems, and this is due to three kinds of things: * a bunch of technical weaknesses which are being actively worked on, * the lack of collaborative features on Hackage to help users fix dependency problems themselves (they're the ones who care most about them, after all), * the fast development pace in areas that could be considered part of the stable foundation, combined with Haskellers being physiologically unable to stay away from new features (I'm as guilty as anyone here). Point 1 will hopefully be history two years from now. The new Hackage, which is slowly but surely developing, should give us the infrastructure we need to address point 2 and some parts of point 3 as well, but even when that infrastructure is in place we'll still need some time and experimentation to come up with the right policies, so I'd put this a bit further in the future. 
&gt; You seem to be purposefully ignoring the fact that Coq proofs are a recording of an interactive proving process On the contrary I emphasized it and, for what it's worth, praised it. Note though that this 'experience' is entirely dependent on an opaque binary. Could I write a parser of .v files on the strength of which I could independently figure out the terms or, what would be more worthwhile, something like a representation of the relations of dependency in the style of Gentzen? Maybe, but I would be surprised; it is often inscrutable to me which of various lemmas I had bundled was being applied to produce the next goal; for all one can tell it is frequently randomizing. But if the .v script is hostage to an inscrutable binary in that way I don't see that we have to do with a language, rather than a little something between the user and e.g. coq-8.4. An Agda file by contrast contain actual proofs, and the activity of constructing an agda file is an activity of proving. Everything is of course done in mind-numbing detail, and this is an objection. One would like forms of representation that are, for example, coarser grained, ways for the construction to be easier to compose and easier to take in. But the idea of evading the difficulties by recourse to an opaque imperative scripting language spoils every hope one might have had for 'programming with dependent types'. I saw a quotation from P. Hancock comparing Coq to 'doing brain surgery over the telephone'. But even with brain surgery over the telephone we will have some interest in patient if he or she survives, and this is the point. That you concede that the Coq user has no interest in the content of the script, much less the actual content of the proof, makes it seem more like imitating doing brain surgery over the telephone. The barking of imperatives has nothing in common with the omission of details by a mathematician with which you propose to compare it. I can ask a mathematician to labor the bits I couldn't follow; with coq-8.4 you can't and anyway, as you concede, you don't care. 
It's not much harder than debugging pure fix.
We're getting closer to that. The world is becoming more and more automated, so in a metaphorical sense we already are kind of wired inside a giant computer.
You shouldn't need fix for this kind of program.
You don't.
What version of bytestring do you have?
 Versions installed: 0.9.2.1
 Versions installed: 0.10.0.0 Looks like your problem.
How do I tell it to use 0.10 instead? $ ghc-pkg unregister bytestring-0.9.2.1 ghc-pkg: unregistering bytestring-0.9.2.1 would break the following packages: ... and $ cabal-dev info bytestring ... Versions installed: 0.9.2.1, 0.10.0.0 Edit: wait I installed 0.10 with cabal-dev so maybe ghc thinks they'll break because it can't see 0.10 outside of the directory. Apparently not. 
&gt; You seem to be purposefully ignoring the fact that Coq proofs are a recording of an interactive proving process. I didn't ignore this, but emphasized it and even, for what it's worth, praised it. If the imperative script is an actual 'recording' of a proving process, then there ought to be an implementation independent way of figuring out what the proof was. If all I can do is get a copy of the version of Coq you were using when you had your 'interaction', then the intellectual situation is pretty dire. The use of the `.doc` format was rejected as a legitimate means of communicating public, state action by European judges for good reasons; here the objections are at least as strong. I don't see that I could e.g. write an independent parser that I could then use to calculate the actual term that constituted the proof, or what would be more useful, something in the nature of a tree of dependencies in the style of Gentzen, and perhaps various more and less detailed views of this. I don't see that there is any hope of this, or any interest in it, so debased is the Coq coterie. In any case you admit that you have no interest in the script, only in its existence -- I think you mean the existence of the invisible proof term -- so the question of comprehension and expression evidently has no effect on you. Though the 'interactive process' generates a nice momentary representation of available premises and the goal, the way the successful imperative advances things to a new state is frequently opaque -- which of the pile of things in my database was it using, how was it applying it to the premises -- it often seems that the search procedure may be using randomization somehow, or may as well be. So I don't see that the script is any more a proof than a bash or ruby script is a proof. By contrast, where things have the right types, an Agda file can be a collection of proofs and the process of constructing it a process of proof -- as you can see by studying one. I know of at least one case where an Agda ignorant mathematician was able to correctly characterize the order of argument in a very complex system of files by the simple expedient of reading them. Agda programming is much too involved and detailed, in a better version of it or in a better language, much more would be automated, and there would be different views of the result. The telegraphic character of the sequence of autos! and destruct!s cannot be compared with the enthymematic character of mathematical reasoning. We can ask a mathematician to labor the bit that passed us by, but with Coq we have no idea what went by.
I can't speak to .NET, but dependency and version management on the JVM is a complicated task that not only do we have tools like Ivy and Maven dedicated to it, but we have complicated interfaces to those tools, and there are special commercial artifact repository managers to manage the sources from which those tools draw. I actually think cabal is no harder, although it requires a bit more comfort with locally hacking other's packages which may break on occasion due to dependency oddities.
The syntax? *Which* syntax? The big annoyance with Coq is that in order to use it you need to learn 3+ languages (Gallina, Vernacular, Ltac, Program) of which no two have the same linguistic style. Every time I use Coq It feels like I'm back in the mid-90s doing web-design (HTML, CSS, Perl,...). Even worse than the menagerie of languages, the syntax for each one of them is atrocious. Gallina is at least somewhat reasonable--- so long as you don't delve into caml4p too much (i.e., at all).
Yeah of course, if there's a tactics language that's one more language than if there isn't any. Saying you have to learn Gallina *and* Vernacular is like saying "In C you have to learn the language of expressions *and* the language of statements". And frankly they are quite minimal... Program, really? That's just one more keyword. Again, syntax is definitely not the determining factor in choosing a proof assistant/language as far as I'm concerned. 
What do you want to learn them for? If it's for practical, large scale theorem proving, then I'd recommend none of them, and recommend either Coq, or better, Matita. If it's just for playing around with them, then once you learn one you can pretty much learn them all quite easily, so just pick one and learn it.
FWIW, GHC *does* attach the package name, version, and module name to every symbol. The problem is that Cabal refuses to construct an install plan when it includes two different versions of the same package, just in case it might lead to a type error when building. I don't honestly know why we don't just have a flag to relax this restriction and let you try it anyway, it should be easy to add.
A question is, how do you specify which version of a type to use from a given code file if multiple versions are available? In .NET land, you have the ability to attach an alias to specific versions of a DLL and you prefix any type with `myalias::` to use the specific version. (Requires an `extern alias myalias` declaration at the top of the code file.) Would we not need to extend both Haskell's import syntax and Cabal's Build-depends to add this kind of feature?
Ahem. I think the majority of the "Coq addicts" are well-acquainted with the difference between a series of tactic invocations and the *real* proof. It takes a special kind of hubris to assume he who learnt just enough of the system to understand a few elementary proof scripts has stumbled upon a truth that has somehow eluded those who work with the system day-in, day-out. &gt; The best you can do is replay the script, if you happen to be living in a decade in which a version of Coq like his can be rebuilt. He merely needs to dump his proof to HELM XML so it can be read however you choose (e.g. with other proof assistants, rendered into English prose, exported to beloved Agda, etc).
Right you are, how long has it been like this? I recall dealing with symbols that were not versioned at some point... probably was dealing with an executable and not a library and got it mixed up in my brain :-)
What's the difference?
None in plain English, but strict in programming language terms means "strictly evaluated", as opposed to non-strict, which Haskell (and Fay) is.
Right, so in fact your claim that the "technology does not exist" in Coq to actually inspect a proof independent of the Coq implementation was completely incorrect, as evidenced by the fact you can dump all Coq logical objects to XML and render them in whatever way you choose. As for the ease of editing the XML representation, well, in the time it took you to produce another wall of text you could have hacked together a tool that suits your needs just fine. &gt; One might have hoped for something from e.g. Agda 7 or Idris 24 or who knows what, but with people like you simply casting derision on such things, it is clear that we will be stuck forever. Indeed, all hope is lost; somebody on the internet disagrees with you.
Haskell is almost exactly where I expected. I'll take slightly more verbosity in exchange for purity and an awesome type system any day...oh, and also speed.
Indeed, I think JSON is something that Haskell excels at, really. I should make a post about it some time. Not only are they easy to work with, but more validation and information is held in the types that's not there in Python. I reckon a key problem here is probably the documentation. When I first dealt with the `json` library, as a newbie, I struggled figuring out how to use the thing. So have others. It's not really obvious that it's all about the instances. And whether one should be converting to Haskell data types, or to the generic Value type. I was confused about which I should be using. And the encode/encodeJSON and inverses are so understated. It wasn't obvious how to (or that one *should*) construct a monadic parser to extract things from the JSON value. It can be disheartening when trying to use very polymorphic functions and not know how to instantiate it properly (like our generic regex library, the ~= is heavily overloaded), that's annoying as an experienced Haskeller, a frustrating roadblock for newbies. It's not obvious, or explained, if I recall, what the output will look like if I rely on the Data/Typeable instances. Maybe they form an isomorphism, but the generated JSON often matters. Of course, now I appreciate the library for all its decisions, and when I would want to use each one. Being a more experienced Haskeller helps. I think one could extrapolate the same experience/criticism for the aeson package. Its docs are a bit nicer but example-lacking. This lacking docs problem may stem from a "the types speak for themselves" attitude, or merely just not thinking about it. Haskell libraries are often like that, very understated. All the types are there, each function documented. But no human side to say where to start. To get off the ground immediately, with examples. Probably also a problem of people behaving like I do. I struggle to understand a library, and then once I understand it I move on. I don't complain or tell the maintainers that it was difficult. So they don't know to improve it. I'll try to do that more often. I do put the effort in as an author to document my libraries with some examples, like [url-generic](http://hackage.haskell.org/packages/archive/url-generic/0.1/doc/html/Web-URL-Generic.html), it's nice when there's a bit of a dialog with the reader.
I'm surprised that clojure consistently beat haskell by a pretty solid margin, lisps are generally pretty verbose. I wouldn't consider the numbers for mathematica to be real for obvious reasons, but in theory the comparison between all the other languages should be fairly unbiased.
Go has a very good debugger called gdb...
I have to ask, with all the type information we have available, why we don't have a package manager that can substitute one package for another as long as the types remain the same? I don't understand why the promise of Haskell is that we'll let computers make sure we're doing the right thing - and then we let humans make terrible choices with how to do package versioning. It doesn't make sense to me. I mean, does the exposed interface, the shape of HTTP-4000.2.6 have any differences from HTTP-4000.2.5? I don't know. If not - what does it matter? Why can't Cabal speculatively even *try* to install cabal-dev with these newer package versions?
How can I as a user easily fix this? What is the sub five minute fix to make this work?
Do we really need to keep re-hashing this? 1. User (deliberately?) creates minor problem, makes a mountain out of said molehill, insists nobody is doing anything about it. 2. Community shows that their specific issue can be trivially solved now, and a long term solution is being worked on to make it so you don't have to deal with that issue any more. 3. User ignores this and continues to insist nobody is doing anything about cabal and that cabal is totally useless. We've had like half a dozen copies of this in the last couple of weeks. Yes, cabal has problems. Yes, they are being worked on. No, they are not significantly worse than the same problems in the dozens of other languages people pretend have perfect library management. No, complaining about how "unacceptable" you find it will not fix cabal.
The problem is that this is a chronic disincentive to new users working on Haskell or its ecosystem. If I want to start working with any other ecosystem, I don't have these problems. It's only because the allure of Haskell is so strong, and the cost of rewriting what I've already done, that I have remained.
I noticed this recently in the context of some online algorithm problems. Some thoughts (I don't know Clojure): - Clojure has pattern matching in function definitions, and syntactic sugar for creating the equivalent of partially applied functions. - Clojure has some iteration constructs which are possibly more overloaded/general than, say, list comprehensions. - In the usual dynamic language fashion, Clojure programs don't really use new data structures/type synonyms, but instead define functions to create lists/vectors. - Haskell programs typically start with ~4 imports, whereas Clojure programs don't use any. This matters for the many trivial Rosetta Code problems. - Haskell programs are split into many more top-level definitions, and so type declarations are significant. I wonder if there are any other explanations, especially for "large" (by Rosetta Code standards) programs ...
yep, oops. unpack fetches for you -- even simpler! good catch.
That worked! Thank you.
Re: your edit, rants have gotten me solutions. They make people who are able to make changes to prevent said rants aware of the problem. I do feel entitled to complain, and I will complain in the future when I feel like something has been a letdown. The idea that because it's open source or because other people are volunteering their time means I don't have the right, or shouldn't feel entitled to point out flaws and deficiencies is wrong.
Non-problems? It's a problem I couldn't find the answer to from googling, lack the expertise to fix, and I consider myself a bit better than a layperson here. Now imagine someone who is truly a lay-person with Haskell, cabal, etc, trying to work through this?
This also more immediately caught the attention of people and resulted in a solution working for me.
really nice! happstack-fay, yesod-fay ... soon snap-fay and cabal-fay? fajQuery anyone? :)
I love haskell community but having to turn to them for every single ( and repeated ) is not ok
From the article: &gt; The strictness of the type system gave rise to a choice of libraries and APIs that made my life harder, not easier. I then had to write glue code to marshall between the dynamic types used by the two libraries I needed. He had to manually write code to convert dynamic types.
First of all, you are doing the same thing now. I don't know how to make this any clearer, but the community is fully aware, and is working on addressing it. No amount of ignoring this fact will make them solve the problems any faster. Who exactly do you think is suggesting the current situation is perfect? Second, he turned to the community either way, the difference is just turning to the community to ask for help vs turning to the community to act all huffy and bitch about a trivial issue that will occur in almost any language. Yes, when a package depends on an older version that what you have already installed, it will cause a problem. This applies just as much to cpan as it does to cabal.
&gt;soon snap-fay Soon? http://hackage.haskell.org/package/snaplet-fay
I see. The explicit type annotation there is a non-trivial detail though that doesn't pop up immediately from the documentation especially as it lacks examples. I think he's stumbling on real Haskell problems, not only of documentation as everyone is agreeing, but also: * Integration of such documentation into the interactive shell * Simpler more concrete types (less typeclass tricks!) in libraries that should be very simple 
When you say that Clojure has syntactic sugar for partially applied functions, which form do you mean? Are you talking about the anonymous function sugar? #(* 5 %) Otherwise, Clojure just has a standard library function that does partial application (partial * 5)
This looks very nice, and it looks like you put the effort to make this into something usable!
I think most people here think of pointing out flaws and deficiencies different then complaining. A statement like: &gt;What is even the point of having a package manager this broken? In many peoples minds this evokes a logical chain along the lines of: cabal is broken and pointless, no one builds pointless things on purpose so therefore there must be something wrong with the people who have built cabal. Similarly "That's failure." evokes the idea that cabal builders are failures or the community is a failure. You did point out a flaw, but it was not all that you did; the rest of what you said can and did have negative consequences as is witnessed by the number of negative reactions to it. I dislike the negative consequence of your post and ask you avoid creating similar negative consequences in the furture.
This is the first I've heard of codata vs. data, too, but I think this is what va1en0k is talking about: http://blog.sigfpe.com/2007/07/data-and-codata.html Pretty neat stuff!
I don't know I think he kinda hits the nail on the head for me (as a static typing fan). The one place where I personally feel dynamic typing really does make things significantly easier is when most of the code is dealing with external "untyped" (in the "schema-less" sense) data with very simplistic actual processing. The cost-benefit ratio of getting the data into type data structures is low in these cases. Once you start actually using that parsed data to do some processing though, I really don't mind some extra up front cost to get the types in order so I can eliminate a whole host of bugs. In fact, the issue he has with picking an instance etc. is precisely because he does no processing on it. Haskell doesn't know what type the data has because he doesn't *do* anything with it other than print it out again. 
&gt; They make people who are able to make changes to prevent said rants aware of the problem. The issues are fairly well known in all honesty (in this case, having overly inflexible package bounds, assuming nothing will change,) they are not insurmountable or unfixable, and merely require work to be fixed. Not even a lot of them are cabal problems as much as packages that bitrot, it seems, and that's not something Cabal can magically fix. It's not really the fact you're pointing out issues that's getting people in a tizz. Even if people already know about the issues - nobody will be mad that you weren't aware of something, or repeating something they know. It's the connotation and attitude associated with the post implying cabal is nothing but a "failure" and acting like nobody is aware of the problem (*but how is that possible? it's such a failure, how could you not see it!?!?*) There are some real flaws with Cabal, some of them tough. But going all bash.org and throwing up your hands saying everything is a failure, in an attempt to incite aggressive response to prove you wrong, isn't really going to win you friends, or make people appreciate your input. Especially when, in this case, it's mostly a red herring anyway.
The aeson and yaml packages support working with aeson's [Value ADT](http://hackage.haskell.org/packages/archive/aeson/0.6.0.2/doc/html/Data-Aeson.html#t:Value).
Type annotations aren't required at all, look at the json/json' Parsers in aeson and combine that with the approach taken [in the comments](http://lukeplant.me.uk/blog/posts/dynamic-typing-in-a-statically-typed-language/#comment-709762695) for a type-class and type-annotation free solution.
That was mentioned in the comments on the post. Apparently J was typically about half the length of the Mathematica code, as you'd expect.
Because the libraries oversell the type-class-based approach.
Wonderful, thank you. Could perhaps be nicer with name-based pattern matches rather than positional ones, for clarity.
I am having a lot of trouble with this assignment question. It reads: "The following function returns True whenever the coordinate (x,y) is inside a circle: inCircle radius (cx, cy) (x,y) = (x-cx)^2 + (y-cy)^2 &lt;= radius^2 The following function draws lists of *s and spaces at positions in a grid: draw f = putStrLn $ unlines $ [ [ if f (x,y) then '*' else ' ' | x &lt;-[-30..30]] | y &lt;- reverse [-19..19] ] Try draw (inCircle 7.5 (0,0((. Write a function which will draw the union of circles of size 7.5 at positions given by a list. Test it with the list [(-13.5, 5), (0,0), (13.5, 5)]." So I figured I would somehow have to adjust the original inCircle code to somehow accept lists and adjust the draw function to be able to read the new things. I just have no clue how to do it. If anyone can help me, it'd be super appreciated. 
&gt; Haskell programs typically start with ~4 imports This reminds me of my favorite Haskell quote: &gt; acowley: Good haskell code is 20 LANGUAGE pragmas, 40 imports, then one line of perl.
Instead on focusing on languages, I think their should be focus on programming environments that move away from text file wrangling, which may include a re-imagining of the Terminal. (I wouldn't mind getting rid of `\t\r\n` and odd invisible characters. There is supposed to be a separation of concerns, so why is formatting hints littered through the source code?)
I never called it a REPL and you weren't exactly clear that that's what you meant. Most of your post was about using debuggers and then suddenly you decided to veer off course and take a shot at Go for lacking "interactive programming or debuggers."
Awesome. I'm looking forward to using Haskell on more platforms.
That's pretty cool. Would you consider writing an ML for mobile app development?
You might consider using [libssh2](http://hackage.haskell.org/package/libssh2).
Agreed. Another minor niggle: usually schemas go top to bottom, with what's outside of the system at the top. That said, it's a nice and accessible technical writeup. Thanks for the hard work.
“Type-class trickery”? 
Would it be impertinent to ask what the ETA is for the parallel IO manager?
&gt; The text I work on and edit and submit to the typechecker and compiler must itself be directly intelligible, it must express my thoughts and the connection of my thoughts. And my retort is the same throughout: if that's what you want to do, then do it. Nobody or nothing is stopping you, apart from how incredibly tedious the process of proving without tactics is. Further, try and convince Voevodsky to author his proof scripts in the same style. Finally, if you want to engineer a declarative proof layer above Coq's standard vernacular syntax that meets your needs, then get to it. I'm sure patches will be gratefully accepted by the Coq community. What you want to do, however, is go further, and prevent others from using the very tools that Coq and similar systems provide their users to write realistic, verified software in. You're not interested in the process of large scale proof engineering, that much is clear. Your only exposure to formal proof appears to be a few small vernacular files that Voevodsky has written, as well as a bit of fiddling you may have done working through Coq'Art or another similar book. However, many people who use Coq and similar systems are interested in actually proving *realistic* software correct, including me. What you characterise as an unwillingness in the theorem proving community to switch to a declarative proof style, or, even worse, abandon tactics all together in favour of Agda-style reliance on unification, is anything but. Everybody is well aware of these technologies and techniques. They just don't scale well to the sorts of problems we are interested in. Look, the last big development I worked on had proof states with over [400 goals open at one time](http://cerco.cs.unibo.it/browser/src/ASM/Interpret.ma#L483) and had multiple statements of lemmas that took [15 minutes to type check](http://cerco.cs.unibo.it/browser/src/ASM/AssemblyProofSplit.ma#L100) and "proofs" 2500 lines long. From a mathematical viewpoint the statements of these lemmas, and the proofs themselves, are pretty god-damned ugly, and not in the least bit interesting, being more tedium than any sort of intellectual challenge. But proofs like this are exactly why tools like Coq, along with their automation and tactic-technology, are needed! What you are proposing would not work for formal proof of this sort: we need automation and tactics to work with software like this. Declarative proof styles and reliance on unification just doesn't scale.
By "type class tricks/abuse" I mean things like the regex library - completely suprefluous use of type-classes. Similarly, there's the old example of: Monad m =&gt; lookup :: .. -&gt; k -&gt; m v Instead of simply using m=Maybe, and then converting to a "fail" call by the user. I don't know the JSON library, but I do sympathize with the author about APIs doing things like the above and making themselves harder to use for no noticeable benefit. Can you explain why decodeGeneric is useful as opposed to returning an ADT that contains all of the parsed information in concrete form? I agree that the issue isn't really "dynamic vs. static", but otoh I do think Python API designers place more of an emphasis about ease of learning. Haskell libs also need to think about safety and so they sometimes sacrifice ease of learning. With more effort, you can usually get both. 
I agree, the regex library's use of type-classes complicates it way too much, with sod all documentation. I think it put many off using it. Also `lookup` in Data.Map is stupid, yeah. Though there aren't many like that in the standard library, I don't think? &gt; Can you explain why decodeGeneric is useful as opposed to returning an ADT that contains all of the parsed information in concrete form? Because you often want to do something with the data in the JSON, rather than manipulating an AST. λ&gt; decode "1" :: Result JSValue Ok (JSRational False (1 % 1)) λ&gt; decode "[1]" :: Result JSValue Ok (JSArray [JSRational False (1 % 1)]) It's better to say: λ&gt; decode "1" :: Result Int Ok 1 λ&gt; decode "[1]" :: Result Int Error "Unable to read Int" λ&gt; decode "[1,2]" :: Result [Int] Ok [1,2] λ&gt; decode "[[\"Chris\",1],[\"Dave\",2]]" :: Result [(String,Int)] Ok [("Chris",1),("Dave",2)] λ&gt; do o &lt;- decode "{\"name\":\"Dave\",\"age\":2}" age &lt;- valFromObj "age" o name &lt;- valFromObj "name" o return (name ++ " will be " ++ show (age*2) ++ " in 2 years") Ok "Dave will be 4 in 2 years" λ&gt; do o &lt;- decode "{\"name\":\"Dave\",\"age\":2}" age &lt;- valFromObj "age" o when (age&lt;20) (throwError "too young!") name &lt;- valFromObj "name" o return (name ++ " will be " ++ show (age*2) ++ " in 2 years") Error "too young!" You can make complex uses and requirements of the input JSON and from whatever you specify in the type. It handles parsing (i.e. validation), and if it succeeds it will give you exactly what you ask for. It separates the parsing phase from the use phase. To do this with Python's library you will just "use" whatever its `decode` function returns and do some manual type checking or otherwise your code will throw random type errors, confusing the parsing and use phases. When you have more interesting types in real programs you can define a JSON instance for it: λ&gt; data Person = Person { age :: Int, name :: String } deriving Show λ&gt; instance JSON Person where readJSON json = do o &lt;- readJSON json Person &lt;$&gt; valFromObj "age" o &lt;*&gt; valFromObj "name" o And decode it in the same way: λ&gt; decode "[{\"name\":\"Chris\",\"age\":1},{\"name\":\"Dave\",\"age\":2}]" :: Result [Person] Ok [Person {age = 1, name = "Chris"},Person {age = 2, name = "Dave"}] And use that recursively, just like `Read`: λ&gt; decode "[[{\"name\":\"Dave\",\"age\":2},{\"name\":\"Dave\",\"age\":2}]]" :: Result [(Person,Person)] Ok [(Person {age = 2, name = "Dave"},Person {age = 2, name = "Dave"})] You can tell the JSON library exactly how to parse an object of a given type. Usually, you're working with someone else's JSON, possibly rather crappy JSON, and you don't want to work in the way that they do, or you only want some of it, etc. Maybe you're (like me) dealing with Common Lisp JSON servers, that think that null, false and the empty list are all the same data type. To do this with Python's library you need to define a set of functions that will walk a JSON structure and produce a value of a single type. Possibly there can be hooks in an aspect-oriented approach. In all cases, it's tricky, and you're emulating what is already provided here by type classes. So that's why I say you could define some non-polymorphic version: λ&gt; let decodeSimple :: String -&gt; Result JSValue; decodeSimple = decode λ&gt; decodeSimple "1" Ok (JSRational False (1 % 1)) But I'm not sure I'd ever use it. You could also avoid this class entirely and use the ADT and something like the Data/Typeable class and decode everything generically according to some mildly reasonable scheme. Which would put you in the Python style, but with type safety, and less control over individual type serialization. I [use this approach in Fay](https://github.com/faylang/fay/blob/master/src/Language/Fay/Convert.hs#L114) for serializing data structures from/to the server, because the client and server share the same Haskell data types. Anyway, this post is the kind of documentation I'd put in the `json` or `aeson` libraries. 
"pretty-printing parallel SSH tool written in Haskell". I hope the goal from doing this was to learn some Haskell libraries, cause I could think of about half a dozen languages that could do this in a quarter of the code and more readable ;-). Here's an incomplete example in Tcl, since I troll that language all day long and don't want to make this comment entirely critical. Your code is at least a good example of several useful libs in System. #!/usr/bin/tclsh lassign $argv hostfile command array set output {} set running 0 proc readable {host fd} { append ::output($host) [read $fd] if {[eof $fd]} { incr ::running -1 } } set hosts [split [read [open $hostfile]] \n] foreach host $hosts { set fd [open "|ssh $host $command" r] incr running fconfigure $fd -blocking 0 fileevent $fd readable [list readable $host $fd] after 10 "close $fd ; incr ::running -1" } while {$running} {vwait running} foreach host $hosts { puts "$host:\n$output($host)\n\n" } 
As advanced as the Haskell type system is compared to mainstream languages, types alone still don't cover the full interface of a function. Identical type do not mean identical behavior.
The first section mentioning Slowloris feels out of place since all it really does is explain the attack and then reference a later section for how it is solved in Warp before some sections about something totally different.
I recently came across [When is one thing equal to some other thing?](http://lambda-the-ultimate.org/node/1338).
s/reduction/evaluation/
I have gotten it to build successfully on GHC 7.6.1 by bumping the GLUT upper bound versions manually.
I enjoyed this, it was a nice read. Thanks!
Comparison to [gnu parallel](http://www.gnu.org/software/parallel/)?
Saw it posted in /r/dependent_types, but it has few subscribers, thought people here might be interested.
https://en.wikipedia.org/wiki/Beta_reduction#Reduction
Strictly speaking the fix is already in the upstream repository. I'm not sure why it hasn't been pushed to Hackage yet but I submitted a bug about the matter yesterday. Sadly it doesn't seem like cabal-dev has the most active maintenance. Thankfully, true sandbox support in cabal-install is coming soon.
There is something magical about how Fay was designed; it really hits a sweet spot of interoperability and ease of use. Looking forward to a more rigorous analysis of what subsets of Haskell can easily (and efficiently?) be translated to other languages, and why.
&gt;I can do half as much stuff in half as much code! Yeah, that's super impressive. Did you have anything to actually add to the discussion, or are you just here to troll?
Really excited to try this myself, great work! Repa is a lot of fun to work with, eh? Anyways, I ran into the same issues with installation via cabal-dev on a fresh Haskell Platform install (2012.4). There must be another package somewhere that prefers bytestring 0.9.2.1 because if I just cabal-dev install bmp directly it installs bytestring-0.10.0.0. So not sure how I can get this to work without using a fresh ghc (not haskell platform).
Reduction is fine.
Builds on my box... I know they're using it for CloudHaskell too.
If you look at the build log, it looks like something weird happened and it built fine, but failed while installing the package or something? I clearly see the `libssh2` package building fine at the very end of the build log.
I think my buzzword-processor just overheated. [This will not stop me from excitedly reading this pdf, even though Agda is not comprehensible to me yet].
I do not have an answer for you unfortunately so here is what I would do. The first thing I would try to understand it better is add cabal -v, or --verbose=1|2|3 where -v is the same as --verbose=1. In my experience stackoverflow is often a better place to ask these types of questions so I recommend you try there. As was pointed out in a another post hard to understand error messages are worthy of a bug report pointed out by tibbe:https://github.com/haskell/cabal/issues Check to see if your issue has been reported and if not you can file one. It is probably best to wait until you get an answer to your question before filing a bug report, that way you will be able to articulate exactly the information you would have wanted in the error message. Or at the ver least wait until it is clear you question is not going to be answered. You can file issues for cabal at its github issues page: https://github.com/haskell/cabal/issues
This comment explains how to use typeclasses with the JSON library to ease in extraction of values. http://www.reddit.com/r/haskell/comments/136pnn/static_vs_dynamic/c71phgh
I don't think cabal install --only-dependencies -fblaze_html_0_5 does what you want. You probably meant for `-fblaze_html_0_5` to be passed to pandoc, but I don't think that happens. The right fix here is to fix the pandoc Cabal file, which now includes: Flag blaze_html_0_5 Description: Use blaze-html 0.5 and blaze-markup 0.5 Default: False If we were to remove the `Default` clause Cabal should backtrack and try both options automatically, without you needing to pass any `-f` flags to the `cabal install` command. If that works I suggest you send a patch to the maintainer.
`Value` is an instance of `FromJSON`, so the function you wanted was `decode` in `Data.Aeson`. It's unfortunate that wasn't clear, but there was really only one thin layer of type class stuff to get through. I won't comment on whether that one layer *should* be there, but it's not as bad as you implied.
I was quite pleased with how many buzzwords I managed to fit into the title. I missed the word "framework" though.
A `-f` flag only applies to the global target. So `-fblaze_html_0_5` is not propagated to pandoc. The line pandoc-1.9.4.5:-blaze_html_0_5 indicates that `False` is chosen for that flag. The later line rejecting: blaze-html-0.5.1.0/installed-ef1..., 0.5.1.0, 0.5.0.0 (conflict: pandoc-1.9.4.5:blaze_html_0_5 =&gt; blaze-html&gt;=0.4.3.0 &amp;&amp; &lt;0.5) says that the listed versions of `blaze-html` cannot be chosen because the choice of the `blaze_html_0_5` flag for `pandoc` has introduced a dependency that excludes `blaze_html` in these versions. If you want to set the flag for a specific package that isn't a global target, you can still do that using a constraint flag: --constraint="pandoc blaze_html_0_5"
A `Default` will only set the default value. Within its standard backtracking limit, cabal-install will automatically consider the non-default setting.
The core web-routes library itself does not require you to use template haskell, quasiquotes, generics, or anything else. You simply need to provide a pair a functions, one to show the url and one to parse the url. The difficult question is how do you create that pair of functions. If the user has to write the parser and the printer individually, by hand they will surely make mistakes. There are a number of different approaches that have been implemented to deal with that issue. Most of the revolve around the idea that the user should only need to specify the way to map between the types and the url strings once, and the two separate functions can be automatically derived from that single specification. These include: web-routes-th: uses template haskell to automatically derive the printer and parser from the type / constructor names. web-routes-quasi: via QuasiQuotes, the user defines the route mapping using a special syntax. The types, printer, parser, and default routing function are generated automatically. web-routes-regular: use the 'regular' generics library to automatically print and parse the url based on the type. Similar to the TH solution. web-routes-boomerang: allows the user to specify the mapping between the route type and the url string using a set a combinators that simultaneously define the printer and parser. boomerang uses plain old haskell code, it does not require the use of template haskell, generics, etc. That said, there is a bit of TH code in boomerang that is useful for deriving some primitive combinators for the constructors of the type. Unlike the TH in web-routes-th, this has no impact on the appearance of the URL. It's just a convenience utility to save you from having to write those boring combinators by hand. Looking briefly at your solution, I am not clear on the following: 1. you seem to only have a single route defined in your application. How do you handle multiple routes 2. if I have a type like, data Foo = Bar | Baz | Buzz, that I want to use in a url.. how does that work? 3. it seems like your url function 'trial' is going to expose all the innards of the type in the type signature, resulting in something unreadable? And if the url changes, you would have to update all the type signatures of functions that use that url? 
This is the first I'd heard of web-routes-boomerang. That looks very appealing. Thanks for mentioning it.
my pleasure! It is documented in the crash course: http://www.happstack.com/docs/crashcourse/WebRoutes.html#web-routes-boomerang The boomerang library itself is also available for use completely independent from anything having to do with web routes at all. Obviously, the Happstack team needs to do a better job about raising awareness of what we have to offer :-/ 
My upcoming web framework (geordi) has a GADT-based approach to type safe urls. It doesn't require Dynamic and it offers a nice concise url pattern syntax.
Well to add other routes you just put them in the bottom on the route constructor. myURLs = do putURL trial trialFunc putURL trial2 trialFunc2 The idea would be to have a URLs.hs file and have it without a type signature, since you only need it to generate URLs and to initiate the URL state. If you do have to include it in your type signature for some reason, you can include it by the type class URLPiece. You can use any data type with them, you just have to pass the constructor to your url being built. Yeah, I guess the only difference between them is that all your types have to be defined in SiteMap whereas this can take any Type as opposed to only being able to take 1 type (SiteMap) generated with different constructors. This is a polymorphic solution compared to the typical monomorphic solution. --- EDIT: I updated the code to show 2 URLs. 
Let me explain my comment: reduction is a technical term that implies a term-rewriting or lambda-calculus backdrop. Embedding basic addition in a fancy backdrop detracts from the point of the comic (which is pretty cute), because it's a distraction, like an unused stage prop.
Why are the part-of-speech tags stringly typed?
This looks fun. It also reminds me why I should work on that book idea I had. Not to write a book, but to make a book writing Wiki-like engine that supports displaying different content to different types of readers. I already know Haskell, so I don't care to read through the introductions of lists and what foldr is and standard library functions (explanations the type of which it seems to bring up throughout the book), I just want to read the meat. If I could choose "I already know Haskell" and the book was smart enough, it could re-format itself to cater to me. In the same way, Haskell introductions could target people who already know an ML, or never programmed before, or whatever, by configuring the book.
No.
The content still has to be hand, made. So, you're essentially going to write one book for each possible scenario.
The point of the engine would be to avoid/mitigate that. Otherwise you'd just write two books.
Really nice explanation, I now understand why nothing I do works even remotely.
He does mention that he asked questions in the #haskell channel and even thanked a few people. Then again, should it really require expert knowledge to simply install and use one simple library? I am struggling for &gt;4 days now to just get the dependencies for a project
Thanks, it compiles and installs now but the resulting binary doesn't seem to behave the way it does in the video. For one thing the bar with options is small and located to the lower left of the window and only the word Fire ever shows (no matter where the mouse is), clicking anywhere doesn't seem to have any effect. I am running Gentoo Linux 64bit with the xmonad window manager and two screens (one 1920x1080 and one 1680x1050). Despite using quite a bit of CPU the only way I can tell that the process is not completely frozen is that it redraws when it was in the background and reacts in some limited way to window resizing operations (it seems the bar is redrawn on a much larger scale when the window is smaller). It does not seem to react to the window manager's close window command. 
I actually tried removing the blaze-html flag from pandoc, installing that package and retrying the whole process. Didn't change anything in the error message though
Both fail. If no solution is found within the backtrack limit, the very first error path is reported.
What GHC version do you have? 7.4.2? Sorry, you're basically running into the "conservative upper bounds" problem, i.e., gitit2 sets upper bounds for a number of packages that are more conservative than needed and make building on newer GHC or platform versions a pain. Try to edit `gitit2.cabal` and remove all upper bounds for the packages `directory`, `containers`, `bytestring`, `http-conduit` and `network`. (Note that there are several occurrences for some packages.) Then try again, without extra `--constraint` flag. I think it should work then.
&gt; The text I work on and edit and submit to the typechecker and compiler must itself be directly intelligible, it must express my thoughts and the connection of my thoughts. That amuses me, because you seem to spend very little effort constructing intelligible, expressive prose. Sheesh — when pompous verbosity meets insufficient command of language, everybody loses; you'd do well to tone it down a bit.
I was using 7.6.1. I already changed some upper bounds, though not all the ones you just mentioned. This is what I was getting: http://hpaste.org/77818 (this is without changing all the upper bounds you mentioned) I'll try again later today, changing the ones you mentioned.
For another approach you can look at [zwaluw](http://hackage.haskell.org/packages/archive/Zwaluw/0.1/doc/html/Web-Zwaluw.html). Very simple and elegant bidirectional routing.
To my knowledge this is the first full-time Haskell sales job in history. We'd like to find someone who is really great at sales to business IT departments; we can teach them Haskell. The best ways to apply are by emailing "jobs" at fpcomplete.com or by replying to the linked advertisement on monster.com or LinkedIn. This doesn't go to a robot or headhunter; Gregg Lebovitz or I will personally view every application. PS: It pays well; we know selling is serious work.
I don't think this requires a special engine. A good way of handling is to chose the level of granularity to be chapters or sections or whatever, write the sections of the union of all the books you want and produce a table of contents for each one.
There are a lot of books that span the introductory phase into more real stuff. Books like this in particular fall into that category -- it's about natural language processing, but it has to teach Haskell to the reader at the beginning in order to implement some of this stuff. For Haskell users, all of this content could be omitted (and perhaps some more content could be added). The content would have to be hand-made but there are parts of the book that cater to both categories of people that could potentially be shared. I think that's what he's getting at here.
You can see that it tries to downgrade `bytestring` and `network` in several places. This is usually a good indicator that one of your targets requires an older version. So yes, please try removing these upper bounds. I've tested 7.4.2 and 7.6.1 now, and can get it to build with both GHC versions, after removing the upper bounds. It still somewhat surprises me that it tries to upgrade `directory` to 1.2.0.1 in your paste. It doesn't try to do that for me.
Some more related code from the same team (Tom Nielsen), who also published with Nilsson is available at http://github.com/glutamate 
But it will be [outdated advice](http://hackage.haskell.org/trac/ghc/ticket/3202) once GHC 7.8 is released.
A new spin on an all old joke: Avoid Sales at all cost! 
The key ideas in boomerang come directly from zwaluw. boomerang largely just improves on the implementation of the ideas. I feel like there is a paper, but I can't find it. This is a good starting point: https://github.com/MedeaMelana/JsonGrammar/blob/master/README.md I have doubts that you can use Applicative. If you want to give it a try, just fill in the blanks: {-# LANGUAGE FlexibleInstances #-} import Control.Applicative data Iso a b = Iso (a -&gt; Maybe b) (b -&gt; Maybe a) instance Functor (Iso a) where instance Applicative (Iso a) where 
Can you also add an example of incorporating Foo into the url? data Foo = Bar | Baz | Buz deriving (Show) instance URLFragment Foo where urlize ... trial3 = ("/foo" :: C.ByteString) :/: ??? Thanks!
My does not require web-routes, less dependencies.
Very nice! I'm curious, though: Is there any way Hackage could serve an XML sitemap (or something, I'm not quite up to speed on how these work) telling Google to prefer linking to the latest version?
oh come on: avoid $ sales at all cost
It turned out to be a 32bit vs 64bit types issue. We made a patch and I think we submitted it to the maintainer already.
Why not just use the existing CUDA implementations? The only way i can see this being useful is as a benchmark for accelerate.
 avoid $ sales `at` all cost
oh, xmonad. I haven't tested at all on a tiling window manager. I suspect gloss and the way I draw the menu don't cope too well. Thanks for telling!
Good question. I think our initial consideration was to keep thinks as simple as possible, since the book aimed to be an introduction to Haskell and natural language processing. Note that you can't really use algebraic datatypes for tags, since there is no standard tagset, and it's nice if you do not have to touch any code when a tagger is trained on a new data set. The book uses Strings in many places for simplicity, though we later considered using Text/Bytestring. But that would've made the introduction more complicated.
Usually I don't run into any issues other than the occasional program forgetting to specify that a window should not be resized. From the perspective of the program it should be just like any other window manager really. I mostly mentioned it because it resizes programs to full screen by default and 1920x1080 might be harder on the rendering engine than the default window size.
Any plans to expand the book?
Good luck with the job hunt! Looking forward to any additions in the future. I had no idea natural language processing was so interesting until today.
Good point. I think you're right. Looking at the optimised code "fib" gets worker/wrapper transformed into a version that doesn't perform any allocation. One of the unfortunate corner cases of the way GHC implements lightweight threads.
Of course, the next step after Text/ByteString is to use integers by maintaining an intern table. Which is a worthy trick for any NLPer to know about.
Once we get plain fay-jquery into a stable state we can start thinking about how we should abstract it into more haskelly goodness. 
This is a very valid criticism. If and when I turn this into a blog post, I'll make sure I address this question in detail. The short and somewhat wrong answer is that I am trying to illustrate how monads can turn Haskell into Python. Sort of. The point is not to appeal to new programmers and get people to pick Haskell over Python. Rather, the goal is to show Pythonistas how to transliterate their programs into Haskell, and coincidentally, teach them about monads. As they see the mechanisms we use to transform a pure functional language into an effectful imperative one, I think that they will see the value of Haskell for themselves.
Don't forget you're talking about a Cartesian-closed comic.
Haskell IO is certainly heavy to use, but has the advantage of being explicit. It is certainly not worth for a small script (&lt;100 lines), but can become a time-saver for larger programs.
Just do this: -- Assuming some hypothetical "StateT s (MaybeT IO)" stack liftIO = lift . lift liftMaybe = lift liftState = id -- or use shorter names if you prefer Then if you rearrange your stack or add additional layers, you just change the above functions and your code still works.
&gt; Most people who don't "get" Haskell yet just haven't tried to deviate from their previous language's way of doing things and just try to emulate their old language in Haskell. This does work, but it doesn't do the Haskell language justice. This is why my evil plan is so deviously delightful. They want to write Blub in Haskell? OK, here's how: let's learn monads! &gt;xD
What I like to do is give both versions: the educational one that shows off language features and the simple one that shows off ... simplicity.
Has anybody played around with SugarHaskell? It looks like it can enable something I've been looking for--a way to use applicative functors with infix operators without making the code uglier and harder to read. It would be nice to hear how people feel about it and what it's like in practice.
I would wager that it's easier to write a space-leak-free program in Haskell than it is to write a bug-free program in Python, so I think it's more appropriate to say that in Haskell you trade one class of bugs (runtime exceptions) for another class of bugs (space leaks). I can offer anecdotal evidence of this, in that I just wrote a client-server application in collaboration with a colleague. I wrote the Haskell server backend and he writes the Python client front-end. The Haskell server is considerably more complicated and runs indefinitely without space-leaks, whereas his client still has LOTS of bugs. Edit: Also, I want to clarify that he's been programming in Python as his main language longer than I have been programming in Haskell.
You covered monoids, but not semigroups. The latter don't have identity, so there is no `mempty` to apply at the end.
I'm sorry to hear you think it's silly. What I wrote is about much more general cases than semigroups, though. It covers any number of associative operations. I think its very interesting that such structures can be modelled as categories. 
I'm fairly certain that semigroups can encode any number of associative operations (through an approach that feels to me kinda like [defunctionalization](http://www.kennknowles.com/blog/2008/05/24/what-is-defunctionalization/)); namely, have a single operation, but separate your data into disjoint sets, and depending on which set an element belongs to, you apply the corresponding underlying operation.
I'm not claiming that they *are* categories. I'm claiming that for any associative operations, the structure of compositions of those operations can be encoded by composition in a category. 
&gt; Categories are the only structure that provides associative operations, at least for a class of associative structures so broad that I can’t see how to extend it. This is all I intended to call into question. Categories have more structure than is necessary, since they also include identity operations.
But you can encode just about anything with categories (along with n-categories). That's sort of the point.
If you're looking at rings (the natural notion of things with addition and multiplication), the natural extension of this idea leads to endomorphism rings. That is, take any set with an associative operator that we choose to call multiplication. Then we can encode multiplication in the same way, by identifying each element of the ring with a multiplication (if it's non-commutative, as most interesting rings are, then let's say left-multiplication just for grins). That is: liftVal a = (a *) Then in a similar way we have: liftVal (a * b) = ((a * b) *) = (a *) . (b *) = liftVal a . liftVal b so as before, `liftVal` respects the multiplicative structure. But throw in addition, and we want to be able to "add" two such functions together. We want the equivalent property, that: liftVal (a + b) = liftVal a &lt;+&gt; liftVal b It turns out the definition that works, assuming the distributive property of multiplication over addition, is pointwise: f &lt;+&gt; g = \x =&gt; f x + g x which is, incidentally, just `fmap (+)` in the `(-&gt;) r` functor. The relevant result from ring theory, if you pursue this line of reasoning further, is that every ring is isomorphic to the ring of endomorphisms of its left (or right) regular module. A fair number of arguments in ring theory work by turning arguments about rings into arguments about their endomorphism rings in this way. It turns out to be equally interesting to talk about the endomorphism rings of other modules, and it also turns out that the modules of a ring themselves form a category, and that category can be used to do plenty of interesting things with rings. So that path leads quite far, actually. If you're interested in it, I'd love to talk more offline, but we've digressed from Haskell here! *Edit:* Rereading the original, this is less similar than I thought. But I don't see how to extend that idea about mutually associative operations, while the stuff I wrote is all pretty standard from the last 75 years or so of algebra.
It's very common to say that a given set admits multiple semigroup structures or the like. A semiring, for example, is defined by two monoidal operations (one commutative) with an appropriate distributive law.
The example of semirings does not come under the purview of my post since the two operations do not associate with each other.
Usually when people speaking in favor of Python say that it is great for "IO bound" problems what they mean is that it is great for problems for which most of the time is spent waiting for some IO operation to complete and therefore the fact that the Python code runs slowly doesn't matter since it is a small fraction of the total time being spent by the program.
You seem to be describing the basis for "difference lists" (as can be found in the [dlist package](http://hackage.haskell.org/package/dlist-0.5) or the [`ShowS` type](http://hackage.haskell.org/packages/archive/base/4.6.0.0/doc/html/Text-Show.html#t:ShowS)) or any other kind of difference structure that arises from an associative binary operation, but don't mention them explicitly. The usual stated benefit on lists is that it's more efficient than repeated application of list appends, which will result in quadratic behavior (this benefit depends on the associative operation in question, though). Unfortunately, the difference representation results in a type that's "larger" than the original type: there are a lot more inhabitants in `[a] -&gt; [a]` than just partially applied list appends, and Haskell doesn't have a good way to prevent us from writing the bad ones. In Agda, on the other hand, we can state the restriction, and it actually comes with an additional benefit: we get associativity for free! Normally, if we have an expression like `(x + y) + z` and we need to unify it with `x + (y + z)`, if `_+_` is defined in the usual way they won't unify. We need to rewrite by an additional theorem of associativity, and it's a pain. If instead we represent the addition in the difference form resulting from this representation, they magically reduce to the same thing (assuming you use irrelevance in the right place in your definition), and it makes many proofs a lot more pleasant to work with. I wouldn't go so far as to say that associative operations _are_ function composition in disguise, as you did, but there are certainly useful things that can be done with the knowledge. If you're interested, the original technique was described [here](https://lists.chalmers.se/pipermail/agda/2011/003420.html).
I tried using cassava, but with all the confusion between all the stupid string types, I decided instead to write throwaway parsing code. My application needs to work with SPARQL, so ByteStrings and `[wchar]`s and Texts and Strings and on and on and on. The GHC overloaded strings directive generates even more compile errors.
I've been a bit busy doing other things. But I did add something today...
&gt;Cassava's got a decent API, but I decided to use throwaway parsing code, just basic Data.Util.split "," stuff, to get past the string type soup.It's surprisingly difficult to convert Strings to ByteStrings; something stupid in the libraries forces the Strings to first be converted into [wchar]s and then ByteStrings. And there is NO official way to do this; I tried using enums and came across out of bounds errors for several of the University names (they're German). So I gave up on cassava and wrote throwaway parsing for the first column of Strings in a CSV file. I have a hard time following you. Could you share the some example rows from your data. I could explain how to use cassava (which handles Unicode correctly) to parse them. You say it's difficult to convert from `String`s to `ByteString`s. Do you mean that it's hard to find functions to e.g. encode/decode from UTF-8? 
Regarding `ByteString` conversion to and from `String`, have you tried `unpack` and `pack` from `Data.ByteString.Char8`? I've actually done a very similar thing in Haskell, using the `curl` package to scrap web pages (in this case, Uniprot) and then doing all the string work manually (at the time, I was not as familiar with parsers). For Haskell development, I recommend these operating systems in the following order: Linux &gt; OSX &gt; Windows I know that Haskell SHOULD be much better on Windows, but right now it is not. You can save yourself a lot of trouble by installing a linux virtual machine, dual booting, or simply spending $20 a month to rent a linode server, if all else fails. Using Linux I've never had a package that failed to compile (assuming `*-dev` dependencies are installed) in recent memory, including the crazy ones like `gtk`. In fact, I would recommend not using Windows as a development OS in general, mainly because of the lack of a proper package management system and terrible command-line support. With Ubuntu specifically, installing a dependency is a simple `apt-get install libXXX-dev`, whereas with Windows it is a chore. Linux operating systems also have excellent support for compilers and interpreters of all languages (since Linux is basically the operating system of choice for all developers).
What you're doing is mapping a monoidal operation on `a` into its action on the carrier set `a`, of type `a -&gt; (a -&gt; a)` (each element `x` is mapped into its right action `(x op)`). This change of point of view is well-known in algebra. A lot of concepts in the theory of automata, for example, can be understood in terms of semigroup actions. In group theory, the idea that a group law determines, for each element, a bijection on the group itself leads to the idea of studying group elements as permutations in different spaces, such as vector spaces, were more tools are available; this is the starting point of [representation theory](http://en.wikipedia.org/wiki/Representation_theory). Now, I don't think it's fair to claim that associative operations are function composition "in disguise"; to me it suggests that the concept of associativity adds nothing more than the concept of function composition, and that all studies of associative operations could be dropped in favor of studies of function composition. That (which I don't claim you said, but still may explain some of the meh-reactions to your post) would be inaccurate, because the structure of associative operations is more restricted that the structure of composable functions; of course, operators that are not associative also have such actions, so you cannot reason accurately on associative operators only by reasoning on the whole function space `a -&gt; (a -&gt; a)`: (unrestricted) functions do not subsume associative operators. 
Nice utility. I wouldn't recommend brew as the first choice for installing the Haskell Platform, though. Why not just use the excellent quality native installation package? Unless you've got lots of spare time, or have some special reason why you need to compile your own GHC (plus everything else you would need to compile if you're not already a heavy brew user).
This mirrors my experience of haskell quite closely. I've been playing and learning for 18 months and there are still huge holes in my knowledge. A lot of what Brad says here rings true! I would love to be able to work with haskell, but the path up the learning curve makes selling the promised land at the top an impossible proposition at my work place. I hate to say it, but rightly so (for my place of work). I'm a 100% percent believer that haskell would let my write better software for my employers, but the [Bus factor](http://en.wikipedia.org/wiki/Bus_factor) would be huge! I'm not sure that I'm qualified enough to agree or disagree on the things in the ways forward section, but I'd like to hear what others think. Both those in my situation, who'd love to be able to use it but face too many barriers to adoption, to those who are able to use it for work.
Congratulations on your new job. Sorry to see that the fantastic CodeWorld project was put on hold, but glad that it's for good reasons. Hope you'll get back to it soon.
Someone please tell the author what that strange "Shift" key on his keyboard is all about.
Yes, good point. One thing I wanted to say but didn't to avoid an essay was that there's probably more room (and need) for algebraic reasoning in younger people than an introduction to haskell via drawing libraries.
It's [tr0lltherapy](http://www.reddit.com/user/tr0lltherapy), seems to eschew stylistic conventions like that, and is not timid about criticising Haskell. E.g. the same [proliferation of operators](http://www.reddit.com/r/haskell/comments/12e3a0/the_good_the_bad_and_the_ugly_haskell_in/c6ug5vf) criticism is mentioned here and in the article, and [racket](http://www.reddit.com/r/haskell/comments/12e3a0/the_good_the_bad_and_the_ugly_haskell_in/c6ufjko). I'm not a stalker, honest. I just recognized the style of writing and complaints.
It occurs to me that the title has several connotations: * I'm taking my faithful hound Haskell to the vet to be put down * I'm putting Haskell down over here for a little while * I'm putting Haskell down because it's stupid and it stinks! I think he meant the second one :-)
Lazy and strict Text are fundamentally different types. It's not just cosmetic. One is an array, the other is a list of arrays. The latter makes it possible to produce a Text value incrementally (which is why it's called "Lazy") while the former doesn't. But they're different data structures. (Which is in contrast to things like the lazy and strict versions of various Monads, which are the same structure with the difference being in the strictness properties of the operations on them.) My dream is that strict Text and ByteString would both be synonyms for Vectors, and we would have a 'data Lazy v :: * -&gt; *' which would make a lazy list-of-arrays structure out of any underlying strict Vectorish type. I haven't dug in deep enough to know if it's possible, but it seems possibly possible. Probably a whole lot of work though.
I found it an odd sort of calming. Didn't really realize how odd it was till I got to the end. Somewhat like the polar opposite of reading stuff by the g-wan guy which seems to always be jarring. 
I wasn't going to comment point for point but I ended up anyway so here we go. I've crossed out the points that I basically agree with: * **laziness by default**: One thing I'm convinced of is that laziness by default is good for a programmer if only so that it makes them aware of what strictness does to their code. Whether or not an industrial and "real world" language should be lazy by default is something I'm not so certain of but since I haven't attempted to make haskell work for me in the "real world" it isn't something I've had to think of. I'm certainly all for the cheapest syntax possible for toggling between strict and non-strict (I find seq cumbersome) * ~~**some basic stuff is/was missing**~~ * ~~**type spaghetti**~~: I think this is a legacy problem more than a language problem, which is really unfortunate * **ghc pragmas**: I think this comes with being a research language. You either have pragmas and experimental language extensions or you don't. Being a research language is part of what I like about haskell although I can understand if someone would like their work language to be a little less experimental. * **performance**: I think this is very closely tied to laziness. However the thing that makes haskell performant when it is is composable optimizations which I think in turn come from laziness. I could be wrong here though. * ~~**memory use**~~ as a result of laziness most of the time. * **cryptic libraries**: again I could be wrong but the trend to me seems to be that haskell libraries are ugly until they reach a final solution, upon which they start becoming much simpler. Since the Iteratee problem seems to be reaching a finale with Tekmo's bi-drectional pipes we may start to see the interface simplify (as far as I know pipes have only one composition operator). 
&gt; but the Bus factor would be huge! That's a good thing. Read the article properly.
I think the problem with Java is different from the problem with Haskell. In Haskell the GC scales quite nicely for many tasks, as long as all your cores are allocating. If one core stops allocating, then the others will all block waiting for it to synchronise, which is what happened in this example. We recently added a flag that adds safe points even when not allocating (-fno-omit-yields), which will help to fix this problem, although with some performance cost. In Java I suspect you're just finding that the GC doesn't scale very well, so to mitigate the problem you avoid doing GC at all, by elimianting all the allocation. 
There's pretty much just one way to implement strict vectors. Lazy vectors, however, can be implemented in many ways.
Two things: If you're using GHC pragmas it's not technically Haskell, but rather GHC :) I love lazyness by default, especially the performance advantages in terms of writing out clear code, and not having to worry about making sure expensive operations will only get run when needed (something I have to spend all too much time on in ruby).
&gt; strictness by default. Nope. With strictness, the fact that algorithms compose well in Haskell goes out the window. You'll pry laziness from my cold, dead hands. &gt; fewer basic types - fix the String problem by having one type that is the one that people should use. Similar issue. `String` is the lingua-franca you can use to translate between these other string types. Other languages suffer through with big flat slow strings for everything, but that doesn't lead to a good inductive (or coinductive style) where you can build up the string a character at a time. So to work with strings you always wind up having to go off and mess around with some awful string builder. In a world without side-effects that is a far less palatable option. In practice if you convert `Text` to a `String` you still get the performance and memory footprint of storing it as `Text`. &gt; no pragmas. &gt; no goofy type annotation syntax for performance tweaking. Okay. I'll give up a 10x performance factor just to add you to the community. That said, there have been research efforts (oh no, research!) that have looked at ways to evaluate naively written Haskell code with good performance. The one that leaps to mind is Rob Ennals' dissertation on speculative evaluation. &gt; fix modularity so that "cabal hell" is impossible. I don't think this can be done, honestly. cabal hell is more or less a direct consequence of the dread diamond dependency problem. We encourage the creation of a lot of smaller packages, so we feel the pain slightly more than most. &gt; basic "batteries included" libraries (see the Go standard library for a great example) and include these in the core deployment. This is the purpose of the Haskell platform. &gt; let Agda, etc be the focus of research/academic development. In many ways it already is. Haskell 2010 is a remarkably usable language, even though it is basically Haskell 1998 with a new coat of paint. It isn't like the language standard is rolling along all that fast.
Laziness is essential for defining your own abstractions, but I agree that we need the ability to selectively mix in strictness clearly. I would really like the ability to say "This data type should be strict". Right now I work around this by using a mixture of `Storable` and `UNPACK` pragmas and `deepseq` if all else fails. Really, we need a language that has an elegant model that unifies both strict and lazy evaluation. I would argue that space leaks are Haskell's single greatest *technical* weakness and we'd get much more users if we had a clean solution for it. However, I find space leaks easier to deal with than the problems of other programming languages, which is why I stick with Haskell.
It's just a lazy way of polling for updates. If someone else creates a module then you'll see it in the list. I quickly whipped this part up while at LXJS so that people could make modules while I was doing the talk. FWIW, if you click on one of them you see information about what it is, in this case a JSON request asking for the module list.
(Data.Monoid does have `(&lt;&gt;)` as a synonym for `mappend` now, but it's a recent addition.)
&gt; This mirrors my experience of haskell quite closely. I've been playing and learning for 18 months and there are still huge holes in my knowledge. A lot of what Brad says here rings true! Suggestion: know what you don't know. And take steps to fix the gaps! Monads is one place tons fall by the wayside. Another is type classes and the various Overlapping, Undecidable, Flexible, and Incoherent pragmas surrounding their use and how they subtly alter typechecking. Honestly, this article would have deserved a great deal more attention if not for the nagging suspicion that the author not only doesn't quite know what he's writing about but also could have but didn't take steps to fix knowledge gaps. Did he even ask for help?!
&gt; "This data type should be strict" What does this mean though? That all functions returning the type should be total? How would you check that?
My intuition is that it should behave as if the entire data type were UNPACKED. In other words, evaluating it to WHNF fully evaluates it. If I had to be slightly more rigorous, a strict data type is inductively defined as either one of a set of primitive types (i.e. Int, Double) or a product of strict data types. I'm still not sure if sums of strict types would also work, but perhaps somebody else can chime in.
This is why choosing a core set of generally applicable type classes matters a lot, in order to minimize the number of operators people have to memorize. This is one thing I actually love about Haskell: the community standardizes on category theory as the basis for these shared typeclasses.
/u/tr0lltherapy, you are one of my few reddit 'friends' because your insightful comments are often downvoted to oblivion as people tend to ignore the second half of your username. I do hope you continue to comment on /r/haskell. :)
Well. The most basic purpose of types is to classify values according to their representation, and to statically ensure that you don't get them mixed up and cause a segfault (or worse). That's your option (b): which representation you're using where is statically known. Option (a) corresponds to dynamically figuring out which type (representation) you're dealing with at any given point based on a tag you store with the data. What's option (c), exactly? This part: &gt; the decision on whether to use the strict or the lazy representation being made either by the programmer or by the compiler based on strictness analysis / whatever is pretty important: it's basically the whole thing. If they have the same type then how does the compiler know when they're actually different? It has to, if you want it to generate correct code. And how would it know which one is best in any particular situation, or which one the programmer intends? I think the reason that we have separate types is that it's a significant decision that the programmer is in the best position to make. (How is your hypothetical where there is a unified type and "the decision on whether to use the strict or the lazy representation [is made] by the programmer" actually different from the existing situation where we have two separate types?) Maybe what you want is a type class that lazy and strict Text are both members of, allowing functions to be polymorphic in the version of Text that they use, with the decision (type inference) being made at the call site? I think that would require all Text-producing functions to be polymorphic in the class, and the consumers to be monomorphic. It *might* be feasible, but don't hold me to it because I've only thought about it for about a minute, and about the advantages and drawbacks for none. (Edit: Or maybe it would just look like Data.Vector.Generic. I have also thought about this for zero minutes.)
Okay. Do Text and ByteString use different ways? Those are the two we're interested in. (I haven't checked.)
Well, my recent experience makes me feel that there's another thing to add to the negatives list: the syntactic gap between pure and monadic code. How many of you have written 1,500 lines of pure code, only to find in that you need some sort of effects, and now you need to refactor most of that code to change all the `a -&gt; b` types into `Monad m =&gt; a -&gt; m b`? Why can't "pure" code, which as far as I can tell, can always be interpreted as `a -&gt; Identity b`, be recast into monadic code with minimal changes?
I think much of this depends on what you want to use Haskell for. If you want it to supplant C++, then many of the problems in the blog post are important. However, for me, Haskell replaces Python and Java more than anything low-level. And here many of the issues are much less important: even unoptimized, the performance and memory usage are almost always good enough. The benefits of expressiveness given by laziness *far* outweigh the downsides. The cryptic operators may be an issue but, on the whole, I've found the ability to define and use custom operators a significant net gain. For the small price of some potentially poorly named functions, we simultaneously avoid the mess Java has with things like BigNum and the promiscuous overloading of a few select symbols endemic to languages like C++ and Python. Also, it's interesting that you mention type spaghetti but then go on to use Racket. I've been using Racket for a project recently and have had a horrible headache from exactly that problem. Not only does Racket have a superabundance of different types, but it also does not give you any good tools for dealing with them. I've had far more difficulty with different numeric types, different string types, mutable and immutable vector types and streams vs lists vs other sequences than I ever have in Haskell. Also, having to use Racket--which is by no means a bad language, I should add--has really made me appreciate Haskell's syntax and laziness (along with other things like the type system, of course). In Racket, you can get functions like `and` that are not strict in their arguments by implementing them with a macro. However, this approach is not compositional: now you can't use `and` in a map or a fold. Ultimately, laziness and immutability let me worry about less things when I program. I don't have to think about whether my where binding will be used in every branch of the body, for example. I also don't have to worry if my lists are sometimes infinite. I don't have to make sure whoever I call does not modify my arguments or state. So, overall, I've found many of the comprises Haskell takes to be good. For me, writing code quickly and correctly is more important than making it fast, and Haskell handily beats every other language in these two arenas. And, even so, the performance isn't bad and can be optimized quite far without having to leave Haskell land. 
I think it is a function of anonymity and the voting mechanism. On the haskell-cafe most people are going by their real names and you have to consciously say something to express your disapproval that can be mapped back to you. On /r/haskell you can just click the down-vote button and move on with your day without having to justify your opinions.
His insightful comments are generally upvoted. He gets downvoted when he makes really crappy posts. He isn't downvoted because of his username, he is downvoted because of the content of his posts.
There is no koolaid. If you don't like haskell, feel free to not use it. Generalizing everyone who does use it as being either "elite masters" or brainwashed just makes you seem like a troll or someone with some serious emotional baggage, and makes people's default reaction to you be to ignore/dismiss you. If you want to share some information, convincing people that you are a nutjob isn't a good way to do it.
&gt;i do notice that haskell-cafe tends to be more welcoming of dissent than /r/haskell If mailing lists came with the ability to downvote, you'd get downvoted just as much there as you do here. There's just no quick and easy way to express "this post is dumb" on a mailing list.
I think you articulated something that I never fully realized until you said that the Haskell community self-selects for kind and helpful people. I remember when I first joined the community I was very aggressive and confrontational, but the more I hang out the more helpful I become. This community very carefully polices and grooms newcomers to continue the tradition, almost like a self-preserving culture.
Were you under the impression that hanging out here constantly repeating "haskell is too hard and academic and has no practical use" and "lazy evaluation is wrong because I say so" was "quite valuable"? I'm sorry that you spent 6 years complaining about haskell. You were not forced to do that, you chose to. Haskell users do not owe you anything for your self-inflicted suffering.
I personally think experiences like yours and the author's are typical. Those who just "get" it get it. Those who don't don't. And those who do can't understand why those who don't don't. As someone who has invested a great deal of intellectual energy to this, I look back at what I've learned and find it would take long, long essays of explanation to describe how things work. Those who "get" it would only require a short explanation and seem to just not understand why this is insofficient for **most people**. They seem to not realize they are a minority, simply for understanding. It's really powerful stuff, but I don't think it's for everyone.
That or you are the next e e cummings.
I agree with this too. The Haskell IRC channel is the only programming language channel I know of that is truly functional (no pun intended). People there are thoughtful, knowledgeable, helpful, intelligent, and kind, and this is because the Haskell community is unusually humane and well-functioning. 
Yep, I think "operators can only be declared as syntactic sugar for existing named functions" would be a good rule. As it is I frequently combine the two in a single declaration: -- | Documentation foo, (@@@) :: A -&gt; B -&gt; C foo = ... (@@@) = foo and for approximately one minor release version Haddock actually kept it that way in the generated documentation\*, but they've since changed it due to some complications with other things\*\*. I've been meaning to look into what it would take to change it back, but I'm basically useless for actually doing things... \* In prior versions Haddock split them into separate declarations with one of them missing its documentation, in newer versions it splits them again but at least with the documentation also duplicated. \*\* IIRC it was something to do with the interaction of declaration groups with selective and/or re-exports.
I think of GHC as the premier, de facto standard for Haskell. I don't know why anyone would use UHC, and Hugs has barely enough features to do anything at all. But they do exist.
&gt; the various Overlapping, Undecidable, Flexible, and Incoherent pragmas surrounding their use and how they subtly alter typechecking. Just keep your sanity and don't use any of those :)
To tie in with edwardkmett's [comment][1] above, I have no idea why this comment was voted down. I don't think I've ever used the downvote button (maybe once or twice?) and have difficulty imagining the mindset of someone who does so regularly. Maybe a downvoter can chime in and explain their philosophy? [1]: http://www.reddit.com/r/haskell/comments/13g0ul/putting_haskell_down/c73rg3e
Isn't that what, eg: data List a = Nil | Cons !a !(List a) does?
I dislike `&lt;&gt;`. If I really wanted an operator I'd do: import Prelude hiding ((++)) (++) = mappend But most of the time `mappend` itself is fine.
Hugs I have used and I'm not sure where this "barely enough features to do anything at all" that I keep hearing comes from? It seems to support all of Haskell98.
&gt; What gets slower by making "experimental features" no longer experimental? GHC. And if the pragmas weren't experimental, we would observe them as built-ins, and their pragmas as deprecated.
One thing from this post: ++ rust! Maybe someday I'll actually build the compiler (or someone will release binaries), but everything I read about it looks awesome. It may all-but replace C for me when it becomes more stable.
well, Haskell - the research project (which is still in part) - was ABOUT laziness, and efficient compilation of it. &gt;Then, in the late ’70s and early ’80s, something new happened. A series of seminal publications ignited an explosion of interest in the idea of lazy (or non-strict, or call-by-need) functional languages as a vehicle for writing serious programs. &gt;As a result of all this activity, by the mid-1980s there were a number of researchers, including the authors, who were keenly interested in both design and implementation techniques for pure, lazy languages. In fact, many of us had independently designed our own lazy languages and were busily building our own implementations for them. .. and in short, Haskell was a product of this lazy community coming together. I don't think there's any point in considering Haskell should be strict - a strict pure language with advanced type system is an interesting beast on its own, but it isn't Haskell pretty much by definition. EDIT: source http://research.microsoft.com/en-us/um/people/simonpj/papers/history-of-haskell/index.htm
&gt;where did i give that impression? Every bitter, misanthropic post you make on the subject. &gt;and if you don't give a crap about what i post or think, why do you keep replying? Who said I don't give a crap about what you post? People like you spreading the "haskell is academic wankery with no practical use" myth steered me away from haskell. When I finally did try haskell and realized that was total bullshit, I regretted that I had allowed myself to be tricked. So I care that people see the other side. I also was offering legitimate help. You spend a lot of time complaining about the reception you get. This means you want to change it. The problem is, you want to change it by complaining until everyone else stops thinking you are a troll. I was suggesting that you could take a different approach: stop acting like a troll. If you stop acting like a troll, people will be more likely to take the time to consider what you say, instead of making a default "not this dumbass again, downvoted" assumption.
&gt;And if the pragmas weren't experimental, we would observe them as built-ins, and their pragmas as deprecated. That's a tautology. They are experimental because they are experimental. My point was that I think several of the commonly used pragmas have moved well beyond the stage of experimental, and people expect that they do become built-in and the pragma become deprecated, but they remain as-is.
&gt; There's just no quick and easy way to express "this post is dumb" on a mailing list. A quality that [I think is good.](https://plus.google.com/u/0/102125020195927401442/posts/5yUg9e3iJyT)
I agree. I didn't mean to suggest that this is a feature that makes reddit *better* than mailing lists. I was merely suggesting that the difference is not the people here vs the people on the list, but rather the medium itself. Reddit is designed to be anti-discussion, to promote trivial non-content, and to punish alternative viewpoints. So that is what it does. On the other hand, HN is better designed than reddit in this regard, and he still managed to shitpost enough to get banned there several times. So I think it is also quite likely that he confuses "welcoming of dissent" with "tolerant of trolling".
And it even supports extensions that GHC does not! One that I would like to see carried over is the support for local/restricted type synonyms.
It wasn't intended to be a direct quote, I was paraphrasing because it is much shorter. Quotes from you suggesting that haskell is not a practical tool for real world programmers: &gt; i did everything i could to arrive at some point where haskell would reward me for my loyalty and hard work. &gt;i couldn't really advocate it to my coworkers. it had taken me years to reach an intermediate stage of haskell proficiency. &gt;some of these puzzle solutions would periodically manifest some performance or resource issue that reminded me why i can't bring myself to apply haskell to real daily coding, but since a puzzle is contrived anyway, i was able to shrug it off When discussing "cons" of haskell: &gt;i'm a practical working coder &gt;in production systems, it is. In response to "haskell is just as useful as java" &gt;we are discovering that this is untrue In response to your own trolling: &gt;or is the final acceptance that haskell is not a useful practical language just too painful to even respond to? Stop trying to pretend you are some dispassionate, unbiased third party just trying to give a helpful critique of haskell, your post history is right there for everyone to see.
Yes, "haskell is not a useful practical language" is nothing at all like "haskell is academic wankery with no practical use".
&gt; I accept I am not a haskell elite master Over the course of 10+ years on the haskell-cafe mailing list I've seen how otherwise ordinary folk gain rapidly in skills. They lurk, ask questions, and read the literature. What have you done? &gt; dismissing anyone who hasn't drank the koolaid I think you did yourself major disservice in that post and I wanted to point that out. As for the koolaid comment, except for one anomaly, everyone I know who seriously uses haskell can't wait to lay their hands on something better. It's a research vehicle to get to better shores! Where's your evidence we worship the ark? 
And they downvote it! It would be self-parody, except they're anonymous. Seriously, what the fuck is wrong with you people? It's an even-toned (unlike this one), reasonably well-reasoned post. If you disagree with it, say why! What does downvoting accomplish? Is it your mission in life to suppress opinions you don't agree with, thinking (because you're obviously right) that it will make the world a better place? It won't.
I have not yet voted on the parent post, but I don't see downvoting as supression, merely a way to express an opinion.
I sometimes dowvote things. I see upvote as "I like this" and downvote as "I do not like this". I have not voted on the post in question.
That's bad reasoning. Iteratees might not exist if strictness *was* the universal default. But people don't think it would be the *correct* universal default, which is one reason why it's not. Life involves tradeoffs. Strictness and laziness both have advantages and drawbacks. Iteratees are an attempt to solve some of the same problems that universal strictness might solve, while maintaining the benefits of laziness. It doesn't mean people don't want laziness. It means they *do* want it. Otherwise they would be using a strict language.
The constant crying about downvotes is much more annoying than the downvotes. This is reddit. It was designed to encourage this behaviour. It is unreasonable to expect any other behaviour here. His post being "even handed" does not mean it is insightful or a good contribution. It makes an obvious logical fallacy that undermines the entirety of the post. You have to learn a whole bunch of OO stuff to use java. You have to learn a whole bunch of functional stuff to use haskell. Pretending java requires less learning simply because you had already learned those things previously is silly. And as many people point out so frequently, haskell requires you to learn absolutely no category theory at all, whatsoever. Whining about the downvotes he received actually served to make me re-read it and realize that his post is much worse than I had originally considered it to be.
Do you think those downvotes have value? Or is it just self-expression? I go by their practical effect: highly downvoted comments get hidden. Therefore that's what they're for: suppression. Moderation of inappropriate or highly unhelpful (or so on or so forth) content if used well, suppression of valid dissent if used wrong.
I was in your boat for quite a while. In fact i think many of successful haskellers go through this step. I also got frustrated, stopped a few times, only to get back after a while. Eventually i made a breakthrough, thanks to yesod that made it practical to use haskell for some of my real work. Now i turn to haskell on every new task or project. 
No, they aren't. You can't post something crappy, include "if you downvote this you are proving me right" and then seriously take any downvotes as proof that you are right. Your post isn't dissent, it is a really fundamentally flawed attempt to prove your opinion is an objective fact. The fact that it is such an old, oft-repeated, oft-debunked argument (haskell makes you learn category theory so it is harder than java where I pretend I didn't have to learn anything), means it can be hard to tell if it is a genuine post or trolling. I realize it can be irritating to be dismissed as a troll when you didn't even know that what you are saying has been used for trolling, but assuming that anyone who has seen that troll before and is tired of it is just "exclusionary" and trying to "ostracise dissent" isn't a reasonable conclusion.
I hope we are able to keep this up as the community grows.
&gt;This fallacy you point out is a straw man. My argument actually implies that Java is exclusionary. No it is not. Your edit implies that, your argument did not. You can't change what you are saying and then claim anyone responding to your original post is misrepresenting you. &gt;However, it's a hard argument to make because most people were taught OOP in school--basically it feels not exclusionary because everyone around you knows it. That's just because they've been included. But "I already learned X" is not the same as "you don't need to learn X". The amount of learning required to use haskell effectively is not significantly greater than the amount required to use python effectively. "I already learned half of python" does not demonstrate that the haskell community is "elitist and exclusive". &gt;What I actually said is that the people who claim it requires no category theory are being more exclusionary because then things like monads whatever else are then inclusive to Haskell only, with no application outside that domain. Yes you did. But it doesn't make any sense, and you didn't support that claim with any further information so that readers could make sense of it. You don't need to learn any category theory to use haskell. This is a simple statement of fact, and has nothing to do with exclusion or elitism. It is not a comparison to anything else. You also do not need to learn how to cook a soufflé to use haskell. This statement is no more exclusionary than the previous one. In what way is pointing this fact out exclusionary? People are welcome to use monads in languages other than haskell, and they do. They also do not need to learn category theory to use those other languages.
Unfortunately in the real world you do have to think about laziness.
Who knows? They would need some really killer features to make me move away from emacs. Now that I think of it, instead of creating an IDE from scratch, emacs could probably be extended with all desired Haskell IDE features one could think of.
Good point, but are you saying that bad design is (or should be) destiny? Even if it's designed in a way that discourages discussion (especially if, as you imply, it wasn't intentional), we don't need to use it that way.
&gt;Why is it everyone takes everything I say adversarially on this subreddit? When a large group of dissimilar people with conflicting opinions come to a consensus on your behaviour, that is most likely to indicate something about your behaviour rather than something about that group of people. You appear to be acting adversarially. You feel that people "pushing low quality posts to the bottom" makes it not-so-nice. But have you considered how your posts effect the niceness of this subreddit? Give people the benefit of the doubt. Ask instead of accusing.
Yes, I wonder too. What do you think about this issue: trolling appears to succeed inordinately well in obtaining instructional assistance [1]. How should one respond to trolling in general? [1] http://bash.org/?152037
It is not unfortunate that most of the really snazzy graphical stuff is out of your reach. It is a lifesaver. It's been done a number of times and it has never taken off. You are far better off trying to build a good IDE for Haskell, one that fits in to what you are saying in the rest of your post, than being just another person to chase the phantasm of overly-graphical programming environments. (As for anyone who wants to cite LightTable as a success... no. Come to me in another four or five years and show me its wild success and we'll talk. I'll put my money down on a lot of people trying it out, and then just sort of... wandering away from it without really being able to explain why, but one day they switch back to a conventional IDE, maybe the latest update crashed, maybe they were borrowing somebody else's computer, and then they just never get around to going back. It's a very pretty demo, there's still not a lot of evidence IMHO that it's useful in the long term.) Oh, and if I can put in one teensy-weensy request, make sure your IDE doesn't crash. It seems like every time I try to use an IDE that is even slightly less than mainstream, it crashes. I'd rather use an IDE with 100 solid, reliable features that work together than a 1000 where 800 of them simply crash. You think I wouldn't have to say this... but experience suggests I do. Hopefully writing it in Haskell will be a big help.
 * real-time collaborative editing: lots of stuff exists, take for example tmux and other screen sharing tools. * SVN/cloud integration: I have no idea what this is. Could you explain? * Hackage/Hoogle lookup: Several ways to get this. You could run lambdabot or goa (not sure if goa is still supported). Personally, I have shortcuts set up in my browser for doing specialized searches and I just alt-tab, do a search, and then go on my way. This could be integrated into emacs pretty easily. * automatic type annotation generation: I'd rather have my code generated from my types, but that's because I strive to write the types first. I'd need more clarification here. * powerful refactoring: HaRe? * debugging tools: The debugging tools would need to be created first. Of course, you can use the ghci debugger from inside emacs already. That's why people seem to be skeptical of not just using emacs or vim to build an "IDE". Actually, I'd go even further and say that if you use a unix-style environment (OSX, Linux), that your IDE is your shell. I think IDEs really only make sense if you're stuck on a platform that doesn't integrate software development naturally (such as Windows).
&gt;Am I not allowed to restate my point after I realize I did not originally put it the right way? You certainly can restate your point. I simply suggest that you can't call my response to what you wrote a strawman based on that later edit. I am not able to go into the future and read things you have not wrote yet in order to respond to them. &gt;OK, how many people have to come here saying that they've been at it for weeks, months, and years without gaining competency before this is prooven false? How many people have to come and say they don't like chocolate milk before it is proven that chocolate milk tastes bad? There is no amount of people sharing one opinion that will ever prove that opinion to be a fact. Bob finding haskell hard to learn and python easy to learn does not prove that you have to learn more information to be able to use haskell. &gt;After a few months of Haskell, by then a programmer of over 5 years, I was only profficient free functions and hadn't figured out functors and monads. My father has had no programming experience at all, ever. I started teaching him haskell 2 months ago. He's already covered functors, monoids, monads and applicative functors. Anecdotes don't turn opinions into facts. &gt;You are twisting my words left and right No, that was a direct quote that you replied to. Do not act disingenuously and then claim you are being oppressed when you are downvoted. http://www.reddit.com/r/haskell/comments/13g0ul/putting_haskell_down/c73rvex He said you were implying "that the Haskell community is somewhat elitist and exclusive" and you responded: "It is. While many are helpful, the fact of the matter is that other languages don't enforce that the programmer learns so much about category theory, type systems, monads, monoids, etc." Stop trying to pretend you didn't say what you clearly said and then getting upset that other people can read what you said. &gt;If you know Haskell, then you're already fluent in category theory. You view functions as arrows between types. Once again, this is entirely false. Stating it again will not make it true. I know absolutely nothing about category theory, and have been teaching people haskell lately who didn't take math past grade 10. I can't even respond to the rest of your attempt to justify this because it is even more nonsensical than the previous one. &gt;And fighting for inclusion in imperative communities when you're working on functional code is just as difficult as attempting to be included in /r/haskell as an imperative coder And fighting for inclusion of cats on /r/dogs, and any other needless, off-topic nonsense. Why do you think that should be encouraged? &gt;But I think they'd be better programmers if they did. That's super for you, but it doesn't support your claim at all. These non-sequiturs really don't help make your point clearer, they just confuse things even more.
&gt; You're saying that if I want to use cassava to parse German CSV files, I have to use at least two string types, possibly more. Screw it, Strings all the way. To be frank, this makes absolutely no sense. Bytes and Unicode are not the same thing. It's like saying: get rid of integers, we represent them as strings: "12". The integer 12 is different from the string "12". The same way the UTF-8 encoding (a sequence of bytes) a of a Unicode string is not the same as the Unicode string (a sequence of code points) itself.
I'm intrigued by the fact that the IDE will be browser-based.
I downvote things that I find boring (not just inoffensively so, but actively, painfully tedious), because I am irritated that I read them. I also downvote things that are wrong, even if I make a post that responds to them. This is because those things are wrong, and I want to indicate to others that there is something problematic being said. Also because I am irritated that I read them and am even more irritated if I feel compelled to respond. Basically I downvote everything which I think makes a discussion less useful, and upvote provocative, genuinely useful contributions.
Apologies, I did not.
&gt; if everyone thought laziness-by-default was always the best choice, Iteratees wouldn't exist. My perception is that iteratees exist because strict IO is so painful, and lazy IO is merely a failed attempt to address the same problem which already existed. Note that lazy IO has little to do with pure, lazy code and pretty much nothing to do with this whole lazy-by-default issue.
If you do 99% of your coding in the IO monad, well, then you are doing it wrong. :)
I mean hyperstrictness, which is why I inductively define it the way I did. Functions, for example, would be inadmissible. This means that a hyperstrict type can't have polymorphic fields.
That's pretty cool. I'd point you to meteor, a javascript web framework that's good at this instant updating but that'd probably defeat the point of fay.
Not sure how long you've been around, but I did this last year. It was a phenomenal success. If I ever get around to getting appropriate parent permission and such, I'll try to put together a highlight reel from the panel discussion we had at the end of the year. I'm still in touch with those students, several of whom are now starting algebra in the 7th grade, and the feedback I'm getting from them about their experience learning algebra is very promising as well.
Lazy means delayed computation. In theory, you can have a completely non-trivial data structure from which chunks can be derived, though not through simple pattern matching. A tree-shaped string builder comes to mind: you'd need something like a zipper to get the next chunk. data LazyTextTree = Empty | Concat LazyTextTree LazyTextTree | Cons !T.Text LazyTextTree This is perhaps a bit of a pedantic point, but this might be something you want to have for fast concatenation.
&gt; Hopefully writing it in Haskell will be a big help. Too bad you won't be able to install the GUI package. 
Except that I couldn't find that quote anywhere on the page except in your post, but fine. Fuck it. Pay attention to what I say, ignore what I mean. Have a happy life.
Let's digress from the topic and just address this, suitably `forall`'d: &gt; If X really has what it takes to succeed then no amount of trolls or critics will stop it. Whence the underlying strain of German romanticism in that kind of statement? Is that "aesthetic" even born out by facts/history?
But there's an opportunity cost. Sure, time spent engaging critics (even trolls) is, or can be, productive. But is it *more* productive than how that time would otherwise have been spent? (Yeah, not an easy question.)
I can't contribute effectively to this lil' eternal september because I have limited experience with python. Let me just address this last footnote: &gt; The myth that Haskell does not permit heterogenous structure is easily debunked: data Thing = forall x. Thing x lst :: [Thing] lst = [Thing 1.5, Thing "foo", Thing 3] So let's try `putStrLn $ lst !! 1` shall we? Why does the existential blow up?
Flexible instances/contexts are quite often useful.
Haskell makes a lot of trade-offs that are supposed to have short-term inconvenience for long-term benefits. I do believe I gain those benefits, but I can see what he means about never reaping those rewards.
&gt; This is not an open-source project This really saddens me. It's a mammoth project and exactly something that benefits from many hands. That, and I don't want fpcomplete to be supporting an IDE.
Why not keep it pure but fmap it into the monad?
But there already are several open-source IDE projects. Do we need another one? We've decided it was time to try the commercial approach. We welcome community input but we are following a more focused strategy with programmers working full time and being paid for it. 
While the composition of functions, per se, is rather limited; this view of associative operations does have a good deal of support. If we move to monoids (associative ops with an identity element), then it is well known that a monoid is a degenerate form of a category (a "monoidoid" if you will). And categories generalize the notion of "functions" and composition thereof quite nicely. If we drop the requirement that categories have identity morphisms, then we're left with [semigroupoids](http://hackage.haskell.org/package/semigroupoids). From this perspective, semigroups are indeed just something like function composition in disguise (i.e., a degenerate case of semigroupoids). But note that this category theoretic perspective is rather different than the perspective of representation theory. Yet another avenue for exploring associativity is to look at quasigroups and the like, which generalize semigroups in a very different direction than semigroupoids do.
Yeah, he doesn't explicitly say he's looking into using it for the GUI on the IDE, but I wouldn't be surprised; I think LightTable does this.
I can count you out from liking my IDE then. ;-) It's in the browser and pretty monolithic. Well, I use or build Haskell tools, but I won't spend too much effort if it gets in my way of being seamless. But you seem to care about editing “text” so it wouldn't be of interest anyway!
Why is generalization silly? How is looking at semigroupoids not a fruitful avenue for exploring the structure of semigroups? Sometimes the world ain't so nice as the abstractions we originally had. Generalizing those abstractions is helpful both for capturing the world, and for uncovering what it was *precisely* that made the original abstractions so nice.
Getting 404. EDIT: And 403 on everything else.
I think we should distinguish here between 'translate' and 'transliterate.' The question asked for the former, but the linked answer provided the latter.
This worked for me: http://webcache.googleusercontent.com/search?q=cache:http://www.stephendiehl.com/posts/types.html
Well, my comment was a massive simplification of what I'm tackling right now. I've been prototyping/brainstorming a system of [OLAP](http://en.wikipedia.org/wiki/OLAP_cube)-like "calculations." The first shot at it was purely functional, but well, I'd like to augment the prototype to actually read data from files. The cruder way of doing this is to read all the data necessary in the IO monad, use the results to construct pure calculations that consume that data. What I want instead of that is to have the calculations themselves be similar to monad transformers—parametrize them by the effect type, and have the execution work something like this: execute :: Monad m =&gt; Calculation m in out -&gt; in -&gt; m out As opposed to the initial, pure version, which is more like this: execute :: Calculation in out -&gt; in -&gt; out I'm enormously simplifying this, still, because `Calculation` is only *one* of three interrelated types that need to get this treatment. Making this switch is turning out to be a big pain—and the old `Calculation in out` is really the same thing as `Calculation Identity in out`.
Well, "crashing" before I even execute the program is sort of in the spirit of Haskell and its attempt to provide compile-time assurances. Call it "efficient".
Yes, because when you address a critic's issue you know you are solving a real problem. When you work on whatever captures your interest you may not necessarily produce something useful. Also, it is terribly fun to solve it so that you can tell next the critic "we already solved this" and watch them squirm.
Actually, I would like a simple browser text editor just in isolation, especially if you could securely access files on a remote computer.
The type problems were mostly what I mentioned before: strings vs bytestrings, streams vs lists vs other sequences, different (exact and in-exact) numeric types, mutable and immutable vectors, different regular expression literals, input/output ports of different sorts and different structs. The core problem is that Racket functions are very picky about what they accept, but since there is no static typing (I haven't used Typed Racket) and many of the types look very similar visually, it is often very difficult to follow what type you're passing into what function. Then it only comes up when you test and only if you test the branch of the code where you already have a type error. Essentially, you have many similar difficulties to what you experienced with Haskell types, except without the type system to help you ensure everything is correct. So I, at least, spent a good amount of time at the repl dealing with these sorts of runtime type errors. It was not much fun, or at least not nearly as fun as Haskell. I should add that Racket's still a very nice language, but I would personally choose Haskell over it for virtually any task.
I'd love to see an example of your workflow. It seems unfathomable to me that you could get along with Haskell in just vi. How do you handle all the indentation?
I've been trying to stay out of the comments on the FPComplete IDE posts, but I think I can hold back no more. I wish FPComplete the best, but I feel compelled to speak up about what I see as flaws in the current strategy. I intend to be helpful with my criticisms (not harmful!) but my passion is probably getting in the way of judgement (eg., perhaps I should just delete this post and keep my mouth shut). I think what annoys me the most about the focus on an IDE is that the assumption the community needs an IDE seems to be flawed. If you look through comments and discussions about having an IDE for Haskell I think you'll see an overwhelming trend for **existing** Haskell users to say, "Vim/Emacs/etc works great. We don't need an IDE." Now, there could be a group of Haskell programmers (or current non-Haskell programmers) that would benefit from a Haskell-centric IDE. I would be shocked if FPcomplete, or anyone else, can point to a significant group of people (pick a reasonable metric for "significant" such as size of group and/or willingness to pay for an IDE) that would say, "Yeah, we've been meaning to adopt Haskell except for the lack of IDE and we also want a non-university Haskell course to train our staff". I'm quite skeptical about the existence of said group. On the other hand, I absolutely agree with the spirit of bringing in new users and increasing adoption of Haskell (that's what has me so fired up right now). That's good for everyone here (FPComplete included). I think new users adopt Haskell when they see Haskell as an obvious win. So I think the question to ask, is how can we get people to see obvious value in solving problems with Haskell? I believe that comes from success stories that programmers and managers trade during informal conversations. Basically, the awesomeness that is Haskell needs to be in the collective "background of obviousness" in the industry as much as possible. Take an example from the history of Haskell. The [Pugs](https://en.wikipedia.org/wiki/Pugs) project brought over a number of perl hackers because they were amazed at the ease with which Audrey Tang was able to create a perl6 implementation before the perl devs could do it themselves. Other examples include darcs (even though it's now fallen out of favor/use) and XMonad. Based on the above, here are things I think the **existing** community at large would greatly benefit from. The theme below is that I'm focused on things that will help existing Haskell users produce "production" quality releases, so that we can create the success stories necessary to get Haskell into that collective background of obviousness: * Hackage 2 deployment * A vetted subset of Hackage ([Level 3 in Snoyman's recent blog post](http://www.yesodweb.com/blog/2012/11/solving-cabal-hell)) * Robust/mature support for currently unsupported platforms (eg., .NET, JVM, web browser, ARM(s), iOS) * Good cross platform GUI libraries that are easy and painless to install. Try making a GUI program with Haskell and see how many people can install it on Windows, OSX, and Linux. Your choices are wxHaskell and GTK. GTK doesn't feel right on Windows/OSX. Both are a pain to install anywhere other than Linux. * Improving the infrastructure such that developing for windows is easier (porting libraries might be a way to help here, getting husyhox to be usable as a posix shell environment is another) * Travis-ci provides a free continuous integration server, but they lack windows build slaves. Haskell is terribly weak in windows. How about working with Travis-ci to develop windows build slaves for Haskell? * Hackage/github/travis-ci integration. Imagine being able to create a tag ("hackage v0.1.1.3") in your github repository and the resulting push fires off a travis-ci build. If the build passes the tag is uploaded to hackage using the version number from the tag. Your good releases automatically go up on hackage. * Powerful debugging tools I think providing an IDE is most likely to help Haskell users who spend all their time in windows (where they don't have a good shell; and I'd argue that providing them will a good shell will provide them with so much more value). Finally, look at it this way, is ruby successful because of rails or because of a good IDE? How about python? Java? C++? Go? The list goes on. In some cases, good IDEs grew up with the languages and in other cases not. I don't think in any of the cases that you can say the good IDE caused the language adoption. Therefore, I would argue that we need tools to build success stories (like the tools I enumerated above) not a shrink wrapped IDE. Edit: Fix some typos.
1. When I was switching from BASIC (remember it?) to C or pascal I had so disappointed how can I write programs without numbering lines and without "goto" (I was a schooler). But after some practice I can write programs without "goto". And after Dijkstra's famous article I understand why structured programs are good and using goto is bad. After too many years I often meet programmers (php mostly) who cannot listen about structuring programming (I don't say about category theory, but only about structuring programming)? I think it is because of bad math culture. On what departs do programmers study? I suppose on math. Aren't they? Why engineers HAVE TO study hight math, have to study derivatives, integrals? Why electronic engineers have to study Fourier transform? Why engineers have to study functional analysis? But why programmers haven't to study category theory? Yes, I suppose in most concrete situations it is not necessary to use category theory, but it is very useful to know it for programmers. Isn't it? I suppose It is because if you built a bridge than was crashed then you can be arrested. But if you built a web app that is crashing you can ask more money to fix bugs. (or You can make some OTP infrastructure for auto restarting your crashing web app (hello, erlangers)). 2. Yes, to write programs in php is more easy. You can write they in style than I named as "as I see it and I sing". Have a look at most open source php projects. You can see there much interesting. Supporting that code is nightmare. Yes, I know there are people who write in php very well (as it is possible in php). But that means then these people have a good math culture and these people can understand lisp, can understand haskell, can understand category theory and (what is more impotant) can understand why other people thinking up not only php, rubby etc, but thinking up haskells, agdas, category theories. 3. Who can say me why "goto" is recently appeared in php? 4. Sorry for my poor English
Thank you! I appreciate the compliment!
I use vi's search and replace feature to add or remove indentation in blocks, or if it is just a few lines I indent (or unindent) one line using a single command and use `.` to repeat the command on each line. However, it's not really about how easy or hard `vi` is to use. For me, the act of typing out all the code line by line with little automation gives it more of a feel of crafting my code as opposed to mass-producing it so I get more personal satisfaction out of the act of coding. When I work with others I don't have as much liberty to do this, but on my own personal projects like `pipes` I go nuts with this and it makes even mundane parts very pleasing to work on because then I feel like an artisan as opposed to an assembly-line worker. Also, when you get into the `vi` zone, you sort of get addicted to the challenge and thrill of it. I actually think that `vi` is NOT a very practical text editor but rather it is a very entertaining text editor, where you feel like you are doing an elegant and challenging dance with your keyboard. This makes the act of typing very enjoyable, too. Since typing is rarely the bottleneck for coding, I feel like I can permit myself that luxury.
The problem here is that no matter what I introduce the argument would be: "Yes I agree that it's less code and I maybe agree that haskell gives you all these benefits, but you could write the same thing in 3-4 times the LOC and twice the time in C# and we wouldn't have to wait 18 months for the rest of the team to understand it and be able to support it"
It would help me to understand what you're saying if you explain specifically what you think I am generalizing and how it is actually a restriction.
One more gripe, that deserves its own comment: "category of function composition" isn't a meaningful phrase. A quick google reveals that in fact it existed nowhere on the internet prior to this post. Usually we say "category of x" where x are the objects of the category. Sometimes we can define a category by its morphisms, but usually explicitly so. In any case, is there a category where its objects are "function composition"? What are its morphisms!? Or is there a category whose morphisms are function composition!? What are its objects? I'm all for people learning things, and thinking things through, and posting interesting insights and questions and even half-baked ideas. But please, if what you have is a partial idea, present it as such! And if you don't have the proper language, don't just put mathy terms together that sound proper! Instead, use normal english words to describe what you really mean, and then ask if there's a concise mathy way to summarize it. This stuff isn't about pushing words around to get other words that have more prefixes and suffixes and greek and latin roots. It should be about finding the right abstractions for the job -- so providing a new abstraction should also be coupled at least with the question -- what does it buy? 
A strict language that looks like Haskell is still very much Haskell. We have many people at work who can attest to this. 
Haskell98 is no longer enough to be a competitive language. A lots has happened in the last 14 years, especially in Haskell.
Certainly tempting, I guess that's what credit cards are for lol.
I had never thought about strictness before haskell. We have wasted several man weeks over the last year hunting space leaks. While a couple of people I know have added lazy like features to other languages (C++,Java) those situations were at quite high scale and extremely specialized. I don't think this argument holds much water.
Yes, but Go specially is very thin in the set of programming abstractions the language offers, as such many people without CS background are able to pick it up.
I have worked on Leksah's code briefly. It's rather horrible, IMO.
&gt; I actually think that vi is NOT a very practical text editor That is a weird thing to say. I hear vi is the editor that can be used more efficiently than any other, up there with Emacs.
Not sure what is in vi, but as an editor similar in power to Emacs: can't you just hit [TAB] on the line to go through all meaningful indentations?
Could be. Just based on here at reddit, most of these kinds of posts are on the same topic: cabal, space leaks, performance. I'll admit that after the Nth discussion within M days on the very same topic, I usually zone out. Is there actually progress being made in these threads, or is it just the same issues and arguments getting rehashed over and over? Of course, *solving* the problem would be excellent. But does endless discussion -- engagement of the critics and trolls -- on reddit further that cause?
...have you given up on `haskell-mode`? :-/
Macros as in unrestricted AST transformations? That way lies madness. It's impossible to reason about code written in DSLs that way because the code you write gets transformed every which way before being evaluated. In my opinion mix-fix syntax is the answer to your problems, not macros. /recovering-schemer
A problem I have with your post is that this encoding of operators into functions can be done with any operators, not only the associative ones. The crux of the matter is the way this encoding behave, ie. the equational rules on `lift` and `unlift`, that are structured by the associativity assumption. Yet you essentially do not detail this part (I'm not even quite sure you *say* this besides noting that some equation doesn't depend on the associativity condition), it's all hidden in your notation `x op y op z` that only makes sense for associative operators. (My intuition would be that essentially anything could be encoded as a category if you insist on seeing it that way, and that without giving more structure than that (in particular if your translation is not full) you aren't actually saying much.)
[Goto still has it's use.](http://forum.codecall.net/topic/53691-why-is-goto-so-bad/)
Well, it seems my post was widely misinterpreted. I wasn't speaking against emacs. All I did is listed some features, which are might be viable optinos for a "killer feature". Little did I know that emacs supports most of these. Of course, with this knowledge, expanding emacs is a fair enough alternative, I have to admit. I should have stated explicitly that these are a list of would-be killer features.
I was wondering if it was because if this distinction, but was not sure. Thanks!
&gt; In general people say "Never use goto" for the simply point that when most people start using it they never stop. We can reformulate it: "Never use OOP" because it is difficult to stop and switch to FP. :)
That existential is near useless, unless the only thing you want to do is operate on `[Thing]` but not actually work with individual `Thing`s. Why? Well, lets look at `[a]` first. This is a homogeneous list, but still - what can you do with it? You can't operate on the elements, because you have nothing that can do anything with an `a` - there's simply not enough information. This is the same with `Thing`. If you know there are operations that you want to do on `Thing`, then you need to encode that earlier: data Thing = Show x =&gt; forall x. Thing x lst :: [Thing] lst = [Thing 1.5, Thing "foo", Thing 3] mapM_ print lst That will now print all `Thing`s, because we know all `Thing`s are instances of `Show`.
I don't have much to say, except: * The mention of Bret Victor is very encouraging. That guy is smart. At the same time, "Good Haskell IDE" and "IDE With Ground-Breaking New User-Friendly UI" are both difficult, unsolved problems. Are we sure we want to solve both of them at the same time? * I really like KDevelop. * I really like KDevelop because it gets out of my way and doesn't force the usual IDE bells and whistles on me if I don't want them. I can just open the file browser and the terminal and use it as a glorified text editor with great semantic highlighting, completion, and tooltips. Similar flexibility in "the Haskell IDE" would be great. 
If it's important to you, then perhaps you should change jobs. Sounds like logic of companies that are now supporting COBOL.
&gt; I think every mainstream language reflects this, especially the ones most reviled. PHP succeeded, for example, because it solved a real problem, despite widespread criticism. Isn't that a tautology? X succeeds because X succeeds -- ain't no stoppin' the **force of destiny** man! So if X doesn't succeed, s/he/it can only be because no damn good.
Vi is not similar in power to Emacs; you're thinking of Vim. Vi is just a very basic editor (though still extremely powerful compared to stuff like nano/pico or notepad).
&gt; Could be. Just based on here at reddit, most of these kinds of posts are on the same topic: cabal, space leaks, performance. I'll admit that after the Nth discussion within M days on the very same topic, I usually zone out. Is there actually progress being made in these threads, or is it just the same issues and arguments getting rehashed over and over? The discussions on reddit about lenses and their lack of polymorphic record updates eventually started me thinking about how to do [simple polymorphic updates with lenses](http://r6.ca/blog/20120623T104901Z.html). That said, I had been thinking about the topic for a year or two already, but it was perhaps helpful to keep seeing the problem reiterated.
After years of programming in Haskell, I find the strictness a pain to deal when when I program in OCaml. * I cannot arbitrarily abstract out functions for fear of introducing non-termination or taking a performance hit. * I end up sprinkling my code with delays and force randomly until I get the performance I expect. * I cannot (dynamically) tie-the-knot without resorting to mutation. Basically I now have the dual problems that ML programmers have with Haskell.
Capital letters are not an option, they ease the reading.
This submission has been randomly featured in /r/serendipity, a bot-driven subreddit discovery engine. More here: http://www.reddit.com/r/Serendipity/comments/13ibfq/putting_haskell_down_xpost_from_rhaskell/
I just started using it this weekend and found it to be a welcome improvement over the dual WinGHCi and Notepad++ system I was using (Windows 7). I agree that it could use some polishing, but I've found it very usable (as a beginner, admittedly).
&gt; Yes. pack and unpack actually required yet another step between String and ByteString: converting between [char] and [wchar]. No, no, only Data.Bytestring.pack require that (for correctness sake), Data.Bytestring.Char8 is the package to use to interact with String since it try to pretend that all Char will be in the 0-255 range, so the type of pack in this module is "String -&gt; ByteString". On the other hand you don't need these conversion to interact with CSV files, you just have to read them with ByteString IO function instead of the Prelude ones (see tibbe post for an example where he don't need to make a single conversion to accomplish the task you wanted to do)
Yeah, I use `vi`, complete with terrible undo. I have tried `vim` before and it is a significant improvement.
I agree. People will adopt Haskell if they see successful projects completed in it. This is why, for example, I wrote my recent protein structural search engine entirely in Haskell (and I'll be making a lot more noise about it after I publish the corresponding paper). Successful projects not only inspire confidence that Haskell is ready for real-world applications, but they also provide concrete source code that users can inspect, learn from, and adopt to their own projects.
&gt; It's impossible to reason about code written in DSLs that way because the code you write gets transformed every which way before being evaluated. Upvoted! Thanks for the insight (as someone with little scheme/lisp experience). Initially reading GP, I wondered aloud: "What's the space of reasoning about programs?"
There's [ACE](http://ace.ajax.org/) and similar projects. The problem I find with in-vanilla-browser editors is that keyboard shortcuts go out the window. Each browser steals different shortcuts, and there are only a handful that none of them use and which could change in the future. (For example a version of Google Chrome [removed](http://stackoverflow.com/questions/7295508/javascript-capture-browser-shortcuts-ctrlt-n-w#answer-7296303) the ability for scripts to capture certain keystrokes that the previous version allowed). Additionally someone may be using a browser you don't expect (oddball browsers like luakit) and would not be able to use it.
Pretending shitposting is "dissent" is absurd. Read his post history, it is full of blatant trolling.
How about you show me one of these wonderful contributions he's made where he got downvoted for it instead of just tossing thinly veiled insults at people. Look, here's an obvious, blatant troll post that was upvoted: http://www.reddit.com/r/haskell/comments/13cz6b/how_to_write_python_in_haskell_state_and_either/c730ndm Here's one of his few posts that actually got downvoted. Wow, what a great contribution! http://www.reddit.com/r/Frugal/comments/12yxfw/frugal_ive_been_broken_into_twice_ideas_for/c6zlzqf Stop pretending he is some oppressed martyr. The vast majority of his posts are upvoted, even those that contribute nothing, or are a negative contribution. The rare downvoted posts are almost exclusively outright shit that obviously shouldn't have been posted at all.
First comment after your first point from drb226, the OP, is "This is a very valid criticism". Hardly counts as trolling. I'm not pretending he's an oppressed martyr. I'm saying that he's not a troll and I personally value his contribution
Good find, thanks. I'm a bit disapointed that it's web-based though.
I also use vi for my haskell editor. Indentation isn't a problem, just hit tab or turn on autoindent. Arbitrary alignment is the issue, lining things up at some character in the previous line, which requires some understating of the language's syntax to do. How I deal with alignment is to not do it. I find the standard haskell style quite horrible, and instead just treat it as though it were any other programming language. Messing with alignment forces a reliance on specific tools that are language-aware, and creates noisy diffs. I don't find the benefits of alignment out-weigh those downsides (in fact, I don't even know what the benefits are supposed to be).
That's a large part of the motivation for my experimental language, [Frank] (https://personal.cis.strath.ac.uk/conor.mcbride/pub/Frank/), as featured in my recent [Fun in the Afternoon](http://sneezy.cs.nott.ac.uk/fun/) talk, whose "slides" are a [Frank source file](https://personal.cis.strath.ac.uk/conor.mcbride/pub/Frank/fun.fk). As with CBPV, Frank separates "value types" from "computation types". Types are checked with an ambient context giving value types to local variables, and an "ability" enabling certain local effects. The corresponding semantic objects live in the free monad of the ability at the given value type. A computation of computation type [effects]V can be invoked if the `effects` are in the current ability, delivering a value of type `V`. But any computation type `C` can be suspended to give the value type `{C}` (here, braces are "suspenders") meaning "the knowledge of how to do a `C` computation" which is pure, and can be manipulated as a value even without the ability to do the effects in `C`. That's to say, Frank, unlike Haskell, clearly distinguishes knowing how to do something from actually doing it. Frank is call-by-value. Like in ML, you don't need any noisy monadic/applicative combinators to write effectful programs. Like Haskell, effects are policed by types which indicate which monad you're in, and you can still write control operators. Crucially, `{[]V}` is the type of "knowing how to do a pure computation to get a `V`", as distinct from `V` itself. As a result, you can be clear about where programs and data structures are lazy, and you can still do the call-by-need optimization, ensuring that a `[]V` computation is memoized the first time it is forced. I'm not advocating Frank as a technology to retrofit onto Haskell. But yer Idris and yer Agda will be needing their own story about programming with effects, so revisiting these issues with a fresh eye might help future FP language generations. I'd certainly encourage any designer of a new functional language not to follow the Haskell presentation of monadic computation just to be reassuring, but rather to exploit the generation gap for the freedom it gives to think it out again.
No I deleted my post because I couldn't put up with your shit. You hunted down almost every post of mine in this thread and I didn't even realize it was just you being an ass until after I deleted them. 
&gt; If you look through comments and discussions about having an IDE for Haskell I think you'll see an overwhelming trend for existing Haskell users to say, "Vim/Emacs/etc works great. We don't need an IDE." I hate Vim/Emacs. I want a goddamn IDE. However I agree that a closed source IDE is a huge mistake, especially when the maintainers stop maintaining it and haskell advances leaps and bounds.
Your existential is also quite useless ;-) since you could just as well store the results of show in the list. This is the [existential typeclass anti-pattern](http://lukepalmer.wordpress.com/2010/01/24/haskell-antipattern-existential-typeclass/), by the way.
You are upset that I replied to you each time you replied to me? Stop with the self-righteous indignation and consider how unstable you sound acting like you are being stalked or something because someone tried to have a polite, rational discussion with you. Despite your posts being very trollish, I gave you every benefit of the doubt and proceeded as though you were being genuine.
May I kindly suggest that you flesh out your situation in a blog post to afford the community a closer look, to better assist? This is clear enough: &gt; read all the data necessary in the IO monad, use the results to construct pure calculations that consume that data but you've lost us at "calculations themselves be similar to monad transformers". How are pure functions similar to monad transformers? 
I assisted last year to PLMW and has been one of the most rewarding experiences in my life :)
[tmux has a mode where multiple people can share the terminal](http://www.howtoforge.com/sharing-terminal-sessions-with-tmux-and-screen) when you do that multiple people can edit the same file. It's not the only option here. I guess ghc-mod might be able to do the automatic type annotation thing. At least, you can query subterms for their types. Anyway, I see your point that they are not bundled together at the moment in an easy package.
"You are elitist and exclusionary because pumpkins are orange." "That doesn't make any sense." "OMG I am just disagreeing! Stop oppressing me!"
So *that* is how an intelligent person responds in a debate! You're a walking talking straw man.
&gt; I guess ghc-mod might be able to do the automatic type annotation thing. And I guess that's how I get the idea from... now, that you mention that... :D
Show up to Edward's lens talk. It is really good.
Seconded, it was a great talk at SF and I'm sure will be just as good at NY.
Useful to know! Thanks!
&gt; My intuition is that it should behave as if the entire data type were UNPACKED. In other words, evaluating it to WHNF fully evaluates it. Given the value of lazy infinite lists, wouldn't people also want (arguably even more) strictly-valued elements but lazily-structured possibly infinite container datatypes? "This data type should be strict" you say, but ok, how strict? What's the right default?
From a superficial skimming, your first example pair is screaming for an auto-derived Traversable instance :) I agree that such traversals often need to be monadic and if written purely may need to be rewritten. I think often the Traversable instance solves that. But most pure functions can remain pure when adding monadic effects. I might be missing something but I don't see why you changed printJS to be a Writer?
&gt; no pragmas. Okay, saying that shows a problem of understanding of why you have to enable extensions in Haskell (I guess it's what you mean by _pragmas_): Extensions are not default because in some places they change the behaviour of your code, i.e. it will compile both with and without the extensions **but** it won't show the same behaviour at runtime. Haskell privileges correctness over *everything*, so for a tiny feature that makes your life easier 99% of the time, if it also makes nasty bugs pop up 1% of the time then it just **can't** be the default (GeneralizedNewtypeDeriving is a good example of that). You just have to be *explicit* about it. That's all Haskell spirit: nothing is forbidden, but if you need a possibly (and by "possibly" we mean "not _never_") dangerous behaviour -- such as an extension or the use of code with potential side-effects within pure code --, Haskell makes sure you _can't_ get away without knowing what you're doing. One just can't at the same time relish Haskell's ability to dramatically reduce bugs _and_ want the pragmas to disappear: it's the small price to pay.
Oh really, like as thin as Java? *EDIT:* Ok, that was outright dumb troll, sorry.
I think I figured it out. I was testing with lines of text. If I paste text without newlines I see the behavior you describe. 
&gt; This is reddit. It was designed to encourage this behaviour. No, no, no, sir. This argument is void here, couting the number of times people posting stuff like memes or jokes were called to order by being told that /r/haskell is not regular reddit. So it cannot be reddit when it makes someone's point, and not reddit when it makes another point. Too easy.
@dagit: In discussing the need for an IDEs among Haskell developers we have to take into account selection bias. Current Haskellers are pioneers. They are not afraid to take things into their own hands, fix tools and libraries, write plugins for their favorite editors, and (unfortunately) ignore Windows. Our own developers are like this too. But the people we want to attract to Haskell don't have the time to deal with all this overhead, and a lot of them work on Windows or in mixed environments using Visual Studio, Eclipse, or some other environments. We did our market research and the lack of an IDE turned out to be a big obstacle in wider acceptance of Haskell. You are right about the need to convince people that Haskell is a big win. My blog post was about the IDE, but it doesn't mean that this is the only thing we are working on. We do have a comprehensive plan and we are working on several fronts. You mentioned Michael Snoyman's blog posts: Guess what, Michael is part of the FP Complete team. 
&gt; From a superficial skimming, your first example pair is screaming for an auto-derived Traversable instance :) &gt; &gt; I agree that such traversals often need to be monadic and if written purely may need to be rewritten. I think often the Traversable instance solves that. Hm, I didn't know that Traversable was derivable. I wish it said that in [the documentation of Traversable](http://www.haskell.org/ghc/docs/latest/html/libraries/base/Data-Traversable.html), I would have known that years ago. I see that also [Foldable is derivable.](http://www.haskell.org/ghc/docs/7.4.2/html/users_guide/deriving.html#deriving-typeable) I'm both happy and annoyed at the same time. Traversable's `traverse` works within a functor so I could still use it here with the State monad. &gt; I might be missing something but I don't see why you changed printJS to be a Writer? `printJS` is a `State`, and I changed it so that I could track indentation level and output source mappings. That's not clear from that version, because that patch was the transitory refactoring commit, the ones following implemented the rest.
Its easier to remember doThisFunction so long as the name is descriptive than ~~= or whatever the special operator is.
I've been using Kate, but my biggest frustration is that the file list in the sidebar has no hierarchical structure. I tried switching to KDevelop for this reason, but I couldn't figure out how to get rid of all those bells and whistles. Any tips on how you did this?
I actually needed the `Writer :. Const ()` Applicative semantics (there is no Monad Transformer for `Const ()`) in order to get the writer tells of a traversal without having to generate the actual results in one particular case. So it was actually useful that a Traversable is generalized to Applicatives (and doesn't work with just Monads). 
&gt; Seriously? Even our most entry level PHP guys have strictness problems (although they don't know to call it that). I've been asked "is there some way I can make this not run the rest of the code sometimes, like with &amp;&amp; and || in my if statements?" many times. I've seen that kind of question for every language except for (Common) Lisp, and Haskell, of course, because those languages teach you to control execution flow instead of undergoing it. But doesn't PHP have closures?
That's the point: names are seldom descriptive enough (or else you end up with endless names). Is 'map' a precise name for what the function does? Or would 'applyFunctionToEachElement' be better? So to me, custom operators don't form a new problem by themselves and then are pointless to be discussed on their own: they just fall under the neverending debate of concise names vs. descriptive names.
I don't think your assumption that `Show a == String` is correct - I have a list of things that can be passed to anything that expects a `Show a`. `print (show foo)` is different from `print foo`.
In my experience, there's rarely a monstrous syntactic change for such things. If every part of the syntactic change corresponds to new functionality in that part -- rather than just lifting to satisfy the type checker, then that seems like the straightforward addition of that functionality to me. The parts that "monstrously change" are those that traverse, iterate or otherwise recurse on some structure -- as suddenly they may need to compose actions from recursive calls. So far, I think these instances were solved by Traversable for me when I encountered them. Maybe I mis-remember though.
I would enjoy seeing this as well. I get the impression however that creating a macro system that people by default use responsibly is non-trivial interesting problem. I do not think I have the required insight however to advise GHC, or other haskell compiler, developers to redirect there efforts to this area.
As a Haskell user, I strongly disagree that emacs/vim is "good enough". Using Haskell in emacs/vim is immensely painful for me in various ways that an IDE can fix: * Handling imports properly. Stopping my train of thought to go and add an import above is annoying. Throwing unqualified open imports to avoid doing so is even worse (and it's what most Haskellers resort to). * Seeing subexpression types easily: When debugging a type error, I could really use something that told me the types that *are* inferred, so that I don't have to do so much type inference/checking in my head. * Refactoring/renaming/etc: These are still far more painful than they need to be, requiring far more keystrokes than needed. And many many more.
How far along is your IDE effort?
All that stuff takes up extra space, which I don't like since I work on a laptop with a small screen. I guess I thought you had got the interface looking more or less like Kate's.
There are a few of us in NY too :-)
This works fine. The idea is that lazy types can have strict members, but not the other way around. The strict member is fully evaluated as a single unit whenever any portion of it is evaluates.
Your effect types look like something from Wouter Swierstra's data types a la carte: http://www.cs.ru.nl/~W.Swierstra/Publications/DataTypesALaCarte.pdf Something like: a [X, Y, Z] b Kind of looks like: (X :&gt;: f, Y :&gt;: f, Z :&gt;: f) =&gt; a -&gt; Free f b I haven't looked in detail at Frank yet, but I assume that your effect signatures are formulated similarly, in terms of a free monad? 
&gt; But yer Idris and yer Agda will be needing their own story about programming with effects Could you explain why? Why wouldn't Haskell's monadic approach work?
Ah. IIRC there's just a thin panel-bar on the left and one on the bottom, and a tab bar at the top. But Kate has these too (at least in my configuration). There might also be a panel on the right that Kate doesn't have. I'm not in front of my Linux box right now so I can't check. I think you can remove the individual panel-bar-items through the context menu, but there's probably not much point unless you can remove all of them along a given screen edge.
I would rather have an easy way to write performance oriented applications in Haskell than an IDE, which enables newbies to write slow code. If I write 2 programs, one in C and one in Haskell and used a naïve approach for both. Unless, I am a Haskell wizard, the performance of the Haskell program is going to be 10s of times worse than the one in C. It shouldn't require a master's degree in Haskell to write a performant program. If you guys really want to write an IDE, more power to you. But why not add Haskell capabilities to existing IDEs like kdevelop or leksah, instead of starting from scratch ?
Idris and Agda will need their own story about programing with effects for the usual reason: in order to manage interaction between programs and the world. I am not at all saying that Haskell's monadic approach wouldn't work. Rather, I am saying that there is at least an opportunity to redesign the way effects are managed, and it would be sad if we just followed Haskell without really thinking about it. For certain, we shall need to consider whether there's any interaction between monads and dependent types. On the one hand, it is surely inadequate to restrict our attention to endofunctors on `Set`. Already in Haskell, we can have fun managing invariants and protocols with various forms of indexing. We need to figure out ways to scale that up. On the other hand, monadic programs in Haskell are systematically crufty anyway, so maybe the application of some hindsight might lead to a bit of standing-on-the-shoulders-of-giants progress: at this stage in the rise of new languages, it wouldn't be too disruptive an innovation.
Login required to read this? Ugh.
Today I learned the word “algorave” and I like it. Not sure that *I* could dance to such music (don't get me wrong, I listen to [stuff like this](http://www.youtube.com/watch?v=qUOpfTWTGPY)), but this is a little too disjointed, changes rhythm/pattern too much, for me. Not sure if that's due to the livecoding implementation making it hard to do smooth progressions or the performer. But cool to see people dancing, or algoraving, to Haskell code. :-) Is there a link to source code?
Wow, that sounds fantastic! I wish I was at that rave :)
No requests!
&gt;I've seen that kind of question for every language That's my point, I wasn't suggesting it was unique to PHP. &gt;But doesn't PHP have closures? Yes.
Maybe they could build a stand-alone application using something like WebKit, which didn't restrict key combinations like browsers do.
I just want to chime in and say that I bookmarked this when it came up in /r/bestof, and have used it like a bible ever since. I would never go as far as calling something PHP-esque, though. That's just.. mean. Just plain mean.
Is there a monadic solution for turning 3D off? Something feels dirty about pressing the button and mutating the player state. The page should reload.
An amusing technical achievement! the music....errr....:) Oh well, they seem to be having fun with it
Yeah, Detrimentalist was good, but Rossz csillag alatt született was just a perfect piece of work. The combining of [classical violin pieces with lounge jazz and breakcore](http://www.youtube.com/watch?v=FbJ63spk48s) executed so well. I'd recommend it to anyone. Guess we're off topic now. Talking about the *outside* of Haskell. We'll be sent to cleaning if we continue!
Yeah, that is also my favorite.
Too bad it sucks so much. Can you please shut up about your piece of crap? Nobody is using your plugin; I have never seen anyone besides you demonstrate for more than 30 minutes on a non-trivial random project (like GHC) how it works. As such, your plugin has forever been nothing but an elaborate hoax. To FPComplete: there is currently no competition in the IDE features space for Haskell, other than existing editors, which you are, no doubt, familiar with. However, a closed source IDE is a product that is even a hard sell for mass markets. As such, there is about a very slim chance that your company will survive if it depends on that. I don't know what you promised to your investors, but the returns would have to be astronomical for that to work out even if there is a software success. I just ran the numbers and there is no way that this is going to work out. Summary: - 85% chance of complete project failure (less than 50 users will enjoy using this software over more than one year of time) - 99.9% chance of economics not working out assuming Haskell doesn't grow dramatically (1000+ % growth) within 5 years. 
Regarding 2: You can always add more type declarations. Generally you should always include them at the top level, and if you're having trouble with an error, you can add inline type declarations (e.g. `f (x :: Type) y` ) to check your understanding of the types at each stage is correct. Once you've fixed the problem, you can remove them again. Regarding 3: If you're having this problem, it definitely sounds like you should include more top level type declarations. Whenever I use do blocks, I know what the type is because it's almost always declared just above the block.
So it's definitely an educational problem, then.
What is? I'm not talking about a problem at all, so I don't know where we're going with trying to classify what kind of problem it may be.
Sorry, I assumed it would be clear given the context of the previous posts. This isn't a real problem, it only seems like a problem for beginners who haven't learned how to deal with it yet. I was pointing out that "you have to think about laziness" isn't a problem, because you also have to think about strictness. Just as 5% of the time laziness is not the ideal behaviour, 5% of the time strictness is not the ideal behaviour. In both cases it is simple to force laziness/strictness as needed, and in 90% of the cases you don't even think about which is happening.
Well, Frank is an experiment in that direction, and it's beginning to look like it might be a good fit with dependent types. The Haskell do-notation approach of splitting a type application `m v` into a monad and a value becomes problematic (ie potentially ambiguous) when you have lambda and computation in types, so some more explicit way to administer that split will be important. Given that need, we may as well try to resolve the nonuniformity of "pure" and "monadic" styles and do as much implicit lifting between monads as we systematically can.
I have gotten it to work, in so far as that it does. It's just wrong on so many levels. I am fine with people writing software for themselves, but the moment you put up a website saying that it has certain features, you better have something that matches what you promises, because otherwise you are just a lying bastard, and at that point, you don't deserve kind words. As such, it would be better, if he would just withdraw his project from the Internet or heavily revise what he promises on his website. He isn't alone in this and as such everyone who distributes software (including commercial parties!) should consider whether they are making a contribution to this world or a net destruction. 
&gt; Haskell removes so many parentheses, it takes a while to learn to parse. It takes awhile getting used to operator precedence and the various "fixities". You probably want to hover over an operator used infix style and see not just the declaration but also the infix level and whether it's neutral, left, or right. 
Thanks for the reply. I'm already looking into Frank for a project mothballed partly because the effect abstractions swelled over my head. And it wasn't just the syntax although the verbosity did add to the fatigue. Mind if I seek Frank clarification with you should the need arises? "Computation in types" reminds me of a 7+ years idea to see if IO could be factored out into a plain library rather than baked deep inside the Haskell runtime.
I'm not sure what you are addressing here. `lift` and `unlift` can certainly be applied to non-associative operations, and in that case it corresponds to the right-associative ordering of operations. &gt; My intuition would be that essentially anything could be encoded as a category if you insist on seeing it that way Anything can be encoded as a category in the same was as anything can be encoded as a set. Not every encoding has such a close relationship between composition in the category and application of the operators. I find it surprising that this relationship *is* so close. I'm surprised no one else does. 
Thank you for your support, but I *am* trained as a mathematician so I am happy to receive technical challenges.
You're right about the terminology "category of function composition". It's clumsy. I chose it because I do not know how to express the right concept easily. The right concept is "the category in which the category in which the associative operations exist is enriched". If I was writing for mathematicians I would say Set, but I'm about writing about Haskell so the category is Hask. But it's no good to say that because the construction is actually general, not specific to Hask. What's important though is that the morphims are functions and the composition is function composition. I don't know the right way to express this. Any ideas? 
&gt; "avoiding success at all costs" Note that it's really "avoiding (success at all costs)", not "(avoiding success) at all costs".
That you read of your own free will.
Why a `strict` keyword rather than more simply a `Strict` extension? [BTW](https://github.com/thoughtpolice/strict-ghc-plugin).
Depends, are language extensions allowed to both extend *and* remove a lot of stuff that you can normally write?
Will Edward's lens talk also be recorded? I'm really excited to hear that one but I can't make it in person.
I guess I just don't get what you're getting at? enriched over what? Typically if you want morphisms to be functions then you just have Set. But the question, of course, is functions over *what*? Otherwise you're just talking about all categories where we commonly call the things that are arrow in the category "functions", as a matter of language.
&gt; ghci&gt; let x = 1 in _1 +~ x $ _3 +~ x $ _5 +~ x $ (1,1,1,1,1) &gt; &gt; (2,1,2,1,2) &gt; &gt; Consider what that would look like without lenses: &gt; &gt; ghci&gt; let (a,b,c,d,e) = (1,1,1,1,1) in (a+1, b, c+1, d, e+1) &gt; &gt; (2,1,2,1,2) Perhaps I'm getting old and grumpy but this doesn't exactly sell me on lenses.
I wish Simon all the best. His help over the years has been invaluable. 
&gt; Today I'm announcing that I'm leaving Microsoft Research. [Hm, interesting. I'm cool with that.](http://dl.dropbox.com/u/62227452/Internets/FP-NOOO.gif) &gt; My plan is to take a break to finish the book on Parallel and Concurrent Haskell for O'Reilly, before taking up a position at Facebook in the UK in March 2013. This is undoubtedly a big change, both for me and for the Haskell community. I'll be stepping back from full-time GHC development and research and heading into industry, hopefully to use Haskell. It's an incredibly exciting opportunity for me, and one that I hope will ultimately be a good thing for Haskell too. That's actually *really* cool! So excited for Simon about that. If there's anyone you're going to hire that would reassure you about using Haskell at your company, it's one of the Simons. It would be sweet if he blogs about challenges at FB and solutions in Haskell and such. Best of luck. :-) &gt; In due course I hope that GHC can attract more of you talented hackers to climb the learning curve and start working on the internals, in particular the runtime and code generators, and I'll do my best to help that happen. That's a good point, I've never tinkered in GHC but I should at some point, I don't want it to remain a black box to me. I wonder what percentage of Haskellers have hacked on GHC? 
Thank you for all the good work on GHC.
But that makes things confusing for people reading your code. I'm not opposed to operators, per se. I like the fact that we have `+`, `&gt;&gt;=`, `.`, etc. But operators are usually less descriptive than functions. I'm happy that we have `&lt;&gt;` now, but `mappend` is more descriptive than `&lt;&gt;`. So we should be careful adding operators, because they make reading code and learning libraries harder. Another thing is searching. Function names are easier to search for (both in code and in Google) than operators. This is especially true for single symbol operators. And since we have backtick notation for functions, the advantage of operators is diminished a bit.
I still have to know what all those operators do to understand someone else's code. For a library like lenses that is likely to be very widely used, I can't really just stick my head in the sand and pretend I don't need to know.
I am, too, giving thanks!
Tell me what's on the website that's not part of the product. 
How about? "Get full autocompletion when typing your Haskell code. All the information from your installed libraries can be shown at will, helping you to find the correct function or type. Just type Ctrl+Space at any moment to show the completion popup (the key combination can be changed in the Eclipse preferences: General -&gt; Keys -&gt; Content Assist)." Like I said, show how an empty virtual machine running some standard Linux can be converted into a version of EclipseFP running on the GHC code base displaying all of the features you display on the website in some Youtube video. Since, frankly, I don't buy your crap anymore. I give most people a fair chance in this world, and so I have given you not one, but two chances. Now, unless you create such a video, I will never look at your projects ever again. You don't have to care about that, but I would think twice before ignoring it, since you can bet lots of other people have a similar idea about your work.
I run on Windows, not Linux, so I don't see why I should invest time installing Linux to create a video for some asshole that thinks that insulting people that provide open source software for free is somehow doing a service for the community. You think being insulting is going to trick me into doing things for you? Think again, punk. If something doesn't work when it should, fill a bug report. If EclipseFP doesn't light your fire, go and use something else. Autocompletion works, the screen grabs on the website are real, and incidentally not done by me, but by somebody else that actually contributed to the project, which is a lot more that can be said of you, who judging from previous posts didn't even bother submitting a bug report when you encountered issues (so I don't understand what you call "giving a chance". You mean installing free open source software, running into issues and bitching about it a year later on a forum?). So the lying bastard, is, in fact, you. You contribute nothing and feel that gives you the right to insult me? On a more technical point, I'll freely admit I haven't tested the GHC code base with EclipseFP, because I don't code on GHC, and my free time is too short to try EclipseFP with every Haskell project under the sun, but I've never got a bug report from anybody saying that GHC code disagrees with EclipseFP. But all the packages I put on hackage I've developed with EclipseFP, including buildwrapper that underlines EclipseFP, so I eat my own dogfood. And since I don't get any money for EclipseFP, what's wrong with scratching my own itch? I hope it can be helpful to people, and if not, well, there's other options around. So quit your bitching. Shut up and use something else, or join the project and help make it better, I happily take pull requests, bug reports, and be happy to explain the internals to any developer that want to contribute. 
I am hoping to see some more articles in this series. I have looked into the lens library recently myself and have been struggling with the indexed variant in particular, or rather how to handle the Maybe result of the at lens properly when composing with other lenses, in particular when the resulting value is not an instance of Monoid. My recent Stackoverflow question helped me understand that I can get the value with headOf but it feels wrong to have to modify the function I use on the lens as it breaks the abstraction of having a read-write accessor value to be passed in as a parameter.
I think there is room in the world for a competing lens/traversal implementation ... assuming you think competition is good and not divisive.
&gt; I am hoping to see some more articles in this series. I have looked into the lens library recently myself and have been struggling with the indexed variant in particular, or rather how to handle the Maybe result of the at lens properly when composing with other lenses, in particular when the resulting value is not an instance of Monoid. You can compose it with `traverse` to see through the `Maybe` though there are probably some combinators that will do this for you. The result won't be a lens, but rather a traversal.
talk to these guys http://tech.groups.yahoo.com/group/SeaFunc/
So you guys actually write Haskell there?
thanks!
I think competition (and collaboration) is a good way to develop packages. Though at some point a 'winner' will probably emerge. I also think we have at least 2 or 3 good lens packages now. 
You don't need it to be an instance of monoid to set or modify the value; you only need a monoid to get the value, in which case you can use the `First a` or `[a]` monoids indirectly via the `^?` or `^..` operators in `Control.Lens.Fold` (IIUC). Not that any of this is obvious, but tutorials and experience will come.
Best of luck to him! Hopefully I can end up as one of those talented young hackers!
I submitted a report in an earlier comment to you on Reddit. You did not fix that when you said that you in fact had. If you only develop on Windows, put it on the fucking page that you only tested it under Windows. Fucking moron. I don't mind you scratching your own itch. I do mind that you share it with other people without saying it's only for Windows, because on any other platform it is likely broken. Your comment regarding that you didn't test on GHC only shows what an unbelievable idiot you are. If you release an IDE, don't you want that people can actually use it on real projects? Why don't you test it on one of the largest projects to establish a base line? As such, you have wasted the time of lots of people. It's just misrepresentation and nothing else. You are the asshole here. You call your socalled product EclipseFP, which suggests that it runs on Eclipse (Eclipse works fine under all of the major platforms the JVM supports), but no, it doesn't. Just rename your fucking project EclipseRunsOnlyOnWindowsFPItSucksPleaseDontUseMyItch or something like that. That would be a good name as it describes what it is. Better yet, just withdraw your pathetic project from the Internet, as it is just an elaborate form of spam making it harder for search engines to find things which are worthy of the attention of people. Let me conclude by again pointing out that you are an idiot who should not be developing software. I hope you got the message this time. 
I can use those operators but only if the code using the lens actually knows that this is required for the lens in question, it kind of breaks the abstraction of the lens type if I have to use different operators depending on the kind of lens inside.
What's Haskell used for at Facebook?
Yes! The [video](http://www.youtube.com/watch?v=3q8xYFDYLeI) is finally on YouTube. If you want to look at the the slides there is a [PDF with animations](http://files.londonhaskell.org/2012/10/24/WhyDoMonadsMatter-WithAnimations.pdf) for on-screen. A [PDF with most of the animations removed](http://files.londonhaskell.org/2012/10/24/WhyDoMonadsMatter.pdf) for printing. And finally the [PowerPoint Presentation](http://files.londonhaskell.org/2012/10/24/WhyDoMonadsMatter.pptx) with all the animations! Enjoy.
They even have open-source projects written in Haskell: https://github.com/facebook/lex-pass
I am the only person who is desperate to know the prospective job at Facebook that Simon has landed? It must promise to be a truly outstanding and pioneering opportunity, to persuade him away from the title of Mr GHC?
Yes. I expect it's something we will talk about when the time is right, but it's not a project of mine, so I can't let the cat out of the bag. We also use Ocaml quite heavily.
Wow thanks! Coming from you I will take that as a huge complement! BTW I think (from reading your comment history) that perhaps you need another novelty account for some of them, "rude_and_unhelpful" might be a good name for it.
I wonder why they implemented pfff and lex-pass in OCaml and Haskell respectively? Would it not have been easier to write a php parser once?
How does game development in Haskell work? Any good articles or starting points?
I certainly agree with the anti-foundationalist bent. Foundationalism doesn't seem to have worked out very well for mathematics, and I can't see it working much better for CS. Though, to be fair, this sort of universalizing isn't anything new in CS. [Edward Yang](http://blog.ezyang.com/2012/11/extremist-programming/) just did a post recently which is relevant. In it he points out a number of "everything is X (in disguise)" memes in CS, many of which are still alive and well. The big difference being that many of these are homegrown, as opposed to things like monads and the like which have been repurposed from mathematics. Ironically, because they are born of analogy we engage in the same practice I criticized mathematicians for, namely using warm fuzzy names like "files" and "objects" which then come to mean something quite different from the standard English word (not to mention the ensuing academic arguments over which technical definition is correct/perspicuous). Though I'm curious what you mean by "relationalist attitudes" and whether you mean the same thing by foundationalism as I do...
There was a short discussion on an earlier post about this game that may be helpful: http://www.reddit.com/r/haskell/comments/106ocf/first_commercial_haskellpowered_game_logic/c6auhoi
Perhaps you should learn to spell first before coming to a forum. 
One thing I think would help here is if the infix form (whatever it's named) took arguments the other way around. That is, the following would read quite nicely: (+1) `over` both And would facilitate sectioning to let programmers make their own compound operators when/if the need arises. Whether this argument order is objectively better than the current order, however, depends a lot on how one anticipates using the lenses. That is, do you make lots of ad hoc lens compounds, or lots of ad hoc modification functions? I'd imagine that in practice the answer to that question is: both. In which case, it'd be good to minimize the amount of cruft needed to switch between the two. Making a suite of compound operators doesn't seem like the correct solution for reducing cruft. As you say, the reason C has them is because it is a helpful way to reduce cruft in imperative languages. I'm not so sure that applies to lens-based code in Haskell.
Want to be able to hack on GHC 'someday' when you are smarter? Consider reading these: http://research.microsoft.com/en-us/um/people/simonpj/papers/pj-lester-book/ http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.53.3729 They are both very well written and will help give you a much better picture of functional languages can be compiled. Obviously, the GHC compile has changed a lot since those were written, but not so much as to make that knowledge useless. Even if you never hack on a single line of GHC code, it will still give you a much better idea of what the heck is going on behind the scenes. That can help you reason better of strictness, space leaks, etc. 
Nice!
Given a `l :: Lens a b`, there is no way to construct a valid `Lens (Maybe a) (Maybe b)`. For example if we take `l = _2 :: Lens (Bool, Int) Int)` and we somehow constructed `ml :: Lens (Maybe (Bool, Int)) (Maybe Int)`, what would you expect ml .~ Just 5 $ Nothing to be? `Just (undefined, 5)`? 
This video is very good, except for the audio and the ending. It seems the talk falls apart at the end and the speaker loses direction. I would definitely recommend it to programmers new to monads.
Why do you think it falls apart?
This lens you want cannot exist, but the task you want to perform with this lens can almost assuredly be performed in other ways.
I feel like laziness vs. strictness shouldn't be so black and white. When do we use it? When we use recursion, right? So why can't the function using recursion specify when it wants to use it? For everything else, let the compiler optimize to choose which should be used. Maybe I'm too much of a purist, but I'd like to throw away "undefined", and abstract a bunch of things in monads, things like "error" and primitive recursion. And this would be how we get the choice whether to be lazy or strict: LazyFix and StrictFix monads, with their accompanying "lazyFix" and "strictFix" functions. Of course, we would get the Haskell syntactic-sugar guys on the case. I don't even know how this would effect efficiency, but I feel like it would give the compiler more chances to optimize. It has complete range over functions written with no monads (which would be easier to do if we have inductive recursion). This is obviously way too far from current Haskell to be realistic.
I thought algebraic data types were called "algebraic" because we think more in terms of initial (or final) algebras generated by certain algebraic laws that describe the operations on the data type. See e.g. Martin Wirsing: Algebraic Specification. Handbook of Theoretical Computer Science, Volume B: Formal Models and Semantics (B) 1990: 675-788. I don't think algebraic data types are called "algebraic" just because types form algebraic structures (e.g cartesian closed categories) up to type isomorphism (this is a fairly straightforward observation once you observe that x is isomorphic to y if |x| == |y|, which means all the usual properties carry over) 
Also, if you ever wanted to add annotations to your expression types, then it could be useful to make them `* -&gt; *`. That's what we do to represent expressions with various kinds of annotations in bottle, which allows us to heavily use Traversable. 
Well, I did realize since writing that that I can not set with a lens like that from existing value Nothing to Maybe a since it wouldn't know what else to fill in for all the other fields in the context. To be honest I am reaching the conclusion that lenses are not really worth the trouble for my use case (the one where I want to set values in a greater program state structure but need values to be referenced from various places in that structure and thus need them to be accessible by some sort of ID).
I voted, and will buy it when it becomes available on Steam. When I have some minutes to kill and no good reading at hand, Nikki is wondeful.
"Making sense" is very vague. What do you want to do with your comonoid? If you have some use in mind it can be evaluated whether or not this definition works for that. I can tell you that this is not what a mathematician calls a comonoid, that would be just the categorical dual of a monoid. It's really only useful for monoidal structures other than the Cartesian product (pair formation). In the case of the Cartesian product, a comonoid has a counit : a -&gt; (), which is forced to be constant, and a counit : m -&gt; (m,m) which must satisfy coassociativity and counitality and counitality already forces it to be the diagonal map \x-&gt;(x,x). If you have a monoidal structure M (which in Haskell speak would be a type constructor M a b together with a unit type I, and isomorphisms M I a -&gt; a, M a I -&gt; a and M (M a b) -&gt; M a (M b c), satisfying some conditions --chiefly the so-called pentagon identity), then a comonoid for M has a counit : a -&gt; I, and a cojoin : a -&gt; M a a.
Unitality would be \a b -&gt; join (unit b) a == a \a b -&gt; join a (unit b) == a I guess this makes counitality \a b c | cojoin a == (b, c), counit b == () -&gt; c == a \a b c | cojoin a == (b, c), counit c == () -&gt; b == a As a result of `\a -&gt; counit a == ()`, this degenerates to: \a b -&gt; cojoin a == (b, a) \a b -&gt; cojoin a == (a, b) So: \a -&gt; cojoin a == (a, a)
Interesting. Then the dual of that `Comonad` would be class Monoid m where join :: [m] -&gt; m unit :: Monoid m =&gt; m unit = join [] -edit- It appears you get a listly typed anamorphism with this definition of a comonoid :( class NotCoMonoid m where chop :: m -&gt; [m] class (Functor f) =&gt; CoFoldable f where cofold :: (NotCoMonoid m) =&gt; (m -&gt; a) -&gt; (m -&gt; f a) instance CoFoldable Tree where cofold f z = case chop z of [] -&gt; Leaf [x, p, q] -&gt; Node (f x) (cofold f p) (cofold f q) Similarly, with `chop :: Maybe (m, m)`, you might end up with a `Nothing` before you have three sub-values. With only `cojoin :: m -&gt; (m, m)` which violates counity, you can't distinguish between nodes and leaves.
I have this conjecture that 1. Allowing installs of multiple instances of the same version of a package that have different dependencies (a la Nix). 2. Distinguishing internal and external dependencies (something Nix doesn't have). would entirely solve the dependency hell. However I haven't really thought it through enough to be sure.
At 45 sec. into the trailer, this message appears in the game: "Thank you for coming here, Nikki! We need you help!" Is that English mistake intentional? If not, is it too late to fix it?
Take away: if you want to read structured, interesting and potentially advanced information about Haskell, reddit is a better choice than arstechnica. Out of the three answers highlighted in the article, one is about referential transparency (related, but still mostly off-topic), one is inane (lazyness = database), and one is of good quality (John Hughes, modularity, yada yada). If the question you're interested in is "from a language design point of view, why would we want to choose lazyness by default?", and you're interested in hearing about the upsides more than about the downsides, have a look at augustss' blog post: [More points for lazy evaluation](http://augustss.blogspot.co.il/2011/05/more-points-for-lazy-evaluation-in.html). As a bonus, the first few comments are interesting. Now maybe the people are interested in a different question (eg. "I'm a beginner, please make me interested in lazyness"), and maybe the arstechnica discussion is a good fit for that. I don't know, but that would be fine.
To be fair, the last answer had the most upvotes, although I don't know why it was listed last.
I thought about it, but took the bet that people will recognize the website's name and infer my intent. Won't make the error next time.
&gt;Now, side effects are a different matter. If side effects could occur in any order, the behavior of the program would indeed be unpredictable. But this is not actually the case. Lazy languages like Haskell make it a point to be referentially transparent, i.e. making sure that the order in which expressions are evaluated will never affect their result. In Haskell this is achieved by forcing all operations with user-visible side effects to occur inside the IO monad. This makes sure that all side-effects occur exactly in the order you'd expect. Well, yes, on some level of abstraction this is true, but isn't this ignoring the fact that forcing thunks causes memory usage to grow/shrink? On a more concrete level, isn't that a kind of side effect too? And the timing of thunk forcing is not entirely predictable, as far as I know. (Disclaimer: Haskell newbie)
Depends on your definition of side effect, but for most purposes, no.
no. Comonoid should simply reverse all the arrows giving you counit :: m -&gt; () and cojoin :: m -&gt; (m,m). The laws it should satisfy are exactly the duals of the ordinary monoid laws. The thing is, that in a CCC, all objects are trivially comonoidal. Think of this another way, comonoids are useful in languages with linear types as they are the types of things that you can use in a "structural" way. Since all types in Haskell are structural, this is not really useful. On the other hand, in other categories than Hask you have a use for comonoids. Consider the Kleisli category over IO counit :: m -&gt; IO m, cojoin :: m -&gt; IO (m,m). This could suddenly be useful since with side effects you can now add some structure to copying and deleting. For example, you could use these to wrap up a file handler such that you got proper termination. In the category of Haskell functors and natural transformations, comonoids are comonads and are super useful. 
&gt;Is there a statistics forum or subreddit that this has been posted to? I'd like to read responses from the perspective of a statistician. Not yet, but I'll probably cross-post it in a minute. &gt;How come datapoint, in the definition of a Gaussian, is lowercase? I wasn't aware of that being possible for haskell types This is so that we aren't stuck using Doubles if we don't want to. Floats take up much less space, which will matter when we start talking about Naive Bayes and other applications that require many distributions. Also, LogFloats give us more accuracy when the probabilities are very small, but are much more expensive computationally. Similarly, Rationals would make all our calculations exact, but take more time. &gt;When you talk about storing m2 as the variance times (n-1) to avoid extra divisions you then say that haskell must be getting pretty fast if the number of floating point divisions is slowing performance by 4x. Is this the sort of thing that SIMD support would improve? Probably, but I couldn't get llvm to work on my computer &gt;You compare performance with the statistics library but I feel obligated to ask if there is a C implementation you can compare things to? I thought about making one really quickly, but I didn't know how to perform the tests in a meaningful way because: (1) Using criterion would become much more difficult. (2) I think most of the time is spent creating/destroying the vector/lists, so I don't know how to make the comparison meaningful on the actual computations themselves. &gt;I'm guessing this is so that you can use STM without parallel being in the IO monad. Is this really safe, if say I then used the resulting function again inside of a different STM monad? It uses the `par` combinators and strategies. unsafePerformIO is just to check how many processors are available, then split the data set into that many slices. I think this is safe, but I could be mistaken. &gt;The only part of the blog post that made me grimace was the introduction of the dummy points. Totally agree about how ugly that is. I spent a lot of time messing with the algebra to figure out how to remove the division by zero, but I don't think it can be done. It would be cool to at least prove that there's no better solution, but I haven't thought about how to do that.
I forgot to mention that no matter how many slices you split it into, you're guaranteed to get the same result. Only the computation time is affected.
&gt; Out of the three answers highlighted in the article, one is about referential transparency (related, but still mostly off-topic), one is inane (lazyness = database), and one is of good quality (John Hughes, modularity, yada yada). I had a better opinion of sepp2k's referential transparency answer than you seem to. In fact, I'd say it's the only one of the three singled out that answers the OP's question directly. I think the third answer is good, but it clearly is addressed at the other answers more than at the original question. It is still very relevant to the OP's question, but doesn't address the bulk of the question, which lies in the misconceptions.
I wrote one some time ago: http://ro-che.info/articles/2012-04-08-space-usage-reasoning.html
[EDIT: incorrect explanation deleted]
Then it must be *me* who doesn't understand the semantics of Hoogle!
&gt; if I replace a type variable with a concrete type then logically the new results should be a subset of the old ones. I don't believe Hoogle works that way, or a search for a -&gt; b would return every function in the database. Here's a slides that explain how it works: http://community.haskell.org/~ndm/downloads/slides-hoogle-08_dec_2005.pdf
 (1) Using criterion would become much more difficult Why's that? I used criterion before to benchmark a Haskell implementation against a comparable C implementation, which was really trivial by using Haskell's FFI to bind the C code, and create criterion benchmarks accordingly. Similarly I used Haskell QuickCheck tests to validate C code using the same approach ;-)
It appears that your particular domain problem requires eagerness at the base with a sprinkling of laziness on top. Have you tried programming in *ML?
&gt; This is because in Haskell we've conflated the idea of lazy evaluation with the idea of partial evaluation. There's also the conflation of the syntax, if not the semantics, of partial evaluation with partial evaluation. (Did a double take just there and absolutely felt I need to make note of that distinction. Sorry to bump an old thread.)
I wasn't aware that `a -&gt; b` *didn't* return all the functions in the DB. It keeps giving me "Show more results" for as long as I care to click it. That doc seems to be concerned with implementation rather than a spec. What I can glean is that search is meant to be fuzzy. In which case the question is, why does the result in the second search fall below the threshold for inclusion in the first search?
No. It's approximate because of floating-point. Edit: Still O(n) applications of &lt;&gt; whether it's a linear scan or in parallel. So the error bounds should be the same no matter what.
I don't understand the semantics of Hoogle either, and I wrote it!
Why is `getNumCapabilities` needed? Shouldn't the RTS take care of work distribution if you just sprinkle `par` throughout your algorithms?
par takes up a lot of computational juice (relatively speaking), so it's best to have only the exact number of pars that we need. Edit: eventually I would like to have a similar higher order function for mapreduce style distributed computations as well
We can write code that controls strictness perfectly well without `seq` and strictness annotations. Additionally, `seq` only screws up equational reasoning when applied to things you can't otherwise pattern-match into strictness -- i.e., functions. Note that these are things that we typically don't need to `seq` anyway! I've written Haskell for years, barely ever encountered space leaks, occasionally needed to force values, and when doing so have either forced them "directly" (i.e. | x == x at the head of some function, or just pattern matching on enough to ensure the strictness I want) or using a direct technique in conjunction with `evaluate`. That, along with strictness annotations *in data definitions* (where they belong) has been fine. There's an art to doing it right, it takes a little work, but when people are encouraged to just litter bangs and `seq`s at the drop of a hat, then they never learn to think these things through more carefully.
The problem is that strictness annotations on data types play poorly with polymorphism. If I have: data List a = Cons !a !(List a) | Nil ... that only guarantees that `a` is in WHNF. Similarly, if I try to use `UNPACK` to achieve the same trick, it doesn't work since you can't `UNPACK` polymorphic fields. I'd really like something like `UNPACK` that you could constrain the polymorphic value with, sort of like: data Type a = (Unpackable a) =&gt; Type { field :: {-# UNPACK #-} !a }
So, use an inner data type with strictness annotations too! (Or don't, because sometimes WHNF is what you *want*, or maybe you want a tuple-like thing which is strict in one value and lazy in the other -- this is handy for certain sorts of recursively defined structures). (Edit, expanding this point: The fact that a strictness annotation only guarantees a value is in WHNF isn't a bug, but a feature! WHNF is the "right" way to think about strictness -- it is very granular, and lets you reason a step at a time about what aspects should and shouldn't be evaluated with relation to one another. Deep-strictness is a weird non-local property and I don't see why people are so keen on it. I've only ever worried about it to "squeeze out" exceptions before serializing something over the wire, at which point I'm actually paying a cost in memory for the advantage of knowing upfront that there are no errors. Having something be made deeply strict in some other way than careful construction of it to begin with seems totally weird to me. And in fact many places where I've wanted "deep" strictness, it turns out I've only wanted spine strictness, which I can get by e.g. writing `evaluate (length xs)` or the like.) There are lots of "right" solutions depending on what structure you're using for what purpose. Hitting everything with the strict-hammer is seldom correct. Occasionally having to define data types that do exactly what you want doesn't seem terribly hard to me. If you don't check yourself, it's easy to spend more time and code in search of minor code-reuse than in just writing occasional different things that act differently.
The error bounds are only the same if you're combining the values in the same manner; i.e., the abstract tree showing the association of the operations is the same. Consuming that abstract tree in serial or in parallel doesn't matter; however, linearizing that tree changes the association and so can most definitely change the result as well as the error bounds on that result.
cabal's default should be a "user install" and to install to $HOME already - so it isn't necessary to have --user --prefix=$HOME unless your cabal config is somehow different from the default.
Good idea, I'll certainly bare it in mind. Assuming this mode is rarer, you can always do a standard search then filter afterwards, although I'll have a think about what it means to the searching algorithm.
Regarding the problem with not hitting the maybe in the echo handler, does he need a "/" on the end of echo? That's one thing that bothers my a little with web frameworks (all of them not just haskell): Some of them consider the trailing slash in a route and url implicit and some don't. In pyramid for example you have to explicitly set up redirection from "echo" to "echo/" with routes, but slashes are implicit separators when doing graph traversal. As an aside, graph [traversal](http://docs.pylonsproject.org/projects/pyramid/en/latest/narr/muchadoabouttraversal.html) in pyramid is a wonderful thing, it would be great if something similar was to make it's way into haskell frameworks!
The suspense is killing me!
I'm fascinated by Soostone's business model. Management consulting with a heavy reliance on analytics sounds a bit like a oxymoron(Management consulting with FACTS!!!)... but also awesome...
This was basically my "learn haskell project." I knew I wanted to work with machine learning type stuff, and I knew I really enjoyed haskell's algebra system, and I've just been fiddling around with the two to see what turns up. I've been working on this project for quite some time now though.
On first glance, the type signatures look less scary too -- almost usable :-) Cool stuff.
I get some build errors: $ cabal install unittyped Resolving dependencies... cabal: Could not resolve dependencies: trying: unittyped-0.1 rejecting: base-3.0.3.2, 3.0.3.1 (global constraint requires installed instance) rejecting: base-4.5.0.0/installed-7af... (conflict: unittyped =&gt; base&gt;=4.6 &amp;&amp; &lt;4.7) rejecting: base-4.6.0.0, 4.5.1.0, 4.5.0.0, 4.4.1.0, 4.4.0.0, 4.3.1.0, 4.3.0.0, 4.2.0.2, 4.2.0.1, 4.2.0.0, 4.1.0.0, 4.0.0.0 (global constraint requires installed instance) $ ghc --version The Glorious Glasgow Haskell Compilation System, version 7.4.1 $ ghc-pkg list haskell-platform /Library/Frameworks/GHC.framework/Versions/7.4.1-x86_64/usr/lib/ghc-7.4.1/package.conf.d haskell-platform-2012.2.0.0 /Users/[me]/.ghc/x86_64-darwin-7.4.1/package.conf.d 
You take away is quite misleading. This was posted on arstechnica true, but it's a write up of answers on stackexchange. So your conclusion should have been that reddit is better then programmers.stackexchange.com. I guess you where to lazy to read the elements of the article you didn't need to make your criteque. 
They are listed in reverse order. If you have a slightly correct answer and awsome answer and you think it would be interesting for your readers to go through both, it makes sense to put the lesser one first, so they don't feel like the second one is redundent.
I'm using 7.6 myself. I tried it with 7.4.1, but got a strange panic when building. That might also be caused by my own mess of ghc versions, so it might work fine for you if you follow codensity's advice and replace "base &gt;= 4.6" with "base &gt;= 4.5" in the cabal file.
Well: how much algebra do you know? You don't even need to go so far as category theory for the article in question. I have no nontrivial knowledge of machine learning, but the key insight here — that aggregating two samples from normal distributions gives a sample from a distribution itself normal — isn't *terribly* out there. All that's left is to know what a monoid is.
[A Pure Language with Default Strict Evaluation Order and Explicit Laziness](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.10.5608)
The biggest problem is that one needs two versions of each combinator-based library.
And indeed we already do this in Haskell. * strict [either , maybe](http://hackage.haskell.org/package/strict) * strict, unboxed [either ,maybe](http://hackage.haskell.org/package/adaptive-containers) I'd say the 'strict constructor unless you know what you're doing' is a pretty good way to program Haskell.
The currency one looks a bit dodgy. Are those exchange rates hardcoded into class methods? ;) Also the `factor` for currencies would be typically in market convention terms. So e.g. EUR/USD is always quoted as 1 EUR in USD, at say, 1.29690, rather than as its reciprocal.
As said in the comment at the top of the file, it's the exchange rate of as of now 4 days ago. ;) My focus was more on the SI and SI derived units for 0.1, I haven't researched an (online) API to obtain real time exchange rate information. (Yeah, I'm not going to update the package every day for a new exchange rate.) I guess I can define `factor` as `1/1.29690` for USD. For most other units it was easier to define `factor` as the amount of "base units" that 1 "derived unit" is.
Doesn't GHC have no way (at the moment) of telling that `m*m*m` and `m^3` are identical, though? Hopefully the solver for type-level arithmetic will make it into the next GHC.
You need to have seen a little of the CBPV material before the notation will make sense. CBPV divides types into "value types" V for things which are (i.e., have been or hypothetically will have been computed) and "computation types" *C* for things which do (i.e., have some computation (possibly effectful) still to be performed before a value is arrived at). Paul is careful to use plain metavariables like A and B to stand for value types, and underlined variables to stand for computation types. I can't seem to do underlining here, so I'll go italic. Value types include strict datatypes, but also suspended (thunked) computation types U *C*, where U is just syntax. Computation types include function spaces V -&gt; *C*, but also possibly effectful ways to obtain a value *F* V, where again *F* is just syntax. In call-by-value, types correspond to CBPV value types, and the function space is the value type of thunked functions from one value type A to another B, hence U (A -&gt; *F* B). In call-by-name, types correspond to CBPV computation types, and the function space is the computation type which takes a thunked *A* to a *B*, hence (U *A*) -&gt; *B*. In the latter case, the function can choose whether or not to force the thunk which is given. In the former, one must force the thunk and supply an already computed A in order to get a B. CBPV is a calculus in which the thunking and forcing interplay between values and computations is explicit. Both call-by-value and call-by-name languages can be translated systematically into suitable patterns of forcing and thunking in CBPV. In that sense, it can be seen as a metalanguage (in the sense of Moggi's monadic metalanguage) into which the syntax of your favourite functional language might be systematically elaborated, making its evaluation strategy as plain as day.
I liked hyphens from Scheme, mindful that the primary advantage is for lazy shifters.
Ah, it might make sense to put a constraint on the GHC version in the Cabal file if it is indeed incompatible with versions less than 7.6
I'm always a little unsure about making excuses for the downsides of languages by citing editor support.
Thanks a lot! I think I mostly understand it but I really need to get some sleep. I might have more comments tomorrow. (FWIW the [Wikipedia article][1] on linear logic explains what `!A -o B` means, which was the remaining mystery (though I also don't have time to read it right now). On a naive and superficial reading it seems to be saying "(Of course A) implies B" so maybe it's just the Curry-Howard correspondence?) [1]: http://en.wikipedia.org/wiki/Linear_logic
hrm, i have a Lisp book here, from around 1970. entire book is in M-expressions, so it doesnt' even look like lisp, more like Ruby. i wonder if S-expressions even came first?
Absolutely. camelCaseIsHarderToRead.
None of these are especially attractive. I quite like being able to subtract with a convenient infix operator. There's still plenty of Haskell code with underscores out there. I don't see a big aesthetic win in either case. As for languages with nice naming possibilities, Algol 68 actually allows spaces in identifiers (as do Scheme, but only between bars which is kind of cheating).
I think [this article](http://codejury.com/pascalcase-camelcase-and-underscores/) makes a well reasoned case for Camel Case. The point about underscores looking like spaces seems especially important in Haskell where whitespace is semantically important: findWithDefault defaultPersonRecord personName personToRecordDict find_with_default default_person_record person_name person_to_record_dict In the first case it is quite clear that you have four unique identifiers. I find the second case much less clear, and syntax highlighting would not help. Note: Using the hyphen in Haskell would definitely break lots of code; it appears like this only really works when you do not have infix operators such as subtraction. Having both seems pretty problematic.
I don't agree. Camel case visually glues together the words into a single token in a way that hyphens or underscores don't. And maybe it's from decades of using C++ and Java, but I have no trouble reading it whatsoever. But for the sake of completeness, Haskell's grammar would also permit `this'somewhat'unfortunately'long'function'name`.
If I recall my lore correctly, M-expressions were the original target, but S-expressions were the original implementation (partially due to the fact that they were much easier to parse) and stuck, partially due to homoiconicity.
This ship has sailed. There are more important things to worry about. :)
I agree--you need time tags too to do anything interesting. basis points vs. percentage vs. millions etc as dimensionless units are extremely useful though...
Repa has type tags to disambiguate. A delayed (essentially lazy) array and a unboxed (essentially strict) array have different type indices, but the type constructor is the same. This lets you write code that's polymorphic on both. 
I'm just barely getting up to speed with monads. Every time I've tried to read about arrows and lenses my head starts spinning.
One option is just using one let: reduce3 :: (Num a, Ord a) =&gt; Set a -&gt; Set a reduce3 n = let (max1, n') = deleteFindMax n (max2, n'') = deleteFindMax n' in insert (max1-max2) n'' I find this form close to the original but a bit more readable
This is, by far, the closet to what I was aiming for so far. I don't understand why the "state" is there for max1 &amp; 2. The error msgs out of the compiler suggest wrapping the insert (in my second example) with a state constructor should do the job but when I try it complains there aren't enough arguments (I assume it's trying to bind from the previous step to it but failing)
What's up with snap ? It used to be very active, but it seems that not much has happened to the core libraries for a while now. Is that because they are considered stable now ? Are there plans for a 1.0 release ? Are there major features yet to be implemented ?
I seem to remember one of ezyang's old posts which gave the prescription that most structures should be spine lazy but data strict.
Most of the data types in the containers package are spine strict to ensure correct asymptotic time complexity.
I've seen elsewhere the concrete example of or . map p given on why lazy evaluation is useful. In a world with strict datatypes, the result of map p is strict, so the example breaks. If this is correct, then what is a good way to sense the delineation between full lazyness and strict datatypes + lazy functions?
Unless they're using notepad, nano, or pico, they have built-in autocomplete and should be using it. I don't know of any other editors without that feature built in.
Others will pop up with the somewhat theoretical answers; let me commend to you a way of getting your feet wet with practical answers. If you use the [GHCi debugger](http://www.haskell.org/ghc/docs/7.0.1/html/users_guide/ghci-debugger.html), and single-step through some non-trivial code (though it doesn't have to be enormous), you can actually watch the order Haskell is physically evaluating things in. You can definitely write yourself some state-based code easily to step through. (Also be sure to see :steplocal and :stepmodule. They're very helpful. You'll probably also want `:set stop :list`.) You can read a lot of explanations but I think this is the fastest way to get an intuitive feel for what is going on. After that, you may find it profitable to re-ask the question to yourself.
In essence a lens is (or is at least equivalent to) 2 functions: get :: a -&gt; b, and put :: b -&gt; a -&gt; a. so getfst (1,3) =&gt; 1 putfst 3 (1,2) =&gt; (3,2) getfstfst = getfst . getfst getfstfst ( (1,2), (3,4) ) =&gt; 1 There's also a couple of assorted laws that get and put should abide by; they just codify that get and put aren't doing anything tricky behind our backs. For example, put (get x) x == x -- getting something and immediately putting it back should be a no-op. The assorted libraries provide better syntax (than explicitly naming every put or get), additional combinators, etc. But this is the basic idea.
And then there were 9 ...
Because it's monolithic work. Also, it is visible for people who are more involved by contributing, hanging out in our IRC channel, etc.
FRP is easy to grasp but may be hard to use in substantial systems. That may be the reason why Joyride did not use it for Nikki (this and the fact that it's really hard to settle on one FRP library). My opinion would be to try to make the game using a more conventional approach (i.e. game state ADTs, which still let you benefit a lot from the functional style while *possibly* using MVars or TMVars to make your game world evolve, or just STRefs if you really don't need multithreading) and then, if you have the opportunity to make a second game, you may try FRP. Because if you've never developped games using the functional paradigm, I don't think it's a good idea to add another complexity layer with FRP (_even more true_ if it's AFRP like Yampa!). I don't know if others will agree with me, but I find FRP beautiful, but pervasive (it becomes the _core_ of your game's architecture), which means if one day you come up against something that's tricky to solve with FRP, you're in for a bad time trying to circumvent it. But note that some libs look better when it comes to pervasion ('sodium' and maybe also 'reactive-banana', which I seem to have been designed with that concern in mind, and which conversely to Yampa have _proper_ documentation).
I had no idea steplocal and stepmodule existed. That makes the debugger so much more useful!
&gt; So how does the state monad (for example) fit into the above ? The state monad is not dependent on IO but it seems that due to the semantics of get and put, evaluation should happen in a linear order so that you get what has been put earlier. It is very important to understand that evaluation is not necessarily done in a linear order for the StateMonad or even the IO monad. test1 :: State Int Int test1 = do put undefined -- is never evaluated put 3 y &lt;- get put 4 return y Notice that the `undefined` isn't evaluated at all. To make it even more clear, consider the following recursive do code {-# LANGUAGE DoRec #-} test2 :: State Int (Int, Int, Int) test2 = do rec put z x &lt;- get put 7 y &lt;- get put 8 z &lt;- get return (x, y, z) Where the `put z` manages to put `z` even "before" the `z` is gotten. But it is okay, because there is no circular data dependency. The evalutation of `z` is deferred to when it the third value of the resulting triple is demanded, if ever. The moral of the story is that monadic notation, just like all of haskell, simply a mechanism for introducing data dependencies, and the runtime system will evaluate the required data, in some order respecting the data dependencies, which may have little bearing on the way the code is actually structured.
Is there much interest in Haskell in Brazil, or at the University of Sao Paulo in particular? Last year I visited USP@Sao Carlos and nobody I spoke to had heard of it :-/
Your undefined is not a "monadic" value and is never used, so it is not really a surprise (to a haskeller) that it is not evaluated. However even the following works just as well: do undefined put 3 y &lt;- get put 4 return y 
I wouldn't say so, but it's a start. Haven't heard much from my contacts at USP SP.
&gt; if the parser on the left succeeds, the parser on the right is skipped. This is not the same as "If its left hand side parser can make any progress then it commits to that alternative."
Did he finally get banned from every other group?
Agreed, magnitude type tags are super useful.
I feel like I am missing something. Who is Jon Harrop and why is this getting downvoted so much?
Look here [1] and search for his posts in /r/haskell. You might need to start from a few years back. [1] http://www.reddit.com/user/jdh30/ 
Now you're just teasing. :) Give me some concrete arguments of why.
Alright, I read through it. For other people who want a summary of his comment history, the main problems were: * He had an unhealthy dislike of the Haskell language * He debated dishonestly and in bad faith However, I'd disagree with the decision to ban him, mainly because critics like him do a great job of pointing out where we need to improve (even if dishonestly so) which ensures we don't lose focus of what issues matter to people outside our community. We should welcome criticism to avoid becoming intellectually inbred.
&gt; when you compose pure functions, because of the laws of associativity, the order in which things are evaluated will not effect the evaluated output of the program. Perhaps you mean commutativity? Where a+b=b+a? 
I'm curious about it, too.
Thanks for the helpful feedback. I try to keep the Google Group free of spam. Occasionally it has gotten bad, but it seems to have been better lately. We do have a decent amount of discussion on IRC because we tend to leverage the higher level of interactivity. But you can't just come into the channel and then leave. We're all in different timezones and there aren't enough developers to keep chatter going all the time. So the best bet is to stay in the channel 24-7 and read the scrollback. Typically our conversations tend to happen during US and Western European business hours.
If you need a new one - make it by yourself.
Your persistence and idealism are terribly admirable!
There are 2 senses of "a world with strict datatypes." One of them means "everywhere only strict datatypes", so this is a world of only finite data and total functions. The other, is data declarations have bangs applied everywhere, so data is strict by default. In such a world, you'd have a lazyMap function of type `(a-&gt;b)-&gt;[a]-&gt;{a}` where `{a}` means lazy finite lists. Then `lazyOr . lazyMap p` would give you the short-circuiting performance you desire. 
Then I'm misunderstanding that sentence. How is it different?
Although "Abelian" apparently applies to another abstraction, alliterations are aurally attractive.
Indeed, I share your opinion. I have used it myself recently, and found it mature and effective. My initial question was not intended to mean anything about the current state of Snap. It was just about what is going on at the moment.
Said evil part is specifically [here](http://hpaste.org/78401#line152). The plus side is that you can just define your data type [like this](http://hpaste.org/78401#a78404) and write [things like this](http://hpaste.org/78403#line42): `select a [field taskId :=: Int id] [desc taskId]` If I can figure out a nice way to implement `FromRow` using `Data` also, this will be quite usable. It could then be extended into a monad. Question is, is the use of exceptions like that the most evil thing ever? It's such an awful hack, but I can't think of another way to get the name of a Haskell field and still have the data type be native and normal. Just thinking aloud, this is a technique I've been pondering since forever but never bothered to try it to see how it feels.
Looking further into this library, I am less certain of my assertion... I haven't actually used it and I'm not completely sure of its semantics, so I apologize if I'm just adding noise. But anyway... Parsers can partially succeed; a parser for "a b c" can match the first two tokens of "a b d" before failing. "*If its left hand side parser can make any progress then it commits to that alternative.*" suggests that the right parser will be discarded in this case and the combined parser forced to fail. This is what Parsec does with `&lt;|&gt;` if you don't wrap your left parser in `try`. OTOH, "*if the parser on the left succeeds, the parser on the right is skipped.*" suggests that the left parser must completely succeed for the right parser to be discarded. This is the more typical behavior of `&lt;|&gt;` when only one alternative can ultimately win, left-biasing.
The whole idea behind this `&lt;&lt;|&gt;` was to get the Parsec interpretation of `&lt;|&gt;`, so I think you're right. I understand the difference now, but I understand the usefulness of `&lt;&lt;|&gt;` much less. I guess I'll have to read up on it. Luckily enough, this doesn't influence the unitality and associativity of `&lt;&lt;|&gt;` :)
Aye, that seems pretty much the same approach. *Exceptionally* evil. ;-) Although, I think at least that approach won't bail on bad functions passed to `upon`, as it returns a Maybe. I suppose I could make `field` return a maybe, or “do nothing”. Like: -- | Get the a record field. dataField :: Data a =&gt; (a -&gt; b) -&gt; Maybe String dataField f = unsafePerformIO $ do catch (let !_ = f dataDescribeFields in return Nothing) (\(FieldName name) -&gt; return (Just name)) data Submission = Submission { submissionId :: Int, submissionTitle :: String } deriving (Data,Typeable) λ&gt; dataField (\Submission{} -&gt; ()) Nothing λ&gt; dataField submissionId Just "submissionId" λ&gt; At least that is "safe". I see Edward also considered this: &gt; -- 1. When passed an illegal field accessor, 'upon'' will give you a 'Lens' that quietly violates &gt; -- the laws unlike 'upon' will will give you a legal 'Traversal', that avoids modifying the target. But in all cases, it _is_ rather unHaskelly to bail at runtime instead of compile time. Hm.
Are all the `error`s necessary?
If you mean could it return an error value instead, yeah. But it would still be a runtime-handled problem, not statically handled. The presence of all the errors indicates how little help Haskell gives at compile-time to ensure that these things are so. It makes me curious about Ur's type system which apparently can concisely handle all this in a concise, well-typed way, without resorting to macros. Does that unsettle anyone else as much as it does me?
For `dataDescribeFields` couldn't you just pass it a Proxy to indicate what type you want to describe? data Proxy a = Proxy dataDescribeFields :: (Data a) =&gt; Proxy a -&gt; String ... and invoke it like: dataDescribeFields (Proxy :: Proxy MyType)
What would be the purpose? How would you implement `dataDescribeField`?
Pretty much the same way you would implement upon. This difference is that with upon you have a structure to take apart present, here you'd have to fromConstr each possibility. Afterwards, however, the data work would be done.
Back when I was at USP I never saw many Haskellers around, and most of the CS faculty isn't very into FP. The only time I heard about Haskell was someone giving a lecture about it at the physics department but I missed it.
ofcourse A *linearly* implies B. Of course, with the bang the linearity isn't such a big thing anymore. But it's good to be precise.
Sure, post-filtering is always an option. But you mentioned that one of the big goals is speed, and that one of the big problems is returning too many irrelevant/distant matches; and pre-filtering is good for that sort of thing. In any case, it's an interesting problem. If I had any time I'd take a look at it; it's similar enough to the kind of search we do in natural language processing to think I could apply a few tricks, but it's different enough to make me wonder if I could learn something on that problem and then apply it back to NLP... Definitely an interesting problem academically.
I sometimes get to the end of Conal's posts and find I'm not entirely sure what has been demonstrated. I think part of the issue is, like pan-dimensional beings of old, I was never entirely sure what the problem was. I get the same feeling when I'm reading Oleg's posts. _He's solving a problem and I can't even understand what the problem is._
Yeah, I think these modules need a lot more documentation/elaborate examples.
I think it would have been a stronger post if he gave examples using it.
He stated the example in the introduction. It turns a binary addition with carry of n bits from a depth n to depth log(n) by parallelization and speculation. Doing more work than necessary, but in parallel. The concrete speculation is doing the computation for both values of the carry, at certain bit positions. What is not clear is what are the positions where the speculation occurs and what is the parallelization overhead, i.e. how many bit adders are necessary to complete the computation.
I'm a bit disappointed that Nikki didn't prove FRP's worth, but I agree, I don't think FRP is quite to the "useable" point yet, at least for games. I feel there is a lot of potential improvement with it, however, and once that comes around, an era of limitless beauty will ensue.
Love the examples. very cool. I'm wondering though, since the type system is figuring out all the nitty gritty of unit matching, would the performance of this code be better or worse than dimensional? how much boxing/unboxing is happening?
It seems rather clear to me that Conal didn't specifically wanted to solve the problem of parallelizing binary addition. He used this example to show a far more general technique of using speculation to potentially parallelize seemingly sequential algorithms. In a, to me, elegant way. Maybe the example wasn't the strongest, but that doesn't invalidate his post the slightest bit.
I'm sure that Conal knows how to design parallel adders. Rather, I think that his point was how to formalize speculative evaluation into a design pattern (memoization), using adders as an example.
HList lets you represent heterogenous collections, yes.
Can you give an example of when you think it would be good to use this instead of existing functions? What problem is it trying to solve?
I don't see why you would want to index a collection by the type of an element. It seems very strange at first glance. 
Agda's standard library haa this: http://www.cse.chalmers.se/~nad/listings/lib-0.6/Data.AVL.IndexedMap.html
This was answered in a [limited G+ post](https://plus.google.com/117852249512245938101/posts/PN2AgXTenRn): *Gregory Collins Yesterday 11:57 AM - Limited* &gt; People curious about streaming I/O in Haskell: last night I pushed my work-in-progress io-streams branch to public github. This library is going to form the underpinnings of the next Snap Framework release. Quick feature list: &gt; &gt; * simple types (InputStream a, OutputStream a) &gt; * NOT continuation-passing style &gt; * as a result, interacts well with asynchronous exceptions (you can use "bracket"!) &gt; * 100% test coverage &gt; * streams are composable, and io-streams comes with a slew of combinators (folds, maps, filters, zips, take/drop, etc) &gt; * integrates with zlib, attoparsec, blaze-builder, vector &gt; * comprehensive documentation with lots of examples, and a work-in-progress tutorial written by +Gabriel Gonzalez of "pipes" fame &gt; &gt; Please download it, build the haddocks, and see what you think. and about the motivation: &gt; the short answer is that I came to the conclusion that continuation-passing style is not so great for doing I/O in Haskell, the biggest reason being asynchronous exceptions. Conduit/pipes/iteratees, for example, don't have proper "bracket" semantics, and it's difficult to see how they could be implemented. I consider ResourceT to be a pretty awful kludge. &gt; &gt; The other reason I wrote this library was that conduits and pipes, while having some nice properties that this library doesn't (categorical composition is in some ways better than the Kleisli composition that we're using in io-streams, and we don't have a pipe type here at all), are really complicated, and I wanted something easier for Haskell newcomers to understand. Both conduit's Pipe and pipes's Proxy types have six type parameters. I just want to do IO!! &gt; &gt; Iteratees/pipes/whatever might be a dead end for me personally, but it was pretty clear that some sort of abstraction for data streams is necessary. Honestly, and it pains me to say this, the biggest inspiration for this library comes from Java object streams. Looking at the landscape for doing IO in Haskell, it was clear to me that Java's programming model for getting input into and out of programs was a lot better than ours: not elegant, but it works and you don't need a rocket science merit badge to understand it. Our language is a lot more powerful, and my hypothesis was that the same underlying model ("object handles") plus Haskell combinator style would result in something pretty elegant and easy to use. This turns out to have been the case. &gt; &gt; You may also note that there are no monad transformers in io-streams. This is intentional. I'm also coming to the conclusion in my own code that monad transformers are overused/abused. Each level added to a monad transformer stack is a tax on every line of every function 
I agree with the whole analysis, athough I wished we could totally get rid of exceptions at some point. Thank you Gregory for pushing this direction.
I intentionally picked a technical title to help people decide quickly if they would be interested in the content or not. Not sure if that's the best strategy.
Shucks, I didn't quite monopolize the entire quote section. There were two unrelated ones. ;)
Seems like you could do that using type classes. Maybe start with an idea like the following (the code I've posted is untested, and may not even work; but it should probably lead you in the right direction): -- basically an heterogenous list data Sink :: [*] -&gt; * where SinkNil :: Sink '[] SinkCons :: a -&gt; Sink ts -&gt; Sink (a ': ts) add = flip SinkCons class Get ts a where get :: Sink ts -&gt; a -- base case: the item you want is at the head of the Sink instance Get (a ': ts) a where get (SinkCons x _) = x -- inductive step: the item is somewhere else instance Get ts a =&gt; Get (t ': ts) a where get (SinkCons _ xs) = get xs 
My favorite zinger in the intro is "if you have a first-order language with sequential and parallel composition, plus feedback, then you've really got a higher-order language"; which is fairly accessible, and very interesting :-)
I have a hunch that this is actually a proof that arrows are equivalent to applicative functors.
ooh -- nice intuition. by "equiv" do you mean "can embed" (as in, not iso)? I think the same sort of stuff as is here is presented in the rosetta stone paper too : http://math.ucr.edu/home/baez/rosetta.pdf
Even if we ditch exceptions for IO errors (and i'm not sure we should), we'd still need to interact properly with asynchronous exceptions.
Hmm, looking into it at the moment. I will not achieve compile time type safety here will I? edit: Data.Data might make that feasible... I should perhaps clarify what kind of type safety I mean here. I want it to be known at compile time whether '(get s :: Int)' will work out, by looking at the type of the sink. So when I (set s 3) Int will be added to the type of the sink. That or something as semantically powerful.
If you are really using types as indexes, your tuples or records won't be very general as you couldn't encode (Int,Int) for instance. 
I'm implementing the first bit in Haskell and I noticed that there's no equivalent to `trace` in `Control.Arrow`; my first reading I mistakenly assumed it was `ArrowLoop`, but that's a more fix-point-like operation. I don't think it's possible to implement `trace` just in terms of `Arrow` and `ArrowChoice`; but what *is* required to implement it? Obviously `ArrowApply` would do the trick, but that's a pretty big hammer to use. And I don't think you can implement `ArrowApply` in terms of trace, either. Or is it a new construct? If so, should it be in the standard arrow library? EDIT: I guess it is implementable in terms of `ArrowChoice` and recursion. trace :: ArrowChoice (~&gt;) =&gt; (Either a b ~&gt; Either c b) -&gt; (a ~&gt; c) trace f = arr Left &gt;&gt;&gt; f &gt;&gt;&gt; trace' where trace' = id ||| (arr Right &gt;&gt;&gt; f &gt;&gt;&gt; trace')
Thanks for writing this. Really. This is exactly the functionality I end up rewriting for scratch (and getting wrong) on every new project because pulling an iteratee library would be overkill.
I think it *is* `ArrowLoop`. Patterson's "Arrows and Computation" makes this point explicitly (`ArrowLoop` as a generalization of `trace` [as defined on `(-&gt;)`] ): http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.123.1351 See section 2.3 in particular. Also note that one can define fix in terms of trace and vice versa: trace f x = fst $ fix (\(_, z) -&gt; f (x, z)) fix f = trace (\(x, y) -&gt; (f y, f y)) undefined Finally, see more general work on traced premonoidal categories by Benton and Hyland: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.9.9365 That paper also makes the connection to `ArrowLoop`, with more explanation.
See my answer to roconnor.
I see.
I had [an exchange with drb226 once](http://www.reddit.com/r/haskell/comments/y7gjd/state_values_that_travel_forwards_and_backwards/c5t6aqf) where I figured out one way of implementing the `SeerT` monad transformer, a relative of `Tardis`. For some strange reason, I find that easier to grok than reverse state. But more than anything, the example in [Luke Palmer's post](http://lukepalmer.wordpress.com/2008/08/10/mindfuck-the-reverse-state-monad/) (same link as above) just makes my head hurt...
Ok, now I'm confused. I see in the post's code that ⊗ is being interpreted as a sum type, rather than a product type. But the typical interpretation is as something which has `swap`! And indeed Neel is explicitly talking about symmetric monoidal categories (categories equipped with `swap`)! But now, looking at the linked paper, I see this is actually the "trace" of a "sub-traced monoidal category" where a little recursive gadget lets us recover a typical trace (I think?). So there's more going on than I realized at first. I'd be interested in seeing the Haskell code you end up cooking up for this.
This is Haskell! :)
What if the state has n possible state? Then you need to do possibly n speculative works.... You gain parallelis, but performance? 
Does `runghc app/stackage.hs build` build all of Platform?
I guess the Haskell way of doing the second example would be to use the first one, isn't it? At the point where the HashMap is created the parsing is done. So it doesn't really matter if the value is read from the map or the data type. Lazyness will prevent us from having to read more than what is necessary.
I am surprised that [Stackage.Config](https://github.com/fpco/stackage/blob/ac709e93b4d1e12df072233fbd64a1344c0e9f25/Stackage/Config.hs) doesn't list the versions of packages. I'm not sure how this is supposed to work. (edit: apparently there is support for version ranges, it's just not used. Still not using it defeats the purpose IMO.) Also, I'd like to see precise responsibilities of maintainers stated somewhere. Sending a pull request with your name and your packages doesn't look like a big achievement — it's what happens later that matters.
&gt; Usage of Aeson isn’t exactly the most well documented thing in the world After some criticism and discussion in [a previous post about Haskell and JSON](http://www.reddit.com/r/haskell/comments/136pnn/static_vs_dynamic/) I decided against writing such a blog post and wrote [some example-ridden documentation](http://chrisdone.com/aeson/dist/doc/html/aeson/Data-Aeson.html) (compared with [the Hackage version](http://hackage.haskell.org/packages/archive/aeson/0.6.0.2/doc/html/Data-Aeson.html)) for aeson's Haddock instead, which [Bryan merged](https://github.com/bos/aeson/pull/91) and I expect will be on Hackage by the next release. Anyone is welcome (including Raynes: maybe I missed some stuff, I also don't have much time either) to improve upon this, which I would (and I assume others) prefer rather than writing a blog on a separate site which will become dated. I will also port my [HaskellDB](http://chrisdone.com/posts/2011-11-06-haskelldb-tutorial.html) tutorial to the Haddock docs soon, as part of this general initiative. Can I also ask anyone reading the posted blog who found it useful to consider writing documentation for any libraries that they have figured out and that are scantily documented, including pitfalls and edge cases that were not apparent but bit you. I think it's quite important for our general ecosystem. Also if you are an author of a library, consider asking the community to write docs for you. You might not have the time but someone else might.
This. Tutorial and narrative style documentation are an incredibly important addition to api docs!
That is great! I will need to take time to understand it. Do you think it's possible to do it as a record instead of list? I'll try to figure it out once I've understood this. edit: I reckon I can take inspiration from hlist records for that; although hlist has seem a bit hard to absorb so far.
Possibly because being a typed functional language you start to expect too much of it. I'm quite fond of C because I know it's nothing like Haskell so I don't get disappointed.
What is that http CLI command?
Good stuff. On the mechanisms side (rather than social organisation side) my recommendation is that we should aim to use the main hackage server / archive, and to use tagging to define the subsets. That will let us make subsets more easily (e.g. a set for each stable platform release perhaps?), and allow everyone to benefit from common infrastructure like automatic build reports etc.
&gt; Syntax wise clojure and haskell are much closer than scala. Eh. This feels misleading, possibly true but only in some unimportant sense. The flexibility given by s-expressions sets Lisp apart syntactically from other families of languages. Neither Haskell nor Scala have that and, outside of snippets like you've provided, Haskell code in my experience looks nothing like Clojure code. I mean, it doesn't look much like Scala code, either. But both are infix languages with sophisticated type systems and those with Haskell experience are likely to use them similarly. Clojure's a different beast.
What ever happened to this: http://new-hackage.haskell.org/
I mean, what is its purpose compared to the Haskell platform? I think he means that it is like the Haskell platform in that is is curated and vetted, but unlike the Haskell platform you don't install it by default, but I'm not sure.
I sympathize with outdated blog post syndrome, but I really didn't have time or feel like putting effort into writing documentation for Aeson. I also don't think I know enough about it to do so well. I found other blog posts/documentation lacking for my specific purposes so I tried to make that easier for future users. I am excited to see the documentation you're talking about though. And I'll certainly try to contribute to documentation where I am able.
Oops, I thought you were replying to my other comment. Sorry about the confusion!
I actually tend to favor Scala over Clojure for exactly that reason. On the other hand, I also tend to use far more type annotations than are actually necessary in both Scala and Haskell, because I like being able to trace the type-level flow of information in my head. Scala's type system is in some ways *more* powerful than Haskell's (neglecting the trivial equivalence of Turing-complete systems), since it has built-in and full support for subtyping, and you can pull off some stuff with variance annotations that put the usual demonstrations of type-level programming to shame. The only thing I wish Scala had, type-system wise, is first-class universal quantification, which, aside from Java interoperability concerns, would fit nicely into the existing type system, since, given any `X[_]`, `L`, `U`, and, `A0 &gt;: L &lt;: U`: (X[A] forAny {type A &gt;: L &lt;: U}) &lt;:&lt; X[A0] &lt;:&lt; (X[A] forSome {type A &gt;: L &lt;: U}) (Incidentally, I'd like to see Haskell gain a proper sub-constraint operator `(&lt;:&lt;) :: Constraint -&gt; Constraint -&gt; Constraint` for similar reasons to the above.)
&gt; I sympathize with outdated blog post syndrome, but I really didn't have time or feel like putting effort into writing documentation for Aeson. I also don't think I know enough about it to do so well. Of course! That was *not* a criticism of you or your work, it's very much appreciated and it's a really nicely written post! As you said you're just starting with that package, and we're all working with limited time and it takes some effort and patience to work with Haddock's markup (I think it's actually a bit of a deterrent sometimes… I might even write an app just for writing Haddock docs “live” to ease the burden). I ended up doing the same thing for HaskellDB. I was gonna write the same kind of post as you for aeson. I also found some stuff on your blog post that's also worth documenting (e.g. `.:` and such), it shows what people coming to the library found notably unstraightforward. Actually blogging is cool as it shows off Haskell, which I think was part of your intention. There's also some good comments resulting from your blog post. I'm even pondering that it might be nice to include links to blogs in the docs, even if they will expire over time (like “here are some blogs that might be of interest”). My point was more “let's (as a general community) *also* apply this really nice wordy description to our Haddock docs, too!” (Also Bryan did [very helpful docs](http://hackage.haskell.org/packages/archive/mysql-simple/0.2.2.4/doc/html/Database-MySQL-Simple.html)—I mean you can just hit the ground running from these docs—for his mysql-simple library, I think it was just assumed that ‘aeson is like json, that package you all know’.) I think there's also some crowdsourcing of documentation in the works/planned for Hackage 2, if I remember rightly.
Totally agree with everything you said in both this and the original comment.
We're looking at expanding the platform more aggressively (see Mark's talk at the Haskell Implementors Workshop this year). We added 3 new packages this batch and I hope we can add even more in the next one.
Has this been taped?
abovementioned talk: http://www.youtube.com/watch?v=st22QE-g0uo
hackstage is a much better name
Yeah I've seen ⊕ for that type of "tensor" product. 
Indeed! We'd love to see efforts focused on expanding and improving Haskell Platform itself. There is general agreement to do so (my talk was well received) and what we need is for the community and the package maintainers to help work together to grow the platform. The [timeline for HP 2013.2.0.0](http://trac.haskell.org/haskell-platform/wiki/ReleaseTimetable) has been published. I'd love to see maintainers of stable packages start thinking now about becoming part of the platform. 
I posted [`lens`](https://github.com/ekmett/lens) to the list of projects. One easy way to get involved is to join the `#haskell-lens` channel on freenode. We can definitely use some help with adding more doctests and better examples. Also there may be some low hanging fruit in [the issue tracker](https://github.com/ekmett/lens/issues).
Whole this thing seems like an almost obvious idea at the end. If you look at things like Gentoo it has Portage which has masked packages, unstable packages and stable packages. Right after installing Gentoo you get package versions only up to stable ones. After changing one setting you can get newer packages but they play well together and shouldn't require manual fixes by the user. And for those who want newest things at the cost of unstability there are masked unstable packages and overlays. They have a set of package versions called unstable that *should* work just fine without having to install anything that is masked. I wouldn't be surprised if all we had to do was to set up similar filter of package versions and focus work on keeping biggest set of packages possible that always plays well together (by using older versions until silly constraint fixes are done and so on).
This is interesting timing. I just started work on a similar project focused on Haskell but not specific to a season. My initial experiment is [here](http://haskelltasks.org/). Mine differs in that the purpose is * To track tasks that people in the community need, on any project, like, a feature to do this, some documentation to do that, collect some data on this, etc. things that people can just approach without much prior knowledge and finish in 5 minutes to a few hours. * You can't see it on the online version, but you can also provide links to a bug tracker e.g. Github or Trac, and it will monitor that and detect when the issue has been closed so that you don't have to maintain two places. * You can express interest in a task being done, similar to Google+'s +1's, which can be a way of setting priorities (I think anonymous upvotes carry less weight). It's supposed to be like a developer-aware reddit / StackOverflow with work instead of answers / /r/haskell proposals, except with realistic tasks that aren't intimidating, much smaller faster turn around tasks / an on-going hackathon -- actually I imagine such a tool would be particularly useful for our hackathons. There are people all the time posting about how they need a pet project to work on in Haskell, or some small feature or even writing docs, anything. There are 10k subscribers to this reddit, 28~ people online at any one time, if only a fraction of those people visited and contributed to haskelltasks as much as reddit, we'd have a whole new kind of ongoing contribution. I'm stuck here at home with an awful internet connection so it's pretty hard to work on it at present. Not ready to release it. Just wanted to note that. I think it could be cool.
I was taking a CT perspective. Category theory doesn't concern the details of an operation. It only concerns the objects (types) and the laws. See http://en.wikipedia.org/wiki/Traced_monoidal_category for the laws of trace/loop.
`getRHLines` can be written nicely with the new `aeson-lens` package (which is *fantastic*): import Control.Lens import Data.Aeson.Lens getRHLines :: Value -&gt; Maybe Integer getRHLines json = json ^. key "lines" As long as you can produce a `Value` (which you can get via a `decode` and the case analysis to make sure the parse succeed), you can maybe get the lines property out of it, by looking at the "lines" path. If the `Value` isn't an `Object` this will fail, and it will also fail if there is no 'lines' key, or if the 'lines' key is not an integer. All of this in hardly any characters!
I just saw this http://www.reddit.com/r/haskell/comments/141hww/stable_vetted_hackage_call_for_participation/ which is trying to do the same thing but not with hackage itself
It's the same as the one he defined. I believe the reason was that you can't distinguish between: Ap (fmap k x) f = Ap x (fmap (. k) f)
They are not the same — in gergoerdi's version both arguments of App are Free.
No, I mean the one Paolo defined is the same as Edward's definition, both of which have the above equivalence.
Here's a way to have free applicative functors that doesn't break the laws: http://hackage.haskell.org/packages/archive/free-functors/0.1.1/doc/html/Data-Functor-HFree.html I've rewritten Gergo's code to use HFree Applicative: https://gist.github.com/4183639
related: http://hackage.haskell.org/trac/hackage/ticket/227 (aka https://github.com/haskell/cabal/issues/220)
I don't think it would be a good idea for packages to depend on a Haskell platform version. That would just mean they would have to bump the Haskell platform dependency version in their package even if nothing they actually use changed. What is wrong with directly depending on individual packages? I can see the usefulness of a metapackage for use in cabal install executions but not in dependency lists.
Stupid question: as a Haskell newcomer, it seems one of the features of the language is to enforce a strong firewall between I/O and pure functions. It seems like this library makes it easy to mix to two, defeating the purpose of main Haskell feature?
Very cool! But it only worked for me after I also added `:set -XNoMonomorphismRestriction`, because myPrint has no type signature. And is there a way to fix the prompt? Now it is `Prelude IPPrint HsColour HsColour HsColour&gt; `
how can you detect whether GHCi is being used from Emacs or not?
I just aliased ghci in my shell so that it only invokes the `:set prompt` stuff when I run it from my shell (by using an additional ghci script). Without the additional script, I have it set so the prompt is just always `ghci&gt; `. Emacs of course isn't even aware of the alias, so that's what it ends up getting.
if you think of negation as A -&gt; Bottom then \(x,a) -&gt; case x of Right b -&gt; b Left na -&gt; ex_falso_quodlibet (na a) is a proof that (~A ∨ B) at least implies A -&gt; B. Similarly \f -&gt; \g -&gt; g (Left (\a -&gt; g (Right (f a)))) should be a proof that A -&gt; B implies ~~(~A ∨ B). Now, you really get somewhere if you understand ~A to be a "hole" or "covalue" in a continuation based formulation. Then "filling" a hole is really nothing more than "jumping back" to where you called the law of the excluded middle and everything just works. In classical logic -&gt; is not special. In fact, in Curien's more recent versions of "System L Syntax" or "Lambda Mu Mu Tilde" you have a notion of "generalized connective" for defining data types. The function type constructor becomes just another algebraic data type. Actually, you get two function type constructors (one each for call-by-value and for call-by-name), but that is because classical logic can't really be made constructive and computable without polarizing it.
That interpretation makes more sense when ~A is on the left side of the turnstile... when it's on the right hand side I still think of it as trying to derive absurdity. I guess, in the context of the process calculus, it is the "shut up and do something else" protocol. I'm not terribly familiar with all of the historical backgrond for Curien's System L; AFAICT, it unifies a lot of existing systems, but I don't know how beyond "well it is focused".
Too soon!
In particular it's now got all the old accounts imported, and the per-package maintainer groups are set up. e.g. http://new-hackage.haskell.org/package/cabal-install/maintainers/ For a summary of the server interface, see the automatically generated [api page](http://new-hackage.haskell.org/api).
Classical mistake. Happens in gcc as well.
I don't see how this behavior is acceptable. Shouldn't a compiler wait until it's performed the checks to make sure the source is valid before destroying the target?
The way I see it, the eagerness of highlighting is a consequence of the packages you chose to use. If that assumption is right, would it be possible to simply write a highlighter specifically for GHCi that gets rid of that problem?
Possibly, but note that this doesn't only do highlighting. It also restructures it to fit in a certain width without making it ugly. It's harder, albeit maybe not impossible, to be sufficiently lazy for this task.
It doesn't match in this case. The source is Test.
It would be nice if pages used haddoc ocean style.
Giving the proposal the benefit of the doubt, Tekmo is asking that compilers use all available inference to determine (chasing symbolic links, you name it) whether the output will clobber the source, and refuse to do so without an extra flag. He was probably aware that a string compare of Test.hs and Test would fail.
We're concentrating on the underlying stuff for the moment, but we welcome patches for the html skin. There's a new design too if someone wants to work on that.
Well, Zuckerberg is a ruthless capitalist geek, which is kind of like a god to both modern geeks and entrepreneur types, who see either validation of themselves or an inspiration to single-minded pursuit of profit, or, in the case of the Y-Combinator crowd, both. Stalinists are much rarer, though for some reason do exist despite the massive campaign against leftism and the now well-known brutality of his rule. I don't understand them at all.
I would suppose that he just misread it for `ghc -o Test.hs Test.hs` or something like that.
No, they don't. Also, yes, it can be lazy, but it's at least a little harder than it might seem since it means you have to produce a certain amount *beyond* the character you're about to output to know how readable it would be to insert the newline here vs. a bit later.
Link is broken -- this one works though: http://oleksandrmanzyuk.wordpress.com/2012/12/01/category-theory-helpline/ Great article, though. The function we want on Eithers that we can't always get for functors is `costrength`: http://comonad.com/reader/2008/deriving-strength-from-laziness/
Yeah, one of the biggest problems with Haskell right now is wrangling all the different string types. The community needs to compromise on one and Hackager submitters need to agree to use it.
I'd expect if you do this you will also have the file open in an editor, so if you realise what you have done, save the file from the editor without loading changes "from disk".
So, the test instance is up; what do you want us to do with it? How can we help speed the path to deploying hackage 2 for real? ---- &gt; We'd like to make it as simple as possible for anyone to set up a Hackage server. This is a great goal. Never lose sight of this.
You must have tried something different that I did or done something I don't normally do to produce this problem, because I haven't encountered this yet. Edit: I encountered this. It only seems to happen once after I use `:!clear`. I'm sure there are other things I can do to trigger it though.
&gt; So, the test instance is up; what do you want us to do with it? How can we help speed the path to deploying hackage 2 for real? I mainly posted it to reddit at this stage so that people can see we're making progress. At the moment we mostly need people hacking on the code. There's a bunch of issues we know about and we don't yet need testers to tell us that again. But that phase won't last for ever, and we'll come back and ask developers to play with it. We've also got a live mirroring tool (not currently active) so we can have an extended period when we run the servers in parallel. &gt; This is a great goal. Never lose sight of this. Thanks.
http://blog.raynes.me/blog/2012/11/27/easy-json-parsing-in-haskell-with-aeson/#comment-724150175 indicates a seemingly equally succinct version. I tend to prefer it because I'm running out of room to stuff more weird concepts in my head for the time being.
I pitty the fool who doesn't write makefiles
Do you commit every small change you make before compiling and testing? I definitely don't.
 The idea `ocharles` [announced](http://ocharles.org.uk/blog/posts/2012-12-01-24-days-of-hackage.html) is obviously good.
Personally, if it’s not committed then it’s still open in my editor, where I can just resave it.
I thought I'd leave some advice on Haskell performance tuning here. It's definitely a topic that could use some more documentation. In particular it could need a comprehensive tutorial that covers everything an industry programmer might need to know. Here's a grab bag of stuff related to things mentioned in the article: &gt; "preferred" libraries Our medium term solution here is a better Hackage, with votes, download counts and other library quality signals. Hackage 2 is hopefully here soon, but the drawn out timeline has meant that this has been a problem for quite some time. In the mean time I made [a list of good libraries](https://github.com/tibbe/haskell-docs/blob/master/libraries-directory.md). &gt; strictness annotations Put them on your data type fields and don't worry about them elsewhere. The only big exception to this rule is that you should put them on accumulators in your recursive functions (e.g. if you're not using predefined functions, such as `foldl'`). &gt; never-ending space leaks I've had the opposite experience. Java tends to be quite memory hungry due to the heap object header being bigger and the lack of `UNPACK`ing. &gt; UNPACK I've mixed feelings about `UNPACK`. On one hand it's great, it gives us more precise control over data layout. In particular we can unpack composite types which you can't in e.g. Java, leading to lowered memory usage and fewer pointer indirections. On the other hand it's a bit verbose; putting an `UNPACK` on every scalar field (e.g. `Int`, `Double`) makes code uglier. I'm working on a solution for the latter. 
&gt;i can't deploy a haskell web-server for example, that uses five times the memory per connection as a java web server.. Really ? Of all the examples you had to chose the most unbelievable ? Java web servers as a standard of good memory consumption ? Are you fucking kidding me ? Do you know what's the most often asked and answered question on gazillions of java web development forums is ? It's java memory startup options: -Xms512m -Xmx3g I'm running both java and haskell web servers, and my java web servers take up to 8(!!!) times more ram than my haskell web server. That's is not to say that haskell web server memory consumption is good, just to point to you that comparison with java web servers is ridiculous.
The art of "tuning" *in general* is lore-oriented, no matter what the language, imho. If a tuning technique is common knowledge, then it is simply the norm instead, and would probably not be considered "tuning".
I'm not really sure what to say on the topic; I don't know your level of PL sophistication or how your interests map onto this language. The project is in a very volatile state: the implementation is about to undergo a major rewrite to catch up to the documented theory/semantics, which themselves need to be significantly extended to catch up to recent developments and ideas that we haven't had time to codify yet. About the only similarities it will have with Haskell are that it will be strongly statically typed and that it will use juxtaposition for application (although the latter is subject to change).
Is it a bad idea to use -funbox-strict-fields?
For the TUI console idea, taking a look at IPython's QtConsole (http://ipython.org/ipython-doc/dev/interactive/qtconsole.html) could give some ideas of what might be possible...
IIRC tibbe's working on -funbox-strict-primitive-fields.
&gt; * **types** - (again!) from time to time we all feel like we are serving the type system instead of it serving us. when misused, the type system can create inflexibilities that make it impossible to tune code. I don't get how this relates to tuning.
Search for "Spineless Tagless G-machine".
When Zuckerberg puts in his effort, you get Facebook. When you put in your effort, we get an Internet comment, riddled with profanity and youthful maximalism.
This is exactly how I recovered my file this time. This usually bites me in the following way: I keep a directory of simple tests and one-off scripts which I usually run with `runhaskell`. But sometimes people will ask me for a binary version so they don't have to have GHC installed and I don't want to bother with a Makefile or Cabal. That's when this becomes a real danger.
Why would you add strictness to something you know should be lazy? &gt; can lead to just as many performance problems Strictness can be equally bad, but usually we mean hyperstrict, across all values. But this post is about the practical, boots-on-the-ground experience of optimizing Haskell. And in doing and teaching that I simply do not find that strictness annotations are a fragile or dangerous mechanism - just as likely to "explode in my face" as not. For the vast majority of users, they'll find either no effect (strictness on something irrelevant), or a boost (strictness where not determined by the code already). And you do still have to think about your code, even in Haskell.
&gt; Foreign.* - Foreign is of course part of how haskell provides a great FFI, but i find it is abused as an escape hatch to make haskell programs faster. I think if a library uses FFI and provides an idiomatic Haskell interface then I don't see the problem. If you get to call write code using Haskell then who cares what backs it? However, if you're saying that you disagree with end users having to write their own FFI interfaces to C code then agreed. I think there's an important distinction between the two. &gt; for honesty's sake, all shootout contributions that feature Foreign libraries should be rewritten. they are as much of a cheat as the php contributions that lean on c libs. i really wish the shootout was tightened up in this regard As above, I don't see this as cheating. It's a measure of how performant the language is for the average user. If PHP provides a function which happens to wrap a C lib, then that's an accurate measure of the performance that PHP offers you.
This, 100%. He references java, hah. I took a college course in 2008. Our professor made us implement one relatively simple function in java, which had an equivalent in matlab standard library, then we spent the other 4/5ths of the semester improving its efficiency. Actually, after the first 2 weeks of improving it, we covered all the "sane" improvements, and the students all gave up on making their own improvements, but the prof would have a new version each week, and each week he would spend half the class explaining crazy arcane crap on how the new version was faster. It was truly amazing that this went on week after week after week, until the last lecture, it reached parity with the matlab version.
Update this wiki page plz, http://www.haskell.org/haskellwiki/GHCi_in_colour 
But it's not necessarily an optimisation, which is why it's not the default. That's the idea behind tobbe's `-funbox-strict-primitive-fields`. The conjecture is that those types are more likly to be an optimisation in more cases, enough so that it's worth unboxing them all.
Could we have an RSS feed of new tasks? Great idea though! I mostly ask for RSS so I can subscribe and remain 'interested' while you add more features. I'm sure when an interesting task is added, I'll click it from my RSS reader and then will see when you add more features like +1 or comments.
&gt; Why would you add strictness to something you know should be lazy? I wouldn't, but I've met a couple new Haskellers who take the "make it strict for performace" articles out of context and think that if they just "add bang patterns everywhere" their code will go faster. As long as the user understands *why* they want strictness, I have no issue at all with `seq` (or, more often, `$!`).
But isn't that what GHC does? Main is expanded to Main.hs by GHC, read, checked, compiled and the output file (which happen to also be the input file) is then overridden.
Yup, haven't decided on mark-up yet, but I've just made it preserve lines at least. Markdown is probably OK for everyone, I'd imagine?
I think markdown sucks, but duno if there's anything better, it's ubiquitous and everyone else seems to love it.
Is this written in Ruby on Rails?
I habitually request OpenID/FacebookConnect for new little websites like this. Sure, there's LastPass, but it just feels better to "log in with Google" or whatever.
[My feelings too.](http://wiki.chrisdone.com/Markdown)
It's written in Haskell on the server and Haskell on the client. Maybe you can call it Haskell on Everything.
That's the safety it keeps from earlier DF. The extra safety it adds is the "proofs" that lets you track validations in the types, in a reusable and composable way.
Russian, but yeah, space is required. EDIT: Turns out i'm illiterate in my own native tongue. Space is not required before question mark in Russian.
Perfect, thank you!
Just out of curiosity, what are your objections to markdown? I don't really know much about markdown or any other mark-up languages other than TeX. Perhaps if the problems are laid out, a solution may be clear.
Hm, I might blog about it.
for some reason GHC gets confused if the very first string in the `.hs` file is `import`; try adding anything, such as a newline or `module Main where` and it works as expected...
Who told you that?
Ни фига себе! Не знал :))) Всю жизнь ставил пробел.
+1. Maybe x-reference SoC list http://hackage.haskell.org/trac/summer-of-code/report/1
Nice. Which Haskell to Javascript compiler did you use?
Please do, I'd be interested in reading a serious comparison and analysis of the different mark-up languages.
As a devout Stalinist, I can tell you that I have not yet downvoted you, at least.
&gt; they just "add bang patterns everywhere" their code will go faster. Beginners can't do effective optimizations, in general, since they can't reason about the computation model, machine model costs, nor the compiler transformations involved. They just guess. But note: there's even a tool to tell you when your [strictness annotations have no effect](http://hackage.haskell.org/package/strictify). 
Wait for it... Actually, if you're serious about being a Stalinist, I would be curious to hear what you find appealing about Stalinism.
Digging through the comments, I think we have here a prime example of why thinking of "a monad" as a noun, rather than an adjective, and using it in that place in an English sentence, is a mistake. It's an interface, with a set of implementations. This whole post is an argument that a particular concrete implementation (Actors) is better than an interface (Monad), which is just gibberish, logically. "A monad" has no referent.
I would love to read something like this.
"Monadic IO" "Monadic state" "Monadic parsing" Sounds right to me.
You'll probably want to wait for donri's blog, but one (maybe minor) thing about markdown that really leaves a bad taste in my mouth is that it semantically differentiates between 1) zero or one trailing spaces, and 2) two or more trailing spaces, on a line. That's just ridiculous. Trailing space shouldn't affect the semantics of any language, and even if it does, the language should surely at least put the split between "no trailing space" and "some trailing space", not "at most one trailing space" and "multiple trailing spaces"... I personally use [reStructuredText](http://docutils.sourceforge.net/rst.html) since it's pretty popular in Python world for documentation (though really it's great for a lot of things).
I prefer qualified imports because otherwise people put a qualification inside the name itself which pollutes lots of names. It's nicer to have: import qualified Control.Monad as M M.for M.map M.filter Than: import Control.Monad forM .. mapM .. filterM The qualification is useful - and once you think of these little prefixes/suffixes as qualifications you see them everywhere. I use unqualified imports to avoid double-qualification: import Data.Map (Map) import qualified Data.Map as Map So that the double-qualified name Map.Map does not have to be used. It would still be great to use a tool to fix the imports, especially something that can do so interactively, in the IDE. Perhaps a tool could interactively suggest a few qualifications for unimported names and add the qualifiers.
Added. 
Nah, just kidding :)
Missing code examples, but it sounds like it is on the right track.
Not my code, but an interesting read :)
&gt; you should put the on accumulators in your recursive functions Minor amendment: only do so when actual work can be done. For example, if your accumulator is a function and you're just composing a bunch of them, strictness just forces the allocator to do more work.
Learning the STG is immensely helpful for understanding memory usage and how to optimize GHC Haskell code. Just peeling everything away and saying: this. this is the assembly we're working with. these are our primitive representations. this is how function invocation is actually executed. this is how laziness is implemented. this is how automatic un/currying is made efficient. These are the sorts of things they teach you in any decent non-intro course on Java or C/C++. This is equivalent to learning about object representations and vtables and stack discipline and all that. They teach all this stuff as a matter of course, usually in second-year courses. The problem is, they don't teach you that these things are C/C++/Java-specific, and they don't teach you that this kind of knowledge is something you need to seek out and teach yourself when you learn a new language. The best overview paper on STG is somewhat out of date, but that's not the point. It's not that the exact details of the STG are crucial, it's that it's crucial to get the general picture of how it all works. The computational model of the STG is radically different from the computational model of languages like Scheme and ML, let alone C and Java. And learning about the STG gives you a really good idea what this alien world is really like. It's that peeling away of misconceptions that's the most crucial aspect to reading the papers. (Unless you're a compiler writer, in which case there are some other nice tidbits to take away too.)
Just so you know: the "yesterday" link is going to `http://0.0.0.0:8000/posts/[...]`. You probably want to fix that.
See my response to winterkoninkje. &gt; If you can't answer that, you don't have "a monad" at all. If you can answer that, you have an instance of some concrete type that you've implemented the interface on. There's no in between. So if I understand correctly, you're pointing out that there's a lot of room for confusion when people say X is a monad. Some think that X is "naturally" endowed with a monad implementation. Generally, that's not true. But there are those who say X is a monad with a particular implementation in mind. The latter is what I'm referring to. Zooming out a bit, given that I don't do OO very much these days -- why would someone want to instantiate as opposed to implement an OO interface? Because "everything is an object" (including interfaces, presumably)? I can't wrap my head around that mental glitch.
So I guess my confusion with what /u/jerf is saying is that he's using "noun" to mean OO-object, right? An interface is not an object, an object is a noun, a monad is an interface, ergo a monad is not a noun!
I see links were added to options for some other frameworks. Thanks! Interesting that you omitted [digestive-functors-yesod](https://github.com/softmechanics/digestive-functors-yesod). That's not really surprising, since the the yesod version is not on hackage. It appears that it may not even be under active development. It looks like digestive functors never really caught on for yesod.
Thanks, that is very helpful. Since this is a numbered series, could you also please add some way to navigate between the posts? Previous/next, or a link to a TOC that you update as you go along, or whatever. This series is a nice idea!
Let's try harder and play devil's advocacy a bit. (I'm a bit hampered since I haven't done OO in ages and am unfamiliar with the Actors literature.) As I see them, the bigger questions are, 1. how do we reason about IO? 1. how does a PL become "practical"? 1. what is intuitive? what is non-intuitive? how do we know? 1. is Category Theory overrated "as a tool for programming and programming languages"? 
How about filing a bug report? 
Actually, I proved (formally, in Agda) that my version of free applicatives (which is just like this one, but without the third constructor) does indeed obey all the laws, if you assume parametricity.
No. (But I understand where you're coming from. ;)
Harper uses it throughout his blog, I don't remember reading a formal definition, but it seems to amount to localised-effects which pretty-much-preserve the same behaviour as if there were no effects, from the perspective of the containing (possibly parallel) code. E.g. a self-updating data structure, something like the ST monad, stuff like that.
It seems to be a huge irony that in this post he proposes a certain sort of safety through "dynamic classification" but completely rejects the safety benefits Haskell provides through effect typing. But I admit I always find it hard to understand what he is trying to communicate.
A formal definition would be nice because it seem to be key to his entire argument.
As far as I have been able to discern, his objection to typed effects is that it makes it syntactically clumsy to write things that use "benign" effects. Especially since the end result is usually changing your types around, and then switching between binding and simple name assignment. (The fact that the effect type is monadic really isn't as important as the fact that effects are typed, and that effectful actions have *different* types from the values they produce.) I don't buy this argument, though. "A good programmer will know what effects are benign, and not use the other ones incorrectly" reads a lot like "a good programmer will always free manually-allocated memory exactly once". The purpose of programming languages is to make the creation of correct software easier. In my experience, typing effects, even as primitively as IO does in Haskell, is a valuable tool for statically elimination certain classes of bugs.
this new book of his might be a good source: [Practical Foundations for Programming Languages](https://existentialtype.wordpress.com/2012/12/03/pfpl-is-out/)
Thanks. A short chapter on "Benign Effects" begins on p362. As far as I can understand the presentation is not formal, but seems to confirm the intuition of others that a benign effect is an effect that is not observable from within the program. 
I agree, my implication about what you wrote was not quite right.
Yeah, thanks for this. I've been using postgresql-simple for a long while now, but I never bothered looking into digestive functors properly until this blog post. I guess all I needed was a reminder ;)
&gt; Many parts of the program can hold references to the old tree at the moment it is rebalanced. Wait, how does this work? So you've got parts of the original tree referenced in other places but you're destructively mutating it at the same time? You'd end up with a wrong program! 
Thinking you can instantiate an interface or an abstract class (depending on local terminology) is a common OO error, that's all, and it's very, very nearly the same error. It's not that you'd "want" to, it's a common error to think that you can. When someone tries that, they are not trying to get a handle on the interface itself, they are thinking they are going to get some sort of real object out of it that is an instance of class IEnumerable. The people who do this are nowhere near a level of understanding in which they might want to have a reference to the Interface itself and go do something with it (perhaps dynamically create a new specification containing two interfaces or something). You shouldn't say "X is a monad", you really ought to say "X has a Monad implementation" or "X is monadic" (in the same way would would say "X is red" rather than "X is a red"). Yes, I am well aware that we can define "X is a monad" to mean those things and it is perfectly sensible. Definitions are our slaves, not our masters, and if that's what we want it to mean, it does. What I am observing is that using the phrase "X is a monad" to mean "X has an implementation of the Monad interface" _confuses the hell out of people_ who don't already understand what's going on, and that's a valid reason to not want to use that definition. And in the first sentence of this paragraph, I'm saying what I _believe_, not the general community sense (though I hope it becomes so) or a mathematical truth (certainly not!). The other English construct that might be used in this situation would be the -able suffix; "X is an Enumerable". That said, I've noticed in teaching OO that _that_ formulation still leads to confusion in newcomers, too. And who wants to say "X is a Monadable"? I'm obviously not saying there's no such thing as monadic things. I'm saying, you can't have "a monad" any more than you can have "a red". You can have a red car, a red marker, a red _thing_, but you can't have "a red". You can have a monadic list, a monadic state, a monadic IO value, but you can't have "a monadic". The type system _can_ have a slot that fits "a monadic ..." (`Monad m =&gt; a -&gt; m b`), but you have to fill that slot with "a monadic Thing" to actually compute with it.
&gt; empty rhetoric I didn't say "empty rhetoric".
Almost entirely Ocaml code, but I think it's helpful for the community here to see some denotational semantics since it's solidly embedded in haskell's roots, if not in haskell itself (arguable). Plus, totality is purer than partiality! 
No: &gt; the API for accessing the tree does not reveal the difference between the new tree and the old one The type is abstract, you won't be able to observe any difference with the API.
&gt; and some terms seemingly made up or obscure like “benign effects”. It's really tricky to read his articles. I sometimes cannot figure out who the audience is, or if it's some published internal monologue. I'm not trying to score any points here, just pointing out that the cognitive dizziness is similar to outside folks reading monad tutorials and category-theoretic haskell-coded blog posts.
As I understand this post, Robert Harper wants to be able to use exceptions as an extra channel for higher order functions. For example foo = try (map bar xs) catch (\ErrorTwo -&gt; there_is_a_two) where bar 2 = throw ErrorTwo bar x = stuff x But Haskell already has a better mechanism for handling these type of extra channels: monads. In this case `foo` would use `mapM` with the `Either` monad. And functions that have the type `Monad m =&gt; m ..` are guaranteed not to mess with the side channels.
"benign effects" are compositions of effects that don't have externally visible semantic changes. For example, `(+) :: Int -&gt; Int -&gt; Int` uses effects behind the scenes, but the composition of effects carefully hides that so that it has the semantics of a pure function. "unsafePerformIO" is really behind pretty much all our pure operations on a modern imperative computer. In short, any composition of IO actions which is safe to use `unsafePerformIO` on is a benign effect. One example is "ugly-memo". The effect is only externally visible through change of performance characteristics but it does not affect semantics.
I guess Bob's use of exceptions is different from most Haskell code I've seen. Exceptions are rarely used in Haskell; if they happen it's really because there's some serious error, and not part of normal control flow. So given how little exception handling there is in Haskell I don't really care if it doesn't compose properly if Haskell had functors. That said, he raises a valid point that should be considered when designing languages.
As long as it's thread-safe
&gt; But Haskell already has a better mechanism for handling these type of extra channels: monads. Which is part of why I don't understand the presence of Exception in Haskell at all. We have a better solution.
The story I've always been told is that in MLs, exceptions are fast compared to functional desugarings of the same control flow, and so leaning on them is an optimization hack? In Haskell, on the other hand, function invocation is relatively cheap and exceptions are relatively expensive, so it just tends to lend itself to a different style. For exceptions-as-control-flow we of course just have the `Either` monad or the like... Arguably this is more readable in that we can build a special `Exit` monad, etc. which has functions corresponding to the semantics we're actually taking advantage of. I suppose early exit is less an issue when we're able to just traverse infinite structures until we get what we want as well -- i.e. we move control flow directly into data.
This works great so long as you have one exception and one handler. But when you have multiple exceptions and different handlers for each one, it gets awkward. You could use successive layers of EitherT to get as many channels as you need, but then each handler needs to be lifted to the proper level to receive its errors. Or you can use one channel for everything, but then you run into the problem Robert Harper's talking about: if you're not careful, the wrong handler could catch a given exception. Either way, you need to think carefully about the global behavior of your code; you can't just divide the problem up into small bits to be solved one at a time. I wouldn't give up monadic IO for this---not even close! But it does seem like there ought to be a better way.
The ST monad can be used to operate destructively on data in the scope of the function that created the data. The mutable reference can not leak outside of the scope of the ST monad, which is why is it safe to use runST. runST :: (forall s. ST s a) -&gt; a
I'm talking about Exceptions again! I've been having some worries recently WRT people using custom exception types with the GHC exception system. I don't want to catch `SomeException` because that will catch "uncatchable" things that should (almost) never be caught. Then I realised, I may not know what all the exceptions there could be are, but I *do* know which ones I *don't* want to catch. This code re-throws them.
Isn't this what ST is for? Externally invisible mutation?
I think you certainly could, in principle. I'm just not sure whether the result would be fun to work with. My perspective may be a bit warped by what I'm working on at the moment, though. I'm currently dealing with two separate MonadIO stacks, plus STM. It's not awful, but it's not exactly trivial, either.
Do bear in mind that laziness ---which some would argue is crucial to Haskell, as opposed to ML--- requires destructive updates. That's part of the semantic content of laziness. The "purity" of laziness is only because the compiler/RTS is the one doing the mutations, not us fallible humans. Assuming the compiler/RTS is correct, then we can't observe the fact that these mutations are happening all the time (without diving into IO).
The destruction is, I presume, to ensure that others holding references to your tree also get the benefit of the rebalancing. It's sorta like `become` in SmallTalk, or like what happens when your force a thunk in Haskell. The problem, of course, is that you need to ensure that you don't mess up anyone with concurrent access to the structure. Thunk forcing has this exact problem too, which is why we BLACKHOLE things during evaluation. You can always fall back to using an MVar or similar, but then you're into monad land which can require extensive restructuring of code. It'd be awesome if we could hack GHC to allow user-specified semantics-preserving destructive updates. Trick is, how do we ensure semantics preservation? "Trust me" doesn't seem like a very good approach... And unlike other places where Haskell compromises on purity, I don't see this as a big enough problem to warrant sacrificing for the sake of pragmatism.
Right, I now think I see the issue.
I chose haskell over SML specifically because of the community. Though OCaml also has one, and is more similar to SML in many cases.
Why not balance it preemptively, rather than spread it around and then worry about it being imbalanced in other hands?
I think the headline of this reddit post should have been "Robert Harper quotes Edward Yang as support for one of his attacks on Haskell".
I'd get some good use out of it- just starting to learn the language and a paper reference can be nice in addition to the online stuff. But if not, not biggie. 
He is also claiming the *fundamental idea* of exceptions is transmitting secure information between pieces of code such that noone else can access that information. That may be an interesting use case, but claiming that is primarily what exceptions are for elicits a big "wat".
The problem is that it gets tedious very fast. How do you compose: `EitherT SomeErrorSum IO a` with: `(a -&gt; EitherT SomeOtherErrorSum IO b)`? You can't do it monadically. If you do it with some sort of indexed monads, after a few binds, you're going to get a difficult type sum to work with there. There is a [type-class based approach](http://hackage.haskell.org/package/control-monad-exception) but I don't know how well it works in practice.
There are a couple of learning Haskell subreddits, but they look empty. Seems like I'll find someone here who will make use of it.
Actually, the fact that the effect type can be *any* monad has some effect. If it could only be free monads, it *would* be easier to compose (since coproducts exist for free monads).
Indeed, while I dislike the (pervasive) dynamic typing, my bigger concern is the invisibility. Pure code throwing exceptions that I can only handle when I get back to the `IO` is an example of that invisibility. Certain uses of the dynamic typing (such as not knowing what sorts of error might come out of any given action) are reasons I don't prefer this solution in general, it's just one that works if you really feel you need it.
Top comment? Damn. I thought you meant first comment. Now I have to write something cool. Okay, my daughter who is 9, loves reading funny computer programming books. One of her favorite books is "Head First into Design Patterns." Her other favorite book is "Land of Lisp". Thanks to her reading these books, she has developed better abstracting skills. She also makes lambda jokes. Your upvoting this comment will allow a sweet 9-year-old girl to learn function application, functors, and monads. Do it for the kids. Specifically, mine, but you get the point. Edit: documentation that my daughter and the two books exist [picture of daughter with two land of lisp books](http://i89.photobucket.com/albums/k239/hugoestr/sheExists.jpg) 
&gt; The fundamental idea of exceptions is to transfer a value from the raiser to the handler _without the possibility of interception by another party_. I think it's the use of the word "fundamental" here that turns this from an unusual perspective to nonsense.
Upvote for the kids!
Would you be interested in a copy of Land of Lisp for your shelf too? I have an extra one. I will send you the nicer one; the beaten up one has sentimental value :)
What new exception type would I not want to catch? I only don't want to catch things that are not really runtime errors, but programming errors (like assertions and `error` calls and pattern match failures. failed invariants). And, like I said, the reason I in particular want it (usually) is to move exception-using code into `EitherT`
Thank you so much! I worry a little bit that some of these posts are a little low quality, dull or otherwise spamming /r/haskell - but they seem to be going down nicely. I'll try and keep it up!
Thanks, I got that fixed :)
Ah, I didn't know about that library. I actually just searched for reverse dependencies to `digestive-functors` on Hackage and enumerated everything that was there. It does look like this one has sadly been abandoned.
These are great. Of course, I'm biased since you just praised my library. :)
Meh.
I wasn't aware of that package; seems like something I could have used.
Over at Google+, His Royal Pumpkinness mentioned that the `MarketRequest` type I came up with to turn `MarketData` into a functor is actually the dual of the Yoneda lemma. Which sound like something I could brag about. One of these days, I'll have to learn myself a category theory for great good...
I'd also agree that the vast majority of Haskell code is *eventually* hitting calls to `unsafePerformIO`. Mostly through `ByteString` and `Text`. But it's one thing to say that `unsafePerformIO` is needed somewhere in the call tree, and quite another to say that Haskell programmers regularly need to think about it.
Can you give a *simple* example of how this makes things easier to compose?
It's used for the Isabelle theorem prover (which is _horrible_ code), that's my only experience with it. Every time I look at that SML I want to run back to Haskell.
I've used hand-written typeable instances to fool the type checker intentionally in the past. It's a shame that that stuff will no longer work (I did this when interpreting some haskell code via hint).
You win. Most votes, but the father/daughter seals it. Send me your details.
Awesome, I like how this worked out. I too have a paper copy of Land of Lisp to pass along if there are any takers here.
Can't you use unsafeCoerce for this?
&gt; "1 is a number" ~= "red is a color" ??? I thought we're discussing "1 is a number" ~ "State is a monad" ~ "The ball is a red" versus "1 is numeric" ~ "State is monadic" ~ "The ball is red"? I'm not championing math-speak either, one way or another. Actually I'm kinda opposed. Case in point: In Coursera's 100k+ strong Stanford-based Intro to Math Thinking students are asked to recognize how deeply flawed this is: "A driver is involved in an accident every 6 seconds." Not me, the English is fine! 
No, because hint handles the coerce with typeable.
I chose Haskell over Ocaml because it was more unlike anything I knew before. That turned out to be wrong, because Haskell is way math-y. I should go learn SML now, the Harper "mechanically verified" version, that is.
I'm sure there's still an evil way to unsafeCoerce your way out of the situation, but I'd need more details to decide just *what* evil way.
&gt; so there are a plethora of ways to evaluate Haskell programs. Yes! This can't be explained enough to beginners who confuse non-strict with laziness. The goal is to leverage this flexibility into capturing multicore and cloud computing domains. 
It's yours if you want it. I already read it and have the ebook if I want to re-read. If someone wants it, then it avoids recycling.
I just logged in just to say this. As someone who is in the middle of reading LYAH ( on the chapter about Applicative Functors right now ) your posts have been some of the most useful things I have seen. Please don't stop writing these articles, since I think that more things like this ( and Real World Haskell, which I plan to read once I'm done with LYAH ) will be extremely useful to people trying to pick up Haskell. I'm not saying I completely understand everything you've written so far, but as I got about reading more I expect to use these 24 posts as the basis for writing useful things with Haskell.
If you really want to drive yourself crazy, just spend any amount of time trying to make your program safe against asynchronous exceptions. It's possible, but the current asynchronous exception paradigm is very brittle and non-compositional and contaminates everything it touches, even worse than `IO`. For example, the `errors` library only really protects against synchronous exceptions. The following code is not safe against asynchronous exceptions: do scriptIO m1 scriptIO m2 An asynchronous exception could occur between `m1` and `m2` in the middle of the code for the `IO` monad's `(&gt;&gt;)`. Creating any sort of asynchronous exception safe abstraction requires exposing all sorts of brittle implementation details to ensure that masking always occurs in the right place. So a good rule of thumb is just to never use any library that uses asynchronous exceptions, ever.
 tellme lovely :: String tellme _ = "I was born to make you happy" int main(:: IO*()) { do $ System.out.print $ "I don't know how to live without your love"); echo "Cause your the only one within my heart"; read you $you | espeak *((void *) me)-&gt;touch me espeak $ tellme "somethin" } Always and forever you and me That's the way our life should be I don't know how to live without your love I was born to make you happy 
I forgot what the arguments were for $ being right / left associative. Could you repeat them here?
Scratch is great as an introduction. It takes care of the basic concepts such as commands, sequential order of commands, and loops. It also teaches the basics of data hiding, variables, and events/message passing. Some people dismiss it because you don't type. I believe that is a strength. Typing too much can discourage small children. With Scratch is is just drag and drop. There is also a comic book tutorial book on Scratch called "Super Scratch Programming Adventure." It is pretty good. Once your daughters can type more, you can try MS Small Basic. You do type, but there is intellisense, so there is less typing in involved. For trying other programs, get the source code and let them modify that. 
An exception I "wasn't expecting" has two recoverable cases: * When I'm changing the format of stuff in a generic way so that I can mix with nicer functions, such as moving to EitherT. * When any failure means the same thing (such as, "HTTP fetch failed") and I can report to the user appropriately (instead of crashing on an unhandled exception...)
&gt; When I'm changing the format of stuff in a generic way so that I can mix with nicer functions, such as moving to EitherT. Well, why not just put *all* of the exceptions into EitherT? I mean, no matter what you are going to have to figure out what to do with them, so it doesn't matter whether you apply a filter to them before stuffing them into EitherT or whether you let them all in and wait until you examine them later to decide what to do (with the possibly exception of AsyncException which should be propagated immediately). &gt; When any failure means the same thing (such as, "HTTP fetch failed") and I can report to the user appropriately (instead of crashing on an unhandled exception...) I think that if you are assuming that there is such a case as where all exceptions mean the same thing then the problem is with your assumptions. If you are expecting the possibility of an HTTP fetch failing, then you should be able to know in advance what exceptions might be thrown and catch only those. Anything that you weren't expecting is by definition something you don't know how to recover from and so should be propagated upward. In fact, the exception system makes it really easy for you to do this. You could implement a new type, HTTPFetchFailure, and create an instance of Exception that automatically wraps the exceptions that would result from an HTTP fetch failing into this type, so you could just catch HTTPFetchFailure. It seems to me that part of the problem here is that the error type you are using is of type SomeException which is completely open, when it sounds like you would actually like a datatype that has a closed set of possibilities so that you know what to expect when an error occurs.
System F is good for Rank2Types, RankNTypes, ExistentialQuantification, PolymorphicComponents, and ImpredicativeTypes (though the latter two are more about issues/questions raised by System F, rather than being part of F itself). However, System F is no good for GADTs. GADTs are halfway along the axis towards dependent types (LF, MLTT, ITT, OTT, CIC,...) so you need to either use a system with full-blown dependent types, or a system designed for GADTs such as System FC (what GHC uses). Don't confuse F, Fomega, and FC; they're all quite different. The only similarities are that they take System F as a subset. Indeed, it would be good to have a text aimed at teaching this sort of thing. Unfortunately, there's a balancing act between teaching Haskell vs teaching the whole of type theory, programming language theory, or category theory. While you don't need to know all the high points of TT/PLT/CT to understand Haskell, knowing TT/PLT/CT really helps if you want to wring Haskell dry and write programs like Oleg, ccshan, and ekmett. There comes a point when Haskell is no longer the goal, it's just a means to an end for doing other things. Haskell is full of theoretical engineers (or engineering theoreticians), so the division between engineering work and theoretical work is seldom marked with signposts. *We* know which hat we're wearing at any given point. But we rarely feel the need to point it out, and so newcomers to Haskell often have difficulty distinguishing the theory from the Haskell. There are lots of good books for introducing the theory. The question is: how to introduce Haskell with (a) minimal assumptions about prior knowledge, but (b) minimal repetition of what's in theory books.
Of course. But the point remains: GHC (and Hugs, and presumably UHC, jhc, yhc,...) implements the semantics by laziness, and laziness relies on side effects like pervasive mutation.
&gt; It seems to me that part of the problem here is that the error type you are using is of type SomeException which is completely open, when it sounds like you would actually like a datatype that has a closed set of possibilities so that you know what to expect when an error occurs. Indeed, and in all my actual code that is what I use. The problem is that other people's libraries (especially base libraries) will throw dynamically-typed exceptions and I cannot stop them, so I need some way to deal with that at all.
&gt; If you are expecting the possibility of an HTTP fetch failing, then you should be able to know in advance what exceptions might be thrown and catch only those. That would be nice. Unfortunately nothing about the function type gives me this information, so in the best case I hope they're all documented and in the worst case I have to grep the code of the library. But if any non-error exception is thrown by a "get HTTP" function, I can know that whatever else that might mean specifically, it *definitely* means the HTTP fetch failed. I mean, it also failed in case of an error exception, but catching assertion failures is not a great idea.
&gt; That would be nice. Unfortunately nothing about the function type gives me this information, so in the best case I hope they're all documented and in the worst case I have to grep the code of the library. I agree that it is a problem that you cannot know what exceptions a function might throw unless they are documented.
[I agree](http://www.reddit.com/r/haskell/comments/1499nq/robert_harper_exceptions_are_shared_secrets_a/c7bjqlk) that in practice, Haskell is implemented with laziness.
Remind me again where there's a good list of introductory-type bugs I could use to get my feet wet in GHC-land?
The wiki page on [Joining in](http://hackage.haskell.org/trac/ghc/wiki/GettingStarted) suggests two lists, one for [bugs](http://hackage.haskell.org/trac/ghc/query?status=new&amp;status=assigned&amp;status=reopened&amp;type=bug&amp;order=priority&amp;group=difficulty) and one for [tasks](http://hackage.haskell.org/trac/ghc/query?status=new&amp;status=assigned&amp;status=reopened&amp;type=task&amp;order=priority&amp;group=difficulty).
For some unknown reason reading this almost brought me to tears.
Is your code for grading student submissions in Haskell posted online anywhere? Can it be? I bet I'd find it a decent intro into hint, and a good resource if I need to TA a Haskell course again.
I imagine he probably needs a change of pace and new challenges to work on. He's spent a significant chunk of his life on GHC and a great mind like his needs diversity, too.
I think it looks great in Idris sans the characters I don't have on my keyboard.
I would prefer something more structured and permanent like blog posts or a HaskellWiki entry teaching how to hack on GHC. Otherwise, people who miss the webcasts or IRC sessions are out of luck. I would be pretty embarrassed for the community if I had to refer somebody to a stale IRC log to teach them how to properly hack on GHC.
Asynchronous exceptions are the problem. With synchronous exceptions I can at least restrict catching to `IO` and then decouple exception-handling from my implementation, but with asynchronous exceptions I have to bake `mask` into my implementation to ensure that I don't get asynchronous exceptions during the pure code segments. Honestly, I'd prefer if pure code were masked by default and you would only receive them during `IO` actions. In other words, I'd prefer if all exceptions were synchronous (in the sense that they are restricted to `IO`-land) and pure code was really pure. It would make a lot more abstractions composable and exception-safe.
Clearly we need someone to formalize and extract our Core definition from Coq! Oh, wait, it seems Adam Megacz [did just that](http://www.cs.berkeley.edu/~megacz/garrows/) :D (More precisely he made a representation of Core in Coq, wrote a compiler pass in Coq, then extracted it and plugged it back into GHC. This is a lot like the Verified-LLVM project called Vellvm, which works on very similar principles.) I don't think it's updated for FC with equality proofs, kind polymorphism or constraints, but it's a crazy interesting project nonetheless.
What I had in mind was something more structured. We need pictures with circles and arrows and a paragraph on the back of each one explaining what each one was. Guthrie style documentation, if you will.
something like http://hackage.haskell.org/trac/ghc/wiki/AboutVideos ?
[Previous discussion](http://www.reddit.com/r/haskell/comments/143wpd/static_analysis_with_applicatives/)
PHP code? ... :/ edit: I know FB isn't a PHP only shop, but still wondering what he'll be doing there..
I was/am very excited about visi. And certainly the community could have used someone like David. I first met him when he presented Visi at the BAHUG. Visi is exactly the tool a machine learning developer wants. Kind of an excel on super steroids.
It seems like the author has some interesting ideas, but IMO so far he has not really communicated them in a way that the rest of the FP community can understand what he is talking about and engage with them. I would love to see a series of complete examples, with type signatures, discussion, and working code.
@ocharles there is a mismatch in example code and resutl: And now if we try: fmap (map toUpper) getLine -- should be: fmap head getLine? HLint rightfully suggests: ocharles.org.uk/foo.hs:3:8: Warning: Use &lt;$&gt; Found: fmap head getLine Why not: head &lt;$&gt; getLine 
I don't understand. Catching is always in IO. You *should* get asynchronous exceptions during pure code segments. That's the entire point! Disallowing them is doing it wrong. As long as your pure segments are wrapped in a proper catch, it's fine. All you really should be masking are IO cleanup actions that must always occur in response to other actual exceptional conditions.
"Harpers gonna harp" got screams of laughter from me!
&gt; He's a genius, any software company would love to have him. The better question is what on earth does Simon want from Facebook? "Money" and "the feeling that your code is being used by hundreds of millions of people" would be a good start. 
Sure, but it seems like he could get both of those things at places that would also let him work on far more interesting code. Of all the "big internet" companies (Google, Microsoft, Apple, Twitter, Netflix, etc), Facebook does the least amount of interesting computer science. Maybe Facebook is planing to have him head some new project, though, who knows? We can only speculate.
&gt; Facebook does the least amount of interesting computer science. Not sure what makes you think that. Any company dealing with hundreds of millions of records of data, any data, is facing tough challenges that must be fascinating to work on. 
&gt; I guess Bob's use of exceptions is different from most Haskell code I've seen. A real programmer can write ML in any language.
I'd have to clean it up a great deal, but it may be possible to make public.
I think it is a criticism of the semantics. Haskell's exceptions are not generative. That is all this is about. Haskell (like Ocaml) has only non generative exceptions. Harper thinks that is wrong (and I kind of agree with him). But, to get generative exception requires you to have a pervasive side effect (newsym). ezyang showed that you don't need dynamic classification built into the language at all, you can just as easily build it in a library. My takeaway as a language designer is that the toplevel should be in some monad (exceptions are far from the only reason for this), not that typing effects is a bad thing.
 exception Foo of int fun foo x y = if x &lt; y then raise (Foo x) else x - y - (foo 1 2) handle (Foo x) =&gt; x val it = 1 : int The exn type is an extensible ADT. The exception declaration creates a new constructor of this type, only visible in the context of the surrounding module. The handle clause pattern matches on the exn value, essentially inserting an extra "e =&gt; raise e" clause at the end. 
"The Glasgow Haskell Compiler"[1] is a short tour though it is only an over view and doe not seem to get into the nitty gritty. [1] http://www.aosabook.org/en/ghc.html
Add "in the category of any practical language ever" to my post.
Thank you astute reader! I've got this fixed now, and think I shouldn't be blogging after midnight :)
In this blog post, DP writes that despite the attractive features of Haskell as a language, some road blocks have slowed down the development of Visi : * Cabal (not as good as maven / sbt) * Targeting various plateforms in hard because cross-compilation is hard (as opposed, I suppose, to the JVM bytecode solution) * the "bridge" between Haskell and C/Objective-C is painful (what does "bridge" mean here ?) * non-strictness and lazy evaluation is hard to reason about For these reasons, he has decided to re-implement Visi in Scala. My limited point of view on these points : * Cabal is good enough for many projects, and keeps improving which is very important for the community. Could DP, or somebody else knowledgeable, give examples of what major features maven offers but lack from cabal ? * Targeting multiple platforms could certainly be easier, and a JVM bytecode backend might be a solution. My understanding is that it is technically doable, but performance would probably suffer from the lack of direct TCO support in the JVM. Also, if we add a JVM FFI into the mix, the problem becomes harder. But such a FFI is an other topic, isn't it ? * the Haskell FFI to C is very good. No idea about Objective-C
Yes, I'd love to have this rule in HLint - sadly there are plenty of pieces of code that break if you just do the translation (you can usually fix them up pretty easily, but they still break locally, which is what HLint is all about)
I would like to learn more about this proof. The question "In which sense are 'free applicative functors' applicative?" has been bothering me for a while. In particular, what is meant by "if you assume parametricity"?
Does anyone have some pointers on integrating hlint into your workflow? Some things I'd like some guidance on: * How do you run it? (manually, ghc-mod etc) * is it built into any of the haskell development environments (haskel-mode/emacs specifically)? * Do people integrate in in their build process with cabal? Cheers
I use a .ghci file, like this: http://neilmitchell.blogspot.co.uk/2010/01/using-ghci-files-to-run-projects.html
Yeah, they've already got [lex-pass](https://github.com/facebook/lex-pass) written in Haskell. [Their github page](https://github.com/facebook) shows they're working in plenty of languages. Could be they'd like to have more parts done in Haskell.
Hmm, yeah, that is a bit dodgy... Not sure if that one is reasonable, at the very least it needs to say it only works if it is also a Functor. 
I may need to blog at length comparing the two approaches but for now: `foo1 (foo2 x)` becomes `(foo2 x) &gt;&gt;= foo1` which is also `foo1 =&lt;&lt; (foo2 x)`. 
Shouldn't every Monad instance also be a Functor instance anyway?
There is [this comparison](http://awelonblue.wordpress.com/2011/05/21/comparing-frp-to-rdp/) written by the RDP author... I was not able to make much sense of it. The only information I was able to extract from that article though is that RDP is maybe kind of like arrowized FRP... but different and way better according to the author.
Where can I put custom hint rules? It seems I can make a module and use a CLI switch to use it... but where does hlint search for these files? Also, the docs have an example where you extract hints from a local Util module: if I put hints directly in my code then? That doesn't seem to work...
By this logic all Win32 apps should be written in C++
Which is it preferred to use? I usually use fmap, but I can see why liftM might be viewed as more descriptive as to the intent (though fmap is fairly explicit as it is).
&gt;And certainly the community could have used someone like David I'm not sure his past behaviour in the lift community supports that.
Cool, thanks. Another question, maybe beyond what `hlint` can do, but I have this: warn = case x of {_ | y -&gt; z} ==&gt; select [(y, z)] Which works, but I'd like to make it work for any `case` construct that only pattern matches on `_` -- possible?
I hope the supported Haskell subset gets larger soon; Fay looks very promising but it feels unsatisfying to me coding in Haskell without support for typeclasses (mostly for being able to use the standard typeclasses mentioned in the [Typeclassopedia](http://www.haskell.org/haskellwiki/Typeclassopedia))
But that would mean adding extra constraint to type signature, all the way up.
Could you elaborate?
Yes-and-no: I know the constraint isn't in place, but what I meant is anyone writing some datatype and providing a Monad instance, should add a Functor instance on the go as well.
It didn’t affect me emotionally. But — having spent some time over the past few days wading through nasty academic disputes in an entirely different area — I’m struck by the awesome graciousness of the Haskell community, developed in the template of someone like SPJ. Very much a class act.
HLint is already suggesting the use of `void` in place of `&gt;&gt; return ()`, which has the same problem (introducing a `Functor` constraint).
I find it interesting that people seem to prefer `fmap` to `liftM` and still they use `return` instead of `pure` in monadic code. The reason is probably that `fmap` and `return` are in the Prelude, and the others require imports.
Is there no link back to your blog home, or can I just not find it?
I really don't like all the duplicate functionality. Should I use `liftM2` or `liftA2`, `fmap` or `liftM`, etc. Too many choices. 
I've always been partial to the [haste compiler](https://github.com/valderman/haste-compiler) for exactly this reason. It also comes with the benefit of GHC optimizations and isn't as onerous to setup as ghcjs. Its biggest drawback, when compared to Fay, is that Fay's FFI is dead simple while haste feels unnecessarily complicated and error prone. 
The only way to fix it is to make `Functor` and `Applicative` super-classes of `Monad`. I know that `msgloan` is working on solving this problem.
It really depends on the constraints. If you can get a pure `Functor` constraint, then use `fmap`. However, if you have other `Monad`-specific functions, then you can use `liftM` so that only the `(Monad m)` constraint appears in the type signature and not `(Functor m, Monad m)`, which is slightly uglier. Ideally we will eventually fix the super-class problem so that we can always use `fmap` without reservation.
Emscripten says it supports LLVM to JavaScript compilation. Can we get some tutorials going for compiling Haskell code to LLVM, then LLVM to JavaScript?
I investigated this a while ago. The hard part is compiling the GHC runtime with llvm.
I've never tried it myself, so I'm curious what the problems are for compiling to LLVM.
On the other hand, I've never considered compiling Hugs or Yhc (which IIRC is written in C?) with Emscripten which would immediately be a step-up from Fay feature-wise, and probably not slower. Interesting. **EDIT:** Looks like I *did* try to do use Yhc (for its JS generation) in the past and [had trouble building it.](http://www.haskell.org/haskellwiki/Yhc/Building) There were some arcane problems to do with darcs and the build system that I kind of threw my hands up about and forgot about it. Has anyone managed to build Yhc and if so, how's its JS generation?
York PhD student here. As far as I know Yhc has bit-rotted. My supervisor and I spoke with Neil Mitchell about it at ICFP, and if I remember correctly, he was pretty adamant that getting it back into working condition would be a huge task. Hopefully Neil will see this and confirm/correct me on this. As a research group, we don't do anything with Yhc sadly. I've been trying to see if there's enough interest for a few of us to revive it but right now it seems that the more realistic (and likely smarter) goal would be for us to help with the [Haskell Suite](http://www.nbroberg.se/haskell-suite/) idea presented at the HIW this year. I know that's not exactly what you were asking, but maybe you'll find it relevant. Edit: I said it was the Haskell symposium when it was actually the Haskell Implementor's Workshop.
For now, you can go to http://ocharles.org.uk/blog. I have a to-do task to sort this theme out a bit, and I *might* get chance to look at it over the weekend. For now, I'm focus on writing :)
* liftM* if you don't want to impose a Functor or Applicative constraint in monadic code * fmap and liftA* if you don't already have a Monad constraint and you want to stay as general as possible I'm not saying the situation is any good, but perhaps this list will help you choose.
Go go go! Sadly, travelling from London is a bit out of my way - but if you can make this, I highly suggest checking it out :)
That 404s for me.
`lens` is one of the more awesome libraries I've encountered, and I've only really discovered a small part of it! If you're around, you should go!
Please continue posting videos!
The RTS has quite a bit of assembly magic dealing with stack layout on different platforms. That might be a problem. Also ghc LLVM backend (stuff generate LLVM IR) uses a custom calling convention. I think not all LLVM backend (stuff generate binary) supports it.
One solution is to write a monoid instance for newtype KleisliEndo a = a -&gt; m a, then use the writer monad ;)
Have you seen [IOSpec](http://hackage.haskell.org/package/IOSpec)? It's a formalization of (some of) the IO monad in Haskell. I don't know if it's been machine-verified though.
 Couldn't match expected name `tibbe' with actual name `Tibell'
&gt; To be written. There is a great deal of syntax not yet supported, which I will document. For now it is best to simply try and see if you get an “Unsupported X” compile error or not. It will not accept things that it doesn't support, apart from class and instance declarations, which it ignores entirely. Inspect the compiler source if you are unsure, it is rather simple. From the website.
True. I would also like to do this modification: modifyM :: (Monad m) =&gt; (s -&gt; m s) -&gt; StateT s m () modifyM f = get &gt;&gt;= (lift . f) &gt;&gt;= put
Wow, I hadn't seen this package before! Just yesterday I was writtng some code that maps something into a pair of `Semigroup`s, and then folds it all together. I wonder if I can use `reducers` myself too :)
It will be a great day when all the parts of a full Haskell compiler are packaged in delimited modules, available from Hackage.
I'd rather see the missing features ported to test-framework because I think it is a better base to build on (modular and extensible core, parallel execution...). The automatic test discovery is already covered by test-framework-th so the remaining features missing are the line locations and the value diffing. For those features I have some ideas about how it could be done even better than what HTF provides: * Using just TH and expression quotes, or possibly a QQ with haskell-src-exts, we could provide assertions in the form of plain boolean expressions and more helpful failure messages, for example `let x = 1; y = 2 in [assert| x == y |]` could fail with a message including the line number and the message `failed: 1 == 2`, thus giving you more information about why the test failed, not just which test. * For value diffing I imagine we could use the [data-pprint](http://hackage.haskell.org/package/data-pprint) package together with the [ansi-wl-pprint](http://hackage.haskell.org/package/ansi-wl-pprint) package to produce colorful, pretty-printed diffs that exclude irrelevant parts of the values. Are there any other features in HTF that I've missed, that test-framework doesn't already have?