Heh. On my Uni, once there was some open days promotion, and to show how good it is, they posted % split between female vs male. They had to format % for my department (electricity, electronics, computers), so that it showed 3 values after dot or it would stick out with 3% vs 55% (or more some departments run 60%+ ) &amp;#x200B; So math is doing great job at being accessible and attractive to Women, CS does very purely :( &amp;#x200B; PS As a bonus there is India where something over 40% of majors in CS are female. They put the rest of the world to shame :(
&gt; it's beginning a write to a file path, getting killed, and then never rebuilding that artifact. [...] I don't have hard evidence to back this up But I have: [#14533 - Make GHC more robust against PC crashes by using atomic writes](https://ghc.haskell.org/trac/ghc/ticket/14533)
I'd love a writeup on the topic if you ever get into that.
I have faced a similar problem while writing batch processing using CSV as the input source. Basically, I don't want to abort the entire batch because a few rows failed. In the end, it depends on how you want to present the batch processing results. In my case I wanted to present each row's success / response next to it AND I wanted to parallelize the processing of each row. So, I used `processRow :: (Int, Row) -&gt; AppM (Int, RowStatus)`where the Int is the row's index. In a subsequent refactor, this also allowed me to group together related rows and process them as a single unit. I changed the function to `processRow :: ([Int], RowGroup) -&gt; ([Int], RowStatus)`
I don't have much intuition for them other than what I've written. In order to create the paragraphs using them above, I basically had to work backwards, and wrote notes like c |- exists b. (b - exists a. (b - P(a))) c |- c - exists a. (c - P(a)) c |- c - exists a. (c - P(a)) \/ exists a. (c - P(a)) c - exists a. (c - P(a)) |- c - exists a. (c - P(a)) exists a. c - P(a) |- 0 c - P(a) |- 0 to keep track of each step. The only reason I even got the idea was because I knew the `forall` encoding relied on the "negation-ish" properties of the exponential types, and because I knew `1-` is just as valid of an encoding of negation as `-&gt;0` is. Other than that I pretty much had to work with vague anecdotes, and I wasn't sure it would work until I wrote it up. The only intuitions I have for `a - b` are: * Sorta like a call stack that has an `a` parameter and a `b` return position. * (Curry Howard) true if `a` and not `b`. * Can be roughly encoded as `(a, b -&gt; Void)`... * ... or if in the `Cont r` monad, `(a, b -&gt; r)`. * Sorta like "takes parameter `a`, throws exception `b`". * Functions that take it and never return are the same as functions from `a` to `b`. But those intuitions are very hard to apply when nesting coexponentials like I did, and I don't really have an intuition for why the original worked other than "it's the dual of the `forall` representation". I guess one material that you and /u/agnishom might be interested in is [Declarative Continuations and Categorical Duality](https://pdfs.semanticscholar.org/3a68/6ab25229599b4e68d45f84196cad9cdb3817.pdf), which brings up the concept. But it's a pretty huge read. One thing to note is that you usually think of the exponential as being a "function type", but really the coexponential is just as legitimate a "function type". While exponentials `a^b` correspond to functions `b -&gt; a` because such functions are in 1-1 correspondence with functions `1 -&gt; a^b`, coexponentials `a-b` also correspond to such functions because they are in 1-1 correspondence with functions `a-b -&gt; Void`. I guess that leads to a different intuition for coexponentials: * A function `a -&gt; b` is a thing that consumes coexponentials `a-b`. (The "continuation" of such a coexponential is a function.) This and all the other ones are of course closely related.
As far as I know, there is still some pointer-chasing involved. [Unboxed sum types](https://ghc.haskell.org/trac/ghc/wiki/UnpackedSumTypes) are supposed to help with the situation, but the optimizations haven't been applied to \`Maybe\` yet, and I'm sure there is a good reason for it.
Good catches, thanks! And yeah, this is a great place for such comments --- my email is also good: sandy@sandymaguire.me
The discord channel is harmless. Unlike the functional programming Slack where [some people actively scare off people who are interested in haskell](https://twitter.com/argumatronic/status/1019684717640192000).
Wild stuff. Why are these people not getting kicked out? 
I always enjoy posts about Curry; it's a shame it doesn't pop up more often because it has some really interesting ideas :) One thing that I found interesting about Curry is that it started out with "let's encode free variables into functional programming to get functional-logical programing"; so a lot of earlier code feature things such as `last xs | xs =:= _++[e] = e where e free`. It was later found that tree traversals could be more explicitly and easily done with `?` and `choose` instead; in fact, I remember Sergio telling me he found out you can implement any `where x free` function just by using `?`. It's worth noting that `?`, the choice operator, can actually be defined as: a ? b = a a ? b = b In other words, most curry compilers encode its non-determinism as tree traversals where the tree is possible permutations of a search space. (This is done for very similar reasons as to why Haskell's nondeterminism monads always end up looking like list transformers/monads). Of course, the compiler implementation is always free to implement nondeterminism in a different way (and the kics2 compiler for curry attempts a brave heuristic approach to guessing whether BFS or DFS traversals of the tree will result in finding the answer, as well as the future(?) ability to explore some of the branches in parallel). I always found the idea of replacing `free` with `?` to be fascinating because it's reminiscent of replacing data structure manipulation with functional encodings in order to be able to compose things together more easily. That is: -- given a foldable structure, reduce it down to one item by throwing away the other choices "at random" choose = foldr (?) -- &gt;&gt;&gt; choose [1,2,3,4,5] -- 1 (or...) -- 2 (or...) -- 3 (or...) -- 4 (or...) -- 5 reads much clearer than -- find all solutions to the equation "the list xs is equal to three concatenated lists where one is a singleton list" where the solution is the singleton list choose xs | xs =:= _ ++ [x] ++ _ = x where x free -- &gt;&gt;&gt; choose [1,2,3,4,5] -- 1 (or...) -- 2 (or...) -- 3 (or...) -- 4 (or...) -- 5 In a fun way, it reminds me of how fruitful it was to replace an encoding of lenses as data types with an encoding of lenses as functions; things composed much more naturally and integrated better with the language, even though the encodings were otherwise equivalent. (Most of this is just what I'm recalling from taking Sergio Antoy's class at PSU about a year or so ago)
If you turn off TypeFamilies and leave MonomorphismRestriction on, the error goes away.
I'm also wondering if this might be modelled better using some type-level stuff. Consider the following hypothetical function: workdayDiff :: Workday -&gt; Workday -&gt; Int It only makes sense for this function to work against two `Workday` instances created using the same `AbsentCalendar` instance. Ideally, therefore, this function would not even compile if the two `Workday` values were not "compatible". However, I feel like I might be starting to go down a very deep hole here which will distract me from solving the actual, real-world problem that I'm trying to address.
When you have some parameter that you want to keep unchanged across many recursive calls, it can be useful to save it in a closure by delegating to a helper function, often called `go`: decodeString :: HTree -&gt; String -&gt; String decodeString root = go root where go :: HTree -&gt; String -&gt; String go (Branch l r _) (x:xs) = case x of '0' -&gt; go l xs '1' -&gt; go r xs go (Leaf c) xs = c : go root xs go _ [] = [] Observe that the structure of `go` is basically the same as your original `decodeString`, except that it has access to the root of the tree that was originally passed to the enclosing `decodeString` call, so it can use that to start fresh once it's decoded a character.
`cabal init` default to non-interactive (`-n`), and `--interactive`, (`-i`) is added
`cabal init` defaults to `--is-executable` &amp;#x200B; &amp;#x200B;
`cabal init` pins the ghc version via `with-compiler` in `cabal.project` for executables
&gt; thanks to bidirectional type checking! I'd probably call that thanks to Lennart Augustsson, see [Better type error messages](https://www.youtube.com/watch?v=rdVqQUOvxSU). I implemented it in a simple HM unifying language for a client, and yielded error messages like this: Couldn't unify type: Number From: &lt;interactive&gt;:1:16: ... let x = [ 1.0 , True ... ^^^ With type: Bool From: &lt;interactive&gt;:1:18: ... = [ 1.0 , True ] in ... ^^^^ It tells you exactly where types came from. GHC (as of 8.4.3) still doesn't tell you were types really came from in examples like this: &gt; [1 :: Double, True] &lt;interactive&gt;:125:15: error: • Couldn't match expected type ‘Double’ with actual type ‘Bool’ • In the expression: True In the expression: [1 :: Double, True] In an equation for ‘it’: it = [1 :: Double, True] So Lennart's talk is probably still accurate. There's definitely loads of work that someone could do if they just sat down, like Unison's author, and said "let's make type errors very high quality". Elm's are pretty good too, for the same reason. 
Well, it's true for any frequency distribution if you require that each code has a fixed bit sequence. Arithmetic encoding gets rid of that requirement.
Combined filter and map can be implemented as such: newtype FilterMap f a = FilterMap { runFilterMap :: forall b. (a -&gt; Maybe b) -&gt; f b } instance List f =&gt; List (FilterMap f) where nil = FilterMap (\_ -&gt; nil) 
I think using compact regions have already been shown to help their use-case: * [Mirror(?) of original post](https://meta.tn/a/7e44bfe847ebaea028cf7544b6747e92d5f6362b3426defe269fc5a9a3c82115) * [Original post (down?)](http://fuuzetsu.co.uk/blog/posts/2018-03-03-GHC-compact-regions-for-improved-latency.html)
Interesting, thanks for sharing! I have a question about, for example, `dropErrorLinePosition`: Would it not be more "small and practical" to use a regular expression, or even `drop 3 . splitOn ':',` rather than defining an inline parser for the line errors?
why not both? cabal now supports as of a year ago having init generate both library and executable sections
i'm on the fence here, its a good idea, but it breaks how i'd work since all the defaults are always wrong for me always. we'd probably want to fix up the cabal defaults for init. eg default to including the docs for the fields (we currently dont)
has executable and has library? cabal supports both
I would be in favour of that default as well, especially with the current cabal support for internal libraries, I think this is a better way to structure the code. Currently `cabal -n --is-libandexe` creates a `.cabal` file with both but there is no "library" files created, and the `library` stanza is not internal. IMHO that option should create: myfirstproject.cabal ``` library my-internal-lib exposed-modules: MyLib build-depends: base &gt;=4.9 &amp;&amp; &lt;4.10 hs-source-dirs: src default-language: Haskell2010 executable lib-and-exe main-is: Main.hs build-depends: base &gt;=4.9 &amp;&amp; &lt;4.10, my-internal-lib hs-source-dirs: src default-language: Haskell2010 ``` Directory structure: ``` myfirstproject.cabal Setup.hs src/Main.hs src/MyLib.hs ``` _Note: defaulting to creating a `/src` directory is optional._
Good point, there's the `--is-libandexe` flag, the shorthand of this should probably be `--libandexe`, although this isn't much shorter than the original (compared to the other two) but should be included for consistency.
That's a fair concern, is the lack of comments the only item you'd like to see changed? Personally I would prefer if a `src/` directory was created by default as well (but that's my personal opinion). Going through the other questions, most of them seem to be reasonable defaults (package name taken from `$(dirname)`, BSD3 licence, `Main.hs` for main module, Haskell2010 for language). Also, since there's no plan to remove the fully interactive initialization workflow (you simply need to ask for it with `cabal init -i`), is it really that onerous to ask folks already familiar with the tool to add 3 keystrokes? I completely understand the pain in having to modify a workflow you've become accustomed to, and I'm trying to weigh that overhead with an improved experience for those new to the tool. Additionally, I can imagine this being configurable in `~/.cabal/config`, maybe something like `interactive-init: False` as the default, which could be changed if a user always wants interactive initialization.
Awesome! I got 75 kills, but totally collapsed there at the end.. :-)
Cool! I didn't spend a ton of time on difficulty scaling so it's pretty arbitrary haha. BTW I've really appreciated your posts on recursion schemes! I've been working on compiling a collection of examples ☺️
Can you elaborate on "it didn't work well", including specific error messages or misbehaviour?
 fmap :: (a -&gt; b) -&gt; MyTree a -&gt; MyTree b fmap f (MyTree v t) = Tree (f v) (fmap (fmap f) t) Here you go.
After looking at your comment, I can clearly see the point. That t is a list and I need two fmap on that. Why haven't I thought of that? Thanks a lot!
Absolutely, here is the start of `Symantics` from [Finally Tagless, Partially Evaluated](http://okmij.org/ftp/tagless-final/JFP.pdf) class Symantics f where int :: Int -&gt; f Int bool :: Bool -&gt; f Bool instance Symantics f =&gt; Symantics (Yoneda f) where int :: Int -&gt; Yoneda f Int int n = Yoneda $ \f -&gt; int n `runYoneda` f bool :: Bool -&gt; Yoneda f Bool bool b = Yoneda $ \f -&gt; bool b `runYoneda` f 
Hey! Thanks for the explanation. Can I ask you one more question? Just for the above data type, I'm now extending it to a foldable tree. So far, I have a base case when there is an empty list. And I guess base case seems okay since it just returns an accumulator when it reaches. However, in else case, I think I need to pattern match on each item in v because they are MyTree data type. foldr :: (a -&gt; b -&gt; b) -&gt; b -&gt; MyTree a -&gt; b foldr f acc (MyTree v \[\]) = acc foldr f acc (MyTree v t) = f v (foldr f (foldr f acc t) t) &amp;#x200B; So maybe in second foldr, I guess there should be a pattern match but I don't know if my approach is right. &amp;#x200B; Any idea for this one? &amp;#x200B;
Use `case` and pattern matching, don't use `==`. This is how we 'get' the values passed into the function - We use a technique called 'pattern matching' to "assign" the values present under those constructors to terms in the present scope. I apologize if the following is pedantic but it took me awhile to get this to really sink in when I first started: When you define a type with the `data`, you're defining (at least) two things: One or more constructors, and the name of the type itself. ``` data MyType = Constructor | OtherConstructor | ConstructAn Int ``` In that example, we've defined three functions that all return values of type `MyType` `Constructor` and `OtherConstructor` are functions that have no arguments. Their type signature is like this : `Constructor :: MyType` `ConstructAn` is a different constructor. It needs an argument which we said needs to be an `Int`, so it's a function with a type like this: `ConstructAn :: Int -&gt; MyType` So lets say we want a function with this signature: `MyType -&gt; Int` We can do that as follows: ``` myFunction :: MyType -&gt; Int myFunction Constuctor = 0 myFunction OtherConstuctor = 1 myFunction (ConstructAn n ) = n ``` This uses 'pattern matching' to dispatch behavior based on the constructor we receive in our first argument, and 'destructuring' to assign the argument that passed into the construct `ConstructAn` to a local term, `n`. We know that `n` has to exist and that it must be in scope because our function is taking a value of `MyType` as an argument - We couldn't just get the constructor with no `n`, because the 'naked' version of `ConstructAn` has a type `(Int -&gt; MyType)`, which `myFunction` couldn't accept. For reasons, it's typically idiomatic to use the `case` statement to pattern match in this way, instead of doing it at the function definition, so, if we wanted to be idiomatic, we would define `myFunction` like this: ``` myFunction :: MyType -&gt; Int myFunction a = case a of Constructor -&gt; 0 OtherConstructor -&gt; 1 (ConstructAn n) -&gt; n ``` Whether or not you choose to do it that way is up to you, but I'd personally recommend getting in the habit early. Here is a brief example of how to apply this lesson to `inferType`: ``` inferType :: Expr -&gt; ExprType inferType a = case a of IntLit _ -&gt; ... FloatLit _ -&gt; ... BoolLit _ -&gt; ... OpExpr lExp bop rExp -&gt; ... IfExpr lExp mExp rExp -&gt; ... ``` Since we don't actually need to know what the values for the `&lt;&gt;Lit` types are to see what type they are, we use `_` to denote a term that we're not going to use in the output expression. We do care what values are passed to `OpExpr` and `IfExpr` though, because we need to do 'something' to them to figure out what the ultimate return type for the expression should be. Exactly what you do is the crux of the question here, but of special interest is the instruction: `This function should be implemented recursively...` Which should give you a strong hint about what to do with all of those values with type `Expr`
I think so yes. Unlike a list or a normal vector where each element is boxed which means work for the GC.
How about overloading the other flags a bit, so that specifying `--exe` *and* `--lib` is equivalent to `--is-libandexe`?
That could work too, I don't have strong feelings one way or the other on that. Semi-related, what I would like to see is something like Stack templates working for cabal.
I think trying to get an Enum is a hard ask, because fromEnum and toEnum are typically expected to be O(1) and existing function using Enum probably rely on this. Also it seems easy to get wrong. You might be better off using your present data type along with another data WorkDays = WorkDays AbsentCalendar Day Day with the invariant that the second day is later than the first one and working with these as ranges with holes. There is also the union-find data structure that provides some fast operations for finite disjoint ranges, not sure if those operations would be useful in your case.
You don't want to `foldr f acc t`, because t :: [MyTree a] but f :: (a -&gt; b -&gt; b) `f` only wants to work on values stored in the tree, so you can't use it to fold over a list of trees. Instead, you need to construct a new folding function to use when folding over the list of trees. That is, you will write something of the form foldr f acc (MyTree v t) = f v (foldr _g acc t) And in fact, I encourage you to write exactly that! GHC will tell you what type `_g` needs to be in order for this expression to work. If you do this, GHC will say that _g :: MyTree a -&gt; b -&gt; b and also helpfully point out several functions it knows about whose types involve `a` and `b`. Of particular note is foldr :: (a -&gt; b -&gt; b) -&gt; b -&gt; MyTree a -&gt; b This is very close to the type we want! We can partially apply `f` to take care of the first argument, and then we just need to flip the `b` and `MyTree a` arguments around. If we do this, we end up with: instance Foldable MyTree where foldr f acc (MyTree v []) = acc foldr f acc (MyTree v t) = f v (foldr (flip (foldr f)) acc t) A bit of an inscrutable expression, and you might do well to expand it out for clarity, but with hole-driven development GHC has helped us figure out the only reasonable legal expression of the type we were looking for. But there's still another problem: it doesn't actually produce the correct results! Double check your base case for the empty list. Or, better yet, delete it: you've already got a way to handle lists of any size, including empty, using foldr.
Thanks! I was taking a look at the Stack templates repo (as well as the plan for the new way of providing templates) and I'm wondering/hoping that they could easily work from cabal-install as well as Stack. If so, then it would be great if both \`cabal init\` and \`stack new\` would create the same directory structure by default, something like the following: &amp;#x200B; ./myproject.cabal ./cabal.project ./app/Main.hs ./src/Lib.hs ./tests/LibTest.hs
Thanks! I'm coming to the conclusion that Enum does not make sense for this application.
Good question; I chose the parser to make it more likely that I drop the beginning only if it looks a lot like the expected prefix, so that I don't lose data when it doesn't. For example, `drop 3 . splitOn ":" $ "hello"` is `""`, which isn't what I want. A regex would also do, but the `ReadP` based parser is available in `base` (while regexes aren't) and I find it quite readable.
You might have a look it `summoner`: it allows you to scaffold Haskell projects with `src/` and `app/` directories and much more cool stuff! * https://github.com/kowainik/summoner
I think this might be an effect of `-XMonoLocalBinds` (which is implied by type families), just a guess. See if the same thing happens with just that turned on. I don't know whether the interaction is intentional, probably best to open a ticket on the GHC Trac.
This flag is awesome, thank you!
How do you write`last` with ?
Indeed, the same error shows up when you replace TypeFamilies with MonoLocalBinds (ghc 8.4.4 here).
&gt;It is true that there are some useful cases of multiple type-class constraints that have a common ancestor. An example we mention in the paper is (MonadState m, MonadWriter w m) =&gt; ... where Monad m is the common ancestor (as well as its superclasses). This is the reason why we turned it into a warning instead of an error (until we find a better solution). In this case - why is it not possible to detect if MonadState and MonadWriter share a common ancestor if both dicts are passed explicitly? (Or more generally if the diamond property is satisfied for all passed instances?). I am looking forward to your GHC proposal!
Thank you for writing this up! The more I learn about your goals the more it's becoming clear to me how Stackage puts Haskell light years ahead of every other ecosystem. &gt; There is one requirement for getting a package into Stackage: it must build and pass test cases with all of the other packages in the snapshot. What if a package doesn't have any test suites? What if the test suite depends on packages that aren't in Stackage or require a different snapshot? &gt; Stackage is fully opt-in, and therefore there's only positive pressure to be a part of it, no negative backlash for failing to comply. We've seen some maintainers whom I don't want to name here be indifferent or even hostile to Stackage. What if a maintainer of a popular package doesn't want to opt-in? Does this break Stackage for everyone else?
&gt; I have no idea why you think the 2017 results are so much more robust / protected against this kind of thing. It's just as likely that there is skew in the 2017 results as it is that there is skew in the fixed 2018 result The ratio of stack-to-cabal users seems closer to reality to me in the 2017 report. Everywhere you look people mention almost exclusively only Stack and almost never Cabal and yet the 2018 data wants to make us believe that Cabal is only a bit less popular than Stack in a 654 / 995 ratio? I am very skeptical of that.
Puts it into a whole new category, if you will
&gt; What if a maintainer of a popular package doesn't want to opt-in? IIUC, one does not have to be a maintainer to add a package to Stackage. This is similar to how e.g. Debian works.
Yep, with the exception that updating [a restricted subset of package metadata is allowed](https://github.com/haskell-infra/hackage-trustees/blob/master/revisions-information.md). The tarball itself is, however, never modified.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [haskell-infra/hackage-trustees/.../**revisions-information.md** (master → dd7aae9)](https://github.com/haskell-infra/hackage-trustees/blob/dd7aae93f1171fef358d74d72979a7a80d802b50/revisions-information.md) ---- 
I mean all these packages are typically BSD/MIT licensed anyway. So anyone can do (almost) anything with your code without your consent. The opt-in part I took to mean that you can completely ignore the existence of stack and not be negatively affected / expected to do anything. 
&gt; without the maintainer's consent This is kinda the point of open source. Quoting BSD3: &gt; Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met [...] "fully opt-in" means that if I don't care about Stackage, I don't have to care whether or not it contains a package I maintain.
I will definitely try out your cachix recommendation, thanks for the tip! It also backs up the issue of how much work is needed to use Reflex with the current Haskell ecosystem.
Yeah. I may have the revision/hash wrong, I'll let you know when i get to my computer.
/r/lostredditors? 
I mean it's making it clear that your package won't be removed from hackage or nix or anywhere else just because you are neglecting stackage. I do agree that it is what everybody would have expected. 
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [haskell-CI/hackage-matrix-builder/.../**cabal.project** (master → dfa8e9d)](https://github.com/haskell-CI/hackage-matrix-builder/blob/dfa8e9dbb9f40ec5cec9ca094dce9f648304c58e/src-ui.v3/cabal.project) ---- 
This is borderline issue material. To exit I had to interrupt it, it might be nice to have a binding for quitting at the game over screen like `q` or `Esc`.
Don´t work with ghcjs
The constraint would not advisable since primitive 0.6.3 has many broken typeclass instances. The GHCJS bump makes sense though! I wish 0.6.3 was marked broken on Hackage.
It is marked unpreferred/deprecated: http://hackage.haskell.org/package/primitive/preferred Fwiw, `primitive-0.6.4.0` too is considered bad for having other bad instances and will sooner or later be marked deprecated as well one `primitive-0.7.0.0` is ready ;-)
Inside the `compile` block, you're supposed to construct an event network. No events are fired yet, no Behaviour changes. _Then_ you call `actuate`, and now you can call `fire` and see Beheviours change.
If we are interested only in the max. prime, wouldn't it be better to reverse the comprehension and stop at the first prime found inside a \`filter\`? 
* GHC fork mentioned in the talk: [https://github.com/mrBliss/ghc/blob/explicit-dicts-8.4/testsuite/tests/explicit-dicts/should\_run/TwoEqs.hs](https://github.com/mrBliss/ghc/blob/explicit-dicts-8.4/testsuite/tests/explicit-dicts/should_run/TwoEqs.hs) * Paper: [https://arxiv.org/abs/1807.11267](https://arxiv.org/abs/1807.11267)
Regarding Debian being "fully opt-in", I'm reminded of [xscreensaver](https://www.jwz.org/blog/2016/04/i-would-like-debian-to-stop-shipping-xscreensaver/).
The keyword is "dual-intuitionistic logic". You might have to dig through references to find an accessible introduction
This *sounds* right, but I have trouble constructing some system where actual coexponentials would be used for it.
If you're comfortable with linear logic, the coexponent a - b is just a ⊗ b^⊥, modulo throwing in ! or ? to deal with pervasive sharing in (co)intuitionistic logic. I'm unable to prove lend-left with your definition of - Do you have a writeup of your explorations somewhere? 
And it needs to be the case that the types of patterns become the dual of the types they're matching in some sense. This isn't necessarily invalid, but it rules out some simple ways of turning patterns into types.
See if you can prove a lemma that `forall f e xs ys. foldl f e (xs ++ ys) = foldl f (folde f e xs) ys`
You may want to look into embedding techniques, such as HOAS or Compiling To Categories. [http://dev.stephendiehl.com/fun/evaluation.html#higher-order-abstract-syntax-hoas](http://dev.stephendiehl.com/fun/evaluation.html#higher-order-abstract-syntax-hoas) [https://www.google.co.uk/search?q=haskell+compiling+to+categories](https://www.google.co.uk/search?q=haskell+compiling+to+categories)
I am incredibly confused by this whole (for lack of a better word) flamewar going on between stack and non-stack. I haven't done Haskell professional work in quite a while now, can someone explain why this is happening? I remember Stack being a godsend when I was doing professional work. Did something happen in the community as a whole?
Actually the game intercepts the interrupt and shuts itself down, see src/App.hs 😉 Ctrl-c is the preferred way to exit.
AFAIK (didn't actually get to put any package there, but I read the docs a while ago), if the package has no unit tests, stackage only checks if it compiles. If your package has any dependency that is not there, it's taken out of the snapshot. Of it depends on a version that isn't on the snapshot, it's taken out again. If you stop pushing your package there, it will stay as long as it works, and will be taken out when it stops working.
Yeah, this is true. Good clarification.
Lots of Haskellers use stack, and lots of Haskellers use cabal. And plenty of Haskellers use both of them. There are a few people who are very good at antagonizing each other online about build tools. The vast majority of us are just quietly getting on with writing software in Haskell.
I simply ignore that corner of the "community", and find there isn't a flamewar, as if by magic. Every language's community has its kooks, zealots, and cranks who need to plant their flag and demand that some nefarious plot go no further or civilization itself is at stake, blah blah. Such characters only thrive when they have an audience. 
I am managing to develop a GHCJS application with reflex-platform which works fine, this is mentioned as project A in the post. However I am also developing a supporting library (B) which is not GHCJS specific, does not use anything from reflex-dom and aims to simply use Reflex as a library to support reactive data types, Event and Behavior. Thank you for the response and insight however I really would like to support user's of library B which are not developing GHCJS applications but just common Stack applications.
I have some previous experience with reactive-banana but I'm not very experienced with FRP. I am trying to provide an API based on FRP. What I am trying to accomplish is to provide a typeclass for remote data access. One function in the typeclass `view` is your class request and response for a value of type `a` but I also want a `viewRx` which returns some kind of reactive value `Behavior a`. Ideally I want this `Behavior` to come from the Reflex package so it can be used as part of reflex-dom applications. But I am using reactive-banana for now simply because I can get it to compile.
Yes. That sounds excellent. Ever since I first used `new-build` and `new-test`, I have found them to be speedy and very good. Now that the project isolation problem (sandboxing) is a default, I'd like to use cabal directly more frequently. Can we have all of these suggestions, please? They sound great to me.
&gt;Thank you for writing this up! FYI, in case you weren't aware /u/snoyjerk is not Snoyman (/u/snoyberg), hence the flair.
Also, should I comment on the linked proposal as well? Will that be helpful?
Thanks for both your responses!
Also, Stackage snapshots pin the cabal files via their SHA256.
Please do :)
I've been trying to ignore that corner of the "community" for half a decade ever since they funded their company. But it's not easy if everyone else still keeps getting riled up and talks about them which as you say is the very audience they thrive on and enables the recurring controversy. I wish everyone would just stop taking them seriously and ignore them so we can all go back to live in "boring" times again.
This is my experience as well. There was more hostility in 2016 on both sides, but recently, almost all trolling seems to come from folks not affiliated with either "side." I suspect it's a troll operation from folks that don't like Haskell at all and want to see it fail and perceive this as the biggest leverage point.
When in doubt, [look at the laws](https://www.stackage.org/haddock/lts-12.19/base-4.11.1.0/Prelude.html#t:Foldable). There's nothing here that suggests the fold must be deterministic, merely that `foldMap` folds equivalently to `foldr` and that `length` is sane.
It's quite uncommon for laws to explicitly specify extensionality, though, isn't it?
I think this sort of thing is almost why `Foldable` exists: if everything were well-behaved, we'd just use `Traversable`. It looks like `HashSet` in [unordered-containers](https://hackage.haskell.org/package/unordered-containers-0.2.9.0/docs/Data-HashSet.html) is `Foldable` but not `Traversable` for the reasons you mention.
See https://www.reddit.com/r/haskell/comments/7dmjh8/why_does_traversable_need_foldable/dpz30ep/ data Repeat a = Repeat Int a deriving (Functor) instance Foldable Repeat where toList :: Repeat ~&gt; [] toList (Repeat n a) = replicate n a
Eq is merely an equivalence rather than identity, and preserving equivalence everywhere would be a bit of an extremity. I think it's already broken in a lot of places anyway. But of course extensionality would be nice to have and I don't know whether library users actually rely on that in this particular case.
I feel it is probably unhealthy to assume that community disagreement is an outside plot to take us down
This isn't really relevant to the issue of `toList` breaking extensionality, though.
The xscreensaver kerfuffle is really just a matter of a downstream maintainer not communicating adequately with upstream, for whatever reasons. Honestly, it's probably best solved with a separately-maintained hard fork of xscreensaver with a different name. Stackage does not modify upstream packages, but only picks the latest snapshot according to purely mechanical criteria (it builds, passes tests, and doesn't break its dependents) so I don't see the same situation arising for stackage. &amp;#x200B;
Community disagreement is one thing. Single-purpose accounts taking up every opportunity to throw poisonous barb's is something else entirely.
That is certainly true
&gt; The Set type, being an unordered collection, has this same problem, but it solves it by always folding in sorted order. However, it's unclear whether this choice was made for correctness reasons, or just because it happened naturally (as Sets are automatically represented sorted internally). Looking at other languages (which do something similar, in the absence of a hash function), the reason that makes most sense to me is that Set needs Ord (or Hashable or something similar) is for efficiency (insertion, deletion, lookup). &gt; For efficiency reasons, I can't have a normalized internal representation The question is which operations do you want to be efficient? E.g. if getting counts/occurrences should be efficient, then now you're going to have guaranteed linear time, instead of worst case logarithmic time (by implementing Bag a = Map a Int). GHC uses a [different representation](https://www.stackage.org/haddock/lts-12.19/ghc-8.4.4/src/Bag.html#Bag) than yours (while operating withing the same constraints), presumably for fast unions. &gt; Would it be considered acceptable to have undefined folding order in Foldable implementations? I can't speak for others, but that would make me personally quite wary of using such a data structure, because now I need to be extra careful with substituting like-for-like. 
&gt; This is just a test for the library by the way, to see if the Behavior is updated as expected Update: This is just a test for the library by the way, to see if the Behavior is updated as expected Update2: I think I can use `reactimate` in the event network to feed a value into an MVar which I can then read from in IO after the call to `actuate`. This will allow me to call fire after `actuate` and read a value that has propagated through the network.
Nope, not without digging into dark and scary things. There are two questions to ask here, the first is "why would you want such a thing?" (the answer is usually: this is a programming problem from javascript), and the other is "what would its type be?" The second question is the problem. You'll notice that the type of the result depends on the _value_ of the first parameter. Such a thing is called _dependently typed_, and while we can approximate it in Haskell, you aren't going to like it. But the good news is that you'll never need such a function anyway, so it's all OK in the grand scheme of things.
Update: This is just a test for the library by the way, to see if the Behavior is updated as expected I think I can use reactimate in the event network to feed a value into an MVar which I can then read from in IO after the call to actuate. This will allow me to call fire after actuate and read a value that has propagated through the network.
Update: This is just a test for the library by the way, to see if the Behavior is updated as expected I think I can use reactimate in the event network to feed a value into an MVar which I can then read from in IO after the call to actuate. This will allow me to call fire after actuate and read a value that has propagated through the network.
The changes in the internal representation would not change the order of the elements as observed by e.g. `Foldable`/`Traversable`, though. It seems too strict to me to require that `fmap id` shouldn't be able to change even unobservable properties of the data.
i'd actually argue you should have all three, internal lib, public, and app, because this is supposed to educate, the user can easily delete or go interactive :) 
It's built into haskell-ide-engine :)
Personally I got stuck in dependency hell using stack and some people suggested switching back to cabal, haven't have problems with that yet but wouldn't mind switching again if I get something out of it. Tried nix but the syntax was too obscure for me and the tutorials didn't answer my questions.
Reminds me of this post by Nikita Volkov (he didn't use transformers though) http://nikita-volkov.github.io/a-taste-of-state-parsers-are-easy/ Interestingly, his post was also motivated by parsing values from a database response.
You may also have a look at the `fixed-length-lists` package. It’s not (yet) available on Hackage but can be fetched from GitHub: https://github.com/input-output-hk/fm-ouroboros/tree/master/Haskell/fixed-length-lists.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [input-output-hk/fm-ouroboros/.../**fixed-length-lists** (master → f900f7e)](https://github.com/input-output-hk/fm-ouroboros/tree/f900f7eb87380b7dbf06781b57ee66afb8e97056/Haskell/fixed-length-lists) ---- 
Like you I've seen hints of the flame-war but didn't see any complete explanation for its origins. Following is an explanation, which is possibly wrong as it contains some speculation - There was a period when `cabal-install` was broken. At that period some people theorised that if packages followed some rules about version numbers and upper bounds then these breakages would stop from happening and that Hackage will be great again. Other theorised that the proposed solution will not work, or will even cause more problems than it solves, and some of them kept looking for other solutions, and from this Stackage was created. IIUC, that situation of Hackage not working was stressful for many people (it was somewhat disheartening for me personally as a user that liked many aspects of Haskell but felt like at its state I just can't really recommend it to anyone). Adding to that, it was mostly Snoyman's popular packages which have seemed to trigger this breaking of Hackage, and some people suggested that his packages doing the versioning and upper bounds thing wrong was causing it. I'm not aware if anyone made a convincing case of whether any side of the argument between "suggested scheme will save hackage" vs "suggested scheme worsens the problem" was more correct than the other. To sum ip up, IIUC, there was a situation of "you are breaking hackage, so you have to do what I say to save it, and I can't really convince you that this will make things better and not worse but trust me because I'm smarter than you", and such a situation is prone to cause bad blood.
Yup, that's the culprit. I think the error is fine (although the message is misleading), so not worth opening an issue.
This war has been very good since the result is a much better cabal and a better stack. It's a pity that they both are going to fail in the long term since they choose to use a pre-internet centralized schema. I suspect that people will care less about uploading files to hackage/stackage. A good extension to haskell would be `-ImportURL`
I'm not sure if this is intended behaviour or not because `picker` is top level rather than local, I'll open an issue if you don't.
 -- Using functional patterns last :: [a] -&gt; a last (_++[e]) = e -- Functional patterns essentially desugar to something like this, but compiler -- implementations can leak: functional patterns are often more performant, -- search termination occasionally differs due to subtle nuances of how the -- search space is constructed, etc. last' :: [a] -&gt; a last' xs | xs =:= _ ++ [e] = e where e free -- Read as: -- Find all solutions to the equality constraint "xs is equal to some -- list ++ some element where that element is a random choice in xs" last'' :: [a] -&gt; a last'' xs | xs =:= _ ++ [e] = e where e = choose xs -- Interestingly enough, since this is fairly equivalent to tree traversals, if -- I want to write last specialized to lists of Nats, there's no reason why I -- should be constrained to a choice in xs right? Sure, it's an optimization, -- but is it a necessary one? lastNat :: [Int] -&gt; Int -- we'll pretend ints are nats here lastNat xs | xs =:= _ ++ [e] = e where e = choose [0..] -- "any possible valid choice" -- In theory, the compiler shouldn't really care that the search space is -- infinite because there's only one branch and it's well ordered, right? More -- importantly, there's only one solution! -- -- Well, it turns out that we haven't specified that there's only /one/ -- solution (nor is there really an easy way to do so), so the compiler will -- happily find our correct answer... and then try every other nat for infinity -- trying to find the other solutions. -- -- Possible fix... But doesn't work -- (hint: what does constraining e do to the resulting search space?) lastNat xs | xs =:= _ ++ [e] = e where e | e &lt;= max xs = choose [0..] -- -- There is, however, a way to fix it up for lastNat lastNat xs | xs =:= _ ++ [e] = e where e = choose [1..(maximum xs)] maximum = foldr max 0 -- fold works only on lists; there aren't typeclasses here choose :: [a] -&gt; a choose = foldr1 (?) -- Bonus question: why does 'where x = free' terminate but 'choose [0..]' does not? -- hint: pretty sure it's a compiler implementation detail :) 
I've [created one](https://ghc.haskell.org/trac/ghc/ticket/15931#ticket).
i use a header fine with some aliases for readability / maintainability, but my custom Prelude https://github.com/sboosali/spiros/blob/master/spiros/spiros.cabal builds with both ghcjs and eta (as of last month), and from ghc 7.10 to 8.6, using the pattern I mentioned. 
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [sboosali/spiros/.../**spiros.cabal** (master → b9be5ce)](https://github.com/sboosali/spiros/blob/b9be5cef80fbff5047f26da6b87b46a8af24f549/spiros/spiros.cabal) ---- 
I've just written a small function for it (defun format-haskell-on-save () "Function formats haskell buffer with brittany on save." (when (eq major-mode 'haskell-mode) (shell-command-to-string (format "brittany --write-mode inplace %s" buffer-file-name)) (revert-buffer :ignore-auto :noconfirm))) and then add it to haskell-mode `(add-hook 'after-save-hook #'format-haskell-on-save)` 
Well Traversable still makes sense here for commutative applicatives (commutative writers, Maybe)
"Normal form" in Haskell is short for "head normal form". It's fully evaluated, no thunks. eg (3,4) "Weak head normal form" is anything where the outer part is fully evaluated. There can be thunks in side. So both (1+2,3) and (1+2, 2+3). Even looser is "not in normal form", so just a thunk. Eg (,) (1+2) 3
`(,) (1+2)` is not a clearly good thunk example. because it could also mean a closure which is in whnf. I'd prefer something like `id 10`.
What I got from your explanation is that NF = HNF, presumably because lambda expression can not be fully reduced, it's always reduced to the head. Basically NF just means reduce as much as you can, even it's only the the head. But WHNF is different, sub-expression inside WHNF can be not reduced completely. Is that correct? Also, seems like there exist some ambiguity in terms of terminologies in Haskell community, in my textbook - Thinking Functional with Haskell, a list that only reduced to the first element, for example: 1:\[2,3,4\] is HNF, has HNF, but according to your definition, it's WHNF, because \[2,3,4\] is not reduced, one source I found online also claim the same thing. Although I think your definition make more sense, but there probably exist more than one definition for the same term here...
Operationally a thunk is a closure - a function pointer on the heap followed by some data: (1 + 2, 3) Could be seen as -- Int's are heap allocated Int#'s 1_addr &lt;- push Int# push 1 2_addr &lt;- push Int# push 2 -- allocate the thunk thunk_addr &lt;- push plus_int push 1_addr push 2_addr 3_addr &lt;- push Int# push 3 -- allocate the tuple tuple_addr &lt;- push (,)_Int_Int push thunk_addr push 3_addr So without optimizations we consume 9 words in total. Anyway, the tuple constructor is whnf - the toplevel isn't a thunk so we don't have to execute the function pointer before using it. Some value x is nf if x is whnf and all contained data in x is nf.
&gt;"Normal form" in Haskell is short for "head normal form". It's fully evaluated, no thunks. eg (3,4) “Fully evaluated” is the requirement for a normal form, but a head normal form doesn’t need to be fully evaluated. &gt;"Weak head normal form" is anything where the outer part is fully evaluated. There can be thunks in side. So both (1+2,3) and (1+2, 2+3). “Outer part evaluated” is the requirement for both a head normal form and a weak head normal form. The difference is what the outer part is. For head normal form it is all outer lambdas (if any) plus the first non-lambda layer; for weak head normal form it is just the outermost layer. &gt;There's a special case in Haskell for constructors with strict fields (denoted by an !). For it to be WHNF, the terms in those fields also need to be WHNF. Actually, strict fields don’t *need* to be in WHNF for the whole expression to be in WHNF. It’s just that they *cannot* stay completely unevaluated as soon as the surrounding data constructor is determined.
&gt;\[…\] there probably exist more than one definition for the same term here … I think there exists exactly one definition for each of these terms and people providing different definitions just misunderstood something. 🙂
I don't think there's any problem with having toList or any other functions that don't preserve (==). [unordered-containers](http://hackage.haskell.org/package/unordered-containers) works like this, as does `splitRoot` from the `containers` package which exposes the internal representations of Sets and Maps. In principle there could exist a subclass `Monoid a =&gt; CommutativeMonoid a` which would let you define `foldBag :: CommutativeMonoid m =&gt; (a -&gt; m) -&gt; Bag a -&gt; m` as a way of exposing the foldable elements without exposing their ordering (assuming that CommutativeMonoids are in fact commutative). Such a class would have no methods though, and its instances would serve only as an unchecked assertion that `mappend` is commutative, so I think it's generally considered 'not worth having'.
While captainattertot is correct, if you wanted to _emulate_ this behavior, you can define a type: data MyType a = FinalResult a | Indirection (forall b . b -&gt; MyType a) Then you can pattern-match to apply myFunc :: Int -&gt; a -&gt; MyType a myFunc 0 = FinalResult myFunc n a = Indirection (const (myFunc (n-1) a)) While the Hindley-Milner type system forbids recursive/dynamic types, you can always emulate the behavior on the type level
Don't mix up Haskell and lambda calculus. They're not the same. Haskell expressions can be fully evaluated, lambda calculus doesn't have constructors etc Lambda calculus has another strong normal form: beta normal form, eg `1+y` where y is a free variable. So there BNF and HNF aren't the same. `1:[2,3,4]` is a notation issue. Depending on your syntax and semantics it could be both. If that means 1:(2:(3:(4:[])))) then it's fully reduced. If that is 1:thunk where thunk evaluates only on inspection, then it's not. 
Thanks for this, a nice simple solution! 
Jep, same here. I was reasonably happy with them; i.e. the interface is fairly easy to work with. For very small vectors, say of length 2,3 and 4 or so, they were not as efficient as something like 'data Vec2 = V2 !a !a' (and those were the ones I needed most), hence now my [hgeometry](https://hackage.haskell.org/package/hgeometry) package has a vector type that (automatically) specializes the implementation of those.
This is great thank you so much!
&gt; So normal form is to reduce as much as possible. Correct. &gt; Head normal form is to make sure head and 'one layer' beneath it […] is reduced. That’s not right. It’s not necessarily two layers but all lambda layers and one layer beneath. It’s only two layers in my `replicate 3` example because there is one lambda layer there. &gt; Weak head normal form only make sure head is reduced. If you consider the outer layer to be the head, then yes.
`HashMap` and `HashSet` also have this problem with `toList`. I reason my way around it by thinking of it as another invariant on the algorithms I use: they must work regardless of the order of elements. This isn't statically checked, much as I wish it was, but there are many desirable properties of programs that we can't (or don't want to) enforce via the type system. Maybe when we all move to Idris++, we won't have to make compromises like this...
I think a nondeterministic `Foldable` instance might be fine, but why is your `Eq` instance deterministic? It seems like it should be neither or both.
Neat! I hadn't heard of that before, I looked at it briefly and liked what I saw :)
interesting. yeah some macros that cabal provides get "merged upstream" into ghc itself. so there might be different behavior in different compiler versions for the same macro, because it's being defined by different tools. 
Yeah, you can do this. It just takes the right amount of dependent types. ``` {-# LANGUAGE DataKinds #-} {-# LANGUAGE GADTs #-} {-# LANGUAGE TypeFamilies #-} {-# LANGUAGE KindSignatures #-} data Nat = Z | S Nat data SNat :: Nat -&gt; * where SZ :: SNat 'Z SS :: SNat n -&gt; SNat ('S n) type family Unroll a (n :: Nat) :: * where Unroll a Z = a Unroll a (S n) = a -&gt; Unroll a n foo :: SNat n -&gt; Int -&gt; Unroll Int n foo SZ x = x foo (SS n) x = \i -&gt; foo n i three = SS (SS (SS SZ)) main :: IO () main = do print (foo three 5 4 3 2) ```
I'd say just provide a `stack.yaml` (and maybe also a `cabal.project` file) that points to the git commit of `reflex` that you've tested with. That will make it pretty straightforward for people to do the same thing for their packages.
My own experience and data point: I saw a growing demand to include Stack instructions and documentation alongside other installation options like Cabal-only and Haskell Platform. At the time, both Cabal-only and Haskell Platform were broken on some platforms including Mac OS, punctuated by brief periods of working correctly. And Stack worked well, and was much easier than the purported behavior of the older methods. (I wouldn't be surprised if this is still the case, but wouldn't know. I gave up trying to use these long ago.) I think that some people reacted emotionally to the news that these two "official" methods were blocking people from starting with Haskell. There was a tremendous push-back to not discuss Slack as a first-class option, and for sure not to list it in parallel on a main web page about how to get started with Haskell.
There have been improvements to the tools, but we're a very long way from being able to say the "war" was good. It was, and sometimes remains, a horrible blemish on the Haskell community, and caused a lot of pain for a lot of people. We need to do better. This is not okay, much less healthy.
Also, specific revisions of the metadata can be retrieved for true immutability
I consider the export of `splitRoot` from a "public" module a historical wart. That sort of abstraction-breaking function should be in a module labeled `Internal`. Frankly, by the time you're exposing that much, I think you might as well expose the raw constructors. `unordered-containers` indeed has a non-deterministic fold. While that makes be quite uncomfortable, all the alternatives seem to suck.
A little simpler is to make the body of that function just: `(haskell-mode-buffer-apply-command "brittany")`
The usual convention for typeclass laws is that equality is fiat equality. Few interesting `Monoid` instances would be valid otherwise. Do you know where I can read up on what "intensional", "extensional", and "propositional" equality mean here?
An expression is in normal form if it is one of the following: * a data constructor applied to arguments which are in normal form * a lambda abstraction whose body is in normal form An expression is in head normal form if it is one of the following: * a data constructor applied to some arguments (which may be any expressions at all) * a lambda abstraction whose body is in head normal form Since we tend not to write evaluators or compilers which try to evaluate the body of not-yet-applied lambda abstractions (which requires dealing with unbound variables), these notions of normal form are still too strong. We don't want to have to try to ensure anything about the body of a lambda abstraction. An expression is in weak head normal form if it is one of the following: * A data constructor applied to some arguments (which may be any expressions) * A lambda abstraction This tends to be the practical form to reduce expressions to in real-world lazy evaluators. It's just enough to ensure that a case expression which matches against a single data constructor will be able to make a decision. It's also enough to ensure that as much work has been performed on a function as we can practically manage without having an argument to bind its parameter to. So for instance, a function which is an application expression such as `(\x -&gt; (\y -&gt; (x + x) * y)) 1` will have `(\y -&gt; (1 + 1) * y)` as a weak head normal form. Note how we aren't required to reduce the 1 + 1 yet, before y is known, which is how evaluators usually work. (1 + 1) * y is an application of the function (*) to the arguments (1 + 1) and y, and so isn't in normal form or head normal form, but the definition of weak head normal form doesn't care.
"Intensional" means "internal" equality in a sense -- actual equality in structures in their guts. "Extensional" means "observable by probing via functions." I think the use of "propositional" here is confusing, so let's leave it aside for this discussion. However, in general, it means that it is an equality you can "prove" using various equational relations in a specified logical system.
&gt; The usual convention for typeclass laws is that equality is fiat equality. Few interesting Monoid instances would be valid otherwise. I'm not sure what you mean here -- the equality in the `Monoid` laws is on elements, not morphisms (and even then there's no mention of a user-defined `Eq`!). In contrast, notice the laws for `Functor`: the first law is that `fmap id = id`, it is *not* `forall x . fmap id x = x` and certainly not `Eq a =&gt; forall (x :: a) . fmap id x == x`. Regarding equality flavors: I'm not the best person to describe this, as I'm not a professional type theorist and there is a lot of subtlety here, but I can at least give a basic intuition. The most fundamental kind of equality is definitional equality -- if two things are defined in the same way, then they are equal. For example, if I have an `x`, if I define `let a = x` and `let b = x`, then `a` and `b` are *definitionally* equal because they are defined in the same way. From a language perspective, definitional equality doesn't even exist "inside" the language in the sense that you as a user can never define it and have no control over it -- it's something intrinsic to the type system itself. The most common time this sort of equality is important is when doing type-level programming, because the type checker will be literally comparing the structure of the terms in unification. This often throws people off when they first start dependently-typed programming -- what do you mean my `Vec (a + b) x` isn't the same type as `Vec (b + a) x`?! Well, `a + b` and `b + a` may be equal in some sense, but they're not *definitionally* equal, and definitional equality is what the typechecker concerns itself with. Propositional equality, on the other hand, is defined inside the language, usually something like this: data (=) (a : Type) : (b : Type) -&gt; Type refl : a a This structure is interesting because it gives a way for you as the programmer to write constructions capable of convincing the type checker that two things really are equal. Using this, you can convince the type checker that yes, `forall (a b : Nat) . a + b = b + a` and subsequently, that your `Vec (a + b) x` can be used as a `Vec (b + a) x`. So, what about functions then? Our `Functor` laws are specified using equality on functions, so... which one should we use? Well, an intensional equality is basically just definitional equality, meaning that the functions are defined in the same way, e.g., `let x = \a -&gt; a` and `let y = \a -&gt; a` are equal in an intensional way. But what if I had functions like this: f :: Bool -&gt; Bool f x = x g :: Bool -&gt; Bool g True = True g False = False Well here, `f` and `g` are not defined in the same way, so they're not equal in this intensional sense. But if we define our function equality as `forall (x :: a) . f x = g x`, then these functions are equal. When you define function equality this way ("for any input, these two functions give the same output"), it's called "extensional equality." Anyway, all of these equalities have importance in a type theory sense in a way that arbitrary user-defined equalities defined in `Eq a` simply do not. I can define `instance Eq Bool where (==) _ _ = True`, and that's valid in the sense that it satisfies the equality laws (reflexive, symmetric, and transitive), but it doesn't have any interesting theoretical properties from a type theory perspective. So to my above comment: the laws in `Functor` and `Monoid` do not explicitly state which equality they're using, but since there is no predication on an `Eq` instance, I don't think it's justified to contend that's the equality they mean - I think the only sane case is to contend that they mean one of these more fundamental equalities. If you were to decide the `Functor` laws were talking about your user-defined `Eq` instance, the problem is that this `Functor` is now living in a fundamentally different category than the other `Functor` instances used in Haskell, and that's... I mean you can have functors (lower case!) between different categories, but I think that's severely abusing what the typeclass `Functor` is about: all typeclass instances of `Functor` should be living in the same category, thus subject to the same notion of equality for their laws.
When you're doing type-level programming, yes, some equalities enjoy special privileges. But when you're not, things can be rather looser. I don't understand the real significance of your distinction between laws on elements and laws on morphisms in an extensional context (which is what really matters for correctness at the term level). What I meant about the `Monoid` laws is that there's an *implicit* reliance on a relaxed equality of some sort. We have `Monoid` instances for sets, sequences, and all sorts of other types that can represent a single abstract value by multiple structurally different ones. The equality is on abstract values, not representations.
&gt; We have Monoid instances for sets, sequences, and all sorts of other types that can represent a single abstract value by multiple structurally different ones. But these are orthogonal: whether I define other equalities on my type has no bearing on whether the `Monoid` instance respects propositional equality or not - and actually, I wouldn't be surprised if it does for `Set a`: I bet `&lt;&gt;` just looks to see if either of the inputs is `empty` and if so, returns the other, and that definition would respect the propositional equality regardless of any user-defined equality. And again, the justification for interpreting the laws as stated on propositional equality is that `Monoid`/`Semigroup` are *not* predicated on an `Eq` instance. Further, if instance does respect propositional equality, it is guaranteed to respect any (law-abiding) user-defined `Eq` instance, since propositional equality is a stronger equality than any other user-definable equality.
Based on hindent: https://github.com/tonyday567/brittany-emacs
Fantastic reply, very clear examples!
I have been using the FRP library reactive-banana as a proof of concept instead of Reflex because I can't get Reflex to compile :( I tried adding `reflex` to `extra-deps` which on `stack build` results in a prompt add the following to `extra-deps`: - base-4.9.1.0 - haskell-src-exts-1.19.1 - ref-tf-0.4.0.1 - template-haskell-2.12.0.0 After this a `stack build` results in a prompt to add the following to `extra-deps`: - base-4.10.1.0 - template-haskell-2.12.0.0 I didn't add `template-haskell` again but now `base` is in there twice which causes an error on `stack build`. So then I removed all the `extra-deps` apart from Reflex and tried with `allow-newer:true`. This prompts me to add `- ref-tf-0.4.0.1` to `extra-deps`. But now the `stack build` causes the following error: reflex-0.4.0.1: configure reflex-0.4.0.1: build Progress 18/19 -- While building custom Setup.hs for package reflex-0.4.0.1 using: /Users/jeremy/.stack/setup-exe-cache/x86_64-osx/Cabal-simple_mPHDZzAJ_2.2.0.1_ghc-8.4.4 --builddir=.stack-work/dist/x86_64-osx/Cabal-2.2.0.1 build --ghc-options " -ddump-hi -ddump-to-file -fdiagnostics-color=always" Process exited with code: ExitFailure 1 Logs have been written to: /Users/jeremy/cs/progress/backend/simple-store/.stack-work/logs/reflex-0.4.0.1.log Configuring reflex-0.4.0.1... clang: warning: argument unused during compilation: '-nopie' [-Wunused-command-line-argument] Preprocessing library for reflex-0.4.0.1.. Building library for reflex-0.4.0.1.. [1 of 8] Compiling Data.Functor.Misc ( src/Data/Functor/Misc.hs, .stack-work/dist/x86_64-osx/Cabal-2.2.0.1/build/Data/Functor/Misc.o ) [2 of 8] Compiling Reflex.Class ( src/Reflex/Class.hs, .stack-work/dist/x86_64-osx/Cabal-2.2.0.1/build/Reflex/Class.o ) /private/var/folders/wv/4rc7b5p56bvfr2ttgs64jgmm0000gn/T/stack17464/reflex-0.4.0.1/src/Reflex/Class.hs:314:10: error: • Could not deduce (Semigroup (Event t a)) arising from the superclasses of an instance declaration from the context: (Semigroup a, Reflex t) bound by the instance declaration at src/Reflex/Class.hs:314:10-54 • In the instance declaration for ‘Monoid (Event t a)’ | 314 | instance (Semigroup a, Reflex t) =&gt; Monoid (Event t a) where | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ I think you can understand my frustration at this process, and I'm really trying to find a solution in getting Reflex into my library while not ditching Stack users. If anyone can shed light on what is needed in getting Reflex into Stack (note that I am on LTS 12) I am willing to expend considerable effort in doing so.
This is the answer.
For very small vectors there is also [http://hackage.haskell.org/package/tup-functor](tup-functor), which provides 4 different implementations, one of them being that.
Indeed, I too would be very keen to try it out but that warning is kind of discouraging.
Probably worth pointing out that Haskell implementations don't typically evaluate unapplied lambdas. If anyone wants to run the above example, you can paste the below into https://chrisdone.com/toys/duet-delta/ and see evaluation to WHNF. Technically speaking, this evaluator will do the equivalent of a deepseq (evaluate the arguments of WHNF constructors), so `replicate 3 0` should display a full list, but it will never evaluate unapplied lambdas. data List a = Nil | Cons a (List a) replicate = \n x -&gt; case n of 0 -&gt; Nil n -&gt; Cons x (replicate (n - 1) x) main = replicate 3
If I remember correctly, I started `fixed-length-lists` because all similar packages I could find didn’t just implement fixed-length lists but something more complicated.
Because we're envious of languages which can do functions between types. 
Indeed, the advantage of the type family is that it unifies the types of `spreadsheetGet` and `spreadsheetsCreate` into a single function, `send`, parameterized by the request you want to make. If you always call `send` on a concrete request, e.g. `send (spreadsheetGet ...)`, then there is no advantage, and indeed, creating and using a wrapper with a more concrete type is likely to give you clearer error messages. But if you want to write a wrapper around `send`, say, a version which automatically sleeps and retries if the server tells you that you're hitting its API too hard, or a version which automatically fetches your credentials from some ambient `ReaderT`, then you'll be very happy to be able to write a single wrapper around `send` rather than having to write one for `spreadsheetGet`, one for `spreadsheetCreate`, etc.
MetaML is the standard example of a typed language that does this: [https://www.sciencedirect.com/science/article/pii/S0304397500000530](https://www.sciencedirect.com/science/article/pii/S0304397500000530) [https://www.usenix.org/legacyurl/staging-metaml](https://www.usenix.org/legacyurl/staging-metaml)
Ok thanks !
If you aren't taking advantage of the one…advantage you note, the type family operates like a type synonym, thus being type-equal to the "straightforward" approach you mention. So, in a type sense, this pattern already implements the "straightforward" approach, and it does you no harm in this case. If on the other hand you *do* wish to take advantage of the abstraction, perhaps by means of some type-aligned data structure, the declaration of the relationship is essential, and its absence would be a genuine hindrance.
There is also [`vector-sized`](https://hackage.haskell.org/package/vector-sized), which builds upon [`vectors`](https://hackage.haskell.org/package/vector), and serves mostly as an alternative to `fixed-vector`. I find its API easier to understand than `fixed-vector`. `vector-sized` also provides a few more features that `fixed-vector` seems to lack, like safe indexing with run-time values.
Just an aside, but you can implement \`fmap\` and \`foldMap\` using \`traverse\`, so in practice \`Traversable\` has no extra requirements. (But equally, if your class can't implement Functor, you're going to have a bad time with \`Traversable\`.)
I'm looking for something more list-like (ie not based on \`vector\`). Thanks for the link.
Is it not possible to achieve these benefits using simpler machinery? For example: spreadsheetGet :: (MonadGoogle m) =&gt; SpreadsheetId -&gt; m (Either String SpreadSheet) spreadsheetGet sprId = do let req = prepareRequestObject sprId -- googleGet is a function provided by MonadGoogle, which -- wraps Wreq.get and implements retry logic, etc -- (googleGet req) &gt;&gt;= (pure . eitherDecode) 
Nice article! I admittedly had some difficulty understanding the first example (you should probably reiterate the type signature of `const` for reader's benefit) until I realized that the purpose of `const` in `zipWith` is to _select the first sequence_ while using the second one as a delimiter. In other words, `const` is just another way of writing `curry fst` -- and conversely `flip const` is just `curry snd`. Although these forms may be longer than those that use `const`, I think the symmetry and explicitness of `fst`/`snd` would be worth it for better readability.
My first thought was &gt; But `zipWith const xs ys` breaks fusion while `take (length ys) xs` doesn't... But you always use the same list for both arguments. So to be intentionally obtuse I could argue your favorite is ap (zipWith (flip const))
&gt; A note on my frequent mentions of »works on infinite lists«. This is not just about working with infinite data, which I agree is not that often the case. Rather, it also means that the algorithm only consumes as much input as is required, so it does not do any redundant work I pronounce this "streams".
This is weird and cool.
You gave an implementation of `spreadsheetGet`, so I'm assuming you're suggesting that the gogol-sheets implementation of `spreadsheetGet` should be responsible for fetching your credentials and retrying. For those two specific features, perhaps, but I am talking about features which _you_ would want to implement around both `spreadsheetGet` and `spreadsheetCreate`, features which are not already provided by `gogol-sheets`.
As a counterpoint, `const` is much more obvious to me than `curry fst`.
Stackage blesses *set* of packages. from a consumer point of view, it's a major guarantee. from a contributor point of view, it provides a target.
That’s awesome. I’ve really enjoyed Will Kurt’s book, Getting Haskell. And I’ve got ALL the Haskell books. And they are good, just depends on how you learn best. https://www.manning.com/books/get-programming-with-haskell
Neat trick, thanks for sharing. &gt; Rotating a vector, array or list means removing the first couple of elements, and appending them onto the input. &gt; &gt; rotate 3 [0,1,2,3,4,5,6,7,8,9] ====&gt; [3,4,5,6,7,8,9,0,1,2] &gt; The naive &gt; &gt; rotate n xs = let (a,b) = splitAt n xs in b ++ a &gt; &gt; once again does not work for infinite lists (even with out nifty `splitAt` function from before, since we require the second half in the result). But maybe we can do better? But that works fine for infinite lists. take 10 $ rotate 3 $ [0..] ====&gt; [3,4,5,6,7,8,9,10,11,12]
Very nice. However, the fact that we have a useful function, and the fact that `zipWith const` is a way to define that function, are different facts. The former fact adds a new tool to our toolbox; the latter just means we have a new yet another to write incomprehensible code. I propose, therefore, that there ought to be a semi-standard name for this function. The flipped version seems most natural, and it works (I think) for every example in the article. So: takeWhileHasItem = zipWith (flip const) I readily admit that `takeWhileHasItem` is hardly the best of all possible names for this thing. Can anyone think of a better one?
It's neat to see these different applications of this pattern. One way of looking at this is that `length` is not lazy enough; if `length`, `take`, and `drop` worked on peano numbers instead of Int you could write your functions in the naive (and more comprehensible) way and get the same properties. I wonder if you could do this and get the best of both worlds using rewrite rules.
Also we had a similar problem to the last one in a codebase I worked on: given a start and end time of day, does time `t` fall within the window? (Note start might be greater than end, representing a nighttime window)
Worth mentioning that since `zipWith` is an alterative `liftA2` for lists, that `zipWith const` is the more familiar `&lt;*` operator, and `zipWith (flip const)` is `*&gt;`. Prelude Control.Applicative&gt; ZipList [0,1,2] &lt;* ZipList [3,4] ZipList {getZipList = [0,1]} 
`genericLength` can be used with lazy Peano numbers. I don't how how it compares performance-wise, but I think it might read more obviously.
The problem here is that this doesn’t scale past zip(2). I’ve had zipWith3, where one argument is for the length, one for the desired values, and one for some predicate.
Hi! Author of `fixed-vector` here. That's weird. Performance should be comparable (for boxed vector) or better (in case of unboxed vectors). Which data types did you use? 
I used the regular boxed versions. Or more specifically, a newtype wrapper around those. The benchmarks that I ran are [here](https://github.com/noinia/hgeometry/blob/master/benchmark/Algorithms/Geometry/ConvexHull/Bench.hs), but this was a while ago. I can try to rerun them to figure out if something has changed. 
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [noinia/hgeometry/.../**Bench.hs** (master → 16e5924)](https://github.com/noinia/hgeometry/blob/16e5924a726225e2bab1ad8beecd3941eb00fdaf/benchmark/Algorithms/Geometry/ConvexHull/Bench.hs) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply ea9agvi.)
I have a feeling you can extend this idea: You're manipulating the "shape" of a construction in one parameter, to create a "view" on the other parameter.
I remember that I changed representation from Array to SmallAray which could've changed performance. Also allocation of V3 versions could be more efficient since array is first filled with dummy values and then they're overwritten. I'll try to run benchmarks. Do you remember how big performance difference was? 
Maybe `punch`? Im trying to think of a word for cutting around a pattern or stencil. Or maybe there's a word in carpentry for "to cut flush"
I think, intuitively, we can say that typically "normal form is what strict languages do," "weak head normal form is what lazy languages do," and "head normal form is a historical artifact that turned out not to be what people typically wanted to think about."
Or else problem with fusion are to blame. Either way it needs investigation
Presumably the binary version is much more common and might justify a special case similarly to the way we have `fst` and `snd` that operate on 2-tuples but not tuples of any other size.
Honestly I'd know what `zipWith const` meant before I knew what `takeWhileHasItem' meant and would rather see the former in my codebase. 
I like vec, because it's * lightweight * uses all the modern techniques * has its own little fin ecosystem that it works well with * maintained by phadej, who is an experienced and very active and responsive maintainer who is easy to reach and has a reputation for quality
Good content. The last example was good.
&gt; (&gt;&gt;), return and fail We're in 2018 now, not 1998 😅.
One example was newtype Polygon = Polygon [(Double, Double)] instance Eq Polygon where Polygon xs == Polygon ys = normalize xs == normalize y This looks like a rather inefficient way to compare polygons. You don't actually need to *normalize* the polygons; you just need to *align* them. The normalization procedure you proposed traverses each list to get a "minimal" vertex, then rotates it to the front. All you actually need to do is rotate the *second* list until its head matches that of the first list (or the list is exhausted). You should be able to get away with an `Eq` constraint on vertices and considerably less extra work.
I laughed, but then again someone once had to tell me to stop making burrito-monad jokes in the haskell IRC.
Funnily enough, stack is flat out not working for me right now. *Nothing* will build. At all.
\&gt; Is it not possible to achieve these benefits using simpler machinery? &amp;#x200B; I do think so... I just have the "preparedRequestObject" take the requestType and responseType as phantom params e.g. \`data **Request** req res\` then your \`spreadsheetGet\` simply set the phantoms to the appropriate types. Then, you still get to centralize logic in your \`send\` function instead of the \`spreadsheetGet\`
```class Burrito b where om :: u -&gt; b u nom :: b u -&gt; (u -&gt; b r) -&gt; b r```
I really like articles like these because it helps me think in Haskell. I always figured const was in prelude for completeness' sake.
wonky twisted and crazy. hell, I like it.
Strict languages typically consider lambda to be a value
That's a really convincing argument, but it depends on how frequently it's used. If it permeates your codebase, and everyone has to know it to do any work, then yea giving it an easily recognizable name is probably a good idea. But if you only use it once or twice, then just using combinators that everyone's familiar with is going to be a lot smoother.
Yes thank you for the help,! I managed to get through it and posted what worked on this sub.
Can you think of some examples of features that one would want to implement on top of `spreadsheetGet` and `spreadsheetCreate` that can be done easily only via a unified `send-like function?
You’re right, I’m going to implement and use that now! Unfortunately, your solution won’t work for creating an `Ord` instance, so I’ll have to keep the normalization.
Since `(&gt;&gt;)` is already redundant, OP could've also used `join`.
`(setq haskell-mode-stylish-haskell-path "brittany")`
If I have \`newtype Compose f g a = Compose (f (g a))\` and constraints \`Traversable f, Traversable g, Monad f, Monad g\`, is \`Compose f g a\` then a monad? &amp;#x200B;
Record and reply for testing. Caching idempotent requests?
Tried to do this as an exercise and accidentally ended up with foldl-via-foldr. Not quite sure how to justify the eta expansion that would be needed as the next step, though. https://gist.github.com/Tarmean/948b35420d5280c4ead232e7ebbd25d5
Is this not possible by providing your own implementation for `MonadGoogle` instead of using default behaviour? class MonadGoogle (m :: * -&gt; *) where googleGet :: (FromJSON a) =&gt; Request -&gt; a googleGet = defaultGoogleGet googlePost :: (FromJSON a) =&gt; Request -&gt; a googlePost = defaultGooglePost -- and so on
Is it bad to crosspost from questions? Sorry
See also: [Composing Monads](http://web.cecs.pdx.edu/~mpj/pubs/RR-1004.pdf) They introduce this type class with laws sufficient to prove `Compose f g` is a `Monad`. class (Monad f, Monad g) =&gt; SComposable f g where swap :: g (f a) -&gt; f (g a) And give us some instances, including the following three: -- Today we know this as MaybeT instance (Monad m) =&gt; SComposable m Maybe -- Today we know this as WriterT instance (Monad m, Monoid a) =&gt; SComposable m (Writer a) -- Today we know this as ReaderT instance (Monad m) =&gt; SComposable (Reader e) m And the implementations for the first two instance is exactly `sequence` for `Maybe` and `Writer a`. Maybe you can work out how `SComposable` laws look like when you can assume `Traversable g` in `SComposable f g`.
Yes. How this can in principle be done is described in the paper ([https://www.kosmikus.org/DerivingVia/](https://www.kosmikus.org/DerivingVia/)), specifically in Section 4.3. I've also been working on a library to take this idea further and make it easy to combine various isomorphisms. This is very much work in progress (as in: it may totally change, and it is completely undocumented right now), but you can look at it here: [https://github.com/well-typed/apply](https://github.com/well-typed/apply)
why not rotateUntil p xs = zipWith const (dropWhile (not . p) (cycle xs)) xs zipWith takes the shortest length. Also, your note about Double equality is wrong. It is *not* fine to compare doubles because seemingly equal numbers might not be
Thanks, I'll try that. I thought that GHC Nat was equivalent to Peano number (if not what is the point ?).
The typelits are just there to provide the literals. You can hook in a typechecker plugin (or write a conversion to &amp; from the peano numbers) to get back the ability to prove things about them.
I'm not sure I understand why + needs to be injective ? My type is basically equivalent to \`N a where N :: F a -&gt; N a\`, so why do you need \`F\` to be injective to write get :: N a -&gt; F a get (N x) = x Is it not just part of the definition of \`N\` ?
I see, the problem is that I use `(n + 1)` do determine the type of `NMap` instead of using it in its argument.
&gt; Could not deduce: n2 ~ n from the context: (n GHC.+ 1) ~ (n2 GHC.+ 1) `+` is not an injective type family, so ghc cannot drop simply subtract 1 on both sides in order to learn that `n2 ~ n`. Here's a minimal program reproducing the problem: {-# LANGUAGE DataKinds, RankNTypes, TypeFamilies, TypeOperators #-} import GHC.TypeLits -- error: Could not deduce: n ~ m from the context: (n + 1) ~ (m + 1) stripPlus1 :: (n + 1) ~ (m + 1) =&gt; proxy n -&gt; proxy m -&gt; (n ~ m =&gt; r) -&gt; r stripPlus1 _ _ body = body The easy solution is to use a different representation for `Nat`s, one for which `Succ` is injective: data Nat where Zero :: Nat Succ :: Nat -&gt; Nat stripSucc :: ('Succ n) ~ ('Succ m) =&gt; proxy n -&gt; proxy m -&gt; (n ~ m =&gt; r) -&gt; r stripSucc _ _ body = body Now let's look at the hard solution :) While ghc doesn't know that it can subtract 1 on both side, we do know that it is true, so we can tell ghc to trust us: import Unsafe.Coerce -- error: Could not deduce: n ~ m from the context: (n + 1) ~ (m + 1) stripPlus1' :: (n + 1) ~ (m + 1) =&gt; proxy n -&gt; proxy m -&gt; (n ~ m =&gt; r) -&gt; r stripPlus1' _ _ = unsafeCoerce Huh, it didn't work? I think ghc is specializing `unsafeCoerce :: a -&gt; b` to `unsafeCoerce :: r -&gt; r` and then trying to resolve the `n ~ m` constraint. No matter, `unsafeCoerce` is like violence; if it doesn't solve your problem, you need to use more of it: stripPlus1'' :: (n + 1) ~ (m + 1) =&gt; proxy n -&gt; proxy m -&gt; (n ~ m =&gt; r) -&gt; r stripPlus1'' _ _ = unsafeCoerce unsafeCoerce There you go. The first `unsafeCoerce` changes the type of the second `unsafeCoerce` from `a -&gt; b` to `(n ~ m =&gt; r) -&gt; r`. Since `n ~ m` is just a type-level constraint, not a typeclass, it's probably not represented at runtime by a dictionary, this should be safe. Right? I better use this on a slightly bigger example, just to check the program doesn't crash... {-# LANGUAGE GADTs, ScopedTypeVariables, StandaloneDeriving, TypeApplications #-} import Data.Proxy data Vec n where Nil :: Vec 0 Cons :: String -&gt; Vec n -&gt; Vec (n + 1) deriving instance Show (Vec n) -- | -- &gt;&gt;&gt; :{ -- zipVec ("foo" `Cons` ("bar" `Cons` ("baz" `Cons` Nil))) -- ("FOO" `Cons` ("BAR" `Cons` ("BAZ" `Cons` Nil))) -- :} -- Nil zipVec :: Vec n -&gt; Vec n -&gt; Vec n zipVec Nil (Cons _ _) = error "ill-typed" zipVec (Cons _ _) Nil = error "ill-typed" zipVec Nil Nil = Nil zipVec (Cons x (xs :: Vec n)) (Cons y (ys :: Vec m)) = stripPlus1'' (Proxy @n) (Proxy @m) $ Cons (x &lt;&gt; y) (zipVec xs ys) Huh, that's not the result I expected at all! I'm guessing that I was wrong and that `n ~ m` _is_ represented at runtime as a unused dictionary, and that I'm lucky that the incorrectly-coerced result could be interpreted as the value `Nil` instead of crashing the program. All right, since the dictionary argument is unused, we should be able to use any other value, for example `()`, and it won't be used so it won't crash, right? stripPlus1Helper :: (n + 1) ~ (m + 1) =&gt; proxy n -&gt; proxy m -&gt; (n ~ m =&gt; r) -&gt; () -&gt; r stripPlus1Helper _ _ = unsafeCoerce unsafeCoerce stripPlus1''' :: (n + 1) ~ (m + 1) =&gt; proxy n -&gt; proxy m -&gt; (n ~ m =&gt; r) -&gt; r stripPlus1''' proxyN proxyM body = stripPlus1Helper proxyN proxyM body () -- | -- &gt;&gt;&gt; :{ -- zipVec' ("foo" `Cons` ("bar" `Cons` ("baz" `Cons` Nil))) -- ("FOO" `Cons` ("BAR" `Cons` ("BAZ" `Cons` Nil))) -- :} -- Cons "fooFOO" (Cons "barBAR" (Cons "bazBAZ" Nil)) zipVec' :: Vec n -&gt; Vec n -&gt; Vec n zipVec' Nil (Cons _ _) = error "ill-typed" zipVec' (Cons _ _) Nil = error "ill-typed" zipVec' Nil Nil = Nil zipVec' (Cons x (xs :: Vec n)) (Cons y (ys :: Vec m)) = stripPlus1''' (Proxy @n) (Proxy @m) $ Cons (x &lt;&gt; y) (zipVec' xs ys) Much better :)
Could you clarify on what exactly are those layers supposed to represent and how do you expect them to behave? As for typeclass aproach, have you taken a look at existential types?
const has plenty of uses, eg: eitherToMaybe = either (const Nothing) Just
… including me. :) I'm all open for explanations.
Awesome! I need to spend some time with Brick, so I'm digging into this code. Thanks!
Its an in-joke: &amp;#x200B; [https://blog.plover.com/prog/burritos.html](https://blog.plover.com/prog/burritos.html) [https://chrisdone.com/posts/monads-are-burritos](https://chrisdone.com/posts/monads-are-burritos)
Ah, this my first time using Brick, so maybe there are better ways to do some of the things 😐.
That looks cool, hadn't seen it before
This was an amazing talk! It inspired me to dive way deeper into category theory. Program Synthesis unlocks so many possabilities for truelly correct programs.
The recursive data type idea is an interesting idea. In practice it seems like the ADT approach. The existential types approach seems promising. It is like another idea I was thinking of where instead of a list of layer types I make a list of partially applied computeValue functions. I will look at using the existential types method.
It is, but then, DRY. You could write one wrapper, or you could have to write many. One is better.
No, because the result would have different types based on a single type. That is, you would need \`foo 1; foo :: Int -&gt; a -&gt; a\` but at the same time \`foo 2; foo :: Int -&gt; a -&gt; b -&gt; b\`. That doesn't work until we have dependent types (this is why there is a push for dependent types.) In other words, you are only giving it one input type (\`Int\`) so you can only get one output type (\`a -&gt; a\` *or* \`a -&gt; b -&gt; b\` *or ..)* HOWEVER... with a type family \`NAryConst\`, you can make \`foo :: KnownNat n =&gt; NAryConst n; foo @1 :: a -&gt; a; foo @2 :: a -&gt; b -&gt; b\` , because now you are feeding it a *different* input type for each of the different result types you want.
You can keep a central send if spreadsheetGet only builds a data structure describing a request and response instead of sending it. Put the response type as a type parameter on that data structure. The key is keeping the build and send phases separate 
You know more than I do at this point :)
Brick author here. Cool project! I'm glad you enjoyed using Brick and I'm very glad you found the documentation helpful. If you have any suggestions on improvements, especially about pitfalls, please let me know and I'd be happy to consider adding new material to the User Guide.
[Int] is a linked list of Ints. In a purely functional setting, linked lists and stacks are the same thing (you always operate on the head, never partway through or at the end). The rest of the question is not clear to me, though. I don't really know what a symbol is in this context, and it doesn't say how to transform the stack. All I can guess is that you are intended to do some constant time operation on the head of the list. 
Ah, by symbols he means operators. I reckon he wants to be able to input something such as: 12 4 5 8 + , (Int list AND single String) and have the function operate the last two (8+5) , then return 12 4 12 as a new stack. I have no clue how to approach this yet, though. &amp;#x200B; Thank you for your input.
Hi! In terms of pitfalls, I don't think there are that many - * The current implementation of `Markup` is really slow (it also doesn't have a wrapping widget, unlike `String` or `Text`). I tried using it initially but the app became very sluggish to the point of being unusable. I realize that this is experimental, so I ended up writing my own type which is much faster. * I'm not sure what the best way to have state transitions is. I tried using GADTs but the types wouldn't work out... Do you put everything in one big ADT and branch on the constructor? That seemed quite messy (also, now you can't enforce which state can jump to which other state), so I didn't do that. By state transitions, I mean that your app flow might look like start --&gt; S1 --&gt; S2 --&gt; S3 --&gt; S6 --&gt; exit ^ | | v S5 &lt;-- S4
Great work, really great. The example of programs templates with holes in [synquid](http://comcom.csail.mit.edu/comcom/#Synquid) has a direct applicability to IDE's. I expect to see this working in Haskell in no so distant future. And maybe the missing killer functionality of the language.
&gt; It tells you they build together This ain't nothing... this was *the* game-changing brilliant idea that made it possible to defeat cabal hell once and for all! The only downer is that not all of Hackage is in Stackage. But I've been able to avoid depending on anything that wasn't in Stackage so there's that.
I'm not sure I understand fully the \`unsafeCoerce\` bit, but it indeed works using \`Nat\`. Thanks
As for `Markup`, yes, that is experimental and dates back to earliest releases of Brick. I don't use it much, but it's something I thought would be important in the beginning. In practice I think that hasn't turned out to be true. However, if you implemented something with equivalent functionality that performs better and wanted to contribute it, I would be happy to take a look! As far as state transitions are concerned, it sounds like you want something that will statically enforce legal state transitions, and to do that you want to be able to transition between different state types rather than having to use one state type for the entire application. Is that right? That's something I've wanted in my own Brick usage, too, but I haven't given much thought to how it would be done. When I've done it I've used different apps just as you did, but that definitely has some disadvantages. I'll give this some more thought. Thanks for the feedback!
Really makes me want to try this. And hopefully, they will get it to scale sooner rather than later.
&gt; A top, top qualitee name. What does it mean?
\`haskell-mode\` has this built-in. The default formatter is \`stylish-haskell\`, but that can be easily changed: \`\`\` (setq haskell-stylish-on-save t) (setq haskell-mode-stylish-haskell-path "brittany") \`\`\`
That's an in-joke related to [Arsenal](http://www.justarsenal.com/wenger-top-top-quality-arsenal-should-have-scored-more/65612). One of the players is named Özil, he is famous for assisting other players.
And of course http://emorehouse.web.wesleyan.edu/silliness/burrito_monads.pdf
Perhaps, but it's still exquisitely delicate.
WOW!!! That was amazing! This reminds me of my concept of a programming language based on disjunctions and negations instead of implications, where it starts with some sort of input which constrains the output, then breaks down the set of possible outputs to find the one which best fits the input...
When I saw the title on my front page I thought it's from /r/soccer for a moment.
Luxury program. ;)
[`const`](http://hackage.haskell.org/package/base-4.12.0.0/docs/Prelude.html#v:const) 😜 Given it mentions the use of [`read`](http://hackage.haskell.org/package/base-4.12.0.0/docs/Prelude.html#v:read), I'm guessing it means transform the given string onto the number and push it on the stack (the `[Int]` first argument). I'm not sure how that squares with what you mentioned in another post about the string standing for operators, though. You should try and get clarification on that.
[*Did you mean: -XColonectomy](https://github.com/ghc-proposals/ghc-proposals/pull/118)
&gt; *... written `::` with spaces, that saving drops to 137,9140 characters.* Was "137,9140" meant to be "1,379,140?"
What you think commas are cheap? I don’t care if the scientists across the street are using delimiters every three digits. In this sub we ration our commas. 
My implementation of markup is [here](https://github.com/theindigamer/ozil/blob/master/src/Brick/FastMarkup.hs). Right now, it only supports limited operations (the ones I'm interested in), primarily the ones I've needed. Yeah, the state transition thing is how you describe it. Maybe there's a clever way to enforce things statically, but I couldn't figure out how. --- Also, on an unrelated note, do you have suggestions for testing? This is my first time writing something GUI related, and I'm at a total loss on how to test things. I have a few tests for the parsing side of things but nothing really for the display side.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [theindigamer/ozil/.../**FastMarkup.hs** (master → bbdce2d)](https://github.com/theindigamer/ozil/blob/bbdce2d2b33a35ce7cfab77980a7e1886abeb3e3/src/Brick/FastMarkup.hs) ---- 
A proper Ordering hierarchy would honestly be fantastic. With a proper `Enum` class and lattice / partial ordering support as well.
The real mistake is storing and thinking about programs as flat text files with burned-in syntax.
Source code length is not a cost. The misguided desire for terse code is the real cost. (insert SCIP quote on the purpose of source code here)
Hot damn! We could have saved a whole 1.7MB in hackage!
&gt;**Results** &gt;Instances of `::` = 1,991,631 &gt;Instances of `:` = 265,880 (of which 79,109 were surrounded by two spaces, and 26,931 had one space) I'll assume each instance of `::` had a space character on either side, since that's how type signatures are always presented. There were 265,880 – 79,109 – 26,931 = 159,840 instances of `:` on its own. 1,991,631 × 4 chars + 159,840 × 1 char + 26,931 × 2 chars + 79,109 × 3 chars = 8,417,553 characters dedicated to colons. Now suppose `:` and `::` swapped roles, keeping everything else the same. &gt;Instances of `:` = 1,991,631 &gt;Instances of `::` = 265,880 (of which 79,109 were surrounded by two spaces, and 26,931 had one space) 1,991,631 × 3 chars + 159,840 × 2 chars + 26,931 × 3 chars + 79,109 × 4 chars = 6,691,802 characters dedicated to colons in the alternate scheme. 20% reduction in characters used. If people would prefer to out spaces around every `::`, as the designers expected, we'd get 7,038,413 — a 16% decrease. Probably doesn't really matter. 
This is basically `&lt;*` from Applicative. You are returning the results of the left hand side with the shape / effect / context of them both combined (e.g intersection or non deterministic selection or zip). If ZipList was more ergonomic to use instead of List then you would probably use it instead of zipWith. Although you really want `Apply` from semigroupoids to really utilize this effect, ideally it would be a superclass if `Applicative`. A lot of cool uses of `&lt;*` don’t work because the type isn’t able to implement `pure` (Map, IntMap etc.) I have personally used the above pattern all over the place for an ECS for a game. Using a combination of `&lt;.&gt;` and `&lt;.` depending on whether I want to actually use the component or just to make sure I only operate on objects who have the component. 
The more general pattern is `&lt;*` / `&lt;.` from Applicative / Apply. Specifically the ZipList instance and all the various Map-like instances. If ZipList was as ergonomic as List and treated as a real alternative list type rather than a temporary wrapper then we could just use the above and IMO it is a very intuitive consequence of the way Applicative works.
I really cracked at that monstrous code that got posted to the OP though. Holy hell what a monstrosity! 
UI testing is notoriously messy and I don't think the situation is any better with terminals than it is with graphical environments. I think at one point I saw a screen-scraping thing that might have worked, but I imagine that expressing the tests was probably not fun or easy. My suggestion would be to make sure the rest of your data model and state transitions are testable, use good abstractions to make sure your event handlers are constrained in how they change your application state, and focus your testing efforts on that.
Thanks. Since you mention screen-scraping, I was just reminded, you might want to include a small note or something in the Readme regarding [Asciinema](https://asciinema.org/). It is a popular tool to record terminal casts (e.g. the recent VOIDSPACE game had one), but it doesn't seem to always work with the Brick TUI. For example, I had some highlighting (the one shown in the video as "current link"), but in the recorded casts, it would show up as italics (even when testing it on the same machine). So I ended up using another tool for recording.
Meh. The mental cost when typing is pretty much the same. The time cost only slightly increased when hitting the same key twice. Increase in line numbers should be close to zero, lines mostly just get longer. So pretty much no difference in code density. Lines don't even get longer with fancy editors that substitute a unicode symbol for ::.
But it looks so nice when you align `::` with `=&gt;` and `-&gt;` myFunction :: (MonadFoo m, MonadBar m) =&gt; Text -&gt; Configuration -&gt; m ()
waves
i kinda agree, but text files with a consistent syntax have nice properties. portability, robustness, legibility, etc.
In our codebase we always leave the `::` on the same line as the identifier it declares. So this is good: ``` thing :: a -&gt; a -&gt; a ``` But this isn't: ``` thing :: a -&gt; a -&gt; a ``` This way, if you multi-file sear
use hasktags
WHen we say `v` is in scope in `e1` it means the expresssion, `e1`, can mention `v` directly. For example: let v = f v 4 in v In the above the expression `e1 ~ f v 4` as in, the function `f` applied to variable `v` and value `4`. 
Or `fast-tags`
Also look at `&lt;.` and `.&gt;` from `Apply` in the semigroupoids package, they are equivalent to `&lt;*` and `*&gt;` but more general as they work on types that lack a reasonable/lawful definition of `pure` such as various Maps. 
I read the use of "cost" as being tongue in cheek. 
Parallel list comprehensions are zips, so you could use those as well, like this: {-# LANGUAGE ParallelListComp #-} dropFromEnd n xs = [ x | x &lt;- xs | _ &lt;- drop n xs ] firstHalf xs = [ x | x &lt;- xs | _ &lt;- halfAsLong xs ] rotateUntil p xs = [ x | x &lt;- dropWhile (not . p) (cycle xs) | _ &lt;- xs ] The second list is used only for the zip length and not its values.
Syntax trees! (Like the Union does/will do)
If I could control the universe I would make Unicode easy to type and standardize Haskell on a constrained subset of it, but since I don’t I would rather make it easy for people to type Haskell code. 
I would say this is one of those: “flat text is the worst format for code, except for all the other ones we have tried” situations. 
I do agree, but replacing the first colon with a space IMO does not detract significantly from its appearance. I’m somewhat used to it anyway with combining `=` and `&lt;|&gt;` or similar.
OP works on this editor - https://github.com/lamdu/lamdu
I think Eta will be a net negative in that regard, fracturing the ecosystem and leading to the creation of libraries not compatible with Haskell proper. I totally understand using a totally different compiler for the JVM, but changing the language name and not guaranteeing compatibility with Haskell proper completely kills any excitement for me. 
Replacing first colon with space also doesn't help to save keystrokes, there's still a character ¯\_(ツ)_/¯
You dropped this \ *** ^^&amp;#32;To&amp;#32;prevent&amp;#32;anymore&amp;#32;lost&amp;#32;limbs&amp;#32;throughout&amp;#32;Reddit,&amp;#32;correctly&amp;#32;escape&amp;#32;the&amp;#32;arms&amp;#32;and&amp;#32;shoulders&amp;#32;by&amp;#32;typing&amp;#32;the&amp;#32;shrug&amp;#32;as&amp;#32;`¯\\\_(ツ)_/¯`&amp;#32;or&amp;#32;`¯\\\_(ツ)\_/¯` [^^Click&amp;#32;here&amp;#32;to&amp;#32;see&amp;#32;why&amp;#32;this&amp;#32;is&amp;#32;necessary](https://np.reddit.com/r/OutOfTheLoop/comments/3fbrg3/is_there_a_reason_why_the_arm_is_always_missing/ctn5gbf/)
Hey, that makes sense!
Well, even ignoring that this was mostly tongue in cheek, just because something is a cost doesn't mean it's bad. We pay costs because it yields rewards. Even if that exchange is positive, it's still a cost. So yea, source code length *is* a cost... in terms of space usage. But the rewards are often great. Furthermore, there's not a cognitive advantage to the colon situation haskell has compared to if we swapped `:` and `::`. So we're paying a cost without getting a reward.
Have you tried your code? It won’t work for empty lists, as mentioned in the article. let rotateUntil p xs = zipWith const (dropWhile (not . p) (cycle xs)) xs in rotateUntil (const True) [] :: [Int] *** Exception: Prelude.cycle: empty list And your note about my note about Double equality is wrong, but since you didn’t put forward any arguments neither will I.
Like [lamdu](http://www.lamdu.org/) does for its own ML-derivative?
I feel like you missed my point there. I’m not saying single colon is better for multi line signatures, I’m just saying it’s not any worse. Single colon is better for every other situation. Particularly things like inline type signatures for ambiguous types. 
I have another statistic: "Counting the colons in hackage" for the glory of type level programming: 39 point, 32 comments is 11 hours "Type driven program syntesis in Haskell" 68 points 5 comments in 24 hours Conclussions: type-driven bike-shedding wins
They are talking about 1 character difference per function, not 16.
This never bothered me until I stared doing typescript. Now I'm getting them mixed up constantly.
[removed]
Such a programming language has been implemented at KULeuven: it's called [IDP](https://dtai.cs.kuleuven.be/software/idp/), and supports "constraints on output" in First Order logic (and more). From a given set of constraints, many different sort of inferences are made possible.
Why? What’s the alternative?
Would these syntax trees have a textual serialization format? And, if so, what’s the difference between this and regular code?
What are the pause time we are looking at? Sub 10ms? Sub 1ms? 
I know researchers who are working on improving FRP, but that doesn't mean it's an under-studied field, far from it! Different FRP libraries indeed have different APIs. My [FRP Zoo](https://github.com/gelisam/frp-zoo#readme) repo lists and explains the main differences. It also demonstrates how to bind every Haskell FRP library to gloss so you can do graphics. The most popular FRP libraries are reflex and reactive-banana, and they both use a monadic API, not an Arrow-based API.
For arrow-based FRP, [Yampa](http://hackage.haskell.org/package/Yampa) seems to be a well-maintained library.
I think this is a good suggestion, but it still doesn't provide the full benefits of this style. With this, I can, for instance, easily search in the repository history to gather information about a particular identifier. I'm all for high-tech tooling, but nothing beats simple string searchability :) 
Cool! Thanks for telling me about that!
Oh nice! I remember Yampa was looking for new maintainers, I'm glad it fell into capable hands.
Maybe we can also count the cost of using spaces instead of tabs, especially when people align things like this: myFunction :: Int -&gt; String myFunction a = case a of 0 -&gt; "zero" 1 -&gt; "one" 2 -&gt; "two" 3 -&gt; "three" _ -&gt; "unknown" I mean, sure, spaces are compressed to practically nothing when storing files in Git repositories, but still... Sarcasm aside, I would also prefer single colons for types, double colon for `cons`, but I'm never going to lose sleep over this. 
[removed]
I get paid $$$ to write Reflex apps so in that sense Reflex is ready. However, you have to go off the beaten path to use it correctly. For example, the hackage upload is very out of date and it is recommended to use the `develop` branch of the git repo. Reflex is most developed for use with the DOM which may or may not be adequate for your use cases. I don't think an OpenGL binding would be hard but it is a lot of work no one has put in yet.
Brittany (source formatter) doesn't seem to accept unicode for me :(
Hi! We're using Haskell for server backend at work and Elm for frontend and were hit but this issue as well :( One of the major drawbacks in both `elm-export` and `elm-bridge` libraries is the inability to handle data types with phantom type parameters, like this one: newtype Id a = Id { unId :: UUID } We even had to write some data types, encoders, decoders and API request functions by hands (which is quite unpleasant). And all libraries become broken after Elm 0.19... It's so unpleasant that we're planning to write our own library that supports ordinary data types (both records and plain constructors), sum types (including very useful special case with enums), automatically generated encoders and decoders and of course API requests for `servant` :)
You're right about that `fmap`. Check out the `Functor` instance for functions http://hackage.haskell.org/package/base-4.12.0.0/docs/src/GHC.Base.html#line-818
1. Could you fix the code formatting by indenting the code by 4 spaces? Having the asterisks is important 😅. 2. `mF` is present under Control.Applicative and is called sequenceA. 3. If you look at the instances for Applicative (in that module), you'll see `Applicative ((-&gt;) a :: * -&gt; *)` is listed. This means that functions from a type (in your case `a = Int`) have an applicative instance. In your case, all the functions have type have the same input type, so you end up using that applicative instance. `pure` acts like function application, and splat feeds the same input argument to both sides. That's a super terse explanation, so please feel free to ask followup questions.
Oh I wasn't pulling the connection between "famp (:) (*2)" and the Arrow Type. instance Applicative ((-&gt;) a) where pure = const (&lt;*&gt;) f g x = f x (g x) liftA2 q f g x = q (f x) (g x)
Thanks for the linking. I'm not well versed with the (-&gt;)-type yet. 
[removed]
Relevant functor/applicative stuff for functions: f :: b -&gt; c g :: a -&gt; b fmap f g = \x -&gt; f (g x) pure x = \_ -&gt; x f :: a -&gt; b -&gt; c g :: a -&gt; b f &lt;*&gt; g = \x -&gt; f x (g x) 
Thanks for your comment. instance Applicative ((-&gt;) a) where pure = const (&lt;*&gt;) f g x = f x (g x) liftA2 q f g x = q (f x) (g x) This was what I was missing. 
No, the _ means "an argument that we won't be using, so we don't even bother giving it a name". Could also write this: pure x = \y -&gt; x
It's pure *specifically for functions*. It looks different when you implement it for other types. Maybe: pure x = Just x List: pure x = [x] 
Yeah thats where I was wondering at first. 
Indeed
just figured it out! instead of \`runEffect\` -&gt; \`Pipes.fold (&lt;&gt;) mempty id\`!
A generalization of this pattern is using `Pipes.fold`, along with adapter functions like [`purely` and `impurely`](http://hackage.haskell.org/package/foldl-1.4.5/docs/Control-Foldl.html#g:6), to run folds from the [foldl](http://hackage.haskell.org/package/foldl-1.4.5) package. This gives you the ability to compose folds applicatively, and it works with all streaming libraries too boot.
Actually, `pure` is like `const`. pure :: (Applicative f) =&gt; a -&gt; f a pure @(b -&gt;) :: a -&gt; (b -&gt; a) ~ a -&gt; b -&gt; a const :: a -&gt; b -&gt; a &gt; pure 27 6 27 &gt; const 27 6 27 &amp;#x200B;
What's the performance of Reflex like in the DOM? In [performance benchmarks](https://stefankrause.net/js-frameworks-benchmark8/table.html) it seems to preform rather slowly, yet I haven't seen much discussion about this on Reddit or elsewhere. We've been using Elm in production and are extremely pleased with the performance. However, we've started using Haskell with Servant for our APIs and it might be nice to one day use Haskell on the front end as well. Reflex looks great but its reported performance makes me apprehensive. Any insight would be appreciated.
You can avoid having `unsafeCoerce` in your own code with the help of the `Data.Constraint.Nat` module from the `constraints` library. It's filled with proofs (in the Curry-Howard sense) of various propositions involving `Nat`. I scanned the type signatures in the module to find that what want in this case is [plusIsCancellative](https://hackage.haskell.org/package/constraints-0.10.1/docs/Data-Constraint-Nat.html#v:plusIsCancellative): plusIsCancellative :: forall n m o. ((n + m) ~ (n + o)) :- (m ~ o) This is an *entailment* (a concept from `constraints`), that says that "if we get the same result adding either **m** or **o** to the same number **n**, then **m** and **o** are equal". So how do you use this? Let's start from /u/gelisam's minimal example: -- error: Could not deduce: x ~ y from the context: (x + 1) ~ (y + 1) stripPlus1 :: forall x y r proxy. (x + 1) ~ (y + 1) =&gt; proxy x -&gt; proxy y -&gt; (x ~ y =&gt; r) -&gt; r stripPlus1 _ _ body = body I've added an explicit `forall`, so that we can refer to `x` and `y` in the function definition, with `ScopedTypeVariables`. \`body\` on the right-hand-side is the expression that needs the context \`x \~ y\`. At the moment it only has the context `(x + 1) ~ (y + 1)`. In order to apply an *entailment* to an expression, `constraints` gives us `(\\)`: -- | Given that @a :- b@, derive something that needs a context @b@, using the context @a@ (\\) :: a =&gt; (b =&gt; r) -&gt; (a :- b) -&gt; r If you told me that this type signature was tricky to wrap your head around, I would be prepared to agree. Basically, `a \\ e` "enriches" `a`'s context with whatever constraint that the *entailment* `e` entails (the rhs of the `:-`), given that the constraint that `e` needs (the lhs of the `:-`) is in context. So let's try using this! We're going to need `TypeApplications` to tell `plusIsCancellative` which `Nat`s we care about\*: stripPlus1 :: forall x y r proxy. (x + 1) ~ (y + 1) =&gt; proxy x -&gt; proxy y -&gt; (x ~ y =&gt; r) -&gt; r stripPlus1 _ _ body = body \\ (plusIsCancellative @1 @x @y) Now — you might have already suspected this — we get a new type error: Could not deduce: (1 + x) ~ (1 + y) from the context: (x + 1) ~ (y + 1) The "same number" **n** was always on the wrong side of the `+`, after all! The context that we have isn't exactly what `plusIsCancellative` wants. That's a bit annoying and one workaround that you might go for here is to go back and rewrite your other definitions, like `Cons` as `1 + n` instead of `n + 1`. That can work, but it might mess up other things or be otherwise undesirable. So let's just prove this equality too! Of course, `Data.Constraint.Nat` has what we need: [plusCommutes](https://hackage.haskell.org/package/constraints-0.10.1/docs/Data-Constraint-Nat.html#v:plusCommutes). plusCommutes :: forall n m. Dict ((m + n) ~ (n + m)) `plusCommutes` is not an *entailment*. It's just a `Dict`; a constraint that is true in all cases. We have `withDict`, which is very similar to `(\\)`. It brings the constraint inside a `Dict` into an expression's context. With such similar functions, I'm not sure why one of them is an infix operator and the other one isn't. Anyway, we just have to add these extra proofs (there are two sums that are "backwards") to make GHC stop complaining: stripPlus1 :: forall x y r proxy. (x + 1) ~ (y + 1) =&gt; proxy x -&gt; proxy y -&gt; (x ~ y =&gt; r) -&gt; r stripPlus1 _ _ body = withDict (plusCommutes @1 @x) $ withDict (plusCommutes @1 @y) $ body \\ (plusIsCancellative @1 @x @y) Certainly, `Data.Constraint.Nat` uses `unsafeCoerce` on the inside. And that's great, because now you don't have to. `unsafeCoerce` is incredibly easy to misuse, so it really should be hidden behind safe and reusable abstractions; as few and as general as possible. &amp;#x200B; \* We can't just claim `plusIsCancellative` once as some kind of global truth. A typechecker plugin should probably behave a little more like that (I haven't tried one).
Yup, apologies for the mistake. Crossed it out now.
They'd have several textual serialization formats, so long as they all compiled to the same AST. This is theoretically doable right now -- just write an alternate compiler front-end -- but compilers (especially one as monolithic as ghc) aren't typically a lot of help there, save perhaps for llvm's toolchain. You could even edit the serialized form directly as we do now, but in a more limited scope: think of how image-based Smalltalk enviroments do it: by editing just one method at a time, not the whole codebase. Smalltalk is also a warning though: such an editor needs to evolve with the times as well, which is something most smalltalk environments have failed to do. There needs to be a generic structure editor that isn't tied to a specific language and we've yet to see a usable one of those. That was a promise of XML (sort of) , but the less said about that, the better. The DOM however has taught us some lessons about dynamically modifying structured data, and that might give us some insights on designing a better structure editor.
Or an editor that can search across lines. \`/thing\\s+::/\`
 instance Default (Vec 'Zero a) where def = VNil instance (Default (Vec n a), Default a) =&gt; Default (Vec ('Succ n) a) where def = def @a `VCons` def @(Vec n a)
Do you really want a `Default` instance for `Vec ('Succ n) a`? Why is that useful? Same question for monoid's empty? Writing class instances for inductive families like `Vec` probably requires lifting your classes as well. Not entirely sure, because it has been a while since playing with this, but I do remember lifting `Functor` to take an additional index so I could map over GADTs with additional type for the proofs. class PFunctor phi h where pfmap :: (forall jx. phi jx -&gt; a jx -&gt; b jx) -&gt; forall ix. phi ix -&gt; h a ix -&gt; h b ix (I used this to write instances for a FingerTree type written as a GADT https://github.com/sebastiaanvisser/islay/blob/master/src/Container/FingerTree/Abstract.hs) You could probably do something similar for Monoid.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [sebastiaanvisser/islay/.../**Abstract.hs** (master → 96a8f05)](https://github.com/sebastiaanvisser/islay/blob/96a8f058f7bb4dd6e0455b2e47eddfc2be8a254b/src/Container/FingerTree/Abstract.hs) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply eadxyjv.)
It's impossible, and for a good reason: `v :: Vec n a` claims that it is parametric in `n` and `a`, and must thus treat both as opaque types, but it nevertheless claims to be able to construct a vector of length `n`! If I then ask for the length of this vector, I'll magically get an `Int` out of thin air, which makes no sense: magicInt :: Int magicInt = vecLength v So you do need something like a `SNatI n` constraint on the `n` to say that no, this isn't a completely parametric `n`, it's an `n` whose value is known at runtime. By the way, there is nothing wrong with the recursive `Default (Vec n a)` constraint, it's just [polymorphic recursion](http://gelisam.blogspot.com/2014/12/the-99-bottles-of-beers-of-type-systems_21.html) at work.
&gt; I eventually want the library to be used so I want the library available easily to developers, to me this means compatibility with the current Haskell ecosystem including a modern Stack snapshot Anyone using your library will need to have the same set of overrides in their own `stack.yaml` file, so I don't see how this approach actually helps compatibility.
`v` doens't really claim to be parametric in `n` but in `n :: Nat` and all member of `Nat` are also `SNat`. About the polymorphic recursion, the problem I have is I'm writing a function which require `NMap` to be a Monoid (NMap is a nested Map from a previous post). Before I change `NMap a` to `NMap (n :: Nat) a`. Everything was fine because I could write a `Monoid` instance. Now the `Monoid` instance fof the new `NMap` needs a `SNatI n` instance. This causes a problem because my function `f` becomes `SNatI n =&gt; NMap n a -&gt; NMap a` but is recursive on sub element of `NMap` and therefore requires a `NMap (n-1) to be a monoid too, so the new constraints becomes `(SNatI (Succ n) , SNatI n) =&gt; Nmap (Succ n) a -&gt; NMap (Succ n) a` etc ... I simplified the problem with `Vec` and I'm trying to find a equivalent function for Vec (which produce recursive contraints).
we use the following hack: \`instance ElmType (Id a) where toElmType \_ = toElmType (undefined :: Text)\` For 0.19 we're using even more hacks in stack.yam: \`\`\` \# Elm 0.19: [https://github.com/krisajenkins/elm-export/pull/54](https://github.com/krisajenkins/elm-export/pull/54) \- git: [https://github.com/hercules-ci/elm-export](https://github.com/hercules-ci/elm-export) commit: 99b58d040f93d405aa5d13ba24170fc6ca2ed89c \# Elm 0.19: [https://github.com/mattjbray/servant-elm/pull/46](https://github.com/mattjbray/servant-elm/pull/46) \- git: [https://github.com/domenkozar/servant-elm.git](https://github.com/domenkozar/servant-elm.git) commit: db0262b67afef73e53773dd9a9fb8f49f028f73f \`\`\` I hope that once we launch [hercules-ci.](https://hercules-ci.om)com, we can help with [https://github.com/agrafix/elm-bridge/issues/40](https://github.com/agrafix/elm-bridge/issues/40)
It's a bad example indeed , as it works :-(
&gt;Do you really want a &gt; &gt;Default &gt; &gt; instance for &gt; &gt;Vec ('Succ n) a &gt; &gt;? Why is that useful? Same question for monoid's empty? No, I don't. It is just a simplified example of a real problem which is to write a monoid instance for \`NMap\` (which can be found [there](https://www.reddit.com/r/haskell/comments/9znzpz/problem_with_typelits_nat_deduction/))
`mF` as you present it is a specialization of `sequenceA`. As it happens, `sequenceA [f, g, ...] x` is the same as `[f x, g x, ...]` because `c -&gt;` forms a monad (and hence is an applicative functor). λ:&gt; sequence [(+1), (+2)] 3 [4,5]
RemindMe! 3 years 14 days
I will be messaging you on [**2021-12-08 23:59:27 UTC**](http://www.wolframalpha.com/input/?i=2021-12-08 23:59:27 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/haskell/comments/9wxkeo/a_touch_of_topological_quantum_computation_in/) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/haskell/comments/9wxkeo/a_touch_of_topological_quantum_computation_in/]%0A%0ARemindMe! 3 years 14 days) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! ____id____) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
You could existentialize the n.
Make a type like ``` data SomeVec a = SomeVec (forall n. KnownNat n =&gt; Vec n a) ``` Then you can have a Monoid instance ``` instance Monoid SomeVec ```
Sorry for taking a while to get to you. If you do the following: &amp;#x200B; git clone [https://github.com/chessai/yi-chessai](https://github.com/chessai/yi-chessai) cd yi-chessai nix-build -j 8 #8 = cores on my machine. adjust this number to taste nix-env -i $(readlink -f result) &amp;#x200B;
Due to type erasure fully parametric in `n :: Nat` is equivalent to fully parametric in `n :: Type` or `n :: A` for any `A`, in that you cannot have any runtime behavior be based off of that `n`, you NEED some sort of typeclass constraint. One way to think of it is that Haskell requires constructive proofs, something just being true isn't enough (e.g. it is true that `n :: Nat` is always a member of `SNat`), it has to be true and you have to prove it in a constructive way (e.g. typeclass constraint).
``` data Nat = Zero | Succ Nat data Vec (n :: Nat) a where VNil :: Vec 'Zero a VCons :: a -&gt; Vec n a -&gt; Vec ('Succ n) a deriving instance Show a =&gt; Show (Vec n a) instance Monoid (Vec 'Zero a) where mempty = VNil VNil `mappend` VNil = VNil instance (Monoid a, Monoid (Vec n a)) =&gt; Monoid (Vec ('Succ n) a) where mempty = VCons mempty mempty VCons x xs `mappend` VCons y ys = VCons (x &lt;&gt; y) (xs &lt;&gt; ys) ```
I came to Haskell 3 or 4 years ago and was completely overwhelmed with everything I had to learn both in the language and around it. For me, stack made the entry point really simple. I could 'forget' about everything except the language, which is all I cared to learn at the time. I doubt I would've gotten much traction learning Haskell without it. Thanks so much for your work.
If you’re looking into Haskell/GHCJS on the front end I would also consider Miso, its not FRP, more Elm/React-Redux style, but the performance has been very nice in our experience. 
&gt; all libraries become broken after Elm 0.19 Jesus. Stuff like this makes me grateful for all of Haskell's warts :(
the frp zoo is very cool, thanks
Hello, I cannot find how to do this in HXL... I want to select all elements "a" which contain element "b". I tried: `hasName "a" /&gt; hasName "b"` and various other arrows, but I wasn't able to get this to work -- it returns elements "b" instead of "a". Any ideas? :)
There is already support for newtypes in elm-bridge, but it depends on what you want. As newtypes are just a chore in Elm, you can "unwrap" them by asking for it when deriving the type. There is a bug however in the currently released version. I have to work on it, but I have a ton of stuff on my plate right now, and I have abandoned Elm for good, so it is low priority to me.
Wow, thank you! I’ll make sure to try it when I get a chance 
This is tangent to the entire discussion, but I would argue that simple polygons do not have a natural ordering, and hence should not have an Ord instance to begin with. 
Lensref is an FRP framework which is built around views of the program state. https://github.com/divipp/lensref 
If you try f :: Vec n a f = mempty &amp;#x200B; You get vec2.hs:20:5: No instance for (Monoid (Vec n a)) arising from a use of ‘mempty’ In the expression: mempty In an equation for ‘f’: f = mempty Failed, modules loaded: none. In other you have a Monoid instance for \`Vec Zero a\` another on for \`Vec (Succ 0) a\` but not one for a general \`Vec n a\`. &amp;#x200B;
You can by adding indeed \`Undef (Vec n a)\` but that shouldn't be necessary and still has the problem related to recursion (see [https://www.reddit.com/r/haskell/comments/a01zwt/how\_to\_instantiace\_totally\_a\_type\_with\_natural/eae62ke](https://www.reddit.com/r/haskell/comments/a01zwt/how_to_instantiace_totally_a_type_with_natural/eae62ke))
That depends on what you want to use Ord for. I see Ord only as meaningful in special cases, like with Int, where it has special properties together with another, technically unrelated, class (Num). In all other cases, I treat it as "some lawful ordering". And some lawful ordering is all for example Set and Map need, so I can have a Set Polygon. 
Thanks a lot for the nice explanation! And sorry for super late response, I am not very good at keeping track of reddit. The last sentence feels quite limiting to me (we could know that some things succeeded, but not their results, IIUC) so I guess this wouldn't be useful for OP.
I do not want to choose among many packages. I want other people to choose for me, and tell me it works because 9999 people have checked it does work. The punchline as a user is "dont waste my time". As a builder, the punchline is "let's be open about which is the best library". Clearly user or builder are a different point of view. Just like you do not care of the internal that make the version 1.6.4 of a library, as a user, users of sets of package do not care about the internal of what make LTS 12.5
I was in total shock when I discovered there was a global mutable database of packages when I first tried out haskell, which was causing a lot of trouble. It's ok to have early stage issues, as long as one is mindful about it. I dont see the point in insisting it's not real...
look at the most simple example in https://haskell-servant.github.io
I have now updated [H-Calc](https://github.com/pcarbonn/H-Calc) to use OpenADT: I will publish the new version on GitHub as soon as open-adt is on hackage. I have also updated another DSL interpreter to use open-adt, and it works like a charm. Many thanks !
[Does this help?](https://docs.haskellstack.org/en/stable/faq/#i-need-to-use-a-package-or-version-of-a-package-that-is-not-available-on-hackage-what-should-i-do)
You might need to define what you mean by "web-based MIDI". If your needs are as simple as selecting or uploading tracks, then the problem is relatively simple, and you could write server-side logic with something like servant or scotty. However, if you're talking about an interactive music app like a keyboard or other kind of synth that reacts in real time to user input, you have a very tall order that Euterpea is probably not going to fill. In that second case, you'd need some sort of WebAudio-based MIDI library, which would provide a javascript API you'd talk to via GHCJS -- which is pretty non-trivial to do these days. If you're after a true web-based synth, you might want to look into something like Purescript instead, which will give you a very Haskell-like experience with a much smoother path to integrating with other JS libraries.
Wow! That is super exciting. Sorry it isn't up on Hackage yet. I just want to add a bit to the tutorial to address the issue you posted, which I'm working on right now!
Great to see progress on that issue. Again, no hurry. I just wanted to let you know what I found.
Yes, that's exactly what I needed. Thank you! 
I think you want `hasName "b" &lt;/ hasName "a"`. There might be more efficient ways to do it, but this finds all the "b" nodes, then looks at their parents and filters to get the parents that are "a" nodes.
My understanding is that the `H_` convention is more general, but maybe I've just reading too much of "Haskell Programming with Nested Types: A Principled Approach". I've thought about throwing something up con hackage, but I always end up being unsatisfied with the result. The combinations of Rank-2 types, and the brand new `HFunctor` hierarchy that no existing types are instances for, or the fact that the fold is a natural transformation (as opposed to the simple `-&gt;` used by recursion-schemes) all combines to make me question the utility of it. If *you* throw something on hackage, please do let me know. :) I'll contribute patches if I can and be a hype man.
Hmm, I like this approach. It seems like a good mix of the other ideas I was looking at.
I concur about the naming but /u/edwardkmett has already done a bunch of stuff for Ix. There certainly will be less utility from it if it only exists among a handful of invisible codebases... I’m thinking of trying something but I’m not sure which conventions to follow. For one, there’s the higher kinded classes; for another the explicit fixpoint of the first gist is more elegant on my view but I don’t know what’s sacrificed from the canonical recursion schemes hierarchy. 
I concur about the naming but /u/edwardkmett has already done a bunch of stuff for Ix. There certainly will be less utility from it if it only exists among a handful of invisible codebases... I’m thinking of trying something but I’m not sure which conventions to follow. For one, there’s the higher kinded classes; for another the explicit fixpoint of the first gist is more elegant on my view but I don’t know what’s sacrificed from the canonical recursion schemes hierarchy. 
Change f l () (sft, rem@((`shiftInAtN` replicate (pow2SNat l) 0) -&gt; (r0, r1))) into f l@SNat () (sft, rem@((`shiftInAtN` replicate (pow2SNat l) 0) -&gt; (r0, r1))) what you're doing there is pattern-matching on the [SNat](http://hackage.haskell.org/package/clash-prelude-0.99.3/docs/Clash-Promoted-Nat.html#t:SNat) constructor which brings the encased `KnownNat l` constraint into scope for everything that occurs on the RHS of it.
The answers given by ElvishJerico and Gelisam both really hit the nail on the head: parametricity/erasure/irrelevance. The only solution right now is the technique's described in [Hasochism](http://homepages.inf.ed.ac.uk/slindley/papers/hasochism.pdf). But, the future is bright. [Dependent Types in Haskell: Theory and Practice](http://www.cis.upenn.edu/~sweirich/papers/eisenberg-thesis.pdf) describes a dependent relevant quantifier (named `pi` in the paper but `foreach` in a [GHC proposal](https://github.com/ghc-proposals/ghc-proposals/pull/102)). Some additional clarification on [a mailing list thread](https://mail.haskell.org/pipermail/libraries/2018-June/028878.html) suggests that with dependent haskell, you will be able to solve your first problem with: data Nat = Zero | Succ Nat data Vec (n :: Nat) a where VNil :: Vec 'Zero VCons :: a -&gt; Vec n -&gt; Vec ('Succ n) instance foreach n. forall a. Default a =&gt; Default (Vec n a) where def @Zero = VNil def @(Succ m) = VCons def (def @m) Note the use of type applications to bind invisible arguments.
I've published v1.0 on Hackage! I'm going to continue looking into your issue, but figured I shouldn't hold off on the upload anymore since it's taking some time. Let me know if you have any other questions. I'm super excited that someone is using my package (my first Hackage upload too!).
The code with your modification could compile. Thank you so much. My issue is related to [this post](https://www.reddit.com/r/haskell/comments/7mhafv/dfold_clash_dependent_types_and_knownnat_l/). I checked the post before write mine. But I didn't know that pattern-matching is required for bring constraints into that scope. &amp;#x200B; &amp;#x200B;
&gt; nothing beats simple string searchability Given that dedicated tooling exists for Haskell, it kind of does? 
&gt;I dont see the point in insisting it's not real... Are people insisting this? Afaict the only real problems came with the way politics were introduced via the respective tool's fans handled their interactions. Stack solved major issues for many people, in the same way that `cabal new-*` solves major issues that emerged as a result of that solution. Can we look at this as a positive evolution of build tools without accusing the other party of malcontent? 
It could be a compile transformation, yes, but Formality doesn't change your code. (The later isn't always faster, just different; there are reasons to want both versions in different scenarios.)
Hmm. How different is it? To me it seems to be the difference is equivalent to the following (to borrow from Haskell syntax as I'm only starting to follow Formality). not True a b = b not False a b = a vs not True = (\a b -&gt; b) not False = (\a b -&gt; a) The former will always do the same as the later. GHC (I believe) will realise that b or a are smaller than the lambda and will evaluate and discard the lambda as soon as it receives the bool. When is this not an optimisation? Is formality lazy by default or is the laziness controlled somehow?
Answered [elsewhere](https://www.reddit.com/r/haskell/comments/9t0p5n/monthly_hask_anything_november_2018/eafusfn/).
/u/SrPeixinho: Cool! The language is much more usable given the native types. I wonder about two things. First, do you have some intuition for which algorithms/problems Formality would really shine? I can always formulate an algorithm in an optimal way by hand. However if my problem can be decomposed in many functions recombined in many ways Formality would give good results for all those algorithms due to fusion and sharing? Haskell offers similar benefits due to its laziness but doesn't go as far as Formality. I think the optimal evaluation strategy performs well in particular if functions are of very high order. Second, I would also like to hear a bit about the downsides of your approach. Introducing a maximum amount of sharing leads to a maximal amount of space leaks, I've heard (resulting in large interaction networks?). Is that true? What kind of algorithms cannot be formulated in Formality as of now, but can be written in lambda calculus? Formality also doesn't have a GC. This means cyclic datastructures are disallowed?
I think the main reason why recursion-schemes took off so well in the haskell ecosystem wasn't necessarily that it was theoretically nice or clean, but rather because a *lot* of recursive functions fit well into the generalists catamorphisms, anamorphisms etc., and it was able to cleanly generalize a lot of code that was already out there. Nothing is ever *required* to use recursion-schemes, but it just happened to fit a lot of existing use cases of explicit recursion. However, there is still a lot of explicit recursion code that is *not* easily fittable into generic recursion-schemes combinators. But there was enough to make recursion-schemes take off. In order for an indexed variant to be as useful, it's not just sufficient to be theoretically clean. It isn't meant to enable code that wasn't possible before, remember. It has to be able to provide combinators that can be used to refractor a large swath of different types of explicit recursion. I'm not yet convinced that this is the case, as rarely does my explicit recursion with indexed types follow as regular a pattern as does my non-indexed recursion. But I could be wrong!
No it let's you append two Vecs of different lengths. A SomeVec can have any length. So you can write the initiative version.
I'm actually looking for this myself as well [and also playing with the idea in the last few days](https://github.com/yairchu/syntax-tree). I'll look into the various links you shared, thanks!
&gt; Stack solved major issues for many people, in the same way that `cabal new-*` solves major issues that emerged as a result of that solution. Interesting! I'm very curious about these **major issues** Stack supposedly suffers from and how Cabal solves em. Maybe the fixes can be ported to Stack so everyone can benefit even if they don't use Cabal. 
That sounds like a plausible account; I also wonder about the number of indexed use-cases that conform to the generic recursion. Would be nice to get some anecdotal evidence One of the things I'm interested in though, is the "development interface" of what might be a complex section of domain-specific code. I think that seeing `foo = icata someAlg` with a few variants is much easier to understand than an explicit (catamorphic) recursion for someone just coming to a codebase for the first time. And, if I'm allowed to add an index to a data type without much additional burden, I could do so and potentially speed the comprehension of whoever the code comes to after me.
I think it's easier to understand if you just write out the concrete types: mF :: [f a] -&gt; f [a] What are `f` and `a`? Well, we're applying `mF something` to `3`, so take `f` as `Integer -&gt;`. This is the Reader Applicative / Monad / whatever. So we have mF :: [Integer -&gt; a] -&gt; (Integer -&gt; [a]) There's not very many functions of this type, especially if you aren't conjuring integers out of thin air. The obvious one is `(\fs x -&gt; map ($x) fs)` And it turns out this is exactly what's happening. To go further, you need to derive / find the Applicative instance for `(e -&gt;)`. Deriving it is not too hard, just follow the types; there aren't any choices. instance Functor ((-&gt;) e) where -- f :: (a -&gt; b), k :: (e -&gt; a), -- result :: (e -&gt; b) fmap f k = \e -&gt; f (k e) instance Applicative ((-&gt;) e) where -- pure :: a -&gt; e -&gt; a pure a = \e -&gt; a -- &lt;*&gt; :: (e -&gt; a -&gt; b) -&gt; (e -&gt; a) -&gt; (e -&gt; b) ff &lt;*&gt; fa = \e -&gt; ff e (fa e) -- interesting side note: these are the K and S combinators -- from combinatory logic From there we have mF [(*2), (+1), negate] = mF [ \e -&gt; e*2, \e -&gt; e+1, \e -&gt; negate e ] = \e -&gt; [ e*2, e+1, negate e ] 
As mentioned elsewhere in the thread. ``` f :: Vec n a f = mempty ``` will never work, no matter how you encode things. The runtime representation of `f` depends on the type of `n`, which is impossible without a typeclass constraint propagating the necessary information. You also don't have a `Monoid a` constraint on it, so if you instantiated it later to `Vec (Succ Zero) Int` what would you expect the value to be? However: ``` f :: Monoid (Vec n a) =&gt; Vec n a f = mempty ``` This will work, and is morally equivalent to the above, as for any `Monoid a` and known length `n`, you are going to get that `Monoid` constraint, and thus you can use `f` basically anywhere you want. If you can give examples for where the above doesn't work that would be great, as either they are just fundamentally hard use cases that you can't prove to GHC, or they just require some restructuring / constraint adding.
I really want to see this under more real workloads. Sorting a list of random numbers for instance. Most of the benchmarks you show seem tailored to Formality's strength, in that they do a ton of redundant work, so it's easy to imagine that preventing unnecessary copying would make these dramatically faster. But that kind of redundancy isn't the typical source of performance issues in most code. I'm much more concerned about having to copying every time you need to share data.
You should create a `stack.yaml` in `sample-project` and use that for both `domain` and `web`, like: # stack.yaml packages: - domain - web # the rest o the stack.yaml...
I was in the FRP boat for a very long time as well, I was mainly looking into FRP to do web apps. In the end my conclusion was that that FRP ecosystem in Haskell isn't just there yet, at least not at my level of comfort to use it for production systems. Since then I have switched over to r/elm for everything UI related in a functional reactive way. A thing to note is that initially Elm chose to go the hard FRP way, since then they have moved the FRP concepts under the hood, so you don't directly deal with the FRP concepts when programming but the idea is based on that. I am not sure what your project is but you could go Elm + Electron for a desktop app, or just Elm as a web app.
&gt; First, do you have some intuition for which algorithms/problems Formality would really shine? I'm still experimenting, I want to first work on improving more the GPU runtime up to a point we can get closer to GHC in raw pattern-match / second. When that's done, I have a lot of ideas to explore, and then I'll be able to tell you where Formality shines and if it can truly beat GHC in real world scenarios. &gt; Second, I would also like to hear a bit about the downsides of your approach. Introducing a maximum amount of sharing leads to a maximal amount of space leaks, I've heard Yes, I have heard that argument a few times, but it makes no sense to me. I'm not sure (or don't remember?) exactly what that is trying to say. If you could be a little more specific about the potentially bad cases I could think about it and answer. But in general, no, I don't think that's true. Interaction combinators are very similar to Turing Machines in the sense you can be develop your code in a way that resources used are very predictable, so that doesn't make a lot of sense in general; to me that sounds like saying C has a maximal amount of space leaks... 
Both of your functions are identical, but the two versions of `not` I presented on the post are not!
&gt; However, there is still a lot of explicit recursion code that is not easily fittable into generic recursion-schemes combinators. I don't think this is true for any uniformly recursive data type. Nested data types, and mutually recursive data types do require additional work.
&gt; I don't know how to format those small boxes. Just backticks around them. `` `1` `` yields `1`, for example.
I'd love it if you could elaborate. I've been thinking about data fusion a bit lately but am just getting started in learning about programming language design. Thanks
Wow! What an amazing resource! 
This is great! I was just thinking about this same idea at work last weej while writing a FromJSON instance.
I feel that some Haskell libraries (like database libraries) and patterns (lie "higher-kinded-data") can require quite invasive changes to a data type. That is letting external concerns seep into your model. It seems that Java has a better story here: use annotations to customize the handling of individual fields. Perhaps using "synthetic" data types like in this post can be a way to reduce the need for such invasive changes.
First small project inspired by node js version and for practice purpose :). 
Disclaimer: I actually don't have a problem with \`Vec\` but with \`NMap\` which is working code where I'm trying to add to it a type encoded length, I'm just trying to give similar example with Vec &amp;#x200B; The problem is if you don't \`Monoid (Vec n a)\` but \`Monoid (Vec (n-1) a\` (and your function is recursive) example &amp;#x200B; emptyTailIf _ VNil = mempty emptyTailIf f (VCons x v) = if f x then VCons x mempty else VCons x (emptyTailIf f v) Actually it might even not work if \`emptyTailIf\` is not recursive and you have \`else VCons x v\` instead For information my \`NMap\` is similar to &amp;#x200B; data NMap a = NLeaf a | NMap a (Map Text (NMap a) It's just a nested \`Map\` which also contains a margin : which I'm transforming to data NMap (n :: Nat) a where NLeaf :: a -&gt; NMap Zero a NMap :: a -&gt; Map Text (NMap n a) -&gt; NMap (Succ n) a I have \`normalize\` function along &amp;#x200B; normalize leaf@(NLeaf _) = leaf normalize (NMap _ m) = NMap (getMargin (mconcat (toList m)) m Which can't compiles anymore (because of the \`mconcat\` on the children). I get around and not use the \`Monoid\` instance (\`Semigroup\` will suffice) but I'm not keen on doing refactoring until proven that there is no other solution. &amp;#x200B;
I suppose I mean "easily" as in cleanly. Sometimes the refactored code can be messier than the original explicit close.
I suppose I mean "easily" as in cleanly. Sometimes the refactored code can be messier than the original explicit close.
Wow! Congrats &amp; thanks. (But please remove the legacy uglinesses of the Haskell prelude ;-) )
Ah ok, yeah this again comes down to GHC/Haskell being limited to very constructive proofs. GHC needs proof that the tail of the Vec is still a Monoid when the full Vec is, which we both know but you have to construct it for GHC. Unfortunately you cannot deconstruct instances, so `instance a =&gt; b` does not allow you to retrieve proof of `a` even if that instance is the only way to provide the existing `b` instance. This seems like something that would be theoretically possible for a Haskell compiler to do, and I am assuming a proper dependently typed language would have an easier time. So we need to define our own way to provide these various length Monoid Vec instances. For simplicity we'll just make `emptyTailIf` a class of its own and encode the two possibilities alongside them. ``` class EmptyTailIf f a where emptyTailIf :: (a -&gt; Bool) -&gt; f a -&gt; f a instance EmptyTailIf (Vec 'Zero) a where emptyTailIf _ VNil = VNil instance (Monoid (Vec n a), EmptyTailIf (Vec n) a) =&gt; EmptyTailIf (Vec ('Succ n)) a where emptyTailIf f (VCons x v) = if f x then VCons x mempty else VCons x (emptyTailIf f v) ``` Now you can freely call `emptyTailIf` on a `Vec` of your choosing. If anyone knows a way to do this without any additional classes I would love to hear it.
Sharing can only happen when you use the `copy` keyword, so, if you don't, you will have 0 sharing. Is that what you mean?
That's basically my problem indeed. As I said, in my particular I can find a workaround and I just wanted be sure that it was not me missing anything but a GHC limitation. 
Really good, thank you :-)
I am glad to hear that stage is past, as it was definitely there. Too often the immense value of vetting a set of packages was not seen clearly. The point of tooling, Imo, is to be able to smooth what are otherwise conflicting choices, so it's important to understand the value of each design, which certainly does not involve arguing about petty stuff : If one tool chose global version, and the other chose local, it's because each provide value, to a different use case. I must confess of not being up-to-date with cabal new-*, as stackage makes sense and that "just works" for me. Is there a small guide on how to use cabal new-* from a stack point of view, and vice-versa ?
It's actually your excellent serie about the \`Singleton\` library which got me started with this whole idea. Maybe you can add section about the \`emptyTailIf\` problem in this post ;-)
Hey would you mind asking your professor to upload the source to the book if one exists? I'm just assuming he generated both the PDF and HTML layouts from some singular codebase, and kind of want to port this text to an [mdBook](https://github.com/rust-lang-nursery/mdBook)-powered repository to make it a little easier on the eyes.
This could be a place where the ability for a single package to expose multiple libraries might be useful. https://github.com/haskell/cabal/issues/4206
At which point we wonder what is the point of data types at all when we can just use the generic representation.
The Internal convention is not just for implementation details that may change. It is also for implementation details that break invariants, and there's no getting around that with a separate module.
Ask Dr Cunningham why they refer to values as "parameterless functions," e.g. zeroRat. There are a lot of articles, blog posts, etc. explaining why values aren't functions, but I'm interested to hear if Dr. Cunningham has different reasoning.
I don't understand why a majority of packages expose Internal modules (you can always expose them _after_ some users make a request), but one thing that is annoying about packages which don't, is that you can't navigate the Internal modules on Hackage/Stackage anymore, so now you have to go to GitHub and miss out on the hyperlinked source.
&gt; I don't understand why a majority of packages expose Internal modules (you can always expose them after some users make a request) The users shouldn't have to wait hours or days or weeks for you to respond to their request. &gt; you can't navigate the Internal modules on Hackage/Stackage anymore Yeah that's weird. Why is it?
&gt; The “containers” package would depend on “bit-queue”, and no matter how the “bit-queue” package would iterate on its releases and how frequently it’s gonna bump its major version, the “containers” package would only have to bump its minor version, because it’s not gonna reflect on the API. Presumably, this means that e.g. Map's constructor is not exported, so that the fact that it is wrapping a bit queue or some other representation is not reflected in the API? If so, the streaming or whatever libraries which do depend on this internal detail still won't be able to use Map in their public APIs, open up the internals, and stream them or whatever, because depending on bit-queue doesn't grant them access to Map's constructor. So I think we still need Internal modules to expose constructors which wouldn't be exported otherwise, but otherwise I agree that if Internal modules include datatypes and functions on those datatypes, then that's a good hint that those should probably be moved to a separate library. While we're on the topic: when using the smart constructors pattern, instead of not exporting the dumb constructor or only exporting it in an Internal module, I now prefer to rely on a naming convention: data Foo = UnsafeFoo {..} mkFoo :: ... -&gt; Maybe Foo The idea is that the smart constructor `mkFoo` checks the invariants, and so it may fail, whereas the dumb constructor `UnsafeFoo` doesn't, and so it cannot fail, but it's unsafe because it is now the responsibility of the caller to make sure that the invariants are already satisfied, they can't just rely on the heuristic that "if it type checks, it works". Similarly, if I wanted to hide the implementation details of a datatype because I think they're likely to change (not that I'm ever that forward thinking...), I think I'd use a similar naming convention: data Foo = InternalFoo {..} Meaning that while this time we _can_ rely on the type checker to verify that the call won't fail at runtime (otherwise I'd call it UnsafeInternalFoo), we can't expect code which uses InternalFoo to continue to type check in the next minor version.
do you plan to ever publish the framework as open source? how will it compare to the existing ones? will it be based upon any of the existing numerical/scientific computing frameworks/libraries?
If you want people that are authors of "multiple popular libraries" you might want to spend more than $70k to attract them...
I get what you're saying but A. As a maintainer, you can be quick about responding to minor requests like this one. B. Users can always work off a temporary fork if it is urgent. Given that you can easily point the build system to repos on Github or elsewhere (I know stack has this, I'm guessing cabal-install has it too), it is a quick ~5 line change.
&gt; There's plenty of code that has no business being version controlled I think one of the best reasons to expose an `Internal` module is so that users can use `derive (Generic, SomeOtherClass, etc.)`. You might not want to expose the constructors in the general API, and in any case renaming fields won't matter. 
The UI looks good! How does the search suggestion thing work? Is it just a list of topics that you wrote? It is a little annoying that if your search term is a substring of one of the suggested ones, then you can't search for it (pressing enter selects the first from the suggested list).
I'm not sure why B. would involve a lot of friction -- that is probably my inexperience speaking. Can you explain?
They're not? Considering values as being nullary functions makes intuitive sense to me. 
A user has to * fork your repo * make the edit and commit it * switch to Stack, because Cabal doesn't support arbitrary sources, AFAIK * change *every* dependency in their codebase to the new source * wait for you to merge it * change *every* dependency back * every downstream consumer of your package has to change their dependencies too and even then it's not guaranteed that you're exposing something safe so it may have to go in Internal anyway. I have come across examples of unexposed internals enough times that I just wish people would do it by default. Off the top of my head * https://stackoverflow.com/questions/20435797/how-can-i-get-esqueleto-to-generate-an-sql-string-for-me 
it is rare to see such well written job advertisement
There are compelling arguments for why they shouldn't be, like [Conal's](http://conal.net/blog/posts/everything-is-a-function-in-haskell). I think the clearest is simply that we define a function in Haskell as a type whose outermost type constructor is `-&gt;`, and if you want to motivate this then you can use Conal's arguments.
Fyi cabal does support arbitrary sources these days using `cabal.project` file
OK, thanks!
Tried searching for "intro", "prelude", "first", "introduction". As of 2018-11-26T16:19Z, https://www.haskellbazaar.com/ is not very useful for a beginner.
All fair points. I was thinking more from an application perspective, but this is much more painful from a library perspective, especially if you have to pass down the dependency to downstream users. If your PR was merged quickly, say in a day or two, then perhaps not such a big deal, but if it takes a couple of weeks or more, this can get bad. As an aside, I wonder what the OCaml and Rust people do about this - they seem to have fairly rigid conventions about not exposing internals.
Ah. Yeah, I think my own reasoning here falls into the "different conversation" category of that post: I tend to treat functions and values as equivalent in my own internal informal reasoning. For some reason it didn't occur to me that the person I was responding to probably meant "values aren't functions, *in Haskell*". You'd think I'd realise given the subreddit we're in. 
Perhaps I've failed to deliver an important point across. I was implying that whatever the internal details that need to be exported should be exported, but in different libraries instead of Internal modules. In case of the Map constructor this means that it should be exported by another lower-level library, e.g. "containers-core". "containers" in that case would become a wrapper, which reexports a fraction of the API. The authors of streaming libraries would be depending on "containers-core" as well. The benefit from the perspective of the streaming libraries then would become that they'll again become able to depend on version ranges. Compare this to the current situation, where they have to depend on specific versions, because according to the Internals convention the authors of depended packages can drastically change the exposed internal modules without major version bumps. To give you an example, I've already successfully employed that approach in my "potoki" ecosystem, where the lower-level API is exposed by the ["potoki-core"](http://hackage.haskell.org/package/potoki-core) package, which is intended for low-level integrations, while the ["potoki"](http://hackage.haskell.org/package/potoki) package hides the low-level details. 
I thought I was going to disagree with this proposal, but I actually really like it. I'd even extend it: - *All* modules for a package should be in `exposed-modules`. - *Any* `other-modules` should be factored into other packages. The `other-modules` feature is a mistake, IMO, and has only brought me frustration and speedbumps.
Looks very promising!
&gt; It is also for implementation details that break invariants, and there's no getting around that with a separate module. Can you elaborate on that please?
For example, Opaleye exposes \[Opaleye.Internal.PrimQuery.PrimQuery\]([https://github.com/tomjaguarpaw/haskell-opaleye/blob/b314a0d705916628afc30e9fce3e78741755ad1f/src/Opaleye/Internal/PrimQuery.hs#L42](https://github.com/tomjaguarpaw/haskell-opaleye/blob/b314a0d705916628afc30e9fce3e78741755ad1f/src/Opaleye/Internal/PrimQuery.hs#L42)). Not every value of that type is valid and if you mess around with it you could generate very bizarre behaviour. Nonetheless, I want to expose it so that third parties can write SQL primitives that I haven't implement yet.
But functions are values.
Thank you! I think it might be useful to get some of the code back into cereal, which seems to be the most used serialization lib for strict bytestrings. But I am afraid that's not directly possible due to some differences in the internals. I also did not supply an Alternative instance for deserialization.
Seems like what you actually aim to provide is an "unsafe" API. Neither of your requirements implies a need to abandon the versioning policy, which is the essence of the Internals convention. IOW, if you rename the `Opaleye.Internal` namespace to `Opaleye.Unsafe` and maintain versioning according to PVP, there'll be no conflict with what I am proposing.
[Me too!](https://github.com/blamario/language-oberon/blob/master/src/Language/Oberon/AST.hs) I found it necessary to specify two type parameters for every node type, as in data Tree f f' = Node (f (Tree f' f')) but I may have missed a better solution. 
Think about all the printer toner you'll save, though.
Wow. I could easily apply for this if not for the fact that I live in the opposite side of the planet. Like, almost literally... Any chance of this working remotely?
Even for executables? I can get behind this for libraries, though.
Hi! I'm using `uniplate` to do some simple transformations on an AST. The problem is `descend f ast` starts applying `f` to immediate children but not to `ast` itself. Using `transform f ast` applies `f` to `ast` eventually but I need to apply `f` in top to bottom manner. Is there a function that does what I want? Here is a more concrete example: data Expr = LitExpr Int | IfExpr Expr Expr Expr | ListExpr [Expr] something = descend f where f e@(IfExpr _ _ _) = ListExpr [e] f e = e So calling `something (IfExpr (LitExpr 1) (LitExpr 1) (LitExpr 1))` does not put that `IfExpr` inside the `ListExpr` because `descend` does not apply f to expr itself. As a temporary solution I added `WrapperExpr` to `Expr` like this: data Expr = .... | WrapperExpr Expr something expr = unwrap $ descend f (Wrapper expr) where unwrap (WrapperExpr e) = e f ... But of course this does not seem not good. Can I do better than this?
Yes, that makes sense. I suspect the amount of pain it would cause would not be worth it, unless you know some way of making it painless and easy.
If a module is useful enough to be in an executable, it probably also benefits from being in a library as well. Most of my executables look something like -- app/Main.hs module Main where import MyLib.App (app) main :: IO () main = app
Ok, this clarifies things a bit.
That's what I mean by "different conversation": I forgot what subreddit (not to mention topic!) I was on, and wasn't thinking of `-&gt;`, or Haskell, or even programming. I interpreted "function" to mean something else. 
If we are going to have two packages—not sure this is the best idea, but if we did—perhaps we could go a little big further and turn the main one into a Backpack signature package, instead of hiding the internal through selective re-exports. Users would have to import two packages instead of one, but perhaps the gain in flexibility would be worth it.
I don't value following the PVP for its own sake. I despise the proliferation of bureaucracy which is what splitting packages to satisfy the PVP is.
To be fair, that is considered a decent wage in Japan (I just left last month after working there for 11 years). I’m not defending that salary level, just pointing out that it’s fairly average for developers in Japan.
This post has good examples of internals that could be broken into smaller packages. But I'm not convinced I'm doing a bad thing when I use `Internal` modules with a big caveat to users. It's not an unreasonable trade-off to make given none of the alternatives (separate package, `other-modules`) are strictly better.
It does seem like violating PVP is the only cited downside of `Internal` modules..but if you have big bold text in your `Internal` haddocks I don't see the issue. There are benefits to having an exposed `Internal` module (haddocks, testability)
Yes, I agree. The PVP itself is a convention. Having one main convention and a convention for violating the main one is very good. The PVP says that a user has rights and the maintainer has duties. The Internal convention says that a maintainer has rights (to do what they please) and the user has duties (to adapt when things break). Let us uphold this sacred covenant between user and maintainer.
If a Stack user wanted to use this library and this wan't a Stack project wouldn't they have to go through the dependency resolving steps, finding out that they need to pull in Reflex from GitHub etc. ?
I am very excited to start reading this book! Though I'm probably not ready for it :) Right off the bat, I'm not sure how the cardinality of a function type `a -&gt; b` is exponential; it seems like it should be the product of the values in `a` and the values in `b`. Maybe an example using types where the product and exponentiation are different would be better? The example I'm using is with `Bool` and `data Blah = Foo | Bar | Baz`. It seems like the only inhabitants of `f :: Bool -&gt; Blah` are: ``` f True = Foo f True = Bar f True = Baz f False = Foo f False = Bar f False = Baz ``` What are the other 3 inhabitants of that type? 
Nice love the spinner. 
Was on my phone yesterday, but here is how I would implement this using modern Haskell techniques. Note that this is basically the same thing as a `pure` for an `Applicative` instance for `Vector n` data Nat = Zero | Succ Nat data Vec (n :: Nat) a where VNil :: Vec 'Zero VCons :: a -&gt; Vec n -&gt; Vec ('Succ n) genSingletons [''Nat] -- you could define these by hand as well, it's not bad at all replicate :: Sing n -&gt; a -&gt; Vec n a replicate SZero _ = VNil replicate (SSucc n) x = x `VCons` replicate n x instance Semigroup a =&gt; Semigroup (Vec n a) where VNil &lt;&gt; VNil = VNil (x `VCons` xs) &lt;&gt; (y `VCons` ys) = (x &lt;&gt; y) `VCons` (xs &lt;&gt; ys) instance (SingI n, Monoid a) =&gt; Monoid (Vec n a) where mempty = replicate sing mempty mappend = (&lt;&gt;)
What is `emptyTailIf`?
`read [x] :: Int` is mostly like `int(i)`. It will try to parse the (singleton) string given and throw an exception if it doesn't parse as an `Int`. A more direct translation of your python code might be: digits n = [read [x] :: Int | x &lt;- show n] Idiomatic Haskell will but a type annotation at the top-level though, which actually eliminates the need for the type annotation on the inside: digits :: Int -&gt; [Int] digits n = [read [x] | x &lt;- show n] You can hit the Haskell Report for the exact desugaring of list comprehensions, but `for` -&gt; `|` and `in` -&gt; `&lt;-` will handle the simple translations. The report also has a library reference that includes not only `read` but also `show` and `map`. In Haskell, `String = [Char]` and there's no implicit conversion between then two, so while your python code can treat `i` as both each character of a string and as a string by itself, the Haskell code sees `x` as a `Char` and not as a `String`, but `[x]` is a `[Char]` with exactly one value (sometimes called a singleton list) and `[Char] = String` it's a `String`. `read` serves as a QnD way to parse a `String` into some other data type. It's polymorphic in the return type (!) and your code wants a monomorphic return type, so a type annotation (inside or outside) is required. HTH
&gt; instead of not exporting the dumb constructor or only exporting it in an Internal module, I now prefer to rely on a naming convention One issue is that not all uses of the unsafe constructor will use it by name. Consider `Coercible`.
At the current moment pressing enter will search the first autocomplete result. I typed "haskell" and instead it searched "template haskell". Clicking around it seems there is no way to just search a term if there's autocomplete available, it will always search the autocomplete. You should change it such that you have to hit the down arrow to search one of the autocomplete options, and you should make the magnifying glass an actual button.
That's the example I gave to /u/Tysonzero, which can't compile because the additional constraints (either \`Monoid (Vec n a)\` or \`(SNatI n)\`. &amp;#x200B; emptyTailIf _ VNil = mempty emptyTailIf f (VCons x v) = if f x then VCons x mempty else VCons x (emptyTailIf f v) &amp;#x200B;
What does it mean to apply `f` in a "top to bottom manner"? If we take your first example, after applying `f` once to some `IfExpr`, you get `ListExpr [IfExpr _ _ _]`, and now descending into the immediate children you get an `IfExpr` again, so doesn't that just expands infinitely: `ListExpr [ListExpr [ListExpr [...]]]`
That's the Haskell 98 report. For what you need right now, it will work. However, the [2010 report](https://www.haskell.org/onlinereport/haskell2010/) is what I currently reference right now, unless I'm dealing with a GHC extension, and then the GHC User's Guide is generally pretty good. `show n` turns the number (`Int`) into a `String`. `show` is polymorphic on the input value, so it will work not only for `Int` but also other types. It's somewhat similar to `str()` (or `.__str__()` or `repr()` or `.__repr__()`) on a python value. Since `String = [Char]` we can use it as a source of `Char` values in a list comprehension.
Orphan instances of `Generic` are truly awful, except maybe for experimenting.
Actually `descend` will stop applying after first level of children. Then I manually continue to apply to children if I need to. So after applying `f` to `IfExpr`, `descend` will stop there. This code reflects my real one more precisely (Sorry about not mentioning this in the OP): ``` something = descend f where f e@(IfExpr _ _ _) = ListExpr [e] f e = descend f e -- changed here ``` Basically what `something` function does is, if it sees an `IfExpr` it wraps it with a ListExpr and stops there (Which means I don't care about wrapping `IfExpr`s inside the `IfExpr` that I have just wrapped into a `ListExpr`. That's why I need to use `descend` but not `transform`). More generally speaking,`something` only wraps top-level `IfExpr`s into a `ListExpr`. And I can't really achieve this without using `WrapperExpr` because as I said `descend` starts applying `f` to immediate children of the expr, but not the expr itself. 
I care
One tricky thing is that some internals are of a somewhat "throw-away" nature. The bit-queue in `containers` is used only to implement `alterF`. As its documentation indicates, it is *not* a general-purpose data structure. It has quite severe limitations that may seem a bit arbitrary, but that enable excellent performance as it's used in `Data.Map`. It's exposed from an `Internal` module on the off chance that someone else finds it useful. As it happens, /u/edwardkmett found a use for it (or something very similar) elsewhere, but it's still a rather odd beast. Is it really worth a package? I have my doubts. There are some other throw-away types in `containers`, like pairs that are strict in various ways. Why are these exposed? Only because there are exposed "internal" functions that others might find useful and that handle those types. Do I really want a separate package for each such type? That sounds like a lot of maintenance overhead. Moreover, `containers` is needed to compile GHC, so it really has to be extremely conservative about its dependencies. Dependencies that other packages won't think twice about (e.g., `vector` and `unordered-containers`) are often completely unacceptable for `containers`.
Yeah. I put it wrong. show n returns something as a string which is \[Char\] in haskell, therefore it can be used for a list comprehension. Thanks for the link!
Then isn't `f` the function you want? something = f where ... -- or something e@(IfExpr _ _ _) = ListExpr [e] something e = descend something e 
Thanks! I'm still a bit skeptical, though. I appreciate your comments, but I think you're going a bit too easy on the official platforms. \&gt; exactly at the time This gives the impression that this was a particular *moment* in the past. But this actually a long *period* of time - over a year of confusion and failures as seen here: [https://github.com/haskell/cabal/issues/2653](https://github.com/haskell/cabal/issues/2653) And for sure, my perception was that it took me a full year to get Haskell up and running - trying it, rage-quitting, coming back, quitting again... until I wrote that post and was told about Stack. \---- I really appreciate you comments about the actual software. But my takeaway was only 25% about the software quality. 75% to me was the *process* quality, or lack of it. So learning about fixes to the development process to rule out this kind of problem is the kind of thing that would change my mind about Cabal and Platform.
I don't think you need to strip off padding for the google authenticator key since (20 \* 8) % 5 == 0 (maybe that's why they suggest 20 bytes?). Although it makes sense to have it in there so it will still work if you change the key size. Awesome talk btw!
$70k is the low end of the salary range. Someone matching the points in the "ideal candidate" section would definitely be at the high end of the range. Unfortunately, I don't know what the high end of the range would be. If you (or anyone else reading this) are interested in the position but unsure of the salary, I'd encourage you to send us an email. I'll loop in HR and we can try to see if we can figure out a salary that would work for both of us. (Well, this will happen for every candidate, but I just want to make it clear that there is some flexibility here.) -------------------------------------------------------------- That been said, I agree with you. $70k for someone who has authored multiple popular Haskell packages and has a deep knowledge of machine learning is too low. I tried to write the "ideal candidate" section really as an "ideal candidate" and not as a "reasonable candidate for $70k". When I see job ads that read like "10 years Haskell experience required, must have written widely used libraries, $45k a year", it makes me feel bad. I feel like the (Haskell) experience and knowledge I've worked so hard to build are being undervalued. I feel like *I'm* being undervalued. It was not my intention to write a job ad like that, but I'm worried it gave off that feeling. I appologize if you or anyone else reading this ad felt like this. As \/u\/DisregardForAwkward noted, on average salaries for developers in Japan are lower than than those in the US, especially places like Silicon Valley or New York. We want to offer a competitive salary for Tokyo, but it may be too low if you are coming from a high-paying company in the US. Again, if you're interested in this position but worried about the salary, please send us an email. We can try to work out a salary that would work for both of us.
He has made it very clear that he does not want to publish it in the near future. So I guess we can’t do much. 
Or rust or any other language
One of the authors of this project here: &amp;#x200B; Several members of our team feel it would be a good idea to release portions of this product as open source, however there are certain other considerations we need to discuss before we do so. So for the time being, no. However it is yet possible that the end result will end up as open source on Stackage within a year or two.
So Yoneda is just like pulling strings on a crossbow and firing arrows. Some of these crossbows are fancy: `\f -&gt; [f 1, f 2, f 3]` fires an arrow three times at once! (or does it fire three arrows?) Dangerous but fun!
&gt;MPTC Wow, thank you for your suggestion. I will take look at the "Questioner" MPTC from the [https://github.com/yamadapc/haskell-questioner](https://github.com/yamadapc/haskell-questioner) and the problem first. &amp;#x200B;
&gt; Do you accept remote people so far removed from Tokyo's timezone? We're pretty flexible with hiring. We would rather hire people close to Tokyo's timezone, but we accept strong candidates from US timezones. If this sounds interesting to you, I would encourage you to apply. 
There is actually a simpler solution than I initially thought for `emptyTailIf` since unlike `mempty` you actually do have a runtime representation of `n :: Nat` that can allow you to recover from erasure, specifically the length / structure of the Vec itself. ``` instance Functor (Vec n) where fmap _ VNil = VNil fmap f (VCons x v) = VCons (f x) (fmap f v) emptyTailIf :: Monoid a =&gt; (a -&gt; Bool) -&gt; Vec n a -&gt; Vec n a emptyTailIf f (VCons x v) = if f x then VCons x (mempty &lt;$ v) else VCons x (emptyTailIf f v) ``` Everything else discussed is still useful but if you do have access to a physical `Vec `of length `n` you can avoid a typeclass constraint on `n` and just use the `Vec` itself.
Well I mean what I was planning on doing was just pulling the HTML out of the sources (or even better finding the pre-pandoc sources), and just putting them in to a format mdBook likes. I assume it's very well structured so it wouldn't be hard to script I think. I just don't want to step on any toes!
This is cool and all but what are some cool applications that would contrast with the more traditional approach in its respective domain? Is it just theoretical or is there some application that is worth the ridiculous level of abstraction? It would be hilarious to use this in application code and handed over to an OOP person.
Yoneda is used in lens and traversal fusion to improve performance. 
I worry this is going to scare people off, I was going to keep it simple
I worry this is going to scare people off, I was aiming to keep it simple but I type too much
Nice! What exactly does it have? And maybe you would want to make t open sourced so anyone can contribute? Just noting... :)
&gt;One tricky thing is that some internals are of a somewhat "throw-away" nature. Well, there's no problem about that. We can have a "containers-core" package, which will expose everything that is in the Internals namespace of the current "containers", no matter how temporary, and that package will have its own versioning. "containers" then will be clean of any internal stuff and will merely depend on "containers-core". The most important part about versioning in such scenario is that a major version bump of "containers-core" can often be satisfied with a minor version bump of "containers". The absolute majority of users will never have to depend on "containers-core". This will let us have both: a freedom to experiment in "containers-core" with a stable API in "containers" and no versioning violations. &gt;containers is needed to compile GHC, so it really has to be extremely conservative about its dependencies That starts a whole different argument. If there really is no other way to have GHC depend on both "containers" and "containers-core", as one of the options, GHC could depend on just "containers-core", with a presumption that apart from internals it exports everything that's needed by GHC. If there is a way to have both dependencies in GHC, then there's no issue. --- Bottomline is there is a multitude of alternative solutions to Internals, which don't establish a culture of violating the rules. 
Why does it improve performance?
If you look at that issue it isn't one issue, and it spans a period of time including the _betas_ of El Capitan. I remember that period well, and was involved with trying to sort out the problems, and it was a mess. However, as you can see from the catch-up on those issues, it wasn't just cabal or the platform for some span of that time, it was the whole ghc toolchain, right down to the `unix` library. And even _once_ that stuff got sorted out, then there was the final issue of getting the `platform` installer to use the right directories to target for binaries as well. (And the fact that the then-recommended not-platform minimal installer for the mac had picked right around then to go completely unmaintained, which led to further frustrations, etc). (In fact, your above-linked question is not about the platform, it is about the ghcformacosx installer, which indeed was a nightmare because the maintainer stopped accepting PRs and then walked away, right when everything was breaking). That said, El Capitan was released properly in September, and the updated platform installer which shipped all the right stuff was released in October. My point is not that this wasn't a mess. Just that this mess, which _was_ for a discrete (if for those who lived through it, seemingly interminable) span of time, was about a specific set of technical issues, and unrelated to the broader discussions. On the last point: I don't know what improvements to "process" to "make unrepresentable" apple deciding to break 50% of all existing software with SIP are possible. My personal improvement to "process" has been the hard won lesson, which I first learned back around 2007, to never upgrade my development box to a new OS until I hear from ~~victims~~ early adopters that things have been sorted out. One improvement to process is that there are no recommended installers other than from the core and the stack and the platform teams. So the possibility of a single maintainer just dropping the ball and walking away as in the case of ghcformacosx is diminished. The other improvement to process that is underway, very slowly, is the continuous process of the small and beleaguered and heroic ghc core team to try to improve overall CI. This effort is guided by the ghc devops group (https://ghc.haskell.org/trac/ghc/wiki/DevOpsGroupCharter). However, moving to testing against new OS _betas_ seems still quite a distance away from our current capabilities. My one other bit of not-so-consoling commentary is that if you think the problems with bleeding-edge os updates on _mac_ have been bad, boy are you lucky you don't use windows!
Currently you have to know what you want in order for this to help you. Adding a "top searches", "most popular resources" or something like that (including i'm feeling luck) could help someone coming to just learn something new a lot in getting started. Maybe tags (monoid, monad, typeclasses, currying, etc) would be even better and simpler to implement. The important thing is to have something ready for people to click on, without needing to know what they want to starting to type. The next level would be to add "tracks": a list of resources to read in sequence to understand a concept or a range of concepts (like "understand functors, from basics to advanced uses") Overall the site is nice and as a haskell novice i really appreciate your effort, so keep up the good work!
You mean that there would be a case where you would want to keep the "internals" (say for example bytestring's internals) but may be add few fixed helper combinators ? I see. May be the internal convention hides multiple scenarios like for example 1. Things are internal because exposing them will lead to some unsafe stuff. 2. Things are internal because they are duct-tapey and might be cleaned up in future. Both these probably needs to be distinguished. I think the multiple library from the same package would work well in case 2 but not really in case 1. Was this what you had in mind.
Nothing really , it's just a simple example of something which doesn't compile because a recursive constraints.(it sets recursively the tail to mempty if one element matches the predicate 
I'm interested in this as well, specifically if I were to make a game using SDL2 and/or OpenGL. I usually use Nix to manage my dependencies but I understand if stack has better support for Windows in this scenario.
What if I want to make a symbol defined in an internal (not necessarily `Internal`) module available to my test suite, but I'm not interested in exporting it publicly? Can I do that without putting the module in the `other-modules` list? 
This *regularly* surprises and annoys me also. Does anybody actually use the ability to declare multiple bindings within a single `let` inside a `do` block? Could we get rid of it, in favor of just writing multiple `let`s (maybe with a language pragma, to avoid breaking compatibility too hard)? The fact that you could even put multiple bindings there was entirely non-obvious to me. The use case this would hurt is of course the ability to write mutually-recursive bindings (you'd have to express that some other way), but for whatever reason I haven't ever had the need to do that inside of a `do` block in particular.
Since the _let_ itself is already special syntax for do blocks (lacking an associated _in_), personally I have to agree it would have been nice to choose something less noisy and that doesn't impose too much indentation. It would also further the illusion of programming in an imperative language, with mutable variable assignment: do x = 3 y = x + 2 z = x^2 + y^2 return z However this highlights a big problem with the idea. Take the following for example: do x = [0,1,2,3] x = reverse x x = drop 2 x return x What is the value produced by this do block? `return [1,0]`, right? Unfortunately it's actually `return (fix (drop 2))`, which does not even converge. With the old incarnation of a binding shadowed out of reach by the new one in its own declaration, the most "obviously correct" looking pseudocode will just produce infinite loops. If nothing else, enforced usage of _let_ at least serves to remind us all that we're using local, lexical scoping. *** Also, while I'm not sure I can actually recommend it, I have genuinely considered something akin to this before: do x &lt;- pure $ [0,1,2,3] x &lt;- pure $ reverse x x &lt;- pure $ drop 2 x return x And in fact, that works precisely how the previous example was naively expected to, since the result of a bind is not in scope in the expression producing it. Using `pure` like this also avoids that pesky indentation bump.
Is it possible to fetch binaries from Travis CI locally somehow? I'm on Ubuntu, so I would like to fetch binaries for OSX and Linux locally and then I can upload releases to GitHub manually (using `github-release` Haskell tool) when I have full control over the process. * https://github.com/tfausak/github-release#readme Personally I'm not that comfortable when Travis does all this things automatically for me... I usually attach tags to existing commits from GitHub release page, not doing it with release commit.
It seems that Flatpak can be used to distribute Haskell software for Linux as explained \[here\]([https://szibele.com/stakpak-flatpak-a-haskell-stack-app-into-flathub/](https://szibele.com/stakpak-flatpak-a-haskell-stack-app-into-flathub/)). &amp;#x200B; I didn't know about Flatpak support via the \`Stackpak\` tool, I hope it works as expected.
&gt;One improvement to process is that there are no recommended installers other than from the core and the stack and the platform teams. So the possibility of a single maintainer just dropping the ball and walking away as in the case of ghcformacosx is diminished. Thanks! Very interesting. And yep, this is definitely an issue for many open source projects. I've even made an app to check the health of a repo's management. [It says that the Cabal team is "Doing fine"](https://repocheck.com/#https%3A%2F%2Fgithub.com%2Fhaskell%2Fcabal). (I wrote that to help me decide which open source projects to get involved with. Someday I'll upgrade it from Coffeescript.)
&gt; Does anybody actually use the ability to declare multiple bindings within a single let inside a do block? Could we get rid of it, in favor of just writing multiple lets (maybe with a language pragma, to avoid breaking compatibility too hard)? Yes, I exclusively use let that way (well, with the exception of having a single binding with let)
You can run a bash script to scp back or upload to s3 or something like that. But, I'd still recommend using Travis release since the continuous deployment is really awesome. And, you can tag existing commits too. I mean there is no magic at all. GitHub release is simply git tag and push. &amp;#x200B; Plus, you can combine with github-release. For example, [github-release](https://github.com/tfausak/github-release/blob/master/.travis.yml#L60) uses Travis to release its binaries for all platforms.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [tfausak/github-release/.../**.travis.yml#L60** (master → eb9aebc)](https://github.com/tfausak/github-release/blob/eb9aebcaf291b418bc8761ef66658454e3804277/.travis.yml#L60) ---- 
Unfortunately this doesn't seem to work. My test sample: import qualified Data.Text as T import qualified Data.Text.Encoding as TE import Text.XML.HXT.Core xml = TE.decodeUtf8 "&lt;root&gt;&lt;a&gt;&lt;b&gt;&lt;/b&gt;&lt;c&gt;bb&lt;/c&gt;&lt;/a&gt;&lt;a&gt;&lt;c&gt;cc&lt;/c&gt;&lt;/a&gt;&lt;/root&gt;" parseXml :: IO () parseXml = do let selector = (deep $ (hasName "b") &lt;/ (hasName "c")) let cs = runLA (hread &gt;&gt;&gt; selector /&gt; getText) $ T.unpack xml mapM_ (\c -&gt; putStrLn $ c ++ "\n-----") cs putStrLn $ "Length: " ++ (show $ length cs) In the output I'm getting: ``` Length: 0 ``` When I remove the `hasName "b"` part I have: ``` bb ----- cc ----- Length: 2 ``` So 2 elements match. I only want the second one to match :)
It allows you to associate composing functions to the right via the law: fmap f . fmap g = fmap (f . g) &amp;#x200B; You can the similarity of this approach with Difference Lists :)
:) no problem!
Hmm. I guess I'll have to stay tuned for the next installment to understand more. These are interesting but I don't see the benefit over this compared to, say, lambda or pi calculus.
I have a use case for it that I'll be posting soon! It shows the benefit of the right associativity perfectly :)
That looks wonderful. I like short concise libraries that do the work fast and without sucking the entire hackage to do the job.
Github issue for collecting feedback: [https://github.com/haskell-distributed/distributed-process/issues/336](https://github.com/haskell-distributed/distributed-process/issues/336)
Bah; I understand why `Coercible` wants the constructor to be in scope, but I still find that "feature" super annoying. The only time I use `coerce` is implicitly via GeneralizedNewtypeDeriving, which sometimes succeeds and sometimes fails depending on whether some other seemingly-unrelated type like ReaderT is in scope or not. Sometimes the error message is kind enough to tell me what I need to import, but [sometimes it isn't](https://ghc.haskell.org/trac/ghc/ticket/15850), so now I have to remember which random imports I need to add in order to derive some of my typeclasses :(
[Here is what I found](http://gelisam.blogspot.com/2014/12/how-to-package-up-binaries-for.html) last time I wanted to package a Haskell game on Linux, OS X and Windows.
There's also appImage and the platform specific Deb and rpm files. I have also seem single file programs that are self extracting executables but I can't remember the tool for making them anymore.
I definitely need some more coffee before trying to grok this, but that's all the fun, so don't worry about it. :)
The following is in the context of do blocks: Disagree. Sometimes mutually-recursive let bindings are useful, and I have used them. I primarily use a single let per binder, but saying that it's surprising or annoying is indeed surprising to me - it's shown in examples with do blocks, in source code, makes complete sense to do...the reason i write primarily one let per binder is so that they can be moved around more easily when the definitions are not mutually recursive, and i find it more readable. Some people don't find this more readable. Some people think it's noise.
There's nothing wrong with using curly braces (or semicolons!). In some cases it makes code more readable/makes parse errors clearer. There are some Haskellers who prefer to use them.
I did these problems when I learned Haskell, I actually remember this question and having thoughts about how to restrict myself to maximise the fruitfullness of the exercises. One rule I had was that if a question was too trivial to answer with a function(s) defined in base/Prelude, then i would write the function(s) myself. This is one such case.
Welp so much for that idea.
won't Formality be compiled to Cedille Core anymore, as mentioned in previous posts ? and what about Cedilleum ?
Do you have an example of such simple package? Travis configuration in `github-release` package is not that simple and (as I understand) also uploads to Hackage, which I don't need.
That second option actually looks quite reasonable compared to the first, does it have any downsides?
I like (if possible) putting all the constraints on the same line and the arguments on distinct lines: f :: (Constraint, Constraint) =&gt; Arg1 -&gt; Arg2 -&gt; Result
I was talking about the awfully compressed indentation 😜. Braces are certainly alright.
I am thinking of open sourcing at some point to make it community driven. I want to make sure it is in a nice shape before moving forward though
Oops, my bad. You're technically right but I hope no one uses 1 space, that would be awful.
That's actually a big problem as beginners and you are making a great point here. Tracks could be a nice way to address the problem. If you have any idea how to actually implement this we can chat. 
I prefer whatever keeps the function name and the double colons on the same line. It makes it easier to grep for the function definition. Otherwise it becomes somewhat of an annoyance to find the definition.
Thanks for taking the time to take a look at it. It is very valuable. It currently sucks a bit because the dataset is really small. I need to dedicate more time to add resources and search keywords. 
I'm pretty sure it's asking you to implement `last` yourself, not use the existing prelude function.
The test suite for a library is the first client of that library. If you want it in a test suite, your users will likely want it as well.
It sounds to me like Models "own" their Mesh objects, so you're expected to toss away the Mesh pointer once a Model is managing it. I see that as a confused poor design, i.e. definitely raylib's fault, but it's what you're stuck with. You could make the Mesh finalizer clever by making Meshes aware of whether some other object is managing them, but you could also just expose the unload methods and require manual management on the haskell side. You could then add a convenience API that wraps everything in guards that clean things up properly -- still the same problem, but now solvable in pure haskell. Another possibility might be to write a different set of unloader functions in C, link them in with the rest of raylib, and use those instead. Not sure if that would run afoul of internal uses of the memory management, but I suspect the worst case would be you'd leak memory rather than double-free.
I'm a complete bystander and don't do any work with Cloud Haskell, but wouldn't it be easier to actually make a second project for this? I agree with the premise of the redesign but it seems like really hard work to basically re-steer the entire community and the users of the code as it sits now with it's architectural choices.
It would be also interested to know, how people format function type signatures when there're `forall`s and constraints that don't fit on single line.
I have an AST for a language like this: data Expr a = Lit Value | Add (Expr a) (Expr a) | Mul (Expr a) (Expr a) | ... Now, I would like to convert an expression of type `Expr X` to `Expr Y` using `Control.Lens.Plated`. The reason is, my `Expr` ADT is quite big, and the X and Y are only really used in a few data constructors, so I would like to pattern match on just those data constructors. What worked is a manual function that pattern matches all the data constructors and returns most of them unmodified. But I don't seem to be able to write a traversal from `Expr X` to `Expr Y` using `Control.Lens.Plated`. Is there a way?
Fantastic! Do I understand correctly that this relies on the type of the right-hand-side of the clauses has to be syntactically known? Does the following create a `not1` that does fuse, i.e. `λb. λt. λf. b f t`? let not2(A : Type, b : Bool, x : A, y : A) =&gt; A{ case b -&gt; Bool | true =&gt; x | false =&gt; y } let not1(b : Bool) =&gt; not2(Bool, b, false, true) 
They are, but they are sometimes necessary. I've needed to derive `NFData` for upstream data types, for instance. 
I'd suggest solving each problem at least twice: * Once using builtin functions * Once not using anything but constructors and functions you've already written yourself * (Optional) Using whichever of folds or direct recursion you haven't already used.
Your example doesn't have any "c" elements with a "b" parent. Both "c" elements have an "a" parent. The first one has a "b" previous-sibling, and the second one does not have a previous-sibling. `(&lt;/)` and `(/&gt;)` deal with parent-child relationships. If that's really what you want, then you need to change your example. If what you are concerned with are sibling relationships, you'll have to find the HXT combinators / operators for them.
For Windows I'm compiling with "stack install" and then using [Inno Setup](http://www.jrsoftware.org/isinfo.php) to pull the relevant binaries out of the stack install and then package them up into a standard Windows install.exe. My application runs under GTK3, so I'm also having to package up the relevant msys64 library files and install them in the application bin folder. It takes a bit of playing around with pathnames in the Inno Setup file, but once it works the whole thing is automatic. I also used the [Dependency Walker](http://www.dependencywalker.com/) to find out which .dll files I had to bring along.
That's going to look for a "b" containing an "a", which you also don't have. You specifically asked for "a"s containing a "b", which is `hasName "a" &lt;/ hasName "b"`.
&gt; I am confused by, `cabal install --only-dependencies`. Is it for installing the dependencies into the global package DB (to be used in the future builds)? It would install into the user package DB, for use by the next command (`cabal build`). But, I think `install` is "smart" enough to use the current sandbox / local package DB if detected. (Global package DB is only for stuff installed by the administrator for multiple users.)
Oh, you're right! We were previously using a different key length which made the stripping necessary, though I agree with your reasoning about keeping it in there in case you change the length. &amp;#x200B; Thank you :)
Read the doc: http://hackage.haskell.org/package/lens-4.17/docs/Control-Lens-Lens.html#v:fusing
Hm, so it should be ```haskell let selector = (deep $ (hasName "a") &lt;/ (hasName "b")) ``` ? However I still get no elements in the above code...
Is the rejuvenation of enthusiasm in any way related to the publication of Unison's talk recently, or is that coincidental? Unison being a by-default distributed language, it seems you would be solving similar problems.
Oh, not exactly, but I see how the syntax looks misleading now. What this does is the following. Think of the type `data Foo = A | B | C`. Previously, `A` was translated to `λA. λB. λC. A`, which is the λ-encoded `A` constructor. But, on SIC, you can express things like `λA. λB. λC. (if cond then A else C)`, which is a λ-encoded that is `A` if `cond` is true, or `C` otherwise. This is different than `if cond then (λA. λB. λC. A) else (λA. λB. λC. C)`. The later is a conditional that returns either the constructor `A`, or the constructor `C`. The former can be seen as a "conditional constructor", if that makes sense. What Formality allows you to do is express those conditional constructors like this: `Foo{ if cond then A else C }`. In your case, that should a syntax error actually, since `A` isn't a datatype. (For some reason it works, though, I might investigate. But doesn't fuse, obviously.)
Could be because you are then doing `/&gt; getText` and I don't think that will get anything out of the selected "a" element. `getText` results in Nothing for all nodes that aren't `XText` or `XBlob`, and the children of the selected "a" node are all `XTag`.
Are they? I would have thought that they would be the *best* orphan instances since there's only one sensible way to define them.
Ah indeed this works! I'm an idiot :)
&gt; First, are people interested in having bindings to this graphics library? I'm interested! I know nothing about game programming, but would definitely like to give it a try sometime, and the fact that I know this exists makes it infinitely more likely that I will try this one first, before other libraries I'm too lazy to research. As for the API design question, I think it would be a good idea to improve the API by taking advantage of Haskell's features, rather than make a plain function-to-function binding. It seems you're already trying to do that to some extent with this memory management issue. Instead of `ForeignPtr`, you might also want to consider bracket-like patterns such as `ResourceT` for more predictable memory management. That was my rather uninformed opinion. Please feel free to take your project in whatever direction you feel most comfortable with, this will surely benefit the Haskell community.
If no further development is going on with the old branch, but the new version carries on in the same problem space, it befits keeping the name. Especially if the old design is slated to to be abandoned. Such is the lifecycle of 0.x versions. Hopefully after some shakeout period, this will kick it into a 1.x release, which is an arbitrary number, yes, but there's at least a convention of stability that will be expected from then on. Plus, it's too nice a name to abandon :)
I think if you added more info about what your implementing would get more responses.
Good thing you didn't hit 1.x before :) It certainly is an excellent name to have, I figured that had to be part of the reason. Good luck and God speed to you all! I felt like I was one of the only ones that couldn't buy into actors completely despite seeing how clearly useful they are, always found myself wanting just a bit of the paradigm and not all of it.
Haskell's syntax is already extremely sparse and context-dependent, and the parse errors for bad syntax are typically pretty bad. Increasing ambiguity would make these problems much worse.
You'll have problems if you try to recurse with a constraint, because you don't automatically get the constraint in scope without extra work. It's easier to just pass in the type information explicitiy (and so use the explicit version of `mempty = replicate sing mempty`: emptyTailIf :: Monoid a =&gt; Sing n -&gt; (a -&gt; Bool) -&gt; Vec n a -&gt; Vec n a emptyTailIf SZero _ _ = VNil emptyTailIf (SSucc n) f (x `SCons` xs) | f x = x `VCons` replicate n mempty | otherwise = x `VCons` emptyTailIf f v If you really want to use a typeclass constraint then you can bring it into scope with `withSingI`: emptyTailIf :: Monoid a =&gt; Sing n -&gt; (a -&gt; Bool) -&gt; Vec n a -&gt; Vec n a emptyTailIf SZero _ _ = VNil emptyTailIf (SSucc n) f (x `SCons` xs) | f x = withSingI n $ x `VCons` mempty | otherwise = x `VCons` emptyTailIf f v Your original function is then just: emptyTailIf' :: SingI n =&gt; (a -&gt; Bool) -&gt; Vec n a -&gt; Vec n a emptyTailIf' = emptyTailIf sing /u/Tysonzero
In my real case, I indeed have a \`vec\` (an \`NMap\` actually) so I might indeed be able to extract the instance from it.
Isn’t that like saying there should be a keyword for binding then? Like do bind a &lt;- someAction If do syntax were originally written without the let, it’s hard for me to imagine someone proposing adding it. Seems like it’s just another thing that people get used to.
Unless I am missing somehting, carrying around a \`KnownNat n\` doesn't allow to implemenation of things simliar to \`emptyTailIf\`, am I right ?
how about data Expr a = ... deriving Functor fmap :: (X -&gt; Y) -&gt; Expr X -&gt; Expr Y
Very interesting! 
i’d really like to see a clear migration path for those of us who’ve already adopted, and are using, cloud-haskell via distributed-process.
\&gt; selective receive &amp;#x200B; Were the benifits of erlang-style selective recieve even possible in Haskell? \&gt; Erlang supports literally sending the code for any native (non-foreign) function from node to node. \&gt; It is in general hard to share optimized compiled code across a cluster of machines.... Erlang keeps to comparatively slow but easy to handle and easy to distribute interpreted bytecode instead. Enter static pointers, which I understand is what makes the distributed aspects of Cloud Haskell even possible. [https://ocharles.org.uk/guest-posts/2014-12-23-static-pointers.html](https://ocharles.org.uk/guest-posts/2014-12-23-static-pointers.html) \&gt; re: Haskell - do we really need our language runtimes to distribute *code*? \&gt; all we need is a means to control which computations happen when, and where, by sharing *references* to functions. This works because, if \*all nodes are running the same program\*, then they all have access to the same functions. This is why I think it was odd to claim Cloud Haskell is doing what Erlang is doing. My understanding is that we are not sharing code/bytecode like erlang does. All Haskell nodes have to have the same binaries (\*all nodes are running the same program\*) - that is very Un-Erlang-like and non-distributed. &amp;#x200B;
Even if it would be cool to support as many alternatives as possible, the good thing about Flatpak is that it works in various distros. In contrast deb/rpm/etc are distro-specific, maybe it would be better to use them only for low-level package management.
Are there specific reasons that drive the preference for individual let in your case (as opposed to eg something like it just being the first style you learned)? I tend to prefer the latter approach because it's nearly always easier for me to understand whats going on when there are less tokens, but one thing that is nice about having individual `let` statements is that you can more easily reorder them or move them to a different part of the `do` block via "move this line up/down" type of editor commands.
&gt;selective receive Were the benifits of erlang-style selective recieve even possible in Haskell? &gt;Erlang supports literally sending the code for any native (non-foreign) function from node to node.&gt; It is in general hard to share optimized compiled code across a cluster of machines.... Erlang keeps to comparatively slow but easy to handle and easy to distribute interpreted bytecode instead. Enter static pointers, which I understand is what makes the distributed aspects of Cloud Haskell even possible. [https://ocharles.org.uk/guest-posts/2014-12-23-static-pointers.html](https://ocharles.org.uk/guest-posts/2014-12-23-static-pointers.html) &gt;re: Haskell - do we really need our language runtimes to distribute *code*? &gt; &gt;all we need is a means to control which computations happen when, and where, by sharing *references* to functions. This works because, if *all nodes are running the same program*, then they all have access to the same functions. This is why I think it was odd to claim Cloud Haskell is doing what Erlang is doing. My understanding is that we are not sharing code/bytecode like erlang does. All Haskell nodes have to have the same binaries (\*all nodes are running the same program\*) - that is very Un-Erlang-like and non-distributed. I claim that Static Pointers and the constraint it places on all nodes of a cluster is really what prevents the Erlang model from being realized. Of course maybe we don't want that goal to be like Erlang in that way - but it should be an explicit design point called out for or against, whether all nodes have to be running the same binaries. &amp;#x200B; take for example, Microsoft Orleans. It is actor-based - but makes no promise of selective-recieve. Orleans actors (grains) must conform to a typed interface, and actors/grains can only recieve certain well-typed messages. This is less flexible than the erlang model, but probably targets certain uses-cases with greater specificity. [https://dotnet.github.io/orleans/](https://dotnet.github.io/orleans/) [http://christophermeiklejohn.com/papers/2015/05/03/orleans.html](http://christophermeiklejohn.com/papers/2015/05/03/orleans.html) &amp;#x200B;
`KnownNat` does allow you to implement `emptyTailIf`, as you convert it to a value level Nat (or specifically a singleton, as that is much safer than the unsafeCoerce solution obviously), then you recurse on that. I won't bother writing it out as it would basically have been a non-singleton version of [this](https://www.reddit.com/r/haskell/comments/a01zwt/how_to_instantiace_totally_a_type_with_natural/eal0n4z), but now that I am reminded of the singletons aspect you should definitely use singletons instead of `unsafeCoerce`.
**What is the** ***most real-world*****, physical example of adjunctions?** Usually, people give examples that are very abstract, but I'm looking for something physical. &amp;#x200B; One cool example I found is between real-world objects (category C) and their shadows (category D). Let F: C -&gt; D be a functor which takes an object (a lamp, for example) to its 2d shadow. Let G: D -&gt; C be a functor which generates the simplest 3d object for a given shadow. The unit natural transformation is a morphism in C which, for a given object, generates the simplest 3d object that has the same shadow. The counit natural transformation is an *isomorphism* in D which is just identity. &amp;#x200B; I've omitted many details here for brevity (morphisms in C and D can be defined in many coherent way, what does it mean "simplest object for a given shadow"...). My main question is: **Is there a real-world example like this where both counit and unit aren't isomorphisms?** Here the counit is an isomorphism and because of this I feel this example doesn't really capture the full generality of an adjunction. 
With what we've seen, you should still be able to derive (or, if not derive, implement)`Functor` for both `Expr` and `Function` and use `fmap`. :/ The `fmap` implementations will be mutually recursive, but that's fine.
I think the singleton with \`withSingI\` is definitely what I was looking for, a way to give a \`n\` (but also a \`n-1\` and so on, if needed).
As i said, i think tags are the easiest thing to get started with (from an implementation standpoint), then tracks could follow. For tags, each resource (or post) would have one or more tags. The site would include a list of tags and the option filter searches by tags. Ideally, there should be tag categories, like: * topic (monoid, monad, state, IO, etc), * level (beginner, intermediate, advanced), * type (tutorial - theory, more focused on understanding, real-world - may assume knowledge of a topic, shows how it is used in real applications), but just a tag field with all the tags that apply to the resource would also suffice. Tags would also help to organize the resources a better and since you're at the beginning there aren't that many resources to go back and add tags to. Tracks are a bit trickier, since they need human input to create (reading the resources to be included in a track and ordering them in a way that makes sense). My suggestion would be tackle tracks (if) when you open source the project, to have some help. I think one of the biggest issues here is finding resources (tutorials) where the prerequisites of one is completely covered in an other one - i can't comment here since i'm just learning haskell from Learn you a Haskell and Haskell from first principles, i'm not at the online research phase yet. Also from a programming standpoint tags would work like tags on blogs (like blogger) and tracks would just a be list of links, maybe with a summary or comments (like "uses deprecated features" or "covers more than the next link needs").
Basically you have the choice between different bindings: \- FLTK: [fltkhs](https://hackage.haskell.org/packages/search?terms=fltkhs): nice binding, fast, can be statically linked, examples are available, graphical builder can also generate Haskell code, portable \- GTK: [gtk](https://hackage.haskell.org/package/gtk) and [gtk3](https://hackage.haskell.org/package/gtk3): very mature, also works with old GTK versions, graphical builder from GTK (Glade) can be used for creating the UI (these are the ones I use) \- [gi-gtk](https://hackage.haskell.org/package/gi-gtk) uses the GObject introspection mechanism to bind to GTK, works on newer versions of GTK, otherwise a bit similar to gtk and gtk3, but more modern \- [gi-gtk-declarative](https://hackage.haskell.org/package/gi-gtk-declarative) is a declarative layer above gi-gtk, which is very nice. It is very new and I have no clue about performance (I use GUIs with about 800 widgets, so it is a concern to me and I haven't tried). Interface is much nicer since it is not so imperative like the others. GTK in general is harder to install and use on Windows. \- QT: [qtah](https://hackage.haskell.org/package/qtah-qt5) a binding to QT, low level binding to the C++ library which is quite fast, generated via a interface generator. Can use QT's graphical designers for creating GUIs (I think). QT itself is portable, I have not tried the binding on Windows &amp;#x200B; For all GUI libraries, the documentation and books in their native language apply, just often need to be translated to Haskell. There are small examples with each binding. Note that GUI programming is not a very easy task (especially in Haskell) and requires some knowledge. Also with multi-threading a whole lot new problems arise, so for serious tasks prepare for quite some effort to learn how to do it. 
Ooh wow. The automatically derived `Traversable` instance did what I wanted. It's way more powerful than I had expected. Thank you!
I didn't take a detailed look at this, but one way I think cloud haskell could gain use is if it "proved" that it works well with the existing eco-systme. For example: - Create *a set* of configuration options that works well with the existing deployment options - like using DNS (works with docker) or etcd, consul, or eureka. Not a text-based config file! - 99% of TCP requests served by Haskell are likely served by WAI. Use WAI even if some other option might be slightly faster. - Make using warp an option and make it easy to plug in CH as just another WAI application for whatever you are already serving in your app. - If I create some API in servant and it would be useful to implement this as a map-reduce, make it easy for me to also define the Map and reduce APIs in servant and just re-use servant and warp for this. - Make a library of standard distributed utilities like: sharded caching of HTTP requests, sharded data structure, replicated data structures (RDD like in spark), possibly some consensus stuff. Just add what's missing to the existing eco-system, don't create a parallel eco-system.
[Here's a script](https://github.com/lamdu/lamdu/blob/master/tools/make_archive.hs) that bundles things for Windows, macOS, and Linux. * For Windows it invokes InnoSetup (with an accompanying `.iss` file) to create an installer with the `.exe` and `.dll`s (iiuc without an installer Windows often warns about executables) * For Mac it creates a zipped `.app` bundle including the required dynamic libraries and invokes `install_name_tool` to make the executable find them in the bundle * For Linux it creates a `.tgz` with the executable and required dynamic libraries
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [lamdu/lamdu/.../**make_archive.hs** (master → 3311d56)](https://github.com/lamdu/lamdu/blob/3311d56bd8a043c0a962d15fa4340180eea5799a/tools/make_archive.hs) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply ealhacm.)
1. Is every profunctor a bifunctor? 2. If yes, then why is Bifunctor not a superclass of Profunctor? If not (this is my guess), then what is up with "Intuitively it **is a** bifunctor where the first argument is contravariant and the second argument is covariant." (emphasis added) in the [docs](https://www.stackage.org/haddock/lts-12.20/lens-4.16.1/Control-Lens-Combinators.html#t:Profunctor)?
Np
Is there some way I can get a function of type a -&gt; [(String, String)] which returns a list of field names and serialised values. In Haskell-ish code what I am looking for: class Show b =&gt; forall (b in a) a where fields :: a -&gt; [(String, String)]
I looked at `ResourceT`. It looks relevant. I'll have to look at it more closely later. Thanks for the suggestion.
I like: f :: (Constraint1, Constraint2) =&gt; Arg -- ^ comment -&gt; Arg2 -&gt; Res
Yes, that's the kind of thing generics are good for.
Would you have any pointers at functions to look at in a Generics library?
Why would you ever have to do that? Couldn't you request that from upstream? You can also write `NFData` instances rather than relying on generic defaults, but that's another issue.
Okay, that makes sense. Thanks!
I've never gotten around to understanding Yoneda, so this is good timing for me. Thanks! The metaphorical language about crossbows was not helpful to me, but the one thing I got loud and clear is that the Yoneda lemma is about the fact that `flip fmap :: Functor f =&gt; f a -&gt; (forall b. (a -&gt; b) -&gt; f b)` and `($ id) :: Functor f =&gt; (forall b. (a -&gt; b) -&gt; f b) -&gt; f a` are inverses, witnessing an isomorphism between the types. That is, `flip fmap . ($ id) = ($ id) . flip fmap = id`. One direction is easy to show. ($ id) . flip fmap = \x -&gt; ($ id) (flip fmap x) = \x -&gt; flip fmap x id = \x -&gt; fmap id x = \x -&gt; x -- functor law = id The other direction is more interesting. flip fmap . ($ id) = \f -&gt; flip fmap (($ id) f) = \f -&gt; flip fmap (f id) = \f -&gt; \g -&gt; flip fmap (f id) g = \f -&gt; \g -&gt; fmap g (f id) Implying that any `f :: forall b. (a -&gt; b) -&gt; f b` must satisfy `f g = g &lt;$&gt; f id`. That feels like it should be a free theorem?
How many megabytes of space have Haskell programmers used arguing about `:` vs `::`?
The first thing to look at is the types. The general idea is that every data type can be associated with a "generic representation", for which you can write an implementation of `field` once, which gives you an implementation of `field` for any (generic) type by mapping it to its generic representation. -- some generic library type Rep a :: * -- generic representation of type a from :: a -&gt; Rep a -- Your generic implementation, works on all generic representations gfields :: Rep a -&gt; [(String, String)] -- Now you get fields for free fields :: a -&gt; [(String, String)] fields = gfields . from -- In reality, all these functions will have some possibly complicated constraints To get some practice, you might want to start with [generics-eot](https://hackage.haskell.org/package/generics-eot), which is meant to be simpler by using `Either` and `(,)` to define `Rep`, but it won't have enough information for you to actually construct `fields`. Then the two main libraries to work with nowadays (IMO) are [`GHC.Generics`](https://hackage.haskell.org/package/base-4.12.0.0/docs/GHC-Generics.html) (a module in base) and [generics-sop](https://hackage.haskell.org/package/generics-sop). generics-sop links to some of its examples, and for GHC.Generics you can look in [generic-deriving](https://hackage.haskell.org/package/generic-deriving).
Yes. It's a bad idea to blindly distribute random DLLs. You should know the licenses under which those DLLs were distributed before attempting this in order to avoid possible legal trouble. 
It's dynamic linking, so I don't see what problems there could be
I always use the second form for the very reason you underline. I absolutely do not understand why Haskell people like the former. It's very nice to have only one line to add/remove/comment to change arguments to a function. &amp;#x200B; For the same reason, I wished Haskell list syntax accepted an extra colon, so that one could write: [ a, b, c, ] and be able to comment out any element at will... For now I settled down with: [ a , b , c ] but every time I want to comment the first element, it upsets me... When I often need to comment in-and-out, I even do the (arguably ugly) following: [] ++ [a] ++ [b] ++ [c] and hope that the compiler simplifies this.
[removed]
TIL: that “maybe” is a handy function. 
I played in this ecosystem quite a bit a few years ago and made some modest contributions to it that are still in use and that I keep alive on stackage. I've also used Elixir/Erlang and write Elixir for a living presently. I no longer really think the actor model is the right way to build most *distributed* systems, and I don't think that historically its been nearly as successful as most people seem to think it has been. I don't think that long-running stateful processes are the easiest way to structure most server-side applications. So unless you are actually building a distributed message queue or a distributed database system, I think in most cases you should simply *use one* from otherwise stateless processes. This makes it super simple to perform rolling updates, which is more or less impossible in Cloud Haskell and can be very difficult to do properly even in Erlang/Elixir. If I were building a new distributed system today in Haskell, I would base it on either [gRPC](https://github.com/awakesecurity/gRPC-haskell) or REST/JSON using Servant and let an external framework handle the server mesh, discovery, health and welfare parts (e.g. Kubernetes). With this approach you can create true language-neutral schemas that can be versioned, and those versions can be articulated in a way that is not dependent on the binary representation of a particular type, yet it can be used to deterministic-ally generate those types in Haskell and many other languages. 
Functions like `maybe`, which pattern match on constructors, tend to be generally useful and are closely related to [Scott encodings](https://en.wikipedia.org/wiki/Mogensen%E2%80%93Scott_encoding). Another example is `either`. either :: (a -&gt; c) -&gt; (b -&gt; c) -&gt; Either a b -&gt; c 
And now we take up twice the namespace on hackage, twice the version maintenance, twice the headaches, and users will bend over backwards to figure out how to use just `containers-core` to get one fewer dependency.
I often have several layers of "internal" constructions that I don't want to have as part of my public API, each reaching back under the hood of earlier abstractions. I'm not in a hurry to ship seven different packages to ship the internals of `lens`. I'm somewhat more inclined to do this using multiple public facing libraries inside one single package using `backpack` and newer cabal features in new code. This is the approach I've been talking with coda. If you need access to the cuts my clone of the BitQueue stuff from containers (I'd cloned it before I got dfeuer to expose it) then using import "coda:common" Util.BitQueue gets it, but I don't clutter up hackage with 50 highly volatile names that will change from release to release.
I've been blocked for months by folks who claim to follow your strategy. (Years in at least one case!) Having to fork means I cannot ship my code to hackage in the meantime. I'm just dead in the water. By the time you get around to patching your code 6 months later, I've probably forgotten my name let alone what I was working on.
I do this too, but splitting out the constraints if they get too long: f :: ( Constraint1 , Constraint2 ) =&gt; Arg1 -&gt; Arg2 -&gt; Result 
That's silly whinging. It's a tiny, tiny GHC boot package and there's absolutely no reason to avoid it.
As an example, it happened to me when `primitive` took all my instances for `Array` except that one.
I get what you're saying, but that's true of many things if you only claim to follow X but don't actually follow X. Anywho, the community decides what the conventions should be, and I do not feel very strongly about my position. Since we have lots of evidence that this has only been problematic in the past (as you and tomejaguar have pointed out), I'm happy to change my perspective on the matter :).
&gt; I claim that Static Pointers and the constraint it places on all nodes of a cluster is really what prevents the Erlang model from being realized. It seems like you're arguing the core of the erlang model is letting things hot-swap/hot-patch. I would argue, based on my discussions with erlang devs, that this is entirely inessential to the model, and a feature often not used even in serious deployments.
I was offering it as an example. It is far from the only one.
I pushed for a convention of exposing `.Internal`s by default after many years of being bitten hard by this when the community was leaning the other way around exposure of internals. Not every package _can_ release quickly even if you want to be a nice proactive maintainer. For instance, boot packages can get tied to GHC releases locking you into a year-long window, and then you can only support the version for a given version of GHC. `containers`, which was one of the examples being discussed, falls into this bin.
Thanks for the explanation. I did not realize that boot packages get pinned by GHC releases.
I think this was implicitely used in the haskell mooc by Erik Meijer and I wish he told us about scott encoding..
Thank you, I think this is the solution I was looking for!
brittany author here. consider what your brain/eyes need to do to determine "what was the specified type for the 3rd argument". - With brittany's layout: "scan down along the left side and count, at the second -&gt; read the argument type" - With your style: "for each line, jump to the end, if =&gt; continue, if -&gt; start counting and continue, at 3 parse the argument" That is the gist of it, really. Of course the difference is negligible for small signatures, but I do encounter/create signatures that look like this regularly: apcIkavimj :: ################# ####################### ####### ############## ########################## =&gt; ############ -&gt; ##################### versus apcIkavimj :: ################# =&gt; ####################### =&gt; ####### =&gt; ############## =&gt; ########################## =&gt; ############ -&gt; ##################### now for which of these two is it easier to quickly see - which lines are constraint(s) - how many arguments this function has - where argument n / return type starts and ends That said, I definitely see a couple of advantages in your proposed style. You can very well argue that the "visually separating the context" issue can be resolved using a parenthesized context just fine: apcIkavimj :: ( ################# ####################### ####### ############## ########################## ) =&gt; ############ -&gt; ##################### at least if you ignore the rare cases where an argument type does not fit into one line, because then you end up with strange "-&gt;" potentially. Well that is subjective, i guess. The rest might come down to wanting to be consistent with other "operator" usage, which I prefer prefixed for the same "scan left side" reason. (brittany has [a rationale where I try to provide an explanations for the stylistic choices](http://hexagoxel.de/postsforpublish/posts/2017-10-28-brittany-rationale.html#stylistic-layouting-rule). Although I notice that the "scan along the left side" is not really mentioned over there - still, that is precisely the reason for the "layouting rule", especially b) in this case.) 
What about desugaring `x = v` to `x &lt;- pure v` in `do`-blocks?
Have you looked into [flat](http://hackage.haskell.org/package/flat) at all?
You already need to parse arbitrary far ahead in various circumstances such as function declaration vs pattern match: Foo a b c d e f g h +++ ... Foo a b c d e f g h :+: ...
I would more or less agree, specifically only newtypes should exist, data should just be syntax sugar for a newtypes over an extensible record / variant. 
I fully agree that heralding the type of argument on the left is preferrable, and I wish “real” layout would be available, like bullet lists that are necessarily aligned – similar to the ideas under “Less parentheses 1: Bulleted argument lists” in [https://www.joachim-breitner.de/blog/730-Less\_parentheses](https://www.joachim-breitner.de/blog/730-Less_parentheses)
&gt; Come to think of it, are these also catamorphisms? Talking of catamorphisms only really makes sense in context of recursive data types. But these are eliminators, and catamorphisms are essentially recursive eliminators for recursive data types. (You can also think of non-recursive data types as being "trivially recursive", and the catamorphisms obtained that way are these very eliminators, but yeeucch.)
&gt; They infectiously break abstraction barriers Makes sense &gt; two libraries each deriving them can't be used together That's surprising to me. Why not?
I like this idea
It's not a bad idea (I have to say I actually quite like it), but it would have to be a complement to the current do-block desugaring—it can't replace our current _let_, just some of its usages.
So I guess it turns out that a monad is more like a blunt wrapper.
&gt; I mean why would anyone familiar with Haskell expect that to work? You might notice I referred to the expectation as _naive_. I'm sure a seasoned Haskeller would see very quickly the issue with it, but not so much a beginner.
a monad is still a burrito, but the blunt analogy pretty accurately maps to an abstraction for monad transformers
I guess I'm much less perplexed about people who are hoping for a subreddit about a place called Haskell, rather than about a subreddit dedicated to anyone with the given name Haskell.
Oooh, SPJ's incredible enthusiasm explained? * I joke, SPJ is brilliant and a joy to watch on stage.
But that “issue” exists all over Haskell. For consistencies sake we should either fix it everywhere (too breaking and controversial to ever happen), or embrace the recursive approach and have everything work that way. 
What is you opinions about i3wm, /u/mschr , /u/akirakom ?
Can't help but think using recursion is a better choice for excludeNth. splitAt is a poor fit because it first creates "left" list which then needs to be prepended to the "right" to form the result. `excludeNth _ [] = []` `excludeNth 0 (_:rest) = rest` `excludeNth n (x:xs) = x : excludeNth (pred n) xs` &amp;#x200B;
I haven't tried it yet, but is there any advantage of using it over bspwm or xmonad? 
I wonder if there could be a Generic-based eliminator, for arbitrary sum-of-product types.
I told you too much -XMagicHash would get you into trouble
Does this work for you? Saw it a while back but never tried it myself: https://github.com/harpocrates/inline-rust 
The limitation that all Haskell processes run exactly the same binary is pretty limiting, though. It means that to upgrade it I have to bring everything down. 
I didn't try it either yet. But when pondering a new wm, for some reason I excluded both dwm and bspwm, but i3wm stayed on the shortlist. I really don't remember the filtering criteria any more.
They are not dual, they are orthogonal.
That's precisely what I mean under "yeeucch" in the last sentence of my comment above. :-)
I wish you could write Rust with Haskell syntax. Then my life would be complete.
&gt; Haskell tries to obtain some of these benefits with the use of developer-written rewrite rules which are notoriously brittle and hard to debug. Does inspection testing make this a solved problem?
I ended up deleting my comment above because it felt excessively nitpicking. We can agree on `(Const (Maybe a) -&gt; r) -&gt; (Fix (Const (Maybe a)) -&gt; r)` not being a terribly enlightening thing in and of itself :)
I've never touched Generics, but I'm pretty certain it should be possible. IMHO, it's a relatively simple problem compared to datatype-generic recursion schemes, even if we ignore irregular inductive types.
Sure, you replace data types with codata types, constructors with coconstructors, etc. Eventually you realize the resulting language has exactly the same structure as before, so the only benefit you get is if 1) the new names helps you think in a different way, or 2) there's some way to mix the data and codata fragments into one language, so they interact Path 2 is very long and eventually leads to linear logic and lots of other interesting stuff. Maybe [this paper](http://ps.informatik.uni-tuebingen.de/publications/rendel15automatic.pdf) will whet your appetite
&gt; replace [...] constructors with coconstructors So just nstructors.
I will admit in turn, that I do not *know* whether there's anything enlightening about the whole thing, and that my dismissive attitude is coloured by past experiences. I felt dirty, and experienced no particular insights, when I wrote this: https://github.com/pbl64k/gpif-idris/blob/master/IxFun.idr#L616
Good bot.
Just wait till you define the algebra of mmutative comonoids
The reason we can construct the categorical duals of things like products, data, and recursion is that the have a formal definition in terms of objects and arrows. Then you can flip the arrows, and voilà! You cannot take a vague concept like love, cats, or FP and "flip the arrows". You have to give a formal definition of those words in terms of objects and arrows first, and good luck with that.
Then again... a kind of transcompiler cannot be that hard to write.... I mean not Haskell -&gt; Rust, but Haskelly sytax Rust\*) -&gt; proper Rust (Yeah, pub and such declarations are probably tricky. \*) to be made up. Could also be simpler than Haskell in that it could drop quirky, antique or discouraged styles.
This is a really nice app! I like to see more full-stack applications with Haskell at backend :) Don't worry about code style. I quickly skimmed through your code and it actually looks pretty nice :+1:
Coming from Lisp I was inconsolable when I discovered that I liked Haskell but its syntax was so full of line noise. I feel your pain. But these things don't matter in the end.
But Lisps parentheses are noise themselves! Precedence rules are IMHO abdolutely ok and a godsend. So, what would be your ideal syntax, mixing Haskell and Lisp features?
One feature of functional programming that is often considered defining is purity. One way this can be defined is that a function is pure if running it and throwing away its result is the same as throwing away its input: const () . f = const () (This is not the only possible definition, and it has the slightly odd property that all functions are pure in a lazy programming language, but there's some reasons why this oddity might be reasonable.) The dual of `const () :: a -&gt; ()` is a function `absurd :: Void -&gt; a`: data Void = Descend !Void absurd :: Void x -&gt; x absurd (Descend x) = absurd x (This is not the only possible definition of `Void`/`absurd`; the important point is that `Void` has no valid elements.) As such, the dual of purity can be considered to be a form of strictness: f . absurd = absurd Strictness is useful for some forms of equational reasoning in the presence of side effects, e.g. in that you know you won't throw away the side effects you want to run.
Despite my other comment: okay, I'll bite :) The reason FP is often compared to OOP and imperative programming isn't because FP is dual to one or the other, but because OOP and imperative programming is the dominant programming paradigm in the industry. If Prolog's logic programming was the dominant paradigm, FP fans would be comparing FP to logic programming instead. For this reason, when we give approximations to FP such as "FP is what you get when you take programming with immutable values seriously", that only makes sense because the dominant paradigm is to use immutable variables everywhere. Prolog's values are also immutable, so we wouldn't be saying that if logic programming was the dominant paradigm. So I don't think taking the opposite of the FP paradigm would necessarily lead to using mutable data. We should instead look at FP's intrinsic properties, not its non-properties such as not having objects with internal fields and methods which can mutate those fields. I happen to think that [combinator libraries](https://www.youtube.com/watch?v=85NwzB156Rg) is the unifying concept behind a huge chunk of FP. Higher-order functions are combinators for functions. Monadic composition is one particularly-common combinator, and similarly for many other typeclasses. Lenses are combinators for data accessors. Let's try to take the opposite of combinator libraries. A combinator library provides: 1. a collection of primitive solutions to simple problems 2. a collection of combinators, each of which takes one or more solutions to one or more problems and combine them into a slightly more complicated solution to a single slightly more complicated problem I guess the opposite would be: 1. a collection of primitive problems which are known to have simple, known solutions 2. a collection of decomposers, each of which takes a problem with an unknown solution and splits it into one or more slightly simpler problems So... I guess the opposite of combinator libraries is reductionism?? I certainly didn't expect to get here when I started writing this comment!
Lisp syntax is regular because it's just plain words grouped with parens. You can write a parser in an hour. It reads like pseudo code. Editors have no problem parsing it, formatting and editing it. Haskell's syntax is full of exceptions, writing a complete parser takes a month. I suspect if it's hard to write a parser for, that translates to being hard for humans to comprehend too. Not to mention hard as hell to write a good editor and code formatter for. The fact that precedence rules come from user-specified rules that the compiler holds onto makes that even harder to tool, and hard for newbies to grok. That's Haskell 98 - not including the plethora of syntactic extensions. The learning curve is also ridiculous.
``` (when (file-executable-p "~/.local/bin/brittany") (setq hindent-style nil) (setq hindent-process-path "~/.local/bin/brittany")) ```
Moritz had a whole series of blogposts about cross-compiling to Arm with GHC: https://medium.com/@zw3rk
does it let you go straight to the doc's of rsync's `-s` option without having to do `n` a million times first like in `man rsync/-s`? that would make it worth quite a bit :)
Well 1 is already represented by `FreeGroup []`. Thus the monoid instance is simple to implement : (FreePregroup xs) &lt;&gt; (FreePregroup ys) = FreePregroup $ xs &lt;&gt; ys
Thank you.
Amazing, thank you!
We can indeed - here's a pretty simple approach that just uses a bunch of type parameters: -- dummy types data Address = Address data Total = Total data Tax = Tax data Delivery = Delivery -- dummy calculations calculateTax :: Total -&gt; Tax calculateTax _ = Tax calculateDelivery :: Address -&gt; Delivery calculateDelivery _ = Delivery -- a basket data Basket total tax addr deliv = Basket { total :: total , tax :: tax , addr :: addr , delivery :: deliv } -- pipeline functions addTax :: Basket Total a b c -&gt; Basket Total Tax b c addTax b = Basket (total b) (calculateTax (total b)) (addr b) (delivery b) addDelivery :: Basket a b Address c -&gt; Basket a b Address Delivery addDelivery b = Basket (total b) (tax b) (addr b) (calculateDelivery (addr b)) -- two different pipelines, both require input basket to have -- a Total and an Address, and the output will have everything order1 :: Basket Total a Address b -&gt; Basket Total Tax Address Delivery order1 b = addTax (addDelivery b) order2 :: Basket Total a Address b -&gt; Basket Total Tax Address Delivery order2 b = addDelivery (addTax b) The idea is to replace the parameters we don't yet have with unit: unfinishedBasket :: Basket Total () Address () unfinishedBasket = Basket Total () Address () -- looks dumb because all the types are placeholder stuff And calling it in ghci, it looks like this: *Basket&gt; order1 unfinishedBasket Basket {total = Total, tax = Tax, addr = Address, delivery = Delivery} *Basket&gt; order2 unfinishedBasket Basket {total = Total, tax = Tax, addr = Address, delivery = Delivery} 
That is hugely helpful, thank you!
So how exactly is one to interpret "Da coconut-nut is not a nut", then?
Update: It looks like generics-eot can do pretty much everything I need and it has a great tutorial! https://generics-eot.readthedocs.io/en/stable/tutorial.html
&gt; The fact that precedence rules come from user-specified rules that the compiler holds onto makes that even harder to tool, and hard for newbies to grok. What are you referring to? Custom fixity declarations?
Not at the moment, but I can add that as a feature request.
Rust's noise doesn't help, but then again just basic stuff like lenses in Haskell result in an explosion of typographical noise. Unfortunately for Rust, the real deal-breaker is the the lack of TCO.
As a vehicle to understand category theory, this one is probably not going to help much. Duality is at one level a technical trick and another a _point of view_ which escapes pure category theory and connects up with the physicists' constant eye for symmetries. In practice, though, duality is a side-effect of "categorification", the process of taking something you know and representing all of its properties in terms of categorical structure. This typically entails identifying a sufficiently rich category such that your object of interest is just one of the objects of that category. Categorification is related to formalization. One way you can make a hazy idea concrete is to figure out how to represent it as a category. This could be considered a complementary alternative to figuring out how it could be represented as a structured set, e.g., the standard mechanism of mathematical formalization. In this case, all of FP, OO, and imperative programming are hazy. So hazy that essentially nobody truly agrees with what they mean. To talk with any sense of formality you have to pick out simpler parts and dissect them on their own and this is the tack taken by other posts here so far. So let me provide one of those as well: OO arose in a large way out of the study of existential types and data hiding. In categories where we can model existential types, they have a dual in universal types---so one might talk about a feature like Rank-N Types as exploring a space of ideas dual to OO. Indeed, Rank-N types are a major part of Haskell-style functional programming.
Reminds me of my favorite FP joke: A worker takes work and produces solutions. At my job, I have coworkers.
At Formation we ran into some code bloat in our Dhall use case, so I reached for Yoneda to slim down the output, associating some chained maps to the right, which allowed us to keep things running in our test suite :) I finally put some words together about it, enjoy! 
On an a1.medium instance, I got a basic stack-based build going. &gt; wget -qO- https://get.haskellstack.org/ | sh &gt; wget http://releases.llvm.org/7.0.0/clang+llvm-7.0.0-aarch64-linux-gnu.tar.xz &gt; wget https://downloads.haskell.org/~ghc/8.2.2/ghc-8.2.2-aarch64-deb8-linux.tar.xz &gt; sudo yum install gcc &gt; sudo ln -s /usr/lib64/libtinfo.so.6 /usr/lib64/libtinfo.so.5 &gt; sudo ln -s /usr/lib64/libgmp.so.10 /usr/lib64/libgmp.so &gt; sudo ln -sf /usr/bin/ld /usr/bin/ld.gold &gt; export PATH=/home/ec2-user/clang+llvm-7.0.0-aarch64-linux-gnu/bin:$PATH &gt; stack new foo &gt; stack build &gt; stack exec foo-exec someFunc 
Ah, thanks – appreciate the more correct explanation. I'm still sorting out these concepts myself as you can see. :-)
Nicely done, and quite a stack for a first project! Heck, I might use it as a starting point for my own web project. Oh, and and I'm going to start shouting "Bingo!" when people enumerate the techs in their stack :) 
Oh I see, it does have access to field names!
Shortest answer: no. Short answer: I've looked for such a thing, and haven't found it. Emacs will still happily edit the script with the full power of haskell-mode; you just won't get automatic docs or linking of dependencies. What would be neat, and would serve IDEs nicely, would be if stack could "upgrade" a script to a full project by parsing the magic stack comment. 
&gt; TCO Please, expand the acronym.
[removed]
Tail call optimization
Oh, completely missed that :P :)
I remember trying that in the past but wasn't able to get it to work, maybe I'll give it another try one day :) I'll probably wait for someone to put together a blog with it working on my setup though, I've already spent more time than I'd like on this haha
Tail-call optimization (don't know about the rust TCO story)
Not at all. It's nice to be able to see the breakage, but you still have to fix it, and this might not even be that easy.
* Stick a load balancer in front of your existing homogenous-V1 cluster. * Add a small homogenous-V2 cluster behind your load balancer. * As time permits, remove machines from the V1 cluster, upgrade them to the V2 code, and add them to the V2 cluster. * When the V1 cluster is small enough, remove it from behind the load balancer. * Remove the load balancer once there's only one cluster and no upgrade plans. It's not ideal, but it does allow you to incrementally upgrade without ever having a heterogeneous cluster, and having to deny yourself Static Pointers.
I love adding type parameters to things :D
Hey, I got a notification on my phone that I think was a reply from you, but when I clicked on it, there was no comment. Did you write something and maybe accidentally delete it? 
I think it should be possible to have a Haskell-like language and a compiler that generates Rust compatible HIR or MIR code. And then use the Rust toolchain to handle code generation. See: https://blog.rust-lang.org/2016/04/19/MIR.html They are close to move the borrow checker into the MIR stage.
[removed]
&gt; I absolutely do not understand why Haskell people like the former. I preferred symbols-first since before I learned Haskell. If you do any alignment, they are often easier to align and have a more consistent width so the alignment continues throughout the rest of the line. Also, I use leading operator as a clear indication that the line is continued from the previous one. I do mostly C/C++ and Java when I'm not playing with Haskell or Idris. That said, when the language does support the trailing separator, I will sometimes take advantage of it, particularly when I'm experimenting. Easily working with any element individually is nice. When trailing-separator is not available, you basically have to have either the first element or the last element require special treatment, and I (mildly) prefer that be the first element.
I'm guessing yes. Fixity declarations can be specified anywhere, including in a `where` clause. So now you have to handle lexical scoping of fixities as well to parse the code correctly.
[removed]
[removed]
Just so you don't miss it! :) [https://www.reddit.com/r/haskell/comments/a176qa/yo\_yoneda/](https://www.reddit.com/r/haskell/comments/a176qa/yo_yoneda/)
Minor, but `compose` is in the Dhall prelude: https://raw.githubusercontent.com/dhall-lang/dhall-lang/0a7f596d03b3ea760a96a8e03935f4baa64274e1/Prelude/Function/compose
&gt; Rust is stronger for systems programming, embedded, game development, and high-performance computing I don't get the impression rust is widely used or mature in game development or embedded programming, though it probably is stronger than Haskell.( And I don't really know what HPC is): http://arewegameyet.com https://news.ycombinator.com/item?id=16488227 I'm still very early days with Rust, but my current assessment: I might choose Rust over Haskell when: - There are real-time constraints I can't *possibly* work around in other ways, so GC pauses aren't acceptable - I'm so resource constrained that a runtime and GC are unacceptable (though again I'm not sure Rust is great yet for these kinds of platforms either) - I'm shipping binaries to customers, rather than doing SaaS, where the occasional space leak experienced by a customer might be difficult to debug or upgrading is difficult/expensive for users (like a Smart Fridge or something terrible like that) - I want to stay very close to the metal for some reason; maybe for fun... Obviously there are other non-language reasons you might choose Rust, like a particular library or something. But in general I've found learning Rust has mostly made me sad because I don't generally have any requirements like the above in the work I do (thus far). At this stage I don't see myself wanting to use rust where Haskell could be made to work. I'd be really interested to hear more Real Talk from a haskeller with more rust experience though. Trying to keep an open mind
Could you explain why you see the last equation as a strictness property? 
Perhaps I was a little cryptic, so feel free to ask if you'd like more discussion/resources on duality. It's a fascinating subject. Ultimately there's multiple ways to do duality; one ends up being trivial, the rest reveal deep symmetries of logic/computation/programming languages
HPC tends to be Fortran or C++
That ast-transforming was impressive! but emacs kills me. The key combos are neither mnemonic (lesser problem) nor ergonomic *or* economic. ^(Of course there are keybindings/packages that make it ergonomic or modal, but if I set up and learn such a mode, I'll be out of sync with the majority of the user base, incur possible incompatibilities with minor and major mode packages, and will be all but lost on a foreign machine with a standard keybinding....)
These are some legacy uglinesses one should reconsider in a Haskell "2.0"
Keeps saying check my ships. I tried to play star wars mode
Doh!
**Battleship (game)** Battleship (also Battleships or Sea Battle) is a guessing game for two players. It is played on ruled grids (paper or board) on which each players fleet of ships (including battleships) are marked. The locations of the fleets are concealed from the other player. Players alternate turns calling "shots" at the other player's ships, and the objective of the game is to destroy the opposing player's fleet. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
Is a coconut just a nut then?
I would use `StateT Basket` to express the fact that there are some basket fields which are always available (and can be modified; I'd use `ReaderT Basket` instead if `addTax` and `addDelivery` are not supposed to modify those fields). addTax :: Total -&gt; StateT Basket Tax addDelivery :: Address -&gt; StateT Basket Delivery addTaxAndDelivery :: Total -&gt; Address -&gt; StateT Basket (Tax, Delivery) addTaxAndDelivery total address = (,) &lt;$&gt; addTax total &lt;*&gt; addDelivery address -- in the other order addDeliveryAndTax :: Address -&gt; Total -&gt; StateT Basket (Delivery, Tax) addDeliveryAndTax address total = (,) &lt;$&gt; addDelivery address &lt;*&gt; addTax total While the types may make it look like we're only asking for e.g. a Total and not a Basket with a Total, I assure you that all those functions do, in fact, receive a Basket as input. That fact is just hidden in the definition of `StateT Basket`, which both adds an extra Basket to the input and an extra Basket to the output.
I did have them placed, they may have been touching though. There's no indication of that in the UI
We need a co-Yoneda week to balance it out. (Heyyy~♫ coyoneda!)
In theory, I can return more verbose message and even to show what's wrong. But sorry, I just didn't think I need to do it. Will think when have time. :)
Funnily enough, Coyoneda was the solution to my Set problem I mention at the end of the post :D
it would be nice if it showed the rules during the boat placing
You can move your mouse over underlined game type in header to see current rules. But yes, UX is not the best my skill. And it's not commercial product. Just practice. If someone has interesting idea about this project future, I'll be happy to discuss it. :)
Obviously. Also consider cocoa and cocoon.
&gt; basic stuff like lenses lenses are pretty advanced material imo
I seem to be having real problems with Reddit today. I have posted three replies to you but none of them are visible here. I can see them if I'm logged in with the account that I used post them but not otherwise. It's like I've been shadowbanned but I have no idea why that would happen, unless Reddit considers questions about Haskell to be spam! &amp;#x200B; Anyway, I mostly just wanted to thank you for taking the time to respond: it is much appreciated! I did have one followup question but I'll skip that for now and hope that this account is allowed to at least send my thanks...
Thanks again
Thank you very much! I'm afraid I don't know enough about Haskell to fully understand what \`StateT\` or \`ReaderT\` do here but now I have something else to read about, which is always good.
That's why I use spacemacs.
Aaand now I'm back home again. Feel free to ask your followup question :D
I'm sorry that project has been under dev for so long - I keep meaning to go back and finish it off. Especially now that 8.6 is released, there isn't anything particularly blocking...
I'm going to see if Reddit will let me post my original reply now: That's brilliant, thank you very much for taking the time to respond! This looks good: the individual functions clearly indicate what they consume and what they produce, and we can freely change the order. It looks like `()` is functioning a bit like `null` in other languages: we have a record and uninitialized fields start of as `null`... but in this case the compiler won't ever let us pass a `null` field to a function that needs an initialized one. Nice. The only problem I can see is that I must define `Basket` in advance with all the possible fields that I might ever need. If we were to add, for example, a discount voucher or something, I'd have to update the definition of `Basket` and then go around adding in another argument to every function that takes or produces a `Basket`. Is that right?
&gt; Is there some function `a -&gt; Proxy a` I can use? proxyByExample :: a -&gt; Proxy a proxyByExample _ = Proxy
Yay, it works! &gt; It looks like () is functioning a bit like null in other languages: we have a record and uninitialized fields start off as null... but in this case the compiler won't let us ever pass a null field to a function that needs initialized data. Nice. It's not quite like null, no. Unit, or (), is a type that has only one member, which is also called (). So you can't put it in place of any value like you can with null, you can only put it in places where something of type () is acceptable. &gt; The only problem I can see is that I must define Basket in advance with all the possible fields that I might ever need. If we were to later add, for example, a discount voucher or something, I'd have to update the definition of Basket and then go around adding in another argument to every function that takes or produces a Basket. Is that right? Sort of. If the field is always present and always have the same type, then you just add it: data Basket total tax addr deliv = Basket { total :: total , tax :: tax , addr :: addr , delivery :: deliv , voucher :: DiscountVoucher } If it may or may not be there, but you don't care about expressing this in the type, you can add it as a Maybe: data Basket total tax addr deliv = Basket { total :: total , tax :: tax , addr :: addr , delivery :: deliv , voucher :: Maybe DiscountVoucher } But yeah, if we add more type parameters, then we do have to go around and change our functions. In that case, though, I'd start thinking about splitting it up into multiple data declarations, and dealing with the ordering some other way. Perhaps I'd enable the `DataKinds` extension, and have a typelevel list as a phantom type parameter... I can write a post about that if you want, but it'll have to wait until I've had dinner :D
Here is the same code without `StateT`, does that help? addTax :: Total -&gt; Basket -&gt; (Basket, Tax) addDelivery :: Address -&gt; Basket -&gt; (Basket, Delivery) addTaxAndDelivery :: Total -&gt; Address -&gt; Basket -&gt; (Basket, Tax, Delivery) addTaxAndDelivery total address basket1 = let (basket2, tax) = addTax total basket1 (basket3, delivery) = addDelivery address basket2 in (basket3, tax, delivery)
Thank you! I don't understand where this reputation of lenses being basic stuff comes from. I was just chatting with someone else a few days back and they were turned off from Haskell because they found the lens library very hard to use as a beginner. I was very surprised to learn that someone would point a beginner to using lenses.
&gt; `[Either a a]` for free groups. Huh, how does that count as free? Presumably, `[Left x]`, `[Right x]` and `[]` represent `x`, `x^{-1}`` and `ε`, but `[Left x] &lt;&gt; [Right x]` reduces to `[Left x, Right x]`, not `[]`.
xmonad is also fine, but I never used any of the more advanced config options and it used more resources than dwm, and especially longer startup time irritated me. Out of the box xmonad is not as polished as i3
HIR looks more palatable. Reimplememting all those optimisations, including the borrow checker, to directly generate MIR seems to be a source of error (on the first read).
&gt; The behavior when given a diverging input distinguishes many strict functions from many non-strict functions. It's sort of in the definition. Limiting ourselves to unary functions, "`f` is strict" means "`f _|_` = `_|_`" and "`f _|_` != `_|_`" indicates that "`f` is non-strict". That's why `id` is strict, even though it doesn't force (evaluate to WHNF) its argument.
Does it work with polymorphic recursion? That one thing that Haskell has that makes many of the optimizations from the OCaml (and larger ML) world not work so well.
I have nothing but a full of frustration when it comes to cross compiling. From Linux -&gt; ARM yes. But OSX-&gt; ARMV7, I have never succeeded cross compiling a small web application. I'm interested in how it would turn out. It is one thing I'm jealous of golang where you can build a static binary to any platform with a single line command. 
Thanks a lot for the paper. It would be nice if you suggest a beginner book for category theory. The only material I ve seen so far is bartosz blog and e book
It's just that I can imagine there are other definitions of strictness that are subtly different, so I'm trying not to commit to saying that this is the One True Definition. But I don't know for sure that there are useful alternative definitions.
&gt; I prefer whatever keeps the function name and the double colons on the same line. It makes it easier to grep for the function definition. Otherwise it becomes somewhat of an annoyance to find the definition. The fact that grep is so often the best way to find a function is an issue all by itself to me. But things being as they are I agree that it's better to have :: and the name on the same line.
I've been having trouble with Reddit too, I posted something to the Haskell Subreddit earlier and it just didn't show up.
I need to add Haskell (.lhs and .hs) support to ctags.
Nice work on the refactor. Tidal is on the recreational end of my haskell usage, so I have tended towards finding sweet sounds and not bothering about code correctness. But since you asked ... I think that filling in the class hierarchy you have in Tidal.Pattern would be productive. Derive the usual instances and think about how you could then use these in subsequent code. And group the operators according to what they operate on, so put together all the Arc -&gt; Arc -&gt; Arc's say. It really helps clarify the basics. If you combine an Arc monoidally, do you take the earliest time and the latest time (you do, but that information is buried in the Applicative instance for Pattern - Arc is a SemiLattice) or is it a discontinuous list of Arcs. Is start&gt;end valid,, is a negative Arc useful? Time is cyclical, so at some point Arcs get normalised to (0,1) or something right? normalise belongs here, if so. subArc is important to understanding the Pattern instances, but it took a bit for me to find it, and realise that its intersection (may as well call it intersection). A Part that needs to carry around it's own Whole - feels a bit weird - how do you squish together two Parts with different Wholes? Downside of all of this is more Modules and an increase in code complexity. It's a tradeoff and I suspect my advice is at one end. With +, |+ and +| we have: ``` "2 3 4" + "6 7" = "6 [7 8] 9" "2 3 4" |+ "6 7" = "6 8" "2 3 4" +| "6 7" = "6 7 9" ``` It reminds me of quotients or complex numbers. If you could work out, say: ``` left "6 [7 8] 9" = "6 8" right "6 [7 8] 9" = "6 7 9" "6 8" ?+? "6 7 9" = "6 [7 8] 9" ``` Then there's a radical refactor in there that simplifies everything. You may not need innerJoin and outerJoin. If you can't, then something is being thrown away that might be better retained. I go back and forth on the String issue. I'm with you on density of expression - the above examples would be a headache to express as Patterns. But I also expect that, by hiding the Whole/Partness detail, I missed easy answers to some of the above questions. Stick with strings - it's closer to the music. 
I thought about this when I saw the PR to the `generic-lens`, this is very cool! I have to ask: How long it takes to compile `Printf.hs` example? :)
&gt; Of course all of the above could be done a lot more efficiently with compiler support, and there’s no reason for that not to happen at some point in the future. A type-checker plugin for `Listify` could be the `Hello world` of type-checker plugins!
Yes, you're right! Good idea
&gt; I'd start thinking about splitting it up into multiple data declarations, and dealing with the ordering some other way. I think this is the answer that I was really expecting. Although a lot of programs consist of ingesting some data then processing it in stages, most programming languages (especially statically-typed languages) struggle to express data munging pipelines simply. The original motivating example for this exercise was a snippet of Clojure: ``` (-&gt;&gt; basket (add-VAT 20) (add-delivery location)) ``` Here you can clearly see how `basket` is threaded through a series of functions, each one modifying the `basket` as it passes through, and you can imagine how easy it is to rearrange or reorder that pipeline. This works in Clojure because, in effect, all of those functions have the same type: `map -&gt; map`, where `map` in this instance is Clojure's name for a hash-map. However, although it looks neat in this small example, I find it quickly becomes unwieldy. Pretty soon, you find yourself wondering where on Earth that key-value in the map came from, or exactly what shape the inputs and outputs of a certain function are. So I wondered what this would look like in other languages, especially statically-typed ones. In most cases, the answer seems to be, "I wouldn't write it like that in this language". Most languages don't lend themselves to expressing processing pipelines like this. Rather than passing one data structure through a sequence of functions, they would calculate each intermediate answer separately and only bundle them up into a single data structure at the end, e.g. ``` let vat = calcVAT(20, basket) let delivery = calcDelivery(location, basket) return new FinalBasket(basket, vat, delivery) ``` I am interested to see what other options there are. 
Well, if that's good enough for you, then your `FreePregroup` representation should work too given the right `normalize` function.
&gt;It is one thing I'm jealous of golang where you can build a static binary to any platform with a single line command. Rust has something similar. You pretty much just need to download or compile the standard library. Part of that is thanks to LLVM being way better than GCC when it comes to cross compiling. Cross compilation should not require jumping through hoops.
I'm not alone! \^\_\^
In my pet project I did it using Docker. Check deploy directory here: [https://github.com/DKurilo/battleship/tree/master/deploy](https://github.com/DKurilo/battleship/tree/master/deploy) I read about cross compile. But I was to lazy to build environment. So I just used special docker image.
I also do use rust but still not as close to golang yet though it is improving rapidly. For example, from OSX to ARMv7 is mainly the problem of all. armv7-unknown-linux-gnueabihf won't work on osx. It's mainly osx problem though. From Linux to others is fine. Go doesn't suffer this problem because everything is already builtin.
What remains to be done on it before its ready for release? Are the issues on github comprehensive?
Now that my intuition is back from its holiday, I see your point pretty clearly. Define drop :: a → 1 drop x = () absurd :: 0 → a absurd v = case v of {} dup :: a → a × a dup x = (x, x) merge :: a + a → a merge (Left v) = v merge (Right v) = v mapPair (f, g) (x, y) = (f x, g y) mapEither (f, g) (Left x) = Left (f x) mapEither (f, g) (Right y) = Right (g y) They are nicely dual, although pseudohaskell syntax does its best to disguise that :P Now, in a lazy language, drop ∘ f = drop, as drop will produce () before forcing the result of f. dup ∘ f = mapPair (f, f) ∘ dup, as this will produce the pair (f x, f x), with a thunk in each slot; we get the same result, just doing the computation twice. Neither of those properties hold in the presence of effects, as you can distinguish between performing the effectful computation f 0, 1, or 2 times. Still in a lazy language, refer to an f such that f ∘ absurd = absurd as *relatively strict*. Such a function may do some computation, but eventually forces (looks at) its argument and thus reduces to absurd. The condition f ∘ merge = merge ∘ mapEither (f, f) similarly represents a relatively strict function. Note that given the usual preorder on values, those two strictness conditions hold as *inequalities* but not *equalities*; they're only equalities for functions that are sufficiently strict. In a strict language, all the above (in)equalities now go the other direction. I'll leave that as an exercise for the reader :) you get a notion of *relatively lazy* functions: those that can produce a value before inspecting their argument. Again purity breaks the "natural" equalities. Any thoughts? Happy to go into more detail.
Awesome, I’ll take a look ASAP 
I was thinking ``` excludeNth n lst = take n lst ++ drop (n+1) lst ``` is easier to read, but probably not *as* efficient. Trusting library functions is generally good though. Isn't your `excludeNth` just an explicit reimplementation of `++`? 
&gt;based on my discussions with erlang devs, that this is entirely inessential to the model, and a feature often not used even in serious deployments. I'm not sure if hot-swap and hot-patch is the same as what selective receive is, but in any case if it's true it's not a feature erlang devs use then I guess we should pursue the fully typed model with red/black deployments, yes.
&gt; I feel like I could go further and have one big lens that could both &gt; &gt; 1. modify my deck &gt; 2. return the picked card &gt; 3. modify the player &gt; That sounds like a simple function `Table -&gt; Table`, and nothing like a lens.
Erlang devs do use it, but it is not free and most of the time it is not worth it. However if you use versioned APIs the same binary can serve both endpoints, so you can just do a rolling update on the back-end, followed by rolling update on the front-end. This is difficult to achieve when you have stateful servers, even before you add in the binary compatibility issues.
Your wish is my command [Gluon](https://github.com/gluon-lang/gluon) ;)
&gt; I do think the dependencies of default should only be base, and the instances supplied should only be for things in base. I think that would be a worthy 2.0. It is already set up in this manner. [`data-default-class`](http://hackage.haskell.org/package/data-default-class ) provides just the data type with no instances other than those for `base` and `ghc-prim`. `data-default` is a layer atop this that provides a bunch of other instances, which imports from 3 smaller shims that individually provide orphans for containers, dlist and old-locale.
&gt; It looks like the `Basket` is an opaque value that is just being passed through. `addTask` can examine both the `Basket` and the `Total`, neither is opaque. &gt; The original intention was that we use the data within the `Basket` data structure to calculate new values, which are then added to the original data structure. `addTask` may use the data within the `Basket` and the data within the `Total` to compute a new `Basket` and a new `Tax`. Isn't that what you meant by `addTax :: basket-with-total -&gt; basket-with-tax`?
This is a fantastic hack. Well done! I also suggest asking Iavor to add Uncons in the next ghc. :)
Lists are wired in pretty hard, so I imagine it wouldn't be too difficult to support `SymbolToList` and `ListToSymbol` natively. It would be considerably prettier if the `Char` type were promoted as well, giving `SymbolToString` and `StringToSymbol` instead. It could also be valuable to allow more direct inspection as well. Does the type checker know about `Maybe`? That would allow direct implementations of `UnconsSymbol` and `HeadMaySymbol`.
I'm now more interested in hurbstluftwm. I think dynamic tiling is a bad idea (especially with a big screen), and the design of hurbstluftwm seems to be sensible. However, I'll probably end up with EXWM, because I'll use my Linux computer mostly for development. 
That was a rollercoaster of emotions 
Thank you. You can ask me if something is not very clear. I'm using http://supervisord.org on server to launch web-server and bot. I also wrote very simple python script to redirect http to https and today I added very simple bash script to check web server health and restart it if something goes wrong.
I like [the IBM VGA9 font](https://int10h.org/oldschool-pc-fonts/fontlist/#ibm_vga9), because of its centre-aligned `*` and fat `!` characters, and because it reminds me of simpler times.
Ow, my eyes! ;P
Perhaps it would be better served by ad-hoc overloading, for me I would be very happy if we could have more of it like Idris - such as being able to re-use (+) without being tied to Num. Haskell seems to suffer compared to other languages in having a lot more symbols, because it's the functional way to break down your program into small, composable pieces. Combine that with having no ad-hoc overloading, no name spacing based on the dot and I'm finding myself struggling to come up with original names or operators for things compared to other languages, for example C++.
&gt; does the type checker know about Maybe From my small foray into type checker plugins, the type checker can look up whatever identifiers it pleases. Of course, it's possible for that identifier to not exist, so you'd need to handle that case; but Maybe is in base, so it's safe to assume it'll always be there.
I think so. A potential downside is that it might be a bit slower due to the additional appends needed, but I'll try and see what comes out of it
This is extremely vague. Post the code you're having trouble with and explain the problem.
Yet another recommendation for resource management: [`io-region`](http://hackage.haskell.org/package/io-region).
I would recommend the lens-micro package instead. It's less bloated, has better documentation and is compatible with lens.
&gt; the error that I see now is that the parentheses after input.txt is wrong. Which opening paren is it supposed to close?
do I ignore it now.if so, that is not good. The argument in the right case should be the s in the sentence 
You bind `right` in the lambda, but then you use a point-free style in the body!
you are right that one closes nothing at the moment
oke, and how to I solve this ? 
[https://github.com/ezyang/ghc-proposals/blob/backpack/proposals/0000-backpack.rst#motivation](https://github.com/ezyang/ghc-proposals/blob/backpack/proposals/0000-backpack.rst#motivation)
Gratkó!
:)
Hi ya'll, could you try not downvoting comments for suggestions you disagree with? It's obnoxious and makes me feel bad for even commenting. Thanks.
This looks much more complicated tbh.
&gt; The reason you can write a parser in an hour is the structure of lisp code is literally an AST. That’s just a cute thing to do. "That's just a cute thing to do" is not a convincing way to belittle something. It's not literally an abstract syntax tree. It may look like that because it has fewer syntactic forms. &gt; It doesn’t indicate the usefulness and “beauty” of the language. Its usefulness is demonstrated by the fact millions of lines of code have been written in it, and that its editor support is far better than most other languages with minimal effort or maintenance. Beauty is subjective with regards to anything, yes; but at least one common measure of beauty is simplicity or regularity.
I'm not talking about _which_ library should be recommended. I think lenses shouldn't be recommended _at all_ to beginners. Complexity-wise, the type errors with microlens are equally poor and mysterious. As a beginner, I feel that one can get away with using the default record projection and update operations just fine.
&gt; I think the term "lawless typeclass" is banded about in Haskell like an insult, when "conceptless typeclass" should be the real insult. Yup. See also: how lawlessness tends to be hurled as a denunciation of `Foldable`.
The `Total` is within the `Basket`. It is a field (or property, whatever you want to call it) of `Basket`. Likewise `Tax`. The problem, which may or may not be a problem in practice, is that we are updating a data structure as it passes through our processing pipeline. In Lisp, we'd be calling `assoc` on the `basket` in each step. This makes for extremely flexible code that is easy to refactor. But it also makes it hard to quickly see what a function is actually doing: if every function just takes a `Basket` and returns a `Basket`, it is hard for the programmer to know which fields in the `Basket` are required by the function and which fields the function will populate. (Let alone for a compiler to enforce anything.) I am interested to see if there is a programming language that can preserve the wonderful flexibility and ease of refactoring granted by this pipeline construct but also make each function more easily understood by a human programmer (bonus points if the compiler can enforce something useful too). One perfectly valid answer is to say "no": modifying the object as it passes through the pipeline is inherently confusing. You'd be better off calculating each intermediate value separately and only combining them into a single data structure at the end. In Clojure (which is where this pipeline approach is so often found) this: ``` (-&gt;&gt; basket (add-tax taxrate) (add-delivery location)) ``` becomes this: ``` (let [tax (calc-tax taxrate basket) delivery (calc-delivery location basket)] {:basket basket :tax tax :delivery delivery}) ``` But I was wondering if a language like Haskell might be able to let me have my cake and eat it too.
I think you mean thio let surround s = "Santa enters floor " &lt;&gt; s &lt;&gt; " at the end" in either (\(Left y) -&gt; "Invalid input") (\(Right x) -&gt; fmap (surround . show) . floorEnds ) &lt;$&gt; readFile "input.txt" but then I have problems to find out where to put the x
No. A no-op interpreter will yield different results, which will cause different code paths to be taken. There's just no way around this. If you want the ability to optimize programs, you'll have to fallback to something more static like Applicative or Arrow. With ArrowChoice and Traversing, Arrow can give you many of the benefits of both Applicative and Monad; it can be very optimizable like Applicative and dynamic like Monad. But I've found it comes with some terrible constant factor costs.
This is a second edition, [https://2018.monadic.party](check out the last year's programme). This time we will be having three tracks over four days, with each speaker giving a four to six hours talk/workshop.
Compile with `-O2`.
&gt; we are updating a data structure as it passes through our processing pipeline. In Lisp, we'd be calling assoc on the basket in each step. &gt; &gt; This makes for extremely flexible code that is easy to refactor. Sure, we use such pipelines all the time in Haskell. For example, here is a silly function which adds `[1,2,3]` to the end of a list by successively adding `[1]`, then `[2]`, then `[3]` to the end of the list: -- | -- &gt;&gt;&gt; add123 [0,0] -- [0,0,1,2,3] add123 :: [Int] -&gt; [Int] add123 = (++ [3]) . (++ [2]) . (++ [1]) &gt; if every function just takes a `Basket` and returns a `Basket`, it is hard for the programmer to know which fields in the `Basket` are required by the function and which fields the function will populate. (Let alone for a compiler to enforce anything.) I feel like you and I have a different definition of what it would mean for a function to take a `Basket` and return a `Basket`. For me, this means that `Basket` has a precise definition, for example a list of items: data Basket = Basket { items :: [String] } And then a function which takes a `Basket` and returns a `Basket`, well, takes a `Basket` (and _only_ a `Basket`! only the list of items!) and returns a `Basket` (and only a `Basket`! only the list of items!). But I feel like you mean something different by that, that you're somehow saying that a function which takes both a list of items and a total is still "taking a `Basket`", and that a function which returns both a list of items and a tax amount still "returns a `Basket`"? Doesn't that make the word "Basket" meaningless? &gt; I am interested to see if there is a programming language that can preserve the wonderful flexibility and ease of refactoring granted by this pipeline construct but also make each function more easily understood by a human programmer (bonus points if the compiler can enforce something useful too). Sure, that's what types do. In my `add123` pipeline above, `(++ [3])` is a function whose type is `[Int] -&gt; [Int]`, and this type documents the fact that the function will work correctly if given a list of `Int`s (any list of `Int`s!) and will produce some list of `Int`s. If I were to add a call to `map show` in there, whose type is `[Int] -&gt; [String]`, then the compiler would prevent me from accidentally calling `(++ [3])` after `map show` because the compiler knows that (++ [3])` expects a list of `Int`s, not a list of `String`s. -- error: Couldn't match type String with Int add1show3 :: [Int] -&gt; [Int] add1show3 = (++ [3]) . map show . (++ [1]) &gt; The Total is within the Basket. It is a field (or property, whatever you want to call it) of Basket. Likewise Tax. Why do you care about whether `Total` and `Tax` are "inside" the `Basket` or not? `Total` is inside of `basket-with-total`, but it's not inside `basket-with-tax`, so to me it seems much cleaner to model the `Basket` and the `Total` as two separate values. If I define my `basket-with-total` as a combination of `Items` and `Total` instead of a combination of `Basket` and `Total`, does that sound better? &gt; {:basket basket :tax tax :delivery delivery} Is this an example of an output you want, or an example of an output you don't want? If the latter, what would be an example of an output you want?
Did you try hasklig-mode or some other config snippet to get ligatures working? If you manage to install a custom font in emacs, in this case Hasklig, hasklig-mode should just work out of the box :)
The issue is here: \right -&gt; fmap (surround . show) . floorEnds Try asking `ghci` the type of `either` and that lambda: :t either :t (\right -&gt; fmap (surround . show) . floorEnds)
there I have a problem. Surround is not know or I get the same error message as before. so I cannot find the type of the last expression 
Hmm, maybe it was a different config snippet, I'll give it a shot :)
Will the talks be available online afterward? 
Yes, on our [Youtube channel](https://www.youtube.com/channel/UCCeiYYR2fCXarkfSqqFBwuA). This year we'll also record the screens of the speakers to make them easier to see.
Oh indeed I forgot about this ! Yup I had the feeling that I was expecting to much from Haskell (and my grasp of it). I'm a bit experienced with Coq, but I've never used Idris. What do you think about it ? Is it "worth learning" for what I'm trying to achieve ? &amp;#x200B; Though I think I'll take a look at it anyway, with language seems damn interesting
Please note that OCaml has a typed `printf` function: https://caml.inria.fr/pub/docs/manual-ocaml/libref/Printf.html This was the first time for me to see such a thing implemented.
Great. This seems like a pretty interesting event and I'm considering attending, I think I'm somewhere between "beginner" and "advanced" though so there might be some talks that I'll miss that would be interesting to watch after.
A huge chunk of hackerrank can be solved with mapAccumL
Let's write a type-level Haskell compiler
Sorry, the title should read "Help with an interpreter"
I'm really looking forward to Monadic Party this year. I wasn't able to fit it into my schedule last year, so I went out of my way to ensure I could go this time. I'm also happy to take some suggestions as to what to talk about for those segments.
do the recyling
I always keep this quote in mind when designing anything: https://twitter.com/id_aa_carmack/status/53512300451201024?lang=en In that context, why do you need STM and lenses? Here's how I would attempt to write a poker game: 1) Write a data type for the state of the game, and write the rules of the game as pure functions. After this point you should have poker encoded into haskell, but not UI yet. 2) Write a UI. If you've done (1) correctly you should be able to write multiple UIs if you wanted to using the same game logic from (1) exactly. STM doesn't need to get involved until (2), if at all. Lenses are handy, but don't need to be part of the fundamental design of the game, just use them to make writing the code easier. What about this for the `Game` type: `` data Rank = Ace | King | Queen ... 3 ... 2 data Suite = Hearts | Diamonds ... data Card = Card Rank Suite data Player = Player1 | Player2 ... data Table = Table { playersInGame :: Set Player , deck :: [Card] , playerCards :: HashMap Player (Set Card) , randomSeed :: System.Random.StdGen } data Game = PlayerDiscards Player (Set Card -&gt; Game) -- The `Player` tells the UI which player we want to request to make a decision. | Winner Player -- In a more complicated game there would be more constructors here for things like raising, folding, etc ``` Then you write an initial game type: ``` initial :: Game initial = PlayerDiscards Player1 startGame initialTable :: IO Table initialTable = undefined -- Use randomness to fill out the deck and initial hands firstPlayerDiscards :: Set Card -&gt; Game firstPlayerDiscards cardsDiscarded = undefined -- delete `cardsDiscarded` from player 1s hand, give them new cards from the top of the deck, and return a new Game (in this case another PlayerDiscards, but with Player2 as the player, and a new (Set Card -&gt; Game) continuation ``` Thoughts?
I found this useful, in that I was able to understand this Yoneda type generally in a Haskell context. I don't feel like I have any intuition of what Yoneda is in Category Theory yet, which I guess wasn't the point...but it does seem that having this little bit of Haskell-specific intuition will help me as I build more CT knowledge, so thanks. My only gripe with the piece is that the examples using `TypeApplications` weren't particularly clear in terms of how I would use them--for example, trying to replace the `yoList` type declaration with ```haskell yoList @Int :: (Int -&gt; Int) -&gt; [Int] ``` threw an error, so I just ended up making ```haskell yoIntList = yoList @Int ``` and then I could at least see how I could use those annotations. So it may be helpful to those of us who are less familiar with that extension to provide more of an explanation/disclaimer of how you're using that. Minor complaints aside: thanks! I look forward to more.
For more in-depth info: https://aws.amazon.com/blogs/aws/new-for-aws-lambda-use-any-programming-language-and-share-common-components/. A relevant snippet from that: _With this selection, the function must include (in its code or in a layer) an executable file called bootstrap, responsible for the communication between your code (that can use any programming language) and the Lambda environment._ _The runtime bootstrap uses a simple HTTP based interface to get the event payload for a new invocation and return back the response from the function. Information on the interface endpoint and the function handler are shared as environment variables._ _For the execution of your code, you can use anything that can run in the Lambda execution environment. For example, you can bring an interpreter for the programming language of your choice._ _You only need to know how the Runtime API works if you want to manage or publish your own runtimes. As a developer, you can quickly use runtimes that are shared with you as layers._ _We are making these open source runtimes available soon:_ - _C++_ - _Rust_ 
`take n l ++ drop (n+1) l` will do roughly 4n cons/uncons steps. GP's `excludeNth n` will no roughly 2n cons/uncons steps. Same complexity class, but "fused" so that intermediate cons cells don't have to later be uncons'ed. In fact, if `(++)` is a "good" consumer (foldr) and `take` is a good producer (build), you will fire the rewrite rules and get foldr/build fusion on that, if I'm reasoning correctly.
Not sure exactly how to do that: This is my current ghcMonad function: varToString :: Var -&gt; String varToString = showSDocUnsafe . ppr findNameInCoreBnds :: String -&gt; [(Var, Expr Var)] -&gt; Maybe (Expr Var) findNameInCoreBnds name = fmap snd . find ((==) name . varToString . fst) findExpr :: CoreModule -&gt; String -&gt; Maybe (Expr Var) findExpr coreModule name = findNameInCoreBnds name . flattenBinds . cm_binds $ coreModule generateCode :: FilePath -&gt; String -&gt; IO (Maybe (Expr Var)) generateCode source functionName = do cwd &lt;- getCurrentDirectory runGhc (Just libdir) $ do dynFlags &lt;- getSessionDynFlags setSessionDynFlags (foldl gopt_set dynFlags [ Opt_SpecialiseAggressively , Opt_ExposeAllUnfoldings ]) liftIO $ putStrLn "venturi: Compiling Haskell Syntax to Haskell Core" coreModule &lt;- compileToCoreSimplified (combine cwd source) liftIO $ putStrLn $ "venturi: Expanding Core Expression " ++ functionName let expr = findExpr coreModule functionName let env = buildExpandEnv dynFlags coreModule liftIO $ putStrLn $ pprShow $ envMap env instances &lt;- getInsts liftIO $ putStrLn $ pprShow instances let expr' = fmap (expandCore env) expr return expr' buildExpandEnv and expandCore are my custom functions which basically aggressively expand some recursion and reduce some case expressions. I could use this to reduce these (+) applications but it feels like reinventing the wheel. 
readable compilation errors
Following up on this, I've hacked together a library that could serve as the basis for other Elm codegen libraries: https://github.com/mitchellwrosen/elm-codegen Would you mind taking a look? Should I throw this on Hackage?
The main thing that needs to be done is to properly dogfood it (ie. write some sample packages using it - for example, something that has the interface of `IntMap` but uses https://docs.rs/immutable/0.1.1/immutable/map/struct.Map.html). I'm less than confident about all of the platform specific linker stuff. Other than that, just the usual maintenance (support 8.6.1/8.6.2/8.8.1, fix up CI, bump dep bounds).
hah, yes, indeed there are similarities — spawning processes on demand, not keeping them alive all the time. My take on this is that instead of going back to CGI, we should [just use HTTP over Unix sockets and kill the server when there are no requests](https://github.com/myfreeweb/soad) :) BTW, it's really unfortunate that Lambda and its clones are wrapping HTTP in their custom event thingies instead of doing the opposite (delivering events like "new file in S3" as plain HTTP requests). It's extremely stupid that binary files need to go through *Base64* between the API Gateway and Lambda.
Hey thanks making the effort of writing a comprehensive response to my question, it’s highly appreciated. What you wrote above is comforting me in my choices. I think you are absolutely right for most of the things you described in your answer. To give you some background my thoughts went through this process: - i will make a poker multiplayer game, so what should I do - i have some kind of concurrency, i will use stm, since it’s multiplayer and « real time » - let’s model all my records with stms. - oh wait, i can just have one stm that can encapsulate my game state and then when a player does an action, it’s basically going to modify my game state in a pure way - cool !! How can I do this nicely ?! I have heard about lenses, because basically, to update my game state I think i can just use lenses and they will compose nicely so as you can see, I got the core right (with an admittedly very long thought process) but decided, conciously to take the hard path. My goal in the end is to learn haskell and cool stuff about it ! And now this is where I am at. So basically when I am stating I use STM it is in a very scarce and dumb way ( readTvar, apply function, writeTVar). So the STM part is really not a problem. But now, and since I have a clear lack of experience using lenses, I am just wondering if I have to use state monad on top of my lenses to do those modifications. For instance I am wondering how do I update two records that are located at a difference depths in my game state, given that the first update will also return a value and I need this value to update my second record (like pick a card, which will return a deck minus one card and the picked card, then update the hand of the player)! How the heck do you do that ?! Also can you shed some light on how lenses and state monad are cool ? With an example that’d be awesome ! I am very aware that I could do this without lenses with just pure functions pure I have heard so much about lenses that the appeal is strong 😎
Sure thing! Here's an example of using a `State` monad (actually a custom monad with a `MonadState` instance) with lenses. The custom monad is defined here, and you can see the `MonadState` instance being derived as well: https://github.com/mitchellwrosen/boston-haskell-arcade/blob/43b8f6d2fea18c4d52fc19a2f249faf00be1e98b/src/Internal/Bha/Elm/Prelude.hs#L59 Here it is in use by a specific game: https://github.com/mitchellwrosen/boston-haskell-arcade/blob/f7ffa02dab2a4c81309a23a7ed852477b7a56b3c/src/Bha/Game/Impl/BlimpBoy.hs#L140 You can see how handy it is in the next 100 lines or so -- the code does lots and lots of updates to the `Model` that would be a pain without lenses + `MonadState`. However, the key point remains -- none of this is necessary! `State Game ()` can always be replaced with plain old `Game -&gt; Game` and lenses can be replaced with record updates without losing anything from a design perspective. So the State and lenses stuff is definitely an implementation detail.
That might be true. I guess my point was that the difference between 4n and 2n is basically nothing, and using `List` is pretty much an upfront admission that we don't *really* care about efficiency. In which case readability wins, right?
We're really good at reinventing the wheel. But I think generally we are able to improve upon it, though it may not always be the case, and marketing buzz with the internet and all also plays a bigger role than it used to.
&gt; I'm in a situation where I have to bind multiple variables to the same [name]... Hm, I can't think of why you'd want to do that. Can you elaborate?
Your "nasty" alternative seems pretty good, what's wrong with it? The `8`? How about: a,b,c,d,e,f,g,h :: Int a:b:c:d:e:f:g:h:_ = repeat 42
Javascript. I grew tired of chasing down who did what, when to mutable variables, and realized Haskell basically doesn't have that problem. I then started pet projects, and learned everything I needed to know along the journey. Now I have my first paid projects in the language!
I'm not sure I think the one using take/drop that much easier to read. Also, not having to "repeat" the `n`/`n+1` argument is nice, which would send me looking for a `splitAt` rather than a combination of take/drop. I wouldn't make the decision based on efficiency (something something premature optimization), but I also wouldn't necessarily use your solution.
Yeah I think the §splitAt§ solution is probably nicer too - still uses library functions and it wears its semantics on its sleeve.
Ah thanks -- yes, this is exactly what I learned about ApplicativeDo from the first video. The second video also basically accomplishes the same in Scala (non-automatically of course), you just combine operations with `|@|` or `&gt;&gt;=` them when you need each functionality. Right now my current plan is to read through the two papers on functor applicatives and start trying to see if I can work with a library like [`freer-simple`](https://hackage.haskell.org/package/freer-simple) from there. The `Eff` monad has an `Applicative` instance so I'm fairly sure it works, I just don't know how to do it yet. I don't know enough to start trying to build a free monad with an applicative base functor -- by that do you mean that I should just make sure that `f` (in `Free f a`) is of the `Applicative` type class? If you have any pointers to that approach I'd love to read more. I figure after reading through the papers I'll start with trying to reproduce the basic applicative functor approach, then try it with freer-simple, see if I can use the Free Transformers that John mentions to make code that works in either context (basically he moves the function into the actual operation type), and see where I get. Also huge thanks for all the great haskell work/talks/content over the years.
I don't really get why you want to do this, so I'm not sure this will meet your use-case exactly. But if you only need to bind the values in some local context, you can do this: let a @ b @ c @ d @ e @ f @ g @ h = 42 :: Int in foo or foo where a @ b @ c @ d @ e @ f @ g @ h = 42 :: Int
There’s wasn’t one. Instead, there were dozens, maybe even hundreds. I cannot remember them all, but I certainly remember some of them: groking monad transformers for the first time, understanding how currying makes the applicative functor trick work out, realizing for the first time why the `Applicative` and `Monad` instances for `((-&gt;) r` are what they are, and so much more. Some—probably even most—of the insights were not individual moments in which lightbulbs clicked on, but long and steady cycles of repetition and recall that involved seeing each technique from several different angles and using it to solve a variety of distinct problems. Haskell has been, at times, intimidating in scope, and there are still so many things I do not know. I know zero category theory, I couldn’t tell you a thing about recursion-schemes, the Van Laarhoven encoding of lenses is opaque to me, and I haven’t the foggiest idea when I’d want to use a comonad. Fortunately, one of the wonderful things about Haskell is that I’ve been able to pick up the things I want to learn as I want or need them. I doubt I will ever “master” Haskell (and perhaps no one ever can or will), but I can be productive in it all the same, and I almost never feel that unsatisfying feeling of realizing a solution I’ve come up with cannot be improved, should I someday wish to do so. Do not worry about “getting” Haskell. Worry about understanding the essence of functional programming, about reasoning with pure functions and immutable datatypes, about building programs with function composition and data-driven design. When you find something inelegant or too verbose, seek out abstractions that will make your life simpler and your code more understandable: abstract over sequential repetition using monads, decouple your code with effects, obliterate boilerplate with generic programming and Template Haskell. But these things are only tools, not philosophy, religion, or even particularly deep insights (though deep insights can certainly be sometimes had about them). Write software, and learn from the ways you get stuck. If you wish to grapple with an idea for its own sake, by all means, do so! Learning can be fun, fulfilling, and freeing. But do not ever worry that you somehow have a quota to fill, a checklist of concepts to work through before you can be a “real” Haskeller. Build things! Take pride in what you make, and let that motivate you to learn more. There will come a time when you are frustrated, you’ll stumble into an abstraction you’ve seen a dozen times but never knew you needed, and as it dawns on you that now is the time and place for its use, then you will understand. Haskell is a programming language. Program!
Clever!
It seems ghc is trying to open these files in the cwd [https://github.com/teto/ghc/blob/b1af0aed08c78f42c7dd2505ed9b96d0cbf1d076/hadrian/src/Rules/Generate.hs#L158-L169](https://github.com/teto/ghc/blob/b1af0aed08c78f42c7dd2505ed9b96d0cbf1d076/hadrian/src/Rules/Generate.hs#L158-L169) is that expected ?
Congratulations on your first paid project!!
I could not have asked for a more thought out, positive, encouraging response. This hits on so many strong points that go beyond just Haskell or programming. Thank you for taking the time to share and contribute in a really meaningful way. 
It seems that pairs = [ [x, y] | x &lt;- list, y &lt;- list, x &lt; y] should [do what you want](https://repl.it/repls/SecondPitifulLinkedlist), although I'm not sure if it's the most efficient way
In much the same boat as you, and agreed with /u/lexi-lambda. Now that I've relaxed into the idea that the learning curve is just different from other languages, it's less frustrating to get stuck on specific things because: learning! Somehow what's helped me is working more on web apps and command-line tools for open data stuff, and finding examples of those things on github to learn from.
I'd do this. Prefer do notation over list comprehensions. pairs list = do (a : as) &lt;- tails list b &lt;- as pure (a, b) 
Problem solved, I had wrapped incorrectly the Ghc libraries or so it seems... sorry for the noise
What about `[(x, y) | (x:xs) &lt;- Data.List.tails [1,2,3,4], y &lt;- xs] `
Haskell [List comprehension](https://wiki.haskell.org/List_comprehension) allows conditional guards inside the clauses: let l = [1,2,3,4] in [[x,y] | x &lt;- l, y &lt;- l, x &lt; y] will give you what you [[1,2],[1,3],[1,4],[2,3],[2,4],[3,4]]
I think it's mostly about throwing yourself in there and start using it. It'll be a lot easier if someone's there to guide you, but I guess the point is, if I just kept learning it, I wouldn't try hard to fully understand certain parts of the language. But if I have to use it to solve a problem that I care, then you bet I'll figure it out no matter how hard it may appear. &amp;#x200B; I do maintain a few production Haskell apps in a commercial environment, and the fact that I do some Haskell in my day job has really pushed me to learning a lot of things I'd otherwise had kept postponing learning. Like I'd never really have looked into what a natural transformation really is until I had to in my job. &amp;#x200B; I think getting good at a language is a slow grind, it's never like it just clicks and suddenly you start writing awesome Haskell. 
Please don't format it like the last example, formatting it like that makes me read it like they are type applications (which will be more important once we can actually bind invisible variables with that syntax). Use spaces on both sides or neither side, it is easier to recognise it as an at-pattern that way.
@pokemonplayer2001 Please post results of your research here! 
Whenever Rich Hickey talks about static typing I feel like that he doesn't argue in good faith. Not that he is intentionally deceitful, but that his reasoning is more emotionally motivated than rationally motivated. I think he misrepresents what proponents of static typing say. For very small scripts, (50ish lines) I would prefer a dynamically typed language. I don't think there are that many people saying static types have zero cost. It is a trade off, but he is not being honest that it is a trade off and instead is being snarky. More annoyingly is his talk about either "Using English words to try to give you some impression is not good" yet he also criticize haskell for talking about category theory, which is where non-English words like Monads come from. His arguments make sense on their own but do not make sense when put together. He also tries to argue that static typing is worse for refactoring. I would rather have false positives I know about than true negatives I don't. Again, there is a trade off to be had but you would not believe by listening to him. His whole thing about "No code associated with maps" also does not make sense to me. Dose he conjure hashtables from the ether? And if he means a more abstract notion of a mapping, then the same can be said about functions. His example of a map can just also be just as easily be written as a function in Haskell. f "a" = 1 f "b" = 2 f "b" My point isn't that he is wrong. A map can me thought of as a function, it is that I don't know the point he is trying to make. Also, Haskell has maps. Does he say that? No, because he is not trying to be honest. Even his arguments against Haskell records, which are easy to criticize, don't make sense. (Almost) No one would think that his person type is good. So who is he arguing against? Why does he make up this term "Place oriented programming?" He knows that you can name records so why does he call it place oriented? "Lets add spec!" Yes! Spec is great, but the problem is that I am lazy and am probably not going to use it in all the places I should. Types make sure I am not lazy and do it before my code runs. Most of his rant about maybe sheep seems like he would be happier if it was named "JustOrNothing". Because he is being sarcastic over actually trying to communicate I have no idea what he is trying to say. Yeah, having to annoy a bunch of nearly similar types is annoying. That's why you shouldn't do it. The portion about his updated spec framework is interesting thought. It reminds me of classy lenses. Don't tell Rich about classy lenses though or he will make a video saying "classy lenses? that makes no sense. Lenses don't go to school" I would like his talk a lot more if he just focused on that instead of arguing against Maybe in an unconvincing way. Rich is wrong. [a] -&gt; [a] does tell you that the output is a subset of the input. I get the point he is making, but Haskell does have laws, and I don't think he understands the thing he is criticizing. It is also hilarious he spends so long criticizing types for not capturing everything, then five seconds latter says about spec "Its okay if it doesn't capture everything you want". Like, dude, did you just hear yourself from five seconds ago? Haskell also uses test property based testing. Quickcheck exists. If challenged Rich would probably agree, but he isn't going to bring it up himself. I am getting way too worked up about this but Rich Hickey's style of argument annoys me. You can have a debate about static versus dynamic typing, but you can't have one with Rich. P.S. Shout out to the people upvoting this five minutes after it was posted. Way to watch the whole thing. 
He talks about evidence. If the big companies writing type checkers for dynamically typed languages (PHP, JS, Ruby, Python etc.) is not evidence, I don't know what evidence he'd accept. [ClangMR](http://www.hyrumwright.org/papers/icsm2013.pdf) can't be written for a language like Clojure.
Thank you so much for articulating exactly why this was frustrating to watch for me. So many small misrepresentations. "Also, Haskell has maps" was on my mind for a full quarter of his talk - for those instances he describes where maps are the perfect representation, we use them too!
I don't get it either. His "Simple Made Easy" is a great talk, and static types is the perfect example of what he is talking about! I tried to get at this in my rambling comment, but I really do feel that he has just decided static types are bad and then stops thinking. 
I think he did say that he is not talking about haskell in general but, he is talking about MayBe in Haskell. He mentioned that haskell got it wrong and he got the notion wrong in Spec but, Dotty got it right with union types. Keeping aside his jokes, do you think its worthy to discuss his idea or just dismiss it straight away?
Most of my criticism is of his delivery, not his ideas. It is cool how Kotlin converts from non-nullable to nullable types implicitly, making it easier to do the type of refactoring he is talking about. These ideas are worthy of discussion. I just wouldn't want to have that discussion with him.
one difference to note is that haskell maps need homogeneous values, but in clojure, they can be heterogeneous. So I think `Map` is not the same concept as clojure's map, but record types seem to be! Even the fact that a haskell record field is a function from `ProductType -&gt; FieldType` seems to fit with one thing he appreciates about clojure maps (though which thing is a function is reversed)
1. He pointed out breaking API changes even though you're being more liberal in what you accept/more restrictive in what you emit. He conveniently forgot to point out that those "breaking API changes" can be _entirely_ automatically fixed. In the input case, the caller can be changed to `foo (Just x)` instead of `foo x`. In the output case, the caller can be changed to `Just (foo x)` instead of `foo x`. 2. With union types, all nils are the same. Not so with sum types. It probably doesn't make sense to have nils with different semantics in the same homogeneous structure like a list. 3. Types get refined by checks (instead of binding new names using patterns), so the effect is as if the type corresponding to a name is context-sensitive. let x : string | int = ... -- very much like implicit shadowing in if (x `is` string) then bar x -- x : string here else foo x -- x : int here Does this mean union types don't have their place? No, they certainly do. You get more convenience at the cost of weaker guarantees (at least in some cases). A fair discussion can be had once both parties are interested in hearing both pros and cons...
Yeah I think I agree that having some built-in language features for the nullable/not-null case is interesting (and I might even go far as saying it's better than Maybe). However, from another one of his talks, he explains how you shouldn't break things, and you should give new things new names. I would argue that a function that used to return a `Maybe String` and now returns a `String` is different and should get a new name. You probably don't want the caller code handling the `Nothing` case if it's no longer possible. Instead, we could just make a new function with a new name that returns `String`, then just call it from the `Maybe String` function with `pure . otherFunction`. Now old callers still work, and they can upgrade easily by calling the new function. Seems like something Rich would suggest, but doesn't because he was trying to make a different point.
Beautiful, this is clean.
Clojure actually supports both - `({:a 1} :a)` and `(:a {:a 1})` are equivalent, as long as your domain are keywords.
Yep you can do this at the top level with things like `servant-client` too. It's nice!
Does the dedicated tooling work over a git history? How about when dependent packages don't compile? As I said, I'm glad that tooling exists, and I use them all the time, but sometimes you're all alone with a simple terminal and no tooling.
My thought was: since I have to modify multiple records located in different places in my table model, and since I will need some kind of composition, let’s go with lenses. But sure thing, the most pragmatic approach is the most straightforward: just functions
&gt; Prefer do notation over list comprehensions. May I ask for the reason? Performance? Avoiding yet another syntactic extension?
Ok, what he is describing is an approach to type records. He suggest using 2 types of declaration: * "schema" - description of fields which _may be_ in the data record, at various usage places. So that they don't need to be described in each place, and wait is also important cannot happen to be different. Can be nested (a field contains a record with its schema itself). * "select" - specification of the which only lists the fields, from the schema, for the usage place. May refer to the top-level field of sub-field of field. 
Have you read ["String and Lazy Semantics for Effects"](http://www.cs.cornell.edu/~akhirsch/pubs/strict_and_lazy_semantics_for_effects.html)? Can recommend!
&gt; The extension bit seems unlikely too as list comprehensions aren't an extension. I forgot that "extension" has another meaning. I meant syntactic sugar. But since the list comprehension syntax exists since Haskell98 this indeed seems unlikely as well.
Same as /u/pyry and /u/lexi-lambda here (no category theory, relearning better every day). To add up : After stopping and re-starting a few times to learn Haskell it really started to "click" when I began to feel comfortable with making simple command line-tools and servers * Feeling comfortable with Text and Bytestring, both strict and lazy, removing String * Feeding a file to a JSON Aeson parser * Outputting some statistical computations (summation, top 10, transformed series ..) * Thus using Array, Map and other basic containers * And surprisingly coding a server in WAI felt as simple as, say Express or WebPy At that point I stopped feeling discouraged every week or so and now use Haskell on a daily basis - ok not as much/fast as python yet but **yes**, it had clicked (2 cents). &amp;#x200B;
&gt; I feel like you and I have a different definition of what it would mean for a function to take a `Basket` and return a `Basket`. Or it could just be that I am misunderstanding your examples. If so, I'm sorry - I'm very new to Haskell! &gt; Why do you care about whether `Total` and `Tax` are "inside" the `Basket` or not? What really matters is how hard it is to change the code later. Do we ever see the words `total` and `tax` anywhere in the code apart from the points where we create or use them? The motivating example for this came from Clojure. In idiomatic Clojure code, we typically pass around maps (i.e. hash-maps). So a function will take and return a single argument, which consists of multiple key/value pairs. The input might happen to have a `:total` key but, if that's not something used by this particular function, we won't see it: (defn add-discount-voucher [basket voucher] (assoc basket :voucher voucher)) The above function could be used at any point in the pipeline: if there's a `:total` key in the basket it will just be passed on through the pipeline, if there isn't then it doesn't matter. This function actually does require a `:total`: (defn add-tax [basket tax-rate] (assoc basket :tax (* tax-rate (:total basket))) But it didn't care whether or not there was a `:voucher` in there. Unused keys are just passed on through the pipeline along with the rest of the map. I may have misunderstood but in your example it looked like `tax` had become a separate variable that I would have had to plumb through the rest of my code. &gt; Doesn't that make the word "Basket" meaningless? Yes, it pretty much does. What I want is a way to say that, right here for this function only, the input object must have _these_ properties and it will create _these_ new properties, without that fact infecting the rest of the code. For example, in Clojure we would use destructuring to pull out the keys we want. The `add-tax` function above would actually look more like this: (defn add-tax [{:keys [total] :as basket} tax-rate] (assoc basket :tax (* tax-rate total)) We can see from the function signature which keys are required by this function... but it has no impact at all on any other functions in the code base. The problem is that this is just looking up keys in the map at runtime, there's no compile-time checking going on. &gt; {:basket basket &gt; :tax tax &gt; :delivery delivery} &gt; Is this an example of an output you want, or an example of an output you don't want? For this particular example, it is an example of output that I don't want. Here we have a `basket` map inside another map. I don't want nested maps (or tuples). I gave it as an example of a different approach, in which we don't have a pipeline of functions like `add-tax` and `add-delivery` at all. Instead, we just calculate the intermediate values in a `let` clause and bundle them up, if needed, into a single data structure at the end. This is the pattern that I generally see in strictly-typed languages (and is what I thought I saw in your example too). I was saying that a good argument could be made that that different approach is actually better than the Clojure-style passing around of amorphous blobs. But this particular enquiry was an attempt to find out if something like the Clojure-style could be preserved, only with better compile-time checking and perhaps easier comprehension for humans.
I'm sketching an stochastic process that picks a data constructor from a data type with a given probability. In principle this probability can be different for each one, so I need to have a different probability `pCi` for each constructor `Ci`. The thing is, in the particular scenario of uniform distribution of probabilities, I would like to be able to assign each variable name to the same value in a batch. Does this makes sense?
Hey! I've answered that here: [https://www.reddit.com/r/haskell/comments/a1in90/feature\_binding\_multiple\_variables\_at\_once/ear26v5/](https://www.reddit.com/r/haskell/comments/a1in90/feature_binding_multiple_variables_at_once/ear26v5/)
&gt; BTW, my other frustration with him was when I tried Clojure after already becoming a haskeller. My experience turned out to be so awful that I just couldn't wrap my head around why anybody would recommend dynamic typing. This was exactly my experience. The only reason someone argues for dynamically typed languages is being lazy or not understanding what types are about. There are also so much concepts in Clojure that mix up things everywhere that you just get easily confused. It's like you're put in a maniac's mind.
That's clever! Although I'd love to avoid having to use a list in the first place :)
That's clever! Thanks for sharing!
Hey, I'm currently working on a similar project and while this is still very much work in progress and i'm not entirely sure it works yet, maybe it can serve as an example on how to work with stm and sockets: https://gitlab.com/gilmi/pokeworld/tree/master/server/src/Pokeworld/Poker/Server
The whole thing is a shell game. &amp;#x200B; Where do I want to put nil? Under every type, or under a Maybe type? &amp;#x200B; Also, even though he denies it, his "shape" solution sounds like parametricity.. &amp;#x200B; Hickey popularised immutable data. Truly hope spec is amazing, but somehow doubt it on this showing..
Absolutely - and the confusion between variants (the 'or' types he is looking for) and `Either` is annoying. Sure, `Either` could have been named `Validation` or `Result` which does better communicate its intent, but nobody is suggesting it isn't right-biased. It's hard to tell whether this confusion between variants and parameterised types is genuine confusion or a straw-man. 
It minorly irked me that he never pointed out the important difference between maps as morphisms and finite maps. Pretending that `{:a 1 :b 2} :b` is total is precisely where bugs come from.
Thanks for a thoughtful acknowledgement of a good answer! No amount of upvotes can compare. I believe it's exactly the kind of communication that encourages people to make contributions and hence evolves our community :)
&gt; And in those languages, `[]`, `Maybe`, `Int`, `Double`, `Bool` all have a default, and the awesome thing about this concept is it's so well understood that absolutely everyone knows what those defaults Count me out, "absolutely everyone" is an overstatement. What's the default for `Bool`? 
Or if his list elements don't have an Ord instance.
This is what mobile haskell (Moritz &amp; Co.) are all about! Take a look at the documenation: [https://mobile-haskell-user-guide.readthedocs.io/en/latest/#](https://mobile-haskell-user-guide.readthedocs.io/en/latest/#) There's even a docker container (for Raspberry Pi) :)
`False`
Rich Hickey, in my experience, has a history of having "revolutionary" ideas of paradigms and functionality that have been already invented elsewhere and more in-depth, and criticize everything around it without fully comprehending what he's saying. In the past, that was macros and "you don't need hygiene, just have quasiquote resolve the symbols lexically". Half baked implementation of an idea that Kernel develops fully with no gotchas, criticizing hygienic macros without actually understanding the problem that they solve. Today, it is contracts and how they relate to static typing. The guy is smart, don't get me wrong, he gets 3/4 the way other people have already gone, and sometimes his view from another side is helpful as an incipit to understand the problem slightly better, but that's on you to take away from what he says. He's got a really bad case of conceptual NIH syndrome.
Will do! Cheers. 
&gt; His whole thing about "No code associated with maps" also does not make sense to me. Dose he conjure hashtables from the ether? And if he means a more abstract notion of a mapping, then the same can be said about functions. &gt; &gt; His example of a map can just also be just as easily be written as a function in Haskell. Maps are more restrictive than functions, and therefore offer more guarantees and more capability for introspection, particular in a language without static types. For example, you can guarantee that a map is both a pure function, and will terminate in a small amount of time. You can also query its domain and range, and serialize it to a string. Idiomatic Clojure prefers data structures over arbitrary code, because assuming you trust the underlying implementation, data gives you more guarantees. It's a similar idea to using types to narrow down usage in Haskell. &gt; My point isn't that he is wrong. A map can me thought of as a function, it is that I don't know the point he is trying to make. Also, Haskell has maps. Does he say that? No, because he is not trying to be honest. Maps in Haskell are a somewhat different animal, because they're homogenous, whereas maps in Clojure are hetrogeneous. Clojure maps are often closer to record types in Haskell, except that records are closed whereas maps are open. Clojure also focuses on key/value pairs as the unit of typing information, rather than the map as a whole. So where one might use a type constructor in Haskell: Mvn.Version "1.3.0" In Clojure: {:mvn/version "1.3.0"} So while Clojure and Haskell both have maps, their use in their respective languages is rather different.
&gt;Haskell has maps. Does he say that? No, because he is not trying to be honest. Is a Haskell map itself a function? I think the point he is trying to make that Clojure maps themselves are functions. \&gt; Why does he make up this term "Place oriented programming? The record object itself not an object which one can call with a slot name. A place is for example a slot in an object. For example if a user object/record with name, age, ... is allocated, it has a place for the name. An user as a map does not have. There is simply no mapping from the identifier name to a string in the map. Not that it makes too much sense.
For the first thing, it sounds similar to a ZipList where \`pure\` will give you a repeated value and then you could zip with a list of your constructors. I think there's similar ideas from Tikhon Jelvis [https://www.youtube.com/watch?v=qZ4O-1VYv4c](https://www.youtube.com/watch?v=qZ4O-1VYv4c) &amp;#x200B; Second thing doesn't seem major to me :)
I think people usually structure these libraries in two parts. The raw bindings and then an abstraction layer on top. Also I wouldn't really say that `ResourceT` is a good fit here. Allocation patterns and object lifetimes in games are vastly different from lifetimes in streaming/processing types of programs. I think `ResourceT` will only force you to keep everything alive forever (or for the duration of the level or something). Especially since the control will probably involve an event-loop of some kind meaning you can either allocate and free everything each frame or you can allocate once and free at the end of the game. `ForeignPtr`s and GC are definitely the way to go here, possibly with a mechanism for explicit freeing. Unfortunately I don't really know how to deal with the transitive ownership problems you described.
Haskell has some support for [dynamic typing](http://hackage.haskell.org/package/base-4.12.0.0/docs/Data-Dynamic.html) so you can do that in Haskell too. 
&gt; It is cool how Kotlin converts from non-nullable to nullable types implicitly, I think Haskell's approach of using `Maybe` with a monad instances is quite nice, actually. Better than anything I've seen elsewhere. 
&gt; Is a Haskell map itself a function? I think the point he is trying to make that Clojure maps themselves are functions. Until you look up a key that isn't there, in which case it's suddenly non-total. You could easily treat a lazy map as a function. 
&gt; Hickey popularised immutable data. Eh? 
It looks like it's picking up a different libwinpthread-1.dll than the one it's looking for. I assume starting ghci has the same issue? I'm which case open the msys2 shell that comes with platform and type `strace ghc --interactive` which should tell you which winpthread it loaded.
I got this idea from Jake McArthur on this twitter thread: https://twitter.com/BanjoTragedy/status/1008778263920037889 There, we were trying to think of a way to bind both a "long name" and a "short name" for a variable, to strike a balance between documentation and readability, like this: tax a@originalAgi r@marginalTaxRate = a * r 
That kind of "refining" exists in Haskell too: you can get additional information about a type variable by pattern-matching on a constructor carrying a constraint. GADTs are the typical example. data Tag a where TagInt :: Tag Int TagString :: Tag String foo :: Tag a -&gt; a - &gt; Int foo tag x = case tag of TagInt -&gt; x + 42 -- x :: Int here TagString -&gt; length x -- x :: String here
I guess that means Haskell people can say we had immutable data *before* it was cool.
If someone has a record of saying silly things, I tune them out. 
Fantastic!
"I and my community frequently disagree with this person so I'm tuning them out" is a great way to build an echo chamber. Yes, I'm aware of his recent "Open Source is not about you" tirade. Despite this I still think he's an intelligent person and I think that paying attention to his critiques of type systems can sometimes be interesting and informative.
Using functional languages that weren't Haskell. Specifically Coq/Gallina, but anything that has strong pattern matching with very explicit syntax is probably just as good. Haskell is extremely idiosyncratic, and if your way of thinking is not amenable to the way Haskell wants to write/express/do it, you're really fighting an unnecessarily uphill battle IMO.
I was saying that before it was cool.
I think that's actually not as bad as needing to iterate over the keys.
Regarding #1 Just because the fix _could_ be automatic, doesn't mean it is, and it still means that a fix needs to be applied. The social consequences of this are that if package `blorp` needs fixing because package `boop` made this change, then usually only the latest version of `blorp` will be fixed; the fix will not be backported to old versions of the `blorp` api. Consequently if you want to use the new version of `boop` you *cannot* use older versions of `blorp`. "Well it _should_ be easy to upgrade everything" you might say. True, it should in theory, but in practice this can end up being a lot of code churn that people weren't ready for or didn't necessarily want. There is huge value in having new releases of a library be API COMPATIBLE because it reduces churn and gives more flexibility in what you can upgrade when. tl;dr a breaking API change can have a big ripple effect on the hackage &amp; downstream user ecosystem, even if the process of upgrading to the new API is trivial.
It is total. As long as you understand that "the result of this expression might be `nil`". The use of `nil` is pervasive in Clojure, and is certainly a source of bugs when the programmer forgets that a value might be `nil`. I'm not sure why Rich acts like Clojure is not affected by "the billion dollar mistake."
Looks like it’s a shallow embedded edsl, like some of the iohk stuff too?
&gt;has been making me wonder how such a bright guy who clearly keeps looking for a deep understanding of things could be preaching things that I find myself in a great conflict with. If you agree with everything someone says you probably aren't thinking for yourself. You should expect to disagree with even smart people on at least some things.
I think Rich Hickey has some cool ideas about testing code and ensuring properties through dynamic tests. Clojure spec is cool, because it has this emphasis on testable documentation, or readable tests, or however you want to look at it. What I also think is cool, though, is *static* analysis, *static* verification, *static* guarantees. His talks never really address these. His philosophy seems to be "oh we can just write dynamic tests for that." On one hand, I get it. When your testing is dynamic, then you have all the flexibility to really test complex stuff that might be hard to express in a type system or other systems for static analysis. On the other hand, I prefer having a type checker to watch my back and be my buddy while I'm coding. Testing is only sufficient when humans remember to write comprehensive tests. Type checking, on the other hand, is comprehensive by default.
Certainly, I'm not saying the issue is cut and dry, at least not until we have awesome tooling for our package ecosystem like companies (e.g. Google) have internally, as well as a community consensus that we're willing to let tools upgrade all our packages at once. Right now, there has to be a gradual process of deprecation followed by upgrades. My view of the talk's intention is "we're at a roadblock, let's head back home". My perspective is "huh, we can theoretically get around the roadblock, let's try getting that to work in practice too, so we can keep going forward, instead of going back".
Yeah, this. List comprehensions are syntactic sugar for `do` notation specialized to lists, which means they're simultaneously less powerful, less reusable and more of a syntactic burden for everyone involved.
Exactly my thought. Haskell changed the way I saw programming. A powerful type system gives you (formal) guarantees that you cannot get from dynamic languages nor lesser type systems. When I first learned the "true" power of types (as simple as ADTs (e.g. `Maybe`, `Either`) and phantom types) it blew my mind and thought "how have I being leaving without them all this time". It also gave me "hope" about the possibility of writing "correct" systems (which just keep getting bigger and more complex). I honestly believe that people that categorically diminish types either: a) work on systems that do not require certain level of guaranties; b) haven't really used/learned a powerful type system; c) are just dishonest. Not really sure which one could be the case with Hickey.
When these types of eternal discussions pop up \_(static vs dynamic)\_ I find it very interesting watching how either side finds the other side bizarre. Interestingly both sides believe there way is \_"liberating"\_ in terms of programmer productivity. &amp;#x200B; I'm starting to think this is some kind of "left brain vs right brain" thing, with some outlier aliens that are both left and right and feel at home in both camps. 
One of the things he briefly mentions is that he wants extra data to be able to "pass through" functions. I used clojure at my last job, and this philosophy was quite pervasive. You can write a small function that just looks at one or two keys on the input map, throws a new key into the map, and then returns the whole new map as a result. That map might have a bunch of keys in it, but this small simple function only cares about 3 of them, and just passes the rest along unchanged. This is "composable", in its way. This philosophy also found its way into our Kafka-based message passing philosophy. Message types would be huge, because messages included *all* information that needed to flow downstream, even if said information was not needed for the *current* task at hand. And yet, the message was only used for the current task at hand, so the *output* of the current task had to append its thing, and then pass everything along in its output message. What irked me about this pattern was that "you *must* pass on keys XYZABCHIJK" was frequently a requirement of such services. Small services which *should* have been simple to test became real beasts to test, because testing them *had* to also be aware of what was composed downstream of this function, and *had* to make sure that this function was accepting *all* the info necessary for the rest of downstream, and was producing all the info necessary for downstream. In short, this "pass it along" style of composition rubbed me the wrong way. I'd rather have a manager service that *knows* exactly what a function/service needs, and then give it *only that*, *directly*, each step of the way. As opposed to flowing all of the data through every step of the composition chain. And then a downstream component needs a new bit of info and so you have to update *all* of the upstream messages and services to make sure they are now also accepting the new message data and test them to ensure they are passing it along. (This may be overstating it a bit.)
Yes, it would seem the %PATH% does not have ..../ming/bin on it, or perhaps comes after mingw/x86_64-w64-mingw32/lib (which shouldn't be on the %PATH% anyway?). Using strace, I see my ghc.exe from 8.4.3 is loading libwinpthread-1.dll (not .a) from ....ming/bin . Is that right that ghc is trying to load a static library rather than the dynamic library?
[removed]
&gt; He's got a really bad case of conceptual NIH syndrome. Honestly it's just NIH full stop. It's one of the most frustrating things about Clojure, and why people were so mad the other day. Rich let's almost no one contribute to core, but conversely, when he does add stuff to core is often just a note Rich-y solution to something the community had already come up with solutions for. So rather than either adopting or recommending Schema, we get Spec, which is critic and harder to use. Instead of standardizing on lein/boot for a build tool, suddenly we have clj, which isn't even feature complete compared to either of those. Now, after lecturing the community in how dare they complain about the way they handle open source, Cognitech instead announced REBL, an entirely proprietary dev tool for Clojure. 
Wow! That looks pretty handy. The TUI and CLI demos, and docs, made it very clear. I love the real-time feedback in the TUI.
Thanks :)
That’s not in the hands of the research group. The program that funds this internship has those restrictions (see the respective link in the above post). So the rightful receiver for any complaints about that would be the DAAD or the German Ministry of Foreign Affairs (“German Federal Foreign Office”), which gives the money.
Another person mentions dynamic types, but more commonly what you want is for the key to determine the type of the value, and for that, we have [DMap](http://hackage.haskell.org/package/dependent-map-0.2.4.0/docs/Data-Dependent-Map.html), which I've been using more and more lately, and have come to realise that it pretty much is the extensible record system everyone always wanted in Haskell.
No, it's not a static library, it's an import library, it points to the dynamic library, which is why in the screenshot you see it loading the import library and then amending the system search path with the folder the import library is in. So /lib ends up on the path. It then finds that the import library is pointing to libwinpthread-1.dll and attempts to find it. It's at this point it either doesn't find it or it finds it but can't find one of it's dependencies. The error unfortunately doesn't distinguish between these two events. Can you paste the output of strace? 
I've been writing Clojure professionally for almost six years now--I started back when it was at version 1.2 I think. It was my first functional language, my first lisp (let's leave aside whether or not classifying Clojure as a functional language or lisp is debatable, just know that I probably agree with you or otherwise don't care). Some of the wonderful ideas in Clojure--immutable data structures, higher-order functions, etc. etc. made it so programming finally seemed manageable to me; I've struggled my whole career to understand what OOP is, and Rich Hickey helped me realize that maybe the problem was not with me. That was liberating! His approach of taking inspiration from academic computing to apply to the industry was also incredibly refreshing--learning that Clojure used HAMT in its map implementation, and then struggling through Phil Bagwell's [Ideal Hash Trees](https://infoscience.epfl.ch/record/64398/files/idealhashtrees.pdf) was challenging, but at the end incredibly rewarding. Naturally I started digging deeper, and I found Haskell and OCaml/ML-family languages--and that was really exciting! Here we have a lot of the same powerful stuff Clojure introduced me to along with a whole world of sophisticated type systems which it turned out could encode all kinds of things so that you could express your domain logic in the types. For me this provided a new powerful expressive capability, safety, and more. So I got into Haskell, naturally, and it's been really wonderful. For me it's all just a continuum of interesting exploration, and I owe it all to Clojure and Rich Hickey. I'm really grateful to him and the Clojure community for that. Which makes this really ironic and disappointing to me: I can't really take him seriously any more. I thought he was someone who, like me, didn't come from an academic background, struggled to understand what OOP is all about, and then realized there was a whole world of academic research to draw from and explore. I thought he was someone who could see that there are tradeoffs and complexities to e.g. static typing vs. dynamic (or uni-typed, if you are more in the Bob Harper camp, I don't have a dog in the fight) and it was interesting to think about and talk about without being dogmatic or misrepresenting any given position. But now I realize--and it pains me to say this, and I know it's harsh so I'm sorry Rich, I really am: his actions are those of someone who is attempting to present himself as an authority on subjects he doesn't really understand. In a curmudgeonly fashion at that. He's still got valid positions I still agree with, and I think he's a smart guy. But there are any number of people I'd rather hear from these days than him. "So long, and thanks for all the fish." 
Hooray for Haskell in Germany :)
Okasaki's book on data structures. The difference with Haskell is laziness. It's what takes the language beyond OCaml and beyond PureScript or Idris. There are many nice things in Haskell that can distract you (`mtl`, `lens`, etc.) but ultimately what's really nice is that you have a whole new set of immutable data structures that other functional programming languages don't have access to.
&gt; Haskell is extremely idiosyncratic, and if your way of thinking is not amenable to the way Haskell wants to write/express/do it Really? I've found Haskell to be quite expressive. 
I think you make a great point. I'm more excited about the "static" part of "static typing" than the "typing" part.
Well, I haven't seen a Haskell job in Germany in ages. I think that's rather disappointing.
Good point, and thanks! This did originally start out as a PR. However, I made the decision to turn it into a standalone library (something that I don't take lightly) because I realized that there is too much added to be practical/feasible as a PR. 1. *non-empty-containers* contains ~600 lines of code, and ~100 lines of comments/documentation. 2. *nonempty-containers* contains ~7500 lines of code (5000 library, 2500 test), and ~3500 lines of comments/documentation. It also integrates into its own CI system. A PR would basically turn the original codebase into 8% of the final codebase by lines of code, and 4% of the original codebase by documentation. At that point, it's more or less a full rewrite, and not something that would be practical as a PR.
Does this just make things less hacky, or are there performance benefits? I didn't really understand the architecture from the announcement, and haven't used lambda before.
As a maintainer of non-empty-containers i wouldn't have been opposed to a full rewrite. It was written to accommodate some needs at work, and wasn't fleshed out due to time. 
&gt; Now, after lecturing the community in how dare they complain about the way they handle open source, Cognitech instead announced REBL, an entirely proprietary dev tool for Clojure. This really pissed me off. Based on the wording of the EULA, not only is it closed source, but it is also not licensed for commercial use.
That's reasonable, but it is important to note that there is a cost for end users too, not just the library implementer. The issue is that I might have a large map on hand, and I may want to run some code on it in the event that the map is non-empty. If the "non-emptiness" predicate is enforced by a `newtype`+encapsulation, then the library can give me *the same* map back, coerced to the `NEMap` type (a compile-time notion only). On the other hand, with the correct-by-construction-types approach, the library will have to construct an entirely new huge map. This means there is a possibly high run-time cost for what was supposed to be a compile-time concept.
Maybe a middle ground would be to change the invariant on `nemK0` from `must be smaller than smallest key in map` to `must be *equal to* smallest key in map`? Then you could just hang on to the original map, augmented by an element that witnesses its non-emptiness? There is still a tiny penalty for the indirection, but that's not so bad.
Too much fear. Read post. :)
This is an interesting idea, in that it's a "best of both worlds" kind of thing. You preserve sharing, but functions that motify the map have to do some bookkeeping to make sure the "finger" stays up to date. I'll give it a look, thanks!
I think the style I am discussing originated with Smalltalk in the 70s, it's basically [method cascading](https://en.wikipedia.org/wiki/Method_cascading).
 scanr' f q0 (x:xs) = f x q : qs where qs@(q:_) = scanr' f q0 xs is equivalent to scanr' f q0 (x:xs) = let qs@(q:_) = scanr' f q0 xs in f x q : qs --- `let qs@(q:_) = scanr' f q0 xs in f x q : qs` is equivalent to `let { qs = scanr' f q0 xs; q = head qs } in f x q : qs` Which is: scan over the tail of the list and call that `qs`, take the first element of `qs` and call that `q`, combine the head of the list and `q` using `f`, and cons that combination with `qs`. --- Any specific questions?
(Software Foundations)[https://softwarefoundations.cis.upenn.edu/current/index.html] is a free online book that I used to learn Coq, and I agree, it totally helped my haskell.
It doesn't need to, and in GHC it definitely doesn't, because there's no `case` in the desugaring of `id x = x`. Select your favorite encoding of lambda bound variables, and `id` can just return whatever the binding target is, without inspecting it or doing anything else that would force / evaluate it.
Greater than or equal to sounds pretty arbitrary. Is that lawful?
That would make more sense to me if `containers` weren't already full of things that are *not* correct by construction. Sequence size annotations, `Map`/`Set` ordering and balance, etc.
I've setup the default spacemacs settings for my Haskell editing. In the left fringe colours are shown for most lines: green, blue and red. What do they mean and how do I get more information on each line? My first guess was that red was bad but hlint does't complain and the code does compile.
What makes it only 50% of the sharing that is preserved? I would have thought the conversions would both be O(log n), so could only be reallocating a log amount of the structure (and sharing the rest).
 extract . duplicate = id -- this law and the next would break if we used strictly greater fmap extract . duplicate = id duplicate . duplicate = fmap duplicate . duplicate The first two laws are pretty easy to see, the third one less so but I believe it holds. 
[This cabal file](https://github.com/markandrus/twilio-haskell/blob/master/twilio.cabal) declares two different test targets. What command can I use to test `apitests`, `integrationtest`, both? - Thank you.
&gt; Is a Haskell map itself a function? Yes. It is (modulo the ordering constraint) iso to a function `a -&gt; Maybe b`. This is one of the basic standpoints that conal starts from in his discussion of type class morphisms.
For the sake of diversity, here's a simple solution using only map and recursion. pairs :: [a] -&gt; [(a, a)] pairs [] = [] pairs (x : xs) = map ((,) x) xs ++ pairs xs If you have TupleSections enabled, you can also write this as pairs :: [a] -&gt; [(a, a)] pairs [] = [] pairs (x : xs) = map (x,) xs ++ pairs xs
Very exciting! I'm definitely enjoying using the library and playing around with it. The biggest pain point I've yet to solve for my own usage is just the inherent non-composability of very strict types, which can't really be helped. I've gotten used to being able to pull things out and use lists or small composing functions as a way of reducing boilerplate, but when every attribute or element is a different type, you can't really glue things together any other way than the long way (or at least, I haven't discovered how to if it's possible :) Although, amusingly, trying to write a generic `withAttrs` function did lead me down an entertaining rabbit hole of heterogeneous structures, open records, and row types before finally realizing it wouldn't be nearly as ergonomic as just writing things out. Just out of curiosity, I'm looking to build a small and simple static site generation type of deal using this; what are other people using it for?
Ah, you're right. I was thinking that only half of the top level would be shared, but also now realize that half of the other half could also be shared, and half of that other one. It's actually closer to full sharing than I thought, so maybe it's not that big of a cost after all?
Thanks for the answer. I'll seriously consider applying.
I thought the same thing. I don't have much practical Haskell experience, but it seemed to me that you could solve this similar to the *strict* and *safe* functions. It is also similar to how C# solves the "new" *Async* functions that calls the old one asynchronously. I would love to hear if people actually does this, and what their experience is.
Sorry, I mis-read the message in the image. Yes, *.dll.a got it. (Note: I am not the OP, so the strace from my working installation would probably not be interested for you.) Perhaps the OP could provide the PATH, along with the strace, as that might be useful for the next step in looking at this? Also, could possibly be related to "[GHCi failed to load pthread on windows.](https://github.com/haskell/haskell-platform/issues/314)" haskell-platform issue?
`stack test apitests`, `stack test integrationtest`, or just `stack test` for both. I don't know what the command for cabal-install is. 
Hit `SPC e l` to list the flycheck errors. At least in Vim mode. 
Thinking of `flip fmap` F a -&gt; (a -&gt; b) -&gt; F b [`Yoneda F a` = (`forall bb. (a -&gt; bb) -&gt; F bb)`](https://hackage.haskell.org/package/kan-extensions-5.2/docs/Data-Functor-Yoneda.html) is the return part packaged up (universally quantified : `forall`) F a -&gt; (forall bb. (a -&gt; bb) -&gt; F bb) F a -&gt; Yoneda F a F ~&gt; Yoneda F and [`Coyoneda F b` = (`exists aa. (F aa, aa -&gt; b)`)](https://hackage.haskell.org/package/kan-extensions-5.2/docs/Data-Functor-Coyoneda.html) is the two arguments packaged up (existentially : `exists`) (exists aa. (F aa, aa -&gt; b)) -&gt; F b Coyoneda F b -&gt; F b Coyoneda F ~&gt; F
In our case, probably just less hacky. If you had really short running lambdas, it would probably save some latency.
Interesse in een Haskell-gerelateerde internship?
beautiful
Make sure you removed the correct one. ;)
List will probably be optimized away.
I think Haskell 1.4 had Monad comprehensions. :)
As a paramorphism, to add to the zoo: pairs = para pairsAlg where pairsAlg Nil = [] pairsAlg Cons x (xs, rest) = map ((,) x) xs ++ rest (Using [`para` from recursion-schemes](http://hackage.haskell.org/package/recursion-schemes-5.0.3/docs/Data-Functor-Foldable.html#v:para).)
This is essentially the Comonad instance for `NonEmpty`.
Making straw-men out of alternative solutions to more effectively preach to the choir is definitely something that deserves a critical response. If you're breezing over shared knowledge, that's one thing, but if you're bringing up 'foreign' topics about alternative solutions and lampooning them as ineffectual it neither helps to reinforce your choice, or educate your audience. It's a technique to make an audience feel good without imparting any actual value, and it's a huge part of why language devotees keep getting stuck in stupid religious wars about who has the more better technique instead of actually sitting down and thinking critically about which situations benefit from which approaches.
thanks
As a futumorphism of anamorphisms, to add to the zoo: pairs :: [a] -&gt; [(a,a)] pairs = futu pairsCoAlg where pairsCoAlg [] = Nil pairsCoAlg (x:xs) = case ana psCoAlg xs of Free.Pure _ -&gt; Nil Free.Free t -&gt; t where psCoAlg [] = FreeF.Pure xs psCoAlg (y:ys) = FreeF.Free (Cons (x, y) ys) Using [`ana`](http://hackage.haskell.org/package/recursion-schemes-5.0.3/docs/Data-Functor-Foldable.html#v:ana) and [`futu` from recursion-schemes](http://hackage.haskell.org/package/recursion-schemes-5.0.3/docs/Data-Functor-Foldable.html#v:futu). The "futu" is because we want to provide a whole "segment" [(1,2),(1,3),(1,4)] at once. (Sorry about the ambiguous constructor names, `Pure` and `Free` are popular.)
That’s really cool! What would need to happen to make this behavior pervasive? Eliminating a class of inscrutable type errors seems like a big win. Is this generic enough that it would fit in the compiler itself, or would people need to implement their own type families to make it work? In the latter case, is there a way to generalize this into some kind of framework library that type family users could to provide sensible error messages?
&gt; “It is difficult to get a man to understand something, when his salary depends on his not understanding it.” --Upton Sinclair He has a financial stake in clojure. Your assessment that he's arguing in bad faith, especially with regard to static typing, is something that's struck me more than once in the past.
I think this technique is general enough to report errors about any stuck type family.
I think row polymorphism like in purescript may be one of the better approaches to this problem. &amp;#x200B; It seems to me like the pain point is a lack of expressiveness in terms of structural vs. nominal typing. If one portion of the program only cares about a few fields, it needs a way to say "I operate on something with these fields" without introducing an ad hoc nominal type to represent that subset of fields (ThingUsableHere).
I appreciate you taking the time to write this, but this is so far above my head! Care to explain what it's doing?
Brilliant.
This is great! I've been using `generic-lens` a lot and those type errors are pretty bad as they are! The example error at the start of the article is pretty typical.
Hopefully soon they will be no more!
 Yoneda F a vvvvvvvvvvvvvvv F a -&gt; (a -&gt; b) -&gt; F b ^^^^^^^^^^^^^^^ Coyoneda F b 
Because the kind of totality he is interested in is avoiding exceptions, not avoiding `nil`. A lot of work is done to make it a safe value to pass around.
Yeah. I tried to find a way to remove a ForeignPtr finalizer, but couldn't. I was going to look into using WeakRefs on Ptrs as an alternative. I used newForeignPtrEnv with a Ptr to signal to the finalizer whether it should free or not. That was just kicking that can down the road though, because then I have to manually manage the env Ptr. So then I used unsafeForeignPtrToPtr which worked, but I think it's only working by coincidence.
Clever!
&gt; but record types seem to be! Closure maps are extensible, so they are more like the open record types in OCaml.
This is a common mistake, it comes from the fact that it is harder to write point-free functions with more than one argument. A simple way of checking this is using the [pointfree website](http://pointfree.io/) : it turns a `\xs ys -&gt; map (chr) $ zipWith f xs ys`, the expression you want to write point free, to `(map chr .) . zipWith f` instead of the trivial `map chr . zipWith f`. (I've used `f` instead of your lambda to prevent pointfree from pointfreeing it, for sake of readability) I'd advise to either keep the non point free version, or to use the one Pointfree.io is generating
I'm in the GHC Monad and I've compiled a simple piece of code with compileToCoreSimplified. My code is: data MyData = A Int | B Float | C Char mkA x y = A (x + y) The generated core syntax for mkA looks like: \ (x :: Int) (y :: Int) -&gt; A (+ @ Int $fNumInt x y)) I want to do a CoreExpr -&gt; CoreExpr transformation on mkA. How can I determine that A is the first of three possible constructors for type MyData, and find out the names of the other constructors? 
just wanna hop by and say that it's a great shame that lpaste is no more. thanks for everything
Could you be more specific about what you think he misunderstands? Most of the disagreements on this thread are either a misinterpretation of Rich's point or a flat-out attempt to paint him as a buffoon. So I'm curious, as someone who respects him to a degree, or at least used to, what specifically do you think he's wrong about?
This seems to work mostly by accident, unless I'm missing something. You write type family Break (c :: Constraint) (rep :: Type -&gt; Type) :: Constraint where Break _ T1 = ((), ()) Break _ _ = () The (error) constraint never makes it to the RHS! So we end up with the stuck constraint Break (TypeError ...) (Rep Text) but it's not at all obvious to me why GHC would translate this into a call to the given error!
``` pairs :: [a] -&gt; [(a,a)] pairs [] = error "Empty List" pairs [x] = error "Cannot pair a value with itself" pairs [x,y] = [(x,y)] pairs (x:y:xs) = ([(,)] &lt;*&gt; [x] &lt;*&gt; (y:xs) ) ++ (pairs (y:xs)) ```
The recursion is a little tricky, could you give me specific example to show what's happening inside this function? Like scanr' (+) 0 \[1..10\]. 
TypeError is a type family, which means that it will be picked up by the flattener and reported as an error at the end. Incidental? Yes, I agree
How about final year undergraduate students from India?
Impressive, but I don't think I would ever use it. If I needed non-empty containers, I'd probably take a stab at a "Ghosts of Departed Proofs"-style API.
If you can't accept [FineSherbert's comment](https://www.reddit.com/r/haskell/comments/a1ofh2/maybe_not_rich_hickey/earp0is) as a reasonable response (to a talk which, in its title alone is snarky and provocative rather than inviting), such that you would categorize it as either "a misinterpretation" or an effort to "paint him as a buffoon," it's unlikely you're going to be interested in anything I have to say.
 $ cabal test --help Run all/specific tests in the test suite. Usage: cabal test [FLAGS] or: cabal test TESTCOMPONENTS [FLAGS] So you just pass the name of the component after `cabal test`.
was able to solve using this code: basically maps functions eH and Eo over inputs w and z while storing the values in a array called eO calculateErrors :: ([Int], [Int], [Int]) -&gt; ([Int], [Int], [Int]) calculateErrors (_ , _ , [] ) = ([], [1], [1]) calculateErrors (eO : _, w : ws, z : zs) = (eO', ws, zs) where eH = [eO * w * z] eO' = eH ++ first (calculateErrors (eH, ws, zs)) first (x, _, _) = x &amp;#x200B;
I mean, it was a non-aggressive, seemingly earnest response. And I'm not going to debate all of his points here. Suffice it to say that, for example, he _says outright_ that he doesn't understand Rich's argument about maps. And he pokes fun at Rich for saying types don't capturing everything followed by Rich saying "it's okay if you don't capture everything in spec." This is a perversion of Rich's point. Rich was saying fn specs _can_ verify more than types can, and they can do so _optionally_. I think it's pretty fair that I came away with the impression that he might be saying something valuable, but it's hard to find where he's addressing many of Rich's _actual points_. The one comment FineSherbert made that I would like more information about was the statement that [a] -&gt; [a] tells you that the output is a subset of the input. In my mind, [Integer] -&gt; [Integer] could mean I'm adding 10 to each integer, meaning the result isn't a subset of the input.
I’ve never done this before but noticed the post about it on hacker news! Looking forward to using these to learn more Haskell!
A version that don't use monads, comprehension or recursion, but uses 2 functions `pairHead (x : xs) = map ((,) x) xs` `pairHead [] = []` `pairs xs = concat $ map pairHeads $ tails xs`
does it support custom scaffolds, like `stack new`?
We are not planning to drop the support of CLI. As we mentioned in the post, TUI is the additional feature, which probably some users can find more convenient, but we plan to support both and to add the same features to both CLI and TUI (wherever it's possible). &gt; Do i have to maximize terminal screen ? We were trying to make it accessible at all terminal sizes. The small one like 80x30 fits all the sections just nice. But if you would like to use it on even smaller screens, probably the Status and Help sections will not be displayed entirely. Anyway, you could check it on your screen and open the issue, if you think that it's unusable on your screen. We will try to help with that. &gt; Personally I would prefer an uglier UI for a project scaffolding tool. I don't completely understand, but if you mean "prettier", we are open for any suggestions (and if you say you want it uglier, you can suggest that also, but it's better with examples)! The goal is to make it as convenient for users as possible.
A version that don't use monads, comprehension or recursion, but uses 2 functions `pairHead (x : xs) = map ((,) x) xs` `pairHead [] = []` `pairs xs = concat $ map pairHeads $ tails xs`
To be honest I don't really want to get in a debate here either, but I can explain the part about being a subset. You are right that a function like [Integer] -&gt; [Integer] could add ten to every number. But a function f of type [a] -&gt; [a] could not. Counter intuitively, the more generic the function, the more you know about what it does. Two important features of Haskell is that polymorphic functions must do the same thing on all inputs, and that there is no "Object" type from which all other types are a subclass. If I say, "Value x is of type 'a'", there is not any operation you could apply to x. You can't add a number to it because it might be a function. You can't use it as a function because it might be a number. Since there is no "Object" you can't call .to_string or .hashcode on it either. So our function f has to do the exact same thing on every input of type [a], but there is no way to create a thing of type "a" from thin air, because every value is created in a different way. Since it is impossible for f to create new values, all values in the output have to come from the input. Now, this still doesn't tell us everything we would like to know. The output could contain duplicates or just return the empty list, but that is why testing is useful. 
Thanks for your feedback! We discussed the `hpack` issue on GitHub. We have a bunch of reasons why there's no much profit but many difficulties in supporting `hpack`. Long story short, there are already tools that can convert `.cabal` files to `package.yaml` (like [`hpack-convert`](http://hackage.haskell.org/package/hpack-convert)) and it doesn't seem right to take the responsibility of other existing tools.
These visuals are amazing!! Great resource. Thanks 
Sorry, it's my bad. I love summoner and now I use it every day and that's why I got freaked out. And thank you for keeping the CLI! When I said uglier. I meant I prefer the current CLI to *pretty* TUI. I will try the TUI and write a ticket if I run into an issue. Thank you!
Oh man, I’m a sucker for a good book. Almost to a fault. This one looks great. I picked up this one recently, and am looking forward to it over the holidays. https://bartoszmilewski.com/2014/10/28/category-theory-for-programmers-the-preface/amp/
I guess you're asking about stack-templates feature. At the moment we have predefined files for the library, executable, test and benchmark stanzas, but you can choose any of them that you would like to include in the project. If you think that templates feature could be useful you're more than welcome to open the issue on GitHub and we could discuss the details in there. https://github.com/kowainik/summoner/issues/new
I’ve heard good things about Idris, but frankly have been a bit scared off by the self inflicted idea that if I’m struggling with Haskell, than it would only be worse in something like Idris. That you found it helpful in understanding Haskell is very helpful. 
I dove into just the beginnings of Elixir and WOW, really liked the pattern matching in that environment. You make a valid point. For me, the struggle is mental muscle memory of Imperative programming to solve problems. The ideas and foundations of Haskell is indistinguishable from magic at this point in the journey. And I very much want to understand the how behind it. Even if I don’t totally get the piece I’ve been working on, the exercise in trying has made me much better at the things I do understand. 
&gt; How can I determine that A is the first of three possible constructors for type MyData, and find out the names of the other constructors? - `DataCon.dataConTyCon :: DataCon -&gt; TyCon` gives you `TyCon` of your data constructor (`A`). - Then you can pass the `TyCon` to `TyCon.tyConDataCons :: TyCon -&gt; [DataCon]` to get all data constructor of that type constructor. - To get tag of your constructor use `DataCon.dataConTag :: DataCon -&gt; ConTag`. There's also a zero-based one `dataConTagZ` in the same module
Was again able to simplify after looking at the scanl function layerErrors :: Int -&gt; [Int] -&gt; [Int] -&gt; [Int] layerErrors _ _ [] = [] layerErrors eO (w : ws) (z : zs) = eO : eH where eO' = eO * w * z eH = layerErrors eO' ws zs &amp;#x200B;
Cool tip :) One caveat is that it probably won't print correct information as the function is written to be used when all capabilities are paused. The RTS itself uses this when you enable scheduler debug prints (`+RTS -Ds`), but only uses it after all capabilities are paused. Because the function reads run queues of capabilities which will be running Haskell code (e.g. spawning more threads, which change the run queue) and doing context switches (which again change the run queue) while one thread runs this function, there will be races between the thread that reads the queue and the capability that updates the queue, and you'll probably get incorrect information. It should work fine with non-threaded runtime though.
The interesting bit here is the idea of foo :: {bar :: String, baz ?: Int} where I can pass a {bar :: String, baz :: Int} and it will coerce `baz` to a `Maybe Int`.
Fantastic, that you.