Note laziness doesn't really have to tie to a `data` vs. `codata` split. You can evaluate `data` lazily and all you get is more programs terminating faster. Whether you allow the user to use those properties in reasoning about termination is really more the nature of the `data`/`codata` divide.
Now let me use it in patterns and data declarations like syntax. =)
Having personally benefitted from your focus on solving problems in a language already afflicted with pervasive laziness, far be it from me to ask you to point your efforts elsewhere! I agree that Haskell can't be made strict, for the same reason most problems with Haskell can't be fixed—it wouldn't be Haskell anymore if you did it. Haskell has always been based on taking a hard-line position in each area, and many of those positions are IMO not aging so well anymore. But Haskell's always managed to double down on it, and sometimes interesting things can come of that. I also agree with you that it's a useful thing to have different people focussed on the hard-line versions of differing approaches, so we can see just how far you can push each idea. I'd say that the Kmett Canon speaks for itself in this regard… So I guess I'm more reacting to what I perceive as constant dismissal of really good research in the fine type/judgement structure which serves as the matrix for both strict and lazy evaluation. I love that you spend so much time making things nice in Haskell—I honestly would probably dump Haskell in a second if I didn't have the benefit of your oeuvre. I just hope that everyone can keep an open mind, and I suppose that includes me—I think maybe it is not so helpful if I just keep shouting “INDUCTION” and you keep showing “COMPOSITION”… ;-) 
Indeed
Lazy evaluation does not "abstract" evaluation order, it simply proposes a different one.
Maybe "abstracting" is the wrong term. As a programmer I don't know exactly when a thunk will be evaluated if ever, only that it will be as I ask for it's value. That is what I mean by abstraction.
Oh mybad.
I am working on this *right now* Edit: well, me and carter.
It could and it does according to the paper linked in another comment. Not as reliably as with newtypes it would seem.
I understand why RankN makes inference undecidable. But type **checking** should still be possible with RankN, shouldn't it? i.e: runST . forever Should type-check, without any extra annotations. Shouldn't we only need to annotate our parameter types for RankN type-checking to work? Why can't GHC type-check RankN code without extra annotations in such a simple case as above?
EclipseFP also supports this.
Isn't that like a popular song?
Agreed, it's probably my favorite too. The more I work with wanting to "type the world", the more I understand that `RankNTypes` is crucial. This is nicely demonstrated in the work on singletons, but also goes back to McBride's "Klesli Arrows of Outrageous Fortune" paper. Fascinating/mind-bending stuff :)
Oh wow, I never noticed that they worked. Thanks :)
Great example! Thanks! &gt; parseArgs :: B.ByteString -&gt; [(B.ByteString, B.ByteString)] &gt; parseArgs = &gt; -- split to (key, val) pairs &gt; map (fmap BC.tail . BC.span (/= '=')) BTW In case anyone was confused about this: instance Functor ((,) a) where fmap f (x,y) = (x, f y) &gt; -- split to key=val strings &gt; . BC.split '&amp;' &gt; -- drop the prefix &gt; . BC.tail . BC.dropWhile (/= '?') 
The problem with runST is *impredicativity*. The dot operator wasn't declared with polymorphic types in mind, so what the dot ultimately passes to runST is less polymorphic than runST wants. You need to instantiate a type variable with a polymorphic type, and that suddenly makes things much more complicated.
Here we start debate about the style. For me (not surprisingly) my Haskell definition is clearer. I have to think hard about what type is `f` in your version. Also you can play with my version in repl. That is important IMHO.
Is it possible to write a more generalised dot operator that doesn't suffer this problem? Or is it impossible? 
And here we start debate about dynamic vs. static typing. In JavaScript, I won't get a compile time error, but I will get a very early test-failure. Yet static types advocates say I can write less tests, omit many trivial ones. This one I still had to write, and run. It's not trivial. The static types benefits aren't that clear in this case.
I think that in theory there's nothing that would forbid a Rust version like: fn parse_args&lt;'a&gt;(input: &amp;'a String) -&gt; Vec&lt;(&amp;'a str, &amp;'a str)&gt; { input.skip_while(|c| c != '?') .split('&amp;') .break_at('=') .collect::&lt;Vec&lt;_&gt;&gt;() } Sure, it's a bit more verbose then the Haskell version, but it also contains a few really nice properties of Rust. The lifetime parameter 'a indicates, that the returned '&amp;str' are just pointing into the memory of 'String'. There's no need for allocating new memory and the compiler can verifiy, that the '&amp;str' aren't used after 'String' is freed. Also the 'Vec' created in 'parse_args' isn't returned by copy by allocating new memory, because again, the compiler can verify that it isn't referenced anymore and can be safely moved out of 'parse_args'. 
I don't think it's possible to write a *generalized* operator, but it is possible to write a *specialized* one, for this particular case: {-# LANGUAGE RankNTypes #-} import Prelude hiding ((.)) import Control.Monad.ST (ST, runST) type M a = forall s . ST s a (.) :: (M b -&gt; c) -&gt; (M a -&gt; M b) -&gt; (M a -&gt; c) (f . g) x = f (g x) forever :: M a -&gt; M a forever a = a &gt;&gt; forever a b = runST . forever If you look at the type signatures, you'll understand why it's hard to generalize. You have to abstract over which types are polymorphic, which is, well, exactly what impredicative polymorphism is about :)
Nice beginner-friendly examples there, thanks.
Could Agda or Idris type-check `runST . forever` fine? Is impredicativity of ordinary type variables a problem everywhere or just in Haskell?
More than that. This article provides *the* answer to the random colleague's question why statically typed functional languages are so much superior to the rest from the assurance and modularity standpoint. Unfortunately, to understand the answer one has to play around a bit with rank-N types.
&gt; If CBPV can satisfy its promises and strike a true compromise between strict/non-strict then maybe the whole level of discourse can be elevated a bit. I'm still hoping you'll write an article on CBPV so I can finally understand it :)
Despite having built this in Haskell, I do agree that GHC Haskell will ultimately end up as exactly what you described: a badly thought out dependently typed language. The other day, I was reading about the whole impredicative types mess and how a data type with kind * can actually mean two different (incompatible) things. Thanks for referring "The power of Π". I'm started on it last night, and agda is surprisingly less difficult to read than I thought it would be (still not easy though). But, I do still long for the day when a real relational database (with truly composable semantics) becomes available. And whenever that becomes a reality, I think that the languages with expressive enough type systems for realtional algebra (even if they are ugly or require "Frankenstein"ing) will come out way ahead of those who don't. In my field, the database and HTTP APIs are the two non-type-safe FFIs I have to deal with every day, and knocking out one of those would be a huge boon.
I love the mention of Scott encoding there. Its much more common to see references to Church encoding but I always felt that the Scott encoding is easier to understand.
`runST . forever` doesn't check in Agda. However, `runST $ return ()` checks in Agda while it doesn't technically check in Haskell (GHC has a special rule for checking `runST $ x` expressions). It depends on the details of the unification algorithm, I guess.
Oh this is so devastating to hear confirmed, though I suspected as much. Well, I really hope 7.12 w it happens after all. &gt; The TL;DR is the patch feels complex for what it's supposed to accomplish, and this is the main hold-up from what Simon told me. I honestly don't see how a decent overloaded record system could be anything other than fairly complex. 
Here's a [working link](http://www.fremissant.net/deepseq-bounded-seqaid-leaky).
Thanks gelisam. I guess I wasn't supposed to type the "http://" part. (My first post, oops.) I have deleted the link and reposted. The discussion link is actually here: http://www.reddit.com/r/haskell/comments/2pscxh/ann_deepseqbounded_seqaid_leaky/
Indeed. The Scott encoding is the natural one from a practical perspective. 
It's possible to make a type checker work with impredicativity. Indeed, ghc used to have such an extension but it was removed because Simon PJ thought it was too fragile and hard to make work with all other extensions. 
The original link changed to http://www1.eafit.edu.co/asr/pubs/cain-screen.pdf
What are the two things that a type with kind `*` can mean?
I think that it's reasonable to enforce top-level type annotation for *every* function. Sometimes I even write annotations for local functions if the type is not exactly easy to picture. 
But this is why we want to do the work to put real dependent types in haskell! Exactly so this stuff works out much nicer...
Why would GHC end up a badly thought out depndently typed language. The team we have working on thinking out dependent types is thinking it out quite carefully indeed!
Are we actually thinking of bring full dependant type to haskell? I would like this but I thought many people (SPJ in particular) didn't want this. 
&gt; Have you ever seen anyone do that in an OO language? Of course not, because "things that have a name" is not a useful class, and no OO language has the limitation haskell's records does. Does Scala count as an OO language? Because the [KeyedEntity](http://squeryl.org/api/index.html#org.squeryl.KeyedEntity) trait from the [sqleryl](http://squeryl.org/) library basically says "I am an object that has a name". &gt; it doesn't cut down on repetition at all. Well, it avoids you having to define twelve separate "name" fields for twelve separate record types. That does seem to cut down on repetition.
"Combinators for Impure yet Hygienic Code Generation". I'm trying to decide if "impure" means this has nothing to do with Haskell, or it has everything to do with Haskell. The fact that he's apparently using "impure" to mean "proceduralish monads" makes me suspect it's the latter.
I think we missed the merge window in this though :/
are we going once more into the breach thisafternoon or evening?
Missing type inference algorithm.
Richard Eisenberg has been putting quite a bit of work into this project: https://vimeo.com/112963969 The objections to doing dependent types have historically been about the fact that we didn't want to get it wrong. The current round of work is giving hope that we can get it sufficiently right.
I do hope that I am wrong, and I don't yet know enough about dependent types to actually be confident of my speculation. Typically, when languages turn into something that they weren't initially supposed to be, there tend to be problems. Ex: Java's generics (how it works with subclassing) and it's new optionally nullable type, everything in C++, attempts to turn PHP and PERL into OO languages. With haskell, it seems like MPTCs and related type system extensions are consistent with the original goals, but dependent typing is more outside of that original space. Anyway, I'll be quiet now because I can't really defend my claim, but that's sort of where I'm coming from.
&gt; No it doesn't. I was assuming you were being serious, but now it is pretty hard to tell. If you are trolling, congrats you did a good job. If you aren't, please consider how your posts are very hard to distinguish from trolling. Perhaps those precious keystrokes you used to accuse me of trolling could have been better spent in explaining where the difference lies. &gt; Except "inherit namehavingthing" 12 times is not reducing anything compared to "string name" 12 times. DRY is not primarily about economizing characters, but about economizing concepts. Having twelve unrelated "name" fields means you have reinvented the concept "value with a name" twelve times in your code.
Wow, now I'm even more excited about the future of haskell :) Edit: Also see https://ghc.haskell.org/trac/ghc/wiki/DependentHaskell
It wouldn't change anything. Named parameters are only used by the compiler to order the parameters in the correct order, and the function will always get the parameters in the expected order.
Did you see the issues that LiquidHaskell has had due to laziness? Basically, you can get around refinements (e.g. `{v : Int | v &gt;= 0}`) by using laziness [1], so they had to add strictness tracking to the type system [2]. The main property of my ideal language is refinement types, that's why I'm leaning towards strict. [1] http://goto.ucsd.edu/~rjhala/liquid/haskell/blog/blog/2013/11/23/telling_lies.lhs/ [2] Refinement Types For Haskell http://goto.ucsd.edu/~nvazou/refinement_types_for_haskell.pdf
Yes please.
Lens actually goes out of its way to avoid taking a polymorphic lens as an argument. Can anyone comment on why that is and what it has to do with the drawbacks of rank n types?
I've packaged ghcjs with the Nix package manager: https://github.com/ryantrinkle/ghcjs-setup It builds ghcjs with all its dependencies with a single command, and you can use it to manage packages built *with* ghcjs, too.
[Obligatory reference](http://okmij.org/ftp/tagless-final/course/Boehm-Berarducci.html)
Isn't it what categories are for, saying what (.) is like? Would GHC be able to infer instance (isn't it kleisli)?
No, this is the ordinary function composition, specialized to polymorphic types.
In my view, compositionality is as much a consequence of controlling side effects as it is of laziness.
Roughly, `lens` tries to create things as generally/polymorphically as possible so that they can be used as flexibly as possible but it tries to *consume* things as specifically as possible so that consumers can accept the widest set of things. In particular, this is the kind of thing that lets the `lens` subtyping hierarchy work. It's also a policy of maximal honesty (in contrast to a policy of maximal simplicity). `view` doesn't require a `Lens s t a b` it requires only a particular `Const` transformer `(a -&gt; Const a a) -&gt; s -&gt; Const a s` which just happens to be a supertype of `Lens s s a a`. If `view` *did* require a `Lens` then it wouldn't work on a `Traversal` (we can coerce a `Lens` into a `Traversal` but not the other way around) but since `view` only demands `Const` it can use `Const`'s `Applicative` instance—e.g. `Const` is a supertype of both `Lens` and `Traversal`.
Checking fully-annotated expressions is no problem. Checking that kind of application still requires deciding what types to use to instantiate polymorphic things like (.), and things get complicated if you can instantiate a polymorphic function with polymorphic types. A good solution is the MLF type system http://gallium.inria.fr/~remy/mlf/ It does about as good as you could possibly hope: It can typecheck anything legal in System F, as long as you annotate function argumentst that are used polymorphically in the body. However, it doesn't necessarily get along well with other extensions. This paper describes an approach to supporting type classes, but it requires some complicated coercion functions to adjust types as it goes. www.andres-loeh.de/qmlf-str.pdf
Ease of read is my primary concern :) Still that's better than nothing.
Please correct me if I'm wrong, but it looks like first-class IO is much harder without non-strict evaluation. IO is essentially an infinite datastructure which gets projected onto the real world by the runtime. Making language strict by default would require one more wrapper around all IO functions.
&gt; Scala makes lazy val's a thing, but nobody uses them I used 'lazy' in scala. 
What's the difference between $ and |&gt; ?
What support is there for reusing vanilla Reactjs components from a ghcjs component? What about the other way around: using GHCJS components from a vanilla reactjs component? If the support isn't there yet, do you plan on adding it? Does the possibility of a GHCJS component's render function throwing an error complicate things?
Impure == with effects. But perfectly controlled effects. So very Haskell-like!
Fair. There _are_ tons of issues with refinement typing around laziness. Why? There are now three parties involved in every 'blame' decision, not two. You have the thing being passed, the thing it is being passed to, and now the strictness of the resulting program in the result. This leaves 3 parties to consider when assigning blame, not two, so you can't just say "its not my fault, so it must be yours" when you are one of the parties in the blame assignment and have proven yourself correct. This was one of the things that pretty much killed Dana Xu's work on extended static checking / static contract checking for Haskell. I don't think she was expecting the complication.
I use it too, but note, it has very different semantics than say, laziness in Haskell. The lazy val holds the reference itself. This means copying from one lazy val to another lazy val doesn't give you Haskell like semantics. They memoize separately rather than together, with a very heavy synchronization mechanism to boot, while never being able to get out of the way and simultaneously keeping their entire host object alive. A Haskell thunk would be closer in semantics to having an object that had nothing but a lazy val in it and then having a normal val referencing to that object. I can't count how many bugs I've found in Scala code from people thinking you can treat a lazy val just like a Haskell value and not thinking about what the change means. Basically there is a level of indirection missing that I find folks don't account for when using them.
Honest curiosity: Do you have a link to a formal result? I'm actually a pretty big fan of uniqueness typing. It opens up lots of possibilities. It is just usually conflated with getting rid of monads.
Good questions. For using existing React.js components, you can take the result of `createClass` and wrap it in the ComponentFactory newtype and use it in Haskell. So, I think it's pretty easy to do already. Yes, as far as potentially throwing an error during render goes, it's possibly a big caveat depending on the architecture of the application. It's hard to ensure that no blocking operations occur in the render function at the type level, although not necessarily hard at the application level. For example, you could require all of the data used in render to be part of a data type that's an instance of NFData and ensure that it's forced before you call render: data ViewModel = ViewModel { thing :: Int } instance NFData ViewModel where ... safeRender :: NFData a =&gt; a -&gt; (a -&gt; ReactElement) -&gt; ComponentT IO ReactElement I'm not addressing the issue yet because I haven't really solidified things, but I think there are viable strategies to avoid ever having to throw. edit: That idea above wouldn't quite work. You'd have to actually force the value outside of the render function, but hopefully it conveys the idea.
In fact, precisely because of this observation, you could use Category to something that /u/PokerPirate might consider acceptable: instance Category Matrix where id :: Matrix a a id = mkIdMatrix (Proxy :: Proxy a) (.) = matrixMultiply
`main = getLine &gt;&gt;= putStrLn` doesn't work. &gt; Looks like Haskell is taking a nap. This problem has been reported and our automated robots will fix this issue for you. 
cabal install bindings-portaudio -fBundle -fWASAPI Unfortunately, this doesn't work on windows 7 with 64 bit ghc.
Thanks for the answers. If I understand your first answer correctly, you mean: 1. Obtain a reference to a vanilla React component (something which has been created with the JavaScript `React.createClass`) using whatever means GHCJS uses for that. 2. Wrap that reference in the `React.Types.ComponentFactory` newtype. 3. Pass the wrapped reference to `React.Internal.createElement` passing along any properties, which returns a `React.Types.ReactElement`. The returned `React.Types.ReactElement` can then be used in the construction of composite `React.Types.ReactElement`s before finally being rendered to some DOM element. That seems pretty straight forward. I think this is also the direction of integration most likely to be desired. I can see applications being written with GHCJS making use of existing JavaScript libraries much more than I can see JavaScript libraries or applications making use of GHCJS libraries. A blog post about wrapping vanilla React classes might be worth writing. At least I'd enjoy reading it :-) Preventing throwing an error during render, seems like just another occasion where Haskell makes the operational characteristics of the program less-than-obvious. Or at least require the programmer to have a good understanding of the evaluation model. I guess any one skilled with reasoning about that will be OK, and the rest of us will eventually learn what we need to.
No, but is really simple really. There is a proof that a pure functional language with strict evaluation (and no uniqueness types), is at worst logarithmically slower than the imperative version. This is because you can fake mutable references by manually index into some sequence, but the best pure strict sequence algorithms have logarithmic access time. But with uniqueness typing you can implement the sequence using a unique array with constant time access.
Yes, that's exactly how you'd do it w.r.t. the first answer, sorry if I was a little terse :) As for preventing errors during render, I hope to provide some opinionated layers on top of the raw bindings once the basics are nailed down that don't require you to reason about it at all. I just haven't gotten around to it yet. 
Just out of curiosity, have you taken a look at https://github.com/edgurgel/poxa ?
&gt; Exercise: Write the uncons operator for ListC. Here's my solution: -- Church-encoding of Maybe (a, b) newtype MaybePair a b = MaybePair (forall r . (a -&gt; b -&gt; r) -&gt; r -&gt; r) pair :: a -&gt; b -&gt; MaybePair a b pair x y = MaybePair (\ casePair _ -&gt; casePair x y) none :: MaybePair a b none = MaybePair (\ _ caseNone -&gt; caseNone) unconsC :: (a -&gt; ListC a -&gt; r) -&gt; r -&gt; ListC a -&gt; r unconsC caseCons caseNil (ListC foldr) = choose caseCons caseNil where MaybePair choose = foldr combine none combine x (MaybePair acc) = x `pair` acc consC nilC You can do it using plain-old `Maybe (a, b)` instead of its Church-encoding but I figured I may as well stay in the Church realm to make it fun! Does anyone know if there's a more elegant way to write this function?
Everything without binary dependencies, anyway.
Customize the shm-current-face face. :-)
I'm sure that would be the reasonable course of action. I just wonder when we'll be able to develop 64 bit applications. 64 bit desktops first started shipping in 2003.
&gt;Type-level integers Could you explain why please? This is the second time today I've heard this (it was in an F# post too) and I feel I'm missing out on something!
Note even here? http://www.fremissant.net/proj/seqaid/seqaid.html http://www.fremissant.net/proj/seqaid/extra.html There are lots of further links from the OP.
Makes sense to me. Thanks.
I've spent approximately half my answering time on Stack Overflow explaining and expanding type synonyms. I wonder if using lower-case identifiers would clarify that for new users. They'd look like type variables instead, so maybe it'd interfere with developing an intuition for free theorems. We want people to recognize on sight that `f :: a -&gt; a` is `id`, and `g :: a -&gt; b` is `unsafeCoerce`. If `type a = String` and `type b = Int` that's out the window.
Well, I live in Padua, and I would go to a haskell Meetup even further than Trento. Unfortunately, I do not know any haskeller in my city or in the surroundings area, which is quite a shame.
Where do you come from, if I may ask?
Are you stating that `undefined` (bottom) is required for lazy evaluation? I am not convinced that always taking bottom into account when reasoning would be beneficial. Having a data/codata split sounds nice, but without that it is probably fine to treat bottom as exceptional in most cases.
Well, explicit foralls would make it obvious again. 
Verona
Lovely collection :) Could it be the case, that you wanted to link to the [root directory](https://github.com/papers-we-love/papers-we-love) of the repository instead? The `README.md` in the root directory seems to give a more gentle introduction to the project than the `distributed_systems` subdirectory.
are you talking about the applicative-monad-proposal?
&gt; The Haskell implementation was generally pleasant to write; the Vector.modify function proved to be extremely convenient for building the node vector. It takes a vector-mutating function and returns either a copy of the vector with that function applied or the same vector mutated by that function, depending on whether or not it is safe to do the latter. I have a doubt: how does the compiler know when it is "safe" to mutate in-place? 
Not a lens library fan, I take it? :)
I haven't done much purescript but it has types like this showPerson :: { firstname :: String, lastname :: String } -&gt; String The first parameter for showPerson will match any type that have the fields `first` as a `String` and `last` as a `String`, which includes data Person = Person { first :: String, last :: String, age :: Number } data BirthCert = BirthCert { first :: String, last :: String, dob :: Date }
For the `or` operator I would recommend looking at `Data.Monoid`, specifically `First`.
&gt; It's complex and it's going to get worse. It also brings its own problems (e.g., join can no longer be made a method in the monad class. WAT?!?!) Woah, is that true? That sounds pretty terrible! Sounds like the type-check solution is much much simpler and better, and removing the identity functions as an optimization without forcing the user to explicitly use `coerce` sounds much better too.
&gt; The details of this encoding are rather gruesome, but I think it could be a lot nicer using the new type level strings, type level numbers, and type families. Time for a small redesign? I'd add that the exposed internals of all data types is a huge problem to address with `Generic` too.
It can be made a method, but then Monad cannot be GND'erived, which is even more sad. As for me, though I'm not a huge fan of the roles, I can easily live without join in the class.
I also feel that the cost of roles wasn't worth having our particular flavor of GND.
Can you explain the problem in more detail?
At least it's what [the documentation](http://hackage.haskell.org/package/vector-0.10.12.2/docs/Data-Vector-Generic.html#v:modify) says: "The operation will be performed in place if it is safe to do so and will modify a copy of the vector otherwise."
Sorry, I should have made it clear that it's the deriving that breaks, not the monad class itself.
But roles also get us coercions, don't they.
And what's so great about coercions?
That they're zero cost.
I believe the difference is direction. You feed from the left to the right, whereas $ stops the left until the right is done and then feeds it in.
But why should I have to write them myself? I prefer the compiler recognizing and removing identity functions. The compiler can use coercions internally if it wants, but I'd prefer not to see them. 
very nice!
I would prefer that, too. I suppose that such an optimization is hard to write in a reliable way.
I've not done it, but for the stylized kind of identity functions that the GND will generate I think it's doable. You do a fix point iteration of each strongly connected group of definitions starting by assuming all functions are identity functions and then you iterate. That would be my general plan. 
no. VDM and Z are basically useless IMHO. real running code is what matters. I'd rather people just wrestle with Haskell as the spec language, at least it must make considerations for real systems VDM is the perfect distillation of the uselessness of ivory tower approaches to building sw
Jesus, what a mess that Haskell sample is...once you are using unsafe* and inline pragmas, codify all the hacks into something like "ghclang" and drop the mixing of syntax what is Perl? the syntax accepted by the Perl runtime. Haskell and ghc now have the same relationship everyone here poops on go, but it handily beats Haskell with short, clean idiomatic code and no { ## hacks ## }
I'm just here to say thank you. I can't tell you how many times this has bitten me, and I wasn't aware of this extension.
This may make terms more defined than they are: let f = f in f (I don't know whether it's considered ok for ghc to do so.) Also, it could be costly in terms of compile time. But it'd be interesting to hear whether ghc devs have considered something like this.
There are details to work out, of course. :) My guess is that it's cheaper than strictness analysis. Simon PJ has considered it, but thinks it seems to complicated. Maybe it is. 
minor nitpick: the disregard for PVP-adhering version constraints is quite disturbing. Uploading stuff to Hackage comes (or at least should come) with an implicit social contract to reduce the burden on the user by providing sensible version bounds. If you're not serious about maintaining a package properly, there's no need to upload to Hackage, just publish HTTP urls, which `cabal` can install directly from.
there's a reason the seminal text is titled The -ART- of Computer Programming...the vision you are describing is an academic pipe dream barring some fundamental breakthrough that makes performance considerations irrelevant
&gt; I think it could be a lot nicer using the new type level strings, type level numbers, and type families. Time for a small redesign? Someone is already working on this... check the `wip/GenericsMetaData2 branch`: https://github.com/ghc/ghc/blob/wip/GenericsMetaData2/libraries/base/GHC/Generics.hs
How are performance considerations a problem here? seL4 is faster than all the competition. Its IPC benchmarks are better than any other microkernel.
Thats... a good idea. I'll do that when I get the chance :) Luckily, the html file itseld has all of the information anyways and the slides just make it look pretty. EDIT: Done! The link is in the OP
Mostly because I have done some nice things with them in C++, and trying to use the same approach in Haskell is significantly less elegant. Vector math is a nice example: with type-level integers, I can implement vector addition in a generic fashion (write function once, have it work for any given vector space), while still maintaining type safety (compiler won't allow adding vectors of different spaces, because the signature says `operator+(const vector&lt;n, T&gt;&amp;, const vector&lt;n, T&gt;&amp;)`, which means the `n`s are required to match). I haven't found a satisfying way of doing this in Haskell yet.
&gt; the exposed internals of all data types is a huge problem to address with `Generic` Isn't that the point of `Generic`? Any datatype-generic function will operate on the internals of your type, not on whatever API you expose. If you want to use `Generic` internally to generate a closed set of instances without exposing internals to consumers, couldn't you use a `newtype` (and GND, I guess) to hide the `Generic` instance? 
Indeed. I think a big problem is that people think that the types and proofs should be strict for every function in the program. But they can be used just like test-based design. You don't need to be super precise about all the helper functions. You can't specify them up-front either. There will be enough refactors and and bugfixes with formal specification, too. You'll just have more certainty about the end-product.
And Haskell is now outperforming C++ in some cases, so yeah, you'll never if you give up.
Very interesting. Haskell would not be my first choice for writing a rhythm game. You want smooth scrolling and perfect audio synchronisation, which means predictable frame timings, which means avoiding laziness and garbage collection, two of the most compelling features of the language. It’s not that Haskell is slow—that’s a blatantly false meme that needs to die—but laziness means that your fast-looking code can have slowness leak into it from outside. What laziness gives you in composability, it also costs you in locality of reasoning about performance. 
*Edit for posterity*: /u/snoyberg addressed my points in a [follow-up blog post](http://www.yesodweb.com/blog/2014/12/hackage-docs-live-again) and in the comments below. I think my concern/accusation in the first paragraph below was incorrect, and I retract it. With my "potential library consumer" hat on, I find this blog post, and the practice it advocates, a bit alienating. It boils down to an attack on Hackage and an attempt to draw users to a competing site. I don't know whether that is actually anybody's intention. If it is, why not make that argument explicit? http://www.stackage.org/package/yesod is missing a lot of the information that is available at http://hackage.haskell.org/package/yesod. And the [Stackage documentation for `yesod`](http://www.stackage.org/haddock/nightly-2014-12-20/yesod-1.4.1.2/index.html), when you get down to it, is identical to what would be on Hackage. With my "/r/haskell subscriber" hat on, I'm a little surprised at the vitriol in the linked bug reports. Everybody knows that buildings docs is surprisingly complicated because of the way Haddock works. I had been under the impression that people were pretty happy with the doc-upload system. I had thought that the Hackage web interface, despite its warts, was universally considered a community asset and common ground.
If you're trolling, go away. If you're not trolling, make your point without being a jerk.
I too would prefer improvements to hackage and its related tools over splitting documentation over two separate web sites. 
I remember thinking VDM was ridiculous when I was exposed to it as an undergraduate back around 1993, and I literally haven't heard of it again until today. (Same thing for Z.) This is the funniest case of "so you're saying there's a *chance*?" wishful thinking I've encountered in a while.
Nice! I've also done the same just last week :) my slides - https://cdn.rawgit.com/svalaskevicius/haskell-from-scratch/master/index.html
And Hackage is missing a lot of information that is on the Stackage page, like reverse dependencies. The sites don't have the same goal. My message here is simply for end users looking for documentation: you'll find it more reliably on Stackage. I'm not sure what vitriol you're referring to. I'm simply not willing to put a lot of effort into changing my package release process and duplicating my readme documentation. I maintain something like 85 packages, and have tooling to help me cope with it. Changing that tooling at this point would be very onerous. I don't know what the major difficulties are with generating documentation on Hackage. Considering the fact that I put together the Haddock hosting on Stackage in a short amount of time after documentation generation on Hackage inexplicably stopped a few weeks back, I would have guessed I'd be in a good position to judge that. Care to elaborate on the problems?
Nothing I've changed here would affect Hoogle or Hayoo. I'm not certain if manually uploaded documentation is feed into those services, perhaps someone else can explain.
You could, but it might get quite tedious.
Another example of practical use of software developed through formal methods is the CompCert C compiler that is going to be used by Airbus for its flight control softwares relatively soon.
Yeah. It would be great if customers knew what they wanted from day one and no one ever made a mistake in writing them down. Formal methods work well for control systems because you know the parameter space almost entirely from day one. Landing on the moon, vehicle control, when your system is bounded by knowable physics. Now try and write a formal spec for a feature a customer wants but can barely describe and it changes as the project goes on.
Ow you're just coming here to be a dick. That's okay. Want me to play along and start a flame war or are you good for now?
Well it does reserve the word `forall` in types, if there's some code in existence somewhere that uses it as a variable.
You are conflating strictness with totality. Even in strict languages like ML, *bottom still inhabits every type*. The only difference is that you have the property that for all f, f(⊥) = ⊥ (although strict languages generally allow you to work with eta-expanded bottoms, which means you lose eta-reduction when you consider strict bottoms). This leads to some undesirable results, such that (5,⊥) = ⊥, which Haskell avoids. Non-strictness also happens to have some similar unsatisfying equations such as (⊥,⊥) ≠ ⊥. Haskell allows you to not simply blow up when encountering a bottom, but they're still there in ML, waiting, biding their time. ML just forces you to blow up whenever you see one, so you have even less reason to think about them than you do when you're working with Haskell. For a detailed look at the problem (and the fact that it's mostly inconsequential anyway), I recommend "Fast and Loose Reasoning is Morally Correct". The paper basically shows you can do all the reasoning you want even though there are bottoms. You are not giving up on induction or easy teaching if you use ML or Haskell. PS: Not having bottom inhabiting every type is fundamentally in conflict with turing completeness, you lose turing completeness if you get rid of `undefined` inhabiting every type. Also, if you remove bottoms by making a sub-turing language, strict and non-strict semantics become indistinguishable from a denotational point of view, so a futuristic compiler could pick lazy evaluation automatically only when it makes sense for a sub-turing language with no bottoms. PPS: As an example of why bottoms still inhabit strict languages consider this: type 't List = Cons ('t * 't List) | Empty let rec getBottomForList x = Cons((), getBottomForList x) (* To get a bottom for 't List, simply invoke getBottomForList() *) (* Note that if you actually try to do this, you will hang. *) (* It does exist though, same as in Haskell *) You can make similar bottoms for Nat and other "inductive" types.
You are right that it doesn't really matter! See "Fast and Loose Reasoning is Morally Correct". So long as you never encounter an actual bottom in your code, all reasoning about code like it is a total language works as you'd expect.
This appears to be the maintainer of a commercially sponsored competitor to Hackage, saying that he's not prepared to put in fairly minimal effort to fit in with community infrastructure. I have to say that if I came across any Hackage package whose documentation field is a snide redirect to Stackage of form you suggest, it would be a significant deterrent to my using the package; I don't recommend that others follow u/snoyberg's example on this.
I'll try to look at this and optimize/rewrite when I get to a computer.
Oh wow, I had no idea this was available!
Not from the language, but it is silly that we have things like `head` that throws an exception into IO rather than a pure exception that can be handled using something like `Maybe` or `Either`.
&gt; I don't know what the major difficulties are with generating documentation on Hackage. Neither do I. I was talking about Haddock, and in particular its need for a something resembling a complete build environment--which presents an infrastructure challenge that might be surprising to a non-expert. I don't know why you found it trivial to make a doc builder while the Hackage maintainers have had trouble. (*Edit*: I apologize; the previous sentence sounds sarcastic in a way I didn't intend. I imagine there's a difference between the two systems, but I don't know what it is, what the challenges are, or what's been going on.)
He said *all* of its competition.
I agree with you in general, but not for this application. On modern hardware, you've simply got so much time per frame to do work, I have a hard time believing that the GC is going to get in the way. Of course, that's just anecdotal - I could be wrong!
/u/tailcalled posted the story here. /u/tekmo actually wrote the blog post.
I can't answer the first question. I would select at least 4, probably 5 of the listed domains I use Haskell for. Impossible to select one.
I agree that version bounds are good things to have for most of your dependencies. However, In this case, I don't see that big of a problem. All of the dependencies used are fairly stable, I bet any versions from the last few years will work. Lower bounds would be ideal, but realistically it'll be quite a challenge to encounter issues.
Wow. Looks like you covered a lot more than I did! How long did it end up going? And how did it go in general?
On the contrary, stronger type systems improve performance because you can compute more things at compile-time instead of at run-time. The simple example is avoiding run-time bounds checking by performing the check at compile-time. However, this principle is not limited to just verifying program correctness: in a total programming language you can actually do [arbitrary computation at compile time](http://www.haskellforall.com/2014/09/morte-intermediate-language-for-super.html). In fact, you can view advanced type-checking as a special case of this more general principle: type-checking is conceptually just running a significant amount of your program at compile time.
http://i.imgur.com/bYrqi9Z.png So it's allocating the entire program? It seems like we should be able to throw something away right?
I don't think the README issue is minimal for someone with ~100 packages. Maybe there is a way to solve this with pandoc though? Why it is considered snide to state that the hackage haddock building is unreliable? This seems like a simple statement of fact. Is this is a sensitive issue for people?
I didn't quite get why this extension introduces the forall keyword. What is it good for? I thought this extension was just to enable the use of the same type variables in subexpressions.
The `forall` keyword is what makes type variables accessible to inner definitions. For example, this is an error with `ScopedTypeVariables` on: f :: (Num a, Eq a) =&gt; a -&gt; Int f x = g x where g :: a -&gt; Int g 0 = 5 g x = f (x - 1) But this is not: f :: forall a. (Num a, Eq a) =&gt; a -&gt; Int f x = g x where g :: a -&gt; Int g 0 = 5 g x = f (x - 1) 
Is Church encoding really a common pattern? I don't think I've ever seen it in the wild.
Why is it necessary though? I thought the leftmost forall was implicit anyway, according to the RankNTypes post two days ago.
shouldnt the haskell standard be updated to have that minor change then?
I think so. As a beginner (which I guess I still am, but even moreso then) I had a hell of a time figuring out that this wasn't how Haskell worked by default. ("It works only *without* the type signature? HOW IS THAT EVEN POSSIBLE???") The existing default is counter-intuitive and the resulting error messages are totally inscrutable for the uninitiated.
I agree with everything you have said, but I still support my comment. And there is no contradiction here. While I think Overloaded Strings are so natural that they could be standardized, I consider partial functions an enemy in most cases. This case is no exception. I firmly discourage partial IsString instances. When a text might or might not parse, I'd rather use a parser function able to return parser errors. The same applies for the `fromInteger` method of the Num class.
If you don't use an explicit notation for introducing type variables then the meaning of any particular type expression becomes ambiguous: a -&gt; a What would such a thing mean? 1. Does it mean `forall a . a -&gt; a` (shadowing any external free variable `a` if it exists)? 2. Or does it mean `a -&gt; a` where `a` is an external variable? (And raising a type error if it doesn't exist?) Without the ScopedTypeVariables extension, it always means the former and hence you gain conciseness at the expense of expressivity: there are some type signatures that are simply impossible to write without an explicit `forall`, even if you allowed implicit `forall`s to introduce scoped variables. When ScopedTypeVariables is enabled, ~~it always means the latter~~ it will choose between the two based on whether the variable is in scope and is explicitly introduced by a `forall` (thanks cdsmith!). Personally, I think it's a bad design to infer the meaning from context (choose between 1 and 2 depending on circumstances) because then suddenly an expression has two completely different interpretations depending on whether it's nested inside something else or not. One can additionally enable ExplicitForAll to get consistently the second behavior.
Thanks hagda, I've added upper bounds for all deps in all three projects. These are typically the latest versions of the dependencies. Minimum bounds are of course more difficult. In the case of deepseq-bounded, which is the only one of the projects really intended for use as a library, I think it's possible that any versions consistent with base ==4.* will work, but I've not had time to test that. I guess minimum bounds are important on executables too, so I'll add a [ticket](http://fremissant.net/seqaid/trac) and keep refining...
&gt; In fact, once we step into the wondrous world of the infinite we can’t rely on structural induction anymore. Remember that induction relies on the “well foundedness” of the thing we’re inducting upon, meaning that there can only be finitely many things smaller than any object. However, there are infinitely many things smaller than an infinite data structures! I am very confused by this. I have seen the term "well founded" before only in a set theory context, and this is a different use. In set theory a relation is well founded if there isn't a infinite descending chain, (and it's possible to have infinitely many things smaller without an infinite descending chain). Does induction in constructive logic really break down without this stricter version of well founded? That seems really surprising.
Yeah, this is still in very early stages so I took a somewhat cavalier attitude about honing constraints. But, having it brought to my attention reminded me that it's a relatively small about of work, and it is important. This is only my second library, so I'm still getting the hang of what's ecologically important.
Yes it does. It ends up corresponding to nontermination unfortunately. We can see this in Haskell because applying `foldr` to an infinite list might not terminate. Inductive proofs are based on the fact that we can start at our base cases and repeatedly apply the inductive steps to build up to a specific case. With infinite structures we may need an *infinite* number of applications, which means we loop forever. Well foundedness just means that we'll never need infinitely many applications of the inductive step. edit: That specific quote is poorly phrased (sorry about that). An infinite structure has an infinite descending chain of things structurally smaller than it. This means that structural size isn't well founded and structural induction falls apart. edit 2: This isn't specific to constructive logic by the way, it's just an artifact of how induction works. The same is true in classical set theory or pretty much anything else.
To me the comment sounds snide because it doesn't acknowledge the general reliability of Hackage or the good will and effort that maintainers put into it. There's a feeling of negativity and adversarial marketing in the tone. I'd be really happy to chat offline if you're interested - there could be a sincere misunderstanding or difference in style if someone feels a statement is snide when you or OP think of it as a simple matter of fact. Getting to the bottom of that may help Stackage advocacy move forward without polarizing people.
Your formatting is horribly broken, but you probably need a `do` before your `s &lt;- readLn`
I'm genuinely curious about the role formal specficiations and theorem provers can play in commercial software development(outside of realtime/control/embedded contexts where they're already uqiquitous). I've read the AWS paper[1] on the topic, but are there any other case studies you know of? Something with an emphesis on how the people in charge of the specification reacted to changing requirements would be very interesting. http://research.microsoft.com/en-us/um/people/lamport/tla/formal-methods-amazon.pdf
Greg hit the nail on the head here. It's amazing how many people are willing to tell me what extra work I should be going through to meet their standards, however arbitrary they may be. Hackage is an open source project. Stackage is an open source project. I find it very strange that you seem to imply that one is community and one isn't. Even though Stackage is younger, there's already a community of contributors around it. If someone really wants to put their money where their mouth is, then volunteer to be responsible for uploading documentation to Hackage for my packages, or to keep my description fields in sync with my readmes. Other than that, you're just demanding work of me that you wouldn't do yourself, and I find that hypocritical. And for the record, I explained on the cafe just a few weeks back that some documentation cannot be written in description field due to limitations in cabal, and requested that readme file support be added. That hasn't happened.
Very cool. Out of curiosity, why did you opt for openssl instead of the tls package?
Got it. May I recommend that you link to the one package from the other's description?
Let me ask you: what part of the statement comes across as snide? Is it calling the doc generator unreliable? Is it having a link to the Stackage page on Hackage? I can tell you that I'm not trying to advocate for others to do the same (though I don't resist that either). The fact that this is posted on Reddit may make it seem like that's my goal, but note that I didn't post it here, I was simply trying to head off a lot of questions that I'd have to answer. I'd be happy to modulate the message somehow if that would help people understand the content without jumping to conclusions about motive. And I can assure you that assumptions about my motives in this thread have been completely wrong.
Good idea, done
And I want to address the topic of "Hackage reliability" separately. Even if it offends people, I'm going to just give the honest truth: Hackage is *not* a reliable service. We've all experienced it going down constantly. In fact, the hackage.fpcomplete.com mirror that I mentioned in the most recent Stackage Server post came about because FP Complete was contractually obligated to provide certain Service Level Agreements (SLAs) to our customers, and we were unable to do so due to the low uptime of Hackage. I had a conversation with one of the maintainers just two weeks ago, and said quite clearly that even if Hackage developed all features that Stackage needs, I'd still be very concerned about using it for commercial customers due to its problematic reliability. FP Complete is not making any money off of having Stackage Server running. We're running it because we need a reliable host offering features that Hackage does not provide. Believe me when I say that I would much rather not have engineers (myself included) working on this, and instead be able to rely on existing infrastructure. But that wasn't an option here.
I don't know why you're being downvoted, I didn't read anything sarcastic in your response here. I took the comment at face value. Yes, Haddock is difficulty to configure to build correctly, but once it's configured, that's usually it. That's where I don't understand the problems with generating documentation on Hackage: it seems like a solved problem already.
&gt; FP Complete was contractually obligated to provide certain Service Level Agreements (SLAs) to our customers, and we were unable to do so due to the low uptime of Hackage. ... &gt; Believe me when I say that I would much rather not have engineers (myself included) working on this, and instead be able to rely on existing infrastructure. But that wasn't an option here. This is a great pair of points. Did FP Complete look into whether it would be cost-effective to work with Well-Typed or the IHG to get Hackage's reliability to the level you require?
Hoogle indexes all packages, but you need to write +yesod to include a specific non platform package currently. If your package has no docs on hackage, it will not be indexed by either service. 
I think the answer is backward compatibility. Without requiring the explicit forall, compiling with `-XScopedTypeVariables` could break valid Haskell2010 code. This is a kind of breakage that is likely to happen pretty often, meaning the extension might be a pain to start using because it would cause too much older code to break. By requiring the explicit forall in order to change the meaning of type variables in nested scope, you guarantee that the extension doesn't change any meaning unless there's an explicit forall used. Frankly, though, I think it's still the wrong choice.
I can’t help but notice 'debugging' doesn’t appear anywhere. I hope I’m not alone in filling the 'Other' field here, because figuring out why my programs go bad has not been very fun (and I don’t mean for the obvious reasons, but compared to how it happens with other languages/toolchains) and I’ve never found any resources to improve on the workflow.
I already have tooling that does orthogonal things in my release process, namely to deal with mega-repos. Simply using `neil` may not be too bad, but I wouldn't be able to do that. I'd instead have to reimplement that functionality in the mega-sdist tooling. That's actually [exactly the conversation I had with Greg](https://github.com/yesodweb/yesod/issues/887#issuecomment-66885210) that kicked off this blog post: I said I didn't want to go through that process, when there was an already available source of reliable documentation.
What about [http-streams](http://hackage.haskell.org/package/http-streams)?
I don't agree with your reasoning. *Especially* if a package is stable you should set the usual major-version upper bounds, since if it's gonna major bump, it's gonna be a serious API change.
&gt; the README issue The README issue is a bit different than the doc-builder problem. I think /u/snoyberg's point about `README.md` and friends makes a lot of sense; see the [Hackage issue he opened](https://github.com/haskell/hackage-server/issues/262). I'm a little bit tempted to look into what could be implemented. On the other hand, if the rationale is "I have to have this `README.md` file for Github anyway", why not include a link to the `README.md` on Github itself in the description? I'm looking at [`lens`](http://hackage.haskell.org/package/lens), which (as part of its unusual Hackage front page) includes just such a link. It'd be nice if it could be hosted on Hackage, but... I think as a consumer I'd really prefer a brief identification in `description` of what the package does or provides, along with links to the Readme, a `pipes`-style `.Tutorial` module, and whatever other documentation is out there. &gt; Maybe there is a way to solve this with pandoc though? That was discussed a bit on the mailing list. Among other things, it seems to be inconvenient because the "description" displayed on Hackage is in the `.cabal` file, which has its own formatting requirements in addition to Haddock's.
After playing around with Haskell for a year, I still don't know enough to answer these questions.
Just to be clear, the inline pragmas there were setting profiling points; they have no effect on the behaviour of the code.
cocaine's a hell of a drug
As for a tool to create such constraints: If you run `cabal init` on an existing source-tree, you get a `.cabal` file with filled in `build-depends` **including** proper version bounds!
hagda, I know that, but that gives you frozen deps. A good library should offer a range of dependencies when it's safe to do so. That's why you see ==2.3.* etc. in most .cabal files.
Alternately you can just upload the docs to hackage yourself when you submit the package. This can be done from the command line pretty easily with a tool like: https://github.com/ekmett/lens/blob/master/scripts/hackage-docs.sh e.g. $ hackage-docs.sh EdwardKmett from a directory with a cabal file uploads new docs. This works even if your docs need weird local libraries to be installed, [need to be built with newer versions of ghc](http://hackage.haskell.org/package/gl) to get hackage support, etc.
Yeah, I tend to be the one who posts /u/tekmo's blog posts here, because he doesn't post them himself and I've got his blog in my RSS reader.
I am a Knower of 4 corner simultaneous 24 hour Days that occur within a single 4 corner rotation of Earth. 
Not really. It's just that the debugging techniques you need are somewhat different for that reason, and you have to get used to it. That said, the debugger included in GHCi is pretty similar in style to the command-line-style debuggers for many other languages, and it works pretty well. It is perhaps missing a few features that would sometimes come in handy, but I don't think those would be particularly hard to add in principal.
That's a good start. It does merit a bit more attention than that, though. Sometimes the default constraints are too tight, e.g., if you are only using a tiny bit of the API that is much more stable than the library as a whole. And sometimes they are too loose, e.g., if you are using internals that are likely to break with any subsequent version of the library.
Huh? Oh, I see, in the module comment. Might be nice to have the cross links in the package comment, too, so that it appears right up front on Hackage.
This is an interesting approach. You are using the exact same module name in both packages, and then (presumably) promising that the external APIs of the two will always remain identical. Another common approach is to provide a single package with the front-end API, and a separate companion package for each backend. That solves the problem of code duplication and the required manual management to maintain compatibility. But for that you would need to give up the convenient re-export of the backend API. A third approach is to use the same physical front-end module within a single package, and provide a cabal flag that chooses between the two backends. I think the third approach is the nicest in principle, because it fully expresses the dependency picture in machine readable code that could potentially be exploited by people's build systems. Unfortunately, there are currently limitations in support for flags by cabal and hackage which would cause some serious awkwardness. For one thing, casual users of your package would sometime experience cabal build problems which they would need to know how to work around. Secondly, you would be forced into CPP land, which is kind of ugly and a bit fragile. Thirdly, the dependency information displayed on hackage for packages using your library with a specific backend would be incorrect.
i don't know should it
Yep. That's the only way in which h2010 isn't fully supported in the new ghc.
This library provides indices that are usable with `vector`, but there is no `newtype Array dim a = Array (Vector a)` if that's what you're looking for. I'm using `Data.Array` in this post simply because the only thing needed to do so is to provide an `Ix` instance. In any case using `UArray` or `StorableArray` shouldn't really hurt performance, though they'll hurt your RAM slightly :). This library is intended to be the the backbone of a library not yet on Hackage (but on GitHub at https://github.com/mikeplus64/static) since the API doesn't feel complete enough to me. This one does define a new array type, `newtype Array dim a = Array (ForeignPtr a)`, which is slightly advantageous to being a wrapper over `Data.Vector.Storable.Vector`, since this way the size of the array is not carried around at runtime. This also means that we can often know statically that e.g. indexing or writing operations are on valid indices of the array, translating to less or ideally no runtime checks - but still retaining safety (unlike `unsafeRead` or `unsafeWrite`) In terms of performance, for "small" n, like lots of 4x4 matrices and 4-vectors, `static` (with a lot of help from `indices`) easily outperformed `hmatrix` and then `linear` after I had gotten the loops to unroll. What made `linear` perform surprisingly well was that its operations are very easily optimised by GHC to unboxing, work, then boxing, without any recursion, since its matrix/vector type is nice and simple: `data V4 a = V4 !a !a !a !a; type M44 a = V4 (V4 a)`. Unfortunately though that's not the nicest to use with OpenGL, or for memory use, since the elements are all boxed. At the moment though, `static` is very raw, and until recently I haven't had time to work on it again. Since it's just a `ForeignPtr` it's fairly easy to create e.g. a storable `Vector` for temporary use, taking advantage of `vector`'s impressive optimisations.
I think you ment to use italic where you are using upper-case. You are screaming every fifht word in my head.
Quite a musical kind of farm - and a bit different than [this one](http://www.schickele.com/shoppe/pdqrec/ontheair.htm).
Nice. I may have to investigate switching to this
When posting code on reddit, please indent your code four spaces in order to preserve the formatting.
I can't tell whether this is serious, satire or art. 
I switched. Works great.
https://github.com/dmjio/http-client-streams/pull/1
I think you might have misunderstood the question. Consider the inductive definition data Tree a = Node (Integer -&gt; Tree a) | Leaf a There are potentially an infinite number of `Tree`'s that are structurally smaller than any given `Tree` but structural recursion is still guaranteed to terminate because all descending chains eventually terminate with a `Leaf`. In this case the "structurally smaller" relation is well founded according to the definition redxaxder gave but not according to the definition he quoted.
Even the very first version of [linear](https://hackage.haskell.org/package/linear) on hackage has instances of Storable.
There is no such thing as infinite. There are only very large numbers / data-structures / step counts. For any problem of practical interest there is an inductive formulation.
Yes, that's not the issue. Having to unbox every element, and write those to a `Ptr` is. It's less work if you can pass the `Ptr` directly (or with a `memcpy`). edit: Ah sorry, where I say "built on Storable", I should have said something to the effect of "hmatrix's arrays are internally `ForeignPtr`s" - I assume that's what this is in reply to.
I never use it because of the `forall` which make IMO signatures cluterred. When I really need a type signature for a *sub function* I write it as main one and don't export it. However, I would use it , if the `forall` wasn't needed. 
Really ? I'm pretty used to cmd line debugger but I never managed to understand how GHCi debugger works. You set a breakpoint in a function. run you program and GHCi stop ... somewhere. With a normal debugger I would expect to be able to see the argument of the function I stopped i. I understand Haskell is lazzy and I might have to force the argument to see them (which I can do with `:force`) but no. The arguments are even not in scope. I'm probably missing something (maybe the fact that function have only one arguments) but I think GHCi debugger is not that straight forward and would benefit of some tutorials. 
Wow, I've never met an actual [finitist](http://en.wikipedia.org/wiki/Finitism) before. Nice to meet you! If you don't mind, I'd like to ask you a few questions in order to better understand the finitist worldview. 1. Would you say that there is no practical difference between `[1..]` and `[1..2**1000]`? 1. Do you think Haskell would be better if the former was forbidden? 1. In the context of a proof, do you think that it is valid to assume that any input must be finite and that we can therefore proceed by induction over its size? 1. How many natural numbers are there? Is that an ill-defined question? Thanks for your time.
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Finitism**](https://en.wikipedia.org/wiki/Finitism): [](#sfw) --- &gt; &gt;__Finitism__ is a [philosophy of mathematics](https://en.wikipedia.org/wiki/Philosophy_of_mathematics) that accepts the existence only of finite mathematical objects. The finitist philosophy of mathematics is best understood in comparison to the mainstream philosophy of mathematics where infinite mathematical objects like [infinite sets](https://en.wikipedia.org/wiki/Infinite_set) are accepted as legitimate mathematical objects existing in the [Platonic](https://en.wikipedia.org/wiki/Plato) universe of mathematics. &gt; --- ^Interesting: [^Temporal ^finitism](https://en.wikipedia.org/wiki/Temporal_finitism) ^| [^Ultrafinitism](https://en.wikipedia.org/wiki/Ultrafinitism) ^| [^Sufi ^cosmology](https://en.wikipedia.org/wiki/Sufi_cosmology) ^| [^Primitive ^recursive ^arithmetic](https://en.wikipedia.org/wiki/Primitive_recursive_arithmetic) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cn1czpz) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cn1czpz)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
That is not a good approach. By the time you are releasing a project, there are a lot of dependencies that weren't there at init. By this time, the .cabal file may have flags and other customisations. Also, IIRC, cabal init picks up deps in other .hs files, example code etc. anywhere under your project root. It's simply an awful choice for the job! I'd sooner use cabal freeze --dry-run, examine the actual versions, and decide for myself how to soften the ranges, if at all. As OP, I'd like to ask that we stay on topic a bit? I mean, just at first even?...
I was also surprised at the omission of TAPL.
My bash-fu is not strong enough to debug this, but the script fails for me on the stackage package: ~/haskell/stackage$ ~/Desktop/hackage-docs.sh MichaelSnoyman Unable to determine package name As I mention in the issues linked and this thread, this script would be insufficient for my use cases, as I already have tooling set up to deal with large numbers of packages. __EDIT__ I still don't know why the bash script didn't work, but I figured out the idea from it. I'll explain a bit more shortly.
The functions `maybe`, `foldr`, and various "fold" functions translate from a data-type to its church encoding (modulu some flips). Sometimes the Church encoding of a data-type has some performance or other benefits, so it is useful to know.
Judging from from "any problem of practical interest" they don't doubt the existence of infinite things, just question how they could be useful in the programming we do today.
The reason I dislike pervasive laziness is that it makes functions no longer like mathematical functions. In addition to taking a value as input and producing a value as output, a function is also about which parts of the input will be evaluated as a response to evaluating a particular part of the output. Whether the composition of two functions is correct depends not only on which inputs they take to which outputs, but also on the laziness of the functions, which is invisible in the types. This is similar to how functions in imperative languages are not really mathematical functions from their range to their domain; they also have effects. In haskell we encode those effects in the type of a function. This is generally seen as a good thing, and I'm sure you would agree. For the same reason it is a good thing to encode the laziness of a function in its type. A potentially infinite list is really a different type than a finite list. The function `sum` should only work on finite lists. Inductive/coinductive types are a step towards this, but just like the last word on effects hasn't been spoken yet, the last word on inductive/coinductive types also hasn't been spoken yet. For example you don't want to write two different `map` functions for finite and infinite lists. You want to write only one `map` that takes finite lists to finite lists, and infinite lists to infinite lists (similarly, we shouldn't have to write a separate map and mapM).
Absolutely (see my above response)! Induction isn't suddenly broken, it's induction + a particular ordering that breaks. Aside though, I'm not honestly sure how to use the prefix relation to prove for example that `map id = id` for all lists. In general when your recursive calls don't respect your relationship so you can't just apply your inductive hypothesis. In cases like these, coinduction makes the proof exceedingly natural.
&gt; map id = id for all lists What are you using to characterize list equality? Is (forall n. a!!n = b!!n) insufficient?
I agree with you that the third approach would be the nicest. Allowing users to do `cabal install -tls` for a different backend would be very nice. As you alluded to, there is serious awkwardness with specifying cpp flags and cabal. For one, you cannot specify a package to use a specific build flag in a cabal file (i.e. snap-server -fopenssl can't be set in your cabal file), Albeit, this can be done in the `cabal.config` file after calling `cabal freeze`, this does assume users are building packages solely with cabal, which would be the majority of our use cases (I assume). CPP land doesn't scare me because we'd only need to `#ifdef` the imports, no code change required. I think this is a good route to go, we could then deprecate `io-streams-http`, and have a single source of truth.
Sure that seems reasonable :) You'd have to prove `map f xs !! n = f (xs !! n)` though
Not a finitist here. Imagining this for an answer... 1. Depends on the application. In general, [1..] is often a pretty good approximation for what you really meant. To reiterate, an infinite term is never more than an approximation. 2. No, just don't kid yourself. main = print [1..] -- not a practical program main = print . (take 1000) $ [1...] -- "every" practical program You always need something that performs the role of take. I would not serve you an M&amp;M without the candy coating, and you should always wrap your gooey infinity in a hard shell. Otherwise your program must run in a world where I hit Ctrl-C, which is proof you have not finished writing your program. 3. Just choose N large enough not to notice any problems. Integers are like violence. If N is not working, you need a bigger N. 4. We use the natural number to count things. They are all finite, so you do the math. :-) There is always number large enough that you won't need to use all the smaller numbers. You can say there's infinity, but I will never count that high. Don't know if that was a fair representation. Perhaps my knowledge of finitisim is infinitesimal, but actual finitists are in limited supply, naturally.
Thanks for asking. I heard snideness in the bluntness and the unusual &amp; its conspicuous placement in the docs. Your description could have said simply, "Alternative docs at (link)," and the job of explaining could have been left to your blog post, for those curious enough to look. But, as you picked up, the conclusions people draw about your motivation are the more important thing than the tone of that particular line - perceived snideness is just one factor in that. When someone advocates for buy-in of his or her new tool/book/style etc., I'm trying hard to gauge how sincerity, divisiveness, and self-promotion are balanced in the person doing the advocating. I want to know how much of a personal loss he/she is willing to take for the good of community cohesion and promotion of other people's work. In the case of Stackage, there has been some visible history of friendly cooperation with Hackage, but there aren't actually many bits of information there, because failing to do this wasn't really an option (it would have been seen as too blantantly anti-community). What carries a lot of bits are things like slipping criticisms in, failing to empathize openly with the arguments made by the other side, and generally skirting around the borders of being fair and unbiased. Specific examples: Imperative blog-post title "Use Stackage for Docs" Soft doublespeak "my main motivation here is explaining my reasoning for these changes" ... followed by instructions to others to include Hackage criticism directly in their documentation Not mentioning the scripts for running doc builds locally and uploading w/ the library to Hackage I'm anticipating a very convincing rebuttal of all these points :) I just hope I did an Ok job of laying out the factors that drive my impression (maybe other folks' impressions) without it sounding like an attack, and that it's useful to you, for anticipating responses to your next posts. Probably the style recommendations I'd make are obvious, for faithfully geting your true pro-community intentions across? They could be summed up as just - go way above and beyond in respecting the body of work that you're trying to change or supplement. Niclas Broberg for example did a really beautiful job [pitching the haskell-suite](https://www.youtube.com/watch?v=bQP4wldPX4c) as a solution for some of the ghc project's deficiencies, while making his respect for ghc extremely clear. This might be a little over the top for your style. But modulo style, it's that essense of collegiality that makes all the difference to a reader/listener like me.
I can't help but feel it is kind of odd to ask which libraries/language features/... are underrepresented or misunderstood as well. Wouldn't it make more sense to ask the opposite and then infer from that which ones are not well known or understood well? After all how should I know that I don't know about something that would be useful to me.
I feel like im missing something. This doesn't look like a problem.
So I did honestly not put much thought into that blog post. It was a Saturday night, I got *yet another* report to upload docs, after *already* having put a comment on that package that the doc generation was unreliable. So I wrote up a quick blog post to say, "hey, I'm putting docs on Stackage, stop asking me about it." I honestly did not expect it to get posted to Reddit and for a firestorm of people assuming the worst about me to ensue. If I'd known people were going to read into it, I would have spent more time thinking about language. I was shocked to have things thrown at me like "vitriol." Am I annoyed about the current situation? Absolutely, I'm tired of having to find workarounds for broken things in our infrastructure. But to be maligned to the degree I have been in this thread was beyond what I expected. If you want point-by-point responses: * Yup, bad choice in blog title. Didn't think about it. * That really is my main motivation. I put in instructions for (1) me to refer back to myself later, and (2) because I anticipated *someone* might ask me the right way to do it, and it's easier to just write it in the blog post once. * I never even considered talking about the scripts. There are *many* things I didn't say in the blog post. I didn't explain why I think the Stackage documentation solution is superior, in that all docs are part of a cohesive snapshot and are guaranteed to have links with all-compatible package versions. See the previous point: I just wanted people to know what I was doing, so I didn't get a million questions about it. I think a lot of people don't understand that I get a *lot* of questions about any action I take. If I write a blog post, it's often times simply heading off a torrent of inquiries. Really, that's it. No need to dig any deeper. If I want people to hear something, I'll say it directly. (For example, I *do* think that the lack of README support on Hackage is really bad, and should be addressed. I said it here, said it on the cafe, said it in the Hackage issue tracker, etc.) OK, that's enough of that. I've written up basically [a retraction blog post](http://www.yesodweb.com/blog/2014/12/hackage-docs-live-again) since this caused so much drama. __EDIT__ Forgot to mention: thank you for the obviously well thought out response, it's much appreciate.
Replying here because why not: [problem solved](http://www.yesodweb.com/blog/2014/12/hackage-docs-live-again), let's move on to more important matters.
I confess the script there was cobbled together for our own internal use and isn't terribly robust.
I'd be much obliged if you cracked this one. &gt; I think as a consumer I'd really prefer a brief identification in description of what the package does or provides, along with links to the Readme, a pipes-style .Tutorial module, and whatever other documentation is out there. Trust me, it's *really* difficult to figure out something meaningful to say in a synopsis, a description, *and* a README, and very tedious to write tutorials in Haddock format (at least for me). That's asking for a lot. I think asking authors to write a decent synopsis and a README with good examples is about the best balance we can get between useful for users and what authors can meaningfully do.
My approach is over the top and actually uses the Cabal parser to get the name and version, as well as using http-client to do the upload, and dealing with multiple directories at once. Cutting down on the number of times I have to type in my username and password is *very* convenient. https://github.com/yesodweb/cabal-src/blob/master/hackage-docs.hs
Fair enough. I plead the fifth on my actual real world hackage related security habits, they are pretty horrible. ;)
If I've been marinating in online (especially earlier on) Haskell and (especially later on) type theory related material for the past five years, have read a ton of blog posts and articles, a bunch of papers, and half of the HoTT book along the way, would it still be important / worthwhile / a good idea for me to read TaPL? Obviously given its 600+ pages, I would be sure to learn new and valuable things; but also taking the opportunity cost into account. Reading 600 pages could also mean a *lot* of papers!
I see, thank you for clearing that up. I was mislead by the [Hoogle Wiki page](https://www.haskell.org/haskellwiki/Hoogle) sec. 1.2.4 because I had skimmed over 1.1 first. Will add a note to 1.1 as soon as I've wrestled the HaskellWiki into granting me the privilege of contributing.
Probably more accurately part of a manic episode.
&gt; Frankly, though, I think it's still the wrong choice. In *opposite-world* GHC has an explicit non-scoped **forthis** type variable keyword. Otherwise type variables in *opposite-world* GHC are implictly scoped **forall**. My guess is that the *opposite-world* GHC folks discovered that their fancy *opposite-world* alphabet was much larger than our minimalist two letter alphabet, so fears of reusing the same letter to mean utterly different things in the same scope were minimized. Clearly *opposite-world* is wrong. *Opposite-world* suffered an obvious case of feature creep when their alphabet grew to contain more than two letters.
Hey, re: new blog post, kudos for being so responsive. and thanks for the doc tool and your thoughtful response! :) Two curses of populariy - tons of documentation requests and heavily scrutinized blog posts.
Laziness is certainly not parallel to induction/coinduction (as you say you can have lazy inductive data types), but it is certainly not orthogonal either (you can't have non-lazy coinductive data types). The thing is though, even for inductive data you'd want to be explicit about laziness, even though they terminate with either implementation. If not being lazy turns a O(n) algorithm into a O(2^(n)) algorithm that's usually about as incorrect as non-termination. So the last word on this topic hasn't been spoken yet :) For me at least it greatly helps to have the places where laziness is required marked in the types. Just like with I/O it helps a lot to know where it's *not* being used, so that you can reason about those parts of the code as if they were ordinary mathematical functions. To me accepting that functions are really monotonic functions in a CPO is like accepting that functions are really heap state transformers. Not quite as bad, but not ideal. Laziness also has more down to earth disadvantages, for example laziness is inefficient and requires compiler optimization passes to undo the damage, laziness can easily lead to huge memory usage (foldl), and laziness sequentializes execution order and makes it much harder to control where time is being spent, which doesn't play well with parallelism.
I'm not really sure what you mean. When I said this was the wrong choice, the alternative I meant was that all type variables used in a containing scope are implicitly available in nested scopes, but (as is the case today) you can still choose to shadow them by adding an explicit forall. So this would be an error: f :: a -&gt; Int f x = foo 5 where foo :: a -&gt; a; foo = id because foo is declared to operate on the same `a` as in the definition of `f`. But you could fix it with: f :: a -&gt; Int f x = foo 5 where foo :: forall a. a -&gt; a; foo = id Is this the same as your opposite world? I can't tell. Aside from backward compatibility, there doesn't seem to be anything particularly awful about this situation. It seems much more logical to me than the current state of affairs.
&gt; Thank you for your request. Unfortunately, to all all users access to documents on our system we need to restrict downloads of large media to two files at a time per user. This is going to take a while.
Thanks a lot! The wiki page needs some love, and I'd be very grateful for any edits you can make.
I was considering mentioning TAPL and/or PFPL but decided against it for mysterious reasons lost to time. I'll put them in tho.
&gt; things thrown at me like "vitriol." Fair objection; I again apologize. That was too strong a word; I think "frustration" would have been more appropriate both to characterize your comments and to express my concern. My objective in writing the top-level response here was, roughly: "Oh dear, it looks like this is going to cause drama. I'm not sure whether there already *is* drama already. Can I kick off discussion in a way that will get everybody's cards on the table and lead to a resolution?" I'm not sure *I* did a great job in that, but I really appreciate your engagement here.
If there is a local university library not too far from you, go read bits of it there. If it's "not worthwhile" you'll know it soon enough -- without having to buy anything. From what you say you're more technically advanced than the TAPL's target audience. That said, it has content that is programming-languages oriented that you don't know. There is a second tome, ATTAPL (Advanced Topics in ...) that cover extra things such as logical relations, and is also a good read (though different people will appreciate different chapters of it, the goal is not to read it in full). I don't think you can compare 600 pages of a well-written book and 600 pages of papers put together. Papers tend to be much denser (because of page limits), and the book as a lot of extra material (like: explanations, detailed proofs, detailed bibliography...) that makes it both worthwhile as a complement and easier to read. Another option you have is to go to [Software Foundations](http://www.cis.upenn.edu/~bcpierce/sf/current/toc.html) directly. It's an online book that's designed as a series of Coq exercize, so you formally prove everything you do while reading. The topic coverage is a bit less broad than TAPL (there is nothing about module systems for example which are a fascinating read that *need* an introducory chapter) but it goes into the detail much more deeply -- that's what proof assistant do. That would be my advice: go for Software Foundations. If you find the beginning easy (you'll probably do), you can go fast.
Sorry, needed to give my kids a break.
&gt; If X ⊆ Y then L(X) ⊆ L(Y) because x ∈ L(X) means x = nil ∈ L(Y) or x = cons(h, t), but since t ∈ X ⊆ Y then cons(h, t) ∈ Y. Should the last Y be L(Y)?
Absolutely, thank you :)
Do you know how this notion of coinduction relates to [coinduction in Agda](http://wiki.portal.chalmers.se/agda/pmwiki.php?n=ReferenceManual.Codatatypes)? For those who are not familiar with Agda, it is a language in which well-founded datastructures must be defined separately from datastructures which are possibly infinite. For example, data List (A : Set) : Set where [] : List A _∷_ : A → List A → List A is the type of finite lists, while data Colist (A : Set) : Set where co[] : Colist A _co∷_ : A → ∞ (Colist A) → Colist A is the type of possibly-infinite lists, like `[a]` in Haskell. Most Agda code uses structures of the first kind. Since Agda is used to write proofs, and programs only correspond to proofs when they are guaranteed to terminate, Agda has a built-in termination checker. When our code recurs on well-founded data-structures, the termination checker works by verifying that for at least one argument, all the recursive calls are performed on values which are structurally smaller than the input. The "for at least one argument" part is important: if one of the arguments is a Colist, then recurring on the tail of the colist does not guarantee termination, but if we are implementing `take : {A : Set} → Nat → Colist A → List A`, then the recursive call will use both the tail of the colist and a structurally-smaller `Nat`. The fact that we recur on a structurally-smaller `Nat` is sufficient to guarantee termination, irrespective of whether the colist argument in the recursive call is bigger or smaller than the input colist. But that's not actually sufficient. Sure, the structurally-smaller argument guarantees that the function will terminate after a finite number of recursive call, but how do we know that each step along the way takes a finite amount of time? This depends on whether the next element of the colist can be computed in a finite amount of time. In particular, compare the following two definitions: natsFrom : Nat → Colist Nat natsFrom n = n co∷ ♯ natsFrom (suc n) bottom : Nat → Colist Nat bottom n = bottom (suc n) Neither definition terminates, since they recur on a larger argument, but only the former *progresses*, that is, is guaranteed to produce the next element in the sequence after a finite amount of time. The coinduction rule I know from Agda is that `natsFrom` is accepted by the termination checker because the recursive call `natsFrom (suc n)` is *guarded* by the `(n co∷)` call, that is, the recursive call occurs as an argument to `(n co∷)`. Therefore, `natsFrom` will use `(n co∷)` to produce a new element in a finite amount of time, and so will the recursive call to `natsFrom (suc n)`, and so on. I know how to use that coinduction system to implement terminating Agda programs, but I guess I don't understand why the system is called "coinduction". Where is the relation and/or the monotone function?
me either
&gt; Now I'll put out a request: will someone please implement READMEs on Hackage? Yes, I could do this, but I've never touched the code base, and have lots of other things to do. The best way to start contributing to an existing open source project is to find something small to add or fix. It gives you focus when reading through the code.
&gt;For OpenGL, you can happily dump a Storable vector of linear types into OpenGL without having to do anything as they are stored in the usual densely packed manner. ~~This isn't true. `data V4 a = V4 !a !a !a !a`. Those fields are strict, but not unboxed. Here's a little program to verify this:~~ (removed that) Nevermind, I see what you mean, I hadn't used a Storable vector of linear types. That's quite clever.
I do this all the time in libraries I contribute to (`linear`, `vinyl`, `vinyl-gl`, `Frames`, etc.). It's a great compromise trick to get the most out of GHC that I probably don't advertise as much as I should. That said, the technique you show here is awesome. Carter and I, and I'm sure others, do this in several numerical libraries, so I think it's great that you've written it up like this.
T\_S\_ has given an excellent answer. My 2 cents: 1. f: Integer -&gt; List Integer, f N = [1 .. N]. I don't know what [1..] is ;) 2. All functions are total. Evaluation order is eager. Or arbitrary, in which case I'll pick the eager variant for all my arguments. I want to reason about CPU / RAM at a glance. 3. Yes. Fun exercise: model a CPU behavior using explicit time. forall N, M . P(N, ...) -&gt; Q(N + M, ...). 4. I have no use for the count of all the natural numbers. All I need is to prove that a given property holds forall N.
The montone function is the function we're defining. We adopt the `&lt;` that domain theorists use where everything is greater than bottom and is is defined structurally else where. This means that `Just bot &lt; Just x`, but `Nothing /&lt; Just x` or anything like that. You should think of `&lt;` as ordering things by "how much information" they have. With this `&lt;` all computable functions are monotone since we cannot pattern match on bottoms. Now in order to define a greatest fixed point we need cocontinuity which is exactly what productivity gives us. So in the end we find all productive functions have a well defined greatest fixed point just like all structurally recursive functions have a least fixed point. That's how the two are related, Agda forces you to conform to a pattern to ensure cocontinuity which is very similar to how we require structural induction and strict positivity :)
&gt; I hope I haven't just set a precedent that complaining at me gets me to do work I don't like. Well, as someone who produces packages for others to use, isn't that usually how it works :-(
Honest question, do you have any example where laziness worsens the amortized asymptotics and not just constant factors / worst case bounds? [Edit: Er.. I think I misread you as saying laziness would blow you from O(n) to O(2^n ), sorry.] I have plenty going the other way, but off the top of my head I have no situation where the opposite occurs.
If you have some elementary topology (or even real analysis), but little or no category theory, domain theory, or advanced topology (which homotopy theory is normally reserved for), then you really can't go wrong with: *Synthetic Topology of Data Types and Classical Spaces* [entcs87](http://www.cs.bham.ac.uk/~mhe/papers/entcs87.pdf) (M. Escardó, 2004) You'll see domain theory along the way, a wee sampler of pertinent category theory, but no direct mention of homotopy type theory. It would probably still be good preparation for HoTT, but I've only glanced at the latter and am not widely versed in the field. You don't need previous exposure to topology to read these notes, but having had some will give you an intuition about the significance of the handful of key notions that comprise topology (continuous function, closure operator, products, compactness, dense embedding, extension, separation axioms, connectedness, exponentials ... a few more and you have the core definitions which drive all of topology). If you like analysis/topology as well as theoretical CS ... for me, [entcs87](http://www.cs.bham.ac.uk/~mhe/papers/entcs87.pdf) was the most enjoyable technical read of my life. It's written in the classical style, although the content is going to seem pretty radical (even iconoclastic) if you're coming from classical topology. Thanks to Gibbard Cale for pointing me towards it, and to Martín Escardó for his authorship! `Broken links fixed; thanks gergoerdi, I will always check them in future!`
Cool post. I was inspired to see if I could remember enough about F-algebras to implement one solving this problem, even if it's not strictly a correct solution to the problem you gave (since the type of the data structure is always `Fix Tree`). data Fix a = Fx (a (Fix a)) data Tree a = Leaf String | Tree a a instance Functor Tree where fmap f (Leaf x) = Leaf x fmap f (Tree x y) = Tree (f x) (f y) -- generate the 'tree' from an initial n pairs :: Int -&gt; Fix Tree pairs 0 = Fx $ Leaf "42" pairs n = Fx $ Tree (pairs (n - 1)) (pairs (n - 1)) -- the definition of the 'algebra' stringify :: Tree String -&gt; String stringify (Leaf x) = x stringify (Tree x y) = "(" ++ x ++ "," ++ y ++ ")" -- cata eval :: Fix Tree -&gt; String eval (Fx t) = stringify $ fmap eval t Maybe this approach using a fix type is somewhat analogous to the use of recursion in "regular code"? *Main Control.Monad&gt; forM_ [0..3] $ putStrLn . eval . pairs 42 (42,42) ((42,42),(42,42)) (((42,42),(42,42)),((42,42),(42,42)))
@yitz, http://hackage.haskell.org/package/http-client-streams-0.3.0.0, now supports both backends via CPP flags
Can you bin on numerical analysis? I selected that one and it's the fifth highest :( I'm surprised that web design came #1. &gt;F-Algebras is also a puzzling response, but was overwhelming the most mentioned response from the total count. There are some great articles about the relations between F-Algebras and catamorphisms, and although they are used somewhat rarely and I’m genuinely surprised about this result. &gt;On a personal note, I’m somewhat saddened by how often llvm-general shows up given how much time I spent on, what I thought, was a very extensive tutorial on the subject. I'm not - after all, the questions didn't seem to be asking about blog posts, but /documentation/. &gt;The most mentioned write-ins were: &gt;SIMD &gt;Compiler Passes &gt;Compiler Plugins Hah, those were the ones I wrote in.
Is there such a thing as a free arrow and is it on Hackage?
Your link 404's; this seems to be the correct one: http://www.cs.bham.ac.uk/~mhe/papers/entcs87.pdf
Arrows allow "locally stateful compositions", as opposed to carrying the "entire" state in a parameter: http://blog.jle.im/entry/auto-as-category-applicative-arrow-intro-to-machines#proc-shines 
Thanks for your excellent comments hastor! I've been mulling this stuff over for months, but reading those two lines gives me new perspective. It sounds like you're more interested in the pattern-matching than the forced evaluation aspects. I think your assessment of the potential is realistic. [SOP](http://hackage.haskell.org/package/generics-sop) generics is shape-respecting. The `seqaid` side of things could help test these invariants (pre/post) during execution. More work is definitely needed in `deepseq-bounded` to support this approach. For instance, at present a (forcing) pattern **P** always matches any term **T** whose shape is a proper subpattern of **P** (roughly speaking). So there's no way, currently, to use `NFDataP` to check the depth of a term. Here are a few tests showing some of the nuances of the present semantics. (You need to rebuild `deepseq-bounded` with the `WARN_PATTERN_MATCH_FAILURE` flag set to `True`, in order to see the match failures. Each of these GHCi lines also echoes the term argument, but I edited these out.) &gt; forcep ".{.*}" (True, [1,2,3]) &gt; forcep ".{.*}" (True, [1,2,3], 'x') NFDataP: warning: instance (,,): bad PatNode child list NFDataP: warning: couldn't match WR (having children [Node WS [],Node WW []]) &gt; forcep ".{..}" [1] &gt; forcep ".{..{..}}" [1] &gt; forcep ".{..{...}}" [1] &gt; forcep ".{..{..}}" [1,2] &gt; forcep ".{..{...}}" [1,2] NFDataP: warning: instance [a]: bad PatNode child list NFDataP: warning: couldn't match WR (having children [Node WS [],Node WS [],Node WS []]) The change would not be hard to implement, but it's pretty fundamental and wants consideration. I guess rather than modifying of terms by setting select subterms to `undefined` (as is done in many of the `deepseq-bounded` tests), you could take the complementary approach and have a pattern node type which raises an exception when hit. Then you have a non-invasive means of gauging the shape of the term being forced. I've started a [ticket](http://fremissant.net/deepseq-bounded/trac) or two around this.
There's an implementation and some discussion in [this SO question](http://stackoverflow.com/questions/12001350/useful-operations-on-free-arrows). I don't know if it's on Hackage anywhere. There has been discussion of free *categories* on and off; there's a [branch of the `free` package](https://github.com/ekmett/free/tree/categories/src/Control/Category/Free) with some code that I've never taken the time to quite figure out.
There is a good reason that the work on arrows lost its momentum. The focus switched to applicatives. And then /u/cdsmith demonstrated (not completely, but nevertheless quite convincingly) in a [pair](https://cdsmith.wordpress.com/2011/07/30/arrow-category-applicative-part-i/) of [blog](https://cdsmith.wordpress.com/2011/08/13/arrow-category-applicative-part-iia/) posts that `Arrow` is essentially equivalent to `Category` + `Applicative`. That pretty much put `Arrow` to bed, because most people seem to find `Applicative` and its notation easier to understand and use than `Arrow`. Nowadays, new libraries that use `Arrow` instead of `Applicative` are a rarity, with Tom's Opaleyes one of the exceptions. Does anyone have a convincing story about why `Arrow` is worth the additional learning curve over `Applicative`?
I posted a [type class puzzle](https://www.haskell.org/pipermail/haskell-cafe/2006-October/019166.html) in 2006 which might provide the basis for another "Hello World" for type systems. A [solution](https://www.haskell.org/pipermail/haskell-cafe/2006-November/019475.html) to the puzzle was published jointly by Chung-chieh Shan and Oleg Kiselyov.
No idea what Impredicative Types is or when to use it. Maybe /u/ocharles will tell us for Xmas :) 
&gt; Does anyone have a convincing story about why `Arrow` is worth the additional learning curve over `Applicative`? As you said, it's for the `Category` part. If your datatype takes input via a contravariant parameter then you probably want to be able to compose them in a `Category` manner. You can't do that with `Applicative` alone. And if you are going to do that you'll want some additional notation to make writing such expressions tractable. As there's no `Category + Applicative` notation, `Arrow` notation is the way.
I love hoogle (even have a custom Chrome search for it) but is there a way to automatically search all the packages?
There's something curious about the juxtaposition of these two statistics: &gt; The self-rated skill level turned out to be a fairly typical distribution with a median of 5 [...]. Curiously most Haskellers in the poll rated themselves below 5. Otherwise a very interesting poll to see the results for. The popularity of impredicative types is pretty mysterious to me, too.
In this example: printTree :: Show a =&gt; a -&gt; Int -&gt; IO () printTree v 0 = print v printTree v n = printTree (v,v) (n-1) When is the Show constraint resolved? Is there a typeclass search at runtime?
Can we see some kind of an aggregation based on skill-level? Or just a substatistics for skill-level &gt;= 7 perhaps? It'd be very interesting I think. (Also, hrm, `ImpredicativeTypes`… why on earth?!!) 
That might be a bit hard for a "Hello World" :)
https://hackage.haskell.org/package/hermit
No, instance search is only performed at compile time. The way this works is that `printTree` receives a `Show a` dictionary at runtime, and uses it to construct a `Show (a,a)` dictionary. The code to do that doesn't depend on which `a` this is. Information about which `a` it is isn't available at runtime anyway.
If most people rated themselves below 5, how can the median be 5?
My question exactly. =)
/r/haskell/comments/2pxo0k/informal_poll_of_underdocumented_haskell_topics/
It allows instantiating type variables with polymorphic types. There's rarely a good reason to use it. https://downloads.haskell.org/~ghc/7.8.3/docs/html/users_guide/other-type-extensions.html#impredicative-polymorphism
For me I voted for Impradictive and F-Algebra because they were the only things there I had not seen a dozen blog posts about already. I seem to have slightly missed the intention of the poll, in that I have little to no idea what they are or if they are useful, but chose them simply because they seem to not be documented.
&gt; &gt; The join2 function constrains the function parameter types (f a) on the typeclass Applicative. In its body, join2 uses the "," type constructor (function?) &gt; &gt; ~~Type~~ Data constructor is the correct terminology. In this case, the precise type constructor is (,). In Haskell, (a, b) is syntactic sugar for (,) a b. The (,) in the body is a data constructor. 
Thanks. FP is fun, conference websites should reflect that. :) We're shooting for BBQ or bacon for one of the conference meals... hey, it fits with the whole *sizzling* theme!
Maybe non-Haskellers rated themselves above 5. Damn holier-than-thou Idris users.
 &gt; Behavior-Driven Development DSL &gt; This library is intended for developers, not business people Why would you do that? Why not just write "ordinary" tests?
False alarm, that's Google customizing links for me. Might still be interesting to see what Google recommends for everyone else.
Check out "Notions of Computation as Monoids" for a definition of free arrows. http://arxiv.org/pdf/1406.4823v1.pdf
How does `ImpredicativeTypes` compare with a type checker which is more capable of handling impredicativity like (I think) MLF and was it Boxy types?
The motivation is to give your tests a clear structure. This also helps ensuring that you test one thing per test ("when" bit can occur only once). Generally I guess this comes from an experience of reading long tests that are a string of commands, it is uncertain what is being tested, and that test many things. I found it very useful in my personal experience of doing this for several years (before Haskell in other languages too), especially when reading tests written by other people. I guess it also depends what is an "ordinary test" for you. I'm talking here about the style of HUnit (for system or acceptance tests), not QuickCheck properties. But, sure, one can be very successful with "ordinary" tests as well.
[Heh](http://i.imgur.com/yuhCPzJ.png)
I went to this last year, and it was a lot of fun. Thoroughly recommended.
I really wish `Arrow` syntax was `GArrow` syntax where `arr` doesn't exist and then `class GArrow a =&gt; Arrow a`.
Does Haskell just figure out that in order to reference t_p_hs or h_p_xs it has to call p_xs which in turn calls pack' xs?
I've only used one system with impredicative types: Coq. And the only reasons Coq is impredicative is because it has `Set` for large elimination. Otherwise LEM becomes meddlesome. However, Coq makes far fewer attempts to do type inference than Haskell so I found it a lot more reliable to work with. Haskell's type checker tries really hard, but I have no idea how to accurately tell when it can't figure out somethign on it's own.
`p_xs` is not a function – it's a variable containing the entire list `pack' xs`. You can find out more about it in [Learn You A Haskell](http://learnyouahaskell.com/syntax-in-functions#pattern-matching). (Search for `@` on the page and you'll find it.)
Note that there are several patterns (and also guards) for the pack' function, so when it gets certain values as its arguments, it doesn't continue to recurse. So there's no ambiguity in the order, the path execution will take is dependent on the arguments. Also, since Haskell is lazy, the binding in the where block is only evaluated when it's actually called for. It's not a "variable" like that other person said.
[Hackage](http://hackage.haskell.org/) hosts documentation (with varying degrees of success, if the docs for the latest version of something aren't online, try looking at an older version), so you could try looking around for libraries which interest you there.
Thanks, I do spend some time there, although typically to introduce myself to libraries, not so much to spend time reading. It's just not really what I am looking for - more like how a dictionary organizes and describes words; this is the sort of format or organization I am seeking, in addition to conveying clearly the code, reading documentation for fun constitutes organization with the intent to be pedantically organized. I am aware it is rather far fetched, and I don't consider myself expert enough to craft such a thing. It's more that there's extra care put into it, that's something I latch onto. If that rings any bells, I'd be very thankful. If not, maybe it's just an idiosyncrasy of mine. 
Yes, we took our cue from Montague. http://www.codeshare.io/mDVbx &lt;- link to html source for syllabus...I think the instructor moved his website somewhere else. "This is a graduate introduction to Semantics and Pragmatics, introducing students to basic logic, compositional interpretation, and the strategic reasoning involved in deciding what people mean by what they say. By the end, you should be able to follow a substantial portion of current work in the area. Topics covered include different notions of inference and entailment, presupposition, implicature, ambiguity, set theory, propositional logic, first order logic, lambda calculus for compositional interpretation, extraction and binding, quantification and generalized quantifiers , pronouns and ellipsis, basic dynamic semantics, basic intensionality and modality, event semantics, and tense and aspect. Texts: Irene Helm and Angelika Kratzer. Semantics in Generative Grammar. Blackwell 1998." Text we used: http://amzn.com/0631197133
No, it does not break down. For example if you have these: data Tree1 = Cons Tree1 | Nil data Tree2 = Node Tree2 Tree2 | Nil data Tree3 = Node Tree3 Tree3 Tree3 | Nil data TreeN = Node [TreeN] | Nil data TreeF = Node (Int -&gt; TreeF) | Nil In order, this corresponds to induction with 1, 2, 3 things smaller, or a finite but varying N things smaller (assuming we're using [T] to denote the type of finite lists), and a countably infinite number of things smaller (Int -&gt; TreeF). These are all fine *inductive* types, because the level of nesting (the length of a chain) is finite. Coinductive types are infinite but they are not *just* infinite. The point of coinductive types is that the consumers of the type are finite. For example a consumer of a potentially infinite list is only allowed to look at a finite number of elements. It's important to note that this is: there is an N such that for all potentially infinite lists L the consumer only looks at the first N elements. It's not: for all potentially infinite lists L, there is an N such that the consumer only looks at the first N elements. The N is fixed with respect to the list structure. So you could say that inductive types have finite depth, but the consumers of inductive types can have infinite depth. A function that sums all elements in a potentially infinite list, except when the list is actually infinite, then it returns 0, can't be implemented as a consumer for a potentially infinite list. Summing on finite lists is a fine consumer of finite lists, even though it's in a sense an infinite consumer: the longer the list, the more elements it will sum, unboundedly. On the other hand, the consumers of coinductive types are finite depth ("coinductive types are cofinite"). You can't sum a potentially infinite list, because sum may look at an unbounded number of elements. You can however take the first 10 elements of a potentially infinite list. You can also map a function f over a potentially infinite list, because for each element of the result only a finite number of elements of the input have to be examined. TL;DR: to ensure termination we can either require the input to be finite (inductive types) or the consumer to be finite (coinductive types).
Probably because not many people taking the poll know what it is or why anyone would care!
Let me know and I can link you to some of the pdf's for the lectures/study materials we used for class.
The system implemented in GHC was a subset of MLF called FPH, if memory serves. There was an ICFP paper about it.
Then you'd need both arrow and garrow syntax, because garrow syntax wouldn't support applying a pure function on the right hand side of a `-&lt;`.
It's probably also worth taking a look at the implementation of [group](http://hackage.haskell.org/package/base-4.7.0.1/docs/src/Data-List.html#group) in GHC. The GHC implementation is at once more idiomatic and battle-tested.
 Have you used your bdd test-code on any projects that you can share? I'd like to see it in action.
What are you talking about? Never heard of Haste? Elm? GHCJS? Reactive-banana? Netwire? Yampa? Wrappers for QML, GTK, wxwidgets, ncurses, sdl?
People build front-facing software in Haskell all the time. Web apps, mobile apps, desktop apps, all kinds of stuff
I can definitely see that point of view, in that the mechanical overhead pushes out the possibility of a snappy ten line demo. Fundamentally, however, the library is a "mid-level" binding and it requires almost by design that the programmer be explicit about the glue they're creating. My feeling is that it doesn't become a significant burden in larger applications, but I admit that's not a particularly great excuse. It would be interesting to see how large the the different versions of the notes application would be if written using the GTK+ binding. Having said that, the audio doesn't capture it so well, but there were some interesting ideas from questioners at the end about the potential for higher-level abstractions. For example, auto-generating a class full of accessors based on examining the fields of a record type with one of the generics libraries. These are all things which can, should, and hopefully will be built on top of the core library. If you have repetitive verbose code then, as a Haskeller, feel empowered to factor it out :-)! Going forward, the libraries will try and pick up that slack too.
Isn't elme another language? No I never heard of reactive banana (?) nor any of that, sorry
There are. Tell me a killer app I have on my desktop or using on the web written in haskell, though. Having (incomplete) libs doesn't mean having applications. Don't be mad, but the question is real. It's a fact. EDIT: I'm not claiming haskell to be bad, I'm telling fanboys that missing apps is real. Haskell has plenty of good libraries, but there are more libraries than actually used applications :)
Perhaps you should ask about something specific that you think is difficult to build in Haskell /u/LamXX_LamXX. Haskell is experiencing quite a lot of growth in industry at the moment. It's probably true that Haskell lags in some areas in terms of libraries. Other areas are pretty mature, you might be surprised. The server side also tends to be more specialized, benefits less from "mainstream" economies of scale and more from maintainability. On the client-side I'm somewhat hopeful that [Elm](http://elm-lang.org/) will be able to appeal to a large enough portion of mainstream programmers in order to capture some of that economy of scale, but we'll have to wait and see. [PureScript](http://www.purescript.org/) is also an interesting development. PS I don't think anyone is angry :)
Elm is another language, indeed, based on the premisse of having a Haskell-like language that compiles to JavaScript, and a built-in FRP library. Nowadays you could probably just use Haste or GHCJS together with some FRP library (like Netwire, Reactive-banana or Yampa). FRP (Functional Reactive Programming) is a way of structuring interactive programs, like GUI and games. And it works greatly with Haskell's type system.
&gt;pedantically organized Do you mean pedagogically here?
To be honest, I filled out the survey on my phone and thought the skill level field was from 1 to 5 because I did not see the lightly colored scroll bar. Others may have done the same and artificially skewed the results.
&gt; Do you mean pedagogically here? " Characterized by pedantic formality " http://www.thefreedictionary.com/pedagogically Sure.
Looks like this one: http://repository.upenn.edu/cgi/viewcontent.cgi?article=1417&amp;context=cis_papers
I've played games written in Haskell, and seen one recently that ran on Android. I've seen several web apps written in Haskell and there are a handful of good web frameworks you can use. Frontend work can be done using Elm and GHCJS, among other projects. I think the problem is that Haskell is barely on the radar compared to Java and C#, so the vast majority of applications are written in C-derived languages. You personally haven't seen Haskell applications, and it's true that they're comparatively rare, but that doesn't mean that you can't do those things in Haskell.
An intriguing counterfactual!
How can you run a Haskell game on Android?
But if those are so good, why there is almost nothing made with them?
The formatting for this post is a bit wonky for me (Chrome on Linux). The code under the `Syntax` section is spilling way of the page.
I get the same thing in Firefox on windows
Don't read anger into someone's rejection. You don't know that people are angry. All we know is that people are telling you that the assumptions built into your questions are invalid. The fact that there are several web frameworks in Haskell and you make a claim that you can't use it to write web-apps… I mean, you want people to take the question seriously? Well, the serious answer is: Your understanding of the situation is wrong. The real situation is: People *can* use Haskell for all those things, but the examples of finished, popular projects is quite small at the present time. You're observing the lack of lots of user-facing Haskell apps and asking a loaded question that makes a completely invalid claim about the *cause* of the situation. Try again with a question like "why don't I see many front-end, user-facing apps written in Haskell?" (Note that this even gets at your experience and doesn't claim that your experience is universal.) Then, people have a reasonable question they can answer.
Although, Haskellers can occasionally get a bit carried away - don't let anyone frighten you off!
I actually meant didactic/educational, so it appears I need to re-read what you said. Sorry to be a bother. I'm trying to understand what people want for documentation, learning, etc. - that's my angle on this.
It's a combination of a few things: 1. TH is untyped in aspects. A macro like "logDebug :: Q Exp" tells you zero about what it actually generates. The type signature is practically useless. 2. Macros in general do not compose. The only way to abstract a macro is to write a brand new macro. Also, they exist on a separate namespace and runtime to normal regular code, unlike lazy functions, which make for nicer control structures like `when`. 3. Use of macros when they're not necessary, both in the Lisp world and the Haskell world, is, for the above reasons that macros have a bunch of caveats, disdained. 4. Sometimes macros are seen as "cheating" because they are a shortcut around possibly defining a more general, principled abstraction based on values, types and classes, which people are more familiar with. The ideal scenario is that you identify a quality abstraction first, e.g. Monad and then add some special syntax (do notation), or Applicative with [||idiom brackets||]. 5. Tooling support isn't that great. As you saw, you just see a "dump-splices" compiler option. If you get a type error in a macro, GHC doesn't show you the expanded code with the error, it just shows you the line $(foo). Generally they're hard to deal with when they go wrong, and you tend to just hope they don't. When something goes wrong in Yesod's macros, which is rare but can happen, you basically have no idea why and have to start reading library source code. 6. Compile-time overhead. This is less of an issue with nice SSD drives these days, but still, GHC will load every package you're using at compile time, which can be 30ish on a big project, which slows the compile cycle down with a cold GHC run. All in all by comparison to regular lazy functions, deciding to use TH requires good judgment, incurs a readership overhead, a compile-time overhead, a syntactic overhead and lack of composition. In Lisp, there is negligible compile-time or syntactic overhead, but the other points apply and newbies generally reach for macros too soon when very few macros tend to be defined in expert Lisper projects. I think generally people don't frown upon simple boilerplate generation, e.g. making lenses, making database entities, deriving instances in general. Things like that.
"Old-style" C++? Is it just because of the `printf`s, or is there a newer, better way to do the template part?
Chicken and egg problem.
Unfortunately not, but I will try to give some real-life examples. Do you need something that compiles or would just seeing the test code be satisfactory? The trouble is that we usually operate within ErrorT, not pure IO, and we have a wrapper for that which is not yet published as a part of the library. I will try to bring some examples tomorrow (it's 1am in GMT+1)
I'm having trouble working with the GADT example and the SomeC newtype. I can't seem to make any invocation typecheck. Here is my failed attempt: runSomeC (SomeC (\f g h -&gt; g 'b')) (+3) succ id What would be the correct way to use this newtype?
In addition to /u/chrisdoner's answer, I also want to mention [this Stack Overflow question and answer](http://stackoverflow.com/questions/10857030/whats-so-bad-about-template-haskell) about the disadvantages of Template Haskell.
This CSS will fix it: pre { overflow-x: auto; }
Towards the end, Robin mentions that he has GHC on his phone. I'd be quite curious to hear more about his setup, since I write a lot of Haskell code on my Android, but I don't currently have a way to compile it.
Thanks for the great response! Is there a good non-TH solution to specifying that a computation be done once, or at compile time? My use case is for using Elm (a compile to JS language) in a web-server. It's extremely inefficient to re-compile my Elm into JS every time it gets served. My current solution is to use TH to compile it into JS Text once at compile time, then serve that. It means that the compilation of my Elm happens whenever I compile my Haskell, and that it only happens once. Is there an obviously better way to do this, or would this fall under "simple boilerplate generation" that isn't frowned upon?
Don't worry about it, there's a fuzzy distinction between the terms. Something that is formally documented in an organized fashion is enough to be educational to teach both the thought process of using the 'construction' as well as retaining the essence of it. Code that is documented well teaches the reader how to think in terms of the language. I just wanted to see if any Haskeller's here have stumbled upon such a thing. I've oddly enough, found this characteristic in Java, lisp, and Perl. It's more a way of thinking than it is merely code. It affects the entire way in which you view and interact with the world. I'm personally looking for something like this to serve as a companion in teaching myself Saunders Mac Lane's "categories for the working mathematician". It's not so much the specifics of what the library does, as it is a tool for validating my own understanding, assuming biasing myself to one source of material may accidentally allow for self enforcement of misunderstanding and misapplication. As for wants: there are people who scan code to get something to work, people who browse documentation to learn new features (or short cuts), people who browse tutorials to learn new ways of approaching a problem conceptually, or tutorials to learn new aesthetics (different languages - which can certainly lead to creative combinations, it is not entirely superficial in terms of terseness or verbosity), or explanations that connect things to other domains (or connect things to a base domain), or things that attempt to unify understanding across all domains, or things that attempt to add new things onto an existing domain while managing to ensure that thing is actually new. But that's just my own experience. You might benefit from phrasing a post less verbosely than I have, or a survey, with key words that people pick up on. That's another thing, word choice. Standardizing a selection of words to use for explanations allows people to move from language to language. Standardizing a format allows for retention of the concepts. But at this point, we may as well invent another programming language depending on how strict we want to be - or formalize documentation rigidly, but that would require more thorough analysis on what constitutes effective documentation and what is fluff. 
That's awesome =)
no. Haskell cannot be used to build programs like web browsers, operating systems, or rdbms, as its garbage collection (by design) is compounded by pervasive space leaks. for very large programs, c/c++ are still the best solutions. rust may be a solution for these types of programs in the future, Haskell never will be (and no, ghc is not a valid responses, no one cares if it takes 1 or 5 seconds to compile a file)
cross compiling, but these end up being proofs of concept.... if you want to make money in mobile, bite the bullet and adopt the blessed tools
5) I patched GHC 7.10 so that you can use -ddump-splice -ddump-to-file. I think every problem with TH is exacerbated by the fact that it is difficult to see what is generated, so I actually view that as the biggest issue.
[detexify](http://detexify.kirelabs.org/classify.html) has the backend written in Haskell. Yesod's website is written in Yesod. Haskellers too. There is the [Hakyll](http://jaspervdj.be/hakyll/) static blog generator that's been used by at least 50 people. Search and you will find, troll and you'll remain in the unknown.
I never claimed anything about systems programming, where did you take this idea from? 
There is no silver bullet.
nor did I, one of my suggested examples was a web browser this isn't far fetched, a rendering engine is the first goal of the rust project
I'm pretty sure that JS uses a weird array representation that is not actually O(1) read/write.
Containers, unordered-containers, vector, and array are definitely the "big ones" in terms of general-purpose data structure libraries. Oh, and lists of course.
The inline image URIs are all wrong: they point to a GitHub page with metadata about the image, instead of the raw image data itself. E.g. https://github.com/seanwestfall/templatehaskell/blob/master/syntax_tree_bool.png should instead point to https://raw.githubusercontent.com/seanwestfall/templatehaskell/master/syntax_tree_bool.png
How not? Of course it is. There is a reason it is so fast. Edit: downvotes, seriously? It is O(1). [Source.](http://www.cplusplus.com/reference/vector/vector/operator[]/) To explain, 1. this is about writing, not appending. Writing to the memory is O(1) by definition since it requires only one asm op. 2. even appending is still O(1), and that accounts for resizing, since resizing requires, in average, "N" elements to be copied for each "N" appends. N/N = 1.
Seriously. What a worthless post. I got all excited for nothing :(
Interesting! Is this mostly shaving yaks or do you have a nice blog post on this topic?
Honestly, it sounds a little crazy to ask for compile-time code generation for this. What you want is essentially a file, which is served from your web server. Why not do exactly that? At server startup, or at deployment time, invoke the Elm compiler, and store the result in a file. Serve the content from that file.
How are Python arrays O(1) insertion? I believe they auto-resize, and AFAIK the best algorithms we have for that are O(log n), unless you are willing to sacrifice lookup performance. Overwriting values in-place obviously has the same complexity as reading, but that's not what people generally mean when they talk about write performance.
As I said above, it is O(1). It is just a write to the memory. And even accounting for resizing, it is still O(1) in amortized time.
Are those the same as *Indexed monads*, or how these are different?
I don't see how you can compute amortized time that way. Let's say you have a 100MB vector, and you need to append a single element at the end. You can have reallocation, which will result in the whole vector being copied for a single write. And this can happen for every subsequent write, giving you O(n^2 ) performance ... 
[XMonad](http://en.wikipedia.org/wiki/Xmonad) is a graphic application, written in Haskell, that I'm quite happy to use. [Pandoc](http://johnmacfarlane.net/pandoc/) is my preferred markup conversion toolkit -- and I think there is a large community using it. I used [darcs](http://en.wikipedia.org/wiki/Darcs) as my preferred decentralized version control for years. It has unique ideas that makes it a pleasure to use, some youth issues, but sadly it lost steam after losing the popularity war against Mercurial and Git and grew up increasingly isolated in the ecosystem. I don't think any of the software I use daily is implemented in Java -- I use Libreoffice sometime but that's about it. Certainly none of it is implemented in C#. (Javascript, I suspect, cannot be avoided.) I think your assumptions may be overly restrictive of what people do with their computers.
That would be great if you could! Thanks!
How would this work for Haskell since it only has the [Hask](https://en.wikibooks.org/wiki/Haskell/Category_theory#Hask.2C_the_Haskell_category) category?
These non-endofunctors are non-*endo* in the sense that their domain is a subcategory of `Hask` (demarcated by a typeclass) but their result might not be in that same subcategory. For example, consider something like `Set`; there is nothing really that necessitates an `instance Ord a =&gt; Ord (Set a)` so `Set a` might not be in `Ord`. So `Set` is not an endofunctor since it goes from `Ord` to `Hask`, not to `Ord`.
Indexed monads are something else entirely. An indexed monad has kind `M :: k -&gt; k -&gt; * -&gt; *`, but for any indices `i` and `j`, `M i j` is a functor from `Hask` to `Hask`, i.e. an endofunctor. An indexed monad is useful for restricting the transitions you can do with a bind; but it, by itself, doesn't restrict the domain of the monad as a functor.
One reason why "Hask" is a badly chosen name is that it draws people towards the false conclusion that Haskell has only the Hask category. There is plenty of other categorical structure in the area, and we can certainly construct relative monads. A favourite example of mine (despite the fact that it's in my thesis) is the de Bruijn indexed lambda terms over n variables. data Nat = Zero | Suc data Fin :: Nat -&gt; * where -- each Fin n is finite, of size n Fz :: Fin (Suc n) Fs :: Fin n -&gt; Fin (Suc n) data Term :: Nat -&gt; * where Var :: Fin n -&gt; Term n (:$) :: Term n -&gt; Term n -&gt; Term n Lam :: Term (Suc n) -&gt; Term n Each of `Fin` and `Term` induces a category whose objects are numbers and whose `m -&gt; n` morphisms are, respectively, functions in `Fin m -&gt; Fin n` and in `Term m -&gt; Term n`. Moreover, there is a functor between these two categories: it's the identity on objects, but it takes a map between `Fin` sets and uses it to renumber the free variables of a `Term`. It's not an endofunctor. But it is a "relative monad", and you get a perfect sensible Kleisli structure with arrows `Fin m -&gt; Term n` which represent simultaneous substitutions, and a `&gt;&gt;=` operator which delivers their action on terms. Crucial to the construction is the `Suc` endofunctor on `Fin` whose action on some renumbering `f` maps `Fz` to `Fz` and `Fs i` to `Fs (f i)`. That is, `Suc` is for `Fin` what `Maybe` is for types-and-functions. Correspondingly, it is indeed a monad, just not in the category which allows it to be recognized as a `Monad`. Once you learn to look beyond types-and-functions, you'll see structure everywhere, in all shapes and sizes.
`Set` is not a monad because it isn't an endofunctor. But can it be shown that `Set` is or is not a relative monad (since it seems to be a forgetful functor)? 
Every time I think I'm getting the hang of functional programming and the surrounding theory an article like this comes out and I'm once again forced to hang my head in shame.
Thanks for the notifications that the formatting was off. I've republished this post and I think it's come out a lot better this time.
Haha, I read the parent commenter's post and thought "only has Hask?" I think Conor would say otherwise. Scrolled down. Suc (Suc (Suc (Suc Zero))) post would scroll again.
The simple solution to that is to not allow an optional argument before a non optional argument, but that would cause a world of problems when combining functions. Another option would be some sort of named tuple type with a variable arity so the function foo (a,b,bar=c) c, foo (a,b) c and foo a c all being acceptable
There is a project for a rendering engine, though this kind of thing involves lots of things and take huge amounts of work no matter the language being used. 
I note [the Python wiki](https://wiki.python.org/moin/TimeComplexity) does say list append is O(1) in CPython ... doesn't specify how it achieves that, though. IIRC, if you grow the allocated memory exponentially, you only have to copy O(n) elements to do n appends - so that'd be one way of getting amortized O(1) append. Consider a mutable vector representation which allocated space in powers-of-two: whenever an append puts the length past 2^p for some integer p, you have to copy 2^p elements... so after 2^p appends you'd have done a total of 1+2+4+...+2^p = 2^(p+1)-1 copies; ie, after n appends you've done no more than 2n-1 copy operations - and 2n-1 is O(n).
Given JS arrays can have an arbitrary mixture of string and sparse integer keys, I imagine most implementations use hash tables under-the-hood. I've noticed people seem to like saying hash tables have O(1) read/write, even though I understand it's actually O(n) in the most pathological of worst-cases... I suppose that's because the probability of that case (every key in the same bucket) actually happening is effectively zero.
Thanks, Herbert! &gt; If you happen to run into compile errors for Hackage packages which claim to work with GHC 7.10.1 (e.g. by allowing `base-4.8`), I'd be interested to hear about it. Alex specifies `build-depends: base &lt; 5` and it fails with missing Applicative instance. It's fixed in master, but hackage version is one year old and still doesn't have that instance.
`Set` is a relative monad from the category of `Ord`-preserving functions* to `Hask`, which can trivially be seen by implementing it in the obvious way. *can technically be done even if the functions only have to preserve equality, but that will yield worse performance.
It's not really a proof, it's a generalization of a definition.
Sweet! That's terrific. I'd be interested to see how more complicated examples look and feel with the servant+ghcjs combo. By the way, you should drop by **#servant** on freenode one of these days. If you end up implementing complete support for ghcjs client functions generation, I (and probably my two collaborators too) would really like to make it a first class citizen of the servant ecosystem, since ghcjs is pretty much ready for prime time now.
I will drop by! I think with with a bit more love this code can be merged back into `servant-client` and it will support both ghc and ghcjs :) 
On the topic of cross-compiling for ARM? There's information on building a cross-compiler from GHC on the trac wiki.
Auto resizing arrays are worst case O(n) not O(log n), but they are amortized O(1) for a sequence of inserts at the end.
Haskell is a research language so expect that to happen often. If you use ocaml you are less likely to have your head explode each time an article is published.
Is this the cabal-install which now works out of the box with ghcjs? 
server startup time is not a good time to find out that your code does not compile.
[These appear to be the release notes](https://github.com/ghc/ghc/blob/ghc-7.10/docs/users_guide/7.10.1-notes.xml). In XML format, but readable with a bit of effort.
In Yesod we do this sometimes to have a development mode where the files are always getting re-compiled as they change and a production mode where the files are compiled once during the compilation process. You could instead do this with a file watcher and in your production build process, but doing it in Haskell lets us have a type-safe reference to the static file.
Does that yield a source file that ghc can compile without TH?
Not that I know of. But then again, servant is (arguably) only a couple of weeks old, so hopefully in due time!
Are we going to get builds for trusty? :-)
What would be the advantage of using a 'purely functional' alternative ? What's wrong with the mutable vector from your point of view ?
Well, it is much more complicated to write non-pure code in Haskell. I have to use monads all the time, the code gets bloated and I can't get all the benefits of purely functional values that we all know so well.
Edison is worth a mention. It has many instances of a single type class and documents the asymptotic run time for each method of each instance.
For the common type classes, there's an excellent Monad Reader article. But, I'm not exactly sure what would be the "words" in a "Haskell dictionary". You have read the 2010 report, right?
Thank you. I found the appropriate wiki page. Currently making my way using crosstool-ng. Happy I shouldn't need to emulate anything.
IIRC, [Yes](https://plus.google.com/116476108003497242388/posts/H3yTkHgMzcW); you can set up an ARM qemu instance on an amd64 system (I think you can even have it as a "chroot") and use the fast remote servers to run the ARM code. No cross compiling required, if you already have GHC and cabal for ARM. 
Oh, great! I think a chroot solution with qemu would be lower complexity after initial setup than a cross-compiler, which definitely is a plus. Definitely worth considering -- thank you! edit: As an after-thought, I do believe cross-compiling will be faster than using qemu, no?
&gt; I can't get all the benefits of purely functional values that we all know so well. Haskell makes it easy to mix non-monadic code and monadic code. For the parts that need the speed of mvec, you use ST/IO. For the parts that look much better without a `Monad` decorating their type, avoid them. Use return / fmap / lift to inject your non-monadic values into the monadic context. You've got to do this at least once to bind to `main` anyway.
&gt; For the common type classes, there's an excellent Monad Reader article. Thanks. &gt; But, I'm not exactly sure what would be the "words" in a "Haskell dictionary". You have read the 2010 report, right? It's not the greatest analogy. I probably should just get better at reading context-free grammars more fluently. 
With [cuckoo hashing](http://en.wikipedia.org/wiki/Cuckoo_hashing), you can achieve O(1) (worst case) read and O(1) (expected amortized) write complexity.
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Cuckoo hashing**](https://en.wikipedia.org/wiki/Cuckoo%20hashing): [](#sfw) --- &gt; &gt;__Cuckoo hashing__ is a scheme in [computer programming](https://en.wikipedia.org/wiki/Computer_programming) for resolving [hash collisions](https://en.wikipedia.org/wiki/Hash_collision) of values of [hash functions](https://en.wikipedia.org/wiki/Hash_function) in a [table](https://en.wikipedia.org/wiki/Hash_table), with [worst-case](https://en.wikipedia.org/wiki/Worst_case_analysis) [constant](https://en.wikipedia.org/wiki/Constant_time) lookup time. The name derives from the behavior of some species of [cuckoo](https://en.wikipedia.org/wiki/Cuckoo), where the cuckoo chick pushes the other eggs or young out of the nest when it hatches; analogously, inserting a new key into a cuckoo hashing table may push an older key to a different location in the table. &gt;==== &gt;[**Image**](https://i.imgur.com/aUmerJP.png) [^(i)](https://commons.wikimedia.org/wiki/File:Cuckoo.svg) - *Cuckoo hashing example. The arrows show the alternative location of each key. A new item would be inserted in the location of A by moving A to its alternative location, currently occupied by B, and moving B to its alternative location which is currently vacant. Insertion of a new item in the location of H would not succeed: Since H is part of a cycle \(together with W\), the new item would get kicked out again.* --- ^Interesting: [^Hash ^table](https://en.wikipedia.org/wiki/Hash_table) ^| [^Michael ^Mitzenmacher](https://en.wikipedia.org/wiki/Michael_Mitzenmacher) ^| [^Open ^addressing](https://en.wikipedia.org/wiki/Open_addressing) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cn3fa16) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cn3fa16)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
Happy holidays! :)
First time I've seen the vertical-bar-brackets for meaning the objects in a category; I'm more used to Ob(C). Overall, seems like a pretty natural extension; I didn't see any assumptions that make me think the collection of things they are calling "relative monads" is too small. That the Kleiski and E-M constructions are still available for "relative monads" is a very nice result, but it far too technical for me to check it. Might it be that in the future we'll just call all the objects in the new collection monads, and begin using the endo- qualifier on what we current call monads?
Maybe you could use a CI service like Travis CI that supports Mac and Linux? I think Circle CI might work as well (its Mac support is in beta though). I've seen several cross platform open source projects use Travis to test on Mac and Linux, so you'd just need to upload the artifacts to s3 after building or something. I don't think either of those support Windows, though, if that's what you're looking for. 
None of the code you posted is complicated. Do you mean it gets tainted by IO? I see you benchmark ST but have you tried using it to get mutation in your pure code?
Cross-compiling has a higher initial investment, I believe. However, once you've got the amd64-to-ARM cross-compiler producing good packages, yes it will run faster than emulating the ARM toolchain via qemu. You might still want to have the ARM qemu chroot around though; you can run your ARM test code in your cross-compiled packages there. They may run faster on the amd64 hardware than on actual ARM hardware.
32 bit windows is missing - is that intentional?
Was about to upvote you, than I saw the whining at the end.. Uniqueness/linear types would be nice indeed.
hrm, i guess the fixes to prefetch need to be added to the change log
Just checked, ghci-ng still works with this.
That sounds like good advice, thank you! &gt; I don't think you can compare 600 pages of a well-written book and 600 pages of papers put together. Papers tend to be much denser (because of page limits) I agree and thought so even as I wrote it. But even if you take some appropriate "conversion factor" (e.g. consider amount of time to properly digest instead), it's probably still a lot.
so, now we are at the point when having an updated [minghc](https://github.com/snoyberg/minghc) would bring a lot of value to all Windows users. Thanks @snoyberg and @ndmitchell.
We're in discussions with GHC HQ about this. MinGHC is 32 bit only. This release seems to be 64 bit only (either by policy, or the files haven't been uploaded yet, or they have the wrong name). As soon as we can get our hands on 32 bit, we'll update. We're also looking at doing the same for GHC 7.10 RC1.
Don't confuse someone else's pet research project with your own ignorance. Monads have about half of century worth of pedigree. To contrast, "relative monads" were submtited to arXiv yesterday. To think you have to be responsible for every new finding is akin to thinking you should be well-versed in the implementation of every project on Github. You don't owe anyone that. You should be intimately knowledgeable with one or two things, have a working understanding of a few dozen more, and recognize the names of the ones people tell you are important. Also, this is not "functional programming". This is theoretical computer science. Learning what's here will hardly make you a better programmer. EDIT: Just to clarify, I don't mean to dismiss the work linked here. I haven't read it. My point was more about the expectations of how much someone should understand. Knowing even the Yoneda lemma is probably far more theory than anyone strictly needs to know to program proficiently in a functional language.
Arrays. I'd recommend the ST variety : http://hackage.haskell.org/package/array-0.2.0.0/docs/Data-Array-ST.html I think IntMap also gets pretty good performance, and it's not inherently monadic. http://hackage.haskell.org/package/containers-0.2.0.1/docs/Data-IntMap.html
What languages do support linear types?
[Rust](http://www.rust-lang.org/) and [ATS](http://www.ats-lang.org/) as far as I know.
I agree. What makes Haskell so great is how easy it is to customize. Many features of other languages are just libraries in Haskell. The same goes for any library as it does for relative monads: if you don't like it, don't import it. I'm not a math guy, so until there are nice libraries which rely on my types being instances of relative monad, I'll stick with using the features of libraries surrounding more useful (yet mathy) sounding typeclasses like "applicative functor" or "monoid" or whatnot. EDIT: Oh, right. This paper is purely about math. Parts of it can be implemented in Haskell, but there really isn't much reason for me or any other non-mathy Haskeller to care right now until other people do the work for me XD
All you do is load data once in your test code, this is hardly a refutation of his point in the general sense.
The imperative version does the same. Edit: I believe I misunderstood what you were trying to say, that just because this one example is easy to implement efficiently in purely functional style doesn't mean that the same holds in general. I don't mean to imply otherwise, but I do mean to refute the exact opposite point, claimed by julesjacobs, that you must resort to imperative style to reach the peak of efficiency.
I would love to know more about your Leksah issue. Can you please file it [here](https://github.com/leksah/leksah/issues)? Were you using the latest version (currently [0.14.3](https://github.com/leksah/leksah/wiki/Leksah-0.14.3))? What version of GHC did you have in your PATH (also was it 32bit or 64bit)? When you say "right pane" I presume that would be the Modules pane. That should be populated when the metadata is collected successfully (it might be failing for some reason). Does the new package show up in the "Workspace" pane? Did it create any files in the directory you selected? If you choose file Package -&gt; Build from the menu, do any errors show up in the log Pane?
:D
Why not just use Data.Map with some integral index? Sure, it's technically not O(1) insertion and deletion, but if you study log functions, you'll find that the difference between O(log(n)) and O(1) is pure pedantism. Also, reading through a script requires regular jumps to maps by the interpreter. While using a map might seem like an overkillish solution, the worst case for using a map in a compiled binary is probably going to be about the same as the best case for using a regular-old resizing array with a plaintext script. Anyways, to *really* answer your question: no, Haskell's standard libraries do not have anything with the exact interface which you are requesting, which is rather unfortunate. Still, I'm just gonna say that what you're describing is **not** a silver bullet, since it derps a bit when you want to remove elements or insert them between other elements.
[idris](https://github.com/idris-lang/Idris-dev/wiki/Uniqueness-Types)
I suggest trying Eclipse with EclipseFP. I've used it successfully on Windows.
Thanks for your reply. I am cleaning up and removing redundant software at the moment, will be removing all Haskell-related products, re-boot, re-install the most recent versions while taking notes and will get back to you with answers, notes and results when done.
I'm not sure what it means by using ST to get "mutation in a pure code". Suppose I'm writing a type, Matrix2D, which does what the name says. I want it to be fast, so I use a mutable unboxed vector internally. Is it possible that the interface of my Matrix2D is completely pure?
I'm just wondering in a brainstorm-esque fashion, wouldn't it be possible for GHC to detect when a value isn't used anymore, and perform an in-place mutation instead of keeping the old value? For example: foo arr = set 0 0 (set 1 1 (set 2 2 arr))) main = print $ arr Notice that `arr`, `(set 1 1 (set 2 2 arr))`, `(set 2 2 arr)` are values that are only used once on this program. Can't GHC perform an analysis for that and reuse the same memory?
How do you redesign, for example, a matrix 2d in which you are going to render the screen of a game at 60FPS to avoid mutation? I'll take it as this is a case where pure structures just don't work, then.
I don't know, but I've never needed to accomplish that task. I have done some very light OpenGL stuff and I didn't need mutation directly. But, I was able to simply use the gl/glu/glut operations for manipulation the V/P/M matrices. Depending on what you are doing with the matrix, mutation operations will actually be overshadowed by arithmetic operations. For example, multiplying two 4x4 matrices takes ~64 arithmetic operations but writing the result only takes 16 mutation operations. In this case, your mutation benchmark makes less sense than a numeric benchmark. Taking the dot product of two 4D vectors take ~8 arithmetic operations, but only a single mutation. But, sure, direct mutation in ST/IO may be the best way to accomplish a task; they weren't added to Haskell for no reason.
&gt; I'm not sure what it means by using ST to get "mutation in a pure code". [runST](https://hackage.haskell.org/package/base-4.3.1.0/docs/Control-Monad-ST.html#v:runST) can exit the ST monad and give you a pure value. That is you can have a function that uses ST internally, but ST does not appear in it's type. &gt; I want it to be fast, so I use a mutable unboxed vector internally. Is it possible that the interface of my Matrix2D is completely pure? I *think* so, but I'm not completely sure. You also may need some GHC extensions to hide the necessary `forall`s.
I don't see any whining.
Is using Emacs not at all an option?
I have now un-installed all Haskell-related software (and others), ran ccleaner, rebooted and installed: - HaskellPlatform-2014.2.0.0-x86_64-setup.exe: normal completion, no error reported. - leksah-0.14.3.0-ghc-7.8.3.msi: normal completion, no error reported. Starting Leksah: - GUI window opens: contrary to the first time I tried the program, this time there is no meta-data window asking for details. Might the uninstall program have failed to remove everything? The main window is what opens up. - Console window opens behind it. It goes through various command-line processes. Unlike the first time I installed the program, this console process is taking much longer than it did the first time. Minutes pass... Several warnings of deprecated features scroll by, also many warnings that "could not find link destinations for" this and that and "could not find link destinations for" one thing and another... Going to the kitchen to make coffee. Back at my desk watching commands scroll by. This is clearly not a typical Windows program. What is it doing, compiling itself? While this continues, let me answer some of your questions... --- &gt; I would love to know more about your Leksah issue. Can you please file it here? Will do if the same problem re-occurs. &gt; Were you using the latest version (currently 0.14.3)? Yes. &gt; What version of GHC did you have in your PATH (also was it 32bit or 64bit)? The only installed version, created from HaskellPlatform-2014.2.0.0-x86_64-setup.exe --- Ok, installation has now stalled. Two message windows have appeared. One reports that "leksah-server.exe has stopped working - Windows is collecting more information about the problem. This might take several minutes..." and the other that opened on top of it asked "Do you want to send more information about the problem?" Details are: Files that help describe the problem: C:\Users\Anyone\AppData\Local\Temp\WERFD71.tmp.WERInternalMetadata.xml C:\Users\Anyone\AppData\Local\Temp\WER1018.tmp.appcompat.txt C:\Users\Anyone\AppData\Local\Temp\WER10D4.tmp.hdmp I don't know of the files in question would be useful to you as well. I click the "Send information" button. The remaining window only has one button to "Close program", which I click. The command window has now stalled. The last commands reported were: Warning: Distribution.Simple.UserHooks: could not find link destinations for: GHC.Types.Bool GHC.Types.IO Data.Maybe.Maybe GHC.Base.String Warning: Distribution.Simple.Bench: could not find link destinations for: GHC.Types.IO Warning: Distribution.Simple.Test: could not find link destinations for: GHC.Types.IO Warning: Distribution.Simple: could not find link destinations for: GHC.Types.IO GHC.Base.String GHC.Types.Bool Data.Maybe.Maybe GHC.IO.FilePath 78 leksah.exe: &lt;socket: 700&gt;: hGetLine: failed (Unknown error) The main Leksah window is still open. I will try to use it. I click menu option "Workspace | New" to create a new workspace named "testWorkspace" in directory c:\development\Haskell\. File "testWorkspace.lkshw" is created. Now I select "Package | New" and enter "testPackage" for the name. The pre-filled directory field says "\Development\Haskell\" and the "Existing package to copy" shows "hello". I click the "Create Package" button. Unlike the first time I used the program, the workspace pane now shows the testPackage.cabal and a directory has been created. This is progress. Holy crap, I am just noticing now that, while I was using the main window, the command window has done more work after reporting failure earlier. GLib warnings, Gtk warnings, Gtk-CRITICAL with more failures... What the heck is going on? I'm not sure if I have a usable system or not at this point. 
If you aren't willing to go the Emacs/Vim route IntelliJ also has some Haskell plugins that you can use.
I am testing it now. Installation was immensely simpler than Leksah: just extract the directory. I installed eclipseFP and created a project in a couple of minutes. This looks promising. But now, the console pane is showing a long series of commands being executed, probably going through similar steps as what Leksah does in a separate command window, but with less warnings and no error so far. I will let it finish.
&gt;We plan to make the 7.10.1 release sometime in February of 2015. Looks like i'll skip this one. 
The best IDE for windows i know so far is VirtualBox. 
This construction seems interesting but scanning the paper I could only find two examples: Example 1.1 about finite dimensional vector spaces and the discussion of arrows in Section 5. I haven't looked much at the discussion on arrows, but I'm not convinced that anything has been really gained in the example at the beginning. They claim that the monadic structure of finite dimensional vector spaces can't be made to be a monad on Set because of needing to sum over an arbitrary index set. But this problem only arises because they wrote down Vec m as functions J_f m -&gt; R. For infinite sets, this doesn't correspond to the free module construction, and so it shouldn't be surprising that the monadic structure goes away. The free module construction for arbitrary sets should give you the following definition for Vec m (here, m is in Set so there is no J_f). An element of Vec m is a finite subset m' of m, and a function m' -&gt; R \ 0 (I exclude 0 here so that each element has exactly one representation). Then it's easy to define the function (m -&gt; Vec n) -&gt; (Vec m -&gt; Vec n) because any element of Vec m only has a finite number of basis elements composing it, and we can sum over finite sets. It just so happens that for finite sets, there is no difference between the two constructions.
I tried twice to use it, both times with this result: Language\Haskell\GhcMod\Monad.hs:370:5: Wrong category of family instance; declaration was for a type synonym In the newtype instance declaration for `StM' In the instance declaration for `MonadBaseControl IO (GhcModT m)' Failed to install ghc-mod-5.2.1.1 cabal: Error: some packages failed to install: ghc-mod-5.2.1.1 failed during the building phase. The exception was: ExitFailure 1 
I'm looking for an IDE. Isn't Emacs an editor?
One of its dependencies was included without the upper bound made a breaking change. Try: cabal install --constraint="monad-control&lt;1" ghc-mod
Not defending /u/julesjacobs's edit; but consider this: a comment which calls your attention to a misunderstanding, allowing you to address it and explain why it's incorrect, *is a net positive.* Might not deserve an upvote, but certainly not a downvote.
To be fair, I downvoted when there were no such responses and didn't have time to write one of my own. I have since removed the downvote (but have not upvoted). My actions were solely to reduce the damage of misinformation.
That did it, thanks. EDIT: Working nicely now, thanks again.
Well, it's a extensively-customisable environment, of which editor functionality is but one part. Many people set up Emacs to provide most (if not all) of the functionality provided by IDEs for various programming languages. With regards to Haskell in particular, check out [this blog post on Haskell development in Emacs](http://tim.dysinger.net/posts/2014-02-18-haskell-with-emacs.html). (i don't use `el-get` myself, and instead just use the package manager built-in to Emacs 24 to browse the [MELPA package repository](http://melpa.org) and install packages from there - try filtering the list of MELPA packages via the term 'haskell'.) One of the many advantages to using Emacs in this way is that you don't need to continually swap between different IDEs for different dev enviroments: you can use the same application for programming in Haskell, Idris, OCaml, Clojure, Python etc.
In the Haskell world, I'd say applicative functors and monoids are vetted enough that you probably want to consider learning about them. I think monoids especially should be in the vocabulary of *any* programmer. They are about the simplest concept you can define in abstract algebra. But moreover, any time you have a state machine which accepts inputs (and tell me if that doesn't sound like literally anything you do with a computer), then the set of all inputs forms a monoid. 
[Clean](http://en.m.wikipedia.org/wiki/Clean_(programming_language\)) has uniqueness types, which is close enough.
I'd suggest you upgrade. We've probably broken a lot of stuff with the 7.10 release due to a number of API changes, so I'd say you're probably at least 3+ months off from a well supported release anyway. The release date is quite different from the date of general usability. On the other hand 7.8.4 should be quite stable at this point with nearly 3 dozen extra bug fixes over 7.8.3.
The file names are correct. My windows build bot was just being a piece of junk and I had a flight to catch today. I'll get around to uploading 32 bit binaries sometime next week probably.
IIRC cuckoo hashing ends up being worse than some form of sequential probing in practice becuase every rehash is almost garunteed to cause a cache miss. 
I use EFP and it was a train wreck at first. It got into a bugged out state that I couldn't fix and Eclipse doesn't have a way to fix/delete broken plugins. It has a half-assed "uninstall" which doesn't do much because a reinstall doesn't work. So I have to use a separate Eclipse for Haskell now because I don't want to lose all my existing plugins/settings. You said you've already installed it, but my recommendation would be to make sure your Cabal version is at least 1.20 so you can use the sandbox! That was my original problem. Their buildwrapper/scionbrower/hoogle libraries wouldn't install with 1.18 without breaking a ton of packages.
Also, even though it works now, the source is frequently out of sync with what the code tools have parsed. It's infuriating. It'll tell me there are type errors and when I hover over the code it gives me code/types for code I already changed. I end up having to "select all, copy, delete, save, paste, save" to fix it. And I'm not a fan of having to add packages to the cabal file in order to import libraries. Still, I'm grateful for what it offers.
I'd recommend keeping a separate installation of Eclipse anyway. Nothing good comes about from having uneeded plugins installed.
Please consider logging an issue and posting a link here. I think it will be easier for people to find in the future than the reddit thread. It sounds like the metadata collection is failing. Check list for this is * Make sure Leksah was built with the same version of GHC that is in the PATH? (it was) * Check to see if `chk-pkg check` contain any errors? If so run `ghc-pkg recache` (may need to be run as administrator). * If it still does not show metadata then check the ~/.leksah-0.14/collectSystem.report file for clues. * If that does not show anything that hints as to what might be failing. Run Leksah with --verbosity=DEBUG (or just leksah-server with --verbosity=DEBUG before starting leksah) &gt; GUI window opens: contrary to the first time I tried the program, this time there is no meta-data window asking for details. Might the uninstall program have failed to remove everything? Uninstalling does not remove the .leksah-0.14 directory from your home directory. This contains configuration and metadata. Leksah only stores information in that directory, so if you want a clean Leksah install just delete it and it should take you back to square one. &gt; One reports that "leksah-server.exe has stopped working - Windows is collecting more information about the problem This dialog does come up when leksah-server terminates and that is a known issue. But leksah-server should only be terminating when you are closing leksah. The early exit could be caused by the `ghc-pkg recache` problem. &gt; GLib warnings, Gtk warnings, Gtk-CRITICAL with more failures... What the heck is going on? These are unlikely to be related to the missing metadata (although they do need to be fixed at some point). 
It is still just -ddump-splices. So you would need to remove stuff from it. I am told the output of the splices is better now, but in the past it wasn't always valid Haskell. If you find a bug with respect to that, let me know and I will look at fixing it. I have another patch though that is a little more what you have in mind, but it did not get merged into 7.10: https://phabricator.haskell.org/D518. I am going to give a shot at creating what you are actually interested in since I am not too far off from it now.
I'll echo on "Q Exp" point. Compile time errors with TH are tantamount to working with a language that just "oops" when something goes wrong. You really need to understand the TH types in order to work with it effectively otherwise things can go very wrong very quickly.
Hopefully someone can make GHC cross compiler work. Linux is already the best place to go for open source Windows binaries. I use Fedora, but I believe SUSE is good too. Just type `yum install mingw64-` press tab and prepare to be amazed! Stuff that would take hours to build or install on Windows is all there. Not only that they are bang up to date (official Gtk+ binaries for Windows are 3.6 and Fedora ones are 3.14). Want to build something from source? Just follow the Linux build instructions, but use the mingw64-configure wrapper script instead of configure. You might still need Windows for testing, but that is the same with any legacy OS you want to support.
ATS, Rust, Clean, and to some extent C++ (you can create non-copyable types in C++, but afaik you can't invalidate existing variables).
Ehh, what you did is *remove the actual benchmark*. No surprise that it's fast. If you do the same for list then it will be fast as well.
In theory this is possible, yes. There are some difficulties: You actually want to allow a value to be used multiple times for reading, and then one time for writing. Consider the following program: v = some vector x = get v 3 y = get v 5 v' = set v i a This program can be implemented using a mutable vector. However in Haskell, there is no guarantee that the get operations will be executed before the set operations. Evaluation order starts to matter. Secondly, what if you pass the vector to some function: v = some vector' x = f v y = g v v' = h v How will the compiler know that f,g are read operations, and h is a write operation? You need an interprocedural analysis. Thirdly, what about tree structures? Consider this program: v = some vector v1 = set v 3 3 v2 = set v 4 5 v1' = set v1 i a Can we implement the `set v1 i a` update with mutation, or do we need to make a copy? With flat vectors we can use mutation: the `v1 = set v 3 3` operation copied the entire vector into a new vector, so there's only one reference to it. But what if the vector is implemented using a tree data structure under the hood? Then v1 and v2 may well share structure internally, so the `set v1 i a` can't be implemented using mutation, because that might mutate something that is shared with v2. Fourthly, given a persistent update, how do you get an efficient mutable update? For flat vectors we know that a functional update copies the vector and then sets the given index in the copy. For a tree structure the update usually copies the path down to the node that we want to update. It's not clear how the compiler could automatically derive an efficient mutable version that does not do the path copying. You could require the programmer to provide two versions of each function, a functional one and an in-place one. How do you write the in-place version in Haskell, which is pure? Fifthly, how do you make all this reliable? You want predictable performance. You don't want to have to pray to the compiler gods that your code will optimize, or be 100x slower if you're unlucky. If your code needs to be fast then if it runs 100x slower then it's a bug. You don't want the correctness of your code to depend on magic compiler optimizations that may or may not fire depending on a seemingly unimportant edit to your program. The 'performance correctness' should be evident from the program text independent of compiler optimizations that are not specified in the language spec. This is already a problem in Haskell with other optimizations... Perhaps all of these problems can be solved, but I don't think it will be easy. Furthermore, a linear type system is useful for other things too, such as safe manual memory management, safe concurrency, and access to external resources such as files and databases. So I think that actual linear types provide a better power to weight ratio.
No. The version with mutation does the same thing. It just fills in an uninitialized array. 
Clearly that's not the spirit of the benchmark since the author wrote this: seq_empty = Seq.fromList [0..(size-1)] seq_test size = List.foldl' (\a i -&gt; Seq.update i i a) seq_empty (enumFromTo 0 (size-1)) And not this: seq_test size = Seq.fromList [0..(size-1)]
&gt; And I'm not a fan of having to add packages to the cabal file in order to import libraries. Leksah has the same problem. I have logged an [issue](https://github.com/haskell/cabal/issues/2299) for cabal that might help. In the short term you could try writing a wrapper script that removes `-hide-all-packages` from the list of arguments passed to ghc. Then use `cabal configure --with-ghc=yourscript`, but remember to switch it off and add the `build-deps:` before sharing the `.cabal` file.
That is better but it's still not entirely apples to apples. You're doing a bulk update. This means that the update pattern is fixed and you cannot do read operations in between individual updates, which the other versions of the benchmark do support. Bulk updates work well in special cases, but in the general case this does not work.
You are only claiming that an imperative algorithm (one which by definition prescribes mutation) must usually be written imperatively to perform optimally. I don't disagree with that. Anyway, I think this is as apples to apples as we're going to get. The OP's benchmark doesn't test what you are describing.
The OPs benchmark indeed doesn't test that, but he is asking for an efficient functional replacement for mutable vectors. Clearly the actual use case that he has in mind is not filling a vector with the numbers 0..n, even though that's what the benchmark is doing. Data.Vector is very nice for the cases where it works, but not a replacement for the general case. Whether it works for the application that he has in mind we do not know. Linear types *do* provide a replacement for the general case. Hopefully someday Haskell will support them.
Is there a solution for hot loading code yet? Alternatively, since most deployed code might be containerised is it now assumed that the solution to this lies at the container level by replacing old running containers with newer ones?
Relative monads really _are_ monads in a sense, just a bit "stretched out" along an inclusion functor. But depending on how you look at them, you can find the "real" monad in them, and decide you're just looking at a sidewise aspect of it, which happens to be more convenient for programming purposes.
In full generality that's essentially what an interactive theorem prover like Coq does. Equational reasoning in particular is automated using the congruence closure tactic.
Not sure this is a goal per 'So Erlang keeps to comparatively slow but easy to handle and easy to distribute interpreted bytecode instead. Moreover, if new code can be loaded into a running program at any moment or existing code monkey patched on-the-go, what tools do we have to reason about the resulting state of the program?' Seems somewhat dicey to bet against the erlang method since it's already proven itself the 'gold standard' of distributed computing. I would take 'slow but easy to handle and easy to distribute' over fast but unproven and less flexible in a distributed setting, imho.
Niceo one !
That was succinct and readable (and I'm totally incapable of c++). I like this series: he seems to make the theory approachable for novices like myself.
I suspect the editor's note at the top will be amended soon but just to mention my co-conspirators: it's actually Facundo Domínguez and Alexander Vershilov who are the ones who did the most on the implementation front. We worked a lot with Simon PJ to get the design right. As is often the case, even just the design part of work took a while to converge, but I hope result will work nicely. The extension is new in GHC 7.10, so should be considered something of a tech preview until the next release. GHC HQ may well decide to change the API in 7.12 (release notes being updated to spell that out).
yes, I'm working on it...
The posts before were called "24 days of ghc extensions"
Great list! I'd add to that one annoying limitation that can be pretty crippling in practice (e.g. for mutually recursive blocks of definitions): a splice only has access to whatever is defined *before* it in a file, and only code defined *after* it has access to any new identifiers it brings in scope. That feels pretty unhaskelly to me: it's the only place where *order of definitions* matter. About typing, this is getting somewhat better though oddly enough this actually never made into GHC 7.8 and I'm not sure what the story is for GHC 7.10: https://www.cs.drexel.edu/~mainland/2013/05/31/type-safe-runtime-code-generation-with-typed-template-haskell/
This of this extension as providing one of the primitives you need to get to the Erlang model. It's not powerful enough on its own, and even once you have all the primitives, you need a lot of libraries on top. But it's a step, and a step in a sensible direction. In contrast, Cloud Haskell (in my opinion) provided the library but without any primitives, and thus wasn't particularly helpful.
Please do! That would make Haskell on windows bearable for me for those cases where I cant avoid it...
Question about Leksah, did you create a workspace before you tried creating a package? It's a stupid detail, like forgetting to hit the on button, but it might be what the problem is. Packages in Leksah can only be created in an already open workspace.
Yeah of course it is. It's clearly still too early in the morning for me, sorry!
Awesome, look forward to it. Thanks for the update.
On Windows, I use Emacs, with haskell-mode and flycheck-haskell. It doesn't have the refactoring capabilities you might expect from an IDE, but the instant visual feedback (without needing to save or compile) for errors, warnings and suggestions make it feel like much more than just a text editor. Screenshot: https://twitter.com/bmjames/status/471743614696296448/photo/1
*Facepalm*. Sorry, clearly wasn't with it when I published this!
With proper modules installed, Emacs can do things like saying the type of an expression, autocomplete code, load your file on an interpreter, and [automatic error checking](https://github.com/flycheck/flycheck) (note: I don't use flycheck ATM, I'm linking just to show what Emacs is capable of). But I found the setup to not be straightforward, and there are a lot of knobs to turn. Currently I use the syntax/type checking of ghc-mod after every save, but it's a bit slow. I might want to use [hdevtools](https://github.com/bitc/hdevtools) (but not sure how to compare the two in terms of features). Overall I wouldn't say Emacs is the most friendly integrated development environment (IDE), specially on Windows.
This extension could be used to remove one of Haskell's most embarrassing warts - the possibility of *run time* errors generated by syntax errors in *static* literals. This wart has existed for numeric literals since Haskell 98. It spread to string literals with the advent of the (admittedly very useful) `OverloadedStrings` extension, and that changed the wart from being an oddity that might occur in rare corner cases into a serious defect that happens quite frequently. It's embarrassing because one of the most prominent advantages of Haskell over previous languages is its ability to turn *run time* errors into *static* errors using the type system; yet here a common error which is always a *static* error in almost every other programming language becomes a *run time* error in Haskell. And now we can finally get rid of the wart: make the magical type class methods `fromIntegral`, `toRational`, and `fromString`, which are used to interpret literals, require functions which can be computed statically - and then go ahead and actually compute them statically. Obviously this static check would be turned on or off with a pragma. In my opinion, the default should be to require static literals, with dynamic literals enabled by a pragma or implicitly enabled by the `Haskell98` and `Haskell2010` pragmas. That would break some code, but in my opinion libraries that sneakily introduce potential run time errors in this way should be required to announce that via a pragma.
If you want a "bit of an IDE" then ghcid might fit the bill. I wrote it to be super easy to install and use (after having much the same experience as you!), and the trade off is it doesn't do a massive amount of things: http://hackage.haskell.org/package/ghcid
This is very, very nice. It seems to me that a historical reference to [WASH](http://www2.informatik.uni-freiburg.de/~thiemann/WASH/) was called for, though. WASH already implemented many of these ideas in the early days of the web, when the only popular kind of interactive page was CGI and the idea of a session was just beginning to be explored.
Nowadays you would usually use [vectors](http://hackage.haskell.org/package/vector) rather than arrays - they have a nicer interface, and they have some significant optimizations. The `IntMap` trick is more richly developed - and optimized - in [unordered-containers](http://hackage.haskell.org/package/unordered-containers). See the post of /u/singpolyma for a more complete list.
I'm adding my disappointment also because I too thought it was a done deal.
That response isn't really on topic. The purposes of keter and Halcyon have quite a bit of overlap, and it's clear that there are some applications where keter would be better and some where Halcyon would be better. I don't understand Halcyon well enough to be able to make the comparison, and you apparently don't understand keter well enough to be able to make the comparison. &gt; Halcyon serves a complementary purpose. Keter is a Haskell application, which is the kind of thing Halcyon is intended to deploy. No, keter is intended to *deploy* web applications. It itself happens to be written in Haskell - isn't that also true of Halcyon? &gt; Consider the Keter setup instructions… With Halcyon, you can achieve the same result in one command… That's not a fair comparison. It's true that Halcyon might be useful for deploying keter itself, but that wasn't my question. The purpose of keter is to deploy web applications with a single simple command. Keter then not only deploys the application, it does safe hot-swapping of subsequent versions for live web sites, sets up SSL if needed, and does various other proxying and monitoring tasks. Keter also seamlessly deploys multiple web applications with different domain names on the same server, without needing a heavy web server such as Apache or nginx. How does Halcyon compare?
I wrote about something similar, without StaticPointers, but never published. Guess it's obsolete now :( https://stackedit.io/viewer#!provider=gist&amp;gistId=05ff34e0f352be06f04c&amp;filename=typeable-reify
Yeah, I had heard that this syntax was now supported, but my version of gcc (4.9.0) rejected that. *edit*: turns out C++11 isn't the default yet, we must pass `-std=c++11` or `-std=c++14` to specify which language standard we expect.
When you break at a function, the arguments are not in scope *yet*. That's what you want, because you may want to step through that evaluation process. Also, once you step past that and they are evaluated and in scope, you may need to use a trick to examine them if their type doesn't have a `Show` instance, such as when their type is polymorphic. All of those things are expected, and what you want, in the context of Haskell, but you do need to get used to it.
&gt; As there's no Category + Applicative notation I'm not sure what you mean by that. It's true that `Control.Category` and `Data.Functor` from base are quite spartan. But those are fleshed out significantly by the wide variety of category-related packages on Hackage such as the many packages authored by /u/edwardkmett (including [contravariant](http://hackage.haskell.org/package/contravariant)), and by Sjoerd Visscher's [data-category](http://hackage.haskell.org/package/data-category) package. What do you find lacking?
As the (presumably) last bugfix release of the 7.8 series - does this support being built with 7.10, as discussed in [this thread](https://www.haskell.org/pipermail/glasgow-haskell-users/2014-October/025389.html)?
While I applaud the attempt at innovation, there are a number of problems I see with this model, just around the implicit routing. There is no discussion of how to roll out a new version without downtime (you would need to force a refresh on the client-side every time). de-functionalizing routes into a data structure solves that and makes it easy to add new kinds of clients with incremental levels of integration.
Thanks GHC team! I have just realised there is no pre-built binary for macosx, and installing from source, on my macbook fails in the "make install" step. /u/aseipp, Any plans to release a pre-built macos binaries, for the lazy ones? ;)
I mean there is no syntactic sugar notation for `Category + Applicative` like there is `do` notation for `Monad` and arrow notation for `Arrow`. There's no reason there *couldn't* be, but there just isn't as things stand.
/u/Crandom was merely expressing that the equational reasoning in [the post you linked](https://cdsmith.wordpress.com/2011/08/13/arrow-category-applicative-part-iia/) was impressive.
Great response, Don. This link especially was a hidden gem for me: https://ghc.haskell.org/trac/ghc/wiki/Commentary/Rts/Storage/GC
&gt; There is no discussion of how to roll out a new version without downtime (you would need to force a refresh on the client-side every time). That can be forced at each client-server communication by checking versions against each other. As this is Haskell and the state is isolated, doing the XMonad thing and just restarting with the old state is probably straight forward. We also already have good mechanisms for versioned data structures, including migration. &gt; implicit routing For what they're doing it's nice and proper but yes, one probably also wants predictable URIs for other things. But I don't really see why everything should be in one paper. It's not that there'd be a shortage of routing-capable libraries on hackage, either.
Why are you reading grammars?
Lately I've been working on a concurrent queue and using a lot of mutable boxed structures where I know e.g. that a cell of an array will only be written to once, or multiple CAS are racing but only one will ever succeed (and the same for IORef), and I've heard this is a pattern (mutable variables moving monotonically) that others have stumbled upon as well. I wonder if a family of unsafe write/CAS functions might be beneficial for eager promotion.
i linked a mac build on the ghc-devs/ghc-users threads, but not on main cafe :) (reason being: i'd like other people to actually try it out and see if it works, rather than claim its official :) )
CFGs show the form of the language without relying on usage or application. Language is difficult for me to reason about purely, because it is hard to separate the purpose of the program from it's linguistic formalisms. This makes it hard to objectively evaluate programming languages without relying on a sense of aesthetics, personal anecdotes, cute / clever uses, and so on.
That's what I meant, though I really obscured my meaning. Even though they're mathy sounding, monoids are super useful (as opposed to other mathy sounding things which aren't useful yet). All you have to do is implement two functions in a way which fits a couple of easily testable laws, and suddenly a dozen libraries and a hundred functions are perfectly compatible with whatever you're working on.
From https://github.com/haskell/cabal/pull/2269 I'm guessing it is?
Yep, I'm the same guy who was proposing to share it here eventually ;) It works for me, even though haddock seems to be a bit sketchy, but it's definitely progress! Thanks Carter!
Readers might also be interested in the hdiff bookmarklet [(link)](http://hdiff.luite.com/) Clicking on the bookmarklet when visiting a hackage home page like http://hackage.haskell.org/package/data-interval-1.2.0 will show you a diff of all files with the previous version. 
Stackage already shows the changelog, including the Markdown, so that might be an easier interface: http://www.stackage.org/package/shake
You don't need a windows IDE to make a windows binary.
This was one of my favorite 24DOGE articles. I had to read it a few times on faith, but lo and behold the third time it made a lot of sense, and then some other aspects of distributed-process that I didn't previously understand started making sense. Thanks!
I believe yitz means something like {-# LANGUAGE OverloadedStrings #-} import Data.String data Foo = Bar deriving Show instance IsString Foo where fromString = error "runtime sadness" foo :: Foo foo = "foo" main = print foo
That is very interesting. Is there some more detailed discussion somewhere?
If [including readme files on the package page](https://github.com/haskell/hackage-server/issues/279) ever gets implemented, it'd be (technically) trivial to include the changelog too. In fact, I did my draft implementation of the feature by stringing along the changelog contents and then generalizing to support readmes.
I'm not sure I buy 4. at all. In particular since traverse is just generally applying a monadic operation to a container that isn't necessarily a list. I use this all the time when traversing an AST for example.
I use it too, but when I was starting a lot of my uses were of the streaming IO variety. Maybe not avoid so much as be aware of.
I'd recommend avoiding writing your own type class until you are more comfortable programming in a functional style. Programming using type classes emphasizes logic-based programming instead of functional idioms
I made enough to buy a burger with my bitcoins, so I'm happy :)
Thanks Ollie!
&gt; And I'm not a fan of having to add packages to the cabal file in order to import libraries. This is particularly galling in GHCi (i.e. `cabal repl`). It's a Cabal thing, not an issue with the IDE itself.
I think /u/drcomputersir was referring to the notation used in the Report.
This was great! I'm now using some of these extensions in my code. Thanks!
Previous discussion: http://www.reddit.com/r/haskell/comments/20u8qf/ann_typesafe_clientserver_communication_in_web/
If this is true then shouldn't those be marked as deprecated?
ST vectors are purely functional. None of the destruction and lack of persistence is observable outside of the implementation of your algorithms themselves, no state leaks out. What kind of alternative are you looking for, really? Why not just use the vectors? Right now you're looking for a pure alternative to something that's already pure.
Among other things, including these patterns (which are known as non-linear patterns) would mean that pattern-matching could accidentally introduce an `Eq` constraint, and using these would have different strictness properties. [Here](http://www.mail-archive.com/haskell@haskell.org/msg03721.html) is a very old mailing list discussion that lists some very good reasons against non-linear patterns, with at least one (admittedly unsubstantiated) claim that it was proposed and rejected by the original Haskell committee in the interest of equational reasoning.
Alright, and as someone pointed out on that link, if the arguments are functions, how does one determine if they are the same..? Okay it's clear that this would create more problems than it would solve.
We were working on Mucheck, but had suspended our work for the time being. 
There's an even simpler option for duplicate detection, with the caveat that the returned list might not be in the same order: map head . group . sort
Just so you know, it was discussed and rejected when Haskell was designed. 
I don't think I've ever actually used Either for error handling. I don't use it very often, but I recently wrote a function with type `(a -&gt; a -&gt; Either (a,a) a) -&gt; [a] -&gt; [[a]]` as part of a program for finding duplicate entries in a database (the database itself is running on a computer I don't control and apparently administered by someone who thinks disabling constraints is a good idea).
Quick question about some of the source: how does one actually inhabit the type `newtype VRef_ = VRef_ (forall a . VRef a)` ? Where/why is it used?
It is really jarring to suddenly have to read chains of composition from left to right.
Go get a Haskell FFI tutorial and a Python ctypes tutorial and swap out 'C' for the other language in each.
Even though there's no 32-bit version available, there's [an experimental](https://github.com/snoyberg/minghc/issues/15) 64-bit version for GHC 7.8.4. This has only been lightly tested so far, but seems to be working for people. You can get it from: https://s3.amazonaws.com/download.fpcomplete.com/minghc/minghc-7.8.4-x86_64.exe If you run into problems, please let us know in that issue. (And if it works, that would be valuable data also.)
I don't think anyone has made a list of all the differences (you could try contacting the author, he might know), but the website is a bit more up-to-date than the book (to my knowledge).
One thing I'm curious about, is what exactly does a StaticKey *mean*? E.g. is it essentially a (package_name,module_name,top_level_identifier) triple? And, more to the point, how does it change when the source code is modified? Often (usually? almost always?) it's impractical to upgrade an entire distributed app in one go, so this is an important bit of semantics.
I might be misremembering - but I think that the website is older. I remember reading that the website is the first edition of the book, while there is a second edition in print.
thanks!
&gt; But to understand the model of quantum computations, we need a much more severe paradigm shift, than, say, a shift from structured programming to object-oriented and even more so to the functional. I am not talking about some programmers being better or worse than others. The new model comprehension is a real revolution in our brain and the way of algorithmic thinking. ... toffoli :: Int -&gt; Int -&gt; Int -&gt; (Int, Int, Int) toffoli 1 1 0 = (1, 1, 1) toffoli 1 1 1 = (1, 1, 0) toffoli x y z = (x, y, z) getZ :: (Int, Int, Int) -&gt; Int getZ (_, _, z) = z not :: Int -&gt; Int not = getZ . toffoli 1 1 Better not compile with optimization enabled.
Acid-state *does* compose well. You just have to keep in mind that you cannot compose IO actions atomically. This holds true both for acid-state and for STM.
&gt; presumably because the underlying string constructors aren't? That’s right—in general `std::string` does not lend itself to be a literal type. There used to be an `""sv` UDL in the `string_view` proposal which could have been `constexpr` (and maybe it was, I don’t recall), but it hasn’t made it in just yet.
Wow, it has been 24 days already? I was hoping for `GADTs` and `DataKinds`...
I believe it is an existential type -- any VRef could be wrapped in it, and if you unwrap it you cannot assume anything about the value of the type variable.
A colleague tried that a few years ago and IIRC the runtime management wasn't trivial. 
One of the best series of the year!
My brain has been revolutionized!
With STM, you can compose updates to two different TVars into one atomic action, which is then performed in IO by `atomically`. With acid-state, you cannot compose updates to two different structures obtained via `openLocalState`; you're forced to perform two separate IO actions via `update` or `query`. So, what is it you mean by "acid-state *does* compose well". Because, to the best of my understanding, it doesn't.
Reversible is related to quatum computing? I'd expected something about partial isomorphisms, combined parser/pretty-printing libraries etc.
Anyone knows how to comment out selected code in FP haskell IDE?
I'd be interested to know why functional programming is better suited to modelling QC. Is functional programming particularly suited to parallelisation? 
You do have to be diligent about which garbage collector will own shared resources, but I'm not sure what else would be problematic.
No, it does not. I'm on my phone but ISTR that thread was sort of left at a stand still, and the issue hasn't been on my radar for a while.
A bit late, but whatever... I wouldn't do it because `Set` doesn't form a monad. That's rather nitpicky, I guess. 
That is by design. There's a trade-off between acid guarantees and parallelism. By using several AcidState handles you're letting the library know that the states are completely separate and updates can therefore be executed in parallel. If this is not true then you should only use a single AcidState handle. In fact, unless you're unhappy with the performance, always use a single AcidState handle. Note that you do not compose TVars; you compose STM actions. Likewise with acid-state, you only compose Query and Update actions. The IO actions you get with STM's 'atomically' (or the acid-state equivalent 'makeAcidic') do not compose well. 
This isn't existential - an existential type would look like `data VRef_ = forall a. VRef_ (VRef a)`.
It doesn't form a Haskell Monad, or it doesn't form a mathematical monad?
We can compose STM resource abstractions - which include TVars and TArrays and others - to model new, ad-hoc, larger STM resources. These larger resources are updated by STM actions, which also compose. STM's design is convenient if we wish to compose applications from smaller components (or, conversely, decompose applications into simpler components). Acid-state is not compositional in the above sense. Whether potential benefits for parallelism are a worthy design tradeoff is a separate issue. STM also does a decent job to support parallelism, and allows more flexible tradeoffs in the domain model. Consistent with your argument, I suppose you also believe that IORefs compose well for concurrent applications on the basis that we can in fact compose `atomicallyModifyIORef` actions, and that state in an application should be centralized to one global IORef anyway. :)
How is this not spam? You don't even say what it is.
They're relatively well known on this subreddit. They wrote a [ajhc](http://ajhc.metasepi.org/), a fork of the [jhc](http://repetae.net/computer/jhc/) compiler (not ghc) which allowed them to write [Haskell code targetting Android](http://www.reddit.com/r/haskell/comments/1oq22l/demo_movie_ajhc_haskell_compiler_can_create/). They eventually [switched from Haskell to ATS](http://www.reddit.com/r/haskell/comments/2nr0im/ajhc_is_no_more/). The Haskell for Android torch has since passed to [Keera Studios](http://keera.co.uk/blog/2014/11/24/haskell-android-games-adventure-engine-beta-testing/), who is using a different solution.
What really pisses me off about this otherwise great (really, really great resource) is that it's essentially dead. There are no exercises. There is no new content. There is no ... beyond. It's ... oh here's a flash of brilliance... now I move on. Really, for a resource that has become the main entry point to Haskell, it needs... more.
By that account, we should disallow literal patterns (`f 0 = …`) because they introduce `Num` and `Eq` constraints, and (at least so far as I’ve seen) they’re only used in toy examples. Personally, I would love nonlinear, disjunctive, and regular patterns: equal a a = True equal _ _ = False data X = A | B | C | D f x = case x of (A | B) -&gt; "ab" (C | D) -&gt; "cd" halve (Left a | Right a) = a partitionEithers ((Left as | Right bs)*) = (as, bs) 
I don't understand your argument. Are you saying that you cannot build applications from smaller AcidState components? I'm pretty sure you can do exactly that. If you meant something different, please explain. AcidState, by design, does not offer concurrency. AcidState simply gives you ACID guarantees. These are the guarantees I'm talking about when I say AcidState composes well. The guarantees you have for component A and component B will stay the same when you compose A&amp;B. To reiterate, you never lose concurrency because it was never there to begin with. Now, why have a data store without concurrency, you might ask. There are things you can do with a deterministic system that you cannot do with a non-deterministic system like STM. Replicating the state out to several machines, for example. You can also look at the Remote backend to see something that doesn't ship with STM. 
There's tons of other resources though. Exercises are nonexistent in LYAH so I practice on project Euler and am planning on taking up /r/dailyprogrammer challenges in a few months. They all serve their own purpose and I think that's fine.
Can the calendar (https://ocharles.org.uk/blog/pages/2014-12-01-24-days-of-ghc-extensions.html) please be updated to reflect all the posts?
GHC's existing `TyCon` considers the version to be part of the package name. We made `StaticPointerInfo` match that behaviour. Out of curiosity, what is your current use case, and in and ideal world, how would you see each node in your cluster handle requests while an upgrade is being rolled out? Part of the reason why upgrades haven't been implemented yet is because we'd like to better understand how people want this to work, so we'd much appreciate feedback or even a helping hand on this matter. :) In particular, would you like some form of late binding, whereby a `spawn there (static foo)` spawns *any* action called `foo` in the correct module (past, present or future versions)? What if the type signature isn't constant across versions? Is it okay for nodes at a future version to ignore or turn down spawn requests from past version client nodes, or is that a big problem? If it's a big problem, and if the "late binding" of above is not what you want, then presumably this means you want all past versions of a function or action to be somehow be present in all future version binaries?
Compare: ghci&gt; newtype VRef_ = VRef_ (forall a. VRef a) ghci&gt; :t VRef_ VRef_ :: (forall a. VRef a) -&gt; VRef_ to ghci&gt; data VRef_ = forall a. VRef_ (VRef a) ghci&gt; :t VRef_ VRef_ :: forall a. VRef a -&gt; VRef_ 
Is there a reason to use this instead of writing scripts in Haskell directly?
Writing scripts in an environment where Haskell is available, but which may need to be executed in environments where it isn't?
There is no general way to take 'AcidState A' and 'AcidState B' and produce a composite resource that has A&amp;B data and ACID properties. Thus, the ACID properties don't compose. Also, it's unclear how to even define 'isolation' without concurrency. Concurrency has always been part of ACID semantics.
It's licensed under CC BY-NC-SA 3.0 so if there are modifications or extensions you want to make to it, you're free to do so.
WASH was server side. Haste.App is client side. WASH was continuation based. Haste.APP is not. It stores the state in the server. The framework closer to WASH is MFLow.
I understand the difference between these two. However, how do you actually create a value with type `VRef_`?
The idea seems to be very close to TCache. https://hackage.haskell.org/package/TCache There are similarities: in TCache DBRefs are persistent TVars that can be serialized/deserialized to/from persistent storage automatically. A DBRef pointing to another register can be a field of a register, so arbitrary persistent graphs can be constructed. They can be transacted using STM, together with TVars too. It uses also System.Mem.Weak to garbage collect unused references. What you say in the description about the absence of garbage collection of TCache is not accurate. Unlike Vcache, it has a SQL like query language based on register field names and support triggers. The cache policy can be defined by the user. Instead of using a fixed persistence mechanism, The persistence mechanism in TCache is defined also by the user by means of a class instance, either in files or in any database. I do not undestand what you mean by structured immutable values, since a database is supposed to deal with mutable values. Data with very large acyclic references can be serialized and deserialized efficiently using RefSerialize and many DBRefs can share the common pointers. I think that this is an orthogonal problem that need a different package like RefSerialize. I do not understand well all the description until I see some example code. I also don´t understand your stress in the composability and concurrency of STM when you don´t use STM to perform transactions from registers coming from more than one database, which is not possible in VCache, while it would be one of the killer features of a transactional cache.
I use Leksah under Windows. The last version is quite slow however. I now use the version 1.13. Try to create cabal packages using cabal init.
&gt; I understand the difference between these two. I'm not sure that I do. Is the `newtype` an [impredicative type](https://downloads.haskell.org/~ghc/7.8.3/docs/html/users_guide/other-type-extensions.html#impredicative-polymorphism)? So it can be inhabited with a normal *polymorphic* value of type `forall a. VRef a`, whereas the `data` can be inhabited with a *monomorphic* value of type `VRef a`, where `a` is any type, and pattern-matching on `VRef_` recovers the (type) value of `a`? If that's right, I'm suddenly more sympathetic to people complaining about the meaning of `forall` in GHC Haskell.
Fully up to date here, but you might need to force refresh to clear your cache.
Thanks, I have IntelliJ working with help from this thread. It addresses my tutorial needs for now. I will figure out cabal and the rest over time.
Compositional in this context has a rigorous mathematical definition. Let's pretend that `ACID a` represented an ACID external resource of type `a` that I can `get` or `set` values of type `a`. To be "compositional" such an `ACID` resource must support these three operations: import Control.Lens (Iso', iso) -- Create an empty `ACID` resource out of thin air unit :: ACID () -- Combine two `ACID` resources into a single `ACID` resource mult :: ACID a -&gt; ACID b -&gt; ACID (a, b) -- Transform an existing `ACID` resource to work with a different type map :: Iso' a b -&gt; ACID a -&gt; ACID b ... such that they satisfy these laws: map id = id map (f . g) = map f . map g map assoc (a1 `mult` (a2 `mult` a3)) = (a1 `mult` a2) `mult` a3 where fw (a, (b, c)) = ((a, b), c) bw ((a, b), c) = (a, (b, c)) assoc = iso fw bw map lid (unit `mult` a) = a where fw ((), a) = a bw a = ((), a) lid = iso fw bw map rid (a `mult` unit) = a where fw (a, ()) = a bw a = (a, ()) rid = iso fw bw Note that there is no `ACID` type constructor in `acid-state`. I'm not exactly sure what `ACID` would correspond to in `acid-state` since I've never used the library, so I just made up a hypothetical type to illustrate what I mean.
Maybe if i was SPJ that would be useful. 
There are. But they not synchronized / integrated with the material, nor even that relevant to the material. 
It'd be great to see a high level compare and contrast with TCache, VCache, and ACID-State so we could understand the tradeoff. Not that this is the responsibility of OP, just that I'd like it ;)
I really like that series. Please continue!
Maybe I don´t understand well, but a DBRef is directly serializable, so TCache don`t need VRefs in what VRefs I understand are used for. Two registers that point to a comon pointed structure can reference it with a DBRef field. That means that a DBRef can point to a register that has DBRefs inside, that point to other registers.... and so on. When deserializing, two registers that have the same DBRef field will point to the same memory address when the register corresponding to the common pointed structure is accessed. The retrieval of the pointed data is automatic when the program dereferences the DBRef. There is no need of RefSerialize for that kind of memory sharing. RefSerialize is used mainly to serialize large structures with many internal memory references in a single DBRef in order to produce shorter serializations and recover the structure when deserialized. but it is possible to serialize "contexts", that are chunks of memory shared in order to be used by more than one register. The deserializer is informed about the context used with setContext, so, upon loaded the context data, the deserializer create the registers pointing to the shared addresses in memory. Let´s say that the second method is automatic, since there is no need of a manual definition of memory pointers in terms of DBRefs registers, but the first is more progressive in terms of register loading and more database friendly since it ends up in database normalization. In both cases the reconstruction of the pointers in serialization/deserialization happens 
PVars fulfill the same role as DBRefs. I didn't mean to suggest any dependency between them. It is true that you can use DBRef fields to model immutable values. You could actually do the same with PVars. You just need the discipline to not mutate the variable, a property not protected by the PVar or DBRef types. Also, you need the knowledge for when to delete a value from persistent storage, e.g. performing some kind of GC or reference counting by hand. In terms of TCache, you might think of VRefs as specialized DBRefs for immutable values only, thus allowing access to values outside the STM or IO monads, also providing GC. Unlike TCache, VCache doesn't provide any mutable 'fields': currently, a PVar is second class (according to the README). Though you could store the name of a PVar, which is basically what TCache does.
I wonder what is the user case in which a database cache need immutable values. In TCache, deletions in persistent storage is done explicitly with delDBRef. I do not conceive an automatic deletion of something that is in permanent storage. It is intuitive for me that what is necessary is to be sure NOT to delete it, even if the cache count becomes 0. These two things tell me that perhaps VCache is intended for some user cases that I can not envision right now...
I always see Free as being adjoint to the functor which forgets "a monad into a functor" and Operational as adjoint to the functor which forgets a monad into merely a type function `* -&gt; *`. 
IMO, if you are not GCing external storage, then it is (probably) a (transaction) log. If you do GC, then it is external (transient) memory.
the article says basically the same (with less jargon). that characterization does not say anything about asymptotics though.
Future work did not mention: Port this to Cloud Haskell, implement parallel remote monads, map/reduce etc. Seems like it could work similar to Apache Spark if ported to Cloud Haskell.
Oh `folds` is pretty cool, I'm definitely going to poke through this.. PS: thanks for the kind words! :)
Agreed! On both counts
How about `transformers`?
When you use 'makeAcidic', you get an AcidState handle and a bunch of IO functions for accessing it. Those IO functions /do not compose/. Again, this is exactly the same situation you have with STM. If a library uses STM under the hood but only exposes a set of IO functions, good luck composing new atomic actions. The AcidState handle might be the source of some confusion. It is /not/ like a 'TVar', it is not even a monad like 'STM'. Is it there to mark the boundary of the system. We're at an impasse if you believe you cannot have ACID guarantees without concurrency.
How'd you like the price?
It certainly seems like it would be more helpful to print the location than to not print the location, even if it's not as helpful in assert. Doesn't seem like a valid reason to me.
&gt; This package also drops the positivity requirement that `Free` implies with its functor constraint But note that you can always use [`Coyoneda`](http://hackage.haskell.org/package/kan-extensions-4.1.1/docs/Data-Functor-Coyoneda.html) to make a functor from your non-functor.
There's an ongoing discussion about this right now, I think, with some changes potentially happening in 7.10 or 7.12. Specifically, there's a function `errorWithStackTrace` in `GHC.Stack`, and there is discussion of replacing `error` with `errorWithStackTrace`: http://osdir.com/ml/libraries@haskell.org/2014-12/msg00000.html I am not in touch with what exactly is going on here, just have seen it on and off on the mailing lists. /u/cartazio may chime in with more info.
Personally, I just don't see the point. Compare with debugging in traditional languages like C or C++: in my experience, the location where the error is detected is almost never useful, it is almost always just a distraction from debugging, so it would be better if it were never shown. The first thing you do is get a stack trace, and scan up the stack trace until you find your code. head :: [a] -&gt; a head (x:_) = x head _ = error "Prelude.head: empty list" main = putChar $ head "" I don't need the file and line number of the `error`, because there's nothing wrong with `head`. The error is in `main`. You want the stack trace.
My attempt: http://lpaste.net/117260 Uses HashMap instead of Map, ByteString instead of String and drops the use of Sets to maximize performance. The run time of this version, Norvig's Python version and a C++ version I wrote (https://gist.github.com/jmoy/9123a1303634501996b2) are about the same when the run time is dominated by making corrections. However, the Python and C++ version are about three times faster than my Haskell version if the time to just build the model is measured.
No need to evaluate `powerset xs` twice: powerset [] = [[]] powerset (x:xs) = here ++ there where there = powerset xs here = map (x:) there
I think errorWithStackTrace should pretty much always be used 
You can think of the lambda calculus as (more or less) the "internal language" of a nicely behaved Cartesian closed category. Linear logic on the other hand, is the "internal language" of a symmetric monoidal category. Linear logic is what you get if you take the rules of logic and say that every variable can and must be used only once in the resulting proof. This is related to the Curry-Howard-Lambek 3-way correspondence between propositions, types, and categories. Everything you can say in linear logic holds in the world of quantum computing. No information is gained or lost. With languages like Clean you can fake linear types with uniqueness types. With Haskell you can build nice abstractions in the type system that let you typecheck a linear logic statement. The functional programming world cares an awful lot more about these abstractions than the object oriented world. You could always try to build all this stuff on objects and catch your errors at runtime, or write some kind of object-oriented language with linear types, but the existing functional languages we have right now can model the latter, and the former, well, if you are trying to prove something its usually better to separate the structural proof part from the execution part as much as possible. &gt; Is functional programming particularly suited to parallelisation? If everything is a just a pure function, who cares what core executes it? So, yes.
https://www.haskell.org/pipermail/haskell-cafe/2013-March/107215.html
One can't mention powerset in haskell without the infamous one liner: `powerset = filterM (const [True, False])` https://www.haskell.org/pipermail/haskell-cafe/2009-July/064337.html 
Why is it infamous? I was really astonished when I saw it.
&gt; they use different exception handling modes than GHC's mingw does, in both 32 and 64 bit Could the people responsible for building official GHC Windows binaries be convinced to switch? We use the openSUSE treasure-trove of cross-compiled binaries for a bunch of packages in Julia, would be neat if the same setup could be made to work for Haskell. I've been fiddling with a shell script (yeah, bad idea, I know) that uses `xmllint --xpath` to parse the rpm repodata and grab dependencies without needing to have Julia installed, might be fun as a toy project in an actual language instead of shell. I'm slightly afraid to ask, what's the situation like for GHC compiling to statically linked self-contained exe's?
Thanks for the in-depth reply. &gt; If everything is a just a pure function, who cares what core executes it? So, yes. It's so simple really. 
Actually, in this case it's probably better to evaluate twice, because your power set may be large but you still want to iterate over it in constant memory. Try your version with print $ length $ powerset [1..30] or something like that (I'm on 8 year old machine right now, so 26 is already more than enough :) Also, rather curiously, the order of arguments of `(++)` matters for the memory consumption - one is constant space, the other blows up (at least with this old GHC version I have here). I guess that's because of an overly enthusiastic common subexpression elimination optimization in GHC...
It's only useful when the program is compiled with -prof(and probably -fprof-auto or something like that too) though.
I know what you mean, but since a stack trace would include the location at the top too I can’t rally behind you. One way or the other, and no matter the form, I want that information. And if it means that one day an error displays its location while the stack trace does as well, then so be it. It really is the worse that can happen.
Finally Haskell is enterprise-ready to turn XMLs into stack traces!
Which is too expensive to turn on in production software. :/
Good point. But that only really applies if you're going to fold it, right? Thinking about it more, what about the overhead of `++` having to traverse the lists? If you want to eventually just fold it, would some kind of tree structure make more sense?
&gt; Could the people responsible for building official GHC Windows binaries be convinced to switch? Answering my own question here, it looks like the answer to this is yes, it just needs people familiar with MinGW to help out. https://ghc.haskell.org/trac/ghc/ticket/9218, and https://phabricator.haskell.org/D339, and several Windows-related threads on ghc-dev over the past few months.
I don't know about what *else*, but I strongly disagree with much of the linked post. 1. When possible, use `map` in preference to `fmap`. Excessive polymorphism makes your code less readable, not to mention what it does to error messages. While polymorphism is definitely a powerful and useful tool, when there is no specific need for it gratuitous polymorphism is a kind of "premature optimization" that reduces the maintainability and overall quality of your code. This is especially the case for fundamental types like lists. 2. Like lists, Monad is a fundamental semantic type. So strongly prefer the type-specialized functions in `Control.Monad` and `Data.List` over their much more polymorphic counterparts in `Data.Traversable` and `Data.Foldable`, unless there is a specific reason in the design of your software that the polymorphism is needed. 3. Partial functions - here I agree, avoid those. 4. Asynchronous exceptions - occasionally there is a legitimate use for them, but yes, usually avoid them. 5. There is nothing wrong with the functions `isJust` and `isNothing` *per se*. I think what the author is mostly complaining about is using them to protect partial functions like `fromJust` or the equivalent unsafe pattern matches, and there I completely agree. There's something else in the author's description of point #5 that I take issue with, though it is a matter of personal style. In my opinion, using `case` to pattern match `Just` and `Nothing` is a code smell. Given our rich toolbox of semantic combinators and class instances for the `Maybe` type, the verbosity and cascading indentation of `case` will almost always make your code less readable, less maintainable, and uglier (in my opinion). The same goes for pattern matching `True` and `False`.
it's enabled by `-XTypeFamillies`. It is very useful when typelevel programming.
I didn't have any issues using Sublime with Haskell on my macbook air, strange.. All I did was install Haskell, install Sublime Text, install Sublime Text Haskell Module
right, but did you also install gch-mod to get those added benefits when using with the SublimeHaskell plugin?
Halcyon and Keter do different things. `halcyon install` is like `cabal install`, only more comprehensive. There is no service management, as this is not the purpose of Halcyon. And no, Halcyon is [not written](http://www.reddit.com/r/haskell/comments/2ogoot/deploy_any_haskell_application_instantly/cmn1bvq) in Haskell. If you want to know more, come chat in #haskell-deployment on freenode.
I've never managed to get SublimeHaskell to work for me on OS X. Definitely can't use it with nix either, now I use that all the time. I tried to modify it for my needs but in the end decided to make my own Sublime Text plugin (hopefully a first release coming soon*) ^(*for a given value of soon) 
Will DWARF info be included in non-prof builds?
Not too familiar with either of the libraries, but my take is that the library does *not* expose a set of IO function. You get a set of TVar, VRefs whatever that are not IO. They compose and they can later be transformed into IO for execution. That's the whole idea of the STM composition. Everything composes only up until the point where you make them into IO using atomically. If your library starts with IO functions, you don't get any guarantees by the type system. If you start with STM, you do get these guarantees.
Related follow-up, for instance, how do I get [1,3,5,7,9] by specifying step. Edit: okay, that would be [1,3..9] so it's more like "1,3".."9" rather than "1","3..9" . So I'm like specifying the arithmetic step with the first two, using the .. to suggest it keeps going up to the 9. And [1,3,5..9] gives an error, hmm.
Are the `toffoli`'d boolean functions linear in a way that the regular ones aren't? *Edit*: Or rather, is the point that you can write linear boolean functions using a `toffoli` approach? I guess the `toffoli`-based functions in the article, i.e. written in normal Haskell, seem redundant because we're used to being able to substitute: not = getZ . toffoli 1 1 -- definition of getZ not = (\(_, _, z) -&gt; z) . toffoli 1 1 -- definition of (.) not = \z0 -&gt; (\(_, _, z) -&gt; z) (toffoli 1 1 z0) -- definition of toffoli not = \z0 -&gt; (\(_, _, z) -&gt; z) (case (1, 1, z0) of (1, 1, 0) -&gt; (1, 1, 1); (1, 1, 1) -&gt; (1, 1, 0)) -- ??1 not = \z0 -&gt; (\(_, _, z) -&gt; z) (case z0 of 0 -&gt; (1, 1, 1); 1 -&gt; (1, 1, 0)) -- ??2 not = \z0 -&gt; (case z0 of 0 -&gt; 1; 1 -&gt; 0) GHC's optimizer produces Core equivalent to the last line. I don't know the rules justifying the last two substitutions. Presumably one or both doesn't hold in LLC?
I wouldn't take this syntax too seriously. It's too cutesy to be really useful, in my opinion. Anyway, my understanding of `[a, b .. c]`, on numbers at least, is that it means "start with `a`, take steps of size `b - a`, and stop one element before you would produce something strictly beyond `c` in the direction of `b - a`". This interpretation explains all the results you are seeing. In practice the meaning of this notation is controlled by the [`Enum` class](http://haddocks.fpcomplete.com/fp/7.8/20140916-162/haskell2010/Prelude.html#t:Enum).
Thank you, I see what you mean. And having moved on a little, "comprehensions", seem to be able to handle these types of processed ranges plus much more.
As side note, can't haskell-src-exts declare a system dependency on "happy" so that you actually know why it fails to install?
Thanks. I updated the post and quoted you at the top :)
Yeah, that was my thought too. 
And yet we have been, for better or worse.
No, if the external storage is.. well, a database. I suspected that VCache persistent storage is intended as a kind of virtual memory. But a database is not virtual memory, since virtual memory is transient. A database store data even if this data is not processed for a long time and there is no pointer to each register in RAM. Many different programs can access the same database with or without TCache by means of his own query language, using an internal structure that is not established by any particular program. I think that it is more aligned with acid-state in that approach. it that sense, it is not a general database cache like TCache, which can access database data created externally with any schema and perform transaction with it, but it is a kind of acid-state with virtual memory, with his own proprietary format.
Thanks for posting this - it helps to understand why. There will be a lot of breakage; [lots of packages](http://packdeps.haskellers.com/reverse/monad-control) with no upper bound on monad-control.
ah, correct, I also don't understand this subtly. Disregard my earlier comment. 
This is easily solvable with Template Haskell as libraries like ["loch-th"](http://hackage.haskell.org/package/loch-th) prove.
This doesn't work for me. I literally just blew away my cabal and ghc dirs and started from scratch and still run into this problem. Language/Haskell/GhcMod/Monad.hs:370:5: Wrong category of family instance; declaration was for a type synonym In the newtype instance declaration for ‘StM’ In the instance declaration for ‘MonadBaseControl IO (GhcModT m)’
That's strange. Do you have a version of `monad-control` and/or `-journal` in your global package db? It seems that the latest version of m-j isn't being used. Does `--constraint monad-journal==0.6.0.2` help?
PVars are persistent 'registers' in the sense you're using. They don't need to be in RAM to hold onto data, and may only be deleted explicitly. But VRefs aren't data registers. They're just pointers to values in a virtual memory backed by a key-value store. The important bit is that not only PVars can hold onto VRefs; so can the Haskell process. And VRefs must be garbage collected to be useful. VCache does have its own format, a schema on LMDB. I wouldn't use the adjective 'proprietary'; it's open source. But it is like acid-state in this sense.
https://github.com/haskell-suite/haskell-src-exts/issues/14
Yes, I think this only applies when you want to fold over it. I wasn't thinking about more complicated structures, I like lists for simplicity. Unfortunately here it seems that we have to choose between a fast version which blows up and a constant space version which is much slower... (10 times, here, for a powerset of size 2^25 )
Linear types are a basically a contract that you won't contract (use the same term twice) or weaken (forget to use a term) *in the future*. Uniqueness types are basically a contract that you haven't contracted (used a term twice) *yet*. (Weakening comes for free, you have the variable left, and it was the only copy, so it has to be here.) Unlike with linearity, in many ways, you can forget this fact and carry on as normal. You can take a uniqueness typed term and make it non-unique by giving copies to your friends. Linearity plays the other way around. In linear logic, you have a (!) modality (it's actually a comonad) that lets you work with a term unrestricted in linear logic that you can forget, and you then get linear terms. They both control access to contraction and weakening, but they do so by 'facing' in opposite directions. Uniqueness tells you something about what has happened to the thing up until now, while linearity tells you something about what can happen to it in the future. If you don't go back/forth to the unrestricted modality then uniqueness types act a lot like *affine* types, not linear types. Affine types only prevent contraction, not weakening. You can't contract them. If you pass them out of the function you can know they are unduplicated, but you _could_ drop them on the floor (weaken them). You could also pass them to functions that expected unrestricted versions of the term, but they are definitely suitable for every use of affine typing if you type the combinators correctly.
`getZ` isn't linear; it discards information.
Haskell's `Monad` fwiw.
This is the only example of an use of "filterM" in haskell code. I never seem to find any other example anywhere. Is there any other situation where you would use "filterM"? How would you realize if it'd be better to use it or not?
We could yes
Range notation is nice if you want to work with infinite lists of numbers. For example, `[1..]`, which produces [1,2,3,4,5,etc...], and is limited only by time, and your hardware.
Yeah, you're demonstrating the step in the beginning, then giving it an end point. E.g. "Give me numbers 1, 3, and so on, up to 9."
I believe the chapters don't sync up between the book and online versions. This lead to some confusion when talking with a friend who was reading the book, while I was using the online version. I think one of the versions has an extra chapter.
Ah; thanks for pointing out the distinction.
SWIG to C++.
Using getZ and the like are technically not legal operations. You need to keep the other bits around and consume the bit you are using. So while the basic toffoli and fredkin gates he has in the article are legal unitary operators that you can use in a quantum computer, the `getZ` operations and the like are a bit naive. You formally need to keep the other bits around, even if you don't use them further. Otherwise you are working, at best, in an affine fashion.
I always build tools in a sandbox. Tools with shared dependencies (ghc-mod, structured-haskell-mode, what-have-you) get a single sandbox, and bigger stuff like pandoc gets a separate one. I just cabal install alex and happy beforehand (sometimes even in a sandbox, though it's not necessary), and everything proceeds smoothly. Just add the sandboxes to your path or symlink the bins somewhere you like and everything is perfect.
Patterns with numeric and string literals _intentionally_ introduce `Num` and `Eq` constraints, whereas it's much easier to _accidentally_ introduce a non-linear pattern. For example, accidentally typing `x@(y:x)` instead of `x@(y:x')` would silently introduce a non-linear pattern where the programmer did not intend. That said, I think the arguments about equational reasoning and strictness are much more convincing than the ones about the `Eq` constraint. (On the other hand, I too would love to have disjunctive patterns, which are one of the things I really miss about using OCaml.)
To mitigate the potential problem of scripts changing while running (the shell reads the file lazily) you might want to wrap the body in a function. E.g. #!/bin/sh main() {...} main Of course that's easier in a shell generating script. Good work.
Why should I use them? Does they not pollute namespace? I'm OK with importing Data.List, Data.Function, etc separately when I need them, so my preferred alternative prelude is empty.
We have `&gt;&gt;&gt;` and `&lt;&lt;&lt;` from `Control.Category`.
Make sure you don't use tabs. GHC doesn't handle them very well at all.
Seconded. Thank you so much Justin for the excellent text and Alexey for the submission here. Now I'm really eager to get my feet wet with AFRP.
This post is really excellent, but stops a fair bit short of AFRP (by Conal's strict definition). In particular, AFRP inherits the need to represent continuous signals from (classical) FRP. This means that all of the things you represent in AFRP are theoretically "sampling rate independent" which ensures that they compose better. Adding this kind of thing to an Auto essentially comes down to adding a "time delta" parameter to the input so that continuous functions can understand how much time passed (behind the scenes) since the last update. This is "behind the scenes" in order to ensure that people don't make code sample rate dependent on accident. The trouble is that once you add this you start to have difficulty determining what the "right" behavior for non-continuous events is. They ought to be instantaneous, but that's no always easy to represent. But essentially, Auto's are the core idea behind some of the efficiency tricks for making AFRP work. Just be sure to try out the continuous semantics at some point and see how that feels.
In the intro (to this intro) he says he's going to move on to looking at netwire and it's `Wire` type which has a time delta in the next post. 
Morning of day 4 -- made Var a phantom type, so there can be Var String, Var Integer, etc that all represent the same untyped shell variables. Now the haskell type checker is type-checking my shell variables!
Actually, I thought about making a self-recompiling haskell shell script, like: #!/bin/sh haskell &lt;&lt; EOF -- haskell source code here EOF checksum=xxxx -- check here if the haskell code matches checksum; if not recompile and restart -- compiled shell script here I haven't tried it yet mostly because I'd have to teach my editor to syntax highlight such a "shell script" as haskell code, which seems hard. ;) 
Oh! That I missed. The downsides of reading on your phone.
It feels like sudden enlightenment.
Is such a general framework actually useful? For my work in quantitative finance, I write ad-hoc simulations on a daily basis. In my experience, it has been more efficient to write each one basically from scratch with very little code reuse. Anytime I try to do something general, I find that I sacrifice performance and it's never really flexible enough. I suppose that's also why I've never found Haskell to be useful for writing simulations. It's much faster to write someting in q/kdb+ with a few calls to R libraries when needed. The investment in Haskell code only pays off for production trading system code, in my experience.
(If newcomers like the OP are wondering why this post is so highly upvoted, the explanation is probably that /u/augustss is speaking from personal recollection.)
Reasoning about non-continuous events has me confused. In Netwire, there are functions that depend on time (for, after, periodically, etc) but they aren't fired in the precise time, but rather in the "frame" that represents the delta. If, for example, you set a periodically with t less than the length of dt, it will still only be fired once. The use case I thought of is a game where an enemy throws a fan of projectiles in an spiral with a smaller delay between projectiles than the frame length. Instead of a perfect spiral, projectiles might get batched in each frame. It can be worked around (for example by locally shorter frames in creation), but it's far from trivial and is not something programmer can ignore.
I don't have anything in my globals - completely cleared them. I started over and tried adding the constraint you suggested, still no luck. There is a [pull request](https://github.com/kazu-yamamoto/ghc-mod/pull/423) outstanding for this. I'm new to all this, so I'm pretty baffled (and stuck). Thanks
I asked him when his next post in the series was coming on IRC, but he said that he probably wouldn't be continuing the series, and that made me sad.
the productivity curve in haskell is missing the steep dive of trying to write a ui. also, haskell is rather easy to learn until monads, so i don't get the "lol, my brain hurts, lol" at the beginning.
Simulation is quite useful in certain situation when analytical solution is hard to given and real experiment is expensive. For example, internet company like Facebook/Twitter usually build their products using service-oriented architecture, deploying to hundreds or thousands of machines. In such a distributed system, machine or network resource outages and failures are normal. It's very helpful to compare different fault-tolerant strategies by Discrete Event Simulation, to gain maximum resource utilization. In fact, I found Aivika when doing a similar job. It's amazing that the framework has lots of features: different queuing strategies, random number generators(such as poisson distribution for simulating RPC requests), saving the result to CSV files, plotting charts, and so on. They are all I need. 
That requires some explanation, or a citation at least :-)
Ugh, category theory, as though it is some how necessary for programming in Haskell. Additionally frustrating that the two points are not features of the language (typeclasses anyone?)
Autos and Mealy machines are one and the same from what I've gathered, but I'm not sure what is the purpose of visualizing one as "mealy machine" and the other as "auto". For example, take the Mealy Machine from the "machines" library: https://hackage.haskell.org/package/machines-0.4.1/docs/Data-Machine-Mealy.html The way you can "construct" one of these machines is with the following function (in one case or the other): unfoldMealy :: (s -&gt; a -&gt; (b, s)) -&gt; s -&gt; Mealy a b autoFold :: (b -&gt; a -&gt; b) -&gt; b -&gt; Auto a b So both are basically the same construction, but you construct an Auto by assuming the output of each step is the state of the next one. What is the reason behind this? What is the difference between an Auto and a Mealy machine then?
What's the argumentation there? I feel there value in hiding sampling speed from algorithms. Where does discrete time improve composability?
If anyone has the graphic for the learning curve of monads, functors, and applicatives / traversals (the one that involves a bulldozer and punji sticks, please send!
It's a matter of taste and sometimes skill in using said function. For example, there are times when you have local knowledge that a list is non-empty and `head` might be exactly what you want. However, if someone is blindly taking someone else's advice on style then chances are they should follow these suggestions until they have a better grasp of things. Not sure what the down-vote is about, but if anyone doubts the idea that using `head` is a matter of taste they need only grep a largish expertly maintained code base like GHC or Cryptol.
But to effectively apply any of these features, I've found that it's critical to fully understand them -- not simply choose something because it's part of a framework and sounds appropriate -- and once I've understood them, they're often trivial to implement myself. Maybe I just tend to use more simple features and others use more complex ones, so they benefit more from a predefined framework?
I've been harboring some kind of hope that arrowized autos would be able to represent fully faithful clojure-style transducers (sorry for bringing 'ducers up again) complete with their stateful behaviors such as preemptive stopping before end of stream, take, skip, etc.. which the haskell versions tend to trade off for gains in more easily expressible types in terms of folds etc..
It is possible to program in Haskell without knowing any category theory. It's just a lot less pleasant and easy that way.
I feel like I spend way to much time refining abstractions rather than getting work done in Haskell. Haskell probably provides more room for this than any other language. I'm inclined to indulge what sometimes turn into weeks-long diversions because they can be so educational but I'm not sure I'll ever extract a net return on that knowledge, there is always still so much to learn and yet I already know more than enough to complete almost any real-world project.
These graphs are very, **very** accurate.
I'm taking a class on category theory next semester, I can't wait to finally crack that nut open.
Thanks, that looks good. I love your site. It would be nice if you classify contents based on beginner, intermediate and advanced groups.
Actually the chart provided for Haskell appears to indicate that category theory isn't necessary to be productive in Haskell, since it has no impact on the "productivity" line.
This. Haskell isn't particularly hard to learn, unless you come from C/Java/Python, in which case there are some things you must unlearn.
I think that graph oversells the Haskell programmer's eventual productivity. I would even say that Haskell is distinctly below average. Source: 10+ years of (occasional) struggle, b) very little notable software written in Haskell
Another angle is that if you spend enough time and effort around Haskell and the community you will unwittingly learn bits and pieces of category theory...
Great post!
But it does make you feel stupid when reading about it :p
I don't think there's really much of a benefit unless you want to get into that subset of Haskell. There are a lot of folks like /u/bos who have cranked out a lot of really amazing libraries and don't really deal with category theory much. It really is a niche subset of Haskell.
I wrote another post examining `folds` [over here](http://jozefg.bitbucket.org/posts/2014-12-27-folds.html). I hope you enjoy it!
I'd be interested to see how one would actually measure that. I've recently been hanging out with crowds where purity is regarded as a premature optimisation, and it's hard not to grant a certain logic to that viewpoint. But my gut tells me that with Haskell's terse syntax and layers of abstraction it must be easier to write more, faster.
I feel the Haskell graph is missing a downward productivity spike right after learning about monad transformers, when one tries to build custom monads for all kinds of things that could be done much more easily by just passing an extra parameter around (followed by an upward movement once one learns when to use them of course).
I agree. What I was trying to say that Haskell isn't *particularly* hard to learn... in comparison to learning other languages as a "first programming language" experience.
Found what I was looking for on ircbrowse. My memory was a bit incorrect but it's OK. 2014-01-30 20:06:45 +0100 &lt;edwardk&gt; replace the legend with Foldable/Traversable: red, Applicative: blue, Monad Transformers: green, Lens: black in http://mmoreporter.com/wp-content/uploads/2011/02/LearningCurve1.jpg and it is pretty accurate.
You can find the implementations in the source for the [Enum instances](http://hackage.haskell.org/package/base-4.7.0.2/docs/Prelude.html#t:Enum) since the notation you are concerned with is just syntactic sugar for a call to enumFromThenTo. The default implementation is based on fromEnum and to Enum and the implementation for Int which can be found [here](http://hackage.haskell.org/package/base-4.7.0.2/docs/src/GHC-Enum.html#instance%20Enum%20Int). 
Yep. Fortunately I had some familiarity with currying, sections, lambdas, lexical scoping, list comprehensions, recursion, how Lisp-style lists are used, apply, eval, etc., etc. from other languages when I first encountered Haskell. If I hadn't, the basics would have been much more difficult.
What often happens is that experienced Haskellers, when exposed to some Category Theory, experience it as a series of "Oh, so *that's* what that is called" epiphanies.
I find that I spend to much time playing with lists. Want the 4th element of a list? Check if a list is a pallindrom etc? Takes a lot more code and thinking that most othter languages.
Jezz, Categories are just digraphs with objects as nodes and object morphisms as edges, and a morphism composition operation that is associative and transitive. So *simple*...
It's just a load of circles and arrows. Follow paths of arrows to get from one place to another. Switch all the arrows around and wave your hands wildly to get a proof. Simples.
 xs !! 3 xs == reverse xs Is this more code or did you mean something else?
My experience is different. I'd say I'm more productive in Haskell than any other language -- with some caveats. Caveats: * Sometimes the libraries suck (e.g: signal processing) and then Haskell turns out not to be productive at all * Sometimes the performance is decent, but I need great. And then fighting performance issues for a long period of time feels like more effort than just writing it in a language like C or C++ in the first place.
That actually worked. I can't believe I have been coding in haskell since October and I am still learning such baisic things. 
Are you trolling? You'll still be learning in 50 years time. You can't expect to master a language like Haskell in three months. That said, this is pretty basic stuff. 
Hmm, true. I suppose I could make `var1 + var2` yield `APlus var1 var2`. And so ease construction of Arith expressions.
The essence of reactive systems is feedback -- i.e., treating a past output as a present input. In terms of programming, this means we want to write reactive programs as recursive definitions. However, not all recursive definitions which make sense when viewed as purely mathematical functions make sense when viewed as reactive programs, because reactive programs have to be causal: you can make decisions based on past values, but not on future values. So checking the guardedness of recursive definitions is one of the key issues in designing a reactive programming language. Discrete time has a built-in notion of infinitesimal delay, and continuous time doesn't. As a result, checking the guardedness of recursive definitions ends up being much, much easier for discrete time than for continuous time, and it lifts to higher-order in a natural way. On top of this, the naive model of continuous time basically just doesn't work, because there is too much junk in the model (Zeno processes and the like). Fixing this is really complicated; Wan and Hudak's *FRP from First Principles* is one of the best attempts to date, and it doesn't scale past first-order functions.
I wonder how many people have learned Haskell as a first language and how that affected their programming later on. 
Nice. I am looking forward reading it.
It might be possible to reduce feedback to specific classes of feedback that make sense in continuous time. Integration, for instance.
I think the most idiomatic way to do it would be using lenses, but that can also be quite confusing. I dont have a tutorial at hand at the moment, so I recommend googling for one.
I'm partial to concerns about FRP being too large, but I am not sure that pushing the causality concern to be solved by discrete time steps isn't throwing the baby out with the bathwater. That paper is one that's been on my list for a while...
From looking at the code, I'd - break the `gameRound` function up into a lot of smaller functions - run hlint on the code (even better if you can configure your editor to run hlint and/or ghc-mod on every save) - look at using `State` or `StateT` to handle the manipulation of State instead of IORefs If you end up splitting your larger functions up into smaller functions, it'd be worth keeping an eye on where the state / randomness is occurring - if you can get to the point where your game logic is in / mostly in pure functions you might have an easier time working with the code in the future. Personally, at this stage I'd leave lens alone. If I get a chance I'll have a go at making some of these changes later today - if I do I'll comment the changes up and send you a pull request. Edit: I'm taking a deeper look now (while making some changes) - it looks like a solid effort for a first Haskell project. Congratulations :) 
I don't believe that is a particularly common pitfall.
One currently "contains" an output value (e.g. it can be computed from the state alone) while the other requires a fresh input prior to to receiving and output.
Theres a lot of unnecessary manipulation lifted into IO. For instance: ludus &lt;- newIORef initial -- Advance to the next player modifyIORef' ludus (\g -&gt; g { players = rotate . players $ g }) game &lt;- readIORef ludus Can be replaced with let game = initial { players = rotate . players $ initial } Theres a lot that could be done purely, and construction of new Game objects is very cheap as most of the data is shared between old and new, unlike in python where it is literally a copy of the previous object and all its data.
I spent all day doing this before thinking about it on the way home from the store and realizing it should probably just be some normal functions instead of `ErrorT String (ReaderT MyState IO) Whatever`. What a mess.
I think that's part of the misconception, yes. Sure it's easy to crap something out in python *very* quickly that does what it looks like it does most of the time. If that's all you're interested in then I'm not convinced Haskell is any faster or more suitable. However, if you were to rewrite the turd of an application you made as a proof of concept so that it actually *works reliably* and has known failure modes it would take you much, much longer and you'd probably never really feel confident in it, relatively speaking. I think a lot of Haskellers are talking about that second style of writing when they praise the language and most other people that would bother to complain about it are thinking of the first.
Thanks for your help, you got some good points! I'll definitely use hlint in the future, guess I'll configure my editor to run it automatically. I've used IORef because I was already in the IO monad, even though State may have been the better option... Still need to look into monad transformers. Lenses still scare me away but sooner or later I will need to deal with them. Thanks for your effort :)
Thanks for your input! I've heard about lenses, but they seemed like overkill and too confusing for this little project, so I stayed away from them, as /u/dalaing suggested, I'll probably leave them alone for now.
Right, those are remnants of earlier experiments, I didn't get the function working on the first try. I'll try to clean them up and remove the unnecessary IO work, I thought it might produce cleaner code in the beginning, but I probably should've used State for that. Thanks for your thoughts!
An update to the section on the writer monad would be nice. The examples don't work anymore. I wouldn't mind a chapter on monad transformers either. Lens maybe?
You're right to doubt that interpretation. I can confidently say that Edward isn't expressing pride in lens being hard to learn. The lens library explores a powerful way to generalize many algorithms and to build up complex data access simply. Discovering all of this was a wild journey, and bringing new users along can be challenging. This is especially true when people arrive thinking lens is about updating records only to discover that the well goes much deeper!
&gt; 50 years Well you'll never stop learning, but that doesn't mean he won't be able to do big things with good code after already three years.
This was way too accurate.
That looks a lot like some code I've been working on, and I've been having similar concerns. Does anyone have a good rule-of-thumb for when to use this kind of stuff?
I've got a fork with StateT in the mix [here](https://github.com/dalaing/Huno). It still needs some cleaning up (for example, I could have made better use of `modify` and `gets` when dealing with the state), but I'll get to that eventually. I was about to start hacking WriterT into the mix (for educational purposes - there are reasons to avoid Writer), but it doesn't look like it'll work well with the prompts for user input.
Wait, so where do the pigs and fireworks come in?
I keep wanting for rules-of-thumb, then looking back on those wants a month later and realizing that I just get it now, and don't need any thumb-rules. So wonderfully frustrating.
Classes on category theory form a category with morphisms from the domain of information on category theory to the codomain of neurons in your brain that represent that information, with identity morphisms that map those parts of your brain back onto themselves, which form your memory of whatever it is you've learned. Good luck!
&gt; I've used IORef because I was already in the IO monad `main` is in the IO monad, so why *ever* leave? Turn this thinking around. Escape to purity ASAP. :)
I think monads need about 12, parallel learning curves, some with loops and barrel-rolls, as there are least that many non-overlapping, wholly-unrelated ways in which I learned them over the course of a year. I really grokked the burrito thing, while simultaneously squinting my eyes suspiciously at the spacesuits thing, and nodding along to the Alice metaphor, while straining mightily under the onslaught of the "just a bunch of examples to motivate intuition" tutorials and videos, and tilting my head askance while being told that burritos are stupid, and it's really all just concatMap, and tilting almost onto my side with that gonads video, yet feeling a light at the end of the tunnel with John Hughes' "Monad and all that" videos, while feeling like I was overthinking it all, as the types were so simple-looking, and all of that while secretly seeking salvation with a fish symbol and the reverse fish (cofish? Which one to believe in!?), which might be an easier way to view all of it, as the laws work out better. ***WTAF?!***
I think imperative is very natural for humans. We evolved doing operations in order. Kill something, skin it, eat it, sleep, make some babies, try not to die, repeat. `goto` was so obvious; we're practically born crawling from one place to another. OO makes a ton of sense to us, too. I remember the first paragraph I read in a book on OO, thinking "This is great! What a smart idea." Of course a dog is an animal. Of course a tree has leaves. It felt like I already knew everything, because it immediately seemed to model my view of the world around me. Even the confusing bits were wrapped in cloaks of familiarity, comforting me, and urging me forward (into horrors untold). I would argue that the earliest bits of FP are not very natural to humans. Algebraic data types had me squinting and scratching my head when I first saw them. It took a few days before I started to believe I knew what they were. I needed a bunch of descriptions, corrections to my assertions, and time to let it all settle. I'd never seen anything like this before. It looked so simple, but I just couldn't fit it into my understanding of making a computer do things. I couldn't match it to anything I knew. I was stranded. Imperative words were easy - if some condition, do some thing, else do some other thing - that's straight up English. For loops didn't take much work to get - do this thing to/for/with every item in this collection of items. Slightly harder - especially syntactically - but not bad. While loops were also obvious. OO words - many of them - are fairly obvious - implements, interface, hierarchies, is a(n)/has a(n), instance, etc. They're fairly attainable to the novice. FP words are anything but easy - functor, applicative, monad, algebraic data type, product/sum types, higher kinded types, categories (which aren't what people think of as categories), return (which doesn't return anything), co/bi/endo/homo versions of everything... Mom never told me to map setting a place for a person at the table for every person in the list of people who will be eating dinner tonight. Every single step starts with complete confusion, and a climbing out of the confusion into a new world where I know things that feel very new and unrelated to a lifetime of experience. With imperative, I felt like I was just telling a computer the same things I was telling other people and myself all my life. With OO, I felt like I had learned an abstraction that in most ways seemed to model my view of the world around me. With FP, I feel like I'm changing as a person. I feel like there are all these things around me in the universe that I've never known about, and FP is not only teaching me that they exist, but also how to harness their powers. It's very powerful stuff, but it's anything but obvious and easy. Things *become* obvious and easy (moreso than ever before) the more I learn, because these are really beautiful abstractions, but they're not beautiful because I grok them quickly, as with OO, but because I'm finally pretty much doing math, which is to say I'm finally respecting reality to some degree. This is the first time I haven't felt like I'm being clever and coming up with some cool, inventive way of doing something, but rather, I'm feeling like I'm working hard to learn how the universe works, and then just deciding what I want to do, and using the tools the universe provides - which we've worked diligently to discover - to make those things happen. It's humbling, because the code isn't about me anymore; it's about natural laws and such. It's lifted my efforts from how to do things up to the actual things themselves. I'm having to learn how to more appreciate having good ideas to then represent in code, rather than showing off "cool code" around some ideas I have.
Intellij and the Haskell plugin do what you are looking for, I believe (auto completion given an instance of the expression). As far as loading documentation, I have not yet found a way to configure or do this. I typically google it, if the expression is not self descriptive, but switching between windows is still a minor annoyance.
"You mean this *does things right now*?"
Regarding ADTs, Slightly silly, but without Erik meijers channel9 haskell lectures I don't I would've ever grasped that ADTs are the dual of OO abstract/Concrete base classes, and that pattern matching is the dual of OO polymorphic dispatch. This helped a lot to relate something I already knew (OO) to the new and unfarmilair (FP) 
I really liked PhP.
It's not much of a big deal for `Num` and `Integral`, because in practice most implementations `toRational` and `toInteger` are total. Although there are indeed some exceptions, and it's still embarassing. But `OverloadedStrings` is a whole new ball game. There are partial `fromString` implementations all over Hackage, and in my opinion that is a big deal. Here is an example: look at the `IsString` instance for `Name` in the [xml-types](http://hackage.haskell.org/package/xml-types) library. If you mis-type the "Clark notation" for an XML namespace, your string literal will throw a run-time exception. In the XML world, it is really convenient to be able to specify the namespace of a name using the standard Clark notation. And that notation really does need its syntax checked. A TH dependency isn't appropriate for this library, which is intended to be a lightweight fundamental XML library with few dependencies that only provides a basic ADT for XML. So the only real solution is some kind of mechanism to parse string literals at compile time.
Heh, I guess it's a small consolation that there is now another mainstream language whose literals can throw run time exceptions. :)
wat
So where are these great reliable Haskell programs? I haven't found Haskell applications to be more reliable. Well, it's hard to say because there are so few of them, but my experience hasn't been good: in particular in the week that I used it, I had xmonad lose windows or crash a couple of times, which is just comical for a 500LOC program whose tagline was 'the first formally verified window manager'. git-annex was an awful experience as well. I never used darcs, but people seem to have a lot of issues with it.
In my experience, fast Haskell code is (on average) very difficult to write and looks very ugly[1], and at the end of the day you /might/ reach the performance of idiomatic C/C++[2] [1] http://benchmarksgame.alioth.debian.org/u64/program.php?test=nbody&amp;lang=ghc&amp;id=2 [2] http://benchmarksgame.alioth.debian.org/u64/program.php?test=nbody&amp;lang=gcc&amp;id=2
I don't think the benchmarks game represents contemporary fast Haskell. Things have improved greatly in various aspects and many of the benchmarks are written to run well on very old GHC versions. That said, I agree that reaching C/C++ speed is often impossible, and near-C speed for inner loops may often take more effort than it does to write it in C (it's pretty much what my second bullet is). However, what's nice with Haskell is that unlike Python/Ruby/etc I get all the productivity while having *decent* performance out of the box. Python/Ruby have abysmal performance and anything remotely algorithmic that can't be offloaded to numpy/etc will be terrible.
It's true, but it's way more common in Haskell. 
Haskell's productivity varies wildly I think. Haskell programs tend to be very rigid, so if I'm familiar with the problem I'm solving and can plan much of it in advance it works OK, but more loose programming can be difficult to impossible, while in C or Python it might merely be somewhat inelegant looking. But it's a lot more pleasant to improve inelegant working code than do the n-th rewrite of Haskell code that doesn't do what you need (and, for all you know, might never do what you need). Since you mention Python and Ruby, have you looked at Julia or Nim? They're both designed to look and act like dynamic languages while having performance consistently close to C.
[Fixed](http://i.imgur.com/TTBBeJs.jpg). :P
&gt; Turn this thinking around. Feels like I have to do this quite often when switching from imperative to functional programming, I'll try to stay as pure as possible in the future :)
Oh okay - I will keep that in mind when learning lenses, seems like I already got a use case then.
The verbosity can be both good and bad. I have a tendency to not read the whole thing and try to "second guess" what the problem is likely to be. The guessing works about 70% of the time and is usually fairly quick (often the result of typos). If it doesn't and then I read it more carefully and try to analyze why the types don't fit together even though my brain thinks they should. This part can take a lot of time though, depending on the complexity of the code. It is a bit more verbose than needed: some of it is just telling me *where* the problem ("In the expression: blah, In a case alternative: blah, etc"), even though it already told me the line+column numbers! To help with this I've written a wrapper that just prints a line containing where the error occurred with an error pointing at the offending value.
&gt; as point-free as possible without sacrificing readability I like to call that "combinator style". You express a function as a composition of simple operations. It is often the most readable to describe only what the function *does*. If you are always forced to mention the input and output of every step as a variable, you often find yourself struggling to find names that won't confuse the reader, when the simplest and clearest would be just not to mention them. Of course, there are times when it is more clear to give an explanatory name to an intermediate result. There is no need to be religious about combinator style, either. Both styles are always available in Haskell. You can do what is clearest in each case.
You can watch all 31C3 talks via the streaming website: http://streaming.media.ccc.de/ Recordings will also be available.
Simon Peyton-Jones' article [Practical Type Inference for Arbitrary Rank Types](http://research.microsoft.com/en-us/um/people/simonpj/papers/higher-rank/) destribes a "real" system (more akin to what's actually used in Haskell) and reads like a tutorial. It comes with an example prototype too. 
This looks great, State and modify were really the things that I've been wanting. I completely forgot that the State is passed to subfunctions too and can be modified there. It's nice to have the game everywhere inside `State Game`. You've also broken up the function into little pieces, especially the whole game modifying thing in doMove, it really looks cleaner that way. And you've removed the batches of `where` blocks, which I'll use more sparingly in the future. I'll study your code more and adopt your changes - thank you for the help :)
I use typed holes A LOT in my workflow. Just drop a typed hole in a place where you still need to write an expression. and the interpreter poops out a nice type signature. And then usually with some hoogling or thinking I can come up with an implementation. In my ideal world it would look like this though: Any place I have a typed hole in my code I get a dropdown with all functions that are in scope and that fit in. 
Nice overview of some nice posts about folds. Thanks! I had to go read the posts themselves again to remind myself what all of this is for though - a few more words of summary at the beginning would have been helpful.
Nice post. As always. Note, that the author since posted a supplementary package on hackage that provides a few batteries: [folds-common](http://hackage.haskell.org/package/folds-common). I assume its content will be part of folds at one point.
This, or something like it, is indeed the only way you'd be likely to use it in the list monad. But `filterM` is also useful in other monads, such as the IO monad or a state monad, where your filtering function happens to have side effects in the monad. It comes up all the time.
How do the transactional guaranties of `VCache` - which seem to be implemented entirely on the Haskell side using `TVar`s - interact with the transactional guaranties provided by the database backend, when using a backend that also provides that? Same question for acid-state and `TCache`.
It's organized enough to replace Google for my documentation needs, which makes me very happy inside. 
Nice idea. A few random comments: * As usual for this kind of AST, you probably would rather write `Expr` as a GADT. That way, your type could prohibit things like piping into a comment, for example. * It seems to me that `Quote` and `Quotable` don't exactly describe what you are trying to do. You probably won't be writing any `Quotable` instances that don't go via `Text`. My first instinct - without thinking it through - would be to make `Quote` a newtype wrapper for `Text`, and then type the `Quotable` method as `quote :: a -&gt; Quote`. It's worth it to devote some thought to this design, because you're going to be needing something similar for `$()`, `&lt;()`, `$(())`, etc.
Have you see Jason's [bash](http://hackage.haskell.org/package/bash) package? EDIT: Two other previous efforts are [language-bash](http://hackage.haskell.org/package/language-bash) and [language-sh](http://hackage.haskell.org/package/language-sh). A lot of nice ideas there, and a lot of good work for you to re-use. But, hmm, no one seems to have thought of the simple idea of using a GADT...
First time I've looked at one of these "Examining Hackage" posts&amp;mdash;and I think it's a great idea. I find myself wishing other languages had equivalents.
I did go through a phase in which the solution to everything was a monad transformer. I'm past this now, except the solution to everything is now a [combinator library](https://www.youtube.com/watch?v=85NwzB156Rg&amp;list=UUj3bTCRO7WUVBQLtOhnc6Sg). Is that also just a phase?
If you wanted to get really ambitious with your ideal world, it could include some kind of djinn-like functionality to suggest compound expressions of the right type built from in-scope terms. I'm thinking of an experience similar to the "code editor as proof assistant" model you see in (IIRC) Agda, Coq, etc. You could even throw something like Hoogle into the mix too - think "this expression would fit, if you imported that". [edit] it occurs to me that the combination of Djinn and Hoogle would quite possibly be intractable. Still, either on its own could be pretty snazzy to have hooked into something like an autocomplete dropdown.
For your enjoyment, have a look at the @oeis lambdabot command on the #haskell Freenode IRC channel. For example, given the command `@oeis 2,3,5`, the output begins with `[2,3,5,7,11,13,17,19…]`.
Haven't done Julia, but after having done Haskell I don't feel the need for dynamic typing. Ironically, I believe this "rigid" typing makes my Haskell programs much more flexible. I can feel safe making drastic changes to the code and after I fix all compiler errors, my code will be in good shape. Often it will even work as soon as the huge rewrite compiles! I'm not sure why you feel Haskell is my more rigid. I think I also felt this way before I learned to use combinators that let me lift code from one type to the other easily. For example, refactoring types before the lens library was significantly more painful.
Without watching the video I would say that yes, anything that looks like a solution to every problem is just a phase. After all there is no Silver Bullet.
Purescript would be a relevant real world example I guess. Not sure if Phil has written anything up or not?
I like Simon Peyton Jones's paper on [Henk: A typed Intermediate Language](http://research.microsoft.com/~simonpj/Papers/henk.ps.gz). My [morte library](https://github.com/Gabriel439/Haskell-Morte-Library) is essentially a Haskell implementation of that paper with some very minor deviations.
I'd very much like to see part 3 of the series, but I doubt there will be one... Then again, his blog was not updated since July, so he just might be short on time to spare.
&gt; Without watching the video I would say that yes, [...] Oh, the video isn't explaining why I think combinator libraries are a solution to everything, it's a talk recently I gave about combinator libraries, for an audience who is not familiar with Haskell. &gt; [...] anything that looks like a solution to every problem is just a phase. Perhaps I should clarify my viewpoint. It's not that I now use combinator libraries *instead* of a stack of monad transformers for everything, it's that my understanding of the solution space has expanded so that I now see that everything *is* a combinator library. Monadic composition is one way to compose components together, so any solution which involves monads is a "monadic combinator library". Function composition, categories, monoids, applicatives, indexed monads, lenses, everything seems to consist of primitives and ways to combine those primitives into larger systems. The tricky part, of course, is to figure out what those combinators should be in each concrete case. Anyway, I find this way of looking at things very convenient, because it unifies everything into a single framework. But if anybody went through that phase and now thinks that combinator libraries are an instance of something more general, I'm all ears.
That is a challenge. The idea tends to be that the semantics are properly continuous as the sampling rate tends to 0, but that can't help for something like what you're asking for. I think this is a big problem with many of the event semantics in (continuous) FRP. Amsden's TimeFlies solves this by allowing non-deterministic interleaving of "simultaneous" events. That might be sufficient.
Isn't Kaleidoscope a [single-type language](http://www.stephendiehl.com/llvm/#the-basic-language)?
Well it's the changing the code part that's often the problem. Changed a product type? You get to rewrite everywhere that you've pattern matched against it, etc. The whole "my code compiles so it's correct" thing varies a lot. Certainly I've had plenty of runtime errors in Haskell. I share your preference for static typing, but I think Julia is a nice language. You can add type anotations, it has immutable types, parametric polymorphism, sum types, etc. Plus the looser typing makes it possible to define things like scalar * vector, which, after suffering the operator soups in Haskell seems like heaven. The performance is great too. I rewrote the nbody benchmark in Julia: https://gist.github.com/dvolk/bc4a6c1fae88e80819c3 this runs with the same time as the C version above, and does 0 allocations
You're talking algorithms. I'm talking "doing things." One of the first things I wrote asked you a question, let you type in an answer, then told you if you got it right or not. I wouldn't exactly call that an algorithm. It was in BASIC, and probably looked like this: 10 PRINT "What is the secret word?" 20 INPUT word 30 IF word = "puppy" THEN 40 PRINT "You guessed it!" 50 ELSE 60 PRINT "Nope!" 70 END I would have been around 8 when I did this. I didn't have to learn much, and it went pretty much straight into my brain and made sense. There's almost no abstraction. In two days I was already writing a bunch of little programs, using a few keywords, and basic logic. In contrast, I've - as an adult, mid-30s - spent exactly a year now (I started last holiday break) reading and reading and reading and reading about Haskell, spending an extraordinary amount of the time being completely baffled and confused, and I'm only just now beginning to attempt to write some real code. Almost nothing has just gone into my brain. I've had to fight to understand each bit, and the good folk of #haskell on freenode get into debates constantly about how best to explain these difficult topics. We have a host of poor monad tutorials and analogies, but no one ever says "This if/then/else tutorial is all wrong - this is just confusing people! The best way to learn if/then/else blocks is through lots of examples and just using them." I thought IO contained things of its type. I thought Haskell was pure, *except* for the IO stuff. I had ADTs wrong for days. I had newtypes all confused for a couple of weeks. I spent two nights getting my head around fmap, and a week or more on functors as a whole. The things you're describing come later. Pointfree is nice and elegant now, but blew my mind when I first saw it. Seeing monoids all around me took months to start happening, and it was all quite confusing at first; it felt like a totally random grouping of unrelated things for confusing math reasons. I never had to fight to grok anything in imperative. It was all just things to do, and maybe 3 or 4 block structures, which also took almost no effort to get. I'm not arguing for imperative's goodness. I'm saying it's basically devoid of abstractions, and Haskell is born of them. Abstractions are necessarily more work to grok, because they're at a higher level than simple imperatives.
The whole `Box`/`An` thing is a clever trick worth calling out. data Box a = Box a deriving (...) newtype An a = An a deriving (...) Most of the time people say that a newtype is just a more efficient single element data constructor, but it also has different laziness properties: newtypes are strict! This is great in their usual use case of swapping out some instance definitions since it won't introduce a new layer of indirection and thunking. On the other hand, `Box` does what's on the label and boxes up a value in an extra layer of indirection and laziness. As shown it can cleverly lazy a strict fold. I've never personally seen any other use, but it's good to know how it differs from `An`.
Well I can't refute your personal anecdotes, of course, but there is an important point to consider, which I feel many people totally ignore and treat as a normal "bug": - Implementing a program correctly requires that your algorithm is theoretically correct. When I say that I'm more confident that my Haskell program is "correct" over the same thing in Python, I'm talking about bugs that stem from programmer error, where I've forgotten about some invariant or data-dependency or some other feature that's easy to overlook. For example, if your program assumes the input is sorted but at runtime you discover that it isn't, Haskell is not going to save you. However, if your program requires that action B absolutely never happens before the corresponding action A, then being able to express that statically is really valuable. This is doubly true when you start introducing things like concurrency and undo-redo functionality, which I've had to retrofit onto an application more than once and is a complete nightmare. If programmers never made these errors then the only real argument for not using C everywhere is verbosity, which is clearly not the case. As for my own anecdotes, I've used XMonad for a few years now and despite *trying* to do so (writing weird programs that create and destroy windows, resize themselves, write non-ascii gibberish to `WM_*` properties, all at high frequency) I've had no luck crashing it :(. I'm on a laptop as well, so in normal use I'm constantly doing suspend-resume, often with the multi-monitor configuration changing while the computer is asleep, etc. I've been very happy with it.
Oh, I would certainly agree that composition is a very central concept in programming in general and code reuse in particular.
http://i.imgur.com/hyW52Y5.jpg
Syntax-highlighted version: https://gist.github.com/chrisdone/0075a16b32bfd4f62b7b
I've updated with a quick two sentences about what "Examining Hackage" is. Hopefully that helps a bit :)
It's also worth pointing out that one of the traditional advantages of DRY is canonicity of implementation, but once you have really polymorphic types free theorems start kicking in and enforcing canonicity for you. That's a fancy way of saying that there's only way (with caveats) to implement: swap :: (a, b) -&gt; (b, a) ... so there's not much benefit to having a canonical implementation and it's perfectly fine to just write it yourself instead of importing it.
&gt; You get to rewrite everywhere that you've pattern matched against it, etc How is that different from changing the fields/methods in an object - and then fixing all the uses of those? &gt; The whole "my code compiles so it's correct" thing varies a lot. Certainly I've had plenty of runtime errors in Haskell Indeed it varies. For some code, the type checker only helps a little, and the code may be riddled with runtime bugs. But for others, it's nearly a pancea :) Still, the overhead I pay for static types seem very small to me and there are also expressivity gains compared to dynamic typing (e.g: QuickCheck and other type-class magic). &gt; but I think Julia is a nice language. You can add type anotations, it has immutable types, parametric polymorphism, sum types, etc That sounds nice indeed. &gt; after suffering the operator soups in Haskell seems like heaven Well, that's more a library issue, isn't it? There are some alternative preludes which have more polymorphic multiplication operators, etc. 
It's ugly, but not difficult. The trick is straightforward, albeit verbose and unsafe: * Learn about strictness and `UNPACK` annotations on data types * Use the `Foreign.*` modules in `base` to translate verbatim everything you do in C to Haskell This very reliably gets within a factor of 2x-3x performance of C and it's actually easier to write than the equivalent C code despite the verbosity. Then you wash your hands of the ugliness by wrapping it in a nice functional and typed API and you are good to go.
Giving suggestions for typed holes could be huge. As someone who's not very familiar with the libraries, it's nice to find functions that fit exactly what I need. Doing it without an external browser could be great.
If I understood yitz correctly he meant overview of folds. E.g. The canonical `avg xs = sum xs / length xs` and its problems.
I agree, but not at the same level, perhaps. For loops are a bit of a pain. I made a point of not giving them too much of a free pass in my earlier comment. I think that was about the limit for the first few years of my learning, though, starting around age 14. Nothing was more complex than for loops for awhile, and most things weren't that complex. I think the abstractions in Haskell, and FP in general lead to simpler, more readable code, but that comes later. You have to learn each abstraction first. In BASIC, they said that `PRINT "something"` was the way to print something, and I got it in seconds. In Haskell they say `putStr "something"` prints something out, but you have to either be inside of main, or running in something in GHCI. Underneath, there are more compliated things allowing the BASIC variant, but there was no way to get to them, nothing more to learn about `PRINT`, and no one talking about monads, do notation, and 8 other things surrounding the topic. You're edging toward better code right away. I was really discussing more the level of kids jumping in and creating things, as I did at 14. Haskell would have melted my mind at that age. C++ was too much for me at that point, especially with no guidance. I grew up in a remote area, with no technical people around me, and long before the internet and cell phones. I think you could teach a kid `map` pretty quickly. I think functors, applicatives, why do notation matters, and a host of other 'basics' of Haskell/FP would be a bit more of a climb. I'd love to learn that I'm wrong, though. I don't have any kids in my life to experiment on, unfortunately.
Yes, IIRC the only type is Float. But it is statically typed :-)
http://streaming.media.ccc.de/relive/6162/
I've thought about this a fair amount with IHaskell, and found that the best thing right now is large quantities of `:t` and trying cells, evaluating them, getting type errors, and fixing them until it works. I'm interested in using type holes to get smarter autocompletion in IHaskell, though... I imagine it should be possible to get autocompletion that respects types, so that in the following context: main :: IO () main = p&lt;CURSOR&gt; 1 triggering autocomplete would only yield `print` and not `putStrLn`, as the types don't match up for the latter... Maybe a feature that I will someday have the time to implement for IHaskell :)
Notes https://github.com/runarorama/lambda/blob/master/LambdaLambdaJam.pdf
i would assume that `parseRelativeFileLoc` would only make sure the location is valid, not that a file exists there
(For the downvoters, I think that's a fair question and I want to know the answer as well. I could look them up, but it's more meaningful to understand why they're considered experts on DRY/DAMP/What-Have-You.)
Wow, thanks for all the responses. Lots of great examples here!
I thought data produced in TH could be inlined into the result, so long as data constructors aren't hidden? So, I guess the issue is that with smart constructors they usually *will* be hidden?
i.e. Use `fail` instead of `error`.
They're both Rubyists but I think the general principles apply cross-language. Sandi Metz has a [book](http://www.poodr.com) on OO design and given a [lot](http://www.confreaks.com/presenters/211-sandi-metz) of [talks](https://www.youtube.com/results?search_query=Sandi+Metz&amp;oq=Sandi+Metz), [here's](https://www.youtube.com/watch?v=8bZh5LMaSmE) one in particular about this subject. Corey Haines is less well known but shows up here and there in the Ruby community - he has a [book](http://articles.coreyhaines.com/posts/i-wrote-a-book/) coming out and has appeared on [podcasts](http://devchat.tv/ruby-rogues/186-rr-the-4-rules-of-simple-design-with-corey-haines) about this subject, but by no means is an absolute authority on the subject. Here is a [course](http://www.pluralsight.com/courses/advanced-unit-testing) about brittle tests in C# (DAMP) by someone else. I think the general observation I'm trying to make is that it's not that these are authority figures we should listen to, but rather just paying attention to the general sentiment circulating in those communities. I would also try to not balk at *good* ideas coming from the OO community - they will not *all* apply to FP, but when they discover issues with their designs, it just might be an issue that's general enough to apply to programming in general, that's worth paying attention to. Here's another example from Sandi's book: Code should be TRUE: Transparent - The consequences of change should be obvious in the code that is changing and in distant code that relies upon it Reasonable - The cost of any change should be proportional to the benefits the change achieves Usable - Existing code should be usable in new and unexpected contexts Exemplary - The code itself should encourage those who change it to perpetuate these qualities The do not seem like OO-specific concepts to me. I'd love to see more code, in any language, with these qualities!
Fantastic demos -- clear, practical, and easy to understand. This is what I hope the future of programming looks like.
Talk starts at 0:15:25
If you ever need to combine continuous-time and discrete-time dynamics, along with continuous-value and discrete-value states, all in the same simulation model with complicated feedback loops and state transition logic, it becomes a serious effort to implement by hand and quite difficult to get right. If you've ever used Simulink for this kind of thing, you can see why it comes up as an irreplaceable feature of Matlab that prevents many of its users from being able to switch to Octave, Python, Julia, or other alternative environments. Aivika looks interesting, I'll have to see if I can figure it out.
 Continuous time was the founding idea of FRP and one that still matters very much to me. I'm always sorry to see it overlooked or dismissed. I made some notes a while back on [why continuous time matters](https://github.com/conal/talk-2014-bayhac-denotational-design#why-continuous-time-matters). The reasons are much the same as for infinite data structures (and hence non-strict evaluation) and the usual reasons for favoring continuous (&amp; infinite) space (mainly modularity). See also [a StackOverflow answer](http://stackoverflow.com/questions/5875929/specification-for-a-functional-reactive-programming-language#5878525) (and links found there). 
Thanks for the links! I agree that the OO community advice can often apply in FP just as well (though I admit some derision for acronymatic advice like TRUE even while agreeing with all of its points). Honestly, I think there's a big hole in Haskell-like FP as you get to larger, more modularized programs. I'm not sure that OO has all the solutions, but they have a lot of experience!
Is it just me or are those curves not actually difficulty curves? It seems to suggest your gaming skill grows incredibly quickly with EVE, which can only be a good thing?
There are plenty of ancient algorithms expressed functionally, too. For me the example that first comes to mind is the calendar algorithm of Maimonides, written in the early 11th century, which not only was written purely functionally, but even came with non-trivial Quickcheck properties. Medieval English church bell ringers expressed their permutation algorithms recursively. The first design for a Turing-complete computer, although it was never actually built, was hardware based on a recursive functional design, not imperative, and 19th century mathematicians around the world immediately set about writing algorithms for it. Of course, there are plenty of examples of ancient algorithms expressed sequentially, too. And probably plenty that could be interpreted either way. When you're not tied to a pre-conceived notion of algorithm that is tied to a particular kind of hardware, you write algorithms in what ever way is most natural - either sequential, or not.
I think the example you give is a little overblown. What if we were to use a modern imperative language like CoffeeScript instead? "Put all the objects on the table into the bag" becomes putIn(bag, obj) for obj in table Barring some obvious grammatical differences such as word ordering and `in table` versus "on the table", it starts to look a lot more like natural language. My point is that your example of an English version of a piece of imperative code seems to me too specific to a particular language or level of abstraction.
Some universities now teach Haskell as a first programming language, and they report that students have no more trouble with that than with an imperative language. A recent post here by an adult who learned Haskell as a first language on her own did report problems, but that was more because of the toolset and the surrounding culture. But you yourself say that your situation was completely different - you were ingrained with exclusively imperative programming for almost 30 years, ever since you were 8 years old, before you came across Haskell. Of course it was much harder for you.
Seriously. This subreddit has 18,502 subscribers. If only 1% of them built a useful Haskell application we'd have 185 useful Haskell projects to show the world. Get to work, people!
Sure, I just couldn't be bothered figuring out the exact code for that in my example. :-)
&gt; I guess the issue is that with smart constructors they usually will be hidden? Yeah, indeed. 
I pitched in, it's the least I could do. I encourage all who can to do the same so we can keep improving Haskell :)
Keep experimenting, having fun, and submitting code for review :) It gets more exciting.
Thanks for the comments. Especially thanks for noticing that piping to a comment would generate broken shell code. I have fixed that. I have seen at least 2 (now 5) other haskell libraries that provide some form of DSL for shell. A lot of them have nice and detailed data types that reflect a full AST for shell, and I agree GADTs would be useful there to ensure it all fits together in only valid ways. I realize that the Expr I'm using is relatively unprincipled by comparison, but as long as Expr represents any valid shell expression, they can be combined in arbitrary ways without needing to prohibit anything illegal. I think that Quoted L.Text is more evocative of what's going on than a newtype. Also, I might want to use strict Text later. But, I'm only used Quote for single-quoting of literal values. The necessary double-quoting to safely expand $var and $() and $(()) seems like a different animal (said the blind man). It does support $() and even $(()) shell arithmetic expressions generated by haskell code like "succ (val var)", which gets converted via a nice data type into "$(( $var + 1))" BTW &lt;() is a bashism, and the same thing can be accomplished portably using a shell function. I am not implementing any bashism support. FWIW, I am using GADTs in other places. For example, my Test data type is a GADT, and this lets it ensure that the shell vars in "test $x -eq $y" are (intended to be) numeric values and not strings. -- Are the Vars equal? (Compares integer to integer, not string-wise.) TEqual :: (Integral p, Integral q) =&gt; Term Var p -&gt; Term Var q -&gt; Test Hey, here's haskell code to generate a shell script that does fib! main :: IO () main = T.writeFile "fib.sh" $ script $ takeParameter () &gt;&gt;= fib &gt;&gt;= cmd "echo" fib :: Term Var Integer -&gt; Script (Term Var Integer) fib n = do prev &lt;- new1 acc &lt;- new1 forCmd (cmd "seq" prev n) $ \_ -&gt; do setVar acc (val acc + val prev) setVar prev (val acc - val prev) return acc where new1 = newVarContaining 1 ()
I'm really sad whenever I see FRP bandied about without mentioning continuous time. I have no problem with event transformers either, but I just don't want one to muddy the other too much.
A google search doesn't turn up anything on hackage AFAICT, but you might roll your own with https://hackage.haskell.org/package/statistics, perhaps looking at https://hackage.haskell.org/package/statistics-linreg for inspiration.
I also bought it because I wanted a simple intro to FRP. After reading the first two chapters, I feel I got my money's worth.
I don't think this is less icky than fromJust.
I would be happy to donate USD $100 right now, but the pay server requires Adobe Flash, and for whatever reason it isn't running in my browser properly.
The donation site doesn't seem to work. Bitcoin address?
We develop and use haskell web services, haskell web applications, and haskell services that work off the queues from database tables. All deployed on linux and connected to MS Sql Server. I'd say it is as much "enterprise" as any other stuff. For haskell web services there are simple micro web frameworks you can use that are very easy to learn, like Scotty. As for the ESB, i think it is yet another over-engineered solution that chases the pipe dream of enabling an army of cheap and replaceable coders to create a non trivial solutions without much knowledge. Just like whole EJB fiasco in java world several years ago, and UML diagrams that were supposed to generate code before that. The entire idea is a wet dream of clueless and misguided CEOs. 
Another option would be to use quasi-quoters: [uri|https://www.reddit.com/|] :: URI [uuid|550e8400-e29b-41d4-a716-446655440000|] :: UUID That's a bit harder for `NonEmpty.fromList` - you'd need to embed a Haskell parser in your quasi-quoter to fake anti-quoting.
Yes. [HLearn](http://github.com/mikeizbicki/hlearn) supports logistic regression along with a number of rather fancy modifications (e.g. lots of non-standard regularizers and many different optimization techniques). Unfortunately, the code base has been a bit unstable over the past year and will be for the foreseeable future. It's not exactly user friendly for people other than me to use right now. If you're goal is to just "get some stats done", I'd definitely recommend against haskell at the moment. If you have some other goal in mind let me know. If your goal makes sense to use HLearn with, and you're willing to do a bit of digging through some rather unconventional code, I'd be happy to help you out.
I think you can avoid the `Lift` requirement if you have the TH function return something like `fromJust (fromFoo "string validated at compile time")` instead of the raw data structure. It's a bit of a hack, though.
I tend to use `where` blocks and very few `let`blocks - but I tend to use the `where` blocks when the contents of them need access to something from the surrounding scope. If they're independent, you may as well separate them out (and, if possible, generalize them).
&gt;it's actually easier to write than the equivalent C code I find this claim laughable for at least two reasons: 1. C has pretty nice array/pointer arithmetic syntax. Haskell's, in comparison, is crap. 2. Tracking down segfaults in haskell programs is the most hellish programming experience I have ever had. 
&gt; nobody provides instances of Data That's indeed disappointing, but not very surprising, since that typeclass isn't very widely-used yet. Did you try sending the package maintainers of those data structures a pull request? Since `Data` is in `base`, there doesn't seem to be any reason not to accept it.
Hi all! I kept planning to polish this more and more but I've finally decided to just release it and get feedback asap. This is my first Haskell library and I would *highly* appreciate any and all feedback. There is also an example: https://github.com/codygman/hs-scrape-paypal-login
Haskell has shakespeare suite of languages for writing type safe documents. (Use Hamlet for producing HTML). Also, Chris Done has written a [DSL](https://github.com/chrisdone/lucid) for writing HTML. Also, note that Haskell is quite a practical language.
There are, but it's just hidden by nicer syntax. `putIn` is still an impure procedure, as is the verbal command to "put" the objects in the bag. I think the sentence "put all the objects in the bag" is definitely an imperative instruction, whereas "all the objects are in the bag" is a functional/declarative one. Possibly "I want all the objects to be in the bag". Doesn't say how to go about doing something, just describes the desired outcome.
Can you switch of that platform in the future and to something more donation friendly, like maybe paypal?
Sadly there is no equivalent of R currently in Haskell. You could look at this for a start http://idontgetoutmuch.wordpress.com/2013/04/30/logistic-regression-and-automated-differentiation-3/ You might be better off using the standard formulae via HMatrix: https://hackage.haskell.org/package/hmatrix Much as I dislike the "if I were you you I wouldn't start from here" type of answers, I really wouldn't start from here if I were you. Unless you really, really need p-values because of some exogenous reason then you should employ a Bayesian approach. It should be straightforward to create in Haskell but if you want to get something running quickly then I recommend stan http://mc-stan.org If you are used to R then it can be used in a moderately straightforward way with R (the stan program is passed in as a string - yuk). You could probably run the stan program from the command line and analyse the results with Haskell though. For a pure Haskell approach to (Bayesian) logistic regression you could look at these for pointers http://idontgetoutmuch.wordpress.com/2014/06/15/gibbs-sampling-in-haskell/ http://idontgetoutmuch.wordpress.com/2014/04/09/gibbs-sampling-in-r-haskell-jags-and-stan/ http://idontgetoutmuch.wordpress.com/2014/03/20/bayesian-analysis-a-conjugate-prior-and-markov-chain-monte-carlo/ I am currently working on a blog post on estimating stochastic volatility probably using Julia / stan rather than Haskell. Let me know which approach you are going to take and I would be happy to help. Creating a logistic regression package (preferably Bayesian) for Haskell would be great!
Replying some of your thoughts: &gt; I would have been around 8 when I did this [in Basic]. I guess you were even younger when you could understand this: (3 + 4)(5 + 2) which is a non-imperative algorithm description. So I wouldn't say that imerative is more natural for humans. If we are free to choose the problem domain as you did, we could easily find convincing examples for both sides in the argument. Describing interactive processes seems to be easier in imperative style. But you can do this with the IO monad like this: putStrLn "What is the secret word?" word &lt;- getLine putStrLn (if word == "puppy" then "You guessed it!" else "Nope!") If you need more complex interaction then imperative style will lead to solutions like the callback hell. We definitely need a declarative way to describe interactive computation.
hello! If you want to get usefull feedback, you might want to put some documentation in the source file. See here: https://www.haskell.org/haddock/doc/html/markup.html Putting some examples to show how the components fits together is really helpful. You could just copy-paste your paypal Main.hs there.
I hook into the optimisation and regression - I'm doing online SGD with the ad library in tow. I personally just consider the algebra and HomTrainer stuff as a bit of technical magic that all seems intuitive. The monoid structure throughout is a breeze to work with, and a good portion of the library is what I would consider the iconic haskelly way to proceed in the problem domain, as well as production quality. I'd love to see a pure haskell implementation but it might be very hard to catch up speedwise with storable, and the tried and true c libraries. The library may benefit from busting up into training/learning, algebra, optimisation, regression, classification and so on. The components could have more general application than just machine learning and cover off areas that don't seem to exist in the haskell ecology. Creating a bayesian logistics regression API is just one example.
Random thoughts. Sequential algorithms feels more natural maybe because spoken language is sequential. It may be easier to tell an algorithm sequentially even if it is not sequential. So as yitz said this is hardware specific. On the other hand, pictures not being sequential may be a opportunity to express algorithms non-sequentially. Regarding this I consider expressions like (3 + 4)(5 + 2) more like a "picture" than a sequential description.
[I started working through this but haven't completed it yet.](http://en.m.wikibooks.org/wiki/Write_Yourself_a_Scheme_in_48_Hours)
This paper concludes that reactive programming is easier to understand than OO: [http://www.guidosalvaneschi.com/attachments/papers/2014_An-Empirical-Study-on-Program-Comprehension-with-Reactive-Programming_pdf.pdf](http://www.guidosalvaneschi.com/attachments/papers/2014_An-Empirical-Study-on-Program-Comprehension-with-Reactive-Programming_pdf.pdf) "In this paper, we presented a controlled experiment to evaluate the impact of RP on program comprehension. Results suggest that RP outperforms OO. The RP group provided more correct results, while not requiring more time to complete the tasks. In addition, we found evidence that understanding RP program requires less programming skills than OO programs."
http://hrothen.github.io/2014/09/05/lets-build-a-browser-engine-in-haskell/ This is one I really enjoyed.
Checkout [this](https://github.com/chris-taylor/aima-haskell). Its an unreleased machine learning library. You can pick up the logistic regression core from there. With that and hmatrix you can easily pick up the formula for covariance-matrix from [David Hosmers Applied logistic regression](http://eu.wiley.com/WileyCDA/WileyTitle/productCd-0470582472.html) and come to something like this covariance :: Matrix Double -- Model matrix X -&gt; Vector Double -- Tries per row in X -&gt; Vector Double -- Estimated theta -&gt; Matrix Double -- take the diagonal vector from this matrix and you have the variances covariance x ns beta = inv (trans x &lt;&gt; (v &lt;&gt; x)) where y = logit (x &lt;&gt; beta) sigma = zipVectorWith (\n p -&gt; n * p * (1 - p)) ns y v = diagRect 0 sigma r r r = rows x From there you can easily do the Wald-test for p-values. I am on my phone right now, I can post the full code when I am back home. It has never been deployed in production, I remeber writing an exhaustive test-suite for it though. 
I've wondered the same. A lot of the enterprise integration patterns ( http://www.enterpriseintegrationpatterns.com/ ) look very functional. They provide primitives that compose well, tend to reify as data, and control effects. Some of the patterns match Haskell types in obvious ways. It does look like pipes, conduits, machines, arrows, etc., should be a good fit. At work, I sometimes use Apache Camel. I'd love to see an idiomatic port of that to pipes or a similar Haskell library. For example, the Camel file component, http://camel.apache.org/file.html, is high level and very useful in practice. This isn't to say that the enterprise integration patterns (EIPs) are the last word in integration. I'd say that nowadays they need augmenting with web architecture and event sourcing. For example, the EIP solution is usually to put a message on a queue but nowadays, you should also consider the web/event sourced solution of providing an ATOM feed. So I'd love to have this in Haskell too. 
I second this. I just ran through the donation process. Although I had to give an email, I haven't received any confirmation. The webpage doesn't really inspire confidence: http://postimg.org/image/d68lvbemz/
Postimg doesn't seem to be anything better, 3 ads popping up, but no picture shown. 
I'd like to donate too, but hearing one needs flash enabled, scares me away, since it really don't like flash, don't even have it installed. Are there alternative ways of donating? 
[Implementing a Toy Language with LLVM](http://www.stephendiehl.com/llvm/) is a nice little intro to compilers and LLVM.
Well, you can withdraw the drawback by pushing the 'guarantee that it is unique' into how you produce a linear value from an unrestricted one. If the process of producing a linear value makes a copy, or gets to exploit the structure of the heap, e.g. the way baker's [lively linear lisp](http://www.pipeline.com/~hbaker1/LinearLisp.html) worked, you can get a lot of benefit. Consider that if you work entirely with linear types, you don't need a garbage collector at all. On the other hand, since uniqueness types act more like an affine type system in practice, you still need a GC.
I actually bailed out, I don't know why they are the payment processor (how do I know they respect the PCI DSS standards, and that they don't store my card information, or if they do how do I know they do so safely). Also the fact that they don't seem to have EV certificates for their SSL connection is a warning sign in most situations (for me at least, when it comes to payments).
I hear [git clone in Haskell from the bottom up](http://stefan.saasen.me/articles/git-clone-in-haskell-from-the-bottom-up/) is good.
hi gonzaw308: I wrote the article. I agree that the example is not an Enterprise integration scenario, but the framework has the base services for it. I have been working in Application Integration scenarios using Biztalk Server and TIBCO in the industry. The example present a true Workflow with long running transactions, although it is very simple, and a web application using the same framework (MFlow). it was intended to show that there is an essential commonality between web applications and the weak coupled integration of processes typical of a integration scenario. Essentially EAI need orchestration, transactional queues, logging, execution state recovery and long running transactions. These are provided by the architecture of the example,although they are shown at a very basic level. Other pieces of EAI such are Data adapters FFI etc can be done with many optional Haskell libraries. I wanted to write an additional article with a more focused EAI scenario if there were some interest in the Haskell community but the interest/effort did not reach the threshold. Perhaps I can write the continuation of the article soon. (edited for clarity, spelling)
&gt; I'd love to see a pure haskell implementation but it might be very hard to catch up speedwise with storable, and the tried and true c libraries. The only problem with speed is poor linear algebra support. hmatrix is okay, but it's not quite good enough. There's essentially no sparse matrix support and there's way too much copying of large matrices going on. As an example, nearest neighbor queries using the cover tree data structure are roughly twice as fast as equivalent C/C++ implementations on the exact same problem. As another example, Haskell makes implementing optimization algorithms trivial, which is why I can have so many of them (and they can be combined in pretty nice ways) compared to standard C++ libraries. &gt;The library may benefit from busting up into training/learning, algebra, optimisation, regression, classification and so on. This is very true, and the main reason I haven't put out a release in a long time. I want to get the interface into something stable. The [subhask library](http://github.com/mikeizbicki/subhask) contains a completely refactored prelude. The idea is to make developing fast numeric libraries much easier and more intuitive. Some highlights include: 1. Completely refactored numeric hierarchy that makes working with matrices (and other numeric tasks) as pleasant as matlab. 2. Support for monads in subcategories of Hask. The cover tree data structure mentioned above is a monad in the category of metric spaces, and I'm working on a DSL built off of this. I think this has a lot of potential to make lots of code really fast. 
I'm reading about TimeFlies right now, it'll take a while to ingest. Thanks for the recommendation. What I'm envisioning is a system where you can have some of the computations done on a sort of "time exact signals", that would know to pass correct time deltas to e.g a pure function. These "time exact signals" could be composed in a way that makes their computations exact, at the cost of running certain functions many times. Eventually you could transform the computation into a combined result for the given time delta, in order to pull them into the main program, which is has one event per time step (at least in Netwire). The result could be returned as a monoid, i.e a list of items. The input could be either a const value equal to the last or next input, or possibly a user given interpolation function between them. Using such as system, you could have exact timing where you can and big computations for a complete time delta where you must. EDIT: Finished skimming over the thesis. Really cool stuff. I like how it allows multiple events in one time step. However, I find it awkward it has a non-deterministic joining operator, I have a feeling it could be solved with a timestamp of sorts.
By the way, have you in mind some integration scenario that I can use for a new article?
Yep, we're starting to look at Lambda for this as well. It's pretty interesting.
No problem with the event model. But the event handling model quicky becomes a hell. It is different to create an event handler (basically a trigger) that creates a miniature when a image is uploaded. That is the first example that lambda suggest in the home page. Another different is to manage the company integration that way. 
Hi, here is a suggestion: I am working on a banking project where we want to integrate legacy systems with multiple service consumers, essentially implementing the multi-channel endpoint pattern http://soapatterns.org/design_patterns/multi_channel_endpoint. How would you solve this using Haskell?
Yitz: TCache perform the transaction in memory. It writes syncronously or asynchronously depending on the `syncWrite` configuration. In asynchronous mode It writes the modified registers with the periodicity defined by he programmer. There may be a programmer defined call before and another after writing the registers (to force commit for example). In synchronous mode, with `atomicallySync`, the the registers are written to the database after the STM transaction. If there is a IO exception condition, it can be detected and redo the STM operation, but that is up to the programmer. Since the write operation is configured by the programmer for each kind of register and the exception handler of the `atomicallySync` expression too, he can program and detect a variety of conditions and define the appropriate actions. The deletions are ever synchronous. The IO deletion operation ever ends orderedly and it is not aborted if the STM transaction is interrupted, since the IO operation run in another thread. (edited for clarity/spelling)
I've been considering adding more spider-esque functionality but haven't personally hit that use case yet. A lot of freelance jobs seem to be a variation on traditional spiders where you get to the next page via form submission. Though there are also login and scrape a set of links use cases that your recommendation would work for.
The source code of the slides (as he mentioned, they're written in Idris) is available on his github over [here](https://github.com/raichoo/31C3Slides).
YouTube mirror over [here](https://www.youtube.com/watch?v=uFwh3Uv8Nrw) (starts directly with the talk).
Typed holes work really well with ghc-mod, too. You can navigate between remaining holes, see what’s in scope for each, etc., all right in your editor. Integrated hoogle querying would get part of the way to suggesting relevant functions. (I don’t use this because setting it up in each sandbox always seems unnecessarily painful.) One thing I do to help myself find functions is use qualified imports in conjunction with `company-ghc` and `helm`. This lets you search through all `Map` related functions, for example, with `helm` ‘s incremental filtering.
https://github.com/haskell-infra/hl/pull/42
We don't have an official Bitcoin address - the main reason being that none of us want to handle the Bitcoin itself. We'd much rather deal with straight USD, meaning going through some kind of payment processor. We're trying to work on integrating Stripe into the new website ASAP; their Bitcoin support is rolling out to GA soon, so hopefully if everything works out you'll get a way better experience and be able to donate with USD or Bitcoin in seconds. See https://github.com/haskell-infra/hl/pull/42 which I hacked up the other day; it supports all major credit cards and Bitcoin too.
We're working on a Stripe-based donation setup soon that will hopefully pan out, which IMO is even better than Paypal. :)
Sorry about that. :( Hopefully we can get something else set up for you soon (see my other replies here).
Why do they need my address and phone number?
That is a very common integration problem. I did a consultancy using TIBCO for a mobile phone operator with almost exactly the same configuration represented in the link. There were a ERP Siebel a big Oracle database, CRM and a OLAP database. The channels were: Web, workstations for a call center and a web kiosk for dealers. The project was a mess, not only because each request implied modifications in each backend, but because the legacy backends generated exception conditions that triggered rollbacks, so for each flow generated by a request, there were a number of rollback flows depending on different conditions. Some flows for background processes required manual approvals an lasted for days. TIBCO worked as a event handler, requests and responses used XML, the flow was defined "declaratively" using XML, calling Java routines. There are declarative hells. If not, you can read any legislative document. It is written in a declarative language. It is a good case. But the legacy systems have to be simulated if I want to create a running example in FPcomplete.
Hi! That sounds really interesting. Do you still have that java code? Are you planning to make it public? Also, I was planning to do a btc haskell library, not a client, just as a tool; Something like what is already on hackage, but a bit higher level and easier to use (well, at least that's the idea). If you are interested, we could collaborate. (right now I only have a bunch of lines...).
Have you got your dotfiles in version control? Would love to look at your editor setup.
[Yes](https://github.com/acowley/dotfiles/blob/master/emacs), with the caveat that it’s written for me, so no guarantees about it working perfectly smoothly for everyone. 
Also in regards to tagsoup, it is a pretty nice library. I can't remember why I chose to use xml-conduit but I remember there being a reason for it. I think it was being able to compose things easily, but IIRC tagsoup does the same.
That's good to hear. However since Stripe offers an API and not hosted checkout I still wouldn't see the [EV certificate browser sign](https://www.thawte.com/assets/products/images/thawte-ev-bar-examples.jpg), like I do when payment processing is done on hosted pages (like paypal). If click &amp; pledge would have had that I would have been less prompt in complaining. I'm not that much bothered with the flash interface, but also I'm not that keen on completing forms for each website :). I mentioned paypal because I tend to donate to the FreeBSD foundation, and while I know they have the preferred option for click &amp; pledge, at least I they give me the paypal option too.
Thanks everyone for the heads up, I've decided to go with python/pandas/numpy/statsmodels for now, and port the application to haskell in the future. Further suggestions/references are always welcome. :-)
In some countries (as in Germany) full information about the donator is needed for the tax payback. 
PDF version: https://synrc.com/publications/cat/Lambda%20Calculus/Henk.pdf
It's excellent! There are a couple of bugs in the paper's code though, and it was uploaded to hackage with the bugs. There are some others' git repos with the bug-fixes though.
Their tax payback or mine?
Yours... 
Making good optimizations in DTP is being very actively researched. In theory, it should be able to optimize away stuff like list index checking. But in practice, it can be very hard to distinguish information that only is needed at compile time. Haskell has a very nice job of this - type info coincides with compile-time only info (except for type classes, but those have been well studied at this point I think)
Hi. I guess I was trying to see where your solution "fit" into the typical enterprise models (ESB, broker, etc) and I couldn't really see it. Does that framework support integration as middleware? Or does it necessarily have to be "attached" to a web application? If your web application is just a front-end to your services or system, then the EAI could take place somewhere else (depending on the architecture you are going for), and it's there where I fail to see how to solve this issue with the current frameworks and libraries in Haskell.
I dunno. I guess you could use a generic "enterprise-y" problem. Like having the sales of your company on one server exposed as a web service (SOAP), a database with the users of your company on another server you can access via JDBC/ODBC/whatever suits you, and a legacy FTP server that stores the history of all the transactions. If you wanted to create a service for external users (via a web application for example) that interacted with these systems, how would you do it in Haskell? (Sorry if the example sucks, couldn't think of anything better :( )
Also some payment providers may be required to save information about the payer, because money-laundering-laws (hope this google translation comes close enough to the actual English word ;-))
**edit**: Please disregard this comment. As /u/sclv pointed out, the cyclic version is still fast. &gt; Recursive function &gt; &gt; doubling a = a : doubling (a * 2) &gt; powersOf2 = doubling 1 &gt; &gt; Cyclic value &gt; &gt; powersOf2 = 1 : map (* 2) powersOf2 While the cyclic version is cute, be aware that it has a performance cost: each new value will be computed by starting from 1 and doubling an increasingly large number of times, leading to a quadratic cost to obtain the first *n* entries. The recursive function, on the other, hand, remembers the previous value and so only requires a single doubling in order to get the next value, leading to a linear cost to obtain the first *n* entries. &gt; [...] And you've probably seen examples of doing a similar thing with the fibonacci series using the memoization to do this This is of course a reference to this famous one-liner: fibs = 1 : 1 : zipWith (+) fibs (tail fibs) In this case, the performance is good, because the `(+)` is applied to values which have already been computed.
I was referring to compile-times. It doesn't seem to me like erasure should be that big of a problem. At worst, the program could be slightly more explicit about it. 
Dealing with Bitcoin is pretty easy and just makes it way easier to donate. Check out Coinbase.com or Bitpay.com. Should take a few min to set up and you end up getting dollar donations all the same.
Explicit recursions scare me, so I don't actually like the cute one-liners very much. Also, I demo'd Haskell a bit to various people recently, and the reaction to stuff like the `fibs` one-liner is more "Too cutesy, terrible engineering" and not "Cool language". Very few are impressed. I think even in demos, we should emphasize recursive combinators rather than explicit recursions: doubling a = iterate (*2) a fibs = iterate (\(cur, next) -&gt; (next, cur+next)) (0, 1) &lt;&amp;&gt; fst No dangerous and confusing explicit recursions!
How about something like this as a resolution of the two issues: class Mk a where mk :: a -&gt; Q (TExp a) instance Lift a =&gt; Mk (Maybe a) where mk (Just x) = TExp &lt;$&gt; lift x -- Note: could add a Typeable constraint to a, and output the type, -- in order to make this exception more informative. mk Nothing = reportError "Smart constructor failed" (Not really recommending these naming choices, just didn't want to spend much time thinking about it) Usage would look like `$(mk (parseURI "http://chrisdone.com/"))`.
Using [HStringTemplate](http://hackage.haskell.org/package/HStringTemplate), you can integrate R script into Haskell source import Text.StringTemplate.QQ logistic :: Stringable a =&gt; FilePath -&gt; FilePath -&gt; FilePath -&gt; StringTemplate a logistic dataFile labelFile outFile = [stmp| library(MASS) dat &lt;- read.table('$`dataFile`$') label &lt;- read.table('$`labelFile`$') fit &lt;- glm(dat ~ label[,1],family=binomial(logit)) write.csv(fit$coefficients,'$`outFile`$') |] excuteR :: String -&gt; StringTemplate String -&gt; IO () excuteR debugStr script = do (ex,_,err) &lt;- readProcessWithExitCode "R" ["--vanilla","--slave"] (toString script) case ex of ExitSuccess -&gt; return () _ -&gt; throwIO (RException $ debugStr ++ err) 
Congrats on your first Haskell library! Thanks for contributing to the community.
We'd still have to clear this with SPI as another payment processor, and we'd have to implement support for it. We're already going through the motions for another one. I don't think we'd like to add one more (and it's not like we'll magically get the clear to use e.g. Bitpay any faster than Stripe). We'd ideally like a processor who can deal with *both* - this is (part of) the reason we didn't do it before, because we didn't want to add a whole new endpoint just for one thing. With the new donation form I wrote, it takes about 20 seconds to donate with either a credit card or bitcoin (which I think throws the easy-ness complaint up in the air), and we get a unified API for both of these kinds of transactions that we can manage all in one place.
&gt; While the cyclic version is cute, be aware that it has a performance cost: each new value will be computed by starting from 1 and doubling an increasingly large number of times, leading to a quadratic cost to obtain the first n entries. Pretty sure this isn't the case. I suspect you're losing track of sharing while expanding this out. This hinze paper is a great aid when trying to do equational reasoning on things like this: http://www.cs.ox.ac.uk/ralf.hinze/publications/ICFP08.pdf
Three ways: A) Substitute and check the proof by hand. B) decompose your operation into a simpler algebraic structure if possible, and observe if it can be represented as e.g. a sum of monoids. If so, the monoidal associativeness will "percolate up" to your compound operation. C) Create a quickcheck property and run it a whole lot of times, and see if you're probabilistically satisfied :-)
I will be a devil's advocate, by being both overly literal and overly pedantic. (+) and (*) are not actually associative operations for the type Double since ((0 / 0) + (0 / 0)) + (0 / 0) = NaN and (0 / 0) + ((0 / 0) + (0 / 0)) = NaN but NaN /= NaN (and similarly for (*)).
In general this is definitely undecidable by Rice's theorem. The property of being associative is always invariant over different representations of the same function. This means the set of associative functions is an index set, so deciding if a function is a member of it is undecidable in general. In practice the suggestions others have given are very useful though. 
Wow. That would be ridiculously neat. I'd pay for that.
Oh my, you're right! The cyclic version is indeed fast, for the same reason `fibs` is fast: the (* 2) is applied to values which have already been computed. Thanks for pointing out my mistake! Let me try this again. &gt; powersOf2 = 1 : map (* 2) powersOf2 While this cyclic implementation is cute, be aware that proper sharing is crucial to its performance. As the `map (* 2)` walks down its argument list, it will always encounter values which have already been computed, starting with 1, then the 2 which it has just computed, then the 4, and so on. Therefore, only one multiplication per element will be required, and performance will be good. It is easy, however, to accidentally write similar expressions with much worse performance: powersOf :: Int -&gt; [Int] powersOf n = 1 : map (* n) (powersOf n) In this case, the recursive call to `powerOf n` creates its own list, which isn't shared with the one currently being created. As the `map (* 2)` walks down its argument list, it will always encounter new values which haven't been computed yet. Since those values are themselves created via the `map (* 2)` of the recursive call, the amount of work required to compute each new value grows for each new value: `1`, `2*2`, `2*2*2`, and so on. *n* new multiplications will be required to compute the *n*th element, and therefore the performance to obtain the first *n* elements will be quadratic. The following implementation recovers sharing, thereby achieving linear performance: powersOf :: Int -&gt; [Int] powersOf n = powersOfN where powersOfN = 1 : map (* n) powersOfN And yes, this time I did measure the performance :)
I find it better to consider (/) a partial function that breaks with 0. Then (+) and (*) don't lose associativity because of NaN dumbness. But then, Haskell decided something different. 
I fail to understand why this post is in /r/haskell. Are you trying to: - Write a function IsAssociative :: (a -&gt; a -&gt; a) -&gt; Bool? - Enforce via the type system that certain higher order functions only accept associative inputs? - Determine associativity for a particular function?
That's the trick. We technically aren't dealing with reals.
The problem isn't just expanding them out -- its rendering them into the proper canonical form so that you can actually decided equality! Your code doesn't detect things that really _should_ be equal because it itself doesn't take into account associativity, commutativity, etc.
... I didn't just expand them out, I also put them in canonical form. I didn't bother to hide the dumb constructors (which would be necessary if using this as a library), but the `sum` smart constructor * sorts and groups all terms by the variables they contain * gets rid of terms with coefficient of 0 * sorts and groups all variables so they occur at most once per term * gets rid of variables with an exponent of 0 For example: λ f a b a^2 + b^2 λ f b a a^2 + b^2 λ a + (b + c) == (a + b) + c True λ a - a == 0 True Now, it's possible that my implementation screwed up somewhere, and I don't cover all the cases I should. If so, please point it out. 
IOW `+` is associative for reals - the only problem is that real isn't a Haskell type, but even that's not a problem if you happen to know all your values (intermediate results included) *are* reals. Except that even then, `+` isn't associative for floating point because rounding errors can depend on the order you do your additions. A particular floating point value (excluding NaN etc) is a real value, but floating point operations aren't precisely the same as the mathematical operations on reals they approximate. 
About [your paypal example](https://github.com/codygman/hs-scrape-paypal-login/blob/master/Main.hs), something you really should have done is to annotate the types of top level definitions.
One can do a lot better - see here: https://ifl2014.github.io/submissions/ifl2014_submission_16.pdf
I can do that tomorrow no problem :)
It is an error to describe the coincidence of "type info" with "compile-time only info" as "a very nice job". On the one hand, we (Haskellers) have to go round the singleton houses to give run-time data type-level avatars. On the other hand, we have to construct tricky "representation types" (also singletons, but for types, not values) to implement datatype-generic functions. Nobody is in an especially good position to be pleased with themselves. DTP-ers have drawn too few distinctions between the phases where things belong. Haskell has put them in the wrong places, so we need to be a bit crafty (or crufty, depending on your point of view).
The framework does not need to be attached to any web application. But it is very common to have a web interface, either for web services, like in the pattern below or some kind of monitoring/administration. The nomenclature of integration is confusing since there are a accumulation of history, problems cases, marketing, levels of integration etc. Something based on Haskell either is a copy of something that already exist or it is something that makes rethink the integration problem. In this case I tried to do the second. The Flow monad of MFlow can be used to execute a sequence of IO actions which may have undo actions attached. the actions can return exceptions. These exceptions can be detected by the programmer and trigger the undo of the previous IO actions in reverse order. The monadic process log the execution state, so the process can restart at the execution point where it was when the orchestration/workflow process was executing when it was shut down or the hardware failed, possibly in another backup machine. With these effects, you can modelize any weak coupling between N applications. The IO actions can be web services, library calls, FFI, data transformations etc. Brokering, Routing, orchestration, workflows are particular cases. 
Today I've once again stumbled upon [the notorious `GeneralizedNewtypeDeriving` issue of GHC 7.8](https://ghc.haskell.org/trac/ghc/wiki/Roles2), when trying to derive the boilerplate instances for unboxed vectors. After writing a couple of lines of an annoying boilerplate like the following: {-# INLINE basicLength #-} basicLength = unsafeCoerce (VGM.basicLength :: VU.MVector s Word8 -&gt; Int) I thought about implementing a TH macro. Then I made a few searches to make sure that I won't be reinventing the wheel to find this marvelous jewel! Hope this saves some of you a couple of hours in the future. 
That is more like a Web application with some integration. I for sure oversimplify the problem in this example. Please tell me any relevant complication that you may find. The databases usually are passive, dont produce complex interactions and it is a matter of retrying the FTP server if it fails. It is overkilling to queue the FTP send using a queue and a separate process. The only exception that I consider here is the login failure. loginMenu wait for login and password. If the pair does not validate with the JDBC database, the process fail, and run again from the beginning, which present again the login page. It is not necessary a specific undo action that may limit the rollback, since it is the first action in the process. Although this can be done without rollback, with a simple loop within the user validation step Once the user validates, the login is not presented again for this user, even if the server shutdown and restart, since this step is already done. The same happens for soapProducts. They are asked once to the SOAP database. Then what the user see in all the remaining sessions is the list of products as the first page. The user buy products and for each product, the loop will send the product to the FTP server. I have not compiled/tested it yet. I have no time now. So I probably will reedit this since there are errors for sure. From time to time the user may need to login again and the product list must be updated. The easiest thing is to create a session timeout with `setTimeouts`. In this case it is 15 days (the second parameter). The first parameter is the time that the process stay alive before being shut down. If it is necessary to query the product database everytime with `soapProducts`, simply suppress `step` and put `liftIO` Then the products will be consulted on every process restart. `ask` is a MFlow call that present a web page to the user. `step` execute an IO action, log the result and recover this value from the log when the process is restarted. I also trigger a rollback if the action fails with `fail`. the rest of the primitives in the example have to be programmed with other Haskell libraries. import MFlow.Wai.Blaze.Html.All imports.... main= runNavigation "" $ do setTimeouts 2000 $ 15 * 24 * 60 * 60 (v,user)&lt;- step $ do (user,pass) &lt;- ask loginPage v &lt;- JDBCvalidate user pass return (v,user) case v of False -&gt; fail "" True -&gt; return () products &lt;- step soapProducts step loop loop= do productBought &lt;- ask menu sendFTP productBought user loop 
[Haskforce](http://carymrobbins.github.io/intellij-haskforce/) works really well.
[Haskell deconstructed by jekor (youtube series)](https://www.youtube.com/playlist?list=PLxj9UAX4Em-Lz5btngxMVZxf_B44GETVz) It would be cool if there would be a dedicated website with the index of all Haskell learning material which is tagged both by humans and automatically. A good tag functionality alone could pretty much satisfy every possible scenario. One would only have to do #project-based and find all project based tutorials. For videos one could do #video and find all video presentations. Or combine #video and #project-based and it would give haskell deconstructed like tutorials. For non-video pages, the site could crawl pages and construct search engine similar to hoogle or hayoo, but for learning material. One could search there type signatures and function names and find links to blog posts which contain certain code examples. Perhaps the same crawler could also submit the page to some cache for links never to be dead (like http://textmirror.net which will create a text copy of the page). To make it a bit more complex, the tags on that site could have relevance values. So people could vote how relevant certain tags are. So if monads are mentioned in the article only a couple of times, it's not as relevant to the #monad tag as dedicated blog posts on monads. Writing documentation is time consuming. But there might be a faster way. So another great thing to have would be to have some campaign like "2-3 examples per function". In most cases examples without any words can be almost as good as documentation. So for each exported module an author can quickly just write a couple of examples without bothering with explanations (to save time). It won't take that much of an author's time, but it will provide a lot of assistance to somebody wanting to use an author's library. And examples can mirror the project structure like src/Data/List.hs exp/Data/List.example The `List.example` looks similar to this for humans: &gt;&gt; intersperse '.' "hello" &gt;"h.e.l.l.o" &gt;&gt; intersperse "," ["hello", "world"] &gt;["hello", "," ,"world"] &gt;&gt;map (* 5) [1..3] &gt;5, 10, 15 Very simple and fast for an author to make, but, IMO, very useful for anybody trying to learn a particular library. The crucial detail here is that that List.example file has several properties: 1. It's interactive. It can be opened in an interactive interpreter like IHaskell. So people can play with examples. E.g. each module can generate / be represented by a big .ipyth notebook file. 2. It should always work. No cabal hell should be acceptable. An example should contain an info about how to reconstruct exact same environment / sandbox, in case something does not work. With Docker (and docker hub) and Nix it's now possible to easily provide a user with 100% working examples / interactive documentation. Nothing confuses a newbie like outdated examples, where s/he does everything right according to a tutorial, but, because the code was changed, something does not work. A user should be able to run examples to check whether they work with a particular version of the package they have. Otherwise, they should be able to rebuild an environment where that library will work. 3. It runs like unit tests. If input does not match the previous output, the author is warned. So if after "compilation" of examples `map (* 5) [1..3]` suddenly gives `[10, 20, 30]`, while in previous version exactly the same input gave `[5, 10, 15]` the author is warned. If something gives an error, the author is warned as well 4. It has an option to warn an author about missing functions. So an author should be able to easily see which functions he is yet to document. 5. It can be rendered as HTML and act as an addition to pandoc. [That's why an IHaskell hack would be great for this thing.]. In IHaskell can write there code alongside nice formatted explanation. You can render safe HTML, as well as give a link to a working notebook for user to download. The only hacks needed would be: a list of all examples in a notebook compared to the list of functions exported from a module (to compare which functions an author is yet to implement). 
I use unboxed vectors all the time. Thanks a lot for this!
ah, i see! Apologies for not reading your code carefully earlier :-)
My [understanding](http://www.reddit.com/r/rust/comments/2k6zai/wandering_through_linear_types_capabilities_and/clil3tx) had [been](http://www.reddit.com/r/rust/comments/2b02mb/rust_will_be_the_language_of_the_future/cj1sbcq) that Rust [has](http://www.reddit.com/r/rust/comments/2b02mb/rust_will_be_the_language_of_the_future/cj8n5zf) affine, but not (yet) linear types. It avoids the need for garbage collection (most of the time; there is a library-provided reference counted box type, `Rc`). How would you characterize what Rust has, if you're familiar with it?
I confess I've kinda given up on trying to keep track of whatever the Rust memory management scheme du jour is. ;) The issue with affine types is you either need to plumb a bunch of manual scope management through all your code, and then chase after finalizing / cleaning things up. For a language like rust, that seems like a reasonable fit for all the other stuff they have to do. Even in a language with real garbage collection, there is a role for mixing and matching all of these. though, For instance if you had a type system with a nice lattice with chains like: linear -&gt; relevant -&gt; unrestricted -&gt; unique, linear -&gt; affine -&gt; unrestricted -&gt; unique, linear -&gt; relevant -&gt; unique relevant -&gt; unique, etc. then you could have mutation in place for anything relevant (or even just strict), and accumulate affine thunks that get evaluated once to produce a normal unrestricted or unique result. It isn't perfect though, e.g. relevant turns out to be a bit too strong for most usecases, so you probably really only want strictness not relevance, but then that becomes a difficult thing to model in the type checker.
It'd be a decent first step that I'd be super grateful for! But it'd also be probably only be about 25% of the way towards the level of sparse matrix support available in a C++ package like Armadillo. Things it's missing: 1. Sparse * dense matrix ops 2. standard BLAS has a lot of formats "between" full sparse and full dense like banded and symmetric; most machine learning algorithms use symetric matrices as a subcomponent and storing them that way gives an automatic &gt;4x speedup 3. I suspect it might not be all that fast compared to better-known (at least to me) libs. It's part of a book, so the main goal is pedagogy over speed. Also, I don't know of any numerical libs that use this as a backend. That said, it looks like a pretty simle interface, so making the bindings would be relatively easy. And the fact that there's only two C files would make it easy to include the C code in the haskell package. Let me know if you end up doing this, and I'll use it!
I've a pretty sweet approach that allows unboxed style storage for ANY base vector type or a hybrid mix, https://github.com/wellposed/numerical/blob/master/src/Numerical/Data/Vector/Pair.hs and https://github.com/wellposed/numerical/blob/master/src/Numerical/Data/Vector/HPair.hs respectively. then for any format you want an unboxed view of, you can just newtype my (H)Pair format and roll with it. Its actually crucial for a few algorithms i've written recently that I have access to such a format
I'm not the author.
I'm really fond of the **pipes-text** trick of representing a streaming source of lines as **FreeT (Producer Text m) m x**. FreeT basically wraps the monadic return value (here **x**) with zero or more layers of the functor (here a **Producer** of Text values). So, to advance to the next line, you have to "peel off a functor layer" by consuming the Producer that represents the current line. And you aren't forced to have a whole line in memory at any time!
I view the "free" constructions as "recorders" that record the type class method use. However, if they record everything, they violate the laws, because they distinguish between method applications that according to the laws should not be distinct. Thus, they lose just the minimum amount of information they can from the recording to abide by the laws. When are they useful? They let you postpone the actual interpretation of the free structure, and interpret the same construction with different interpreters. For example: * an IO value is opaque, and not much can be done with it. But a free monad construction that's later interpreted as IO can instead be interpreted via fake IO allowing to test various things and reach various behaviors. * a Binary Get computation can be re-interpreted as a big endian or little endian computation. * the free construction is Showable as a trace of what computation was actually built, which can be useful.
&gt; observe if it can be represented as e.g. a sum of monoids What do you mean by that? The OP's example is literally a (numeric) sum of two monoidal operations (multiplication), yet it clearly isn't associative. &gt; (0 `f` 1) `f` 2 5 &gt; 0 `f` (1 `f` 2) 25 Perhaps you mean that if `g1` and `g2` are associative, then so is `g`? g x y = g1 x y + g2 x y But that's not true either: -- | -- prop&gt; (x `g1` y) `g1` z == x `g1` (y `g1` z) g1 :: Int -&gt; Int -&gt; Int g1 x _ = x -- | -- prop&gt; (x `g2` y) `g2` z == x `g2` (y `g2` z) g2 :: Int -&gt; Int -&gt; Int g2 x y = x * y -- | -- prop&gt; (x `g` y) `g` z == x `g` (y `g` z) -- Falsifiable (after 3 tests and 4 shrinks): -- (1 `g` 0) `g` 1 = 2 -- 1 `g` (0 `g` 1) = 1 g :: Int -&gt; Int -&gt; Int g x y = g1 x y + g2 x y Or do you mean a type-level sum? If you're not careful with how you handle the heterogeneous case, the resulting operation might still not be associative. -- | -- prop&gt; (x `h1` y) `h1` z == x `h1` (y `h1` z) h1 :: Int -&gt; Int -&gt; Int h1 x _ = x -- | -- prop&gt; (x `h2` y) `h2` z == x `h2` (y `h2` z) h2 :: Int -&gt; Int -&gt; Int h2 x y = x * y -- | -- prop&gt; (x `h` y) `h` z == x `h` (y `h` z) -- Falsifiable (after 7 tests and 4 shrinks): -- (Right 1 `h` Left 0) `h` Right 0 = Right 0 -- Right 1 `h` (Left 0 `h` Right 0) = Right 1 h :: Either Int Int -&gt; Either Int Int -&gt; Either Int Int h (Left x) (Left x') = Left (h1 x x') h (Right y) (Right y') = Right (h2 y y') h (Left x) (Right y) = Left x h (Right y) (Left x) = Right y Type-level products work just fine, though, as do sums if you impose a total order on the constructors. Which is too restrictive. I'd be really happy if there was a way of combining associative functions into larger associative functions, but I'm still on the hunt for an expressive enough set of combinators. One day...
To add emphasis to your point, it turns out that (as usual) Tony Hoare was prescient. What we DTP'ers variously call computational irrelevance or refinement types are what were called logical variables in Hoare logic decades ago. It was essential then, and it remains essential now. 
The simplest explanation I found, was that they simply take a functor and make it recursive. For instance, since `Free` is defined: data Free f a = Pure a | Free (f (Free f a)) Suppose our functor is a list, then substituting `[]` for `f` and changing the names, we have data Tree = Leaf a | Tree [Tree a] So if we're working with such a tree datatype, we're better off using `Free`, since it gets us a law-abiding Monad instance for free. As others have pointed out you can make imperative mini-languages with free monads, and some people call them abstract syntax trees, though I found that analogy to be kind of confusing. Monads can't easily be analysed, except in the process of running them, and that applies to Free monads too. Not that they're not useful, but if you're coming at them from a compiler view other techniques are preferable. As an aside, recursive data types like this can be somewhat inefficient, since subsequent left-associated binds have to repeatedly traverse the tree, in pretty much the same way that left-associated appends have to repeatedly traverse a list, but just like we have `DList` there are ways to make `Free` more efficient.
A non-technical "conceptual" explanation: A free mon*oid* is a list, i.e. a sequence of elements. A free mon*ad* is a sequence of actions where subsequent actions can depend on the result of previous ones. 
[I just recently wrote a quick example of using this trick.](http://stackoverflow.com/questions/27692514/how-to-detect-end-of-input-with-pipes/27694211#27694211)
&gt; stm-containers is substantially slower than what I'm using right now. It makes the whole thing about 15% slower. I highly doubt that. According to [benchmarks](http://nikita-volkov.github.io/stm-containers/), "stm-containers" performs better than plain "containers" even on a single thread. Your solution wraps a logic around "containers", so an extra overhead should also be taken into account. And so far we were only talking about a single-threaded performance. There is no way this thing could scale as well as "stm-containers" in concurrent access, since the probability of collisions is orders of magnitude higher. 
Yes, I meant at the type level. And "sum" was a lazy shorthand -- I really meant to refer to any "composition of monoidal types that preserves associativity" of which product is the most straightforward.
&gt; Monads can't easily be analysed, except in the process of running them, and that applies to Free monads too. What do you mean by this, they are just a data structure, should be analyzable.
Ahh, apologies then.
hey, this is a great initiative, really! Looking forward to contribute.
Thank you, looking forward to any help I can get!
I find it regrettable that the GNU FDL includes a giant blob of text in the end; for online works this doesn't matter but for a PDF it can be a nuisance, specially if when printing someone doesn't carefully exclude those pages (think of the trees!). CC BY-SA 3.0 is a similar copyleft to GFDL (the FSF even published an exception to permit Wikipedia to migrate to it) but is more well behaved.
I also don't see how `lookupStaticPointer` is any safer than `unsafeLookupStaticPointer` - couldn't you just ignore the `Typeable` constraint and subvert type safety in the continuation? lookupStaticPointer key (deRefStaticPtr :: StaticPtr Foo -&gt; Foo)
I thought about this, but the ccsa license is far too ambiguous to be useful. I've been meaning to add an option to the build script allowing you to build it without the license. Sorry for any formatting errors, I'm on my phone 
Not sure i understand your question. I want a library binary, not an executable. I do not care if it is statically or dynamically linked. How do i create a library binary (without the source code) that is easily installable, either via cabal or via some linux distribution package manager ? 
Looks super awesome! Some of the indentation seems to be off from the list on page 13 to 14 (although it might just be my computer). EDIT 2: It seems that all of the pages alternate in indentation. EDIT: &gt; First of all, there are the integers: these are just the **whole numbers**, like −3, −2, −1, 0, 1, 2. Traditionally, this set is denoted Z. Just a nitpick, but I believe whole numbers do not include negative numbers.
Sometimes (e.g., when doing a deep DSL embedding) you just want to build a syntax tree for all the monad operations that you have, rather than actually doing anything. This can be done perfectly fine using a data structure (GADT) that builds structure for `return` and `&gt;&gt;=`. (And this is what I often do.) But doing that means that your monad instance no longer obey the monad laws. Nobody is bothered by this if you're not, because the Haskell implementations do not assume the monad laws. Still, it's a lot nicer if the monad laws are obeyed, and the free monad is a very clever construction that does enough rearranging of the structure so you build to maintain the monad laws.
How is [this](https://creativecommons.org/licenses/by-sa/3.0/legalcode) ambiguous? Any specific things which are not clear? I thought he CC licenses were well thought out, so I am a bit curious. 
&gt; any "composition of monoidal types that preserves associativity" of which product is the most straightforward. Any others I should know about?
but both of these formats stand a good chance of being modified during runtime anyway (urls modified to add query args etc)....so i would think you still need to parse them at runtime anyway
My understanding is that it's against the GFDL license to redistribute or make derived works without having the full text of the license attached. From the [license text](https://www.gnu.org/licenses/fdl-1.3.html), that would be section 2 &gt; You may copy and distribute the Document in any medium, either commercially or noncommercially, provided that this License, the copyright notices, and the license notice saying this License applies to the Document are reproduced in all copies (...) And section 4.H &gt; You may copy and distribute a Modified Version of the Document under the conditions of sections 2 and 3 above (...) In addition, you must do these things in the Modified Version: (...) &gt; H. Include an unaltered copy of this License. In other words, you the author can send people a pdf without the full text of the license, but in all GFDL's obnoxiousness they couldn't legally send the pdf to someone else. (one way to work around that would be to print it in a really small font) Also, I don't think the creative commons license aren't actually ambiguous, otherwise Wikipedia wouldn't use it. The vague "summary for dummies" [here](https://creativecommons.org/licenses/by-sa/3.0/) is just an overview, but the full legalese [here](https://creativecommons.org/licenses/by-sa/3.0/legalcode) is on par to GFDL and other licenses.
How do you analyze IO?
There are $\mathbb{N}$ Natural numbers which depending on definition might or might not include zero; Integers (whole from latin) as $\mathbb{Z}$ form a ring... The only problem is, there is no single view way how to introduce basics ( I would hope for at least Chapter 1(a-z) where each would be different.); I studied maths a couple of times each was different each time it was fun. I would bet that each country/university does it differently. 
This is just my impression, which might be wrong, but suppose you have a functor like the following: data IOF a = PutChar Char a | GetChar (Char -&gt; a) | LaunchMissiles a It's difficult to write a total function like `launchesMissles :: Free IOF a -&gt; Bool` that tells you if your monad uses the `LaunchMissles` constructor. The problem is that if `LaunchMissiles` is hidden behind `GetChar` then there's no way to tell if it gets reached without supplying a `Char`. For instance, if we define `getLine` in the obvious way, then I don't think we can easily distinguish the two following programs: 1) do { x &lt;- getLine ; when (x == "launchMissiles") $ launchMissiles 2) do { x &lt;- getLine ; return () } Analysing the first example requires analysing the function in the `GetChar` constructor, which can only be done by supplying the write sequence of `Char`s and seeing that the first example is sometimes unsafe. If you're interpreter has the type `Free IOF a -&gt; IO a` then you're really hooped, since even if you're interpreter stops when it encounters `LaunchMissiles` it will still have caused a bunch of side-effects in the process of getting up to that point. Though in this case (and maybe in general?) you can write a pure interpreter, which is one of the cool things about free monads.
So a free monad is just a free monoid in the category of endofunctors? :p
So how do these compare to their non-free counter parts?
Sounds just like the usual monoid and monad.
&gt;The book is open-source, so anyone is welcome to contribute/fork/what-have-you. The book is licensed under the GNU Free Documentation License This is great! There are still very few free books around. There are more and more allowing redistribution (at least noncommercially), and some allowing noncommercial modification, but history has shown how useful it is to have the full four freedoms. So kudos and good luck. I'll keep my eye on the project.
Thank you!
Pandoc may be able to do that. I originally tried to make it a website, actually, but doing fancy math formatting on the web is very difficult. Pandoc was sort of helpful. 
Well it is of course a free (monoid in the category of endofunctors). I'm not sure that the concept of (free monoid) it the category of endofunctors is well defined. But in any case, I like the way you think.
I posted this below, but here: &gt;The most important thing, to me at least, is that the FDL requires the book be made available in source form. The CC-SA license has no such requirement, which is why I went with the FDL. 
This would be even better if built upon type theory instead of set theory :)
I was thinking of starting with some of the more abstract stuff (categories, monoids, functors, what-have-you), because they turn out to be pretty fundamental. To answer your question, no, I don't have any plans to do so (for now), but I don't see any reason not to. In this case, I think making plans to go into higher-level math would be putting the cart a bit before the horse.
Vaguely speaking they satisfy no other laws than the laws for a monad, which makes them "initial" among all monads and allows them to be interpreted in different contexts. A slightly different characterization is that they are left adjoints of a forgetful functor. A related meaning is that they are "generated freely" by the elements they consist of, that is there are no relations among the generators other than the monad laws. 
Would you mind elaborating? I hadn't considered this idea, but I'm interested to hear it.
No, see here: http://www.reddit.com/r/haskell/comments/2qtr4q/what_really_are_free_monads/cn9qc1m
Well, keep me posted!