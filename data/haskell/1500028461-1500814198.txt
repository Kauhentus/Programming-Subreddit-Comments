I successfully use [webdriver](https://hackage.haskell.org/package/webdriver) to let click through dozens of post forms and links
Yeah. I know. Transient is excellent. I want to create some kind of standard for composable components in which a substantial part of the community would agree, so I can not sacrifice sophisticated type safety. It seems that the Haskell community is more concerned with types than with composability. I think, like you, that this is an error, but I want to let everyone have their types an make them composable too.
how was your experience ... have you used hs-scrape ... I am having problem installing selenium driver (of version mismatch)
no issues until now with 'selenium-server-standalone-3.4.0.jar' and chromedriver. I am running the hub on 4444 and the node on 5555 and Xvfb.
Thanks .... I have to fix then.
I like the idea of `ImpredicativeTypes` but I find CHC's implementation pretty unsatisfactory. I hardly ever do a project without `RankNTypes` though.
PSA: [Some replies to this comment appear to have been silently censored](http://i.imgur.com/rUz1Gew.png). I've submitted a [new text post to discuss this](https://www.reddit.com/r/haskell/comments/6n8qj0/censorship_in_rhaskell/) (unfortunately the moderators consider discussing this "obvious trolling").
&gt; It is a common complaint that Haskell libraries are hard to inter-operate Do you have any concrete examples of this complaint? I'm interested in reading some.
I still think you only lifted the problem to the type level, which doesn't make it go away. To make `A e1` and `A e2` possible to be implemented independently, `e1` and `e2` will need to be as well. And now you face the problem that `e1` and `e2` needs to be of the same *kind*. Which is exactly the problem you've mentioned, on the type level. Do you have something in mind to solve this type level problem that does not lead to a value level solution? The 'constraint' thing is one, as I've mentioned elsewhere, but it's not fully general.
&gt; No remote, onsite only. &gt; a few years of practical software engineering experience is required. &gt; can only consider people authorized to work in the US. I suspect you're going to have a hard time finding experienced Haskell programmers in the US willing to move to Alaska. But best of luck!
Around minute 55, things get really hard to follow. Perhaps all the silly questions earlier wasted time she was counting on for this part.
I used 1 and 2. 1 is not well-supported :( I see some recent commits, but they are about mostly migrating Java to Kotlin. Probably relates to summer internship in JetBrains and after two months nobody will touch it for about year. 2 is in active development but the UX is quite good! I must admit that things are improving. I used it for writing compiler. And it was rather good. Even go-to-definition works most of the time. But I must admit that 2 is not so mature. 
I have been using `intellij-haskell` for the past year or so and I'm quite happy with it. The last few updates have improved things a lot in terms of speed. I was using Haskforce before, but it was somewhat unstable, and required manually entering executable paths, whereas stuff just works automatically now (although it's still a bit slow).
Nixpkgs literally uses `Cabal` to build packages. It doesn't use `cabal-install`, but it does use `Cabal`. What Nix allows you is to have multiple environments with different versions of some packages. So you can have an environment for building package A against `foo-0.2` and another for building package `B` with `foo-0.3`. But Nix does not, and cannot, solve the problem of having two conflicting versions of a software in the same environment. That must be supported by the software in question itself. It is different from using two versions of the same software in different, isolated environments.
I'm curious why you don't want to use Stack for this. I don't know how to make this work with `cabal`, so ignore the rest of this post if you're dead set on it, but stack does make the flow easy. You can `stack build $package` into the default/global project snapshot and it'll be available in `stack ghci`. The default global project is also used for single files, so `stack ghci MyFile.hs` works with all the packages. The [`stack script`](https://docs.haskellstack.org/en/stable/GUIDE/#script-interpreter) feature is also unmatched in `cabal-install`, and makes it really easy to make executable scripts without a `cabal` project. So, if I want to play with lens, I'd do $ stack build lens $ vi LensPlay.hs ... $ stack ghci LensPlay.hs Prelude LensPlay&gt; import Control.Lens Prelude LensPlay Control.Lens&gt; and it just works.
In your example if you subsequently wanted to use to see some output would you need to first exit your `stack ghci`, and then in subsequent steps`stack build printf`, and then again `stack ghci LensPlay.hs`? The forum is so sensitive to issues of stack v. cabal that I don't really want to derail my request for advice into a debate on merits. I am just curious in learning the tools that were originally built by the builders of this weird language I have come to love, and before I give up on them I want to make an effort to use their tools. edited for clarity
Not disagreeing. Just stating that's more Cabal's complaint than GHC's
[varying](http://hackage.haskell.org/package/varying) also uses mealy machines with effects.
&gt; I would add this stuff to the article. I didn't write the article :) /u/fumieval ^ Hint Hint Rather than making messages a simple GADT, it might possible and more convenient to do something extensible, like DTalC or type level symbols or overloaded labels.
Have sandboxes gone out of favor? I just add modules as build depends on the cabal file, `cabal sandbox init` and play around. `cabal install` honors that sandbox once setup. `cabal ghci` gives you a shell with your packages ready to use. 
So I guess you mean that do let x = !(foo &gt;&gt;=) print x desugars to foo &gt;&gt;= \a -&gt; do let x = a print x which is the same as do x &lt;- foo print x So is it really true that this notation would allow us to remove `&lt;-` binds in `do` notation? If so, that's really cool! But how do you know how far the `!(...)` gets lifted outwards in an expression?
Exactly what I mean (sorry for the crappy syntax, I'm improving it). But I had to add bang-and-return-next (i.e., `&gt;` - otherwise you'd need to have a ton of useless `lets`), and start-of-scope (i.e., `|`, which does what the `do` keyword does). &gt; But how do you know how far the !(...) gets lifted outwards in an expression? To either the closest binder (i.e., the body of an enclosing lambda, the body of an enclosing `let`), or to the closest enclosing scope, marked with `|`.
OK, well if you can replace do bindings with a more powerful syntax then I'm all for it. Looking forward to seeing how this turns out in practice!
Thanks! Well if you have some free time to spare you can always try installing Moon and see how well it plays in practice :) but there is no type-system and the syntax is quite bad from the feedbacks, so that might be painful for now.
I can't for the life of me recall exactly where, but I think this has been discussed before by "the community". I think the main objection was that it could become a bit confusing with multiple !'s in a single expression, it would effectively define an order of evaluation of function arguments (ish), etc. etc. I was personally a bit on the fence. I mean for simple things it seems pretty nice, but for complex expressions I think it could become a bit confusing. If we're ~~stealing~~borrowing, I'd much rather steal the pattern-match-while-in-(&lt;-)-do-syntax-which-allows-you-to-short-circuit-succinctly thing.
&gt; But what I wanted to talk about was Idris's bang-notation [links to http://docs.idris-lang.org/en/latest/tutorial/syntax.html]. That page does not include the "!" character, did you meant to link to http://docs.idris-lang.org/en/latest/tutorial/interfaces.html#monads-and-do-notation? &gt; For those who don't know, the bang-notation takes an expression such as: &gt; &gt; [1, 2, !(foo "bar"), 4] &gt; &gt; And automatically translates it to: &gt; &gt; foo "bar" $ \ A -&gt; [1, 2, A, 4] Are you sure? The example I found at the above link is that let y = 42 in f !(g !(print y) !x) desugars to let y = 42 in do y' &lt;- print y x' &lt;- x g' &lt;- g y' x' f g' So I would expect your example to desugar to do a &lt;- foo "bar" [1, 2, a, 4] or equivalently foo "bar" &gt;&gt;= \a -&gt; [1, 2, a, 4] and therefore there is no need to add the extra `&gt;&gt;=` in `!(foo &gt;&gt;=)`, the `!` notation already adds it.
Yes, if you hadn't build `printf` before. Once you've built it once, it is part of your profile's global stack project (until you clear or change the resolver), and will be available in any non-project `stack ghci` invocation. So an `import Text.Printf` (or whatever the module name is) would work without the `build`, unless it hasn't been installed yet. If `printf` hadn't been installed, it'd look like: $ stack ghci LensPlay.hs Prelude LensPlay&gt; import Text.Printf Can't find module Text.Print Prelude LensPlay&gt; \d $ stack build printf $ stack ghci LensPlay.hs Prelude LensPlay&gt; import Text.Printf Prelude LensPlay Text.Printf&gt; If the library is already installed: $ stack ghci LensPlay.hs Prelude LensPlay&gt; import Text.Printf Prelude LensPlay Text.Printf&gt; This is *kinda* like installing everything into the global `cabal` package dataset, but `stack` will not ever pick two versions of a library that don't build together, so you can't ever get cabal hell like this. As far as I know, there's no way to get this sort of behavior in Cabal without having a project and `new-build`-ing it (along with specifying dependencies explicit, to ensure you don't get build errors).
Sandboxes are less nice than `new-build`, which is much more convenient about sharing build artifacts. IIRC, `new-build` works a lot like Nix to reuse build artifacts where possible and isolate them where not.
&gt; I can't for the life of me recall exactly where, but I think this has been discussed before by "the community". [Here](https://www.reddit.com/r/haskell/comments/54z31o/supporting_idris_notation_in_haskell/)?
You're right. I'm not automatically adding `&gt;&gt;=` on Moon's bang-notation, thus I was expressing myself incorrectly on the original post. The reason I'm not doing it is Moon is (still?) untyped and, as such, there is no type-class system to infer the right `&gt;&gt;=` for you. That means you need to explicitly write it. Instead, though, what I do is hide the `&gt;&gt;=` behind names. For example, you could define `oneOf vals = (vals &gt;&gt;=)`, and use the List monad as `... !(oneOf [1, 2, 3]) ...`. Sorry for the confusion.
Lol, I see what you did there. Too lazy, I guess. :) (I was thinking mailing list.)
Ah. Thanks.
I guess this is another point for the notion that this notation may actually be confusing :).
Recursions are a good way to show Haskell, I think; I once showed example on stuff like using recursions on List, or binary tree, it usually can get someone's attention due to it's simplicity and elegance. Composition is another killer feature, such as building a simple parser from scratch; it amazes people how powerful it can be based on simple building blocks.
A lot of the advice in this thread assumes a wholesale switch to Haskell. But I think there's a lot of value in using Python for the stuff for which it already has good libraries and maybe using Haskell to orchestrate. I also think that's a great, incremental way to learn a language and possibly a good way of getting it into your day job. There's a few Haskell-&gt;Python bridges out there. Not sure of maturity though. Might be work looking into.
I guess that's true. But IME it can make for a really annoying API if a new name is introduced for each variation. Dealing with overloading is, I think, the central problem of designing a nice API around OO code which is why I'm so looking forward to TDNR. 
I use the last option, myself. playground/ ├── ChangeLog.md ├── LICENSE ├── Setup.hs ├── playground.cabal └── src └── Playground.hs My current `Playground.hs` looks like module Playground where import BasePrelude import Control.Lens import qualified Control.Foldl as F import qualified Pipes.Prelude as P and the Cabal file looks like -- Initial playground.cabal generated by cabal init. For further -- documentation, see http://haskell.org/cabal/users-guide/ name: playground version: 0.1.0.0 -- synopsis: -- description: license: BSD3 license-file: LICENSE author: name maintainer: email -- copyright: -- category: build-type: Simple extra-source-files: ChangeLog.md cabal-version: &gt;=1.10 library exposed-modules: Playground -- other-modules: -- other-extensions: default-extensions: NoImplicitPrelude, RankNTypes, GADTs, TemplateHaskell build-depends: base &gt;=4.9 &amp;&amp; &lt;4.10, lens, pipes, foldl, base-prelude hs-source-dirs: src default-language: Haskell2010 I used to set this stuff up in my `.ghci`, but I've actually been preferring this since it doesn't stomp on any other repl instances. I just have an alias to `pushd` me to the directory, run `cabal new-repl` and then `popd` when I'm done. This is what I usually use when I'm just trying to figure out a basic idea or work on code for an SO answer. If I add a library, I just add it to Cabal file, `cabal new-build` and then right back to `cabal new-repl`. When I find a neat trick, I can write it to `Playground.hs` to keep it around for further invocations, so it's kinda like interacting with `lambdabot`'s `L.hs`.
But please don't actually do this because it silently shadows the name
Really helpful. Thank you for the very clear example. 
I will actually work on this as part of my [Summer of haskell](http://summer.haskell.org) project. The [`new-install` command](https://github.com/haskell/cabal/issues/4558), when invoked on a library, will do exactly what you want. I'm planning to do it right after fixing [#4120](https://github.com/haskell/cabal/issues/4558), so just wait a few ~~weeks~~ ~~months~~ *some indeterminate amount of time* and you are set. In the meantime, i suggest doing what /u/NihilistDandy [says](https://www.reddit.com/r/haskell/comments/6n9b8k/asking_for_advice_on_how_to_make_playing_with/dk7z388/), just put them in the build-depends of a dummy package.
&gt; What would such a minimal .cabal file need to have to be acceptable to the cabal new-build tool? [This earlier thread](https://www.reddit.com/r/haskell/comments/5zeg7n/minimal_cabal_files/) about mimimal Cabal files might be relevant. 
&gt; Dealing with overloading is, I think, the central problem of designing a nice API around OO code I find that surprising! Overloading feels like such a minor feature of OO compared with overriding, inheritance, dynamic dispatch, etc. I guess it must be one of those things whose importance is invisible until you lose it! &gt; which is why I'm so looking forward to TDNR Type-driven name resolution? Are you referring to [OverloadedRecordFields](https://github.com/ghc-proposals/ghc-proposals/blob/master/proposals/0002-overloaded-record-fields.rst), or is there a more general proposal I haven't heard of?
You could use a secondary way of highlighting like underlining or background color.
When I read about this Idris feature I thought it looked more like a party trick than something I'd ever want to use. The "closest binder" is really what kills it for me. I suspect idiom brackets would give us more of what we want and less of what we don't.
I think TDNR and OverloadedRecordFields are [pretty much the same thing](https://prime.haskell.org/wiki/TypeDirectedNameResolution). Re overloading: it's just one of those things that I never realized was so tricky until I actually wanted to create a uniform, type-safe and extensible API around an OO library. No prior art I could find addressed it directly. I read whatever I could find from the implementation of WxHaskell to OOHaskell to various other white papers. What I came up with solves the problem but has significant tradeoffs including increased compile times and namespace pollution but I was, and still am, at a loss as to how I could have done it differently.
Just update [Haskell Amuse-Bouche](https://www.youtube.com/watch?v=b9FagOVqxmI) for the latest GHC and use it directly. Great intro targeted at existing programmers.
&gt; the tools that were originally built by the builders of this weird language According to [A History of Haskell: Being Lazy With Class](https://www.microsoft.com/en-us/research/publication/a-history-of-haskell-being-lazy-with-class/): &gt; In 2004, Isaac Jones took up the challenge of leading an effort to specify and implement a system called Cabal that supports the construction and distribution of Haskell packages. Subsequently, David Himmelstrup implemented Hackage, a Cabal package server that enables people to find and download Cabal packages. Haskell is from 1987. The GHC compiler was first released in 1989. Before `cabal` and Hackage, source code was distributed as archives, builds were orchestrated with `make`, and you had to carefully craft long `ghc --make` incantations on the command line. Incidentally, someone needs to tell the team behind [Quipper](http://www.mathstat.dal.ca/~selinger/quipper/) about all these improvements. `cabal` certainly made Haskell tooling far less painful, and although it was not an initial feature, careful use of `cabal sandbox` has long made "`cabal` hell" a non-issue. Both `cabal` and `stack` are third-party tools, separate from the Haskell language and GHC compiler. You need to evaluate each one on its own merits.
Ah, yes, that was (the gist of) it!
M-x package-install RET haskell-mode
I heard about `intero` in emacs side. How do you think about it?
Never used it, sorry
Coincidentally, I just recently [open-sourced a library](https://github.com/grafted-in/web-scraping-engine) that provides many of these features. In fact, the code looks very similar to your proposal! My library is still very crude and needs a lot more polish, but I've used it on websites of more than 60,000 pages without any issue. My library has an example project that uses scalpel-core to collect data. The features are: * Pipes scraped records to a CSV file * Caches all page downloads * Never requests the same URL twice (within a run of the scraper) * Allows for configurable throttling * Supports concurrent and anonymous requests by distributing all download requests across any number of concurrent threads connected to Tor proxies * Supports "cache-only" mode for testing purposes (i.e. it only uses cached data and will not touch any servers) * Uses User-Agent spoofing * Live app monitoring using EKG The seeding process is very simple and could easily start with a file or some set of known web pages. Things that are not yet supported: * Automatic adherence to `robots.txt` (right now you must configure your rules and throttling manually) * (Closely related) Per-domain throttling * Making HTTP response headers available to scraping rules * Queue control (though you could do this without too much hassle even now) * Distributed scraping (sharing the queue and results across machines) * More advanced reporting/logging There are a few things generally that could improve: * The program runs forever and doesn't know when it's done. It could detect an empty scraping/download queue and stop. * Documentation is sparse. * The code is rough in general, as I blitzed through this.
Thank you for the clarification. 
Agreed, but such is the life of a low population state hanging all lonely like off the side of Canada. Hiring locally and training someone into the language is the most ideal approach of course, however, doesn't hurt to post around on the off chance there is someone interested that is lurking here :)
For more context you may also want to read the ["Why is stack not cabal" blogpost](https://www.fpcomplete.com/blog/2015/06/why-is-stack-not-cabal) as well as [this blogpost about the relationship between Haskell.org, Hackage and Cabal](https://web.archive.org/web/20170225054418/http://www.snoyman.com/blog/2016/08/haskell-org-evil-cabal) 
I think the biggest source of confusion might be the order of binds, especially with nested structures. Like, `let y = 42 in f !(g !(print y) !x)` as `print y &gt;&gt; x &gt;&gt; g y' x'` is sensible but not at all obvious at a glance.
Frankly I think in this day and age gluing together an HTTP client library and an HTML parser library is far from sufficient. You need to have JavaScript execution and a full DOM implementation. Many websites are now based on react or some other UI library and have empty HTML. I think efforts should be focused on making the existing webdriver library more robust. 
But [Stackage](https://www.stackage.org) already does all that! Stackage provides docs [reliably](https://mail.haskell.org/pipermail/cabal-devel/2014-January/009629.html) for all LTS and nightly snapshots in a single location!
Sometimes it's necessary - I'd estimate about one time in ten. In those cases I usually just use phantomjs - if it's an important use case to you, I suggest focusing your efforts on it.
I'm using intero and it has everything I need completion, show type under cursor, on the fly type checking, switch to tests, send code to REPL... Plus all the awesome features of emacs. Really great stuff and far easier to setup than my previous config with 10 different tools. I tried Haskforce a bit but I'm not an IDE person and always get back to my beloved editor.
Before reading this, I already know hs-scrape (which is just scrapy updated to use wreq and xml-conduit) is missing a lot. I think the recent scraper framework may be the way forward. I'll look at it this weekend and see if I agree with design and can contribute. Now to read the article!
I've documented the features of various Haskell plug-ins in [this chart](https://github.com/rainbyte/haskell-ide-chart). Feel free to make a PR o any comment about it. Good luck!
&gt;Sometimes it's necessary - I'd estimate about one time in ten. In those cases I usually just use phantomjs - if it's an important use case to you, I suggest focusing your efforts on it. I've been very close to making a phantom.hs many times... 
Great article, it covers scraping well. I now use and enjoy lens-aeson very often. You pushed me to at least want to add lens support to hs-scrape (maybe just with taggy-html) or to /u/eacameron's library if it works well for my most common use case of light scraping and automation.
Thank you! We should probably talk about collaborating on one of these projects - more wood behind fewer arrows and all that.
Sounds very useful. Good luck!
Hello, I am in the same situation as you, currently learning Haskell for the sake of expanding my knowledge. It has currently eaten up quite a bit of hours, and I am currently grasping typeclasses et al. All in all, it is a very elegant, precise language, and one can see the well defined pattern of computation that is made up by lambda calculus. Am I going to pull it through? I don't know, I worked through 200 pages so far, and learned cool stuff. However, I need to finish 800 more pages, and things start to be more and more complicated, and I have not seen a single dreaded Monad yet. Clearly, delayed gratification is at play here, and I am not even sure if I will be able to build software after I have finished the read. Am I going to use it? One thing is sure, I realized that I do not like writing tests for Python, and any tool that helps me avoiding it is a blessing. For me Python, unfortunately, occupies a local maximum in usability when it comes to *hacking something quickly together to get to a one time result*. Right now, my hope is to use Haskell in web development/exploring other things such as prototyping software concepts - and benefit from correctness and speed that comes with Haskell. And if not, typeclasses and such is a tremendous entry into Rust, and even that introduction level knowledge has given me interesting concepts about how I view software. 
!RemindMe 3 months
I will be messaging you on [**2017-10-14 22:58:27 UTC**](http://www.wolframalpha.com/input/?i=2017-10-14 22:58:27 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/haskell/comments/6n9b8k/asking_for_advice_on_how_to_make_playing_with/dk8fhiz) [**5 OTHERS CLICKED THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/haskell/comments/6n9b8k/asking_for_advice_on_how_to_make_playing_with/dk8fhiz]%0A%0ARemindMe! 3 months) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! dk8fhxb) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
One thing that sucks about lens-style parsers is that you throw away the information about *what* failed. Given foo ^? _Object . ix "foo" . _Array . ix 2 . _Object . ix "user" . _Object . _ix "name" . _String this is going to return a `Maybe` with 0 information on *which* part failed. I ended up writing a tiny utility for parsing ill structured XML that keeps track of breadcrumbs -- essentially `ReaderT [Text] (Either ([Text], XmlParseError))` where `throwError` grabs the current "frame."
Good point. Mostly just wanted to contrast it with the "parse to a datatype" model, which I do think is the right model for almost every use case but this one. 
[removed]
Watched your talk! Really neat!!
No, that got fixed a while back. I got to deal with this when porting ghc to alpine linux. Post 8.0.2+ you're fine, ghc detects what gcc/clang are compiled with. PIE is the one that bites you more.
I think I did this thing a while ago: http://hackage.haskell.org/package/each-1.1.0.0/docs/Each.html Nobody really thought that was of much use. Me neither.
 &gt; import Data.Map as M &gt; import Data.Function &gt; M.fromList [("a",1)] &amp; M.unionWith (+) $ M.fromList [("a",3), ("b", 0)] It already works today with ever so slightly more baroque syntax. Chung-chieh Shan proposed a pair of operators to do this in-language about a decade ago in passing. It never went anywhere, but when we incorporated (&amp;) into Data.Function we gained an equivalent pair.
All of the alternate encodings of this idea as operator pairs care about relative precedence, too.
 :(
He says "A python programmer could", that's correct, a Python programmer might form that argument, since the non-sense will not be detected until conversation time :p 
And then some one in one-hundred you need a real browser and pull out selenium.
Wouldn't it be cool, if those tasks in the [Call for participation section](https://haskellweekly.news/issues/63.html#call-for-participation) were rated from beginnerfriendly to expert level? (Beginner doesn't need to mean easy, just that a beginner of haskell can do it, so the surrounding code should be understandable without a lot of monad transformers fu or so.)
I liked this assignment, and generally the effort spent to assess each candidate individually and transparently. I received some useful feedback along the way, although it would have been nice to receive more specific criticisms of good and bad parts of the code submission. It would also be nice to limit the scope of the assignment a little more. Overall, LeapYear has been my favorite Haskell (or any language) interview process and programming assignment. I have never received such useful feedback as I have from interviewing with them. Their transparency and commitment to help all applicants improve their knowledge and skills is really admirable.
It turns out that even discussing the removal of comments falls under the "Obvious trolls are obvious, and also useless, so they will be removed with extreme prejudice."-clause.
So after some digging I discovered Arch changed [gcc to enable SSP and PIE by default](https://git.archlinux.org/svntogit/packages.git/commit/trunk/PKGBUILD?h=packages/gcc&amp;id=5936710c764016ce306f9cb975056e5b7605a65b) and although I don't understand what that means (or even how stack detects that), it seems obvious that it did affect stack and this is the response - it now requires the nopie version of ghc. I'm now wondering whether I need to re-install/re-compile all the packages that stack has installed? Any help is appreciated.
A pretty lame but enlightening trick: So the free monad gives us a monad for every functor. Now every monad is a functor, so every monad gives us a monad! This means every monad comes with a scripting language for free, we do not need an additional data structure describing the operations. IO is already that data structure : type ReadWrite = Free IO putStrLn' :: String -&gt; ReadWrite () putStrLn' s = liftF (putStrLn s) getLine :: ReadWrite String getLine' = liftF getLine myscript :: ReadWrite () myscript = do { putStrLn' "hey"; x &lt;- getLine'; putStrLn x } interpretIO :: ReadWrite a -&gt; IO a interpretIO = foldFree id 
That's really interesting. Could you modify the behavior of the interpreted program by substituting `id` for another function? I am still trying to wrap my head around the definition of [foldFree](https://hackage.haskell.org/package/control-monad-free-0.6.1/docs/Control-Monad-Free.html#v:foldFree).
I'm probably very biased toward my own point of view, but I have always assumed that * tag is the "correct" terminology for a field that exists at runtime and distinguishes between two variants of a type; and * class is the correct terminology for a set of types that have some parts of their interface in common (both Car and Helicopter belong to the Vehicle class of types and support some of the same operations). You don't strictly need tagging to have classes, but that would mean once you view a Car as a Vehicle you don't know it's a Car anymre. So most languages tag their classed types, and perhaps that's the confusion? 
Yep, thats one of the core tenets. You decompose your problem space into an algebraic representation, and then define interpreters into your target monad.
Can we do something interesting with this even if IO is completely opaque? How do we actually tell apart getLine from putStrLn in our interpreter?
You can't. `Free IO` is pretty uninteresting, and it's hard to imagine a scenario where it's useful.
Except using `Free IO` doesn't really offer any of those benefits =P
[removed]
starting from the fact that every package uses his own monad/monad stack, every program that uses two of them is an example. You have to put yourself in the foot of a beginner... Many Haskellers would think: "This is the way things are in Haskell". I don´t think so. I think that this is the result of an inmutare and exploratory stage of the Haskell community and ecosystem. With this experience we should advance towards some standard for the composition of effects. 
Your `&lt;` reminds me of shift and your `|` reminds me of reset.
In this case the interpretation isn't very exciting since there aren't any interesting natural transformations `IO ~&gt; f`, other than `id`, to feed the interpretation (`foldFree` here is equivalent to the other `foldFree` in eg. https://hackage.haskell.org/package/free-4.12.4/docs/Control-Monad-Free.html ). Also you can just `liftF` myscript as a whole, no need to define the separate operations. Where this trick might be interesting though, could be the free applicative/free arrow. Then we can lift the operations and combine them into a structure which we couldn't get before. I'll have to think about that some more to see if it makes sense.
It sounds like he was talking about `Free` in general, not `Free IO`.
Why do people write articles inside git repositories in github? Great comparison though.
Noted and updated! Started with a larger project that required more dependencies and did not remove each unneeded dependency. 
I'm using HaskForce, because it has stack support and somehow works (comparing to the other two). 1. **haskell-idea-plugin** - REPL integration does not work - it is unable to locate GHCi. No Lint, no code completion, _goto definition_ sometimes works, sometimes not. No documentation tooltips. 2. **intellij-haskell** - in addition to /u/rainbyte chart, documentation in my case does not work, _goto definition_ - navigation inside single file works, between modules seems not 3. **HaskForce** - in addition to /u/rainbyte chart: _goto definition_ works properly
It is on github because is easy for people to contribute there. People can fork it, make PR's, create issues to follow specific features. Also, I did not test every editor. Various items were provided by other users. I learnt about new features. Win-win experience :D
I tend to use unit tests as a sort of repl when I need a little more importing ceremony to start coding. It's definitely slower than ghci interaction though.
&gt; "Today, object-oriented programming (OOP) rules the IT industry absolutely. It is impossible to dislodge. While functional programming (FP) has seen a resurgence of late, it is typically used as an adjunct to OOP." – Louis Cyphre, 'Why are functional programming languages used so rarely in practice?' Because it has a killer advantage: composition, that in the industry scale means componentization, has not been taken seriously by functional programmers. That's why programmers including the funcional ones (or so called) continue using state machines, a.k.a object-oriented-patterns to do their job. Concernig haskellers, out of abstract loops over abstract lists, parsing strings and bytestrings and little more, almost all of them use type safe imperative programming for the rest with the weird idea that type safety is functional programming. There is more usage of functional patterns in languages like Scala or FSharp than in Haskell.
About 2. Navigation to other modules (also to test and libraries modules) should work. If not, take a look to the Haskell Event log window to see what's going wrong.
I disagree with the crowd here. `Free IO` lets you break up a program into steps. This isn't as useful as it would be with a more informative functor, but I doubt it's entirely useless. instance MonadIO m =&gt; MonadIO (Free m) where liftIO m = Free (Pure &lt;$&gt; liftIO m) Each application of `liftIO` produces a "step", so `liftIO x &gt;&gt; liftIO y` is a two-step program. We can run just one step and pause: step :: Free m a -&gt; m (Free m a) step p@(Pure _) = pure p step (Free m) = m We could then do something like checking after each step whether we'd accomplished our purpose or run into a problem we need to work around before continuing.
About `intellij-haskell` plugin. It is not possible to build project within IntelliJ. The project is only automatically build when project is opened to be sure that dependencies are installed so Stack REPL can load project files.
Seems a bit like the iterative monad transformer, that lets you interleave computations, for example to insert retry delays: https://gist.github.com/danidiaz/e9382127d1c4025b9845
Is there an either equivalent of `^?`?
I think that would work iff you only use prisms. The moment you want to use a `Traversal`, you're hosed.
I think `Cofree Tape a` is a binary tree, rather than a zipper (unless I've misunderstood something). So when you move left and then right you *shouldn't* see the same element again. However, if you want to adjust every element in that "position", you could write something like this: adjust :: (a -&gt; a) -&gt; Cofree TPossible a -&gt; Cofree TPossible a adjust = go . overFocus where go f (x :&lt; TPossible l r) = f (x :&lt; TPossible (go lf l) (go rf r)) where lf (y :&lt; TPossible ll rr) = y :&lt; TPossible ll (f rr) rf (y :&lt; TPossible ll rr) = y :&lt; TPossible (f ll) rr Although you might have better luck with a simpler zipper type: data Zipper a = Zipper { left :: [a], focus :: a, right :: [a] }
Are you sure your `overFocus` shouldn't just be `fmap`? Writing `overFocus = fmap` gives the behavior you describe for your two test cases.
fmap affects ALL 'a', not just the focused ones :P
I'm mostly doing this as an experiment in Cofree to see if this is possible rather than actually solving a problem ; otherwise I'd just do the simple zipper :)
https://hackage.haskell.org/package/comonad-5.0.1/docs/Control-Comonad.html#v:kfix https://pdfs.semanticscholar.org/89d3/57437a7b0d069fa81037b79fd96c819985c7.pdf
Oh, you mean like lensing in at a position? Yeah you need to recurse over moving left and right I think. overFocus f (a :&lt; (TPossible x y)) = f a :&lt; TPossible (moveRight $ overFocus f (moveLeft x)) (moveLeft $ overFocus f (moveRight y)) EDIT: Nope, that's wrong too, lol. I see why you're confused!
The **OP link is broken**, here it is on the way back machine: https://web.archive.org/web/20170412173619/http://blog.tweag.io/posts/2017-03-13-linear-types.html
This looks extremely interesting; and I've actually watched Kenneth Foner's talk on his Comonadic Spreadsheets, but I'm not sure how kfix helps in this case; can you please offer an example?
Or if the optics contained failure information, but not sure if that's feasible. I haven't found it that bad to break apart long lens chains and see what's failing, though it's obviously trading ease of writing for maintainability by not have error information.
Impredicative polymorphism doesn't really work in GHC. As the [GHC wiki](https://ghc.haskell.org/trac/ghc/wiki/ImpredicativePolymorphism) says: &gt; We've made various attempts to support impredicativity, so there is a flag `-XImpredicativeTypes`. But it doesn't work, and is absolutely unsupported. If you use it, you are on your own; I make no promises about what will happen. In you're particular example, the problem is this: &gt;&gt;&gt; :t Just id Maybe (a -&gt; a) Remember that there is an implied `forall` on the far left of any type that doesn't already have it. So, the type is actually: forall a. Maybe (a -&gt; a) But this isn't the same type as: Maybe (forall a. a -&gt; a) That's why you're getting a type error. You might notice that any expression of the first type sort of "has" the second type as well. But GHC's type checker doesn't do any sneakiness like that to make it typecheck. The real solution is to not use impredicative types since they barely work and are barely supported. For the `Maybe` thing, you could write this instead: data MaybeFunc = JustFunc (forall a. a -&gt; a) | NothingFunc And that will work just fine.
The [GHC manual](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#impredicative-polymorphism) mentions that `ImpredicativeTypes` is very fragile, so you are most likely running into a limitation of its current implementation. Typing `id id` does not require impredicative types. id id :: forall a. a -&gt; a That only requires two specializations of `id` with types `(a -&gt; a) -&gt; (a -&gt; a)` and `a -&gt; a`.
Given [] = [R,L] you have an extra law. The construction you are describing is by its very nature not (co)free. So, let's fix that. You want an 'a' for every final location. You have a sequence of Rs and Ls such that Rs cancel Ls and vice versa. The path you take to get there is irrelevant! This is just an Integer of how many more Rs you have than Ls. And for that you have an a. `Integer -&gt; a` is a perfectly legitimate comonad using extract f = f 0 duplicate f m n = f (m + n) You can get this with `(-&gt;) (Sum Integer)` using the `instance Monoid e =&gt; Comonad ((-&gt;) e)` provided by the `comonad` package today. This encoding is isomorphic to the usual infinite stream zipper. (On the other hand being functions it may wind up doing a lot of recomputation. If you go to memoize all the results along the way you can get encodings where you use the usual data Zipper a = Zipper (Stream a) a (Stream a) using something like the causal streams in http://hackage.haskell.org/package/streams-3.3/docs/Data-Stream-Infinite.html With that access at distant points is slow, but we memoize intermediate results. You could also go and use something like https://hackage.haskell.org/package/MemoTrie-0.6.8/docs/Data-MemoTrie.html#v:memo to memoize the direct functional form of this. https://www.schoolofhaskell.com/user/edwardk/cellular-automata/part-1 uses this sort of approach. Later articles exploit something closer to the functional form i mentioned above. Now, if you _do_ really want to modify all the locations in a cofree comonad as if the [R,L] = [L,R] = [] equality held you could turn your Cofree Tape into ([Dir] -&gt; a) and build a new function that takes [Dir] and gives back an 'a' by first normalizing the input [Dir] using that law, then supplying it to the original input function. Then you can convert back to a cofree Comonad. This is not a Cokleisli arrow. It'd have to take Cofree Tape a -&gt; Cofree Tape a in one go, and it ignores everything but the 'canonicalized' members of the tape when producing the new tape. At which point you might as well optimize down to `Integer -&gt; a` anyways. Or define an embedding of `(Integer -&gt; a)` into the larger `Cofree Tape a` and a corresponding retraction of that embedding.
The only things `IO` really observably encodes are exceptions, threads, and refs. Stands to reason that taking advantage of these is pretty much the only meaningful thing you can do with `Free IO`. But I really can't think of anything useful you can do with these anyway. You can record what step index in the program an exception occurs at, but that's relatively meaningless info. One silly thing you can do is use an `IORef Int` or something so that when you run a step, it runs the next step `n` times concurrently, where `n` is the number in the `IORef`, then resets the ref to `1` so the next steps don't fork. But yea, that would be insane.
 `Cofree` doesn't have ways to work with such "quotient structures". This seems difficult to do in the first place since sharing can't be observed within the language, so a generic solution would need an explicit description of that sharing structure. I'm not convinced we can do this in a way that is expressive and performant enough to be useful. More specific solutions I could think of all boil down to working through an isomorphism to zippers. More generally, to transform a recursive structure while preserving sharing, I haven't found anything better than to convert it to an intermediate "canonical structure" with no implicit sharing. We rely on domain-specific knowledge to safely ignore repeated occurrences of shared substructures and replace them with explicit pointers/names/references (your tape example behaves nicely enough that this structure can be optimized further, yielding zippers).
`Ap IO` is actually kind of cool. It's basically saying "I want these things to run, but I don't really care how they do it compared to each other." So when I write: foo :: [Int] -&gt; Ap IO [String] foo ns = for ns $ \n -&gt; do ... -- Expensive operation return ... I'm merely enqueuing a bunch of programs, without regard for what order they execute in (they can even [be forcibly reordered into a different list](http://elvishjerricco.github.io/2017/03/23/applicative-sorting.html)). So it's up to whoever calls `foo` to decide what the execution model is. They could just do things sequentially, or they might do any number of `IO` things like catching exceptions. As a fun example, here's something that will run all the programs concurrently in a max sized thread pool, which will return all exceptions thrown rather than short circuiting on the first one: import Data.Validation (AccValidation (..)) import Data.List.NonEmpty (NonEmpty (..)) import Control.Concurrent.QSem (QSem, waitQSem, signalQSem, newQSem) import Control.Concurrent.Async (Concurrently (..)) import Control.Exception (bracket_, Exception, try) import Control.Applicative.Free (Ap, runAp) import Data.Functor.Compose safeConcurrentPool :: Exception e =&gt; Int -&gt; Ap IO a -&gt; IO (AccValidation (NonEmpty e) a) safeConcurrentPool n x = do -- This semaphore will block more than `n` threads from running at a time sem &lt;- newQSem n runConcurrently $ getCompose $ runAp (run sem) x where run :: Exception e =&gt; QSem -&gt; IO a -&gt; Compose Concurrently (AccValidation (NonEmpty e)) a run sem a = Compose $ Concurrently $ bracket_ (waitQSem sem) (signalQSem sem) $ do e &lt;- try a return $ either (AccFailure . pure) AccSuccess e 
Right! I was thinking about a Representable instance using [Dir] as the "index" then just summing up L and R as -1 and 1 respectively inside the index function, this is good stuff, I've got a bit more reading to do!
I was just thinking about this exact problem, and came up with [this result](https://gist.github.com/paf31/c18665a026e5bf0599c9af609e3d2845), which says that the equalizers of certain comonad morphisms like these ones (move left, move right) are also comonads. So you can work in `Cofree` (or some other comonad) and treat it like the quotient you want, as long as you only work with comonadic values which equalize the morphisms in question, and functions which preserve that property. The problem is that `overFocus` is a function from `Cofree` to `Cofree`, but it does not factor through the equalizer. Note that you can turn a value in your `Cofree` comonad into a value in the quotient comonad by just forcing the appropriate equalities to hold everywhere. We know that left then right must be the same as right then left, which in turn must be the same as staying still, so we can enforce this as follows: fixUp cof@(a :&lt; Tape (b :&lt; Tape ll_) (c :&lt; Tape_ rr)) = a :&lt; Tape (fixUp (b :&lt; Tape ll cof)) (fixUp (c :&lt; Tape cof rr))) (I think ... I haven't tested this)
This is partially motivated from your sketch actually! Unfortunately I got a bit lost amongst the phi/gamma/psi's to follow it all the way through; I'd love to see a more complete example in the gist if you have time! If I understand your fixUp code snippet I think we still run into some problems; it should properly point back to the original element after [R, L] inside a given `fixUp w`, but as soon as you make any modifications the recursive loop would be lost since Haskell uses values instead of pointers. I.e. it doesn't matter how we obtained the value (even if we looped back to the top 15 times along the way), it will expand that recursion if we make a modification and won't write back the change to the other branches (unless you implement fixUp for every transformation you want to apply). If I'm misunderstanding then please clarify; thanks for the post!
You can modify the program to stop after the fourth command
But... Why? That doesn't seem to qualify as "something interesting" beyond novelty.
Only in /r/haskell would this not be construed as a free offer... https://twitter.com/eacameron88/status/886429155574808576
You can interleaved two or more actions
Could you elaborate?
 {-# LANGUAGE ApplicativeDo #-} import Control.Monad.Free interleave :: Applicative m =&gt; Free m () -&gt; Free m () -&gt; Free m () interleave (Pure ()) r = r interleave l (Pure ()) = l interleave (Free l ) (Free r ) = Free $ do l' &lt;- l r' &lt;- r return (interleave l' r') To a first approximation, most things that you can do with lists have an analogous operation on free monads. So if you can interleave two lists you can interleave two free monads
I agree with your assumptions, but I do think that realistically you need at least either tagging or a suitable type system to implement classes. In my case the source of the 'confusion' is this [2011 blog post](https://existentialtype.wordpress.com/2011/03/19/dynamic-languages-are-static-languages/) (found it!), that also seems to have provided the thought that weaponized the term 'uni-typed' for use in flame wars. It was not the first time that this thought was [presented](https://link.springer.com/chapter/10.1007%2F978-1-4302-1990-3_6?LI=true) though, but it does seem to be the earliest blog post about it.
&gt; other languages become more difficult? They become more painful. So, more difficult in that sense. But not more difficult in the time it takes to get a job done. In that sense, other languages become *easier* after you learn Haskell.
I think you should take a look at this [website](https://wiki.haskell.org/Typeclassopedia). It does not contain all of the terms you listed, but it is a good starting point.
You may be interested in [Clean](http://clean.cs.ru.nl/Publications). From what I understand, the fact they're using graph rewriting means it's really easy to send any parts of a program over the network, execute it remotely and send the result back.
Thanks, I did mean the graphs paper.
Your best bet is to start with a `stack` binary and build `cabal` if you need it, not the other way around.
You need to install `ghc-static` package (see https://wiki.archlinux.org/index.php/Haskell#Problems_with_linking)
Do not start with the whole buffet of abstractions. Focus on writing some programs that you would find useful. When you hit a pain point, some repetitive or ugly code that you feel could be better, look for some abstraction or technique that could help. But only then. Going on abstraction safaris in search of new and strange conceptual beasts *can* be useful as well, but even then it's better to have a battery of practical cases under your belt, so you can go "Can I apply this thing here? Is this simpler than what I already do? How about using it for this other thing?"
My understanding was that the problem was how long it took to derive them for each type not that they were being derived multiple times. Unless there is a way to cache something generic to improve deriving performance. Is that what you mean?
I took it to mean once you've derived an instance then you can write that to disk in a cache so that next time you compile the same file you just load the cached code in if the type is the same. That would be an improvement if it's really the deriving that's slow and not compiling the actual resulting code.
See https://github.com/commercialhaskell/stack/issues/3268 for a workaround. You will need to reinstall all packages once you switch to a different GHC (which you will need to do) but stack will do that automatically for you.
I'm surprised you got `No instance for Num a`. This is what I got (using GHC 8.0.1): Lit.hs:16:5: error: * Ambiguous type variable `a0' arising from a use of `f' prevents the constraint `(IsLit a0)' from being solved. Relevant bindings include x :: X a0 (bound at Lit.hs:16:1) Probable fix: use a type annotation to specify what `a0' should be. These potential instances exist: instance IsLit Int -- Defined at Lit.hs:7:10 instance IsLit String -- Defined at Lit.hs:10:10 * In the expression: f 1 In an equation for `x': x = f 1 &gt; Why doesn't the compiler see that the only intersection of the set of instances for the two type classes is Int It may see it, but the semantic of instance is that they should not be ambiguous and that class are open. It means that instance resolution should not change its result if later you add a new instance for the class. The issue here is that you have an instance for `Int`. However, `1` is not an `Int`, it is any `Num` possible, and it will be converted to any `Num` using `fromInteger` of the `Num` typeclass. Think about it as a "polymorphic literal". This literal is usually "forced" to a known type by some function call. However here that's ambiguous, hence the compilation error. To force it, you can force the type of `1`: x = f (1 :: Int) or force the type of x: x :: X Int x = f 1
Arch packages are borked for development, you're gonnna want to install the binaries for ghc/cabal/stack directly.
You should get the haskell book. It starts off from the basics and gradually builds up. It will make learning far much easier than piecing things yourself from reading random articles, and far more thorough than other haskell books which are often written by academics and presuppose some familiarity with the concepts already. http://haskellbook.com/
A tip for finding abstractions once you go looking: when you start to notice repetitive code that you think there might be an abstraction for, try to figure out what the type signature for a generic function to replace it might be. Then use hoogle to search for functions with that signature.
Also I would *definitely* stop reading random articles if you're coming across terms like coalgebras and comonads. That stuff is quite advanced and mostly discussed by people who are trying to push the language in new theoretical directions. You can safely become a proficient haskeller without them, and you won't have the expertise to understand why those things are useful without quite a bit of experience with the more commonly used parts of the language first. 
But the derivation can depend on imports, so it is not obvious when to invalidate that cache, right?
Now, ideally making a scraper would be a few hours of work, including finding CSS selector. My description of the task: scrape = withWebDriver ... $ do get "http:///...." elts &lt;- cssSelect "a .docLink" forall elts $ \elt -&gt; do click elt -- we enter new page subElts &lt;- cssSelect "a .textLink" forall subElts $ \subElt -&gt; do contentElt &lt;- cssSelect ".content" liftIO $ writeFile (uuidFrom (show subElt ++ show elt)) $ htmlText contentElt I have seen a lot of people willing to talk about it, but few willing to offer solution. Even one I hired, has just reposted the question on Reddit, instead of writing the code :-).
Try this: $ pacman -Rns stack ghc (whatever other haskell packages you have) $ pamcan -Syy $ pacman -S stack
I ran into this problem at boot as well, and the above is exactly what I did, and then I used stack to setup the compiler, and then modified my environment such that `stack path` is used to generate the correct `$PATH` for everything. Then I reinstalled the programs I was missing(xmonad, xmobar, etc) and everything worked like a charm again.
seems nice ... you should have written it yourself in one hour :-).
and the question asked here is not only for me it will also help other beginners ...who are interested!
It seems its better that you should have understanding of datakind in Haskell so that you can get the meaning of type level data definition you can find it here &gt;&gt; https://downloads.haskell.org/~ghc/7.8.4/docs/html/users_guide/promotion.html 
the haskell book is super great. Also, looking at other people's code. Haskell is pretty expressive and people write differently - reading code can give you a different perspective even if you don't quite understand it yet.
This has fixed cabal for me, I'm still getting issues (`collect2: error: ld returned 1 exit status ``gcc' failed in phase ``Linker'. (Exit code: 1)`) with `stack build` though. If anyone has any additional ideas. 
I still think it is such a weird decision by Arch to dynamically link Haskell... 
&gt; Why doesn't the compiler see that the only intersection of the set of instances for the two type classes is `Int`. Because it isn't. There might be an instance of `Num String` in a different module. The typeclass system has to work under the open world assumption. &gt; Is it possible to create a constraint kind/type that restricts the set of types. (I imagine something of the form `a ~ Or (Int, Or (String, Bool))` Yes, it is possible to implement (inclusive) OR on `Constraint` kinded types. Pretty sure it's also in one of the libraries that do type-level constraint "magic",. but I don't have a link for you. You wouldn't write it quite that way though. Probably more like `OrC1 ((~) Int) (OrC1 ((~) String) ((~) Bool)) a`, though maybe it can be made a bit prettier than that.
What is your definition of an “ordinary shell script”? Something written in POSIX shell? Things like `sh` and `bash` are effectively programming languages, so if that’s what you’re asking, realize you’re basically asking for a Haskell to `sh` compiler, or at least a Haskell DSL that compiles to `sh`. If that’s what you’re asking for, I don’t know of any such tool, but the good news is that a Unix “shell script” is more general than that, depending on your definition. You can use the shebang line to write scripts in just about any language you want. Consider using [stack’s script interpreter](https://docs.haskellstack.org/en/stable/GUIDE/#script-interpreter) to make your Haskell scripts reproducible and self-executing without needing to compile a limiting subset of Haskell to another language.
By ordinary shell script I mean something that can be run via bash / sh without needing ghc / an executable to be run.
Think I finally fixed this by removing the `.stack-work` directory within my project.
Not obvious doesn't mean it's not impossible. Author just suggested an idea.
I'm not sure what the advantage would be. I couldn't call even the simplest Haskell functions (since they aren't natively implemented by the shell). The library might some with "lifted" versions for some functions, that it had a shell "RTS" implementation of, but that would be quite limited. I certainly couldn't use my own lambdas directly. At that point, I'd rather just write in shell. You might just want to use `shellcheck`, which performs a at least some static analysis on shell scripts and is written in Haskell. Or you could jump over to Idris, and update this backend to work with 1.0: https://github.com/mietek/idris-bash
Then yes, you’re asking for a compiler. That’s really hard. As mentioned, you could write a Haskell DSL that compiles to bash instead of trying to write a full Haskell runtime in bash, but I am skeptical that such a thing would be worth the trouble. Either way, I don’t believe any of that exists. You could try to write something and prove me wrong, though!
So far, badly. You seem to be asking questions on reddit instead of learning Haskell. ;P /s
You can compile the program. Now you have an executable binary that you can run without bash
I occasionally need to write shell scripts that need to be handed over to other individuals. So it's not ideal to hand over an executable or Haskell 'script'. Rather than having a 'Haskell -&gt; sh' compiler I was thinking more on the lines of having a DSL of somesort that could compile to sh. And woah I never realized shellcheck is written in Haskell, that is pretty cool!
…yes, this is sort of the most obvious solution, isn’t it? I am a little amused I didn’t think to mention it. My guess is that the OP wants something more cross-platform, hence the desire to compile to some other language, which I agree *is* something that’s sometimes nice about other languages. Having a nice way to distribute cross-platform Haskell programs would be great, but it sounds tricky due to the way Haskell is compiled.
there's a dsl for bash scripts https://hackage.haskell.org/package/shell-monad
/u/spirosboosalis mentioned https://hackage.haskell.org/package/shell-monad in [another comment](https://www.reddit.com/r/haskell/comments/6nnagw/anything_like_turtle_shelly_that_generate_actual/dkarzvz/). Could this be what you are looking for?
To start, focus on internalizing the distinction between pure and effectful code. Basically, there is a difference between a pure function that produces a value, and an effectful function that does a thing before producing a value. (this is the famous "IO monad") Make sure you really understand the distinction and how to use the one from the other. For example, how to take the result of an IO function and pass it on to a pure function, and vice versa. I don't know a book or article that specifically shines when it comes to explaining this idea, but it's one important big idea that prevents you from writing Haskell that's awkward and terrible, but can accomplish anything you can do in any other language you know. When you reach that point, you'll be ready to dive into things like the [Typeclassopedia](https://wiki.haskell.org/Typeclassopedia) for ideas and tools to clean things up. 
&gt; Are there any resources which describe all this kind of stuff with applications and if not how would you approach learning it? I would seriously suggest *not* learning what all this stuff is and just writing some Haskell code that does something. The abstractions are for applying to Haskell code that already works when you already understand Haskell really well.
I went a step further. Stack is still dynamically linked on Arch and pulls in some stuff iirc. I wiped and purged my arch install of all arch related things and then installed stack manually as a static binary from the stack website. Then from that I put stacks stuff in my path and I link to one of the stack installed ghcs in my path. Pandoc and one other Haskell package I had (shellshock), I installed the static linked versions through the AUR
It's a decision made from a purity standpoint from people who just don't understand the Haskell ecosystem and how the packages work. I remember the sentiment "just because Haskell does things wrong doesn't mean we need to fix their stuff for them" being thrown around as a justification to link dynamically.
Could Conal Elliott's work from [Teaching New Tricks to Old Programs](https://www.reddit.com/r/haskell/comments/6kncaj/yow_lambda_jam_2017_conal_elliott_teaching_new/) be used to get around &gt; I couldn't call even the simplest Haskell functions (since they aren't natively implemented by the shell).
I had been learning haskell from this book called "Haskell Programming from first principles" and I have found it very useful, really helpful with much of haskell stuff. 
I thought the mathematical convention *was* right-associative? https://en.wikipedia.org/wiki/Order_of_operations#Special_cases
What mathematical convention are you referring to? FWIW Python also has a right associative `(**)`.
Must have been a late-night thinko on my part!
Must have been a late-night thinko on my part!
is it possible to setup the plugin to automatically show every type under the cursor while in Normal mode? (ie. without typing :InteroGenericType)
&gt; something that can be run via bash / sh without needing ghc / an executable to be run That's a lot harder than it sounds. Nearly any useful shell script will call a variety of executables in addition to `sh`, such as `rm`, `cp`, `date`, `mv`, and who knows what else. These are not built in to the shell. That's fine if you are writing a script that will be run only one one system. But across different systems, portability is an issue. Different platforms have different options. If you restrict it to GNU systems it's a bit easier...although typically the reason to write shell scripts in the first place is for portability...especially if you want to compile Haskell to shell, because if you're writing something to just run on one system, it's a lot easier to just install `stack` and use that to run your Haskell script. That means writing a shell script compiler is tricky business. [Autoconf](https://www.gnu.org/software/autoconf/autoconf.html) is a shell script compiler, and reading its documentation shows what a complicated thing this is to do portably.
[Here you go](https://gist.github.com/gelisam/ff86251fee1a87a1357c0b4745ba8b96)
I don't have a direct answer. Random, loosely related topics that might help - Unison might be of interest (I feel like I am becoming some sort of strange promoter for it now). There seemed to be a number of languages/frameworks that are either implemented in or spun off from Haskell for seamless server/client side web dev (Ur/Web, others) which could apply to mobile apps that are not native. There were also web efforts like MFlow and PLT Web Server, which used conversions to continuation passing style, more for allowing simpler expression of "wizard" like UIs rather than for performance optimization. To make this more complex, you could throw in verified distributed computation like Pepper.
Impressive initiative to advocate for getting it into the public domain, and putting Morgan Stanley GitHub on the map!
I would have thought that the compilation would only occur once and the object file would then be loaded instead of the source. Unless there's a lot of inlining which would need those derived functions to be added into the code multiple times.
That's exactly what I meant. It sounds like I should write some benchmarks. Unfortunately life is pulling me in too many directions for me to do this in the near future.
Describe some application you have enjoyed contributing to or developed in the past in some other language. That could help us come up with some Haskell projects that you could start reading the internals of or help steer you towards small projects you could attempt.
Thanks for the explanation. So with GHC is it not possible to create the type Maybe (forall a. a -&gt; a) ?
This is cool IMO. They basically picked Haskell, but a bit differently (in the form of another language). This reminds me of the Standard Chartered approach mentioned in [Don Stewart's talk from 2016](https://skillsmatter.com/skillscasts/9098-haskell-in-the-large-the-day-to-day-practice-of-using-haskell-to-write-large-systems). They're using Haskell, but they're using some other compiler instead of GHC. 
Yes! Thank you, this is exactly what I had in mind. I'm amazed this exists!
I cannot say for sure that it isn't possible, but I am unable to find a way to do it. Even trying stuff like Just (id :: forall a. a -&gt; a) doesn't work.
Yes! Just need to figure this out. I've also found this: https://github.com/corajr/shmonad
Check the Haskell Pyramid https://docs.google.com/presentation/d/1bSANLVcGnfVIFjicj81Uo_MYQhsF0FZi_EF-NEKFecE/mobilepresent?slide=id.g21ecc886bf_0_64
The problem here has to do with Arch packaging ghc for dynamic linking by default without really understanding the Haskell ecosystem.
Should be - just invoke InteroGenericType on the CursorMoved event.
Too bad it's written in c
I am pretty sure cabal new-build handles this just fine. 
It's C++. What's too bad about it?
The font looks like [Ubuntu Monospace](http://font.ubuntu.com/) ([yep](https://github.com/haskell-infra/hl/blob/032bf7e477d99a5af7a520581286df39c0ab577f/static/source/scss/_typography.scss)).
Thanks, in my case `stack setup` *was* able to install GHC, so rebuilding/reinstalling packages is doable - especially since, as you say, stack is managing that anyway. I'm lucky that I'm just starting out with Haskell so I don't have any legacy projects or dependencies.
What kind of articles do you read? To start with Haskell you have to learn functions, recursion, pattern matching and how to use do-notation for input/output. Many people in Haskell community are interested in PL research so they use terms that you've mentioned, but you don't have to learn them in order to write Haskell program. [Real World Haskell](http://book.realworldhaskell.org/) [Learn You a Haskell for Great Good](http://learnyouahaskell.com/)
You might also consider the `[haskell-core]` repository maintained by the Arch Haskell guys.
I feel like the color scheme might have been a custom one tuned for the website specifically. The code is here if you're interested: https://github.com/haskell-infra/hl/blob/032bf7e477d99a5af7a520581286df39c0ab577f/static/source/scss/_code.scss#L1-L18
He probably wants turtles all the way down (: . I am guessing the c/c++ was intentional for the embedded mode.
I've had the same feeling about programs and modules in general. Can't incremental/iterative typechecking be a thing, so that only "touched" sections are typechecked again and recompiled? Considering how GHC recompiles dependent modules when a module is touched (presumably this is mandatory for cross-module inlining purposes even if the API doesn't change), and perhaps also the existence of TH (I'm happy to have "dumb" compilation in that case), this is probably not easy, or maybe even impossible: but if it can be done, there could probably be _huge_ gains in compile times. Cascading wins in recompile times for those dependent modules (which only re-inline those pieces of code that transitively depend upon the changed code), and perhaps the precise parts that change in these dependent modules could be tracked when recompiling the modules dependent upon them, and so on. "Whole-project incremental compilation".
Not directly related but I'm actually developing a toy language in my spare time that compiles down to bash, mostly because I want a scripting language that can run pretty much everywhere (so I can deploy it without having to install whatever language's interpreter or runtime) and is statically-typed (this rules out most popular scripting languages).
I want purity, type safety, and memory safety.
At some point everything running directly on current hardware is going to be translated into machine code and that isn't type or memory safe. 
An example application where this doesn't work is in submitting scripts to, say, a job queuing system (in high performance computing). In this context, you need to compile *subprograms* to shell scripts.
Instead of searching with Google, I recommend searching with [Hoogle](http://hoogle.haskell.org), a search engine specifically designed for searching through the Haskell package ecosystem. For example, searching for the string ```^.``` will give you all the functions named ```^.``` within all packages on [Hackage](http://haskell.hackage.org). The very first item in the [search results](http://hoogle.haskell.org/?hoogle=%5E.&amp;scope=set%3Astackage) links to the function [```^.```](https://hackage.haskell.org/package/lens-4.15.3/docs/Control-Lens-Getter.html#v:-94-.) defined within module [```Control.Lens.Getter```](https://hackage.haskell.org/package/lens-4.15.3/docs/Control-Lens-Getter.html) of the [**lens**](https://hackage.haskell.org/package/lens-4.15.3) package. Reading the docs, it turns out that function [```^.```](https://hackage.haskell.org/package/lens-4.15.3/docs/Control-Lens-Getter.html#v:-94-.) is very similar to function [```view```](https://hackage.haskell.org/package/lens-4.15.3/docs/Control-Lens-Getter.html#v:view), defined within the same module. You can also use Hoogle to do *type-based* searches. For example, searching for the type ```a -&gt; a``` gives you a list of all the functions matching that type. If Hoogle can't find an exact match, it will list matches in descending order of similarity.
These are function names. Using symbols since they're often used infix. Check Hayoo or Hoogle for details on each. 
&gt; I was thinking more on the lines of having a DSL of somesort that could compile to sh. Sure, but if you have `pure :: a -&gt; ShDSL a` or `fmap :: (a -&gt; b) -&gt; ShDSL a -&gt; ShDSL b`, unless you are very, very careful, you have to execute arbitrary Haskell code *after* you've generated the output. And, if you lack both of those, it will be difficult to use any existing Haskell code with your DSL.
One handy way to use something like this is communication between a shell script and haskell program. Here's an example: &lt;https://joeyh.name/blog/entry/shell_monad_day_3/&gt; Also, type checking of the shell DSL can be done using phantom types.
What is your use-case? I'd probably consider it only if you need a lot of interleaved, concurrent reads and transactional writes with where threads often access the same locations at once. Without transactional writes, a simple memory-map should suffice. If the threads do not access data coherently (eg if you need to consume the entire result every time), memory map will not help much, so you can just use streaming IO
Also, `(&lt;&gt;)` is usually a synonym for `mappend` (the semigroup operation). This function is defined in `base` (nowadays, i.e. in base 4.9, [you can find it in Data.Semigroup](http://hackage.haskell.org/package/base-4.9.1.0/docs/Data-Semigroup.html#v:-60--62-) whereas prior to GHC 8 it was defined in Data.Monoid ) and the Turtle package simply re-exports it for convenience.
You might be able to do something like A(Many '[effs], Many '[states]) a -&gt; A(Many [effs'], Many '[states']) a -&gt; A(Many '[effs, effs'], Many '[states, states']) a using my [data-diverse](http://hackage.haskell.org/package/data-diverse) library, which is a record + variant library, by using the [append](http://hackage.haskell.org/package/data-diverse-0.8.1.0/docs/Data-Diverse-Many.html#append) to combine the two states, and [item @effs and item @effs'](http://hackage.haskell.org/package/data-diverse-lens-0.1.1.0/docs/Data-Diverse-Lens-Many.html#item) lens to get to the sub states.
You can do something similar for sum types using [pick](http://hackage.haskell.org/package/data-diverse-0.8.1.0/docs/Data-Diverse-Which.html#v:pick) and [facet](http://hackage.haskell.org/package/data-diverse-lens-0.1.1.0/docs/Data-Diverse-Lens-Which.html#v:facet)
How does this compare to Adam Gundry's uom package?
Yes, which is why you shouldn't have to write machine code by hand.
The main difference is: a) Adam Gundry's package extends GHC with a [type checker plugin](https://ghc.haskell.org/trac/ghc/wiki/Plugins/TypeChecker) to solve abelian group unification (units of measure form an abelian group) for determining when two units are equivalent. b) The simple proof of concept in my article requires no GHC plugins (or any language extensions at all), but it's much less convenient to use. Sometimes the programmer will have to provide evidence to the compiler that two units are equal, for example `m*kg = kg*m`. There is also the [units](https://hackage.haskell.org/package/units) package, which is more convenient ~~but requires Template Haskell~~.
You can minimize time it takes to repeatedly recompile your project if you separate data types declaration and instance derivations into separate module(s) and use smart(er) build system. For example it takes 15-20 minutes to compile all the things in a project I'm working at, but depending on what you change - it might take only a few seconds to compile it again.
And compared to dimensional ? https://hackage.haskell.org/package/dimensional
Last time I talked to Lennart Augustsson (admittedly a few years ago), he told me Standard Chartered had about 1.5 million LOC of GHC Haskell and 1 million LOC of their internal Mu compiler. So it's not all custom stuff.
No that unfortunately didn't work out for me :-/ Now I get this error: Process exited with code: ExitFailure 1 Logs have been written to: /home/schnecki/Documents/projects/ai4industry/dplt/.stack-work/logs/old-time-1.1.0.3.log [1 of 2] Compiling Main ( /tmp/stack9789/old-time-1.1.0.3/Setup.hs, /tmp/stack9789/old-time-1.1.0.3/.stack-work/dist/x86_64-linux-tinfo6-nopie/Cabal-1.24.2.0/setup/Main.o ) [2 of 2] Compiling StackSetupShim ( /home/schnecki/.stack/setup-exe-src/setup-shim-mPHDZzAJ.hs, /tmp/stack9789/old-time-1.1.0.3/.stack-work/dist/x86_64-linux-tinfo6-nopie/Cabal-1.24.2.0/setup/StackSetupShim.o ) Linking /tmp/stack9789/old-time-1.1.0.3/.stack-work/dist/x86_64-linux-tinfo6-nopie/Cabal-1.24.2.0/setup/setup ... Configuring old-time-1.1.0.3... configure: WARNING: unrecognized options: --with-compiler checking for gcc... /usr/bin/gcc checking whether the C compiler works... yes checking for C compiler default output file name... a.out checking for suffix of executables... checking whether we are cross compiling... configure: error: in `/tmp/stack9789/old-time-1.1.0.3': configure: error: cannot run C compiled programs. If you meant to cross compile, use `--host'. See `config.log' for more details Anyways, I go with the package provided at https://www.haskell.org/downloads. The Arch packages seem to be somehow broken. 
I'm considering using it in a desktop GUI application written with Reflex; saving, loading the application state. It's attractive for me to think that it may be enough to loop some `Dynamic t a`s through a database layer and get persistence almost fully transparently and for almost free. (Barring having to write VCacheable instances.) I've also considered using acid-state since seemingly it can also do that, but I'm weary of the limitation that the whole state of the application has to fit into memory at once; which may not be so small in the case of my application. So, knowing this much about my use case, would you recommend against using `vcache`? And if so, what would you recommend instead for transparent saving and loading of application state on the fly?
Thank you. Yes that is what fixes the problems for me. I got no clue who came up with the dynamic linking idea on Arch. This is ridiculous. 
&gt; Sometimes the programmer will have to provide evidence to the compiler that two units are equal, for example `m*kg = kg*m` This is a showstopper bug.
The problem is that the stack package already depends on a lot of haskell packages, which results in many error messages while registering the haskell modules. And starting with ghc-8.0.2 they decided to not use static linking anymore. This explains the errors I get when installing haskell packages. So there lies the problem. The only fix I could find so far is using the package provided at haskell.org.
I'm guessing here, but I believe this project has been in the pipeline for quite a long time (it was being discussed in 2010 and I think earlier) and Haskell had not had the success stories it has had since then. C++ as an implementation language would have been a much easier sell (to the company that employs Stroustrup) and helped get the project off the ground. And there's nothing to stop the compiler being rewritten; this is, after all, precisely how GHC came into being.
Is it possible to write a proof that `Product` is commutative?
Yep! It's a direct application of the `quotientAxiom`: foo :: Product Mass Length foo = construct 5 bar :: Product Length Mass bar = quotientAxiom foo It's inconvenient that this implementation requires the programmer to provide manual proofs like this. Other libraries are more automatic, but I still think this is a neat demonstration of the power of vanilla Haskell.
You shouldn't have to make orphan instances to speed up a build.
Lennart also mentioned "It's silly for bank to maintain its own Haskell implementation" :)
Instances of `MonadUnliftIO` are those types that are equivalent to `ReaderT context IO` for some `context`, is that correct?
I have also done some experiments in this regard, but I have tried to tackle the problem from a different direction. The result of my experiments is https://github.com/mtesseract/physical-quantities. The main idea (see https://github.com/mtesseract/physical-quantities/blob/master/src/Data/Physics/Quantities.hs#L26) is that I define a type 'Dim' (using DataKinds) as follows: data Dimension where Dim :: DimLength -&gt; DimTime -&gt; DimMass -&gt; Dimension DimLength, DimTime and DimMass are basically type level integers. Thus, a Dim corresponds to a integer-triplet, where each number denotes the integer exponent corresponding to the base units for length, time and mass. As an example, a length dimension would have the first number one, all other zero: type DimensionLength = Dim (Length One) (Time Zero) (Mass Zero) A derived dimension like speed would be have type Dim (Length 1) (Time (-1)) (Mass 0) (For simplicity, I have used 0 and -1 instead of the type-level analogs.) By definition, the dimensions are always a quotient of products of base units. A quantity is then simply a number together with some dimension type tag: data Qty (d :: Dimension) = Qty Double deriving (Eq, Show) The package provides functions like: mult :: FormsProduct a b c =&gt; Qty a -&gt; Qty b -&gt; Qty c divide :: FormsQuotient a b c =&gt; Qty a -&gt; Qty b -&gt; Qty c Given the following definitions: kilometer :: Qty DimensionLength kilometer = toScalarQty 1000 `mult` meter hour :: Qty DimensionTime hour = toScalarQty 60 `mult` minute we can now write for example: &gt; speed = kilometer `divide` hour &gt; speed Qty 0.2777777777777778 &gt; :t speed speed :: Qty ('Dim ('Length ('LInt 'Plus 'PosNatOne)) ('Time ('LInt 'Minus 'PosNatOne)) ('Mass 'LIntZero)) &gt; In other words, speed has the dimension length/time encoded in its type. EDITED: typo.
This does seem cool, but it seemed pretty close to re-implementing the thing you're testing in your tests. But at least you're not re-implementing the same implementation, but rather testing against a model. I'll have to give this a play, certainly looks interesting.
Characterizing [units](https://hackage.haskell.org/package/units) as TH-based is totally inaccurate. It works using types. The author is /u/goldfirere, ostensibly the reigning king of types in Haskell. It's an elegant and complete solution, without TH. As an added bonus, an extra small TH-based tool is provided in a separate module. You can optionally use it to remove a bit of boilerplate in certain specific cases. If you don't like TH, don't use it. Personally I wouldn't have a problem with an entire solution based on TH. TH is great. But anyway, units is **not** a TH-based solution.
But I don't think it actually is possible. The libraries that do something similar all require a ton of instances or use TH to generate those instances for every class in scope. Besides, even if you accepted that, GHC would still not do the kind of checking OP wants. The compiler has no idea how to use that constraint to figure out that it should only try Int, String and Bool in subsequent type checking.
Yes, including when `context ~ ()` (which is the case with `IdentityT`). Instances included out of the box in `unliftio` are (IIRC): * `IO` * `MonadUnliftIO m =&gt; ReaderT env m` * `MonadUnliftIO m =&gt; IdentityT m` * `MonadUnliftIO m =&gt; LoggingT m` * `MonadUnliftIO m =&gt; NoLoggingT m` * `MonadUnliftIO m =&gt; ResourceT m` The last three will ultimately move into their own packages (once unliftio-core is considered stable and useful enough to warrant it). There are also some interested trade-offs with the `ResourceT` instance around threading; specifically: using it with `mapConcurrently` and the like is fine, but with `async` or `forkIO` could allow the resources to outlive their lifetime.
It looks like the user would need to jump through quite a number of hoops in order to prove mundane things like `kg*m*m/(s*s) == (m/s)*(m/s)*kg`. Too many to be considered useful in practice IMHO.
My mistake—thanks for the clarification!
[removed]
I don't think that deriving those instances is the culprit, rather than the size of the generated code. As you can see in the dumped derivations hidden in spoilers, most reference other floated-out bindings. These multiply quickly, leading to quite nightmarish `-ddump-simpl`s. 
Not orphans. Just datatype declaration + all the instances for it and nothing else.
Starting at 48:50 it is explained how to emulate a neat feature of Go: [struct field tags](https://stackoverflow.com/questions/10858787/what-are-the-uses-for-tags-in-go) that let you decouple the serialized name of a field from its name in the record, without having to get down to explicitly writing the parser-serializer.
Please add what worked for you and the error messages you saw along the way to the Arch wiki. Don't get emotionally invested, admins are quick to delete things, but it's nice to leave a signpost for other users.
Just to mention, you can do the same with regular Generics, like it is done in `aeson`, for example.
I seem to remember that the `aeson` generic facilities allowed you to [modify the field names](http://hackage.haskell.org/package/aeson-1.2.1.0/docs/Data-Aeson-Types.html#v:defaultOptions), but not to specify completely new names.
This is a good approach IMHO. There are several directions in which it can be extended: 1. Rational powers rather than integral (things like s^1/2 are needed sometimes). 2. Include all 7 base SI dimensions (length, time, mass, current, temperature, amount of matter, luminosity). 3. Allow user-defined dimensions. I'm not sure it is really possible within your approach. I have tried to implement a library with arbitrary user-defined dimensions, and failed miserably. 4. Allow different systems of units to co-exist (SI, imperial, CGS, relativistic...) This is also hard, and not that clear if really useful in practice, as it is always possible to express other units in terms of SI ones. Still it could be nice to have speed of light to be physically represented as 1 rather than 299792458, for numerical reasons (I have no idea if it actually matters in any real computations). Edit: style
Tail recursion also means there's no chance for stack overflow -- but in Haskell because of laziness there is also the option of guarded recursion too! Haskell doesn't even need a stack! (Although GHC sometimes uses one) Haskell variables are variables, not functions, though the lazy evaluation order may make that a helpful intuition for some. Everything is not immutable, check out IORef
For your case you don't even need State, just Writer, but if you hate transformers so much this is equivalent to just returning a tuple of `(value, [String])` where the list is the list ok keys seen. Then use a wrapper function to check the list of keys seen against your desired list. OR thread your desired list *in* to the parser and have it error as soon as it encounters a key not in the list.
Thanks for your feedback. So far it is basically just a proof of concept, nothing fancy. IIRC my package had some issues regarding automatic type inference, but I don't remember exactly what those issues were. Would you be interested in contributing to his package? Currently it is just getting dusty. ~ Moritz 
/u/jaspervdj this is the tweet you probably want to embed: https://twitter.com/acid2/status/874170993778073600. Edit: my bad, I see you linked to that one too later. Great project, a lot of fun!
Except for packages like directory or network that have nonversioned symbols from C code. Using these will result in linker errors.
&gt; But isn't standard looping better than tail recursion because there is no chance for a stack overflow. No. If your function is eligible for tail-call elimination, its stack usage doesn't grow as it loops. Any C compiler that is worth its salt would do the same. &gt; I also would make the case that a 'while' loop in C/C++ reads better than tail recursion. That depends greatly on 1) the problem at hand 2) how well you separated your solution into functions. Pose a specific challenge and I'll take it. &gt; Variables are functions in haskell that immediately return a value. There's an important distinction to be made between imperative and (typed) functional languages. In imperative programs, functions return a value. In functional programs, functions *are* the value. The only distinction between "values" and "functions" is the type. "Returning" doesn't really exist, at least not as you know it from C. &gt; Why is everything immutable? I actually find it quite useful to be able to manipulate the value of something. So what is the philosophical (and practical) reason for making everything immutable. Immutability requires you to be explicit about the effects a function can have. In turn, if everything is immutable, there are suddenly 'pure functions'. It's much easier to optimise code where some building blocks are guaranteed to evaluate to the same value, regardless of the order in which they are evaluated. If you really need mutability (and you should really consider the possibility that you don't), there are a few options; in particular, the `ST` and `IO` monads come to mind.
I guess it might have a problem inferring e.g. the type of `y` from `x = y * z` when `x` and `z` are known. I will test it and perhaps try to fix the problem if it's there.
My first attempt would look something like: class OrC1 c1 d1 a where orC1Elim :: Either (c1 a =&gt; b) (d1 a =&gt; b) -&gt; b But, yeah, generating instances is a pain and inference is trash, especially once you get done implementing all the instances.
What are "fun-callable objects?" Could you share an example of what you think this would look like in Haskell?
You can always use the `const` modifier, no?
Forget about cabal-install, that's only going to cause trouble. Please follow the instructions at https://haskell-lang.org/get-started and you'll be up and running in no time.
Then all fields would end up with the same name, wouldn't they?
Couldn't you have supported stateful monads (with non-dependent state) through something like: class UnliftIO m where type StM m unlift :: m a -&gt; IO (a, StM m) replaceState :: StM m -&gt; m () lifted :: UnliftIO m =&gt; (forall a. IO a -&gt; IO a) -&gt; m a -&gt; m a lifted io act = do (res, st) &lt;- io (unlift act) replaceState st pure res
ZuriHac is a great opportunity to learn so much stuff*s* and meet all those amazing people that make Haskell's community so great! 
I think this is the motivation for the type-checker plugin that is part of the uom package.
Thanks for the nice post and the inspiring work! I noticed a request in the original haskell cafe email for people who have worked with Typescript or VS Code plugins to pitch in and help with the VS Code integration. I looked briefly but didn't see a list of any specific issues listed for this. Is there a thread on the Google group or elsewhere listing specific issues for anyone who wants to help with this? The demo on the GitHub repo of the VS Code integration looked pretty good, by the way.
Does intero use HIE or have plans to use HIE at some point? 
If Intero used HIE, Intero would basically be doing nothing itself at that point. Maybe it could handle target switching better or something?
Zubin has done the major work of getting HIE ready for prime time, as his HSOC project for 2017. The vscode plugin code is [here](https://github.com/alanz/vscode-hie-server), but is very much a copy and paste stick-poking affair. It currently has at least one problem, in that if you ask for all symbols in a project you get each symbol twice. So any attention from someone who knows the right way to do things would be appreciated. One of the major drivers from my side is to get HaRe commands integrated, which means general purpose commands requiring input from users. The unsuccessful attempts to do this are littered through the plugin.
The `liftIO` and `unliftIO` functions are dual to each other: liftIO :: forall a. IO a -&gt; m a unliftIO :: forall a. m a -&gt; IO a So clearly, `unliftIO` ought to be named `coliftIO` instead, no?
My understanding of Intero is that it is a modification of ghci to add some extra commands and a few other tweaks, in order to serve as a backend for some powerful elisp in the emacs side. It focuses on doing this use case well, and succeeds. HIE is aiming to be something more general, and to push as much of the heavy lifting into the backend as possible. It is possibly to use HIE with emacs at the moment, using [emacs-lsp with lsp-haskell](https://github.com/emacs-lsp/lsp-haskell), but this work is on pause at the moment as the main dev is doing an unrelated GSOC project.
Isn't that the point of HIE? 
&gt; Haskell doesn't even need a stack! (Although GHC sometimes uses one) Haskell absolutely needs a stack, otherwise how would the scrutinee of a `case` statement know where to return to the branches?
Yes, my point was that's why having a compiler written in C isn't a bad thing.
You'd have to find a differentiator between Intero and the existing Emacs frontend for HIE, considering the existing frontend already takes advantage of 100% of HIE. Otherwise Intero would just be a clone of that frontend.
&gt; Variables are functions in haskell that immediately return a value That doesn't really mean anything. &gt; What is the advantage of doing this rather than making functions expressions like in ocaml (let add = fun x y -&gt; x + y;; VS. add x y = x + y) There's no difference between Haskell and OCaml here. In Haskell you can do add = \x y -&gt; x + y or add x y = x + y and in OCaml you can do something that's pretty much the same. 
I just went back to that talk - one comment of dons' was illuminating: "we use our in-house Haskell so we can talk to C++ (quant libraries) with lower overheads than if it were GHC". A lot of what I took from the intro document for Hobbes was about its close relationship with C++ interop. Maybe I'm reading it wrong, but it sounds like both places saw and tried to address the same problem.
If your main draw is getting persistence "for free", then VCache would work well for that. This approach seems really cool for user-setting type application state, but I'm having trouble imagining how the state for a gui could exceed memory. Is it possible to separate the state into lightweight config stuff that would benefit from lightweight free caching, and document-like stuff that would benefit from more traditional persistence like sqlite? That way you could use acid-state for one and sqlite for the other and it would be more portable than lmdb. My main issue with vcache is that it was kind of a pain to set up and package with an exe, but otherwise it should be fine for your use if you want to keep it simple and use just one persistence. Out of curiosity, what are you using with Reflex for the GUI?
If you still see a while loop as the solution you are still thinking imperatively and not functionally. It takes time and experience to shift gears. Functional code is often data driven. Walk a list, fold a list, etc. Once you learn these patterns solutions based on them become easier much like using the STL in C++ instead of raw loops. Read other people's code. Ask questions about why they implemented a solution that way.
I've seen "lower" used to mean the opposite of lift before. 
I see. I was not aware there are emacs frontends for HIE. I would love to hear from someone who is using them currently on their level of completeness / usability. 
[Part 1](https://www.youtube.com/watch?v=30xjfaxhdps) [Part 2](https://www.youtube.com/watch?v=joDKi-gQonM) Feel free to ask questions if you have any!
GHC has always been written in Haskell.
The available implementations are listed [on the LSP github repo](https://github.com/Microsoft/language-server-protocol/wiki/Protocol-Implementations) as well as [langserver.org](http://langserver.org/)
True, but you can always use `\case "name1" -&gt; "newName1"`, etc.
I've started watching but then I noticed it's over 3 hours non-stop coding. But I already got questions! What IDE are you using? And what is it with the headphones? How long did it take you to become proficient in Haskell? Will there be cursing later in the screen-cast? 
&gt; What IDE are you using? I don't use an IDE, properly speaking. I use a pretty vanilla Emacs setup with terminal(s) on the side. My dev environment is 100% version-controlled and shared across all my computers here: https://github.com/bitemyapp/dotfiles &gt;And what is it with the headphones? I almost always wear headphones when I am programming, but beside that, I am listening to Alexey speak to me and sometimes I have music quietly going in the background. I'm trying to keep my microphone from picking up sound that isn't me speaking, so I don't use speakers. I don't use speakers outside that context anyway, there are no speakers in my office. &gt;How long did it take you to become proficient in Haskell? I gave a talk titled "How to learn Haskell in less than 5 years" a couple years back. &gt;Will there be cursing later in the screen-cast? I honestly have no idea.
Bother - my mistake. I think I was getting confused, with GHC emitting C for compilation by GCC. Thank you for the correction!
Oof. That's a pretty important shortcoming that I hadn't considered.
**EDIT**: this comment is describing non-persistent amortization only, see [below](https://www.reddit.com/r/haskell/comments/6ns3jk/comment/dkckkw5) for a description of how to adapt this to work for persistent structures *** Discharging debits isn't really a thing that the operation does- it's a trick used in the analysis. I'm going to explain this in the non-persistent case, just because it's somewhat simpler- go ahead and ask if it's not clear now to extend it. Let C(i) be the "actual" time it takes to complete the ith operation; Let A(i) be the amortized cost for the ith operation. The amortization is valid if C(1) + C(2) + ... + C(k) &lt;= A(1) + A(2) + ... + A(k) for any k; in other words, as long as the commutative amortized time is never more than the cumulative actual time. The monetary analogy is that you have a fixed income of A(i) on month i, but you have costs of C(i). The amortization is valid if you never go into the negative at any point, no matter what "kind of month you have" (read: performed operation). Now the banker's method is a way to make this work: The idea is pretty simple: think of your data structure like a bank account. Some operations put money in, other operations take money out. But the bank account can never go zero. This is "extra spending money" that you use to balance out months that were more expensive than expected. There are two requirements: you start with 0 credits, and you can't withdraw more than you have. We're going to *define* an amortized cost of each operation (which is not intrinsic- there are many possible amortized costs, depending on how we want to split up credits, but usually there is a "best" (i.e most representative) choice). The definition is: A(op) = C(op) + Deposit(op) - Withdraw(op). For brevity, call Deposit (op) - Withdraw (op) as Exchange (op). Here's the intuition. We want it to be the case that C(1) + ... + C(k) &lt;= A(1) + ... + A(k) So, we want C(1) + ... + C(k) &lt;= C(1) + Exchange(1) + ... + C(k) + Exchange(k) But here's the thing: we can cancel out the C sum, to obtain the equivalent 0 &lt;= Exchange (1) + ... + Exchange (k) In other words, we're required to never go into the red. But this is precisely the requirement for credit assignment! We're never allowed to withdraw more than we have, so this must be satisfied since it's just computing how much money is in our account at the end of the kth month. Now, how do you actually assign credits? This depends on the data structure. For example, for the basic functional queue (in the non-persistent case) you've got two stacks, A and B. To push, push onto A. To pop, pop from B if it's NonEmpty, otherwise pop all of the things in A into B and then pop from B. We deposit a credit whenever we push to A. We withdraw a credit whenever we pop from A. This way, popping from A is "free" so that the "slow" operations get a constant amortized time, since the operation was "paid for" by the extra savings we deposited every time we pushed to the queue. Since we can't pop more than we push, there's no problem.
The problem domains are quite different. Hobbes is aimed at low-latency analysis/algos, a typed take on KDB/Q. The Hobbes runtime uses LLVM for JIT compilation and relies on its host to reset the heap (it has no GC). Mu is used to script a C++ quant library (pricers, solvers etc), using the existing runtime and data structures provided by the quant library. Even ignoring the need for these custom runtimes, GHC Haskell has other problems supporting both cases. GHC is not very embeddable in its present form, e.g. the runtime cannot be resumed by a host if there is a critical error like heap exhaustion. Hobbes also relies upon native support of row polymorphism and extensible records, something GHC lacks. Mu probably should have made use of GHC code, but this would have been more work as GHC historically has not been very modular. 
!RemindMe
**Defaulted to one day.** I will be messaging you on [**2017-07-18 17:30:28 UTC**](http://www.wolframalpha.com/input/?i=2017-07-18 17:30:28 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/haskell/comments/6nr81l/type_safe_dimensional_analysis_in_haskell/dkcajsp) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/haskell/comments/6nr81l/type_safe_dimensional_analysis_in_haskell/dkcajsp]%0A%0ARemindMe! ) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! dkcakfi) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
[removed]
Here is a simplified example based on your code. I just return a list of `GameResult`. An `evalResult` function could then make the text output. module Main where import Control.Monad.State import Data.List import System.Random import Text.Printf data GameResult a = GameResult { prize :: a, playerChoice :: a, poorChoice ::a, secondChoice :: a } deriving (Eq, Show) main :: IO () main = do let doors = [1..3] -- You can change the total number of doors or their labels let m = 5 -- Number of game results to display let n = 1000000 -- Number of games to play g &lt;- newStdGen let results = playN n doors g putStrLn $ show $ take m $ results playN :: Eq a =&gt; Int -&gt; [a] -&gt; StdGen -&gt; [GameResult a] playN n doors g = take n $ rounds doors g rounds :: Eq a =&gt; [a] -&gt; StdGen -&gt; [GameResult a] rounds doors g = let (v, g') = runState (choices doors) g in v : rounds doors g' choices :: Eq a =&gt; [a] -&gt; State StdGen (GameResult a) choices ds = do car &lt;- choose ds pick &lt;- choose ds goat &lt;- choose (ds `excluding` [car, pick] ) repick &lt;- choose (ds `excluding` [pick, goat]) return $ GameResult car pick goat repick choose :: Eq a =&gt; [a] -&gt; State StdGen a choose xs = do g &lt;- get let (i, g') = randomR (0, length xs-1) g put g' return (xs!!i) excluding :: (Eq a) =&gt; [a] -&gt; [a] -&gt; [a] excluding xs ys = xs \\ ys -- filter (`notElem` ys) xs 
One question. Shouldn't the location of the prize and the poor choice happen before the user makes their choice?
I was going to as *why*, when there are readily available solutions available. Then I remembered how bad Magento (arguable the most popular one) is. Seriously, it is pure shit. Still, why? Is it really commercially feasible to build a custom web shop? Or is it just for fun?
There's a fair bit of custom functionality I want. I'd spend more time learning Shopify/Magento/etc. to get what I want than it would take for me to just make something that makes me happy. &gt;Is it really commercially feasible to build a custom web shop? Is it really commercially feasible to spend three years writing a book? Converting 2,000 hours into consulting money would've made a lot more money, but I cared about solving a problem. I work as a programmer partially so I can work on problems I care about, not just the ones with the highest dollar amount tagged onto them. I spend a lot of time doing things for free because it'll help Haskellers, let alone something like Boutique which would help me sell stuff.
A tiny suggestion: `Quantity` should have a `Monoid` superclass. I agree with others, though, that this approach (elegant though it is in some ways) isn't convenient enough to really be practical.
The problem here has to do with Arch packaging ghc for dynamic linking by default without really understanding the Haskell ecosystem.
But how do you decide, how much does one operation deposit / withdraw. Can it be any number or must it follow some rules? And how do you decide which operation is depositing and which withdrawing? What if I decide all my operations will deposit so I never go into debt?
Note that it exists `Rand` (and `RandT`) (in https://hackage.haskell.org/package/MonadRandom-0.1.3/docs/Control-Monad-Random.html ) which behaves exactly like this implementation, based on `State`, but which is a bit more convenient because you don't need to unpack the random state using `get`, instead you have direct access to some `random...` functions.
Fair. I was demonstrating the State since the request was more understanding of the monads.
Or check invariants?
As long as you never withdraw more than is in the account, the credits will be valid. That doesn't mean they'll be useful for the analysis. In terms of "how do you know when to charge them?", that's hard. A good start is to try putting credits onto values that will move through your data structure (for instance, in the queue, I attached a credit to each incoming item, and "spent" it when it moves from one queue to another). If you only deposit, your amortized operation costs will all be worse (or at least, not better) than their actual costs. So this will be valid (because the cumulative amortized cost is still an upper bound on the cumulative actual cost) but it will not be useful, because it won't tell you anything meaningful about the actual amortization that is possible.
Are you unironically trying to argue that OOP composes better than FP? That's just silly. I mean I know you are a known troll and I shouldn't feed you but come on man.
Thanks!
`while` loops are pretty boring if your code is pure. A pure `while` can't read or write any references, so it needs to know whether or not to loop based on some value in the returned expression. It also explicitly needs some seed value. So we might write: while :: (a -&gt; Bool) -&gt; a -&gt; (a -&gt; a) -&gt; a while continue value next = if continue value then while continue (next value) next else value We can have imperative-style `while`, but we need to have our IO stuff to make it work. whileIO :: IO Bool -&gt; IO () -&gt; IO () whileIO continueAction action = do continue &lt;- continueAction if continue then do action whileIO continueAction action else return () However, this kind of pattern is almost never used in real world Haskell code.
What are you going to sell on the site, out of interest?
Thanks, now it all makes sense! (At least for ephemeral data structures)
Books, I'd think. Maybe video tutorials too. Edit: https://lorepub.com/ has a mission statement on it, I'll work on and sell any content I think that'll help people work towards that goal. Doesn't have to just be my own content either, I've been talking to a couple programmers about being their publisher. I have some educational app-y ideas but I don't take them as seriously as educational content for now.
&gt; Discharging debits isn't really a thing that the operation does- it's a trick used in the analysis. According to my understanding there are two versions of banker's method. The second is modified for persistent data structures. The first one operates with savings (the one you described). The second one operates with debits. I fully understand the first one now. But I still don't get the second one... Say you have list [1..10] and the theoretical debits on elements are [10, 0, 0, 0, 0, 0, 0 ...]. * How do you discharge those 10 debits? * Can you reverse the list for amortized cost of 1?
What kinds of requirements does it pose on your project structure? E.g. decreasing in order of specificity: - Does it require `stack`? (Well, I'm sure it doesn't) - Does it require a `Cabal`-based build? - Does it also work reasonably for standalone files? - May we even hope for some smart convention-over-configuration approach, so that `make`-based (or `shake`-based) projects are supported? I dread the typical situation I run into when hacking on more complicated projects in Atom or vscode, where the `ghc-mod` plugin throws error after each save, becoming more of a nuisance than providing any benefit.
This matches my understanding of both projects as Intero's author. 
Right, Okasaki adapts the banker's method to work better on persistent data structures. The idea is not that different: Originally, we had savings, that we accumulate in our account. This time, we're going to have loans that we need to pay off. The rule changes: Wherever we create a suspended computation that will take T time to force, we create T debits that need to be paid off. (Money analogy: a loan of $T). The creation of this debt doesn't affect amortized time (so it's free to *create* more debt [the bank is very generous]). Once we have created a debt, we can pay off parts of it whenever we like. Paying off a debt *increases amortized time*. This increases the amortized time of the operation that performs the discharge. There's no rules as to when you do this. However, we introduce the fundamental restriction: **a suspension can only be forced if its entire debt has been paid off**. *However,* we *also* say that the unshared work of forcing a suspension is constant, *because we already paid for it.* This means that before running an expensive computation, we must have eliminated the debt that it will introduce *ahead of time* (or we have to pay for it right now, which is not common because you tend to lose amortization). Each suspension has its own debt which is paid off separately of all others. How and when you discharge these credits depends on what you're trying to do. It only makes sense to allocate debts when you create a suspension, so you have to decide when you're doing that in order to know when you're going to pay it off.
&gt; So we might write Note that strangely, `until` is in prelude ;)
I think you're barking up the `MonadBaseControl` tree?
It also uses ghc-mod at the bottom, so supports stack, cabal and standalone configurations. But it has the same limitations as the atom plugin based on ghc-mod, because of that. Providing some concrete setups on the issue tracker for any of these projects would help in sorting them out. Make and shake based support is likely to be a long way off, the next big step will be cabal new-build support.
So this is like `Representable1` ? :=)
Nevermind, I get the structure now.
I think dobkeratops mean something like this: let xs = ['a','b','c','d'] in xs 1 where `xs` is an array (or list, etc) but when treated like a function it behaves as one. In this case, `xs 1` would expand to `(xs !! 1)` so the result would be `'b'` in this case.
Thanks for your efforts! :)
Yeah, although maybe a little simpler. I never tried to implement MonadBaseControl besides what could be derived automatically.
Use `FromJSON` to a `Generic` type that only has the keys you want.
Continuations. Continuations, everywhere.
`Data.Array` specifically calls out that they are isomorphic to "functions whose domains are isomorphic to contiguous subsets of the integers." and that "Functions restricted in this way can be implemented efficiently; in particular, a programmer may reasonably expect rapid access to the components. To ensure the possibility of such an implementation, arrays are treated as data, not as general functions." You can partially apply / left section `!` to get the accessor you'd like to pass around. --- Also, while they are not part of the standard library, the vector package on hackage provides high-performance array-like objects, with a very similar `!` accessor. --- You would not want to partially apply `!!` and pass that around, at least not for performance-sensitive code. You'd end up repeatedly walking the head of the list many times.
Fwiw, the gap in complexity between the model and the real implementation starts to get pretty wide once you have web servers and databases involved. If you want some more details on how these models pan out, I highly recommend: * John Hughes - Testing the Hard Stuff and Staying Sane ([video](https://www.youtube.com/watch?v=zi0rHwfiX1Q)) * John Hughes - The Mysteries of Dropbox ([video](https://www.youtube.com/watch?v=H18vxq-VsCk))
Now someone run AFL on it and see what happens...
I'm pretty sure that your comment refers to how a code audio was done on OpenVPN, declared clean and then someone found 4 relatively important bugs using fuzzing. The difference between this case and the OpenVPN case is that in the OpenVPN case, it was a code review of C code, by humans (probably with the help of static analysis). I think that code review by itself is weaker than formal verification but think it would be highly valuable to fuzz WireGuard anyway and even more interesting if that fuzzing actually found any problems.
You are quite correct in what I am referring to. It may also be the case that what was verified were the cryptographic properties of the application, rather than its invulnerability to forced inconsistencies in state, data, metadata and memory. In what language is WireGuard written?
&gt; My main issue with vcache is that it was kind of a pain to set up and package with an exe because LMDB? I've never installed or even use that database. 
@ Simon M, provide some context about Tamarin, within the field of theorem provers and formal verification systems, if you get a moment.
`^.` is set up to start a field accessor chain using lens. With it `foo^.bar.baz.quux` should feel like `foo.bar.baz.quux` in an object oriented language. `(^.)` performs a trick to make the function composition `(.)` into the field accessor dot. `foo^?bar.baz.quux` will return the first thing found by a traversal or fold. It is similar to some of the automatic null collapsing stuff other languages have started exploring lately. This performs a similar trick to `(^.)` but gets you back a result wrapped in `Maybe` result. The (?) indicates generally that it returns an optional thing, kind of like the `(?)` in a regular expression. By contrast `(^..)` returns all of the results of a traversal or fold as a list rather than just the first in a Maybe and the `(..)` is intended to evoke the .. in the `Enum` sugar `[1..10]`. `(.:)` is some Aeson thing. =) `(&lt;&gt;)` is used as a "neutral"-looking symbol for the associative binary operator `mappend` in `Monoid` and `Semigroup`
&gt; Beware of bugs in the above code; I have only proved it correct, not tried it I think this is pretty important, a proof of correctness assumes that the assumptions made in making the proof are valid, and define all the properties you want it to. The proofs are still subject to human error in their formulation, but having a proof of important properties which has been machine verified is something which should build a lot of confidence in the reliability of the code. Would be really interested to see if AFL turned up anything. 
Wireguard is written in C. It was written to be as small as possible (about 4k LOC) to reduce the attack surface, where as OpenVPN is &gt; 100k LOC.
The first sentence reads &gt; The WireGuard protocol, described in the technical paper, and based on Noise, has been formally verified as opposed to any particular _implementation_ of that protocol.
Maaan, I would kill for a general Haskell based ecommerce system :O Having been involved with Magento several times, it certainly brings a lot to the table immediately, but at the cost of slooow sites for anything not cacheable, and a hell of a shitty developer experience when creating extensions and trying to debug it, with all it’s runtime errors (PHP after all). I can definitely see the lure of starting from scratch if you already know it’s gonna be customized a lot, and you want stability. 
Interestingly both pages you linked to make no mention of haskell whatsoever. 
That's actually a reasonable amount of code, as far as auditing goes. Pretty amazing, plenty of projects can't fit their socket handling code in 4kloc.
I am trying to remember without digging up Security Engineering; I think there were cases a decade ago (or a case maybe) where verified protocols weren't really fit to purpose, due to the challenges of stating all the guarantees you need formally or something.
Why is this in `UnliftIO` instead of somewhere under `Control.Monad.Unlift.IO` or similar?
&gt; Immutability requires you to be explicit about the effects a function can have. In turn, if everything is immutable, there are suddenly 'pure functions'. It's much easier to optimise code where some building blocks are guaranteed to evaluate to the same value, regardless of the order in which they are evaluated. So this allows lazy evalations to be possible, and also insures that a function evaluates the same every time
Don't use the stack package, use the binary from [here](https://www.stackage.org/stack) directly.
I'd be quite interested. I'll get in touch on Github tomorrow.
It all depends on what you're trying to prove about the system in question. Is it useful for your tool to model and report e.g. timing attacks? Or is it useful for a tool to model things and report stuff like "this protocol is forward secret"? *It depends on the problem you are approaching*. In some cases I want the former. In other cases I want the later. Tools like Tamarin and ProVerif operate under certain assumptions that allow you to model specific properties, like forward secrecy, for a given protocol you write down (normally called the "Dolev-Yao model"). By making certain assumptions about the model the tools can be proven to uphold certain properties. For example, ProVerif can give false negatives, but never false positives. So it might say "This protocol is vulnerable to attack X", but X cannot ever happen. But if it says "This protocol is safe from attack Y", then *it is always true and Y cannot happen*. In practice: this is totally acceptable for most of the protocols we would actually ever write down (like SSH). The domain of these tools is generally restricted. They can't prove "every security property holds", whatever that means. Certain attacks may not be included in the model. An example is timing attacks, which ProVerif or Tamarin ignore. But this is not their intended use anyway, of course. Does this result mean WireGuard is "secure"? No. The implementation could be wrong or deviate from the spec. Techniques like extracting or refining implementations of code from abstract specs is still a difficult area of research for many problems. And WireGuard may implement the model correctly, but be vulnerable elsewhere, or in ways the model does not (or cannot) account for. Tamarin may also be wrong itself, which is entirely possible. That said, I still think results like this are useful. One reason is that you *you should check your assumptions and these make it easy*. Systems like ProVerif/Tamarin have good power-to-weight ratios all things considered, and they can root out many design problems very quickly compared to other approaches like full blown Coq proofs. Many, if not most, useful properties are relatively easy and very practical to check. (Little point in designing a modern crypto protocol these days if it's not forward secret.) High level formal specs are also easier to execute, write, and evolve -- changes to WireGuard's protocol for example can first be explored in a tool like Tamarin to ensure none of the security properties get broken over time. And hopefully in the future, things like refinement or extraction tools will get us closer to verified implementations, too, but right now it's tricky.
Do you have a link to the definitely or Haddocks for that type class?
Please see the linked unliftio-core library
Do you have an example of automatic deriving for MonadBaseControl?
Reddit uses a different form of mention: /u/simonmar If he's got the checkbox checked, he''ll get a notification and can respond to your comment.
Ah, neat. I retract about half of my hasty and ill-informed comment. From a library design perspective I can see the point of having two packages, but what's your thinking behind not using that namespace for `unliftio`?
GeneralizedNewtypeDeriving.
You mean like this? {-# LANGUAGE FlexibleInstances #-} {-# LANGUAGE GeneralizedNewtypeDeriving #-} {-# LANGUAGE MultiParamTypeClasses #-} {-# LANGUAGE StandaloneDeriving #-} {-# LANGUAGE UndecidableInstances #-} import Control.Monad.Trans.Reader import Control.Monad.Trans.Control import Control.Monad.Base import Control.Monad.IO.Class newtype FooT env m a = FooT (ReaderT env m a) deriving (Functor, Applicative, Monad, MonadIO) deriving instance MonadBase b m =&gt; MonadBase b (FooT env m) deriving instance MonadBaseControl b m =&gt; MonadBaseControl b (FooT env m) 
A few people have been making the case that we should go for package name = main module name (or something like that). I don't remember all of the arguments and don't want to claim to represent that movement, but the points that stick out to me are: * No need to come up with a second name * No need to remember a second name (e.g., is it `Unlift.IO` or `IO.Unlift`?) * Shorter to type * Since package names are already namespaced on Hackage, theoretically avoids module name conflicts * Easier to read code, since you can quickly figure out which package an import comes from I'm not 100% sold on this approach, but the few times I've tried it I've liked it, and I haven't heard solid arguments against other than breaking tradition. On that front, I'm not planning on renaming existing libraries, but may continue using this for new libraries.
I don't *think* I had to use StandaloneDeriving, but it's been a few years.
What is the unshared cost of lazy operation: 0 or 1?
I disagree with the approach because it's one of the things I really liked about coming to Haskell from other language ecosystems: the global module namespace felt a lot more solid and cohesive, and you didn't have to remember the cutesy names of the different projects to access their functionality. (Fortunately, the hackage ecosystem seems to not have too much of that sort of thing.)
How did you deal with the inability to drive in the presence of associated types?
`tail'` is the same as `tail`, so I'm not sure what you're actually trying to get at. If you don't believe me, write out this module: {-# language BangPatterns #-} module Tail where import Prelude hiding (tail) undef :: a undef = undefined tail, tail' :: [a] -&gt; [a] tail (_ : xs) = xs tail _ = undef tail' (_ : (!xs)) = xs tail' _ = undef and compile it with `ghc -fforce-recomp -O -ddump-simpl Tail`. At the very end, you'll see `tail' = tail`. ### Edit If you're curious about why I define `undef` instead of just using `undefined`, that's because of a newish feature introduced in GHC 8: call stacks. `undefined` no longer has type undefined :: a but rather undefined :: HasCallStack =&gt; a This is helpful for call stack-based debugging, but it means that no two copies of `undefined` are the same: each is tagged with its own source location information. To get CSE to produce `tail' = tail`, I needed two *identical* bottom values. So I manufactured an `undef` value without a `CallStack` constraint.
Maybe I'm misremebering. I can't put my hands on any example code right now.
Yes, there is also a pitfall of interpretation in formal verification: _something_ has been proven true with a high level of confidence, but does the expository language surrounding the formal proof correspond to the theorem that was actually proved? And, if there are practical implications of said theorem, are those implications correctly articulated and understood? I ran into this issue when contemplating some machine checked proofs of Arrow's Theorem.
I definitely agree that the `Data.` and `Control.` prefixes are mainly noise.
Representable1 doesn't exist yet afaik but Representable does: https://hackage.haskell.org/package/adjunctions-4.3/docs/Data-Functor-Rep.html
Thanks. I think I can guess at what you're getting at with `Representable1`, but I'm glad we can avoid the associated type in `MonadUnliftIO`.
That is possible in GHC 8.2
I was trying to create strict version of tail...
Unordered tuples sound like `HList`s to me. There are several libraries implementing them -- just search and see which one seems most appropriate. 
Unfortunately, I think he's currently hired by McGraw-Hill.
That would fail silently if you rename a field of your record. Two desirable properties are: being able to have "serialized" names different from "internal" field names, and not silently breaking or changing the serialization when performing some refactoring (we should get a compilation error instead). The generics-sop solution is still a bit naive because it is sensitive to reorderings of the fields. But it could be improved using type-level metadata.
You are probably referring to the Needham-Schroeder protocol, which was "formally verified" under the Dolev-Yao model, but then a relatively simple MitM/replay attack was found later by Lowe (also using automated formal verification tools, as it happens). The good news is formal verification has moved on significantly from those days. One way is the computational power that is available these days allows us to work in the unbounded setting (i.e. no limits on how many sessions are initiated). Another is the tools have become more generic in their handling of the protocols. In particular, Tamarin is designed such that any possible combination of attacker/honest actions should be reachable, which gives us greater confidence that their results are right, which is especially important for proofs of correctness.
And how do you implement continuations? Either with a stack or some datastructure that grows every time you force a case scrutinee.
&gt; What advantages over Haskell's Types? HoTT makes it possible to write "higher inductive types" for things like the circle. For example, this gives you an "inductive" definition for the integers, not just the natural numbers, without anything funky happening at zero. &gt; Are there significant barriers (besides immaturity) to incorporating HoTT ideas into Haskell? Haskell's type checking at this time assumes "Axiom K". Axiom K is incompatible with univalence, the thing that makes Homotopy Type Theory work. Axiom K basically says that any two proofs of equality are trivially equal. It would be non-trivial to remove that assumption from GHC's type checker.
Wouldn't it be more like a HSet, if such a thing exists?
What exactly is the limitation imposed by ghc-mod? Or are all the errors mentioned by /u/sgraf812 (and also experienced by myself) more likely the result of poor integration with other tools? Or all these completely separate issues?
Not Marlow, Meier. :)
Start with Elm and switch to Haskell once you grow out of it. If you don't get it and you don't grow out of it, then Haskell probably isn't right for you. Being embarrassed by mathematical language is not a good sign. I'm usually curious and intrigued and at worst annoyed that it hasn't been translated into programming language. But *embarrassed*? Not ever.
Definitely, afaict they only verified the *protocol* not the implementation. That said, that’s still awesome and important! It’s just important to not leave it at that and also analyze the code (using formal verification, fuzzers, human code audits or preferably a combination thereof).
Now where's the progress report on a functional Haskell IDE? Or is the closest thing to that still just Leksah release notes? I don't see the benefit in building the engine before designing the UI. If all this amounts to is better support for existing text editors, then that just lengthens their lifespan. I'd much rather let them die so work can be redistributed to an actual IDE.
So *why is everything immutable* - because functional programming is (IMO) about pure functions and mutating a variable is a side-effect. *Why is pure a good idea?* - Because it's a way to achieve referential transparency - that means you can replace `f x` with with the value `y = f x` everywhere and it will not change the meaning/result of your program. This in turn enables you to _reason about your code_ and has all those shiny properties like making concurrent programming much easier. --- if you are just starting I would recommend forgetting technicalities like tail-recursion and if a value is just a function without arguments ... Just try to get used to functional thinking (everything is immutable and you are using pure functions as you main building block)
Do you mean that you would actually describe the integers as pi_1(S^1) ?
Oh yes! I’ve been annoyed by this quite a few times.
Interesting point! I think if a `Representable1` existed it would still not be enough because while it'd let you turn the monad into its explicit reader form (e.g. `Rep t -&gt; m a`) it does not give you a way to grab the current context (e.g. the current `Rep t`) -- which is quite crucial for what we need to do. Edit: for clarity, I believe /u/bennofs is hinting at something like -- Things isomorphic to @x -&gt; IO a@ class RepresentableIO m where type RepIO m :: * tabulateIO :: (Rep m -&gt; IO a) -&gt; m a indexIO :: m a -&gt; Rep m -&gt; IO a 
A perhaps [useful link](http://www.well-typed.com/blog/2016/01/efficient-queues/).
You can grab the current context: that's what `tabulateIO` is for: `tabulateIO return` for example.
I have no detail on the errors experienced so cannot comment. Non-traditional build systems (make, shake, etc) are not really in scope for something like ghc-mod, so should not be expected to work in general. With a clear statement of the problem consideration can be given, I guess, but as with all things PRs welcome.
Why's that? Doesn't sound like an `HList` at all to me.
Oh, these talks are really cool! I think they should be linked to from the documentation :)
This is great. Does anyone have any other Haskell livestreamers they recommend?
How/is this related to "cubical" type theory? Is cubical just a way of giving constructive (computable?) meaning to the equality?
I think you're right. What would be the properties of an unordered tuple? 1. Order is not important. 2. Membership and size are fixed from the beginning. The last point is what distinguishes, for example, `(Int, Int)` from `[Int]`. An `HList` over some fixed universe of types would fail because we can't contain the size nor the membership. By membership I mean, if your unordered tuple says (I'm inventing syntax now) `&lt;Int, Char, Int&gt;`, then `&lt;3, 3, 'z'&gt;` should be valid, but `&lt;'a', 'b', 9&gt;` shouldn't be. An `HList` obviously allows you to have any number of each type -- from 0 to infinity. I think an unordered tuple would be isomorphic to a fixed-sized list of homogeneous tuples. That is, `&lt;Int, Char, Int&gt;` is actually equivalent to `⟦(Int, Int), (Char)⟧` where we use `⟦ ⟧` to denote a fixed-length list, and `(Char)` for a singleton. Now, what's a fixed length list? We can simulate the fixed length with a(n anonymous) sum type, but we need it to be "commutative." I'm sure there's some easy way to create such a beast using `TypeFamilies` and other type-level goodies... 
Good to know, thanks!
[removed]
_slaps forehead_ of course! then they're definitely similar, although `RepresentableIO` might be more powerful. I'll look more into that, thanks a lot for the hint. i wonder how much you can prove about operations implemented with `tabulateIO`/`indexIO`, using their laws.
To elaborate a bit, this is what it would look like in full: {-# LANGUAGE RankNTypes #-} {-# LANGUAGE TypeFamilies #-} import Control.Monad.IO.Class class (MonadIO m) =&gt; RepresentableIO m where type RepIO m :: * tabulateIO :: (RepIO m -&gt; IO a) -&gt; m a indexIO :: m a -&gt; RepIO m -&gt; IO a newtype UnliftIO m = UnliftIO {unliftIO :: forall a. m a -&gt; IO a} askUnliftIO :: (RepresentableIO m) =&gt; m (UnliftIO m) askUnliftIO = do ctx &lt;- tabulateIO return return (UnliftIO (\m -&gt; indexIO m ctx)) to recover the laws we have for `MonadUnliftIO`, we'd like that indexIO return ctx = return indexIO (a &gt;&gt;= b) ctx = indexIO a ctx &gt;&gt;= indexIO b ctx tabulateIO return &gt;&gt;= \ctx -&gt; liftIO (indexIO m ctx) = m but i don't know we can prove that with the representable laws. /u/edwardkmett , opinions ? the one involving `liftIO` will need some ad-hoc rules, but maybe the monad transformer laws can be proved in a more general way. 
But `lower` is also used for a comonad operation that's symmetrical to `lift`.
/u/snoyberg pointed out that you can go the other way around too: {-# LANGUAGE RankNTypes #-} {-# LANGUAGE TypeFamilies #-} {-# LANGUAGE FlexibleInstances #-} {-# LANGUAGE UndecidableInstances #-} import Control.Monad.IO.Unlift class (MonadIO m) =&gt; RepresentableIO m where type RepIO m :: * tabulateIO :: (RepIO m -&gt; IO a) -&gt; m a indexIO :: m a -&gt; RepIO m -&gt; IO a instance (MonadIO m, MonadUnliftIO m) =&gt; RepresentableIO m where type RepIO m = UnliftIO m tabulateIO = withUnliftIO indexIO m (UnliftIO f) = f m so, they seem to be equivalent.
&gt; All of these iteratees, coalgebras, comonads, invariant/covariant functors, bifunctors, *morphisms etc. They aren't necessary to write Haskell effectively. I'm of the opinion that the only Haskell typeclasses you need to know your way around to be productive are functor, applicative, and monad, with the first one being both the simplest AND the most important. Later on, you can add monoid to the mix, but between those and everything else, there is huge gap in terms of how useful they are and how often they're used.
Besides Ed, I would look into this guy: https://github.com/lukexi who released a groundbreaking livecoding VR haskell app on steam and open sourced it: https://github.com/lukexi/rumpus. Also, there were some people at Chucklefish that were doing haskell graphics that I think have since moved on. They probably weren't doing 3D though.
/u/meiersi
Here's a related reply from Kevin Milner, one of the 2 authors of the WireGuard protocol proof: https://news.ycombinator.com/item?id=14791254
&gt; `&lt;Int, Char, Int&gt;` `&lt;A, B, A&gt;` (using your syntax) is isomorphic to `&lt;A, A&gt; x &lt;B&gt;`, and in general there's no point in having unordered heterogeneous tuples. All we care about is homogeneous unordered tuples of size n for some n &gt;= 0. 
What's been working for me (even for that specific package, I believe) is to install everything using stack, but for each ghc stack has installed, you need to apply [this fix](https://github.com/commercialhaskell/stack/issues/2712)
When is the paper version of haskellbook.com coming out? You guys've been a bit silent on this subject lately.
That's interesting - thanks for the insight! It makes sense that there may have been influences from the KDB direction, especially as MS were heavy users of it (at one point anyway). The lack of a type system (worthy of the name) in KDB was always going to be a limiting factor when scaling it out as an infrastructural component IMHO. It's a great result that when they looked for a way to introduce this, Haskell was the main influence. OCaml would probably have been a much easier route, but wouldn't have been half as impressive an achievement.
Can you show your entire code? From the error message, it looks like you're ending a `do` block with a `let`, which does not make sense in Haskell.
When you use let, that just creates a binding. An expression produces some sort of value, for example `1 + 1` versus `let x = 1 + 1`. A `do` block needs to end with an expression, because the do block needs to evaluate to some sort of value. So for example: do putStrLn "Hi." let x = 1 + 1 This won't work, because the last statement in the block just creates a binding, it doesn't produce a result. do let x = 1 + 1 putStrLn "Hi." This will work, since the last line is an expression, and putStrLn has the type `String -&gt; IO ()`. The `do` block will return a value of type `IO ()`. It seems like hackerrank runs your code as a program, which means you need to define the module and provide a `main` function. It seems like you should print out your solution. So something like: module Main where main :: IO () main = do -- Other stuff? let arr = map read $ words arr_temp :: [Int] print arr That will print out the array in Haskell format which may not be what they expect. This is just an example of the general approach you probably need. 
It's just a template. You can't actually end the definition of `main` with that `let` statement. You are expected to write more lines of code after it in the `do` block.
I would be interested in knowing about snippet/template plugins as well.
Indeed. Dan Licata and Michael Shulman have written up a [very nice paper](https://arxiv.org/pdf/1301.3443.pdf) on the construction.
The goal of cubical type theory is to give a type theory in which there is a constructive interpretation of the univalence axiom. The idea is to use the insights from models of type theory in cubical sets and transform them back into syntax.
One of the biggest differences is the difference in purpose of the two systems. Haskell's type system is a mechanism for enforcing certain properties of Haskell programs ("well-typed programs don't go wrong"). HoTT on the other hand is intended to serve as a foundational theory for mathematics. Behind this is the principle that types can represent propositions (e.g. "∀x. even x → even (x+2)") and inhabitants of such types are proofs. In Haskell, there are inhabitants of every type (⊥), so we can't claim that a program of a given type represents a proof. Contrast this with, for instance, Coq, where every program must be terminating and so inhabitants of types really do represent proofs. HoTT is fundamentally dependently typed (i.e. types can refer to data, such as "even : nat → Type"). Haskell is not, although [it has been extended with dependent types](https://www.cis.upenn.edu/~sweirich/papers/eisenberg-thesis.pdf). Dependent type theories have been around for a lot longer than HoTT, and there are certainly some benefits to using dependent types in Haskell. As a type theory, I think the particular advantage of HoTT is how it lets you reason about equality. (Here I mean equality as a proposition, rather than "(==) :: a → a → Bool".) This is really nifty for theorem proving (I can define and work with quotient types), but is not something that I really care about in my day-to-day Haskell programming.
While waiting for a production ready HIE, i have developed a plugin for VSCode based on intero. It supports many features like goto definition/find usage/stack targets/types info on hover/auto completion/etc. You can give it a try : https://marketplace.visualstudio.com/items?itemName=Vans.haskero 
Thanks to all you guys!
https://hackage.haskell.org/package/type-level-sets
i'd appreciate to see someone do a 2017 version of this; I'm currently using vs code with intero, but would like to go back to vim.. just haven't taken the time to figure out what plugins are available to get me a similar experience
Hey! Could you tell something more about the project ? I would be happy to help, though I admit I'm somewhat more on the junior side. 
The stream is me working on something meant to enable that. I have a concurrent effort going on but I wanted to reduce some of the risk by working on something in Haskell. Currently the book is ready to go out as a digital release candidate as soon as I get the go-ahead. Once the ecommerce/shipping integration is dealt with, we put in a print order.
&gt; Maaan, I would kill for a general Haskell based ecommerce system :O That's not what I'm trying to make, but I think a good way to start towards such a thing is a sort of template project. Just start by making something that works, ya know? I could eventually take what I'm doing in that direction but truthfully I don't care for "frameworks" that much, I'd rather split things out as libraries... Which _has_ been done: https://github.com/alexeyzab/ballast If anything else seems like a good candidate for splitting out, I'll do so.
There is some new Neovim plugin based on Intero, and it works well! There is a post about it currently in the mailing list. I'll post a link to it once I return home.
It be curious if they just made the code extremely dense, performing multiple tasks per line, to accomplish that, or if they were able to do it mostly by avoiding accumulation of useless cruft and duplication, etc. If It's the latter, that's awesome, but the former is arguably worse than accepting more LOC.
It's here \o/
Setting aside implementation feasibility, would HoTT Haskell be a useful improvement over dependent Haskell?
I would be interested to see real world system stats. TechEmpower is all over the place. I'm curious where there are still performance issues in the Yesod stack. I'm also interested in concurrent mainstream ORMs like Django/Persistent or ActiveRecord/Persistent. You should be able to have Persistent emit Django, or Django emit Persistent. 
Nix does exactly this.
Not to be pedantic, but here's what cloc picked up: github.com/AlDanial/cloc v 1.72 T=1.76 s (60.9 files/s, 15795.0 lines/s) -------------------------------------------------------------------------------- Language files blank comment code -------------------------------------------------------------------------------- C 39 1574 1091 10443 Assembly 9 341 737 7994 C/C++ Header 37 387 231 3141 make 5 77 3 400 Bourne Shell 7 49 58 306 Bourne Again Shell 2 31 4 282 Go 1 11 5 171 Haskell 3 50 6 170 Rust 1 13 1 123 YAML 2 5 0 37 Markdown 1 4 0 6 -------------------------------------------------------------------------------- SUM: 107 2542 2136 23073 -------------------------------------------------------------------------------- So, more like 13kloc of C and 8kloc of asm. Still almost an order of magnitude better than OpenVPN, so not bad really.
The code is not obfuscated or otherwise shortened. From a brief survey, it is commented moderately well and has readable variable names.
My code metric doesn't take into account crypto primitives. OpenVPN's of course doesn't either, since it uses OpenSSL's libcrypto.so. And likewise all of WireGuard's crypto primitives will be part of the kernel's crypto API, so it'll be the same situation. If you want to actually measure the size of the real WireGuard code (which _does_ include the Noise protocol and plenty of other trixy crypto bits), run `make cloc`: github.com/AlDanial/cloc v 1.72 T=0.02 s (1214.1 files/s, 207345.6 lines/s) ------------------------------------------------------------------------------- Language files blank comment code ------------------------------------------------------------------------------- C 14 542 182 3200 C/C++ Header 14 145 119 594 ------------------------------------------------------------------------------- SUM: 28 687 301 3794 -------------------------------------------------------------------------------
Fair enough, that is what I get as well.
As I understand it (and am happy to hear corrections) Tamarin proves properties of a model of a protocol using a first order logic and assuming a symbolic (Dolev-Yao?) adversary. It is important to keep in mind this doesn't say anything about: * implementation correctness * how closely the model and actual protocol agree * any weaknesses exploitable by a computational adversary and not a symbolic one This isn't to say such a model and proofs aren't valuable - they can catch their share of broken protocols - but you sometimes see people asking questions about orthogonal vulnerabilities such as "What if a fuzzer finds a vulnerability? What does that mean about the value of this proof?". Such questions reveal a failure to communicate the benefits of protocol verification and how these tools are complementary.
My plugin [intero-neovim](https://github.com/parsonsmatt/intero-neovim) has recently received some great support and attention, and is now working really well for me. Includes type-at-point (and type-of-visual-selection) with both the maximally generic type and also the specific instantiated type, go to definition, highlight uses, and a Neomake maker.
I personally use `hdevtools`, `hlint`, `syntastic`, and `vim-hdevtools`.
The standard ones all work about the same. I use ultisnips with honza's vim-snippets.
Does it only work with stack and neovim?
There are example build scripts in the [xmonad-testing](https://github.com/xmonad/xmonad-testing) repo, including one for `new-build`.
The Simons!! (: I had not idea what Simon's Reddit handle was.
I wouldn't be surprised, given that intero works only with stack.
[Relevant](http://gelisam.blogspot.com/2013/07/the-commutative-monad.html)
For those moments when you don't want to open a book, but you want a few minutes of thinking about Haskell, try something like HackerRank warm-up exercises, rather than passive reading. Anything where you are typing Haskell. Note: It would be nice if the Haskell book authors agreed to and placed a few very short exercises from their book onto HackerRank, as a very brief way to get minimal Haskell exposure and motivated to work through the book.
I'm happily using https://github.com/begriffs/haskell-vim-now . It works well with both `vim` and `neovim`.
&gt; We're generally hiring onsite, however several of us work across Europe and we welcome remote applications within suitable timezones What is a suitable timezone in this context?
Ah, I don't know their reddit name, if they have one.
Thank you for the pointer to that excellent resource.
It says that it starts on September 18?
HoTT would have better support for quotient types - but it could be added as an extension independently. I'm more interested to see how HoTT solves linearity which _would_ be a dramatic improvement
It uses neovim-specific features. I've seen some vim/neovim plugins, and I wouldn't mind a PR to add vim support.
What's the relationship between HoTT and linearity? I know practically nothing about any of this, but it would be awfully nice to be able to work with quotient types cleanly.
&gt; Setting aside implementation feasibility Just to pick on this a tiny bit. Implementation feasibility seems to suggest that a theory is very fleshed out, and just needs someone to sit down and "encode it into the language." However, with very cutting edge theories such as HoTT, Cubical type theory, and other theories you hear floating around on type theory nerd blogs, there's a bit of a problem; that problem is that we don't really know the "correct" way to think about certain things. Equality is an *incredibly* tricky subject because it has an obvious human intuitive meaning that has been very tricky to nail down formally. Other certain things are also very tricky (like commutative properties) because, well, it's all somewhat related to equality in an abstract sense. There are deep philosophical questions and questions regarding the actual structure of how HoTT or other theories even work and whether or not such theories are even "good" or "practical" for programming. How do you reconcile an infinite hierarchy of universes with the human desire to want to write code that works based on relationships between universes at a relative level? Something might be working over Set1 and then working over Set3 and then over Set2 all in the same module and it'll be obvious to a human that the code "should be correct" but how do you construct a formal theory to really capture that in a sane way? What about boilerplate? Having to annotate everything defeats the purpose of having an elegant and 'simpler' theory. In reality, it seems that as research progresses in type theories, we're going to discover better ways of thinking about certain things and I imagine there will be some sort of 'breakthrough' in how to think about equality and other tricky concepts that will allow us to encode the theory into a programming language in a sane way. Until that happens, we're stuck with our attempts using things such as Coq, Lean, or Agda. Additionally, I find it enlightening to remember that "when we think of a language as expressive, we mean it's easy to use; when we think of a type system as expressive, we mean that it's not." The more powerful a type system, the more difficult it tends to be to use; figuring out how to get more power without increasing programmer complexity is one of the challenges that programming language such as Haskell will have to deal with. As it stands right now, I think "full HoTT" is a terrible idea to use in Haskell for that reason. (tiny disclaimer: This is all based on my current understanding of things; a grain of salt is suggested to aid with digestion)
I strongly suggest swapping syntastic for ALE (link in OP) Quality of life improves dramatically, and there's basically no configuration needed
Hi there, Good question - some overlap with our office hours in London would be ideal, as an estimate I'd say around +/- 6hrs. We've tried longer before, up to 11 hours difference, but felt we weren't really ready and set up to handle that.
Why not data Func = Func (forall a. a -&gt; a) Maybe Func
Wonderful! This is exactly what I was hoping to see. Though it will take some time for me to digest, it's clear that I'm missing a really useful and elegant way of thinking about things. 
Well, your version isn't any stricter. ``x `seq` x`` is always the same as `x`. You could, as a better example, write a version of `head` that forces the tail: head' (x : !_) = x Such a function might even be somewhat useful, although partial functions really should be avoided.
Intero for Emacs only works with stack. Intero the executable is just GHCi with extras, so what build system the vim mode assumes is up to the author.
Maybe transient-universe can give you some base upon which to build your ideas: https://github.com/transient-haskell/transient-universe 
Use [spacemacs](http://spacemacs.org/) with [dante](https://github.com/jyp/dante). As a pretty sophisticated vim user for almost 20 years, I would say that Spacemacs is pretty much unambiguously better than vim. Spacemacs has everything I want from vim with a bunch of nice improvements added on top (space as the leader key, better discoverability with mnemonics, etc), all while being built on the vastly better platform that is emacs. I still use vim for one-off file editing, but I simply can't recommend it any more for Haskell development.
Couldn't you just do: g :: (forall a. Maybe (a -&gt; a)) -&gt; b -&gt; b g (Just id') b = id' b g Nothing b = b Since as the other commenters have established, `-XImpredicativeTypes` is unsupported and does not work.
I wouldn't. It's easy enough to define them inductively: data Nat = N | S Nat data Int = NN Nat | NS Nat negate :: Int -&gt; Int negate (NN Z) = NN Z negate (NN (S n)) = NS n negate (NS n) = NN (S n) You may have to deal with 4 cases sometimes, but mostly it's just the zero case, the positive case, and the negative case. 
That's a better way of doing it. I don't know what I was thinking.
Full disclosure. Your code uses a LOT less memory. My code uses around 100mb. Yours uses 1mb. So my simple code has room for optimization. I intended to strip it bare and show the thinking.
The course says 4 hours per week for six weeks. I wonder if this is enough to achieve the goals? When working with *Haskell from First principles* it took me 5-10 hours to get through the first 200 pages, and there are 800 more to go. 
I also like that a qualified import looks more like import qualified UnliftIO than import qualified Control.Monad.UnliftIO as UnliftIO
I've tried to mimic the emacs mode as much as possible. Requiring `stack` and interfacing the plugin through `stack` seems like a pretty great idea, since the vast majority of my issues with ghc-mod and hdevtools are that they're not compatible with different GHC versions typically.
Is dante integrated with spacemacs in the Haskell layer? I quitted Spacemacs some months ago, but I was thinking about coming back.
Just to reinforce, `vector` is a de-facto standard of Haskell. And as an opinion, using `(!)` is superior since there is no ambiguity in its use.
That'll be lectures, but not time to do exercises I expect. 
I just use vim 8 with some syntax highlighting plugins. Any time I've tried to do anything fancier, I have something that sort of works but breaks depending on the project I'm working on. For what it's worth, I work on some projects using stack, some using cabal new-build, and some using nix + cabal.
Do you think having a certificate for a course like this on your CV can help during a job hunt? (Same goes for other online certificates, like Coursera.org certificates.) I'm guessing that it depends on the employer.
Different people have different learning styles. Haskell was *so* different, that I read 1/3 of LYAH and took a break to let my brain digest the info. After a few weeks, I read 2/3 of LYAH starting from the beginning. Finally, the whole 3/3. I like to take my time.
I'm skeptical. At this point, companies hiring Haskell programmers (at least if they are paying competitive market rates) still have the luxury of setting a very high bar. Having passed this course would demonstrate that you meet a much lower bar. So you should completely take the course if you think you would learn from it; but expect the benefit to be the learning, not the paperwork.
First word of the title is misspelled on the actual post.
You don't need something as complicated as HoTT to have better-behaved quotient types. [Observational equality](https://dl.acm.org/citation.cfm?id=1292608) (which has proof-irrelevant equalities just like we're used to in GHC) is more than enough.
..but the first objection - that GHC can't be resumed couldn't be an issue for Morgan Stanley if Hobbes has no GC. Surely GHC can be hacked so as to remove its GC.
Study the [`MonadRandom` library](https://hackage.haskell.org/package/MonadRandom), both the API and the source code. Some exercises you can try: 1. Rewrite your program in terms of the library's `MonadRandom` class. Experiment with how to refactor your code to make best use of the class. 2. Write your own, simple version of the `State` monad, and use it to write a simplified version of the `MonadRandom` library's `Rand` monad. Don't worry at all about feature-completeness here, just do enough to get to a point where the library no longer feels magical. 3. Work out how to use the `RandT` monad transformer in the `MonadRandom` library to write a program that mixes random choice with IO. 
I did this course, and while it had its benefits, overall I wouldn't recommend it. I found its coverage of most topics to be very superficial, the complexity of assignments wildly erratic, and the lectures slow and boring. On the bright side, I like studying topics from a variety of angles, perspectives, and media, and as a supplement to something like Haskell Programming from First Principles, you could do worse. 
Ha, that just cuts out Singapore at 7hrs :-)
Dante doesn't have hoogle lookup. The hoogle lookup in haskell-mode is not good either, since it only gives function definition and not description in comparison to vim-hoogle. Do you know of any solution?
Yes, it is in the latest version on the `develop` branch. All you need is this: (haskell :variables haskell-completion-backend 'dante)
`dante-info` is good though and gives the description. Hoogle with descriptions would be awesome. There is also hoogle server `h-H` you can launch from emacs and get a full description. If you added it as an option to standard emacs hoogle `h-h` that would be amazing.
https://twitter.com/kmett/status/883020199817072641
&gt; `cabal haddock --haddock-option="--hyperlinked-source"` is different from `cabal haddock --hyperlink-source`. Who thought this would be a good idea?
Thanks for bringing up the tension between types and composability. And I think this is an important reason why Haskell is still uniquely relevant in a world where we have Agda, Idris and Coq. I think the way you're describing the shape of the solution, you're unnecessarily narrowing down the search space. First of all, IMHO, in such an abstract discussion (where we reason about all software) we shouldn't be talking about monads or effects, since these are concepts with specific sets of constraints, which won't apply to all software components. The second point is trickier, I think. By giving up a little bit of convenience, we may gain A LOT of wiggle room; You say you want an operation `&gt;&gt;!=!` with signature A(effs,states) a-&gt; (a -&gt; (A(effs',states') b) -&gt; A(effs+effs',states+states´) b And then you want people to write components that have the type `A(effs,states) a`, so that you can compose components `a` and `b` as `a &gt;&gt;!=! b`, but if you were willing to write `someKindOfUnlifting (someKindOfLifting a &lt;someOperator&gt; someOtherKindOfLifting b)`, you'd have infinitely more alternatives. So, maybe we need an army of such lifting and unlifting functions as well as compatible composition operators. The difference between what you want and what I suggest is mere syntactic inconvenience, but in the big picture, all that matters is the architecture. Just as a side note, when we say a "monad", we think of the signature `m a -&gt; (a -&gt; m b) -&gt; m b`, but if any `Applicative` has an array of operators with type `f (f a) -&gt; f a`, then it's essentially monadic, and the user can chose, at every step, in which way it should chain.
If it makes the analysis simpler, in most cases you could say it's zero, by amortizing (in the vague sense) with other operations; but if you're forcing k thunks, that's going to take O(k) time. You can imagine that it's the same cost as reading an lvalue, I suppose. 
And what does `cabal haddock --hyperlink-source --haddock-option="--hyperlinked-source"` produce?
Previously haddock didn't know how to highlight source code and this task was handled by HsColour. Recent versions of haddock gained the ability to highlight source code. This functionality is enabled by passing a flag to haddock.
That said, it would probably be nice to make the latter do what the former now does, no?
Slightly off-topic: Do I have to somehow enable the Neomake integration? When I have some errors in my code, they are only showed in the GHCi terminal. Also, when I query the type of an expression, it is only showed in the GHCi (and not in the :command line of nvim). That means if teh terminal is hidden, I can't see the type. 
FWIW haskell-ide-engine does have hoogle lookup support, but I am not sure if it is worked through into the emacs-lsp client yet. And it plays nice with spacemacs/haskell-mode.
Good tip! Unfortunately the highlighted sources are buggy if they contain template haskell splices. Something like `` makeLenses ''MyRecord `` causes everything after (and including) the `''` to be considered a string. This was with haddock 2.17.3. I will investigate and file a bug report.
Are you sure it's not `cabal haddock --haddock-option="--haddock-option=\"--hyperlinked-source\""` ?
ghc as equality prover, I see what you've done. I bet you were expecting identical definitions rather than `tail' = tail` :P
Good point! :) I think that could still work if they would be willing to shift their hours a bit to increase the overlap - but I understand that's not always possible. Ultimately we're pretty flexible, so if you're interested or would like more info please do get in touch and we'll see how it goes! :)
I get this weird syntax error: $ stack setup error: attribute ‘ghc82020170704’ missing, at (string):1:43 (use ‘--show-trace’ to show detailed location information)
This is because the implementation doesn't use GHC's lexer. 
Nice! FWIW, this is definitely law breaking, as the `Tree` constructor let's you observe associativity =P Also, I'd be interested in seeing a more comprehensive benchmark. That test seemed a little small scoped. That tree structure looks like its performance could collapse at larger scales since it's not balanced at all. Also, did you benchmark this under 8.2? I'd be curious to know how join points affects stuff like this. The tree structure could be faster to traverse due to join points. I think the Reflection Without Remorse structure that freer-effects uses (or maybe just the way it uses it) may be less amenable to this, but I'm not sure.
I think much better benchmarks are needed to substantiate this claim. Please can you provide the code for your current benchmarks so that we can reproduce the results? 
Sorry I forgot to add a link in the article. This is the code I used: https://github.com/fumieval/extensible/blob/3d9030397ab39ec539b30ffd73804784386b94bb/benchmarks/eff-comparison.hs
&gt; since it's not balanced at all FWIW, monad-skeleton does optimistic balancing (same as freer), and it should scale. I tried replacing `replicateM_` with this left-associative replicateM_: badreplicateM_ :: Monad m =&gt; Int -&gt; m () -&gt; m () badreplicateM_ 0 _ = return () badreplicateM_ n m = badreplicateM_ (n - 1) m &gt;&gt; m The result w/100 times was benchmarking rws/extensible time 13.00 μs (12.87 μs .. 13.12 μs) 0.999 R² (0.999 R² .. 0.999 R²) mean 12.95 μs (12.84 μs .. 13.08 μs) std dev 406.1 ns (335.1 ns .. 479.5 ns) variance introduced by outliers: 36% (moderately inflated) benchmarking rws/freer-effects time 23.10 μs (22.82 μs .. 23.37 μs) 0.999 R² (0.998 R² .. 0.999 R²) mean 23.32 μs (23.04 μs .. 23.67 μs) std dev 1.050 μs (808.7 ns .. 1.385 μs) variance introduced by outliers: 52% (severely inflated) benchmarking rws/mtl-RWS time 1.543 μs (1.527 μs .. 1.560 μs) 0.999 R² (0.998 R² .. 0.999 R²) mean 1.548 μs (1.529 μs .. 1.571 μs) std dev 67.99 ns (53.95 ns .. 90.31 ns) variance introduced by outliers: 59% (severely inflated) and with 1000 times: benchmarking rws/extensible time 142.3 μs (138.2 μs .. 149.3 μs) 0.976 R² (0.955 R² .. 0.994 R²) mean 152.4 μs (147.4 μs .. 161.4 μs) std dev 21.23 μs (14.21 μs .. 32.40 μs) variance introduced by outliers: 89% (severely inflated) benchmarking rws/freer-effects time 313.2 μs (309.2 μs .. 317.7 μs) 0.999 R² (0.998 R² .. 0.999 R²) mean 314.1 μs (311.1 μs .. 317.2 μs) std dev 10.28 μs (8.577 μs .. 13.13 μs) variance introduced by outliers: 27% (moderately inflated) benchmarking rws/mtl-RWS time 26.32 μs (25.80 μs .. 26.87 μs) 0.997 R² (0.996 R² .. 0.999 R²) mean 26.09 μs (25.77 μs .. 26.43 μs) std dev 1.110 μs (929.5 ns .. 1.349 μs) variance introduced by outliers: 49% (moderately inflated) I haven't done benchmarks on 8.2, I need to fix other libraries first.
Ok it was NixOS-specific issue: https://github.com/commercialhaskell/stack/issues/1787
&gt; FWIW, monad-skeleton does optimistic balancing (same as freer), and it should scale. My bad! At that point, I struggle to see the difference between the two implementations. Is it really just the empty case? I'm surprised that's such a huge difference. I wonder if something else is going on...
In case anyone missed it, in episode 13 of the Haskell Cast John discusses this effort, very good episode
Seems reasonable. Is there any reason to keep around the legacy option? Does it have any benefits?
I've tried spacemacs three or four times and just can't get used to it. Which is odd because I happily use worse vim emulators for things like intellij. I think that is because I do want to use spacemacs as a better vim but there is enough impedance mismatch to make things annoying. The terminal emulator requires you to be on the correct line, for instance, so just switching to the buffer and entering insert mode ignores all key presses. I probably could get around all these little qol issues by learning enough emacs but I haven't seen any haskell killer feature I couldn't get with vs code. That is generally is much less opinionated and integrates nicely with vim plugins and in the intermediate future it might be able to use neovim as backend.
Note that `whileIO` can just be `whileM :: Monad m =&gt; m Bool -&gt; m a -&gt; m [a]` which works on any monad and actually keeps intermediate results. Take a look at [Control.Monad.Loops](https://hackage.haskell.org/package/monad-loops-0.4.3/docs/Control-Monad-Loops.html).
Is there any theoretical reason a sufficiently smart compiler couldn't optimise an extensible effects framework to something as fast as the MTL implementation? Or is it something that could be done without too much difficulty if the compiler had some kind of built-in support for extensible effects?
And which one is `stack haddock --haddock-hyperlink-source`?
I was expecting what I got. See [Joachim Breitner's blog post](http://www.joachim-breitner.de/blog/717-Why_prove_programs_equivalent_when_your_compiler_can_do_that_for_you_.html.en). BTW, there's a [follow-up post](http://www.joachim-breitner.de/blog/720-ghc-proofs_rules_more_now).
I have the same experience: each time I tried some Haskell plugin with vi, it broke after some month. Now I prefer to use a setup than works for programming in any language: http://www.corentindupont.info/blog/posts/Programming/2016-01-05-vim-tmux.html 
By the way, I remember that in Windows, using Visual C++ and the standard Win32 library, you can create a fake memory-mapped file, which only exists in memory, and has a unique ID (if you try to create a file with the same ID, then Windows will quietly give you the file that already exists). Then you can write and read to the "file" and essentially share a memory region with other processes, including treating it as a mutex. I wonder if such a trick is possible in Linux, whether Haskell exposes it, and whether it's as terrible an idea as it sounds...
Why not finesse the whole question by logging to a SQLite database? Then you can replay and reinterpret the logs any way you like.
I already have a dedicated thread per process sending logs to a Redis queue, and another process reading the queue and persisting the logs (log stash, files, DB, whatever) from various processes/machines. Here I just wanna see the logs while on a machine, and not have them garbled. They are already being persisted. :-)
By the way, are you assuming that your locking is not working, or that it is working and the three output streams are nonetheless getting mixed up?
I have no idea. If the locking were working, I'm assuming the output would not be as is. I'm also assuming that `hFlush stdout` will block the process (at least) until all output has been displayed. If the second assumption is true, then the locking is not working the way it should be. Of course, I might also just have a wonky algorithm...
Yeah, that's also where I learned about this :)
I will highly recommend this course for people beginning to learn Typed FP. I went through its first edition - I didn't complete the entire course, but it was enough to get me started and I've been writing pure Typed FP (OCaml not Haskell) in production for a few months now. The teachers are active in the forums, and the learners are supportive and helpful. It was my first MOOC and I was really happy with the sense of shared learning and accomplishment.
This seems to me to be a perfectly possible interleaving of your system * A acquires lock * A prints "A acquires lock" * A releases lock * B acquires lock * B prints "B acquires lock" * A prints "A releases lock" * B prints "B releases lock" * A flushes stdout (but there was nothing in it) * B flushes stdout (but there was nothing in it) This seems to be what you're getting at with &gt; The code above had a race condition where a released lock could be acquired by another process before the current process wrote the `LOCK RELEASED!` line. Unless you can release the lock and print "released" atomically then I think you're not going to get the results that you want.
I don't know the answer but I can provide a link to [some discussion](https://www.reddit.com/r/agda/comments/5d1hmf/guaranteeing_reduction_in_reifying_free/).
I'm pretty sure optimizing the free Monad away is a super-compilation problem, meaning it is currently unsolved.
Reminds me of the [concurrent-output](https://hackage.haskell.org/package/concurrent-output) package…
The Neomake maker works when you run `InteroReload` -- since it's not a process that neomake can call, the maker has to juggle a bit out of band of Neomake's flow. Querying the type of expression should definitely happen in the command line. Can you post an issue with your `init.vim` and maybe try minimizing the test case?
Will do! Thanks. Great work with the extension!
Yeah. That's correct. I have noted the issue in an edit. My actual use case is still failing, though. But in that case I also have multithreading. I'm investigating if that might be the cause of the problem...
[removed]
Just released this feature with version 0.3, great idea :D
You can maybe ask the the dev in the issue tracker?
https://github.com/colah/ImplicitCAD - maybe this person too
It'd be different. Assuming the foundational issues could be resolved, you'd get to talk about interesting notions of equality and higher-inductive types, but GADTs and pattern matching (which get used a lot!) would become weaker. 
You'd at least have the option.
That works for this simple one-dimensional case. It gets a heck of a lot messier (to the point of impossibility) when you have more equalities involved in your higher inductive type.
it's fixed in haddock shipping with GHC-8.2 *EDIT:* https://github.com/haskell/haddock/pull/512
GHC doesn't know CPP syntax...
Note that the "traditional", partial solution to this issue on Linux is relying on the fact that a single write(2) call of less than a page is atomic. Haskell String output functions call write once per character.
Right. Seems a little excessive though :)
Is there a reason why you use a shared memory file over semaphores? As you seem to not actually write to it that could be easier. https://hackage.haskell.org/package/unix-2.7.2.1/docs/System-Posix-Semaphore.html It seems to have a pretty nice interface for Haskell as well.
Ah! Which might explain why my Google searches for "Linux synchronise stdout between processes" and such didn't help me much! It's not a real problem in C! Makes sense. 
Yeah. Reason is that I somehow never encountered that module! I *am* simulating a semaphore, anyway. I didn't realise the `unix` library could give me the real thing. The interface looks perfect. `semWait` appears to solve both the process *and* thread synchronisation problems. Thanks! 
Its benefit is that it produces smaller files. I doubt anyone actually thinks that's more important than getting the nice hyperlinks.
If you're on Linux or similar, throw all that crap away and just use syslog. In syslog.conf you can set up a log file for your set of applications, syslogd will handle log messages and keep them from splitting, and there are a host of preexisting utilities for rotating and storing the logs.
Have you used neomake by any chance? Is ALE better than it and how?
You have the go-ahead to release the final digital version.
Totally agree. I just wouldn't do it for the Integers. General quotienting and interestingly "shaped" types are much better with language support for HIT.
thank you for your work! I am currently using Haskero and it's working great ;)
I have, and ALE is way better. Neomake is basically Syntastic with async on save so it doesn't block the editor. ALE makes async first-class, you don't even need to save to get lint messages, it gives them as you type, and puts them in the command line so you don't need to mess with the location/quick list to see the message.
All the options for hoogle ( either does not give you the description, or it launches in the browser. vim-hoogle have local lookups with descriptions in the editor window..
How do you get spacemacs/haskell-mode to use haskell-ide-engine?
Thank you so much for sharing your notes.
Thank you very much for writing this!
I just spent roughly the last two months writing this library, feel free to AMA.
I think it's similar to things like list/stream fusion in a sense. The representation you want to compile to depends on how the `Effect` is actually used. If reflection (in the sense of reflection-without-remorse) is used enough (for some value of "enough"), then a heavy representation is justified. Otherwise a simpler one (`Codensity` or whatever) will be faster. Unfortunately, that information isn't generally available locally.
I'm not sure about the technicalities involved and whether this could be easily solved, but another barrier could be the fact that Univalence seems fundamentally incompatible with Haskell's heavily nominal type system. Univalence would force isomorphic types to be considered equal, in a language like Haskell where making isomorphic types look different in the eyes of the type checker is a central paradigm.
Right. I agree what vim-hoogle gives is desirable and unique. Just trying to be helpful.
Some more detailed instructions for the noobs like myself and assuming Linux. NB: the sections on build scripts are not necessary for what follows. In fact it was so much easier than I thought it needed to be, that I was very confused for awhile. 1. create some directory on your system. 2. In that directory git clone https://github.com/xmonad/xmonad-testing 3. assuming that you created xmonad-testing as a sub-directory when you did the cloning, cd ./xmonad-testing and you will see empty subdirectories for each of xmonad, xmonad-contrib, and x11. cd into each one those in turn and repeat the cloning (but don't add any more subdirectories by mistake). For example, `cd ./xmonad-testing/x11; git clone https://github.com/xmonad/x11 . `Note that last dot. Repeat for xmonad and xmonad-contrib. 4. cd into the x11 director and run autoreconf 5. cd back into .../xmonad-testing and run cabal new-build 6. When I did this it failed due to the config.hs file. Following suggestions on the xmonad-testing readme I copied the ./xmonad-contrib/Xmonad/Configs/Example.hs into xmonad-testing as config.hs. and re-ran cabal new-build. 7. The executable lives very deep inside xmonad-testing/dist-newstyle/build ... just keep tabbing and you will eventually find it. 8. Then copy or link to that executable in place of your old xmonad or put it in .xinitrc or an .xsession file depending on how you are starting your X session. Then resume being confused, because it is is probably a very different configuration than what you were using. This required some fits and starts. If my memory is faulty I hope the more experienced redditors will feel free to edit, comment or fix these instructions for the benefit of us less experienced users. 
Wow! This looks great!
Very generous of you to share. Thanks.
Cool. Next level: avoid partial functions (i.e. `error`) :)
I've only been using Haskel in anger for about six months but I find that code pretty opaque. My inexpert attempt would be something like: import Data.List ( inits , tails ) import Data.Set ( isSubsetOf ) import qualified Data.Set as Set solve :: String -&gt; Int solve s = minimum . map length . filter valid . substrings $ s where valid = (`containsAll` distinctChars) distinctChars = Set.fromList s substrings = concatMap (tail . inits) . tails containsAll = flip isSubsetOf . Set.fromList I'm guessing your code probably has better time / space characteristics but I'm not sure how to go about figuring that out.
Notes? This is a book! Maybe you should contact Bartosz to collab on a book lmfao. Good job, I watch that whole series and it's amazing, so will review this to refresh. Thanks!
This is one of the best haddocked packages on hackage. 
Could we ever expect to see this kind of approach available in a futre DependentHaskell? It'd be incredibly amazing to be able to specify ByteString dependently and at high level and recover the performance optimized lowlevel code!
This is fantastic! 
Looking forward to the Haskell Almanac. When will it be available?
Shake already has a Ninja parser/interface built in (I think): what do you make of it, as prior art?
I based the Ninja parser in this on that parser. I hope `shake` replaces its Ninja parser with a dependency on this library.
If the largest Haskell team in the world can handle SG/London (40+ developers), it might not be doable :-)
Thank you! I'll get back to work on it after the print version of Haskell Book is available. I don't think it'll take nearly as long though.
I'm certainly interested to hear your point. Let's do that over beverages on the 2nd of Aug :-)
Sounds like a plan! :-)
Given the dependency footprint of your library I don’t see that happening tbh. Neil has always said that he wants to keep it relatively light but now that Shake is going to be used to build GHC that has become a necessity.
That's quite simple :P Replace `error "Shouldn't"` with `length inp` however I'm not sure whether that's actually useful in this case..
Your solution is interesting, but I find it semi difficult to grok. Here's my "simpler" but probably not so great solution: import Data.List uniqueChars :: String -&gt; String uniqueChars = sort . nub solve :: String -&gt; Int solve st = go st where allChars = uniqueChars st go [] = 0 go s = let fs = drop 1 s ls = drop 1 (reverse s) in if uniqueChars fs == allChars then if uniqueChars ls == allChars then min (go fs) (go ls) else go fs else if uniqueChars ls == allChars then go ls else length s 
You can follow the instructions for [lsp-haskell](https://github.com/alanz/lsp-haskell/blob/master/README.md), the emacs LSP configuration for [lsp-mode](https://github.com/emacs-lsp/lsp-mode) This is still early stage, and dev has paused while the upstream lsp-mode dev works on a separate GSOC project.
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [alanz/lsp-haskell/.../**README.md** (master → 0d30c2a)](https://github.com/alanz/lsp-haskell/blob/0d30c2a481feb86084918c8d3c0ca88e23d5328c/README.md) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dkgqz26.)^.
`drop 1 x == tail x`, `drop 1 (reverse s) ~ init s -- (for the purpose of the algorithm)` Your algorithm is `O(n^2)` and follows the same approach as that of @lgastako. Mine is `O(n*x)` where `x` is the number of distinct characters in the string. 
I might split Shake into a core, and separately a Shake-ninja executable, then the dependency wouldn't be a big deal. Splitting out the Ninja stuff makes a lot of sense. Being able to point people at this library is also pretty handy. However, the Ninja parsing is both a bottleneck and highly optimised - Chrome spits out a huge amount of Ninja files. Are there any benchmarks to my original version?
The Ninja feature of Shake is completely orthogonal to its use in GHC, so he could solve that problem with conditional compilation. Neil's feelings about dependency footprints are another matter, however. As a Nix user, perhaps I am a bit too liberal about depending on things :-)
Your welome, and excellent!
There is a [test](https://github.com/awakesecurity/language-ninja/blob/master/tests/Tests/ReferenceLexer.hs) that ensures that the lexer is still the same as a modified version of your lexer, but I don't have a benchmark. Currently the lexer is just using `megaparsec`, but since the parser can take a list of lexemes as input, I suppose I could just expose your fast version in the main library (instead of keeping it in the tests).
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [awakesecurity/language-ninja/.../**ReferenceLexer.hs** (master → 8c60207)](https://github.com/awakesecurity/language-ninja/blob/8c60207cc6202faae8136ec8d4f20974d6a50b0e/tests/Tests/ReferenceLexer.hs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dkgrfnx.)^.
Good call on `tail` and `init`. And yes, I was aware of it being slower, but I wrote my code with the assumption that speed didn't matter much. However it's definitely interesting to try to optimize for speed and I'll spend some more time investigating your approach. Thanks for the follow-up!
`res = Nothing:zipWith g res (zip [0..] inp)` makes me nervous. Are you sure that doesn't introduce a quadratic term? You're zipWithing a list with the result of the zipWith.
Thanks :)
GHC is very conservative about depending on things. I convinced them on Shake because it depends on very little, and almost zero they didn't already depend on - and even then that's only convinced for the build of GHC itself.
First let's see just how fast it is... Certainly don't assume my one is faster just because I hand optimised it. The Chrome .ninja files are a good benchmark, if you have them.
Agreed. I'll take a look at doing that tomorrow.
What I wanted to show here are some core numbers: - 1% use Haskell. The number of people who plan to adopt Haskell is healthy at 3%. - 67% write web back-ends - 64% write web front-ends - 50% are doing both In mobile development: - 84% are developing for Android - 58% are developing for iOS Main take-aways for me is the mobile story: - A good front-end development story would increase impact (GHCJS production ready, or a polished ARM+Android setup) - Haskell has an enormous opportunity in the full-stack area if the mobile setup gets some polish. 
Good idea! I'll add them
Awesome! I will switch as soon as possible.
That's just two linear traversals, and probably only one traversal due to fusion.
EDIT: 15 upvotes, really? Don't functional programmers care about performance at all? That has quadratic complexity, and is very inefficient even for not so big strings (see my benchmark ~~in my answer to /u/ChrisPenner~~) [here](https://gist.github.com/kuribas/a7a46f9fd71fbac1f45ca16a5ee68372) ([results](http://kuribas.hcoop.net/results.html)). I haven't even included it for long strings because it took to long... This is clearly worse that OPs solution, which works rather well. It can be done in O(n log(k)) (k being the number of distinct characters) , which is basicly O(n) (since log(k) is small and constant for any given alfabet) : import qualified Data.Map as M import Data.List import qualified Data.Set as S solve :: Ord a =&gt; [a] -&gt; Int solve [] = 0 solve s = getMin $ foldl' solve1 (M.empty, maxBound, s, 0) s where getMin (_, mn, _, _) = mn maxD = S.size $ S.fromList s shorten :: (Ord a) =&gt; M.Map a Int -&gt; [a] -&gt; Int -&gt; (M.Map a Int, [a], Int) shorten mp (c:cs) len = case M.lookup c mp of Just n | n &gt; 1 -&gt; shorten (M.adjust (subtract 1) c mp) cs (len-1) _ -&gt; (mp, c:cs, len) shorten _ _ _ = error "unreachable" solve1 (mp, mn, init_, len) c = (mp2, mn2, init2, len2) where (mp2, init2, len2) = shorten (M.insertWith (+) c 1 mp) init_ (len+1) mn2 | M.size mp2 == maxD = min mn len2 | otherwise = mn EDIT: (recursive case for shorten)
Your solution constructs ALL possible subsets of the given string and validates them because of the use of minimum. If you create the subsets ordered by length you can take advantage of laziness stop evaluation when you find the first valid subset instead.
I'm sure this approach, connecting things closely w/ Haskell and maths examples, will be helpful to many, as it has been to me.
I made an O(n) solution assuming O(1) hashtables, but it's imperative and I'm too tired to translate it to Haskell. It keeps track of the start and end positions and moves them forward step by step. function solve(str) { var set = {}; var total = 0; for (var i = 0; i &lt; str.length; i++) { var c = str.charAt(i); if (!set[c]) { set[c] = true; total++; } } var start = 0; var end = 0; var count = 0; var bag = {}; var range = [0, str.length]; while (end &lt; str.length) { while (start &lt; end) { var c = str.charAt(start); if (bag[c] &gt; 1) { start++; bag[c]--; } else { break; } } if ((count == total) &amp;&amp; (range[1] - range[0] &gt; end - start)) { range = [start, end]; } var c = str.charAt(end); if (bag[c]) { bag[c]++; } else { count++; bag[c] = 1; } end++; } return range; } alert(solve("aaabbabccc")); Edit: this is similar to /u/kuribas's solution, but with much fewer allocations.
That's the price to pay for immutability... You can try using mutable hashmaps in the ST monad: https://hackage.haskell.org/package/impure-containers
That's really nice but (of course) much more can be added. That is difficult for a single person to do so, as there is a lot underneath. you might want to team up with Bartosz of course, but not only. There are a few flavors of category theory, which is just a basic (not trivial) tool, on top of which building things (and where haskell is still lacking a few pieces) It would be nice to have a reference place where haskell + category gets discussed in a concrete format, that really fill a useful place. This can also help to illustrate why we really need extensions like Typeintype and UndecidableSuperclasses for CT, and other features we need
Is S Data.Set?
It sounds to me like you might want to do some reading about why functional programming is worth doing at all, when it seems to be defined by what you can't do. I recommend googling "why functional programming matters".
Yes. 
Yes.
Do it with a slightly french accent.
I've written up some stuff about [learning type theory](http://github.com/jozefg/learn-tt). The principle take-away is, while it's not focused exclusively on type theories, practical foundations of programming languages is a good place to start. I recommend aggressively skimming the first few chapters because they're quite opaque and difficult to grasp if you don't have examples in mind (the rest of the book). In general I would suggest that if you're a programmer, then "programming language theory" in general may prove more useful than pure type theory; the former will provide you with more tools for systematically analyzing a language, the latter is the more specific discipline focused on using types to study math/computation in general. Most type theory literature assumes a grounding in normal programming languages theory anyways.
Soon, you too will be numb to the dumb Coq jokes.
You mean significantly smaller files? Or files that are a few characters smaller because literally only the hyperlinks are left out?
And perhaps, given the domination of Java, the importance of [ETA](http://eta-lang.org/), GHC ported to the JVM.
[removed]
And it does mean the same thing. Well, the literal meaning, aka adult male chicken, not the raunchy meaning.
Yep
What are some practical applications of this? (sorry for the dumb question!)
If you're using POSIX, you don't need to use any locks at all if your writes are &lt;4k (PIPE_BUF). The reason is that POSIX guarantees that a file descriptor opened in O_APPEND mode will see atomic writes for writes less than some given size (for writes to a pipe). For typical console logging output, this is all you need. To be fully POSIX compliant, you just pipe your stdout/stderr output to `dd` or `cat`, and then to file. But you don't need to do that on Linux. What this really means is that you just have to scrap all the higher level buffers - they are the ones causing problems.
What bothers me is that I continually need to pull the Coq mailing list out of my spam folder.
A short answer is "anything a SAT or SMT solver is good for". How about we also ask: "What is SBV currently used for?" * [Cryptol](http://www.cryptol.net) is build on SBV * [SAW](https://saw.galois.com) uses SBV for some solvers * [Copilot](https://github.com/Copilot-Language/Copilot) has an SBV backend * I've written a couple custom solvers on top of SBV. One encoded information leaked from order revealing encryption as constraints then generated satisfying models which I could compare to ground truth for an empirical study on the schemes' security. 
I was going to say TAPL + Software Foundations, but this is a much more complete list.
I second /u/jozefg's link, but i feel like I am obligated to plug my advisor's [textbook](http://pl.cs.jhu.edu/pl/book/dist/). It is meant to go along with [this](http://pl.cs.jhu.edu/pl/dateline.shtml) course; although it is taught in OCaml, it's certainly possible to follow along in Haskell, as long as you are mindful of the differences. It's how I learned PL theory.
I think generally when the isomorphism is witnessable, the two types only exist separately due to language limitations, like the strict ordering of type parameters, or the form of an instance head, etc. In those cases, it wouldn't actually be bad to have the types treated interchangably. In the case of opaque types and their matching transparent type, the isomorphism most generally isn't witnessable, so you can't apply the univalence axiom.
&gt; drop 1 x == tail x Not quite. `drop 1 [] = []`; `tail []` is an error / ⊥.
&gt; It would sure be nice to have fmap :: (a ⊸ b) -&gt; f a ⊸ f b (note the regular arrow), letting us transform all values in the functor with a single linear function. But unfortunately that would not allows us to use fmap on linear monads, because it will have an unrestricted (nonlinear) type! Would it work if the Monad class looked like this? class LMonad m where (=&lt;&lt;) :: (a -o m b) -&gt; m a -o m b I don't know linear types that well, but I think that would translate to `(&gt;&gt;=) :: m a -o (a -o m b) -&gt; m b`.
While this is a great overview of what SBV is good for, I think the "this" in the question was "incremental solving", not "SAT solving".
Good thinking! This was one of my iterations actually, but unfortunately it screws some stuff up since a linear variable won't be allowed to be passed as that (unrestricted) argument. For example, it makes the linearity checker nag on `&gt;&gt;` because the `k` is passed to an unrestricted context: • Couldn't match expected weight ‘1’ of variable ‘k’ with actual weight ‘ω’ • In an equation for ‘&gt;&gt;’: m &gt;&gt; k = m &gt;&gt;= \ () -&gt; k | 23 | m &gt;&gt; k = m &gt;&gt;= \() -&gt; k | ^ the obvious solution to this is to change _that_ type to (&gt;&gt;) :: m () ⊸ m a -&gt; m a which unfortunately opens up for some naughty behaviour. :) 
In addition to what Tom said: Anthony Cowley's talk is good at giving some motivation: https://vimeo.com/77164337 (The whole talk is worth watching, but the first 20 minutes talks about using SBV for establishing correctness in Haskell programs.) He also has a set of slides: http://acowley.github.io/NYHUG/FunctionalRoboticist.pdf Tikhon Jelvis also has a nice set of slides that's worth reading through: http://jelv.is/talks/analyzing-programs-with-smt.html It's also good for a nice pastime: You can do lots of puzzles etc. with it and looking through those might give you a nice lightweight introduction. If you go to https://hackage.haskell.org/package/sbv you'll see a lot of Examples baked right into the package itself for easy browsing the source code.
Good point! Incremental solving is good for optimization, for instance. You can assert constraints, get a model, and inspect the results and "improve" them programmatically by asserting new constraints based on those results until you are satisfied with the final model. Comes in very handy for cases when there's no global minima/maxima, or the global one is extremely hard to compute. Another application of incremental solving is CEGAR (counter-example guided abstraction refinement), where models can be incrementally inspected and used for further refinement for more complex verification scenarios. The new query mode also allows for SBV to return regular Haskell values from symbolic computations, which is a lot easier to use compared to what you had to do in the previous "batch" mode.
Thanks for posting this, I submitted a ticket to Cabal's issue tracker: https://github.com/haskell/cabal/issues/4613
O(n)?? How about 'n' instead! (Assuming O(1) hash lookups) Here's a fun one, it relies on laziness to get the answer in a single pass! Notice that we use the 'allCharMap' in the expression which computes itself. Since we only use 'allCharMap' to compute the left half of the result tuple, and 'allCharMap' is computed as only a result of the right half of the tuple the laziness figures it all out and we can use the map of all characters before it's been computed! You can write this sort of thing using `Data.Function (fix)` if you like, but I find this way more readable. minSpanningSubstr :: String -&gt; Int minSpanningSubstr s = let (ans, allCharMap) = foldl (go allCharMap) (Nothing, M.empty) (zip s [0..]) in fromMaybe (length s) ans where go allCharMap (shortestSoFar, acc) (currentChar, currentIndex) = let updatedCharMap = M.insert currentChar currentIndex acc seenAllChars = M.size updatedCharMap == M.size allCharMap earliestInd = minimum $ M.elems updatedCharMap diff = (currentIndex - earliestInd) + 1 smallest = if seenAllChars then case shortestSoFar of Just soFar -&gt; Just $ min diff soFar Nothing -&gt; Nothing else Just diff in (smallest, updatedCharMap) If you're interested in this sort of programming, check out the time-travelling [Tardis Monad](https://www.google.ca/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwjPrJrQmZjVAhUD0YMKHdJLDJkQFggoMAA&amp;url=https%3A%2F%2Fhackage.haskell.org%2Fpackage%2Ftardis%2Fdocs%2FControl-Monad-Tardis.html&amp;usg=AFQjCNFEqKqkh_UHUMERBE811SjsoDcmsw)!
I can't wait until Hadrian is merged in and we can ditch `make`. GHC's build is long enough that any improvement and avoiding extra work is important. It's not as bad as Qt's build system which even takes ages for a simple `make clean`. This won't happen, but it would be cool if making use of Shake was simple enough that more projects could use it without complaining about GHC and Shake lib dependencies when `cmake` + `ninja` pulls in very little in dependencies.
Interesting, I hadn't considered the use of SAT solvers for optimization before. This is especially relevant to a hobby project of mine just now. So now I have a feature request/inquiry. Since there is already `SInteger` it seems SAT solvers can handle integers. How difficult do you think it would be to have an `SFixed`? The base package's [`Fixed`](https://hackage.haskell.org/package/base-4.9.1.0/docs/Data-Fixed.html) is a type for fixed-point numbers. They're basically just an `Integer`, except that they are interpreted as being implicitly divided by some constant determined by a phantom type parameter. (Hence a few of the arithmetic operations have to do a tiny bit more work, notably `(*)` does a multiplication and a division on the representation.) Right now I am doing a gradient ascent optimization, but if I could get SAT to do the optimization for me in a more principled way, that might be an interesting thing to look into.
Hey, first of all nice work. I do wonder why the examples in C++ and python end so quickly during the chapters, it was the same with Bartosz' posts. Are the more abstract constructions not as interesting in C++/python as they are in haskell?
Would you please create a version with sans-serif font? For me personally the serif font is difficult to read. Or can someone teach me how to do it myself ?
Bit early to post it to the wider programming world, but I figure Haskellers are okay with something a bit rough around the edges. Plus this language is a bit weird. It's not turing complete, it just provides standard folds, filters and maps. It's statically typed as far as functions are concerned, but the other type supported is `JSON`, and functions throw errors if you try to map over a number, for example. So it's a bit dynamically typed, too. From that view, you could say dynamic typing is just a way of saying you have a lot of partial functions. Anyway, I never remember jq syntax whereas I'm already trained for Haskell's.
How does `monad-squeleton` compared to the `operational` package ?
I haven't read all yet but it is great. Thank you very much!
No list of books on type theory can be complete without Pierce's brilliant _Types and Programming Languages_. As /u/jozefg mentions, PFPL is very good for what it is (and certainly worth reading) but it is not a book about type theory per se.
I made a _sans-serif_ branch here: https://github.com/jwbuurlage/category-theory-programmers/tree/sans-serif, the pdf is in the `doc` directory.
I tried to turn your GitHub links into [permanent links](https://help.github.com/articles/getting-permanent-links-to-files/) ([press **"y"**](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) to do this yourself): * [jwbuurlage/category-theory-programmers/.../**6eba5a85160a042b5d86e4fc3a70390d1d4cf8c1** (sans-serif → 6eba5a8)](https://github.com/jwbuurlage/category-theory-programmers/tree/6eba5a85160a042b5d86e4fc3a70390d1d4cf8c1) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dkhd1u8.)^.
For the past 584 days, this has been my favorite response to this question: https://news.ycombinator.com/item?id=10728995
I write mostly C++, so I really wanted to try and give examples in other languages too. In the end, it turned out to be either simply impossible or prohibitively difficult (or ugly) to do. If you want to do some reading on this: * Eric Niebler has some excellent blog posts on functional programming in C++, the most exotic one is on F-algebras. http://ericniebler.com/2013/07/16/f-algebras-and-c/ * Jonathan Boccara has recently been talking about functors and monads in C++ for some simple examples, in particular `std::optional` (`Maybe`) and `std::vector` (`Vector`) in his blog http://www.fluentcpp.com/2017/07/14/the-vector-monad-in-c-without-the-ugly-stuff/ I think many other languages (and their communities) are in a very different place compared to Haskell. A recent blog post that sparked some discussion is http://jackieokay.com/2017/01/26/functors.html, where Jackie Kay talks about the (arguable) misuse of the word 'functor' in C++. Currently, to a C++ developer, a functor is any object that overloads `operator()`. That is to say, it is not only difficult to _technically_ apply these concepts in other languages, it also does not help that there is already a well established (and deeply anchored) way of doing things in these languages that prevent (either technically, for legacy reasons, or because of the respective communities) functional constructs from being 'first class citizens' in these languages.
Creating `SFixed` should be absolutely doable. In fact, [SBV's SReal type](https://hackage.haskell.org/package/sbv-7.0/docs/Data-SBV.html#t:SReal) is a good example of how to wrap an existing type to make it symbolic. I'd be happy to collaborate if you want to go ahead with that. Regarding optimization: I'd recommend first experimenting with `SInteger`and `SReal`. SBV has out-of-the box support for optimizing objectives for both of these types. [Here](https://hackage.haskell.org/package/sbv-7.0/docs/Data-SBV-Examples-Optimization-LinearOpt.html) is an example using `SReal`, and [here](https://hackage.haskell.org/package/sbv-7.0/docs/Data-SBV-Examples-Optimization-VM.html) is an example using `SInteger`. Those should give you a good idea regarding whether your particular problem is good for global optimization; and they should be really easy to use. Based on your findings, you can either use the `Query` mode to do something fancier or go for the `SFixed` route. (Note that optimization can only be done over `SReal`, `SInteger` and `SBool` types, but if you come up with your own `SFixed`type, you can code up your own optimizer for it using the query mode and the underlying representation using the integers, most likely.)
Nice work! I came as far as looking at RebindableSyntax when I was looking for a way to restrict Monads in a way that prevents duplication of Values. Cool to see that and how one can make that work with RebindableSyntax and linear types.
It looks nice. Thank for your effort.
Thanks for the links. I was actually thinking in a somewhat different direction though. Your links seem to mostly concern itself with a very Haskell-like mindset of explicitly representing the math completely inside the language and things end up looking very bulky and unwieldy because the languages are not made for that. But I think we could still find certain principles from category theory in standard C++/python/.. code. Eg. having `None` in python is probably monadic in a way (it is still different than `Maybe` because there is no nesting of `— + 1` functors), even without explicitly encoding monads in the language. Thinking about it more it probably is fairly difficult to recognize, much harder than in Haskell where a lot of the stuff is (or can be made) fairly visibly present.
Does it keep formatting and key order?
This is great! I use jq quite a lot, but find myself reading the manual a lot, having forgotten its syntax. I expect I'll remember this much more easily.
Let's try... $ jl 'id' &lt;(echo '{"bar": 1, "foo": 2}') {"foo":2,"bar":1} nope. I would have been surprised if it did!
It uses Aeson which discards that info.
Legend has it it was invented by English speakers, who wanted to call it "Bit", much to the chagrin of French speakers. So they settled.
I'm glad to see `inf-haskell-mode` is getting some love, as it is my current standard repl for haskell in emacs.
JSON -&gt; JSON -&gt; JSON -&gt; JSON -&gt; JSON -&gt; JSON -&gt; JSON -&gt; JSON -&gt; JSON -&gt; JSON -&gt; JSON -&gt; JSON -&gt; JSON -&gt; JSON ^^mushroom^mushroom
Thanks! Right? It's neat indeed! May I ask what your use case was? 
I don't think type theory is what you're looking for. Type theory (and programming language theory) are mostly interesting from the perspective of a language designer or implementer. If you're just looking to upgrade your Haskell skills, then focusing on specific libraries and techniques will be faster. With that said, here's my type theory track: - [Type Theory and Formal Proof](https://www.amazon.com/Type-Theory-Formal-Proof-Introduction/dp/110703650X) is a fantastic introduction to the lambda calculus, starting with the simple untyped lambda calculus and exploring each of it's additions. It's very friendly to novices, and includes a guide to Greek letters and an introduction to sequent notation (the weird horizontal bar logical notation). Ultimately, it develops the Calculus of Constructions with Definitions which can be used to prove things using constructivist logic. - [Types and Programming Languages](https://www.amazon.com/Types-Programming-Languages-MIT-Press/dp/0262162091/ref=la_B001IR3BUA_1_1?s=books&amp;ie=UTF8&amp;qid=1500577437&amp;sr=1-1) is a good read after that. It also starts with the untyped lambda calculus, and develops extensions on top of it. You'll implement a type checker and interpreter for the simply typed lambda calculus, and you'll add subtyping, record types, type inference, objects (!!!), parametric polymorphism, and existential types.
A JSON is just a JSON in the JSON of JSONs, what's the problem?
A JSON is just a (JSON in the JSON of JSONs) in the (JSON in the JSON of JSONs) of (JSONs in the JSON of JSONs), what's the p^r^o^b^l^e^m^?
It is a starting point for much more interesting structures. Consider the comparative difficulty of moving up to talking about spheres? I offered it up as an example because it is literally the simplest interesting example: A space with a single hole in it is the first non-trivial case.
My use case was more of an thought experiment resulting from a Thread on reddit [about a year ago](https://www.reddit.com/r/haskell/comments/4vt8mf/why_do_we_need_mutable_objects/d62cqso/). The question I pondered was what programming model would be required if you want to enable a hypothetical evaluation model that recycles (by mutating them) objects behind the scene instead of creating new ones. So instead of having to rebuild the whole spine of a data structure when replacing a subpart you just replace the part that has actually changed instead. In the end Fall Semester started and I kinda forgot about the whole thing but as far as I can tell Linear types are pretty much what you would need if you want to be able to enforce that optimization. I don't think a System like that would fit into GHC well since it violates the assumption that old objects can't point to newer ones that the GC depends on for the most part. Also I'm not sure if such a System would pay off performance wise outside of edgecases. It would make updating deeply nested structures (e.g. appending to the end of a list) more efficient but would I assume make GC more expensive and might cause other issues I haven't thought about. Maybe I will reevaluate this for my Master or a Toy compiler at some point though.
Yeah, I knew my code constructed all solutions, which is why I figured whatever his code was doing he probably had better time characteristics, it's more the inscrutability (to me) of what his code does that makes it problematic to figure out how it's better. I'm going to try to spend some time after work today teasing it apart and understanding it.
[removed]
Don't be afraid American, do it with passion.
Oooo, `SReal` looks very tasty.
Sure, I understand :) building algebraic topology objects is nice, but using it for integers seemed crazy to me. 
This is incredible, thank you so much for your work! I'm hoping to get into category theory this summer in preparation for my master's, and this is definitely going on top of my (overly long) reading list!
[Relevant.](https://shlevy.tumblr.com/post/148221765470/in-line-with-the-tradition-laid-down-by-the)
how does this compare to jq?
This is why I don't understand the desire of every Haskell-alike to have gratuitously different syntax. If there are new/different features, sure. But for the same features use the same syntax! Also, this article provides a good reinforcing argument for why you should *never* have things like a "backend" team or a "frontend" team.
I'm not worried about `zipWith _ _ (zip _ _)`. I'm worried about the fact that this is *defining* `res`, and also applying `zipWith` to `res`. I think that builds a big old triangle of calculations. As for fusion, GHC versions 7.10 and later only fuse `zipWith` with their *first* list argument. Older versions try for either, but that's not actually a valid rewrite, and the whole thing was rather fragile anyway. So the `zipWith` will not fuse with the `zip`. The recursive stuff going on will prevent `zipWith` from fusing with itself either, so no fusion. But the lack of fusion is really not what I'm worried about anyway.
I'd recommend "How to prove it" by Velleman alongside the other books. It'll take you from basic maths to having enough maths for TAPL, and it'll help with a lot of other books as well. Beware the introduction (or maybe it's the first chapter), which discusses proving things about primes etc to show where it is all going - the book is much gentler than that section would have you believe. The first proper chapter starts with English sentences as propositions, then introduces Booleans and truth tables. This is the book I would send to myself in high school if I had access to a time machine (with a sticky note on the front telling me to learn Haskell as well). 
Also discussed in [this](https://www.reddit.com/r/haskell/comments/6nr9ya/announcing_the_new_unliftio_library/) reddit post.
 λ&gt; compare "jl" "jq" LT (lexicographically speaking, I'm not commenting on form or function :))
Alright. I've been meaning to make a fork of Aeson which keeps that info. At work reformatting json and discarding order is a huge nuisance to others.
&gt; If there are new/different features, sure. But for the same features use the same syntax! We have different deriving syntax to distinguish things like stock deriving from newtype deriving, something which GHC has recently supported as well.
`(maybe (error "Shouldnt") (+1) . getMin . mconcat . map Min $ allAnswers) == minimum answers + 1` Here's my attempt: data S = S { _rightestpositions :: S.Set Int -- gives access to leftest required position , _rightestposofchar :: M.Map Char Int -- allows removal of old required positions } makeLenses ''S solve inp = foldl' (flip ($)) undefined $ (`evalState` S S.empty M.empty) $ for (zip [0..] inp) $ \(i,c) -&gt; do rightestpositions %= S.insert i rightestposofchar . at c &lt;&lt;.= Just i &gt;&gt;= \case -- discard previous solutions - they haven't seen all chars Nothing -&gt; uses rightestpositions $ const . ((i+1)-) . minimum Just j -&gt; do rightestpositions %= S.delete j uses rightestpositions $ min . ((i+1)-) . minimum
The mini tutorial https://github.com/chrisdone/jl#mini-tutorial is more or less a direct translation of the jq tutorial https://stedolan.github.io/jq/tutorial/ jl is more like writing one liners in GHCi, jq is a tacit language with differing syntax and variable and looping constructs and no types that I know of. jq has a few more functions (date parser, regexes and string interpolation) but which are trivial to add in a few hours in Haskell (probably took some time in C). I'd like to add regex syntax as &lt;code&gt;/foo/&lt;/code&gt; as a matcher and &lt;code&gt;s/foo/bar&lt;/code&gt; as a function from string to string, stuff like that.
Glad you like! I took inspiration from PureScript for the _.foo syntax :) 
I should note here: Don't use `--ghc-option=-optl-lm`, that won't work. You need to use `extra-libraries: m` if it complains about missing functions from `libm`, like `exp` or `tan` etc. Explanation [here](https://github.com/NixOS/nixpkgs/pull/24692#issuecomment-316856374).
But doesn't each `minimum $ M.elems updatedCharMap` take O(k)?
Ahh, yup good catch! Could use a sort of heap that allows updating. The important bit is that you can see to the future to know how many distinct chars there are.
A colleague of mine has been working on a JSON library, argonaut, which does this (as well as preserving white space so you get the property `fmap encode . decode == Right . id` [roughly]). The name is no coincidence, as he also made the Scala one. Hopefully he'll see this and comment. 
Now this is a shitpost style I can get behind 
Nice work, I always found jq really painful or confusing, and it make it really difficult to do anything must the most basic querying. With just the addition of lambda so I think I'll find this much more useful
Buffalo buffalo buffalo Buffalo buffalo etc.
Z3 has a really cool solver for reals, even in the presence of nonlinear equations. Also, optimization with reals can properly deal with epsilon/infinity values, which is also quite impressive. See here: http://www.cl.cam.ac.uk/~gp351/infinitesimals.pdf
&gt; Linear types are pretty much what you would need if you want to be able to enforce that optimization. I'm pretty sure you need [uniqueness types](http://edsko.net/2017/01/08/linearity-in-haskell/#uniqueness-versus-linearity), since linear types don't guarantee anything about aliasing (someone else could have a reference to a value that's linear to you, in which case it would not be safe to mutate). IIRC [Mercury](https://en.wikipedia.org/wiki/Mercury_(programming_language)) has [uniqueness types](https://www.mercurylang.org/information/doc-release/mercury_ref/Unique-modes.html#Unique-modes) and uses that for destructive updates in a referentially transparent way.
I tend to pronounce it somewhere between "caulk" and "coke". But my pronunciation is incorrect.
I've played around with some of the examples of SBV, and I love the technology - but when I work on my own examples I seem to end up in dead ends / with longer run times than I was expecting. Is that something that folks tend to get past with experience / by continuing to tinker, or are there things that I should be reading to give me a better feel for how to approach problems with SBV? I think I've seen a few references in various snippets / usages to "XYZ is decidable" or "Z3 has a decision procedure for XYZ", which has made me think that there's some meta-knowledge out there that might be helpful when approaching problems with Z3.
Sometimes the article says withRunInIO, and sometimes it says withRunIO, and i can't tell if this is intended or just an editing error.
I welcome the innovation! Haskell doesn't necessarily get everything right. Trivial example: Elm using `:` for type declarations was the right way to go IMO. (Sorry, I realise I am opening the door of the bike shed)
8.0.2a not fixed yet, tried to install yesterday... W10 Creators as well.
I nearly did my master's thesis on writing a Coq editor plugin, just so I could title the thing "Coq-au-vim".
Elm follows the MLs, there.
The name was chosen by Gérard Huet specifically to annoy the English speaking. I finally managed to get him to admit it. :)
and type theorists
Also worth mentioning that the generated purescript types come with appropriate lenses and prisms. This actually probably less exciting now that you can get these from generic..
and everyone else in all of math
Good point!
Thanks! I plan to get this on Hackage within the week (and Stackage), and eventually get back to that web app that inspired this distraction. I'm still very open to breaking changes (names, etc.) at this point as it's not officially released anywhere. If you think of any improvements don't hesitate to share.
&gt; IIRC Mercury) has uniqueness types and uses that for destructive updates in a referentially transparent way. I think you mean that [Clean](https://en.wikipedia.org/wiki/Clean_(programming_language)) has uniqueness types. I can find no mention on uniqueness typing on the Mercury wkipedia entry. 
head $ sort ["jq","jl"] "jl" 
Sure. On the other hand, it _does_ have the nice property of putting the integers on the same footing as the Peano naturals.
It can also be pronounced as "the French Theorem Prover", but probably better to just pronounce it as you suggest.
It's actually possible with linear types as well, but it's a bit more awkward natively. I go into this a bit in the previous post, with this clarification: https://www.reddit.com/r/haskell/comments/6ievrg/comment/dj98aeu (also ping /u/VincentPepper) 
Cool! Thanks. The GC remark might be prematurely pessimistic, linear types can probably be used to limit/eliminate GC since you can limit the number of references to a data structure. It's not _automatically_ so, but they can hopefully be used for this. 
If you try something out and it doesn't work as you expected, please feel free to get in touch. Either file a ticket at the github site, or just reach out via e-mail. I'm very interested in hearing about people's experiences and am happy to help out. It could very well be a bug or something we can improve upon. Regarding the usage of SMT: The technology is indeed quite awesome, but not without its limitations. As a general rule of thumb, if your formulae involves non-linear arithmetic with Integers, or if they make heavy use of quantifiers; the solvers will have a hard time. For linear arithmetic, and problems involving real (infinite precision, `SReal` in SBV parlance), they should do rather well. For most programming related issues, they are quite well suited. For deep mathematical theorems, not so much. Also, anything that requires recursion/induction is typically beyond reach, unless you carefully tune the problem itself by stating exactly the right axioms to work with. On the other hand, bit-vectors are usually quite well supported. I think the best approach is to try things and ask for feedback. StackOverflow has a nice forum, use the tags `sbv`, `z3`, `smt` and similar, and someone will chime in. After a while you'll get a good sense of what sorts of problems are best targets. For starters, you can read through some of the questions/answers for z3 on stack-overflow, for instance: https://stackoverflow.com/questions/tagged/z3
SnakeCase! SnakeCase! Ooooooh it's SnakeCase !
Nice to hear things played out nicely. Sounds like a sound extension of the project too! I've been working on the linear types project (blog [here](https://m0ar.github.io/safe-streaming)). It's starting to come together quite nicely as well. :) It's kinda late now I guess, but it'd be cool to get together the stipendiates in an IRC channel or something to get to know &amp; help each other. I suggested this as an improvement for next years SoH in the midterm evaluation, I think it'd make it feel more like a workplace instead of work in solitude which would be nice.
I have been thinking the same way, but I get anxious when I remember there will usually be quite in-depth discussions about time and space complexity and I'm not sure how I would handle that... :)
The further I get into Haskell the more I feel like I should be using emacs (in evil mode ofc) instead of vim :p Particularly since stack breaks lots of the vim plugins... Nice to see that it goes well! I'm working on the linear types project (blog [here](https://m0ar.github.io/safe-streaming)), it's starting to come together as well. As I replied to another SoH-stipendiate, it would be nice to get together in an IRC channel or something to get to know and help each other. :) 
Bear in mind its not just an fmap, its creating and then traversing and then summing (which means traversing again) a list. I suspect that stuff isn't getting inlined, so that's actually a lot of extra work for something that otherwise would become a very tight process. In the "manual composition" case I wonder if your benchmark is just off? In particular, you're reducing a _constant_ expression? So the compiler might be smart enough to retain the result even though it takes an argument that's immediately discarded... If that's not the case, then there's still plenty more inlining and simplification possible when the function composition is written out explicitly than when there's some form of iteration -- in particular, since you're constructing a big compound function basically which takes a single input, imagine all the inlining that can happen in a given fixed case!
You know, deep inside, that the pronounciation is *different* ;)
fantastique :D
Looking at the core output, it seems that the 'loop' in [`kerApp`](http://lpaste.net/357084) of mapping and then summing a list of two elements is not completely unrolled. E.g., although the `DirUp` part is perfectly inlined, GHC refrains from inlining recursive functions, especially the continuation of summing up the rest of the singleton list. Edit: Likewise for the manually unrolled composition: GHC can completely inline all applications and simplify them to a single constant (seems to be 16 for `t8`). In `tX`, that's not easily possible because of recursion. Also, `kerApp trialKernel r` doesn't simplify in a good way, because some shared bindings cannot be floated under the lambda binding `b` in the simplified form, which obstructs strictness analysis. Things look better if you deactivate full-laziness (`-fno-full-laziness`), which is responsible for floating out stuff in the first place). All in all, I'd 1. Implement `kerApp` as kerApp :: ( Direction -&gt; St Direction ) -&gt; St Direction -&gt; St Direction kerApp ker s = St { st = \b -&gt; mul DirUp + mul DirDown where mul x = st s x * st (ker x) b } ~~Or write that `Num` instance (`-XGeneralisedNewTypeDeriving`) for `St` in the first place.~~ Not so sure about whether this improves readability. 2. Think about a better abstraction for your kernels. They all seem strongly reminiscent of linear algebra, where you represent linear functions as data in a matrix, which frees GHC from having to reason about floating let bindings in and out of lambdas. `tX` will be just composition of matrices, which should reduce fine, albeit you can't assume complete constant folding for the same reasons as above.
&gt; I'm pretty sure you need uniqueness types, since linear types don't guarantee anything about aliasing (someone else could have a reference to a value that's linear to you, in which case it would not be safe to mutate). Your link (and /u/ehubinette) already mention a way to model unique values with linear types. In the link it's in the conclusion. If you have a way to guarantee that you create a *non-shared linear value* then to my knowledge there is no risk of aliasing. If eg the `restrict` function makes a implicit copy of the object being linearized this wouldn't be an issue. But then it's essentially modeling unique types using Linear ones so the distinction is somewhat artificial. For destructive updates where you would probably want the programmer to opt in anyway (since it's unlikely to be worth it for all cases) I don't see a problem with the idea of providing a `makeUpdateable :: a -&gt; 1: (Updateable a)` function or the like as a starting point. But as mentioned I don't know if this would be a good fit for GHC or how/if it could be reasonably integrated.
Can you explain how that last type leads to naughty behavior?
I switched from vim (after 20 years of use) to spacemacs for this exact reason and I don't regret it. Even though there are a few stuff that vim still does better ...
Gallina
[I was told job openings are ok here. If not, please excuse me.] We are a small and very productive team of software engineers located in Tübingen, Germany. We are looking for a fellow software engineer who shares our love for functional programming or has encountered some of the languages already (Haskell, Scala, Clojure, Ocaml, F#, Erlang, Elixir, ...). If this interests you, please PM us! Looking forward to hearing from you! [Job description (in German, but English-speaking engineers are fine, too!)](http://funktionale-programmierung.de/2017/07/19/stelle-active-group.html) 
Cool, I should look into that! Thanks for the tip.
I noticed before that ghc doesn't like unrolling small lists. For example I write `(max a (max b (max c d)))` instead of `maximum [a, b, c, d]`, because the first is more efficient. It would be nice if ghc did such an optimization.
Hmm... It this a Haskell job? Or, you know, a Ruby-on-Rails gig which bait-and-switches Haskell programmers? 
Cabal [new-build](https://github.com/haskell/cabal/projects/4/) here. Yup, I lurk on freenode/#haskell-gsoc (which is the official channel i think), but it's pretty much dead.
How is O(n) different from n? I think this is similar to my solution. I use a Map. My solution is O(n log(k)), where k is the number of distinct characters. Since k &lt; alphabet_size, where alphabet_size is constant for `Char`, it is actually becomes O(n).
We compare the two definitions. In `operational` we have: data ProgramViewT instr m a where Return :: a -&gt; ProgramViewT instr m a (:&gt;&gt;=) :: instr b -&gt; (b -&gt; ProgramT instr m a) -&gt; ProgramViewT instr m a while in `monad-skeleton` we have: data MonadView t m x where Return :: a -&gt; MonadView t m a (:&gt;&gt;=) :: !(t a) -&gt; (a -&gt; m b) -&gt; MonadView t m b data Skeleton t a where ReturnS :: a -&gt; Skeleton t a BindS :: t a -&gt; Cat (Kleisli (Skeleton t)) a b -&gt; Skeleton t b We can rewrite the `ProgramView t = ProgramView t Identity` as: data ProgramView t a where Return :: a -&gt; ProgramView t a (:&gt;&gt;=) :: t a -&gt; Kleisli (ProgramView t) a b -&gt; ProgramView t b Which shows that `ProgramView` is equivalent two `MonadView`, besides the strictness on the fist element of the bind. I believe the main difference is that in `monad-skeleton` the `MonadView` is constructed from `Skeleton`, whose distinction is that `Kleisli` arrows are constructed with `Cat`, whose definition is: data Cat k a b where Leaf :: k a b -&gt; Cat k a b Tree :: Cat k a b -&gt; Cat k b c -&gt; Cat k a c Therefore, one can compose step by step the action using `debone :: Skeleton t a -&gt; MonadView t (Skeleton t) a`.
I'm still waiting for the introductory lecture "A taste of Coq".
This a functional programming job. We write software for our clients in Scala, Clojure, etc., and, yes, Haskell. We are not a web shop (of course, sometimes clients ask for a web interface for their users). Does that answer your question?
Strictly speaking, there isn't really a Haskell 7 or 8. Instead there's a GHC version 7.X and now GHC version 8.X. The main upshot of this is that Haskell the language is very stable and the main differences between 7.X and 8.X are found in the so called "language extensions" that GHC implements. You won't run into any of these when learning Haskell so you'll be fine learning from a book working with 7.X.
But my alignment!
It doesn't seem like this is the type you want. Remember that `IO Int` is a *computation which generates an `Int` when run*. This means that `IO Int` may produce different random numbers whenever run or duplicate computation or whatever. Instead I would imagine you having something like `Deck {initialValue :: Int}` with mkDeck :: IO Deck which, when run, will generate a random value, then store it in the record and return that.
Stickied. :-) 
Yeah, GHC. I just assumed GHC was the standard. It's only early versions that had language breaking changes between major versions? 
GHC is indeed the standard implementation and Haskell is, for better or worse, a one-implementation language. However, because GHC tries to stick to implementing "Haskell 98" the specified language (with enable-able extensions) there are relatively few truly breaking changes in the language. Most notable differences are in how the standard library is structured and of course the canonical libraries that people use do change more regularly. These can be breaking but to my knowledge the big one (known as the Functor-Applicative-Monad Proposal) was implemented in 7.10, not 8.0.
This is something that's badly need in Aeson imo. I understand that the order of keys is irrelevant for computers, but one of the reasons to use JSON instead of a binary format is because it's actually possible for humans to read it. Some people would argue that formats like YAML are better for that, well we've got the same problem there because Data.Yaml is built on top of Aeson. Aeson lets you specify a key ordering, but that is inadequate if you need the same keys in a different order depending on which object they're in. So totally agree that a fork is needed, maybe there's some clever way it can be merged into Aeson itself without breaking the way Aeson currently works.
If Purescript had support for language pragmas we could imagine a `{-# LANGUAGE HaskellLikeSyntax #-}` to switch the parser into a mode which is closer to Haskell's grammar.
Hey, I didn't know of that! I'm coming in! 
Mercury does too, or at least it did; and it is/was broken; see Section 3.6.3 of [my thesis](http://edsko.net/pubs/thesis.pdf). I have no idea about the status of it now.
Thanks for this, jq is nice but the more advanced features are hard to remember the syntax is sometimes not evident Would you mind publishing it to Hackage ? That would make it easily available to install on NixOS/nix ;)
I'll gladly elaborate. It isn't that it is directly _abusable_ but rather very useless. You *force* the requirement that `m a` *must* be duplicable, that means it cannot refer to _anything_ linear in it, which is very crippling in the context of linear monads. :)
Shout it from the rooftops: GHC extensions are not Haskell.
An `Int` and an `IO Int` are two very different types! The first is an integer, while the second is an action which, when executed, will produce an integer. Here is a short program illustrating the difference. -- randomRIO :: (Int,Int) -&gt; IO Int import System.Random data Foo = MkFoo { randomlyGeneratedInt :: Int , generateRandomInt :: IO Int } mkFoo :: IO Foo mkFoo = do generatedInt &lt;- randomRIO (1,100) let generateInt = randomRIO (1,generatedInt) return (MkFoo generatedInt generateInt) printFoo :: Foo -&gt; IO () printFoo foo = do let x = randomlyGeneratedInt foo y &lt;- generateRandomInt foo print (x, y) -- | -- &gt;&gt;&gt; main -- (92,16) -- (92,48) -- (92,65) -- (5,3) -- (5,4) -- (5,5) main :: IO () main = do foo1 &lt;- mkFoo printFoo foo1 printFoo foo1 printFoo foo1 foo2 &lt;- mkFoo printFoo foo2 printFoo foo2 printFoo foo2 In the example execution, `mkFoo` first randomly picks the number 92 and constructs a `Foo` whose `randomlyGeneratedInt` field is 92 and whose `generateRandomInt` field is an action which returns a number between 1 and 92. When we call `printFoo foo1`, we print the already-generated number 92, and we run the `generateRandomInt` action, thereby obtaining a new random number every time. So whether you want an `Int` or an `IO Int` depends on whether you want a new random number every time you construct a `Deck`, or every time you execute that field's action.
Haskell is far from one implementation. GHC is just massively more popular than any other Haskell implementation. Also, Haskel 2010 is the current standard (though sticking to Haskell 98 will of course get you more compatibility). And yes, as of AMP GHC does not have an officially-recognised way to compile either Haskell 98 or Haskell 2010 code. So GHC is at best a "Haskell-like" language now, but no longer supports Haskell.
Me neither! I'll add that to my list of channels :) (Perhaps this is a sign that \#haskell-gsoc should be slightly more heavily advertised?)
Indeed. I pointed out that it'd be nice in the midterm review, maybe they'll promote it next year at least :) 
Let me know if you have any questions!
Yeah, good point, it's quite hard to analyze time complexity let alone space :P What I usually do is build up intuition by case analysis or explain that I'd profile this if I was concerned about its performance.. 
I don't know if it can do a full snapshot but i found zeal (or dash) to be the easiest to build offline doc. There is a way to generate doc set for all the packages if a project. You might create a fake package with all the dependencies you are planning to use. I think zeal haddock see contains the GHC documentation.
Wait, how does double colon help with alignment? Afaik, colon and double colon were swapped because the idea was that you'd be writing more cons's than type declarations, types being inferred and whatnot. I actually like the idea of double colon for cons because that'd be consistent with the "constructor operators should begin with ':'" rule. (I don't consider the colon part of the name so the constructor for cons ":" looks like a special case, allowing the empty string as a name.)
I made an issue awhile back, and they convinced me that it really isn't aeson's job to do that. There would most likely be a performance hit. That said it would be incredibly useful to have.
Yeah, sure, but the interviews I've had with Facebook, Google etc are very focused on exactly complexity analysis so I wouldn't count on that being a valid way out :) 
Both of our solutions are O(n), but if you drop the O and talk about actual number of iterations they're different, yours is n + nk, mine is just nk. The reason is that your solution computes maxD initially before iterating through the string to solve, mine computes the equivalent of maxD DURING the solution iteration, thus saving one iteration. It may sound impossible to do so, but laziness makes it possible.
You can align the `::` with a `-&gt;` in separate lines, but not so much the `:` with a `-&gt;`.
The latest Haskell version is 2010. The specification is freely available via the Internet, just search "Haskell 2012 Report". GHC is the (Glorious) Glasgow Haskell Compiler, one of several implementations. HUGS, UHC, JHC, AJHC, were / are others. Currently the only Haskell implementation in Debian main is GHC. GHC comes with an extensive User's Guide which is also available via the Internet, just search "GHC 8 User's Guide".
Is this remote or on-site? I've been contemplating moving to Germany, but I've been thinking more in terms of Berlin. If it's on-site then can you tell me some selling points about Tübingen?
Thank you, typeclassopedia is pretty nice.
Eh, there's a point to O though. Talking about iterations can just be an exercise in considering large operations as one operation. One iteration with twice as expensive operations is still twice as expensive. 
We'd prefer on-site, but we might be able to make things work remote, it depends. Do you have a CV/Profile/homepage/etc. that you could send us? Please mail our CTO Michael Sperber &lt;michael.sperber@active-group.de&gt;. Tübingen has a large university and student community and is thus very international (you can find folks from all over the world here). The locals are used to them and very laid back; getting around in English is not too difficult. Tübingen is not as over-crowded as Berlin, nature is more accessible and the weather is way better :-) And we have mountains here, if you are into hiking or climbing.
As someone else who switched to spacemacs from vim, I'm curious what you think vim does better.
That would make my solution n log k + n log k, not n + n*k. Which is still O(n log k). It doesn't really say which one performs better, you'd have to benchmark it for that. Since yours has a different dependency on k, you'd also need to benchmark on different k (number of distinct characters). There will always be a k where my algorithm performs better (though it may be very high).
Yea I don't think GHC itself does loop unrolling basically ever. But does anyone know if join points allow GHC to output loops that LLVM can unroll? My understanding is that one of the things join points sometimes allow is turning tail recursion into normal loops.
your solution gives a wrong answer for two of the three strings: &gt; map penner ["aabbc", "geeksforgeeks", "abababcdefababcdab"] [3,7,5] that should be [4, 7, 6]
I don't know ... I never tire to tell my girlfriend that I played with Coq all night :P I'm just sad that I can't say that anymore now that I use Lean.
Maybe that just means we've been doing `(&gt;&gt;)` wrong all along =P
Cool! Looks like it could use some fleshing out though. Maybe we could make a tree of topic-specific nodes and link to tutorials/blog posts for them?
Yeah since all maps are open source, anyone can contribute to the content of them. There is also a [mind map for learning category theory](https://learn-anything.xyz/mathematics/category-theory) which is a bit more filled.
I created a benchmark with all solutions (except for /u/want_to_want, as it wasn't in haskell): https://gist.github.com/kuribas/a7a46f9fd71fbac1f45ca16a5ee68372 the results: http://kuribas.hcoop.net/results.html As with all benchmarks, don't take this to seriously! Some notes: /u/ChrisPenner 's solution gives a wrong result, but I included it anyway. For large strings /u/lgastako and /u/halogen64 's takes forever, so I didn't include it. In my own second version I moved the unique elements calculation into the main loop, but it didn't make much difference. /u/joshcc4 's solution works quite well when k (number of distincs elements) is small, but for large k (200) it is much slower.
For all intents and purposes, you can consider them as part of Haskell. Almost nobody uses/maintains the other compilers.
&gt;learn you a Haskell for greater good! That should be "great good" shouldn't it?
Changed it, thank you. Will be up soon.
That's a pretty unfortunate attitude
 They will be soon! (At least many of the more common ones.)
How often does that happen though? I think :: and = or | would overlap more often and those are both single character width.
Thank you so much for posting this, I was unaware. We actually use more-extensible-effects in production code, it is nice to see some development in this field, that promisses alternative implementations of this idea. BTW, what is the current state of research on extensible effects vs monad transformers?
Very nice article, thanks so much.
I really like it. Looks very good for your particular use case. Can't see any downsides really.
If you don't mind using the nix package manager, it currently has a great way of generating documentation locally and running hoogle as a local server. Check the haskell portion of the nix manual.
Nah, just that in most contexts we don't really care about linearity. But when we do, it is a problem. :) 
It depends on the coding style I guess. One of the projects I've worked on is full of multi-line signatures like this: tell' :: (MonadWriter w m) =&gt; State w k -&gt; m () So in that project it happens all the time. But I think I'd have preferred `:` instead of `::` anyway.
Thanks! This is really great! I'm trying to convert the notes into EPUB through pandoc, to no avail for now. Has anyone tried ?
I'm wondering. Will learning this help me in everyday, practical Haskell programming? Or is it more like a "good to know" or "good to have at the back of your mind" thing? (I have a background in mathematics (no category theory though), and in my experience the latter is usually the case—I rarely use the math directly in practice.)
GHC has always turned tail recursion into loops. The difference with the new join points work is that previously GHC could accidentally destroy join points. Not anymore. Additionally there are some new optimizations involving case expressions and join points. 
I thought GHC emitted LLVM's tailcall instruction
Ah, I misunderstood your question. I don't know the details of the LLVM code generator. 
- Regexp are different, and unlike there is a way to do it, there things missing in emacs regexg, like \s \S \w \W etc ... - vim seems to perform much better with big file. I also have problem with intero taking forever. I have to kill emacs a few time a weeks. I've never have this problem with vim. On the other side, is not the end of the word, I just kill it and restart it ;-) - Another things which really annoys me and I am probably the only which need it is to visit files in a particular order, you can do `vim a c b` and when you go to the next buffer (using `:n`) visit the file in the given order. I haven't managed to do it using spacemacs. There are only minor issues, but overall spacemacs is much much better ( I just use vim when I need to)
There seem to be a few competing libraries, but nobody has market dominance yet like `mtl` has.
Oh yeah, sorry. Didn't even notice the recursion.
Category theory is about as useful for programming as chemistry knowledge is for metal smithing. Sure, knowing the chemical properties and molecular structure of steel might be useful, but it won't exactly make you a good metalsmith... Now, a master metalsmith will absolutely be able to tell you quite a bit about that stuff, even if they don't use proper chemistry terminology. Similarly, a "master level" Haskell programmer tends to pick up a decent amount of category theory and type theory over time (the relevant parts, anyway).
&gt; Created by Packt Publishing I have read some pretty garbage books by this group :\ However, it appears /u/gelisam is the one running it, which makes me think it'll be pretty good!
There are a bunch of Haskell courses on Udemy, all of them from Packt Publishing. Much of this material seems to be worth studying, but what's confusing is that some of the courses seem to overlap quite a bit.
I am, AMA! *edit*: let me address a few obvious questions first. ****** Q: Why Udemy? A: Actually, I didn't know my video course was on Udemy, thanks for letting me know! I did make a video course for Packt Publishing, and they have the right to publish it wherever they please, I'm just a bit surprised that they didn't tell me about it. Otherwise I would have posted it here myself! ...or maybe not, see below. ***** Q: Why Packt Publishing? A: Because they asked. I had never heard of them before, and creating a video course about Haskell sounded like fun! ***** Q: Is the course good? A: Well, I like to think that the material I wrote is pretty good, but there is a major problem which might prevent you from enjoying that material: as you can see in the free sample, the video editing is terrible. They told me to repeat sentences when I fumbled them and that they would remove the fumbled version during the editing phase, but they didn't, and it's really distracting. I told them that I was ashamed of the quality of the result and that I would not share it with my peers until they fixed it. Last time I talked to them, they said they had fixed it, but that doesn't seem to be the case :( ***** Q: Which material does the course cover? A: Look at the "Curriculum For This Course" section at the bottom, it's fairly complete. Ignore the "What Will I Learn?" section, it's from a completely different course we never made, it's another thing thing they promised to fix but didn't. 
How much of my money does go directly to you, and would you rather I paid you in full and we exchanged videos + you make a professional friendship.
That's not true.
Other than GHC I'm definitely interested in JHC. Thanks, I think i'll pick up a book for the last version, and dig through the users guide to get the code working with the current version.
This is a very non-traditional exposition of Haskell. You talk about free monads within the first hour! What exactly is your target public?
Sorry, what *is* your GSoC project? I can't tell from this reddit post and even scanning the blog post fails to turn up this info.
&gt; How much of my money does go directly to you 16% &gt; would you rather I paid you in full and we exchanged videos + you make a professional friendship. I would prefer to give the videos for free on my youtube channel, but Packt Publishing owns those videos so I am not at liberty to publish nor to sell them myself.
This is definitely not intended to be an introduction to Haskell! Packt has other courses for beginners, and they asked me to make their advanced-level course. So at first I wanted to cover all sorts of crazy advanced type-level magic, but it turns out that what they really wanted me to cover was things like streaming and FRP, so I did. As a result, the target audience is intermediate Haskellers who already know the language, including monads, laziness, and a few basic GHC extensions, and want to learn more about the heterogeneous list of topics which the course covers: advanced IO (including unsafePerformIO and unsafeInterleaveIO), streaming, functional reactive programming, parallel programming, concurrent programming, and distributed programming.
I've looked into it, hoping to learn about things which are not usually covered in beginner tutorials (or more advanced ones which focus on mathematical abstractions), such as parallelism, streaming, distributed systems, etc. I personally found it quite hard to follow, but that was most likely because I wasn't the intended audience! :) I think the course assumes an intermediate knowledge and experience with Haskell which reading a beginner book might not give. However, I do intend to go back to this course in the future, when I feel that I'm on the right level. The lecture format is also not the usual "explain-while-typing", as there is no actual live typing – the course has complete Haskell programs as "slides" which are not explained step-by-step. Again, it was difficult for me to understand what some of the programs were doing, and often had to pause to look at the code in order to catch up with the explanations. This makes the lectures quite concise, which, again, would most likely be the preferred format to someone with a strong grasp of Haskell. I suggest looking at the preview videos on Udemy to see if the teaching style suits you. As /u/gelisam said, the lack of editing is noticeable and sometimes distracting, and it's really strange that it wasn't fixed – not the first complaint I've heard about Packt Publishing. Also, I found the course on Safari Books Online, which comes with the ACM student membership – it's a great deal for consumers, but it seems that these courses are not always distributed with the knowledge of the creators...?
Roughly how many employees does your company have, and how long has it been in business?
I thought JHC was unmaintained for several years at this point, but good luck! I'd love a Haskell compiler to C89. I have an "architecture" that doesn't support C99. :(
16%? That seems really low. Did you get an up-front payment?
What in particular are you having trouble with? How specifically does the build fail?
Note that this was due to (the rather misleadingly-named) GHC bug [#13974](https://ghc.haskell.org/trac/ghc/ticket/13974). This will be fixed in 8.2.1 final.
&gt; 16%? That seems really low. Is it? I don't really know what the going rate is; like I said, I thought creating a video course about Haskell sounded like fun, I didn't do it for the money. &gt; Did you get an up-front payment? Yes, 1100$.
Did you mean λ&gt; id "Hello world!" "Hello world!" ---- **Edit**: It always helps me if the letter for each argument of `Const` doesn't change {-# Language InstanceSigs #-} newtype Const a b = Const { getConst :: a } instance Functor (Const a) where fmap :: (b -&gt; b') -&gt; (Const a b -&gt; Const a b') fmap _ (Const a_val) = Const a_val
You can find it in one of his earlier blog posts[0]. To spare you some time, he is working on “Bringing Sanity to the GHC Performance Test-suite” :) [0] https://jaredweakly.com/blog/haskell-summer-of-code/
I didn't really understand lenses until just recently but I've been finding them really useful.
Packt asked me to write a Go book because I posted something in a Go forum once. I hardly knew much Go and said no after actually creating an outline for them that got approved. It left me feeling like they don't make great books sometimes. At least in this case they didn't vet me well. Some Pakt books do seem to be not bad. Maybe it's hit or miss. They did say that that the book would receive a technical review, so maybe that's where the vetting comes in. Maybe they get someone to write a book and then the actual vetting comes in. I actually really wanted to write the book, I just ended up thinking I'd create a shit book and it would have my name on it. I guess with Pakt, as a reader it's more about considering each book for its merits instead of judging by the Pakt brand. I don't think the book I would have written would have been good. 
I'm also interested in this question for Nix. The manual [mentions](http://nixos.org/nixpkgs/manual/#how-to-install-a-compiler-with-libraries-hoogle-and-documentation-indexes) how to get Hoogle working for specific packages, but not for an entire Stack snapshot. Has anyone out there done this before?
Yes, thanks for catching that! I do agree that the way you wrote `Const` is easier to read. On the other hand, I was using the definition of `Const` directly from base. Since my intention is to help people learn these things I suppose it is ok to have a different definition from base that is more readable.
Any feedback on this would be great. I am actually interested in writing a book on lenses and functional references. I am starting with a series of blog posts to gauge interest in the topic and my writing style.
Is there any plans for expanding it past basics into intermediate or advanced topics? Thanks for this awesome resource I never knew existed :) edit: Now see you answered another comment saying it's open source, hopefully someone of experience can fill it out further. Maybe adding it to 'request for contribution' in the Haskell weekly would do it?
Thanks for linking, I'll update my post to make that obvious.
Edit the videos to get rid of the repeated sentences and then they're clearly different material than Pakt's stuff, so you should be fine... ^^/s
Thank you for your comment! You are right, generally this is linear algebra. For discrete bases, the linear algebra package with matrices and vectors works just fine. The problem arises when we have a continuous basis, where the operator * vector involves an integral. I have been trying to come up with a general abstraction that allows one to apply operators onto vectors. I have it written down, but the issue is how inefficient it is... I was hoping that if I could tackle the slowness in a simple discrete case, it would be possible to translate the method to the continuous one. 
Is ajhc still maintained. They all definitely sound interesting for certain niches. Really wish there was diverse range of compilers, in active use like C.
In addition to hoogle, you can also ask ghci for more info on any currently loaded symbol. Just type ":i (^&lt;.)" for example, and you get the type and what module it appears in.
&gt; In order to do this, we need to write native JavaScript code that is called by Elm. &lt;bangs head on desk&gt;
This isn't a mind map, this is a link dump. And it isn't even organized by topic. A mind map would show you how to get from types to type classes to functors to applicators to monads. This just gives you another *list* of books to read. Emphasis on the word list because of how little structure they have, while maps should have lots.
Do I have to create a stack project to use this or is there a way to make this the default and just use `stack exec ghc` etc. to compile single-file Haskell apps?
For really "simple" lens I'd go for paired getter/setter, rather than a van laarhoven style one :)
Torrent that shit! :D
Agreed, this looks like one of my org-mode link dumps. It's a start, though.
YES! I signed up!
&gt; are there any plans to have type Arrow arr = (Category arr, Strong arr) or similar. Literally? I doubt it. That would break all packages that have `instance Arrow ...`. I could maybe see an AMP style thing happening, adding them as superclasses. But the whole idea is predicated on having `profunctors` in `base`, which is a harder problem. Plus, I don't think anyone has gone through and proven that the laws are equivalent. I wouldn't be surprised if they were, but the Arrow laws were sort of thought up because they seemed right, so they could easily be different. Though if they are, maybe the laws derived from category theory would be the better laws =P
Thanks!
Hi, in your examples you have stuff like `x = 1` and not `let x = 1` at the ghci prompt. Is that a typo or is there a way I can get that behavior in ghci too? EDIT: Oh yeah newer version of ghci have this :).
13 comments and no book suggestions yet... [HPfFP](http://haskellbook.com/) is recommended by many.
Should be fine, if you look at it step by step it's just: res_0 = g Nothing (0, c_0) res_n = g res_(n-1) (n, c_n) And res_x is memoized. Written (maybe?) somewhat more clearly: lastOccurrences input = map (\letter -&gt; lastOccurrencesOf letter input) (getLetters input) lastOccurenceOf c = smear . map (wrapIf (==c)) smear = map getLast . scanl mappend Nothing . map Last getLetters = S.toList . S.fromList wrapIf True = Just wrapIf False = const Nothing Written on my phone so sorry if there are mistakes. 
Where do you place `(***)`, `(&amp;&amp;&amp;)` and `arr`? The fact that every arrow is a `Profunctor` (as a functor in `Hask^op × Hask`) relies crucially on the `arr` method, which also happens to be controversial because there are several types which would be arrows without `arr`. Conversely, we have for example `ProductProfunctor` providing a profunctor version of `(***)`, but most originally intended instances (enabling some form of generic programming) are not `Category`. That may suggest breaking up all methods in `Category` and `Arrow` into individual type classes for an extremely fine grained hierarchy, but my feeling is that profunctors and arrows have actually too little useful overlap.
Yes, they have alot low quality books. Especially their ebooks have often terrible layout and formating 😣 but there are some gems that are really good. Example: https://www.packtpub.com/web-development/clojure-reactive-programming That book is really great. Even when you are a haskell programmer.
I have tried this a couple of days ago (as I was unhappy with the memory usage of `intero`). Company code completion is not working (https://github.com/jyp/dante/issues/13). If I understand the issue correctly it won't be fixed by `dante`. It is such an obvious `nogo` for me as a mere user though. 
&gt; (haskell :variables haskell-completion-backend 'dante) Does it give you `company completion` within your code (not in the ghci repl) ?
You don't need to read anything about monads to be productive in the same way that you don't need to know about hardware to use a computer. This is a program with a monad: main= do name &lt;- getLine print ("hello " ++ name) Don't learn Haskell. Just use it
Unpopular opinion: never align.
I also have no idea what the going rate is, but I assumed it would be far better than that. The distribution costs are nearly zero (hello, internet!) so nearly all the revenue should be going to the content creators. I guess Packt were involved in content creation in the sense that they did some (very) rudimentary editing, but was their contribution really worth 84%? I don't know how the business models work in this area but it doesn't seem like a great deal for the primary content creator (you) in this case.
Don't be afraid of Haskell concepts, they are not hard as the legends say. When you look at concrete types like `IO a`, `Maybe a` or `[a]` you may sometimes find that there are similar stuff that you can do that has similar behaviour. For example, you can turn an `a` to a `Maybe a`, or an `a` to a `[a]`. aToMaybe x = Just x aToList x = [x] Some of these kinds of pattern/property/quality have been shown to be useful and apply to many types so they are given names along with some laws that state what you can expect these functions to do for concrete types you see. Monad is just one of these names for useful properties that can be found on many types. It's useful because saying "this T is a monad" is like saying "these 50 functions you already know are going to work as you expect for T as well." A good way to learn these things is just work with types regularly and when you see a pattern that repeats for multiple types read on it. So just start writing code and it will come.
We're almost a dozen and in business since 1999. At the beginning we were only placing freelancers for other companies. In 2011, we shifted our business to software development, which we're doing exclusively now.
GHC has detected that `a` and `b` are the same in `Foo`. With `-ddump-types`: TYPE CONSTRUCTORS type family Foo (x :: a) (y :: HList (b : cs)) :: HList (b : a : cs) where [(cs :: [*]), b, (y :: HList (b : cs)), (x :: b)] &lt;---- no "a" Foo b cs b x y = 'Cons x y axiom TestCoerc.D:R:Foo COERCION AXIOMS axiom TestCoerc.D:R:Foo :: forall (cs :: [*]) b (y :: HList (b : cs)) (x :: b). Foo x y = 'Cons x y -- Defined at TestCoerc.hs:18:3
`Cons` is the type family declaration and in the function are not the same, as one is a type constructor and the other a constructor. Where does the `Cons` type come from ?
As for the royalties, is that only for direct sales from packt pub's own website? I'm wondering since you didn't even know your course was on udemy. If you are interested in doing more screencasts or books in the future, perhaps consider pragprog.com or leanpub.com. Pragprog is a brand with great reputations. You get 50% of the royalties (minus direct costs like shipping) and get to work with great editors who have really high standards. Leanpub is for self publishing. I don't think they do screencasts. For books they provide you with a platform to host, market, and distribute the books. Think App Store. You get 90% royalties minus 50 cents per paid sale. Disclaimer: I'm not affiliated with any of these companies. I just feel like it is a pity that your videos never got the post production they deserved. For me, I get distracted easily while I'm learning and your description of the quality of the videos has dissuaded my interest.
&gt; As for the royalties, is that only for direct sales from packt pub's own website? My royalty statement includes a line for "Subscription 3rd party", so assume I do get a cut of sales on other websites such as Udemy. The "Video" line is not similarly subdivided, but I assume it includes sales on all platforms, it would be absurd if it didn't. &gt; If you are interested in doing more screencasts or books in the future, perhaps consider pragprog.com or leanpub.com. Thanks for the suggestion! I do plan to make more videos, but I plan to publish them for free on [my YouTube channel] (http://haskellcat.com).
I have some questions: * Is there a deadline for applications? * When do you expect applicants to start? Is September too late? * Are you interested in developers without any job experience (i.e. directly from university)? * I would consider my Haskell skills as pretty solid. I wrote both of my thesis projects in Haskell (~16kLOC) and also a lot of personal projects. I wrote some code in Racket, Clojure and F#. I'm good at learning new languages. However, learning a language and writing productive code are two different pair of shoes. I wonder what you expect from applicants.
`TypeInType` implies `DataKinds`.
To me this is [the best take on Monads I have seen so far](https://arcanesentiment.blogspot.nl/2017/06/purely-algebraic-abstractions.html), and more generally on what can seem difficult for newcomers. Emphasis: &gt; This is why monads are so hard to learn. Each student of Haskell asks what monads mean, and invents a variety of wrong answers (typically semantic generalizations of IO actions), because they're sure that such an important abstraction must be meaningful, and have never heard of algebraic abstraction. Eventually they learn to use monads without asking what they mean, because monads don't mean anything. Thinking in terms of interfaces (i.e. moves you're allowed to make against the provider of the interface seen as an opponent) rather than trying to try to find an all-encompassing meaning for the M word (or the A one, or whatever) makes it easier IMHO. When you throw in laws, it simply means your opponent is more restricted in what they are allowed to do because from your point of view various combinations of moves should be indistinguishable.
Yup, this gives the expected error: type family Foo (a :: *) (b :: *) (x :: a) (y :: HList (b : cs)) :: HList (b : a : cs) where Foo _ _ x y = 'Cons x y ETA: Actually this is enough type family Foo (x :: a) (y :: HList (b : cs)) :: HList (b : a : cs) where Foo (x :: a) (y :: HList (b : cs)) = 'Cons x y
Monads aren't "hard"; they are actually incredibly simple, the stumbling block is realizing what Monads are not, and developing a good intuition for the concept. The "Monad Epiphany" tends to be a rather anticlimactic experience, closer to a facepalm than a deep insight. To answer your questions: In my case, after almost 20 years of programming imperative languages, it took about 3 months to get productive, and maybe another 3 for Haskell to become my strongest language. Right now, several years later, I find Haskell the easiest of the dozen or so languages under my belt (including languages designed for being easy, such as Python) - but then again, a large part of this is that Haskell aligns well with how I think and work, and another part is being familiar with the ecosystem and idiosyncrasies. As far as hard things in Haskell go, I think the hardest parts are reasoning about performance, and driving the type system to the limits of its expressiveness.
You have to get used to new syntax/operators and if you've never done functional before, maybe a new way of thinking. Concepts are overblown and you can live without most of them
&gt; The fact that every arrow is a Profunctor (as a functor in Haskop × Hask) relies crucially on the arr method, which also happens to be controversial because there are several types which would be arrows without arr. I don't think there's any chance of that changing. Aside from the majorly breaking change it would be, the whole point of `Arrow` was to serve as a more generic alternative to monads, which relies crucially on `arr`. If you don't like `arr`, chances are you're actually just looking for a fuller `Category` hierarchy (which, conveniently, Conal Elliott [recently made](https://github.com/conal/concat) a library enabling DSLs over arbitrary categories). &gt; Conversely, we have for example ProductProfunctor providing a profunctor version of (***), but most originally intended instances (enabling some form of generic programming) are not Category. Mind sharing some examples? I have not found an instance of `ProductProfunctor` that couldn't be equally well represented with `Strong + Category`. As far as I can tell, there's no compelling reason for `ProductProfunctor` over `Strong + Category`. &gt; That may suggest breaking up all methods in Category and Arrow into individual type classes for an extremely fine grained hierarchy This is basically the `profunctors` library.
Ok but type and constructor are still different name space aren't they ?
You can make this the default by setting it in Stack's global project at `~/.stack/global-project/stack.yaml`. 
Let's figure it out! My guess is that GHC inferred this: import Data.Kind (Type) type family MyFoo (a :: Type) (b :: Type) (cs :: [Type]) (x :: a) (y :: HList (b : cs)) :: HList (b : a : cs) where MyFoo a a cs x y = 'Cons x y And indeed, with `MyFoo a b cs x y` instead, we get the error you expect. Here's how using MyFoo works. Its only equation expects `a` and `b` to be the same type. So if I use the same type, everything works fine: &gt; :kind! MyFoo () () '[] '() ('Cons '() 'Nil) MyFoo () () '[] '() ('Cons '() 'Nil) :: HList '[(), ()] = 'Cons '() ('Cons '() 'Nil) But if I use different types... &gt; :kind! MyFoo () (Maybe ()) '[] '() ('Cons ('Just '()) 'Nil) MyFoo () (Maybe ()) '[] '() ('Cons ('Just '()) 'Nil) :: HList '[Maybe (), ()] = MyFoo () (Maybe ()) '[] '() ('Cons ('Just '()) 'Nil) I don't get an error, instead the type-level evaluation gets stuck! This is not what I expected, but whatever, let's roll with it. Let's use that behaviour to determine whether your Foo indeed got inferred to be like MyFoo: &gt; :kind! Foo '() ('Cons '() 'Nil) Foo '() ('Cons '() 'Nil) :: HList '[(), ()] = 'Cons '() ('Cons '() 'Nil) &gt; :kind! Foo '() ('Cons ('Just '()) 'Nil) Foo '() ('Cons ('Just '()) 'Nil) :: HList '[Maybe (), ()] = Foo '() ('Cons ('Just '()) 'Nil) Indeed, your Foo behaves the same as MyFoo. One last thing: you can get the equivalent of `MyFoo a a` at the value level using an equality constraint: -- no error! myfoo :: a ~ b =&gt; a -&gt; HList (b : cs) -&gt; HList (b : a : cs) myfoo x y = Cons x y So, one mystery remains: why does GHC infer the equality constraint in a type family but not in a regular function?
I don't understand your question. `Cons` is a value constructor and `'Cons` is a type constructor. They are indeed not the same. Where does the type constructor `'Cons` come from? It's automatically defined by `DataKinds`.
I feel like Haskell is very "vertical" compared to other, "horizontal" languages. E.g. C++ basically stops at classes and for loops, every feature being a horizontal addition. Haskell keeps building on its own concepts. This might make it harder to grasp right away, but more powerful later on. (C++ might not be the best example due to memory management etc.)
I mean in Foo x y = Cons x y How does this `Cons` relates the constructor defined in `Hlist` definition ?
Good point!
Keep at it. The more the merrier. The [lens over tea](https://artyom.me/#lens-over-tea) series is also great (what little I've read of part 1) and the dude is now co-authoring https://intermediatehaskell.com Looking forward to that one. I'm also looking forward to your book! We need more intermediate books on lenses, prisms, arrows, etc.
Personally, I think this article would be a lot easier to understand if the order was a little bit different. Jumping straight into `Identity` and `Const` without providing motivation for their use seems a little confusing. Maybe if you showed the getter/setter lens first and then showed how it could be reimplemented in the van laarhoven style? Also, some of the examples seem to rope in things that aren't necessarily essential to understanding lenses (`coerce` in the `fmap` for `Identity`, for example. Maybe write it as `fmap f (Identity x) = Identity (f x)` with a footnote on why it's different in base?). 
Consider the definition data T = MkT `T` is a type with kind `*`, and `MkT` is a (value) constructor with type `T`. With DataKinds, this definition also defines a kind `'T` and a type constructor `MkT` of kind `T'`. Similarly, the definition data HList :: [*] -&gt; * where Nil :: HList '[] Cons :: a -&gt; HList ts -&gt; HList (a : ts) defines both a type constructor `HList :: [*] -&gt; *` and a kind constructor `'HList :: [*] -&gt; *`, both the value `Nil :: Hlist '[]` and the type `'Nil :: 'HList '[]`, and both the value constructor `Cons :: a -&gt; HList ts -&gt; HList (a ': ts)` and the type constructor `'Cons :: a -&gt; 'HList ts -&gt; 'HList (a ': ts)`. In `Foo x y = Cons x y`, the `Cons` is really `'Cons`, because when doing so is unambiguous, GHC prints a warning and replaces the unticked name with the ticked name. Also for some reason only the type constructors are ticked, not the kind constructors; above I have used `'T` and `'HList` for clarity, but GHC requires the unticked names `T` and `HList` to be used. Does that answer your question?
Thanks. I know the difference between `Cons` and `'Cons` but didn't realize the quite wasn't needed.
Oh ok, that makes total sense. I wasn't thinking of multi-line type signatures. Personally I wouldn't align types like that though, it seems to imply the arrows relate to tell' instead of the type but that's just a matter of preference I guess.
Reading this: https://two-wrongs.com/the-what-are-monads-fallacy gave me a profound sense of relief.
I think the hard part is getting to where you can easily read other peoples' code. You can write your own Haskell code to be about as simple as you need it to be, but you can't just remove all the lenses and type families and so on from somebody else's code that you need to read.
My advice is to try doing stuff as you are learning. I focused a lot on just reading/watching but didn't really start understanding until I actually tried using the stuff I'd read then i understood. Ps haskellbook.com is awesome.
Personally, purely as an occasional packt customer (books only), you have to approach it with the right attitude. As you say, there seem to be a few gems, but that seems to be down to individual authors. What you generally get is comparable to series of web tutorials - complete with typos, non-native-speaker language issues and other I-assume-there-was-no-editor errors and omissions - but a larger more self-consistent (usually) series than most. Perhaps because I started programming in the 80s, I actually even like having lots of listings in some books, so long as there's some explanation and I don't have to type them in. Even if a book is mostly just repackaging someones learning project code and notes (which at least some are), I can still benefit from that - I just need to be pro-active about extracting what I need. The biggest problem to me isn't really quality but their approach to sales, which is constant spam (not quite annoying enough for me to bother trying to turn it off), constantly farting around with discounts (50% seems to be almost every other day, 20% and even 10% is routine, always timescales that are supposed to trigger idiots into a quick-before-you-miss-it panic) so pricing is a when-you-need-it lottery. And they're constantly trying to nag customers to join their subscription scheme, which might even make sense for some, but I'm not remotely interested. 
Great! Thanks
&gt; Mind sharing some examples? I have not found an instance of ProductProfunctor that couldn't be equally well represented with Strong + Category. As far as I can tell, there's no compelling reason for ProductProfunctor over Strong + Category. The examples I have in mind are in Opaleye ([`QueryRunner`](http://hackage.haskell.org/package/opaleye-0.5.3.0/docs/Opaleye-RunQuery.html#t:QueryRunner), [`Unpackspec`](http://hackage.haskell.org/package/opaleye-0.5.3.0/docs/Opaleye-Internal-Unpackspec.html#t:Unpackspec)) and [one-liner](http://hackage.haskell.org/package/one-liner-0.9.1/) which implements and uses an equivalent of `ProductProfunctor` for `Costar`, `Biff`, `Joker`, `Clown`, `Tagged`, `Const`. These are not categories AFAICT. These two packages define a form of "profunctor traversal" (in the sense of profunctor optics) on generic data types, making `ProductProfunctor` a close sibling of `Applicative` and [`Traversing`](https://hackage.haskell.org/package/profunctors-5.2/docs/Data-Profunctor-Traversing.html#t:Traversing). -- Traverse children of s, whose types satisfy a constraint c type TraversalP c s = forall p. ProductProfunctor p =&gt; (forall a. c a =&gt; p a a) -&gt; p s s `Category` doesn't give nice parametricity properties for these optics.
Did it work?
Link?
 &gt; Link? [Here you go!](https://upload.wikimedia.org/wikipedia/en/3/39/Wakerlink.jpg) --- ^(I am a bot. | )[^Creator](https://www.reddit.com/user/alienpirate5)^( | Unique string: 8188578c91119503)
Those are good points. I think starting with getter/setter is a good idea, terrorjack mentioned that as well, and using easier to read instances than base, but have links to the base implementations.
Sorry for the repost here https://www.reddit.com/r/haskell/comments/6oskyx/lens_tutorial_implementing_a_simple_lens/ I think this originally got flagged for some reason so I reposted it as a link instead.
So good!
&gt; they convinced me that it really isn't aeson's job to do that Well Aeson discards the information, so it's not like you can have some library do post processing. I think the only answer is a fork that doesn't mind the performance hit to achieve this goal.
The examples in Opaleye are not arrows mainly to encapsulate types that shouldn't be allowed to escape IIRC. But this can be achieved with ST style Rank2Types. I did not mean to imply you could provide a Category instance to any ProductProfunctor. Just that I haven't seen a usecase for ProductProfunctor that couldn't be done by something else which is a Category. Why would you lose parametricity in the `TraversalP` example? The `p` is still polymorphic so you can't hack type equalities into scope any more than the `c` inherently allows.
Haskell is deceivingly simple, that's what so hard about it. Rather than learning *a few complex* concepts (e.g. memory allocation, pointers), with Haskell you have to learn *many simple* concepts, and the challenge is getting used to writing code where you utilize one or more of these simple concepts (like a monad). 
&gt; Monads aren't "hard"; they are actuay incredibly simple Simple /= easy! Monads are a difficult concept to grasp because they operate at a very different level of abstraction than most programmers are used to thinking in. It can be very hard to lift one's ability to reason abstractly.
I don't understand how you can achieve what Opaleye or one-liner do with a category and/or `RankNTypes` rather than ProductProfunctor. What is the relation of the types in Opaleye I'm mentioning with `ST`-style tricks? In `TraversalP`, if you put a `Category+Strong` constraint instead, I'm not saying it's not parametric, you just get a different type that does not seem familiar in the optic hierarchy.
Take a look at [Selda](https://github.com/valderman/selda) to see what I mean with RankNTypes. They use the ST-style trick to ensure columns can't be given to illogical scopes, which is the problem that Opaleye uses `ProductProfunctor` to solve.
Also [The Haskell Pyramid](https://docs.google.com/presentation/d/1bSANLVcGnfVIFjicj81Uo_MYQhsF0FZi_EF-NEKFecE/edit#slide=id.g21ecc886bf_0_64) is relevant here.
I don't think we're thinking about the same problem here. I'm talking about Opaleye (and one-liner) using ProductProfunctors for datatype generic programming. It has little to do with scoping AFAICT.
Oh I thought you were talking about the `QueryRunner` stuff. As for data type generic programming, I don't know much about this topic. So I have no idea what I'm saying when I suggest I *think* it should be fine to do `TraversalP` with a `Category` constraint instead =P EDIT: Especially since you can get a `Category` for any profunctor, retaining strength: data Arr p a b where Hom :: (a -&gt; b) -&gt; Arr p a b Comp :: p a x -&gt; Arr p x b -&gt; Arr p a b instance Profunctor p =&gt; Profunctor (Arr p) where dimap l r (Hom f) = Hom (r . f . l) dimap l r (Comp f g) = Comp (lmap l f) (rmap r g) instance Strong p =&gt; Strong (Arr p) where first' (Hom f) = Hom (first' f) first' (Comp f g) = Comp (first' f) (first' g) instance Profunctor p =&gt; Category (Arr p) where id = Hom id f . Hom g = lmap g f f . Comp g h = Comp g (f . h) 
Ahh.. this is very similar to the `fibs = 0:1:zipWith (+) fibs (tail fibs)` chestnut. If you reduce the expression enough times it becomes obvious why its a single traversal..
Thanks goes to /u/algebra4life and his [super helpful tutorial](https://samtay.github.io/articles/brick.html). If you want to make a terminal game, check it out!
Correct; but in this case, I believe my point still stands - Monad, as a concept, is both simple and easy, at least one you realize how little there really is to it. The difficulty many learners challenge isn't really intrinsic to Monads themselves, it lies in the mental hurdle of accepting that there isn't any meaning beyond bind, return, and the Monad Laws. In other words, it is not hard (in any sense) to "understand Monads", it is hard to let go of the feeling that there must be something big to understand there in the first place.
It sounds like you're carving off the hard part and saying "no, this part is not part of learning monads, therefore monads are easy." It is hard to understand what level of abstraction monads live in. Once you understand the hard part about monads, monads are easy. EDIT: I suppose it's not strictly unique to monads, but it's certainly unique to monads from basically any beginner's point of view. Monads are encountered far earlier than anything else at the same level of abstraction (except Applicative, maybe)
&gt; I told them that I was ashamed of the quality of the result and that I would not share it with my peers until they fixed it. Last time I talked to them, they said they had fixed it, but that doesn't seem to be the case That sounds like a difficult position to be put in, I'm sorry!
&gt; lazily ported Turns out OP only started programming this when someone ran it
Hm, very interesting. Thanks! I assume this counts as bad behavior by GHC?
I dont' know of a deadline. Starting in September is ok, I think. Our last hire was straight from uni. You experiences sound great! We are aware that you can't be as productive as usual while you are learning. We are all learning new things all the time, so it's good to get used to it. :-) Please send your resume/CV/Profile/Homepage or similar to our CTO, Michael Sperber michael.sperber@active-group.de. We're looking forward to hearing from you! If you have more questions, please don't hesitate to ask.
Just started to look at it. First thing I see, there's only multiplication and no addition (it's easy to add it though ;) Second thing I see is partial type inference. For example, instead of `Qty Mass` the system may deduce thus type: Mult ('Dim ('Length One) ('Time MTwo) ('Mass Zero)) b ~ 'Dim ('Length One) ('Time MTwo) ('Mass One) =&gt; Qty b "some type which, when multiplied by acceleration, gives force". What could *that* be?! (`MTwo` is a name for `-2` I added). It is probably necessary to make type families `Mult` and `Div` injective for this to work, but not sufficient. Heading to bed now, will continue...
 colin@yumi ~/c/h/streaming-osm&gt; stack --stack-yaml stack-820.yml bench WARNING: Ignoring out of range dependency (allow-newer enabled): time-1.8.0.2. time-locale-compat requires: &lt;1.5 WARNING: Ignoring out of range dependency (allow-newer enabled): bytestring-0.10.8.2. cassava requires: &gt;=0.9.2 &amp;&amp; &lt;0.10.4 WARNING: Ignoring out of range dependency (allow-newer enabled): aeson-1.2.1.0. streaming-utils requires: &gt;0.8 &amp;&amp; &lt;1.2 WARNING: Ignoring out of range dependency (allow-newer enabled): base-4.10.0.0. json-stream requires: &gt;=4.7 &amp;&amp; &lt;4.10 streaming-osm-1.0.0: unregistering (missing dependencies: criterion) cassava-0.5.0.0: configure cassava-0.5.0.0: build Progress: 1/4 -- While building package cassava-0.5.0.0 using: /home/colin/.stack/setup-exe-cache/x86_64-linux/Cabal-simple_mPHDZzAJ_2.0.0.0_ghc-8.2.0.20170704 --builddir=.stack-work/dist/x86_64-linux/Cabal-2.0.0.0 build --ghc-options " -ddump-hi -ddump-to-file" Process exited with code: ExitFailure 1 Logs have been written to: /home/colin/code/haskell/streaming-osm/.stack-work/logs/cassava-0.5.0.0.log Configuring cassava-0.5.0.0... Preprocessing library for cassava-0.5.0.0.. Building library for cassava-0.5.0.0.. /tmp/stack14916/cassava-0.5.0.0/Data/Csv/Conversion.hs:26:3: error: error: #error **INVARIANT BROKEN** Detected invalid combination of `text-short` and `bytestring` versions. Please verify the `pre-bytestring-0.10-4` flag-logic in the .cabal file wasn't elided. # error **INVARIANT BROKEN** Detected invalid combination of `text-short` and `bytestring` versions. Please verify the `pre-bytestring-0.10-4` flag-logic in the .cabal file wasn't elided. ^~~~~ | 26 | # error **INVARIANT BROKEN** Detected invalid combination of `text-short` and `bytestring` versions. Please verify the `pre-bytestring-0.10-4` flag-logic in the .cabal file wasn't elided. | ^ /tmp/stack14916/cassava-0.5.0.0/Data/Csv/Conversion.hs:79:0: error: warning: "MIN_VERSION_text_short" is not defined, evaluates to 0 [-Wundef] #if MIN_VERSION_text_short(0,1,0) | 79 | #if MIN_VERSION_text_short(0,1,0) | ^ /tmp/stack14916/cassava-0.5.0.0/Data/Csv/Conversion.hs:79:0: error: error: missing binary operator before token "(" | 79 | #if MIN_VERSION_text_short(0,1,0) | ^ /tmp/stack14916/cassava-0.5.0.0/Data/Csv/Conversion.hs:882:0: error: warning: "MIN_VERSION_text_short" is not defined, evaluates to 0 [-Wundef] #if MIN_VERSION_text_short(0,1,0) | 882 | #if MIN_VERSION_text_short(0,1,0) | ^ /tmp/stack14916/cassava-0.5.0.0/Data/Csv/Conversion.hs:882:0: error: error: missing binary operator before token "(" | 882 | #if MIN_VERSION_text_short(0,1,0) | ^ `gcc' failed in phase `C pre-processor'. (Exit code: 1) 
http://www.cs.yale.edu/homes/hudak/SOE/software1.htm
[removed]
[removed]
[removed]
You have inspired me to make a terminal game in Haskell 
[removed]
&gt; As far as I can tell, there's no compelling reason for ProductProfunctor over Strong + Category You need to tell a little bit harder :) There are plenty of useful `ProductProfunctor`s that are not `Category`s.
&gt; The examples in Opaleye are not arrows mainly to encapsulate types that shouldn't be allowed to escape IIRC No, by no means. They're not arrows because they're not categories. In fact in some cases they morally map from Haskell types to SQL types or vice versa. That means that composition has no chance! 
Opaleye uses `Arrow` where Selda uses `Monad` and an ST-style trick. The `ProductProfunctor`s stuff is something else entirely!
&gt; Oh I thought you were talking about the QueryRunner stuff. Yes, Opaleye's `QueryRunner` is a `ProductProfunctor`. There's no chance of making it a `Category` or `Arrow` though!
&gt; Where do you place (***), (&amp;&amp;&amp;) and arr? I'm not sure what you mean. All those operations are derivable from `Strong` + `Category`.
I think there's clearly some stuff I'm mistaken about. Further research is required =P *PS: would have been nicer if you had made one comment instead of flooding my inbox ;)*
Huh, I guess I took "merge Arrow and Profunctor" a bit too literally.
monads are easy, monad transformers and all the stuff around them are harder
https://hackage.haskell.org/package/HGL
Awesome! I'd love to see it when it's done
&gt; PS: would have been nicer if you had made one comment instead of flooding my inbox ;) Oh OK. Sorry if it upset you. I thought it would be nicer for anyone following the threads to view the replies on the messages they are replies to!
It looks like this may be a stack issue; perhaps it's getting the flag logic wrong?
I'm just giving you a hard time =P
Ah, I see. You're absolutely right.
You're so quick to shift blame to Stack. To me this looks more like an issue in badly maintained cassava package.
What do you mean? If we have `type Arrow arr = (Category art, Strong arr)` then we can define them on arrows or even slightly lower in the hierarchy. arr :: (Category p, Profunctor p) =&gt; (a -&gt; b) -&gt; p a b arr f = rmap f id (***) :: Arrow p =&gt; p a b -&gt; p a' b' -&gt; p (a, a') (b, b') f *** g = second g . first f (&amp;&amp;&amp;) :: Arrow p =&gt; p a b -&gt; p a b' -&gt; p a (b, b') f &amp;&amp;&amp; g = f *** g . arr (join (,))
Monad is actually pretty easy. My tip is ... just use it and don't think too much. I find that once you get used to it, you can finally make simple programs. But to write more complex programs (usually requiring other libraries), then you need to learn Monad Transformers, mtl (the library) and typeclass. And for me, those were the hard parts. If you are still beginning to learn, my suggestion is to do coding exercise in Codewars / similar sites with Haskell. It helps you get familiar with the syntax faster.
It looks to me like it's a bit of both. The fact that bounds need to be ignored is indeed unfortunate. More active maintenance would certainly help here (although in tibbe's defense GHC 8.2 isn't quite out yet). However, it also looks like something is also a bit funky with the flags. 
Glad I could help! The gif looks fun, I'll give it a try when I get home.
That is weird. I feel like I've seen a ton of Haskell job postings lately, and for a wider variety of fields.
What other Haskell compiler do you have in mind?
For the record, [Arrow is more than Strong + Category](https://www.eyrie.org/~zednenem/2017/07/twist). For example, the type newtype Twist a b = Twist { getTwist :: (a,a) -&gt; (b,b) } instance Category Twist where id = Twist id Twist f . Twist g = Twist (f . g) instance Profunctor Twist where dimap pre post (Twist f) = Twist ((post *** post) . f . (pre *** pre)) instance Strong Twist where first' (Twist f) = Twist (\((a1,c1),(a2,c2)) -&gt; let (b1,b2) = f (a1,a2) in ((b1,c2),(b2,c1))) The instances for `Category` and `Strong` are lawful, but you can't make a lawful `Arrow` where `first = first'`.
Yeah, I wouldn't put too much stock in that statement. It also seems like it might be taking into account number of openings and not number of hires.
What an entertaining post, some thoughts: a board Board (a, b, c) (d, e, f) (g, h, i) can be represented as [nested](https://hackage.haskell.org/package/base-4.9.1.0/docs/Data-Functor-Compose.html) [3D vectors](https://hackage.haskell.org/package/linear-1.20.6/docs/Linear-V3.html) V3 (V3 a b c) (V3 d e f) (V3 g h i) and if you represent it as a `newtype` you get to derive many cool instances import Data.Semigroup.Foldable (Foldable1) -- ... newtype Board a = Board (Compose V3 V3 a) deriving (Functor, Applicative, Representable, Foldable, Foldable1, Arbitrary, Arbitrary1) -- assuming Arbitrary (V3 a) / Arbitrary1 V3 instance Distributive Board -- as in post Being able to derive an instance of a type class with an associated type family (`Representable` with `Rep`) is only possible since GHC 8.2, inheriting the representation of `Compose V3 V3` Rep Board = Rep (Compose V3 V3) = (Rep V3, Rep V3) = (E V3, E V3) We can slap a pattern synonym over it pattern M33 :: a -&gt; a -&gt; a -&gt; a -&gt; a -&gt; a -&gt; a -&gt; a -&gt; a -&gt; Board a pattern M33 a b c d e f g h i = ------------------ Board (Compose (V3 (V3 a b c) (V3 d e f) (V3 g h i))) and the messy `Board (Compose ..)` becomes M33 a b c d e f g h i
Here's the stuff that left me scratching my head while learning Haskell, YMMV: * String, Text, ByteString, and OverloadedString * unable to define records with same field names * Lenses, their operators/functions and error mesaages * error handling and the mess around it * monad transformers, lift, and liftIO, not monads * type families * datakinds * typeclass constraints while defining a typeclass instance * template Haskell and its idiosyncracies * picking which library (out of a dozen) to use for a common task. 
Man, I should really give Haskell another try. My last attempt died when I couldn't wrap my head around Monad Transformers in Real World Haskell. How is the world of Haskell learning material these days? Any new (since 2009 ish) books worth reading/working through?
Interesting, thanks! So I guess it would be better to have `Arrow` be a separate class with `Strong` and `Category` as superclasses. Should `***`, `&amp;&amp;&amp;` and/or `arr` be defined within that `Arrow` class or should they be defined in terms of `Strong` / `Category` primitives. I could definitely see `***` and `&amp;&amp;&amp;` being defined in `Arrow` but maybe not `arr`?
As well, I'm sure there are plenty of jobs not being listed to DICE or CareerBuilder.
Where can I read what's new? Especially about Backpack and compiler performance improvements. 
The author clearly didn't understand their own rankings. Using 2017 data by whatever weird metrics they have you get haskell at 48 or so, and with 2016 data its at 43. I don't know what the numbers mean, but they appear to be going up?
Assuming we don't want to introduce any other potential superclasses for `Arrow`, then `***` and `&amp;&amp;&amp;` at least should require `Arrow`. For efficiency, they should be members for the same reason they are members now. `arr` can be defined using `dimap` and `id`, so it doesn't actually require `Strong`, but it should probably be a class member for efficiency as well. Unless there is a lot of demand for a `PreArrow` class, I'd leave it in `Arrow`.
The operations are derivable, but the laws may not apply.
I mainly learned by reading [Haskell Programming from First Principles](http://haskellbook.com/), which has a decent explanation of monad transformers. [LYAH](http://learnyouahaskell.com/) is also good, but doesn't cover monad transformers. There's also the [wikibook](https://en.wikibooks.org/wiki/Haskell/Monad_transformers); haven't read much of it, but I liked what I have. Besides that, reading various blog posts and asking questions on irc can get you pretty far. That being said, the best way to learn is by actually writing code. Working on a project is the best motivation, at least for me. Who knows, you might not even need monad transformers (I didn't use any) to build something cool! And if you need a "starter" project, [this paper](https://page.mi.fu-berlin.de/scravy/realworldhaskell/materialien/monad-transformers-step-by-step.pdf) provides a good walkthrough of monad transformers.
I'd like to hear if anyone has any plans to use compact regions, and what you'll use them for.
All representable functors are isomorphic to `Reader e` for some `e`. Obviously this is right adjoint to `(,) e` (the `CoordF` functor in this post), but can it be proven that all functors left adjoint to `Reader e` are isomorphic to `(,) e`?
I think you just need `rmap` and `id`. Will that actually be slower in practice even with things like inlining? I think you are probably right about `***` and `&amp;&amp;&amp;` though. 
So stoked for unboxed sums - been wishing for this for years
Seems that git tags for 8.2.1-rc3 and 8.2.1 are not available when the releases are announced..any particular reasons?
No idea what the numbers mean either, but the comparison does appear to show Haskell at the same ranking in the two years (ranking is different from the "score" it seems). Definitely does not decrease, so presumably there's an error in the article or the interactive ranking viewer.
Congratulations on your release. And thank you to all the GHC contributors for your hard work. 
What I get from that listing is that Python is now the lowest common denominator.
Monads aren't nearly as difficult as cobbling together a development environment from poorly integrated tools. And of course, if you had a functional IDE, getting a list of all instances of the Monad typeclass from which you could jump straight to definitions and usages would be trivial. No amount of crappy box metaphors can make up for seeing what they do and how they're actually used.
You can find most information there is on Backpack via the [Backpack Wiki page](https://ghc.haskell.org/trac/ghc/wiki/Backpack). Most notably, you can find string-related Backpack packages on http://next.hackage.haskell.org:8080/.
IEEE spectrum ranking itself seems random or stupid. It shows Arduino at a higher ranking than Haskell. Haven't seen a single Arduino job position in ages. 
No particular reason, they'll soon be provided. For the record, [`0cee25253f9f2cb4f19f021fd974bdad3c26a80b`](http://git.haskell.org/ghc.git/commitdiff/0cee25253f9f2cb4f19f021fd974bdad3c26a80b) is supposed to be the commit tagged for GHC 8.2.1 final.
Hey, just wanted to say thanks for taking the decision to publish your videos on YouTube whenever you do. Compared to other languages Haskell has comparatively less material available, but people like you make it better :) 
As usual, Ubuntu users can use my [GHC ppa](https://launchpad.net/~hvr/+archive/ubuntu/ghc) to conveniently install binary packages specifically built for the currently non-EOL'ed Ubuntu releases - Ubuntu 12.04 (Precise), - Ubuntu 14.04 (Trusty), - Ubuntu 16.04 (Xenial), and - Ubuntu 17.04 (Zesty) Installing is simply a matter of sudo apt-add-repository ppa:hvr/ghc sudo apt update sudo apt install ghc-8.2.1-prof ghc-8.2.1-htmldocs cabal-install-2.0 and add `/opt/ghc/bin` to your $PATH, and you should be ready to go. Please refer to the [PPA description](https://launchpad.net/~hvr/+archive/ubuntu/ghc) for additional information on how to manage multiple GHC versions installed side-by-side.
Aha, nice objection! So we could have `class (Strong p, Category p) =&gt; Arrow p` with no methods just to indicate that the arrow laws hold.
Unless you are doing lot of wacko stuff, you don't need anything other than QuickCheck, then again I am not aware of the complexity of the code you are writing. 
Tibbe is not to blame here, as I'm the current maintainer. I specifically made sure `cassava-0.5.0.0` was compatible with the upcoming GHC 8.2.1 when I released it over one month ago. And in fact it *is* compatible with the final GHC 8.2.1. I specifically added the CPP safeguards you see triggered above because early reports indicated that Stack may cause `cassava` to be configured incorrectly and then resulting in confusing compile errors which wouldn't obviously point to flags being handled incorrectly by the build-tool. In short, there's nothing wrong with `cassava-0.5.0.0`.
Thank you! 
Thank you!
Are you sure `Twist` satisfies [SP1](https://www.eyrie.org/~zednenem/2017/07/twist)? I haven't walked through the proof but after one application of `first'` the "extra data" gets swapped around and after two applications the "extra data" is back to where it started, surely? EDIT: Ah, maybe because it's two different "extra data"s each time! So the next extra data gets swapped into the same position the old extra data got swapped into, so it all works out. This is extremely counter intuitive! EDIT': I can't help thinking that `Twist` is a pathological instance, but I can't think of any law to rule it out!
Can someone explain the numbering to me? I thought 8.0.2 was the current ghc version. Why does it jump to 8.2.1? Or are those two version numbers somehow separate?
https://downloads.haskell.org/~ghc/8.2.1/docs/docs/html/users_guide/8.2.1-notes.html This link is in the email and is 404. The **right link for release notes** is: https://downloads.haskell.org/%7Eghc/8.2.1/docs/html/users_guide/8.2.1-notes.html For those who love to sit and read all the notes like me. :-) --- And https://www.haskell.org/ghc/ is 403 right now.
https://downloads.haskell.org/~ghc/8.0.2/docs/html/users_guide/intro.html#ghc-version-numbering-policy
Yes, see `splitL`. So all representable functors `u` are right adjoint to `(,) (Key u)`, and since adjoints are unique up to isomorphism, the Adjunction class is not really useful, you can always replace it with Representable. 
If `CoordF a` was split into `(Coord, a)`, we get the typical curry/uncurry adjoint back, where `type Rep Board = Coord`. I'd say that makes more sense than carrying around a semantically meaningless `a` within `CoordF`. I think most instances of `Adjunction` arise like this: 1. Define what `Rep u` should be 2. Find an encoding of `u` as data (e.g. `Board a`) 3. Use `(Rep u, a)` as the left adjoint Other than that, I really enjoyed this. For some reason, adjunctions never quite stay in my working memory long enough to solidify.
Or C, depending on interpretation. What I love is how HTML, SQL, and Cuda are lumped in with general programming languages. By their system of in-depth, well-reasoned analysis based on keywords in job ads, I'm pretty sure "is an equal opportunity employer" is the top programming language on Earth.
The release notes don't mention overloaded record fields (unlike the RC release notes and GHC status page). Anyone knows the status on this?
thank you so much. I got it now.
THank you!! 
The right link for &gt; Added support for Compact Regions, which offer a way to manually move long-lived data outside of the heap so that the garbage collector does not have to trace it repeatedly. Compacted data can also be serialized, stored, and deserialized again later by the same program. For more details see the GHC.Compact module. is https://hackage.haskell.org/package/compact-0.1.0.1/docs/Data-Compact.html Unboxed sums link here https://downloads.haskell.org/~ghc/8.2.1/docs/html/users_guide/glasgow_exts.html#unboxed-sums 
Ah, that confirms my thoughts https://www.reddit.com/r/haskell/comments/6ox9ev/adjunctions_and_battleship/dklm8k2/
edit: I worked it out - it's affected by your umask. Be careful if your umask isn't the default. (Arguably still counts as a bug, in that it installed most of the files world-readable but not this one.) --- Anyone else get this? $ ghci GHCi, version 8.2.1: http://www.haskell.org/ghc/ :? for help /usr/local/lib/ghc-8.2.1/package.conf.d/package.cache: openBinaryFile: permission denied (Permission denied) Installed from https://downloads.haskell.org/~ghc/8.2.1/ghc-8.2.1-x86_64-deb8-linux.tar.xz with `configure` then (as root) `make install`. Looks like it's only readable by the staff group which my normal user account isn't in. $ ls -l /usr/local/lib/ghc-8.2.1/package.conf.d/package.cache -rw-r----- 1 root staff 161490 Jul 23 10:02 /usr/local/lib/ghc-8.2.1/package.conf.d/package.cache (Everything else in that directory is world-readable - can I just chmod package.cache?)
I noticed that the announcement of the 8.2.1 release (yay!) linked to an FTP-like list of file downloads, not the usual GHC download page with user-friendly explanations. And the GHC homepage at https://www.haskell.og/ghc is currently returning a 403 Forbidden. Does this indicate that things are changing? If so - could we please have something friendlier than a 403 Forbidden to tell us about it? Thanks!
Looks like there's no default file being served. [It's still there though.](https://www.haskell.org/ghc/index.html)
Something's broken; we're investigating. Use https://www.haskell.org/ghc/index.html in the meantime.
looks like it's not properly signed: $ gpg --verify SHA256SUMS.sig SHA256SUMS gpg: Signature made Sat 22 Jul 2017 13:31:32 BST using RSA key ID 97DB64AD gpg: BAD signature from "Benjamin Gamari &lt;ben@well-typed.com&gt;" same for SHA1SUM ..
Any estimate for when Stack nightlies will switch to GHC 8.2.1?
I'd rather be almost broke and live on the cheap in the woods with my 4G dongle while doing some hobby Haskell dev—than make $150k/year in some soul-killing Java job in SF (where I'd probably still be living paycheck to paycheck anyway due to inflated real estate prices).
I get this: ./integer-gmp.cabal has been changed. Re-configuring with most recently used options. If this fails, please run configure manually. Configuring integer-gmp-1.0.1.0... ghc-cabal: Cannot find the program 'ghc'. User-specified path '/home/ben/bin-dist-8.2.1-Linux/ghc/inplace/bin/ghc-stage1' does not refer to an executable and the program is not on the system path. ghc.mk:986: recipe for target 'install_packages' failed on Ubuntu xenial (under Windows 10 WSL) where GHC has never been installed before. Any ideas?
You're a champ mate, keep it up. Edit: I should also add /u/bgamari to this comment, since he's also done a great job putting together this release
The "fractured situation" is su much less of a problem than you make it seem. Not to mention that you can't pick one type because Text and ByteString have different purposes, and then their lazy variants again have different purposes. When you don't care about performance, just use `string-conv` which provides a single function to convert from anything to anything.
I haven't used lenses so far, and I can't get some things from your article. For example, you define the following type for lenses type SimpleLens s a = forall f. Functor f =&gt; (a -&gt; f a) -&gt; s -&gt; f s and claim that the lens type for `Maybe` is `SimpleLens Maybe Int`. If I substitute type args into the definition, I'll get SimpleLens Maybe Int = forall f. Functor f =&gt; (Int -&gt; f Int) -&gt; (Maybe -&gt; f Maybe) But it makes no sense to me: f in (Functor f) has kind `* -&gt; *`, so the type argument of f must have kind `*`, not `* -&gt; *` as Maybe has. So I believe the correct type for such a "lens" should be `SimpleLens (Maybe Int) Int`. The same applies to some other examples (`SimpleLens Either Int` is wrong, `SimpleLens (,) String` should be either `SimpleLens (a, String) String` or `SimpleLens (String, a) String`, etc.). Furthermore, I decided to write a lens for `Maybe a` using your definition, and it doesn't seem possible: _just :: SimpleLens (Maybe a) a _just aToFa (Just x) = Just &lt;$&gt; aToFa v -- Ok, that was easy _just aToFa Nothing = ??? -- Oups... Have no value of a to get (f a) and apply fmap to it If I understand it correctly, we can't write a "lens" for `Maybe a`, we need "prisms" which are not mentioned in the article. The same applies to other sum types: list and either. Then why even mention them? As presented, lenses can only work for product types. Again, I could be wrong, because I've never used lenses nor prisms before. Thanks for the article, it definitely clarified some things for me.
I'm working on a ghc-related project and frequently needs to read the haddock pages of ghc api. The ghc api docs included in official ghc tarballs does not have hyperlinked haddock pages, so I built a hyperlinked version, in the hope that someone working with ghc may find it useful. The site will get updated every time ghc rolls out a new release. EDIT: The ghc team also uploaded their own hyperlinked version on Hackage [here](https://hackage.haskell.org/package/ghc). My site will still keep updated, but nice job ghc devs!
Text and ByteString co-existing is not a mess, they have different implementation and purposes.
Awesome, thanks a lot! Does anyone have any nice small examples of how one can/should be using the OverloadedLabels stuff as it currently stands? I played around with it briefly back around the first release candidate but found it didn't work as I had expected. And I notice it doesn't seem to be mentioned in the release notes. Isn't there a better place to look for a reference than [that old github thread](https://github.com/ghc-proposals/ghc-proposals/pull/6)? I got the impression from that thread that I should be able to get polymorphic record selector functions by defining the missing `IsLabel` instance. But I don't seem to be able to do that because `‘fromLable’ is not a (visible) method of class ‘IsLabel’`. A minimal example, or some general clarification would be really helpful. Those error messages are glorious though.
This doesn't sound great for locality, and also I think that if the program is hitting these 4K aligned addresses a lot they will be evicting each other from the cache constantly. I'm also not sure how this would play with thunks - would atomicModifyIORef still be possible? Basically doing this turns each dereferences of an Int into two loads from memory, not just one. The fast bit twiddling is unlikely to save much compared to the extra loads and cache misses. Interesting idea nonetheless, would like to see what others think. 
It's fromLab**el** not fromLab**le**
In the meantime, this `stack.yaml` works for me: setup-info: ghc: windows64: 8.2.1: url: https://downloads.haskell.org/~ghc/8.2.1/ghc-8.2.1-x86_64-unknown-mingw32.tar.xz sha1: bdede26c1a2cfcf4ebb08329853d285e32213b3d content-length: 175053796 linux64: 8.2.1: url: https://downloads.haskell.org/~ghc/8.2.1/ghc-8.2.1-x86_64-deb8-linux.tar.xz sha1: 3e52527de6f1cad6d7f1c392ddde8148116d4e36 content-length: 126809836 macosx: 8.2.1: url: https://downloads.haskell.org/~ghc/8.2.1/ghc-8.2.1-x86_64-apple-darwin.tar.xz sha1: 101405b4fef87a44adf34e48bb63a1a4e945c5c9 content-length: 114088292 compiler-check: match-exact compiler: ghc-8.2.1 resolver: ghc-8.2.1 
Ah, indeed! Thank you, now I see how to do it. If anyone else needs it, [here is my minimal example](https://gist.github.com/BlackBrane/cb51a61a10aea99b0916a81c8880c13b).
Note that it seems that I messed up the release notes links. They can be found here: https://downloads.haskell.org/~ghc/8.2.1/docs//html/users_guide/8.2.1-notes.html
The L1 data caches on modern CPUs are typically 4-way set-associative. That means that even if all 4k boundary addresses would fall into the same cachelines, you could still keep 4 of them in L1 cache. As for locality and number of reads, you always need two reads if you want both the payload and the header of for your object. Under the current system, the two reads are adjacent; under the proposed system, they are merely on the same page. That is probably at least a small downside. But does it outweigh, e.g., being able to keep twice as many pages on Ints in any level of cache? I doubt it, but only benchmarks can tell for sure. Finally, as for atomicModifyIORef and such, I plead ignorance and gratitude to those who actually understand such magic and can figure out whether it is (or can be made) compatible with my proposal.
Interesting; what binary distribution is this?
Neither