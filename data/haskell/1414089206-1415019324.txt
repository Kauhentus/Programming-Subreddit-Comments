Finding an internship would be great, but in the meantime you can help yourself. Find an open source project you're interested in and can contribute to that is fairly significant and visible to the community. Jump in and get involved. Talk to the maintainers, submit bug fixes, etc. That's a great way to gain the experience you'll need to get a Haskell job, and it's something that you can do today.
Yes, that much is obvious. But pivotcloud.com redirects to pivotmail.com, and alephcloud is a company that is still around and seems to have no connection to pivotcloud/mail.
Ah, I see your point now. Sure, that varies a lot across the field. I used to work in the HPC industry. I spent several months optimizing an already heavily optimized piece of software for a supercomputer. I managed to squeeze extra 4.5% of speed out of it, and my employer was very happy about it.
Why not submit fixes as patches to the async library? It's already so wildly used, and solves many of the same issues.
&gt; A monoid isn't necessarily commutative which is good as well, since product types and sum types are not exactly commutative, but only commutative up to a unique isomorphism. Non-commutative and coherently commutative are quite different things
I am a professional software engineer who works in (among other languages) Haskell, C, and C++. Specifically, I'm one of those individuals who needs to optimize code to run twice as fast (or faster). I don't think this has as much to do with the languages I use (since, for the most part, those are decided by whoever starts the projects I'm hired to optimize) as it does with the fields I work in (physics simulations, for example). Truthfully speaking, reducing cache misses without changing algorithms has never netted me more than 10x improvements. Generally it's closer to the 2-4x you mention. That's still a lot when you're talking about an exploratory simulation that takes 10 CPU days to run; however, the primary gains come from algorithmic optimizations that become simple once you have your data arranged as a contiguous data structure. There's a lot of cache waste (and pointer redirection time) from looping over an array of objects and pulling out one variable. The cache waste is almost totally alleviated by looping over an array containing only the variable from all the objects. Additionally, given such an array, it's usually relatively simple to implement a SIMD algorithm to provide additional performance gains. Application of all these techniques in concert has, on at least one occasion, netted me 47x speed improvements in code which formerly had runtimes measured in CPU weeks and months. For my clients, that's the difference between doing groundbreaking science and running an expensive room heater. While I'll agree that's it's generally unimportant if, e.g., `cat` runs in 0.2s or 0.4s, it's a somewhat different story in the world of simulation.
But this means that `tail` allocates!
Sounds like an exciting field to work in!
 withContents inFile $ \str -&gt; writeFile outFile str More generally, you'd have to consume the `str` acquired by `withContents` in its entirety within the block it delimits.
I honestly don't see how withContents inFile $ \str -&gt; writeFile outFile str is any different from readFile inFile &gt;&gt;= writeFile outFile ? Could you enlighten me?
Because the former API will more clearly communicate to the user that `inFile` should not equal `outFile`
I'm not sure what you mean. I don't think I claimed otherwise (I didn't mean to anyway). I'm just saying that it wouldn't satisfy commutativity up to Haskell's `==` (like most of these kinds of typeclass laws are defined) even though it is commutative up to unique isomorphism. It does satisfy associativity exactly though, as required by the laws of Haskell's `Monoid`. Actually, it might be a `Semigroup`, since it doesn't satisfy the unit laws exactly (again, only up to unique isomorphism). And looking at this again, this wouldn't be at a value level *anyway* so it wouldn't work with those type classes regardless.
I was surprised when it was a couple orders more difficult than arch for me. 
A. so? B. Not necessarily. GHC is pretty good at eliminating unnecessary allocations- if, for example, the tail value is immediately matched against itself, then GHC will elide the allocation. There are problems with this code- consider the following: filter :: (a -&gt; Bool) -&gt; List a -&gt; List a filter f lst = view (\ x xs -&gt; if (f x) then cons x (filter f xs) else filter f xs) [] lst The problem with my definition above is that evaluating the first element of the list effectively evaluates the whole list to at least weak head normal form, implicitly. This is due to the pattern matching in cons. Which is a problem if you try to do a filter on an infinite list. So this exact code doesn't work, but something like this might.
I never use list comprehensions and just use `do`. I would prefer list comprehensions to go away in the cause of reducing the complexity of the language.
Souce code on [Gituhub](https://github.com/niilohlin/Boids)
Anytime I've been in a situation where I need to use a list in a monadic way I tend to use either &gt;=&gt; or do notation. While writing code I usually don't remember they're a thing.
IMHO list-comprehensions can lead to more readable and concise code if you need to construct something in the style of "all xy for which x in z, and y in zz where zzz", possibly paired with pattern matching. Also, I find it quite convenient to write `[ x | y ]` when I need it, instead of the rather lengthy `(if y then [x] else [])`
Because those are not fixes, but a different approach in whole.
Sorry for any confusion. The company which was formerly called AlephCloud is now called PivotCloud. PivotMail will be our first product.
+1 for [ x | y ]. I've never seen that before.
Notes from the talk are posted [here](http://blog.timsears.com/posts/im_trying_nix.html).
Aha, so http://www.alephcloud.com/ is the same company as pivotcloud. Ok. Must be a very very new name change! :)
My view may be biased because I used to teach Haskell to 1st year colege studentsm but I use list compreensions often, particular for "generate and test" kind of problems. Sometimes I prefer pointfree style, sometimes do-notation. All are idomatic Haskell in my view.
CPP is needed for the preprocessing (#ifdef etc).
I've seen that idiom used with [monad comprehensions](https://ghc.haskell.org/trac/ghc/wiki/MonadComprehensions) to great effect, as it can then work for arbitrary `MonadPlus` instances: f :: Int -&gt; Int -&gt; Maybe (Int, Int) f n m = [ (n, m) | n `mod` m == 0 ] It's not a commonly-used idiom, though, so it might be more confusing than useful.
I rarely use it, but it is very helpful to pull out all data of a single constructor without defining a special function.... If you have "data Pet = Cat String | Dog Int", the list of Cat strings in [Pet] would be [name | Cat name &lt;- theList]
You're relying on `fail`, that's nasty.
yes, and for large n you can apply [Stirling's approximation](http://en.wikipedia.org/wiki/Stirling%27s_approximation). Namely ln(n!) ~ n ln n
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Stirling's approximation**](https://en.wikipedia.org/wiki/Stirling%27s%20approximation): [](#sfw) --- &gt; &gt;In [mathematics](https://en.wikipedia.org/wiki/Mathematics), __Stirling's approximation__ (or __Stirling's formula__) is an approximation for [factorials](https://en.wikipedia.org/wiki/Factorial). It is a very powerful approximation, leading to accurate results for even small values of n. It is named after [James Stirling](https://en.wikipedia.org/wiki/James_Stirling_(mathematician\)). &gt;The formula as typically used in applications is &gt;&gt; &gt;==== &gt;[**Image from article**](https://i.imgur.com/AxhQWDf.png) [^(i)](https://commons.wikimedia.org/wiki/File:Stirling%27s_Approximation.svg) --- ^Interesting: [^James ^Stirling ^\(mathematician)](https://en.wikipedia.org/wiki/James_Stirling_\(mathematician\)) ^| [^Factorial](https://en.wikipedia.org/wiki/Factorial) ^| [^Gamma ^function](https://en.wikipedia.org/wiki/Gamma_function) ^| [^Lanczos ^approximation](https://en.wikipedia.org/wiki/Lanczos_approximation) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cli2p1e) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cli2p1e)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
Perhaps I am misunderstanding, but where am I using fail? The expression I gave will filter out all the "Dog" elements and only return the strings in the Cat elements.
List comprehensions are actually not defined in terms of `fail`; does that change your opinion? If so, it shouldn't! There's nothing wrong with using `fail` in a particular monad where it makes sense, like a monad for backtracking search or for parsing. The trouble with `fail` is that it doesn't make sense for all monads, or at least doesn't "mean the same thing" for all monads, which makes it problematic to use in an overloaded setting. That's why there is the proposal to split it out into its own class `MonadFail`.
`liftA2 (,) xs ys`...? 
Interesting, didn't know that. Although, even if list comprehensions don't use `fail` directly, they should behave in the same way as the corresponding do notation. So in some sense that's still "relying of `fail`". I'm not a big fan of `fail`, in or out of `Monad`. I mean, does the `String` argument really make sense? To have such a weird argument that most instances ignore seems very unhaskelly to me.
Yeah, that is true. Often what I realize is that when you embrace the "everything is a stream" philosophy. You quickly run into circular dependencies. Which solutions makes code which are hard to reason about (in my opinion).
Same. It's the whole "one configuration file for everything" that messes me up, I think. And the fact that arch with the aur turns the entire package manager into a "install everything I can think of, then keep it updated" for me whereas nix doesn't have nearly as many packages, or as intuitive an interface 
I still think you guys might be talking about "&lt;-" like it works in the do monad, where "Just x &lt;-" fails if it matches Nothing. In a list comprehension, this is not an error at all, it just doesn't match (ie- [x | Just x &lt;- [Nothing]] = []).... Given this, what could possibly fail in [name | Cat name &lt;- theList] at runtime? Am I confused here?
that's a a little unfair isn't it? :P that's like saying that someone is expected to eat like a mass murderer does and breathe like one, so they are basically a mass murderer :p
This is why laziness and AFRP seems to be the right way to go in my opinion. 
And I'm sure the guy who ends up maintaining your code won't have either. And I'm sure he'll hate you.
yeah I can just be like "oh chances are official or AUR will have it", nixos just feels like i'm getting lucky or SOL
Oh, I'm sure he would. I wouldn't use that kind of thing without commenting it, so maybe that defeats the purpose. Still kind of neat though I think.
&gt; There are some problems with the above code (mainly unintentional strictness), but the core idea, I think, is sound. This basically becomes a "sufficiently smart compiler" problem, but I think Haskell may make this doable. I'm pretty sure it is impossible to do anything like this in Haskell because of the strictness issue you noted, but it should be possible to implement something along these lines in the GC, which can look at a heap object representing a list and determine whether it's a constructor or a thunk.
do you know if there's a similar SF group, maybe also at the Pivotal Labs here? :)
Everywhere the value fails to match `Cat name`.
Like I've stated twice above, this isn't an error in list comprehensions.... Try it. It fails in monad do notation, but works in list comprehensions. In fact this is the part of the point of using list comprehensions, to filter out cases that don't match. There is no error.
My grief with monad comprehensions is that you cannot mix the monads, i.e. list and maybe. You end up wrapping singleton values in lists.
I did not say `error`, it is just that a particular input fails to contribute to the result list. `fail` is a somewhat controversial method of the `Monad` class, as it has no (nice) mathematical properties. It is somewhat pragmatic. Look at the implementation of `fail` in `Maybe`.
This is actually why I moved to Arch from Slackware. Slackware will always be my first love, but the amount of time I was spending on package management was just unjustifiable.
Wait, Monad Comprehensions are back? Awesome: I had no idea! There have been quite a few times when I wanted to write a short comprehension instead of a do block.
&gt; Am I confused here? Yes you are. List comprehensions are just another syntax for monads. IOW, the following behaves exactly the same way: do Just x &lt;- [Nothing]; return x As well as the following: [Nothing] &gt;&gt;= \case Just x -&gt; return x; _ -&gt; fail ""
Perhaps it depends what type of code you are writing. For mathematical code, list comprehensions are a no-brainer. To me, list comprehensions feel like the more declarative way to express things. But I guess if you're using Haskell as "the world's best imperative language", that might not be a consideration.
I heard quite some research on Nix(OS) happened on the Tech Uni of Delft (The Netherlands). In Dutch "niks" (pronounced as "nix") also means nothing.
 [MyThing (x,y) (x^2) (y^2) | x &lt;- [startX..endX], y &lt;- [startY..endY], x &lt; someFunc y] And the alternative isn't so readable anymore.
Just stumbled upon this, nothing new ... but well written.
I'd prefer the IMHO more readable comprehension. But other than creating indices I rarely use them.
Why would you want to use such an approximation in this case?
I'm not claiming it's an argument. It's just my opinion that this syntax sugar doesn't add anything significant over `do`.
Rather than thinking about putting this into compiler passes, should I keep this in mind as a pattern for manual optimization? I think I'm asking if people typically use this approach when they want to squeeze a constant factor out of their code.
In german a "Nixe" is a mermaid. 
Why did you check in the executable?
Yes, but that's not the desugaring. From the [Haskell 98 report](http://www.haskell.org/onlinereport/exps.html): [ exp | pat &lt;- list, REST ] desugars to let ok pat = [ exp | REST ] ok _ = [] in concatMap ok list No use of `fail` anywhere. Whereas do pat &lt;- list REST desugars to list &gt;&gt;= (\tmp -&gt; case tmp of pat -&gt; do { REST } ; _ -&gt; fail "some error message") 
To me, list comprehensions feel more declarative. Map and filter still sound like "do something to this list".
A thought that just occurred to me: Is it "expensive" to express ideas via types in imperative strict languages? Just yesterday I refactored a conversion tool at work. Basically it reads a file into a vector of euler angles (`FilePath -&gt; IO (Vector Euler)`), converts these into quaternions (`Euler -&gt; Quat` + `fmap`), does some processing on them (`Quat -&gt; Quat` + `fmap`) and writes them to the network (`Vector Quat -&gt; IO ()`). Basically in Haskell I can write each stage separately, chain the calls and due to fusion can expect this to work reasonably fast and probably in constant memory. Thing is: this project is written in C++. I'd like to write some functions ala `std::vector&lt;euler&gt; read_euler(std::string path)`, std::vector&lt;quat&gt; euler_to_quat(std::vector&lt;euler&gt; eulers)`, etc ... but that way I'll produce several intermediate datastructures that each will be evaluated fully and use memory before the next step. I spend some time looking up alternatives like `std::iterator`, `boost::stream`, `boost::fusion` ... but neither really fit the bill. Now I should probably just *use the languages as it is expected to be used* ... and put everything in one big loop thereby throwing out all that "carefully planned" typing that went in there in the beginning.
By the time I finish typing all that out, I'd rather just fix my imports to all be explicit ;).
Is it a symptom of a broken module (type class?) system that we have the same names being exported from multiple modules? 
I feel like I've wasted half my life for not knowing this.
I don't think so. The IMHO right solution in this case would be to prefer the function with the fitting type and only throw an error if there're two functions with the same name and type. This proposal feels more like a workaround.
Why do you say that? I can only see the opposite - if names are globally unique you don't need a module system at all. Even if we had a nice module system and some of these names could have a single common declaration in a module signature, you'd still have multiple implementations of that signature to choose between.
Incredible... This is really cool! Prelude&gt; data Foo = Bar | Qux deriving (Show) Prelude&gt; let x = [Bar, Bar, Bar, Qux] Prelude&gt; let y = [z | z@Bar &lt;- x] Prelude&gt; y [Bar,Bar,Bar]
...tab-completion, does your editor do that? =)
If you've two fitting functions then it's better to just throw an error, instead of having some more or less "magic" resolution.
I recently discovered that [Reactive](https://hackage.haskell.org/package/reactive), an old (maybe even the first?) FRP library, [doesn't compile on GHC 7.0 and above](https://github.com/gelisam/frp-zoo/commit/684e81c687c608afaae44b544625e6fd0d3d36c0). That's a good sign that the library is unmaintained! In that commit message I identify one of the reasons for the compilation error (a dependency defines an instance which clashes with one of its imports). The dependency is marked as deprecated, so the solution probably isn't to fix the dependency, but to remove the dependency between Reactive and category-extras. Does that sound like the difficulty level you are looking for?
The style guide I use for my projects says "never use list comprehensions"
I don't think a project has to be unmaintained to learn from someone else's code. Being unmaintained may even have the opposite effect: instead of having to match the style of the existing authors to get your PRs merged, you'd be able to write code your way without anyone else reviewing it. 
To conclude that O(n log n) and O(log(n!)) are equal. In fact n log n = log(n!) + O(n).
No. Naming things is hard and programmers are lazy. Even outside of programming languages, we often have the same symbol refer to different things depending on context. When programmers get added, overlapping names is a natural consequence of decentralized naming. `unionized :: Person -&gt; Maybe Union` (for a CRM) and `unionized :: Solution -&gt; Solution` (for some chemistry library) are both fine functions, but according to string comparison, they have the same name.
Haha true :) *(I'm half German)*
You can't mix the monads in a `do`-block either.
&gt; when the invalid pattern matches were interpreted as mzero and there was no fail There was such a magic time? It's before I learned Haskell. [I want to go to there.](http://www.reactiongifs.com/wp-content/uploads/2013/01/go-to-there.gif) 
You can write in Agda, but the built-in termination checker won't accept it (since it's independent in MLTT). So he sticks in an assert, on the grounds that you can give a model where it holds. 
The blog is called "Antiblog", the name itself is though-provoking.
How exactly does it 'say nope to mutable configurations'?
Yep ... Problem is that afaik there is no reasonable way to write `iterator do_stuff(begin-iterator, end-iterator)`. (And the data is given to me in a std::vector ... So there is no chance of streaming at all)
Well you can easily just write the functions Euler -&gt; Quat and Quat -&gt; Quat in C++ and then simply iterate over the vector applying them to one element at a time, if you allocate the result vector in advance it should be reasonably fast. The difference between haskell and C++ in this case is that you are in principle able to carefully tune how you want to implement "fmap" and have to do the fusion fmap (g . f) = (fmap g) . (fmap f) yourself.
Agreed. A couple of other reasons: 1. there's no one to ask why things have been done that way and not the other 2. an unmaintained project is likely to be old and have unidiomatic code by today's standards, or have accumulated too much cruft over the time. So, if your goal is to learn, I'd recommend to pick a moderately young and active project. It's also more fun that way.
Grab an open source project of a decent size and try to add a fairly large feature to it. The key here is it won't be your code so you will be reading and debugging someone else's work. This should help you understand what could really happen in a production scenario. There are Haskell consultants out there BTW. Fewer, sure.
Right now I'm reading a paper on implementing probabilistic functional programming using a probability distribution monad, which is a kind of list monad. The authors use list comprehensions I think to great effect, to clearly show the calculation of probabilities for dependent events. It seems like using functions would not be quite so clear and succinct.
the downside of that is though that you need to bracket `x` and `y` (or the full expression) yourself if needed. IOW, `[ x | y ]` is shorter grammar for `((x) &lt;$ guard (y))`, when `x`/`y` are compound expressions.
As for #2, it's worth thinking about whether this is a trust issue with Haskell itself, or the aesthetics that often accompany Haskell code. I've observed that I can let [Hython](https://github.com/mattgreen/hython)'s context leak completely out of my brain, and then come back and be up and going very quickly, mostly due to the type system. It helps that I have an automated test suite, for sure, but types catch many more errors in thinking and prevent them from getting farther than that. I can code late at night, or very early (5 am), and still get things done. Actually, it is very possible to build software in other toolchains that doesn't have the typical BS you see from most 'industrial' software replete with 'best practices,' but it is rare, even among the cognoscenti. The meme of throwing hordes of OSS libs at a problem seems to have seriously maimed design and architecture. But that's not here or there. All I'm trying to say is that you might have a warped perception of what's normal, presuming that you need to fiddle with things to get them to work.
I think there are two things here. The first is, is the language capable of production work. And the answer at this point is clearly yes (even if it is only used by a reasonably small number of shops, it is used, and at reasonable scale to validate that it's not falling down). There is a little disconnect between building a toy app and building something for real (for example, the type system cannot and is not intended to catch all bugs, so you should still be testing). The second is does it have tooling / conventions that get developed over the course of doing production work. And I think in this case the answer is... not yet, really. As a simple example, the jury is still somewhat out as to how you actually run an application in production. Some people are betting on Nix, others on Docker, and other people are just building on production instances (which is a bad idea, due to the high ram usage of linking), or on machines running the same system as production (whether virtual or not-in-use production instances), and some people have figured out how to get it running on Heroku (but that solution seems to me worse than the problem!)... But all of this is more complicated because most web development has been in dynamic languages, where this is a non-issue (and Go side steps this by having completely static linking). As for your last concern, there are definitely people you can throw money at. Here's a starting point: http://www.haskell.org/haskellwiki/Consultants
But I can't just return an iterator because an iterator without an end iterator is useless (?). And returning `std::pair&lt;std::vector::iterator, std::vector::iterator&gt;` ... That will probably confuse the users. In the end I will probably just return a vector of processed data without giving the user the choice to "stream" the data.
This is a good idea. I had been wanting to do this for other reasons, but hadn't considered that it would help me solve this conundrum as well. Thanks.
&gt; All I'm trying to say is that you might have a warped perception of what's normal, presuming that you need to fiddle with things to get them to work. This is certainly a part of the problem. I think I have had more psychological conditioning than I would've thought.
There are name conflicts between the Prelude and other modules in base. As far as I can tell, that's the only reason things like this tend to come up.
If you're more comfortable when projects have a lot of boilerplate, start a project with `yesod init`. You'll get at least a few hundred lines of scaffolding from that :)
Tell me about your father...
The reason I prefer the auto-hiding thing is that type-based name resolution is a bigger engineering problem (you don't want to have to type check for multiple sets of identifiers), and doesn't handle the problem of wanting to more easily use more general functions than in the Prelude, which I think is a more common problem than wanting to use the same identifier for two things in one module. Though my mind may change as things get more statically typed. It's kind of weird you can't have a Monoid instance for vectors, because the result type changes based on the sum of the length of the input vectors. I'd hate to have to use Vector.&lt;&gt; and Nat.+ all over the place. But since the only thing the current system can do is throw an error even if you explicitly imported the more general one, it also seems obvious in retrospect (to me anyway) that GHC should already prefer the one you imported by name. This just seems sensible independently of type-based name resolution. Disclaimer: I wrote the proposal
What are you business ventures? Perhaps people can give some suggestions based on specifics.
They're not really justified, given the massive amount of Haskell code out there now. We know it works. 
I think the biggest thing most likely to trip up a large project is missing an occasional library. The key skills you should cultivate all relate to working around this including: * How to write bindings to an equivalent C library * How to parse and render specialized file formats * How to shell out to scripts in other languages * How to communicate over network/sockets to other services/processes written in other languages * How to write libraries yourself If you cultivate those skills you will be fine
Wow, that is actually a pretty cool trick.
Great list, thank you.
You're welcome!
&gt; Thing is: this project is written in C++. I'd like to write some functions ala std::vector&lt;euler&gt; read_euler(std::string path), std::vector&lt;quat&gt; euler_to_quat(std::vector&lt;euler&gt; eulers)`, etc ... but that way I'll produce several intermediate datastructures that each will be evaluated fully and use memory before the next step. What you're looking for is a [better collections design](http://julesjacobs.github.io/2014/10/18/the-best-collections-library-design-2.html), one that can pipe computations without building intermediate structures.
Done /u/ReinH
To be fair, the post was written before Haskell 2010 when (n+k)-patterns were removed. GHC switched the default from enabled to disabled around version 7.4 (~December 2012), so the code was valid for 5 years after being written...
If you somehow value complexity over simplicity, then that's a problem for sure :-) The number of times I go 'Why in Church's name is this so complicated?!?' when programming Java is directly correlated to the amount of Haskell I've written recently - I still have to find out which way the causation runs, though.
What about `std::transform(v.begin(), v.end(), ostream_iterator&lt;quat&gt;(sout), op)` where `v` is the provided `std::vector&lt;euler&gt;` object, `op` is a function object that is the composition of your `Euler -&gt; Quat` and `Quat -&gt; Quat` functions and `sout` is as ostream provided by your TCP library? That's all C++98. C++11 lets you do op as a lambda, if you want, and might (not sure) have a standard library for function composition. C++11 would also give you rvalue-references, which could prevent some `quat` copies that the C++98 solution might make difficult to work around. It doesn't stream from the file, but the file read is already complete by the time you get a `std::vector` object. It does stream to the network.
I'm not sure about the Applicative/Monad part, but this looks like a problem for GADTs. Very simple example: {-# LANGUAGE GADTs #-} data E a where V :: a -&gt; E a Add :: (Num a) =&gt; E a -&gt; E a -&gt; E a Mult :: (Num a) =&gt; E a -&gt; E a -&gt; E a App :: E (a -&gt; b) -&gt; E a -&gt; E b instance Functor E where fmap f a = V f `App` a instance Applicative E where pure = V (&lt;*&gt;) = App instance Monad E where return = pure m &gt;&gt;= f = ??? I'm not sure your monad instance makes sense for App, since the function part of the AST should have a different type than the argument, so I don't think you can naively recurse on both arguments to App. Monads generally have the property that they're hard to statically analyze without running them. If I'm understanding what you're trying to do then m &gt;&gt;= f should create an AST that when evaluated: evaluates m, uses that value and f to create a new AST, and then evaluates that AST. If that's what you're trying to do I'd look at free monads, but if you want a LISP style AST that's easy to traverse I'm not 100% sure monads are the right tool. TemplateHaskell QuasiQuoters might be another alternative depending on what you're trying to do. There was a post recently that gave some good examples of QuasiQuotation. http://www.reddit.com/r/haskell/comments/2jowjc/new_blog_post_quasiquoting_dsls_for_free/ http://www.reddit.com/r/haskell/comments/2jyl0z/gadts_and_defining_functor/
&gt; an iterator without an end iterator is useless (?). Sometimes, an input or output iterator without the matching end is useful (it's an "infinite" source / sink abstraction). In general though, yes. You either need a end iterator, a count, or (for input iterators) the elements themselves must have some way of indicating the "end". The standard algorithms are (I think) all of the first two types and not all of them have a "counted" variant.
also this one: http://www.meetup.com/haskellhackersathackerdojo/
true, but I don't think that short was really what I was going for. it was more like...working in normal function application syntax with idiomatic functions without invoking special syntax without need.
Yeah. They dropped it though, because it was confusing the newbs. Sadly.
The difference is that list comprehensions don't really add much over `do` notation. Other syntax sugar actually improves the readability of the code, but I don't feel like list comprehensions do.
There is a surprising amount of support you can get for free from the Haskell community. When you need more reliable support, you can turn to companies like FP Complete and Well Typed. You won't have to spend "buckets of money" for the level of support that an "individual" business venture requires.
&gt; I am currently looking at different FRP implementations to decide which one to use in my next project. I see you already know about [FRP zoo](https://github.com/gelisam/frp-zoo), since you commented on the post in which I announced it. Its goal is precisely to be a resource to help programmers like you compare the different FRP implementations available. Was it useful in that regard? Was there anything I could have added to make it more useful?
&gt; I'm not sure your monad instance makes sense for App It's an implementation of substitution. When using the Bound library, the type of `a` in `E s a` isn't the type of the expression, but the type of the free variables (i.e. `Void` when there are no free variables, `Either () Void` when there is only one free variable, and so on).
Ah, well isn't my face red.
While I can't answer the question, I believe I can clarify it. The code is using the Bound library, which requires the expression type to be parameterized not by the type of the expression is it computing, but by a type representing the set of available free variables. For extra clarity, here is an example expression datatype parameterized by the type of the expression which is being computed: data Exp t where TrueLit :: Exp Bool FalseLit :: Exp Bool IntLit :: Int -&gt; Exp Int IsZero :: Exp Int -&gt; Exp Bool and here is an expression datatype parameterized by the set of available free variables: data Exp name where Var :: name -&gt; Exp name IntLit :: Int -&gt; Exp name App :: Exp name -&gt; Exp name -&gt; Exp name Lam :: Exp (Maybe name) -&gt; Exp name The two kinds of implementations help eliminate different types of bugs: the first makes ill-typed terms unrepresentable, while the second kind of implementation makes unbound variable errors unrepresentable. The Bound library helps with the second kind of implementation. I think the OP is asking for a way to mix both kinds of implementations, in order to avoid both kinds of bugs. This desire is quite understandable, but I think it's going to be tricky: we can no longer use a single type variable with *n* inhabitants to represent a scope with *n* free variables, because that's no longer enough information! We now need a way to represent the context as a list of types, to indicate which type each variable has. It's certainly possible to do so, but it complicates variable handling, and probably isn't compatible with Bound: data IntType = IntVar data ArrowType a b = ArrowVar data Var context t where VarZero :: Var (Either t context) t VarSucc :: Var context t -&gt; Var (Either a context) t data Exp context t where Var :: Var context t -&gt; Exp context t IntLit :: Int -&gt; Exp context IntType App :: Exp context (ArrowType a b) -&gt; Exp context a -&gt; Exp context b Lam :: Exp (Either a context) b -&gt; Exp context (ArrowType a b) 
no i mean because he can just apply to jobs in random places, he has demonstrated the value in knowing many languages
How about package and version management? With late binding and dynamic typing, you can often get away with interoperating between multiple versions of the same library.
Do worry, I wasn't trying to diminish it's value. It's just that once I saw that I then looked at the url and noticed it was from 2007.
Not unfounded, sane and reasonable concerns. A year ago, I had to pick a language to bet my company on (I am a founder) and despite being very infatuated with Haskell, we went another way. 1. Chicken/Egg problem, no way out of it except more Haskell. 2. ... and sometimes too hard, cabal issues, nasty pragma behaviors, pathological lazy evaluation issues that have to be hand tuned, hard to reason about at times... 3. This is the really critical one, but it is directly tied to point 1, and there is no shortcut out of it... what makes this even more challenging is the spin up time for new developers. If you hire someone who doesn't know (Python|Go|Ruby|Java), it is likely they can become usefully productive in short order. Haskell on the other hand has a rough learning curve for a lot of reasons, and hiring a good developer in another language and hoping they can learn is not practical. 
Typo in your Haskell code btw; `Strig` should be `String`.
How about `PreferExplicitImports`?
Thanks! That fixed both the Python and Haskell example. Here's the final code: import Data.Map (Map) import Network.HTTP.Client import Network.HTTP.Client.TLS import Data.Aeson getJSON :: IO (Maybe (Map String String)) getJSON = withManager tlsManagerSettings $ \manager -&gt; do req &lt;- parseUrl "https://www.bitstamp.net/api/ticker/" fmap (decode . responseBody) (httpLbs req manager) This correctly retrieves the JSON as a map: &gt;&gt;&gt; getJSON Just (fromList [("ask","356.96"),("bid","356.02"),("high","363.71"),("last","356.96"),("low","344.34"),("timestamp","1414191872"),("volume","16073.49084934"),("vwap","355.89")])
&gt; I am able to create software more easily, and it seems much more reliable, and this worries me. This is what it's like to actually program, rather than bang your head against your programming language. :)
Indeed, that sounds complex… I’m not even sure about how I should handle the problem since it seems I have to make a choice: type bugs or binding bugs.
Playing around with the code from the blog post I ended up with the following. `fromJSON` is not happy because the data is a string not numbers. So I parse it using `read`. import Prelude hiding (last, lookup) data Ticker = Ticker { ask :: Double, bid :: Double, high :: Double, low :: Double, last :: Double, volume :: Double, timestamp :: String, vwap :: Double } deriving Show mkTicker :: Map String String -&gt; Maybe Ticker mkTicker m = do ask' &lt;- lookup "ask" m &gt;&gt;= return . read bid' &lt;- lookup "bid" m &gt;&gt;= return . read high' &lt;- lookup "high" m &gt;&gt;= return . read low' &lt;- lookup "low" m &gt;&gt;= return . read last' &lt;- lookup "last" m &gt;&gt;= return . read volume' &lt;- lookup "volume" m &gt;&gt;= return . read timestamp' &lt;- lookup "timestamp" m vwap' &lt;- lookup "vwap" m &gt;&gt;= return . read return Ticker { ask = ask', bid = bid', high = high', low = low', last = last', volume = volume', timestamp = timestamp', vwap = vwap' } getJSON :: IO (Maybe Ticker) getJSON = withManager tlsManagerSettings $ \manager -&gt; do req &lt;- parseUrl "https://www.bitstamp.net/api/ticker/" let req' = req { checkStatus = \_ _ _ -&gt; Nothing } res &lt;- httpLbs req' manager print $ responseStatus res return $ case (decode . responseBody) res of Nothing -&gt; Nothing Just value -&gt; mkTicker value In real code I would name the Ticker values differently.
You can't make something like that a `Monad`, but you can make it more of an indexed monad. This is the answer I think you are really looking for. http://lpaste.net/79582
Use Wreq, no need to go that low level. 
For what it's worth, I'm currently reading Bird &amp; Wadler's [Introduction to Functional Programming](http://www.amazon.com/Introduction-Functional-Programming-International-Computing/dp/0134841972/ref=sr_1_2?s=books&amp;ie=UTF8&amp;qid=1414198939&amp;sr=1-2&amp;keywords=introduction+to+functional+programming) and they use list comprehensions quite heavily. It's from 1988 though... :-)
 {-# LANGUAGE OverloadedStrings #-} import Network.API.Builder import Control.Applicative import Data.Aeson import Data.Monoid import Text.Read data Ticker = Ticker { ask :: Double , bid :: Double , high :: Double , low :: Double , last :: Double , volume :: Double , timestamp :: String , vwap :: Double } deriving (Show, Read, Eq) instance FromJSON Ticker where parseJSON (Object o) = Ticker &lt;$&gt; (read' =&lt;&lt; o .: "ask") &lt;*&gt; (read' =&lt;&lt; o .: "bid") &lt;*&gt; (read' =&lt;&lt; o .: "high") &lt;*&gt; (read' =&lt;&lt; o .: "low") &lt;*&gt; (read' =&lt;&lt; o .: "last") &lt;*&gt; (read' =&lt;&lt; o .: "volume") &lt;*&gt; o .: "timestamp" &lt;*&gt; (read' =&lt;&lt; o .: "vwap") parseJSON _ = mempty bitstamp :: Builder bitstamp = basicBuilder "bitstamp" "https://www.bitstamp.net" route :: Route route = Route ["api", "ticker"] [] "GET" -- this could just be an empty path if we tacked /api/ticker to the end of the url getTicker :: IO (Either (APIError ()) Ticker) getTicker = execAPI bitstamp () $ runRoute route -- read' :: Read a =&gt; String -&gt; Parser a read' :: (Monoid (m a), Read a, Monad m) =&gt; String -&gt; m a read' x = case readMaybe x of Just r -&gt; return r Nothing -&gt; mempty ghci &gt; getTicker Right (Ticker {ask = 355.23, bid = 355.02, high = 363.71, low = 344.34, last = 354.97, volume = 14801.77232647, timestamp = "1414203177", vwap = 355.69}) `read'` is super-generalized b/c it wasn't worth importing `Data.Aeson.Types`, but this is a general idea of how simple it is to get a nice rest client working with my library [`api-builder`](https://github.com/intolerable/api-builder)
&gt; data Remote :: (* -&gt; *) -&gt; * -&gt; * where &gt; Var :: f a -&gt; Remote f a &gt; Lit :: Lit a -&gt; Remote f a &gt; Lam :: Scope (Equal b) Remote f a -&gt; Remote f (b -&gt; a) &gt; Let :: Vec (Scope (Ix bs) Remote f) bs -&gt; Scope (Ix bs) Remote f a -&gt; Remote f a &gt; Ap :: Remote f (a -&gt; b) -&gt; Remote f a -&gt; Remote f b Nice!! I'll have to study how that works.
[SDL2](http://hackage.haskell.org/package/sdl2) and [SFML](https://hackage.haskell.org/package/SFML) exist. I have used both and prefer SDL2.
Currently there's a new set of SDL2 bindings being rewritten by some haskell-gamedev devs, you can check it out [here](https://github.com/haskell-game/sdl2).
Sounds like a classy dude.
 import Network.Wreq import Control.Lens main = do r &lt;- get "https://www.bitstamp.net/api/ticker/" print (view responseBody r)
One thing that you get for free in dynamic language is what pluggable/optional type systems try to give you: being able to code in different styles in different programs. For example, parametric polymorphism and subtyping don't interact very nicely with each other so most static languages support only one of them or support both at the cost of making the type system much more complicated. In a dynamic language you have the freedom to use subtyping in one program and parametric polymorphism in another - as long as you take care so that the two programs don't mix then there won't be tricky variance interactions there to bite you. Another thing that dynamic systems make easy is those programs where you really want to add dynamic tags to your program (the sort of thing where you would use Typeable in Haskell). For example, its much more natural to manipulate JSON in a dynamic language, because you can use heterogeneous lists and hash tables directly. In a static language you need to wrap everything in a new JSONValue data type, which is mostly just going to add extra noise.
Vectors are not lazy.
Chucklefish uses Haskell with SDL2.
Gloss is a simple graphics library with mouse and keyboard functionality, but it currently has the downside of being incapable of being profiled by GHC.
You forgot Cabal Hell. Guess what I've been up to today. Also poor Linux support. Good luck getting ghc going on RHEL5 which we're stuck with until 2017.
I am still trying to figure out what the submission does. [shake-install](https://github.com/alphaHeavy/shake-install) is a shake build tool that can read cabal files.
If you need 3d `not-gloss` is a 3d version of gloss. Although I have to add that both favor ease of use over performance. (Now that I read it, the first sentence looks like it should be "eta reduced": `why not: "not-gloss is a 3d version"`)
There seems to be a fairly new effort to make game dev in Haskell better: https://github.com/haskell-game/
[HOAS](http://en.wikipedia.org/wiki/Higher-order_abstract_syntax) + [RMonad](http://hackage.haskell.org/package/rmonad) 
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Higher-order abstract syntax**](https://en.wikipedia.org/wiki/Higher-order%20abstract%20syntax): [](#sfw) --- &gt; &gt;In [computer science](https://en.wikipedia.org/wiki/Computer_science), __higher-order abstract syntax__ (abbreviated __HOAS__) is a technique for the representation of [abstract syntax trees](https://en.wikipedia.org/wiki/Abstract_syntax_tree) for languages with variable [binders](https://en.wikipedia.org/wiki/Name_binding). &gt; --- ^Interesting: [^Generalized ^algebraic ^data ^type](https://en.wikipedia.org/wiki/Generalized_algebraic_data_type) ^| [^Nominal ^terms ^\(computer ^science)](https://en.wikipedia.org/wiki/Nominal_terms_\(computer_science\)) ^| [^Abstract ^syntax](https://en.wikipedia.org/wiki/Abstract_syntax) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+clj9xx5) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+clj9xx5)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
I don't know much about SFML, but SDL isn't a game engine. Game engines have a lot more things built-in. They should at very least handle the main loop.
We have run into issues at work, where having a larger community might have found and fixed them before we had to. That is not the typical experience. Usually we find that there is some other tool, in a different language, that is broken in some way. We look at it an think, "between autoparsec and STM we could fix rewrite this in Haskell pretty easily." and many times we do. You probably will roll more of your own infrastructure in Haskell, but it will have a different character than doing it in another language. It won't seem hard. 
Some background on the topic: pretty much all the existing game engines are *very* procedural or object-oriented in their design. This is of course possible to work with in Haskell, but it's not ideal. So there's been some research on how to make functional game engines, and one of the more promising approaches is called *functional reactive programming* which has a few implementations in Haskell. There's a [presentation by Evan Czaplicki that gives a quick overview of three kinds of FRP](https://www.youtube.com/watch?v=Agu6jipKfYw). There are other approaches to the problem which I wish I knew more about, but alas I don't. My main point is that *we don't really know yet* what works best, which might explain why there hasn't been any big efforts to write huge game engines like Unreal for Haskell yet. But given any game engine in C, you can always write imperative bindings for it. *And* if you discover a nice way to interface with game engines in pure functional programming, you've done the whole programming world a big favour. You've actually discovered something really big and important.
`shake-cabal-build` builds shake build systems using cabal sandboxes. You basically write a cabal file describing your build system and its dependencies and the script calls `cabal install` in a sandbox and executes the compiled executable. Users of your build system then just have to install the Haskell platform and the script takes care of the rest.
Indeed, the `-p` option works for me. I'm on OS X.
I think [procedural programming works well in Haskell with lenses](http://www.haskellforall.com/2013/05/program-imperatively-using-haskell.html).
I'd think someone should also mention Oliver Charles's blog posts on [Netwire and SDL](https://ocharles.org.uk/blog/posts/2013-08-01-getting-started-with-netwire-and-sdl.html) and in particular [Asteroids &amp; Netwire](https://ocharles.org.uk/blog/posts/2013-08-18-asteroids-in-netwire.html).
You can make it work *as well as* any other procedural language, but that still doesn't mean it works well. ;)
+1 for Gloss, it is really easy to use. I recently posted a game I wrote in Haskell. Here's the game framework I built for it: https://github.com/egonSchiele/actionkid It has a video walkthrough here: http://vimeo.com/109663514 Here's a game I made with it: https://github.com/egonSchiele/chips
Thanks for the link. I'm still using sdl1, because there was only a low level sdl2 wrapper last time I looked.
I've never used it and don't know what state of readiness it's in, but [LambdaCube](https://github.com/csabahruska/lc-dsl) exists. 
I watched a video recently by someone who had worked on FRP in Haskell (Yampa IIRC) and acknowledged it had some performance problems. He reimplemented FRP for C++. There had been earlier work that pretty much translated the Haskell, and that preserved the performance issues, but by building it from scratch for C++ he avoided those issues. Unfortunately I've lost the link to the video, and am not willing to watch several hour-long videos just to find this again ATM. There's a couple of videos from C++now 2014 that cover functional reactive programming (by David Sankel) and I think one of those is it, but I'm not 100% sure. Best guess is [this one](http://www.youtube.com/watch?v=tyaYLGQSr4g&amp;list=PL_AKIMJc4roXG7rOmqsb_wDG1btCzhS8F&amp;index=39). My best guess from what I half-remember is that the performance issues relate to laziness. I know that GHC does its best to optimize that away, but it cannot always do so, and of course if it tried even harder it would be even harder to predict when that optimization would happen and when it wouldn't. That's not really a problem in most applications, but for performance-sensitive real-time applications it can save up work for the worst possible time, and also turn a simple incremental CPU calculation into a huge fragmented data structure traversal. Mostly that should be easy to prevent with explicit strictness, but "mostly" isn't "always" when dealing with things the other side of a non-trivial layer of abstraction. FRP *is* a layer of abstraction. Of course Haskell can be effectively turned into a strict language by adding strictness annotations everywhere, but there's good reasons to prefer not to do that. I don't know the internals of FRP libraries, but as streams are frequently mentioned, it seems like laziness is a fundamental part of the usual Haskell realization. My guess is therefore that the C++ version takes a different approach, avoiding laziness by using the "wormholes" that are mentioned (which may even be a way of targeting a kind of limited laziness-emulation at key points). Personally, pervasive laziness is still one of the worries I have about Haskell. I know about the advantages and the compiler optimization, and I'm a lot more comfortable with it than I was, but I'm probably heading to a point where I'll eventually be right on the fence - I don't think laziness is ever going to completely win me over. I'm fine using it for the things I use it for, but I'm even learning Prolog right now - those things aren't high-end games or other real-time applications. High performance real-time is still an area where I have doubts about Haskells suitability. I know about those trading systems, but they have different trade-offs to a high-end game. 
Note that the FRP language in the parent post, Elm, has strict evaluation. 
I'm actually working on that right now. It's not ready for prime time yet, but it will be soon. It's basically an abstraction for input and rendering, with support for glfw and ios through opengl (and sdl or whatever could easily be added). And then it also has utility functions and abstractions for handling menus, multi touch dragging/zooming, texture loading, and text rendering. The actual game logic is free form; just implement it as a pure transformation of your model. The docs are still a bit sparse, and the example game isn't currently building, but I am actively working on it. Maybe I'll fix up the example this weekend. https://github.com/asivitz/Hickory
Is Merge associative? Could you make Builder a monoid using Merge and Id? 
I understand this is a project but I would just use HashMap module functions to do this stuff? 
just as a note, you can do simple game development with f# and xna, but there is also the recently announced mono for unreal, which would allow you to target the unreal engine with f#. :)
Whenever I think of the builder pattern I also think of comonads, but I'm not sure how practical that is http://www.haskellforall.com/2013/02/you-could-have-invented-comonads.html
Can you explain? (I'm a relative Haskell beginner and aeson-t is my uneducated experiment)
After a quick look, I'd say I would first look at lens-aeson for such needs. Seems to cater very similar needs.
Thanks, fixed.
Dependent types also give you more "permissive types": f(x) = if x &gt; 3 then "hello" else 42 f : (x:Int) -&gt; (if x &gt; 3 then String else Int) There usually isn't a clear line between more permissive and more expressive (where I assume by more expressive you mean that they give you the ability to encode more invariants in the type system). Most of these extensions give you both. &gt; An F# type provider, again, you only get half the notion with a dynamic language. And you get neither half in a statically typed language without type providers... Sure, in a statically typed language with an IDE that provides autocomplete, that autocomplete also works for type providers. &gt; I don't even know how to think about "higher kinded" types in a dynamic context [... ] the important part is the ability to do class and/or typeclass based ad-hoc polymorphic dispatch on them. You can encode type classes with dictionary passing style even in a dynamically typed language.
on all three points: 1) there's a pretty clear line between permissive and expressive. permissive says "more things check" and expressive says "i can be more granular about what things _don't_ check." 2) You can easily get unchecked dynamic method access in a static language -- just use strings. The _key_ feature of type providers is about IDE integration. Looking at them in an untyped context misses this point. 3) Yes, and you can encode type classes with dictionary passing style in a statically type language too, and get the same effect. I'm not trying to have a dynamic/static flamewar here. I just want to be clear that some features are intrinsically _about rich types_ and describing them in a dynamic setting misses the point. You might argue the _purpose_ of these features can in a sense be realized in a dynamic setting, via different means. But that's a different discussion.
This isn't necessarily directed at you but I hate the term "performance-sensitive". Because that is not what the problem with games in Haskell is at all. If that were the case, it would be much easier to solve as Haskell code can yield very high performance if done right. Instead they are latency-sensitive. This is a much harder problem to solve as the GC simply cannot give latency guarantees (at least the one as used by GHC today cannot). In addition it is stop the world for the most part.
OP is actually asking for a framework though, all they want is graphics and input handling.
1) Yes but the point is that you can't classify those extensions as permissive vs expressive. 2) Type providers aren't about dynamic method access, they are about dynamic method definition. e.g. you generate a class with methods from a database schema. That has been a standard technique in dynamic languages for a long time, and I would be surprise if that wasn't exactly what inspired type providers in the first place. 3) No, encoding type classes as dictionaries in a statically typed language doesn't give you higher kinded types. &gt; You might argue the purpose of these features can in a sense be realized in a dynamic setting, via different means. But that's a different discussion. That is *exactly this* discussion. Look at what my comment was a reply to: "I'm trying to collect a list of things that are "harder with static types" and eventually write about them. Please let me have any suggestions!"
 GHCi, version 7.6.3: http://www.haskell.org/ghc/ :? for help Loading package ghc-prim ... linking ... done. Loading package integer-gmp ... linking ... done. Loading package base ... linking ... done. C:\Users\Gurkenglas\Desktop\Haskell\bitstampticker.hs:2:8: Could not find module `Control.Lens' Perhaps you meant Control.Seq (from parallel-3.2.0.3) Use -v to see a list of the files searched for. Failed, modules loaded: none. Prelude&gt;
My argument is that these aren't just "harder with static types" but they are also "meaningless without static types".
I use [this guide](https://github.com/bitemyapp/learnhaskell) to teach Haskell. The primary way I recommend people learn Haskell is by doing the [cis194](http://www.seas.upenn.edu/~cis194/spring13/index.html) course. I linked the older one because the newer one isn't done yet. The follow-up to `cis194` that I recommend is the [NICTA course](https://github.com/NICTA/course/).
Sounds like a bug. If you can submit it with all the information you can muster, that would be cool: https://ghc.haskell.org/trac/ghc/newticket?type=bug
I did. https://ghc.haskell.org/trac/ghc/ticket/9715
For building up your practical chops, I'd recommend doing the CIS194 course next. For building up your theory, I'd recommend Thinking Functionally using Haskell (based on the previous version and a quick look at the new version). I'm not aware of anything that covers comonad and lens in book form. I'm not sure what your specific learning goals are. My generic advice would be to write a lot of code and to get really comfortable with applicative / foldable / traversable / monad transformers and possibly free monads first. I've introduced the above things (and comonads) at work recently in a single, large pull request, so I've been thinking about the order and importance of these things so I can have whiteboard discussions about them throughout the week. Hopefully that advice translates / is helpful in some way. This is in scalaz though, otherwise there would be some lens in there as well :) 
 &gt; cabal update &gt; cabal install wreq &gt; cabal install lens
I don't know if it covers comonads or lens, but Real World Haskell is very good. It has a lot of practical information (hence the name), and the online version has a lot of fairly good reader comments throughout that you can toggle on most sections. On mobile now, will post link later.
What I suggest to people depends crucially on their background, their goals, and how self-directed they are. Below I outline just a few of the paths I could recommend if I knew more. If your goal is comonads and lens then in my opinion it's worth it to learn some basic category theory from a book. I like Awodey's (pronounced like audi) book, but it is written for people with a strong mathematical background. Conceptual Mathematics also teaches category theory but assumes less about the reader's background. If you're comfortable reading research papers, then I highly recommend reading the old proceedings from ICFP, for instance you can get the 2014 proceedings here: http://icfpconference.org/icfp2014/program.html In recent years the talks have even been on youtube. For a nice intro to the power of comonads in Haskell (in a research paper format) I point people to this paper: http://www.cl.cam.ac.uk/~dao29/publ/codo-notation-orchard-ifl12.pdf Simon Peyton-Jones has some good material on his website: http://research.microsoft.com/en-us/people/simonpj/#current He gives excellent talks including this talk about lens: https://skillsmatter.com/skillscasts/4251-lenses-compositional-data-access-and-manipulation Unfortunately skillsmatter requires a login to view that talk, which I think is a huge disservice. If you're not comfortable reading research papers or just want some other suggestions, then I'd follow /u/dalaing's advice. If you use IRC then ask lots of questions in #haskell on freenode (more info on the sidebar). I hope that helps!
Yes. Using closed type families (ghc-7.8.1), you can write: type family Remove (l :: [*]) (t :: *) :: [*] where Remove (t ': l) t = l Remove (x ': l) t = x ': Remove l t
Have you looked at [HGamer3d](https://hackage.haskell.org/package/HGamer3D)? [Here's](https://www.youtube.com/watch?v=v_GSbObYRkY) an example of a game written using it.
I press the x in the corner of the window.
The Glorious Glasgow Haskell Compilation System User's Guide is good as a second book just after LYAH, seriously. And after it Haskell wiki can be read straight through (with occasional code from GitHub here and there) without the need of a separate book.
I really liked "Real World Haskell" (a lot of practical problems), "Haskell: School of Music" (unfinished, but available on author's website). There are also nice articles on the web on specific practical programming topics: "Tackling the Awkward Squad", "How to write a financial contract". [All these titles are Google-able to free online books or PDF articles.] Finally you might read Chris' Okasaki "Purely Functional Data Structures", if you are interested in algorithmic analysis.
Interesting: OS X also has an x button, but in gloss windows this button is disabled! Anyway, is the .prof file still empty when you quit by pressing Escape?
I've seen a heap of the lens tutorials, and there's lots of them that I recommend to folks. I'm just of the opinion that there's probably some other things worth covering between LYAH and lens. 
I shelled out the twenty bucks for [Thinking Functionally in Haskell](http://www.amazon.com/Thinking-Functionally-Haskell-Richard-Bird-ebook/dp/B00O0RKGTO/ref=tmm_kin_swatch_0?_encoding=UTF8&amp;sr=&amp;qid=) (oh shit, it's gone up to $37). Can't say I'm disappointed. Good combination of exercises and deep thoughts.
Yes
Agreed.
It was downvoted because some redditors live in fantasy land and scoff at the notion of Windows being the main enterprise and development OS for the world.
The Typeclassopedia http://www.haskell.org/haskellwiki/Typeclassopedia
A quick youtube search gave me [this](https://www.youtube.com/watch?v=NBKnY7Z_w3I&amp;list=PLPqPwGvHPSZB-urE6QFjKYt6AGXcZqJUh). I'm not sure if this is what you are looking for though.
Thanks. I did see those videos but, if I understand correctly, they are meant to supplement and not replace LYAH, no?
This [Guide to getting started with Haskell](http://www.haskell.org/pipermail/cvs-ghc/2010-February/052606.html) on StackOverflow is very complete.
I don't see the purpose of the Builder type. Couldn't you have type Builder = Value -&gt; Value ?
that link doesn't work.
Standard RWH disclaimer: this book is pretty old and parts of it might not represent the current state of the art.
Something that doesn't really go into the details, but I definitely recommend having at hand, is [What I Wish I Knew When Learning Haskell](http://dev.stephendiehl.com/hask/). It's less of a guide/course/book, but it gives an overview of most of the things (libraries, extensions, other tricks) that there are in Haskell.
Ugh... all the good jobs are in the US :(
This is the somewhat extended blog post version of http://www.reddit.com/r/haskellgamedev/comments/2fhfmi/extensible_entities/ . Feel free to comment.
My guess is that you need the actual code to look at while listening to something like this.
The principled way is to just pattern match on the constructor: data Dynamic where Dynamic :: Typeable a =&gt; a -&gt; Dynamic apD :: Typeable f =&gt; (forall a. a -&gt; f a) -&gt; Dynamic -&gt; Dynamic apD f (Dynamic a) = Dynamic $ f a
I'm pretty sure that a lot of the latency issue has to do with not tuning the default context switching rate. 
Wow, that was easy. :) Thanks! Looks like I asked my question not completely correctly to the problem I have. What about this: d1 = toDyn [1,2,3 :: Int] d2 = toDyn [1,2,3 :: Double] apD2 :: (Num a) =&gt; ([a] -&gt; a) -&gt; Dynamic -&gt; Dynamic apD2 f d = undefined And I would like to run "appD2 sum d1" which equals to "toDyn (6 :: Int)" and "appD2 sum d2" which equals to "toDyn (6 :: Double)". Is this still possible?
Is there a typo? Seems to not typecheck with the error: `f' is applied to too many type arguments In the type signature for `apD': apD :: Typeable f =&gt; (forall a. a -&gt; f a) -&gt; Dynamic -&gt; Dynamic Edit: It does typecheck in 7.8.3 but not in 7.6.x .
&gt; Edit: It does typecheck in 7.8.3 but not in 7.6.x . That’s the difference between the kind-polymorphic and the non-polymorphic `Typeable`. In the latter case `Typeable f` implies `f :: *`, hence the error.
+1 for reading online w/ reader comments! Very useful!
This is very nice. I found it easier to understand than typical component system tutorials. And it makes extensible effects understandable too.
Turns out this is also pretty simple: apD2 :: (forall a. (Num a, Typeable a) =&gt; [a] -&gt; a) -&gt; Dynamic -&gt; Dynamic apD2 f (Dynamic v) | t == typeOf (undefined :: [Int]) = Dynamic $ f (unsafeCoerce v :: [Int]) | t == typeOf (undefined :: [Double]) = Dynamic $ f (unsafeCoerce v :: [Double]) where t = typeOf v apD2 _ _ = Dynamic () However, is this possible to do it generically for all types?
I'm not sure it could be simplified like that. Builder recursively alternates between two tasks: moving to a new node in the input, and constructing a node of the output. When it is recursively evaluated it keeps track of its position(s) in the input which makes the DSL pretty short (no need to pass locations around as arguments). For instance, here are two examples of converting ungainly JSON responses from a array-column format to nicer objects. How would you specify them if Builder was Value -&gt; Value and atIndex etc were functions? At "data" $ Map $ Obj $ H.fromList [ ("insight", AtIndex 0 $ At "data" $ Keep ["id", "title"] ) , ("user", AtIndex 1 $ At "data" $ Keep ["id", "email"] ) , ("media", AtIndex 2 $ Obj $ H.fromList [ ("src", At "data" $ Attr "image") ] ) , ("created_at", Index 4) ] Next example At "data" $ Map $ Merge (AtIndex 0 $ At "data" $ Keep ["image", "thumb", "id", "title", "text", "keyphrases", "never_forget", "context", "type", "brief"]) (Obj $ H.fromList [ ("divisions", Index 15) , ("created_at", Index 6) , ("tags", Index 7) , ("out", Index 8) , ("in", Index 9) , ("last_comment", Index 13) , ("media", AtIndex 2 $ Map $ Attr "data") , ("brands", Index 14) , ("projects", Index 1) , ("fields", Index 11) ]) 
Those examples would look exactly the same, only with "at" instead of "At" etc. For example, here's an implementation of "at": type Builder = Value -&gt; Value at :: Key -&gt; Builder -&gt; Builder at k b (A.Object o) = b (o H.! k) Essentially, each case of your transform function could be made into a regular (Value -&gt; Value) function with no loss of expressivity or generality -- right now you're creating a data structure for no reason. 
Oh I see, nice. And I could get rid of the transform function too. So my next question is would lenses suffice as other people are suggesting? Can you specify transformations as succinctly with lenses and HashMap functions by themselves?
What if you know for sure the list inside Dynamic has elements with Num a? Here is an example of dynamically constructed Monoid: https://github.com/ekmett/reflection/blob/master/examples/Monoid.hs So I was wondering if this can be done similarly by using reflection package. I'm interested in implementation of pandas like dataframe, i.e. be able to parse a file, automatically determine types during parsing and hide values behind Dynamic, show the data in ghci without specifying types of results, and since you see the data (i.e. you know that column has type Double) you can apply polymorphic functions on top of it expecting Dynamic Double in return.
Let's say we have: type Col = Dynamic And it contains (Data.Vector.Unboxed.Vector a). And if you have an ability to run polymorphic function "Data.Vector.Unboxed.sum" on Dynamic internals - then I believe this will be faster than sum type approach you suggested. I.e. basically my philosophy is that such DataFrame can be treated like any other external untyped (to Haskell) storage, like database. You can run a fast "query" specific to your "database" and get results back to Haskell (for instance, converting them back to types similarly how postgresql-simple does that).
Parallel and Concurrent Programming in Haskell doesn't cover the topics you're looking for but it's an excellent second book: http://chimera.labs.oreilly.com/books/1230000000929/ In particular it does a great job covering the basics of how Haskell's non-strict evaluation works and what weak-head normal form and normal form are right off the bat.
Thanks! Do you have an example of such a component system tutorial? My guess is that while the general architecture is quite easy it becomes quite hard as soon as you begin to search for data structures for individual applications. I probably should have wrote that in a "where to next" part.
 Prelude&gt; take 10 $ (concat . map (sequence . flip replicate "01")) [1..] ["0","1","00","01","10","11","000","001","010","011"] That's not really counting in binary though.
 Prelude&gt; take 10 $ (map ('1':) . concat . map (sequence . flip replicate "01")) [0..] ["1","10","11","100","101","110","111","1000","1001","1010"] Maybe you were thinking of this?
&gt; if you have an ability to run polymorphic function "Data.Vector.Unboxed.sum" on Dynamic internals - then I believe this will be faster than sum type approach you suggested. This is a pretty grand claim, which I challenge. Do you have any data suggesting that `Dynamic` is faster than a sum type? This would be a very surprising result.
Re 3, I invested *a lot* of time learning Haskell, being a professional Java/C++ programmer, probably 1-2 years. I think that the traditional "they can learn Haskell" will not work, but I also bet that doing intensive pair-programming sessions would have cut the time I needed to invest learning Haskell to a fraction of what I have spent. Going back and forth between intensive pair-programming and reading would have helped immensely for me at least. I think I could have done it in a few months inside of an organization. 
How does this work?
&gt; flip replicateM "01" =&lt;&lt; [1..] That's not really counting, though. But it's close. This is counting: "0" : map ('1':) . flip replicateM "01" =&lt;&lt; [0..]
I've fixed all the tests. And added some new benchmark results for the Parsec version.
Maybe [this comment](http://www.reddit.com/r/haskell/comments/28zx87/whats_your_favorite_response_to_the_show_me_the/cig59kz) I made 4 months ago in a thread about short, magical-looking Haskell snippets? &gt; replicateM 3 "01" ["000","001","010","011","100","101","110","111"] 
I doubt if I've seen anything you haven't, sorry.
I guess it's good to have this recorded here, but in general this kind of question is more fun on #haskell. Anyway, here's my solution: map (flip (showIntAtBase 2 intToDigit) "") [read"01"..] EDIT: Or in light of the [current list comprehension thread](http://www.reddit.com/r/haskell/comments/2k50tx/are_list_comprehensions_idiomatic_in_modern/): [showIntAtBase 2 intToDigit x "" | x &lt;- [read"01"..]] 
As you can see from the various responses, it is a matter of taste whether they are good style. Personally, I like them when to my eyes they make code more readable, which is a bit more than half the time for the list monad. Monad comprehensions don't interest me.
&gt; What if you know for sure the list inside Dynamic has elements with Num a? Here is an example of dynamically constructed Monoid: https://github.com/ekmett/reflection/blob/master/examples/Monoid.hs Okay, that constructs an instance from a dictionary of methods. How do you plan to obtain this dictionary? &gt; parse a file, automatically determine types during parsing and hide values behind Dynamic Ah, now this sounds much more reasonable! Do all the types recognized by your parser have a `Num` instance? If so, you could store the dictionary along with the dynamic value, and then later on you'd be able to re-construct the `Num` instance from the dictionary. Or, even simpler: you could use an existential type, it will store the dictionary for you. data NumValue = forall a. (Show a, Num a) =&gt; NumValue a data NumList = forall a. (Show a, Num a) =&gt; NumList [a] parse :: String -&gt; Maybe (NumList) parse "1.0,2.0" = Just (NumList [1.0, 2.0 :: Double]) parse "2,4,6,8" = Just (NumList [2, 4, 6, 8 :: Int]) parse _ = Nothing numPrint :: NumValue -&gt; IO () numPrint (NumValue x) = print x apD2 :: (forall a. Num a =&gt; [a] -&gt; a) -&gt; NumList -&gt; NumValue apD2 f (NumList xs) = NumValue (f xs) 
My TV runs NixOS but at work it's not up to me.
My gut instinct says "yes".
Ha! Can you explain const [True, False] :: a -&gt; [Bool] For me? My brain is not processing that at all. Edit: sleepy me forgot that const just ignores the second argument. Always evaluate a function instead of doing it in your head kids. 
[True, False] is of course just a list value; and const [True, False] is just the function that ignores its argument and returns that value! Pretty simple. Any weirdness in this powerSet function is coming from elsewhere...
Thanks! I need some time to understand your answer regarding why it is impossible to run polymorphic function over Dynamic when you know for sure you can run it over its internal type... Initially when I looked at Edward's answer on SO question where he says "we can abuse my reflection package to lie about a TypeRep" I thought may be we can do something like that here, but since you didn't react on my Monoid URL example, looks like it is also impossible or not applicable here. I will need some time to understand this as well. :)
In the first example under Working with Worlds, can the big type be inferred from `display :: IM.Key -&gt; Position -&gt; Color -&gt; IO ()`? Very cool idea! Thanks
Great read, really illustrates how higher level functions have no singular method of implementation. As a beginner this is huge for identifying those similarities - thanks!
You're welcome!
Mostly this was what I already assumed the compiler was doing. except for: &gt; List comprehensions are equivalent to do notation: which is really interesting and helpful. I know at some point I used a monadic do-block to compute a cartesian product, but didn't really understand why it worked. So this explains alot! Thanks!
Lens can do this (of course): [paste](http://lpaste.net/113245). Might be faster if you can stick to a single monad transformer.
Sorry for the slow reply - I haven't been on Reddit for some time. In case the slides weren't enough: there is indeed a phase distinction, it's just not reflected in the syntax of the language. In a language like Haskell, everything to the right of a `::` is compile-time, and everything to the left is runtime. The type/term distinction carries over nicely to a compile-time/runtime distinction. With dependent types, there are certainly separate times at which you typecheck your program or run your program. I can typecheck once, then compile the program and run it 100 times on different input. It does require a bit more work to tease them apart, however, because you can't just rely on syntactic categories.
We’re just exploiting the monad instance for lists. (=&lt;&lt;) is just an infix concatMap, and (replicateM n xs) will enumerate all the lists of length n whose elements come from xs (i.e. the result has (length xs)^n elements).
I have a [list of reading material](http://reinh.com/notes/posts/2014-07-25-recommended-reading-material.html) you might enjoy. Thompson's book has a 2011 edition that is great as well.
There's no such thing as a free lunch. On the other hand, if you've been overpaying for your lunch the whole time then the *actual* cost of lunch may indeed seem suspiciously low
no doubt I'm interested in mathematics, but please go easy on me. My background is not mathematics -- as much as I wish it was. If I were to read a book on category theory, it would have to be written for the layman. 
I was mostly referring to the Haskell section, but Lawvere's Conceptual Mathematics is a great introduction to CT that is extremely approachable by a layperson, with very few prerequisites.:)
I fixed the part about `let` in `do`
If by "big type" you mean the the types for the individual `get`s, then yes.
&gt; "Of course" ;) Actually I have tried this before, but from a different angle. I was trying to somehow derive the vector-based world state automatically (like `Vector (a,b)` does) and then provide actual entities through lenses. That didn't work quite well ;)
While all of these are true in spirit, the real world is a bit more complicated. E.g., de sugaring of pattern matching with nested patterns. 
I was about to say the same, but I don't think you can write `addProperty` quite as nicely.
`const` is easy: const :: a -&gt; b -&gt; a const a _ = a (weird spacing may or may not increase clarity, Idunno) meanwhile, this is `filterM`: filterM :: (Monad m) =&gt; (a -&gt; m Bool) -&gt; [a] -&gt; m [a] filterM _ [] = return [] filterM p (x:xs) = do flg &lt;- p x ys &lt;- filterM p xs return (if flg then x:ys else ys) so if we insert `const [True,False]` for `p` we get -- return empty list for empty list filterM _ [] = [] filterM (const [True,False]) (x:xs) = do -- p x = const [True,False] x = … flg &lt;- [True, False] -- recursion for xs ys = filterM (const [True,False]) xs -- will be done once for every value flg -- can extract; in our case this is twice: -- once for True and once for False, -- i.e. take both branches; both keep x and -- throw x away return (if flg then x:ys else ys) 
Do you mean "using a better algorithm" or "implement this specific algorithm better"? In the second case, not particularly; you can change some things around, but this is a perfectly fine implementation. In the first case, see http://en.wikipedia.org/wiki/Primality_test.
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Primality test**](https://en.wikipedia.org/wiki/Primality%20test): [](#sfw) --- &gt;A __primality test__ is an [algorithm](https://en.wikipedia.org/wiki/Algorithm) for determining whether an input number is [prime](https://en.wikipedia.org/wiki/Prime_number). Amongst other fields of [mathematics](https://en.wikipedia.org/wiki/Mathematics), it is used for [cryptography](https://en.wikipedia.org/wiki/Cryptography). Unlike [integer factorization](https://en.wikipedia.org/wiki/Integer_factorization), primality tests do not generally give [prime factors](https://en.wikipedia.org/wiki/Prime_factor), only stating whether the input number is prime or not. Factorization is thought to be a computationally difficult problem, whereas primality testing is comparatively easy (its [running time](https://en.wikipedia.org/wiki/Run-time_complexity) is [polynomial](https://en.wikipedia.org/wiki/Polynomial_time) in the size of the input). Some primality tests *prove* that a number is prime, while others like [Miller–Rabin](https://en.wikipedia.org/wiki/Miller%E2%80%93Rabin_primality_test) prove that a number is [composite](https://en.wikipedia.org/wiki/Composite_number). Therefore the latter might be called *compositeness tests* instead of primality tests. &gt; --- ^Interesting: [^Fibonacci ^number](https://en.wikipedia.org/wiki/Fibonacci_number) ^| [^AKS ^primality ^test](https://en.wikipedia.org/wiki/AKS_primality_test) ^| [^Miller–Rabin ^primality ^test](https://en.wikipedia.org/wiki/Miller%E2%80%93Rabin_primality_test) ^| [^Fermat ^primality ^test](https://en.wikipedia.org/wiki/Fermat_primality_test) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cll06bi) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cll06bi)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
As a semi-beginner. Seeing that typeclasses get desugared to records make me question: Why the hell not use records in the first place? Now we moved our typeclasses to value-level. Freakin' cool! Why does haskell have a typeclass system if it has records?
Yes, the types get very complicated: :t positions . at 1 ?= V3 0 0 0 positions . at 1 ?= V3 0 0 0 :: (HasPositions s a, MonadState s m, At a, Num (Index a), Num a1, IxValue a ~ V3 a1) =&gt; m () Also, isn't the typeable instance unnecessary in GHC 7.8? edit: Seems so: [effin](https://hackage.haskell.org/package/effin). Wonder how it compares performancewise.
I don't know why you need to go through the process of translating an image to a Bytestring, JuicyPixels use a storable vector to be able to transfer data to C APIs easily. The OpenGL data await a `Ptr a`, and, the `unsafeWith` function from vector allows you to get... a `Ptr a`. If you want to import a JuicyPixel image to OpenGL, please follow the [StackOverflow answer](http://stackoverflow.com/questions/10468845/juicypixels-texture-loading-in-haskell-opengl), not the version shown in the blog. As a side note, there is a toll free function in JuicyPixel to import an image from Cairo: [imageFromUnsafePtr](http://hackage.haskell.org/package/JuicyPixels-3.1.7.1/docs/Codec-Picture.html#g:15), no copy involved. But as usual, if you mess with the image size, you'll obtain garbage output. You can always reorder the pixel components afterward.
You can stop at the square root of k. Proof is left to the reader.
What type would you give to `id'`?
Is the list comprehensions section accurate? From the report: [List Comprehensions](https://www.haskell.org/onlinereport/haskell2010/haskellch3.html#x8-420003.11)
I C what you did there.
Where can one find a desugared Haskell? Coming from Scala there's a close correspondance to desugared Haskell (certainly for pattern matching, do notation, and lambdas). The "language" syntax therefore clicks immediately. Otherwise, as a beginner Haskell syntax is quite baffling with the sugar on top. The linked breakdown should help, however, thanks for this OP.
`id' :: a -&gt; a`? Which is the intuitive approach of course. I assume, to give the type based on the code, which would infer to `id' :: Bool -&gt; a`, which the conflicts with `id' 3`. Which is confirmed by testing it with GHCI.
Ah ... I wasn't aware of that package.
Are you generalising the type there? Are you saying `id' :: forall a. a -&gt; a`? Because allowing arbitrary generalisation makes type inference intractable or possibly undecidable. Type inference works by generating unification variables for each type. `id'` would get the type `α -&gt; α` and then when you apply it to a Bool it would unify α with Bool, and then substitute the with the resulting unifier `α := Bool`, giving `id'` the type `Bool -&gt; Bool`. If you were to attempt a polymorphic generalisation at the application point, it would give `id'` the type `forall a. a -&gt; a` and then the example would typecheck. The whole point of let syntax being baked into HM languages is to allow for a syntactic indication of where generalisation takes place. 
&gt; Recursive let / where [...] should be treated as a primitive. I think it's better not to think of these as primitive and to *think* of them as defined in term of something like `Data.Function.fix` (with some tupling on top). AFAIK, it's difficult to define `Data.Function.fix` without a single-variable recursive let / where, but that one special form is enough to give you all the rest.
I think you meant to replace `(x,y)` with `(y,z)` in the list comprehension code.
One reason is that typeclasses allow the compiler to automate the record passing for you, which both saves programmer time and sometimes gives nice theoretical guarantees.
&gt; Why does haskell have a typeclass system if it has records? Nicer surface syntax, particularly for `(+)`, `(*)`, and `(/)`. If the `Num a` argument was a record rather than a typeclass instance, it would have to be explicitly passed. You can get some of that back via implicit parameters, but that's a GHC extension that isn't exactly popular. Also, typeclasses really have a different meaning than just an implicit parameter of a (parametric) record type, even if that meaning isn't implemented *completely* accurately. Specifically, type classes introduce the global uniqueness assumption. This allows (e.g.) Data.Set to be efficient (or at least more efficient than without the assumption). But, if global uniqueness is violated, Data.Set may "break" in unusual ways.
I expect that it's mostly because it's a common pattern enough pattern that you want to reify it, and it's convenient. Which are the same reasons I'd suggest are behind us having do notation.
So `let` introduces an implicit `forall`?
Yep, precisely. And lambdas do not. So if you replace a let by a lambda but the body of the let uses the bound variable at two different types then your code will fail.
You could use records (and factory functions) and implicit parameters to replace type classes, and it's actually done in Agda, Scala etc. However, the real power of type classes is the instance selection/creation algorithm. Here Haskell has a different approach in that it only allows exactly one globally matching instance (in contrast to using normal scoping rules). This choice has both good and bad consequences.
Just for the sake of experimentation, you could do it explicitly in lambda's using something like `(\id' -&gt; (id' True, id' 3)) ((\x -&gt; x) :: forall a. a -&gt; a)`?
I would need to check if type inference can handle this case. If I wanted to be 100% sure I would have put the type annotation on the `id'` binding instead of on the `(\x -&gt; x)`.
I will fix the part about let to add a caveat about generalization, but I probably can't get to it before tonight.
Here is a simple implementation of the Miller-Rabin primality test: isSpsp :: Integer -&gt; Integer -&gt; Bool isSpsp n a = let getDandS d s = if even d then getDandS (d `div` 2) (s+1) else (d, s) spsp (d, s) = let t = powmod a d n in if t == 1 then True else doSpsp t s doSpsp t s | s == 0 = False | t == (n-1) = True | otherwise = doSpsp ((t*t) `mod` n) (s-1) in spsp $ getDandS (n-1) 0 isPrime :: Integer -&gt; Bool isPrime n = let ps = [2,3,5,7,11,13,17,19,23,29,31,37,41, 43,47,53,59,61,67,71,73,79,83,89,97] in n `elem` ps || all (isSpsp n) ps You might enjoy the essay [*Programming with Prime Numbers*](http://programmingpraxis.com/essays/) at my blog.
As someone who is a relative newbie to Haskell, why would I use Idris? Is it an alternative? Is anyone using it in production?
Me too :). But they're probably more appropriate to have in the Idris reddit: http://www.reddit.com/r/Idris/
I think it's possible with [2..(floor . sqrt . fromIntegral) k]
I do not mean to insult. Could you provide some examples where the Haskell syntax was confusing? I came from C -&gt; Python background and found it was pretty easy to pick up. What took me a while to learn was the behavior of lists in a monadic context. Maybe the community could incorporate some better descriptions based on your concerns into the haskell wiki book or other introductory texts. 
How would you get `let`-generalization from `fix`?
Usually we want to do isPrime() on lot of numbers.. If you are also doing that.. You can first calculate all prime numbers in your range and then search in that list. There is good algorithm called sieve for prime numbers that make this computationally good for large number of queries.
Or you are a Haskell user and you'd like to play with Dependent types before they get introduced in Haskell.
You can also use takeWhile (\d -&gt; d*d &lt;= k)
The killer feature in Idris (and related languages) is *dependent types*. This means letting types take values as parameters instead of only letting them be parameterized by other types. The classic example is list length: in Haskell you can parameterize a list by the type of its contents but in Idris you could also parameterize a list by its length. One of the many uses of this is that you can say things like "x is a non-empty list of integers", which lets you can safely use `head` without any possibility of a runtime error occurring. You can kind of encode some of these dependent types in Haskell but its kind of hacky. In Idris/Agda/Coq this is all much more supported by the language. For example, type inference often can't deduce those dependent types on its own so Idris comes with a proof assistant interface to help you guide the type checker. As for production, I think its definitely too early. This is still bleeding edge technology.
Idris looks fantastic but I cannot judge how production-ready it is. Would you say that Idris is a valid alternative to Coq when it comes to writing verified compilers? Or is it too early for that?
If you try to desugar Monad or Traversable into a record you'll find you need higher ranked types and to existentially pass subdictionaries around, it involves a bit of bookeeping that makes things slightly more complicated than the simple Num and Ord translations.
Fix and let-rec can both be defined in terms of each other, Haskell just chooses the later as first class although we could just as eadily have a first class fixpoint operator with the appropriate typing judgement builtin.
Depends what you mean by "production-ready". It's still research quality code, and I wouldn't recommend building a business around it, but you can certainly do meaningful research with it (that's what I made it for in the first place, after all). If you tried building a verified compiler in Idris at the moment, you'd probably encounter some annoying bugs (e.g. totality checking making mistakes) and missing libraries, but on the other hand, the only thing that's going to make it production ready by this definition is probably some brave souls having a go and reporting these issues, and ideally being willing you get their hands dirty and fix them!
Off the top do my head yes that would work. The trick is that Haskell needs a standard place to insert foralls if it does it for you. If you do it yourself then you're just doing what Haskell would have done on its own.
This is a good point: polymorphic recursion may not type well with just a fix primitive?
(sorry, had just now seen this) I meant the trivial code, since we know it's a 4-digit number: `{if (n/1000 != n%10) return false; n=(n%1000)/10; return (n/10 == n%10);}`.
Given that in Haskell with the DataKinds extension I can create dependent types, why what does Idris offer above and beyond that? Just curious.
https://ghc.haskell.org/trac/ghc/wiki/DependentHaskell
A simple solution indeed
The GHC team is slowly adding extensions based on specialized forms of dependent types. The reason is that Haskell can't really support full-blown dt's, and it wouldn't be very Haskelly anyways. In some cases it would be really helpful and complementary to Haskell's type system - the plan is to support those cases. 
For the curious, there is a [pretty long writeup](http://www.haskell.org/haskellwiki/Prime_numbers) about prime numbers on the Haskell wiki.
While DataKinds allows you to use values at the type level, true dependently-typed languages use those values to do computation at the type level. For example, in a dependently-typed language, the types `Vec (if True then 2 else 0) Int` and `Vec 2 Int` are considered equivalent, but in Haskell they are not. Using type families and `~` constraints, GHC can do some computations on the type level, whereas in a dependently-typed language, all value-level computations are also valid at the type-level.
It also means you have to do... more things. Scotty is what's called a "microframework", which means it doesn't have core components of your system ready-made. For example, an authentication system, models for your data, form parsing or CSRF protection are things you have to create or integrate yourself, while they come out of the box if you use the scaffolded Yesod site.
&gt; It's still research quality code, and I wouldn't recommend building a business around it, ... That's exactly what I meant. Thanks for your detailed response, Edwin. I will see how much time I will be able to spend on the verification in Idris. Anyway, keep up the good work! I am very excited to see where it goes! :)
Let me give you a simple example where this breaks down. In Haskell, I can say `read "1"`, and get back an Int, a String, a Float, etc. The compiler "knows" what I want because the types _guided_ it. In a dynamically typed language I _can't_ do that. The types aren't just encoding "permissiveness" -- they're actually giving computational structure to the program in a nontrivial way. My contention is _some_ features you're describing really are about precise permissiveness, but _other_ features you're describing are closer to the `read` example. In particular, I would argue any translation that requires dictionary passing is no more or less a translation of an un(or uni if you prefer or even dynamically)-typed program into a static setting by using a dynamic wrapper.
Yes, there is: have other people do it for you! If you get the arithmoi package, you can do: import qualified Math.NumberTheory.Primes.Testing as P isPrime :: Integral a =&gt; a -&gt; Bool isPrime = P.isPrime . fromIntegral -- use P.isCertifiedPrime if you want to be 100% sure, but this is okay -- for numbers under 2^64. With /u/rickfleischer's advice, you're getting close to an optimal standalone one-liner, if you consider the above cheating :-) If you can skip the testing for divisibility by composite numbers (because you are constructing a list of primes somewhere else) you can do even better.
You're arguing against a strawman here. I never said that type classes are about permissiveness, in fact I didn't mention type classes at all: you were the one who brought them up. What I did say was that higher kinded types are interesting on their own, independent of type classes, because if you translate a program with higher kinded types and type classes to dictionary passing style, you still need higher kinded types. I fully agree with you that type classes let you do things that you can't do in a dynamically typed language. In fact in my opinion they are one of the main reasons why static typing is better than dynamic typing. The features that *I* mentioned are *not* like type classes: they enable you to run more programs that would already run if you just erased the types.
If we are saying non-recursive let desugars into a lambda (it doesn't, see other comments), it is just as valid to have recursive let desugar into fix, after minimal mutually-recursive groups are made. But, yeah, have an upvote since I tend to forget about let generalization, too.
Very nice!
I find the symbolic soup in Scala far less regular, actually. The rules of haskell syntax are pretty clear and we just have a bunch of operators thrown in, but those operators are purely defined in userland -- my "internal lexer and parser" in my head can at least see the _syntactic_ structure of haskell code very easily, even if it doesn't know the symbols. Scala's syntax, on the other hand, has a lot more corner cases in my experience.
No, I've never seen any so-called "hardcore haskellers" who have expressed any strong opinions about Idris exploring new branches in the design space much less debate about it. I think parent comment is just trying to caricature the "Haskeller" as being obstinate, when the reality is that most of us are very interested in alternative approaches to pure functional language design.
Or (for the first case) just go for the ["primes" library](http://hackage.haskell.org/package/primes-0.2.1.0/docs/Data-Numbers-Primes.html) which implements a pretty efficient sieve. The method (both the underlying method and the optimizations) is detailed in the papers linked from the hackage page. edit: Misread the question somewhat. `isPrime` as provided by the primes library is no more efficient than the square root optimized version of your algorithm. The `primes` and `wheelSieve` functions (which use a sieve) could potentially be used to create a more efficient function, particularly for primes with very large smallest prime factor (which are sub-optimal for trial division) or scenarios where you need to check lots of primes.
I often think "feature X can be thought of as just sugar for feature Y", and then realize that "feature Y can be thought of as sugar for feature X", too. Are there any principles for deciding which feature is more "primitive"?
From what I heard, the basic reason is that since Idris is a total language (the type checke only accepts programs that terminate), its OK to use strict evaluation. In Haskell, lazy evaluation is the only viable option you have because strict evaluation can't handle things like `length [infinite_loop ()]`
This is false. The [Idris FAQ](http://www.idris-lang.org/documentation/faq/) gives the correct answer: &gt;Idris uses eager evaluation for more predictable performance. You can still make control structures, however, using the special `Lazy` type. Also, Idris is not a total language. There is a totality checker, but its default behavior is not to reject non-total programs.
Your seeing is based on knowing, while a new comer looking at a symbol heavy Haskell dsl will have no guideposts to provide at least some frame of reference. The notion that Haskell is easy to learn stops at the cliff of real world usage; there's a definite learning curve and it's *steep*. Scala is certainly not an easy language to pick up, but given how similar the syntax is to Java/C#/PHP/Ruby/Python/Javascript/etc., a new comer can hit the ground running. In addition to syntax and libraries, tooling is the next hurdle; neither Scala nor Haskell are stellar in this department (sbt/cabal and intellij,scala ide/emacs), although I'd say IDE support is extremely helpful in terms of getting instant feedback as-you-type vs. ghci running to find out about what went wrong. 
No, lazy evaluation is actually harder in a partial language than in a total language, since in the presence of partiality, laziness destroys the induction principles of types, and causes all types to be pointed. I would say that a total language can very nicely be lazy---the operational semantics of Nuprl is lazy, and I believe Epigram was too (but don't quote me on that). [Btw, Nuprl is not based entirely on a total theory, but this is a feature not a bug, since you can reason about totality.] Idris is strict because Edwin wanted it to be—there are good reasons to pursue either strictness or laziness, though strictness is not a moral imperative in a total language, like it is IMO in a partial language. There are differing perspectives on this topic though: I value having extra reasoning principles (like induction), whereas others prefer to trade that for compositionality (a strength of laziness). Your mileage may vary.
The "hardest core" Haskeller of them all rarely misses an opportunity to explain why he is uncomfortable with Idris, or perhaps you have missed all the threads. I'm not criticizing, but I am observing.
This is the best help for a beginner that i can imagine. In particular, it uncovers the mystery around many features of haskell that intimidates them.
For anyone as confused as me: [Wikipedia](https://en.wikipedia.org/wiki/Mustache_(template_system\)) and another [github](https://mustache.github.io/) project. Anyhow, I'm still not sure what this does. Are the templates parameters replaced with data from a yaml file? Edit: Ah, I'm stupid, sorry. From the example I think it allows one to embed HTML strings within code without the clunkiness of `String`s. Add up question: Can `x` and `y` be arbitrary `Show`-able variables or do they have to be `String`s?
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Mustache (template system)**](https://en.wikipedia.org/wiki/Mustache%20(template%20system\)): [](#sfw) --- &gt; &gt;__Mustache__ is a simple [web template system](https://en.wikipedia.org/wiki/Web_template_system) with [implementations](https://en.wikipedia.org/wiki/Implementation) available for [ActionScript](https://en.wikipedia.org/wiki/ActionScript), [C++](https://en.wikipedia.org/wiki/C%2B%2B), [Clojure](https://en.wikipedia.org/wiki/Clojure), [CoffeeScript](https://en.wikipedia.org/wiki/CoffeeScript), [ColdFusion](https://en.wikipedia.org/wiki/ColdFusion), [D](https://en.wikipedia.org/wiki/D_(programming_language\)), [Delphi](https://en.wikipedia.org/wiki/Embarcadero_Delphi), [Erlang](https://en.wikipedia.org/wiki/Erlang_(programming_language\)), [Fantom](https://en.wikipedia.org/wiki/Fantom_(programming_language\)), [Go](https://en.wikipedia.org/wiki/Go_(programming_language\)), [Java](https://en.wikipedia.org/wiki/Java_(programming_language\)), [JavaScript](https://en.wikipedia.org/wiki/JavaScript), [Lua](https://en.wikipedia.org/wiki/Lua_(programming_language\)), [.NET](https://en.wikipedia.org/wiki/.NET_Framework), [Objective-C](https://en.wikipedia.org/wiki/Objective-C), [Pharo](https://en.wikipedia.org/wiki/Pharo), [Perl](https://en.wikipedia.org/wiki/Perl), [PHP](https://en.wikipedia.org/wiki/PHP), [Python](https://en.wikipedia.org/wiki/Python_(programming_language\)), [Ruby](https://en.wikipedia.org/wiki/Ruby_(programming_language\)), [Scala](https://en.wikipedia.org/wiki/Scala_(programming_language\)) and [XQuery](https://en.wikipedia.org/wiki/XQuery). &gt;Mustache is described as a "logic-less" system because it lacks any explicit [control flow](https://en.wikipedia.org/wiki/Control_flow) statements, like *if* and *else* [conditionals](https://en.wikipedia.org/wiki/Conditional_(programming\)) or [for loops](https://en.wikipedia.org/wiki/For_loop); however, both looping and conditional evaluation can be achieved using section tags processing [lists](https://en.wikipedia.org/wiki/List_(computing\)) and [lambdas](https://en.wikipedia.org/wiki/Lambda_(programming\)). &gt;It is named "Mustache" because of heavy use of [curly braces](https://en.wikipedia.org/wiki/Curly_braces) that resemble a [mustache](https://en.wikipedia.org/wiki/Mustache). &gt; --- ^Interesting: [^Web ^template ^system](https://en.wikipedia.org/wiki/Web_template_system) ^| [^Handlebars ^\(template ^system)](https://en.wikipedia.org/wiki/Handlebars_\(template_system\)) ^| [^List ^of ^JavaScript ^libraries](https://en.wikipedia.org/wiki/List_of_JavaScript_libraries) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cllzt75) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cllzt75)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
Whoops, you forgot to resolve the merge conflicts in Main.hs
You are assuming that the candidate pool is large (and so does Paul Graham). If this is true, sure. If not, then there is an opportunity cost associated with not getting work done. I am also a bit wary of Haskell only programmers. At least I would focus most of my interview questions on the candidate explaining the garbage collector in ghc, how to do FFI, and drawing out the memory layout of objects. Knowing Free monads is not going to get the job done, but fixing the space leaks and cooking up FFI bindings in a few hours will. A less "mathy" background is very helpful in my experience, so all things being equal, I would rather have a haskeller that has a strong background in C++ than one who knows all about category theory.
If you do not mind me self-advertising: I have developed something similar in the past that allows you to embed anything that has `Text.Blaze.ToMarkup` instance. It can be embedded inside the source as a quasi-quote or a function `&lt;Some record&gt; -&gt; Text.Blaze.Markup` can be derived from given file at compile time (&lt;Some record&gt; is a record that holds all the template arguments that can them be used inside the template). It's called HSML ([hackage](http://hackage.haskell.org/package/template-hsml), [github](https://github.com/Palmik/HSML), [blog post](http://palmik.net/2012/08/introducing-hsml/), [old reddit discussion](http://www.reddit.com/r/haskell/comments/yyap4/introducing_hsml/)).
&gt; Given that in Haskell with the DataKinds extension I can create dependent types This premise is incorrect.
Well, the code example sure as hell looks exactly like the one in Idris: https://www.haskell.org/ghc/docs/7.4.2/html/users_guide/kind-polymorphism-and-promotion.html
Cool. Kind of like http://hackage.haskell.org/package/mustache2hs for TH-supporting platforms
Usually it comes down to which one the compiler considers more primitive. In the case of Haskell, the primitive language is known as System Fw, which is what we call "core". You can view the core that your program compiles down to by using the `ghc-core` tool or by compiling with the `-ddump-simpl` flag. I wrote up a small introductory tutorial to [the Haskell core language](http://www.haskellforall.com/2012/10/hello-core.html) which explains a tiny bit of how to read it.
&gt; code example sure as hell looks exactly like the one in Idris. For one, Idris also allows lifting functions to the type level, which that particular set of GHC extensions does not. For example: trueSum : 3 + 7 = 6 + 4 trueSum = Refl ...using Nat and (+) from the Idris standard library.
I take the actual definition of _dependent types_, which is that types can depend on values, not just the singleton (if you ignore bottom) types that Haskell uses to emulate this. Write me a function that takes a string and returns a (typed) printf function for that format string, and then I'll believe you that GHC is dependently typed right now.
Write this function in Haskell: foo : Bool -&gt; Type foo True = Int foo False = String bar : (x : Bool) -&gt; foo x bar True = 3 bar False = "hello" 
GHC-haskell is arguably λω right now. No П - no deptypes. You can "encode" deptypes in λω-language with singletons, but that doesn't mean that GHC-haskell is dependently typed.
Yeah I agree that there may be an opportunity cost. I'm just wondering, are there any Haskell only devs? I see that referenced sometimes, "I wouldn't want to hire a Haskell only dev" but is that even a risk? I mean, it's not a very mainstream language. If I really stretch my imagination, I could see a Scheme/Racket only dev, that learned it in his first CS class, dropped out, and only every used Scheme. But no matter how I think about it, I can't imagine a person using Haskell as their first language.
Here's [a colored tutorial-demo](https://github.com/nikita-volkov/hasql/blob/master/demo/Main.hs).
I use shakespeare-text as a template language. It's practically the same thing, just variable splicing syntax is #{foo} rather than {{foo}} 
I think you are correct, but I am wondering exactly how this difference effects practical code, if at all. Scala doesn't have explicit П-types either, but it's been fairly accepted that it's path types serve the same practical purpose, and Scala can generate some of the same guarantees (compile-time checking) that Idris can even for fairly sophisticated applications on dependent types, like the effect system in Idris. [[source](http://www.infoq.com/presentations/scala-idris)] I have no problem calling GHC-Haskell dependently typed as long as we have types-depending-on-terms[1], which was the first good definition I was given on dependent types. Even if we have to use singletons to do this, I don't feel that is reason enough to avoid using the "dependently typed" label for GHC-Haskell. [1] ...in addition to terms-depending-on-terms (standard functions), terms-depending-on-types (usually subtype polymorphism, but Haskell uses type classes instead), and types-depending-on-types (generics, at least; although closed type families are the *complete* version of this)
Oh yes, and about the docs. Since Hackage is acting all so funny lately I had to manually upload them. Unfortunately some hyperlinks are unresolved in those docs. Most of them point to either [the "hasql-backend" library](http://hackage.haskell.org/package/hasql-backend) or [the "list-t" library](http://hackage.haskell.org/package/list-t).
A very similar task could certainly be accomplished. GHC 7.8 supports promotion of [most functions](http://www.cis.upenn.edu/~eir/papers/2014/promotion/promotion.pdf) to the type level, and I think this even got on to hackage with the latest [singletons package](http://hackage.haskell.org/package/singletons), though I've not used it myself so I still routinely forget the package name. Printf in particular is rather troublesome because of how GHC promotes `String` literals; they lose their list structure and become atomic symbols, so you can't iterate over them or append them. I suppose we might be able to avoid that by having a newtype wrapper around `Char` (e.g. `PFChar`), working on a (type-level) `[PFChar]`, and maybe giving `[PFChar]` an `IsString` instance to try and get back good syntax. Of course, [Oleg](http://okmij.org/ftp/typed-formatting/) has been doing type-safe printf-like things since 2008 and the last iteration in 2012 is fairly complete.
Nat and (+) is actually one of the few computations which GHC does support at the type level: {-# LANGUAGE GADTs, KindSignatures, DataKinds, TypeOperators #-} import GHC.TypeLits data Equals (a :: Nat) (b :: Nat) where Refl :: Equals n n trueSum :: (3 + 7) `Equals` (6 + 4) trueSum = Refl
I remember you teasing this a while back and cannot wait to read the first chapter. Really looking forward to it! Thanks so much for doing this!
Ooh, clever name! Looks pretty interesting too. Edit: First impression: I like how ```Identity``` is used to signify a 1-tuple, instead of postgresql-simple's ```Only```.
I'm hearing of it for the first time, but I am eager to read it now that I know of it. I was actually looking for a good resource for category theory, so I hope yours does the job.
 type family Foo (a :: Bool) where Foo True = Int Foo False = String bar :: Sing (x :: Bool) -&gt; Foo x bar STrue = 3 Bar SFalse = "hello" 
&gt; I'm just wondering, for anyone who has worked with FRP frameworks in Haskell, does it really have the problems the article explains? What was it like using them? Try for yourself: [Threepenny GUI][1]. [1]: http://www.haskell.org/haskellwiki/Threepenny-gui
Well, I'm interested, but I don't really have any time to start something new now. I'm still new to Haskell, so I'm just working through snippets of different techniques (currently learning monad transformers).
[Galois](https://galois.com/careers/) seems to hire interns periodically, I'd keep an eye out for them. When I was looking for something similar, I emailed companies offering full-time positions on the subreddit/haskell-café and asked about the possibility of internship. People will generally get back to you. Good luck!
I feel confident that we will.
I've always wondered whether you couldn't make an amazing GUI library by building on top of [diagrams](http://hackage.haskell.org/package/diagrams) and &lt;*FRP library of choice*&gt;. You'd need to add input handling, and, well... what else?
Looks cool. I'd love to see somebody do the whole of [handlebars]( http://handlebarsjs.com/) 
Awesome! I'm looking forward to it. Any estimated dates?
There is also a less publicized purely functional [GUI library based on Arrows (called UISF)](https://github.com/dwincort/UISF), that is being used as part of [Euterpea](http://haskell.cs.yale.edu/euterpea/), the DSL for music and sound processing developed at Yale. Its drawing/event backend is based on the graphics library from [SOE (the Haskell school of expression book)](http://www.cs.yale.edu/homes/hudak/SOE/), which is in turn based on GLFW and OpenGL. There are quite [a few examples in the UISF code](https://github.com/dwincort/UISF/tree/master/FRP/UISF/Examples) to demonstrate its usage. I believe both continuous behavior and discrete events can be easily plugged into this UI framework as just signal functions. 
That's not the same. Singleton types are an encoding of dependent types without actual value-dependent types. They're not the _same_ as dependent types. 
Even so, what you're doing is defining a whole new string type. I want a function that takes a [Char], just a regular string.
&gt; I like how Identity is used to signify a 1-tuple, instead of postgresql-simple's Only Oh, that is clever.
&gt; Even if we have to use singletons to do this, I don't feel that is reason enough to avoid using the "dependently typed" label for GHC-Haskell. If you're using singletons, they're not depending on terms anymore. They're depending on types that happen to have a 1:1 (almost) correspondence with values.
&gt; practical code If you're doing serious "dependently typed" programming via singletons in Haskell for "practical code", you're almost certainly making a mistake. Haskell can't erase _any_ proofs from your program. Some of these proofs are going to be computationally quite difficult, and can make your program much slower, even into a bigger complexity class.
&gt; Singleton types are an encoding of dependent types without actual value-dependent types. They're not the same as dependent types. Yes, I understand: http://www.reddit.com/r/haskell/comments/2kfosg/idris_0915_released_partial_evaluator_uniqueness/clm3nv9 Point is: you don't really need deptypes for writing an analogue of this function.
We (http://www.borde.rs) may well be looking for interns over that period. Feel free to contact me (ben at borde.rs) if you have any questions.
The phrase "delicate ladybrain" tickles my funnybone. For some reason there is a strange conflation amongst folks between themselves and the languages they use/know. Languages are mathematical/engineering tools employed to get something done. To tie one's self-esteem to a language is a silly mistake (unless you happen to be among those who created said language.)
Thank you for this! Slightly off topic: What is the current state of Haste vs GHCJS? I assume once ghcjs is easier to install and use, it would be the defacto Haskell-&gt;JS compiler. Is that correct? Or is there some other reason to use Haste.
Category theory has no place in a programmer's toolset. Categories are useful for studying mathematics. They show up naturally in algebra, topology, geometry, and logic, and they serve as a powerful unifying force, showing how many different constructions can be expressed in terms of universal properties and adjunctions. But a programmer doesn't deal with those things. A programmer's only exposure to universal properties is indirect, through the introduction and elimination rules of the underlying lambda calculus their language is built on. (And that assumes they are working in a functional language, which I guess we can assume here). The way category theory is applied to Haskell is typically obtuse. Countless Haskell blogs and conversations on IRC bring up "the category Hask" without giving a proper specification of which category that is. Haskell itself doesn't even have a formal specification. But even if it did, using types for the objects, it doesn't make sense to use Haskell functions as the morphisms. Typeclass qualifications and parametric polymorphism require more machinery than a simple category. They typically require some kind of fibered category. Conversely, Haskellers often use polymorphic functions when giving examples of natural transformations. Again, this gives a good first-approximation, but it's never mentioned that this is just an analogy, and that it breaks down. Even worse, in Haskell circles, this explanation of natural transformations is as far as most conversations go. The idea is never motivated by the historical examples that were first explored: the double dual endofunctor on vectorspaces and the natural isomorphism to the identity functor and the natural isomorphisms between various homology functors on topological spaces. Natural transformations are especially important because the of Yoneda embedding. Given any category, we can embed it into a category of functors ("presheaves"). Any category of functors turns out to be as "well-behaved" as a category can get, and we can realize any "degenerate" category as a subcategory of a nicer one. Because the objects of this nice category are all functors, we need some notion of morphism between them: natural transformations. This idea of working with categories of functors is ubiquitous in mathematics. Yet, in the Haskell community, the only instance I've ever seen where this idea is used is in Edward K's lens library. Functors and monads turned out to be very useful programming tools in Haskell. But even so, it's good to keep in mind the distinction between the *programmer's* functor and the *categorical* functor. They are similar, but the definition are subtly different and mindset behind them are entirely unrelated. To a programmer, a functor is something like a container (but not exactly, because they've been yelled at for expressing that thought before). It's something that can be "mapped" over. Most likely, a programmer always has a picture of a list or a tree in the back of their mind. It has to satisfy some laws, but for the most part, they never think too much about the laws. It's simply believe that most type constructors lead to functors (since in practice, most do). Categorically, a functor is a mapping that is used to transfer commutative diagrams between categories. Every topological space has a homology group. If you have a diagram of continuous maps between topological spaces, you can lift that diagram to the category of groups, getting a commuting diagram of group homomorphisms between the homology groups. (In fact, the proof of the Brouwer fixedpoint theorem can be proved in half a paragraph once you have established the homology groups for euclidean space and the n-spheres). But categories work because mathematicians already have a deep understanding of the individual theories. A mathematician knows about topological spaces and continuous maps. They understand groups and homomorphisms. They have a good idea of what's tractable and what's not in each context, and by knowing how to transport ideas between contexts using functors, they can solve more problems (or simplify existing solutions... the original proof of the brouwer fixedpoint theorem for higher dimensions was horrible and convoluted). But working programmers don't have a good basic understanding of many of the mathematical tools they use on a daily basis. Every good programmer knows what a graph is, but it's likely most have not ever heard of a graph homomorphism. Many Haskellers are well aware that lists are examples of monoids or even that the list functor gives the *free* monoid for a given set. But again, the homomorphisms of monoids is lesser known. And I don't think I have ever heard anyone in the Haskell community talk about monoid actions on a set. (Group actions give rise to half of geometry and ring actions give rise to the *other* half). Surely, monoid actions (which can be used to represent transition of a state machine) should have some great relevance in CS theory. But even if we had these things, we would be faced with the limitation of Haskell's notion of a functor: it's necessarily an endofunctor. The functors of mathematics let you move to a realm where things become more finite or have more obvious invariants. But an endofunctor will just bring you back to where you started. (Of course, I've heard that GADTs let you work with subcategories of Hask... but remember Hask is still ill-defined, and what we have is a loose analogy). Perhaps it's too late into a rant to make this disclaimer, but I don't mean to discourage people from learning category theory or to belittle programmers. I just want to set the expectations of newcomers. Like many people in the Haskell community, I wanted to learn about categories because everyone seemed to think they were so important. But after all the time I spent learning, I can't advocate anyone learn category theory 'so they can be better at Haskell'. If someone were interested in semantics of programming languages or in algebraic topology or geometry, I would say, yes, learn it. It's important. But don't learn it up front. It would be like learning quantum field theory before quantum mechanics. Worse, while quantum field theory will discourage you with its highly technical language of hilbert spaces and gauge fields, the definitions of category theory are almost entirely transparent. They are easy to state (and this is where the elegance of the subject comes from), but it can be baffling where those definitions came from. On the other hand, if someone is interested in programming, I would instead recommend they study linear algebra, abstract algebra, or set theory. Linear algebra hits a sweet spot. It's used literally everywhere in life and can be studied entirely purely, entirely applied, or anywhere in between. You might also want to take logic. If you do, though, you would do better to focus on syntactic methods. (Semantic methods give "deeper" insight, but are much more sophisticated). That's my take on the subject. Category theory is interesting. It gives a novel perspective on how to do certain kinds of abstract mathematics. But as a tool for programmers, it does not give you enough leverage to justify the steep cost of learning it.
Well, replicating a full set of standard GUI widgets is a time consuming and nontrivial task. For instance, you have to replicate 1. Caret positioning for non-monospace text; multiline text. 2. Navigation in hierarchical menus^* 3. Copy &amp; Paste 4. Drag &amp; Drop and so on. It's certainly doable but by no means simpler than writing a binding to an existing GUI library. ^* [Bruce Tognazzi on some design issues for hierarchical menus][1]: &gt; When I specified the Mac hierarchical menu algorthm in the mid-'80s, I called for a buffer zone shaped like a &lt;, so that users could make an increasingly-greater error as they neared the hierarchical without fear of jumping to an unwanted menu. As long as the user's pointer was moving a few pixels over for every one down, on average, the menu stayed open, no matter how slow they moved. (Cancelling was still really easy; just deliberately move up or down.) Apple hierarchicals were still less efficient than single level menus, because of the added target, but at least they were less challenging than the average video game. [1]: http://www.asktog.com/columns/022DesignedToGiveFitts.html
I read the twitter thread in question yesterday and I was genuinely confused. This post puts into words my confusion better than I could have!
We're getting far away from what the blog post addressed. I would add that most Haskellers came from somewhere else, often untyped languages. These discussions shouldn't be personal. That's what leads to unconstructive and hurtful debates. Making it about taste *makes* it personal and that framing short-circuits the possibility of rational discussion. Cf. "agree to disagree"
&gt; I don't mean to discourage people from learning category theory or to belittle programmers. I think you just did that. 
Where at? I am being critical, for sure. But not belittling.
That is beyond my powers. It is the site style, sorry :(
I believe that is correct. The personal really is the problem. On the other hand, directly asserting that a person exercised poor judgement because they chose to learn a particular language would be insulting. To identify strengths and weaknesses of a tool is not a personal matter unless you have conflated your self-esteem with that tool.
I plan to create a version of these libraries for ghcjs too. The only thing that made me to start with Haste is that it produced less javascript code. I have to say that haste work like a charm.
The whole thread is just frankly [bizaree](https://twitter.com/bitemyapp/status/526431303080898560). One person seems to want to discuss inductive reasoning and the other person wants to talk about political correctness of language, and it ends up reading like the farce you'd expect it to devolve into.
This video is a good example of an application of category theory: https://skillsmatter.com/skillscasts/5787-categories-for-the-working-haskeller#video It's a rehashing from an old post on this subreddit but it's apropos to this post.
I have this problem with a particular acquaintance of mine. He is a hardcore C++ programmer, and every once in a while, if Haskell comes up among our mutual friends, he will take a popshot at the langauge and at me for advocating it. But the line I really like in this blog post was this one: &gt; It’s fine if you don’t want to learn something, I suppose, but it’s surely nothing to be proud of. His motto is pragmatism trumps all, and he loves to bring up the fact that "no one uses Haskell." But the thing that irritates me is he revels in his own ignorance of the subject. He knows nothing about the language at all (beyond what bits he has picked up listening to me, presumably). On the other hand, I know C++ fairly well. I'm not an expert in the language, but I know enough that I could probably get a job in it with a few months of practice. I understand pointers. I know when to write a copy constructor. I know about virtual tables for implementing polymorphism and how templates can be used to write type-generic code. But he wouldn't be able to tell me even the most basic thing about lazy evaluation, the lambda calculus, or type inference. Incidentally, I just posted this post on a neighboring thread: http://www.reddit.com/r/haskell/comments/2kkrd3/category_theory_for_programmers_the_preface/clmhrid It's a criticism of how strongly category theory is advocated in the Haskell community. I think there's a legitimate issue that should be addressed in the community. But in doing so, I bring up how how category is used in mathematics and how the Haskell community perceives the subject. And the one comment I got so far was that it comes off as belittling to programmers. Much like the story in the OP's link, my commentary uses a lot of technical language to hint at the depth of the subject. But this technical language (combined with the strong opinion I'm expressing) make it sound a bit condescending when read in the wrong tone.
tl;dr -- It's not worthwhile for the average programmer to learn category theory. The Haskell community, in particular, vastly overemphasizes the importance of category theory for newcomers.
We'll be making a posting in the next couple of weeks.
&gt; The Haskell community, in particular, vastly overemphasizes the importance of category theory for newcomers. Do they do? I know certain prolific members of the community often discuss the subject in context of advanced topics, but the predominant advice for newcomers seems to be that it's not terribly important to learn. Maybe I'm wrong, but I've never actually seen CT advocated as a path to learning Haskell.
&gt; His motto is pragmatism trumps all, and he loves to bring up the fact that "no one uses Haskell." Seems like a very valid approach to me. 
&gt; And the one comment I got so far was that it comes off as belittling to programmers. I don't think that's what /u/DrBartosz meant. I think he was saying that you were "discourag[ing] people from learning category theory."
Can you recommend a Linear Algebra book or online class? Same for logic? I'm still probably going to read this book as it comes out (honestly, your rant made me want to read it more :p), but I tend to agree that improving my very elementary linear algebra is probably more urgent.
&gt; Conversely, Haskellers often use polymorphic functions when giving examples of natural transformations. Again, this gives a good first-approximation, but it's never mentioned that this is just an analogy, and that it breaks down. Wait what? Parametrically polymorphic functions of the form `forall a. f a -&gt; g a` _are_ natural transformations! This is not an analogy at all. However, not all natural transformations are of that form, of course (parametricity being stronger than naturality). So if one thinks of them only in terms of such functions, one will be misled.
Yes, I think this is the root of the thing. And the little I have seen of the talk was unnecessarily condescending (and not necessarily technically correct). There's no need to alienate people. Instead encourage and welcome them in. 
Perhaps things have changed, but when I learned it, that was the impression I got. Perhaps the issue is more subtle. You end up hearing lots of categorical language in the Haskell forums, blog posts, et cetera. The use of the language necessitates some understanding of the subject. But there's no good story to tell to people who want to start learning the subject. "Pick up a copy of Awodey and good luck," is the best we usually do.
&gt;QT is, by far, the most elegant and feature-rich of all three. Unfortunately, and to the extent of my knowledge, nobody has been actively working on qthaskell for more than 5 years, so the bindings are pretty outdated. As a result, installation (with a relatively recent version of ghc and cabal) is impossible. [HsQML](http://hackage.haskell.org/package/hsqml-0.3.1.1 ) is a viable option. It's not pure Haskell but it has some major benefits and the maintainer is active. HsQML now also supports OpenGL: http://hub.darcs.net/komadori/HsQML/changes Comment from the author of HsQML at the end of the post: &gt; HsQML is an actively maintained binding to Qt 5. The approach is a bit different to qtHaskell in that it specifically targets the Qt Quick framework and hence requires you to describe your interfaces using the semi-declarative QML language. 
This post is some smarter-than-thou smugness I disagree with. On the one hand, I really strongly agree that people shouldn't see cat theory as a _prerequisite_ for anything Haskelly, or as something necessary. But on the other hand it is a great and useful compliment to learning Haskell, and it can help you see things more deeply. Categorical semantics is a rich topic that is deeply intertwined with the theory of programming languages. Does that mean you can just "deploy" it in code? Not necessarily. But it's still a great toolset of applicable ideas. You're right that sometimes we talk loosely and informally about "Hask". But you know who else talks loosely and informally all the time? Actual working mathematicians. What matters are ideas and intuitions, and we can typically, if we really want to nail things down, turn the handle and crank things down to the precise level. But when we do so, things get hard to follow. So often, conversations happen at a higher level. You're right we need to make clear those caveats to beginners. But that's different from just discouraging people and somehow telling them they're not doing "real" cat theory because it isn't about a double dual endofunctor on vectorspaces or whatever. How about, instead of putting that knowledge you clearly have to the use of complaining about how other people get stuff wrong, instead think how you can convey and share that knowledge to others, to help them learn what _they_ want to learn instead of what _you_ want them to learn, but to help them learn it better?
If you're not coming from a maths heavy background, Introduction to linear algebra by Strang is the best textbook I have come across catering to that demographic. There are online videos that go along with it as well. The big win in that book is that, most of the time, the exercises for chapter N will have some kind of pattern or trend to them. When there is a trend like that, it's often using what you've learned in chapters 1..N to play around with what is coming in chapter N+1. That is a really useful thing to do. It's largely what convinced me that working through a textbook from cover to cover in my spare time was feasible if I picked the right books in the right order.
That's a argument is favor of removing the special casing of this promotion from GHC, but it is not an argument that GHC is not dependently typed.
I find all these socially-directed (for want of a better description) arguments a bit diverting. Let's all just be decent to each other and share our enthusiasm with respect and kindness. http://comonad.com/reader/2014/letter-to-a-young-haskell-enthusiast/
Kindle makes it hard/impossible to copy'n'paste code examples (e.g. to try out in ghci). Is there an easy way?
&gt; Parametrically polymorphic functions of the form `forall a. f a -&gt; g a` are natural transformations! This is not an analogy at all. Which functors do they map between? You end up lots of minor technical issues. I think it requires `f` and `g` be functors. It *only* works for expressions with a single type parameters `a`. It relies critically on the parametricity of the language. And lastly, assuming this is meant to be Hask, we have to face the fact that Hask isn't a formally-defined category in the first place. I wouldn't be surprised that if you cleaned up all those things, you would get a valid example of a natural transformation.
I strongly disagree with this article. As someone else in this thread noticed, this twitter thread came on the heels of a really arrogant talk at StrangeLoop that lots of people found both smug and technically deficient. I suspect this talk won't be like that, but I also think there's no harm in admitting that a joke in a title fell flat and was subject to a bad interpretation, and therefore that the title shouldn't have been written in such a fashion. Furthermore, the Haskell is and has been acquiring a bad reputation among some circles because a few people really have been spreading what Tim properly calls "a very popular aggressive, adversarial, confrontational teaching style that many people apply to a broad range of interactions besides those that are understood as teaching situation"(http://tim.dreamwidth.org/1860430.html). When we get criticism we shouldn't react by circling the wagons and being defensive. We should take it to heart. If we do want to reach people, and we find that there are obstacles, we should look how to disarm those obstacles. If that means a certain title for a certain talk doesn't work very well, I don't think that's cause for alarm. I agree, by and large, with the author's description of folks in the Haskell world as people who genuinely love to share and teach, and don't want to condescend. But that doesn't mean we should just crow about it. If other people don't see things that way, that's _not_ their fault. It means we can do a better job. And I don't interpret such criticisms as asking for people to not hold strong opinions. I just interpret them as calls to actually try to convey things in an accessible manner.
For a theory-heavy book, I like Axler's Linear Algebra Done Right. It's usually better to start with a more concrete book to start off with, though, using matrices and column vectors. I learned linear algebra in an ad-hoc way, so I don't know of any good beginner books. If you see me in IRC ever (usually under the nick tac_ in #haskell or ##math), feel free to ask me about any particular books, and if I have time, I can skim through and give a quick opinion on it.
I don't "disagree" with the article because it's a personal opinion, but otherwise I support everything you say. I wish I knew what this meant though: &gt; a very popular aggressive, adversarial, confrontational teaching style that many people apply to a broad range of interactions besides those that are understood as teaching situations
&gt; Looking at the linked Twitter conversation¹, and some other examples I've run into of people complaining about condescension, some people have a real problem with anyone claiming that one thing is better than some other thing. You have a strong opinion? That's not fair! It's also not condescending. They weren't saying that a strong was opinion is condescending, they were saying that suggesting that programming in a certain way was morally correct and others ways are less or are not morally correct is. They may have read it wrong or read too much into it but that doesn't seem to be something that's totally unreasonable. I think people find a lot of Haskell people smug - rightly or wrongly - because Haskell is given this silver bullet image. "If it compiles it (usually) works" makes it sound like the Holy Grail of programming languages and it's just something that you haven't discovered yet. The problem is that nothing is quite as simple as that. With a strong(er) type system there are certain trade-offs one has to make but the fact that this is so rarely discussed makes the people who advocate Haskell on sites like reddit seem smug. &gt;Everything is not the same and so we should not treat it as such. There is nothing wrong with having strong opinions—it helps move everyone forward and, presumably, you have good reasons for coming up with those opinions. On the other hand, a social environment that immediately penalizes anything (or anyone) for trying to rise above itself, getting out of its place, is deeply troubling. And I fear that's exactly where this spirited pushback against "condescension" would lead us. The thing is you can have strong opinions and not come across as condescending. There's a perfect middle ground that suits the vast majority of us and I think that is what people want and will achieve. They won't accidentally cause a world where everything is grey. The opposite is true in my view. If you create a nice, welcoming community that tries to be helpful and nice to all, even if their definition of condescending differs from yours, then more people will come and you will get more opinions. In a pursuit (programming in general, not just Haskell) that has a bit of a reputation for being unwelcoming to newcomers then I think it's important to take their views into account as far as possible even if it means phrasing things in a more careful way. 
If you take category theory in haskell properly, you can treat most types of kind `* -&gt; *` as functors in a few coherent ways. They're not all members of the `Functor` typeclass, but they certainly map objects to objects, and they can always map at least _some_ morphisms to morphisms, so at a minimum you can treat them as functors on some subcategory of `Hask`. In category theory generally, many categories are just subcategories of `Set` but we don't talk about functors to them as all "endofunctors on Set" because that's silly. We can do the same with `Hask`. Furthermore, the only reasons we can't treat `Hask` as a real cartesian closed category in the obvious way are technical fiddly things that have to do with bottom, so at a minimum we can talk about restricting ourselves to the total fragment, as we often do anyway. And if we don't want to do that, we then can use a well understood bag of tricks for translating results to a broader setting. Anyway, we _do_ have a "real" category for Hask, bottoms and all, and it is the obvious one. It just isn't a well behaved CCC that we'd like to work in. All this is well known. And furthermore there are lots of fantastic papers doing real work that lean heavily on category theory. This paper that gave us the laws for `Traversable`, for example, is deeply categorical: http://arxiv.org/pdf/1202.2919.pdf Our entire theory of ADTs comes from category theory, via initial algebra semantics. This wonderful work about extending our understandings of folds to GADTs leans on category theory and brings in things like Kan extensions to get there: https://personal.cis.strath.ac.uk/neil.ghani/papers/ghani-tlca07.pdf Free monads have become a very widespread tool for structuring programs. And furthermore, the basic idea of how to optimize them is tied to yoneda and kan extensions as well: http://www.cs.ox.ac.uk/ralf.hinze/Kan.pdf I'm not saying most people need to learn this, or should necessarily want to. But let's not be dismissive of it just because its not categories for vector spaces or the like.
I entirely support what you say, but please point me to one instance of a Haskell advocate seriously suggesting that "If it compiles it (usually) works". In fact, point me to any you see in the future. I will happily put them straight.
This example shows that type family names should have been lower case. `Foo` is a function.
Can I find benchmarks?
If a type of kind `* -&gt; *` uses its parameter both covariantly and contravariantly I would be surprised you can can treat it as a functor in any useful way. [EDIT: Oh dear, I've committed the mathematicians' sin of not precisely stating my claim. I know objects of kind `* -&gt; *` can be treated as functors on some subcategory of Hask. After all, they're endofunctors on the discrete subcategory of Hask. The context, and the question I really wanted to address, was "when are the polymorphic functions of type `forall a. f a -&gt; g a` the natural transformations between two functors?"] [EDIT 2: Actually, even *that* framing isn't precise enough and nonetheless I may well have misunderstood what /u/sclv was saying anyway]
Haskell only is probably the wrong nomenclature. What I am thinking of is more a "top down" type of developer. A developer who started learning Scheme and ML at university and maybe continued using Java in industry. I have worked with that kind of developer, and they are good in the sense that they design and implement good systems. Another kind of developer (in my generation) started with assembly and C, then moved to C++, maybe Java, Lisp and ended up with Haskell. This other type of developer has the added capacity to understand everything down to the machine level, and if I was betting on Haskell, I think I would want at least one of the second type of developer on my team.
Granted its a bit hard to disagree with the article in a certain sense. But I do disagree with what I consider its "germ of an idea" which is that people should stop complaining about bad interactions they have with the FP world, because it is probably their own fault (I grant the article does not say that explicitly, but it seems the clear subtext to me). I believe these bad interactions, for whatever, reason, do occur, and the fact that often there are good interactions does not make those bad interactions any less unfortunate. (i.e. #notallhaskellers is #notveryproductive) As to the latter point -- I consider the habit of some to just pop up to tell other people how they're wrong, not in a friendly way, but in a blunt way, as the "understood as teaching situations" element. And furthermore, even in teaching situations it is arguable when it is and isn't appropriate -- but at least there is a set of social expectations around it.
The best I can come up with is [this](http://www.reddit.com/r/programming/comments/2a97q4/the_new_haskell_homepage/cisx01z) thread at the moment. It's not the best example because not everyone agrees and those that do generally qualify it but there are one or two comments that are just a "yes". I have definitely seen more of it over on /r/programming though. I'll have to take you up on your offer some time! Hopefully I won't have to! :)
The offending line that people are probably reacting to was removed from the new-www site a week or two ago by the by. There were multiple instances of stronger-than-is-helpful wording in the site that have since been toned down.
Galois is an amazing place to work; definitely check them out. Source: I was an intern there last summer, and those were some of the most enlightening and exciting months I've yet had in my life as a computer scientist.
Haskell has, rightly or wrongly, got a reputation for not being welcoming, being condescending and/or smug to newcomers and those less experienced. I think it's somewhat understandable that when someone saw a talk entitled "Functional Programming is Morally Correct" jumped to the conclusion that it was arrogant, especially after recent talks. It may be that they got the wrong end of the stick and in that case it's perfectly reasonable to say "hey, actually I think it was a reference to this paper with a similar title and the person who gave the talk may not have meant it in that way". It's then their move. If they accept the explanation then great, both people are happy. If they don't then they should at the very least have that view respected and, in my opinion, we should listen very carefully and perhaps alter our behaviour. Writing a blog post condemning them as lazy is a pretty terrible thing to do and is certainly not the way to make people think the Haskell community is not condescending!
That's good.
&gt;&gt; pragmatism trumps all &gt; Seems like a very valid approach Not to me. I don't have a problem with prioritizing pragmatism, but I do have a problem with "trumps all". Absolute positions are those of the zealot.
Sure you can: take morphisms to be representational equivalences (i.e. instances of Coercible), or perhaps less usefully, as equalities between Haskell types (admittedly this is more interesting in Core than in surface Haskell).
&gt; otherwise acknowledged bad behavior. You mean like fast and loose reasoning? Oh wait, this is apparently how we got into this mess.
The research on initial algebras is truly good stuff, as are free monads. In what you've seen, how does Hask handle type classes and parametrically polymorphic functions? Those are the two biggest issues I've had. I know that polymorphism can be handled by introducing fibered categories. But only because I once made the mistake of trying to read Jacob's 1000-page bludgeoning device/book on categorical semantics. Actually, one of my biggest frustrations over the last two years has been that I have wanted to get into categorical semantics more, but there is a complete absence of introductory material on that matter. I would probably be less suspicious of the loose-and-fast reasoning used in the Haskell community if I had some understanding of how things were formalized in the end. But this is a failing of a different community.
&gt; I have never seen a conversation in IRC where the details of Hask were spelled out. And I have also never seen anyone link to a book, paper, or blog where the details are addressed. Are you seriously complaining about a lack of rigor in IRC conversations!? In any case, here's a paper that does a nice job of being rigorous: http://citeseerx.ist.psu.edu/viewdoc/summary;jsessionid=19D870DB0E19F0C29914C60A6B6997B9?doi=10.1.1.406.6999 It makes reference to the basic work everyone leans on in this regard, the "Fast and Loose" paper: http://www.cse.chalmers.se/~nad/publications/danielsson-et-al-popl2006.html If you're writing a paper, you may want to cite this sort of stuff to show how it goes through. But if you're not writing a paper, it really doesn't tend to matter. &gt; The Haskell community is responsible for many people taking an interest in category theory to begin with. But, in my opinion, we don't have a good path to send you on if you're curious about the subject. On this, however, I'm in full agreement! Once you get the basics, there's not a good path to "level up" to understand any of the work I've cited in the thread except to hang out on irc and ask the right people the right questions, read a bunch of similar papers and their citations until things begin to click, and just learn lots of random stuff to assemble it for yourself. I consider this a challenge that hopefully the book Bartosz is writing will attempt to rise to!
That might be so. But to be fair, the kid completes more projects than I do and contributes more code.
I agree with the sentiment, but it's dangerous to dismiss complaints about how a community socializes simply because you don't share the complaint. It may be preventing the community from growing, or even damaging existing parts of our community. We should disclose and discuss social complaints without losing our respect or kindness.
Jacobs' book is indeed very hard. I've not made scads of headway yet but Lambek and Scott's "Introduction to Higher-Order Categorical Logic" is a much more approachable work in the same vein, though clearly not as rich. Parametric polymorphism is also indeed a _hard_ topic to get a categorical account of. The categorical work on it that I think is best is in Mitchell and Scedrov's notes on sconing and relators: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.26.4213 But that is rough stuff! In fact, in general, parametricity is still quite mysterious in some ways, and the "Reynolds Program" paper lays out just how much we still don't know: http://www.cs.bham.ac.uk/~udr/papers/logical-relations-and-parametricity.pdf Type classes on the other hand are easy or hard, depending on how you think about them. One approach says "just desugar them into dictionary passing". Then you can do lots of work well. Another approach just treats them as functions we attach to structures. This is how we get the `Functor` typeclass and such. However, the full theory when coupled with fundeps or type functions, and how constraints are resolved and code synthesized, etc. is pretty complicated. But that's because, I think our current account of type classes is lacking in some ways :-)
This looks nice, good work, thanks for sharing! However, it is kind of unfortunate that we now have both [haskell-stripe](http://hackage.haskell.org/package/stripe-haskell) and [stripe](http://hackage.haskell.org/package/stripe) doing pretty much the same thing, even if one implementation is more complete than the other. Have you consider joining your efforts with those of `stripe` somehow?
I think it would be more worrying if the title of the presentation had been along the lines of "FP is not Morally Correct". As [Eugenia Cheng notes](http://cheng.staff.shef.ac.uk/morality/) morality has a long history in the mathematical field. That it feels *right* (ahem) in discussions of programming should come as no surprise.
hs-stripe was missing quite a few features (see my comment under ocharles) and is based on a 4 year old API. Yes, I'm in talks with mafs and Luke, but want input from startups and companies using hs-stripe.
&gt;&gt; bad behavior &gt; fast and loose reasoning Fast and loose reasoning is good because it is fast. Specifically, it can be done quickly enough that is might even be faster than your edit-compile-test loop, and it provides quite a bit of guidance to what types to define, how use to the existing types, or both to accomplish your goals. Fast and loose reasoning is bad because it is loose. Specifically, if you want to support bottom values (and if high-reliabilty is your goal, you do have to support bottom values in Haskell), you can't stop at loose reasoning. It's not bad behavior by itself; it's bad behavior to stop after you've done your fast and loose reasoning.
Didn't Brent Yorgey have a functional pearl about monoid actions (related to the Diagrams library)?
If so, I'd enjoy seeing it. Monoid actions seem like they should be so natural in programming.
This is so timely. Needed some of the more modern API bits, so glad I don't have to write them myself. Does the library provide the types needed for webhooks for chargebacks / failed payments?
I was under the impression that the talk was not intended for mathematicians or for people who currently program in strongly-typed functional languages, but rather for a general audience—the kind of audience that might not be familiar with the mathematical usage of 'morally correct', with a high likelihood of at least _some_ of the audience misunderstanding the intent. I wouldn't object to this sense of 'morally correct' appearing in the title of an Ed Kmett blog post, but I think it's inappropriate in the context under discussion.
I meant the initial poster kf.
A general audience wouldn't understand functional (from non-functional or dysfunctional) either but I hope they'd watch it first.
Actually hold on, no, the original tweeter says FP or FP-related conference. 
I'm going to advocate [this criticism of advocacy](http://www.perl.com/pub/2000/12/advocacy.html). And to be honest, I don't really believe most people tie their self-esteem to a language just because they spent time learning it and using it. Sure people care about the things they've invested in, but it's nowhere near so simple. The more I use a language, the more grudges I have against it. If you don't hate a language (at least sometimes) IMO you don't really know it. If you're really using **any** programming language for non-trivial tasks then sometimes it's going to drive you nuts. That doesn't mean you stop using the language, it doesn't mean the language isn't the best available for what you're doing - there's simply no such thing as a perfect language. Real experienced programmers, I think, have an understanding of engineering trade-offs and are extremely skeptical about claims where people deny or are overly dismissive of costs and downsides. Attack the people who ask awkward questions and they'll attack you back or just ignore you - either way you've made no new converts and may have put off more people than you're aware of. IMO the best way to advocate something is to be willing to discuss the limitations too - preferably to be open and up-front about them. When people understand the limitations they understand whether those limitations are going to be a problem in real life. When the limitations of a technology are taboo, you simply can't trust that technology. Of course that's incredibly naive in a way. Plenty of people trust in absolute unreasoning confidence and fervor with total intolerance of questioning holy writ. Religion certainly exists in programming, but the real question is who you want to convince. Also, I'm one hell of a hypocrite criticizing people for having sensitivities. FUD-phobia and similar sensitivities are inevitable outside the mainstream - it doesn't necessarily translate to arrogance or condescension etc. A lot of apparently bad attitude is really just the social norms of a different group - academics expressing things the way academics always express things doesn't mean they're acting superior. I'm extra-aware of that because I have Aspergers, some of the symptoms of which are widely interpreted as arrogance and condescension - which is almost funny considering other symptoms include extremely low self-esteem (hence sensitivities). 
Well, it's not just the blog posts. The Prelude is already shipping with the words Functor and Monad in it, and base has Category, Arrow, Monoid. Category theory terms have permeated the Haskell language.
I did something like this for searching spaces using a `MonadPlus` instance to derive what I called a `MergeSplit` instance differently depending on what strategy you wanted (IE depth first vs breadth first) and showed off N-queens described independently of the actual algorithm used to find the solution... I may still have the code somewhere.
Instead of "Haskell functions", how about "monomorphic Haskell functions" as Hask's morphisms? Then a function with universal quantification just represents an infinite collection of those, and additionally with typeclass constraints it narrows down said collection.
I would be happy with a simpler solution (i.e. monospace fonts, no drag-and-drop), if only because it would stimulate work on more sophisticated solutions.
&gt;If the code compiles, it works (almost all the time)! Contrast this with Ruby, for example, where the test code can be 2-3 times the size of the app, and even then, one is not quite sure if the runtime code is correct. http://www.drdobbs.com/architecture-and-design/in-praise-of-haskell/240163246 &gt;A lot of people experience a curious phenomenon when they start using Haskell: Once your code compiles it usually works. This does not seem to be the case for imperative languages http://www.haskell.org/haskellwiki/Why_Haskell_just_works 
&gt;But even so, it's good to keep in mind the distinction between the programmer's functor and the categorical functor. The only distinction I see is that "the programmer's Functor" is restricted to be a certain kind of endofunctor from Hask to Hask. That is what I call "specialization" or "generalization", not "distinction."
You should really not reference that talk without pointing out that the moral correctness mentioned in it is completely unrelated to the meaning that actually would be somewhat condescending (after all in this case they're comparing "functional programming" to "fast and loose reasoning"). Not that there aren't actual ethical issues with language choice, it'll be hard to justify Ruby over something like Ada when you're working on safety critical software. The talk isn't about those kinds of morals though, just a particular "moral correctness" of ignoring demonstrable counterexamples involving things like nonterminating loops.
We don't yet have a comprehensive set of benchmarks. That would be a very welcome contribution if you'd like to get involved! Then you can also pick problems that are relevant to your particular interests.
I hear you. I dug into it, but I realized that Category Theory was not particularly useful to me. It seems like a highly abstracted naming scheme, with the purpose of connecting wildly different areas of mathematics, to provide insights (or am I totally off-base?). Unfortunately, without a strong math background (just casually starting to get back into it), it does not seem to serve that purpose at all. It's interesting, but I cut my losses part way though, and will come back to category theory later.
This is pretty off-topic, but borderline related. I'd really like to see something like an irc channel for haskell beginners, with a strict moderation policy with regard to answering questions. Basically, if someone is trying to work through a question and you dump the answer in the channel while they're clearly in the process of working it out, that shouldn't be OK. And if someone has a beginner question about, for example, fold, and if you don't take the time to work out where they are in their learning before you start talking about catamorphisms and F-algebras, I'd also suggest that shouldn't be OK. Basically, I think it'd be nice to have a channel for beginners where the focus was on helping beginners / there was some kind of opposing force applied to the temptation to show off. I also wonder if some kind of complementary channel about teaching haskell would be useful - so that any meta discussions about the best way to teach something / answer a question in the beginner channel don't end up confusing the message for the beginners.
The best way to deal with these things is to simply not engage them. Build what you want to build, publish the research you want to publish. Ignore everyone who tries to involve social justice or whatever crazy thing this is. I am a developer, a student, and I just want to have nice tools that guarantee some amount of safety and translate mathematics well. That's why I use Haskell. It's the evangelical types that get preachy and scare off people.
i suggest google.
I know I'm just about at the point where my math needs to improve in order for me to progress in Haskell. I expect I'll be poring over the posted article tomorrow.
I have seen that professed a lot, and I would even say it is true (for me).
It seems to me that tact is given a value near truth in many realms. Usually truth still comes out ahead, but sometimes by much less than I expect. There's also the weird effect that some beliefs become stronger when contrary evidence is presented.
`#haskell-beginners` on Freenode doesn't have an explicit policy yet but kindness and staying on topic are enforced. Derailing or distracting people getting helped is also not allowed, which is a problem several Haskellers have complained about WRT the main channel (not me). The channel is mentioned in my guide. Up to about ~130 people now, I mostly don't need to teach anymore - other people better at it than me have picked up the slack. I've been told by a long-time user that `#haskell-beginners` is well run. I didn't have to do much to make it so. Just ask people politely to please do or please do-not-do certain things. Some were a little uncomprehending at the reasons (like not hijacking other peoples' teaching conversations), but people are more apt to cooperate when they: 1. Are asked nicely 2. See that a healthy culture is in place (ideally because of the norms). This means everybody is respected and treated well. Without #2, rules have no credibility.
&gt;Does that happen?? Maybe if you ask Cale a question about category theory :)
At least as far as IRC goes, I'd say I see worse than a 50/50 split between people trying to work out where someone is in the learning process and people doing one of the two kinds of things I mentioned. Sometimes it's more subtle than jumping to catamorphism. For people struggling with the central idea of a fold, I've seen * catamorphism and F-algebras * getting into gory details of foldl vs foldl' (then adding ~100 lines of examples with little commentary) * introducing scans and/or unfolds at the same time as fold * doing something complicated with a tuple as an accumulator to show off how general fold is Some of these have also been very useful for other people in the channel. But most of the time, the person with the question has stopped being part of the conversation. Maybe I should just start pointing people at [this](http://chrisdone.com/posts/teaching) more often :) 
An [FLTK GUI binding](https://github.com/deech/fltkhs) is under active development. Its main advantage is that it is small native GUI with almost no dependencies so it produces a single statically linked executable on all major platforms without the user needing to install anything.
A lot of those examples came from #haskell-beginners unfortunately. I haven't spent much time in #haskell for a while, but I remember it being worse for this kind of thing. That could partly be a timezone thing - I might have been on while the majority of folks were half asleep and distracted. As a random aside - I think I actually created #haskell-beginners (although I handed it over to you in less than a minute). I could be misremembering though. IIRC I spotted a comment of yours on reddit about needing a beginner friendly channel, I typed /join #haskell-beginners and then passed it along. I've only been paying a lot of attention to this kind of stuff since after that, otherwise I probably would have tried to stick around and be more active in trying to keep the helpfulness ratio high.
Daylight is antiseptic.
&gt;being worse for this kind of thing There's a couple of very excited people who are apt to do that. I haven't asked them to tamp it down because some people find it interesting and I haven't seen those conversations be serious answers. That is, serious answers to genuinely new people that are trying to get help with, for example, cis194 homework. People working on cis194 get context-appropriate answers from what I've seen. &gt; I think I actually created #haskell-beginners (although I handed it over to you in less than a minute). No, I was the first to join it from my POV and I registered the channel with ChanServ. Nobody handed +f/+F to me. I believe you might've been the person to suggest using cis194 to teach Haskell though and for that I'm very grateful. It's been an excellent bridge to what I hope will be even better material for learning Haskell in the future. &gt;I probably would have tried to stick around and be more active in trying to keep the helpfulness ratio high. Somebody like you who has a spirit of helpfulness, who is kind, and has some experience teaching and learning Haskell would be very helpful for the people learning in that channel. You'd be warmly welcomed. If you don't like the way people are teaching in #haskell-beginners in future, talk to them about it. The channel is for the learners, not the teachers' egos. Edit: When were you in the channel last? The composition of the channel has changed considerably in the last few months.
Here is my point. Somebody posted on twitter _just_ the title of a talk, and then an emoticon of somebody flipping a table. That's it. To their followers. It wasn't an attempt to communicate a deep point to a larger community or anything else. _then_ a bunch of people jumped in to defend some talk title, when people didn't know what the talk was, why it was being given, or what it was about. (The actual person presenting the talk finally did join, way latter, just to ask, very politely, what the objection to the title was). So here's a suggestion. I know twitter is a public medium that just feels private sometimes when people expect to just be having a conversation with their friends. But let's not run around starting arguments with people we don't know when they post quotes and emoticons. And let's not complain about how they're not communicating well when clearly they weren't trying to communicate with _us_ in the first place, but just venting a little to their buddies. Using such a discussion to attempt to talk about the "Fast and Loose" paper etc is precisely the sort of "adversarial, confrontational teaching style" being applied to "a broad range of interactions besides those that are understood as teaching situations" about which I posted. For crying out loud. Somebody posts a title and an emoticon on twitter. Do you know they're a student even? Why are you talking about "a student". They're just a person. On the internet. Who posted an emoticon of someone flipping a table. They didn't sign up for any responsibilities at all! So here's a concrete proposal: even lots of us love to teach, let's stop pretending that every person we have a passing interaction with is potentially or should be "a student" of what we feel like teaching. And even when we're sharing knowledge, let's recognize there are lots of _non-teacherly_ modes in which to do it, as one often shares and communicates with colleagues and equals, some of whom just may not have read the same papers we have. And in fact, if someone is already out there discussing programming with folks, going to conferences, reading papers, listening to talks, let's really _really_ _really_ __really__ avoid the presumption that maybe they somehow have a case of "laziness and unwillingness to take responsibility for his/her own learning". People that fall in that camp, odds are, will not be discussing programming on the internet to begin with!
Which is partly why I encourage any Haskell related channels get added to http://ircbrowse.net/ - especially ones not managed by haskell.org people directly.
If anyone is interested in accounting in Haskell, there's hledger, which is a clone of ledger, a powerful double-entry accounting engine based entirely on parsing a simple text file of transactions maintained by the user.
The talk is aimed at Scala programmers. The topic is that the purely functional core of Scala is powerful and reasonable to reason with even in light of Scala's impurities. It's very much directly in line with the paper's meaning and is being presented to a technical audience who may have even read the paper already but certainly could. *Edit:* pchiusano points out that I'm wrong in my interpretation of the talk description per the presenter.
It's not nearly as good: You have complete erasure on the type level, and no erasure on the term level, which absolutely sucks. You have a fundamental disconnect between the term level and the type level, because the terms can contain bottoms and the types can't. Here's an example that you simply can't do with singletons, due to lack of function promotion: Prove that [] is a right-identity for ++. Even if you add function promotion, the semantics of type families and of value-level functions are quite different, so there's no way you can know that the promotion is actually "correct".
But I'm not talking about promotion at all. I'm saying I want a value-dependent function from a value of type String to a type. Is that so much to ask? It's a basic definition in any dependently typed language, after all.
Your suggestion is dismissed.
IMVU hires interns, I didn't see any this year. The last batch of interns I know of worked on a Haskell project. I also don't know how to apply. I'm on vacation, I wish I was more helpful :(.
ICFP winner and Haskell research paper publisher has written this 
Moderation in all things, especially moderation
just to clear things up A) no irc channels are managed by "haskell.org" people (by which I assume you mean the haskell.org committee). B) ircbrowse.net is chris done's personal project and the channels there are just the ones he's happened to decide to turn on. if you want to add more channels, i'm sure he'd be fine with it, although you also would need to clear permissions with the channel ops, as many channels, for various reasons, have deliberate no-logging policies (which are really "enforced" as a matter of etiquette rather than something that one can actually enforce by technical means).
&gt;Somebody posted on twitter just the title of a talk, and then an emoticon of somebody This isn't quite true; she replied to her own tweet with this: &gt;Why do all FP and FP-adjacent conferences these days think it's necessary to have condescending presentations about strong typing, etc.? And that tweet has immediately made it both "an attempt to communicate a deep point" about a large community *and* brought the word condescending (not just an emoticon) into it. That tweet was followed by &gt;20 tweets of people agreeing with her that the FP community is full of condescending people who are smug and make people feel uncomfortable. Nobody argued with her until quite deeply into the thread. The person who finally brought up the "Fast and Loose" paper at first only mentioned that it was a reference to the paper and was immediately and repeatedly accused of being condescending. Perhaps 'condescending' was the word of the day. &gt;Do you know they're a student even? I wasn't writing about her specifically. I don't know much about her personally, which is why I have refrained from making comments about her personally. However, one assumes that people go to conferences in order to learn. In that context, one is a student. One may also be a teacher at the same conference, of course, because any one person has things to teach and things to learn. I don't understand why this is even objectionable. Twitter is not less public than my blog, yet at this point many, many people--most of whom I do not know personally--have argued with me today about what I wrote on my blog. Do you have an objection to them arguing with me when maybe I was just venting to my friends? No? No, neither do I. I welcome the discussion, including the constructive criticisms; it's great. The only thing I would ask is that people read the post somewhat more carefully and note that I did not accuse any specific person of being lazy and unwilling to learn. I said that is "frequently" the case in my experience. YMMV, of course, but when I hear vague accusations of an entire community being generally condescending, I find it *frequently* is the case. 
I'm not sure what you're doing here.
That's interesting. Are you interested in supporting view updates? Dataphor does, by applying rewriting to queries, and it works well in practice. I think PostgreSQL also supports view updates, probably (no experience with it).
Somebody mentioned that there was a functional pearl about monoids and monoid actions by brent yorgey. You said you would like to see it. I suggested that google would let you see it. You did not like my suggestion. I then pasted a link to google whereby it directs you to it. Now you can see it. Bon appetit! 
No, I made instances of `MergeSplit` different using something in the form of "(MonadPlus m) =&gt; MergeSplit (BreadthFirst m)", similar to your approach above there, but with a newtype wrapper. `MonadPlus` just got involved because its methods are needed to describe what happens if a search through a space is aborted at any point.
haskell does not need more opinioneering, it needs proofs from the real world that the "better way" is actually better. we've had ten years of advocacy for stable tools twice as old....so where's the beef?
why would someone need to know what lambda calculus is to support an assertion about haskell's popularity?
Heh, same here. I wasn't terribly good in mathematics anyway.
&gt; There's a pervasive meme that "everything is equal" or at least "everything is a trade-off"; all too many engineers just take it as a given. I don't usually agree with it, but often catch myself making or voicing that assumption just because it's what everyone else seems to believe and profess. Also known as a fairness bias. "Bias toward fairness means that if the entire republican caucus were to walk into the House and propose a resolution stating that the world was flat, the Times would lead with, 'Republicans and Democrats Can't Agree on Shape of Earth'." I've sometimes caught myself stating intentionally vague things, or introduced "the other side" in my comment even when it's irrelevant, just to not appear zealous. I guess the problem with having strong opinions is that sometimes they are wrong. And when they are wrong it's bad that they were strong. I'm sure the IS fighters in Syria and Iraq have strong opinions and they think that's what leads to advancement. I'm sure the western military inventions in other countries are powered by strong opinions because that's what leads to advancement. I'm sure the crusades were powered by strong opinions, because that's what leads to advancement. I'm sure the same thing goes for witch-hunts, oppression of minorities and other terrible things. Strong opinions lead to advancement, but they also lead to a few steps back. We need to be humble not because of bias, but because *we might be wrong*, like countless of convinced people before us.
That has the presence of haskell.org people is more what I meant. #ghc comes to mind - tons of regulars. Actually, getting #ghc logged, if not otherwise prohibited, could be kinda cool given the questions that occur there. 
Yeah, the title is that fast and loose reasoning is *morally correct* (i.e. in many cases time efficient), not that it will necessarily produce 100% accurate results. Another thing that's time efficient but not ideal is having just a single person review a piece of code. The second person will uncover more bugs per time unit by reading some other piece of code. This will result in a few more bugs in the code, but a much lower total cost of the software.
This actually happens quite a lot. 
I'll let you off that one as you are clearly recounting your personal experience with one particular niche!
&gt; I have definitely seen more of it over on /r/programming though. I'll have to take you up on your offer some time! Hopefully I won't have to! :) Yes please do, *especially* if it happens here or on /r/programming
Link?
Of course you can, it at least two ways: - You can regard them as functors `Set × Setᵒᵖ → Set` (i.e. profunctors). Then you can for example take ends and coends. A nice consequence of parametricity is that every value of type `∀ a . f a` can be regarded as an element of the end `∫f`. - You can think of them as functors `core(Set) → Set`, where `core(Set)` is the groupoid of isomorphisms of `Set` (`core` is right adjoint to the inclusion `Grpd → Cat`). This is useful, because you can take a Kan extension to convert such a functor to a conventional `Set → Set` functor (e.g. using `Yoneda` or `Coyoneda` from the [kan-extensions](http://hackage.haskell.org/package/kan-extensions) package). 
If by "complaint" you mean the accusation that "Functional programmers are condescending" then I'm not dismissing that. Quite the opposite! But I am saying the correct response is *not* to discuss the minutiae of complaints ad nauseam. The correct response is for as all to make a little more effort to be (more) decent people (than we already are).
[The offensive talk being given on a previous occasion.](https://www.youtube.com/watch?v=hzf3hTUKk8U)
Having looked at the examples my respondants have provided of Haskell programmers making this claim, I agree with what they mean by this statement (and it's true!) because I've experienced it myself. However, the problem is the phrasing used. It seems that when Haskell programmers say "If it compiles it (usually) works" they mean * "If a module compiles it frequently turns out that it doesn't contain any bugs, because the type system prevented a lot of bugs, and my correct coding prevented the rest." or * "If compiles it works, because relational parametricity guarantees there are very few inhabitants of the type I specified and I implemented the correct one" However, programmers of other languages hear * "If a Haskell project compiles then it is guaranteed to be bug free" No one is at fault here, but it *is* a miscommunication and we need to explain ourselves more clearly. Your blog post is an interesting example of the right way to interpret this claim. (Thanks for writing up your experience, by the way. It's good to have examples available of useful things like STM for network servers.) You managed to design many bugs out of the game by using the type system but there *was* an important bug which the type checker was perfectly happy with: &gt; I used traverse instead of both for tuples. I expected them to have the same result, and it “typechecked” because my tuple was of type (a,a), but the Applicative instance for tuples made it obvious this wasn’t the case. That took a bit longer to find out, as it impacted half of the military victory points, which are distributed only three times per game. 
No the fast and loose paper uses the everyday meaning of "within the spirit of the law".
&gt; As to the latter point -- I consider the habit of some to just pop up to tell other people how they're wrong, not in a friendly way, but in a blunt way, as the "understood as teaching situations" element. Ah, I am even guilty of that myself: http://www.reddit.com/r/haskell/comments/2kkrd3/category_theory_for_programmers_the_preface/clml30k Often I will make a short snippy reply to someone just for the sake of "correctness", with no malice in my mind whatsoever. However, I'm sure these snippy replies are actually rather unhelpful, don't really correct anything, and are not even precise enough to be unambiguously correct anyway. Point taken. I will try to be more careful. 
What do you imply by "support"? You're free to emit any kind of statements with Hasql.
That wasn't actually my idea. I've seen people suggesting it in discussions concerning a single element tuple.
I really didn't like that talk either. The authors don't even know enough about the subject matter to discuss it authoritatively. And _this talk_ is the face that type theory has to the outside world. That's a problem.
&gt; Part of what is going on is really bleed-over from the "scala FP wars" where on the one side you have people who really embraced scala as a place to do Haskell/OCaml-style FP, and on the other the "mainstream" of the scala community, as it has gained more commercial usage especially, really wants to treat it as a "better java". Things are fraught there, and both camps have amped up the rhetoric a lot, leaving lots of people caught in an unpleasant crossfire :-(. This is something I've also observed. Actually reading some Scala FP people's discussions they seem so much more hostile than Haskellers.
It already is: https://phabricator.haskell.org/chatlog/channel/10/
BTW, the Arrow of Control.Arrow isn't the same thing as a category theory arrow (morphism) at all, they're more like a profunctor. 
Keeping in mind tight connection between category theory and homological algebra, can someone explain why the methods of the latter do not naturally arise in programming?
Could you please note in your documentation that your package supercedes the older one?
Yes, it will be handled by the weekend.
Super, thanks! Your documentation already looks very good, by the way.
Yes, we'll either merge into stripe or deprecate stripe, soon.
&gt; parametricity being stronger than naturality Is this something which can be briefly explained in a reddit comment? If so, I would be very interested. (To be more concrete: I'd like to know what "naturality" is, and how it relates to, and differs from, parametricity. I think this missing connection was something which bugged me when I was reading the HoTT book. (I got about halfway.))
[Here's the package.](http://hackage.haskell.org/package/monoid-extras)
This post is one long tiresome non sequitar. Someone wants to write a book on cat. theory for programmers. A normative position such as "[c]ategory theory has no place in a programmer's toolset" is a nonstarter for the following reasons: * if you disagree what does it matter what /u/tactics thinks on the topic; he's simply wrong. * if you happen to agree, then he's just preaching to the choir regardless of the argument. * as for others sitting on the fence obviously regurgitating all the introductory blurbs and paragraphs on cat. theory isn't going convince anyone one way or the other; they're still probably sitting on the fence Obviously, the poster's other agenda is to show how much cat. theory s/he has learned in order to establish a position of authority and then then suggest what other people "should" be learning. It's a bullshit position because it's unwanted advice. No one was asking whether one should or should not learn cat. theory. To the individual who wants to write a book on cat. theory for programmers. Go for it with Spaghetti-Monster speed. I believe normative positions have no place in technical discussions.
I'm not sure who you've been talking to, but personally I always tell people that there's no need to learn category theory to use Haskell. By using Haskell one will pick up a few things by osmosis. :)
I guess we found *a* source of condescension! 
Thanks :) big thanks to Fuuzetsu for that
This looks great! One advantage of this approach is that it essentially replaces `c2hs`. If you want, you can still write Haskelly library bindings on top of this, it's really just a different viewpoint on the FFI. I took a somewhat similar approach in [threepenny-gui][1], albeit with JavaScript as the target. I encountered two main issues: 1. *Marshalling foreign objects.* From your slides, it appears that you can marshal primitive types automatically with some annotations. (I use the type signature as annotation.) However, what about compound data, like `struct`s or objects? Is it possible to pass a pointer to an Obj-C around in Haskell world, with the intention of only manipulating it from inlined Obj-C code? 2. *Garbage collection.* Once you can pass pointers managed by the other language around, you get in trouble with cleaning them up. One prototypical example is a UI widget (button, managed by Obj-C runtime) with an associated event handler (`onClick`, managed by the Haskell runtime). The handler is alive as long as the widget is alive and vice versa. This is a circular dependency between two objects in different runtimes, I don't think there is any garbage collector than can collect across runtimes. How do you deal with these? [1]: http://haskell.org/haskellwiki/Threepenny-gui
Oh my god.. I watched like a total of 3 minutes, skipping through. Just those 3 minutes were horrible. The woman seemed particularly obnoxious and condescending. 
Thank You will check it out.. 
&gt; I'm saying I want a value-dependent function from a value of type String to a type. If that's all you want GHC Haskell is willing to give that to you. Strings are available at the type level. (I believe one of modern incarnations of extensible records uses this.) You just have to treat them as not-lists but rather as indivisible symbols. Dependent-printf requires the ability to `viewL` the type-level String, which isn't available in GHc Haskell.
I would argue it actually was a *feature*, as the bug was only found by a disgruntled opponent that complained he shouldn't have lost to me ;) However I think my claim still holds : it *mostly* worked as expected.
It easy to see what side someone is on. Ask them about Scalaz. If they ban it from their projects, they are on the "better Java" side. If they require it in their projects, they are on the "Haskell/OCaml FP" side. If they haven't heard of it, they probably haven't written enough Scala to be drafted, yet. Other opinions are rare [citation needed] and indicate a possible defector (one way or the other).
Oh, huh. I'm not deeply familiar with Rúnar's style so I had actually assumed, just from the talk description, that it might actually be a bit about "fast-and-loose" reasoning in a Scala context where there is a great deal more impurity—e.g. that you actually can't be as fast and as loose in the context of so many side effects.
And this comment is why I find the Haskell community so nice. Someone outright attacks it, and they stop and examine the claim to see if it is true. :)
The subtlety here is that if you try to win someone over purely by contradicting them in a factual way, that will usually backfire. This is because things that we believe in have an emotional component as well as a factual one. If I weren't just making an off the cuff reddit post I would cite more sources. I do not think this is a particularly novel idea though. This is especially important when someone comes to you and says "I don't feel welcome in your community." Ok, sure, you can easily come up with plenty of evidence that the community is welcoming. But that is precisely a kind of response that makes someone feel unwelcome. I think it is important to engage everyone who feels unwelcome because I want everyone to feel included, which means engaging them at an emotional level as well as a factual level. That is at the level of individuals. At the level of the entire community, I sure as hell want a conversation about the social aspect of the Haskell community! It's a community, after all! I've been to ICFP, I've got contacts in multiple local Haskell communities. I've been around IRC more and less actively for several years. I have an emotional connection to many Haskellers and to the community at large. In many ways, when someone says they feel unwelcome, and a small group try to dismiss those sentiments by saying they are a distraction, for example, I feel like that's just an emotional rejection of that person's experiences. I think that's not productive.
I always write it without the space.
I think the former is more common, but it doesn't really matter right?
I don't use a space unless it won't parse, like when using [lazy patterns](https://www.haskell.org/tutorial/patterns.html#sect4.4): ghci&gt; (\~x -&gt; x) () &lt;interactive&gt;:9:6: parse error on input ‘-&gt;’ ghci&gt; (\ ~x -&gt; x) () ()
&gt; Strings are available at the type level. No, those are elements of a different kind, called Symbol. I want a function from a value of type [Char] to the * kind. Ultimately, I'd like to, for example, read my format strings from a file.
+1 dmjio, I'm very happy to see this released.
she literally only chimes in to add a useless troll on to each of his near-worthless observations
i think because quotients are hard.
Naturality is the property of a natural translation. Given two functors `F`, and `G` both from category `C` to `D`, we associate to every object `X` in `C`, a morphism _in `D`_ from `F(X)` to `G(X)`. Furthermore, we specify that these morphisms have to obey a special "niceness" property that says we can "do work" in the image under `F`, and then _later_ move to the image under `G`, or we can first move to the image under `G`, then do work, and the two come out the same in the end. One thing that "clears up" natural transformations is to move up a level. Often we want to work in a functor category. So we fix two categories `C` and `D`, and we say objects in this functor category are functors between `C` and `D`. What are our morphisms? Natural transformations of course! (this is the sort of machinery we need to discuss the yoneda embedding, for example). In any case, how does this relate to parametricity? Well, we can observe that a function of type, say `forall a. Maybe a -&gt; [a]` acts a lot like a natural transformation. It takes discusses objects in `Hask` under the image of two functors, and lets us send objects from the `Maybe` functor to objects from the `[]` functor. Furthermore, it turns out to be true that such a function, because it can't "look at" `a`, obeys that nice property that we can "do work" on either side and it translates. In particular, name our function `may2list` (note that it could behave differently than `maybeToList` in that it could send the element zero times, two times, etc.). Now we have the property that for all functions `g :: Maybe a -&gt; Maybe b` there exists a corresponding `h :: [a] -&gt; [b]` such that `may2list . g === h . may2list`. The existence of such a function in fact comes as a free theorem! But a free theorem doesn't "come from" naturality (it just gives it to us). It comes from parametricity! So, for example, a particular function of type `Maybe Int -&gt; [Int]` could be treated as a a natural transformation as well (in this case the functors would be from a subcategory of `Hask` to `Hask`, rather than full endofunctors on `Hask`) -- suppose this function just fmaps adding one and then calls our prior function. But in this case, our knowledge it is a natural transformation doesn't come from parametricity, but just from inspecting the thing directly [i.e. another function of the same type might not be natural]. So it turns out that parametricity is a stronger thing than naturality, because you can have naturality without it, and furthermore when you _do_ have it, you can prove things that naturality alone won't let you. For example, we know that the church encoding of a list as a fold gives us `[a] === forall b. (b, a -&gt; b -&gt; b) -&gt; b`. However, we can't use naturality alone to prove that identity. We have to take advantage of parametricity more directly. So if you want to use naturality as a tool, sometimes you have parametricity lying around and it makes things very easy. However, if you want to use parametricity as a tool, then simply learning about naturality is not adequate. Hope that helps? (and if not/for more, see the "Reynolds Program" paper I cited above)
I do think that if we build up the right toolset (we're not there yet) then notions of exact sequences and left exact functors, etc. may actually manage to become useful. This requires shifting our perspective a bit to think about functors in a broader and more general setting, however.
Thanks ;)
Take `f` and `g` and separate the positive and negative occurences of `a` in each, treating each as a bifunctor `C^op * C -&gt; C` (where C is your category of interpretation). Then a parametrically polymorphic function of the type you give will be a dinatural transformation. (This can be derived from parametricity.) In the case when `f` and `g` have only positive occurences of `a`, they are both functors, and the function will be a natural transformation. Parametricity is *really* powerful. My hunch is that it will eventually be understood as the most important idea that computer science had to offer mathematics.
&gt; But as a tool for programmers, it does not give you enough leverage to justify the steep cost of learning it. From my admittedly limited experience this is so true. Every time I've tried to understand some category theory related statement about programming I've had to spend an hour deciphering the terms. You google the terms which brings you to a page with more terms that you don't know, and you do a depth first search until you've understood all the terms. Then finally you can try to understand the category theory statement, and invariably the meaning is something so utterly trivial that you kick yourself for wasting the time trying to understand the category theory formulation of it. For example you might spend an hour trying to decipher the [wikipedia page on the yoneda lemma](http://en.wikipedia.org/wiki/Yoneda_lemma), and as far as I can tell it says no more than the utterly trivial statement that if you have a binary composition operation `@` then you can equivalently work with functions `\b -&gt; a @ b` and use function composition instead. No shit! Another time I tried to understand some cartesian closed category stuff and as far as I can tell it says no more than that you can represent variable environments with tuples. Duh. So I'm left with the perhaps incorrect idea that category theory is how you make trivial stuff sound sophisticated.
Again, I find this an argument against the special behavior of treating `"literal"` as `"literal" :: Symbol` instead of `"literal" :: ([Char] :: Kind)` rather than an argument that GHC Haskell is not dependently typed.
&gt; at an emotional level as well as a factual level. I've always found the former more difficult than the later, so I focus on the later. I'm not a good person to be handling outreach, clearly. :/
Lol, I'm well aware that concurrency is not exclusive to Haskell. I was just providing an example where Haskell does well. I agree with the guy above me that you just need to stop trolling.
Congrats man :)
&gt; Parametricity is really powerful. My hunch is that it will eventually be understood as the most important idea that computer science had to offer mathematics. I wouldn't be surprised. I wish I understood it :S (Thanks for your cstheory.stackexchange answer on the subject, by the way :)
A lot of the comments in this thread are indicative of the condescending or dismissive nature Haskellers have been painted with.
And kind of like Scala's embedded XML literals, for which I have a a bit of a soft spot. 
&gt; There's a pervasive meme that "everything is equal" or at least "everything is a trade-off" The problem with the attitude that Haskell (or Python, or whatever) is just better without a trade-off is that it pre-emptively dismisses any arguments that it may not be unequivocally better. It also implies that if you don't agree that means I know something that you don't, or worse -- that you're stupid. That's the attitude that people find condescending. It also doesn't help that people in an echo chamber start to find it acceptable to be a little loose with the truth, so the statements made are often actually false if you scrutinize them more carefully, or statements that are technically true but misleading: What people say | In reality... -------------------------------|------------- When it passes the type checker your program works | When it passes the type checker your program is more likely to work In Haskell you express what not how | Programming in Haskell can be a bit more high level than in other languages in some cases, but usually you're still expressing the ~~what~~ how Functional programming is naturally parallel | When you rewrite your program to make the data dependencies more suitable to parallel execution, and you eliminate laziness, and you divide up the work into large enough chunks, then you can parallelize your program. Admittedly, when you do the same in a non functional language, you can also parallelize your program. The advantage of Haskell is that whether your program is safe to parallelize is apparent in the type system Wild statements about the curry-howard correspondence | The curry-howard correspondence has no bearing on practical programming in haskell, the logic corresponding to haskell's type system isn't even consistent Well typed programs don't go wrong | Well typed programs don't segfault (if the type system &amp; type checker are correct) Functional programming is declarative | Working in a functional language doesn't magically make your programs more declarative, but rather you can or have to restructure your program and put all the non-declarative parts in monads so the rest of your program can be declarative In a purely functional language the compiler can be much smarter because it is safe to reorganize your code in ways that aren't safe in imperative languages | The compiler can in some cases recover some of the performance that we lost by writing the programs in a functional style, for example eliminating big intermediate data structures. Unfortunately in many cases those compiler transformations are brittle and it's hard to predict whether they will be able to eliminate all the inefficiencies we introduced by writing our program in a functional style. A seemingly minor change can drastically slow down a program because an optimization could no longer be performed. An insider knows that when somebody says one of the things in the left column they really mean the thing in the right column, but an outsider doesn't know this and they take the statement on the left at face value which they will rightly find incredulous so from that perspective these statements are smug.
The thing is though, that this happening was the magic moment that got me hooked on Haskell...
I agree with you, but I'd like to point out the tweet &gt; uhm. It's a reference to the fast and loose reasoning paper. is an unhelpful and opaque reply (sort of like responding to "what is a monad?" with "a monoid in the category of endofunctors"). The conversation may have gone in a different direction if he had instead replied &gt; it is a reference to a paper titled "fast and loose reasoning is morally correct". uhm could be intepreted as he is saying something painfully obvious, "the fast" could imply that the op should have known about the existence of said paper. In a medium where it is difficult to convey tone, maybe it is better to ere on the side of caution, especially when your dealing with a prickly person. but then again, with some people you just can't win no matter what. 
I certainly support the spirit of the slogan. I'm going to see if I can clarify my thoughts on it and write an article.
I also prefer without the space, likely due to exposure to [lambda calculus](http://en.wikipedia.org/wiki/Lambda_calculus) prior to spending lots of time with Haskell. Lambda functions in lambda calculus are always (in my experience) declared without spaces before the '.' as in the identity function "`λx.x`". I'd probably duplicate this completely, but the Haskell equivalent, `\x -&gt; x`, looks better to me with spaces due to the largeness of "-&gt;" and the possibility of having multiple space-delimited parameters (not allowed in the strict lambda calculus).
&gt; It's the evangelical types that get preachy and scare off people. Absolutely. It's a very loud minority in the Haskell community who try to spread Haskell with a religious fervor. This leads to these current perceptions of the community at large. Frankly, I don't even understand why some are even spreading Haskell like they're doing god's work. Let the language and the production of the community speak for it self. I haven't seen any other modern language community with this level of proselytizing... except maybe Lisp.
I think you're spot on with your comment on parametricity here. It's something truly novel in logic. 
&gt; but usually you're still expressing the what I think you mean "the how". Good analysis in general.
If you're going to make such claims you have to provide specific examples. I'm not saying you're wrong, but unsubstantiated claims like this are not helpful to anyone.
Is is [Vindinium](http://vindinium.org/)?
It's an undergraduate thesis that's very readable.
I didn't know this existed, but it looks like it stopped logging in the middle of September?
Michael and I are the maintainers of the `stripe` package (Michael is the original author). When `stripe` was first written, Stripe proper didn't have much in the way of protocol versioning or changelogs. This allowed for protocol changes to slip through without a complementing code change. Furthermore, none of the maintainers work on hs-stripe full time so no one really had the time to go back and manually diff the haskell implementation against the protocol. So I agree a refresh is needed. I would prefer to replace the current code with the new project and just call it "version 2.0" or something along those lines instead of having a fork. The community doesn't need two Stripe libraries. The big change between the `stripe` and the new `stripe-haskell` packages are Conduits vs. IO-Streams. If there are enough users using Conduits and enough demand for IO-Streams, that may be a reason to have two projects, perhaps with a shared aeson-based common parsing dependency. I absolutely think we can avoid a fork here.
ya but how often does anyone use that
It's very hard for me to tell what's real in that Twitter thread. I don't know how to make any progress in understanding what's really going on when starting from assumptions about the internals of someone else's psyche, as in claims of "condescending" and "smug". I wonder: could the conversation be rebooted from an objective enough starting point that it doesn't quickly devolve into blame and name-calling? Alternatively, could we interneters somehow foster a safe enough conversation space that people who are feeling embarrassed, insecure, and sad about their experiences in community-based learning could say so vulnerably and directly/honestly---owning their inner struggle with self-worth etc---instead of blaming it on others? After all, I think these inner feelings are part of our shared human experience. My own sad guess is that the internet can never be such a place, because (a) text lacks warmth especially between strangers, and (b) people are quick to criticize, especially under the cover of anonymity.
Because the conversation so far is very subjective, it's hard for me (and some others, I'd guess) to know what's getting interpreted as dismissing of someone's feelings here. I don't know how to progress while keeping these claims vague. Specific examples could help.
I use a space when the argument is a parenthesised pattern. Emacs considers `\(` a bracket whose matching sequence is `\)`, due to Emacs- and grep-style regular expressions, which screws up jumping between matching parentheses. A friend of mine said that `\(…)` can also screw up syntax highlighting in Vim, but I don’t know if that’s still the case. The alternative of course is to use `UnicodeSyntax` and just write `λ` (`\lambda` in Emacs’ TeX input method).
I asked in channel, the URL changed (don't know why): https://phabricator.haskell.org/chatlog/channel/3/
My bikeshed shimmers irridescent.
Ok as a newb to haskell and only reading bits and pieces about Category Theory, I hope you don't mind my question (but would be very grateful if you could clear things up) When we talk about parametric functions with typeclasses (Monoid), in some ways are we dealing with a subcategory of Hask in the same way many categories are subcategories of Sets? Would it be operating on the Category of Monoids in the Category of Hask? Does this have any relation to paramatricity? If I know there's a function (nat ?) from A to B, and I know the category for B has fewer stuff, that seems helpful in reasoning about it, no? I hope i'm not just talking nonsense, I appreciate your detailed responses in this thread! 
I liked the quote too, those are some famous last words.
As hledger author and user of traditional web frameworks, I was very interested in this. It's a really nice tutorial and tour of some pretty amazing libraries. It could be a great way to (just for example) re-do some back and front end parts of hledger-web (hledger's web UI). The [add form](http://demo.hledger.org/journal?add=1) would be a nice starter project ([cf](https://github.com/simonmichael/hledger/blob/master/hledger-web/templates/default-layout-wrapper.hamlet#L96) [current](https://github.com/simonmichael/hledger/blob/master/hledger-web/Foundation.hs#L280) [code](https://github.com/simonmichael/hledger/blob/master/hledger-web/static/hledger.js#L126)).
And comprehensive at &gt; 100 pages.
Well, we _can_ say there is a category whose objects are types that are instances of Monoid, and whose morphisms are monoid homomorphisms, and that gives a notion of the category `Mon`. You can then use this construction to e.g. construct `[a]` as the "free monoid generated by type `a`". But we don't have to do that. It's just a possible way we can construct categorical structure on top of Haskell. That's not the same as a parametric function over `forall a. Monoid a =&gt;` though. Because that's now a function you've somehow defined over "all" instances of Monoid. But we can go through and do something like prove using parametricity (I think) that such a function can always be represented as a function on `[a]` instead. So these are interesting things to play with. I don't know if they're helpful or not, depending on what concrete sorts of things you want to investigate. The trick is to know enough theory so that when you want to understand an aspect of something, you know which tool to pull out of which box to give you the right sort of account. A big leap for me was seeing that the stuff I cited above, for example, didn't all use the precise same notion of how to treat Haskell categorically. Rather, that sort of work knocks together the categorical structure necessary out of the materials at hand to help us in understanding particular things...
Everyone knows proofs are condescending.
Honestly, I would say my suggestion would be to merge stripe-haskell *into* stripe and point people only to the 'stripe' package, as opposed to the other way around (deprecating `stripe` and recommending `stripe-haskell`) Why? Because I honestly really, really dislike the trend of people naming packages with the 'foo-haskell' convention - when you add a dependency on something in Cabal, it's almost certainly Haskell! The extra part of the name is simply redundant. It's great to name your GitHub repositories something like 'foo-haskell', since it helps SEO. But it's really redundant and annoying when used in the actual *package name*. In this case I understand since the name was already taken - but since you took over so well, I think it'd be better if you took the 'official' package name anyway. :) (And I've also spoke of this before about other things, so I hope I don't sound like I'm singling you out...)
Of course ;)
According to the output from GHC, you're using an untested version of LLVM. 
Amusingly, all the original Scalaz developers are of the opinion that Scala is a "better Java", but suggesting that to others is considered hostile.
Well, it's technically tested now! :)
STM makes Haskell concurrency much easier to use than in other languages
Why did you feel the need to invent your own connection pool? Bos's Pool is already pretty much tuned for any high-end scenario you can imagine.
I agree, keeping 'stripe' is the way to go. Michael (Stripe employee) said the same. x-haskell *is* ugly. Once I get maintainer rights on the 'stripe' package we can deprecate stripe-haskell. If only there were a way to remove a deprecated package altogether...
If there is http-conduit demand we could use CPP flags to provide a conditional backend, so 'cabal install stripe -fconduit' would compile with a conduit backend. I could do it, but I'd be more than happy to accept a PR from somebody :)
Yeah... I wonder if it would be sensible to add an option to alert you about using deprecated dependencies when you `cabal install`... Right now it just hides it from the listing and search, IIRC. I think the main thing unfortunately is we don't want to upset people by deleting old packages, who might complain, because maybe your package worked great a while ago before dep XYZ was deprecated, and still does with the deprecated dependency. Deleting it would be rather rude. Also, if you use a deprecated package and it's deleted, it might not be clear which one to move to (in this case it's obvious, but generally, maybe not so much). In any case - great to hear. I hope to be using this Stripe binding myself soon. :)
Hi simonmic Thanks! nice to hear your positive impression about the tutorial &amp; libs. If you do it, don´t hesitate to ask if you have any problem at: https://github.com/agocorona/hplayground/issues or https://github.com/agocorona/haste-perch/issues
Yes. Thank you. I think it's page 14 or 28 (PDF), but you're right. I'll fix that on the [GitHub repository](https://github.com/jpvillaisaza/cain).
Thank you for sharing and for your comments. I'm the author of this undergraduate project and I'm very happy to see people interested in it.
Am I reading it wrong, or does it say that Lewis Carroll wrote Alice’s Adventures in Wonderland and Through the Looking-Glass in 2004? As for the content of the paper, it looks pretty great, and I'll have to take some time to read through it.
I have not used Scalaz in any projects because I think my co-workers would prefer I didn't, and I'm happy using it as "just" a "better Java". A few case classes/objects (some of them value classes/objects), immutability by default (instead of my slathering of `final` all over my Java code) and a clearer separation between static members and instance members are all big wins, even before you get into Scala's powerful type system and the awesome things that Scalaz does with it.
#Tested: Unsafe and Ineffective Hey, guys! I think I found a motto to replace "Avoid success at all costs." ;)
No, 2004 is the year of the Barnes &amp; Noble edition. Thank you.
Also, I totally promised David I was going to release my stripe library months and months ago and then never got around to it. So I take most of the blame for there being yet another library :)
I agree about the io-streams, http-streams, pipes-streams issue. And in my library I opted to have a common, pure, backend-agnostic part that does most of the dirty work (stripe-core) and then a thin wrapper that ties it to a specific http-client (stripe-http-conduit). https://github.com/stepcut/stripe We definitely do not need *3* stripe libraries, and mine is several API versions behind. So I'd much rather work to refactor DMJs than update my own.
That actually sounds super-interesting. I'm planning to teach my self web development over winter break with Haskell (Snap framework) on the back-end and React on the front-end. And I'm used to service oriented apps from interning at Amazon. Can I (or other candidates) contact you somehow?
I personally blame you as well :)
I'm all for a backend agnostic solution, let's definitely discuss a merger.
This sounds like the perfect excuse to finally plan that hackathon. 
Yes, you're right. (that's what I meant by "which might be the error") This wasn't a concious decision from my part, and I don't know if this type of problem is usually connected to LLVM or perhaps the other things I pointed out. Just wanted to get input on which of the things to troubleshoot first, and whatever else.
I've local/llvm 3.5.0-2 installed. I'll check out the mailing lists. Thank you.
I use a library, which is just an improvement over the one you mentioned. It's neither my invention or my work.
There seem to be known problems with LLVM 3.4 and 3.5^[[1](https://ghc.haskell.org/trac/ghc/wiki/ImprovedLLVMBackend)]. The plan seems to be to include a LLVM version with GHC beginning with 7.12 to avoid these kind of incompatibilities in the future (and to make LLVM a first class backend).
&gt; `id ∘ undefined ‌≠ undefined` Really? Why not?
Because `id ∘ undefined = (λ x → undefined) ≠ undefined`. In fact, seq (λ x → undefined) () = () seq undefined () = undefined In particular, the η law for functions doesn't hold in Haskell.
To be fair, #9142 is about things going wrong at compile time - jarlg might be using pre-7.8 GHC to side-step it?
I'm kf, the person who posted the tweets. To start: &gt; Here is my point. Somebody posted on twitter just the title of a talk, and then an emoticon of somebody flipping a table. That's it. To their followers. It wasn't an attempt to communicate a deep point to a larger community or anything else. then a bunch of people jumped in to defend some talk title, when people didn't know what the talk was, why it was being given, or what it was about. (The actual person presenting the talk finally did join, way latter, just to ask, very politely, what the objection to the title was). This, basically. It was a Sunday afternoon, and I wasn't trying to make a deep point. Similarly, I wasn't trying to call out a specific person, as unlikely as that may sound. To be frank: I was posting things about the Backstreet Boys yesterday, and *no one cared.* Likewise, people talk about functional programmers being smug [*basically all of the time*](https://www.google.com/search?q=functional+programmers+smug), and no one usually cares. So, it didn't occur to me that people would care *this* much about a few things I tweeted in haste, when such comments are so common. And they are. *They are very common.* ----- That said, because people seem to have given more thought to my tweets than I initially intended, here's some background: * I've tried on multiple occasions to bring friends to FP meetups, albeit mostly in San Francisco. These attempts have invariably gone poorly for one reason or another; either someone tells my friends that the languages they code in are insufficient (Even for building basic web apps?!), or--on the occasions I've brought friends who are women--someone has made comments about how women belong in people-oriented roles, not in engineering. Literally. Every. Time. When this happens, my friends either abandon the notion of learning FP, or they do not make another attempt to learn FP for at least a few months. One can say, "Not all FPers!" and that's true. But, when I can't even bring a friend to a meetup without having to deal with condescending ("having or showing a feeling of patronizing superiority") comments, I personally think *it's a common enough problem that people should acknowledge it.* * Last month, I went to Strange Loop, a conference that made a wonderful effort to welcome newcomers this year. The organizers did an absolutely amazing job at welcoming these new community members--as did the vast majority of the attendees, since Strange Loop attracts a great crowd! I was super-jazzed to have both my FP and new-to-FP friends in attendance. But still, on more than one occasion, people in the FP community (those who coded in Scala and Haskell, mostly, although there was one "FP in JavaScript" person) said condescending things to my friends about how Ruby/Python/etc. are "obsolete": The language you *just spent a year struggling to learn* is "overly simplistic"! "Your software will never scale!" It "only" happened a few times, but at least one of those occasions was when I was standing around with a group of newcomers, and none of those newcomers had previously tried FP. It was frustrating for me, personally, because *so* many people *did* strive to make the conference inclusive. Then, there was [this talk](https://www.youtube.com/watch?v=SWTWkYbcWU0) at the conference, as well. If you watch all ~40 minutes of that talk and *don't* think it's at least a *little* smug and condescending, I mean... I have no response. One direct quote: "This is exactly why we had so many crappy, crappy, crappy Ruby developers that are now crappy, crappy, crappy Clojure developers." So, to clarify: &gt; This isn't quite true; she replied to her own tweet with this The talk from Strange Loop _was_ the talk that I was referencing in [my second tweet](https://twitter.com/kf/status/526427359071588352), although I was not explicit in stating that and definitely should have been. My second tweet was "in response to" [the first tweet](https://twitter.com/kf/status/526426856380047361) only insofar as Twitter _literally has it linked as a response_, which was unintentional. They are not actually referencing the same talk, as the talk I mentioned [in my first tweet](https://twitter.com/kf/status/526426856380047361) has not actually happened yet. So, I was commenting on the title, itself--how "Functional Programming is Morally Correct" can easily be read with negative connotations. In particular, it's problematic in the context of the Scala community, which references "moral correctness" (and similar phrases) in *absolutely* condescending ways in other contexts. See the Strange Loop talk above. I personally think that Runar's talk, content-wise, will probably be "fine"--by whatever standards--although I do think the title is unnecessarily abrasive, as do many of my friends (thus the table flip). The talk at Strange Loop, by contrast, was *not* actually fine, and was *forty minutes* of unnecessary abrasiveness. To summarize: It didn't occur to me that people would interpret the second tweet as pertaining to the first, mostly because, even though I posted them in quick succession, it was a weekend, and I was not paying that much attention. I also didn't notice that the tweets were linked as replies to one another for a fairly long time, since after posting, I was mostly just responding to replies. ...there were a lot of replies. But, I guess that's what I get for being smug and subtweeting? I'm sorry--not in the "sorry you misunderstood" way but in the "sorry I did that" way. I was unclear, and that's on me. To continue on: * In the past few weeks, there's been a lot of drama in the Scala community surrounding [Scalaz, "an extension to the core Scala library for functional programming."](https://github.com/scalaz/scalaz) To be brief: the steward for Scalaz instated a [Code of Conduct](http://typelevel.org/conduct.html). Shortly thereafter, one prominent community member insulted someone on IRC. As a result, that prominent community member was [banned from the #scalaz IRC channel and removed as a GitHub contributor to the project](https://gist.github.com/ekmett/81a507f50d857345691c#file-demands-txt-L15). People who had previously avoided Scalaz voiced approval of the decision, as having a Code of Conduct *and actually enforcing it* made them feel like perhaps they would be welcome in the Scalaz community, after all. [However, people in the Scalaz community argued that the prominent community member should be reinstated, because not enough stakeholders were consulted before banning him. Further, the steward for Scalaz, who had initially attempted to enforce the Code of Conduct, was encouraged to step down as an owner of the GitHub repo, "as more of a nice gesture."](https://groups.google.com/forum/#!msg/scalaz/EBP_7sfB2ks/TaZP2SIkD0wJ) ---- If you've made it this far, maybe you've noticed that most of this has nothing to do with Haskell, specifically. It was more "people in FP in general are being jerks right now, especially in Scala"--which is the language community through which I know many of my followers, since I tweet a fair amount about Scala. Because this is the Haskell subreddit, I'll say that the tweet I made that *was* specifically about Haskell was [this one](https://twitter.com/kf/status/526428034643922945)--which I stand by, as it is a statement and not an opinion. I know a professor who studies category theory. This professor started FP101x, found the Haskell community condescending ("having or showing a feeling of patronizing superiority"), and in under four weeks opted to invest time in Python, instead. I say this as a person reiterating the experiences of another--although, I have *also* [extended the "condescencion" umbrella to include the community of which I am *actually* a part of](https://twitter.com/kf/status/526428847898497024), since I see the problem frequently enough there, as well. Anyway, at this point, I don't know that I have any additional commentary, other than this: A nontrivial number of people I know who are new to FP have found their interactions with FPers to be abrasive and off-putting. *We* should all work on that. And, sorry, Runar. I was really just frustrated with the title of your talk, and really should have pulled that discussion into a separate thread.
Out of curiosity, why would you assume that?
&gt; I used traverse instead of both for tuples… That's not surprising. Much of the power of correctness in Haskell comes from preciseness of denotational semantics. Polymorphism, while certainly useful in its own way, reduces that preciseness. So if you use a highly polymorphic programming paradigm like lenses, you are not going to experience the same level of "if it compiles, it works".
It does, thank you! Do the conditions for naturality in the second paragraph arise directly from the conditions for a functor category being a category (in the third)? Could you give a simple example of a function `:: Maybe Int -&gt; [Int]` that's *not* natural?
I'd call it a Cabal rewrite. You can see a talk at https://www.youtube.com/watch?v=VUyIu2T1Qss, with slides at http://intelsoft-software.nl/files/mainPresRevised.pdf.
Why is a lot of monad transformers being used bad? My understanding is that monad transformers are idiomatic. Can you provide an example of monad transformer overuse?
Surely the problems is that, at most, a Haskell version cannot be completely typesafe. Presumably whatever Hickey writes in Clojure can be replicated in Haskell but may crash under some circumstances (just like the Clojure version).
&gt; I can't tell whether the concept is of any additional value that Haskellers are missing, whether there's a fundamental logical problem with transducers as conceived of by Hickey that Clojure permits because it's dynamic, or whether it's just like the fold package. I suppose the best way to make sure if it is actually useful or something that's been made to frustrate "typers" is to see how it's used in "real" code. Right now I don't see how it would be more useful than the `fold`* packages. But the limit of my language is the limit of my world, so ...
Copied from my HN comment, but I think the following is correct: You can just include local state. data Fold i o = forall st . Fold { merge :: st -&gt; i -&gt; st , this :: st , view :: st -&gt; o } type Redu r a = (r -&gt; a -&gt; r) type (i ~&gt; o) = forall r . Fold o r -&gt; Fold i r type Trans i o = forall r . Redu o r -&gt; Redu i r Now each "step" has local pure state and no other step can break abstraction barriers and view it. This also gives you all of the effects of the indexed state passing Rich called untypeable. Fold is equivalent to an infinite state Moore machine, so a stack of composed transducers applied to a base Moore machine which produces whatever result you want can be compiled efficiently. Early termination can be done by changing `merge` to `merge :: st -&gt; i -&gt; Either o st`. --- The interesting part is the existential state bit. Previously, the reducer-transformer version explored in this blog post has universal quantification which prevents you from knowing the reducer state. Purity means that you have to thread that quantity in sequentially and makes `Trans i o` equivalent to `i -&gt; [o]`. Unfortunately, it's too restrictive because it means each "stage" of your reduction must share state and that choice of state is made only at the end when you "run" the whole thing. --- Now, Rich's types seem incorrect because `(r (n-1) -&gt; a -&gt; r n)` cannot model filter or replicate. The best I can see is that he wants to make sure you don't "desequence" your state passing. This is a problem when mutable state is involved because then your mutable state does not match your local state. If you eliminate mutable state then "desequencing" is fine—whatever the history is of the state that you return from your reduction is what you get. Existential state means the r's aren't the same across transduction steps which captures other actually useful properties. Finally, I'm not sure I can agree with his distaste for Either since the Clojure code uses (reduced) which, I think, is nothing more than a dynamic dispatch on the reduction state skipping it to a return result. The same lookup happens, you just pretend like your reduction state is smaller than it actually is because dynamic dispatch helps you hide the interface.
Thanks for the link, examples are also very welcome :)
We use the scala version of it in production here at S&amp;P Capital IQ to do a lot of things. `scalaz-streams` also spun off of it. Anthony Cowley uses it for a bunch of robotics and has a ton of machinery (both literally and figuratively) built on top of it. Video of him talking about it (and other abstractions he uses) to make robots go: http://vimeo.com/77164337 The main thing i want to do with machines is find a way to adapt Atze's reflection without remorse to go back and re-unify the Plan and Machine abstractions now that we know it is at least possible.
&gt; but may crash under some circumstances Precisely. Dynamically typed programs defer type errors to runtime crashes. Why do want to replicate that?
The problem with just having `Either o st` in the step function is that you can't preserve early return when you get to `concatMap`. Since the Either is totally self contained within the fold, you won't know when you need to bail, so you have to propagate the Either all the way through to the finalizer. I tried implementing a version based on your HN post in PureScript (https://gist.github.com/natefaubion/38483b5a5e0ebd4e88cb) and its just so tedious wrapping and unwrapping state, wrapping and unwrapping Eithers, etc. That's why the dynamic version is so "nice" is that transducer authors don't have to worry about any of those details. All the state is bundled up with mutation, all you have to do is call the next step function and the `Reduced` value will always propagate.
Types fundamentally do restrict the domain of expressible programs, that's kind of the game we play and perhaps these transducers are in the "region of abysmal pain" that SPJ always refers to. Won't make loose faith in types though, there have been plenty of pathological programs like this throughout the years and invariably someone comes along and finds an elegant solution. I don't think the existence of a few pathological programs really reflects anything about the viability of unityping vs expressive types. 
Me neither, I can think of plenty of correct programs that can't be accepted in Haskell's type system, and plenty of invariants that it can't model. The question is whether that's what we're dealing with or whether it's a bad design that shouldn't be modeled, or just poor communication.
My first thought would be buildings that have tiers of 'monad transformers', except they dig down and whenever you need to 'take the lift', you rise up. The world of IO is the surface. They have a lot of typed-pipes that flow in and out, but they also have untyped canals where other languages send things in through. Oh, and other cities build buildings up (instead of down), and one repeating complaint for them is that they keep having to clean the office windows of what the birds leave behind.
Both Haste and GHCJS uses the GHC API, so they are probably more similar than many people realize. At the very least, both benefit from the extensions (with the notable exception of TemplateHaskell which Haste currently doesn't support), optimizations and other niceties of GHC. GHCJS currently has better library support since Haste doesn't currently support TemplateHaskell and is doing some funny stuff with primitives that it really shouldn't. This is definitely fixable, but requires a little bit of work. GHCJS also has a bit more solid support for sandboxes and other Cabal stuff. Again, definitely fixable in Haste, but just not done yet. GHCJS reimplements more browser functionality than Haste does, and consequently is probably easier to target towards asm.js. There's still going to be a *lot* of work to target any compiler in that direction however, enough that I'm not convinced there would be a benefit to using Haste or GHCJS over starting over from scratch. Furthermore, I'm also not convinced that asm.js is a productive direction to go in, except in some rare corner cases. In any case, asm.js popularity doesn't really affect either compiler since neither targets it (or is likely to ever do, IMO). Haste and GHCJS really target different use cases: GHCJS aims to more or less emulate vanilla GHC running on a traditional OS, paying with unfeasibly large output, while Haste aims to do whatever is sensible in the web space, paying with slightly lower portability. If GHCJS were to start producing small enough programs that I'd be comfortable deploying them, I'd love to see the two projects merged. Unless that happens, I don't really see either compiler "winning" over the other.
OMG, you basically stole my idea, but main is more a Anime than anything else, where characters would have different powers and control of some property of reality base on their language, the character with the power of Haskell would be the main antagonist (too powerful) since he want to purify the world, and JS would be the main protagonist (chidish, foolish, stupid hero), ruby would be the useless female protagonist stalker. Something alone those lines.
Oh, it's just the "Transducer as a fold transformer" type. I use it a lot when I'm writing code like this and just forgot to even explain why.. oops :( As an example, the `flatMap` transducer. flatMap :: (a -&gt; [b]) -&gt; (a ~&gt; b) flatMap f fldb = fldb { merge = \st -&gt; foldl' (merge fldb) st . f }
Thank you for this detailed response and clarifications/corrections. I feel much more informed on this now. :)
I'm not trying to be cheeky, nor to muddy the waters, nor to kick the shins of type systems and people who use them. But I think you have to be realistic about the expressivity of type systems, their diversity, and their ability to communicate to practitioners using varied languages (my audience). There are those who said - I wish Rich had just provided the type signatures. In what language? Java (most popular, least expressive), Haskell, Scala, SML, Idris? In my talk I gave an example: You could have a state machine that when given an X returns a Y, when given a Y returns a Z, and when given a Z returns an X. I think every programmer can understand that sentence. What's the universally understood type signature for that? I also said a transducer step function must accept its immediate preceding return value as its next first argument, and consumers must supply only the immediately prior return value (and not any old value of some type) as the next first argument. Again, I think every programmer can understand that sentence. What's the universally understood type signature for that? I am genuinely glad that people are taking on implementing (and typing) transducers in their various languages. But I think the linked post highlights the challenges of types as universal programming communication given current language (and type system) diversity. My saying "expressing this in various type systems is an interesting problem" is not a ding on type systems nor an assertion of the superiority of dynamic typing. As these various implementations, blog posts and discussions demonstrate, it seems merely a simple fact. 
Thanks. This is a really useful comment. In particular, your stories about FP conferences and meetups are important to bear in mind. As Tim makes the point frequently, we hear from people who did have good experiences, and who did stay the course. It is more rare to hear from people whose first encounters didn't go so well, and so decided not to pursue things any further -- but those are precisely the stories we need to hear if we want to know not only what we're getting right, but also what's going wrong.
I agree there's something a little unsatisfactory about the `Either`, but I don't think it detracts from the functionality or correctness. Do you have example code concerning that failure with concatMap? I have some working code sitting around somewhere...
Do you know lojban? If you want to construct a language this might be a good starting point.
I'm confused. Is it the typing-ness that makes this hard, or is it the effectfulness? My suspicion is the latter... Note that this effectulness that is untyped and latent in clojure-transducers _is_ problematic, because a careless user might accidentally alias the same state when they thought they were cloning, or vice versa...
Pattern matching as "rhyming" ala Cockney rhyming slang? Recursive thinking / phrasing Social hierarchies expressed as Types. "Strict" regions enforce the hierarchies. The cluttered world of parens lived in by the old timers speaking the archaic mother language.
&gt; universal programming communication I think a lot of us are looking for communication of programming concepts via some (semi-)formal system. Prose examples are not good for this; examples and tests in implementation-defined code is a bit better; actual code in an implementation-defined language is better still; but semantics given in a formal, mechanically verifiable system (such as a type system or proof system or dependently typed lang) are best. Best because there is no ambiguity, and no humans in the loop to muddy the content of the communication, and the result is machine checkable. If transducers in their full glory can't be specified independent of a specific implementation there's something very wrong. If they require a lot of mechanical support (e.g. dependent types) that would be an interesting result on the complexity of the system. Mechanize the specification and a lot of the silly blog posts disappear.
Hm, if a functional language was a city, and I was greeted by one of it's inhabitants… &gt; Lo and behold, the city of Kallesh, the purest of them all. What do you mean, you cannot see its purity? Oh, my good friend, let me show you around. See this empty ground? This is actually the tail of our calculation train. We haven't built it yet since it's not needed at the moment. Why dwell on something you might not even use? But don't worry, the blueprints are here. &gt; &gt; You ask about purity? Well, if we construct and deconstruct the calculation train, it will have no effect on the environment. As if it never existed. And it will always be constructed in the same way, if we use the same blueprints, so we can actually use the very same train wherever it's needed. Think of it as a quantum crossed construct, if you will. &gt; &gt; I see some grief in your eyes. You compare it to the null officers, don't you? After all, that's how it works in the other cities, if you try to do something non-strict, isn't it? Oh, some of your comrades where punished for uncatched NullPointerExceptions, I see. Don't worry. This city is built on types and mathematics (\*stomps on the ground\*). Its base is solid. &gt; &gt; Side effects? _Visible side effects?_ Well, that's in another ward _OI_, and we make sure that nothing that goes there ever comes back. It might seem a little bit drastic, but that's the cost of purity. We have many such wards, and you can come back from many of them, back to your original type and comfy calculation. But they're handy. Some of them are like onions, wards inside wards, district inside districts. They get built. They get refined. And as long as they're not in the _OI_ ward, we can come back. &gt; &gt; There are many things to explore in our fine city. You might like them. You might hate them. But either way, be very welcome friend. I'm not sure whether this is horrible or horrible and caffeine induced. Meh. Please write something better than that :D. (Maybe you want to include non-strictness as some of the city being exceptionally lazy, functors, monads and lenses?)
It is *him*!
&gt; I can think of plenty of correct programs that can't be accepted in Haskell's type system Correct with respect to what? Do you mean "has a well-defined denotation in a richer formal system"? 
I would think that a functional-language society would have to be nomadic. The city of Functional is unlike any of the cities, in that it has no buildings. The people of Functional keep only what they can carry, and their language has no concept of time or persistence: things either 'are' or they 'become'. If a building is required, it is constructed, used, and deconstructed. The act of IO is to leave a marking or construct somewhere, and is deeply frowned upon (but is also strictly necessary). 
I once had a dream I was trapped inside the IO Monad. There was a club called "unsafePerformIO" where a bunch of really mean looking dudes and lots of impure actions occurring there.
"do you even lift bro?" when talking about Monad transformers is one of my favorite jokes.
&gt; It is more rare to hear from people whose first encounters didn't go so well, and so decided not to pursue things any further -- but those are precisely the stories we need to hear if we want to know not only what we're getting right, but also what's going wrong. This, really. Like the author of the blog post, I *too* started out in a nontechnical field. I was a literature major, and eventually, I went back to school to do a post-bac in math. After a year or two of that, I was introduced to FP. So, while I don't understand what it's like to come into FP without a math background, I understand what it's like to come into a technical field when you have a personal history of doing nontechnical things. It makes small comments (and big ones) add up really quickly, if you're just starting out--in particular, if you *don't* have a "teacher" or "mentor" to look to for guidance. And most people don't. I feel fortunate to have "made it" in any sense of the phrase, given certain problems I had to deal with along the way. So, when people reach out to me to tell me about how they're really interested in doing Scala (or Haskell, or Clojure, or whatever), but how people in their local meetup group/student group/IRC channel/whatever are being awful to them, and they're not sure it's worth destroying their mental health over, I consider that a problem. Especially when, ultimately, many of us *do* use functional languages to earn a living, and these "social justice" problems disproportionately affect underrepresented groups in programming. These problems can literally prevent people from pursuing certain jobs. I have seen it. I think it would be relatively easy for me to say, "Everything's fine!" and *not* incite a downpour of blog posts and reddit comments. But, like... everything is not fine? It is not.
Rich, I think it would be helpful with concrete examples of different kinds that illustrate where static typing fails for transducers. That way we can try to improve the suggested types for transducers. I can't claim that I've looked for such examples, do you have a collection? 
You might enjoy this article on [relative clauses and lambda calculus](http://cubonegro.orgfree.com/relative.html). I found it one day while trying to discover whether anyone had ever applied concepts familiar to functional programming in a language constructed for human use. Also, I have seen Haskell appearing in a fictional context before--though it plays a fairly minor part--on a really interesting site called the [SCP Foundation](http://scp-wiki.wikidot.com/). Briefly, the SCP Foundation is an organization charged with **S**ecuring, **C**ontaining, and **P**rotecting entities that exhibit anomalous properties and would cause anything from major uproar to an apocalypse if not contained. One of the entities being contained, designated [SCP-411](http://www.scp-wiki.net/scp-411), is a person who seems to live his life from future to past rather than the usual past-to-future chronological flow. According to the document, "SCP-411 speaks an as-yet-unknown dialect of English that has significant grammatical and vocabulary deviations from Modern English. Individuals who are to be given training in this language will benefit from a background in Spanish, Mandarin and/or Cantonese, ██████ and Haskell." Yes, many SCP documents contain redacted bits. Anyway, the document doesn't get into the mechanics of the mentioned fictional language, but I thought I'd include it as possible inspirational material. tl;dr -- I'm glad to see another attempt to include functional programming in fiction.
So yeah, I don't think we should blame "typing" here, because e.g. in OCaml you can close over the hidden mutable thing just as easily. Rather, you get weird nonlocal effects here and in a pure context there are going to be more subtle types to capture that.
&gt; There are those who said - I wish Rich had just provided the type signatures. In what language? Pick a suitably expressive type theory of your choice. Once they have the types, they can translate from HoTT to M-L to CoC. There's a non-zero amount of people that are quite interested in the connection between type theory and computation via programs, and if you are precise enough, they will translate to the myriad of languages used. &gt; You could have a state machine that when given an X returns a Y, when given a Y returns a Z, and when given a Z returns an X. This is a pretty unusual signature, but I think you'll find anything that is at least as powerful as M-L will be sufficient. I personally prefer Idris, but I don't think this type isn't actually complicated enough that I couldn't translate it to/from any of (Idris, M-L, or Agda). The fact that `r n` and `r (S n)` are allowed to be different is very key here, though. If `r` is a type, transducers are already better done elsewhere. If `r` is a `Nat`-indexed type family, I think we may have a whole other ball of wax that might be very interesting. This is something that didn't "click" about transducers before this discussion; I had previously dismissed their importance. &gt; I also said a transducer step function must accept its immediate preceding return value as its next first argument, and consumers must supply only the immediately prior return value (and not any old value of some type) as the next first argument. That seems maybe a bit *too* restrictive. If your next argument must be *exactly* the value you last returned, it can't possibly provide any information you couldn't have already captured in a lexical closure (or other internal state). This restriction would effectively reduce transducers to lazy, hetrogenous lists.
&gt; If your next argument must be exactly the value you last returned, it can't &gt; possibly provide any information you couldn't have already captured in a &gt; lexical closure (or other internal state). That's true. But were we to elide 'r' due to this fact (of stateful processes/steps) we would preclude ordinary use of pure functions as steps - they'd have to set up some state since there'd be no path to flow values through. It can make stateful processes cleaner as well to write them with this flow model. But since they might care about monotonicity the restriction is needed. Note I am not saying 'therefore this must be captured by types or types are useless'. But trying to do so remains interesting. 
&gt; Why do want to replicate that? I don't. But if transducers are useful and can't be expressed typesafely in Haskell then it's something we'll have to live with.
There was Hprometheus, a semi-god of category theory that teach how to program monads to the humans. The humans mastered to use monads in industry and rapidly forgot to worship the Gods. They no longer read their sacred papers full of greek letters. The gods knew from the beginning that this act of hprometheus would destroy their power, so they punished hprometheus http://www.prometheas.org/mythology.html
Leibniz thought of every human being as a monad. But I didn't dive so far into his philosophy to understand how they'd communicate. Otoh, afaik Leibniz didn't use the term *monad* in a mathematical sense.
I'll send my contact info to you privately. Anyone else interested, send me a private message and we can continue the convo by email. Thanks for your interest!
That's fair, it's more explicitly the difficulty of typing transducers in a pure system. I think pchiusano said it well above, though, and then that the purity forces the formal system to explain the full scope of what transducers allow. That is what is interesting to me.
This is amazing. Perhaps all the IO buildings on the surface would be connected by bridges and enclosed walkways. Each building could have pipelines traveling to distant cities, but none of them would have any doors for people to enter. You would have to enter the city through the central building, `main`, and travel within the structure.
Oh, I'm not saying that there's anything wrong with `Either`. Just that (as far as I can tell), having the Either only on the step function isn't enough to propagate an early return within `concat`. If you [look at the source](https://github.com/cognitect-labs/transducers-js/blob/efa1e9ddce78940003aa3bdb9cfb7011fa7170cc/src/com/cognitect/transducers.js#L779-L817) there's a funny `preserveReducing` transducer that double wraps `Reduced` values so that it can propagate correctly. As far as I can tell, I can't write that transducer because I think it means you have to care about the result type, which the type of `Transducer` forbids (I could be wrong, I'm a little out of my element). If you do what's done in `cat` and reduce each intermediate step, you have no way of getting the bail status. You can only ever do concat :: forall f a. (Reducable f) =&gt; Transducer (f a) a concat = mkTrans \step -&gt; mkFold \r x -&gt; Right (reduce x step r id) which means that it will return the correct result, but won't bail early. I think you would need to also pass the Either tag to the finalizer.
I appreciate the reply. I don't know Clojure so the examples and the airport metaphors of the talk didn't aid my understanding beyond "okay, I *sort of* get this... I think." My perspective isn't so much "this is *surely* typeable!" but rather "I don't get this, I wish there were some proper types because those would solidify my understanding." In the absence of an authoritative formal description I'll just file tranducers under "seems interesting" until either I learn Clojure or someone with more time on their hands encodes it in Haskell and you give it a seal of approval. I liked your talk, even though I didn't feel very enlightened at the end. I was considering learning Clojure anyway, because it has the generally accepted `core.typed` system.
Interesting! I didn't realize there was a blessed Javascript implementation. It is *very* similar to the one I just outlined. They pass the Either into the existential state type. I think that's also what I had to do, actually, and it gives you the idea of a "dead" fold data Fold i o = forall st . Fold { merge :: st -&gt; i -&gt; Either o st , this :: Either o st , view :: st -&gt; o } Then a "dead" fold is one where `this = Left _` and it can no longer accept any input. It's also exactly what you'd hope you'd see for early termination. Then a transducer is just as above type (a ~&gt; b) = forall r . Fold o r -&gt; Fold i r
Ok. Here is a sketch of an answer then. First observe the isomorphism, proved elsewhere, between `Reducer a b` (using the standard Haskell signature) and functions of type `b -&gt; [a]`. Now, replace the list there with the f-and-m algebra for an effectful list (like so: http://bentnib.org/interleaving.html). I bet that now, for different choices of monads, we can capture different forms of effects (and to capture the clojure type, we just pick our `m` to be `IO`). However, I bet it will be hard to take that richer type and put it back into the "final" universal encoding that we tend to want to present reducers in (for purposes of compositionality, I guess?).
That paper just introduces newtype FixM m f = FixM (m (f (FixM m f))) data ListF a x = Nil | Cons a x type ListM m a = FixM m ListF right? I think I agree then. `T m a b = a -&gt; ListM m b` feels like it could cover a lot semantically if not operationally.
My code so far : promptForAdd = do print "What would you like to add?" wantsToAdd &lt;- getLine print $ "User wants to add " ++ wantsToAdd parseInput "add" = promptForAdd parseInput _ = print "You chose something else" main :: IO () main = do print "What would you like to do?" input &lt;- getLine parseInput input So I prompt the user for what he wants to do, it matches "add" if he inputs "add", and then asks what question he wants to add, and from here I'm completely lost, what do I add it to? Of course, there can't be any sort of variable to add it to, I know that much, but what can I do with it? The data type advice was very helpful, I'm reading about it now and it's a huge help. I knew Haskell didn't have classes so I kept wondering how data that isn't just primitives is stored, thanks for pointing me in the right direction.
Python classes translate into Haskell as records + functions on those records. You can also simulate object-oriented method notation using lenses to make it look even more Python-like. Here is an example: import Data.Char (toLower) import Lens.Family2 -- | Lowercase a `String` lower :: Getter' String String lower = to (map toLower) With that, you can write: &gt;&gt;&gt; "ASDF"^.lower "asdf" ... just like in Python.
 allMy i xs = if xs then [x | x &lt;- texas] else i in tennessee
I wonder if maybe what could help this discussion be more productive (in the broadest sense of "this discussion") would be a greater focus on specifics and concrete examples (as you just provided). Not necessarily calling people out directly, as that might just lead to friction and defensiveness in the heat of the moment (obviously also depending on how that's done). But things like, rather than saying, "the Haskell community needs to work on being less condescending", saying "please don't do *this specific thing* or act *that specific way*, because people from *this sort of background* or *that sort of background* will likely experience it in *this way*, which is unfortunate and sad". Of course we're all fallen, imperfect beings and everyone always has room for improvement, but it seems fair to assume that there is some subset of individuals who people are having these unpleasant interactions with, and another with who they aren't. (This is reflected in the fact that different people report having had very different experiences.) If we say "the Haskell community is condescending", without reference to particulars, then at best this will lead to a lot of confusion, with the people who mostly aren't the source of the problem, and the people who've interacted with them, not understanding what it's all about, or feeling as if they're being unfairly attacked... while even the people who *are* sometimes the source of the problem won't necessarily make the connection. (Just to avoid the slightest chance of misunderstanding -- this is only in-general musing inspired by your comments, and by the rest of this thread, and is not obliquely referring to anyone or anything in particular; in particular it's not about the tweets you posted.)
That's really cool, what's the up carat mean in that expression?
My advice would be don't try to be elegant right now, just write the whole functionality in the `main` function. Keep your list of questions and answers, your input, processing and decision logic, all there. Get the simplest version possible working: something that adds a single question/answer object to an empty list and then displays the resulting list. Then work on splitting it up into several functions, and then arrange those functions to loop so that the user can keep adding questions &amp; answers, until they choose to exit; at which point display the final list of Qs &amp; As.
Yep. The technique was known waay before that paper, but the way they use the formalism allows lots of neat algebraic reasoning about those sorts of things that we hadn't necessarily been able to do as easily prior.
It's the operator for viewing lenses. It does a few other things, like record field access as well. Check out the `lens` library on hackage if you want to look at how lenses work in more detail. 
I was thinking that it must be a pretty sweet metal song. Only because a big-name band released an incredible album today.
`(^.)` is an infix operator defined within Haskell. You can think of it as having this type: (^.) :: a -&gt; Getter' a b -&gt; b If you're new to lenses, you can try reading [this lens introduction I wrote](http://www.haskellforall.com/2013/05/program-imperatively-using-haskell.html). I would also recommend [the `lens-family` library](http://hackage.haskell.org/package/lens-family), which is a lightweight subset of `lens`.
&gt; text lacks warmth especially between strangers I think that this is partly so... but only partly. In particular... I think it's definitely the case that: * If you write on the internet in the same manner as you might if you were speaking with someone "in reality", without all of the metacommunicative aspects which the latter has, then it will come across as much more unfeeling and even hostile than you had intended. I think it's also *possibly* the case that: * This is especially hard to avoid in mediums with shorter, more rapid messages (chat, twitter, etc.). But I think it's definitely *not* the case that: * Text cannot convey warmth and vulnerability. Some of the most moving experiences I've had were when I was reading text on the internet, written by strangers. Text might even be uniquely well-suited to it, in some ways. After all, people write poetry and books! But it's probably true that it takes much more conscious effort than everyday physical/verbal interaction.
On the 'do' is bad/dangerous thing and how to avoid it. For all practical purposes 'do' should not be avoided, it is a necessary evil if you will to deal with I/O safely for example. As most programs do I/O, including most Haskell programs, Haskell programs will commonly use 'do' syntax for those I/O portions. As a beginner Haskell programmer, if you are banging your head against the wall trying to figure out how to get rid of the 'do' when you are printing, reading, writing, displaying, sending, receiving ... stop ... and just do the 'do'. It's perfectly fine. In your first few Haskell programs you will look at it and say, "what am I doing wrong?" I have all these IO This and IO That functions. I'm supposed to be writing functional now, I shouldn't have all these imperative almost Basic-like 'do' IO-y thingees. Real programs will have them. Depending on the program, lots of them. You just have a program which is doing a lot of IO/Mutation/Side-Effecting stuff. Relax. Do ask yourself if you are doing pure functional stuff in some 'do' sequence and factor that out into a pure function. In other words, reduce or minimize your 'do' but calmly accept the fact that you can't eliminate it all together. 
&gt; Another time I tried to understand some cartesian closed category stuff and as far as I can tell it says no more than that you can represent variable environments with tuples. Duh. Kind of tangential to your comment (in particular this is unrelated to whether or not category theory is useful for programming), but I better appreciated the ways in which a CCC is *not* so trivial (and why category theory *is* fascinating, if not necessarily for programming) after reading the paper [Physics, Topology, Logic and Computation: A Rosetta Stone](http://math.ucr.edu/home/baez/rosetta.pdf). CCCs may seem trivial because they happen to describe the environments we're used to working in. But there are also different kinds. (For instance, even languages with affine/linear types a.k.a. compile-timed-checked move semantics would not be a CCC.) 
As far as I can tell, the point about a reducing fn's being r_n -&gt; a -&gt; r_(n+1) and not r -&gt; a -&gt; r applies equally to normal folds. The normal fold type (r -&gt; a -&gt; r) -&gt; r -&gt; [a] -&gt; r is too restrictive for some things. For example if you have a list of functions and you want to fold function composition over the list, then this restricts those functions to type `t -&gt; t`. If you had a list of functions `a -&gt; b, b -&gt; c, c -&gt; d`, it would be fine to fold function composition over that and get a function of type `a -&gt; d` out, but the type system won't allow you.
So most likely you want to have a main loop function that takes the current set of questions / answers. Then each iteration you can just pass in the new version (which presumably has been updated by the i/o): main :: IO () main = mainLoop initialData mainLoop :: [(Question, Answer)] -&gt; IO () mainLoop questions = do ... mainLoop updatedQuestions Where somewhere, based on the input, you compute `updatedQuestions` from `questions`. At least to start, and to understand how things are normally done in haskell, there is no mutable state. So if you want to 'change' things, usually you want to pass a modified copy to the next iteration of the loop (where 'loop' really means, recursive call). [edit] Also, the comment that 'do' is either bad or dangerous is really wrong! No program is worth much without doing IO, and the idea that it is morally wrong or dangerous to use it is just silly. What you'll probably find after writing a bit of haskell code is that it is often easier / cleaner to write pure functions and then have a shallower layer of IO code that uses those functions. But find that out because it's actually helpful, not because someone said it's the way you 'should' do things :) 
Of course, I'm not saying it wouldn't be a lot of work, especially the fiddly parts like text editing. (And yeah, it seems you couldn't avoid some unpleasant native platform integration work for things like C&amp;P and D&amp;D, and also for things like opening various kinds of windows like menus, tooltips, dialogs etc.... it seems there are [a couple](http://hackage.haskell.org/package/Hclip) [of libraries](http://hackage.haskell.org/package/Clipboard) out there for clipboard interaction, but not at the level of generality a GUI library would need.) But all that said, I suspect that it could be an extremely solid foundation. What's your perspective? &gt; It's certainly doable but by no means simpler than writing a binding to an existing GUI library. Perhaps not, but it would also be *much, much better* than an existing GUI library. Relative to existing solutions, I suspect the expressiveness and power of a library based on compositional drawing via `diagrams` and compositional program logic via FRP would be through the roof. The more advanced current approaches such as QML are like a distorted premonition of this.
JavaScript city. A flurry of activity, with all buildings abandoned partway through construction, the creators turning to a new set of build tools and frameworks every other day. Terrible financial issues due to incorrect rounding of decimals. It's not even one city - as there are numerous "implementations" of the city the people of JavaScript are continuously finding that what works in one zone doesn't exist in another. The people of JavaScript city would have a reputation for not exactly telling the truth. Their statements are truthy or falsey and you have to really live among them to know the intricate and bewildering social rules that reveal if someone is lying or not. They are also expanding massively, attempting to make inroads in the lands of other cities. Node colonies are popping up everywhere and lowering land values (except in PHP city, who have their own civic issues that perhaps someone else can detail). Edit: Come to think of it, it might take some work to get the truth out of Haskell city folk too, for different reasons. "Maybe" they say. "Could be Nothing." And they're lazy!
The reflection without remorse paper of this year talks about these things as type-aligned sequences, although it's about composing monadic functions, i.e `a -&gt; m b` with `b -&gt; m c` and so on.
Very helpful, thanks!
You should have a look at post rock stuff haha
&gt; You could have a state machine that when given an X returns a Y, when given a Y returns a Z, and when given a Z returns an X. &gt; &gt; I think every programmer can understand that sentence. What's the universally understood type signature for that? What about something like the following? You need some extensions in Haskell but the [Idris version](https://gist.github.com/darinmorrison/9e891ce2017f3539de69) just uses simple functions. {-# LANGUAGE AllowAmbiguousTypes #-} {-# LANGUAGE DataKinds #-} {-# LANGUAGE ExplicitForAll #-} {-# LANGUAGE TypeFamilies #-} module Machine where data X data Y data Z data U = UX | UY | UZ type family El (u :: U) :: * where El UX = X El UY = Y El UZ = Z type family To (u :: U) :: U where To UX = UY To UY = UZ To UZ = UX next :: forall (u :: U). El u -&gt; El (To u) next = undefined
Who?
This kind of reasoning was part of why Monad comprehensions were removed from the language for a while. Data-Supported Haskell pretty convincingly demonstrated that they can be useful for DSL implementors and they eventually got re-introduced.
That's a good intuition. Here the observation is that we can _still_ be natural as long as the use we make of the value (such as adding to it or something) doesn't "leak out" of the value in some way that is, so to speak, noncontinuous. For example, I think it _could_ be natural to produce a list whose length was determined by the Int we were handed. I haven't worked any of this through in detail though (it would be a fun exercise to do so), and as you can see, you have to ask a lot of specific questions to get to an answer (while with parametricity you just always _know_). The particularly tricky part is that its easy to show something isn't natural -- just come up with a counterexample -- i.e. a function that can't translate across. However, to show something is natural, you need to show that there is _always_ a translation across, which typically means coming up with some algorithmic way to produce such a thing. The nice thing about parametricity is it _also_ gives us such an algorithmic translation (exercise: come up with a few may2list functions as given above, and see if you can derive a method for each one to turn the `g` to an `h`). Absent that, we have a lot of other things to worry about, and the reasoning, while possible, becomes much more involved... Also you're right that there is an infinite regress, and not just a holy trinity! This is why we call natural transformations `2-cells` -- because there are also `3-cells` etc on up to infinity. The general study of this is called n-category theory, and its holy grail is a full characterization of the properties of infinity-categories. This turns out, not shockingly, to be sort of hard. In fact, it turns out that we normally don't want "strict" n-categories, but would rather that at each level they only have to preserve the prior level up to some notion of isomorphism, otherwise we can't talk about many objects. But absent strictness, we need a much wider set of weak conditions to make sure everything behaves, and even _writing them down_ for levels above 3 becomes very arduous! However, if we take the notion of higher categories and cells seriously, and then toss in on top of that a notion of higher identities as well, and take that even _more_ seriously, then we arrive on one path to homotopy type theory, and even absent that, we create categories that look a _whole lot_ like topological spaces, and get to do all sorts of interesting topology on them. At this point of course we're far removed from anything that feels computationally relevant any time soon. But I don't mind, because it's so much fun. :-P
That has more to do with the nature of lists than that of folds. -- compare data F a b where -- data L a where Id :: F a a -- Nil :: L a Add :: (a -&gt; b) -&gt; F b c -&gt; F a c -- Cons :: a -&gt; L a -&gt; L a cons :: (a -&gt; b) -&gt; F b c -&gt; F a c -- cons :: a -&gt; L a -&gt; L a cons = Add -- cons = Cons snoc :: F a b -&gt; (b -&gt; c) -&gt; F a c -- snoc :: L a -&gt; a -&gt; L a snoc x f = case x of -- snoc x a = case x of Id -&gt; Add f Id -- Nil -&gt; Cons a Nil Add f' y -&gt; Add f' (snoc y f) -- Cons a' y -&gt; Cons a' (snoc y a) foldF :: (forall x y . (x -&gt; y) -&gt; r y c -&gt; r x c) -&gt; r c c -&gt; (F a c -&gt; r a c) foldF add id = go where go f = case f of Id -&gt; id Add p g -&gt; add p (go g) comp :: F a b -&gt; (a -&gt; b) comp = foldF (flip (.)) id Not to say it's pretty...
There's also the [classic alternative to existential quantification in these situations](http://lukepalmer.wordpress.com/2010/01/24/haskell-antipattern-existential-typeclass/), which involves wrapping the relevant exposed functions in a record. This requires no GHC extensions. data Color = Red | Green | Blue deriving (Show) type Radius = Float type Length = Float data Apple = Apple Color Radius data Orange = Orange Radius data Banana = Banana Length Radius data Fruit = Fruit { eat :: IO () } class ToFruit f where toFruit :: f -&gt; Fruit instance ToFruit Apple where toFruit (Apple _ c) = Fruit { eat = putStrLn ("eating " ++ show c ++ " apple") } instance ToFruit Orange where toFruit _ = Fruit { eat = putStrLn "eating juicy orange" } instance ToFruit Banana where toFruit _ = Fruit { eat = putStrLn "eating banana" } basket :: [Fruit] basket = [ toFruit (Apple Red 5) , toFruit (Apple Green 6) , toFruit (Banana 2 30) ] main = mapM_ eat basket 
I think the normal answer here is what Luke Palmer expresses below. http://lukepalmer.wordpress.com/2010/01/24/haskell-antipattern-existential-typeclass/ data Fruit { eat:: IO () } data Apple data Orange data Banana eatApple :: Fruit -&gt; IO () eatOrange :: Orange -&gt; IO () eatBanana :: Banana -&gt; IO () appleAsFruit :: Apple -&gt; Fruit orangeAsFruit :: Orange -&gt; Fruit bananaAsFruit :: Banana -&gt; Fruit ... main = mapM_ eat [ orangeAsFruit o, bananaAsFruit b, ... ]
recently I've come across this blog: [Matt Might](http://matt.might.net/articles/) here are some posts that seem relevant, I hope you will enjoy them just as I have * [Tree transformations: Desugaring Scheme](http://matt.might.net/articles/desugaring-scheme/) * [Compiling to lambda-calculus: Turtles all the way down](http://matt.might.net/articles/compiling-up-to-lambda-calculus/) * [Writing CEK-style interpreters (or semantics) in Haskell](http://matt.might.net/articles/cek-machines/) * [Writing an interpreter, CESK-style](http://matt.might.net/articles/cesk-machines/) * [Java as a CESK machine](http://matt.might.net/articles/oo-cesk/) * [Compiling Scheme to C with closure conversion](http://matt.might.net/articles/compiling-scheme-to-c/) there's also [SECD by Landin](https://en.wikipedia.org/wiki/SECD_machine) ([PDF](http://www.brics.dk/RS/03/33/BRICS-RS-03-33.pdf)) and [A call-by-name lambda-calculus machine by Krivine](http://www.pps.univ-paris-diderot.fr/~krivine/articles/lazymach.pdf), if anybody would be interested 
&gt; Data ["a,b,c,d"] Do you mean `Data ["a", "b", "c", "d"]`?
Yes I did, sorry I wrote it up quickly
I thought the whole point of having a blog was being able to make fun titles.. Apparently you have to write something interesting under them, yeesh.
I love this blog! I don't think he explicitly ever talks about compiling a lazy language efficiently beyond the "lambda + mutation = laziness" route which is a shame.
 import Data.List (intercalate) dataToString :: Data -&gt; String dataToString (Data as) = intercalate ", " (map show as)
thanks for the reply, but for this certain data type I am choosing to not import Data.List / Set or anything for that matter
I would say don't spend too much time wrapping your head around lenses in the beginning. However, if you end up wanting to do something like use wreq for interacting with API's you'll end up going through the basics. Actually, the wreq tutorial is a good interactive example of Haskell doing practical things: http://www.serpentine.com/wreq/tutorial.html I actually haven't taken a deep dive into lenses yet and only ever use (^.) to apply functions or (^?) to apply functions to maybe values.
 &gt; let xs = [99, 21, 7, 1, 30] &gt; show xs "[99,21,7,1,30]" &gt; init $ tail $ show xs "99,21,7,1,30"
hey! aren't you the author of the blogpost? thanks a lot man. the 100 pages paper by SPJ seems quite threatening to me, but this is accessible. so now I can have at least a rough idea how GHC works. &gt; I don't think he explicitly ever talks about compiling a lazy language efficiently beyond the "lambda + mutation = laziness" route which is a shame. there's this thing by Krivine (and probably many others, I'm just mentioning the things I've heard of), but if I got the idea right, that is the "stupid" call-by-name, not "lazy" 
Why not? Any other unspecified constraints we should know about?
Once you say "immediate preceding return value" you've pretty much required dependent types. I think a single level of indirection suffices: Definition Reducer0 A R := R -&gt; A -&gt; R. Definition Reducer A {T} (El : T -&gt; Set) (R : Reducer0 A T) : forall {t} (r : El t) (a : A), El (R t a). A basic reducer just works over a fixed type R as you want to go beyond. Then a reducer as you ask for has a basic reducer saying what kinds of arguments are allowed. You can get a fixed result type by not using the flexibility and making El a constant function. You could have an input-independent sequence of types by letting T be a number which R just increments. If you want to prevent making extra calls that are ignored, you'd need some kind of linear typing too. Maybe in ATS? If a transducer was allowed to modify the initial value even when one was explicitly supplied, then ones like `take` could include the counter as part of the intermediate reduction values rather than needing to close over mutable state. The question that could most use some precision here is just how picky a reducing function is allowed to be, and conversely what a transducer is allowed to do with the reducer it's given. The type I wrote suggest that you basically can't do anything with a reducer except feed it a sequence of values, but the reducer itself isn't allowed to make any strong assumptions about the sequence of values it's fed (so a transducer is allowed to skip, duplicate, or even shuffle the input if it wants). Of course, if you're happy to implement transducers in Clojure, then you're already fine with using static types that are too coarse to capture everything - the interesting questions about typing are whether there are types which allow writing all and exactly the legal transducers, and, more pragmatically, whether some excessively strict types rule out much of value.
Using CPP flags is a bad idea because there is no way to depend on the version that was compiled with a specific backend. You also are not allowed to change the exposed API in anyway via the CPP flags. It's much saner to have a common core, stripe-core, and then have libraries like stripe-http-conduit, stripe-io-streams, etc. That also allows you to have multiple backends installed at once. While you a single developer might not install multiple backends, you can certainly imagine that you might install clckwrks, which uses one backend, and some future version of hledger that uses a different backend. 
An additional point here -- CCCs also say that if you have something that _has_ that structure, then it is already a "place you can compute in the obvious way". In turn, we know that the functor category `A -&gt; B` is a CCC if `B` is, and so is a "slice category" over a CCC, etc. And as you point out, we can weaken CCCs and look at categories that are just monoidal closed, for example. And now it turns out that monoidal closed functors are what give us the `Applicative` typeclass! (And examining the class as such is what gives us its laws). So yes, if we give a categorical account of some aspect of a structure, that on its own just throws new words over what we already know. But if we then start working with that vocabulary, it turns out we can lean on a wide class of existing general knowledge to start finding results and connections. (This is a general quality of the abstract method in mathematics -- the first step is generally restating what you know in less specific terms, and the next step, in a seeming contradiction, is leveraging the fact that you have thrown away information to find results that this information _stood in the way of_. We can draw a nice analogy to our parametricity discussion here. If we have a function that we use as `Maybe Int -&gt; [Int]` then it is hard to say much about it, in general. But if we observe that it can be given the _less_ specific signature `forall a. Maybe a -&gt; [a]`, then we have suddenly _learned_ something.).
You're right that we'd like to have a signature like `(n :: Nat) -&gt; x -&gt; Vec n x`, and that's exactly what real dependent types let us do. You do have a slight terminological oddity however. &gt; data Natty :: Nat -&gt; * means Natty is a function from a Nat to a Kind at the type level. Not exactly. Natty is a type constructor that takes an argument of kind `Nat` and returns an argument of kind `*`. "Kind" is a name for the "type of types". All types of values must have kind `*`, which is a special kind. For example, `Maybe` is of kind `* -&gt; *` that takes a type of kind `*` (i.e. a type of a value) and returns a different type of a value. But otherwise you're on the right track, yes.
I think 'next' should either be typeclass method or you will want GADT for the state types, otherwise I'm not sure how you could implement the function body.
I thought it was a takeoff on _The Soul of a New Machine_.
I think /u/darinmorrison's answer for the state machine type was lost in all the comments, so I will post my version here. {-# LANGUAGE GADTs #-} {-# LANGUAGE TypeFamilies #-} {-# LANGUAGE DataKinds #-} {-# LANGUAGE KindSignatures #-} module Machine where type X = Double type Y = Int type Z = String data U = UX | UY | UZ data State :: U -&gt; * where SX :: X -&gt; State UX SY :: Y -&gt; State UY SZ :: Z -&gt; State UZ type family To (u :: U) :: U where To UX = UY To UY = UZ To UZ = UX next :: State a -&gt; State (To a) next s = case s of SX x -&gt; SY $ floor x SY y -&gt; SZ $ show y SZ z -&gt; SX $ read z 
Out of curiosity, does adding the extra level of indirection incur any performance cost if at all? I'm thinking of maybe a situation in Graphics where you have a `class Drawable a where...` and you loop through all the drawable things a lot. 
Yea this could work. I'd ideally like to avoid the scenario where consumers of this library have to write boilerplate code to wire up which backend they'd like, or the scenario where we maintain 4 different stripe versions on hackage, or the scenario where a user has to have every iteratee package installed. Unless I misunderstand, CPP flags let you change the dependency in the cabal file and the body of specific methods (or entire code blocks) by using `ifdef` blocks. While its true we can't specify in the cabal file which backend to compile with, we can do so in the cabal.config file after a cabal freeze is made. So I was thinking maybe we could modify the `callAPI` function in ways specific to backends.
&gt; You could have a state machine that when given an X returns a Y, when given a Y returns a Z, and when given a Z returns an X. That's utterly useless, Rich. Because until the state machine collapses that correspondence to one type, I have to treat the rest of my code like a part of that state machine.
 instance Show a =&gt; Show (Info a) where show (Info []) = "" show (Info l) = foldrl cat x xs where (x:xs) = map show l cat x y = x ++ ", " ++ y
Fair point (I had an inkling of what you might reply with soon after posting). Initially I was thinking you were taking about mutable state within the transducer itself. But yes, I agree, if there is no mutable state anywhere, then these are exactly as you described. However, I think the intention in Clojure is that the “reducing context” would often be a stateful process. If all the state is hidden away in that context *and* the linearity of the state is ensured as it is threaded through a stack of transducers, then all is fine and dandy. Clojure’s `into` does this with its transients, to efficiently build a persistent data structure with local mutation. I should try bad1, 2, and 3 to see what happens with `into`!
 instance Show a =&gt; Show (Info a) where show (Info []) = "" show (Info l) = init $ tail $ show l
And java is a prostitute.
 instance Show a =&gt; Show (Data a) where show (Data x) = init $ tail $ show $ x
I think they were referring to Matt Might's blog. I'm a fan of both :)
The point isn't to dismiss *arguments* to the contrary, just *not to* dismiss the possibility that it is indeed better overall. The axiomatic idea that "everything is a trade-off" completely shuts off that whole vein of discussion (and quite a few others, besides). There are other reasons, and you pointed some out, and your points have their own rebuttals or counter-arguments and *lo*, we have a discussion. One that did not need a false anchor, arguing from moderation. Cutting this down just because some people think it smug is not the way we should go.
Yes, strong opinions might be wrong. Which is why they need to be coupled with discussions—discussions we can't have if we let this mindset take over or socially suppress strong opinions. The idea is that active discussions *need* some people with strong opinions to get things moving. It creates a sort of genetic diversity among ideas and keeps everyone from following the herd. That's a key part of my view, and I did not do a good job expounding it. It's also why, if you look at how I actually behave, I often espouse stronger views than I act on. I think this is important both for forming my own ideas and maintaining strong community discourse. And hey, sometimes these opinions actually pay out!
Responding to it defensively? Certainly doesn't help. But that doesn't mean you have to—and, more importantly, doesn't mean you should *feel* you have to—bend over backwards to suit the whims of whoever criticizes you. And I feel that's what this post is all about, putting it in perspective: just because somebody is critical about you does not mean they are right or that you should feel bad. As you rightly pointed out, it doesn't mean you should lash out defensively, but that's not what's happening here.
And the original tweet was defended, in an ironically toxic way, by a small but ardent group of people pushing a particular viewpoint. (One that is *not* obviously correct.) That's definitely worth responding to.
Really? Maybe you were reading different books and sites, but when I was first learning programming I ran into *far* more fervent and sustained arguments in favor of OOP (over procedural programming), Python and even Java. And hey, they worked pretty well until I discovered Haskell, so there's that.
For what it's worth, I suspect that people had a strong reaction not to *your* tweet but to some of the, hmm, interesting replies it generated. That was certainly the experience I had: your particular tweet, while I do not agree with the underlying sentiment, was still amusing and interesting. Some of the resulting conversation, however, was, at best, emotionally charged in a way I find troubling—a way that, at the very least, merits *some* response. It's also a microcosm of some social trends that *seem* to be unchallenged (at least to someone living in Berkeley :P), so I guess this is a nice chance to jot some thoughts down. (Writing also helps with the "emotionally charged" aspect, at least for me.) Well, the details don't really matter: that was just my reaction. Seems reasonable that others' would be similar. Probably part of the reason it became a pretty active discussion. 
Define your own intercalate function in this case. Why wouldn't you import Data.List?
Ah, yes. I was still thinking in terms of Idris since that was the version I wrote first. Unfortunately, in Haskell you do need something like that, or singletons: {-# LANGUAGE DataKinds #-} {-# LANGUAGE ExplicitForAll #-} {-# LANGUAGE GADTs #-} {-# LANGUAGE TemplateHaskell #-} {-# LANGUAGE TypeFamilies #-} {-# LANGUAGE TypeOperators #-} {-# LANGUAGE UndecidableInstances #-} module Main where import Data.Singletons.TH type X = Double type Y = Int type Z = String data U = UX | UY | UZ $(genSingletons [ ''U ]) type family El (u :: U) :: * where El UX = X El UY = Y El UZ = Z type family To (u :: U) :: U where To UX = UY To UY = UZ To UZ = UX from :: Sing u -&gt; El u -&gt; El (To u) from SUX = floor from SUY = show from SUZ = read to = flip from test1 = 42.0 `to` SUX `to` SUY `to` SUZ `to` SUX data Nat = NZ | NS Nat $(genSingletons [ ''Nat ]) type family Iter (n :: Nat) (u :: U) :: U where Iter NZ u = u Iter (NS n) u = Iter n (To u) iter :: Sing n -&gt; Sing u -&gt; El u -&gt; El (Iter n u) iter SNZ u = id iter (SNS n) SUX = iter n SUY . floor iter (SNS n) SUY = iter n SUZ . show iter (SNS n) SUZ = iter n SUX . read test2 = iter n SUX 42.0 where n = SNS $ SNS $ SNS $ SNS SNZ main :: IO () main = print $ test1 == test2 
Should have more upvotes.
`undefined x = undefined`
This is the same problem with making actors (from Erlang, Akka, etc...) type-safe, since currently it isn't reasonable to document a state machine (let alone one subject to non-determinism) in our current statical type systems. However, such protocols of communication can be described precisely in natural language. My problem for example is that I still don't understand the protocol. I understood more by reading the interfaces published in the Java package provided by Cognitec's GitHub repo, than by watching your presentation. And I still don't understand if this protocol would work for reactive streams (Rx or the like) if back-pressure is involved - my gut feeling says it doesn't, but maybe I'm wrong. Protocols, even if inexpressible in a type system, need documentation with rules and do's and don'ts. I would even go as far as to say that protocols need a TCK, after all, the whole point of a protocol is for other people to implement it.
I should get myself a `homework?` stamp.
&gt; But were we to elide 'r' due to this fact (of stateful processes/steps) we would preclude ordinary use of pure functions as steps - they'd have to set up some state since there'd be no path to flow values through. Sound like a lift in an appropriate, possibly opaque and hidden, indexed if needed, State monad to me. Pure functions can be injected fairly simply into such a context; they wouldn't even need to State monad to be indexed &gt; It can make stateful processes cleaner as well to write them with this flow model. But since they might care about monotonicity the restriction is needed. Again, this restriction seems far to severe. The part of `r (s n)` is only the valid input for the next "iteration" of a reducer can be completely hidden by a lexical closure. The rest of `r (S n)` is freely consumable by whatever is calling the reducer, but if it's type varies you get something very much like a hetrogenous, unidirectional `Pipe`.
&gt; until the state machine collapses that correspondence to one type, I have to treat the rest of my code like a part of that state machine. Not exactly. Outside of Haskell, subtype polymorphism would allow X, Y, and Z to sahre a common supertype and you could freely use the features of that supertype. Or, you could fall back to duck typing and just have common attributes between X, Y, and Z. In GHC Haskell, if there's some typeclass where there are instances for X, Y, and Z would could wrap whatever is returned in an existential and use the typeclass methods. But, yes, if there's not some method to "collapse" into a common set of operations all the various `r n` for a reducer, you'll end up having to marry the state of the code surrounding the reducer to the state of the reducer, even if you are in an untyped language.
&gt; I suspect that it could be an extremely solid foundation. What's your perspective? It depends. Actual code is probably too short-lived to work as foundation. I think that a series of design documents (literate Haskell) would work very well, though. (I found that ideas are much easier to reuse than code, as long as they are specified precisely enough.) &gt; Relative to existing solutions, I suspect the expressiveness and power of a library based on compositional drawing via diagrams and compositional program logic via FRP would be through the roof. Yes and no. In general, I agree that it should be possible, but you still have to find an elegant set of widget combinators, and I think that this task is not easy at all. I tried something along these lines in an [old project][1] from which reactive-banana is a spin-off, but I still have no idea what a good set of widget combinators would look like. You really have to break away from the imperative event-driven approach for this to work, and that's not easy. [1]: https://github.com/HeinrichApfelmus/reactive-banana/tree/b8c353bf275ee2c58a05cbc18f33fe481c8e70e4/BlackBoard/src
http://program-transformation.org/pub/GPCE10/ConferenceProgram/dtp-dsl.pdf contains an example of expressing a "network" protocol in Idris and having the type-checker verify that both ends implement it as sepcified. Dr. Brady has quite a few talks and decks where he covers trying to type-check protocols (and sometimes getting the computer to write the boring parts of the protocol).
HXT does have a [schema definition](http://hackage.haskell.org/package/hxt-9.3.1.7/docs/Text-XML-HXT-Arrow-Pickle-Schema.html) that is also automatically constructed if you create serializers/deserializers using [Picklers](http://hackage.haskell.org/package/hxt-9.3.1.7/docs/Text-XML-HXT-Arrow-Pickle.html). It's not quite XSD so I'm not sure if it is right for your use case.
This sounds an awfully lot like this paper: http://okmij.org/ftp/tagless-final/course/optimizations.html Any-one care to make a comparison? (I currently mostly understand the OCaml paper, still reading the Hoopl one).
I guess that purescript streams might count as well then, as it's based on scalaz-streams: https://github.com/purescript-contrib/purescript-streams
The development version of [Clojure](https://github.com/clojure/clojure/blob/master/src/clj/clojure/core.clj) has transducers now. For example there is [map](https://github.com/clojure/clojure/blob/master/src/clj/clojure/core.clj#L2512), [filter](https://github.com/clojure/clojure/blob/master/src/cj/clojure/core.clj#L2652), [take](https://github.com/clojure/clojure/blob/master/src/clj/clojure/core.clj#L2652) etc.
A little more on the `do` thing: `do` is not considered bad. It also isn't used solely for IO. `do` syntax can be used whenever we work with data types that are *monads*. You'll learn what that means eventually - don't worry too much about it for now. IO is one type which is a monad, but there are many others (Maybe, for example, is one). IO isn't considered "bad" because it's monad, but because it's hard to reason about. Let's have an example: If I write something like addQuestionMark :: String -&gt; String addQuestionMark s = s ++ "?" Then if I call it twice with the same input, I know that I'll get the same result each time. However, if I have a function like: addQuestionMark2 :: String -&gt; IO String addQuestionMark2 prompt = do putStr (prompt ++ "?") getLine Then if I call it twice with the same input, it might give me different results, because the user typed something different each time. The way we usually deal with that is to separate the nice, pure, easy-to-reason-about non-IO code from the dirty, evil, hard-to-reason-about IO code. That means that a piece of code that does not strictly *need* to do any IO should be separated from the code that does. A way to do this in your own code is something like this: * Create pure functions for manipulating the list of questions and answers * Create pure functions for parsing input and deciding what to do * Use IO *only* for communicating with the user Example: data Action = Add | SomethingElse parseInput :: String -&gt; Action parseInput "add" = Add parseInput _ = SomethingElse main :: IO () main = do putStrLn "What would you like to do?" inp &lt;- getLine case parseInput inp of Add -&gt; promptForAdd SomethingElse -&gt; putStrLn "You chose something else" promptForAdd could then again make a call to some pure function which would be responsible for adding the new question to the list. Or even better, it could simply return the user input and let `main` decide what to do with it: promptForAdd :: IO String promptForAdd = do putStrLn "What would you like to add?" getLine main :: IO () main = do let questions = [] putStrLn "What would you like to do?" inp &lt;- getLine case parseInput inp of Add -&gt; do add &lt;- promptForAdd let questions' = addQuestion questions add -- do more stuff SomethingElse -&gt; putStrLn "You chose something else" Of course, you'll want to structure your code like /u/dbpatterson said: main :: IO () main = mainLoop initialData mainLoop :: SomeType -&gt; IO () mainLoop questions = do putStrLn "What would you like to do?" inp &lt;- getLine case parseInput inp of Add -&gt; do add &lt;- promptForAdd mainLoop (addQuestion questions add) SomethingElse -&gt; putStrLn "You chose something else"
I always recommend this paper [1] as a high-level overview of STG. To get full picture I'd recommend another two papers: * Handling of unboxed values [2]. * And original paper [3] for details about memory layout and garbage collection. [1] http://research.microsoft.com/~simonpj/papers/spineless-tagless-gmachine.ps.gz [2] http://research.microsoft.com/en-us/um/people/simonpj/papers/unboxed-values.ps.Z [3] http://research.microsoft.com/~simonpj/papers/spineless-tagless-gmachine.ps.gz
[Google](http://www.google.co.uk/about/careers/students/) has Haskell internships in the [Ganeti](https://code.google.com/p/ganeti/) project. I'm doing one right now.
Right? I wish someone makes Unconceal to translate academic papers back to ASCII.
Not so similar. Both are about optimizations and rewriting. Oleg's is really just about the fact that in a tagless settting, performing optimizations and rewrites is in itself tricky, and showing how one can do it at all. In the setting of a typical first-order AST, it doesn't really show much interesting. The Hoopl paper is about a library for _finding_ interesting optimizations through dataflow analysis, and encoding a large set of them in a uniform way.
although, as /u/tailcalled [pointed out](http://www.reddit.com/r/haskell/comments/2ktr5d/advice_wanted_list_of_polymorphic_elements/clozsdx), if all you can do is eat a fruit, even the record is overkill module Main (main) where data Color = Red | Green | Blue deriving(Show) type Radius = Float type Length = Float data Apple = Apple Color Radius data Orange = Orange Radius data Banana = Banana Length Radius class Eatable a where eat :: a -&gt; IO () instance Eatable Apple where eat (Apple c _) = putStrLn $ "eating " ++ (show c) ++ " apple" instance Eatable Orange where eat (Orange _) = putStrLn $ "eating juicy orange" instance Eatable Banana where eat (Banana _ _) = putStrLn $ "eating banana" basket :: [IO ()] basket = [eat (Apple Red 5), eat (Apple Green 6), eat (Banana 2 30)] 
How would these solutions work if I wanted something that operates on itself? I'm running into the same problem but with a statemachine implementation I trying. Currently I have something like this: {-# LANGUAGE ExistentialQuantification #-} data Transition = forall s. StateMachine s =&gt; Transition s class StateMachine s where onUpdate :: Int -&gt; State s (Maybe Transition) debug :: s -&gt; String instance StateMachine Transition where onUpdate dt = do s &lt;- get let (t, s') = runState (onUpdate dt) s put s' return t debug (Transition s) = debug s --one state machine implementation data Counter = Counter {unCounter :: Int} instance StateMachine Counter where onUpdate dt = do modify (Counter . (+dt) . unCounter) return Nothing debug = show . unCounter --another statemachine implementation data Split = Split {unSplit :: Int} instance StateMachine Split where onUpdate dt = do modify (Split . (+dt) . unSplit) t &lt;- liftM unSplit get if t &gt; 10 then return . Just . Transition $ Counter 1 else return Nothing debug = show . unSplit This looks pretty similar to the existential typeclass antipattern to me, but I'm not sure how I'd convert this to something else.
I just made a habit of writing properly indented code straight away; vanilla vim pretty much covers everything I need.
But it could be good to save some typing. Say I'm typing: blaBla { blipityBla :: Integer and I press enter. Cursor should go under the `{` no? Instead I have to lean on the tab key and wait for it to arrive. Maybe there's some shortcut and if so I'd like to know. In a perfect world it would also insert `, ` for me but I can see how that might get out of hand. Also, god help you if you want to change `blaBla` to `blaBlaBla`
I use hindent and the Haskell text object plugin. I am currently using /u/chrisdoner s style, but will be adapting it a little (I find the line breaking way too aggressive).
I use this vim plugin: https://github.com/raichoo/haskell-vim It does exactly what you want.
There are several vim plugins that implement this type of functionality (though not the particular record example you showed, AFAIK.) https://github.com/jaspervdj/stylish-haskell https://github.com/godlygeek/tabular https://github.com/raichoo/haskell-vim 
Yes. Yes it does.
For what it's worth: I've adopted a coding style that involves only changing the indent by a couple indentation levels at most. It makes formatting much more regular, which obviates a lot of the need for tooling help. (For example, in your record above, renaming `blaBla` could necessitate reindenting 7! other lines.) I also find the more-consistent baseline makes code easier to read. data BlaBla { blerg0 :: String , blerg1 :: String } derives Show main = do let longIdentifier = BlaBla "I'm not sure" "what this is for" putStrLn "Not so bad after all..." I suppose this is not for everyone, but it makes my life easier... EDIT: a `def` snuck in there; looks like I need to get off Scala for a bit.
I think that you can achieve a "method" that "operates on itself" by storing a partially applied function in your record. This is how I plan to address my case, at least. Don't know if it will work for you.
Right, but in reality I will also need the Basket for other purposes than eating.
This also helps reduce merge conflicts if a project has multiple active developers.
Structured Haskell Mode for Emacs is pretty good at auto indentation.
&gt; Cursor should go under the { no? That depends on your preference; I tend to format record syntax like so: data BlaSomething = BlaSomething { blipityBla :: Integer , somethingElse :: String } ...which means vim's smart indentation does exactly the right thing. This formatting also means no matter how I change my identifier names, I do not have to reindent (and by consequence, it keeps diffs nice and clean). The only exception to this formatting is when the entire definition fits on one line (80 characters).
If you are in the Houston, Texas area, the author of haskforce will be discussing the design and implementation of the plugin at the next Houston Haskell Users Group: http://www.meetup.com/Houston-Haskell-Users-Group/events/214903722/ We are a relatively new group and looking forward to meeting more people interested in Haskell in the area.
Obligatory emacs comment. Shrug this mostly just works. When it doesn't things like Rectangle support make it pretty painless to fix.
Is there a style guide?
I'm doing almost exactly this. My rules are that if a single function call or statement won't fit within 72 chars on a single line, then break it up into multiple lines, with generally one line for each sub-element; and no line can jump to the right more than a single indent level (i.e. 2 spaces) from its previous line. You can see this in action here: https://github.com/yawaramin/ggspec/blob/97dab55962f63d9692475b171fd6fbd76ac64731/lib.scm And for your example, I would do the `BlaBla` definition the same way, but `main` would be: main = do let longIdentifier = BlaBla "I'm not sure" "what this is for" putStrLn "Not so bad after all..." What I'm trying to achieve is to format multi-line code as if it were a tree structure, so you can immediately see what bigger expression a smaller expression is part of, by just looking at what's to the left.
That makes perfect sense, I think that's the solution I was looking for on this. I can then turn my code into this instead: data StateMachine = StateMachine { onUpdate :: Time -&gt; StateMachine } data Counter = Counter {unCounter :: Int} counterUpdate (Counter c) dt = counterStateMachine . Counter $ c + dt counterStateMachine c = StateMachine { onUpdate = counterUpdate c} data Split = Split {unSplit :: Int} splitUpdate (Split s) dt = let s' = s + dt nextState = if s' &gt; 10 then counterStateMachine (Counter 1) else splitStateMachine $ Split s in nextState splitStateMachine s = StateMachine { onUpdate = splitUpdate s}
Does it work with WebStorm?
This is largely why I use the unusual tab-based indentation style I use. https://git-annex.branchable.com/coding_style/ In this example, I only need to indent the first term with 1 key-press, and then the remaining terms all auto-indent as I add them. Your approach is the same as mine, except you're using a small number of spaces rather than 1 tab. I prefer the tab, but I understand it gives some people the heebie-jeebies.
Does not seem so. Even settings are not available. I just opened an issue.
There shouldn't be much boilerplate in the backends. In my library, this is the entirety of the code required to support stripe-http-conduit: https://github.com/stepcut/stripe/blob/master/stripe-http-conduit/Stripe/HttpConduit.hs#L25 A mere 25 lines of code. These lines of code would have to be written for backends anyway, so I don't think there is much added overhead. To convert your codebase to be almost exactly like mine is actually very simple: 1. remove all calls to callAPI 2. add a phantom time parameter to StripeRequest 3. change type signatures from -&gt; Stripe a to -&gt; StripeRequest a 4. remove unneeded imports 5. tweak 'stripe' function to have callAPI In this solution there is no Stripe monad. Instead each API request is made as a separate call via the 'stripe' function. If there is a desire to retain the Stripe monad, then it is fairly trivial to replace the current Stripe monad with an implementation built around the free monad. (This would be a completely different approach than the one outlined above). I have done this here: https://github.com/stepcut/stripe-haskell/tree/free-monad Though I have not factored out the client into a separate package yet. That said, I am somewhat dubious of the value of the Stripe monad. I will think about that more and write something sensible about my conclusions.
Thanks for opening the issue, I'm digging into it. https://github.com/carymrobbins/intellij-haskforce/issues/65
Amazing. Can't wait to try this. I love IntelliJ products, but I had to revert to using emacs for Haskell because the support is so much better than in other editors. I really hope this plugin changes that. So far the features look really great. Thank you!
It's still in development but you may want to try https://github.com/chrisdone/hindent
Seems very neat, thanks for taking the time for putting in place an appealing project page with some animated gifs :) I would be interested in discovering more about the toolchain integration, where lots of other IDE plugins seems to still be "weak". Does HaskForce supports cabal sandboxes "out of the box", as well as global GHC installations?
You can salvage this signature by checking the input (for example with `all isDigit`), and passing it to `read` if it's all numbers, or calling `getNumber` again. You might also use the `Text` data type instead of `String` (see [parsing](http://hackage.haskell.org/package/text-1.2.0.0/docs/Data-Text-Read.html) and [getting the input](http://hackage.haskell.org/package/text-1.2.0.0/docs/Data-Text-IO.html)).
Is a Shonen Anime, prostitutes are not allowed.
Thank you
Some general tips: Switch off the tabs. Chose the number of spaces the tab will convert to and stick to it in all editors your team uses. Use empty line breaks between expressions. They help readability greatly. Exploit natural rectangular structure of repeating lines and align them using spaces.
&gt; In my talk I gave an example: &gt; You could have a state machine that when given an X returns a Y, when given a Y returns a Z, and when given a Z returns an X. People are overengineering this: type StateMachine = (X -&gt; Y, Y -&gt; Z, Z -&gt; X)
I actually like one space indentation for Haskell. Keeps everything compressed.
[Everyone seems to be saying this](http://www.reddit.com/r/haskell/comments/2kvssf/id_like_to_repeat_ucategorias_question_from_a_few/clp62nx). It's a good example of how the right approach is worth 40 IQ points.
I'd like to understand why you say it's not possible to type a state machine today. I think Moore and Mealy do a good job of this: data Moore i o = Moore (o, i -&gt; Moore i o) data Mealy i o = Mealy (i -&gt; (o, Mealy i o))
My concern is that since filter must produce an `m (S i)` from `m i `, `a`, and `m i -&gt; a -&gt; m (S i)` but, crucially, must ignore the `a` it gets stuck. Your policy method works fine so long as `next a = id` for `a` which fail the predicate.
Looks really nice.
You're probably better off using `fmap readMay getLine` than handling the data validation yourself, and then handle the Nothing case as appropriate. Or if you don't care to refactor much just define a combinator like: totalize :: IO (Maybe a) -&gt; IO a totalize m = m &gt;&gt;= \case Just a -&gt; return a Nothing -&gt; totalize m
I'll be sure to have a look
For one, I like the infix operators.
Thanks for the info. I am just a casual Haskell user and tend to use it mostly on *nix with Leksah. But I was quite impressed with EclipseFP when I tried it a few years ago. As for Windows, I tend to use F#. It is not Haskell, but I spend most of my time in JVM/CLR land.
https://www.haskell.org/haskellwiki/Haskell_Lisp
Isn't that going to yield a partial function though, since you can't prove it will be productive? 
Why are you saying it isn't popular with Haskellers? A lot of Haskellers really dig sexprs. However, Haskell lets you define operators in such a way that some expressions get really convenient to write, which wouldn't work as well with pure sexprs. After all, sexprs are simpler, but also more verbose. For example, consider either error id . eitherDecode . pack &lt;$&gt; jsondata which with sexprs would be something along the lines of (fmap (compose (apply either error id) either-decode pack) json-data) The latter is *much* harder to read, in my opinion. Maybe even worse is something like flip evalState gen . both (state . randomElem) Again, with sexprs, (compose (flip (apply eval-state gen)) (both (compose state random-elem))) which I also find much harder to mentally parse.
Nice work. I think my project was an answering machine. And not a full product just some components on a breadboard that barely worked. But I was passionate about answering machines.
It won't be partial, but the io action resulting from the function will potentially take a long time to execute, if the user persists in entering non-numeric text.
Check out this paper: [Why calculating is better than scheming](http://www.cs.kent.ac.uk/people/staff/dat/miranda/wadler87.pdf) by Wadler.
Well if you pass it something like `return Nothing` it will infinite loop, so it is possible, but for interactive uses you should be fine. Maybe it's poorly named though.
Compared to eclipsefp, very basic, e.g.: can't apply hlint suggestions, doesn't integrate with hoogle, doesn't have GUI editor for cabal files, [etc](https://eclipsefp.github.io/features.html). It does look more polished though.
Windows (from Europe): cabal update Downloading the latest package list from hackage.haskell.org cabal: failed FreeBSD (from USA): cabal update Downloading the latest package list from hackage.haskell.org cabal: timeout 
http://www.downforeveryoneorjustme.com/http://hackage.haskell.org/ Says it's down for everyone.
But the [official status page](https://status.haskell.org/) says it's Operational! What is this status page measuring? *edit*: ah, the official page also says it was last updated 17 hours ago. I guess I understand that it takes fewer resources to update the page once in a while and to serve the cached results than to poll all the services on demand, but then, what's the point of having a status page? To inform us of all the breakage which last for more than 24h? I doubt the current breakage will last that long.
Are functions that aren't guaranteed to return considered partial functions? I thought partial functions were ones that were defined only for some of the possible inputs.
reads is pretty useful for this getNumber :: IO (Maybe Int) getNumber = do x &lt;- getLine return $ case reads x of [(i,"")] -&gt; Just i _ -&gt; Nothing 
yup. I use it all the time.
I encourage any Haskellers who use Vim to check out Emacs + Evil mode. There are some really great Haskell modes out there which have greatly increased my productivity.
The status page gets updated manually, on purpose, so a few ping timeouts don't cause false positives. It now explains there's a problem.
What's difficult about this? :%s/blaBla/blaBlaBla Also, check out Tim Pope's [Abolish](https://github.com/tpope/vim-abolish), which provides a case-friendly `:S`, e.g.: :%S/foobar/barfoo/g foobar, Foobar, FOOBAR -&gt; barfoo, Barfoo, BARFOO 
They are in some sense equivalent. Bottoms are sometimes considered indistinguishable, so that `head []` and `let x = x in x` should be the same, even though one raises an exception, while the other just loops (though GHC might detect the loop and replace it with an exception).
I'm not seeing it move the cursor under the `{` - are you?
I see. I hadn't even thought of the implications to diffs. Even auto-reindentation will have that problem, though.
Sounds good, I'm just not sure we want to make knowledge of free monads and monad-control required reading before someone can benefit from this library. The 25-line boilerplate that you wrote, while simple, might not be as intuitive for a newbie who wants to use stripe in his/her application, my fear is that it might dissuade people from using this library. I don't want to expose too much guts to the end user.
I mostly use F# as well, but wish I could use Haskell more. Some day there will be good interop I hope, but for now I usually reach for C++ when I go native as the interop with C++/CLI is super easy.
This is now fixed.
back now
How does it compare to https://plugins.jetbrains.com/plugin/7453?pr= ? I mean, cabal integration is clearly something atsky's plugin lacks. Is there anything else? By the way, I'm really looking forward to this. Thanks!
Knowledge of free monads is definitely not needed by the end user, it is completely hidden. Only someone implementing a new backend would need to understand it. The boiler plate in that library is just the boiler plate for the stripe-http-conduit backend. And really, there are probably only three packages like that which would exist: stripe-http-conduit, stripe-io-streams, stripe-pipes. If you look at the code in that branch you'll see that I didn't actually have to change any of the existing definitions for the API calls or the test suite -- except for a hack in the Dispute.hs test. The actually end user experience is pretty much identical to what it was before. The only difference is whether they choose to install stripe-http-conduit or stripe-io-streams or stripe-pipes The only reason I had to hack Dispute.hs is because my current implementation does not support liftIO -- which is fixable. Though that also ties into my concerns about the existence of the Stripe monad in the first place. I should try to make it on Sunday so we can hash this out. 
Why wasn't there a user group when I lived in Houston. :| (Well, considering I ended up starting the Austin meetup, I guess the answer is obvious...) EDIT: Damnit, I could have come if only it had been a week later, too, since I'll be there. :(
I'm amazed how much quality work is being done with Haskell. And it's all available for us to use. From papers to PhD theses, just amazing.
The only issue with using tabs is what is fine length wise for a tab at 2 or 4 character indents may not be at 8. One reason I think spaces in principle is overall more consistent. Though its guaranteed to annoy someone that thinks a tab stop should have been some other number than 8/4/2/etc...
Nothing prevents you from restricting yourself to S-Exprs. So, your question makes no sense, as it overlooks that Sexprs are valid expressions. 
[hxt-relaxng](https://hackage.haskell.org/package/hxt-relaxng) is in a more mature state than any XSD library you're likely to find. Besides, RELAX NG in general is a cleaner and more powerful formalization than W3C XML Schema. 
&gt;And finally just to clarify, when the poster is talking about "run-time number", he does not mean that it the numbers will be checked at run-time, he just means whatever number the function is called with at run-time? Correct. With dependent types, we sometimes can (and want to) erase some type information at run time. In Haskell, it's the opposite - it takes extra trickery to make sure dependent values are the same at compile and run time. Unlike map, which is nice to know at compile time that it doesn't change the length but doesn't use the length information at runtime, replicate does need the info, so in Haskell it needs to package compile- and run-time info into a new type so you know the two are the same. Disclosure: Agda programmer who skipped over Haskell, so I'm no expert in Haskell's DT tricks.
I do it this way too.
There are two types of diffs. If for human consumption you can ignore spaces. If for machine consumption you need the whitespace. Typically when querying a system you can ignore spaces and the diffs will look fine (git diff -w ...)
Works for me.
Those are some very nice demo gifs!
This is almost convincing me to stop work on my Sublime Text plugin. 
I fall heavily on the "indent with tabs, align with spaces" side of the debate. If you do this right everywhere, everything is fine. However, ghc annoyingly treats tabs as 8 spaces, which can cause seriously confusing errors if you mix up a tab and a space. 
Thanks for the link. That just appears, if I read right, to be the XML schema data type definitions.
Well, it seems to generate relaxng, which will do quite nicely. If I can persuade it to include documentation somewhere from the source, then I'm set. Thanks. Which leads on to a different question. What's the preferred way of annotating a declaration with additional data for metaprogramming? I've found annotations for ghc; is there a compiler-neutral form that I should be looking for.
Thanks. Happy with RELAX NG; there are enough tools around to make it a useful interoperability format. With pickling, I think I have [some useful further questions](http://www.reddit.com/r/haskell/comments/2ktt25/xml_schema_bindings_for_haskell/clpl3lg).
 { blipity :: Integer , blerg0 :: String , blerg1 :: String , blerg2 :: String , blerg3 :: String , blerg4 :: String , blerg5 :: String , blerg6 :: String } What is the point of having the comma in front?
Got it. Thanks.
Thanks!
It's nice to have the uniform parts of multiple lines appear in uniform positions: * They become less noisy, easier to ignore them as they add little or no information * It's easy to spot when they're missing 
I used this to implement optimizations for a small language as part of an offline interview. Once you set up your data types, everything is pretty straightforward to use. The examples (constant propagation and liveness analysis) are very helpful for understanding the library, particularly if you reimplement them using your own types.
You need it once in a blue moon but when you do need it its really confusing to get that unhelpful "parse error" message. Last time I came across this problem I was doing some some parser combinator stuff.
Don't! Please continue.
&gt; Nothing prevents you from restricting yourself to S-Exprs. This claim makes no sense. A language is how other people use it. You use a language the way others do so that you can understand eachother. Writing everything with prefix notation in Haskell would just confuse everybody and would not be Haskell. Besides, there's too much built-in syntax that is fundamentally infix that you can't just not use.
But they forgot the essence of Lisp which is symbolics.
Javascript city sounds a lot like Dubai.
I can't even come up with a scenario when you would need it, I mean when lazy is the default why do you need a syntax to matching lazy patterns
Yuuuuuuup.
How do you type if j/k are rebound to escape?
damn it henning (i quite adore numeric-prelude but the C's and T's are annoying as fuck)
For those who don't get the reference: henning is what Henning does. (Not only he, of course. I am sometimes guilty of that, too.)
Who is Henning? EDIT: Oh, it's the package maintainer.
Where did you move?
this! Can't upvote this post enough
I am trying something like this with [threepenny-gui][1]. However, I would say that &gt; Is it because FRP is good for this but kinda not yet figured out exactly? is true. I think we have the FRP part down in since the last few years, but we still have to figure out how to design GUI widgets with FRP. [1]: https://www.haskell.org/haskellwiki/Threepenny-gui
I've quite enjoyed the brief bit of GUI programming I did with `reactive-banana` and `gtk`. You have to do some horrible imperative stuff in a few places, but the heart of the system lies in a `reactive-banana` network, which I find quite composable, and really pushes you to build a lot of things as pure functions. The downside is it's one helluva learning curve.
Disregarding usability concerns, the T/C distinction goes against the philosophy that classes are first-class types (of kind that ends with `Constraint`) - they can be passed as parameters to datatypes and so on. If you want to go this way, name classes `T` as well.
&gt; Overall, there's been a lot of separate points made, including CLAs (unlikely), "Developer Certificates of Origin" a la the Linux Kernel, and reworking how we mark up header files, and keep track of GHC's long history of authors. If you work on a big project where some of these concerns are real, we'd like to hear what you have to say! It's a about time! I'm a bit surprised to hear that the GHC project with its size and importance doesn't already follow any such contribution policy... PS: The blog-post "[Embracing the Developer Certificate of Origin](http://www.do-not-panic.com/2014/02/developer-certificate-of-origin.html)" make a few good points regarding the [DCO](http://developercertificate.org/) IMO
I remember a thread/post on this not long ago: the delay you are experiencing could be because you're using tmux.
It has black text for me on mobile. 
It's hungarian notation and increment-i-by-one comments all over again.
The jk thing isn't really default vi anyway; it's just a common thing for vim users to do. Personally, I remapped capslock to esc and didn't touch .vimrc in that matter at all, which is nice because it's convenient for a lot of other programs as well. The obvious downside is that now I have trouble working with other people's computers...
yes, you can customize it in the *Run/Debug Configurations*
I've remapped capslock to backspace, and use the kj as esc mapping because it means I never need to leave the home row. I had some minor RSI, but those 2 remaps fixed it almost completely. Getting the same keybindings to work in Firefox and the terminal fixed the rest of the issue. Using someone else's machine is a bit painful, but that's largely solved by using mr/vcsh to pull down all my config. I can move into a machine in a couple of minutes.
DONT
Not sure if this was the original question, but mine is, why have the comma at all? Is this not clear enough?: data Foo = Foo { x :: Integer y :: String } 
the *HaskFroce* plugin seems to be more actively developed you should be able to see it here, after the statistics catch up with latest developement: * [HaskForce on *Open HUB*](https://www.openhub.net/p/haskforce) * [haskell-idea-plugin on *Open HUB*](https://www.openhub.net/p/haskell-idea-plugin) and there is also [intellij-haskell](https://www.openhub.net/p/intellij-haskell), but that seems quite new 
It's an Ocaml idiom. See e.g. http://caml.inria.fr/pub/docs/manual-ocaml/libref/Map.Make.html. A module is usually about some distinguished type, by convention always called `t`, because the name of the type is redundant with the name of the module. So instead of import Data.Map (Map) import qualified Data.Map as Map f :: Map k a -&gt; ... you can import qualified Data.Map as Map f :: Map.t k a -&gt; ... same goes for Set, ByteString, Text and a much of others... Not sure how much I like that style, but it's a style that encourages pervasive qualification of names. That's probably a good thing.
I moved to Austin, TX after living in Houston, TX for several years... and I've already lived here in Austin for over 2 years at this point. Time flies.
It would be nice if ghci had a switch to start up raw without haskeline enabled, which would make the rlwrap option better.
What do people want with a Sublime Text haskell plugin? The "special feature" of mine is it only uses ghci, so should work "out of the box" with all configurations (plain haskell file, cabal project, nix) avoiding the problems that SublimeHaskell has with installation. I currently have: * Autocomplete (with types/kinds for top level definitions) dependant on which modules are imported. * Error reporting in an output window on save * Marking errors in the source files * Testing on save/successful compile (by getting the user to provide a module and a function to run) Feature I'm considering: * Autocomplete of LANGUAGE pragmas * Autocomplete of modules in import statements * An integration with the GHCi debugger, so you can debug your haskell inside Sublime Text * Getting types/kinds of highlighted subexpressions * Running highlighted subexpressions * Integrated repl window Which of these do people want the most? Is there anything else you could think of? Does anyone know an easy way to get haddock for a function with just ghc/ghci/cabal/haddock from a default install? 
Except that haddock can't deal with it and reading documentation becomes an exercise in link-hovering and cursing.
Could you please expand on what makes designing GUI widgets with FRP difficult? Alternatively, is there a family of tasks for which it is easy to design such tasks with FRP? The examples which come with FRP packages are often games, and I've always thought it was to demonstrate that the libraries were fast. But maybe that's because games are actually better-suited for FRP than other tasks?
This is excellent. I'm a TA for a Haskell course that just started, and the students are all coming from Java, and a lot of people used IntelliJ. Now they can just keep doing that.
whats that increment thing about?
Oh, you mean in the Haskell grammar itself.. Well, that could have been nice, perhaps.
By "Have an FRP part down" do you mean that it has been figured out? I'm not a native speaker, confirming that I understood that phrase right :) Also I saw that threepenny first relied on reactive-banana, but than you wrote somewhere "now it has it's own FRP submodule", so now they are independent? I am a little curious why.
actually haddock CAN. Its just not enabled by defualt haddock --help ... -q QUAL --qual=QUAL qualification of names, one of 'none' (default), 'full', 'local' 'relative' or 'aliased' 
Thank you for all your hard work GHC team!
You might want to checkout a related topic discussed here just a few days ago: http://www.reddit.com/r/haskell/comments/2kkxhg/on_the_state_of_gui_programming_in_haskell_june/ I posted a reply about UISF, a purely functional arrow based GUI built on top of OpenGL and GLFW. http://www.reddit.com/r/haskell/comments/2kkxhg/on_the_state_of_gui_programming_in_haskell_june/clmdelc The design of UISF was initially borrowed from Conal Elliott's [Phooey](https://www.haskell.org/haskellwiki/Phooey) ideas (disclaimer: I wrote the first version for [Euterpea](http://haskell.cs.yale.edu/euterpea/)), and Daniel Winograd-Cort later modified it to work with Arrows and recently made it a [standalone package](https://hackage.haskell.org/package/UISF).
&gt; By "Have an FRP part down" do you mean that it has been figured out? I'm not a native speaker, confirming that I understood that phrase right :) Yes, that's what I mean. Both [sodium][1] and [reactive-banana][2] are what I think is an efficient implementation of the classic FRP API pioneered by Elliott and Hudak, slightly modified because of an observation by [Patai][3]. [1]: http://hackage.haskell.org/package/sodium [2]: http://hackage.haskell.org/package/reactive-banana [3]: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.182.2374 &gt; Also I saw that threepenny first relied on reactive-banana, but than you wrote somewhere "now it has it's own FRP submodule", so now they are independent? I am a little curious why. This is just an implementation detail. At one point, Threepenny needed an API that reactive-banana didn't provide, so I reimplemented a small FRP engine yet again. Threepenny will return to using reactive-banana at some point. In some crucial moments, it's nice to be in control of the whole library stack, because then I can fix integration issues like these. 
&gt; Could you please expand on what makes designing GUI widgets with FRP difficult? Well, I wouldn't say that it is difficult, just that nobody has done it, yet. The thing about FRP for GUI widgets is that you have to rethink the whole approach if you want to avoid the imperative style entirely. I have a [preliminary design document][1] and an [old experiment called "BlackBoard"][2], but this needs to be expanded and iterated upon before I am confident to say "Yes, this is how GUIs and FRP should work." But do have a look at [an old paper by Conal Elliott][3] to see what should be possible. Games are a little easier at the moment, because you are essentially writing a monolithic function of type `Event Mouse -&gt; Behavior Graphics` and don't have to worry much about compositionality. In contrast, GUI widgets are about encapsulating some portion of that `Graphics` output and some portion of that `Mouse` input, and I for one do not have a clear idea of how to do this in an elegant way. [1]: https://github.com/HeinrichApfelmus/threepenny-gui/blob/master/doc/design-widgets.md#three-principles-for-representing-mvc-concepts-with-frp [2]: https://github.com/HeinrichApfelmus/reactive-banana/tree/b8c353bf275ee2c58a05cbc18f33fe481c8e70e4/BlackBoard/src [3]: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.31.1064
In the years I've been using this keybind, it has come up maybe twice? Both times I remember are when I was manually typing `alphabet = "abcd...etc"` in other, inferior languages. Vim (and maybe inputrc?) has a timeout that you can configure.. if you type j, wait a second or so, then k then it's fine. The only niggle is that j by itself in bash/readline won't be displayed on the screen until you type some other character after it. I've gotten used to this by now but it was disconcerting for a while. The ease and comfort of mashing "jkkkkkk" to go up to something recent in my history, or "jkbbcw" to replace a word, and so on, cannot be overstated.
Yes, I read all those blog posts about reactive values and got a good feeling out of it! But it looked like very much work in progress and I dunno whether it will be continued (by keera studios). Also saw your link about UISF, even opened it, but didn't give it much studying, thanks will remember to do this!
I use the mealy machine in Shake to predict progress. See http://neilmitchell.blogspot.co.uk/2013/12/progress-reporting-in-shake.html. I'm not actually using the machines package, but I ended up with something isomorphic to it.
What is the type of `(.)` in that S-expression?
Something like this: // Get foobar from baz function getFoobar(Baz baz) { // get baz.lastFoobar and store it in i var i = baz.lastFoobar; // increment i by one i += 1; // return foobar from baz, at position i return baz.foobar[i]; } In other words, comments that duplicate the information found in the code, instead of adding useful information. The only kind of comment that is worse than this is "just so we have 100% javadoc coverage" comments: /** * @todo: document this class. */ class Foobar { /** * @param int i * @return int */ public int foobar(int i) { // actual code here } } 
Great!
The move is completed; the DNS changes should propagate very soon for users. Please let us know if something is wrong.
Great. I wish it expanded on the "Transformers" slide. dumbSum = execStateC (0 :: Int) $ mapM_C $ \i -&gt; modify (+i) Looks like a conduit accumulating state which seems useful but I can't figure out how to run it in a pipeline like enumFromToC 1 (10 :: Int) $$ dumbSum $= printC &lt;interactive&gt;:29:30: Couldn't match type ‘Int’ with ‘()’ Expected type: Conduit Int m b0 Actual type: ConduitM Int b0 m Int In the first argument of ‘($=)’, namely ‘dumbSum’ In the second argument of ‘($$)’, namely ‘dumbSum $= printC’ Need a way to combine a `Conduit` and a `ConduitM`
Who needs backspace anyway? Also, key remapping makes watching others trying to use my computer fun - at work, my boss ordered a Dutch keyboard on my laptop instead of the US I told him to order; I use US layout anyway, just stopped looking at the keys, and I swapped Fn and left Ctrl, remapped CapsLock to Esc, right Ctrl is Compose, and none of the usual desktop OS key bindings work because I run XMonad with my own completely incompatible key bindings. Basically, people either try to do things and stuff explodes under their fingers, or they just stare and try to find something that looks familiar - but there's just an empty desktop and a tiny status bar. Great fun.
&gt; Which of these do people want the most? For me that would be: debugger, navigate to/find usages (should probably work with library functions too), autosuggest imports based on typed identifier, integration with hlint. PS. What is the name of your plugin?
I'm an Xmonad user too, I have the pentadactyl/pterosaur plugins for firefox too, and I disabled my arrow keys to train myself off using them because of the afformentioned wrist pain, I kinda forgot to reenable them. Also my mouse is a little broken, the scroll wheel just spins freely. It is indeed very amusing to watch people try to use my computer.
I didn't elaborate in the slides, but in the talk itself, I covered this issue in more detail. The problem is that the default fusion operator (`$=`) requires that upstream return no value. The reasons for this essentially come down to making the normal use case easier to write. You can solve your problem in at least three ways: 1. Instead of `$=`, use one of the special fuse functions that preserve upstream results (e.g. `fuseUpstream`, covered in slides 29 and 30). 2. Treat both `dumbSum` and `printC` as `Sink`s, and combined them using `ZipSink` (slide 26 gives one example of that). 3. Instead of using `printC`, use `iterM` from Data.Conduit.List and put `dumbSum` at the end of the pipeline instead. The example on slide 30 is actually incredibly close to what you're going for here (I have a feeling that was actually the motivation for this question, unless I'm mistaken). That's likely your best approach here.
&gt; Could not deduce (Num t) arising from the literal `0' &gt; from the context (Bits.Bits t) It looks like `Bits` [used to have](http://hackage.haskell.org/package/base-4.5.1.0/docs/Data-Bits.html#t:Bits) `Num` as a superclass a few years ago, and that this package hasn't been updated since. I would try installing [GHC 7.4.2](https://www.haskell.org/ghc/download_ghc_7_4_2#x86_64linux).
The sum calculated by `dumbSum` is a return value in the monad, not a value yielded in the conduit pipeline. λ&gt; enumFromToC 1 (10 :: Int) $$ dumbSum 55
alright! thank you! should i uninstall cabal as well?
I don't think that should be necessary, but I have never used such an old version of GHC myself, so I can't speak from experience. Good luck!
To expand on this, here's a slightly modified example that uses the internal state to yield a stream of cumulative sums: λ&gt; let cumulativeSum = evalStateC (0 :: Int) $ mapMC $ \i -&gt; modify (+i) &gt;&gt; get λ&gt; enumFromToC 1 (10 :: Int) $$ cumulativeSum =$ printC 1 3 6 10 15 21 28 36 45 55
Thanks. That's the one I wanted.
Didn't realize jk remap was a common thing I've never heard it before. I also remapped caps to esc, and have the same problem on others computers.
`Conduit` is just a synonym for `ConduitM` with a `()` return value. &gt; type Conduit i m o = ConduitM i o m () http://hackage.haskell.org/package/conduit-1.2.1/docs/Data-Conduit.html So expanding that synonym in the error message Expected type: ConduitM Int b0 m () Actual type: ConduitM Int b0 m Int That's where it is getting the `Couldn't match type 'Int' with '()'` message from.
Have you considered using the [`ViewPatterns`](https://ghc.haskell.org/trac/ghc/wiki/ViewPatterns) extension?
You can pattern match inside guards with `pat &lt;- expr`.
remember that you can do case staments too, even case staments on tuples, and case statements can also have guards :) 
It's not on package control yet, still needs some polishing up. I'm hoping I'll find some time tomorrow/this week to work on it and make a release. 
Not sure how easy navigate to/find usages is going to be with just ghci either - I think to library functions at least is far out of our reach. 
Aka `PatternGuards`.
What does the syntax look like for case statements with guards?
 case flibble of (Zort x) | x == 1 -&gt; stuff1 | otherwise -&gt; stuff2 (Zibble y) -&gt; stuff3 (Flurble q) | q == 42 -&gt; stuff4 _ -&gt; whatever_man One thing to note is that if a guard on matched pattern falls though, it will go back to the case statement pattern matching. So in that example, if `flibble` was `Flurble 6` it would be handled by the base `_` wildcard case. Another fun thing you can do is combine pattern guards (requires `PatternGuards` extension) in your case or function guards. IE: case maybemaybeint of silly | Just x &lt;- silly, Just y &lt;- x, y == 2 -&gt; thing _ -&gt; whatever Of course that example *is* silly, and you probably wouldn't use pattern guards there. Just an illustration.
I like your sample gibberish placeholder words. Reminds me of Pinky and the Brain. Thanks!
I'm not sure about the difficulty to build a GUI library. Game developers (including free software devs) often build own little GUI's from scratch in its games. These GUI's aren't well designed, but they work. I guess, its because of portability and stability to build on top of a existing well defined GUI library. Just like the fact, that there is no real Haskell DBMS, but many well defined bindings to existing DBMS. 
Thanks for your reply. Since you left me an exercise, let me answer that first :) : `afterVowels = map snd $ filter (Alph.isVowel . fst) $ T.zip s (T.tail s)` Unfortunately, this approach doesn't solve the problem. The syllabification rule is about where the first vowel in the word is, and if the following characters are vowels. The case you wrote checks the characters following the second and third vowels in the word, which is not really what we want, that's why the tests fail on this code. I tweaked the code a little bit, then I noticed that the case wasn't checking what we were looking for. (of course, I cannot expect you to understand this rule without any proper specification. I should write better documentation for this function.) Thanks once again.
I was apparently overthinking about the `charAt` logic. This change made it much simpler, I think I'm going to go with this one. Thank you very much. :)
I feel the same way. I'd be interested in hearing from Haskellers who tend to use some idiom which clashes with this argument ordering, or from anyone who knows why the ordering was picked to begin with. It doesn't make sense to me either.
I agree that the other order is better. As far as I know it the only reason it's in that order is because it's defined using a newtype newtype StateT s m a = StateT { runStateT :: s -&gt; m (a, s) }
Ah, that's what I get for not reading the code carefully enough. But that's easy to fix? Just change what `afterVowels` gives -- `drop 1 . dropWhile (not . isVowel)` or the like...
The function also needs to keep track of the index of the character we are talking about, so that it can `handleSubstring` with a specific index. This works really well for that purpose: http://www.reddit.com/r/haskell/comments/2l09it/how_would_you_simplify_this_guards_prevent_better/clqd5t5
you can use `runStateT` to make things more concise. 
I agree. I either end up using flip or refactor the state function out. 
I find it makes a lot more sense flipped when trying to compose with other transformer runners ...
 map (evalState hoodabaga) statefulThings I use it the other way a lot as well, so ¯\\\_(ツ)_/¯ Damnit how do I get that smiley to work...
¯\\\\\\\_(ツ)\_/¯ -&gt; ¯\\\_(ツ)_/¯
I usually borrow a nice `(??)` combinator from `lens` to use it runState ?? My crazy complicated State $ do ... It might be nice if mtl/transformers offered other names for the flipped forms, but there is a hell of a lot of code out there using the existing names with their existing orientation, and there is something to be said for just picking a direction and going with it to maximize recognition and understanding.
Thanks
aka `Haskell2010`
And this is crazy. But here's my number, so [`readMaybe`](http://hackage.haskell.org/package/base-4.7.0.1/docs/Text-Read.html#v:readMaybe)
it would be great to have that ?? operator mentioned in the trans docs....
Although the technical reasons why it's non-trivial to implement is well documented, the "soft" reasons are not. Sure, it's a difficult task, but it doesn't seem impossible. The question is why the interest isn't big enough in the community. Do people even care about this?
Haskell isn't really consistent in its decisions for when separators can be omitted. 
Supported in Emacs and Vim. =)
A few do. Its import arranging and aligning is already supported in Emacs. All my pragmas are added automatically one by one, so they never need reformatting; I don't even look at them. Emacs removes trailing whitespace when I save a buffer. Basically `stylish-haskell` does things my editor can already do (but maybe yours doesn't, so YMMV), except with the additional problem that it discards things like `^L` which I might use for paging my code, but the HSE parser considers whitespace and strips out. An acceptable trade-off, but only if the features provided are needed.
People target JVM because they want to benefit from library / framework ecosystem. E.g., that was a huge benefit for Scala. But I don't see how Haskell's library ecosystem would benefit. 
The upside is that it looks the way it was written. That's a valuable property. It also works in practice. That's a valuable property, too.
&gt; Use empty line breaks between expressions. You mean declarations? Statements?
I actually like the current order as it lets me interpret `runStateT` as "compute the denotation of `StateT s a`". That's an entirely theoretical reason, though. In practice, it's always flip runStateT state $ do ...
Portability of binaries would be the primary reason I'd wager.
There's Frege ( https://github.com/Frege/frege ), but it's not Haskell, it's just 'a non-strict, pure functional programming language in the spirit of Haskell'.
Do you mean type bandings as in for example (Int, Int) -&gt; Int? If that's what you're asking about, it's fairly simple. You have a pair of ints, and you want to return one Int. Doesn't matter which one. If you have [Int] -&gt; Int, you have a list of ints and want to return one int. The last element in such a binding is always the output. If you're asking for a type binding for nim, i think it's supposed to be IO (), since you have no initial input. I am also relatively new so forgive me if i am wrong :-)
You get less bullshit of the [`AbstractSingletonProxyFactoryBean`](http://docs.spring.io/spring/docs/2.5.x/api/org/springframework/aop/framework/AbstractSingletonProxyFactoryBean.html) sort.
Scala (and clojure to a lesser extent) seems to dominate the Kingsgate Mindshare of people interested in Haskell's features and invested in the JVM. The scala compiler is already quite complicated to get right but currently does a good job. So there is little short term gain ratio from porting Haskell. Functional programming and advanced type features on JVM is much harder than many realize (it killed 10 years of Sun backed Fortress development). I would be most interested in seeing a high level lambda calculus virtual machine (as opposed to the class based jvm or LowLevelVM) to replace JVM *cough* /u/tekmo *cough*
The worse C library I know of, API-wise (openssl), has a long way to go before it reaches the horror of spring or jee. I don't think it's presumptuous, "successful" java library tend to evolve into what I see from my hobbyist haskeller eyes as an awful mess.
I have tried compiling my code and experimenting with bindings but every time I get errors which make no sense, such as a0 and b0 which seem to relate to nothing. Also what do you mean by my code contains type errors?
Obviously nobody cares enough to put serious effort into it, as simple as that.
&gt; what do you mean by my code contains type errors? I mean that if you try to compile it, you will get errors complaining that the compiler cannot work out which types you intend to be used. As you do not understand the errors, show us the error messages, and we can try to explain them to you.
 let foo = kjhbasdf lhjb adlfkb labdsf lkjabsd flkjba sldfjb labsdf lkjbasdf kjsdfkj kdb asflkjb asdfb lkjasd flkjb sdlfb kabs f baz = kjanbs flkjb sdlfb lsbdf lkjsbad flkjsabd lfkjb sadfbj kljabsd fkjbl saldbf lkbasd flkjbsdf See the line break between 2 declarations. 
Ok well, currently im trying to fix the getPiles function which is currently as follows - getPiles :: [Int] -&gt; IO(String) -&gt; [Int] getPiles xs = do putStr "Enter amount in next pile : " ys &lt;- getLine y &lt;- read ys if y /= 0 then return xs ++ [y] --getPiles xs else if length xs == 0 then return [5,4,3] else return xs The idea being i'm giving a list of ints as input, taking string input and output and returning a list of Ints. But the errors are as follows - Couldnt match type 'IO [Int] with 'IO String -&gt; [Int]' Couldnt match type '[]' with 'IO'
Damn! That's too bad. It would have been awesome to have you at the meetup. Maybe we will be able to convince you in the near future. Looks like there is an awesome turnout at the Austin meetup. I'll have to make it out there soon. 
Here's an example type error in your code. When you are writing in do notation you often write the syntax someVariable &lt;- someComputation This syntax fits into a larger context do ... ... ... which overall must have a monadic type. It turns out that for each `(someVariable, someComputation)` in your `do` notation, the `someComputation`s have to all share the same monadic type someComputation1 :: IO a someComputation2 :: IO b someComputation3 :: IO c and then the `someVariable`s each end up with the "inner type" someVariable1 :: a someVariable2 :: b someVariable3 :: c So, since you have ys &lt;- getLine and we can look up the type of `getLine` Prelude&gt; :t getLine getLine :: IO String we can see, for instance, that `ys :: String` and the overall monad type is `IO` (like the example above). Unfortunately, you also write y &lt;- read ys where when we look up the type of `read` Prelude&gt; :t read read :: Read a =&gt; String -&gt; a we see that the type is unknown, only constrained by being an instance of the `Read` typeclass. It looks like you want it to be a number though, so let's specialize and think of `read` as having the type read :: String -&gt; Int This works since `Int` is a member of the `Read` class Prelude&gt; :info Read ... instance Read Int -- Defined in ‘GHC.Read’ ... But, unfortunately, while you're correct to pass the `ys :: String` into read, the result type `Int`, is not "in `IO`" so it makes no sense to use the `(&lt;-)` notation here. [0] If you think of `x &lt;- c` as being an "impure binding" then you would note that `read` is pure. You could make it work by making `read` into an impure computation with `return` ... ys &lt;- getLine y &lt;- return (read ys) ... or you could (and this is recommended) use the syntax for "pure bindings", `let` ... ys &lt;- getLine let y = read ys ... [0] You might wonder why we don't just specialize read to have type `read :: String -&gt; IO Int` but it turns out that `IO Int` is *not* an instance of `Read` which we can check using `:info Read` again.
It's not exactly for beginners, but I've been throwing up various blogs and such for lernin' on http://www.dohaskell.com.
Frege was already mentioned. I also found a java FFI library: [java-bridge](http://hackage.haskell.org/package/java-bridge) I'm not sure though if it is abandoned or is in active use. Would love to hear from anyone using it.
Ah that clears things up quite a bit, how would I go about converting say 'IO (Int, Int)' to (Int, Int)?
Occasionally I do this, if the initial environment is more interesting/more important/bigger than the actions. foo = runState go Env { envFoo = … , envBar = … , … } where go = … 
&gt; Ah that clears things up quite a bit, how would I go about converting say 'IO (Int, Int)' to (Int, Int)? To rephrase what /u/tel said, in a way that may be more clear (or less clear, I duno)... You don't (cannot, ever) convert an ``IO x`` into an ``x``. Instead, you take a the function that requires an ``x``, and pass that function into the context where the IO x is available. So basically you are converting the *consumer* of ``x`` into an ``IO y``, rather than converting the ``IO x`` into an ``x`` to give to that consumer. No matter what you do, you cannot ever get that ``y`` out of the ``IO``, although you can get an ``IO z`` etc.. The ``do`` notation makes it *look* like you're doing it the other way around, but you aren't, and you still have to structure your program the way that it actually is.
Is the lack of an SML-style module system preventng you from writing code and solving coding problems in Haskell? Do spend a lot of time writing code in Haskell? If so, try solving the problem instead of whining what other are or not doing according to some silly personal criteria. 
I love you and all you stand for, thank you!
This is the third part of my series on porting the Land of Lisp "Dice Of Doom" game to Haskell - concentrating on performance and memoisation.
Heh, no problem!
The place I work uses this I think.
That's pretty valuable if you're adjusting your code here and there. You'll spend a lot less time commenting and uncommenting code, and you get a lot more opportunity to try things and ask for information in GHCI. Also, all possible errors are shown as warnings after loading the file. Sometimes there are more/different warnings than there'd be errors because the compiler just wings the bad parts so more can come to light. These warnings might be more relevant to the code you're working on. Maybe [typed holes](https://www.haskell.org/ghc/docs/7.8.3/html/users_guide/typed-holes.html) are a better way to get this behavior, though.
I agree that it's valuable, but I don't see how it would help the OP.
I believe that we could design and build a very elegant &amp; powerful GUI library from the ground up, by starting with precise, simple, and compelling denotation. Precision makes it possible to measure the simplicity and do reliable, checkable reasoning, and simplicity (if precise, not handwavy) makes it tractable, enlightening, and fun to do so. I don't think much work has been done on such a foundation. Yes, designing widgets, layout, etc can be a lot of work, but with a denotationally clean foundation, the work would be incremental, experimental, and distributed. Without such a clean foundation, abstraction leaks and composition failures/surprises/ill-definedness sprout up, with no basis for fixing or even clearly identifying the root problems. As a first step, I strongly suggest clarifying the goal. The term "functional" is terribly fuzzy, and modern use even includes structured and typed tools for constructing imperative computations (as in Haskell's `IO` type), which then suffer the same deep semantic flaws as its imperative predecessors. Because of all the confusion following from this broad/fuzzy use of "functional", I've been joining Peter Landin in deprecating "functional" in favor of his suggested replacement, "*denotative*", as a way to capture more clearly (less ambiguously) the essence of "genuinely functional" (also Landin's term) programming. See [this comment](http://conal.net/blog/posts/is-haskell-a-purely-functional-language#comment-35882) for a brief description and pointers to more information. I did a lot of early work on ~~functional~~ denotative graphics, behavior, interactivity, and GUIs. For the last eight years or so, however, I've been stymied by lack of a cross-platform, OpenGL-compatible, and GHCi-friendly low-level library on which to build. I miss working on this stuff and would be happy to participate in such an effort if there's enough interest in a denotative system and some way to address the problem of a base library on which to build. My personal hunch is to build it for fast GPU execution so that it could be pretty and fast. I have a compiler from very a high-level Haskell-embedded DSL to highly optimized GPU code that I could contribute to the effort. 
I think you just misread my comment to be the inverse of what I actually said.
I've read most of it and am very impressed at how easy it is to read, kudos to the author! Two remarks, though: When introducing monads, kleisli triples and some other constructs, the only mathematical examples given are trivial (using the identity functor, identity function and so on). Is that because straightforward examples requiring only, say, set theory are hard to come by? I would have loved some more words on how monads are monoids. The text states that the monad's endofunctor is the set of monoid elements. What does that mean? How exactly does the composition work?
Wow, using haskell to explore social and political processess... genius!
I don't know, I haven't used annotations for GHC. What kind of thing do you want to do? Perhaps a simple newtype could do the trick?
it gave me this error: http://paste.ubuntu.com/8794146/ any help? i already have gmp installed
Another way to do this (using scala this time but no doubt translatable to haskell): http://notes.langdale.com.au/Transducers.html
my apologies. pardon the snit.
The intended type of `nim` seems to be `IO String` because a message stating the result of the game is returned at the end. It's not the type of inputs but the type of outputs that's reflected in `IO ...`.
Re 1, foreign objects are represented as 'ForeignPtr' wrapped in a newtype (the same approach I already introduced in c2hs). This is supported by a special form of annotation (Class annotations) that automatically perform the newtype wrapping/unwrapping and marshalling of the foreign pointer. (The GitHub repo has an example called marshal-array that demonstrates it.) Re 2, Objective-C and the associated Cocoa and Cocoa Touch libraries don't use stop-and-copy or mark-and-sweep garbage collection. They use what they call automatic reference counting (ARC). It's a normal reference counting collector that cannot deal with cycles anyway (even if you stay in Objective-C). Whenever you introduce a cycle, you need to (manually!) make sure that the back reference is a weak reference (that doesn't keep objects alive). It's annoying, but at least, it is standard on that platform.
Random thought: transducers with /u/tekmo-style folds instead of reducers.
Maybe it's looking for `libgmp.so.3`, but you've got a more recent `libgmp.so.6` or something?
`nim` has no arguments and starts with `do`, so I mentally note that `nim :: Monad m =&gt; m a`. The first statement in the `do` block is `putStrLn "..."` which has type `IO ()`, so I mentally notr that `nim :: IO a`. The last expression is `return f`, but the line binding `f` is in error. It's left-hand side is not an `IO a`, but rather just a `String`. Depending on how that error is fixed, I might see `nim :: IO String`. `play` has 3 arguments and start with `do`, so I mentally note that `play :: Monad m =&gt; a -&gt; b -&gt; c -&gt; m d`. The first statement in the `do` block is `_ &lt;- (n1 ++ "...")`, so I mentally note that `play :: String -&gt; b -&gt; c -&gt; [d]`. Unfortunately, the next two lines don't "live" in the [] monad, they live in the IO monad... there's a type error somewhere. I continue to see how the `ps` argument is used in `displayGame ps`, and how the `n2` argument is used in `play n2 n1 nps` to further note that `play :: Monad m =&gt; String -&gt; String -&gt; [Int] -&gt; m String`. I also think `m ~ IO`, although that doesn't agree with all the lines of the `do` block. `getPiles` has similar disagreements with itself about what monad the `do` block is for. I'm pretty sure the type is meant to be `IO [Int]` but that means that `xs ++ [x]` has the wrong type as does `xs` in the `else` clause. `read getLine` doesn't typecheck to begin with.
I wish someone experienced enough and passionate enough would see this comment and you start working together. And I hope that by that time I myself will learn enough to make contributions :) For now I'm busy switching my mindset to pure FP which I like more and more.
Now that I'm taking a closer look, I see that the geordi project is quite active, as its last commit on the master branch is from a few weeks ago. Its clang branch, however, hasn't been touched since late 2011. I would ask the maintainers if they still support this branch.
hehe, my stance about ghc feature requests is "unless you do it, its not gonna happen unless someone else wants it enough to do it themselves" :) oh wait, thats every open source project ever. :) 
Unrelated to all of the above, and fairly minor in the scheme of things: the Prelude has functions `fst` and `snd` that you could use in place of `getFst` and `getSnd`. Good luck!
1. Indentation in haskell is not style. It is part of the syntax. As such it belongs to the source code. 2. Cabal errors out when it encounters tabs. 3. Ghc treats tabs as 8 spaces which may lead to weird errors. As you see, haskell toolset is not tab friendly. This is even before you start arguing with others about your personal preferences. 
I answered a similar question a little while back on SO: https://stackoverflow.com/questions/24228402/nixos-and-ghc-mod-module-not-found/24228830#24228830
If it does not make sense, or if you think there are missing details let me know so that I can help and improve the answer.
Let me try you *.nix now that I see you reference the above SO question.
I was mostly directing my comment toward the somewhat vocal TT-minded group of people who want to revisit the foundational assumptions of Haskell on a new core calculus, but don't actually want to hack on GHC themselves. 
&gt; Well, that could have been nice, perhaps. I think it would horribly complicate the parsing rules, since Haskell types have spaces in them (e.g. `IO Int` and `Maybe String`) and GHC Haskell just makes it worse with type families. 
Raises a number of important questions such as is Bryan single?
a -&gt; b -&gt; c isn't the type of x, it's the type of f, flip's first argument. flip returns a function of type b -&gt; a -&gt; c. I notice what might be a typo: to actually define flip, you need flip f x y = f y x flip f has to have type b -&gt; a -&gt; c to keep the promise its type declaration makes, and hence for the whole expression to be consistent with flip's type, x has to have the type of flip f's first argument, b, and y has to have the type of flip f's second argument, a. Given the type of flip, the definition is just about the only one possible--if I understand rightly, that's a simple example of the "theorems for free" you may have heard about.
Thanks. I'll work on Vim integration. I'd mistakenly run hindent on whole files and when that didn't work, given up. Now I'll try it on smaller blocks. I do prefer your style.
He's probably referring to [my `foldl` library](http://hackage.haskell.org/package/foldl). You can learn more about how it works by reading the main module's [documentation](http://hackage.haskell.org/package/foldl-1.0.7/docs/Control-Foldl.html).
Neat, thank you!
maybe you didn't see the `f` after `flip` :) 
Some general advice: - In Haskell, the type annotations are almost always just for you, not the compiler. If you omit the type signature and you're still getting a type error, it usually means that your code is wrong, not that the type-checker just can't figure it out. - In the very beginning, you should be compiling your code *all the time* -- probably every line or two of code you change. It's common to see people show up here with a big pile of code they can't make work... and when they try and reduce it to a simpler example, the error becomes obvious. When you make a small change and the compiler instantly complains, there are only a few places you need to check for your mistake.
It wasn't me! This should be fun :)
I've adopted something similar to this for all languages since watching [this talk by Kevlin Henney](http://vimeo.com/97329157#t=21m0s). Link should take you to the time in the video where he starts talking about 'structural honesty'. Examples start at 22:51.
You could have the first name be an indent-point, much like the first name in "let", etc. Then, everything underneath a name is part of its declaration, and you don't need the commas. It would be quite consistent with various other Haskell syntax.
let p equal be the number testing for primality. let a and b be the factors. where b is greater than a and a is greater than or equal to root p. a*b &gt; p ? a = p + x b = p ((p+x))^0.5)*(p^0.5) =&gt; p(p+x)^0.5) =&gt; ((p^2) + px)^0.5 =&gt; (p^2^0.5) + px ^ 0.5 as square roots can always be positive. =&gt; p + (px ^ 0.5) &gt; p 
http://codepad.org/gbYohXuc
&gt; foldr ((liftA2 . liftA2) (:)) (const $ return []) . fmap check &gt; beautiful code 
I... really can't tell who is being sarcastic and who isn't... 
you're right :) I'm sorry :'( it's just... I really don't know why you would post the answer to your question saying it's the answer and then ask the question anyway :/
&gt; In Haskell, the type annotations are almost always just for you, not the compiler. If you omit the type signature and you're still getting a type error, it usually means that your code is wrong, not that the type-checker just can't figure it out. One exception: whenever it says "ambiguous" somewhere in the type error, it means that the type-checker couldn't figure it out and you need to give it more clues. Solid advice! 
Unless they change the rules for bling awards to decouple them from winning...
That's right, I should change the name. Anyway, having a separated forum to discuss the game is a design problem. I don't know how to solve it properly yet. It would be nice to be able to comment each rules individually. This could be accomplished inside the game by posting a rule allowing it, of course...
At the moment there is no winning rule. Initially it was: "if you have more than 5 active rules you win" but we disabled it.
&gt; * Objection 1: All this fuss for ONE LINE? &gt; &gt; * Reply 1: Every computer-science academic has once in their lifetime been in a killing mood due to a one-line-to-long printout causing a paper to spill over the page limit. I like your style!
Related: does anyone know if there's an llvm to jvm compiler? That might be a good temporary workaround for anyone needing the JVM, or possibly an easier project than compiling Haskell straight to JVM. Edit: found this, haven't looked at it in detail though. http://da.vidr.cc/projects/lljvm/
f is the first argument of the function flip. (a -&gt; b -&gt; c) is the type of the first argument of flip. x is the second argument of the function flip. b is the type of the second argument of flip. y is the third argument of the function flip. a is the type of the third argument of flip. The function f has only two arguments. The first argument of f has the type a, and you give it the value y, which is indeed of type a. The second argument of f has the type b, and you give it the value x, which is indeed of type b.
I think you are confusing an instance of a gui created for a particular application, and a component system allowing to create a gui. The latter is way more complex than the former (see: programming system definition in the mythical man month book)
&gt; The first argument of f has the type a, and you give it the value y, which is indeed of type a. &gt; This is the exact source of my confusion - I understand everything prior to this. I just... this is such a profound stumbling block for me. If (a -&gt; b -&gt; c) refers to the type of the first argument of flip, the function f, then surely (a -&gt; b -&gt; c) should map to that f? Function f takes 'a' as a first argument, then 'b' then outputs C. In which case 'a' would refer to x, the first argument of f, rather than y? If y :: a, the type definition of flip says that the first argument of flip is a function with a type (a -&gt; b -&gt; c) that takes y as the first argument, x as the second argument and spits out c. But that isn't correct of f. Can you see why I think that y :: a? I really am grateful with you persevering with me, and I must seem very dense requiring it to be spelled out like this, but I guess with this stuff sometimes things just need to click. 
&gt; `x &lt;+&gt; y = x &lt;&gt; Spacing " " &lt;&gt; y` If I'm reading this right, this means `foo &lt;+&gt; mempty &lt;+&gt; bar` doesn't render the same as `foo &lt;+&gt; bar`. They do with `HughesPJ` and don't with `Leijen`, and I've found the former much nicer to work with because of that.
As you noticed, f takes two arguments, of types and and b, and returns c. With flip, we want to transform it into a function that takes arguments of types b and a, and returns c. Which is to say, we want to flip the arguments. Try this equivalent definition instead, see if it helps: flip :: (a -&gt; b -&gt; c) -&gt; (b -&gt; a -&gt; c) flip f = \x y -&gt; f y x The only thing I did there was add explicit parenthesis to show the associativity of the type signature, and move the x and y arguments into an explicit lambda as well. This is _exactly_ equivalent to the usual definition. I think the issue is that when you read the left-hand side of `flip f x y = f y x`, you're assuming that you should try to do function application to that f, x, y. The `flip f x y =` shouldn't be read as `flip (f x y)=`, it's closer to `(((flip f) x) y)=` 