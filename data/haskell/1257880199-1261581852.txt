I use xmonad, and whenever I change my configuration, I press Alt+q and wait for maybe 2 seconds. Are these 2 seconds the one you're complaining about? In Python, startup time may be a little quicker than that (no need to compile) but I pay far more than those 2 seconds later...
Yes, I don't mean to say that we'll never be writing applications in Agda. Just that it probably isn't ready for that yet. Although, I haven't done any experimenting with extracting to Haskell code, so maybe it does better than I expect.
Good post, one serious flaw: Monads aren't about state. ...and, no, that does not mean that mtl comes with no warts, whatsoever, or is easy to grok as a beginner. Oh, and don't ask me how to fix it, I don't know, either. But it doesn't involve lifts scattered all over the place.
If you do, post instructions! In the meantime, I'll install GHC from macports :P
I can't get this to run. Trying the binary version I get: ./Raincat: error while loading shared libraries: libSDL_mixer-1.2.so.0: wrong ELF class: ELFCLASS64 Which I assume is because I am on 64 bit, so then I thought I would try to build it from source, after installing all dependancies I get this error during compiling Game/GameInput.hs:83:14: Not in scope: data constructor `ExitException' help.
Argh, it starts and plays music but it ignores mouse and kb :-(
They are using GHC 6.8, and ExitException is in Control.OldException in 6.10. I changed that, but then there were some other issues and I figured I wouldn't bother trying to fix it now.
so no raincat for me? :-(
Skip macports, go install the [Haskell Platform](http://hackage.haskell.org/platform/).
come back one year
it's a pity there's no license statement, makes it impossible to reuse code or to ship the game in a linux distro :(
Me neither :(. I was pretty pumped to try it...
Unless you have Tiger, or a PPC.
I'm quite surprised to see this paper not having Max Bolingbroke as a coauthor since this was his idea. Or maybe it's been invented independently by the authors? 
Fair point, though if you are on Leopard/Snow Leopard and x86, the Haskell Platform is far easier as it means you don't have to sit through a 6 hour install process.
I grabbed the Windows binary and used Wine.
It works for me in Windows, but if you maximize it the clicks happen in the wrong places so it doesn't work properly. LASER EYES
linux/64 bit binary? Gentoo's emulation libs don't come with libgmp, and I don't feel like tracking down thousands of Double/GLdouble mismatches to make it compile with a recent HOpenGL, right now. Putting it on hackage would be nice, too.
I think a big problem regarding teaching monads is that the student's first encounter is `IO`, which has a lot of magical behavior. The first time monads "clicked" for me was the `Maybe` example in RWH, and I've had success using that example to teach others as well.
...with the result that many Haskell books, don't introduce IO until chapter umpteen, which is a problem in itself. You definitely can't start out with Monads, and you can't leave out IO in the first chapter, either. I think just treating the IO Monad as magical in the beginning, not even mentioning the word and using do-notation is the way to go. You can then build up examples of monads like RWH does (the farther away from IO the better), abstract them to bind and return, invent do-notation, and mention en passant that all that IO stuff is a monad, as well.
Me too (built it myself for 64 bit Linux).
* If you're using xmonad, try resizing the window to as close as possible to 1024x768; these numbers are hard-coded into the program but xmonad wants to tile the window * Although there is no visual feedback when you click on an item to select it, it is actually selected.
Mark's article comes from the "I think I'm ready to write a monad tutorial" phase of learning, in which you think you know what monads are and what they're for. Many people seem to fall off the learning wagon at about this phase.
This game seems incredibly buggy and unfinished... even though it's written in Haskell, it was a big disappointment
The second equation should be [-1,0] = { 0.(-1)a1a2a3... | ak in {-1,0,1} }
Haha, last level actually took a while.
I agree with several comments that doc in Haskell libs are lacking. Here is a fresh Reddit link saying some words about it http://jacobian.org/writing/good-documentation/ coming from Django people which really have good docs.
Not working on Windows XP here. Error message: Raincat.exe --------------------------- user error (Mix_LoadMUS SDL message: Couldn't read from './data/music/project_raincat.mp3') 
A quick and dirty patch to make this compile with GHC 6.10 and HOpenGL (since whatever version the newtyped GLdouble came in): http://hpaste.org/fastcgi/hpaste.fcgi/view?id=12047#a12047
Oops, thanks. Fixed.
&gt; these numbers are hard-coded into the program What a brilliant idea.
Cute game idea, but the interface needs a lot of work. It violates too many native GUI assumptions. i.e. clicking to drag an item from the sidebar just drops it at the edge of the playfield, you can't click on something to select/move it. You have to go down and use an explicit erase tool rather than click delete, etc.
Nice work! I wonder if a more productive path might be to consider the way that non-determinism can be reflected via continuation passing style in the interpreter, perhaps by reasoning about a free theorem after double-negation or transformation into a prolog-style two-continuation machine form. Hand waving: One could theoretically transform the mostly classical logical formulation into a double-negated constructive logic form, and then apply a more traditional free theorem to that.
Totally agree. That was one of the arguments for dynamic languages that has been thrown around a lot. But it does not hold water. It's a lot easier, faster, more complete and less error prone to test a type with the type system than with a unit test. I mean that's what a type system is there for. Ensure that the types are what we intended. There seems to be this mindset currently that the more test cases the better, yet when there is a way to reduce the number of test cases while maintaining the same test coverage, the community is generally mute on this. On top of that, there's this trend where to make code more testable we tend to add complexity instead of trying to reduce it.
This made me laugh: &gt; During development, creating the puzzles proved to be a bit of a challenge on top of the fact that all the code was to be written in Haskell. It never rains but it pours, eh?
Thanks! Some food for thought. We did think about reducing the problem to a more well-established setting. But the direction we briefly looked into so far was the ICFP'09 paper about embedding functional logic programming into Haskell using the explicit monading sharing stuff.
Was pulling out and re-injecting your state the final solution? It makes me sad if that's the only way you were able to solve your problem.
Surely just strictifying the monad would be a better choice (i.e. use a strict `&gt;&gt;=`)
Yes, I could actually have changed the monad. But I was using MTL, and I had to try "something" to get rid of this memory issue. I was not 100% certain that the problem lied ONLY at the state monad at the time, so I thought I'd try ripping out the state, and re-injecting it in the loop. The code for this is not as bad as you'd think... HONEST! :-) But Don is correct, a strict bind would make life much easier, and I can come back to that and do it once I finish the other parts of this system, which is now a bit behind schedule. (not all my fault I swear!) 
If the problem was mainly in your map, you could try using insertWith' when you add/replace items in the map.
This will likely not be the final approach I ship with :-). Control.Monad.State.Strict, may be the solution, as it will have a bind that is more strict. I actually drew a lot of this up on the whiteboard for my boss a few seconds ago. He thought it was pretty cool even if it sucked to have a bug. :-)
Seconded. ``insertWith'`` has saved me from space troubles before.
Why do even press people from a university misrepresent Haskell as being "novel"? It's 20 years old, for Christ's sake. If that's novel, which adjective should we use for Java or Scala, then?
&gt; which adjective should we use for Java or Scala, then? boring and derivative, resp.? :-)
Hmm, yeah, maybe. Though, these don't denote time scales.
Maybe it's novel because nobody seems to have learned from it yet.
I'm getting a 503: not available. Anyone else seeing this?
Generics? 
http://www.merriam-webster.com/dictionary/novel You'll notice the second definition has nothing to do with newness.
Been loving these every Thursday (thanks reddit.haskell !)
&gt; Why do even press people from a university misrepresent Haskell as being "novel"? Literate haskell? (ducks) 
Well, I think the program sets a window manager hint to make the initial window size 1024x768, which xmonad is ignoring; but yes, it would be better for the program to adapt if the window manager resizes its window.
Did you download the source and build it yourself? If so you need the binary download too for the data directory.
No I am referring to the binary. In fact, the so-called missing file is also there.
Okay. And who makes sure that everyone reads it like that? You? :)
Congratulations to both of them. The lectures and material are excellent. But this quote caught my eye: &gt;More than 90 per cent of research at The University of Nottingham is of international quality, according to RAE 2008, with almost 60 per cent of all research defined as ‘world-leading’ or ‘internationally excellent I'd hate to be working on the remaining 10%. What is that anyways? Is it all submissions to the ig nobel awards? The way it is worded sounds like well 90% of the work is great, the rest not so much.
Those are categories used by the Research Assessment Exercise where research is ranked for it's impact, e.g. a paper in a leading journal down to a talk at an obscure workshop.
This lectures are awesome. I'm at the charpter 3 and learing all the amazing stuff Haskell has to offer. Upvoted for usefull resources!
&gt; really interesting *side-effects* of higher-order functions A poor turn of phrase, I feel. Hehe.
The reader bears some responsibility for attempting to understand the intent of the author.
It seems to no longer put a copy of the above function name in the matching indent level. That was a useful feature: myFunction [] = 0 tab at next line used to show: myFunction
&gt;Haskell "gets around" the problem of state by getting you to switch from writing a sequence of state-manipulating commands to composing functions, each of which defines a transformation of the state. That's it. No magic. Nothing clever. And monads don't come into it. &gt;But, if you wish, you can use monads to give a really nice notation for structuring those compositions. All Haskell tutorials and introductions should start with that paragraph. A clear non formal explanation of what some thing is and is not is always better than any analogy.
You know! You know! Kidding, I like these videos, thanks!
I tried Control.Monad.State.Strict and it didn't fix the problem as well as pulling all the state out of the Monad, and re-injecting it into another Monad run via runReaderT,execStateT.
By this I mean that it did not fix the problem altogether. I think a combination of strict Map insertions (which IntMap doesn't have apparently but Data.Map does) and a strict state monad might solve the problem. For now I have to force the IntMap updates to evaluate at each poll, then before each new poll begins, I have to force the state out of the Monad and put it back in to get all PAPs evaluated. Perhaps I don't want a lazy language for this stuff after all. :-(
I'm just a noob, but is there any reason why he's writing `(λi → i + 1)` instead of just `(+ 1)`? Which he does in at least one place. And in the currying example, isn't the first line alone enough? Then for example `f 3 5` would be a function of type `(Num t) ⇒ t → t` which he could call with `7` as argument and get the same result. He probably explained his reasoning in the talk, but it's not there for me to hear. Perhaps it was for illustrative purposes only. :)
Yay! And it looks pretty sensible too. I'd been thinking of a prototype design along similar lines.
I assume it was just because he wanted to introduce lambdas before the (+ 1) syntax.
For too long, Haskell has lived in the shadow of Perl, because it isn't able to `don't`. No more, thanks to [Gracjan Polak](http://article.gmane.org/gmane.comp.lang.haskell.cafe/65958)!
What is this?! don't _action = return () *I demand pointless style!* I paid good money for Prelude.const!
Many people complain about there being too many packages on hackage that do the same thing - in this case, nothing. So perhaps this package should be merged with [hnop](http://hackage.haskell.org/package/hnop).
I agree! this module is not pointless enough
This looks cool. It felt really dirty trying to write/design my [directory tree module](http://coder.bsimmons.name/blog/2009/05/directory-tree-module-released/) because working with filepaths as strings was so sketchy.
We definitely need an abstract type for paths. There are a lot of nice points in this design. It's disappointing that Unicode is still mishandled, though. The underlying representation of paths should be ByteStrings on Unixy platforms, and Strings on Windows and Mac OS X. That's supposed to be one of the most important benefits of a new representation of paths. I'm also a little worried that it's too easy to circumvent the type safety. Since no checking is done, perhaps the "as..." functions ought to be renamed "unsafeAs...".
I believe that you are missing an important distinction: [hnop](http://hackage.haskell.org/package/hnop) does nothing, whereas [don't](http://hackage.haskell.org/package/acme-dont) doesn't do something.
&gt; The underlying representation of paths should ByteStrings on Unixy platforms, and Strings on Windows and Mac OS X. I don't follow. All of those platforms support Unicode filenames, and on Windows it's essential to use the Unicode API because the older API is horribly broken (doesn't support path lengths larger than 1 byte!).
Practically all his articles are like that. He's the Ph.D. version of Jeff Atwood. I had to stop reading any links to his articles on reddit to keep my blood pressure under control.
They support Unicode filenames, but that doesn't mean all filenames are Unicode. A file path abstraction isn't as helpful when it can't represent all paths found on your disk. People are still doing this; see e.g. http://code.google.com/p/chromium/issues/detail?id=25126
Does... does this mean that Monad Transformers can be made simpler?
What annoys me is that when I want to write a function that previously would have looked like this: find :: FilePath -&gt; [FilePath] , recursively enumerating all files and directories, I now need an existential to say "Yes, the information on whether or not it is an Directory exists". That is, providing a value-level equivalent of the same logic would be very nice to have.
This guy (not exactly a master, however.)
Direct download: http://ecn.channel9.msdn.com/o9/ch9/9/0/2/4/0/5/C9LecturesMeijerFPC7_ch9.wmv
Fixed, my fault.
&gt; For example, a ring is a monoid under multiplication and an abelian group under multiplication [...] Ahem :). That aside, I'm quite interested in Eugenia Cheng's paper, but my category theory is at a pretty remedial level; is it safe to assume that the "monad of &lt;insert favorite structure here&gt;" terminology refers to the free monad generated by the functor taking Set to the category of the aforementioned structures, along with its forgetful adjoint? In other words, is the "monad for abelian groups" mentioned in the first paragraph just the `T = G o F` construction with `F` taking sets `s` to (commutative) formal sums over `s \\cup s^-1 \\cup {0}` and `G` sending groups back to their underlying sets? If so, what kind of free structure would I need to have in order for the adjunction to *not* form a monad? &gt; If we think of the action of a monad as building in algebraic structure freely [...] I feel that I'm missing something cool here: I was under the impression that *not* every monad was isomorphic to a freely generated one. What's the free structure for `State`? *[Edit: leaving this up just in case, but I'm now pretty sure that Cheng is just talking about the specific monads discussed in the paper, whose action is of course exactly that]* Hmm, I think I just talked myself through a part of this: if free object constructions yield a monad, then we should be able to come up with a monad from the free monad construction itself -- but then we're looking at Funct rather than Set, so that's where the 2-category bit comes in, because arrows are now 2-cells? And if so, what the hell is a globular set? :) 
Conversions between Unicode and various encodings are a very complex issue. They are not one-to-one, and often not even well defined (e.g., when do you apply or not apply canonicalization algorithms for combining characters). So to minimize problems, Haskell should represent file paths internally in the same way that they are canonically represented internally on whatever platform we are running. In POSIX-like OSes, file paths are byte strings. There are various incompatible ways that some libraries and applications represent Unicode in those byte strings. So Haskell should represent paths as ByteStrings, and provide whatever tools are necessary to convert to and from other representations. In Windows and Mac OS X, file paths are strings of 2-byte characters, essentially Unicode. That is how they are stored in the popular filesystems, and that is how the core system APIs view them. The POSIX-like API layers - well supported on Mac OS X, less so on Windows - need to convert back and forth using some encoding. So here we need to represent the paths as Strings. Until recently, GHC has represented a file path as a string of bytes, stored in a String with every high-order byte set to zero. This hack has caused a myriad of problems. On Windows, GHC is now doing better than that, but on Unix it's not clear how best to proceed within the constraints of Haskell 98. There has been an ongoing discussion about it on the GHC users mailing list and in several Trac issues. The author of this package obviously had other improvements in mind, not the Unicode problem. But once we are moving away from Haskell 98, this package could provide a way forward also for the Unicode problem.
I'm also a category-theory noob, but I think I can answer some of your questions. &gt; is it safe to assume that the "monad of &lt;insert favorite structure here&gt;" terminology refers to the free monad generated by the functor taking Set to the category of the aforementioned structures, along with its forgetful adjoint? The "monad for &lt;structure&gt;" is a monad whose algebra is &lt;structure&gt;. The question of whether a given category is the algebra of some monad over some other category is called *monadicity*. I don't know anything more about it, unfortunately. &gt; If so, what kind of free structure would I need to have in order for the adjunction to not form a monad? The composition of a pair of adjoint functors is *always* a monad.
Probably not. In Haskell, our monads are on the category Hask. This article is talking about monads on the category of monads on ...
&gt; The "monad for &lt;structure&gt;" is a monad whose algebra is &lt;structure&gt;. So to reconcile that with the abelian group example: given a set `X` and the free abelian group `A(X)` that has it as the basis, we have the obvious `X -&gt; A(X)`, and a map `A(A(X)) -&gt; A(X)` that sends linear combinations of sums to sums. So now `A` is a monad in Set, and it's also the variety of algebras. Had we picked `x(yz) = (xy)z, 1x=x1=x` for the variety, say, we would've ended up with the Haskell list monad. Sounds like my generalization is broken in the sense that this has nothing to do with free monads generated by a functor, right? &gt; The composition of a pair of adjoint functors is *always* a monad. Doh. I really should've remembered that. 
&gt; Sounds like my generalization is broken in the sense that this has nothing to do with free monads generated by a functor, right? I think that's right. I actually know nothing formal about free monads (I've heard the phrase in Haskell but never in a category theory class).
&gt; Though the Haskell community make efforts to conceal this fact from you, monads are not tied to any particular category (whether it's Hask or Set). We can consider monads on *any* category C: more generally than that, we can consider monads in any 2-category! Emphasis added.
I remember when I first met forkIO and TChan and Chan. The first thing I did was write a concurrent prime sieve. I seem to do that when I get to a new concurrency system. I've got one for libdispatch (actually I think I have 3 different ones), which is part of GCD for Mac OS X, and apparently ported to FreeBSD 8 STABLE now too. It seems a good benchmark to be able to express a very beautiful algorithm in whatever the new language or library is. 
I know that you *can* make monads on any category, but the point is that we *want* our monad transformers to make Hask monads: `ReaderT IO a` should be an object *in Hask*, just like `IO a`. We don't want things in Mnd(Hask).
Why can't I install both quickcheck 1 and quickcheck 2?
The newer QuickCheck seems to have gotten rid of some very useful utility functions (associativity checking, and others) and I have no idea where they went :-(
What's the status of DDC? Anybody willing to share his experiences using DDC?
Submit a ghc bug report, and we'll take a look.
How is darcs doing these days? I remember it getting some criticism in the past, but didn't a bunch of work go into it?
Yeah, me, I'm the author of the blog post :-). DDC is still an experiemental compiler. There are still some quite egregious bugs. For instance I just found out that it fails the [Man or Boy](http://rosettacode.org/wiki/Man_or_boy_test) test because that test was on r/programming. Fiddling with that test turned up a couple of other bugs. My advice at the moment is download it, compile it, play with it and log bugs, but don't expect more than that :-). 
You can. What you cannot do is have one package that depends on others that use different versions of QC. I was trying to build as much of hackage in one go, with consistent deps. This is also what any distro would try to do. They don't want to ship multiple versions of lots of packages.
Now I can not only do things lazily. I can avoid doing them lazily too!
&gt; What you cannot do is have one package that depends on others that use different versions of QC. Is this always a problem, or only a problem if that package wants to try and share data between the two versions of QC (which seems unlikely for QC, but maybe more likely for the other packages you mention)?
I must still not be getting your point, because the post author later goes on to say this: &gt; The first cool result (due to Street, back in 1972), is that an object of Mnd(Mnd(C)) (i.e., a monad in the 2-category of monads on C) is a pair of monads T1, T2 *in C* related by a distributive law - a distributive law being precisely the data needed to make the composite T1.T2 into a monad! and then this: &gt; The second cool result (due to Cheng, and very recent), is that a monad in Mnd(Mnd(C)) (i.e. an object in Mnd(Mnd(Mnd(C)))) is three monads T1, T2, T3 *in C*, together with the extra data needed to make the composite T1.T2.T3 into a monad! emphasis added Given that C is an arbitrary category, it would seem to me that the result would have some relevance to Monad Transformers in Hask (a category that could be used arbitrarily).
I wrote this last week: http://hpaste.org/fastcgi/hpaste.fcgi/view?id=12190 is that code just regular sort that sorts things based on types and ends with sorting groups of values with the same type?
Can you explain how "field projections" complement type classes? as far as I could grasp, it seems like it: * allows for different data constructors to share the same field names * adds the (.) operator that java/c++ programmers are used to. is (.) still also used for function composition? Both of which don't seem necessary imho 
&gt; That's why the ability to have query comprehensions over your own types is kind of hacked on (they just "magically" work if the right magic method names with the right signatures are there) instead of using the interface mechanism etc. Hmm, I'm not sure whether it's a hack. The query syntax desugars to a sequence of method calls and this desugared version can be typechecked just fine.
In principle it's only a problem when the types would be required to unify. In practice it's always a problem because Cabal does not have enough information to know if they would unify or not, so it takes a conservative assumption. Things would be a lot better if we had an way to specify private build dependencies (though it'd have to be checked and enforced).
since it uses value-level '&lt;' it's going to require a monomorphic list, yes.
So what would be nice is if we had "variants", like in bsd-style port systems - named subsets of additional things to install and their additional dependencies, that you can pick and choose from when you install a package. For example, you wouldn't need the additional dependencies if you don't install the qc2 variant. Obviously, there would be many other nice uses for a variants system.
Sure, but the price you pay is that it isn't possible to write code that is parameterized over the various LINQ style query comprehensions in such a way that it works with all of them. You ultimately have to give up type safety to get any such genericity.
Ah, okay, I missed that. I'll have to read more. Sounds cool.
Ah right, without higher kinded polymorphism it is indeed impossible to write e.g. Haskell's `sequence :: Monad m =&gt; [m a] -&gt; m [a]` in C#.
Short answer: no. Long answer: This provides yet another way to build up some interesting monads. Monad transformers aren't the only way to build a monad, they are monads that exist because of some kind of canonical distributive law that works in any Kleisli category. You can get monads from other compositions than just composing one of these very generic laws. For instance, given monads M and N if you know a distributive law from MN -&gt; NM, even if one of those is a just a pointed functor, you can get a monad. You can also get monads if you know how to collapse MNM -&gt; NM or MNM -&gt; MN similarly. Mark P. Jones had some code for doing all of this in Haskell back in 93, IIRC. You can make monads out of adjunctions, and then make a comonad by composing the left and right side of the adjunction the other way. The right Kan extension of an endofunctor along itself is a monad, although this can be viewed as a form of the same adjunction construction above. But, these are not the only way to concoct a monad and monad transformers are not the only useful group of easily composed monads. For instance, "ideal monads" have a generalized monad coproduct that defines how they should layer. You can construct all of the monads in the monad transformer library (with some possible strictness woes) as right kan extensions, and compose them using right kan extension transformers. What the construction mentioned in the post provides is that given a "monad over a monad over a monad", Cheng's fairly nice result gives you definitions for 3 other monads based solely on the soup of identities you now have available to you.
This very issue is why I find myself unable to use F# for much of note. I went to port my category-extras library from Haskell, and got stuck somewhere around the 5th or 6th line after the imports. ;)
Cabal has support for "flags": the package description can basically say, if -ffoo is set, then my dependencies are blah, I provide module baz and quux; otherwise, I depend on bloop and provide only module baz.
This seems like a great (and easy) way to contribute. I'm sure some of these can be closed as dupes or even already fixed. And if you want to start writing code for GHC, I'm sure you can find some relatively easy tickets.
But that's sort of what I'm getting at. If this construction *gives* the definitions back, then wouldn't there be no need to actually define specific Monad Transformers, and rather, some general Monad stacking mechanism can be constructed that could take any two Monads and yield the stacked Monad that lets you use the properties of both (this is about where I get completely lost even trying to think about what's going on).
The problem is this construction requires a pretty strong pre-condition. There is no magic construction that will take any two monads and build a monad out of the result. They are not compositional and their coproduct provably cannot be defined in general. Consider, logically, the monadic coproduct of STM and IO. There is no way that it could do the right thing and meet all the laws. Neil Ghani wrote a nice paper on this topic. The construction from this post talks about the fact that you can of course build monads in more or less any category. That category could well be the Kleisli category of Hask for some monad, or even a Kleisli category of that, but you still have to find such a monad. Given the latter, all it gives you is three more monads, but there is no real mention of any of the properties those source monads had. Furthermore construction provides you with no new tools for finding those monads, it merely says that given them you happen to get 3 others. This provides you with no ability to share properties between monads. It merely provides a few extra constructions that happen to share the monad laws, given a rather exotic precondition. Most of the interesting monads over Mnd(C) are going to happen to _be_ monad transformers, but nothing is really gained pragmatically from this viewpoint.
Nice. If that's powerful enough to decouple QC dependencies from a typical package, then how can we now make it effortless and thoughtless enough that most people will actually do it? We'll also need some marketing, of course.
Sigh. Here we go again.
Nice idea. I added a link in the **Notices** section on the [front page of the GHC Trac wiki](http://hackage.haskell.org/trac/ghc/wiki/).
&gt; For example, a ring is a monoid under multiplication and an abelian group under multiplication [...] Well, the statement type-checks at least. =)
Shucks. Thanks for explaining though!
I don't think i can explain field projections any better than the [DDC wiki page on that subject](http://www.haskell.org/haskellwiki/DDC/FieldProjections). Struct access in haskell is a bit of a pain. Ie: data X = X { aX :: Int ; bX :: String } main = do x = X 12 "Hello" putStrLn $ aX x Disciple allows that last line to be replaced with println x.aX That doesn't mean much in this example, but with many larger and more complex structs it does make a difference. 
You can fake higher kinded polymorphism using [phantom types and safe casts](http://higherlogics.blogspot.com/2009/10/abstracting-over-type-constructors.html).
For structs containing structs etc the difference will be: Disciple: x.aX.bX.cX.dX Haskell: (dX.cX.bX.aX) x Not a huge difference imho.. 
what do you mean? perhaps adding a link would had explained it?
I don';t understand his whining. Haskell is there for you to shape it as you like. You don't like (readArray a i), well set it up so a[i] works then. Don't like read from references, program your l-values and r-values and automatic dereferencing of l-values when needed. Haskell is very flexible, so no complaints about silly bits of syntax. 
There's a (I think) non-overlapping view pattern suggestion [here](http://cdsmith.wordpress.com/2009/10/04/view-patterns-as-pattern-matching-for-records/). Happy to know that the spurious overlapping pattern warning will be gone! 
Please forgive the stupid question, but is there not some sorting algorithm that is both efficient and functional? If not, I'll go quietly back to my seat, but if so, why try to use this algorithm here? I ask because I've had to learn about a couple of different ways to do things while learning Haskell (trees over hashes, diff lists, etc.) I accept that these are different, but they seem to be fair enough "compromises." I think one of the main strengths of Haskell is that it's closer to a more declarative style: let's express what we want to do. I guess if "what you want to do" is insertion sort, then I'd recommend another language, but I'm hoping someone can tell me why that would be the goal.
Haskell should have pattern synonyms, then ugly view patterns could be beautified. 
Awesome. Mind blown.
you can make a[i] work? Explain how.
Field projections are *not* the main difference between haskell and disciple. Of the differences I listed in the blog post it is the least important (even if it was second in the list). 
I don't think his desired scoping solution could really be implemented. The view patterns already have to pick up names as they move left to right through the pattern list and all of those names, including the ones on the right are in scope in the where, so something has to cut the knot. Consider: foo (f -&gt; x) (g x -&gt; y) = ... where f = ... g = ... 
make `a` a function taking a list of indices and looking up the value in the array?
That's right. See http://augustss.blogspot.com/2007/08/what-about-arrays-after-doing-my-little.html
One completely tangential true statement in this article is that array indexing from 1 is a bloody disaster. I'd say that reading Cormen has been at least 20% more slow for me than if they had just gone with the fucking conventions of almost every language still used today and indexed from 0. The uncomfortable and frequent use of "i - 1" or "j - 1" in their pseudocode is one manifestation of this error in judgement.
This has COOL written all over it in large block letters.
the solution to this problem is rolling your own functional toolkit. by that i mean a list structure with all the tasty things from Data.List. there is no reason why you can't use all sorts of maps etc in whatever language
Link to the pdf? slidesdhare needs registration.
Woohoo! Someone's finally going to handle me! \o/
Well, why not Lazy K then?
Since 2006 (when this was written), C# has actually grown much better support for anonymous functions, list/monad comprehensions (in the form of LINQ) and other Haskell-influenced language features. I'm not sure what the moral is, but I bet knowing Haskell will make you a better C# programmer in 2010.
A classic, and still relevant, but from August 2006, in case anyone is wondering.
Could you please explain further? It seems to me that the semantics of your example are quite clear with Neil's suggestion. I think he means that should be equivalent to: foo a b = ... where x = f a y = g x b f = ... g = ... except that the first two pattern bindings in the where group (trivial patterns in your example) are special - if their patterns fail to match, it is not an error, it only means that this entire function binding fails to match. Note that with this interpretation, there is no "moving left to right" - the bindings are recursive with each other as in any other binding group. So the compiler is free to evaluate the function parameters in any order it chooses, as usual. Do you see a problem with that?
I think a good part of being a programmer is to find the clean program in your language. Not to carry over idioms from more flexible languages that don't cleanly work. When the code forces you to leave traps, comment them. If you can't do the right thing, do the best wrong thing and explain why.
Couldn't he use, or if non-existent, write, a Haskell-to-C# compiler? That way he could program in Haskell, compile to C# and use the result.
Well, I agree with Neil that I don't like the proposed implicit returns for view patterns in the Maybe monad. At all. The reason is the same reason that I don't like pattern guards, either. It is not fair to compare pattern guard expressions to complicated nested cases and multiple function definitions, because they do not replace that. True, the original paper introducing pattern guards compared them to those ugly things. But that was long ago, before monads were well understood. Pattern guards actually replace msums of do expressions with pattern bindings in the Maybe monad. Or in the Exit monad, which comes out looking even simpler. Pattern guards make the expression slightly simpler by omitting the do and the return (and the msum, in the case of the Maybe monad). Pattern guards do still simplify things, a little. Hardly at all after you define a few convenient combinators. And that slight simplification, in my opinion, is not worth obscuring what is really going on. The proof is that even today, people are mistakenly trying to justify pattern guards by comparing them to those horrible tangled case things, forgetting about monads because of the missing return. So I am very happy about view patterns. They finally provide an alternative to pattern guards, without the ugliness of implicit return. Please don't ruin them by adding it back in.
I definitely find that being used to programming functionally in languages like Lisp makes me *less* likely to try to write that way in C++. It's just such a massive verbose pain to use all those mem_funs and bind_2nds or whatever, and I end up just being enraged that it's not as easy as it should be. When I'm in C++ I write C++.
Yeah, for lists, there are nicely efficient mergesorts, and even one in the libraries called Data.List.sort
But in an old-fashioned function definition the compiler isn't free to match the argument patterns in any order. The Report specifies left-to-right matching. So if I define f False False = False f _ _ = True then `f True undefined` is `True` (and `f undefined True` is `undefined`). Presumably view patterns should not change this. If, in Edward's example, `x` and `y` were not trivial patterns but could fail to match, the semantics of `foo` would be confusing, at best.
Lua makes us sad as well :(
Ewww.... (upvote)
My point is that the bindings should be unordered and recursive, as one usually expects. The "special" pattern matches could of course be applied in the proper order, with the expected effects on evaluation order. So, for example, if a data dependency forces evaluation of the second pattern while working on the first, and that results in a bottom, so be it. Also, this does not force full evaluation of the parameter expressions in the given order, only just enough to match the patterns. As usual. I think all of that is what one would expect intuitively in Haskell.
I think it works for that example, but there are ugly corner cases still. foo (f -&gt; Just x) | ... = | ... = where f = ... foo (f -&gt; Nothing) | ... = | ... = where f = ... You wind up having to duplicate the body of f and probably can't spot that they are the same, you now have part of the pattern, but not the guards with access to the where clause and i shudder when i consider the man-or-boy test of: f (f -&gt; f) | f &lt;- f = ... | otherwise = ... where f = ... That said, the real worry I think comes down to that 'but the first two are special' cases in the where clause. you have to run the cases on those two in some order, then the whole knot you tied with the where clause gets thrown away. I will amend my position to say that I suppose none of these seem individually insurmountable. There is just a large number of sharp corner cases to cut yourself on.
&gt; Further, my experience with Haskell means that I now see potential bugs everywhere in imperative code. Wait, isn't this a good thing? Ya it might be a demoralizing, but it will make your code better.
However, they are being considered as an GHC extention: [http://hackage.haskell.org/trac/haskell-prime/wiki/TypeDirectedNameResolution](http://hackage.haskell.org/trac/haskell-prime/wiki/TypeDirectedNameResolution) 
templates are pretty much as powerful as haskell though, you even think similarly, even though they're god-awful to use and haskell is nice.
&gt; the real worry I think comes down to that 'but the first two are special' cases in the where clause. you have to run the cases on those two in some order, then the whole knot you tied with the where clause gets thrown away. That's what I get for hand-waving. Let's truly de-sugar the example from your previous post. We'll write it in the Maybe monad: f a b | isJust v = fromJust v where v = do x &lt;- return $ f a y &lt;- return $ g x b return $ ... f = ... g = ... To make things mutually recursive as I wanted, we would need to use *mdo* instead of *do*, or the equivalent de-sugaring. That is beginning to lead us off the well-beaten path of Haskell intuition, so it probably is ill-advised. We are back to "left to right" evaluation, as you said, but also with *where*-scoping as Neil proposes. I don't think your other examples are necessarily sharp edges of *where*-scoping. They are the same kinds of issues that already arise with multiple function definitions, patterns, pattern guards, and where clauses. At worst, you can probably still do something like whatever you would have done without the *where*-scoping.
Haskell is quite flexible. My solution is here: http://jtra.cz/download/haskell/imperative.hs.html (remove ".html" for source) Though I run into some issues like that it must be specialized either to IORef/IOUArray or STRef/STUArray as there is no common type class for Ref (unlike for arrays).
It's better, I'll give you that. On an absolute scale still pretty terrible, but definitely better.
&gt; Pattern guards actually replace msums of do expressions with pattern bindings in the Maybe monad Pattern guards have nothing to do with Maybe. And what do you mean by "implicit return"? Do you mean that f | pat &lt;- exp = stuff | otherwise = something can be translated to something like: f = runMaybe $ do do Just (pat) &lt;- return (exp) stuff `mplus` (return something) I think this code is much more ugly. This code relies on the desugaring of pattern matching in a do statement, which is one of the less-than-elegant parts of the language IMO. 
Cool, looks like you had fun and got something done. How long have you been Haskellizing, and what would you say productivity was like compared to other languages you're familiar with? (Also, if you want my two cents... err, pence? I think the postfix thing is totally appropriate for a DSL; I wouldn't concern myself with pure-looking-code in that context at all.)
There's a fairly high probability that there would be problems with including that code in an environment where other people need to be able to read and understand it, no?
The problems with matching in monads don't really apply to `Maybe`, though. After all, there's nothing wrong with matching in list comprehensions, and it can even produce elegant definitions: catMaybes l = [ x | Just x &lt;- l ] The problem in Haskell98 is that such definitions are valid for all monads, when they should instead have a more restricted type, such as `MonadZero m =&gt; m a`. However, `Maybe` is such a monad, so making use of it there shouldn't be a problem.
we're doomed!
Why? If the result of said compilation is clean, well-formatted C# source code, preferrably with the original comments in tact, what's the problem? Can't C# programmers read C# code?
Show me such a compiler! 
I was suggesting he write one. At the moment I am looking into [haXe](http://haxe.org). It is a language and a compiler, and it compiles to JavaScript, PHP and C++ code (as well as to Flash and nekoVM bytecode), and other targets can be added. I must admit I'm just starting with it (still reading the book) and I haven't looked at the actual code that is produced. All I am saying is that it is possible to compile from language &lt;A&gt; to language &lt;B&gt;. Is there any reason why the resulting code could not be humanly readable?
Compilers don't generally generate readable code.
&gt; *generally* I agree, but does that mean that they *can't*? Never, ever? If they can't, ever: why so? Of course I am talking &lt;high-level-language&gt; to &lt;maybe-somewhat-lower-but-still-high-level-language&gt; compilation. I am not talking about compiling to bytecode or assembly or binary. As said somewhere else, I am diving into [haXe](http://haxe.org) as we speak (well... when I get home again; at work now), and I'm eager to see how readable (or unreadable) the code it produces actually is.
waiting anxiously for part 2
&gt;Is there any reason why the resulting code could not be humanly readable? No, but it's probably a very hard problem. I've never taken a shot at it, but I've used a couple of language-to-language compilers, and have yet to see one that produces readable code (let alone idiomatic code).
We Haskell newbs are anyway.
&gt; No, but it's probably a very hard problem. I've never taken a shot at it either, so you're probably very right (given your experience with such tools), but question is: *why* would it be hard? I guess most (I hope all) compilers build some kind of AST. Why would the reverse be hard? It's not as if we're dealing with a cryptographic hash function here ;) There are code beautifiers and formatters, etc (which, admittedly, I have never used, since I already always make sure my code is pretty to begin with; I am my own code formatter, so to speak). However, the cognitive processes that make a human being beautify their code as they go, can't be *that* hard to emulate, can they? The rules tend to be pretty simple, I'd imagine (indent upon opening new block; put spaces, or not, under certain conditions; those kinds of things). &gt; I've never taken a shot at it, but I've used a couple of language-to-language compilers, and have yet to see one that produces readable code (let alone idiomatic code). Hmmm. Strange. I'll soon find out just how readable the code is that haXe actually produces, and to what extend it is "beautified", etc. Interesting topic, BTW!
Haskell idioms aren't 1-to-1 with C# idioms, so Haskell code translated to C# will be highly unidiomatic which hinders readability...
Admittedly, I know little-to-nothing about functional programming (though I want that to change). (yes, I know, I am in /r/haskell :) So, if we take the Haskell code for, say, the Fibonacci function, and the C# code for the Fibonacci function, and have their compilers build their respective AST's from them: will these AST's be entirely different, or will they resemble each other?
&gt;The rules tend to be pretty simple, I'd imagine (indent upon opening new block; put spaces, or not, under certain conditions; those kinds of things). It's not those low-level things that are hard to emulate, it's higher-level stuff. How things fit together. You'll probably have to put in extra variables or object that don't exist in the parent code. You need to name them. You might have means of abstraction or combination available in either language that aren't in the other, and you have to be able to translate between them in a manner that creates quality code on each end. If you have really deep knowledge of two languages, it would be easier - but not easy. Again, spitting out C# code that does what it's supposed to do and follows the style guidelines wouldn't be as hard, it's spitting out C# code that other C# coders don't mind reading and working with. The best way to discover the "bang-your-head-against-the-wall" parts of problems like this is probably to attempt it.
Where have you disappeared to for so many months? Miss your musings!
Some Haskell options: fac n = product [1..n] OR: fac 0 = 1 fac n = fib (n-1) * n C# fib doesn't look anything like either of these, AFAIK. EDIT: OOPS :-) fib -&gt; fac 
Also, what's wrong with (!)?
That's, eh..., *Fibotorial*? ;) Doesn't matter that you're using factorial instead of Fibonacci, though. Don't know much about C#, as I'm much more into Delphi (same creator, by the way ;), so: function Fac(n: Integer): Integer; begin if n = 0 then Result := 1 else Result := Fac(n-1) * n end; Something like that. Looks different, but not *that* different...
this could go under EnHaskell too
Travel and work. Guess I need to concont comething. 
Nothing, I was just pointing out Haskell is flexible. 
It's also the case that package version numbers only give the version of that single package, but you may in principle need that package built against multiple versions of the package that *it* depends on. If a project uses both HDBC and HStringTemplate, then it needs HStringTemplate to be built against an older version of time. But one might also use HStringTemplate in a different context, where a newer version of time is needed. In both cases, it's the same version of HStringTemplate, but the dependencies of HStringTemplate may be different versions. This is definitely a tricky problem, and the number of possible version combinations can explode as you get a large collection of libraries with deeper dependencies.
Well, QuickCheck in particular is a peculiar scenario, since most of those packages do NOT need QuickCheck at all to be linked into a program. Rather, QuickCheck is a dependency only for internal development reasons. It does seem rather sensible to provide a flag for building with QuickCheck properties, and eliminate the QuickCheck dependency otherwise. Then by default, cabal-install could grab the ones without the QuickCheck properties, and thereby eliminate the biggest offender here. 
Sorry, this was more a question directed to the post author than you. :-)
Heh, I must have worked too long before doing that above :-) fib is probably even less similar: fibs = 0 : 1 : zipWith (+) fibs (tail fibs) 
How can I test it? There is no documentation and lauchching "astview" raises an error.
How about a screenshot? A picture is worth testing 1000 hackage packages.
Another approach might be to work in an FRP style framework where you simply work as a function of your input, then you could gradually introduce monads and eventually introduce IO, but still allow for interaction.
Thanks. :-) I've been using Haskell off and on for about nine years. I feel like I can usually express my ideas very succinctly in Haskell, plus I get to spend more time tackling "the problem" rather than doing bookkeeping. These two things alone make it my language of choice when there's no other constraints involved (ie. large legacy systems). The biggest productivity downside IMO is the lack of an Eclipse-like IDE; particularly rapid feedback/compilation and refactoring support. I'm a long-time emacs user and use flymake etc, but eclipse knocks emacs out of the park for java development simply because it understands the semantic structure of code. Having said that, its great to see EclipseFP/leksah/yi all making inroads in this area.
install-problems should be fixed now.
Ah, just noticed, you still need the hscoulour package for Setup.hs: &gt; cabal install hscolour
I'm thinking about learning Haskell and this article makes me want to learn it more than before.
the INSTALL file explains that you have to "cabal install hscolour" before. Also you need gtk2hs ...
Here you are: http://img695.imageshack.us/img695/2975/astview01.jpg
That's "graphical"? Talk about disappointment... :-/
You have attained satori.
[!!!!!!](http://learnyouahaskell.com/)
I loved it. It's adorable and fun, and my girlfriend lost her shit when she saw me playing it. She almost threw my laptop out the window. She was just convulsing on the ground from how cute it was. Even without the graphics, it's a decently fun puzzle game. I suppose a tip might be to expose the level editor to us, so that a community could grow around designing our own levels (puzzle games really thrive on this kind of capability. Think of armadillo run.) As well, shouldn't it be easy to just map right click to delete, or to enable dragging of objects? As well, having to re-place all my items each time I tested the run got a little tedious. I think these basic features would definitely improve the gameplay at the relatively small cost of refactoring a couple lines or perhaps a couple functions. Thank you, and keep working!
It's MAGIC.
Direct download link: http://ecn.channel9.msdn.com/o9/ch9/1/1/2/4/0/5/C9LecturesMeijerC8_ch9.wmv 
I've been looking forward to the * Monads * Monad Transformers * Zippers * Arrows sections for ages now :(
UTF-8 is not the answer - we cannot hard-wire into Haskell the assumption that every Unix path byte-string is the UTF-8 encoding of Unicode in some particular normal form. Unix paths are raw bytes strings, period, and we need to represent them that way faithfully in Haskell if we ever want to support that platform properly. I wish you were right about the ubiquitousness of UTF-8, but we have heard that in CJK locales it just isn't true. I would be thrilled if you could show me solid evidence about that. Anyway, even if we were to accept your plan, the current library does not support that either. Its IO functions are just a thin wrapper over the hack that is the current System.IO library.
Cool, I might have to play around with this. (when I get the time...)
Another interesting edition. Thanks to Janis for all the hard work preparing the HCAR, chasing people up for entries and collating and editing it all.
Thanks so much Janis!
Yes, though you pay the performance hit for the safe downcast and they can still crash you with a fake brand.
Well, they wouldn't crash "me", as the exception propagates back to the caller, so they're only hurting themselves. There is an encoding that prevents even this error, but [it's far too unwieldy for ordinary use](http://higherlogics.blogspot.com/2008/01/almost-type-safe-general-monad-in-c-aka.html). As for the downcast, I agree there's a performance hit, though they're not as expensive as you might think. For instance, [test-cast-call is faster than dispatching on a virtual method](http://higherlogics.blogspot.com/2008/10/vtable-dispatching-vs-runtime-tests-and.html). test-cast-call is essentially a polymorphic inline cache at a dispatch site.
Nice find on the test-cast-call speed. Though, it mostly speaks to how slow dispatching through an interface is. ;)
It's just how our current branch predictors operate. Future hardware could change things.
I really wish this was prepared via a wiki page. 
Or `fix ((0:) . scanl (+) 1)` if we want to get ridiculously unfamiliar!
I think I need a little more background to get this one. I'm getting that the Haskell community is being successful, but what's so funny about that in relation to SPJ?
The unofficial slogan of Haskell is "Avoid success at all costs" and it is often attributed (probably, for a reason) to Simon Peyton Jones. [(See e.g. this text)](http://www.codersatwork.com/simon-peyton-jones.html)
All this time I thought we were trying to be successful...
Could you qualify your wish? What is it that you think would be better with a preparation via a wiki page? The resulting report as a net result (content or form)? Or do you expect a more convenient experience from an author's/contributor's perspective? Anything else?
Are you planning to buy me a plane ticket, or why are you posting this to the whole world? Don't get me wrong, I wouldn't mind posting of the _founding_ of a new HUG, but posting a small-scale, local, event on a global reddit, three days in advance, is exceptionally pointless. It might be better to have a dedicated site that collects a list of HUGs and their respective schedules. Have a look at [hackerspaces.org](http://hackerspaces.org/wiki/) and let yourselves be inspired. Feel free to post links to videos of talks, though, _after_ you've put them online.
I'd expect a more convenient experience from a contributor's perspective. For example, I'd probably add some of my hackage stuff, some acquaintances of mine would mention their commercial Haskell work or I would mention their work myself. I actually missed the announcement that gathering of facts for HCAR has begun (if there was one). I think that wikification of contribution strategy could increase the size of HCAR twofold or threefold.
Goodness, did I really write that over 3 years ago now? Anyway, I happily quit my job to train at Bible college, and now I work for a church. I still do programming, but just a little, and somehow I both get to choose what I program and get paid much more per hour! Which means I tend not to write rants about programming so much any more... 
I would find it easier to maintain my entries over time, and I'd be able to add more material. The email based send and recv approach I find inefficient, and time consuming. Imagine if we had a gitit instance (or a haskell.org page) where you could update your entries at any time. Then the editor would make a call for any updates, then the contents of the page would be frozen, and exported to pdf, etc. 
The announcement was on the Haskell mailing list (and a few others), which seems to be the natural place. Moreover, the submission deadlines are on a very reliable schedule, essentially April and October every year. So once you know of the existence of HCAR, there is little excuse for not being aware of when submissions are due. :-) That is not to say that it couldn't be useful to have a page on the Haskell wiki which explains what HCAR is and when the deadlines are. But I don't see it leading to the two- or threefold volume increase that you suggest. In any case, I am looking forward to entries about your and your acquaintances' work next April!
Okay, this is a more radical proposal for change in procedure than jkff's below. But I must say that I do not really buy the efficiency improvement. Whatever you can do on a centralized gitit instance to maintain your entries over time, update them at any time, and so on, you can as well do locally for yourself. Then when the call for the next edition comes, you can take the then current version and send them in via email. And the editing has less to do with putting the entries to together for exporting into pdf, but more with chasing after authors, organizing things and making them consistent, seeing to it that related entries by different authors get cross-referenced via additional links, ... I don't really see how that would become more efficient by short-circuiting the email send and recv. But maybe I am just ignorant of some of the possibilities of gitit or other systems. (This is very possible!)
Well, even a basic move away from email send and receive would help me a lot. My inbox is always overfull. But I can edit wikis with little effort. Also, I imagine it will bring in many Haskellers who don't live on the email list (there are more subscribed to this reddit, than to haskell-cafe!) A community report maintained on a community wiki makes a lot of sense to me, and may be the only way to scale this to cover the broader web community of haskellers. I'd be happy to set that up, if you have the sources available, to see if it would be feasible.
I am open for a concrete proposal of technological advancement. But it would have to come with real added value for the report itself. A two- to threefold increase in submitted entries would be such incentive. And Don, I think that your situation is quite special. Most others don't have dozens of potential projects to report on at any time. :-) For the average contributor who has one to three entries in the report, communicating per email seems to be the easiest route. And when it comes to bugging people about "you haven't updated that entry for a while", "would it make sense to cross-reference here", and "maybe you could add a screenshot there to make your point clearer", I would likely resort to email in any case. Finally, for my organizing it seems advantageous to have just one inbox to check for, rather than dozens of wiki pages to go through to see what has already been updated and what not. (But again, this impression could come from my unawareness of some of the proposed tools' capabilities...) 
Having an online site also makes it easier to solicit from online users via twitter, facebook, reddit etc. We could well see a 2x increase in edits -- there really are a lot more Haskellers than are found via known email lists. &gt; dozens of wiki pages to go through That can be automated, and it depends on how we set it up. The exciting thing for me is that the wikis are designed for massive, distributed editing of live documents, so they're the perfect technology for a community activity report. My proposal would be to try an experiment: take the current HCAR source, convert it into a gitit wiki (backed by darcs or git) that I could host, and then do some experiments to see what the workflow would look like. Finally, I wonder if there's anyway to include all the material announced only online (that we see here on the Haskell reddit). The majority(?) of things announced on the Haskell reddit (and the 1700 packages on Hackage) don't make it into the HCAR. How comprehensive could it be if it we had cheap anddd automated entry editing.
The experiment is easily possible, of course you can have the current HCAR source (by traditional email, once I am back to my traditional university office on Monday... :-)). As for the massive, distributed editing aspect that wikis are designed for: We do have a Haskell wiki, right? Why has it not taken over the function of HCAR in all that time? 
Wikis needs some structure and guidance so that the editing is directed. The haskell.org wiki catalogues tips and tricks. It all depends on what structure you put in place. In our case, it would be the HCAR format, I think. Then the users will find the relevant section, edit their entry, and move on. Also, the haskell wiki runs on MediaWiki, which doesn't have a convenient export function. gitit however, lets us extract to txt, html, pdf, ... making it a more appropriate tool for collective editing before publishing. 
Okay, this sounds all very useful. We should indeed do the experiment of how the current HCAR would turn into a gitit instance. One thing to be aware of is that not all people working on Haskell projects are (apart from their chosen programming language) as technology and web2.0-savvy as you might assume (mentioning twitter, facebook, reddit etc.). There are currently enough people who send in plain text entries and ask for them to be marked up for latex use, because they are not familiar with it. Imposing on them that they should get a gitit account and learn some markup format for it could already set the entry hurdle too high (compared to sending a simple email). 
I think we might be ok. gitit can export to latex, it may be able to input latex as well. I'll investigate (the markup is powered by [pandoc](http://johnmacfarlane.net/pandoc/) which claims (subsets) of latex as input). For the account problem, I'll talk to the gitit developers -- we might be able to have a guest account, or prepopulate it with the hcar author email list. Accessibility will be important.
Tthis is entirely the appropriate forum for community organizing. After all, there may be no other channel to reach as many local users. Having a dedicated HUG site would also be great.
Thank you for making it a properly typeset document instead if an eye sore web page. 
* flymake: IDE-style on-the-fly compilation * hoogle: type-indexed function search * criterion: benchmarking * profiling tools in GHC
If you want to [go high end](http://www.cl.cam.ac.uk/research/hvg/Isabelle/haskabelle.html) static analysis... Also, runtime debugging, [threadscope](http://raintown.org/?page_id=132). Second vote for criterion.
"Starling Software", I believe, although I bet Curt will be startled by the link:)
Where is the code to download that he speaks of? Am I just very blind and missing it?
Holy shit balls!
Is anyone working on getting Hat to work with current versions of GHC? I used it a long time ago (before I knew about QuickCheck), but I can imagine hat-detect and QC would be a nice combo.
SourceGraph for module and function dependencies, among other things. Online here: http://hackage.haskell.org/package/SourceGraph, and going to be presented in a tool demo at PEPM'10. (As well as Haskabelle, mentioned by Don below.)
I'm sorry, but this article has quite a misleading title. I was curious about how these two languages could be sensibly compared and now I see that the author is very superficially examining a small area of the basic types of both languages
oh, yeah that's a little embarrassing (sorry Curt)
I wouldn't use the term CASE.
I've been a fan of test-framework for running your test suites lately. Something I would really like is (seamless) continuous integration for Haskell projects; maybe hudson + a junit xml backend for test-framework would be enough, but a pure Haskell server to run tests, compute test coverage w/ hpc, generate haddocks, bump version number, etc. would be awesome.
Don't forget the wonderful GHC profiling runtime stuff you can build in. See the Real World Haskell book, and buy a copy if you don't have it. It's really great for this sort of stuff, and I'd have been totally lost without it recently. You can use it to add in RTS options for many different kinds of statistics gathering and generate some really beautiful Postscript file diagrams that give a deeper insight into where you should look to optimize code. Also on the horizon is the ThreadScope program that should be shipping with GHC 6.12 I believe, to help analyze threaded/parallel code. I'm personally looking forward to that one like a little kid waiting for Christmas. 
I feel P. Static's pain. Some documents that helped me quite a bit with monads: * [All About Monads](http://www.haskell.org/all_about_monads/html/index.html) * [Monads as Containers](http://www.haskell.org/haskellwiki/Monads_as_containers) * [Monad chapter in Yet Another Haskell Tutorial](http://en.wikibooks.org/wiki/Haskell/YAHT/Monads)
&gt; Unfortunately, it begins by diving straight into the "mathematical laws" that "govern" monads, whatever that means. [...] &gt; To borrow an example from the quote, you don't introduce the == operator by saying that it follows a mathematical law. You say what it actually does first! &gt; So what is a monad, actually? The problem here isn't the answers you're getting. It's the question you're asking. I think you're confused and frustrated because you're missing something *very* basic: *the notion of `Monad` is exactly these laws.* Anything more concrete than those laws is not what `Monad` means. If anyone tells you what the notion of `Monad` *does*, or gives you any concrete explanation (and certainly any explanation in terms of state), they're also confused. You might feel more comfortable at first with a concrete explanation, but only because you're being misled down a familiar road rather than being accurately led down an unfamiliar road. Similarly for other type classes, e.g., `Monoid`, `Functor`, `Applicative`, `Arrow`, `Eq`. For anyone wanting to understand `Monad`, I recommend starting with simpler type classes, such as `Eq` and `Num`, then `Monoid`, then `Functor`, and then `Applicative`. In every case, when you want to know what the type class *does* (or even concretely *is*), remind yourself that you have to ask a different question in order to get an accurate and helpful answer. The paradigm of type classes is *a very powerful* one. If you've studied abstract algebra, you will have encountered examples: `Group`, `Ring`, `Field`, etc. Like `Monad`, these notions are abstract rather than concrete. They are all about capturing commonalities among many different concrete types, and they do so by identifying "mathematical laws" common to those types. If you have not studied abstract algebra, then Haskell might be your first exposure to this paradigm (abstract notions defined by collections of mathematical laws). 
 BTW, I suspect that much of the time people say they want to understand monads, they *really* want to understand how Haskell manages to embrace imperative programming, in spite of being a purely functional (and thus purely non-imperative) programming language. In those cases, I see the notion of monads as being much more harmful than helpful, because it forces them immediately to grapple with an abstract generalization of the information that they want. Haskell `IO` is a simple technical trick: embed a minimal imperative "language" as a data type in a purely functional language. One can understand this technical trick without having to know that the data type happens to fall into the general patterns called "Monoid", "Functor", "Applicative", "Monad", "Num", etc. 
And check out Brent Yorgey's insightful *[Abstraction, intuition, and the “monad tutorial fallacy”](http://byorgey.wordpress.com/2009/01/12/abstraction-intuition-and-the-monad-tutorial-fallacy/)*.
&gt; BTW, I suspect that much of the time people say they want to understand monads, they really want to understand how Haskell manages to embrace imperative programming [...] I see the notion of monads as being much more harmful than helpful, because it forces them immediately to grapple with an abstract generalization of the information that they want How true. What can we do with it? E.g. when someone approaches me with a question "I'd like to grok monads, please give me a link to a monad tutorial", what should I answer?
&gt; when someone approaches me with a question "I'd like to grok monads, please give me a link to a monad tutorial", what should I answer? My advice is *don't* -- at first. Instead, collaboratively refine the question. Find out what the asker is *really* looking for. Do they really want to understand functional imperative programming? The idea and/or technique of type classes? A few monads in particular? How "do" notation desugars? etc
&gt; To borrow an example from the quote, you don't introduce the == operator by saying that it follows a mathematical law. You say what it actually does first! Hm. How *could* you say what == actually does -- in general, as opposed to just a few specific, concrete instances?
He's probably trying to understand how to do IO in Haskell. He has heard that it's somehow related to monads. Do we have a decent article explaining exactly this, without generalizations?
Ah, I wish it was that simple. XD I actually wrote that rant because of an assignment for my programming languages class - implementing state in a toy interpreter, using a monad. I only have the vaguest grasp of how monads work, so it's a bit of a frustrating exercise. 
Wow, never expected this to land on reddit. Wrote it in a hurry while working on a class assignment. As long as I've got the attention of people that know what they're talking about, though... ;) That last bit I wrote about monads being similar to wrapper classes in OO languages. Is that anywhere close to accurate? It seems that it behaves similarly, in that you can add behaviors to a piece of data using monads, but I feel like I've latched on to something familiar, and I might still be missing a lot about what monads can actually be used for.
My favorite monad tutorial: http://ertes.de/articles/monads.html
The [website](http://www.cs.nott.ac.uk/~gmh/book.html) of the book "Programming in Haskell" has links to the slides as well as source code. The file [Parsing.lhs](http://www.cs.nott.ac.uk/~gmh/Parsing.lhs) is covered in this particular lecture.
Always happy to read Conal's blog posts!
Does anybody have a lead on how this _does_ work? I was all excited about the description, then the author didn't know. :)
The [source code for the module](http://hackage.haskell.org/packages/archive/data-memocombinators/0.3/doc/html/src/Data-MemoCombinators.html) is relatively clear, clean and short. It basically works exactly the way you'd expect it to (by building a table of results previously computed from arguments, and returning the cached result when possible).
It's a combinatorization of a standard memo technique. Haskell actually *does* automatically memoize for you. For example, if you build the pair p = (expensive1, expensive2), then the first time you call fst p, expensive1 will be computed, but the result will be remembered for the next time you call fst p. Functions are actually the special case: they are the only things Haskell does not memoize. So the trick is to map your function to a concrete data type without any arrows, and then back to a function again. The function on the way back will hold a reference to the concrete type, which provides the memoization. The concrete type is the "memo table", and what it actually is is quite unimportant (just that it exists), so MemoCombinators hides that. A good illustration is this bool memoizer: bool :: (Bool -&gt; r) -&gt; Bool -&gt; r bool f = cond (f True, f False) where cond (a,b) True = a cond (a,b) False = b It says that the function type (Bool -&gt; a) is isomorphic to the non-function type (a,a), and maps to and from it. The type Memo a is a synonym for forall r. (a -&gt; r) -&gt; (a -&gt; r), a function that takes functions from a to (memoized versions of) themselves. From that basic idea, the rest is pretty simple. You have memoizers which take other memoizers as arguments. So eg. maybe :: Memo a -&gt; Memo (Maybe a) Can be read via the Curry-Howard isomorphism as "if you can memo an a, you can memo a Maybe a". One way to do this would be to use the isomorphism between (Maybe a -&gt; r) and (a, a -&gt; r), the first component being the Nothing case and the second being the Just case (using the given Memo a to memoize the second component). Exercise: give an implementation for the Maybe memoizer above. And, the way combinator libraries do, these simple building blocks start folding on each other and you can make pretty complex memo tables -- complex enough that it would be pretty nasty to write them directly -- from small, simple pieces.
I'm the guy that wrote the article, and I like this explanation; you mind if I use it or something like it and credit you with a link back to this?
&gt; ...and while addition and multiplication are both monoids over the positive natural numbers, a monad is a monoid in a category of endofunctors. It's all very simple. Damn it, I am never going to understand monads from the internet. Every single website, blog, or tutorial that tries to explain monads to me makes me more confused. I think the only way I will understand it is if I learn the actual math this stuff is based on. I need to take a math course that will teach me what monads really are, completely independent from programming. What course do I have to take that will cover monads? Is there a good graduate text to read to understand this stuff? **Edit:** I am very grateful of people trying to help me by explaining what monads are. However I have already read tons of material about monads and it just confuses me more. I am not a software engineer, so explaining things in a way to make a programmer understand it, probably isn't going to help me. At this point I would much rather just learn the math. If you want to really help me, explain what an endofunction is or where I can learn about them.
&gt; 4 Haskell doesn't need Monads &gt; &gt; ...well, apart from the Haskell standard defining the way IO is done in terms of Monads: It could be done differently and still work. Can someone elaborate on how IO was done in Haskell prior to monads? Thank you!
Category theory. ...but you don't want to do that, unless you happen to like having proofs for breakfast. Give it some time, but most importantly, give it some practice and some ignorance: Implement stuff without them, and you're bound to recognize the pattern they capture sooner or later.
&gt; a monad is a monoid in a category of endofunctors. It's all very simple. Whoever added that "helpful" comment should probably be banned from the Haskell wiki. You don't need to understand the words "monoid", "endofunctor" or "category" to use monads in Haskell effectively.
I have written two small haskell programms at this point and completely avoided anything to do with monads (and crying while trying to do any IO, but copy and pasted from others) I could coulntiue that but I would never actually use monads so I wouldn't recognize the patterns. I have to use them some how to recognize the patterns. However the bottom line is that I am not a software engineer, and don't plan on becoming one. I just like to learn stuff. Taking a graduate category theory class sounds more appealing to me than struggling to write programs where I don't understand the tools I am using.
You're not alone in miserable ignorance. My attempts to learn Haskell go something like: * Word, functions n shit * Oh ok monads hmm, let's see now... * (30 minutes later) Open wikipedia -&gt; monads/category theory * (30 seconds later) Insert fork into eyeball
see the interact function -- just thread through an input and output stream, essentially.
Many people say this but I am starting to disbelieve it. I think that is the whole trouble, everyone is trying to explain a monad with out telling me what a damn monoid or endofunctor is. I am starting to believe that what I need is to just sit down and learn the category theory so I can stop being confused by people trying to make them sound simple without using mathematics. 
I agree with "category", but "endofunctor" is useful, and "monoid" even more so. Not to use, mind you, but to understand, which is the thing this page is about. ...and if they'd banned me from the wiki, we wouldn't have most of the rest of the page, either. 
[A monad is a wrapper type around another type (the inner type), which adds a certain structure to the inner type and allows you to combine computations of the inner type in a certain way. This is really the full story.](http://ertes.de/articles/monads.html) Start with the Maybe monad as it is quite simple. The Maybe monad wraps it's inner type, the structure it adds is that of an optional value. So all instances of Maybe have either the Nothing value or the value of the inner type. You can combine computations using this monad so that if one of the values of a computation is nothing, the whole sequence of computations is Nothing. There's more to it than that. But this is a start. I believe a lot of the confusion comes from learning the intricacies of the Haskell type system. Forget category theory (unless you are really interested). You don't need to study electrical engineering to play electric guitar. 
No, just use them for a bit. I wasn't terribly comfortable with monads until I wrote an interpreter using monadic parsing combinators and my own monad to manage stack frames. Another idea is to use ST arrays to implement the Sieve of Eratosthenes or an algorithm based on Union-Find. I still have only the vaguest notions of what the categorical interpretation of a monad is.
I have read that post. It is included in my statement that any explanation of monads on the internet just confuse me. Also, the Maybe monad is simple, I can use it just fine, but it seems to exist only to make things harder. I have never seen why I would actually want it. Any time I use a function from a library that returns a Maybe I throw fromJust at it and get the damn value I wanted.
I disagree: there are many formalisms in CS that aren't necessary for practitioners to master. I think an intuition for monads, if not an understanding of the theoretical abstractions underlying their definition, can be gained by trying to naively implement things like the Maybe and State monads. For Maybe, you want to be able to run a sequence of evaluations that will stop evaluating as soon as one fails. In your effort to do this, you will have to figure out how to glue two actions together (e.g. "First do this; if it fails, stop, otherwise do the next thing."), and you are likely to end up with a solution that has a lot in common with the Maybe monad in Haskell that also has the nice feature of being useful to you today. This will give you a way of using your favorite language to express compositions of potentially failing computations without explicit error checking at every step. That's useful regardless of any abstract definition or property! Similarly for State: try to figure out a way to avoid having to always pass around a state-carrying value among a bunch of functions that all need it. Don't let the abstract definitions obscure the utility from you, and don't get too hung up on losing parametricity if that's making things awkward for you. Go ahead and have one type of state-carrying data structure and write the code for gluing together successive actions with knowledge of that specific type. You can work on extracting the abstraction after you've worked out how this design style transforms your code and how it all fits together.
So it seems you understand monads (at least partially), but you don't "get" them yet. And by "get", I mean what are the implications, why do we want them etc. &gt;Any time I use a function from a library that returns a Maybe I throw fromJust at it and get the damn value I wanted. Well in Haskell there is no null. So if you want to represent a computation that may not return a value then you need to use something like the Maybe monad. You could model this as a computation that returns an empty list or a list of 1 element. But it is not as clean as Maybe. Another big advantage is composability and boiler plate reduction. There is a good example of this in [RWH](http://book.realworldhaskell.org/read/monads.html#id641078). variation2 person phoneMap carrierMap addressMap = do number &lt;- M.lookup person phoneMap carrier &lt;- M.lookup number carrierMap address &lt;- M.lookup carrier addressMap return address In this example, we chain 3 lookups, each depends of the result of the previous one. Yet there is no need to explicitly test for Nothing on number, carrier or address. In java, you would have to check if result!= null at every step. here the Maybe monad implementation of &gt;&gt;= short circuits the chain of lookups as soon as one of them is Nothing. 
All I can lean on is my personal experience. I'd never heard those words before I'd learned to use monads as implemented in Haskell. To this day, I only have a passing familiarity with monoids (essentially only what I need to know to use Data.Monoid), and know basically nothing about category theory. At the same time, I'm sure that learning category theory wouldn't make me any stupider. People learn differently, and learning theory first might very well be the way you learn best. Have you tried Real World Haskell? Their [monad chapter](http://book.realworldhaskell.org/read/monads.html) seems pretty good.
Prior to monads, there were at least two ways (you'll probably find them if you look up the really old language definitions). The first used lazy request/response lists, so main might look like: main :: [Response] -&gt; [Request] main resps = GetLine : case resps of Line l -&gt; PutStrLn l : case resps of Done () -&gt; ... Perhaps there was more to it than that, as that strikes me as somewhat prone to error, with all the non-exhaustive pattern matches and potential for examining the response stream too eagerly and whatnot, but that's the general idea. The other method (built on top of the above, as I recall), used continuation passing combinators. So an equivalent main definition would look like: main = getLine (\l -&gt; putStrLn l (\() -&gt; ...)) That should look a lot like: main = getLine &gt;&gt;= \l -&gt; putStrLn l &gt;&gt;= \() -&gt; ... because it is. Both of the above could be taken as implementations of IO (more or less), in addition to whatever token passing or other implementation you might think is "really" the IO type. The combinators based on the fact that IO is a monad are 'just' convenient ways of writing these computations down; it puts the old, ad-hoc solutions to IO in a nice, general framework.
I wasn't really being serious about banning. However, tacking "It's all very simple" to the end of a sentence where every other word is obscure jargon isn't helpful. In fact, it may discourage or even insult the reader.
bawww baww haskell sucks! there are like 50 libraries on hackage under database category, I tried one and didn't like it. baww!!!!!
[It's a (slightly mangled) quote](http://www.google.com/search?q=%22a+monad+is+a+monoid+in+the+category+of+endofunctors%22&amp;ie=utf-8&amp;oe=utf-8&amp;aq=t&amp;rls=org.mozilla:en-US:official&amp;client=firefox-a)
A Monad is a type class with four functions, only two of which are really interesting: 'return' and '&gt;&gt;='. 'return a' means to make some value monadic, by wrapping it in a type (which is always a type that takes one and only one type parameter). &gt;&gt;= takes a value that's thus wrapped, and also a function a -&gt; m b, which "extracts" the value from its type wrapper, and applies the function, which will always re-wrap it with the same type (that's what the 'm b' means, that b is wrapped with m). And that is it. There is no extra theory, analogies, or magic. That is all they do. Nothing more. There are "monad laws" that define more precisely what return and &gt;&gt;= are supposed to do, but they're secondary. It's just those two things, and every monad does them differently. There is also &gt;&gt;, which is just &gt;&gt;= that ignores its argument on the left, and there's 'fail' which is more or less a wart you can ignore. So everything that makes a monad the way it is is just return and &gt;&gt;= and that's it. No assembly lines, spacesuits, or burritos. Okay, there's 'do' syntax, which is purely syntax sugar. Understand return and &gt;&gt;= without 'do', and the way 'do' works will just fall out naturally. 
I have tried some of RWH but I am probably due for a another crack at it. You are right that everyone learns differently. I am a physicist at heart, so learning higher math is probably easier for me than learning new programming constructs.
Taking myself as an example, I am really interested in FP and I have a Master's degree in Computer Science and I work as an application developer. I programmed in Pascal, C, C++, Python, I know Java, took courses of Lisp and Prolog, have basic knowledge of OCaml and I played with Scala a bit. I like concepts like referential transparency and I begun to learn Haskell. Why Haskell? I don't like Lisp too much, I don't fancy refs and strange operators like .+. in OCaml, Scala is bound too closely to JVM (although I like it a lot) so I got interested with languages like Clean and Haskell, solving some Project Euler problems. I abandoned Clean as soon as I realized it does not support large integers out of the box, its IDE does not support *nix any more, its homepage is half-functional and the community seems rather small Haskell has almost everything, I find it really elegant and powerful, I like algebraic data types, typeclasses and other things. But the learning curve seems to be very steep (hard) once you get out of math relating problems. For Project Euler it seems like one of best choices, solutions tend to be clear and elegant. But how to structure "real world" applications? It's rather confusing, you don't know where to go and then you see a solution: "ah, monads!" I can do basic IO, I can do an infinite loop, but that's it. The whole notion seems to be very abstract and not helping too much. Maybe the question is: how to structure a Haskell application well and (relatively) easy and intuitive? Believe me, I catched most of Scala concepts quite quickly, but when you come from the imperative world (+ OOP), everything is new in Haskell and you may get lost for a while. I am not quitting, I use Real World Haskell and other resources and I slowly get on. But I have a feeling that I am thrown in the middle of an ocean and I have a small piece of wood with me and nothing else. I believe that I will see the promissed land one time and everything will have sense sometime. But I am not there yet and it's a little bit frustrating.
Try [you could have invented monads](http://blog.sigfpe.com/2006/08/you-could-have-invented-monads-and.html) It explains from the ground up how they work using small specific easy to understand examples. No horrible analogies or category theory! Just think of them as a design pattern. 
Don't be discouraged. Monads are very useful in a practical sense. I know it's difficult to get your head around them at first, but the difficulty comes from them being an excessively general concept, not from them being difficult as such. In the unlikely event that it helps, here's yet another perspective on monads: A monad can be thought of as an abstraction that takes some deeply nested code and allows you to jump out of its context into a "flat" context to get the result of some "action". (Almost like a co-routine, such as Python's generators.) In this "flat" context you have complete freedom to manage the "execution" of your nested monadic structure (including state, execution order, exception handling, repeating code under different conditions, etc). And, your monadic code remains a completely pure description of some logic. Here's an example: I made a monad to generate binary data with forward references in it. To do this, I evaluated it once with dummy offsets to find the locations of the places forward referenced, and then a second time to generate the real data with forward references resolved. This was implemented through three "actions": 'mkRef', 'get' (get the address of the thing referenced) and 'here' (to say what the reference points to).
luqui deserves credit anyway. :)
I'll just point out that the author's blog is hosted in Haskell blog software the author wrote.
Oh hey, I see you in the IRC all the time. This is a great explanation. Thanks.
kamatsu, I think this reddit contains really useful information. Could you please copy it over to a Haskell Wiki page, before it slides down the reddit page into oblivion?
&gt; Implement stuff without them, and you're bound to recognize the pattern they capture sooner or later. Oh -- I like this advice very much! To deeply grok the abstraction, begin by *not* using it. Instead, write a bunch of code on lists, state transformers, functions, Either, Maybe, IO, etc, *without* using generic Monad operations. Notice the patterns that emerge in your code. Then invent Monad, in order to make your code cleaner and more general. Since Monad is something of an advanced type (constructor) class, do this exercise first with simpler type classes: Monoid, Functor, Applicative.
As it is explained in RWH, you can see &gt;&gt;= as a "programmable semicolon", such as in your example (or a programmable newline here)
i thought monads are what lets gordon freeman keep evading the combine :/
It made the text more amusing to me. It's pretty clear that those two sentences weren't very serious.
Where would the humour be if the "it's all very simple" were removed?
No prob :) I should probably elaborate a bit and say that the monad laws aren't exactly "secondary" as in you can do without them -- they're actually pretty fundamental to how monads work -- but you only need to follow them when you write your own monads, so you can learn them after you've grokked return and &gt;&gt;= What's cool about the 'a -&gt; m b' function in monads as opposed to just a plain a-&gt;b function is that 'm b' stands for a type that can do pretty much whatever it wants with that type b, including having several values (e.g. a list), piggybacking some other value with it (State), or optionally not using it at all (Maybe). 
&gt; Notice the patterns that emerge in your code. Then invent Monad, in order to make your code cleaner and more general. Which is the approach taken by [You could have invented monads](http://blog.sigfpe.com/2006/08/you-could-have-invented-monads-and.html).
Half the people talking about monoids and endofunctors misunderstand that definition anyway. 
Consider you have an XML utility function: lookupChild :: String -&gt; Element -&gt; Maybe Element If you use it naively, to get the result of applying f to a grand child of the root, whose XML path is "child.grandChild", the code would look like: case lookupChild "child" root of Nothing -&gt; Nothing Just child -&gt; case lookupChild "grandChild" child of Nothing -&gt; Nothing Just grandChild -&gt; Just (f grandChild) If instead, you use the Maybe monad, it looks like: lookupChild "child" root &gt;&gt;= lookupChild "grandChild" &gt;&gt;= \grandChild -&gt; return (f grandChild) With do notation, it looks like: do child &lt;- lookupChild "child" root grandChild &lt;- lookupChild "grandchild" child return (f grandChild) Now, we didn't only make the code shorter and prettier, we also made it more general! Why? Because now, if lookupChild returns all children that have a certain name, rather than the first/single one, and has type: String -&gt; Element -&gt; [Element] -- we can use the same "do" block with the List monad. The list monad's &gt;&gt;= and return will get us f applied to **all** grandChildren with that path. In general, Monads allow us to generalize code so it works with all types that happen to fit the structure of (a -&gt; m a), (m a -&gt; (a -&gt; m b) -&gt; m b).
If your Monad evaluates twice, it probably can't be a Monad Transformer, I suppose?
If I might stick my oar in... Monads are often motivated by the need to model the semantics of computational effects (exceptions, state, printing, etc) but that's missing a big chunk of the picture. *Syntax* is inherently monadic, in a way that's pretty easy to make sense of. A syntax is given by a way to build formulae from atoms (typically by applying operators). Take Fmla x to be a type of formulae (some sort of expression) built from atoms (constants, variables) of type x. Now, we might reasonably expect that (i) every atom is a formula (ii) substituting formulae for the atoms in a formula yields a formula Note that in (ii), substitution might not preserve the set of atoms: we often start with atoms being either variables or values, and end up with atoms being values only. But the point is that (i) is return :: x -&gt; Fmla x -- atoms are formulae and (ii) is (&gt;&gt;=) :: {- to this -} Fmla x -&gt; {- apply this simultaneous substitution -} (x -&gt; Fmla y) -&gt; {- getting this -} Fmla y It's quite hard to do mathematics or programming without encountering substitution as a process that you just learn to do. And guess what? You were working with monads all that time! The monad laws tell you that composing substitutions behaves sensibly (composition of substitutions is associative and absorbs the substitution which does nothing). So, no effects, no state, no strictness. Just the basic apparatus of manipulating formulae. But that's pretty handy, because however we model the semantics of effects, we'll use a syntax of formulae to describe the things which live in the model. And you can bet your arse there's a homomorphism from the syntax to the semantics. Oh no. I said a rude word.
Have you checked out the [Category theory page](http://haskell.org/haskellwiki/Category_theory) on the Haskell wiki? You'll also find some good information on monoids, functors, monads, etc. (if I do say so myself =) in the [Typeclassopedia](http://www.haskell.org/sitewiki/images/8/85/TMR-Issue13.pdf). I dunno why everyone keeps trying to convince you that you really don't want to learn category theory, I think it's awesome. =)
[have you tried typeclassopedia?](http://www.haskell.org/sitewiki/images/8/85/TMR-Issue13.pdf)
Awesome thanks for these links. This is closer to the approach I need to understand this stuff.
I remember your pain well. They are one of those things that you either understand or you don't; there isn't a way to half understand how to implement a monad. A lot of monads also use higher-order functions and/or lazy evaluation, which makes a brain bender even more difficult to figure out. I made a big mistake when first started learning monads. Basically, I confused the State Monad with the basic Monad and ran into all kinds of trouble and it took me a *long* time to figure out what I was doing wrong. It took a lot of perseverance but I finally understand them well enough to write them. I think that the best monad tutorial would use Flash (OpenGL?) animations to show how the basic monads (e.g., Maybe and State) work and how IO is similar to the State monad. At this time, I think your best bet would be to try to find a Haskell programmer in your area that can tutor you in them; a little one-on-one may make things a lot clearer.
I think this was explained in the Haskell HOPL paper http://research.microsoft.com/en-us/um/people/simonpj/Papers/history-of-haskell/
A monoid is any associative binary operator (over some set) with an identity element. That's it. Some examples you probably already know well are (Nat,+,0), (Nat,*,1), (Nat,max,0), (Nat+Infinity,min,Infinity), (Int+NegInf,max,NegInf), (Set S, union, empty), (Set S+Universe, intersect, Universe), (Powerset S, intersect, S), (Lattice, meet, top), (Lattice, join, bottom), (List S, ++, [])... and one you may not have thought about before is (S-&gt;S, . , id) Monoids are really quite trivial. The observation about monads being "monoids on the category of endofunctors" is mainly just a way of formalizing the fact that monads bear a striking similarity to monoids. The (S-&gt;S, . , id) monoid is very close to both the idea of a category and also the idea of an endofunctor. So it shouldn't be too surprising that we do a monoidal thing to a monoidal thing and we get something monoidal out of it. But again, we're just formalizing that unsurprising intuition and commenting about the fact that monoids are *everywhere*.
Byorgey, no offense, but your markup and link accuracy is fail.
Okay, I'll give it a few more days for more contributions and then put them up.
I'd like to see some entries (just so that haskell is represented), although i'm not interested in participating myself.
Seems odd to write lean &amp; mean fp code in a big fat java ide..
"The abstraction is trivial once you understand its prime examples." -- made up on the spot
I don't disagree, but when you're used to an IDE, be it big and fat, being able to concentrate on the language you're developing into because the IDE functions are natural to you is a big plus. And hopefully having multiple choices for a Haskell development environment will mean more people can get to try out the language, and more healthy competition between these environments.
Sounds good. And EclipseFP even worked better than the Scala plugin when I last tried too.
The relevant wiki pages: * [http://hackage.haskell.org/trac/haskell-prime/wiki/DoAndIfThenElse](http://hackage.haskell.org/trac/haskell-prime/wiki/DoAndIfThenElse) * [http://hackage.haskell.org/trac/haskell-prime/wiki/HierarchicalModules](http://hackage.haskell.org/trac/haskell-prime/wiki/HierarchicalModules) * [http://hackage.haskell.org/trac/haskell-prime/wiki/EmptyDataDecls](http://hackage.haskell.org/trac/haskell-prime/wiki/EmptyDataDecls) * [http://hackage.haskell.org/trac/haskell-prime/wiki/FixityResolution](http://hackage.haskell.org/trac/haskell-prime/wiki/FixityResolution) * [http://hackage.haskell.org/trac/haskell-prime/wiki/ForeignFunctionInterface](http://hackage.haskell.org/trac/haskell-prime/wiki/ForeignFunctionInterface) * [http://hackage.haskell.org/trac/haskell-prime/wiki/LineCommentSyntax](http://hackage.haskell.org/trac/haskell-prime/wiki/LineCommentSyntax) * [http://hackage.haskell.org/trac/haskell-prime/wiki/PatternGuards](http://hackage.haskell.org/trac/haskell-prime/wiki/PatternGuards) * [http://hackage.haskell.org/trac/haskell-prime/wiki/RelaxedDependencyAnalysis](http://hackage.haskell.org/trac/haskell-prime/wiki/RelaxedDependencyAnalysis) * [http://hackage.haskell.org/trac/haskell-prime/wiki/LanguagePragma](http://hackage.haskell.org/trac/haskell-prime/wiki/LanguagePragma) * [http://hackage.haskell.org/trac/haskell-prime/wiki/NoNPlusKPatterns](http://hackage.haskell.org/trac/haskell-prime/wiki/NoNPlusKPatterns) 
Forget the category theory! (I have a degree in math and all this abstract nonsense still confuses me sometimes :) Instead, try to understand concrete examples. Start with Maybe, that's very simple, you can just write down all the possible combination for bind. You can think of it as a way to handle errors. Then move on to State and/or IO. Then to Reader. Then to List. If you understand these examples, and they are actually really simple, you more or less understand the concept already! After that you can move on and study the more esoteric animals :)
Question: Why is a Monad called a Monad?
Does this mean that there will be a new revision of the language? New syntax? I'm new to the language.
It means many of the common extensions existing and commonly used in GHC are being made mainstream with an official language specification release. It's exciting because it's the first such release since Haskell '98 which was (as you might guess) 12 years before this one. The team has put a lot of work into grilling over each of many proposed language features and ensuring they're the right thing to settle on. Congrats and thanks to the Haskell' team! 
They're only accepting 10 changes? But there must have been hundreds of proposals made. Why so few?
Ok, thanks for the clarification. What is the difference between this and the Haskell Platform?
Haskell Platform is an optional library and tool suite. Haskell 2010 is a new release of the core language.
The Haskell Platform is a bundle of blessed and tested libraries that is released periodically. This is an actual language change (albeit small and already widely implemented.)
This is the language specification, Haskell Platform is going to be blessed packages and a compiler to get you going in developing neat Haskell code that will work with libraries that everyone is using. So Haskell Platform is just a convenient package containing the latest (stable) version of standard packages.
Wow, FixityResolution is actually pretty hilarious. I didn't know about that little corner of the language.
Meh. I can't say I find this terribly exciting. Half the changes are syntax/parser-related, and the only "real" extension that seems commonly useful is PatternGuards (FFI is useful but definitely doesn't seem like a common case). Have we already failed to avoid success at all costs?
Good job Moresmau
Because they've moved to a more incremental process. There will be a Haskell 2011 with maybe ~10 more changes and so on. This way they can make *some* progress on the most obvious decisions without having to wait for some of the more controversial changes to play out (e.g. associated types).
I wasn't planning to, but if the infrastructure will be there I might be interested.
But I don't think that solves parsing case e of _ -&gt; x == y == z Perhaps it's a matter of interpretation.
It's not meant to be exciting, Haskell' never was. Haskell' is supposed to standardize a set of extensions that everyone is already using. I think this is excellent work!
From description of [DoAndIfThenElse](http://hackage.haskell.org/trac/haskell-prime/wiki/DoAndIfThenElse): foo x = do if x then return 1 else return 2 &gt; ... &gt; &gt; add optional semicolons before then and else, making the above example legal Does anyone know how adding an optional semicolon changes indentation rules? 
Speaking of syntax, I was trying to figure out how to normalize parametric HOAS last night. The problem is that when you see a term like: (\x -&gt; e) t you cannot perform the usual substitution operation, which has type Term (Term v) -&gt; Term v -&gt; Term v It would require `t` to have type `Term v` while the lambda expression would have type `Term (Term v)`, so they couldn't appear in the same syntax tree. So, I thought a bit, and decided what I needed was something with an injection `Term v -&gt; v`, which I could unwrap at a later time. I finally settled on: data Norm v = NZ v | NS (Term (Norm v)) And the substitution case looks like: norm :: Term (Norm v) -&gt; Term (Norm v) ... norm (Ap (Lam e) x) = norm (e (NS x)) ... But, as I noticed a while later, `Norm` is just the free monad over `Term`! So, by using the free monad as our "variables", we are afforded the arbitrary substitution capabilities needed to compute normal forms.
I was interested when I heard of the competition. It would help a lot if someone (or a group) built a starcraft-ai package and uploaded it to hackage. I'm not in the mood to make an FFI binding to some C++ library.
I realize that. I just hope that the relative success of Haskell won't mean that questionable design decisions from times past are set in stone forever.
&gt; questionable design decisions from times past are set in stone forever. Clearly not: n+k patterns were removed.
After reading each of the above-linked wiki entries, I have to agree on the changes themselves being, well, pretty insignificant. But I think that there is change at all is a good thing--I know how hard it is to get people to agree on things :) As an aside, did I correctly understand that the RelaxedDependencyAnalysis change doesn't get rid of the monomorphism restriction?
Well, in non-significant-whitespace Haskell, instead of newlines and indentation, braces and semicolons are delimiters. There is a set process used to convert to and from this form. In the above example, by moving the "else" to the same indentation level as the "if," you are making it its own "statement" instead of being part of the `if`. The non-layout equivalent would be putting a semicolon before the `else`. This change makes that legal; that is, it allows `if` to be broken up into multiple "statements."
Because monads in Haskell are broadly similar to the mathematical object called "monad" over a particular category (the category of Haskell types). A monad in the mathematical sense is a far more abstract object, being definable over many different categories in many different branches of mathematics. Furthermore, the Haskell monad isn't even really a mathematical monad in an absolutely pure sense because strictness annotations and the non-totality of Haskell as a language can be used to engineer exceptions to the monad laws even in a trivial context. So basically, the simple answer to your question is that they're called monads in Haskell because they were inspired by a similar object in math with the same name. Worse still, the object in math has the same name as a well known but entirely unrelated object in philosophy. It probably was recycled for no other reason than it sounded both cool and similar to "monoid". Some mathematicians prefer to call them triples, but this is a bit of an ambiguous name -- lots of "triples" exist in math (e.g., a probability space) that are unrelated.
Edit: Just reread this and realised that my first sentence is unclear - the desugaring doesn't actually have to take place before compilation - it just defines what the layout means in terms of the grammar of Haskell, which is all defined in terms of the braces and semicolons version of the language. Indentation is desugared into braces and semicolons before the language is compiled, according to the rules in [section 9.3 of the Haskell 98 Report](http://www.haskell.org/onlinereport/syntax-iso.html#sect9.3). The compiler then sees the braces and semicolons version. Desugaring your example gives us: foo x = do { if x then return 1; else return 2 } Currently, this is then a parse error. The new rule means that the ; after "return 1" is accepted. The extra optional semicolon after if allows you to do: foo x = do if x then return 1 else return 2 which desugars to: foo x = do { if x; then return 1; else return 2 }
I've been working on a little library I call YamlDb which saves data in Yaml files (I'm considering switching to a binary format instead, but Yaml allows nice version control integration). You can check it out at http://github.com/snoyberg/yamldb. Warning: it's based on a lot of other packages which have yet to be released properly (data-object 0.2.0 for instance). It currently saves straight to Yaml files and handles concurrency by dirlocks (look at the sourcecode; it's a very primitive idea). I could imagine having it backup to an HDBC database instead. However, this is only intended for the most basic of purposes. Nonetheless, that happens to fit in nicely with 90% of web development, so I might end up releasing it when it's ready.
But then virtually no one used them. There are some poor design decisions that should really be fixed that people *do* actually use.
I got as far as writing the socket connection in my attempt to port the proxy, but, I figured it was too much work without a legitimate language binding. Passing the strings around and parsing them is just too goofy and burdening. I then researched how much work it would require to create a binding.. oh man, that was even more work. I needed to practice c++ anyways ^_^. I looked at a little code, and man -- I really wish I was using haskell. Oh how I long for zip/filter/map when I code with STL.
It's a C++ library that's only compilable with VS9 :) I think a couple people are working to get it compilable with mingw. That would make a binding a little easier to develop...
 * Right now, we will start forming a Caleskell 2011 committee to mange the process of deciding on changes for the new Caleskell revision. No committee will discuss how to go about finding a Caleskell committee (the plan is to have Cale appoint members directly). Don't expect more details very soon. * Not everyone can participate in the Caleskell 2011 process. Before you can be considered for appoint you must be able to correctly infer the type of `fmap fmap fmap fmap fmap` by hand. Solutions should be posted to http://haskell.org/haskellwiki/fmap-fmap-fmap-fmap-fmap
I would have thought at the very least they would have added Multiparameter Type Classes and killed off the Monomorphism Restriction. This took 12 years? 
That becomes an unambiguous syntax error, rather than an impossible-to-parse-but-legal expression.
ok, I'll bite. Such as? The MR?
I'm not demeaning the H10 work at all, by the way. I realize that my earlier comment sounded very negative, and do certainly appreciate all the work that went into the new standard. Having said that, what I was referring to is how "ad-hoc" the typeclasses feel in the standard libraries. Even if we don't get cleaner numeric/algebraic typeclasses (which nobody can agree on a good way to do) it seems fairly clear that Monad/Applicative/Functor should be a hierarchy, removing (or at least deprecating) duplication of library functions (e.g., &lt;$&gt;, fmap, liftM, liftA, ack!!). There are other issues with Enum's semantics being unclear, and even if we stick with the oddly-shaped Num typeclass, it should be fairly simple to remove the arbitrary Show and Eq superclasses on it without breaking too much code. But overall I like where Haskell is going, so I'll stop bitching and start working on trying to improve things :) 808140 might have other complaints though.
I'm a bit confused by the `DoAndIfThenElse` proposal -- I learned Haskell from the internet, and thus write my do-if-else statements like: foo = do if someCondition then bar else baz Is the proposal just to allow alternative syntax forms?
People with only a superficial understanding of those terms will misunderstand the "monoid in the category of endofunctors" quote anyway. The notions are useful independently, but even avid users of the Monoid typeclass might not fully understand that the quote refers to monoid objects in monoidal categories, which are a generalization of the "associative + identity" notions in algebra.
&gt; I only have the vaguest grasp of how monads work Monads don't work in any particular way. A monad is just a type constructor that fits a certain pattern that we call "Monad". That pattern is a particular signature and set of laws. Similarly for all type classes.
I agree with you on all that -- however what you're pointing to are library changes, not changes to the core language. The idea, as I understand it, is to move those specifications to a separate process. However, I don't think that there's any serious traction in that regard, despite how many people agree on the general issues. In general, folks tend to propose fairly drastic changes -- for something to take off, the changes would need to garner widespread community acceptance, probably though keeping them modest and incremental, in line with the haskell' process.
The biggest problem I have with HDBC is that it's dog slow, at least with the MySQL backend. I tried to run a medium-sized query (~600,000 rows) to generate a chart and ctrl-c'd the process after a minute or so. I resorted to using a python script to dump the query results to a text file, total process time: 15 seconds.
He's also the author of hack, which I wish more of the Haskell web community would get interested in...
Yeah, it's just to make it look more like it would look in other languages. Harmless and saves some people some confusion. 
What? That's it? Where are the multi-parameter type classes? What about GADTs? Where are all the features that everybody's already using besides these trifles?
Even worse than I thought. I'll avoid it unless I hear of progress taking care of the un-fun aspects.
Why not.. (sorry, no wiki account) * fmap :: Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b * "a" becomes (c -&gt; d) * "b" becomes (g c -&gt; g d) * fmap :: (Functor f, Functor g) =&gt; ((c -&gt; d) -&gt; g c -&gt; g d) -&gt; f (c -&gt; d) -&gt; f (g c -&gt; g d) * fmap fmap :: (Functor f, Functor g) =&gt; f (c -&gt; d) -&gt; f (g c -&gt; g d) * "f" becomes ((x -&gt; y) -&gt;) * "c" becomes (h x) * "d" becomes (h y) * fmap fmap :: (Functor g, Functor h) =&gt; ((x -&gt; y) -&gt; h x -&gt; h y) -&gt; (x -&gt; y) -&gt; g (h x) -&gt; g (h y) * fmap fmap fmap :: (Functor g, Functor h) =&gt; (x -&gt; y) -&gt; g (h x) -&gt; g (h y) * fmap fmap fmap = fmap . fmap * "x" becomes (s -&gt; t) * "y" becomes (k s -&gt; k t) * fmap fmap fmap :: (Functor g, Functor h, Functor k) =&gt; ((s -&gt; t) -&gt; k s -&gt; k t) -&gt; g (h (s -&gt; t)) -&gt; g (h (k s -&gt; k t)) * fmap fmap fmap fmap :: (Functor g, Functor h, Functor k) =&gt; g (h (s -&gt; t)) -&gt; g (h (k s -&gt; k t)) * "g" becomes ((u -&gt; t) -&gt;) * "h" becomes ((s -&gt; u) -&gt;) * fmap fmap fmap fmap :: Functor k =&gt; ((u -&gt; t) -&gt; (s -&gt; u) -&gt; (s -&gt; t)) -&gt; (u -&gt; t) -&gt; (s -&gt; u) -&gt; k s -&gt; k t * fmap fmap fmap fmap fmap :: Functor k =&gt; (u -&gt; t) -&gt; (s -&gt; u) -&gt; k s -&gt; k t * fmapToTheFifth ut su = fmap (ut . su) 
I have a working socket interface [here](http://code.google.com/p/bwapi-hsproxy). I'll look into posting it on hackage at some point.
"unless you happen to like having proofs for breakfast." I like code for breakfast, and by Curry-Howard they are the same thing!
Apparently GADTs are tricksy enough that some folks are squeamish about them. But MPTCs and Rank2/RankN really should have made it in. My guess is that 2010 is just to get something rolling out the door, and so they're doing little more than the standard H98+HM+FFI everyone means when they say "H98". Hopefully with this progress made they'll add standard extensions like MPTCs by 2011. (The tricky bit about MPTCs is the fundep vs associated-types issue. Raw MPTCs are annoying to use for many of the things we want, but fundeps are tricksy and associated-types haven't been around long enough for people to feel comfortable with yet.)
But n+k was mostly left in the H98 report for political reasons (gasp! amazement!). It seems those politics have since grown old and died, so now we can follow through on the H98 comment that n+k will "likely be removed in the future". Even just having that comment in there spelled the death knell for n+k since who would want to depend on something presumed deprecated?
But since Monad is part of the Report (it must be for desugaring the do-syntax) that means the Functor/Applicative/Monad issue does belong in the Report. ...Unless some other mechanism is included that allows another way to get around the problems (and noone can agree on the right one of those).
And Kleisli triples aren't technically the same thing as monads. There's an isomorphism between them, but monads use `join` whereas triples use `bind`/`(&gt;&gt;=)`/`(=&lt;&lt;)`. Minor technicalities, but potentially worth mentioning.
Don't you mean `(.) (.) (.) (.) (.)`?
Arguably the fail issue does. But outside of that, if you have rebindable do notation (as does ghc) then libraries can define monads however they like. Potentially rebindable syntax can also handle pulling out fail into a distinct typeclass as well...
My guess is probably not. Futexes are used on Linux to implement pthread stuff like mutexes, condition vars etc. I doubt there is much to be gained from going down a layer and using futexes directly.
Meh. I was hoping for tabs to get outlawed.
Not directly, but they might be useful in implementing a Haskell RTS. A futex is nothing more than a way to sleep in the kernel until a co-operating task wakes you up; the only thing they add over a SIGSTOP/SIGCONT is verifying that a 32-bit integer has not changed before you go to sleep, and allowing your co-operating task to wake you without knowing your PID (it just needs to share memory with you). They're a useful building block for mutexes, condition variables etc, and might come in handy for building a high performance STM implementation, but they've got no direct link to Haskell the language.
It's not new, why don't you add some fun stuff to your Functional Parsers http://channel9.msdn.com/shows/Going+Deep/C9-Lectures-Dr-Erik-Meijer-Functional-Programming-Fundamentals-Chapter-8-of-13/ Like casino vacation see for more - http://www.casinoagenda.com/casino-vacation.html
In all the mathematical treatments of monads I've seen, they are constructed using unit and join; in fact, it wasn't until I started playing with Haskell that I saw a construction using bind. (This is just my anecdotal experience and shouldn't be construed as a refutation of your point.)
Haskell's gotten a lot more buzz recently than O'Caml.
I did it a long time ago, I even put some GUI bits up: http://community.haskell.org/~ndm/hat/ - that includes a link to a Windows binary
SBCL uses them. This: http://sbcl-internals.cliki.net/Removing%20futexes implies some reasons that probably also would apply to Haskell. Particularly, they note that interactions with GC are easier with Futexes than pthreads.
Heh, sorry, I'm a reddit n00b. Fixed now.
A string parsing problem in Haskell? Sounds like a job for [Captain Parsec](http://www.haskell.org/haskellwiki/Parsec). [Here's](http://book.realworldhaskell.org/read/using-parsec.html) a good place to get started.
If you read his source he's already using Parsec for the string parsing. Serialization over the wire using strings is still nonsense though.
Ah, maybe so. Then again it might be less painful than converting between Haskell and C++ strings *and* binding to C++.
But the rule is to insert a } so that a syntax error can be avoided.
Did you use the strict functions, or the lazy ones ? (quickQuery vs quickQuery')
Infix expressions extend as far to the right as possible in the new syntax, so that rule no longer applies in this example.
Here you go: http://haskell.org/haskellwiki/Development_Libraries_and_Tools
That is an awesome explanation; thank you so much. But I'm not sure your `bool` quite works...
There is not much of a difference between using continuations directly or wrapping them in a monad... Note that this code doesn't support recursive definitions, like for example alice = write 'a' $ read $ \c -&gt; if c == 'b' then return 1 else alice bob = read $ \c -&gt; if c == 'a' then write 'b' bob else return 2 
Without infinitely recursive types, your recursive example is not type-safe. One nice aspect of the original is that the types are checked between sender and receiver (the original is essentially a very basic session type).
Oh god, someone donate a copy of Excel to this man. That thing needs graphs.
Either. I'm planning on benchmarking it to confirm this when I get some time.
mind = blown dan needs to write a blog poast about this
May the best array library win. No really, thin the herd. They're all so attractive yet subtly different. It's infuriating. 
Please correct me if I'm wrong, but isn't this fairly typical Scheme-style code, and don't monads make a nice abstraction for this sort of thing? Why avoid them?
Looks neat, but also monadic..
what does it mean for apfelmus's code not to be type safe? can you give an example of self-in-foot shooting?
Monads aren't quite adequate because the type of the continuation changes from line to line. You need indexed (or parameterized) monads: http://blog.sigfpe.com/2009/02/beyond-monads.html
"Warning: This lecture may contain the use of the term Monad. Do not fear. Everything will be OK. :)" :) 
That was really cool, thanks!
Dang. I'm diggin the tropical theme in the thumbnail
You can try to load my example in GHCi, the type checker will complain.
&gt; Without infinitely recursive types, your recursive example is not type-safe It doesn't type check. Unfortunately, this means that you can't express any recursive message passing protocols. &gt; the original is essentially a very basic session type Yep. It has another flaw besides not supporting recursion, namely the order of `read` and `write` is not checked statically. In other words, bob = read $ \x -&gt; write "Hello, I'm Bob." $ return x bob' = write "Hello, I'm Bob." $ read $ \x -&gt; return x have the same type, but `bob'` will hang when trying to chat with alice = read $ \x -&gt; write "Hello, I'm Alice." $ return x But hey, a lightweight session type in six lines of code is still cool. :-)
like GNUplot
first of all, make Net a newtype, that will save you one constructor per update.
I've never used any of these libraries, maybe because I never understood what they were for before now. I feel the term "array libraries" is a little misleading. It seems they're primarily designed for sequential-access situations where you might otherwise use built-in lists, not `Array`s. Is that right? Is there something to be gained by using, say, `STVector` (from `vector`) rather than `STUArray` if I'm going to do random-access reads and writes?
Typically, you will have either more flexible data type storage, or access to foreign libraries.
It would be great to have something like this integrated with the hackage server so that the RSS feed could give an summary of what changed, rather than just reporting "monadiccp 0.6.1" or whatever.
Indeed. Similarly people ask for changelogs. Want to help work on the new [hackage-server](http://code.haskell.org/hackage-server)?
Thank you for this. I hadn't heard anything before this that pointed out the pitfalls of an optionally-lazy approach. After hearing quite a bit of dissatisfaction with Haskell's choice, it's good to know that there are definitely benefits to lazy-by-default, even if it can lead to trouble. I'm curious to see if there will be any new approaches in the future that somehow mitigate either technique's gotchas. 
I sent you a simple patch to make the RSS feed show changes in the list of exported modules. That's as far as I got without significant additional effort and I think it might be useful. Let me know if you think there's a more sensible way to implement this.
Maybe and List also have a specific relationship: Maybe is like a List that can only have zero or one elements.
What if Scala didn't require the laziness annotations on each function's type, and let a laziness specifier on an argument propagate up the call chain?
I reckon Happstack's state model is pretty nice.
Then parameters could unexpectedly turn lazy. Not sure whether that's a problem. I guess an argument could be made that since laziness makes performance harder to reason about, unexpected laziness can cause even more surprises.
"Haskell, a language usually reserved for big commercial applications" wow, when did that happen? It's a more appealing stereotype than the usual one, I guess.
I'm curious what the "bookkeeping overhead" is that she mentions in her abstract, which she writes is responsible for the better runtime of the Haskell code.
Is the actual paper available anywhere? 
It seems it was a poster session presentation, so there might not be an actual paper. http://scyourway.supercomputing.org/conference/view/spost112_1 has slightly more detail. It sounds like the Haskell code in question is only about 30 lines long. I'm amused that the linked article refers to Haskell as "an almost 20 year-old dog" when talking about a comparison of Haskell and C!
I strongly discourage the use of 'vector' because... &gt; import Data.Vector as V &gt; &gt; func = V.head V.empty Will segfault, as will many similar but much more complex statements. No typechecking Haskell function that avoids explicit use of unsafe* should segfault instead of throwing an exception. If this requires having explicit checks that are only removed by a special compiler pass with a sat solver then so be it.
Configuring with --flags=EnableAssertions helps. Version 0.4 mistakenly had it turned off by default, I've uploaded a fixed version to Hackage. Next time, how about a bug report instead of or at least in addition to making recommendations? This is open-source software after all.
yeah, that was a bit strange.. 
Presumably Haskell's built-in concurrency primitives are a lot higher level. In C you basically have ad-hoc concurrency abstractions built on the lowest level primitives while in Haskell you already have the higher level of abstraction available.
that doesn't explain anything about relative performance at all, unless the higher abstraction level somehow helps the optimiser.
Well, ad-hoc implementations tend to be less optimized than standard library ones.
Here's to the future protein folding solution in Haskell /me raises glass
&gt; Version 0.4 mistakenly had it turned off by default, I've uploaded a fixed version to Hackage. Oh, I figured this was intentional and the lack of safety was perhaps a result of being overly performance oriented. Thanks for the fix and the now-wonderful package! It was nice before but segfaults make me uneasy - even when its a result of me misusing the library :-). &gt; how about a bug report instead of or at least in addition to making recommendations? This is open-source software after all. I will!
From [this link](http://scyourway.supercomputing.org/conference/view/spost112_1): &gt; Although the C implementation mirrored Haskell’s, the C implementation did not account for problems with the run-time stack, such as stack overflow. Once addressed, the C code ballooned to over 1000 lines of code, more than 37 times longer than the Haskell implementation. I'm not sure what's meant by "the C implementation mirrored Haskell's" but, from the description, I'm inclined to say they did a naïve port of Haskell code to C. If that's the case, stack overflows aren't the least bit surprising because C compilers don't normally do tail call optimization or common subexpression elimination. Ask a C programmer to port their program to Haskell and you'll probably see abysmal results too. A few more details would definitely be handy.
I had to double check whether it actually referred to Haskell as a dog. Shame it didn't mention whether the dog was dead or alive, eh?
All proper C compilers today have done CSE and tail-calls for like a decade. I don't know where you get your facts from.
It looks like I was wrong about tail call optimization, but CSE isn't possible with C unless the compiler can guarantee a function call is pure, and there's no support for expressing that in the C language.
Something about this smells fishy to me. I love me some Haskell, but I can't help but think that the 37x code size of C over Haskell and 2.68x performance of Haskell over C is probably due to a naive port of Haskell code to C.
that'd be version 0.0.2, Don. Let's not get ahead of ourselves.
Hmm, what is the advantage of this over some place like Slicehost? Seems like the latter would be much less of a pain
Indeed. I was pretty sure [Haskell was a cat... (PDF, page 10)](http://research.microsoft.com/en-us/um/people/simonpj/Papers/haskell-retrospective/ECOOP-July09.pdf)
And much more expensive for sure. And I don't know the quality of the service. This had a low barrier for entry, and I happen to really like the ideas of: * splitting up static file hosting from dynamic content generation * paying for exactly what you use Also, this was more work to set up, but I have a feeling that doing all compiling on a local system will be more enjoyable in the long run. Only time will tell.
&gt; CSE isn't possible with C unless the compiler can guarantee a function call is pure, and there's no support for expressing that in the C language. True, not in standard C, but there is in GNU C. There's the [pure and const attributes](http://gcc.gnu.org/onlinedocs/gcc/Function-Attributes.html).
[NearlyFreeSpeech](https://www.nearlyfreespeech.net/) is great, but it's still shared hosting. Nowadays, I use it just to handle my DNS—their [RespectMyPrivacy](https://www.nearlyfreespeech.net/services/respect) service is a very good idea. I also have a slice at [Slicehost](https://manage.slicehost.com/customers/new?referrer=af123aa58d672588598e877e2846b2f6), and I'm very happy with it. No noticeable downtime during the past year; complete control over the system; great service—I was able to resize my slice from 256MB to 2GB just for one memory-hungry compile, then resize it back the next day, without incurring any extra fees.
Good to know, thanks for the link.
I can't remember ever having used this function, but I do have a handful of other functions that I use daily that's not in any standard module. 
Back in the day, I used to call something like this thing trail :: (Applicative a, Monoid (a x), Foldable f) =&gt; f x -&gt; a x trail = foldMap pure which ought to be strictly more general than what you propose, but for tedious reasons is not. Modulo newtype isomorphisms, many handy functions turn out to be "trail" in disguise. I also got a cheap Macbeth gag out if it back then. Note: this operation relies in no way on monadic structure. Meanwhile, your concatMap-like thing is really foldMap if (as always should be so) your MonadPlus arises from a parametric instance of Monoid. Again, nothing to do with monads. So, yes and no. Good operation but the monads are a red herring.
Perhaps it *should* always be so, but Data.Monoid defines a Monoid instance for Maybe a which is not the one corresponding to the MonadPlus instance for Maybe.
There's also ?? :: (MonadPlus m) =&gt; Maybe a -&gt; m a useful for generalizing the type of things like `lookup`. (Not totally analogous with `msum . map return` because we have no MonadZero class.)
ghc provides rebindable syntax for various things including the do notation and string literals. If you wrote a patch for rebindable list syntax I'm sure the Simons would be happy to include it.
That would be a great blog/reddit/Haskell Weekly News/whatever post. I'd extend that invitation to any of the several other people who said similar things. (I'm still in the phase where you extract a recurring function, then three days later discover it already exists in the standard library. :)
...much to my incandescence.
There goes someone's weekend.. :)
Rebindable list syntax would be far more flexible than monad comprehension syntax, I think you could make a case for it. Besides, there's a big difference between a feature being kicked out of the language and a feature being included in ghc. If you have a nice use-case then Simon PJ is pretty easy to convince, especially if there's a patch attached.
Neat, but too bad it's all implemented in C and this is just a wrapper.
The Simons have said they'd accept a patch to put monad comprehensions back in GHC (with a suitable extension flag, of course).
Implementing cryptography yourself is widely considered to be crazy. There are *so very many ways to get it wrong*. No, binding to an existing implementation is the only sane thing. Besides, for something like this, you really do want the fastest possible code - which is not haskell.
There's so much emphasis on making things PureHaskell instead of calling out to wrappers. I think that's a little silly. I use Haskell because I think it helps me make programs smarter and more correct. If I can bind to a C library that is already smart and correct without having to re-implement it in Haskell, that's a win. Haskell does a _very_ good job of calling into C, so why wouldn't we want to take advantage of that wherever possible (and a good Haskell variant doesn't exist)?
The point I was making here is that, while packaged nicely, this is nothing new. We've had fast AES on bytestrings ever since the HsOpenSSL bindings matured. As for the performance, yes I know all too well. I've long felt that crypto/digest operations are excellent benchmarks.
In the short term, it certainly is the only sane thing. But to write a Haskell implementation and have it provably correct would be awesome and useful. And the performance may come with time. I'm not sure how dependent crypto is on the GMP library, but if it is, we already know that Haskell is competitive and sometimes better than the equivalent C code: http://shootout.alioth.debian.org/u64q/benchmark.php?test=pidigits&amp;lang=all&amp;box=1 . Memory usage may still be an issue though.
Where do people keep getting this bizzare notion that they need to install software on BSDs using ports? Ports is the development framework to generate binary packages, end users should simply install those binary packages like you would on any linux distro. Installing from ports is like installing via .srpm, you only do that if you need to change compile time options. Installing ghc should be as simple as: pkg_add -r ghc
Thank you for that, I had no idea. You just made my day with a five-second installation of vim. As far as where I got the idea that ports was the only way, my former boss was a BSD user and mentioned ports on occasion. The way he said it, I assumed it was the only option available.
Ah, its a binding. Now, AES in Coq or Isabelle, extracted to Haskell, and on Hackage. Then I'd consider an upmod. :-) Good work though, the binding looks thoughtfully executed.
EDIT: Oops, it lets you use strict and lazy bytestrings. Keys aren't yet protected with a newtype, but I suspect the API is still evolving. BTW, no good cipher implementation should have anything to do with GMP. These days most digests and ciphers are specified in terms of 32, 64, or 128 bit word operations (and bytes too). Digests are interesting to me - I feel like there is no reason GHC has to produce asm of quality so much lower then GCC in this area. GHC could use more loop unrolling to eliminate many of the index variable/increment operations and some partial evaluation would be a huge benefit as it would allow us to express things like MD5 constants in terms of a vector on their generating function and not manually typing in the constants. Finally, some sort of (fast) coercion of contiguous memory to an array of words would be great - thats one of the slowest parts of PureMD5.
I quite miss monad comprehensions... Cale keeps promising to fix Haskell (mostly by reverting all the changes from 1.4 -&gt; 98, it seems). We're waiting, Cale!
&gt; If I can bind to a C library that is already smart and correct without having to re-implement it in Haskell, that's a win. That's absolutely true, but the problem is that (as anyone who's tried to port a C program to Haskell can tell you) C libraries are often *not* correct -- and I'm not talking deep bugs here, I'm talking about things that a smart typechecker (like Haskell's) would have caught. Part of C's problem is that it's so darned easy to introduce bugs and so darned difficult to prove that there aren't any. C was a major improvement in this regard over the assembler that it replaced, but languages like Haskell and the MLs blow it clean out of the water. There are so many common errors that you just can't make. And languages like Agda and Coq take it all even further. C's principle utility at this point is that it's *fast*. There are times when that's very important; there are other times where correctness is more important. There's a place for both but as computers get faster and compilers get smarter C is losing ground. People still write stuff in raw assembler sometimes -- C will never go away entirely. But its dismal reliability when it comes to writing reasonably complex programs means that people will keep trying to write less of it. That's really why everyone wants pure Haskell implementations of things, I think. People who use Haskell for real programs today (as opposed to just screwing around with it to learn some FP) are mostly people who geek out on high reliability, and to those people, C is something to avoid at all costs.
I fully expected someone to correct me on this, but it occurred to me that my above construction may not be a monad, because `Term` is not a functor. It has a constructor: Lam (v -&gt; Term v) and a constructor: Bound v which puts `v` in both covariant and contravariant contexts, making it impossible to write an appropriate `map`. It may nevertheless be interesting that `Norm` would be the free monad over `Term` if `Term` were in fact a functor, though.
Unless I'm missing something, monad comprehensions don't seem that impressive to me. They're the same thing as do notation, except putting the return at the front of the sequence of commands instead of at the end. Overloading list literals to be ListT or a MonadPlus, that'd be pretty nice though!
If nobody else does it, sooner or later I'll add overloaded list syntax. I need it. 
A good site for elegant R-B tree implementations in Haskell: http://www.cs.kent.ac.uk/people/staff/smk/redblack/rb.html
Not sure why you're ruling out GMP like that. Symmetric ciphers tend to use word-sized operations and asymmetric crypto (Diffie-Hellman, DSA, RSA, the various more esoteric "modern crypto" schemes) tend to use number theory and large integers (which GMP is particularly good at). Now one thing GMP is particularly bad at is being consistently slow :) it's also one thing I envision our supposed provable implementations. It's comparatively easy to prove we meet the spec, but many vulnerabilities are from side channels like cache usage and timing, and those seem remarkably hard to model effectively in any proof system I know of. OpenSSL even has its own bignum implementation because it wants consistent timings for bignum operations. You could probably do some "time-padding" for GMP to achieve similar behavior though.
Actually a compiler can spot "easy" cases of pure functions. For example, those which don't refer to any global variable, which have no argument of a pointer type and call only pure functions.
What a great invention, it is almost symbolic. It reminds me of an old story about (some early 20th century composer) in his school days when asked a tiresome exercise of what progression it would take to modulate from one chord to another given chord, and he replied "I would go directly there! No chords in between!" Hubris is like this, sheer audacity.
In my opinion, the HsOpenSSL bindings are not "mature". There are.. let's see. - A lot of conceptually pure operations (like encryption) lack a pure interface, or lack a pure interface that can be conveniently interleaved with other operations. - I've had random crashes and corruption. Seriously. I'll try to write a testcase for that, but my priority was to get working AES and digests (still working on the latter), not fixing openssl. After looking at what was available, I decided it was easier just to write my own. Frankly, OpenSSL is over-complicated if you just want AES.
Strict bytestrings? Where? If you look at the raw (Codec.Crypto.AES.IO) interface then yes, that only covers strict bytestrings. However, the other two modules let you use lazy ones. Only in CFB mode though, really. I should get CTR working one of these days.
What 808140 said, plus: many C libs have just awful APIs. Things which you cannot polish into a nice Haskell API. Usually the culprit will be mutable state, or nasty initialisation requirements (aka more hidden mutable state). Take for example the mersenne-random package on hackage. Don did a good job with polishing this one, but note how all the operations are in IO. Yes, not ST. That's because the silly C code uses a single global generator state. If you've got multiple bits of code that need reproducible sequences of random numbers then forget it, this can only generate one sequence at once.
As long as they're in the same translation unit. Otherwise, the compiler needs a hint via a non-standard extension, like dcoutts pointed out.
If I'd known I was going to implement it this soon, I wouldn't have uploaded the 0.0 series at all, but.. 0.1.0 is up, now with CTR and OFB modes implemented. That just about completes the library, I guess.
Good points (you and dcoutts). When things absolutely have to be correct, then staying away from C is a wonderful idea. Though, I would reemphasize that it's not a bad thing to use good C libraries (should such phantom's exist).
See: http://www.reddit.com/r/haskell/comments/a9jq6/aes_fast_aes_encryptiondecryption_for_bytestrings/c0giy2e Wish I could reply to both comments.
Or it can store such information in object files, as for link-time optimizations. Anyway, that is why I said "easy" cases : it is always safe to assume the worst case and spot every function as impure.
HsOpenSSL is a pain to work with... I'm taking a crack at bindings to libgcrypt... the C codebase seems better than OpenSSL C codebase.
Fair point, but this thread start with a symmetric cipher. Besides, if we got AES performing 1/2 the speed of AES in C and RSA performing faster many people still wouldn't be happy because for their performance issue revolves around the block cipher and digest speed.
I often interpret MonadPlus instances as nondeterminism monads and frequently call this function anyOf (inspired by a similar function in SmallCheck.)
Who's the Bos?
It's not impossible to get a concrete [Integer] from [a], or even surprising. A very viable function of type Ord a =&gt; [a]-&gt;[Integer] would be a function that takes a list of a and returns the sorted list of indexes. Given "bdca" return [4,1,3,2] where the numbers represent the positions of the values in the original list, but sorted by their Ordinal values. Of course, you could also go trivial: f :: Ord a =&gt; [a]-&gt;[Integer] f (x:y:_) = if x &gt; y then [0] else [1] 
It's not really detecting an infinite loop, it's just saying that the output list elements have no relationship to the input list elements. Of course once you start looking at why that is for this particular code, you realise there happens to be an infinite loop. mergesort xs = [] would also get a similar (albeit slightly more general) inferred type, but it doesn't have an infinite loop.
The type [a] -&gt; [b] means if it returns at all it must return a list of elements of type "b". As you note there's one terminating function that sneaks into that type, always returning nil to avoid inventing a 'b'. A little more information like seeing a cons in tail position in the function is enough to be sure it's an infinite loop. A stronger type like [a] -&gt; b absolutely tells you the function will never return normally (without using anything "unsafe")
Please do.
Right, but you still can't use the type to differentiate between failure due to calling error or a pattern-match failure and failure due to non-termination.
&gt; It's not really detecting an infinite loop Well, by parametricity (that is, naturalness in the category of types) there is "morally" only one term of type a -&gt; b, and that is non-termination or bottom: # let rec bottom x = bottom x ;; val bottom : 'a -&gt; 'b = &lt;fun&gt; 
Except that (a) the return type is Ord b =&gt; [b] not b so there are other possible terms of that type, and (b) parametricity doesn't distinguish infinite loops from other errors.
(a) I do think that the type (forall a. Ord a =&gt; a) is the same as (forall a. a) -- inhabited only by bottom (Since each of the Eq a/Ord a methods have "a" only in negative positions). The type [Void] however, is of course inhabited by []. So the type is only inhabited by (const []) or bottom. (b) You are right -- however, if you combine a Void type result with knowledge that none of the operations used could yield any error besides non-termination (all pattern matches are exhaustive, and no "error" or "error" calling function is used), then indeed you know that only non-termination is possible. Since this isn't a Void type, we know the function can only non-terminate or return an empty list.
(a) Agreed. But (forall a . Num a =&gt; a) would have a lot of terms, so you really do have to look carefully. (b) Agreed. But once you've read the code that carefully, you're also likely to spot the non-termination directly anyway. Arguably the compiler could do the checks itself, but it (GHC) doesn't actually do this for you.
You know, having a type-tag to distinguish normal values (that may all have non-termination bottoms) from pure-exception-capable values could be really nice. Of course we already have a way to flag such pure exceptions (Using a Maybe or an Either error type), but for some reason people are highly resistant to (/) having the type: Fractional a =&gt; a -&gt; a -&gt; Maybe a. So maybe Fractional a =&gt; a -&gt; a -&gt; %a or such could garner less opposition.
I believe his point was that a sort function should alway just return a permutation of the input.
Link to the [pdf](http://www.ittc.ku.edu/~andygill/papers/draft-chalkboard-ifl09.pdf).
A better answer to this is a dependent type system that allows you to specify that the divisor of (/) not be zero.
Thus forcing all callers to provide a proof that the denominator is not zero as a parameter? :)
Hehe, yeah, something like that!
Chameneos is a sequential concurrency benchmark (shared access to a resource). Computational overhead isn't the main cost, it is the scheduling of meetings, as a result, the lean, fast, single core concurrent runtime has better performance, better locality, cheaper threads. I'd be surprised if there's a gain to be had moving this to multiple cores, (as I'd be surprised if we could get a speedup out of thread-ring for similar reasons). Try it!!
Yeah, I was looking at the threadring benchmark and seeing it run on one core made me come to that conclusion for that benchmark. But when I saw chameneos, it surprised me to see other languages using all cores. Comparing to the single core results, though, I guess C++ might not have gained anything from running on multiple cores. It still confuses me why almost all languages chose to go the multiple core route, though. How did C make such a leap between the single core benchmark and the quad core? Just some lucky insight? Could we gain by applying this to the Haskell code? &gt;Try it!! Ha, maybe once I'm through Project Euler I'll feel confident enough to fiddle around with language shootout benchmarks :-).
C++ lacks light-weight threads. It's always using a number of kernel threads equal to its number of language threads, which means concurrency is always in effect, and it gets the cost of synchronization even on single-core processors. With that being the case, it's not really a surprise that it got an improvement on the quad-core, but in this case Haskell has the better system.
Are you saying you'd rather have First be the default instance Monoid (Maybe a)? Why? 
Monads are the same thing as triples in regular category theory. Triples are an older name, I believe. When you get to higher category theory, in addition to the "monads on a category" a.k.a. triples, there are "monads _in_ a 2-category"*, which consist of: 1. a morphism/1-cell t : A -&gt; A 2. a 2-cell eta : 1_A =&gt; t 3. a 2-cell mu : tt =&gt; t all of which must satisfy appropriate laws. A monad in the 2-category of categories is a monad on a category from regular category theory (a.k.a. a triple), but you can have monads in other 2-categories, too, and these are not typically called triples as far as I know. You can have monads in n-categories (and they correspond to monads _on_ (n-1)-categories), as well, but I'm not sure what all is done to extend the definition. * A 2-category is like a category, except in addition to objects (also called 0-cells) and morphisms (also called 1-cells) it has 2-cells, which are arrows between the 1-cells, and can be composed in a couple different ways, with some laws governing which composites are equal. In general, n-categories are like (n-1)-categories, except they have n-cells that go between the (n-1)-cells, and which can probably be composed in n-ways (I'm not sure about that last bit).
Haskell's still slower by a factor of almost 9, though. Must be some fat on it somewhere.
(1) We should invest datatypes with a semantics that's more than just the bare fact of what they represent. Conceptually, Maybe represents computations with (uninformative) failure. The associated monoid structure is "first success wins". (2) The current "adjoin a zero" monoid is, ahem ... of questionable value ... if the underlying set already is a monoid and has a zero of its own. (3) The existing MonadPlus/Alternative classes need only introduce new operations (as opposed to new laws) because we can't write (forall a. Monoid (m a)) as a constraint. One day, we may be able to tidy that up. Until then, we should make all these variations agree, for sanity's sake. I propose that the only good MonadPlus or Alternative instance is exactly the one that tediously duplicates the monoid. (4) If we have a choice of monoids for (m a), it should be the generic monoid which comes from m (think lists, for example) rather than some lifting of the monoid on a. After all, the head of (m a) is m. Of course, the m-lifting of a should also be available (e.g., idiomatically). Related rant about not being able to tell values and computations apart, even when it would be useful to do so, saved for another time and probably another programming language. (5) In the main, Data.Monoid's structural rules for lifting monoids through functions, pairs, etc, do a good job. They often give me the monoid I want to use (as an Alternative or MonadPlus instance, typically) for free. Except for exceptions, which aren't what Maybe's about when it's only a monoid. The library monoid for Maybe (sorry, Maybe a, given Monoid a) has never been anything other than sand in the vaseline. Convince me otherwise: show me more people using it than working around it. I work around it all the time. And now I teach around it, which adds embarrassment to irritation.
Note that the straight-forward C version is 17 times slower than the good C version. 
Many years ago I made a class in Bluespec for hooking modules together. It was called Connectable, and the operator was (&lt;=&gt;). Great minds... :) 
If the List monad is the initial object in the category of Haskell additive monads, what is Foldable? Looking at the parallels it would be the initial object in the category of Haskell pointed monoids, except Foldable isn't an object. Maybe an adjoint?
Actually, Marlow had said they'll even accept a patch to re-integrate monad comprehensions. I took a look at the effort that would involved and since that part of the compiler has had a lot of work on it to support parallel list comprehensions and uses a form of list fusion, found that it'd be a fair chunk of work to get monad comprehensions integrated well. I promptly found something else to do. ;)
This [very interesting and easily comprehensible post by Dan Doel](http://pauillac.inria.fr/pipermail/coq-club/2008/004168.html) on the Coq-club mailing list that I happened upon quite by accident led, after quite a bit of searching, to the above. I also found [Anton Setzer's notes referenced in the above link](http://www.cs.swan.ac.uk/~csetzer/slides/wessexSeminarMarch2009.pdf) but found it to be beyond my ability to easily understand, mostly because I don't know enough about this topic.
I think I was actually referring to some agda mailing list traffic by Setzer in that mail (since the pdf appears to post-date the coq-club thread). However, most of the relevant stuff is there. When I talk about: cozero * = inl () (cosuc n) * = inr n That corresponds to definitions in his slides like: case cozero = inl () case (cosuc n) = inr n Where you're defining corecursive functions/values by specifying what observations they produce. There's actually a nice duality between data and codata definitions if you stick with that. data makes it nice to have sums of constructors, eliminated by cases: data List a = Nil | Cons a (List a) null Nil = True null (Cons _ _) = False codata makes it nice to have products of coconstructors, introduced by cocases (?): codata Stream a = Head a &amp; Tail (Stream a) Head (repeat x) = x Tail (repeat x) = repeat x There's still a bit of disparity I haven't figured out how to resolve yet, though, since data is sum-of-products, but product-of-sums for codata doesn't seem as nice (the sums part is lacking).
Thanks for trying to explain it to me. I'm going to have to think about this more before I understand it, that's for sure.
I believe the n-category extension is just having mu an n-cell, eta an n-cell, t an (n-1)-cell. But then I haven't done much higher-order CT outside of the obvious 2-category from regular CT. Triples are indeed the old name. However, I'd still contend that ---isomorphisms aside--- they're not quite the same thing. Partly this is a philosophical issue about how we make definitions and to what they refer, but those philosophical issues do have ramifications for what we mean when we choose to use a particular term. It's like how Newtonian mechanics (as defined by Newton) disagree with relativity theory, even though "Newtonian" mechanics (as taught today) are remarkably similar to Newtonian mechanics and are a subset of relativity theory. As I recall, Kleisli triples were a pre-categorytheoretic notion and it is only through their categorification that we can say they're the same thing as monads. Though the distinction here is less important than the Newtonian case because (so far as I'm aware) no modern text uses "triple" as the term of choice so we needn't worry about distinguishing triples from categorified triples.
Here are some slides and Haskell code based on this paper: http://web.cecs.pdx.edu/~sheard/course/AdvancedFP/notes/CoAlgebras/CoAlgebras.ppt http://web.cecs.pdx.edu/~sheard/course/AdvancedFP/notes/CoAlgebras/Code.html
I have definitely needed this one recently, but usually at the end of a sequence: thingIReallyWantToReturn =&gt;&gt;= thingINeedToDoAfterwardsToCleanUp Another one I think is really handy: foreverFeed :: Monad m =&gt; (a -&gt; m a) -&gt; a -&gt; m b foreverFeed f x = f x &gt;&gt;= foreverFeed f
There's a space in the URL. [Here's a working link](http://blog.finiteimprobability.com/2009/12/03/lambda-calculus-compiler-part-iii-first-order-functions/)
Your =&gt;&gt;= is the same as [(*&gt;)](http://haskell.org/ghc/docs/latest/html/libraries/base/Control-Applicative.html#v:*%3E) from Control.Applicative.
Ah, so it does come standard. Neat. Unfortunately, it doesn't seem to work out of the box for me. I've been using a type synonym: type Compute = StateT Context (Writer [String]) How would you get the type system to allow you to use (&lt;*)? instance Applicative Compute ... doesn't work for me, as I get a kind error. I've played with newtype a bit, but just don't understand it well enough to make things work. A pointer here would be lovely.
I use this at the end of a sequence, as well. It works just fine. Edit: Interestingly, though, I cannot use (&lt;*) as the last connector in a sequence. Hmm.
You can just use the trivial instance cribbing from Monad: instance Monad f =&gt; Applicative (StateT s f) where pure = return (&lt;*&gt;) = ap There's a fairly obvious way to generalise this to get an Applicative out of every (Monad m, Functor m) =&gt; m, but it requires UndecidableInstances, and writing it will cause a thousand mathematicians to immediately descend upon you for inverting the typeclass subset hierarchy.
I really don't think you want a string like type class with all the methods like head, tail etc. Pick the right string type for the task and convert on the boundary if necessary. So a string-like class should support conversions and nothing more.
 newtype Compute a = Compute (StateT Context (Writer [String]) a) deriving Monad instance Applicative Compute where pure = return (&lt;*&gt;) = ap Requires `GeneralizedNewtypeDeriving` for the `deriving Monad` part.
I would like to take this opportunity to point out the existence of the [alt-stdlib](http://trac.haskell.org/alt-stdlib/) project. It's been somewhat dead for the past several weeks, but I plan to revisit it once I have figured out some time management skills. In the meantime, I will grant Darcs access to anybody who wants it, and I would like to see some activity in the [tickets](http://trac.haskell.org/alt-stdlib/report/1). Also, stop by #alt-stdlib on Freenode for discussion, or generate some wiki activity on Trac.
Since StateT s m is a monad, shouldn't making a type synonym qualify Compute as a monad?
As long as that `...` is `where`, you shouldn't be getting a kind error... you do have to enable TypeSynonymInstances, though. Question for the gallery: Is there some reason that the mtl monads don't have Applicative instances? I notice the ones in the transformers package do. Is it worth updating mtl at this point?
Yes, but a newtype is not a type synonym.
You're right. I was accidentally applying to the wrong type. Meant to update the last post. The -XTypeSynonymInstances works perfectly.
I wonder if it would be revealing to design a language with only nonrecursive type definitions but equipped with initial algebra and final coalgebra type constructors. As I understand it, Agda distinguishes between data and codata even when the definition is nonrecursive, even though the two are mathematically equivalent.
I should probably add that it's not the best form to define your own instances of standard library classes for types imported from another module, since the next version of the module may include its own instances for those types, at which point your instances will overlap. If you wrap your monad in a newtype and define the Applicative instance on the newtype, this won't be a problem. (You'll just end up with code that could be replaced by generalized newtype deriving.) In my opinion, it's not necessary to worry about this kind of thing unless you plan to make your code publicly available.
Standard hoogle plug: http://www.haskell.org/hoogle/?hoogle=Monad+m+%3D%3E+m+a+-%3E+m+b+-%3E+m+a Without the `Monad m` constraint, hoogle does even better: http://www.haskell.org/hoogle/?hoogle=m+a+-%3E+m+b+-%3E+m+a
This could presumably be cleared up once and for all if this proposal goes through: http://haskell.org/haskellwiki/Functor_hierarchy_proposal Having the proper hierarchy for all the control classes would be quite nice. Also related would be having MonadPlus and ArrowPlus be replaced as they are just Monoid extensions to Monad and Arrow. See here: http://haskell.org/haskellwiki/Class_system_extension_proposal
To be honest, I doubt you will find papers that compare OCaml with Haskell from a CT perspective. You might as well learn Haskell and do the comparison yourself. Philip Wadler has a lot of good papers that were influential to the design of Haskell on his [home page](http://homepages.inf.ed.ac.uk/wadler/).
I'm not looking for books / papers that compare OCaml to Haskell, just books / papers that explain FP in terms of CT / any other applicable areas of mathematics. Thanks for the link to Wadler's page though; he has a huge number of papers on his site, do you recommend anything in particular?
Just yesterday Erik Meijer recommended "Theorems for Free" during his online lectures on haskell. You might also like Wadler's papers on [monads](http://homepages.inf.ed.ac.uk/wadler/topics/monads.html) which do apply CT concepts to Haskell. 
Thanks for replying. I take it you're talking about this? http://www.amazon.com/Category-Computer-Scientists-Foundations-Computing/dp/0262660717
Yes, I've thought that might be nice myself. Charity was very based in category theory, but even it didn't have quite that type of thing. I guess you can technically think of even non-recursive definitions in Agda as fixed points of constant functors. So: codata CoUnit : Set where counit : CoUnit is a final coalgebra of F X = counit, and thus is inhabited by opaque elements that all produce the same observations (so, extensionally it has one element, but probably not intensionally, though I'm not certain).
This is pretty effing awesome. &gt; This article may be too long to read and navigate comfortably. Please consider splitting content into sub-articles and using this article for a summary of the key points of the subject. What happens if this happens? Do the Wikipolice come along and break things up? Would we end up with 15 different sub-articles? Is that legit? Do other languages have this?
Hooray! It's finally declared stable. Perhaps we can get rid of the 1.13 vs 1.19+ divide we've had for the last few years.
Usually, longer sections will be moved to a separate article, with a summary paragraph remaining under a link. See the articles on [C](http://en.wikipedia.org/wiki/C_%28programming_language%29), [Java](http://en.wikipedia.org/wiki/Java_%28programming_language%29), [Python](http://en.wikipedia.org/wiki/Python_%28programming_language%29), or any other popular language for examples.
What award did it win?
This is very nice. I've had to explain Haskell to a few friends and I've seen them looking at the Wikipedia article and getting confused. Sadly, I'm not qualified to correct the article.
&gt; Occasionally, I have something silly or clever to say here. Usually I give a summary of the weeks news and end it with a bit of a cliche tagline, Yeah, the thing is, I'd rather you didn't and I don't expect I'm on own in that. The previous editor didn't see the need. This is a newsletter about activity in the Haskell community not your blog. I'm grateful for the effort you put in and everything but I don't know who you are and I don't care. 
Please don't use link cacheing sites like bit.ly -- it's part of reddiquette 
stop being such a douche. jfredett: don't mind this guy.
Oops. Sorry, I just read through the reddiquette and will take note of this. Thanks.
That's ridiculous. Take the case of HTML escaping. All it requires is a concatMap. You expect someone using lazy bytestrings to convert their data into a strict text, do the conversion, and then back? Who's to determine *the right string type* for each operation? Do you have any concrete reasons why I don't want the type class besides baseless assertions? I think we have enough of those already. Side point: bytestring-class is already available for those types of conversions, and the upcoming convertible-text will support all five types (String, strict/lazy bytestring, strict/lazy text). Even with those libraries available, I still want the type class.
I believe the adage is, "You aren't doing it right if you aren't pissing at least half your readers off."
How would you compare it to [yst](http://hackage.haskell.org/package/yst) ?
The developers were awarded Most influential paper, ICFP 1999.
The quotes are good this week. &gt;I swear that most of higher-dimensional category theory must have been arrived at by some guys sitting around in a room with a blackboard and saying "What if a drew a diagram like *THIS*!?" and drawing some insane scribble up on the blackboard, and then everyone tries to figure out how to turn it into meaningful mathematics.
I haven't used either one, but after a quick perusal, it seems that there are two main differences: * hakyll uses a more powerful xmonad-style Haskell DSL for its configuration, while yst uses static YAML. * yst uses the more powerful HStringTemplate, while hakyll uses Text.Template from the template package. It would probably be pretty trivial to change hakyll to use HStringTemplate. Or even better, abstract templating so that it's easy to plug in your template library of choice. Text.Template could still be a nice simple default.
There were plenty (I had to cut out about half of them) to choose from. You folks should be this funny more often... :)
Right, I'd accidentally swapped them around (now fixed). Triples are the ones with bind; monads are the ones with join (and map).
Comments? Is this the kind of news journal you could use to keep your friends informed of what is going on in Haskell? (more so than HCAR or the HWN?) 
I read hwn only for the quotes
Could you say more? What kind of news summaries would make sense?
I don't need summaries - I browse all those /r/haskell posts and sometimes even read cafe threads. but quotes are funny and I can't be on #haskell 24/7.
The thing is, you _shouldn't_ need HStringTemplate, because you can do the text manipulations in the configuration file. That being said, it probably could still be handy in some situations, and I'll see if I can put it in.
I, for one, enjoyed this. Granted I follow /r/haskell so it wasn't new news, but I liked how it felt more like someone was telling me what's going on, rather than a "headlines" approach. 
If you really want to jump in at the deep end, read Abramsky and Jung's survey article on domain theory. They explain it in category-theoretic terms, so I'd advise using Awodey's book on category theory as a reference (it's easier than Mac Lane, unless you're already familiar with topology or algebraic geometry). Also, actually prove the theorems they just sketch the proofs of -- you won't get why domain theory is interesting unless you know (a) how to use it to model nontermination, and (b) the inverse limit construction of recursive types. Since (b) is what Dana Scott did instead of proving the independence of the continuum hypothesis, it's one of the central mathematical ideas in programming languages. 
how about extra quotes/uncensored edition? where do you get them from anyway?
yst focuses on separating data from layout. Data is stored in YAML or CSV files, and yst includes a simple query language that allows you to extract the data you want to use with a given template, all without doing any Haskell programming. hakyll doesn't seem to have anything like this, though of course, since the configuration file is Haskell, you can program your own data extraction. That's the most fundamental difference, as far as I can see (I'm the author of yst). 
Well, some of them don't make sense or are attributed to people which I cannot corroborate. I get the quotes via lambdabot (sortof), there is a tool which scrapes the logs for #haskell (as far as I can tell, I'll be honest, I haven't looked through the source much, it's one of the few tools that has (basically) no quirks) and finds the @quote'd lines. TBH, the unincluded quotes are generally just the ones that either don't make sense, I can't corroborate, or are unabashedly unfunny. For instance, there was a quote from Jon Harrop this week, usually those are pretty silly, but this one was just dumb. I try to avoid those ones. 
interesting, I was unaware of that.
Excellent. I have no idea what you are talking about, but I look forward to understanding it. Thanks for the references.
&gt; You expect someone using lazy bytestrings to convert their data into a strict text, do the conversion, and then back? No since the appropriate type in that case is a lazy packed string. A strict one can be converted to a single-chunk lazy one cheaply. &gt; Who's to determine the right string type for each operation? The person writing the operation. They should pick the most general one matching the requirements of the operation. &gt; Do you have any concrete reasons why I don't want the type class besides baseless assertions? It's likely to be slower and/or require more compiled object code to parameterise the function at that level than using the right type and converting on the boundary.
I use FAUST for some of my work so I find this pretty interesting. I was wondering if a more well-established language like Haskell might make a better host for the compiler, so this is interesting. The Heist syntax isn't nearly as nice as the FAUST block diagram algebra but it looks almost perfectly translatable. I wonder even if a FAUST-&gt;Heist-&gt;FAUST translator might be interesting, since you'd get the benefits of the static analysis of Haskell along with the syntax of FAUST. It seems to me like it'd be a fairly trivial translation.
&gt; lilac Says: &gt; Fully-automatic instance generation is almost certainly a mistake About the Applicative auto-generation from Monad, I disagree. Any type whose Functor/Applicative instances disagree with the Monad instances is already broken :-) 
Well, Billy, when a left adjoint and a right adjoint really love each other...
You'll rue the day you stole my comment :)
In general, it's best to try to fix the original functions to return values in Maybe rather than throwing exceptions, but that isn't always possible, and that's what this package is for. There are other packages like it like ChasingBottoms, but I wanted something super-simple and this fit the bill. It makes the purist in me uncomfortable but sometimes you don't have control over your input and validating is expensive. As an example of its behavior: Prelude Data.Binary Data.ByteString.Lazy Control.Spoon&gt; decode (pack [1,2,3]) :: Int *** Exception: too few bytes. Failed reading at byte position 8 Prelude Data.Binary Data.ByteString.Lazy Control.Spoon&gt; spoon (decode (pack [1,2,3]) :: Int) Nothing Prelude Data.Binary Data.ByteString.Lazy Control.Spoon&gt; 
Why is it called spoon?
`try` was taken in the standard libraries, and `attempt` is another package on hackage, so Matt proposed the obvious third choice, and that's what we used.
Unless it's a Tick reference, how is it obvious?
&gt; In general, it's best to try to fix the original functions to return values in Maybe rather than throwing exceptions Isn't there some kind of generic mechanism allowing the caller to decide whether he wants a Maybe, an Either or an exception on errorneous behavior? I think I saw that one a few months back, and that sounds like a better idea. And it would be nice if Spoon could do that as well. edit: there, need to use `fail` instead of `error`: http://www.randomhacks.net/articles/2007/03/10/haskell-8-ways-to-report-errors
I wish the Epigram authors knew that some of us Haskell folks are extremely impressed by their work -- and really really really want them to continue working on it :-)
Well, there was no spoon.
If you backed it up with the failure package, then you could return the actual exception to those who cared. Since Maybe is an instance of Failure, the functions would still return Maybe values when desired.
I considered giving more information on the failure, but this is already treading a fine line on the edge of referential transparency and roconnor would have strangled me if I had given any more information than a Maybe. The lack of information was a conscious decision for my own health :/
Out of curiosity, why do you declare {-# LANGUAGE ScopedTypeVariables #-}?
I was surprised too. For some reason \(x :: Type) -&gt; expr (what I have for the Handlers) requires ScopedTypeVariables. I always assumed that would be a separate extension, as to me scoped type variables meant needing to put forall in front of a type to bring the free type variables into scope for subexpressions. I guess the two are related somehow.
It's true. Maybe is a commutative monad, so it doesn't expose the order of evaluation that the runtime uses. Using Either runs the risk of returning different errors for the same input to a pure function. Edit: I haven't been able to exhibit such a lack of referential transparency yet, but I still think it is a bad idea.
"Do or do not. There is no try." ==&gt; "There is no spoon."
Feature request: functions which catch *all* exceptions, not just the white-listed exception types: catchAll :: SomeException -&gt; IO (Maybe a) catchAll _ = Nothing spoonAll :: NFData a =&gt; a -&gt; Maybe a spoonAll a = unsafePerformIO $ (deepseq a (Just `fmap` return a)) `catch` catchAll teaspoonAll :: a -&gt; Maybe a teasponAll a = unsafePerformIO $ (Just `fmap` (return $! a)) `catch` catchAll
Ahh... yes, you can't normally write that, which has annoyed me on more than a few occasions. I didn't know that ScopedTypeVariables fixes that. All the more reason to like that particular extension, I guess. To be H98, you could write (\x -&gt; expr) :: Type -&gt; IO (Maybe a) which is more cumbersome, but not *too* bad.
Understood. In any event, it's a good idea for a function. I've considered writing a function to grab the Exceptions out of IO and put them in a Failure, but never considered it for pure code. Is it a problem if I borrow the idea elsewhere?
These functions are lying! They are not functions. They are not monotonic. They are not definable within the language (and for good reason).
&gt; They are not functions To be clear, a function has the property: x = y ==&gt; f x = f y but: length [0..] = undefined spoon (length [0.]]) /= spoon undefined
It's not quite the same. Spoon is making a distinction between different kinds of \_|\_ (those that can be easily distinguished). length [0..] = nontermination spoon (length [0..]) = spoon nontermination
That was the original behavior, but it's quite easy to construct impure situations with spoonAll (catching &lt;&lt;loop&gt;&gt; and the like). Maybe unsafeSpoon?
Not at all! :)
To extend on what godofpumpkins says, I conjecture you can make sense of this work by defining a slightly different semantics for haskell than the usual semantics. One where every domain has in addition to the usual Scott's bottom, another error value which is above Scott's bottom, but incomparable to every other element of the domain. Under these semantics `length [0..]` is Scott's bottom, while `undefined` is um.. Haskell's appendix. I think all of Haskell can be well understood with these semantics and spoon is a monotonic function with regards to this slightly non-standard semantics. It is important to not catch &lt;&lt;loop&gt;&gt; exceptions which would pose a real problem. You would lose referential transparency as seen below: import Control.Spoon import Maybe f = maybe 0 (error . show) . spoon y = f y main = print (isJust (spoon y)) &gt;&gt; print (isJust (spoon (f y))) {- $ ghc --make -fforce-recomp foo.hs -o a.out &amp;&amp; ./a.out [1 of 2] Compiling Control.Spoon ( Control/Spoon.hs, Control/Spoon.o ) [2 of 2] Compiling Main ( foo.hs, foo.o ) Linking a.out ... True False -} 
Also, catching Ctrl-C leads to very unintuitive behaviour. (Edit: in addition to non-referentially transparent behaviour).
In that case, perhaps some way to add additional handlers? I'd like to replace some nasty code to do this with Spoon, but the exceptions I'm catching (unicode encode/decode from Data.Text) aren't in the whitelist. And it'd be silly for spoon to depend on every package with its own exceptions.
True that. Maybe a typeclass of safe exceptions (along the lines of what roconnor described above about changing our view on semantics)? Need to think about this a bit.
But that's the point. You are not permitted to distinguish them in pure code.
Why not?
Thanks! I've been trying to put that into words for a while now, but that's exactly what I wanted to say.
&gt; I think all of Haskell can be well understood with these semantics and spoon is a monotonic function with regards to this slightly non-standard semantics. That's not obvious to me. I'd want to see a more formal presentation of that argument. Note: it'd probably break parametricity even more than adding seq did. Polymorphic functions could inspect values to see if they're error and do different things on that basis. With seq they can inspect but cannot do anything different.
Because it is impossible to do so given the semantics of the language. All functions you can define in Haskell are monotone. You've written `isBottom :: a -&gt; Bool` which is not monotone. The semantics of the language are based on domain theory, the domain theory explanation of the semantics relies on functions being monotone. If you want to add non-monotone functions then you've got to go find a suitable semantics for your new language, since it's not the same as the one we're all using now.
Dartmouth has a Haskell course as the second prerequisite for the CS major (Java is the first, along with a bit of math). Unfortunately, they seem to forget about functional programming beyond that :/
I can see what you're saying, but if you step outside the accepted semantics of Haskell, there do seem to be two distinct classes of \_|\_ we lump together. Maybe we can't distinguish them purely in current usage, but assume for a moment that we could. Then it seems that f True = Just () f False = Nothing and g True = () g False = error "" -- or undefined are isomorphic on some level. There are some forms of bottom that we'll never be able to catch, like nontermination or some of the asynchronous exceptions. But other forms can be translated almost mechanically to sum types of error/success conditions. I'm not saying this function is pretty or even belongs in Haskell, but it doesn't really break referential transparency (unlike what would happen if we caught all exceptions), so that does seem to point to there being more than one class of bottom. And given that many functions effectively use error as a Nothing in a referentially transparent manner, does the fact that the mechanism by which this value is "returned" (exceptions) really matter? I think it's something worth thinking about, at least.
As a current CS undergrad, let me say that choosing a school based on the availability one of specific language is not a very good idea. Haskell and FP are very cool, but they are far from a sufficient condition for a good undergrad CS program. There are many, many more things that are needed for a good foundation in computer science. I think most people would agree with me that Haskell is not very good in a pedagogical capacity anyway. Scheme is often chosen to teach functional programming because it supports multiple paradigms and has a very easy syntax that does not get in the way of the theory. In general, you shouldn't think of cs education as a series of classes that are focused on specific languages, and you should avoid schools that structure their curriculum that way. With a good foundation in the concepts different programming languages should begin to all seem like different representations of the same thing. Many schools use a book called Structure &amp; Interpretation of Computer Programs which is a good introduction to CS in genreal and touches on functional programming. You might want to look at school that use it, but that should only be a starting point. Berkeley is one such school, and the book was written at MIT even though they don't teach a course from it any longer. EDIT: And I suppose I can answer your actual question too: Stanford teaches a class on category theory in the maths department, and I've been told that they incorporate Haskell somehow. I haven't heard of any others.
[Portland State University](http://www.pdx.edu) offers a undergraduate/graduate level course in Functional Programming, but it is taught mostly in Haskell. I took the course from Mark P. Jones, who created "hugs" one of the first Haskell interpreters.
Setting aside wether or not it breaks the semantics of the language, it's changing the semantics to make every type "nullable". Having null as a member of every type is widely regarded as a mistake. Suddenly it means you have to deal with null everywhere. This makes error handling worse not better.
I'm not doing that though. This package is compensating for the fact that many libraries (including base, with functions like `head`) effectively use `error "null"` to represent Nothing. I'm not necessarily advocating total functional programming, but for libraries like Binary, `decode` must be fed input you already know to be correct (which means you've already basically parsed it) unless you want to move to impure land and deal with a "pure use of impure exceptions" using regular exception handlers. So in short, I wrote a (== null) checker for Haskell because people already use nulls in their code, not because I'm advocating adding null to the language. I'll add a big warning to the package that says "please do not take this to mean you should write error all over the place" if you think it would help, but I figured people would already know that it's a bad thing.
If you are willing to go abroad we've got John Hughes and David Sands teaching at Chalmers University of Technology in Sweden. Right now I'm taking a functional programming course taught by David Sands.
the first quarter of uchicago's Honors Intro to CS was taught in Haskell this year, but this was the first time. We've had a functional programming class taught in Haskell too, though I'm not sure if that's still around. CS is very theory based here though.
I've heard that RIT has a Haskell class, but I go to RPI where you're lucky if one in many hundreds have even *heard* of it...
I can confirm that, yes, RIT does have a Haskell class.
There are also other courses there, e.g. "advanced programming", which use/teach Haskell as well.
We have a good team here. Mark Jones, Andrew Tolmach, Jim Hook, Tim Sheard, Bart Massey, and numerous students all very interested in Haskell. Oh, and some company called Galois is supposed to be here in Portland somewhere.
Until you go to college though, I'd highly recommend you come hang out on the #haskell IRC channel on freenode. It's extremely helpful and friendly!
&gt; As a current CS undergrad, let me say that choosing a school based on the availability one of specific language is not a very good idea. Haskell and FP are very cool, but they are far from a sufficient condition for a good undergrad CS program. There are many, many more things that are needed for a good foundation in computer science. Certainly true. However, the schools I'm aware of using Haskell all have excellent CS programs in general. Portland State, for example, has a very well rounded department that covers all the bases (PL, OS, DBs, AI/ML, security). They also have excellent infrastructure and sysadmin--- which I've come to learn should never be taken for granted.
I can also confirm this.
spoon is restricted to NFData, if that helps.
Yale, if you can get in.
I'm taking this class next quarter as PSU. Can't wait :)
U Penn has a great PL group, and Benjamin Pierce teaches an "Advanced Programming" course in Haskell. Haskell (and other FP languages) aren't used in much of the remainder of the curriculum, but if you're interested in that sort of thing you can get involved with the PL group and do all the Haskell (and Agda, and Coq, and...) you want.
Seconded, CSE 120 (ugrad) and CIS 500 (grad) are both type theory/FP classes. Unfortunately, I asked ben pierce if he would be willing to sponsor an independent study on functional programming (more focused on practice than theory) and he said no because he didn't feel that there were any practical uses of FP :(
As an American who learned Haskell at Chalmers, I approve of this message. Haskell is used throughout the department, not just in the functional programming classes.
for the pragmatic Pennsylvanian, West Chester requires you to do ML and Lisp to graduate, Haskell is coming soon. edit: And I know for a fact that UT Austin does haskell in their required FP course
TBH, spending time on #haskell is probably better than any specific course in the language
I am not a Stanford student; but I did audit the Category Theory course. There is talk of another, more comprehensive one. The students tell me that CS 242 ("Programming Languages") is taught partially in Haskell.
Jeepers. There sure are a lot of PSU redditors. We should have [a subreddit](http://www.reddit.com/r/portlandstate/)!
thats sad(dening)
It doesn't violate referential transparency (it would have that potential if you returned `Either String a`), but, I think it violates [Amr Sabry's formulation](http://www.cs.indiana.edu/~sabry/papers/purelyFunctional.ps) of "purity" (this is the same definition that Oleg has been referencing to argue that `unsafeInterleaveIO` is impure, although I don't think it works in that context, because I think pushing the nondeterminism into `IO` works, as it already contains things that make it nondeterminstic independent of `unsafeInterleaveIO`). Anyhow, the definition of purity is roughly that the results of pure functions/programs must be independent of evaluation order, modulo bottoms. So, programs may produce well defined results in a language with non-strict evaluation, while they're undefined in a strict language (or vice versa), but they may not produce two different well defined values depending on evaluation strategy (this, for instance, makes (delimited) continuations impure). So, if we consider the expression: teaspoon (head ([5] ++ error "Not in the face!")) under non-strict evaluation strategies we will get `Just 5`, while with strict function strategies we will get `Nothing`, and I believe this violates Sabry's criterion. If the results were in `IO`, we could once again say, "`IO` is evil and nondeterministic," but we're supposedly working with pure functions here. This may not be a big deal if you only use it in situations where the standard library makes bad decisions, but it's something to think about.
I just graduated from [Berry College](http://cs.berry.edu/wiki/index.php/Main_Page). I did a directed study in functional programming, which focused on Haskell with a dash of Coq/OCaml thrown in. The introductory CS courses are using Scheme as well. 
And if you're in Portland, you also get [PDXFunc](http://pdxfunc.org), the [Galois Tech Talks](http://galois.com/blog), and the [HacPDX](http://www.haskell.org/haskellwiki/HacPDX) hackathon series. I think Portland is *the* place in the states to be studying Haskell.
I competed in a collegiate speech/debate tournament at Berry College in 1998 - really beautiful campus! We were there in the fall right when all the leaves had turned color, and I remember the long road that led up to the mansion building lined with beautiful trees, and lovely leaves all over the place. Really charming!
At the University of Texas at Austin when I was an undergrad, Introduction to CS (i.e., the very first CS class) was taught in Haskell (this was in 2000). At the time I was like WTF, but in retrospect I'm very glad that we learned a solid functional language - it really opened up entire new ways of thinking about problems for me. I do wish the teacher had actually given us problems that functional languages can solve well - we had to write programs (a battleship game etc) that I think probably could have been done better in a different language, but it was still a very mind-opening experience. I don't know if they still offer Haskell, but I must say - 10 years later, the absolutely fantastic CS education I got there is still paying off. UT/Austin has (had?) a really top notch program without the expensive genius-douche factor of Stanford or Carnegie Mellon :-)
ScopedTypeVariables is not about *introducing* type variables explicitly using forall but about *using* type variables that are already in scope in local signatures.
I suspect the only way you'd see it is if you got the optimizer to rewrite your expressions. In theory, when GHC goes to evaluate something like: error "left" + error "right" it could choose which side to evaluate randomly. But in practice it's likely to make the same choice everywhere (since the core generated for (+) will always have the case statement for one inside the other, which makes the ordering explicit).
SUNY Binghamton uses Haskell in its "Programming Languages" course (Which explores different paradigms of programming). I haven't used it in any other courses though and I'll be an alumni in about a week, so I don't think there are any other courses here that use it.
The other variant is not restricted teaspoon :: a -&gt; Maybe a
&gt; I'm not doing that though. I know that's not your intention. But what you're doing is adding a new primitive function to the language, one that cannot be defined within the language. Much like `seq`, this extra primitive changes the nature of the language because it lets us distinguish things that were previously identified. In the case of `spoon` it lets us see a distinguished null in every type. That's not a good idea (even if you can make the semantics coherent). I disagree about `head`. That's simply a partial function with a clear and simple precondition. For binary decode, yes that's obviously a problem since you cannot pre-validate that in any sensible way. So there the library wants to be improved (we did the binary lib like that in the first place because at the time we did not know how to mix error handling with the strictness and performance we needed).
I second the above statement about UT. I first learned Haskell as an (ECE) graduate student by attending the excellent bachelors CS course on programming languages taught by [Hamilton Richards](http://www.cs.utexas.edu/users/ham/richards/) who is alas now retired. I also learned a lot in the graduate programming languages course by [William Cook](http://www.cs.utexas.edu/users/wcook/) (who is an phenomenal teacher and has [an interesting history](http://wcook.blogspot.com/2009/10/ecoop-2009-banquet-speech.html)). Due to these experiences, I went on to start a PhD at Utrecht University (Netherlands) with [a great bunch of folks](http://www.cs.uu.nl/wiki/Center) focusing on Haskell and related technologies.
Certainly, one's choice shouldn't rely completely on this decision, but it could definitely play a part. I think every college applicant chooses schools partly based on their interests and how well a school matches those interests. Of course, my interests changed over time. What I initially thought I'd be doing is completely different from what I now am working on (computer architecture vs. generic programming library design). As a result of my experiences, I recommend looking at schools that offer a variety of opportunities with the expectation that you may learn something new that attracts your eye more than something you already know.
Under the non-standard [nullable haskell semantics](http://www.reddit.com/r/haskell/comments/acasn/tired_of_functions_that_could_live_in_maybe_but/c0gwj9p), I'd expect even strict evaluation would consider `([5] ++ error "Not in the face!")` to denote `5 : NULL`. Under strict nullable semantics `([5] ++ fix id)` would be _|_, but `teaspoon (head ([5]++fix id))` would still be _|_. Again, if you consider `error "string"` to be the same as non-termination, yes this whole thing doesn't make sense. But if you consider `error "string"` to be a NULL value, it is less clear that this doesn't make sense (though dcoutts makes a nice argument about [losing parametricity](http://www.reddit.com/r/haskell/comments/acasn/tired_of_functions_that_could_live_in_maybe_but/c0gwqc4) for teaspoon (I'd be happy if Haskell removed seq (although deepseq is defined in terms of seq, it doesn't have to be))). I can't believe I'm trying to defend this awful library. :D
What if I give you par or pseq? How do these interact with error?
Do they still have a course in category theory? When I was there, MacLane was still teaching it, but that was almost 20 years ago, and I got the impression that it was considered his private toy, not useful for any other field. It seemed unlikely to survive him.
Check out CMU. I don't know about Haskell, but I know they're big on SML.
I don't know. I suspect `par x y = y` in all situations, and if `x` is bottom, nothing interesting happens. `pseq` is actually defined to be more ordered than `seq`, so `pseq (error "foo") y` should be `error "foo"` for all `y`. I suppose the best bet is to try something like `teaspoon (seq (error "left") (seq (error "right") ()))`, since `seq` is just supposed to be strict in both arguments, and so the compiler is free to reorder the two errors. But I don't know what situations would actually lead it to do so. Edit: strictly speaking, the compiler probably knows that it can reorder: case x of I# x# -&gt; case y of I# y# -&gt; I# (x# +# y#) as well, because the outer case has exactly one branch, and that branch is strict in `y`, so the expression really is strict in both `x` and `y` (and not conditionally strict in `y` given that `x` has some particular form, unless you count bottom, but presumably strictness analysis doesn't). But I don't know what situations the compiler would reorder that in, either.
This comment likely belongs in either the xmonad or Linux subreddits, but I just thought I'd point out that _Bluetile is awesome_. It gives me just enough xmonad to make me happy. Thanks, you talented and attractive Haskell developers!
Bleh... the more I read about this stuff, the more it seems necessary to explicitly separate data from codata. Non-totality is really frustrating.
But you can always commute down from NH to the Boston Haskell meetings like pumpkin!
Fortunately some computer scientists seem to have latched onto category theory, so it is likely to be around for a long time.
That's somewhat surprising, given his research interests. Did he expand further on what he meant? I presume that his take would be that FP is a good playground for type theory, but the applications will all come in more standard imperative and OO languages, but that's just a guess.
Yeah. Berry is essentially a humongous nature park that happens to have a school on it. That entrance closed several years ago, maybe not long after 1998. It was moved down the road a little bit. If you get the opportunity to come back to Rome you should drop by.
Given the above two confirmations, I too can confirm this.
Wow, that is quite relevant to my interests, even as an unpaid internship. I am wondering, however, how a candidate can have significant Haskell experience when there are so few Haskell jobs currently. It seems that this limits the pool to college students or people working at existing Haskell firms who are willing to work for free. I've got significant industry experience, just not in functional languages... Oh well, either way I'll be applying. It's certainly worth a shot.
little hint: "I gave a talk..." - I have no idea who wrote that (blog is owned by a group of people) and knowing it was dcoutts raises the chance of me watching those slides.
Summer internships are usually for students. On your original point, "how a candidate can have significant Haskell experience when there are so few Haskell jobs" -- the answer is "because of open source and universities".
Well I'm working on the open source aspect of it (the Starcraft competition, rosetta code contributions, etc.). If only I'd known about Haskell a bit sooner, maybe I'd have some GHC contributions by now. I've been looking into going back to college for an Aerospace degree in the Fall, so perhaps I might have a shot. I'll cross my fingers.
True, WordPress is rather inconsistent with displaying the name of the person who posted it. The front page does but the individual page does not. :-( Anyway, it's not a technical talk aimed at existing experienced Haskell hackers. You know this stuff already :-)
Also, he's not to do with Haskell, but if Jim Hein is still teaching in his retirement (he was a few years back) he's the best instructor for discrete mathematics I've known. It's definitely worth trying to get into his automata theory course if you do end up out there.
Love!
\#agda on freenode, you know you want to!
Finally a replacement for `mappend` and `mempty`! :-)
I met some of them in October and it's a pretty amazing group. On my very short list for grad school next fall.
Wow. Dr. Graham Hutton! Watching it now! Download link: http://ecn.channel9.msdn.com/o9/ch9/4/1/2/4/0/5/C9LecturesFPGrahamHuttonC11_2MB_ch9.wmv
Very kind of you to say so. What we have at the moment isn't very impressive, but we're managing to get on with it. The front page of the blog is now wired up to the darcs repo so changes show up in the sidebar and the literate source file (the Epitome) rebuilds at each push. More on the repo (how to get it, how to build it, what to do next) when I've been taught these things myself.
Graham Hutton is a far better lecturer by orders of magnitude than the regular presenter who is doing this series. I really liked this video and I wish there were more videos by Hutton, are there any?
Oh, hi Andrew! Sorry we didn't take you to a better pub than Rock Bottom. Don't forget to check out Oaks Bottom when/if you move here.
unsafeSpoon? http://www.youtube.com/watch?v=9VDvgL58h_Y
http://hackage.haskell.org/trac/ghc/wiki/HackPar More info...
doesn't seem very parallel if its only happening in one place. shouldn't there be multiple threads of hackathons running in parallel?
I often use: while :: (a -&gt; Either a b) -&gt; a -&gt; b while f x = case f x of Left x' -&gt; while f x' Right y -&gt; y Basically, keep applying a function as long as it returns intermediate values; then return a final value. Useful for expressing iterative processes (like root-finding etc.). 
&gt; so `pseq (error "foo") y` should be `error "foo"` for all `y`. `seq` has that property too (in fact, that is the definition of `seq`)
I downloaded them all and started watching them yesterday. after about 10 minutes of him saying “you know” at the start and the end of every sentence I couldn't take it any more and had to turn it off. I then tried chapter 2 and 3, hoping that he would have gotten feedback telling him to cut back on the “you know”s, but nope - still unbearable to watch for me. :( A shame, really, since I have heard very good things about the book the lectures are based on. I think I'll just watch the lecture with Dr Hutton. Hope he'll do the rest of them, too. :)
Every compiler only does CSE over regions where it either knows or has been instructed to assume there are only idempotent side-effects. Therefore, C supports CSE exactly as every other programming language. Now, the side-effect analysis could surely be nicer, i.e. `malloc(3)` is considered to have side effects even when the memory is freed in the same function. But, well, ... if you want to do function calls like variables, perhaps you should use variables instead.
Is there a better introduction to what they're doing than this page, for outsiders? Eden, GUM, SCSCP, GAP, Globus, Artcop, and much of the rest of the stuff on that page are utterly unintelligible to me.
I know some of them: * [Eden](http://www.mathematik.uni-marburg.de/~eden/), extends Haskell with a small set of syntactic constructs for explicit process specification and creation * [Globus](http://www.globus.org/) -- giant open source grid computing project * [GUM](http://www.macs.hw.ac.uk/~dsg/gph/papers/abstracts/gum.html) -- a distributed Haskell 
This is true if you view `error "foo"` as an opaque bottom, but since we're talking about inspecting different bottoms here with evil magic, it isn't true. `seq` is strict in both its arguments, but that doesn't mean that its first argument is evaluated first. The result depends on evaluating both arguments, but the ordering is unspecified, so: seq (error "foo") (error "bar") may behave the same as either `error "foo"` or `error "bar"` at the compiler's (or, perhaps, runtime's, although I doubt that makes such decisions) discretion. By contrast, `pseq` implies an ordering. When evaluating pseq x y `x` must be evaluated first, and `y` only evaluated if `x` is non-bottom. There isn't a difference denotationally, but operationally, there is. Edit: incidentally, the reason for this difference is that `pseq` comes from the parallelism library. If you were to write: x `par` y `seq` x + y it would be unfortunate if it was decided that the second argument to `seq` should be evaluated first, because then `x + y` would be evaluated in a single thread, which in turn would evaluate `x` and `y` in that same thread. Then it would go to evaluate `par x y`, but there'd be nothing to do at that point, so you'd lose the opportunity for parallel evaluation.
Well, I *do* want to have the equivalent of a multilingual Show class. However, I can't figure out where to start with this package. I essentially need the equivalent of: class I18NShow a where i18nShow :: a -&gt; [Language] -&gt; String Where the [Language] is the list of languages I would be happy with (think of the Accept-Language HTTP header). Also, I'd probably replace String with Text. Does this package have something resembling this?
Did anyone else think this was software that had some remote relation to Heath Ledger before reading the rest of the title?
yes, this is a name idea fail.
or win??
yeah, this is the first haskell enterprise library - plenty of docs and you still have no idea wtf is going on.
 data RequiredByRequirerCompositeIsMissing_PCLTE = RequiredByRequirerCompositeIsMissing_PCLTE RequirerCompositeKey RequiredCompositeIsMissing_PCLTE ... are you serious? data AddLngTpl_toPCLT_Error = TplUniquenessViol_APSTPTE TplUniquenessViol_PCLTE | DefaultLngTplComponentsParamsSetsDiffersFromOnesOfNondefault_APSTPTE DefaultLngTplComponentsParamsSetsDiffersFromOnesOfNondefault_PCLTE :o I can't imagine what was going through the mind of the person who invented these identifiers. Do you seriously expect people to *use* a library which looks like this?
Where are you? Were you volunteering to host?
Not that old, 2008-09-26 :-) More significant: you seem to have found out what ACM did with all that video...
I had a look at the email, and couldn't figure out what the package was actually supposed to do, so checked out the hackage page. Big mistake :(
I wouldn't recommend Berkeley to a student interested in functional programming. Yes, the intro course is in Scheme, but the PL faculty have little to no interest in FP as an area worth studying in and of itself. All the other undergrad programming-heavy courses that I know of, except for the intro, use Java or C/C++. (I was a grad student at Berkeley 5 years ago.)
Please don't use link caching sites like bit.ly for reddit-submitted links. It's bad rediquette, according to the official doc.
Congrats dons. Once tried, forever in love.
...FactoryFactoryImpl
Don't forget Spencer Janssen. Iirc, he started the project and did much of the initial coding. Perhaps someone will pipe up and refresh my memory.
That's right. sjanssen wrote the first working version, and he and I wrote about 50% each until v0.5 or so. Now days there's a large team involved.
terrible sound:/ his voice is so quiet but you can't make it louder because of other noises
Don't be ridiculous. Did I somehow wander into /r/emo by mistake? In six months time you'll all have forgotten about Heath Ledger and moved on to the latest Twilight star that decided to OD.
Bet you all thought I'd forgotten... No way! I'm only buried in take home finals and chemistry studying!
If you were in PSU then you would have already been done...
Unfortunately, I'm at Worcester State in Massachusetts, we have finals (on occasion) up till the 23rd (as with the Great Ice Storm of '09). Tomorrow is my last in-class final, all the rest are electronic submission. So I will be able to complete them from the relative comfort of my sofa.
If you couldn't go to school, what would you be doing?
If I _couldn't_, I'd probably try to get a job to pay off my massive school bills. If I _wasn't_, and I could do whatever I want w/o worrying about money. I'd probably try to be like Carl Sagan, producing accessible educational videos about science and math and try to get people to see the wonder of the universe. Chances are I'll never get to do that, but I sure hope I can do something similar someday...
Nice and fine engineered piece of software. Can be used as a showcase of clean and nicely organized piece of code that can be picked up without much effort even by a Haskell newbie.
&gt; analogies are endofunctors in the category of bad explanations *slow clap*
How about: fib n = flip evalState (0,1) $ do replicateM n $ do modify $ \(a,b) -&gt; (b,a+b) gets fst 
That's awesome guys! Top notch work! I've finally gotten around to setting up a FreeBSD box, and I actually was liking my old KDE stuff that I used to rely on (KIO fish:// etc). Anyone using Xmonad instead of kwin for KDE? Just wondering how that works out. I'm about to go look and see if there's DBUS bindings for Haskell, because if I can't have Xmonad for KDE, at least I might be able to script Kwin with Haskell :-)
Exactly. Wherefore all the java-isms? RT @luqui: 90% of libraries I see are overengineered. Why all the tricks? What's wrong with the "functions" design pattern? 
do you have a link to all the videos?
I'm using XMonad with KDE and it works very well indeed. Don't forget to use XMonad.Config.Kde to get basic support.
Why was it deprecated? Seems like an arbitrary restriction... I guess the workaround is to use an existential data declaration wrapper and use that?
I used it in my C-like DSL to get polymorphic monadic bindings. I don't know how to do it without impredicative polymorphism.
Because it adds unsustainable complexity to the entirety of the typechecking code, and makes additional changes very painful, apparently. An existential doesn't work, you'd need a universal wrapper, I guess. `data Moo = forall a. Moo a` vs. `data Moo = Moo (forall a. a)`
Thanks Duncan! Boolean fixed now in 0.0.1 on Hackage
Thanks!
I was thinking something along those lines as well.
I can find a place in my heart for any language feature, including the monomorphism restriction, so yes.
Do you mean f :: (forall a. a -&gt; a) -&gt; Int -&gt; Int ? (i'm confused) 
Sorry, fixed that! That'll teach me to write stuff without testing after staying up all night :)
Also haskell-array-0.2.0.0 (one of the deps of darcs) fails plus the latest cabal-install depends on an older version of Cabal. 
What's with one-line `do` block?
After a short glimpse of the code -- it does seem very nice code. Good job!
There will be a cabal-install-0.8 release in due course. In the mean time you can use the darcs version.
I have joined Haskell long after that mess was cleared, but some reasons I've heard for why it is messy: * You have to carefully align the request result list elements to the response list elements, and any mistake may result in deadlocks or wrong results * Its ugly * It doesn't compose very well 
You'll find a good explanation of the model at http://research.microsoft.com/en-us/um/people/simonpj/Papers/marktoberdorf/mark.pdf.
http://www.haskell.org/pipermail/haskell/2009-March/021064.html http://www.haskell.org/pipermail/haskell/2009-March/021133.html
I was about to post that, but then I realised it's an answer to a different question.
I now finally understand monads.
This is really cool! Just curious - why does hSetEcho also apply to write-only handles? 
Yeah, I would never do that in real Haskell code. But it's supposed to be a "transliteration", i.e. a line-by-line transcription more or less. If it were a translation into idiomatic Haskell, the entire function would be written completely differently.
It had been way too long since I was exposed to mjd's uniqueness.
I hereby nominate this blog for this week's HWN.
Wow, I feel disturbingly much more comfortable with monads this wonderfull meal. Monads oughta be like burritos, this is proof!
I now finally understand burritos.
So MonadPlus is *not* like a burrito, unless you consider the empty burrito to be a burrito (which I do not). What are applicative functors then? Margaritas, ceviche...?
He seems to be picking up the pace, fortunately. I mean, three posts this month.
&gt; some tortillas have the face of Jesus. But those have been toasted what do toasted Monads/containers have? Edit: replaced LoL with question
This will not be complete until there is a burrito store actually selling monadic burritos.
And Haskell is like chile verde.
At least when you try to make a monad for the first time the mess can easily be cleaned up with a quick rm.
I believe sir, that would be a BurritoPlus, where we extend the definition of a Burritio to include empty tortillas.
I'm sure I'm not the only one who is now going to get hungry every time he codes with monads. Well done, Mr Dominus. You've catalysed the beginning of a soon-to-be fat Haskell programmer stereotype. 
Pfft. More like a Burrito_Minus_ if you ask me!
I don't think that would sell well, since you are not permitted to eat the innards of a MonadicBurrito unless you yourself are wrapped in a burrito shell. Hopefully the MonadicBurrito store implements a monad-piercing function for the burrito or you won't be able to leave the burrito shell ever again, either. That could also be used to extract just the innards of the burrito for your consuming pleasure, but then it wouldn't be a burrito anymore. This strikes me as a difficult sell.
You clearly can't consume the burrito with only `unit` and `join`, but that's hardly a shortcoming when you think about it, as those are best used for constructing and modifying burritos from their ingredients. A separate, burrito-specific function could be used, as there is no general monadic analog for consumption. Might I suggest `eatBurrito`? The real strength here is that we can use library functions written generically for the monad interface. So for example, I could use `lookup` with `Maybe` and get the first result or `Nothing`; `[a]` and get every result; or `Burrito` and get a delicious meal. I hope it comes with lengua, but I suppose that will depend on what's in my data structure.
I think you mean runBurrito!
Tortillas are *NOT* indistinguishable. 
I'm slightly confused by some of the complaints against erlang. &gt; Note that in idiomatic Erlang code, the process will crash if the pattern match fails. In that case, we have to handle that problem in another janitorial process. Well, no, not necessarily. You code like that if you have "janitorial processes". If not, you add a catchall variable _ to allow a fail. Generally, you allow processes to crash. The "janitorial" processes allow you to stop having to have all the error handling necessary with catchall variables. It also makes the program more robust since you code under the assumption that things *will* crash. Personally I'd split it to have a decode-message function to replace the big case statement, and put the definitions in there (unless you use them elsewhere). This returns either {ok, Decoded} or {error, Reason}. The recv_message then has a simple case statement: recv_message(Rate, Message) -&gt; case decode_message(Message) of {ok, Decoded} -&gt; MSize = size(Message), {Decoded, etorrent_rate:update(Rate, MSize), MSize}; {error, Reason} -&gt; %Crash or return error end. This allows more control over how it crashes and where. It also splits up the decoding and receiving, which to me should be separate. edited - this markup language is fucking horrible.
The short answer is that if you accidentally evaluate the response too far ahead you can block indefinitely and since there is no clean connection between the two it is very easy for this to happen. Furthermore, if every possible request has to be encoded as some constructor in a closed ADT it is very hard to extend. 
Eating and digesting burritos is clearly comonadic.
It's nice to see that Simon PJ and others making headway on this. Specifically I'm thinking of the implications for hash tables. While hash tables don't fit the Haskell paradigm too well, it is still nice to have them for those few cases that they match the problem at hand better than finger trees, tries, etc. Perhaps this improvement will mean that eventually we won't need an inline hashtable for the k-nucleotide benchmark: http://shootout.alioth.debian.org/u64q/benchmark.php?test=knucleotide&amp;lang=all&amp;box=1 At very least it should help get the trolls off of our backs. I'm surprised jdh hasn't commented yet...
I also have another idea where errors are mapped to a top element in a Scott domain. The top element represents "inconsistent information" from the information theoretic perspective of Scott domains. My intuition suggests that this may be a good mapping; however I don't really know enough about domain theory know for sure if this works out well. Edit: Ignore this. This doesn't help with spoon, Under this idea spoon would still be a non-monotonic function.
&gt; should help get the trolls off of our backs do not expect any change. any positive actions will be recast as negatives, and new sources of complaint will be found. this individual is not sincere or authentic, so do not treat him as such.
Yikes! You should not use IORefs for communication between threads. That's what MVars are for.
I guess I should have stressed, this is for looking inside a process while it is still running. My understanding is that with Mvars, the value only becomes available after the thread is complete. That said, I am starting to think the use case I thought I had (checking happstack web server start ok) is not legit.
The point is it doesn't really matter what you're doing. If you're using forkIO then you just can't use IORefs. IORefs are for single-threaded code, their meaning and behaviour is not defined for threaded IO code. That said, you can use an MVar in much the same way as an IORef (using readMVar and modifyMVar), so you don't need to change your algorithm.
thanks for clarifying that. I'll try and post an update.
Burrito can be an instance of Copointed!
You can use hierarchical card marking and get down to pretty much amortized O(n) in the number of array updates. 
&gt; their meaning and behaviour is not defined for threaded IO code Is this true? Then why does atomicModifyIORef exist?
Can't wait for some blogs/papers -- or a usable cabal package :-)
The truth is that yes you can use `atomicModifyIORef` in threaded code. It's such a narrow interface however that it's not worth bothering with except in rather specialised situations. The simplest and most helpful advice is "don't use IORefs with threads".
Yes, the input is a problem. The facility that should have been perfect for this is abbrev-mode. You simply type forall and it automatically turns into ∀. The problem is that it only works on *words*. I also want it to work when I type &lt;- or =&gt;.. I wrote a small elisp thingie to help me with that. You can get the darcs repo here (warning: I'm an elisp newbie): http://code.haskell.org/~roelvandijk/code/emacs-unicode-symbols/ There are a million things I can improve on that code. Some things I have been thinking about: * Put the symbols and their unicode equivalents in a list * Use the same lists as abbrev-mode * Use a greedy parser which walks backwards character per character until a symbol matches, as opposed to simply comparing the previous token. * Replace all symbols in a region
Bertrand Russell would feel vindicated.
You said "IORefs are for single-threaded code, their meaning and behaviour is not defined for threaded IO code.". Is that really true? If you know that an IORef is only manipulated by one thread at a time (i.e., you have a lock preventing concurrent use), surely they behave as expected? 
In ghc currently (and on current cache-coherent hardware) that is probably true. I don't think that behaviour is guaranteed anywhere though. There are at least two interpretations, one is that IORefs are (in C terminology) "volatile" meaning each readIORef/writeIORef must correspond to a memory operation. If they are non-volatile then the compiler is free to perform standard imperative optimisations on the assumption that other threads are not changing values behind our back.
A quick google didn't reveal all that much. Does this paper describe the technique? [Combining Card Marking with Remembered Sets: How to Save Scanning Time](http://www.cs.technion.ac.il/~erez/Papers/cards.ps) 
(1) What about IOArray/IOUArray? (2) What about STRef/STArray/STUArray?
I think that what the OP meant by "idiomatic Erlang code" is what you mean by "Generally, you allow processes to crash." I.e., it wasn't a complaint against Erlang - just pointing out that that's the way it's usually done.
Ok I'll give them the benefit of the doubt here, I read it my way (cue song) because of the emphasis on the word crash. Still, I stand by my point that you can have the same non-crashing function call. Personally I'd have the catch-all variable there, and allow the crash to happen because of a returned {error, Reason} because then the crash tells you the problem. I'm new to erlang though, so I'm not sure which would be best (both ways have advantages and disadvantages). I welcome any corrections to my way of thinking!
The ST monad is inherently single threaded so the issue does not arise. An IOArray is essentially the same as an IORef so the same issue applies. The more useful memory model would be to allow MVars to guard access to IORef and IOArrays as augustss mentions. As far as I am aware however that guarantee is not defined anywhere. Indeed consider what might happen if you take ST code that manipulates STArrays and run the ST code at type RealWorld (ie lift it to IO), bearing in mind that it's quite legitimate to compile ST Array code on the assumption that it is single threaded.
&gt; I asked Brent if this was actually what he had in mind when he first suggested the idea of tutorials explaining monads in terms of burritos, and if everyone else had understood this right away. &gt; But he said no, I was the lone genius. I wouldn't go that far. Thinking of a monad as a wrapper is hardly new, and I always assumed that must have formed at least part of the rationale for the choice of burrito as example. If not, I'd say Brent's subconscious knows more about monads than Brent gives it credit for. 
Was waiting for this moment to happen! **Download URL:** * http://ecn.channel9.msdn.com/o9/ch9/5/1/2/4/0/5/C9LecturesErikMeijerFPC12_ch9.wmv
Basically, it is a wiki for comparison of different languages' solutions to a set of problems. As a relative newbie myself, I've managed to add several solutions such as [Go Fish](http://rosettacode.org/wiki/Go_Fish#Haskell) and [24 Game](http://rosettacode.org/wiki/24_game#Haskell). Haskell recently moved up to having the [tenth most](http://rosettacode.org/blog/2009/12/stats-update-coming-soon.html) solutions solved of any language, but if a few people on this subreddit contribute, I'm sure we can improve our visibility.
My only fear is that nonidomatic beginner code might convince readers that all Haskell code is that way.
Fair enough. Rosetta does encourage aggressive modification of others' code, so hopefully that can alleviate the problem. In general I think that the value of people being able to get involved outweighs the risk of someone seeing Haskell code that could be more idiomatic. On a related note, I would be happy to have people criticize the code that I've submitted so far. I hardly think it is poor code, but code review is always beneficial.
Good thing that. Why was select chosen originally, was poll not available?
Good stuff. Even that will not be enough for Windows :( It needs to be using IOCP for that.
epoll /= poll. poll() is implemented in terms of select() on BSDs, iirc.
I agree. The more people who contribute, the less likely you will be to see idiomatic code. Additionally, Haskell syntax can be a bit confusing for newer people, so being able see how others have done it should help out.
nice stuff
&gt; The more people who contribute, the less likely you will be to see idiomatic code. Is this the opposite of what you meant to say?
Did you listen to the whole thing? It gets better after the first minute or so.
The problems of interest for Haskellers can be found by following the "View Results" link in the "FP Category" row.
There's so little background about what this page is all about. Even the PDF on the home page only describes technicals of submitting entries. Can the OP please take the time and provide context?
Sure: This year's round of the annual termination competition runs December 16-20. As usual, the most powerful termination provers compete against each other in this competition. The competition has several categories for termination analysis of different programming paradigms, including * Term Rewriting * Java Bytecode * Haskell * Prolog * etc. Moreover, there are competitions on * certified termination analysis and on * automated complexity analysis. More information can be found at http://termcomp.uibk.ac.at/status/rules.html The progress of the competition can be seen "live" on the web at the link I posted. 
An obvious O(nlog n) algorithm suggested by the name is that you just have hierarchical marks, i.e. layer 0 is has 1 mark per (e.g.) 1024 bytes of the array, layer 1 has 1 mark per 1024 layer 0 marks, etc until the number of marks at the highest layer is under some constant size. No doubt some clever tricks can be used to improve this on an amortized basis.
any reason the author picked python over haskell for this task?
See also: [what if everything was burritos](http://buttersafe.com/2009/12/01/what-if-everything-was-burritos-comics/).
This is a bootstrap task, and speed certainly isn't the issue, while for me a working Haskell was an issue. One could equally ask why the authors of the "cabal install" package use the shell instead of Python for bootstrap.sh. The two languages I use currently are Haskell and Python. I've managed to forget dozens of others, and I recommend Python to anyone for whom Haskell would be a poor fit. Python is very easy for this sort of shell scripting application. Certainly if I were to work on the code any further, I'd want to switch to Haskell. My goal however was not to compete with the "cabal install" package, but rather to diversify my risk, to have a robust fallback position for times like this transition to GHC 6.12.1.
BTW, the reason you could not get cabal-install working on your OSX 10.6 is because you did not have a fully working GHC installation and zlib (a dep of cabal-install) was the first thing to trip over this. Since GHC-6.10.4 does not work "out of the box" on OSX 10.6 you followed some hints to modify the ghc wrapper script to pass the gcc flags -m32. The bit you missed is that you need to do the same for hsc2hs. Otherwise hsc2hs generates code that assumes you're targeting the 64bit ABI. That's why the zlib initialisation check fails, because the code calling zlib has been compiled for the wrong size of everything.
Bingo, that sounds right, it's a relief to know what happened. There's plenty of advice on the web to just modify the ghc script itself for GHC-6.10.4 on OSX 10.6. I knew to modify more scripts, but I missed hsc2hs. I had gotten as far as figuring out that zlib itself was broken, and I had set up some "sandbox" clean development volume images for testing, when I noticed that GHC-6.12.1 was out. And that cabal-install wasn't ready yet. 
In regards to point 4 I'd say that it depends entirely on what you want to get done. If it will involve lots of IO then you should probably look at another language. If it involves a lot of non interactive juggling of data through complex algorithms then there is no faster way to implement it than using Haskell.
&gt; 1. Haskell is alien to what I'm used to &gt; 1. all data is immutable (no c++, i += 1, i = 1+1 and similar), &gt; 2. you can handle lists of infinite numbers and other similar lazy data structures like they were peanuts, &gt; 3. above all, you must deal with side effects (including IO) through monads One might argue that this much is only unnatural because you have already unlearned what you learned in mathematics: http://www.reddit.com/r/programming/comments/6tytl/functional_programming_koans/c04v5pc To get the power to support 2, you need to thread through the order of effects that you DO care about somehow. Monads are the most convenient way we have. 2 comes with a lot of benefits. You are used to dealing with a lot of warts from strictness, so used to it in fact you don't see them any more. You never have to deal with being half-way through evaluating a constructor for an object and not knowing what methods you can use before the superclass constructor gets invoked. You can make circular lists without mutation, etc. &gt; 2. Haskell Monads are not exactly intuitive, and mastering them requires time and an iron will The desire to write a monad tutorial peaks the moment you first think you understand them. As a result, most monad tutorials are written by people who barely understand monads -- or only think they do. It is commonly said that people finally 'get' monads based on the third or fourth tutorial they read. The accumulated wrongs wrap around and make a right. ;) This is definitely an area that could be improved, but unfortunately, nothing can stop someone from writing a monad tutorial. We've tried, there was blood everywhere. &gt; 3. Package management is somewhat immature and confusing &gt; This was another thing that put me off Haskell back in the day. Apparently there are many different official repositories of Haskell programs and libraries. Cabal Install seemed, a year ago, the best way to install packages, in a way that is similar to Rubygems or Debian packages. Unfortunately last time I tried it gave me the impression not to be mature enough. How are things going on this front? Hackage has come a very long way since then and is downright pleasant to use these days. &gt; One of the biggest problems of Haskell packages is that the GHC compiler releases are somewhat too frequent. I found myself unable to decide which version to install: one of the latest, which is better and has more features or a (very) old one, which at least guarantees me that the majority of Haskell libraries will work with it? If you think Ruby 1.8.x vs. 1.9.x makes things confusing... well, be glad to know that things can be much worse in the Haskell world. Is this still true? Don Stewart has been packaging up Haskell "Batteries Included" releases, which are the current compiler and the contents of the "Haskell Platform" which are a rather large body of well supported core libraries. Whatever the latest compiler he has packaged is typically the right selection. =) Right now that would be the latest 6.10.x build. While 6.12 has been released, it is in a state ready for the core libraries to move, and not really yet ready for end user consumption. &gt; 4. Ultimately, a more traditional language gets things done with much less hassle &gt; I could handle the paradigm shift well, I guess I could have learned Monads at some point, but the inability to install Haskell programs in a relatively easy way (e.g. Yi) really put me off. At the end of my last trip to the wonderful world of Haskell, I realized I could get things done more quickly, without the extra hassle with another language. That all depends on what you want to do. A lot of the code I write couldn't be written effectively in another language. &gt; Still, I decided to wait a bit (a year) and see if things improved. A year has passed, should I give Haskell another try? I hope I have effectively addressed at least some of your concerns.
I'll give Haskell Platform a try then! I knew it was underway, I just didn't know how stable it was. Thanks!
...which Monad tutorials do you recommend then? :-) 
I recommend reading and running examples from er... three or four of them and letting the information gestalt. ;)
This post just goes to show what I've always thought about haskell, which is that the main hurdle to learning it is that you've learned other programming languages first. Everyone's biggest complaints are that not having mutable data is strange and that monads are confusing. Generally, I think this is because they desperately want to do IO, and they rush through the plethora of monad tutorials out there trying to figure out to read a goddamn file. 
Other Haskell pros: * pretty fast compiler * decent test coverage/profiling tools * QuickCheck! * strong type system means fewer bugs, no NullPointerException * higher-order functions make you a zillion times more productive by allowing you to tease algebraic/categorical structure out of algorithms; a map or a fold is less error-prone than a for loop * competent users can be highly productive Haskell cons: * mutable arrays of boxed values are slow because of GC issues (although the GHC team just fixed this, it'll be out in a new release soon) * lazy evaluation means you'll be spending a lot of time with the heap profiler tracking down "space leaks" (aka thunk buildup) * platform is less mature than many alternative languages * OOP in haskell is difficult/unnatural (although that may not actually be a con!) * takes a long time to master/understand if you're coming from imperative land (which is most people, I expect)
I find that there are often reasons to program in Haskell irrespective of how quickly one can hammer out a pile of code in another language: code security/reliability. Having a solid, well built, speedy piece of code regardless of how long it took to build (within reason of course) may trump anything else. This is especially true when high reliability, speed, and resistance to security vulnerabilities is imperative (e.g. network applications that do lots of IO).
&gt; Ultimately, a more traditional language gets things done with much less hassle Do programming languages really "get things done"? I think it's programmers who do that. You are more able to "get things done" in a language you are used to. Fair enough. That's doesn't say anything about Haskell, though. I've found Haskell a powerful tool for all kinds of programming, including text processing and managing multiple components of a batch processing pipeline. Given the choice -- which I rarely have -- there are few projects for which I would not settle on Haskell as the best tool, all around. This has as much to do with the quality and discoverability of the libraries as it has to do with the language. It took me more than a year to get comfortable with Haskell, in and around other things; but no language has more handsomely rewarded me for learning it. What one is learning all that time is not really Haskell -- it's a different approach to computing. To learn Ruby or Python or C is ultimately to learn just another syntax for the semantics of mutable memory cells; but the lambda calculus doesn't have any of those. If Haskell wasn't really different, it wouldn't be able to offer the many advantages -- in parallelism, ease of debugging and program composition -- its advocates attribute to it. You get what you pay for.
IO in Haskell is accomplished via monads, but you don't have to know monads to do IO. The fact that IO is done with a monad is just incidental. For instance, in Learn You a Haskell, IO is introduced before monads (monads aren't introduced yet, working on it!) and I don't think monads are mentioned at all in the IO chapter. I think the reason most people get hung up on not understanding monads is that they think they have to know them to do IO and then they go through monad tutorials without having internalized knowledge about the type system and higher order functions, two things that monads leverage a lot.
It is a problem though. Whenever I want to learn a new language, I tend to re-implement a small program I've wrote in other languages in an attempt to grasp the new language. Sadly, 99% of those involve some form of I/O, probably half as the main feature. I gave up on Haskell, but then I'm nothing more than a novice programmer at best.
That's the kind of motivation boost I was hoping for, thanks!
I noticed that when I read Learn You a Haskell... I got comfortable with Haskell after that, then decided to learn about monads and that spoiled it for me. I think I'll re-read it once more, and try to digest it properly this time, without being to eager to move on to monads.
In other news, cabal-install 0.8.x is available for final pre-release testing: http://haskell.org/pipermail/glasgow-haskell-users/2009-December/018148.html
I agree. If development time is not a factor then a less "hackish" language should be used as it will save you from a lot of headache down the road. If its just for prototyping then you should probably go with a scripting language
To quote J. von Neumann "In mathematics you don't understand things. You just get used to them." Haskell is math. Get used to certain things in haskell and the rewards are HUGE (just like math). These were the hurdles for me: 0) Monads. Thanks to Real World Haskell and other great resources you can *start* with monads and get stuff done. Your average hacker (I'm no exception) needs to feel productive. So start producing perlish programs right away. Relax, rememebr von Neumann. 1) Types. Once you start annotating everything with their type, you start to better understand, er, get used to haskell programming. Guess-that-type starts to be fun. And when your programs compile the first time, they just seem to work. No more "everything is a run-time error" like p____n. 2) Recursion. The biggest hurdle for me was getting used to recursion over lists and such instead of iteration. I hear this isn't so hard for others. But then you are soon able to pattern match on constructers and getting to some of the powerful and elegant aspects of functional programming. Programs might be no easier to write, but changing them to add features is fun. 3) Modules, packages, tools etc. Major improvements since hackage got going. Seems like the proramming community is getting bigger as a result. Not just for the (friendly) gods anymore. hoogle is is great too. I stick with ghc and ghci since I am used to them. OK what are the rewards? 1) Haskell is a great vehicle to learn more about data structures, algorithms, logic, types and many, many other topics. If you like CS you will love haskell. It has been an amazing experience for me. Compare: Long ago I learned and wrote a program in Postscript. It me took three weeks. I learned about stack machines. I realized my HP 12C is based on a stack. Wow. 2) Some of smartest people in CS are involved with it, working on topics that are relevant for the future of software. (Hint: parallelism, concurrency). What other language is going inspire you put the third homomorphism theorem to work as quickly? Or make map-reduce simple to understand. 3) Thanks to Real World Haskell, Hutton's book and other resources you can make haskell take the place of your "go to" programming language. For a long time I still turned to perl and python to accomplish projects. Now I find I can do the same in haskell with all the side benefits mentioned. A final story: When I was kid somebody said, "You're good at math, take a computer class". When I saw "10 LET X = X+1", I new something was wrong. Programming didn't seem like math. Now it does again, and I'm still getting used that. I plan to keep it that way. 
I'd recommend just writing some simple programs. You really don't have to understand all the theory behind monads to understand how to use them. It's not any weirder than using a list (indeed, lists are monads), and I bet you can use those without much trouble.
&gt; pretty fast compiler Do you mean the compiler produces pretty fast ouput? Because unfortunately the compiler itself is not very fast, it can take several minutes and hundreds of MB of RAM to compile relatively small modules.
Haskell is great with IO though, once you put in a little time! There are some tricky bits when dealing with load, but working with sockets and servers is a snap, for example. Working with files and directories is trivial, you have access to all the posix bits if you really want them, etc. There are reasonable database bindings, and cloud/web2.0 service bindings via curl and the http library, and soforth. Outside of the fact that you have to get used to do notation (not that tough) then there's no reason to turn elsewhere for something that involves "lots of IO".
I think "School of Expression" is a *much* better Haskell primer than RWH or any internet tutorial. RWH is an indispensable tool, but SoE is a such a wonderfully written narrative through the land of FP. Out of nowhere, you realize you're reading a chapter about functional reactive animation - and you understand every word of it!
Haskell has IO, what's the problem? You don't have to understand the monad theory to use it. Oh well I'll write my own quick monad tutorial for using IO here... First you have actions. Running action yields in some value. Type of the action is Foo Bar, where Foo is monad type and Bar is value type - like IO String - IO action that yields a String value. Now you can compose actions using &gt;&gt;= operator. action1 &gt;&gt;= (\value_returned_by_action1 -&gt; some_other_action) Like: getLine &gt;&gt;= \s -&gt; putStrLn s Where putStrLn s yields action with type IO () - putStrLn is a function that returns IO action. The \s -&gt; putStrLn s can be therefore simplified into just putStrLn. getLine &gt;&gt;= putStrLn When you are disinterested in the value of first action, you can use &gt;&gt; instead of &gt;&gt;=. putStrLn "foo" &gt;&gt; putStrLn "bar" Finally, you can construct dummy action that does nothing but yielding a specified value with return function, for example return 5 gives an action that does nothing but yielding a 5 when executed. return "someshit" &gt;&gt;= \x -&gt; putStrLn x Results in IO action that prints "someshit", when executed, exactly like just putStrLn "someshit". What's important to notice, is that there is only a way of composing actions with &gt;&gt;= or &gt;&gt;, but no way of actually executing them in general (some monads other than IO allow direct execution). An IO action can be safely executed in only one way - by giving it to as a main value. The Haskell runtime then executes it. As you usually wish to use many IO actions, they have to be composed into one main action. It isn't really different from how you compose imperative program of subroutines. That's the separation of the pure functional Haskell from non-pure IO code. And that's all there is to using IO in Haskell. The do notation is just syntactic sugar over using the &gt;&gt;=/&gt;&gt; operators. Someone else may write about it, I can't bother now. ;)
Don't read any Monad tutorial. It is completely pointless doing so until you've used Haskell for writing small programs for 3 months plus. As BONUS_ said, there is a reason IO is introduced before Monads in Learn You a Haskell. You don't need to know they exist at all to do most practical things in the language. The fact that so much lip time is spent on them is beyond confusing for me. Yes, eventually they come in handy, but I have written some medium sized programs that just don't benefit at all from them. They are in fact quite simple, just difficult to explain. Their usage comes very naturally without being introduced to them after using Haskell for a while. Please, I beseech thee, forget about monads for a quarter of a year. As soon as any article mentions them, close that article immediately. You don't need to know, and it is in fact detrimental to read on. Learn do notation for IO, and be done with it, for a while.
As the title indicates, there's some dense reading ahead. Also part II: http://golem.ph.utexas.edu/category/2009/12/syntax_semantics_and_structura_1.html This isn't really haskell at all, but I ran across it in the types reddit, and thought it deserved a broader audence.
Personally, I find haskell extremely, extremely productive. Although I've heard mixed things from other people, the error checking in haskell, and the style the language enforces, more importantly, allow me to write programs faster than I ever have before. Only python comes close, for very different reasons, and it sucks balls for what I'm doing.
Yes, that's what I meant.
If you're reading anything else on this thread, you know already, that doing IO is quite easy and similar to imperative languages in Haskell. You could convert your old code almost line by line. IORefs are mutable variables, if you *must* have them. The problem with haskell and new users is that it lets you do *too much*. It lays bare all the mathematical plumbing beneath that imperative nomenclature you are used to. Fact is, you know *exactly* how to use monads, because you have been in every other programming language! You just never knew it.
&gt; OOP in haskell is difficult/unnatural I puked in my mouth a little.
Hmm. Network API comes to my mind. Imagine echo server: echo :: String -&gt; String echo = id So far so good. But we have to send one character at a time or have some smart flushing technic. Since we don't know if the next character is avaible now. We can decide that after 5 ms waiting we just flush without waiting but it is much more complex to code. Also consider such code: main = interact (\name -&gt; "What is your name?\n" ++ "Hello " ++ name ++ "!\n"); Simple? Well - no. It will print "What is your name?\nHello " and then stop. Rather not what we wanted. seq seems to not work (at least in GHC 6.12) and I'm not quite sure why. Compare with: main = do putStrLn "What is your name?" name &lt;- getLine putStrLn $ "Hello " ++ name In other cases it is usefull to have just lists (although I had problems when parsec tried to return it as strict).
&gt; Also consider such code: &gt; &gt; main = interact (\name -&gt; "What is your name?\n" ++ "Hello " ++ name ++ "!\n"); &gt; &gt; Simple? Well - no. It will print "What is your name?\nHello " and then stop. Sorry—I'm not sure that I understand what you're illustrating, so I might have missed the point. I get different behaviour in `ghci` from the command-line behaviour. In `ghci`: What is your name? Hello *J*J*a*a*d*d*e*e (where `*c*` is a character I type). This makes sense to me It outputs as much as it can (the constant string `"What is your name?\nHello "`), then outputs each additional character as it gets it. Since it is waiting for the whole string from `stdin`, it doesn't know to place the `!`. From the command line: What is your name? *Jade* Hello Jade *^D*! Again, we don't see the exclamation point until we terminate input (in this case with `^D`). I'm not sure why, this time, we don't see `"Hello"` until we type input—maybe compiled Haskell executables ‘know’ to do line-based IO? How are you using `seq` to try to resolve this?
I like haskell because it learned me to re-use code as much as possible and write the code always the re-use in the mind. maybe because writing code in haskell for me is still not very easy :) I still didn't get monads but I use them without the problem... I have been able to use Arrows from HXT intuitively without deep understanding :) thanks to compiler, when it compiles it usually works :) 
I can't tell if your comment was in agreement with the poster or against the poster.
&gt; OOP in haskell is difficult/unnatural The relatively-recent "type families" feature has cleaned up the last remaining desires for OO for me, in those cases where it still makes some sort of sense. I don't case if I'm "OO-pure" by any means, but there are times when you've got a heterogeneous collection of things that have "methods" (for some definition thereof) and where I was never able to bash straight typeclasses into what I needed, the type families have been working for me. You might want to check that out if you haven't already. I'm not 100% that I'm using them "correctly" and perhaps someone more familiar with Haskell will come up and yell at me that I'm grotesquely abusing them, in which case, listen to them. But I'm happier so far.
I read a few of tutorials before it sunk in, and I found the most intuitive one was this: http://www.haskell.org/all_about_monads/html/index.html -- particularly Part II, which gave real world examples of monads I was already used to (especially IO, Maybe, and List). A lot of the metaphor-centric tutorials just confused me until I actually "got" the concept.
I completely agree. Offloading verification on to the computer is a positive good. Dynamic languages are not going to get any safer. Static languages have gotten dramatically more convenient; and verification-through-types is a natural idiom with a long history. Tutorials and libraries for Haskell will all get better in the next two years -- as they have in the last two. Python is not going to get safer or faster; Java is not going to rid itself of `null` or get pointers. Architectural faults can not be overcome with libraries or books. Of course, it won't be too long before we have to say the same thing about Haskell... 
Small rant about intuition: I first learnt to program in 1998 in Haskell. I loved it. I then learnt Java as part of my programming course. This convinced me programming was not for me. I spent the next few years focusing almost entirely on systems administration. It wasn't until I tried functional programming again years later that I remembered why I loved it the first time around; everything made sense again. Imperative programming is *not* more intuitive than functional programming. It just happens to be the first paradigm you learned to program in. It'd be like saying Qwerty is more "intuitive" than dvorak or Windows more "intuitive" than Mac OSX. It's a red herring. I guess from how you wrote your blog that you probably already realise this since you mostly avoided the word "intuition". But I felt like ranting anyway; if anything it might give a view of what the world looks like from the other side. 
The lazy list I/O model is actually weirder than that. It's like: main :: [Response] -&gt; [Request] Then, you do I/O by producing a `Request` element, and then matching against the `Response` list to get the result of the operation. So your main function would look something like: main resp = PutStrLn "What is your name?" : case resp of Done : resp' -&gt; GetLine : case resp' of Line name : _ -&gt; PutStrLn ("Hello " ++ name) : [] You could implement `IO` using this by storing the two lists in a `State` monad, which I'm sure you'll agree, is much nicer than the above. :)
It still beats g++ hands-down, for code of similar complexity.
You're talking about value-based dispatch? For that wrapping a type class up using existential quantification works fine. You're basically doing the same as traditional OOP languages, you just have to create your own type that bundles the "vtable" together with the value - then of course you would instantiate that wrapper type into all the classes it wraps so you can use it directly. E.g. http://www.haskell.org/haskellwiki/Existential_type#Dynamic_dispatch_mechanism_of_OOP Could you give an example of how you're using type families?
Did you tell Simon PJ that?
I don't think I did. It's not something that I need desperately. 
Yes, like our lecturer's insistence on using falafels to explain abstract data types in ML. Seriously worst metaphor ever!
It's actually entirely to do with the buffering behavior. GHCI reads stdin character by character, i.e the buffer is instantly flushed, whereas command line by default will do line buffering.
Admittedly, that's probably Haskell's main pedagogical problem, the hype surrounding monads. Sure, experienced Haskellers will use/understand/appreciate them, but most of the noise out there is by beginners. If you ask anyone who has heard anything about Haskell what they know about it, they'll probably say "Strong types and Monads". If you talk to the general programming community, there's going to be at least one person saying "Haskell seemed interesting, but I just couldn't grok monads..." (implying that they're somehow essential to programming in Haskell). This reputation we have of requiring deep monad understanding is harmful. There was a guy on the #haskell IRC channel just the other day who had read a couple of Haskell tutorials, hadn't even learned the basic Haskell syntax yet (things like lambdas and layout), and was already asking people to explain monads to him. We kept using basic haskell examples and he'd (understandably) get frustrated and told us we were being obscure. I don't regard this as a shortcoming of the learner in question but rather see it as a result of the unfortunate reputation the language has acquired. Pretty much every learner who shows up expects to understand monads within a few days, because they're perceived as being essential to Haskell programming. What's more, those who persevere enough and a couple of weeks later finally come into a degree of understanding of them often write a blog post along the lines of "monads were really hard!! [because you started trying to learn them too early...] but I figured them out, and here's the perfect analogy for them!". New learners then go and search for monad tutorials and come across yet another person talking about how hard monads are^H^H^Hwere and often giving inaccurate or misleading information on the topic, and the sheer number of monad tutorials out there will reinforce the notion that monads are fundamental. Anyway: **Haskell Is Not About Monads! (HINAM)** Because abstraction is cheap in Haskell (both syntactically and performance-wise), we tend to do a lot more of it. Patterns that are written in English for other languages can often be encoded elegantly in code in Haskell. Monads are just one such pattern (although probably not one you'll find in Design Patterns), and a fairly simple one at that. They simply allow us to factor out certain code structures that are common but not very obvious in imperative languages. You can write everything you write monadically in Haskell without monads. You have also probably used and written monadic structures in other programming languages without even knowing it. If you don't understand monads at first, please don't get frustrated! Imagine you were new to programming at all (Haskell is sufficiently different from most languages that you're effectively in a foreign country even ignoring monads): would you pick up a book on design patterns and try to figure them out while you're writing your hello worlds and your fibonacci and your simple drawing programs? Probably not. Would you eventually learn good practices and design patterns, once you felt more comfortable with translating your ideas to code? Probably, if you wanted to be a good programmer. It's the exact same thing with Haskell. Learn to feel comfortable with the type system, play with IO (you don't need to understand monads to understand a particular example of one), learn to express your imperative instincts in declarative code, and then, once you feel comfortable with all that, experiment with monads. If you actually understand the Haskell they'll built from, you'll find them almost surprisingly easy and convenient. Also note that I never mentioned any Category Theory. Haskell is a more mathematical language than most, and it might draw you into areas of mathematics that you never knew existed, but being a mathematician (or even having a CS degree) is by no means a requirement. "Scary words" like Monoid and Monad and Functor may abound, but once you realize that mathematicians just like to give names to things they see a lot too (just to avoid repeating descriptions in discussion), and that the concepts behind the scary names are pretty trivial, things get a lot more pleasant (even outside of Haskell). Good luck!
As mentioned, I'm taking a bit of a hiatus till January 9, since I basically won't be around for the next two weeks. Maybe I'll try to pull together some quotes for y'all. :) Happy Holidays Haskellers!
If it's taking several minutes for small modules, something's wrong. Unless you have massive amounts of static data in your source files, which GHC in particular does not like.
Consider this recontextualization: the more editors you have, the better your writing will be.
I use existentials all the time for things like this.
I've been working on my technical-writing skills recently. Here's one of my attempts; it's the result of six months of on-and-off work on implementing, testing, and documenting a parser for the SGF file format (there's a spec spread across various pages [here](http://www.red-bean.com/sgf/)). It uses just about every single feature that Haddock offers. =) There's plenty more work to be done to turn the library end of this into something that deserves the name "library", but I sure would appreciate any comments you have on the documentation of what's there so far.
Yes, I agree, and I think that better writing in this case corresponds to *more* (not less) idiomatic code.
Err, yes, turns out I was agreeing with you, though I didn't realize it because I didn't read periodic's message carefully enough. =P
Thanks, as someone trying to learn Haskell this looks like a pretty good example of an elegant way to solve a particular problem. I'm not sure I completely understand everything, but it seems to use a lot more generalized types than I have so far managed to make use of in my coding, and shows why they are useful. I like how Haskell problem-solving seems to be about building a library of words designed to express a problem domain, and then writing out the particular problem in a very straight-forward manner. I hope to be at this level one day where I can basically write a mini-language to concisely express the problem I'm working on.
Thank you for writing this! I have tried to explain this to so many people, and now I can just point them here. PS You did a much better job than I would have.
I think part of the reason for this unfortunate reputation has to do with featuritis. OO was big on espousing features and telling everyone that they need objects to make their programs better, so people look at OO languages and think "I need to learn about objects to grok this language". But it didn't start with OO, and it hasn't ended with them. The whole X-oriented genre has made this worse by equating a language with a single feature rather than a collection of them. Back in the day C++ was sold with a grab bag of features; but these days it's all objects or aspects or vectors or whatever. So when folks hear about Haskell they'll start looking for a single feature to latch onto which "explains" the entire language. Unfortunately, the history of the static-vs-dynamic type war means that "strong typing" doesn't mean to outsiders what it means to us; so they'll notice it, but they won't latch onto it really. And type classes sound unfortunately similar to object classes, so those are misunderstood as familiar too. Ideas like laziness and referential transparency are too "academic" for newbies to latch onto (i.e., their benefits are too subtle/pervasive to fully explain in a paragraph, including convincing examples). So what's left? Well, there're these monad things... and we need to use IO to print "hello world" and then "goodbye world"... so obviously Haskell is all about monad-oriented programming. *sigh*
nice!
My moment of epiphany with the type system is when I started realizing that it was turing complete... and even though I don't use it as such, phantom typing and witness types have allowed my to create a set of basic pluggable functions and then glue them together using the type system. My favorite example of this was when I wanted to write a cryptogram solver, and it ended up being: &gt; import TQ.ConstraintSolver &gt; main :: IO () &gt; main = interact $ unwords . solve (undef :: MostConstrained) (CGram wordList) . words wordList was just a list of words in the english language, CGram was an instance of ConstraintPuzzle that knew how to expand a certain partial solution to a list of replacements for each variable and tell if a solution was done, and 'solve' actually searched through the problem with the appropriate strategy. It truly felt badass to take that toolkit and write a solver that fast. (The "CGram" part may seem like a cop-out, because it told my toolkit the rules of cryptograms... but it was the basis of an OTP solver that I had written a while back, and so was part of the toolkit. It ended up being around 12 LOC, IIRC) EDIT: Finished post after hitting submit halfway through the first line.
The Haskell type system is not Turing complete (at least not without turning on some scary extensions). That's a feature, it means that type inference and type checking works.
Agree here... and this is an important point. If the type system were Turing complete, your compiler might hang. Remember that type annotations, in Haskell as in C, are only used during compilation and aren't represented in the finished program.
I believe there is a vague idea of types in the final program in Haskell because otherwise something like existential quantification or typeclass overloadery might be difficult? If that's not the case, can someone explain how it works?
So theoretically the type checker might not hang, but with a handful of lines it will need more time than the age of the universe. 
http://www.youtube.com/watch?v=3GwjfUFyY6M good job guys!
no. Just kidding, hee hee. You know you want to :)
&gt; but once you realize that mathematicians just like to give names to things they see a lot too (just to avoid repeating descriptions in discussion), and that the concepts behind the scary names are pretty trivial, things get a lot more pleasant If you walked up to my discrete math teacher and said you knew what a Monad was he'd put 3 questions on the next test asking you to prove non-trivial theorems about Monads. And if you got any one of them wrong, even some minor mistake, he'd tell the entire class that you don't know anything about Monads.. That guy was a dick. I still like Category Theory, though..
Very true. There's more to the language than Monads. It's totally possible to be productive in Haskell without going much deeper than the IO Monad, and you can use that without totally understanding what's going on underneath.
yeah, our kids will be programming in visual epigram :)
The only trace of types left in the final program are typeclass dictionaries. They're basically like vtables in C++, so if you have an existential wrapper with a typeclass constraint, you pass along the dictionaries of functions that know how to operate on your value (because the compiler has "forgotten" what the type actually was). Polymorphic functions work the same way, assuming you don't use whole-program compilation and functions don't get specialized for the actual types used.
Classical pure lazy FP FTW!
If we can be programming in Haskell, maybe, yeah, our kids will get to do that.
woo. When did the Haskell reddit get the old logo back?
I know... *it's about you!!* ;)
It's the festive season!
Coming to Haskell from a fairly strong functional-programming background, I found monadic IO to function mainly as a barrier between pure functions and the real world. Once you leave the world of pure functions, Haskell hurts more than even a terrible language like Java: basic operations like iterating over a sequence, you end up having to rewrite entirely if your sequence is generated lazily. My concrete example is the Unix "find" utility: you want this to generate a list or tree of filenames and stat structures, but monadic IO forces you to either - read the whole filesystem tree before you can do anything to any of its members (mapM); or - rewrite all the basic list/tree operations to work with lazy lists/trees; or - use unsafe*, in which case why am I in Haskell again? This last is the problem. The pleasure of writing Haskell comes from its purity, but efficient real-world IO requires either abandoning purity or rewriting the language. So far Clojure, TCO aside, looks a lot nicer. 
Sounds like a job for iteratees. I still hope for copy-on-write FSs to get rid of most of the purity headaches.
even python has infinite lists...
I'm not sure what you're talking about. I've written thousands of lines of Haskell, including some very IO intensive stuff and never used an unsafe* function once. What can you do in Clojure, or any other language that can't be done in Haskell w/out unsafe*? 
What you're describing is, I believe, a real problem with Haskell, which is currently indeed often "solved" with unsafeInterleaveIO [lazy IO]). I agree this is not ideal -- and more work needs to be done to resolve this in a better way -- but this isn't an inherent flaw - and in the mean time, lazy I/O, with all its disadvantages, is still not throwing away purity completely. The solutions to this problem are probably (I believe this area has not yet been fully explored) using either Iteratee (which can iterate over pure or non-pure sequences similarly, and using highly similar code to pure iteration), or something like the List class (see the List package), with a proper ListT monad transformer. If you look at these [slides](http://okmij.org/ftp/Haskell/Iteratee/DEFUN08-talk-notes.pdf), you see there are solutions that look and feel like lazy I/O, but are in fact properly wrapped (purity preserved), and can be used similarly for pure/non-pure processing.
Oh so they are like burritos. 
It's all about the Benjamins.
While its a beat of a cheat to write the library *after* the challenge has been presented, I find it hard to believe the Arc version itself wasn't chosen based on the things that are easily possible with the library, or that the library was slightly modified in light of thought of this particular example.
All the basic list/tree operations do work with lazy lists/trees? There are lots of ways to do it, but in the concrete case you don't need to generate a lazy tree of an entire filesystem (though you can) -- instead you can use recursion to walk the fs directly. Which is, I imagine, pretty much how you'd do it in clojure.
I understand your basic point that people shouldn't try to learn haskell by understanding monads, but to be a good haskell programmer, you *do* need to understand monads. This is because the haskell solution to so many real world problems involves monad transformers and once you start using mtl, you *do* need to understand monads. Otherwise you are essentially left guessing which lift operations to use and whether or not you need to chain a few of them together. Check out [my conversion with dons](http://www.reddit.com/r/haskell/comments/9vilt/where_is_the_people_who_does_not_like_haskell/c0enr5c) for examples. Notice that when I asked dons to show me some code, he just ignored the request.
Not sure it proves anything that Don ignored your request to provide a link to a substantial amount of proprietary code. Don't get me wrong, I agree that a significant amount of real-world Haskell code *does* involve stacks of monad transformers. On the other hand, I think that's largely because it's a step in the process of moving from an imperative mindset to a functional one. I find that as I become more skilled at Haskell, I tend to use fewer stacks of monad transformers. This is, I think, a function of finding a better separation of concerns. I now consider too many calls to "lift" to be a sign of seriously troubled code.
It definitely doesn't prove anything that he didn't show the code. Though he could have said something like *"Sorry, that code is proprietary, but if you post some example code, I could rework it to avoid the monad transformers."* Ignoring my request seems to match Galois' pattern of cheer leading the use of haskell in industry without backing it up with real world examples. Their customer list is mostly the government, which has unique requirements and spend other people's money, giving them less incentive to keep costs down. I just don't buy that haskell works for real world problems, with normal time and cost constraints and I won't believe it until someone shows it.
In Clojure, I can use lazy-seq to turn my "get some filenames" function into a sequence of filenames, which I can then operate on with all the normal sequence functions. In Haskell, the "get some filenames" function lives in the IO monad. The sequence of actions looks something like IO ("dir" : IO ("dir/subdir" : IO ("dir/deeperdir" : ...))) How do you suggest I turn this into something I can run "print . unlines" on? (Answers which involve executing the innermost IO action before I print anything are incorrect: putting the entire filesystem on the stack is not viable.) 
Just a thought: Perhaps someone familiar with the subject matter could summarize the important differences between Control.Concurrent.CML and Control.Concurrent.CHP. Why use one over the other for a given application type, etc.
&gt; It's totally possible to be productive in Haskell without going much deeper than the IO Monad Perhaps it might help to be able to dabble in the `Maybe` and `[]` monads. :-)
This scores more points for total languages :-) The problem he's experiencing does not exist there, iiuc.
Really? In the only total language I've used (agda), the only way to even create infinite datastructures (like the infinite tree he's talking about) is to use codata, which behaves similarly to lazy Haskell data. Not that I've thought very hard about this, so it may not be a problem :)
It's an interesting question, and one I'll think about. About to get on a plane now, so can't do much now :)
Well, it seems that he's claiming that if you take: s -&gt; (s, a) and use the (a -&gt; m r) -&gt; m r transformer on it: (a -&gt; (s -&gt; (s, r))) -&gt; s -&gt; (s, r) you get a strict-bind, rather than a lazy bind. So instead of (&gt;&gt;=) being: (s -&gt; (s, a)) -&gt; (a -&gt; s -&gt; (s, b)) -&gt; s -&gt; (s, b) you get: ((a -&gt; (s -&gt; (s, r))) -&gt; s -&gt; (s, r)) -&gt; (a -&gt; (b -&gt; (s -&gt; (s, r))) -&gt; s -&gt; (s, r)) -&gt; (b -&gt; (s -&gt; (s, r))) -&gt; s -&gt; (s, r) which is too big for me to work out, but the CPS here should somehow mean that the state is no longer lazy... If evaluation order doesn't matter (e.g total language), then this kind of thing shouldn't matter.
When I read about Iteratees, I didn't really see how it would be helpful. But it looks like your [MList library](http://haskell.org/haskellwiki/Mlist) might be just [what I want](http://www.reddit.com/r/haskell/comments/ag6bv/should_i_give_haskell_another_try/c0hhonw)---modest of you not to mention it. ;) Would I be crazy to give it a try next week? 
Is there much in the way of tutorials and the like around for happstack yet?
http://tutorial.happstack.com/
this is a very useful post :) thanks for taking the time to post it. i hope you were sarcastic. [1729](http://en.wikipedia.org/wiki/1729_%28number%29) is not boring. 
Sadly, a simple "strict" or "lazy" adjective isn't really enough to describe the issue. The state itself is treated lazily in both cases, the difference has to do with the properties of bind. I'm not sure whether the following observation is the entire story or not, but this property can easily be checked: run (⊥ &gt;&gt; return 0) initial_state Given the lazy state monad, this evaluates to `(0,⊥)`, whereas given the continuation state monad, this evaluates to `⊥`. Note that in both cases, you can pass `(error "state")` in for the initial state, and it does not affect the outcome. As for the bind operator for the continuation state monad, it's basically the same as the bind operator for the continuation monad. Written out the full type as you have, you can eta-reduce all mention of the state away. Although, if anybody can explain the bind operator for the continuation monad, I'm all ears. I understand it, but can't explain it very well.
That is one of the worse logos I've ever seen. I know this is /r/haskell and not web_design but... Jeez....
I quite like it. The shape is stark, interesting, and recognizable. I think that while the image could use some styling, but the logo itself is good.
&gt; It should be noted that I knew full well that the code I was trying wouldn’t &gt; work… but after hours of bewilderment, not even trying to load anything into &gt; GHCi, for amusement’s sake I simply had to try something. Oh yes, I've been there. And sometimes it *does* actually work - then you have to work out why ... 
Since you have to model every aspect of a natural language to get a general purpose translator I think the projects has a long way to go (but that is just from the pizza/fridge examples on the site). But I read somewhere that they where hoping to use it in EU to share documents between different languages. And since most of the documents seems to confirm to the same style the gramma won't be super huge. Looking forward to more progress. Perhaps one should try to use it for menus and other small things?
I'll add it to my list of blog posts to write :-) I think the answer is that CHP can do everything that CML can, bar one feature. But I guess CML is more light-weight, and probably faster (I haven't looked at its implementation yet, but CHP has a lot of bells and whistles to support).
Oh, MList is my own attempt at "fixing" lazy I/O, but I didn't like the result very much, it has several problems. The "List" package (along with "generator", iirc) were written by a friend of mine with the "lessons" from the mistakes of MList. MList is really just ListT, but not as complete/correct as the one in List. 
I'm eager to hear of any answer you may have. My dream is that there is a clever solution that unwinds the layers of IO piecemeal in the order in which they are generated and yet still works with sequence operations, and I'm just not enough of a master of inside-outness to find it. Haskell is a very pretty and satisfying language as long as I can push all the IO to the outside layer. 
Am I correct that [this](http://hackage.haskell.org/package/List) is the package? If it is an implementation of the ideas at [ListT done right](http://www.haskell.org/haskellwiki/ListT_done_right), that sounds like what I want. 
C++ compilers in general are insanely slow, g++ in particular is one of the slowest. I think a better comparison is against something like ocamlopt, which is much faster.
Or use any modules that happen to make ghc slow as hell, like Language.Haskell.TH stuff for instance.
I look forward to reading it! In my experience CHP has been very easy to work with, and very well behaved.
Yeah, that's the package. I don't know how much of ListT done right is in it or not. I think it has the good ideas from there, with even nicer generalizations in place. 
I recommend you give that one a try and see if you can get it to work as I described. The problem is that getDirectoryContents :: FilePath -&gt; IO [FilePath] so, as I mentioned in the comment you're replying to, you end up with a tree of nested IO operations. You can't just map something to a flat list of IOs; you have to execute the one layer of IO actions to get the next layer. 
s/deBuisson/DuBuisson
Fixed. Apologies.
I approve. It is quite jolly.
Here's [another one](http://gitit.net/paste.lhs), using happstack-server and HDBC. 
&gt; And sometimes it does actually work - then you have to work out why ... I've been there as well... as I mention in the paper, although on my next attempt I was confident that my code would work (and it did), it was quite difficult to come to an understanding of what I'd just written. I think Audrey Tang has experienced this phenomenon as well. I hope that my paper will help others understand my code much more quickly than I did. ;-)
Ah, the Haskell programmer's burden.
I don't think I've seen such a mix of metaphors in a long time. Yet somehow that very mix accurately expresses our uneasiness about the current incohesive state of FRP. This is one of those literary classics that will never be recognized as such except by the geekiest of geeks.
Well, we can tell from this that you are a haskell programmer and think quite a lot of yourself. [I think this is more accurate](http://imgur.com/P9RnL)
Could you please provide a link to the (unextended) original?
[Reddit thread over in /r/programming](http://www.reddit.com/r/programming/comments/ahaz9/how_programming_language_fanboys_see_each_others/)
Why would the Ruby fans think that Haskell is like Japanese? Anyway, I like that the Haskeller's opinion of Ruby is of Calvin and Hobbes, because Calvin and Hobbes are awesome.
Does it really look I'm being egotistical? You got three upvotes so far, so I conclude some people see it that way. I'm a very mediocre Haskell programmer. My day job is PHP, for what it's worth. The joke I'm going for here is that the Haskell community likes exactly the things that everybody else hates. I picked Einstein because he represents both impenetrability and revolutionary brilliance.
Ruby is the transmogryphier of languages.
I've largely come from a python and c++ background, and haskell looks like line noise to me. Aside from that, half the articles here have titles like "The flux capacitance of the monad is merely a curried homoisomorphism of a finite co-concurrent conjunction!". Haskell appears to be an entirely new language to people used to writing "100.times do x".
&gt; Does it really look I'm being egotistical? It appeared that you were saying that everyone sees haskell programmers as revolutionary geniuses. Anyway, these images should be more about self depreciation than anything, calling yourself Einstein doesn't really help.
&gt; The flux capacitance of the monad is merely a curried homoisomorphism of a finite co-concurrent conjunction! I think you are on to something. Please upload to [hackage](http://hackage.haskell.org) when you are finished.
The original poster made it clear that it was a satire of language "fanboys". I changed that to "fans" only because I didn't want to exclude fan*girls*, but maybe that loses something. Perhaps it should be "zealots".
I had more an impression of "crazy academic " than "revolutionary genious" by that photo, but maybe it was just me.
I came here to say that all fanboys should die (any language) but you already proved my point :)
The term "fanboy" applies to fanboys of both sexes and all ages.
I glanced through the comments of the original post, and it looked like a bunch of idiots. But maybe I missed something.
yeah, was writing off the top of my head without checking it. A bit hard to get it down to a one-liner, though I'm sure more experienced haskellers could do it. Also, its broken up a bit for readability here. mapM_ myAction &lt;$&gt; getDirRec myDir where getDir f = getDirectoryContents f &gt;&gt;= return . map ((f ++ "/") ++) . filter (not . flip elem [".",".."] ) getDirRec f = getDir f &gt;&gt;= \d -&gt; fmap ((d ++) . concat) (mapM getDirRec =&lt;&lt; filterM doesDirectoryExist d) I'm sure this could be done in a better, more efficient, lazier manner but i'm too tired at the moment. 
Man, you don't know what programming language line noise is if you think that Haskell looks like line noise. Ever try APL?
I thought the joke was that Haskell fans are as mystified by Haskell as Java/C/PHP/Ruby fans are, which has some truth to it I think. :)
&gt; The flux capacitance of the monad is merely a curried homoisomorphism of a finite co-concurrent conjunction! Obviously. I think I have a module in category-extras for that.
FTR, I interpreted the use of Einstein for the other fans as "crazy, incomprehensible math" and for the Haskell fans as "eloquent math." That's close to "impenetrability and revolutionary brilliance." I thought it was appropriate and not egotistical.
Stuff that I haven't learned yet is hard; film at 11.
"Duh, it's all about burritos", she encapsulated for effect.
What I like is the "C as viewed by Haskell fans" box. As a Haskell fan, both C and Dynamite are dangerous yet sometimes useful and very powerful.
Sorry, I suppose I didn't intend the hastily created comedic picture to be massively informative. Your expectations of silly pictures is obviously much higher than mine.
Indeed, it doesn't compile, perhaps because I don't know how to get the compiler to recognize &lt;$&gt;. (Apologies if it's obvious: my Haskell experience is narrowly focused on what I found when working on this problem, and "&lt;$&gt;" is tough to Google for.) That said, unless &lt;$&gt; has some magic in it that I'm missing, you seem to be going through the process I first went through when I struggled with this problem in the first place. ;) The problem with "mapM" here is that myAction =&lt;&lt; mapM (somerecursivestuff) contains that pesky sequence operator, so the entire pile of recursive stuff gets done before myAction, down to the very last leaf node of the filesystem tree. If you're not the most experienced Haskeller you know and you're genuinely interested in the problem, please do play with it as a learning experience until you've solved it for sure. Taking successive guesses like this is a little frustrating. As a test for your solutions, use "print . unlines" as the action and "/" as myDir. If you get filenames scrolling down your screen right away, you win; if it sits for a while and then pukes because it's out of memory or the stack is too deep, try again. Note also that, elsewhere in this thread, other Haskellers are saying that this is a genuine problem and may be solved by ListT. I haven't yet taken the time to try out ListT, but that's where I am planning to expend my next chunk of effort on this.
I still don't completely understand exactly how C programmers see Java programmers.
Surely some of them think of us [like this](http://t2.gstatic.com/images?q=tbn:0LfyHNoNA6P6BM:http://4.bp.blogspot.com/_iLSmTPwJGZY/SsDAjGPNPJI/AAAAAAAAgFg/-iU-JfOtxB8/s400/18.jpg)...
yeah, i realized my solution isn't properly lazy. as far as &lt;$&gt;, just add this at the top: import Control.Applicative ((&lt;$&gt;)) its essentially the same as `fmap`
I surrender. They're right, you do need unsafeInterleaveIO .
Feel it; don't think it.
A pain in the tits?
`replicateM 100` does what your `100.times do` syntax does. Haskell doesn't look like line noise! :) [Edit: Made code better because I'm dumb.]
well done indeed! (though i don't get the haskell-as-seen-by-c-fans one)
you do know where ruby comes from, right? (:
we hear you roar.
And you will be ta∴∵s∴∵nn?
:) I was going for "young geeks prone to exaggeration". 
It certainly can though (I'm not arguing in favour of ruby btw). From the language shootout: pidgits n = 0%(0#(1,0,1)) where i%ds | i &gt;= n = return () | True = putStrLn (concat h ++ "\t:" ++ show j) &gt;&gt; j%t where k = i+10; j = min n k (h,t) | k &gt; n = (take (n`mod`10) ds ++ replicate (k-n) " ",[]) | True = splitAt 10 ds j#s | n&gt;a || r+n&gt;=d = k#t | True = show q : k#(n*10,(a-(q*d))*10,d) where k = j+1; t@(n,a,d)=k&amp;s; (q,r)=(n*3+a)`divMod`d j&amp;(n,a,d) = (n*j,(a+n*2)*y,d*y) where y=(j*2+1) Seriously: k = j+1; t@(n,a,d)=k&amp;s; (q,r)=(n*3+a)`divMod`d Haskell can very easily look like line noise
That code is golfed to Hell though, and while the Ruby version is... "prettier" (and, notably, not golfed), I don't think it necessarily expresses the solution any better.
This should be crossposted to /r/CircleJerk.
TECO! ... it actually is indistinguishable from line noise.
I think Java, C, and Haskell are the only relevant languages on here. Sorry PHP, Ruby. 
OK.....I'm gonna admit, there's a problem in the haskell community: the symbol names are too short! I know it's me but I'm used to math and just using x, y, z, f(x), etc as symbols, and that's how it comes out. I fully acknowledge, however, that one of the most common haskell idioms `(x:xs)` for instance, is much harder to parse than something like `(head:tail)`. This above is perhaps the best and worst example, because no one that was actually writing code that other people would use would be so callous, but on the other hand, you're seeing what the typical haskell programmer is going to write first before they polish it.
This makes me kinda sad :( I was hoping to see Perl on the board.
I've updated the library -- improved interface, documentation and a basical sample. See the github page.
I unsurrender. I can't get it as elegant as a lazy list, basically needed to write my own recursion. I think this solves your problem without any unsafe* calls. mapDir :: (FilePath -&gt; IO a) -&gt; FilePath -&gt; IO () mapDir print "/" where mapDir fxn fp = getDir fp &gt;&gt;= \d -&gt; mapM_ fxn d &gt;&gt; (mapM_ (mapDir fxn . ("" ++)) =&lt;&lt; filterM doesDirectoryExist d) getDir f = doesDirectoryExist f &gt;&gt;= \b -&gt; if b then getDirectoryContents f &gt;&gt;= return . map (\a -&gt; concat [f, "/", a, "/"]) . filter (not . flip elem [".",".."] ) else return [f] 
I think that was intended to be a "timebomb". 
Not bad. Unfortunately, there's a reason I said "print . unlines", not "print": I want to be able to operate on the whole resulting data structure with standard tools like "map" and "fold" (or their lifted friends), as I could trivially do in another language. For example, in Ruby, I can implement "each" and then I get all of Enumerable (including map and inject), or in Clojure, I can use lazy-seq and get the whole sequence library (including map and reduce). To get equivalent functionality from a monadic list, it seems one has to directly reimplement every single core list function: I can't just write something of the form toMonadicConsCell :: MList m a -&gt; m (a : MList m a) and get fold, map, zipWith*, etc. for free. I gather that Iteratees solve that problem, in the sense that rewriting all the list functions once to work with Iteratees would make it work for all Iteratees; so if Iteratees were part of the Prelude, then Haskell would be usable for my (extremely basic and ordinary) purposes without thinking deep thoughts about monads all the time, which was the original issue I was replying to. 
i was happy that at least I didn't have to completely surrender. its a fair point that its harder than it should be. iteratees look interesting, I'll have to take a look at them
replicate first of all, second of all, replicateM :P
I [added Lisp](http://imgur.com/1gF1j.jpg).
As a computer scientist, can you parse humour? :)
Bleh... I absolutely disagree. Variables are placeholders in a functional language where they are immutable and typically locally scoped. That's why we prefer point-free style when we can get away with it. Imperative languages are constantly concerned with what a variable contains and so giving them meaningful names is important. This is far less important in Haskell. I find the newbie tendency to give variables verbose names like `theHeadOfTheList` actually *decreases* the readability of Haskell. And frankly, if your function is long enough that the definition of whatever variable you're wondering about isn't defined within three lines of the place it's used, it needs to be broken apart anyway. Just say no to long variable names in Haskell!
Also, you could do something like c &lt;- newChan forkIO $ mapDir (writeChan c) "/" lazyListOfDirs &lt;- (forever $ readChan c) mapM_ blah lazyListOfDirs not in in front of GHCi, so that may have errors, but its close to what you want
It's the comonadic version of http://www.haskell.org/haskellwiki/Zygohistomorphic_prepromorphisms , isn't that right?
Do mind that this is about the _fanboys_, not the languages themselves. Personally, I'm proud to be a bomb-carrying member of the terrorists.
The first is "ecks", and the second is "eckses". See? If you're matching a string, it'll be `(c:cs)`, for "char" and "chars". There's one on the left side of the colon, and zero-or-many (which goes with a plural in english) on the right side, analoguous to the colon that constructs. What I'm saying is that it's only difficult to parse if you don't _know it_.
+1 for /r/Circlejerk
seriously? ruby is very debatable on its relevance, but PHP (although shitty) is unarguable relevant. btw java sucks, ruby will replace it. while we're speculating on languages of the future Javascript will be the new C.
"btw java sucks, ruby will replace it" Really, the open source language will replace the most popular language. When do you see that happening? 
Are you honestly saying that the code above is in it's ideal form? I mean, you honestly think that *that* is as readable as that code could ever be? I know what you're saying, and this is the way I write code too, but there is some common ground somewhere between the massive overcompensation of `theHeadOfTheList` and having 15 different 1 letter symbols that are reused differently in every `where` clause.
So, you're going to write an OS kernel in Javascript? Or, video/audio decoder that runs on mobile hardware?
Here are some ideas. First of all: take n . repeat = replicate n I think your library could benefit from a typeclass to represent values: class Values a where toU :: a -&gt; UArr Double fromU :: UArr Double -&gt; a instance Values (UArr Double) instance Values [Double] Then your functions can have type signatures like computeLayer :: (Values in, Values out) =&gt; [Neuron] -&gt; in -&gt; out which works for both lists and UArrs (and a combination). Oh, and how the hell can you do backprop of a Neuron if only the transfer function is known, and not its gradient?
please read OOHaskell paper. even if you're not interested in type hacks, there's still many examples of similar encodings with their pros and cons.
In this 0.1 version, for backProp, I assume the transfer function is sigmoid (since heavyside can't be used for that). But it'll be arranged soon. For the 0.2, I aim at opening the way to any transfer function we want, only asking for the function itself and its gradient. Your suggestion for the Values typeclass is very interesting and may be integrated into hnn for the 0.2 version too. Thank you !
Thanks for the pointer, will read.
Man, I'm a dummy. Where's the hlint plugin for Reddit?