I see where this is going. It won't be long till we have frameworks with cloud kinds. 
&gt; the last time I closed Leksah it crashes and now it won't start. It would be really good to get to the bottom of this. Are you running 0.15.1.3? If not it would be good to upgrade in case it is something we fixed already. The metadata collection process [leksah-server.exe crashes on Windows when you close Leksah](https://github.com/leksah/leksah/issues/130). There is something going wrong in the way it shuts down, but that should not prevent Leksah from starting up. The warning about libenchant_myspell.dll happens on mine too, so I do not think that is the problem. There used to be a problem starting leksah if ghc-pkg was not in the PATH, but that should be fixed in recent versions. Can you try running: cd C:\Leksah\bin leksah --verbosity=DEBUG That might give us more clues (at the very least it would be interesting to see where it gets to before it dies).
I went through setting up MSYS2 and installing openldap - everything went smoothly and seemed to work great. When I try to install LDAP the dependencies are recognized, but then when it's trying to run this "Init_hsc_make.exe" I get a pop-up about not being able to find liblber.dll even though there's a liblber.dll.a in the /lib directory and I added that to my path. However, if I copy liblber.dll.a to liblber.dll, it "finds" it, but then I get a similar pop-up of "liblber.dll is either not designed to run on Windows or it contains an error". You've been a huge help already and I'm really sorry to bother you, but I was wondering if you had any ideas regarding potential issues?
Haskell isn't a proof system, and never will be. Coq and Agda care a lot about totality, because it's a way of ensuring *consistency*: we know we can trust a proof if the compiler accepts it - because a theorem is a type, and if that type has an implementation, it is amount to proving that theorem. Haskell already had `undefined :: forall a. a`, so we already threw that out the window a long time ago. We can prove 'any' proposition, even `True = False`. So there's no totality checking, although this does mean the compiler might loop forever. We can just say "if your type level program loops forever, that's your fault." In practice the amount of recursion GHC will do is bounded and tweakable. And writing a program that loops the typechecker forever isn't very useful anyway, so this limitation doesn't seem very bad for regular programming.
The examples in the post need much more explanation. Just adding a sentence or two of commentary on how to read this stuff would be tremendously helpful for people who don't follow this topic nor are advanced in type-level programming. For example, I cannot even read the `prox :: Proxy * Bool` part. Is the `*` being given for `k` kind parameter? My problem is that the examples are quite meaningless. Also, as a practical and useful application example, I am quite amazed about how `servant` is done. Can anybody comment about how this change will improve the life of `servant` devs and what they will be able to do that they cannot do today?
Ooh, I was under the impression no progress was being made on that. The next few GHC releases are looking to be very exciting.
My packages are all well maintained with full test suites. Because they are in Stackage, those test suites are run automatically and routinely and I am notified of failures. Hackage on the other hand has no quality standards. I am merely recognizing the truth. Since I do maintain all my work and because I put packages in Stackage precisely to help maintain their high quality, it is odd that you would suggest that my work is of low quality merely because I recognize the truth, which is that Hackage does absolutely nothing to enforce or maintain quality standards.
As /u/int_index said, Idris has implicits and calls them type classes (ekmett's [talk](https://www.youtube.com/watch?v=hIZxTQP1ifo) goes in to the differences and why they matter). IIUC, Idris is fully satisfied with local inference, so every top level function, no matter how simple, must be annotated with a type, no matter how obvious; Haskell, OTOH, degrades more gracefully, and one of the design goals of this proposal is to keep inference at least as good. There are many things to be said for explicit laziness (when done well like in Idris, which AFAIK is the only language that can be said about), but also many things to be said for pervasive laziness; at the very least, it's a reason to use Haskell.
Heh you're a cool sir (:
Is the [ally skills tutorial](http://catamorphism.org/Ally2015/) at ICFP'15 (granted, not exclusively Haskell) the sort of positivity in the spotlight you're thinking of?
You can take this all the way, and end up with a syntax that's similar to the one you get when you do monads via delimited continuations, or C#'s async/await. The idea is that you have an operator `@ : m a -&gt; a`, which works like this: do ... if @doesFileExist f then ... else ... This would work inside any expression, and use left to right evaluation order: do @x + @y Turns into: do x' &lt;- x y' &lt;- y return (x + y) You could express the old `&lt;-` like this: do let x' = @x let y' = @y x + y Personally I think that's much nicer than `&lt;-`.
&gt; the type systems stops being my friend and starts getting in my way. I haven't done much numeric programming before, but I'm curious what exactly does the "getting in the way"? My only guess at what might cause the trouble is that Haskell's default numeric tower is crap, and in any language, plumbing between types (int -&gt; float casts, etc) can be feel pedantic and distracting. Perhaps I am too far removed from the culture to understand, but my impression when I see numeric libraries in dynamic languages is a sense of amazement that the system can avoid problems with mixing up number types.
A mathematician knows better than to fix a rigid notation. 
&gt; I had to manually resolve dependencies on base and text packages... WTF? If it was [issue 167](https://github.com/leksah/leksah/issues/167) it should be fixed in the next release. Also I have fixed a couple of issues Ctrl + R (Resolve Errors) which should have been able to add the missing dependencies.
Elaborating on /u/julesjacobs' reply, `Bool -&gt; *` is a type, inhabited by functions such as p x = case x of {True -&gt; Int; False -&gt; Char} -- Which can be used as follows: y :: pi b -&gt; p b y b = if b then 0 else '0' Now, whether we're going that far is a different question.
Presentation is everything
Add a primitive `fold`?
&gt; multi-line strings are the most horrible feature ever &gt; adjacent strings in C are parsed as a single long string Just to be clear, there's no disagreement between these statements.
There's a dual to this, though. For every sloppy use of notation in mathematics, there is a programmer who clings to needlessly rigid syntax. I see posts every month which uses Haskell syntax to describe category theory. And this works well enough for many people.... provided you don't try to do something crazy like discuss pullbacks with them. (Haskell's type system can't express pullbacks). I don't think it's a coincidence that pullbacks, while being so ubiquitous in categorical logic, are almost entirely absent from Haskellian accounts of category theory.
When looking up which typeclass instance to use, the type checker first matches the type while ignoring class constraints and tries to apply the class constraints afterward (if it isn't possible you get an error at that point). So if you have the following instances instance (Monoid a) =&gt; Monoid (Maybe a) where mempty = Just mempty mappend = liftA2 mappend instance Monoid (Maybe a) where mempty = Nothing mappend Nothing y = y mappend x y = x what happens is that when you compile `(Just True) &lt;&gt; (Just False)`, GHC tries to look up a monoid instance for `Maybe Bool`, picks the first one, and then complains that you're missing `instance Monoid Bool`. So as [Darwin226 said](https://www.reddit.com/r/haskell/comments/3hnz9w/why_does_mtl_have_so_many_instances_like/cu91r9d), your instance would prevent all other instances of the form `instance MonadReader r (t m)`, no matter what `r`, `t` and `m` are, and no matter what class constraints are involved.
The `Proxy` exported from recent versions of `base`'s `Data.Proxy` is poly-kinded. Assuming you have `-XPolyKinds` enabled, your two snippets are identical. The first one, with the explicit `k`, is just more pedantic / more obvious what it's quantifying over.
The Rust community is really good on this front as well. I still look back and smile at an exchange that went something like: * Person A: "Guys" is a gendered pronoun and is potentially making people feel excluded * Person B: I like to use the word "Guys", and I'm feeling excluded by this policy * Mod: banhammer on Person B (with some explanation so that others in the thread knew what was going on) Mostly I just like to imagine Person B blinking at the screen, struggling to process that they'd stumbled onto a community that was proactive about inclusivity (and hopefully reflecting a little on who they'd ended up on the wrong side of that community).
In Agda/Coq/Idris, having `* :: *` would lead to disaster, as it would allow you to prove any equality proposition. This is because these languages use the same part of the language for proofs as for ordinary terms. GHC/Haskell is different, as it uses a different subset of the language internally for equality proofs. This other subset has no recursion of any form, so we're safe. These different subsets don't manifest in source Haskell, but they're vitally important in GHC's internal language. Another big difference between Haskell and those other languages: until we do have some (optional) totality checking, any inductive (that is, recursive) equality proofs in Haskell code will have to be *run*. This is very sad, but it's the price we pay for avoiding having a brutal termination checker. Hopefully a future not-so-brutal checker can help here!
I'm trying to build this example on NixOS and it's probably my misunderstanding of how the development environment works here, but I cannot get stack build to find zlib (declared as a systemPackage zlib in /etc/nixos/configuration.nix). I haven't actually tried --extra-lib-dirs= as I wouldn't know which directory to specify (or how to find out!)
What would it take to have the compiler fall through to other options if a typeclass constraint isn't met? This seems like a good place to apply a strategy similar to C++'s SFINAE.
Here's another example I ran into today. Somebody on my team added one argument to a function in one part of the code base, which caused a completely unrelated implicit expansion to diverge in another part of the code base. Nobody on our team was able to figure out the root of the issue and the workaround was to manually specific the correct implicit explicitly.
1.)The instances actually have to do things that are peculiar to each monad to implement 'local': instance MonadReader r' m =&gt; MonadReader r' (ContT r m) where ask = lift ask local = Cont.liftLocal ask local reader = lift . reader instance (Error e, MonadReader r m) =&gt; MonadReader r (ErrorT e m) where ask = lift ask local = mapErrorT . local reader = lift . reader ... 2.) Without overlapping instances your instance would preclude any _other_ instance of `MonadReader` for any type that took an argument of kind (* -&gt; *) before its final argument. So even if we could lift generically you'd get issues like: newtype Wibble f a = Wibble { runWibble :: Int -&gt; f String -&gt; (a, f String) } instance MonadReader Int (Wibble f) ... instance MonadState (f String) (Wibble f) ... The instances there would collide with yours. 3.) Finally, as for not every instance existing there are some 'effects' that when you stick them together you get things that violate the laws. e.g. `MonadWriter` effects do not lift over `ContT`, while some other effect lift over it just fine. Not every instance lifts over every other monad transformer. This is actually indicative of a general purpose problem with many 'effect systems' because if they are strong enough to include `Cont` in their list of effects, then they have to deal with situations where the effects/theories that they support don't compose and you get handlers that purport to offer you laws but fail to do so.
The problem with this approach is it means that adding an instance somewhere unrelated in a program either now changes the meaning of already compiled parts of the program or requires you give up global coherence of instance resolution, both yield very bad problems and violate the "open world assumption".
Can you be more specific as to what section of the resources you linked addresses the duplication problem above? The wiki page isn't easy to read, and I can't find anything about unifying type family syntax and regular function syntax.
Then how will I ever compute the Ackermann function?!?
How about Kind? 
&gt; Saner import syntax and semantics Once I've tried to write a project in Python (2.7.1) and ended up with following structure. module A module A.A1 module B module B.B1 And I couldn't find simple solution for importing `A.A1` in `B.B1` in a way that I can run `B.B1` in interpreter. A lot of people where telling me that I need to use `__init__.py` file in order to hardcode paths, but I didn't find this as a good long-term solution. Just to clarify, I was trying to load module `B.B1` from it's directory. So `A.A1` was in `../A/A1.py`.
&gt; Can I suggest that it might be very helpful to check whether there's a newer version on hackage and suggest the author checks that I think this is a great idea. I like the current behavior of using the version you've already built because it gives us the most conservative and likely to build bounds. But I think providing more information to the user is almost always a better thing. Why don't you suggest this in the comments on my [github pull request](https://github.com/haskell/cabal/pull/2774)? It took me longer than I wanted to find the time to implement this, so getting your idea out there would improve the odds of getting it to happen.
Just as I was getting excited about /u/rpglover64's example and the syntax he uses! This looks like a function on types, to me. Why are we muddying the waters by calling it a "``type family``" (whatever that means) instead? Aren't type families just another "hack" which gets GHC closer to dependant types without getting all the way there? Why not go all the way? 
`my_pkg/a/a1.py`: def hello(): print('Hello') `my_pkg/b/b1.py`: from ..a import a1 a1.hello() And, outside of the `my_pkg` directory, run: python -m my_pkg.b.b1 It should be invoked that way because otherwise `b1` will not have an information of the "parent" packages and modules. They should be loaded as a whole. Also, there's a difference between Python 2 and 3. Like /u/Peaker said, Python 2 requires you to place an empty `__init__.py` at every directory in a package to make them recognized as modules. However, [Python 3 has no such requirement](http://stackoverflow.com/questions/29216111/when-do-i-require-using-init-py), although you gain some performance gain if you do place `__init__.py`.
Yeah, I've had those `__init__.py` files. But when I have import A.A1 and running from `B.B1`'s dir, it doesn't see `A.A1`. When I have import ..A.A1 and running from root directory, it doesn't see `..A.A1`. Anyway, I don't get the idea of relative modules. What I like in haskell is that when you work on project, you have `.cabal` file describing what is your project, and where your modules are. So when you run `ghci` in any subdir of your project, it knows how to treat `import A.A1` and where to find this module, because it's name describes where to find it. You don't need to create trash like `__init__.py`. And you can run `ghci` session with projects module from dir you want. So I am really amazed on how people in this topic say that in python import system is better than in haskell. For python I found almost-working workaround. Just binding `python` or `ipython` to `cd to_project_root; [i]python` works in most cases. But that's crappy. Also some people were telling me that in order to avoid the hassle of my import problem in python I need to modify path variable (from sys module I bet, don't remember names properly) by appending string literals representing paths to my modules. I tried - it worked, but honestly, wtf? Adding string literals just to make sure that my module is loaded? No way. So basically, whenever I need to code in python it's always very unpleasant experience. But that's just me, right? So you can safely ignore what I've told. 
Atom has autocomplete-haskell, cabal, haskel-ghc-mod-, haskel-hoogle, ide-haskell, language-haskell, and linter-hlint. The basic editing of Atom is decent enough, it seems fast and lightweight, and throw in something like Term 2 and you can get the repl in an editor pane, too. (But mostly I use vim+tmux+repl).
&gt; Everyone cannot use stack because stack is not a sufficiently general tool to be used for all use cases. You are of course aware that stack is capable of using any package from hackage or github, so this is disingenuous. It's just that outside of the stackage set, it provides no more guarantees than cabal install. &gt; Cabal-install is a sufficiently general tool, although it has some shortcomings. Those shortcomings are being addressed, I'm genuinely looking forward to it, and have been following the announcements with hope. &gt; but stack was created to try out some different ideas with a reduced scope. No, it was created to improve the user experience by being faster and easier and avoids "cabal hell" by using known combinations of co-compilable packages and automatically sandboxing. It provides access to all the packages cabal install does. &gt; Stack's main focus is on people who can use a curated collection of packages. It's main focus is on improving the user experience. &gt; Curated collections are not appropriate for all people, and when they're not you need cabal-install's dependency solver. Which stack uses automatically when necessary, so you don't have to worry about it. &gt; Dependency solvers rely on version bounds to work reliably. True! &gt; Stack is not tackling this problem at all. except... &gt; Some might argue that stack is sufficiently general because it does expose solver functionality. But it just calls out to cabal-install, i.e. it's not tackling the solver problem. As in it's reusing the preexisting solver to provide that functionality. &gt; So you cannot excise cabal-install from the system. True. &gt; And even if you could, it's not about any one tool. This. Yes, this. &gt; It's about people who need dependency solvers. That need will never go away. Therefore the need for version bounds will never go away. I agree with your conclusion even though most of the criticism of stack along the way was inaccurate. 
One incentive is that Idris's development-time tooling is already better than Haskell's, and the language is simpler, which means it's more fun to program in, and it's a simpler language, which will make people want to implement libraries and improve its performance. Comparatively, Python and Ruby have never been fast and that didn't stop anybody using them.
&gt; IIUC, Idris is fully satisfied with local inference, so every top level function, no matter how simple, must be annotated with a type, no matter how obvious; Haskell, OTOH, degrades more gracefully, and one of the design goals of this proposal is to keep inference at least as good. Just noting that this is not a strong argument to put to a community of people who write top-level type signatures for every function they define, no matter how simple, as a matter of good practice.
&gt; It's the community, compiler, error messages, tooling, and a whole lot more. Indeed, Haskell has the community and the compiler. The others, not too strong.
No. Writing data in text mode handles platform specific newline issues. The end. Writing in binary mode does not. It's very easy.
This never made any sense to me. Women refer to other women as "you guys" all the time. Sure someone probably doesn't like it, but there are a lot of things in the world like that. I don't particularly like that "white knight" is somehow good and "black sheep" is somehow bad, but I still use the terms freely because that's language. I'm certainly not going to start banning people over it. It's not like he was talking about how Rust makes him sandwiches and does his laundry.
Thank you all for the suggestion about Control.Foldl and its "list" function. Unfortunately, even something as simple as the following snippet leaks: main = do let list = [1..1000 * 1000] (l, _) = F.fold ((,) &lt;$&gt; F.length &lt;*&gt; F.list) list print l In python, I'll do a stateful generator that iterates over the items of list and yield the items one by one with a side effect to store the accumulator, such as: for i in list: sideEffect(i) yield i
That last sentence nails it I think, there are plenty of cases where you just don't care about failure.. Either because it literally never needs to work ever again, or because it just isn't that important. It failed because your data was empty? Fine. Next time it won't if there's data and if there isn't, who cares? It wasn't going to be doing anything with no data, might as well just die.
I'd love to see syntactic sugar for LINQ statements ...
You can redirect stdout and stderr with [hDuplicateTo](http://hackage.haskell.org/package/base-4.8.1.0/docs/GHC-IO-Handle.html#v:hDuplicateTo). And that documentation seems to be wrong: it should state "Makes the first handle a duplicate of the second handle". Anyway this seems to work: import System.IO import GHC.IO.Handle (hDuplicateTo) main = do fstdout &lt;- openFile "stdout.txt" WriteMode fstderr &lt;- openFile "stderr.txt" WriteMode hDuplicateTo fstdout stdout hDuplicateTo fstderr stderr putStrLn "Writing to stdout..." &gt;&gt; hFlush stdout hPutStrLn stderr "Writing to stderr" &gt;&gt; hFlush stderr
Here's what I would do: pick a name, e.g. `Type`, and disable the `*` syntax as soon as you turn on any extension that involves kinds. And allow the use of `Type` in kinds even without extensions. Yes, there's will be clashes, but only in modules that have used advanced type level features. It's better to deal with the clashes now than live with * forever. If you don't like `Type`, how about `Star` or `Univ`? But `Type` is best, IMO.
Calling Haskell from Python doesn't really make sense. Haskell even though really fast is a higher-level language like Python. In fact Haskell is even terser than Python. So you won't really gain anything by doing this instead of doing full Haskell. In fact doing this (calling Haskell from Python) is probably the worst of both world. By using Haskell you lose the benefit of python : nothing to compile, works on every machine. By using Python you lose the main benefit of Haskell, type checking, etc ... Basically overhead of using both are probably not worth it. 
So the kinds `Constraint`, `Symbol`, and `Nat` would all by types now. Are they empty, or do you have any clever plans for them?
Thanks for your answer. I will try your suggestion. Didn't know about python -m my_pkg.b.b1 
Eh, NumPy is 170k lines of Python and 228K lines of C. We all know C is fast, if you have the time to write and test it.
Why is `Star` better than `*`? 
I am quite literally working on that, however time is an issue for me. 
For your first example you could always go back to writing loops by hand countFilt pred f xs = go 0 0 xs where go !n !v [] = (n,v) go !n !v (x:xs) = if pred x then go (n+1) (f v x) else go (n+1) v 
I've been writing haskell code for a little over 20 years, and no, not even a little bit tired of haskell. (Debug.trace I avoid when I can. I prefer to debug via ghci, so the more small pure functions I can split it into, the better. It's not a complete panacea, but it helps.)
There's a few areas where the type system gets annoying in my numeric code. * Unboxed arrays aren't Functors. Grated, they can't be, since the argument of fmap can return any type, but only certain types can be unboxed. A large amount of Haskell infrastructure suddenly become inaccessible, despite being provably correct for the case where the the function returns an unboxable type. * From what I've read, Haskell can't check the array bounds at compile time without dependent types. However, repa does allow you to check an array's *rank* at compile time. With numpy, I can write c=a+b+3, where a is a 1920x1152x3 array and b is a one dimension array of size 1152. The operation that will be performed is cᵢⱼₖ = aᵢⱼₖ+bⱼ+3. With Haskell, I usually have to spend fifteen minutes jiggering the shapes around to get the same calculation. * I regularly use one function that I'm pretty sure is not expressible in the Haskell type system. It takes an array of any number of dimensions and returns a one dimensional array of the radial integration. I'm fairly certain that it's impossible to define a function that will satisfy [[Double]] -&gt; [Double], [[[Double]]] -&gt; [Double], and [[[[[[[Double]]]]]]] -&gt; [Double]. It might be possible under repa, but I think that the type safe array ranks may still ruin me. * You mentioned mixing up number types, but I'm mostly dealing with real world experimental data, so everything is a Double. A quarter of my functions are either Double -&gt; Double or Double -&gt; Double -&gt; Double. The type system won't catch any mistakes there. Granted, numpy can't tell the difference between a cos and sin, either, but Python is supposed to be unsafe.
It worked thank you so much! I was so sure I checked there because that's where it was with Cygwin but I didn't see it. Sure enough though it was there.
Why all these complicated extensions, when you can just use `contravariant` to run a smaller predicate in a larger environment? import Data.Functor.Contravariant instance Contravariant Pred where contramap f (Leaf n g) = Leaf n (g . f) contramap f (And a b) = And (contramap f a) (contramap f b) ... contramap comment :: Pred Comment -&gt; Pred World Just use the most concrete `Pred x` type when defining base predicates then, and fix up the type when you want to compose. 
Yeah, working on something else related to this I made that change to the Dirs. However, how does this net me the equivalence for free? As for the where vs let thing, I hadn't thought of it that way. I was using let for small and not too self descriptive things (i, g'..) and the where for things that you could probably tell what they were (viableNeighbours...). However, may I ask why the nested wheres? Is it a matter of style? First where reflects the more important variables and functions, whereas the second refers to more low-level things that aren't as important to understand the function? Thanks, by the way. I'll change my code accordingly.
Well, I was mistaken about the DependantHaskell addition, I thought it was going to be complete. I'm dissapointed, for sure.
1. Financial calculations rarely involve the Ackermann function. Not even for inflation in Zimbabwe. 2. You can calculate Ackermann in Haskell using just `foldr` and no other recursion. 
&gt; How useful is Haskell for Python developer? Learning Haskell is recommended for all developers because it is so different from traditional languages. This, in turn, will allow you to consider a broader set of solutions to any programming problem than if you only knew the python way. So I think your plan to learn plenty of very different languages is great! &gt; From what I understand Haskell performance is very close to C Sometimes. Writing performant Haskell code is very different from writing performant C code, and unlike what I said in the previous paragraph, this knowledge is not transferable to other languages. I wouldn't pick Haskell for its speed; I'd pick Haskell for its many other advantages, and only later learn how to make Haskell programs fast. &gt; How would I go about mixing Python and Haskell in real world project? Here is a library for calling Python from Haskell: [pyfi](https://github.com/Russell91/pyfi). Here is a library for calling Haskell from Python: [call-haskell-from-anything](https://github.com/nh2/call-haskell-from-anything). &gt; What are the strenghts of Haskell in context of extending Python? The only reason I would use the above two libraries would be if a library I needed was only available in the other language. With new code, I don't see any benefits, as the languages are so different that I don't see how either would benefit from the other's strengths. I've never tried though, so if you stumble upon a useful use case, please tell us!
Weird that the page does not scroll on my Android phone. Either in the builtin browser or Chrome.
&gt; &gt; is not a sufficiently general tool to be used for all use cases. &gt; You are of course aware that stack is capable of using any package from hackage or github, so this is disingenuous. I'm referring to the fact that stack defaults to using a curated collection. [This issue](https://github.com/commercialhaskell/stack/issues/116) captures the essence of my point pretty nicely. "It seems if either of the stackage LTS things fail, or you have non-Stackage dependencies, you are a little bit stuck and have to do a lot more work than Cabal." This issue clearly shows that stack was not *originally* a sufficiently general tool. Since then it has added lip service to the problem by calling out to cabal-install. But later in a [comment on that isssue](https://github.com/commercialhaskell/stack/issues/116#issuecomment-131542648) the biggest contributor says, "Is there anyone out there who's still interested in this? I personally almost never use a dependency solver." So clearly that case is not high on the stack priority list. As long as stack defaults to using a curated collection (stackage) as its package database and doesn't put a serious effort into actually attacking the dependency resolution problem, I maintain that it is primarily targeting a subset of the problem space. That's what I meant by "sufficiently general". We should not place stack on equivalent footing to cabal-install until it has also independently tackled the dependency resolution problem. &gt; Which stack uses automatically when necessary, so you don't have to worry about it. Ahh, but you do have to worry about it because when you're not using the dependency solver it seems like things work just fine without version bounds. But the reality is that you're actually still using them and they're much stricter. &gt; No, it was created to improve the user experience i.e. try out some different ideas &gt; and avoids "cabal hell" by using known combinations of co-compilable packages i.e. reduced scope 
Link please? I can't correct myself if I don't know what comments you are referring to. 
Oh... P. S. sorry for pointless comment, but I really needed to describe and share my feelings. 
I was in office when opening this link. We had a funny time. 
The archived page is not as good though. At the bottom of the page, the swarm of coloured dots following the mouse pointer is not animated anymore. :(
Maybe not, but aside from maybe type hole style stuff I doubt Idris wins over (admittedly a very powerful feature). The last time I tried to use it the repl I gave up due to the atrocious parser error messages.
Adding on to the "ubiquity" point, [sshuttle](https://github.com/sshuttle/sshuttle) relies on the fact that the remote machine has python installed to send its server code over ssh and run it, thus requiring installation only on clients.
I think you missed the point that the line ending in your python source might end up in your multiline strings. And, for all it's power, python libraries don't exclusively control how python source is written.
Not so much _unifying_, but [singletons](https://hackage.haskell.org/package/singletons) lets you [promote](https://hackage.haskell.org/package/singletons-1.1.2.1/docs/Data-Promotion-TH.html#v:promote) regular functions to type families ([paper](http://www.cis.upenn.edu/~eir/papers/2014/promotion/promotion.pdf)).
Sorry, you're not going to get the level of control over object layout that you're looking for here using Haskell types. For example, I often would like to use the old C trick of putting a zero-length array as the last member of a header struct: struct foo_header { int field1; int field2; int fieldN; size_t length; element_type elements[0]; }; In GHC Haskell you aren't able to represent this kind of thing without an extra indirection. A few years ago, I tried (without success) to convince SPJ at a bar in Tokyo that we needed something like a `MutableArrayWithHeader#` type to fix this. This issue plus a single-threading bottleneck with large arrays in the GHC GC mean that unfortunately hash tables are not as viable in Haskell as they should be.
I knew it would be this gif before I opened it.
&gt; y :: pi b -&gt; p b To clarify for anyone not familiar: the `pi` in `pi b` is not a type variable but special syntax, like `forall`, that's not (yet) in Haskell. See the [DependentHaskell page on the GHC wiki](https://ghc.haskell.org/trac/ghc/wiki/DependentHaskell) for more.
I like `KindTypes` for the parallelism with `DataKinds`. Since I don't use an editor with completion, remembering the oddities of what's capitalized, what's plural, etc. in GHC extension names is annoying.
&gt; [[Double]] -&gt; [Double], [[[Double]]] -&gt; [Double], and [[[[[[[Double]]]]]]] -&gt; [Double] This can be done with a type class, as long as the function can be defined recursively, one dimension at a time. Not a great solution, probably.
&gt; Idris's development-time tooling is already better than Haskell's While I'm glad that you like it, we're still missing `structured-idris-mode`, so I can't yet concur :-) 
&gt; which will make people want to implement libraries and improve its performance. Maybe they will write libraries, but I don't think this is going to translate into people doing dramatic amounts of work to replicate the heavily optimizing compiler and runtime. Because... &gt; Comparatively, Python and Ruby have never been fast and that didn't stop anybody using them. Exactly. So why would they spend time on it when what they currently have is sufficient? This is basically 100% in line with what I said earlier: even until you hit a massive, ridiculous scale, it's simply not efficient or economical to focus a lot of your efforts on this kind of nonsense. You're *always* way better off writing libraries, tools or code for your core product than dealing with that, and you can improve performance where possible. When what I said earlier was bemoaned for being a 'sad fact', the same sad fact is true of GHC just as much as Idris. The reality is that developers on these projects do not 'scale' as well economically speaking: there's simply no demand for it in the vast majority of cases. You can just assign that programmer to something else and *almost always* get a better return than working on GHC. GHC has maybe 15 heavily active developers, 40 total. This is not in line with the active number of people using it. Okay, so what does this have to do with Idris? Well, it's not like Idris is terrible and useless if it's slow. On the contrary, it's great - but if I'm going to stop using Haskell for Idris and change gears as the OP alluded to, it had better not just be *as fast*, it had also better have the same feature set, at minimum. But where's the incentive *to do that work*? I don't want to program general applications in a subpar language with subpar feature sets, these days. And GHC is not an easy feature set to replicate. Idris will get there. There are a lot of smart people working on it and it's definitely got the advantage of hindsight vs us. I wouldn't be surprised to see them cover a frightening amount of ground a year from today! From my talks with David earlier this year, there are already several things Idris does I would really like to steal. But I'm not anywhere close to giving up Haskell for it, until it can beat it punch for punch. They have their work cut out for them! And in the mean time, while they're getting better performance and libraries (did I mention they're using an *already* awesome compiler that can compile down to a huge number of languages?), we'll get dependent types and many other features. I'm not sure why anyone is worried, I think we're all going to get what we want roughly. :)
I think it's more fair to say that Idris's type classes don't have a coherence property like Haskell's do, rather than to say that the term "type classes" changed its meaning when ekmett did that talk. Idris also has normal implicit arguments and doesn't call them type classes, after all.
Idris has this with !, correct?
Ron Paul is basically the worst, but that gif is almost certainly the best.
I find that, pedagogically, the sharp distinction between "type classes" and "implicits" (as opposed to coherent vs. incoherent type classes) is helpful. In rereading the original type classes paper just now, I got the sense that the coherence property was assumed and intended, though never outright stated. As such, I'd say that even before ekmett's talk, strictly speaking, type classes had coherence, but relaxing the coherence property and still calling the result type classes (loosely speaking) was accepted practice. As a side question, ignoring arguments about naming, would it be fair to describe Idris (and Agda, as Agda does the same thing IIRC) as having two ~~types~~ ~~kinds~~ ~~sorts~~ (argh!) varieties (that's not taken by type theory yet, right?) of implicit parameters, one that is solved by unification and one that is solved by instance search?
I agree with the others, we should just bite the bullet and go with `Type`.
I think you're looking for `fmap`, or the Maybe monad. They both encapsulate the logic of "if Nothing then Nothing, else if (Just x) then do stuff with x". EDIT: the way you would use it is to have your parsing function be of type`[Char] -&gt; Maybe Int`, and then do `fmap parsingFn opt`. If opt is `Nothing` the whole thing will come out `Nothing` and if not the function gets applied.
as long as you can define your function recursively, a little bit of typetrickery can get you your [[Double]] -&gt; [Double], [[[Double]]] -&gt; [Double] and so on function: data VectorThing a = VT [a] | KT [VectorThing a] deriving (Show) foo f (VT a) = (VT a) foo f (KT a) = foo f (f a) and then we can use foo (feel free to find a proper name for it, I have none) like this: combine (VT a) (VT b) = VT (a++b) combine (KT a) (KT b) = KT (a++b) g = foldr1 combine xs &gt;foo g (VT [3]) VT [3] &gt;foo g (KT [(KT [VT [3],VT [4]]),(KT [VT [5],VT [6,7]])]) VT [3,4,5,6,7]
It is already in an IO function, but mentioning the Maybe monad helped. I managed to find in the docs for "Monad Maybe" that I needed the &gt;&gt;= operator to get readMaybe to apply to the argument.
The &gt;&gt;= operator is the part that I couldn't figure out. Excellent example. Ironically I found this out from the official docs just a moment before reading your comment (if you read the comment I left under jefdaj's post)
Cool! I probably shouldn't have edited it to remove that part then... thought it would just be confusing.
I meant that you should exploit the invertible relationship between `Dir` and `Coord` that you write once and invert at need .. e.g. with a bidirectional map (see `bimap`), which may be a detail here but might come in handy in bigger projects.. Regarding the style, there are some situations in which you can't actually choose between `let .. in` and `.. where ..` but here in this case it's subjective. Making the two sides of the declaration equations explicit is more readable, IMO (you actually see what the recursion branch will be doing). I broke out `nnMv` because only `viableNeighbours` uses it, but as you can see further simplifications are possible, e.g. improving `neighbors` . HTH! Have fun! 
What about Hask?
"suscribe"?
The `readMay` function is also available out of the box as `Text.Read.readMaybe` since GHC 7.6.
Absolutely! The first answer should be case arg of Nothing -&gt; Nothing Just s -&gt; readMay s No need to introduce `&gt;&gt;=` or "monads" at all.
that's a cool idea. also should be easier to Google error messages, as it's a very unique identifier.
I guess this might also be relevant: http://brandon.si/code/translating-some-stateful-bit-twiddling-to-haskell/ You might be able to take that to another level with RebindableSYntax too.
&gt; You mentioned mixing up number types, but I'm mostly dealing with real world experimental data, so everything is a Double. A quarter of my functions are either Double -&gt; Double or Double -&gt; Double -&gt; Double. The type system won't catch any mistakes there. Have you tried using the [uom-plugin](https://hackage.haskell.org/package/uom-plugin) or some other type-level units package? Or making your own newtypes?
I dunno if documentation is fundamentally different. good APIs, tests, correctness, none are strictly necessary either for a project to be used￼
wow, I think heather.github.io is my new favorite blog
What's the point of the `PlusPlus` class? It could just be defined as ordinary functions, more polymorphic than the class: (++) r = modifyIORef r (+ 1) r += v = modifyIORef r (+ v) r -= v = modifyIORef r (subtract v) print r = readIORef r &gt;&gt;= P.print 
I didn't know that about the python community. that's good to hear :)
`diplomacy` is on Hackage but not `diplomacy-server`. Any reason for that?
what a horrible mistake. I fixed it
I've been wishing for an array that would work like a C++ `std::vector` for some time, and been meaning to write some data structures based on top of the FFI. Of course I knew about various mutable arrays, but that `ArrayRef` seems to be the most important thing I was missing in Haskell. Literally doubling is a bit excessive though, despite the origins of the idea. I'm guessing that can be controlled, though - I'll read up on it later. 
ouch, that non-literal list syntax hurts my eyes. (1:2:3:Nil) would look better. but that Nil still takes up too much space. what's wrong with desugarings that are totally obvious?
why decorators? instead of @memoize(size=100) def f(x): ... you have f = memoize 100 go where go = ... not a big deal to me either way.
(haven't used them much but) one is only one State per "monad transformer stack". but you can tuple together two different states. also possibly uglier type signatures, but I kinda like the "m Baz" return type because it tells me "and some effects", which I can read through the constraints of i care which.
Why is the base language not just what Core already is?
I think that, last I heard, he does. It also generates CUDA.
That is a good point about immortality. However it is haskell*2010* that is stuck. What does haskell2020 look like? Imagine I'm going to make all sorts of weird syntax proposals and I don't expect anyone to take up. So Haskell&lt;year&gt; is sort of an offical collection of extensions. Whereas the Weird.Zandy flavour would have all my little quirky extensions and this flavour of haskell would be an experimental haskell-like language. Instead of giving rules for an individual language we give rules for making haskell-like languages. Say you want a different syntax for defining functions. You replace the official grammar for function definitions with a grammar for your style of function definition. In your flavour it is not allowed to define functions in the official syntax but you still have a way of defining functions. The one syntax will be named as one extension and yours would be named as another and in each flavour it would be said which extension was being used. So we have a way of saying haskell-like languages should have a way to define functions but not giving a syntax for defining functions. You would choose your syntax by choosing your extensions. 
[servant](https://hackage.haskell.org/package/servant) allows you to describe a system's RESTful API as a type. From this type, the library is able generate accessor functions for Haskell and jQuery. Concerning authentication, [there is an open ticket here that seems to have gotten very far already](https://github.com/haskell-servant/servant/issues/70).
My only rebuttal is that you get good at things you have to do all the time. If you simply must do things correctly, you adapt, form good idioms, and in general begin to "think better" (I think). It's pleasant to be able to write whatever, but then you don't get any faster or better at writing correct code, and you don't over time build really good ways to get around the extra annoyances quickly and painlessly.
Yup. Fixed at tip of master. Thanks!
Isn't this a bit odd? f :: a -&gt; b ... blah ... f a ... blah ... Changing to f :: b &lt;- a or ... blah ... a f ... blah ... seems more natural.
I can't say I agree with that one. Are you used to this ordering in another language or context (e.g. maths)?
Oh sorry, I did miss that point. Yes.
No and no. There is a lot to hear, but not all of the sources are always entirely reliable. It's correspondingly ok to be worried, but I hope I can reassure you at least somewhat. My reliability is, of course, also open to question, and my bias is undeniable. But I have spent some time in the vicinity of many of the technical points, so I hope I can shed some helpful light. Designers of legacy-free dependent type systems often *choose* totality because of its many advantages, but that does not make totality *necessary*. Haskell can (and probably should) adopt a gradualist approach to dependent types where only a (growable) fragment of the term language (the fragment that we can "promote") is allowed to appear in types. That is, we separate non-dependent abstraction (business as usual) from dependent abstraction (promotable arguments only), and in that way we don't force type level standards of good behaviour on the programming language as a whole. Trivially, strictness is only necessary when we need to see a constructor. What changes (already with GADTs, so nothing new for dependent types) is that we sometimes need to see a constructor rather than bottom to check a fact upon which type soundness relies. That's one place lack of totality bites: in a total language, executing an expression is sure to deliver a constructor rather than bottom, so the execution can be dropped if we don't care *which* non-bottom value we obtain. But in a non-total language, we need to see *that* constructor just as case analysis demands a constructor, which is a far cry from being strict by default. There is no risk that existing programs will be executed more strictly than at present just because some new programs become possible. Indeed, the question "how does the typechecker evaluate a program in a type?" is separate from and only an approximation of "how does my compiled program execute at run time?". Hanging is an accepted risk in the latter; the former need not take the same risk, although other responders seem relaxed on this point. The distinction becomes more critical when execution is supposed to do IO: there is no reason why typechecker evaluation should ever do the intended-for-run-time-execution IO coded for by a program being typechecked. The ugly question of the typechecker's evaluation strategy surfaces as soon as type level programs lose totality (and that cat is already out of the bag, for better or for worse (no prizes for guessing which I think it is)). However, the potential adoption of a strict typechecker evaluation strategy (again, this may already have happened) does not force the run time execution strategy to be strict as well.
&gt; Back in the days of fundeps Have fundeps been deprecated in favor of type families? I thought both were well-supported and which one to chose was mostly a matter of style.
I'm actually pretty excited about trying out the uom plugin once I get all my code working on ghc 7.10. It's the one thing that's made me jealous about F# and I'd love to have access to proper units.
This is awesome. Great to see a tutorial that focuses on getting productive from the start. I would love to see more.
No good reason. [Here it is](http://hackage.haskell.org/package/diplomacy-server-0.1.0.0). Sad about the markdown parsing.
this is the best recursion-schemes library I've found, as far as usability. (I don't know about efficiency, but I assume it fuses stuff, being written by /u/ekmett). I like the typclasses a lot, as I can write my own fixpoint (for better error messaging or whatever): data MyF a b c r where ... newtype My a b c = My { unMy :: (MyF a b c My) } and then implement Foldable and Unfoldable in one line each: instance Foldable (My a b c) where project = unMy instance Unfoldable (My a b c) where embed = My and now I get to use all those morphisms (cata, ana, etc) directly on My.
API-Builder?
As a follow up, I think it'd be clearer to use `join` and `fmap` tbh. EDIT: oh, it's in a comment further down. Ignore me!
I took it to mean "only option available".
According to [this](https://ghc.haskell.org/trac/ghc/wiki/InjectiveTypeFamilies) you will have to wait until 7.12 is out.
I am the author and it's indirectly evangelized as part of the `turtle` library (and I wrote a [blog post](http://www.haskellforall.com/2014/08/managed-100-monad-for-managed-resources.html) about it)
For what it's worth, in my experience Haskell questions like this rarely get downvoted on StackOverflow.
For any cyclic data structures or graphs, the approach that works efficiently in practice in a purely functional language is to not tie the knot and instead define your data structure as a seed plus some unfolding function: {-# LANGUAGE ExistentialQuantification #-} data Graph f = forall ref . Graph { here :: ref, there :: ref -&gt; f ref } ... where depending on your choice of `f` you get different behaviors. For example, if you want the current node to have a list of adjacent nodes, you would pick `f` to be `[]`, which gives you a `Graph []`. Or you could just inline the `[]` directly into the definiton of the `Graph` type to build your own custom type: data Dungeon = forall ref . Dungeon { here :: ref, there :: ref -&gt; [ref] } The type `ref` can be anything. For example, if you only have four rooms in your dungeon, then `ref` would be an enum of four values: data Room = Room1 | Room2 | Room3 | Room4 ... and this would be an example dungeon: example :: Dungeon example = Dungeon { here = Room1 , there = \room -&gt; case room of Room1 -&gt; [Room2, Room3] Room2 -&gt; [Room1, Room4] Room3 -&gt; [Room1] Room4 -&gt; [Room2] } Your data structure has a starting room (the `here` field) and you can navigate to adjacent rooms using the `there` function. Now let's say that we don't just want a network. We also want to annotate each room with some sort of data (let's annotate each room with a `Bool` for simplicity). That just means that we are changing `f` to a more complex type like: type f ref = (Bool, [ref]) ... which would be equivalent to this `Dungeon` type: data Dungeon = forall ref . Dungeon { here :: ref, there :: ref -&gt; (Bool, [ref]) } That means that when we call the `there` function we get the `Bool` associated with the current room and a list of neighbors. Here's an example dungeon: example :: Dungeon example = Dungeon { here = Room1 , there = \room -&gt; case room of Room1 -&gt; (False, [Room2, Room3]) Room2 -&gt; (True, [Room1, Room4]) Room3 -&gt; (False, [Room1]) Room4 -&gt; (False, [Room2]) } We can replace the `Bool` with arbitrary `Room`-associated data. Now we can edit the current room without using any recursion. Given a function on `Bool` we can modify just the current room like this: data Position = Root | Other editCurrentRoom :: (Bool -&gt; Bool) -&gt; Dungeon -&gt; Dungeon editCurrentRoom f (Dungeon h t) = Dungeon (Root, h) t' where -- t' :: (Position, ref) -&gt; (Bool, [(Position, ref)]) t' (Root , ref) = (f b, map ((,) Other) refs) where (b, refs) = t t' (Other, ref) = ( b, map ((,) Other) refs) where (b, refs) = t Notice how `editCurrentRoom` changes the internal `ref` type. This is fine because the `ref` type can be anything as long as it is consistent. The `Dungeon` we pass to `editCurrentRoom` supplies some arbitrary `ref` type (we don't really care what it was) and the dungeon we return has a new internal reference type of `(Position, ref)`. The usual name for this trick is a greated fixed point of some functor `f` (and the `Graph` type is sometimes called `GFix` or `Nu`) and the category theory name for this trick is "F-coalgebras".
I must be too used to the imperative language community on StackOverflow that just loves to spam the downvote button. :)
Neat! How hard would it be to get this merged into cabal proper? Would such an endeavor be worthwhile?
A more modern take on this idea is the paper "[Types are calling conventions](http://research.microsoft.com/en-us/um/people/simonpj/papers/strict-core/tacc-hs09.pdf)", which makes a decent argument that a *strict* intermediate language may be better for Haskell!
Both are still alive and well.
**How do you do this‽** Every time I've tried anything like it in Haskell it's ended in misery for me.
Take a look at this example project: https://github.com/parsonsmatt/servant-persistent. This is how I got started with my project. Also, the servant website's tutorial are very nice and over time I ended up using most of the things they covered in their series. http://haskell-servant.github.io/tutorial/#tutorial
What you've written is equivalent to arg &gt;&gt;= readMay So it's not "too complicated" to suggest using bind for this problem
Actually, I think that's brilliant. Just use `type`. It can't possibly conflict. With the magic fixity for `*` implemented, we could say `type * = type` and have both coexist for some time and deprecate `*` if desired. I actually quite like this idea. Do others?
&gt; disable the `*` syntax as soon as you turn on any extension that involves kinds. But `*` can only appear in code that has such extensions... this means `*` is effectively banned. But maybe that was your intent? :) I think instead of just pulling the rug out, we should offer some sort of deprecation period.
 let rd = read :: String-&gt;Int rd &lt;$&gt; Just "345" -- 
Besides the consistency with the rest of the language, the sugar-free syntax makes it clear about what gets done, while the sugar hides it. This is especially important for the learning purposes, because the rookies might not know such details. It will also never hurt to have the information before your eyes instead of having to deduce it. Besides, it's not like List is some centric data-structure of the language, which it revolves around. It's just another data structure in the list of Map, Set, Vector, Array, DList and etc. I just don't find it logical to make a special case for something that has nothing special about it. Besides, you don't actually use that syntax that often, because it's rare that anyone needs a hardcoded value. Way more often you generate the value dynamically and in such cases you don't even use the special syntax any way. However when you do need to declare a hardcoded value it's often that it's more readable when declared multiline. So consider the following comparison: [ 1 , 2 , 3 ] vs 1 : 2 : 3 : Nil Besides all the weirdness of the first style, which AFAIK only Haskellers don't find ugly, it also has a practical problem - it is not editing-friendly. You can't move the first line around, you can't comment it, you can't delete it without extra editing. That itself also causes poorer diffs. And there is no way to format the thing, which won't cause such problems. The second example OTOH has none of those problems. Every line in it is equal semantically.
Yes, something like that. If you've not written such instances before, it's sometimes a bit mindbending, but it starts to come a bit more naturally. You might try writing or studying some more straightforward recursive instances first; unfortunately my mind is a bit shot right now and all that comes to mind is where I learned all this, i.e. re-implementing the [`vinyl`](http://hackage.haskell.org/package/vinyl) lenses, which may or may not be a good starting point.
It's probably too late at this point, but did you have another module in mind?
Try putting "abc" instead of "345" there and see what happens.
The problem with this is that the type annotation is longer than just giving the named constructor. Compare: construct 'a' 'b' 'c' :: (Char, Char, Char) construct 'a' 'b' 'c' :: T3 Char Char Char Versus: ('a', 'b', 'c') T3 'a' 'b' 'c'
Yeah. I discovered ``Numeric.showFFloat`` this morning, which reminded me that I had to ask this question. 
We tend to shove way too much stuff needlessly into the `Data` hierarchy of the namespace, to the point where the prefix has basically lost all meaning. What is `Data` vs. `Control` anyways? We shove monads in `Control` and data structures in `Data` right? But [x] is a data type that is also a monad... It is kind of a confusing story. Many other corners of the namespace have well defined roles, though! https://wiki.haskell.org/Hierarchical_module_names `Numeric` is one such corner. When I defined `Natural` in the `nats` package I simply went to the hierarchical module names reference given above and picked the most appropriate location.
while this is interesting (because we can do it) it would rather quickly result in rather unreadable code wouldn't it?
Basically _everything_ is a data type. Other things pretend to want the `Natural` name, e.g. natural transformations. These other things could just as well claim to be `Data`. There is also the fact that I've had the `nats` package exporting a compatible `Natural` number type for years now. Transferring the existing name provides a 0-touch migration path for the users -- users who have caused the 204598 downloads of that package to date.
"Hierarchical module names" says: &gt; ``Numeric`` -- exports std. H98 numeric type classes Okay, makes perfect sense. And then [base.Numeric](https://hackage.haskell.org/package/base-4.8.1.0/docs/Numeric.html) tells us: &gt; Numeric --- Odds and ends, mostly functions for reading and showing ``RealFloat``-like kind of values. The GHC folks didn't get the memo? But that's a good explanation, eKmett. Thanks.
You can already add it as a stack plugin. I can't find the blog post on it, though, so I'm calling /u/snoyberg.
What does abstracting over the return type get you? I assume you developed this method because it fits a particular use case for you.
I'm quite a heavy user of the [lens](https://hackage.haskell.org/package/lens) library and currently cabal doesn't use it and I don't know if they would really like to add it as a dependency. Pretty much all of cabal-bounds operations and options translate quite directly into a traversal of the package description data structure of cabal, which is really nice. I think the two main points for having something like this directly in cabal are the discoverbility and that you don't have to rebuild cabal-bounds on changes of the cabal library. 
Not on Hackage?
I don't know how `stack` builds packages, but `cabal-bounds` assumes that after the build a `dist` directory (containing all the build informations) lies beside of the `cabal` file. 
I was thinking it could be allowed in something simple like KindSignatures, but it's better to ban it. At the moment there will be relatively little code to change if * goes away, so it's better to get rid of it now. One compiler version of deprecation warnings might be nice, though. 
The module name is Numeric.Natural.
I don't think Haskell and Python fit together very well. They're both high-level things you can extend with C, so I'd learn C next. It's not just about speed, you'll need C libraries sometimes. I don't know any C++, but it basically requires knowing C first too right? That said, I wouldn't try to learn everything. There are too many things. Better to learn a few orthogonal languages at most (sounds like you'll be fine in that department) and then start focusing on the actual jobs/problems and learn things as they come up. EDIT: Nevermind for some reason I read that you know Haskell already. If not I would prioritize C, honestly. Not that you have to be super good at it, just familiar. It's referenced and actually used everywhere.
Are you saying you emit, compile and then run C code at Haskell run time?
This is great! Afterwards you'll figure it all out and whittle it down to like 5 lines, and then wonder if you're pleased how short it is or upset you spent so long on 5 lines.
FYI [they're called sections](https://wiki.haskell.org/Section_of_an_infix_operator) and can be used both for partially applying on either the left- or the right-hand argument.
globules: Thanks a lot. Besides my bad english, I have just the opposite problem: I can not stay in proof reading mode; I ever think in something more to add, edit etc. so I introduce more errors and so on. I ever try, but I simply can´t do it....
&gt;Basically _everything_ is a data type. ...even data is data: `Data.Data` =))) 
[Here](http://stackoverflow.com/questions/10845179/which-haskell-ghc-extensions-should-users-use-avoid)'s a related stack overflow question. 
That thread is 3 years old though.
Which means anyone who is about to answer this question here should probably edit that answer on SO.
You're relying on `Num` here. I guess, if you wanted to go all the way to nastyland, you could imagine extending these to non-Num types, couldn't you? And this way you wouldn't be doing it through `Num` which is a class you probably don't want to pollute with crap.
Do you use the strict or the lazy version of the maps(and ByteString)?
Here are two more links I found: http://stackoverflow.com/questions/1263711/using-haskell-for-sizable-real-time-systems-how-if https://www.reddit.com/r/haskell/comments/2sahpi/why_no_embedded_systems/
Never use IncoherentInstances. Never. Prefer `reflection` to ImplicitParameters.
Nothing bad about metaprogramming per se, but TH code is plain ugly to write. I'd say it's an implementation issue, we can make TH good (it's now already way better than it was).
Bytestrings of small sizes are not very efficient, sometimes it makes sense to use `copy` on your data.
This only works well for single-constructor data types though doesn't it? data Either a b = Left a | Right b -- instance Constructor ... ??
I haven't ever turned on ImpredictiveTypes, and nor do I ever plan to (unless it gets fixed). Shame, really - there's nothing wrong with the idea!
&gt; Next time it won't if there's data and if there isn't, who cares? I'm reminded of [this](http://www.spaceandgames.com/?p=27) story. If I write 1000 scripts, and I take this attitude toward each of them, what's the chance that I won't be wrong even once? What's the chance that I won't be wrong in a way that matters (e.g. doesn't cost me lots of debugging time or data loss)?
&gt; or am I missing the point, and other things prevent this? You've basically got it. `IO` actions can only happen in programs because the Haskell runtime manages to invoke `main :: IO ()`, essentially by manufacturing a value of type `State# RealWorld`, which is then passed into the function store in `main :: IO ()`. If we assume that many functions, etc., are promoted to types (as I'm hoping to do in Phase 2), then you could conceivably build up a type of kind `IO ()`. This would all be OK. But there would be no way of then executing that action. I don't plan on promoting `unsafePerformIO`! And -- sorry if some of my posts/comments are opaque to those earlier on the Haskell learning curve. It's hard to balance communicating with all the difference audiences out there!
well, then perhaps like this? module Main where import Text.ParserCombinators.Parsec import Control.Error.Util import Control.Monad -- hush :: Either a b -&gt; Maybe b cvt :: Maybe String -&gt; Maybe Int cvt = (&gt;&gt;= hush . parse (read &lt;$&gt; many1 digit) "") main = undefined 
Plans? No. Thoughts? Yes. Basically, once we get regular old types (`Int`, `Bool`) to work in terms -- by using runtime type representations -- then the rest should be straightforward. But by this point, type families should probably be subsumed by promotable term-level functions, and might perhaps be deprecated. But this is all years off, unless someone new wants to jump in here.
That's what `reflection` is for.
Now you're overdoing this. It won't be efficient, since you're parsing twice. At least the final function is not partial now however.
perhaps the `maybeRead` from [here](https://www.haskell.org/hoogle/?hoogle=String-%3EMaybe+a) could help. then we just need `(&gt;&gt;= maybeRead)`
[Readability has an opinion on this...](http://imgur.com/cAGMYAa)
Try running the programs over different inputs, maybe a caching/memoization layer is the cause?
No laziness and a casual attitude to effects are two biggies. Also I find the syntax to be ugly - but this is personal preference.
If you can do something by baking it into the language or by writing a small library, choose the latter. I don't see a single advantage of `ImplicitParameters` over `reflection`. Do you?
This is run over random input, so I don't suspect problems there.
This is running from a simple terminal and can even be reproduced on a 32 core machine with 256gb ram running both versions in parallel.
I don't think I can honestly call myself a Haskeller anymore, but my critique of ML-family languages from the Haskell point of view focuses squarely on the lack of higher-kinded polymorphism. I really like the style of doing things where instead of making up data types etc., you write up a bunch of functors and compose and interpret them in interesting ways. In OCaml, you can simulate the needed higher-kinded polymorphism, but it's pretty painful. In fact, for the non-higher-kinded case, it's standard practice in ML to have a polynomial functor and then an abstract type which is (usually) its fixpoint (or at least, partially isomorphic to its fixpoint). For instance, in the ABT interface: datatype 'a view = ` of variable | $ of operator * 'a vector | \ of variable * 'a type abt exception Malformed (* here's a partial isomorphism, which essentially restricts the fixed point of view to rule out exotic &amp; non-wellformed terms. In Haskell, you might implement this as a Prism. *) val into : abt view -&gt; abt val out : abt -&gt; abt view So it would be nice for this sort of programming to be supported in the general case. Even here, in order to take the fixpoint of `view`, you must use a module functor (or write it out manually)... It's nicer in Haskell, where you can just write things like that directly. As for F#, I'd criticize it specifically as not implementing modules properly (or essentially, at all).
OCaml *does* have laziness (but it lets you choose when you want to use it) [1], and it can be given the same treatment of effects as Haskell via monads [2]. Whether you'd *want* to is another story. Note, btw, before I hear the inevitable "Haskell has strictness but lets you choose when you want to use it"—you do have this, and can use it to avoid (for instance) space leaks, but this is not the only purpose of strictness. One benefit is that if you do not commit to pervasive laziness, you can introduce inductive datatypes even in a partial and effectful setting (like Haskell's), whereas Haskell does not have any inductive datatypes. (Or, another way to put it is, Haskell *does* have inductive datatypes, but they all have different induction principles from what you would naïvely / hopefully expect.) [1] Real World OCaml (a really nice book) has a section about laziness and other benign effects in the chapter on Imperative Programming. https://realworldocaml.org/v1/en/html/imperative-programming-1.html [2] https://existentialtype.wordpress.com/2011/05/01/of-course-ml-has-monads/
Yes, implicit parameters requires I turn an extension on and stick a `?` infront of my variable names and it just works. `reflection` requires I remember how it works, find a good scheme to encode the operations I need, maybe make new data types as a convenient proxy and then implement it all. It's the right tool for the job, but sometimes so is `ImplicitParameters`.
Have you tried using profiling builds (and the various RTS options) to narrow down which parts of the program take up your time?
The one part of your question that is unclear is why the SQL optimizer needs to care about the C++ threading model or iterators. Will you also be consuming the query's results in Haskell? However, the SQL optimizer will definitely be much easier to write in Haskell than other languages.
&gt; OCaml does have laziness (but it lets you choose when you want to use it) I hear this with a lot of languages and it really bugs me. Surely it's easier to wrap lazy functions to be strict than it is to wrap strict functions to be lazy?
What do you mean by "interpreted"?
IIRC there are different defaulting rules too. If the compiler gives any warnings about defaulting this to that, try adding enough type signatures to nail them down and then comparing the compiled to interpreted versions.
I've seen somewhere in passing that you've migrated to SML, I'd love to read what do you prefer in SML (or whatever it is you've migrated to) compared to haskell. Maybe you already wrote it somewhere?
Interesting points... I'll try to read the paper you quoted.
And there's also /u/aseipp's [reflection tutorial](https://www.fpcomplete.com/user/thoughtpolice/using-reflection) tutorial; neither of them got through my skull when I read them first; maybe I should reread them.
Well, I have made a number of comments in this subreddit explaining certain aspects of what I like about SML, but I haven't written anything “comprehensive”. If I could summarize my preference for ML-family languages in a couple words, it would be: “Modules Matter Most, Some Effects Are Benign, and It's Good To Reason By Induction". By the way, there are many problems with SML—but for me (personally), it is the best choice.
Memory safety is a big deal in my opinion. Rust has other advantages too, such as enums and pattern matching which will be very useful for representing the AST, traits, better move semantics, a cleaner syntax, and being a much simpler language. Haskell and Rust each have their own advantages. Perhaps you could write a prototype in both, and see what works best for you?
You have the OCaml vs SML timing somewhat backwards. OCaml is syntactically similar to Caml, which is similar to the original ML. SML decided to change syntax from original ML, but at that point the French ML version had already split off. (OCaml people, please correct me if I'm wrong.)
Very well written. I don't know anything about the Milner story, but I agree with pretty much everything else you wrote.
Honestly, and this is more of an ocaml thing as I haven't written a lot of F# yet, I hate having two separate operators for integer vs floating point math. let foo x y = x /. y just seems too noisy to me. Like I said fundamentally I don't have anything against them, and if I got paid to work in them I wouldn't complain :-)
If I'm reading his post correctly then the plan is to write the optimizer (AST -&gt; AST) in Haskell, but also at least initially run the resulting relational algebra operators in Haskell. Hence the need to use the iterators of the storage engine to iterate over the database and pass a result set iterator back to C++. Those iterators would be represented as lazy lists in Haskell. In the reply he mentions that the execution plan may later be compiled to LLVM IR so that Haskell would be used purely for AST &amp; code generation purposes, which sounds like a much better plan than the lazy lists iterators if you ask me.
Came to recommend the same thing. Specifically the "Linear" implementation will be the least memory-intensive, having the performance close to "unordered-containers". The other implementations will be more memory-intensive but also better performing. One needs to keep in mind though, that he might run into [issues with the GC](https://github.com/gregorycollins/hashtables/issues/14).
* Lack of type classes. * F#'s lack of modules. OCaml has great support for this. * Lack of higher kinded types. * Haskell's syntax is more pleasant.
&gt; No HKTs. ML does have HKTs through modules. Modules subsume typeclasses. It will be even more apparent when OCaml introduces module implicits.
&gt; There's a clear need for something like typeclasses or modular implicits (which are landing someday in OCaml and I'm quite excited about). I know there's a pretty nice proposal for modular implicits, but has there been any hint that it will indeed be included in the language?
My use case for the C generation doesn't involve a Haskell runtime, but the generation of that C code does happen at Haskell runtime. You run a Haskell program, C comes out. Presumably you could pipe that into a C compiler, then open the resultant library all within the one Haskell process, but I've not tried that. I think the existing LLVM libraries would be the best route to take for such a thing. I just want to spit out programs that can then be run through a separate tool chain.
To be fair, you added at least 8 different new stream types to OCaml. :D
Ah, I see, that's a good point! Indeed.
What's the motive for disabling it by default? Encouraging people to use higher-order functions?
Yeah, this is caused by the lack of ad-hoc polymorphism. Module implicits will come soon, though, and will introduce ad-hoc polymorphism. Though I don't think `/.` and others will ever be deprecated. :-/
Oh, probably better to ask someone with real insight. I think I'm assuming off of some comments from people, but I'm not sure that they're normative.
I think it's always good if people write up a tutorial, both for themselves and others. I do have some criticisms which will sound harsh, but please don't think that I'm trying to be mean-spirited. What I liked: * Introduction to the REPL. * ADTs; Maybe as a safe alternative to null. * Simple records. What I didn't like: In my opinion, your tutorial is much, much too hard for people without PL experience. All knowledge tends to appear trivial and obvious in hindsight, but one can't forget how much stumbling learning involves. Back when I started learning Haskell (after years of Java), even simple concepts like currying or function composition appeared like mystical mumbo-jumbo to me. Your very first example contains type classes (Num), for instance, which is an unheard-of concept outside of Haskell, basically. The same goes for `foldr (.)`. A newcomer, or someone used to programming with values like ints and doubles alone, would likely find that utterly incomprehensible. Many HOFs are simply not in people's conceptual universe. Simpler examples would go a long way towards easing the reader in. E.g.: * Summing the elements of an integer-list (after you explained how lists worked); * taking an integer-list and returning only the even numbers; * writing your own `map` and `filter` (and seeing how the previous example generalizes); * explaining why every if must have an else-branch; * uppercasing a string; * swapping the elements of a tuple; * writing a factorial/fibonacci; * writing an efficient fibonacci (without memoization, just fib :: Integer -&gt; Integer fib = fib' 1 1 fib' :: Integer -&gt; Integer -&gt; Integer -&gt; Integer fib' a b 0 = a fib' a b n = fib b (a+b) (n-1) ). Theory is useful, but humans don't learn from theory; they learn via pattern-matching - later, after the examples, theory can be added to allow people to refine and correct the mental models they've constructed.
There's also [Once](https://www.fpcomplete.com/user/edwardk/snippets/once) ([discussion](https://www.reddit.com/r/haskell/comments/1lo15w/deepseq_once/)), but it's not packaged up anywhere.
&gt; Edward Kmett has a \*something-that-solves-your-problem-perfectly\* he's been working on ... _Of_ course he does.
Isn't he saying the opposite? From what I understood, he had a simple quadratic algorithm and a fancy linear algorithm, he expected the fancy algorithm to run faster, but since the simple algorithm was using functions from the prelude, it was actually running mostly compiled code, which was faster for small inputs because the compiler made the constant factors very small. The OP has encountered the opposite situation, in which the compiled code is slower.
The main thing I notice in that in your .cabal file, you give extra flags to GHC to explicitly turn on a few specific optimizations. I'm not familiar with those particular flags, but I know that some optimizations can cause a slow down and are thus not enabled by default. The fact that you had to add those flags makes me think that those are probably the optimizations in question, so it's not surprising that some programs such as yours get slowed down. If you try it with `-O2` and no other optimization flags, does the problem persist?
**F#** Pros: * Compatible with .NET * Offers FPL and OOP style Cons: * Non-Windows support requires Mono * Treated as a second-class citizen within official Microsoft tutorials, like C++ * The language design spec specifically bans hard tab literals as indentation. * Weak support for shebangs **OCaml** Pros: * Decent cross-OS support Cons: * Requires special keyword for recursive functions, in a functional programming language * Weak support for shebangs
I use [the SVG map from Wikipedia](https://en.wikipedia.org/wiki/Diplomacy_%28game%29#/media/File:Diplomacy.svg).
One of my pet peeves is `RecordWidCards`. You end up with code something like foo Baz{..} = do let x = bar ... and without looking at distant code you are not sure whether `bar` is part of the record or some other function/value. For me it makes code harder to understand
I mean, why few people call python from haskell ? That is the real question. I don't understand the intersection argument. Could you elaborate please?
What a fun coincidence - I'm a diplomacy fan and writing my own library to judge orders as Haskell practice. I'm finishing up the cis194 exercises this weekend, was planning to start next weekend. I'll peek at yours if I get stuck. :)
This is an interesting point of view that makes me interested in deepening SML "dialect". Another interesting thing is the existence of a .NET version of SML (http://www.cl.cam.ac.uk/research/tsg/SMLNET/), but I don't know if it's maintained anymore.
I would expect that calls to Python functions can have also well defined argument and return types. 
I heard about Koka years before my interest in FP, but know I can look to it with "new eyes".
We do not exactly wake up in the morning to go and start calling python procedures from haskell :) there must be a use case. Which use case do you have in mind? I would expect a much more compelling reason to call java from haskell rather than python. 
...until they don't. How do I know that the types in my Haskell code are what the Python functions expect? There is no means to verify it statically. In Haskell, we don't accept the answer "I ran it and it worked." as good enough.
Yes. Also, the stack size you get on Windows, eg in Excel, is very small, so recursion is a bad idea. 
&gt; I don't understand the intersection argument. Could you elaborate please? Consider this image: https://upload.wikimedia.org/wikipedia/commons/0/06/Set_union.png The intersection of A and B is much smaller than A and much smaller than B. That doesn't have to be true in general, as the two could overlap a lot more, in which case the intersection will be at most as large as the smallest of A and B. In this case, that would only be the case if everyone who calls python from haskell would want to use the low-level cpython API instead of the high-level [pyfi](https://github.com/Russell91/pyfi) API or if everyone who needs to use the low-level cpython API needs to do so from Haskell instead of from C. Neither of those extreme scenarios are even remotely plausible, so it is clear that the group of people who wants to use haskell-cpython is strictly smaller than those two already small groups.
http://www.mpi-sws.org/~dreyer/papers/mtc/main-long.pdf
Why would type families be deprecated? They provide a type-case that is not available in term-level-land. Or do you plan to eventually add type-case, and another binding form (beyond: forall, pi) so that we can decide for which type-variables we have parametricity, and for which type variables we do not have parametricity? edit: originally I said (-&gt;) is a binding form, which of course it isn't
Other Pros: F# makes it possible to write "compiler plugins" called type providers that provide access to external data sources, such as web services, databases, or other online information sources. The plugin takes the schema of the external data source and maps it to F# types. These types can then be coded against in your program. Idris users may also be familiar with type providers. F# also has 'active patterns' (https://en.wikibooks.org/wiki/F_Sharp_Programming/Active_Patterns). I believe PatternSynonyms in Haskell are somewhat similar but 'active patterns' appear to have a nice concise syntax for general programming with unions and may be more flexible in this context; (PatternSynonyms seems to have been motivated more by FFI concerns) 
We demand "It compiles, ship it!" instead. ;)
Yes, and the code that I have experience with doesn't shy away from them either. Not that it is always a bad thing (for e.g performance reasons) but it is a smell that keeps coming back.
Correct. You can't change strictly *produced* data to lazily produced data. But you can change strictly *consumed* data to lazily consumed data.
Is it broken in the sense that it is unsafe, or just broken in the sense that it doesn't fully support impredicative types?
Actually, Python is at the moment the language of choice in some science-related areas, e.g. in bioinformatics. So anyone who wants to do bioinformatics in Haskell may be interested in bringing in some existing libraries from Python.
Well put. The duality at play here basically means that neither choice will give you EVERYTHING. The name of the game is, then, pick the right types, and pick a language that doesn't rule out types that you need.
This of course is a great use case of Python that Haskell is lacking. However, low level bindings aren't really a solution to this problem (you want a Haskell data structure than has an instance that allows conversion to something like numpy arrays). Also I didn't see any mention of memory being shared being the Haskell and Python runtime. So are you really any better off using this library than writing to a binary file, and then just being very careful when you import the file to Python and of course loose type safety. So, even though what you say is true, the overhead is probably not worth it unless the library is much more sophisticated that marshals between types without copying memory. 
I don't quite understand why this is - could someone explain it?
I'm really glad you liked it! :) It's still very much work in progress though; the part on datatypes should continue with typeclasses, more Prelude, then Monad, Functor .. most of the fundamental bits, including a good bag of examples, small and bigger :) The overall aim of this work is providing a compelling but informal view on FP with Hs, while getting from ~0 to "working knowledge" in a few pages. Anyway, all feedback is welcome. :)
This is a very interesting addition to the Haskell threaded ecosystem. I envision possibly employing this for urgent communications within a Haskell application in lieu of tbqueue and then out to a Websocket.
You know, it's funny: I use OCaml a lot. I've done a couple of OCaml internships, including JSC, and work at an OCaml startup. I like Haskell far more, backed by a reasonable amount of experience, but I've never looked too deeply into *why*. (If you're curious, I wrote about the comparison a bit [on Quora][practical], but didn't go too deep into the languages' designs.) Thinking about it now, it boils down to a single word: **expressiveness**. When I'm writing OCaml, I feel more constrained than when I'm writing Haskell. And that's important: unlike so many others, what first attracted me to Haskell was expressiveness, not safety. It's easier for me to write code that looks *how I want it to look* in Haskell. The upper bound on code quality is higher. Now, this is a vague pronouncement, I'll admit. But it's vague by necessity: the distinction does not stem from one large difference between the languages but from a complex web of small and medium differences in the languages themselves, their libraries, their abstractions and, perhaps most importantly, the programming philosophies that underlie it all. Perhaps it all boils down to OCaml and its community feeling more "worse is better" than Haskell, something I highly disfavor. I do have a laundry list of complaints about OCaml from a Haskell point of view (and the other way around too!), and I'll mention a few of the biggest, but they all too often boil down to minor inconveniences and nitpicks. That doesn't tell us much about language design in general: I might like how Haskell operators syntax works more than OCaml, say, but that's a tiny quibble at best, and it's not worth spending too much effort on quibbles! It's important to see through surface issues and think about the core of each language. Not listing all my specific grievances with OCaml also turned this exercise more into "why I like Haskell" than "problems I have with OCaml", but I think that's fine. The two languages are close enough that "I don't like OCaml because it's not like Haskell in X ways" is actually useful. So, aside from fuzzier philosophical differences, what stands out in Haskell vs OCaml? **Laziness** Laziness or, more strictly, non-strictness is *big*. A controversial start, perhaps, but I stand by it. Unlike some, I do not see non-strictness as a design mistake but as a leap in abstraction. Perhaps a leap before its time, but a leap nonetheless. Haskell lets me program without constantly keeping the code's order in my head. Sure, it's not perfect and sometimes performance issues jar the illusion, but they are the exception not the norm. Coming from imperative languages where order is omnipresent (I can't even *imagine* not thinking about execution order as I write an imperative program!) it's incredibly liberating, even accounting for the weird issues and jinks I'd never see in a strict language. This is what I imagine life felt like with the first garbage collectors: they may have been slow and awkward, the abstraction might have leaked here and there, but, for all that, it was an incredible advance. You didn't have to constantly think about memory allocation any more. It took a lot of effort to get where we are now and garbage collectors still aren't perfect and don't fit everywhere, but it's hard to imagine the world without them. Non-strictness feels like it has the same potential, without anywhere near the work garbage collection saw put into it. I could easily write a few paragraphs more about laziness, but this is getting too long as is. If you're still interested, check out my recent BayHac talk about ["Thinking with Laziness"][thinking] where I explain how laziness helps make my code more expressive. And then you can go on and read through [Conal's blog][conal-blog] which is bursting with examples of expressive laziness. **Typeclass** The other big thing that stands out are typeclasses. OCaml might catch up on this front with implicit modules or it might not (Scala implicits are, by many reports, awkward at best—ask Edward Kmett about it, not me) but, as it stands, not having them is a major shortcoming. Not having inference is a bigger deal than it seems: it makes all sorts of idioms we take for granted in Haskell awkward in OCaml which means that people simply don't use them. Haskell's typeclasses, for all their shortcomings (some of which I find rather annoying), are incredibly expressive. In Haskell, it's trivial to create your own numeric type and operators work as expected. In OCaml, while you *can* write code that's polymorphic over numeric types, people simply don't. Why not? Because you'd have to explicitly convert your literals and because you'd have to explicitly open a module with your operators—good luck using multiple numeric types in a single block of code! This means that everyone uses the default types: (63/31-bit) ints and doubles. If that doesn't scream "worse is better", I don't know what does. I've gotten a lot of mileage in Haskell out of `Integer`, `Word` and `Rational` along with some domain-specific ones like `Word18` for emulating an 18-bit architecture and `Integer/7` for [modular arithmetic][modular-arithmetic]. OCaml throws up too much syntactic noise to make this practical. And if people have to wade through syntactic noise to do something, they'll do it less. People respond to incentives. But of course, numbers alone aren't a big deal, it's just an example. It applies just as well to other overloaded literals and to QuickCheck and to `read`. The point is that inferred typeclasses make *a lot* of things sufficiently easier and more expressive in Haskell than OCaml to constitute a major difference. **More** There's more. Haskell's effect management, brought up elsewhere in this thread, is a big boon. It makes changing things more comfortable and makes *informal* reasoning much easier. Haskell is the only language where I *consistently* leave code I visit better than I found it. Even if I hadn't worked on the project in years. My Haskell code has better longevity than my OCaml code, much less other languages. My view on "benign effects" is simple: they are breaking the abstraction. Full stop. If they actually are benign, it should be fine to make this explicit: `unsafePerformIO` is your friend. But I like the abstraction to hold by default, and I like always assuming it where I can. Haskell gives me the tools to do this, more or less; OCaml really doesn't. Again, it's not *perfect* and makes things like [Clojure-style HAMTs][hamt] harder than they have to be but, on the whole, I prefer it. Room for improvement just means we have more to do. I also don't worry much about the issues some people (*cough* Harper *cough*) have with laziness. There are real costs (exceptions in Haskell are terrifying) but, on the whole, the benefits to expressiveness easily outweigh them. I care more about informal reasoning than I do about writing formal, much less machine-checked, proofs, and for that ignoring bottoms is good enough. Fast-and-loose reasoning being [morally correct][correct] helps too. Honestly, I have more to say, but I already overshot my 1000-word goal for this post by some margin. Perhaps I'll expand it into a full blog post at some point if people are interested. [practical]: https://www.quora.com/What-makes-any-one-programming-language-better-than-another/answer/Tikhon-Jelvis?share=1 [thinking]: http://begriffs.com/posts/2015-06-17-thinking-with-laziness.html [conal-blog]: http://conal.net/blog/ [modular-arithmetic]: https://hackage.haskell.org/package/modular-arithmetic [correct]: http://www.cs.ox.ac.uk/jeremy.gibbons/publications/fast+loose.pdf [hamt]: http://blog.ezyang.com/2010/03/the-case-of-the-hash-array-mapped-trie/
Let's take your suggestion: If you implement a number type like newtype Mod = Mod { runMod :: Int -&gt; Int } instance Num Mod where Mod f + Mod g = Mod $ \m -&gt; (f m + g m) `mod` m Mod f * Mod g = Mod $ \m -&gt; (f m * g m) `mod` m ... then you get a problem if you go to call a function like (^) :: Mod -&gt; Int -&gt; Mod Why? Internally it calls * with the same arguments recursively to square its way toward its goal. So you get x2 = x*x x4 = x2*x2 x8 = x4*x4 x16 = x8*x8 ... x256 = x128*x128 That involves 8 multiplications right? Well, in the "reader-like" version it involves 256! Each `*` is sharing 'functions' but that doesn't share the answer to the functions for `m`! x2 m = x m * x m x4 m = x2 m * x2 m ... It doesn't have any opportunity to spot the common sub-expressions there, because `(^)` was written polymorphically in the number type a decade or two ago by Lennart -- it knows nothing about `Mod` -- so even if it was smart enough to CSE, which generally isn't a good idea in Haskell, it is robbed of the opportunity by separate compilation. We need a way to tell GHC 'we're always going to pass you the same `m`, so its safe for you to lift `m` out of all the lambdas, and share all the results. newtype Mod s = Mod Int is clearly a concrete _value_, not a function. instance Reifies s Int =&gt; Num (Mod s) where Mod n + Mod n = (m + n) `mod` reflect (Proxy :: Proxy s) is going out into the environment to grab the instance, but that instance will lift out as far as it can from lambdas and the like. GHC can know that every time it 'calls a function' Reifies s Int =&gt; y that it will get the same dictionary for `Reifies s Int`, this makes it sound for it to move that out a far as it wants. Really, anything that takes a constraint is really a function from that constraint, but GHC has a great deal more freedom in moving those around in your code than it does actual function arguments.
&gt;I'd love to see more development in tools and **tutorials** we can use to debug problems like this. This. I find space leaks notoriously hard to spot. 
But without pervasive laziness you don't get the nice compositional properties that Haskell has. E.g., look at Haskell's definition of `any`. 
this post provides some commentary on that note: https://ocharles.org.uk/blog/posts/2014-12-03-pattern-synonyms.html
Have you tried compilation with different -O flags? What version of ghc are you using? How is the store function called? In your test case what instance of Storable where you using and what was the definition of getTables?
Agreed. Prefixing field names with the name of the record (`personName` instead of `name`) helps, but I almost always prefer just using `NamedFieldPuns` instead.
No. Impredicativity in *Church-style* System F is not my favourite thing ever, but it's easy to see how to typecheck it. Type inference for *Curry-style* impredicativity is awfully (i.e., far too) tricky. When I see people working on it, I *lose* hope for the future. The problem is that the type abstractions and type applications that the type inference algorithm must magically insert can now appear in many more places. Constraints in type inference which used to say "this type must be the same as that" now say "this type must be a less polymorphic version of that". But the latter sort of constraint leaves more degrees of freedom, so the resulting problems are often underspecified and may not have most general solutions. Trying to make this work amounts to an overdevotion to magic. If you want to allow such craftiness, just allow it to be explicit! The problem is not *impredicative* polymorphism: it's *invisible* polymorphism. GHC's core language already gives us an explicit impredicative type system. The nuisance is that we can't use it more directly: there's a dated conviction that the insertion of type abstraction and application must be a *guessing game* to be played by GHC rather than a thing we can just write. I'm looking forward to the phase of the /u/goldfirere plan where we get the *explicit* quantifier `forall a -&gt; b[a]`, and the associated explicit lambdas and applications. Once we have that, impredicative programming will just work, and impredicative type inference won't have to. Type inference for impredicativity is, as John Peel once said of Emerson, Lake and Palmer, a waste of talent and electricity.
Numpy's codebase is 62% C, [according to this talk on Rust I saw at PyCon 2015](https://www.youtube.com/watch?v=3CwJ0MH-4MA)
In the case of GreatPower, it's just syntax. As far as I know, there's nothing gained or lost compared to the Haskell98 definition of the same datatype.
This is fascinating and thought-provoking: I'd love to read the blog post!
I found reflection easier to understand by looking at the Given typeclass rather than the Refies class (Given is simpler, but I think occasionally less well behaved). Then we can implement the following: implicit :: (a -&gt; b) -&gt; (Given a =&gt; b) implicit f = f given explicit :: (Given a =&gt; b) -&gt; a -&gt; b explicit a b = give b a Aside from the performance issues edward mentions there are some interfaces that you simply can't implement with Reader that you can with reflection. For instance, try to implement Eq for a Reader-based modulo number. The best you could do is `liftA2 (==)`, but that would return `a -&gt; Bool`, while you need `Bool`. But with reflection you can move the fact that you're returning a function out of the type signature into the instance context, so you have: newtype Mod a = Mod a instance (Given a, Integral a) =&gt; Eq (Mod a) where Mod a == Mod b = normalize a == normalize b where normalize a = a `mod` given You don't even really need a Given constraint for Eq; if you move the Given constraint into the various Num methods you can make sure that you can't construct an un-normalized modulo number, in which case we already know it's normalized when we try to equate two Mod values. With the actual Reifies class we can also guarantee that we can't equate a modulo 8 number with a modulo 16 number; the extra phantom type parameter makes numbers in different modulos have different types. We can also provide an Ord instance similarly, which means we can construct Maps which use modulo-numbers as keys, which we can't do with Reader (I think this is only safe with Reifies, not Given). A while back I was experimenting with aping imperative syntax with lenses to write code that's polymorphic over whether or not you're in a monad, so you could do things like: while ((!x) &lt; 10) $ do y =: foo (!x) + (!y) x =: (!x) + 1 Where anything to the right of (=:), or in the while-loops condition were given read-only access to the state monad's state. Without reflection the best I would have managed would be something like: while (fmap (&lt;10) (view x)) $ do y =: ((+) &lt;$&gt; fmap foo (view x) &lt;*&gt; view y) x =: ((+) &lt;$&gt; view x &lt;*&gt; pure 1) Even adding `instance (Num a) =&gt; Num (Reader r a)` would only help for code that's polymorphic over `Num`. In the general case you have to constantly `liftA2`/`pure`/`fmap`pure functions to work over the Reader monad, whereas the reflection-based interface does so automatically. http://lpaste.net/139394 has the implementation if anyone's curious.
If you rely on this then you have to know a lot about the internals of functions, specifically what gets evaluated when. I've never understood how Haskellers simultaneously denounce implicit effects but endorse implicit laziness. Both have the fundamental problem that the denotation of functions is no longer a mathematical function, but something much more complicated. Tracking this in the type seems far preferable to me. I would gladly give up a cute trick like defining any as fold (&amp;&amp;) for a simpler denotation for functions &amp; other values.
Let's transform it into something less magical: stripChars = filter . flip notElem stripChars badChars input = (filter . flip notElem) badChars input stripChars badChars input = filter (flip notElem badChars) input `notElem` is a function that takes two arguments: a `Char` and a `String`, and returns `True` if that character does not appear in the string, `False` otherwise. `flip notElem badChars` supplies `badChars` as the second argument to `notElem`, which results in a function that takes just the character argument. That function is then passed to `filter`. `filter` will execute it for each character in `input`. When the function returns `False`, i.e. when the character is present in `badChars`, the character is discarded.
Author here, let me answer that: * 1) no, I'm using stack's default cabal file, gonna try later, good idea. * 2) 7.8.4, forgot to mention it. * 3) in Main.hs, [it's just called with the list](https://github.com/alvare/discogs2pg/blob/master/src-exe/Main.hs). * 4) I have only two instances so far, both are there in the repo. getTables is just for metadata:`const (TableInfo "table_name" ["col1", "col2"])` (I know, it's crappy, I gotta rewrite that bit)
Why is is not a good idea to perform CSE in Haskell? I'm guessing the answer has something to do with laziness, but that doesn't quite make sense. In fact, you could think of call-by-need as call-by-name + CSE. 
Although the deadline for registration for this event was previously announced as August 15, registration is still open. Please register if you're attending ICFP, and spread the word if you know others who are!
Lifting things out of lambdas can drastically increase their lifetimes compared to what you expect. Sometimes things in memory are cheaper to recompute than to hold onto. Sometimes holding onto something (like a function) doesn't actually let you compute the answer to the function at specific arguments any faster, so the reference to the particular variant of a function closure is just wasting space. When I have multiple uses of a thing I lose fusion opportunities that may have exceeded the gain from the shared structure, etc. There are several different problems that all add up to it being a dicey proposition. As a result I just write all my code as CSE'd as I can by hand, that way I can remove as much potential for things to go wrong as possible. and can -fno-cse whenever the compiler starts going wrong. CSE also can do strange things to NOINLINEd chunks of code. I'm somewhat saddened by all of this because it is part of what I think makes a sufficiently smart compiler sufficiently smart.
Thanks a ton for the replies. Incredibly helpful. Could a sufficiently smart compiler generate both CSE'd and non-CSE'd versions of code and decide dynamically based on runtime properties (size of the data structure?) which one to use?
No idea. Sounds like you have a research project. ;)
The plot thickens... After some fiddling with the repository and the data I've found out that if the `[a]` shows up in the RHS of the `getTables` definition it will also run in constant space (I'm using 7.10.1, for reference). What I mean is that I've made the following changes. class Storable a where ... getTables :: [a] -&gt; ([a], [TableInfo]) instance Storable Artist where ... getTables a = (a, ...) -- in the call site I've moved `getTables values` from the let assignment to their original place like (snd $ getTables values) I'm quite interested to hear from different developers knowledgeable with the internal implementation of typeclasses if this could be explained. While I'm not yet familiar with the GHC Core language I would have expected a noticeable difference based on the implementation (that would be explanatory in itself). [Image of a diff between the core output](http://i.imgur.com/o6JxcNU.png) - on the left the original implementation of `getTables` for `Artist` and on the right the tuple wrapping (which is essentially a no-op since `fst` it's newer used). 
Thanks! These are all reasonable objections to CSE, but none of them seem particularly specific to Haskell. They do, however, seem to be (somewhat) connected to higher-order functions, which makes me wonder if the ML-style languages do CSE.
&gt; But that's not the case, of course. No, that is the case ☺ your initial interpretation is correct.
&gt; But that's not the case, of course. Are you so sure about that? Currying and partial application make this the case.
That's very cool. It looks like I just needed to read a little further in this tutorial to see that: [Haskell the Hard Way / A Type Example](http://yannesposito.com/Scratch/en/blog/Haskell-the-Hard-Way/#a-type-example).
Thanks, that makes more sense. I know see the first argument being pulled in and curried with notElem. notElem is Char -&gt; String -&gt; Bool, but the resulting parameters is asking for two String values. At what point did notElem's Char become a list of Chars? Is this the result of currying? Thanks!
I'm primarily an F# guy with a hobbyist's interest in Haskell. The bad: * Custom equality or comparison in F# is an absolute pain in the ass. * The type system is almost a toy compared to Haskell's. I find the F# type system sufficient for real world development, but still quite limiting. * The standard library is minimal, and the .NET libraries are all intended for OOP languages. * The OOP aspect of F# is pretty much useless, save for interfaces which are very much the shit in a functional setting. * Microsoft seems to half-assedly support F#; it's not supported for Windows Universal applications. The good: * I like the syntax MUCH better, especially the monad blocks. (We even have monoid blocks! Suck it!) * Strict-by-default is absolutely the shit. * The tooling and documentation are out of this world. This is the one aspect that REALLY sets F# apart from most functional languages. The irrelevant: * Everyone talks about F#'s lack of modules like it's a dealbreaker. You can make do quite well just with interfaces! All in all, I'd pick F# over Haskell any day for entreprise development, but they're both excellent languages.
I have the attention span of a goldfish. This ultra-compact pointfree notation thing you're looking at is incredibly useful for me. It lets me focus on the essentials without having to swim in a sea of variable names. Hopefully you'll find it similarly useful.
The instant I heard those trumpets, I somehow *knew* the song was gonna be in Japanese.
Not specifically. I borrowed a lot from Yesod and the workings and defaults from Yesod. I could look into this and see if it would have any adverse effects with reverse proxying. 
Briefly - why would I want to attend such a workshop? Right off the bat, the premise feels slightly condescending. Beyond that, I'm "not sure" what the expected payoff is, i.e. I don't see it stated anywhere.
Yup, that's what I used it for. It's amazing for that.
The workshop is directed at people who want to support women in their workplaces and communities, and intended to help participants get better at doing that. If you're not interested in supporting women in your communities, then the workshop probably isn't for you! (Just like, for example, the Haskell Implementors' Workshop probably won't be that interesting for people who don't work on Haskell compilers or interpreters, or want to.)
Right, this is what I am talking about.... 
Well, then all FFI bindings have this problem. Or not ?
Thanks for the explanation. Interesting.
An often cited advantage of Haskell is that well-typed programs are likely to be correct on the first run. Unfortunately that doesn't seem to apply to space leaks, and the usual advice is to use debugging tools. Is there any way to make space leaks less likely by using the typechecker instead? I guess that's kind of an open-ended research problem, though...
Java being my primary language, I am intrigued the author actually implemented not only Maybe, but apparently Monad in Java. I'd love to see the implementation and the resulting code. Java 8 has some monadic classes (notably Stream and Optional, both having an of(), map() and a flatMap() method), but they do not implement a common interface, so you cannot write functions operating on all of them in the same way.
Sweet!
Very instructive ! repo here: https://github.com/helsinki-frp/yampy-cube 
The denotation of a function fully determines how it behaves with respect to bottom which is all you need here. 
Your intuition is correct, these workshops are condescending. They're taught from the premise that you're not an equal participant, that your opinion doesn't matter as much as others. The workshops aren't an exchange of ideas between equals. See the other response to you here for an example: "... If you aren't concerned about women..." Such BS. You can be concerned about women and be active in advancing diversity yet Not agree with 3rd-wave feminist dogma. See the geek feminism wiki for this mindset that men cannot be feminists. They've been downgraded to mere "allies".
I'm curious which Ada-Initiative-sponsored ally skills workshop you attended (location / date)? I'd certainly like to pass along this feedback if you feel this way.
&gt; &gt; the premise feels slightly condescending... &gt; if you're not interested in supporting women... And your feeling was validated in the very next reply.
Ah. Unfortunately, since the Ada Initiative curriculum is open-source, they can't be responsible for everyone who uses the workshop materials. 
I am always amazed by your patience and benevolence. But I don't think you'll get anywhere talking to someone who seems to be an active member of /r/fatlogic, a hate sub in the same spirit as the now banned /r/fatpeoplehate, and who tries to promote a 4chan hate project ([project harpoon](http://princesswhistle.tumblr.com/post/127164721576/heres-the-plan-made-by-4channers-about-project)).
Could you expand a bit on fexcess-precision remark? All resources I can find encourage it while noting a bug on x86 machines. In general I am on your side and would go with AVX/whatever, since playing with precision is like aligning code by hand, but would like to get some more insights. Sources: [1] [Documentation to 7.10.1 version, optimisation section](https://downloads.haskell.org/~ghc/7.10.1/docs/html/users_guide/options-optimise.html) [2] [Known bugs](https://downloads.haskell.org/~ghc/7.10.1/docs/html/users_guide/bugs.html#bugs-ghc) [3] [Haskell wiki performance article](https://wiki.haskell.org/Performance/GHC).
Named parameters, on the other hand, are kind of great. I miss them from Python.
My guess is that because `getTables` is used later, `values` has to be retained until the end of the function, and `values :: [Release]` can get quite big... I'm not yet sure why this doesn't happen in the fixed version (values is also used twice in the fixed version), perhaps GHC inlines `getTables` (which doesn't really depend on values there) and then applies stream fusion to the remaing use of `values`... seems pretty unlikely to me though. EDIT: Oh yes, this is very likely to be the explaination. `values` is a list that is very big when evaluated fully. In the first example, the following will happen: 1. the first call to getTables is evaluated =&gt; values is still an unevaluated thunk (since getTables doesn't actually depend on values at all) =&gt; no memory leak here 2. now, we do forM values $ ... . Here, the whole list in `values` will be forced. We can also not GC values yet, since we still need it later for the call to `getTables`. 3. now, the second getTables is evaluated. Even though we don't really need values to evaluate this, GHC doesn't inline getTables and so doesn't see that. so `values` wil only be GCed after this call. In the fixed version, the `let tables = getTables values` will be evaluated when `tables` is forced the first time. `tables` will then no longer retain a reference to `values`, so the only reference to `values` comes from the `forM values $ ...`. That means that we can garbage collect `values` *while* we're iterating over it, so we only require constant amount of memory. 
Can you post the the core diff of `store` ?
I'd love to read a blog post, including what OCaml does better than Haskell.
&gt; The value restriction, which must exist due to uncontrolled side effects, leads to very confusing type errors and forces me to monomorphize a lot of code or make it a fair bit less efficient. I think this is a very good argument against side effects.
A very simple example is the following two implementations of the power set (well, power list) function. No CSE (unless GHC accidentally gets too clever, which can happen sometimes...): powerSet :: [a] -&gt; [[a]] powerSet [] = [[]] powerSet (x:xs) = powerSet xs ++ map (x:) (powerSet xs) versus manual CSE: powerSet' :: [a] -&gt; [[a]] powerSet' [] = [[]] powerSet' (x:xs) = let tmp = powerSet' xs in tmp ++ map (x:) tmp Now `length . powerSet` runs in constant memory, while `length . powerSet'` blows up, even though it's much faster.
Hello Togrof, I agree with a lot of your point although I've never done Haskell (came from the /r/scala ). Type inference often has some trouble, null... frak they are stuck with it cause of java, etc. FYI, you can add methods to a class by doing something like this implicit class SeqWithMyMethods[A](val seq: Seq[A]) extends AnyVal { def yourFunction: Seq { ... } } the extends AnyVal tells the compiler that it's not a real class (so no run-time cost) and now your code looks nicer :)
I think **llvm** is the path, but I dont know how or where the path begins.
Haskell only applies one argument and returns one thing. That one thing may be a function that takes an argument and returns another thing - known as Currying* or partial application, e.g.: :t (+) (+) :: Num a =&gt; a -&gt; a -&gt; a :t (3 +) (3 +) :: Num a =&gt; a -&gt; a :t (3 + 3) (3 + 3) :: Num a =&gt; a So + is called with 3 and returns a function that adds three to something - this returned function can be reused, thus: let partialadd3 = (3 +) partialadd3 4 7 partialadd3 2 5 * named for Haskell Curry not the foodstuff
Cool, thanks. Btw, if you're trying to read Core, I'd suggest passing `-dsuppress-all` to GHC when generating the core. It'll suppress a lot of the noise (you'll rarely need the noise when inspecting core). 
But some effects shouldn't commute. For example, `StateT` of `[]` is a different beast from `ListT` of `State`. `ContT` of `Maybe` is very different from `MaybeT` of `Cont`. Why is commutativity of effects regarded as universally desirable?
&gt; Requires special keyword for recursive functions, in a functional programming language I used to believe this was a con, even ridiculous. Then I had created a bunch of accidentally recursive definitions in Haskell (oops, used `foo` (the LHS) in the RHS instead of `foo'`, hang!). Now I want that, as a feature!
OTOH, there is no `unsafeCoerce`, while you have the useful benign effects. In Haskell, if you want benign effects, you have to let in `unsafePerformIO`, and then his drunken friend: `unsafeCoerce` comes along too.
I think that this is probably related to the inliner behaving differently in that case: GHC might think "Oh, applying fst to a function f that returns a tuple? I'll better inline f and evaluate the "fst" statically" (not sure if that's actually how it works, but it makes sense to me). Then, see https://www.reddit.com/r/haskell/comments/3hvrm9/a_haskell_space_leak/cubp2on for an explaination (the reference to values will also not be retained when inlining, since then GHC sees that values really isn't need for getting the tables). 
&gt; I'd love to see the implementation and the resulting code [Here's my attempt](http://earldouglas.com/articles/java-monads.html) at monads in Java. It requires a bit of handwaving, e.g. `@SuppressWarnings("unchecked")` on concrete implementations of `flatMap()`, but otherwise it's fairly monadey given the limitations of Java's type system. public interface Fn1&lt;A,B&gt; { B apply(A a); } public interface Functor&lt;A,F extends Functor&lt;?,?&gt;&gt; { &lt;B&gt; F map(Fn1&lt;A,B&gt; f); } public interface Monad&lt;A,M extends Monad&lt;?,?&gt;&gt; extends Functor&lt;A,M&gt; { &lt;B&gt; M flatMap(Fn1&lt;A,M&gt; f); } *EDIT: Added quote for context*
Yeah, so? It's an implementation detail, because mathematical functions are not at all concerned about how they are evaluated. And since lazy evaluation allows strictly more functions to terminate, it actually brings us closer to mathematical functions.
The actual solution to this is to ban non-termination.
Yeah... The thing I like about CBPV as a core calculus is that it doesn't rule out either approach. You can locally choose what you need.
Java just needs a 'This' type. Swift almost got it right with the 'Self' type, but 'Self' includes type parameters, so a method isn't able to return something of type 'Self' with a different type parameter. If Java had a 'This' type, it'd be able to implement things like Monads and Functors.
I think it might be more complicated than it sounds. Generic types with unapplied parameters can't implement interfaces. Only concrete types you get by applying those parameters can. The `This` type would essentially require at least a limited form of higher-kinded polymorphism.
Hmh... When I first wrote that I was more sure of myself than I am now. What I meant, more or less is this How would you write a function that operates on any monad, and that's not a part of the monadic interface? Say, `sequence`. `M&lt;List&lt;A&gt;&gt; Sequence&lt;M, A&gt;(List&lt;M&lt;A&gt;&gt; list) where M : IMonad` (this is C#, but it should be similar enough). The problem is in the constraint. You need `M` to implement `IMonad`, but the best you can do is have `M&lt;A&gt;` implement˙IMonad&lt;A&gt;`. This is not good enough for the above signature.
It was a link to this anecdote by Philip Wadler in his 2011 class about Haskell: https://www.youtube.com/watch?v=fxrlqni07jo&amp;feature=youtu.be&amp;t=4m29s
&gt; Also, I could go out on a limb and say that currying, composition and typeclasses are "hard" only if you have to un-learn OOP principles That sounds plausible, but my hunch is that OO/procedural knowledge serves at least as a small life boat with which to traverse the functional waters, even if it's a leaky one and made out of duct tape. You might misunderstand a lot of stuff if you come from an imperative background, but at least you have a set of metaphors with which to understand it (e.g. typeclasses are like interfaces, lazy evaluation is like an object that implements Runnable). People with no knowledge at all might just go under. My hypothesis is that you can't impart knowledge "out of the blue"; you have to anchor and relate it to what people already know. Hence that list of examples: stuff like concrete lists and filtering can probably be visualized even by someone who doesn't know anything about computers.
I guess I'd attend ICFP-colocated workshops with professional/spiritual growth in mind. I'd expect them to showcase results and techniques which are otherwise difficult to come across. In this case, what I see is a seminar on basic etiquette with a feminist twist.
I don't know what is the content of this specific workshop, but I tend to be optimistic and assume good things -- my default assumption is that it will be interesting. In fact I will be at ICFP in Vancouver and I am considering attending this workshop. I think that the question of diversity in academia and industry is interesting, and affects our professional activity (if you are a professional in this domain; but even if you only contribute to a free software community as a hobby you are directly or indirectly affected). The content may or may not turn out to be useful to you personally, but other workshops have the exact same bargain (maybe this year the talks will be disappointing). I have attended a seminar on sexual harassment in the past, and I found it to be worth the four hours that I spent in it. It gave surprising data on the occurrence of this issue in academic settings (higher than I would have expected), non-obvious discussion of the persons to contact to report any problem (this is a delicate issue in an academic setting where the power dynamics are subtle), and I think the most interesting part was the question of: how do you react if you witness (or create, or are on the receiving end of) a situation of sexual harassment? (On this part, actors where on stage, playing roles and inviting members of the audience to step in and try to (re)act in situation.) This particular workshop is on a different topic and probably structured in a different way. But if you have even mild interest in the question, it's probably a good bargain to spend one day a year on this question, on the chance that you may learn more than what you expect. There is of course a strong competition with other very interesting workshops happening on the same day -- this is a problem that all ICFP workshops have.
Thanks for the in-depth explanation. I do agree that diversity in professional settings is an interesting/important question, and I would love to have reporting/acting information on sexual harassment. My ex-girlfriend was on the receiving end of inappropriate, demeaning and overly sexual flirting at a hacker space once. I learned about it after the fact, and I had absolutely no clue what to do about it, if anything. (It doesn't help that our local diversity advocates focus their information efforts on "don't do X" rather than "here's what to do if you witness X".)
https://i.imgur.com/xnxQpKV.png :(
It seems unnecessary to change `Enum`; you could just compose `enumFrom` with some function `[a] -&gt; f a`.
&gt; I'd love to see the implementation and the resulting code. Same here. It is clearly possible to write in an FP style in Java. After all one doesn't need an explicit Monad typeclass to design and write in a monadic style. However, I am a little bit skeptical of OP's claims. Firstly, it is not possible to express a generic Monad type class in Java that is type-safe. Secondly, it seems to have been an older project ("over three years") so I doubt it used Java 8, which means things must have been rather clunky on the syntax front. 
Some effects, like `State` and `IO`, commute "in the real world" (eg: in the imperative assembly code we generate), while others (nondeterminism, continuations, exceptions, free monads) don't. Part of the issue is that monads and monad transformers subsume both groups, while people mostly just want a way to mix arbitrary (commutative) effects freely, in all and only the cases where doing so doesn't affect their program's operational semantics.
I don't know if there's a discussion portion to this talk but I would bet the talk organizers would welcome anyone who came in good faith, even if they have some skepticism. Maybe the talk could help you pin down what it is about the "diversity politics" at your university that bothers you and help you guide your community in a more healthy direction, or be more effective than the advocates at your school who you're criticizing. My point is it sounds like you're broadly interested in this issue and I'd encourage you to engage with it; I would guess that's what the organizers are hoping for. Obviously there's a lot of great stuff to choose between at ICFP so whether this makes the short list for you is a different question. Jealous of y'all!
Yep, I know :)
I'm asking all of this hypothetically because there's no way I can attend ICFP; I'm just a poor undergrad with no money. &gt;My point is it sounds like you're broadly interested in this issue and I'd encourage you to engage with it Oh, I *have* been engaging with this issue; I am a feminist, son of feminists, themselves children of early feminists. I have been in the streets for women's causes since I was old enough to march. The thing is, I increasingly see patterns of dogmatism and tribalism under the banner of social justice. I sometimes put on a "blunt contrarian" hat when these issues pop up, because I want feminist community organizers to have a chance to demonstrate inclusiveness; they rarely do. In this case, I feel like my suspicions were vindicated by the condescending, *with us or against us* tone employed by /u/catamorphism. &gt;*If you're not interested in supporting women in your communities...* Of course I am, you shitbird. Hypotheticals like this are a kind of rhetorical mugging - they hand total control of the narrative to the speaker. *You're either with us or with the pedophiles*. And that's what I'm trying to root out in today's social justice activism.
I have non commutative transformers in more than half my monad stacks, I think. And I often throw in some transformer like that in after the fact. It's nice that transformers let me do this so flexibly, and its only very slightly cumbersome: need to define a newtype for the stack and a bunch of useful operations to use it via.
Naa ... doesn't help. :(
&gt;In this thread? In the linked talk description? I'm struggling to see it. In the works of the Ada Initiative, and of other organizations in the social-justice-in-tech sphere. &gt;and what looks like your attempt to win points in some argument that none of us are participating in. I'm mostly laying out my thoughts in case anyone like-minded stumbles by. Right now I have no mental model of what it means to hold feminist-critical opinions in CS academia. I've only met a handful, and role models are scarce; for example, Scott Aaronson [notoriously retracted](http://www.scottaaronson.com/blog/?p=2119) his position on the subject. But I'm sure I'm not alone out there, and I hope to eventually be part of a meaningful dialogue on being feminist-critical in academia.
In hindsight, that is glaringly obvious.
I "just write code" and use these non commutative monads all the time. I guess I need to see such a system and whether the extra kind of effect tracking is worth it. Sounds like an effect system is almost always a subset of RWST IO? In which case, just use that?
That's my first post on Haskell, and second blog post in English. Feedbacks will be appreciated!
I've been using gtk with Haskell in a project and it was considerably less painful than comparable with in Python or C. But it didn't really feel like doing FP for the most part. I'd really like a good FRP solution to the GUI problem.
You're totally right, I didn't express myself properly. My understanding is that there are two "tribes" - feminists and antifeminists - who engage in the rhetorical equivalent of total war. There's no dialogue here, no exchange of ideas, each side is simply trying to suppress the other through means such as manipulation, shaming, bravery debates and ingroup-outgroup politics. For a long time I put myself in the "feminist" camp, if only because the ideals being preached were closer to mine. But I could never make my peace with the movement. And I had to "pull out", sort of. I discovered that there existed a handful of people who believe in feminist ideals while remaining critical of modern feminist practices; first Scott Aaronson (see comment #171 [here](https://archive.is/9mHED#comment-326664)), then [Scott Alexander](http://slatestarcodex.com/2014/08/31/radicalizing-the-romanceless/). I've taken to calling these people "feminist-critical", even though there's probably a decent overlap with actual feminists. 
Yeah, I tend to agree with the Scotts about a lot of things. That the discourse is often toxic is definitely one. I find the best way to fight that is to just communicate empathetically. The process of Othering is the central problem anyway, and assuming that at the beginning of conversations has never been productive for me 
Mmmm curry. I'll have a chicken passanda, please. :) p.s. very good explanation.
Would you apply the same standard to other people who would describe themselves as "feminist-critical" -- if other people adopting that label did things you didn't like, would you reject the label for yourself?
Good question. I personally think labels are just a way to describe the world to an interlocutor. If I'm speaking with someone who thinks "feminist-critical" means "MRA" or "baby eater" or whatever, I'll explain how I conceive "feminist-critical" as someone who is merely critical of feminism. But if baby eaters start calling themselves "feminist-critical" in very vocal and public ways, and if the majority of people start associating the two, I'll ditch the label for something else.
just because FRP exists and it excites people (because it is an area of experimentation) doesn't mean you have to use FRP in haskell. You can make a flappy birds clone in Haskell with an IO-driven event loop just like you would in C. It's just that Haskellers don't give talks on established practices that can easily be done in the style of other languages.
I saw it. Looks great!
What about the implementation of `ByteString` makes them costly? I was under the impression they were close to a raw array of bytes.
There's a difference between putting turkey and peas next to each other on the same plate, and putting them together in a blender. ;)
Do you have this as example code somewhere?
Here is the other video that is an introduction to Arrows. This video is sort of an introduction to Konstantin's talk. https://www.youtube.com/watch?v=6Vab1_icBWU
I wonder why LOLITA faded away ? It looks interesting with its 90000 semantic concepts...
Usually for this sort of thing I've always used fundeps in the past; for, as you say, they're better for this sort of multimoded relational programming. Alas, last time I tried to simultaneously use fundeps and type families in the same project, GHC refused to play along. (Things might've gotten better recently, but we can't rely on anything from 7.10 at present.) As the current project makes significant use of type families already, that limits my choices...
Since your question is answered, I'll ask if anyone knows of any text that says anything about deciding the order of curried parameters. I have an intuition that the first arguments should be the ones "most likely to be constant." Then the partial applications would be most likely to be useful.
Thank you, it's always good to learn :).
It's actually not too late. I started by reading the Wikipedia link to the riddle, so nothing was spoiled :)
Thanks for arranging this Yitz, looking forward to it!
That's probably as close as you can get in Java, but from a type-correctness point of view it's dreadful. Since the main reason for introducing the interfaces is type-safety, there's little point here - you're better off just writing your monad classes without using these interfaces.
Self-important, I am so smart, arrogant @$$holes like this aren't contributing anything useful to the discussion. Reading his blog actually makes me feel ill, and statements like this: &gt; After I implemented [...] the Monad type-class ... in Java make me wonder how much else on there is also bullshit.
Regarding the database section: imho ORMs are a bad idea that came out of the OOP hype and the reason that they don't exist in haskell is that haskell programmers are OOP skeptics.
Sometimes I wonder whether Reddit's karma algorithm and/or automated Reddit clients can be to blame for some of the negative karma. It's still surprising sometimes, like in this case. You made a brilliant effort IMO, and in general I know the Haskell community to be very supportive, friendly and constructive. Or who knows... maybe there are just a couple of folks having a bad day. Whatever the case may be, here's another + from me, thanks for your tutorial, Haskell is not easy for me to personally, so I'm going to study it. And.. even if I don't understand something, I won't treat that with a downvote. ✌
Thank you, much appreciated :) I do hope you find it useful :)
&gt; last time I tried to simultaneously use fundeps and type families in the same project, GHC refused to play along. Do you have an example of something which ought to work, but doesn't?
&gt; The drawback of it being compiled is that the templates have to be available when you compile the program. The way I want to do this site is that I want to have a binary that reads in the templates anew every time I run it. Just for future reference, Hamlet actually has [a runtime module](http://haddock.stackage.org/nightly-2015-08-21/shakespeare-2.0.5/Text-Hamlet-RT.html) that allows doing all of the parsing and rendering at runtime for these kinds of situations.
I heard whispers about that, but it wasn't clear to me that's what RT meant so I overlooked that module in the documentation. Thanks for pointing it out. I might consider porting over to Hamlet because I really like it...
&gt; That's unfairly harsh. Granted, my comment is a bit harsh but it enrages me that a community is so passive/unrealistic in terms of language adoption. If haskell is a better tool and not used, what's the point ? Haskell community don't care or seems to. Individually, the haskellers are a very nice bunch (the irc channel :) ) but as a community, it is a bit cold/indifferent to the exterior world. &gt; we just use means of achieving this goal which have nothing to do with Python, such as implementing better Haskell libraries and tools Counting on pure haskell libs to drive conversion is a bit stupid. (no offense meant). You need to play to your strengths. Building something equivalent to django is hard and very long and not very useful for many. Python dominates the backend. Teams have invested in this ecosystem. Efforts would be wasted to try to change that, too much inertia. Haskell need to bring something new. For example, a nice, documented, binding to celery ( a task queue written in python ) that would allow using haskell for the processing of "tasks" is maybe easier and would allow to use haskell where it is strongest : core algorithmic stuff. How many time I had to build a pipeline of transformation behind django+celery that would have been faster, easier, better in Haskell ! And each time I didn't because, well, as a lowly coder, I don't have this kind of time to build it, test is and to document it. And obviously, I couldn't throw away everything we had. call-haskell-from-anything seems interesting (I didn't know about it) but seems still a bit remote from the dirty reality of integration in a real stack. Give me that binding and you really help me. Just to be clear, I am convinced of the many advantage of haskell, but there are *irrelevant* in many existing software stacks, simply because the integration is non-obvious. A that's really a shame. There are lessons to be taken from the python community, which, despite its size, is very active, stay gentlemanly and promote best practice and diversity. I have followed a few conf about Haskell at Europython... How many were about python at Haskell conf ? There are a few of subjects that could be interesting to haskellers : implementation of the python dict, numpy, pandas. There is a lack of openness at the community level even if the individuals are the nicest coders I've met (cannot underline that enough). The haskell community is small, I know, so choosing its targets should be done very carefully. But that's maybe the root of the problem : there is no stewardship for conversion/integration. FP complete, well typed and functional works are doing a wonderful job but it seems it is not their goals (they are more inward looking). Damn, give a me a few k and I'll do it. 
Haskell would be very useful to a python developer, it could replace the C layer that many python app use. But the bindings are not non-obvious and the community don't really care. So... see a more detailed answer : https://www.reddit.com/r/haskell/comments/3hv6l1/why_is_this_haskellpython_binding_not_more/cuazbnh
&gt; I am convinced of the many advantage of haskell, but there are *irrelevant* in many existing software stacks, simply because the integration is non-obvious. Yes, quite obviously, the advantages of Haskell are irrelevant in any software stack which does not include Haskell. What I don't understand is why you expect otherwise. Is it easier to add, say, a Java component in a Python-based stack than to add a Haskell component in a Python-based stack? I was under the impression that given a software stack based on a particular language, it was easy to add components from that same language, and harder to add components from another language, but still possible via things like FFI, microservices, and serialization at the stdin/stdout boundary.
Is "composing widgets" the same thing as "spatial composition" as mentioned here (dom-gnats)? https://www.reddit.com/r/haskell/comments/3gl3ll/an_frp_gui_library_with_separation_between_causal/
I could be very wrong but couldn't you just keep track of the last two states and then only render their diff?
I accidentally stumbled on some of the design ideas behind `reflection` when I was trying to store types in JSON strings, which I wrote up in this blog post: [Using `Data.Proxy` to Encode Types in Your JSON](http://aaronlevin.ca/post/111871447488/using-dataproxy-to-encode-types-in-your-json). I found writing that blog post helpful to understand `reflection`. Perhaps it'll be helpful reading it.
No, since the text do not mention combinators. Spatial composition is for me to maintain the scope of the events to the section of the screen that produces it. To render the local zone (widget) affected and propagate the effect if necessary trough standard monadic/applicative combinators instead of catching the event at the top level and redraw the whole scene just in case. The later looks more declarative, but it is non composable, and thus, is wrong. By containing the event scopes, the scene can be decomposed in localized reactive domains that interact among them in ways specified by (standard haskell) combinators. [hplayground](https://www.airpair.com/haskell/posts/haskell-tutorial-introduction-to-web-apps) does that. The lack of spatial componentization may not be critical when programming simple games, or simple GUI forms, since everything interact with everything. But when complexity increases either it does not perform well or it is necessary to do complex signal preprocessing or post-processing of the rendering to maintain the performance. 
Where conflicts could arise, editorconfig solves this problem. Why waste CPU cycles in the compiler checking for purely aesthetic details?
It was rather peculiar to the problem I was solving. http://blog.sigfpe.com/2009/01/fast-incremental-regular-expression.html is an implementation of the idea without reflection. Just take your DFA, reflect it, and use arrays of size equal to the number of elements instead of the 'Table' that Dan uses there.
Logic errors can be captured by type systems. It's just a matter of encoding more and more of your logic invariants in the types. 
That's a fair point, I didn't take this into account. 
Is that really intrinsic to FRP or just some FRP implementations?
&gt;I didn't develop the method. It was just an idea that visited me during my morning bath procedures today :) I *really* hope you shouted "Eureka!".
There's three kinds of prepositions: Those that are so simple that they contain obviously no errors, those that are so complex that they contain no obvious errors, and those that are so mindbogglingly complex that they are the error.
Are any of the examples in this mixed style?
 can you go in to more detail about how you used it?
Like in this: https://www.fpcomplete.com/user/thoughtpolice/using-reflection
I mean, they might have said "implemented a monadic interface" in Java, but what's so smug about that? (I only skimmed the article). in Haskell Monad's a class, but it could be a pattern in other languages, like Promises.
Nice, this helped me understand Contravariant and Divisible.
It's the overall tone that offends. As for the "*Monad type-class ... in Java*" claim, Java's type system isn't powerful enough to be able express a Monad interface - try it. You need to be able to say that a Monad type `M&lt;A&gt;` supports an `M&lt;B&gt; bind(Function&lt;A, M&lt;B&gt;&gt; f)` operation, but there's no way to express that. E.g. this won't compile: interface Monad&lt;M&gt; { &lt;B&gt; M&lt;B&gt; bind(Function&lt;A, M&lt;B&gt;&gt; f); } 
Yeah, that's true. like guava is cool, but the point of monads (well, of typeclasses) is that you can write a generic function that works with any instance.
Are attribute grammars related to parser combinators? Or parsing expression grammars? I mean, libraries like Parsec or [LPeg](http://www.inf.puc-rio.br/~roberto/lpeg/) define an "algebra of parsers". But I'm not sure how it maps to "algebras for recursion schemes over data".
I always feel obliged to comment on how much I like Spock. It seems to really nicely have everything you need (routing, sessions, connection pooling, csrf protection, authentication, workers, etc), while still feeling small and simple. I hope to see it gain more traction. edit: I found this very helpful with getting started beyond the sort of "hello world" tutorial: https://www.youtube.com/watch?v=GobPiGL9jJ4 
&gt; prehook :: forall m ctx ctx'. MonadIO m =&gt; ActionCtxT ctx m ctx' -&gt; SpockCtxT ctx' m () -&gt; SpockCtxT ctx m () Am I right in thinking this is still rank 1, so the `forall` is unneeded? Not a criticism, just trying to make sure I understand! &gt; Now if we try to use `getCustomers` in an action/route outside of the `prehook` authHook (e.g. "public"), we will get a type error! That's what I came here for. Great work! I'm going to give Spock a very serious look next time I start a Haskell web service project (my usual habit is Scotty at the moment).
Yeah, I'd guess what's really going on is polysemy between meanings that are mass nouns and meanings that are countable.
`u :: U b`, so`runU u` has type `b -&gt; IO ()`. U $ runU u . f == U $ runU (u :: U b) . (f :: a -&gt; b) == U $ (runU u :: b -&gt; IO ()) . (f :: a -&gt; b) == U $ (runU u . f :: a -&gt; IO ()) == U (runU u . f) :: U a
Oh gosh. Not sure how I managed to confuse that. `runU u` is an accessor which gets the `runU` function out of the `U` called `u`. For some reason I get super confused with record accessors when they're used this way - same with any monad transformer like `runStateT`. I think I always forget that the implicit full type of `runU` when used as a function is actually `U a -&gt; a -&gt; IO ()`. Thanks for the worked example!
Fixes in this release: * Lots of documentation added to the source code. * File -&gt; Open now asks to add the package the file belongs to. * Default template used by Package -&gt; New has been fixed up. * GHCi mode works with when you have :set prompt in your ~/.ghci file. * Panes have minimum size to make them harder to lose. * Panes -&gt; Browse and Panes -&gt; Debug fixed (they now move the related child panes into them correctly). * Package -&gt; Run shows the Log pane (so you can see any stdout output). * Cmd + C, Cmd + X and Cmd V now handled by Gtk+ on OS X instead of being in the Leksah keymap (this seems to make them much faster, though I am not sure why). Cmd + Q is also changed, but still hangs for some reason. * Place holder text used to identify find and replace entry boxes on the find bar. * Resolve Errors (Ctrl + R) now works when no editor pane has the focus. * Background build now works when no package is active. * OS X launch script uses `#!/bin/bash` 
Very much agree - doesn't come off well as an ambassador for FP.
I believe having this is even better for the long term in for example when libraries and compiler changes you will want to know what was from where. Not to add you need something to remind you what everything was doing as much as possible. About the option of clicking to see documentation. I am totally unfamiliar with that I wish I can get something like that in emacs. **edit* * Looked at https://github.com/jfeltz/dash-haskell I should get this in my emacs soon
2) Underlying (judging from the log window), it seems to use ``cabal-install`` and ``hlint``. That is to say, it works as well with sandboxen as ``cabal`` does, which is perfectly. I work exclusively with sandboxen except for thise occasions where I need to install a utility program (like ``happy``) in the global space, in which case I install outside a sandbox and then delete my global package db as soon as the build is finished. 
Not off hand; I'd have to reconstruct the code. The problem I had in mind was when using fundeps to define type-level arithmetic (with all modes), but then using associated types to associate each instance of some type class with it's "size". The idea being that we could give types like this: `foo :: (Finite a, Finite b, LE (Size a) (Size b)) =&gt; F a -&gt; F b -&gt;...` rather than needing to thread the size through everywhere à la: `foo :: (Finite a m, Finite b n, LE m n) =&gt; F a m -&gt; F b n -&gt;...`.
I learned on another Haskell thread that Currying refers to the process of taking a function with arguments (a,b)-&gt;c and converting to a function that returns a function to consume the next argument e.g. a-&gt;b-&gt;c. 
Oh snap! ``regex-tdfa-text`` is still broken on GHC 7.10, and it's impossible to ``cabal install leksah`` without editing the source code for ``regex-tdfa-text`` first... 
[LCF](https://en.wikipedia.org/wiki/Logic_for_Computable_Functions) was a substantial piece of software written in ML in 1972.
That isn't so bad.
 cabal install regex-tdfa-text --ghc-options=-XFlexibleContexts
Proper support for Stack will come eventually I hope, but Stack helps already. The default user package DB is like living in a crowded house (lots of conflicts). Once you can use Stack for everything else you can continue to use the user package DB for packages you are working on in Leksah. You will be free to install new carpet: cabal install --force-reinstalls new-carpet Remove the horrible wallpaper: ghc-pkg unregister --force horrible-wallpaper Or just knock the house down and start over: rm -rf ~/.ghc/x86_64-darwin-7.10.2 
1) Not very, unless you you count changing the source of Leksah scriptable. We might be able to do some fun stuff with GHCJS, but we need to export some Leksah functions to JavaScriptCore first. 2) A bit. For instance if you add packages to your sandbox, but not to your user package DB they won't get included in the system metadata. I do not use sandboxes or stack because I have not worked out how to get them to build quickly (might just be me), compared to Leksah's building of dependencies (which only builds the packages in your workspace that it knows have affected by the change you make). &gt; I tried installing it on OSX last year, and the build failed. either way, I think I'll try it again. developing Haskell in Emacs has been a huge pain (and it's been the best thing I've tried!). Building for OS X is now the most painful (we bundle Gtk in the Windows MSI). The binaries in the leksah dmg files are build with MacPorts and there are instructions [here](https://github.com/leksah/leksah#buildling-on-os-x).
You can have your RWST be indexed by a phantom type with class constraints tracking which effects are used. Or just use mtl even. How are these two approaches different from effect tracking?
They're not. In fact, indexing the monad and using class constraints are how extensible effects are usually implemented.
[I am not sure what is going on](http://i.imgur.com/e2xAZ1A.png), but I'm going to have to think about it later.
I just realised I'd confused Spock and Scotty as the same project.
I like Spock's web site. It's nice when Haskell projects have fancy web sites.
Which version of Leksah was that? (0.15.1.0 to 0.15.1.2 had serious performance issues on Windows) Also what made it unusable?
Looks like a ReaderT with more flexibility by using type-level lists to add things into the readable type, if I'm reading it right.
Perhaps it's worth opening a feature request on cabal about tracking where a constraint comes from. We do that in [stack](https://github.com/commercialhaskell/stack#readme), e.g.: $ stack build While constructing the BuildPlan the following exceptions were encountered: -- Failure when adding dependencies: text: needed (&lt;1.1), 1.2.1.3 found (latest version available) needed for package: foo-0.1.0.0
I was really hoping we could spend some time discussing it at ICFP. I'd like to get to the point where stack &amp; leksah work as well as cabal-meta &amp; leksah did (or better). In particular if I have a stack with gtk3 and all its dependencies (including leksah) open in leksah I would like leksah to be able to build the correct ones when I change any file (or ask stack to do it). With cabal-meta it was just a matter of making sure all the vendor/xzy/xyz.cabal files got added to the workspace and it all just sort of worked (using leksah's build system). I never got it working properly with `cabal sandbox add-source` though (cabal-install wastes too much time checking the unchanged packages). Perhaps the same solution will fix that too. Unfortunately I have lots to do before I leave. I'm arriving in Vancouver on Friday.
It might have been 0.15.1.2, there was a big memory leak, it filled up all my memory in a few seconds. 
Try the latest versions. They do not have such problem (at least the .3)
You mean the `(conflict: connection =&gt; tls==1.3.1)` at the end? I don't think that tells the whole story though, does it? The feature stack has here definitely isn't inherited from Cabal, it was something we wrote directly, to track exactly where each constraint comes from for display purposes (you can credit /u/chrisdone with the foresight on that). Just to be absolutely clear, I'm referring to the line at the end that says: needed for package: foo-0.1.0.0 Which gives a direct pointer: if you want to use text-1.2.1.3, you'll need to modify foo.cabal.
This is very true, in my experience. Good code has either a unique `qualified` or an import list for every single import. That is an absolute requirement for maintainability of a large code base, and so an important habit to be in for small code bases, too. (*Almost* every - there are some rare exceptions.) It worries me that Backpack does not have good support for this workflow.
Unfortunately I'm not going to be making it to ICFP this year, but another stack maintainer (Emanuel "Manny" Borsboom) is going to be there, or at least for part of it (I think HIW at the least). I'd be happy to discuss with you outside of ICFP, and I'm sure Manny would be happy to discuss in person too. I'm not sure of the details of what you're discussing here, but it sounds like exactly what `stack build` does out-of-the-box.
How difficult is it to start using stack on an existing project? Is it basically a usability layer on top of cabal, or is it more independent? If it's pretty seamless to switch to stack, I'll just do that, since I've heard good things about it.
&gt; Seems to expose unsafe coerce indeed, making me wonder why you pasted all that code... Well, obviously, to demonstrate "benign effects" and `unsafeCoerce` in Ocaml. &gt; It seems to be for ffi purposes mostly? What makes you think so?
Oh yes you are probably right! It makes sense, GHC thinks it's being smart when not collecting `values` for the later call. Abusing typeclasses and laziness like this was bound to produce a space leak anyway, but thanks for you explanation! EDIT: I updated the blog post with this explanation.
I suppose not. The really annoying part is the proof obligations to show that certain recursive functions terminate.
Correct, you can think of contexts as readers
Can you post a link to some more information about any of that? The only thing that I can comment on is the type safe routing, which is not an improvement over scotty if you use ScopedTypeVariables, which I do anyway. The rest of what you said... I have no idea, but I would be interested if you could post a link to some docs or examples for what you are talking about.
Read more about it here: https://www.spock.li/2015/04/19/type-safe_routing.html - you currently can not achieve the same thing with scotty and it does not have much todo with ScopedTypeVariables.
Yeah, so this is *not* what I mean when I say that something "just works". In this case ``regex-tdfa-text`` is broken (and has been so for the better part of a year, IIRC). 
I'm anything but an expert on the subject, but I've read that the release of GHC 7.10.2 should make things a lot easier to get up and running with GHCJS. But I have fail to find any relevant information on the topic since then (to be fair, I haven't looked for it much). I'm eager to read tutorials and blog posts from knowledgeable people. 
Spock produces a wai application/middleware, so you can run it in a TLS context using warp-tls. Working on tutorials/docs is always on the todo list and I'd gladly appreciate any help and pointers to what you find laking.
You could use a Vagrant box for building the code on Windows. You can still develop on Windows and use your preferred text editor, etc, just build the code on your VM. That is what we suggested to people [for building the Unison project on Windows](https://github.com/unisonweb/platform#vagrant). Feel free to steal anything that is useful from there. A nice thing about this approach is that once you have your Vagrantfile and provisioning script working, you can check that into source control and then provide a totally consistent build environment for anyone who wants to work on the project. And the Vagrantfile and provisioning script are tiny, it's just the steps to build the VM image, not the image itself.
I see, a closer read of the article made that clear, but what of the other benefits you mentioned? I can't find mention of them on the Spock site. 
I'd go with this route as well. JS is x-platform, so you do not need to x-compile or anything :)
Yeah, `runU` **is** actually a function! :)
Note that currying and partial application are not the same thing as you seem to imply, but they are intimately related. The (+) function is curried. Calling (+) with 1 argument is partial application. http://www.codefugue.com/partial-application-vs-currying/ http://stackoverflow.com/questions/218025/what-is-the-difference-between-currying-and-partial-application
Ooh thanks TIL the difference, many thanks or ta muchly as we say in blighty.
You can usually get a shorter and more direct error messages by constraining versions further, here i'd do `cabal install --constraint='tls==1.3.2'`
Windows software development is shoddy. If you don't want to switch to a full Mac / Linux host, consider developing within a VirtualBox or VMware environment.
that's cool. two questions if you don't mind: 1) so you took the "quoted functions" strategy, rather than the "applying quotations" strategy, as per this video: https://youtu.be/FiflFiZ6pPI?t=1587 thoughts? 2) would using template haskell types like TexpQ be too awkward. I guess you're also interpreting them rather than splicing, so I don't know what I'm getting at... it's just whenever I try to do stuff like this, there's always several libraries. I've been playing around with the GHC API, I just wish it exposed it's parser and types in a nicer/purer way. 
I found this very helpful with getting started beyond the sort of "hello world" tutorial: https://www.youtube.com/watch?v=GobPiGL9jJ4
`-ddump-minimal-imports` is supposed to help with this; I don't know if any editor makes it easier to use that feature.
Why would you use Windows for web development in the first place? It seems like a platform very far removed from the average web server.
yes, Turtle is amazing! I think /u/Tekmo made it to teach Haskell to people at Twitter. and thus, it "just works". the only issue I had was converting between filepaths and text, for which you use the fp formatter, which took me several minutes and a guess to find. (perhaps the awkwardness is intentional, for safety). 
AFAIK, there still aren't any binary distributions of GHCJS, sadly. You'll have to build it from source, and that's something that I would not want to try doing, especially on Windows... Hopefully some day GHCJS will get bindists and a decent web site with an installation guide (and/or stack will make it trivial to use by automating installations of those hypothetical bindists).
I would have thought (although I am by no means an expert in this, please someone correct me if I am wrong) that template haskell is one of the most battle-tested ways of generating Haskell syntax trees. In most cases I think the generated code is sent straight back into the compiler as a part of the library you are compiling, and it doesn't get saved anywhere, but you can use the `-ddump-splices` option to see it, for example.
Perhaps something like [this scotty tutorial](http://adit.io/posts/2013-04-15-making-a-website-with-haskell.html) which will show the capabilities of Spock? Spock seems really interesting, but it will probably take me some time to try it because in order to do so I need to read the docs, do some programming trial and error to get a feeling of what works and decide which complementary libraries to use. Without prior knowledge this can be quite a wall. Using such a tutorial I can get a feeling of the framework just by reading something and can have a default complementary libraries to use, which is chosen for me, until I decide I want something else. A good tutorial can really lower the entry bar to the framework.
How does one use this file? I'm a stack newb. I've tried stack new, init, setup, and build, but can't quite get it off the ground.
I have GHCJS working on Windows, and it can probably be done more easily now. It took [MSYS2](https://msys2.github.io/) (not actually a bad thing), some sandbox contortions to satisfy tool dependencies (`stack` would probably alleviate this), and a bit of [questionable hacking](https://github.com/gtk2hs/gtk2hs/issues/110) (better fix now on Hackage, hooray), and unfortunately my actual goal (`reflex-dom`) is still [crashing GHC](https://ghc.haskell.org/trac/ghc/ticket/10563), but GHCJS at least is confirmed operational!
thanks!
what's wrong with Eq? I know it's laws are ambiguous (I assume the equivalence relation laws, and "try really hard to implement == as structural equality").
And it works very well
&gt; Maybe GHCJS on Windows is a more pressing matter for you. Did I say I was speaking for everybody else? &gt; In my view, it's up to the people who are working on GHCJS to choose what they focus on, and it's also up to the people who are working on the language to choose what they focus on. And is up to people like me who are deciding on which technology to use to give feedback... if they care, of course. &gt; Please don't behave like this regarding free open-source software; it's really unpleasant, especially for maintainers. [link](https://www.americanexpress.com/us/small-business/openforum/articles/6-tips-for-taking-criticism-gracefully/)
I'm looking forward to that day. Meanwhile I'll delve deeper into F# and I'll be back in a few months... as I always do.
Awesome!
Apomorphisms, you mean Anamorphisms?
&gt; Unfortunately I'm not going to be making it to ICFP this year Doh! This is the first year I can. &gt; I'm not sure of the details of what you're discussing here, but it sounds like exactly what stack build does out-of-the-box. Oh I think I worked out what is going on I have been testing `stack build` with `touch` and it seems like it is too smart for that. If I actually modify a file in the package source I have added to stack.yaml I get much better results! Is there a stack command we should use to list the source packages? or is it better just to parse the stack.yaml file and look in packages: ?
Even after I understood the arrow (the Haskell Report calls it the function constructor), I still took a very long time to get over thinking that currying in Haskell was a magical, built-in thing. The `-&gt;` is right-associative, which allows for currying, as each next argument is the single argument for each next function in the right-associative nest thereof, but function application - denoted only by whitespace - binds to the left, which means that `f x y` isn't being magical at all. The `f x` bit is just happening first, being most left, and evaluating to a function `g` that wants one more argument. `f x y` evals to `g y`, which evals to some value `z`. No magic.
Whoops! My bad, ha. I appreciate the sharp eyes.
Thanks. I thought the git and commit lines in the yaml file meant I didn't need to clone, and that it would do that for me. Why was the name wrong in the cabal file?
And I thought that I was about to learn a new concept. Sadly it's just a typo.
If you want a less painful functional programming experience for front end development, Elm does a really really nice job for that, and there's now a [type safe bridge from Haskell back end to Elm front end](https://hackage.haskell.org/package/elm-bridge)! Haskell is really nice for developing web application back ends. It's less nice to use on the front end right now.
He mostly writes Rails apps, all deployed to Ubuntu instances
Oh, apomorphisms are a real thing. (They're the categorical dual of paramorphisms, as anamorphisms are to catamorphisms.) I just haven't covered them yet. :)
You can also install pre-built Leksah binaries
Oh, that's cool. Looking forward to the upcoming posts, so I can read more about para- and apomorphisms (because I'm not smart enough to read the original paper that introduced the concept).
I have only spent a few moments scanning the text, but this passage caught my eye. Instantly memorable insight: &gt; The use of the term “algebra” to in this context may seen a bit discomfiting. Most people use the term “algebra” to describe manipulating numerical expressions (as well as the associated drudgery of childhood math courses). Why are we overloading this term to represent a class of functions? &gt; &gt;The etymology of “algebra” can shed some light on this—it stems from the Arabic root جبر, jabr, which means “restoration” or “reunion”. An algebra as a function that reunites a container of `a`’s—an `f a`—into a single accumulated a value.
&gt; 1) so you took the "quoted functions" strategy, rather than the "applying quotations" strategy, as per this video: https://youtu.be/FiflFiZ6pPI?t=1587 &gt; &gt; thoughts? Not quite! Wadler's talk is about DSLs represented by quoted functions. Here, I am not interpreting the quoted code into something else, like SQL queries, I am simply taking the quoted code and creating more quoted code out of it. Quoting a piece of code turns it into an abstract syntax tree, which can then be examined, manipulated and re-interpreted arbitrarily. For example: &gt; :set -XTemplateHaskell &gt; :set -XQuasiQuotes &gt; import Language.Haskell.TH &gt; let one = [|1|] &gt; let two = [|2|] &gt; let x = [|$one + $two|] &gt; runQ one LitE (IntegerL 1) &gt; runQ two LitE (IntegerL 2) &gt; runQ x InfixE (Just (LitE (IntegerL 1))) (VarE GHC.Num.+) (Just (LitE (IntegerL 2))) &gt; runQ [|1 + 2|] InfixE (Just (LitE (IntegerL 1))) (VarE GHC.Num.+) (Just (LitE (IntegerL 2))) In that context, a quoted function is an abstract syntax tree containing terms for binding and using variables. In the example below, `quoted_f` is such a quoted function: &gt; let f = \x -&gt; x + 2 &gt; let quoted_f = [|\x -&gt; x + 2|] &gt; runQ quoted_f LamE [VarP x_0] (InfixE (Just (VarE x_0)) (VarE GHC.Num.+) (Just (LitE (IntegerL 2)))) &gt; runQ [|$quoted_f 1|] AppE (LamE [VarP x_0] (InfixE (Just (VarE x_0)) (VarE GHC.Num.+) (Just (LitE (IntegerL 2))))) (LitE (IntegerL 1)) &gt; runQ [|(\x -&gt; x + 2) 1|] AppE (LamE [VarP x_1] (InfixE (Just (VarE x_1)) (VarE GHC.Num.+) (Just (LitE (IntegerL 2))))) (LitE (IntegerL 1)) Whereas a function on quotations is, well, exactly what it says on the tin: a function which takes a quotation as input and returns another quotation. In the example below, `g` is a function on quotations: &gt; let g = \quoted_int -&gt; [|$quoted_int + 2|] &gt; runQ (g [|1|]) InfixE (Just (LitE (IntegerL 1))) (VarE GHC.Num.+) (Just (LitE (IntegerL 2))) &gt; runQ [|1 + 2|] InfixE (Just (LitE (IntegerL 1))) (VarE GHC.Num.+) (Just (LitE (IntegerL 2))) Although I am using strings instead of abstract syntax trees, HaskellExpr is as representation of code just like quotes are, and this representation can be examined, manipulated and re-interpreted just like ASTs can. For example: &gt; import Data.HaskellExpr &gt; let one = HaskellExpr "1" :: HaskellExpr Int &gt; let two = HaskellExpr "2" :: HaskellExpr Int &gt; let plus = HaskellExpr "(+)" :: HaskellExpr (Int -&gt; Int -&gt; Int) &gt; code (plus `eAp` one `eAp` two) "(+) (1) (2)" Just like we did with Template Haskell's quotes, we can use HaskellExpr's quotes with quoted functions: &gt; let f = \x -&gt; x + 2 &gt; let quoted_f = HaskellExpr "\\x -&gt; x + 2" :: HaskellExpr (Int -&gt; Int) &gt; code quoted_f "\\x -&gt; x + 2" &gt; code (quoted_f `eAp` (HaskellExpr "1" :: HaskellExpr Int)) "(\\x -&gt; x + 2) (1)" And we can write a function on quotations: &gt; let g = \quoted_int -&gt; plus `eAp` quoted_int `eAp` two &gt; code (g one) "(+) (1) (2)" You probably concluded I was using the quoted functions approach because I said that "length" had the type `HaskellExpr ([a] -&gt; Int)`, as opposed to saying that it had the type `HaskellExpr [a] -&gt; HaskellExpr Int`. But in fact, it's very easy to convert the first form into the second form: length' :: HaskellExpr ([a] -&gt; Int) length' = HaskellExpr "length" apply_length' :: HaskellExpr [a] -&gt; HaskellExpr Int apply_length' quoted_list = length' `eAp` quoted_list And again, neither of those are what Wadler is talking about, because I am not asking my users to write expressions of either type in order to interpret them as something else. By the way, it would be much easier to do that kind of re-interpretation with abstract syntax trees than with strings, because we'd have to parse those strings into abstract syntax trees in order to understand what operations they represent. &gt; 2) would using template haskell types like TexpQ be too awkward. I guess you're also interpreting them rather than splicing, so I don't know what I'm getting at... it's just whenever I try to do stuff like this, there's always several libraries. No, I don't think they would be particularly awkward to use. I used strings because my input was a string (a command-line argument), my output was a string as well (I use the [hint](https://hackage.haskell.org/package/hint) library, which interprets strings representing Haskell expressions), and I'm not sure how to convert back and forth between a string and a TExp. &gt; I've been playing around with the GHC API, I just wish it exposed it's parser and types in a nicer/purer way. Did you learn anything interesting? I'm using the simplified interface provided by the hint library because the GHC API looks frankly intimidating :)
I don't think I have seen that one before. What version of GHC is this? Also what Cabal version is it using? Try running: ghc-pkg list If Cabal 1.22.4 is not listed you might want to try running cabal install Cabal &amp;&amp; cabal install leksah More instructions on building from source are [here](https://github.com/leksah/leksah#building-on-linux).
I am running GHC version 7.6.3 &gt; ghc-pkg list /var/lib/ghc/package.conf.d Cabal-1.16.0 array-0.4.0.1 base-4.6.0.1 bin-package-db-0.0.0.0 binary-0.5.1.1 bytestring-0.10.0.2 containers-0.5.0.0 deepseq-1.3.0.1 directory-1.2.0.1 filepath-1.3.0.1 ghc-7.6.3 ghc-prim-0.3.0.0 haskell2010-1.1.1.0 haskell98-2.0.0.2 hoopl-3.9.0.0 hpc-0.6.0.0 integer-gmp-0.5.0.0 old-locale-1.0.0.5 old-time-1.1.0.1 pretty-1.1.1.0 process-1.1.0.2 rts-1.0 template-haskell-2.8.0.0 time-1.4.0.1 unix-2.6.0.1 /home/fred/.ghc/x86_64-linux-7.6.3/package.conf.d Cabal-1.22.4.0 HTTP-4000.2.20 HUnit-1.3.0.0 QuickCheck-2.8.1 StateVar-1.1.0.1 adjunctions-4.2.1 aeson-0.7.0.6 ansi-terminal-0.6.2.2 attoparsec-0.13.0.1 attoparsec-conduit-1.1.0 base-orphans-0.4.4 bifunctors-5 binary-shared-0.8.3 blaze-builder-0.4.0.1 blaze-html-0.8.1.0 blaze-markup-0.7.0.2 bytestring-builder-0.10.6.0.0 cairo-0.13.1.0 cmdargs-0.10.13 comonad-4.2.7.2 conduit-1.2.5 conduit-extra-1.1.9.1 contravariant-1.3.2 cpphs-1.19.3 distributive-0.4.4 dlist-0.7.1.2 exceptions-0.8.0.2 executable-path-0.0.3 extra-1.4.1 free-4.12.1 ghc-paths-0.1.0.9 ghcjs-codemirror-0.0.0.1 gio-0.13.1.0 glib-0.13.2.1 gtk3-0.14.1 gtksourceview3-0.13.2.0 haddock-2.13.2.1 hashable-1.2.3.3 haskell-lexer-1.0 haskell-src-exts-1.16.0.1 hlint-1.9.21 hscolour-1.23 hslogger-1.2.9 hxt-9.3.1.15 hxt-charproperties-9.2.0.1 hxt-regex-xmlschema-9.2.0.2 hxt-unicode-9.0.2.4 kan-extensions-4.2.2 lens-4.12.3 lifted-base-0.2.3.6 ltk-0.15.0.4 mmorph-1.0.4 monad-control-1.0.0.4 mtl-2.2.1 nats-1 network-2.6.2.1 network-uri-2.6.0.3 pango-0.13.1.0 parallel-3.2.0.6 parsec-3.1.9 polyparse-1.11 prelude-extras-0.4 pretty-show-1.6.8.2 primitive-0.6 profunctors-5.1.1 random-1.1 reflection-2 regex-base-0.93.2 regex-tdfa-1.2.0 regex-tdfa-text-1.0.0.3 resourcet-1.1.6 scientific-0.3.3.8 semigroupoids-5.0.0.4 semigroups-0.16.2.2 shakespeare-2.0.5 split-0.2.2 stm-2.4.4 streaming-commons-0.1.12.1 strict-0.3.2 syb-0.5.1 tagged-0.8.1 text-1.2.1.3 tf-random-0.5 transformers-0.4.3.0 transformers-base-0.4.4 transformers-compat-0.4.0.4 uniplate-1.6.12 unordered-containers-0.2.5.1 utf8-string-1.0.1.1 vado-0.0.5 vcsgui-0.1.3.0 vcswrapper-0.1.1 vector-0.11.0.0 void-0.7 xhtml-3000.2.1 zlib-0.5.4.2 
He said leash only builds with 7.8 and newer
Oh, playground looks cool!
&gt; I'd probably say parse the file, but it depends on exactly what you need. To get a list of all the .cabal files. Leksah would treat them as packages added to the workspace (something similar is done for `cabal sandbox add-source`). &gt; Mind if we move the rest of the conversation to the stack mailing list? Sure. I need to get some unrelated work done before I leave for Canada, so I might not be able to respond until then.
I also like to see the name of package, e.g. -- text import Data.Text (unpack) 
I wear many hats at all of my jobs.
Looks interesting, but no trial version and no support for system-wide GHC are pretty major drawbacks for me.
For those reading along, I pinged /u/hamishmack on this Github issue, which looks relevant: https://github.com/commercialhaskell/stack/issues/839
The names of all these recursion schemes remind me of this scene from Luis Buñuel's "Simon of the Desert": https://www.youtube.com/watch?v=h4ohMYYgnlY
Must have very good knowledge of Haskell tights system... I'll see myself out.
Cool! What's it written in?
Can it show, say, the type of a variable in a function definition, besides global function definitions? Does it support jumping to definition, or something like this? I think these two are the most valuable features when developing serious Haskell projects IMHO.
What is it?
I would be. It so happens that I had to do some programming in Swift a few months back. To this day I can't believe that anything good gets implemented in that chaotically designed language with such eternal mess of an IDE as XCode. XCode is literally the worst programming and most irresponsible product I've seen in my life. As for Swift, being better than Objective C is far from enough to be good. Oops.. Sorry for the rant.
I was looking for the right word instead of IDE. "Playground" is a good one.
I must admit I didn't consider the case of someone doing web development and deploying to Windows servers when I wrote my question above. I had the vast majority of web servers in mind which run some form of Unix and Unix and Windows just differ in so many small and not so small ways that it has to be a pain to develop on one and deploy to the other.
As mentioned elsewhere in this thread, I'd probably recommend using the GHC API if you could stomach it. The biggest problem with haskell-src-exts and its ilk are a tendency to lag quite far behind the current GHC release, whereas the GHC API and template-haskell are in step. Unfortunately it's a bit of a pain to extract the template-haskell source using `-ddump-splices` or other adhoc mechanisms, hence GHC API currently seems most promising. It's just _very_ low-level. There are a couple of packages using it on Hackage which are a good starting point: * [ghc-simple](http://hackage.haskell.org/package/ghc-simple) * [hint](http://hackage.haskell.org/package/hint) 
That is what React.js does, and there are many Haskell FRP's that have benefitted a lot from the same technique of comparing states, but it still is not composable. And it has its limitations: when things gets complicated, the React.js solution is to catch certain events [with event handlers](http://buildwithreact.com/tutorial/events) to make things faster. Anyway React.js use widgets that compose in some way, with some seaming here and there OOP style, since each widget is managed separately. But the combination of two React.js widgets does not make another more complex React.js widget . The standard Haskell FRP model does not even contemplate this kind of loose composability.
See also: - the [GHC Haddock pages](https://downloads.haskell.org/~ghc/latest/docs/html/libraries/ghc-7.10.2/index.html) - [full index](https://downloads.haskell.org/~ghc/latest/docs/html/libraries/ghc-7.10.2/doc-index-All.html) for quick CTRL-F'ing - the [GHC Commentary](https://ghc.haskell.org/trac/ghc/wiki/Commentary) - [HsSyn](https://downloads.haskell.org/~ghc/latest/docs/html/libraries/ghc-7.10.2/HsSyn.html) for the Haskell AST
Some context. This is a _very early_ version released by /u/chak in time for ICFP. There are lots more features planned, so don't view this as the final product.
Those two features are not yet implemented, but are planned AFAIU.
Swift isn't too inconsistent, particularly the new 2.0 release that removed a lot of the weirdnesses in 1.2. What in particular was the problem you had with Swift?
&gt;Any attempt to access other files or to initiate a network connection will be rejected by the operating system. what? You can't make a network connection? That's super limiting
No, it is a custom application written in Swift, Objective C and Haskell.
I didn't mention free monads. I am not sure if `FreeT` commutes with other transformers.
&gt; "all these people whose work I benefit from for free..." If wasting days is considered benefiting from something then you're perfectly right. The problem with (a part of?) this community is that you're too snob to listen to noobs and newcomer. No wonder Haskell is deemed impractical by many. It's your mindset which is hurting the adoption of the language. But, of course, I realize you won't listen to me because that's the problem after all.
I don't like Elm as a language because of its limitations. Basically, it's the same problem I have with F#.
&gt; You mean the (conflict: connection =&gt; tls==1.3.1) at the end? No, not just that. Cabal gives you the flexibility to resolve the conflict whichever way is best for you, and provides exactly the information you need in order to decide. In this example, the answer will *not* be of the form "you'll need to modify foo.cabal," because we are installing a package manually from the command line, not via a cabal file. Cabal reported the three pieces of information that are relevant for this conflict: trying: connection-0.2.5/installed-6be... (user goal) Version 0.2.5 of connection is already installed in this sandbox, so cabal is using that by default. rejecting: tls-[a bunch of versions] (global constraint requires &lt;1) Versions of tls &gt;= 1 cannot be used because of the manual constraint we specified on the command line. rejecting: tls-[a bunch of versions] (conflict: connection =&gt; tls==1.3.1/installed-956...) Versions of tls &lt; 1 also cannot be used, because they conflict with the version of connection that was selected. So you have at least three options: * Relax your manual constraint on tls. * Switch to an earlier version of connection, e.g., by saying `--constraint="connection &lt;0.2.5"`. * Allow cabal to pick a version of connection without preference for the installed version by saying `--constraint="connection source"`. 
Probably my most entertaining experience with the language was when I had to use Git-diffs to find out that declaring an inner function in some scenario causes the compiler to segfault. Boy, that was fun! Unfortunately it turned out to be not just some random bug, which you would expect from a beta-product (which BTW Swift is not), but one of many evidences of a chaos that the compiler is. Awful design decisions like the magical behaviour of the "let" statements, where they somehow affect the mutability of the datastructure instead of just the reference. The omnipresent promotion of the God-object antipattern, where in controllers are supposed to do everything. Despite the language being claimed to be functional, every aspect of it is designed with imperative programming in mind. The preference the language authors give to sugar over abstractions. I imply the mess around the `Optional` type involving the let-statements and if-blocks. Don't even try to program polymorphically, because you'll hit a dead-end right when you will think that you're almost done. Atop of all that it's full of bugs and inconsistencies. With things like that the language left the overall impression of being designed by people who are very bad at abstraction and of being targeted at uneducated devs, who just need to quickly hack their way through the app by pasting snippets they grab on StackOverflow.
Is there any reason for not writing it using cross platform libraries? If not, are there plans on making it cross platform in the future?
Let me start by saying that Haskell, OCaml and F# are my 3 favorite language, and so I would happily pick one of them over most others. I learned the 3 languages roughly concurrently, but used F# at work and therefore got very comfortable with it. When I left my job (at a Windows shop) to start a new effort from a blank slate, I naturally went for F# first, but this time I wanted to use Linux (NixOS). I did a decent amount of work packaging the F# and .NET worlds in NixOS but soon realized that my efforts were pointless as Mono just isn't stable enough. While writing less than 3 kLOCs, I encountered 3 major reproducible Mono issues (1 deadlock bug in a C# library, 2 runtime crashes). None of these bugs existed on Microsoft's CLR. After encountering the third bug, I decided to change course. OCaml lacked proper multi-threading and adhoc polymorphism (typeclasses/implicits). Although both of these are being worked on, the headstart that Haskell has, as well as the speed at which it is evolving made it a compelling choice. So I went for Haskell and I'm enjoying a love/hate relationship with all the fancy abstractions (Monad transformers, type families, etc.). This was relatively recently so I can't make too many comments about the language itself. My story is more about platforms and the TL;DR is: F# is great, but it's a Windows-only language (at least for now).
There are no HKT so you can't even define functors. Another shortcoming is that there are no typeclasses, so if, for example, you want to define a '+' operator over a Vector you defined, you need to resort to operator overloading a la C++. Another problem is that the language doesn't enforce purity. That is, I can't tell if a function has side effects by looking at its signature. This problem is worsened by the fact that many F#er (no, it's not a dirty word) come from C# and use mutability all over the place. Also, I don't like its syntax. I hate the apostrophes in `'a list -&gt; 'b list` for type variables and I hate the fact that many functions take their arguments as tuples + some other minor annoyances.
Marking codata *is* being explicit about where laziness is needed. This improves the situation but doesn't completely solve it. For the data (i.e. non-codata) values you still want strict evaluation by default. A value that takes a lot of time or memory to compute is just as bad as non-termination, so if you rely on implicit laziness for acceptable performance you're right back to having to defensively reason about evaluation order for the entire program instead of just the parts where the lazy evaluation matters. Consider a piece of code like `take 10 (sort xs)` which relies on laziness to be efficient. This is deceptively simple, but what's going on is actually very complicated. It only works because sort is implemented as quicksort, so right off the bat we're leaking implementation details. If sort was implemented as mergesort the code would be extremely inefficient. Not even all implementations of quicksort would be efficient. IMO you definitely don't want to have a potential trap like this in all code. Somebody refactors or changes some seemingly irrelevant detail and suddenly the code blows up. Marking this in the type is a big help because now you only need to be careful in the parts that are marked lazy. And I'm not even talking about the compiler magic needed to (partially) undo the constant factor inefficiencies introduced by pervasive laziness.
Saw it, bought it on app store, opened it, it crashed immediately. Tried again, same results. Running latest official 10.10.5 That was a mistake! Hope the Apple refund process works this time!
And sandboxing.
My vague intuition is the hylomorphism is the `a -&gt; b`, but `gfold`plays a role like a fix-point combinator in generating the hylomorphism. I wish I could play with this now but my lunch break is over.
This blog post reminded me of this paper [Unifying Structured Recursion Schemes](http://www.cs.ox.ac.uk/people/jeremy.gibbons/publications/urs.pdf)
Does a term of type `(f a -&gt; f b) -&gt; a -&gt; b` exist for every functor? It seems to me that it doesn't exist for `Maybe`, because if I give you `const Nothing`it has no way to produce an a (not counting something with an argument-shuffled `maybe`. Do I need to count that?)
These features are available in Atom. Additionally, there's go to definition + all goodies from Sublime, like the go to anything feature.
I got so desperate that I implemented the Applicative from https://hackage.haskell.org/package/foldl in python. It worked, but wasn't nearly as pleasant to use without type errors.
Men In Types would be a great company name.
I don't see what's wrong with `maybe`. It just implements pattern matching. But anyway, you can do something like: \u -&gt; maybe 0 succ . u . unFix :: (Functor f) =&gt; (f (Fix f) -&gt; Maybe Int) -&gt; Fix f -&gt; Int and unify `f ~ Maybe` to get `(Maybe (Fix Maybe) -&gt; Maybe Int) -&gt; Fix Maybe -&gt; Int`, then `a ~ Fix Maybe` and we can construct an `a` with `Fix Nothing`.
I think that **F#** is better supported on Windows due to Visual Studio, but saying that it's Windows-only to me is not correct also as now.
The IDE experience is a point I did not mention. It's probably a bit better on Windows but there's a lot of work being done to support open editors. However, my issue was not with the IDE, but with the runtime.
I didn't mean that there's anything wrong with it, just that it's how I'd provide a default b.
One of the things I miss is Traversable.
Thanks. I've just updated the repo to use stack. It's now much easier to build it. See the updated build instructions in README. https://github.com/helsinki-frp/yampy-cube /u/gfixler , I hope this would help you.
Yeah, those are all fair points. (well, it's a little unfair to talk about the "indirect marketing campaign": the Swift blog certainly never called Swift a functional language, nor did the Swift twitter account. You can hardly blame the language designers for things that the *users* of Swift say. [This blog post by Rob Napier is in line with my thinking](http://robnapier.net/swift-is-not-functional)) I especially agree with you in terms of the Swift solution to the "pyramid of doom" - I would have preferred to see a standard library implementation of Applicative or something. I *don't* think that Swift is simple, though. Simple doesn't necessarily mean easy: in one sense, C is one of the "simplest" programming languages, after all. Python, on the other hand, is very complex. I think the motivation for a lot of the added complexity in Swift, like Python, is to make it easier to pick up, especially for non-programmers. What I meant by "the barrier to entry had to be low" was really that it needed to be familiar to Objective-C, C#, and Java programmers, so that swapping over would be easy. That means compromises in terms of how many new concepts you choose to bring in, and how many old concepts you have to keep. That's why value types and reference types, as well as several other clashing concepts, coexist. I think Swift did a good job of reconciling those concepts. You don't, obviously (which is fine, don't get me wrong, your points are well-founded) but it sounds like you think it could have been done better. But think about other languages that tried to tread the same line Swift does: Scala comes to mind first. Do they *really* do a better job?
I've been looking a bit more into it. Just so I'm not misunderstanding something: you're supposed to pass context data into a template as a HamletMap? Are you meant to build this map yourself through its constructors? Is there any particular reason HamletMap is defined as `[([String], HamletData url)]` instead of something a bit more... efficient? like `Map Text (HamletData url)`? Note the sub-question in here: why is the key a `[String]` and not a `String`? I suspect this is important in some of the more advanced control structures? ~~I noticed you also need a UrlRenderer to render templates. I'm assuming this is a function you're supposed to write which takes whatever route object you have with query params and constructs a URL from that? Like the `@{IndexR}` stuff from Yesod?~~ Yes. ~~How are query params passed to the URL in the template?~~ By some slightly more advanced splicing: according to the Hamlet docs: `@?{(SomePage, [("page", pack $ show $ currPage - 1)])}` I don't know if I'm able to write documentation (never done that before!), but I could certainly attempt a tutorial once I get this going. Sorry for asking you directly, but based on the GitHub history you're the only one so far who's contributed to the RT module... Edit: Also am I misunderstanding something or does $maybe work a bit... unconventionally? Since HDMaybe needs to contain a (Just HamletMap) you get some nesting there which I'm not used to from Yesod.
You truly realize how great a functional language is when you switch back to an imperative language. Python was one of my favorite languages before using haskell. I learnt haskell, but then had to use Python for a project, and realized how superior it was. 
You are not that right about throwing old tree away. I think that parts of the tree can be reused (changing brackets in a+(b+c) should not throw away a,b and c). Which is easy in Haskell. It is also relatively easy to implement dynamic programming in Haskell, and that also should help you with optimizing transformations.
Some responses: * No good reason for `[(x, y)]` over `Map x y`, at least none that I can think of * The `[String]` is like that to support fields within a data structure, e.g. `$forall x &lt;- baz $ bar foo`. You'd need to have a key `["foo", "bar", "baz"]` that contains an `HDList` value * You have the right idea with URL rendering, though it's more limited in runtime Hamlet (since we can't interpolate arbitrary constructors). The URL values must be passed in with `HDUrl` or `HDUrlParams` (and yes, you use those constructors explicitly) Anything you could write up here will be most appreciated :)
I didn't see that question. Even in "real" Hamlet, though, that doesn't work: `getMessage` would have to be of type `ToHtml a =&gt; Maybe a`. You can't run monadic action inside Hamlet like that (by design, to separate actions from pure code). You'll need to use the `HDMaybe` constructor to make that work.
Bought it and gave it a try. Can't use it for [what I'm currently working on](http://lamdu.org/) because it has a fixed set of hackage packages installed and these don't happen to include everything I need. Seems like a nice IDE. Hopefully in future versions it could install additional dependencies and talk to Hackage. I guess it should be possible in MAS apps as Xcode calls home too.
I might be misremembering, in that case. :) Thanks again!
[These people exported it](https://github.com/search?utf8=%E2%9C%93&amp;q=winghci).
&gt; it's a little unfair to talk about the "indirect marketing campaign" I remember how aggressively they've been pushing it for some period last year. I mean, most of /r/programming posts were about Swift. I just don't believe that it wasn't orchestrated by Apple. But yeah, I've got nothing on this one but my opinion, so I agree, it's a subjective argument. &gt; I don't think that Swift is simple, though. Simple doesn't necessarily mean easy: in one sense, C is one of the "simplest" programming languages, after all. Python, on the other hand, is very complex. I think the motivation for a lot of the added complexity in Swift, like Python, is to make it easier to pick up, especially for non-programmers. Oh I've got a good one for you! You absolutely should see this presentation by Rich Hickey (the creator of Clojure) if you haven't already: http://www.infoq.com/presentations/Simple-Made-Easy. &gt; it needed to be familiar to Objective-C, C#, and Java programmers, so that swapping over would be easy Do you really find Objective C any similar to C# (or Java) except from being imperative and in a weird way object-oriented? What I mean is that it's a goal they should have dropped. &gt; it sounds like you think it could have been done better. Yes I do. Generally I would have gone with very different compromises. I also can easily name you like a dozen people just from /r/haskell which I believe would have done that better, including myself. &gt; But think about other languages that tried to tread the same line Swift does: Scala comes to mind first. Do they really do a better job? Actually I'm an ex-scalaist and Swift has seemed to me primarily inspired by Scala. I believe that Scala is fine as a transition language, which lures OO programmers into the FP world. I don't find it a good FP language though. I also lately came to believe that OO is a mistake altogether. So you can conclude my opinion on Scala. I also have recently [expressed it in more detail](https://www.reddit.com/r/haskell/comments/3h7fqr/what_are_haskellers_critiques_of_scala/cu5kdjk) if you're interested. Nonetheless I believe that Scala does do a better job. Primarilly because of one point: it approaches problems with abstractions instead of unscalable syntactic extensions. PS, I enjoy having discussions with people who actually consider opinions of the other side, so thank you!
XCode is on the MAS but it's an Apple product that doesn't have to play by Apple's rules (and indeed it doesn't).
Thanks, didn't think of using github search. The question whether anyone is maintaining it still holds
I tried to create a program with same spirit as original leak, but easily reproducible for learning: https://gist.github.com/tphyahoo/100449003e9254e0a143 I think it still needs more work, but maybe this helps someone.
these are awesome resources. there's like a 100 links from the commentary one. 
drat. given the author, I was hoping the Ovjective-C would be taken care of by a lot of language-c-inline!
viva el zygohistomorphismo! que es el zygohistomorphismo? *shrugs*
GIMME PART 3 PART 3 PART 3 !!! (no really but thanks for parts one and two, they're the best intro to recursion schemes I read :)
there was a poll on this sub a few months ago about "what haskellers want" and "do we need for more people to try or stick with haskell". I dunno whether windows support priority, maybe /u/FPGuy (I think they ran the poll) does? F# is easy to install on windows because it's .NET as for priority, there are only a few dozen GHC devs (I think I read that the other day). I (and many others) don't care about Windows support, while I care a lot about IDE support, just as you care less about other features. for both those reasons, if suggest you to be more diplomatic if you want to ask people to spend their time on what you want them to. 
I submitted a problem report and Apple refunded me --- I'll wait until this thing is more mature
So you only accept "thanks" as feedback. That explains a lot. Good luck with attracting new developers!
I'm not paying you to beta test your software.
I haven't found a function that mimics traverse for dictionaries... you can use a dict comprehension but it is less clean, why do I have to mention keys at all?
AFAIK no one is actively maintaining it, but WinGHCi is pretty good as it is. It's just minimalistic wrapper, which is slightly superior to the plain shell because (a) it prints errors in red, which I have only managed to emulate under Linux with a very brittle hack, and (b) it has a "reload"-button, which is very convenient when you're interactively developing. You'll experience the following issues, however: * The last working directory is saved across runs. If you disable the global package DB and set up the options to use a local sandbox, and then start in a directory that has no sandbox, WinGHCi will freeze. * If you want to print Unicode symbols or use them in your code, you're SOL because hputchar will throw an exception on/misprint certain characters, no matter what encoding-magic you try. This also applies to compiler error messages.
I'm really excited about this, and I'm fully willing to pay the price, perhaps even more, if it were not restricted by the AppStore conditions. My interests lie in making server related stuff. I signed up for the newsletter. If/when you set up a pay,net system for a non sandbox version, do something like Texpad where users can migrate their license if they have an AppStore version installed https://www.texpadapp.com/osx Further, a bit of advice, make the purchase process easy and simple. Use Shopify, or my favorite, Gumroad. https://gumroad.com I've been able to access what I've purchased afterwards at my convenience with no hassle. For soemthing that requires a license, that will be incredibly friendly. I don't work for or have affiliation with Shopify or Gumroad, I've just been pleasantly surprised by both.
Your split view controller delegate is de allocated but you are sending messages to it still. Crash! 
If you ask me, it has more to do with Haskell's static typing than Python's supposed imperative nature. You can write all your functional combinators and your codata in Python too, but as the project grows, the problems of malformed data and unclear format are ever exacerbated. I know from personal experience and from accounts of people who have worked on large Python projects that dynamic typing (along with quirks like default parameters) becomes a huge burden and a source of bugs. Having to write down the types of your functions, and having the assurance that they will only accept one specific thing and no other, is a huge boon for program robustness and reusability. Just the other day, I read the following example in SICP [p. 143, 2nd edition; originally in LISP], which might as well have come from a PHP manual: elem _ [] = False elem y (x:xs) = if x == y then x else elem y xs I have no doubt *someone* wrote something like this up in Python too. Too bad it's provably incorrect, which you see when you try to define and :: [Bool] -&gt; Bool and = not . elem False because `elem False` will return `False` if `False` is in the list! This is the sort of fast-and-loose programming style into which people fall if the compiler doesn't force them to practice disciplined thinking. The fact that even such supposedly simple functions can have such egregious faults lets one guess what sort of bugs will lurk in complex, real-world systems.
There's a (free) Haskell kernel for Jupyter/iPython notebooks. And there's HaskForce for IntelliJ. Why should I plunk down $25 for this?
Is lack of TCO a problem? How about some trampoline code? Given the number of Haskell / Haskellesque languages that compile to JS, it is definitely possible to overcome (and even embrace through FFI) the dynamic typing.
This is true, but do you think it could serve any practical purpose? 
Please send us an email at support@haskellformac.com with what's missing for you (i.e., which packages).
Re type info, yes, it is the same information that GHCI's `:i` gives you.
As other said, this is due to the current sandboxing entitlements. The plan is to allow outgoing network connections in a future version.
It is loosely based on Solarize http://ethanschoonover.com/solarized (or rather the Xcode derivative of that theme).
It seems great and deserves to be paid, but I'm deeply concerned about the fact that it's named just as "Haskell" on AppStore, without any other adjectives like "For Mac" or "IDE" and so on. It seems rather misleading and some people might think "Oh, the programming language Haskell is a proprietary pay-ware! We can't adopt it". Is there any plan to change its name to "Haskell For Mac" or "Haskell IDE for Mac" to avoid such a confusion?
Some potential benefits of using Haskell would be lower costs (fewer devs needed) thanks to its high level of abstraction, as well as fewer defects and better maintainability due to static typing. Disadvantages would be no real precedent or available resources to refer to on this. Enterprise-grade software is opaque, the docs are proprietary, so there's not much info on how to roll your own, unless you have an insider angle. The hardest part will be selling it. Bureaucratic entities like hospitals are expensive to sell software to. Also, a medical records application is probably going to need to do EDI, and there's no open source EDI library in Haskell yet. I'd be interested in contributing to one though.
That caught me off guard too! Also, searching for "Haskell for Mac" doesn't actually return the product for me on the store! (That may be more of an Apple thing)
This was a good read. I think Haskell needs more practical but simple examples like this.
&gt; So theoretically it's possible that a year down the line a new user running cabal update for the first time could get a multi-gigabite index? The new incremental index already contains the meta-data history going back to 2006, and the *uncompressed* index tarball size is 273MiB. To get an uncompressed size of 1 GiB one year down the line we'd have to see three times the activity of the past 9 years on Hackage in just one year. Not sure how likely that is. Also, zlib compression was used for the compressed tarball which is about half as efficient as lzma compression on that kind of data. &gt; What data is in the index? Previously (i.e. the 00-index format): - one merged `preferred-versions` file - the latest `.cabal` file revision for each package version With the new 01-index format: - one `preferred-versions` file (typically in the 100 byte OOM) per package for each preferred-version edit (history for that truncated starting at 2015-08-20) - one `.cabal` file per package version upload and each subsequent `.cabal`-edit - one `package.json` file (under 250 byte) per package version 
This is why I am thinking if it would be possible to combine Python+Haskell in some meaningful way? 
Fascinating! "MUMPS" on @Wikipedia: https://en.wikipedia.org/wiki/MUMPS?wprov=sfti1
Not from my experience. The `let` keyword induces certain properties on the value in contrast to just declaring an immutable reference to it like the mentioned `final` or `const` constructs of other languages do.
A good one. Thanks!
The overhead of additional heap objects. There's a bit more structure around `ByteStrings` to support things like `mmap()ed` ByteStrings and passing them directly to system calls, this also involves `ByteString` using pinned memory (which causes memory-fragmentation). `ShortByteString`s have different trade-offs which can be more desirable in some use-cases. The documentation in [`Data.ByteString.Short`](http://hackage.haskell.org/package/bytestring-0.10.6.0/docs/Data-ByteString-Short.html) explains this in more detail.
Horrifying
404
Well, if I have a local mirror site like [hdiff](http://hdiff.luite.com/), will there be great change for mirroring it?
&gt; The new incremental index already contains the meta-data history going back to 2006, and the uncompressed index tarball size is 273MiB. To get an uncompressed size of 1 GiB one year down the line we'd have to see three times the activity of the past 9 years on Hackage in just one year. Not sure how likely that is. _Thinking out loud here_: If this really becomes a huge problem one could imagine a scheme where users do _not_ download the entire index. After all, how often do you install a package that is from 2006? Probably for most users it would be sufficient to only get data from the last few years. 
This is wonderful. Often in Haskell concepts are explained "bottom-up" and I would argue it is often harder for many developers to learn new stuff that way. Here is such a nice little explanation that works "top-down". Well done !
I'm trying to install this with ghc-7.8.3 and I'm finding this error with the ghcjs-dom dependency: src/GHCJS/DOM/Types.hs:2875:23: Not in scope: type constructor or class ‘AudioTrackClass’ src/GHCJS/DOM/Types.hs:2922:27: Not in scope: type constructor or class ‘AudioTrackListClass’ and more. Any hints?
I don't know of any such attempts, but it would be interesting. Pure FP could certainly make the core domain implementation cleaner and subject to proofs and reasoning and serious verification and stuff. What I've seen of domain modelling in EHR software is pretty incoherent, and working in that industry helped convince me that OO generally confuses more than it helps. If I were to make an EHR system, I'd build it as an append-only set of event logs ("event sourcing"). And I'd definitely focus on small clinics and not big hospitals, because the big hospitals (AFAIK) need an ungodly amount of integrations. There are some startups making EHR software with nicer interfaces, but generally I think the UIs of the established packages are barely usable. In the country in which I worked, the frustration caused by such UIs was the subject of national discussion, and when I talked to people in medicine I felt vaguely apologetic on behalf of the whole industry. So I think there are lots of opportunities for huge improvements. I would also think seriously about a semi-decentralized model with coherent support for at least temporary offline operation, because central outages are *incredibly* bad... they can basically stop a whole hospital. Thinking in a decentralized way from the start will probably also help with scaling to bigger installations. Oh yeah, and any new EHR system now needs to be mobile-friendly.
That's weird, long and not very pretty Python code. Sounds like you learned the wrong things from the Haskell side. *Usually* when you have a function with "x" and "y", they are both of the same type. In your case, y is a number and x a sliceable object. Also, I'd not call it "f". You could write the whole code much shorter in Python like this: In [16]: [range(1+y, 10+y) for y in xrange(5)] Out[16]: [[1, 2, 3, 4, 5, 6, 7, 8, 9], [2, 3, 4, 5, 6, 7, 8, 9, 10], [3, 4, 5, 6, 7, 8, 9, 10, 11], [4, 5, 6, 7, 8, 9, 10, 11, 12], [5, 6, 7, 8, 9, 10, 11, 12, 13]] In Haskell, your code would be something like: import Data.Functor slice1to10withoffset xs n = take 10 $ drop (n + 1) xs main = print $ slice1to10withoffset [0..] &lt;$&gt; [0..5] Even then, I might have written using a list comprehension as well: main = print $ [[1+n..10+n] | n &lt;- [0..5]] Or like this: main = print $ (\n -&gt; [1+n..10+n]) &lt;$&gt; [0..5]
I do not expect to do it in the short term. However, it is a matter of using the reactive framework (package transient) to GUI widgets. It composes elements that trigger IO events. But since I programmed a bit of X-Windows with C time ago, I have no experience programming GUIs. I would need some help. 
You might have an slightly older version of WebKitGTK. Try: cabal install ghcjs-dom -fold-webkit cabal install leksah 
&gt; How deterministic is the haskell compiler? Still an [open issue](https://ghc.haskell.org/trac/ghc/ticket/4012) &gt; Could we prebuild libraries into static libraries and later link them together? We could then set up a build farm that builds all versions of all packages and sign the binaries. This would only work for a low-dimensional "flat" package dependency space such as e.g. Stackage snapshots. On Hackage each package describes a version range of packages it is compatible with, so there's *several* configurations a package like `aeson-0.9.0.1` could be compiled with. In fact, even simpler packages like `text` have already a "degeneracy" too high to build all valid configurations.
factis research is doing something related and uses Haskell for the core backend component doing the heavy lifting of processing clinical data, generating the UI, and data synchronization. &gt; Checkpad MED is an electronic health record for the iPad. It provides doctors mobile access to all relevant medical information such as images, letters, surgery reports, and patient data. The Checkpad MED product consists of an iPad application and several server components for data synchronization and import. 
Will do, thx
Multi-parameter functions are possible using typeclasses, see `printf` for example.
/u/snoyberg, I'd like to note that the codepage 65001 fix does result in some wonky output in the Windows shell for me: D:\Experiments\hask_anything\app&gt;stack build Setting codepage to UTF-8 (65001) to ensure correct output from GHC hask-anything-0.1.0.0-57c8091ea57afec62c051eda2322cc2f: unregistering ( local fil e changes) hask-anything-0.1.0.0: build Preprocessing library hask-anything-0.1.0.0... In-place registering hask-anything-0.1.0.0... Preprocessing executable 'hask-anything-exe' for hask-anything-0.1.0.0.. . [1 of 1] Compiling Main ( app\site.hs, .stack- work\dist\x86_64-windows\Cabal-1.18.1.5\build\hask-anything-exe\hask-anything-exe-tmp\Main.o ) app\site.hs:173:5: Not in scope: ÔÇÿthisÔÇÖ app\site.hs:173:10: Not in scope: ÔÇÿwillÔÇÖ app\site.hs:173:19: Not in scope: ÔÇÿtypecheckÔÇÖ -- While building package hask-anything-0.1.0.0 using: C:\\Program Files\\MinGHC-7.8.4\\ghc-7.8.4\\bin\\runhaskell.exe - package=Cabal-1.18.1.5 -clear-package-db -global-package-db -package- db=C:\Users\blauwers\AppData\Roaming\stack\snapshots\x86_64-windows\lts-2.21\7.8.4\pkgdb\ C:\Users\blauwers\AppData\Local\Temp\stack10656\Setup.hs --builddir=.stack- work\dist\x86_64-windows\Cabal-1.18.1.5\ build exe:hask-anything-exe --ghc-options - hpcdir .stack-work\dist\x86_64-windows\Cabal-1.18.1.5\hpc\.hpc\ -ddump-hi -ddump-to -file Process exited with code: ExitFailure 1 Note the `ÔÇÿ` gibberish. A workaround is to use a minGW shell: BLAUWERS@LBNL06449 /d/Experiments/hask_anything/app (master) $ stack build Setting codepage to UTF-8 (65001) to ensure correct output from GHC hask-anything-0.1.0.0: build Preprocessing library hask-anything-0.1.0.0... In-place registering hask-anything-0.1.0.0... Preprocessing executable 'hask-anything-exe' for hask-anything-0.1.0.0.. . [1 of 1] Compiling Main ( app\site.hs, .stack- work\dist\x86_64-windows\Cabal-1.18.1.5\build\hask-anything-exe\hask-anything-exe-tmp\Main.o ) app\site.hs:173:5: Not in scope: ‘this’ ’ � app\site.hs:173:10: Not in scope: ‘will’ ’ � app\site.hs:173:19: Not in scope: ‘typecheck’ ’ � -- While building package hask-anything-0.1.0.0 using: C:\\Program Files\\MinGHC-7.8.4\\ghc-7.8.4\\bin\\runhaskell.exe - package=Cabal-1.18.1.5 -clear-package-db -global-package-db -package- db=C:\Users\blauwers\AppData\Roaming\stack\snapshots\x86_64-windows\lts-2.21\7.8.4\pkgdb\ C:\Users\blauwers\AppData\Local\Temp\stack14132\Setup.hs --builddir=.stack- work\dist\x86_64-windows\Cabal-1.18.1.5\ build exe:hask-anything-exe --ghc-options - hpcdir .stack-work\dist\x86_64-windows\Cabal-1.18.1.5\hpc\.hpc\ -ddump-hi -ddump-to -file Process exited with code: ExitFailure 1
Why horrifying? It's quaint. There's nothing horrifying about seeing a person riding a horse instead of driving a car. Unless, of course, the horse is put in charge of managing your medical records...
You could write an EDSL to write Python in Haskell :)
Const in C++ is exactly the same. If the structure is not a pointer or reference, const means the fields of the value are also const.
The JVM code in the most visited areas of the program are compiled to native code at runtime, and they apply some pretty advanced optimizations to that code. Also they have invested a great deal more time and money into garbage collection technology, so they may be getting savings in memory management as well. That being said the next benchmark you run may have the opposite bias, so I wouldn't worry too much about it.
It doesn't change the fact of it being an unnecessary complication.
Thanks for writing this, I never really quite understood the reasoning for using MonadReader, MonadIO etc. until it clicked with this article. I always thought it was an alternative to the ReaderT style, and that it did not really give a direct improvement to maintainability.
Nope. I've also never had a job where I had to maintain a horse.
Because Frege is compiled to Java source code which in turn is compiled by the Java Compiler. I did not expect this to perform better than compiled Haskell as information for the JVM to optimize the code is lost on this way. at least I thought so.
I always like to know how much memory was used over the time of the execution. In many ways CPU and mem are to some extend exchangeable. Both the JVM and GHC have some interesting mem characteristics.
Good point. And not IDE, but play ground or something. 
I think we're misinterpreting each other. I'm saying that a correct way to implement `let` would be like `final` in Java or `val` in Scala. I.e., the keyword that defines an immutable __reference__ to a whatever value. The value itself may be a mutable object. Swift doesn't implement it like that.
Yes, but often what I want to reconstitute the map using the transformed values, not simply iterate over them... "traverse" is nice because you only need to supply the function, it doesn't require explicit iteration or referring to the keys.
On a mobile and will be for the next day, so I can't get the link easily, but there's already an issue tracking short term improvements for this. I believe they've all landed on master already. It would be great if you could test that out and report back. The 0.1.3.0 code was known to be suboptimal, we needed feedback in order to figure out the right solution to the problem.
Actually, this is getting worse for me: 9860 ms. O_o
&gt; A workaround is to move your project directory to the root of the filesystem, and to set your STACK_ROOT environment variable similarly to your root (e.g., set STACK_ROOT=c:\stack_root). This should keep you under the limit for most cases. That's great; the places windows puts user-installed software stuff by default can't be described as shallow in the hierarchy. I initially thought "but if I set that, I'll probably have to reinstall stack and then the packages will take ages...", but then I remembered how quick and pain-free it all was, and I don't care if I have to do it again! Weirdly and wonderfully, installing stack and using nightly early on to get ghc7.10 automatically updated winghci as well as ghci. I liked that a lot. 
The easiest way is to use =&lt;&lt; and readMaybe from Text.Read: λ&gt; import Text.Read λ&gt; readMaybe =&lt;&lt; Just "1232" :: Maybe Int Just 1232 λ&gt; readMaybe =&lt;&lt; Just "oops" :: Maybe Bool Nothing λ&gt; readMaybe =&lt;&lt; Nothing :: Maybe String Nothing If monads mess with your head, try this function, 'maybe': λ&gt; :t maybe maybe :: b -&gt; (a -&gt; b) -&gt; Maybe a -&gt; b λ&gt; maybe Nothing readMaybe (Just "123") :: Maybe Int Just 123 λ&gt; maybe Nothing readMaybe Nothing :: Maybe Int Nothing Also, this would be fine to post on SO. Have confidence!
Ah! Thank you. I still think it would be easier to reference the behavior (for newbies), but I digress. __Edit__: How about `pureWhen`? :)
`&lt;$&gt;` is an infix operator though
You can do `[x | b]` if you really want to golf things, or more readably for cases with nested-ifs in general: let foo | b1 &amp;&amp; b2 &amp;&amp; b3 = pure value | otherwise = empty
We didn't, but then we as a general rule don't add every combination of two combinators in base. There is a sort of organizing principle at work here called the "Fairbairn threshold" after Jon Fairbairn. The Fairbairn threshold is the point at which the effort of looking up or keeping track of the definition is outweighed by the effort of rederiving it or inlining it. A longer version of it is that if we had to name all n^2 interactions between combinators, pairwise, indeed we'd have a larger `base` library, but it'd be one that was oriented towards rote memorization rather than discovery of how things interact. In the limit of course we could keep loading up the namespace with simple combinations of k combinators, but we don't gain expressive power, and connections are lost. So if we add a combinator it should be because it _isn't_ a trivial combination of two other combinators, because there is some trick to it -- or at least because it happens over and over and gets considerably shorter, or because it exposes some optimization opportunity by adding it to a class. Without any of those things you take from everyone by filling the namespace, and you gain a net negative in the ability of folks to keep track of the API in their heads. Unchecked, this pushes people to break into little ghettos using different subsets of the language, and leads to write-only code. The Fairbairn threshold isn't the only guideline to organizing things, if there is a strong enough motivation it can be overcome, but I find it is a pretty good "sniff test" for new combinator proposals.
Is there a tax on characters typed now? This is a silly argument, imo - not to mention very noisy for the example the author provided: (&lt;$ guard b1) =&lt;&lt; (&lt;$ guard b2) =&lt;&lt; val &lt;$ guard b3 (Yes, of course you can combine the `guard`s into one)
Simple. As the [swift library docs say](https://developer.apple.com/library/prerelease/ios/documentation/Swift/Reference/Swift_Array_Structure/index.html#//apple_ref/swift/struct/s:Sa) &gt; The value of an array includes the values of all of its elements. Copying an array also copies all of the elements of that array that are value types. This means that changing one of the elements of an array does not change the elements of any of the copies of the array. So, swift arrays are value types. _They are never passed by reference_ (*). The compiler uses stack-allocation for them mostly, although it's free to move them to the heap if it wants. So you obviously can't modify immutable memory by inserting into an immutable value. Value semantics are a _really good thing_ that is missing from Java and Scala. They make reasoning about programs far easier, as you don't need to consider aliasing, and you can pass quickly-constructed data structures around without hammering your heap allocator. (* - as far as you have to care, anyway - reference passing may be used but only as an optimisation transparent to you)
This is a pretty reasonable argument. The main issue with `(&lt;$)` is that it doesn't compose well in its current form.
I never offered to use the Fairbairn threshold to retroactively remove things, merely as a threshold for introduction. The "burden of proof" that it doesn't harm code is then on the other foot. In adding we don't know what combinators other folks have that conflict with a name; in removing we don't know how large the pool of users of given combinator are. Also for things like `const` there are benefits to having a standard name for such a construction. `fromMaybe` in combinator form is at least messy: `flip maybe id` While I likely wouldn't go out of my way to add it, I definitely wouldn't go out of my way to remove it.
I knew it wasn't your intent; just piggybacking on your post to voice some of my own gripes :0
 const x = \_-&gt;x `const` is needed for partial application. 
Yeah. I worked with Vista when I was a teenager. The mumps stuff is a nightmare.
See comment above; we knew it wasn't perfect when we released the current approach, mentioned it in the release notes as something that needs feedback and work, and hopefully have something in place for the next release which will work for all system configurations.
Why not just rename it then? cns :: Constructor a =&gt; a cns = construct
I'm not gonna lie, that was me when I saw my first LaTeX document in college :) A math TA wrote a quiz in LaTeX my freshman year, and I asked him how he made it. He showed me how, and I decided to learn LaTeX on my own. I actually got great comments from professors and other students during undergrad on the quality of the typesetting and diagrams in some of my papers too.
`Obj.magic` is not officially supported. There are cases where it's used and later some new compiler change, such as an optimization, causes that code to break. It exists as an escape hatch for when it's impossible or impractical to otherwise implement something within the typesystem. Ideally it is never used... or that use cases can be removed with improvements to the typesystem. Culturally, talking about it is in hushed tones, lest you draw the attention of the old ones. And to be caught with unsound results due to use of such magic... is to be caught with one's pants down. ;) I'm curious now how many instances of it exist in the opam repositories. 
Hi, for what it is worth :-) I'm currently working for a meddev company and we have had a discussion about using fp (Haskell) for medical devices (and tried some minor prototypes) more from the assumed positive traits such as easier to reason about it during for instance risk assessment and mitigation, easier to prove that it is correct (I write "assumed" mostly because we have not ourself verified/tested the hypothesis in this environment). This to help us fulfill the meddev directives easier and help us produce safer products and also to help us create better testbeds for our own devices etc. I assume you also need to fulfil some kind of directives/regulations as well for an EMR/EHR systems so maybe the same benefits apply for you as well. On the downside we see the availability of resources that are fluent/experienced in Haskell and with some insight into meddev development. When it comes to speed of development it is not our highest priority but I'm pretty sure there are benefits there as well once we get it to a be a stable process, have a platform etc. Cheers! 
How about `pureIf`? It doesn't have a literal connection to `when`, but I think it does have a natural relation in terms of readability. I also read your earlier reply to PokerPirate - the Fairbairn threshold makes a lot of sense. I've been tacking together a few odd combinators in `combinators-extra`, maybe this should belong there or `MissingH`.
Is the root key signed with a password on the Hackage server? It really should be IMO :)
Well, what do you *want* it to look like? ;) As it is, it's already correct. The question is, what part of your monad stack is essential to its functionality, and how do you polymorphically target that?
My personal favorite abuse is List x = 1 + x * List x y(x) = 1 + x * y(x) y(x) - x * y(x) = 1 y(x) * (1 - x) = 1 y(x) = 1/(1 - x) -- taylor expand around x = 0 y(x) = 1 + x + x*x + x*x*x + x*x*x*x + ... Which says, a list of `x` is just unit (`[]`), or one `x` (`[x]`), or two `x`'s (`[x,x]`), or three `xs`, or ... etc.
If anything, the incredible performance of JITs have shown that the 'last mile' of compilation - taking the code from low-level AST to machine-code - is overrated. JITs make up for their small deficit there (not having much time to do this compilation) by customizing the code where it matters -- on the hottest code paths. It shouldn't be surprising that the JVM is faster than haskell then.
Oh, duh. Well that's my fault. But it's still passworded, no matter where it actually exists, *right*? (To be honest this is something that can be taken care of quite easily, but still should be done, since I didn't see it mentioned.)
&gt; for the JVM to optimize the code is lost on this way. at least I thought so. This is not so, IMHO. While the Java code generated by the Frege compiler might look terrible to humans because of variable names like arg$1, v_23475_98765 etc., it is actually quite pleasant for Javac. For example, all classes generated for Frege Algebraic Data Types are final, have only final fields, and a constructor that just sets the fields from the constructor arguments. All local variables in the methods are final, too. The code in the methods is usually like if (expr) { return expr; } // equation 1 else if (expr) { return expr; } // equation 2 else { return expr; } // equation 3 What *is* indeed lost is type arguments. With some effort, we could generate code that maps them to generic types in some cases (namely when the type variables have kind *). But this would probably do very little to the final bytecode, as java itself erases that information and generates the casts that are explicit in the frege generated java code. Since we **know** that our casts are correct (under the assumption that the type checker and the code generator are), we would really win nothing as far as the byte code is concerned. In any case, the JVM has no idea what the type of - say - the first element of a tuple is supposed to be. 
&gt; I would like the opposite : easily calling Haskell from python. I haven't used it, but you might be interested in https://github.com/sakana/HaPy
I would like to know - especially from the Haskell experts - whether this particular data point is because of any flaw in the Haskell code that would lead to invalid data. Please note that this is only one of many data points and it would of course be unjustified to draw any general conclusions from only that one point. However, it is still important to know whether this one is considered valid by the community.
Maybe `guarded` or `guardedBy`? x `guardedBy` cond Though indeed that implies: `x &lt;$ guard cond`
&gt; taking the code from low-level AST to machine-code - is overrated No, it really, really is not. JIT compilers aren't magical silver bullets for optimizing code. But they offer certain strategic and implementation-based advantages (*and* disadvantages) that make them desirable in some cases. In particular, Java's JIT compiler is NOT very memory-light, or even time-light in some cases. It's more than willing to spend even human-perceptable amounts of time optimizing code for a method (like, seconds upon seconds for optimization) and a large amount of memory to make the code fast. That is simply not always acceptable. Tracing JIT compilers are a better tradeoff here, and offer a far better memory footprint, but they are not going to do well on very branchy code - because they will generate a ton of side exits if multiple, common branches fall into the hot path and there isn't a bias for one of them, like an `if (x % 2 == 0) ... else ...`. And solving this is not easy without a heavy amount of work and design. Even LuaJIT, *easily* the most advanced trace compiler in the world, is going to fall flat on its face with branchy code if it manages to get it inside a trace like this. OTOH, your branch predictor in your CPU has been able to do 'periodic branch prediction' in this manner successfully for quite some time. The above example would be a branch of 'period 2', because the branch is taken every 2nd invocation of the loop. Even the original Pentium IIIs could handle a periodic branch of up-to 5. You will be using up branch predictor resources, but that might be preferable to randomly introducing a conditional in your hot trace (due to inlining a trace from far away, perhaps completely in another compilation unit), and your application got noticeably slower (not that this can't happen in other cases...) Static compiles are quite nice in a lot of domains, for example, mobile ones - where the tradeoff in compilation time is easily matched by power, space, and cycle savings from less literal runtime activity. Hell, your bus speeds are normally only a fraction of what a desktop CPU is capable of, making memory an even *more expensive* resources to use. Android doesn't use a JIT compiler anymore, exactly for these reasons (although one could argue Dalvik was rather insufficient anyway). What we really want are better CPUs. Think of what your CPU is doing in the above example: you have a code, it has a branch, it has no obvious bias. You emit machine code, and then the processor has to do *its own dataflow analysis* to reconstruct this fact and ensure the code is fast, for these periodic branches, and do other things like pipelining and out-of-order-execution. The CPU is literally repeating some of the *same* optimizations the compiler and humans have done, and they have to reconstruct a high-level dataflow graph from a low level IR after you emitted it from a high level dataflow graph. Seems kind of backwards, right? :) JITs are impressive. I am constantly amazed by their continuing innovation, and LuaJIT in particular is a testament to what boundaries we can *really* push. But to say that they obviously make 'static compilation overrated' is *quite* a stretch, and a claim we've been hearing for a long, long time. TINSTAAFL, or, as some of us like to call it: [The Full Employment Theorem for Compiler Writers](https://en.wikipedia.org/wiki/Full_employment_theorem).
Excellent rebuttal. I wasn't aware of the fact that Android doesn't use a JIT anymore. However, the very fact that .NET and the JVM come so close to C++ performance indicates to me that indeed, for the most part, static compilation's advantage over JIT, which exists at the last mile, is very minimal. Consider the fact that languages other than C++ and with similar memory management semantics to the managed languages (C#, Java, etc) are beaten by both the CLR and the JVM performance-wise (though perhaps not memory wise. It makes sense that JITs would consume more memory in general since they're still interpreters at heart). Doesn't this indicate that the last mile of compilation is at least somewhat overrated, in your opinion? The CLR is even able to do reified generics, which is a huge win for functional languages such as F#. This is only possible to do in a JIT, unless you do whole-program compilation and close your type universe.
That's not comprehension syntax. Those are guards.
Please don't name anything "return" now that we finally got "pure". The name is a relic from when the Monad typeclass mainly was for providing syntax that would look familiar to people used to imperative programming, and doesn't really describe the purpose of "wraping a pure value in a monadic context".
I wouldn't go with `guardedBy` because other `fooBy`s take their "by" as the first argument (e.g. `sortBy :: (a -&gt; a -&gt; Ordering) -&gt; [a] -&gt; [a]` ).
&gt; very early Is it a toy / proof of concept or will it get extended to a full IDE until the heat death of the universe ? 
This excludes infinite lists, tho; I wonder whether type algebra could be modified to give a correct answer even in the presence of laziness.
[You're not going to get correct UTF-8 output in the Windows console,](https://alfps.wordpress.com/2011/11/22/unicode-part-1-windows-console-io-approaches/) It's best wrapping the UTF-8 functions and using wide character printing for this reason.
Proper Unicode support in Windows's awful, awful shell is as elusive as a pink unicorn. It really does make me wonder whether it'd be better to bypass it entirely. Perhaps one could take GHC and replace all the console output with calls to a Writer monad whose contents then gets picked up by a graphical, Unicode-supporting shell.
 I swear I saw something posted here aimed at doing exactly that.
Can you give an example of this?
Upgrade to a newer GHC using this: https://launchpad.net/~hvr/+archive/ubuntu/ghc
Actually the JVM just has better heuristics. They have spent a lot more time on them then anyone else. None of it is particularly magical. For instance its auto-vectorizer was still quite primitive last I checked. 
Can people really not see past a single example? There is almost always a way to rewrite an example into a better form, but I find it such a poor argument when people are trying to provide improvements. I doubt that /u/Spewface wants this added to `base` to literally improve one line.
Say you have a map of urls indexed by the name of the page or whatever, and you want to transform it into a map of HTTP responses. In Haskell, you would have a **Map String URL**, and if you also had a function **fetch :: URL -&gt; IO ByteString** you would only have to do **traverse fetch urlmap** to obtain a **IO (Map String ByteString)**. And to make the page loads concurrent, something like **runConcurrently $ traverse (Concurrently . fetch) urlmap** would be enough. 
Now, I just wish Haskell powerful enough that I could give a definition for Delta t a = {- Derivative of t (:: k -&gt; *) with respect to a (:: k) -} and use it where I currently have to use `a -&gt; t a`. If t is Generic, I think we can do something close with TH but I don't think we can automatically and reasonably name the constructors. :( I'd expect that if `List a = Cons { head :: a, tail :: List a } | Nil` then we'd want to generate something like: Delta List a = MissingHead { tail :: List a } | MissingInTail { head :: a, holeyTail :: Delta List a }. "Missing" prefix + field name uppercased for single holes. "MissingIn" prefix + field name uppercased, and "holey" + field name uppsercased for recursive holes. For anonymous fields, use an ordinal for that field within the constructor. In cases of remaining ambiguity, prepend the constructor the hold occurs in. (E.g. "ConsMissingHead" and "ConsMissingInTail").
Here's some material on type logarithms. http://comonad.com/reader/2013/representing-applicatives/
Sure, but this example is insufficient motivation as the combinator isn't necessary here
This confuses me: &gt;Then each of First, Second and Third can map to two possible values, and in total there are 2⋅2⋅2=23=8 functions of type Trio -&gt; Bool. &gt;The same argument holds in general. If there are A values of type a, and B values of type b, then the number of values of type a → b is &gt;B^A &gt;This justifies the common terminology for function types as exponential types. Isn't the number of possible functions of type a -&gt; b equal to B*A? For instance where a is Triple and b is Bool: First -&gt; True First -&gt; False Second -&gt; True Second -&gt; False Third -&gt; True Third -&gt; False There are 6 possible instances of this function with type a -&gt; b. I feel stupid because I can't see any other possible combination of values for that function type.
(First -&gt; True, Second -&gt; False, Third -&gt; False) (First -&gt; False, Second -&gt; True, Third -&gt; False) (First -&gt; False, Second -&gt; False, Third -&gt; True) (First -&gt; True, Second -&gt; True, Third -&gt; False) (First -&gt; True, Second -&gt; False, Third-&gt;True) (First -&gt; False, Second -&gt; True, Third -&gt; True) (First -&gt; True, Second -&gt; True, Third -&gt; True) (First -&gt; False, Second -&gt; False, Third -&gt; False) 8 possibilities. 2^3 = 8.
Note, the term has evolved now, but originally "algebraic data type" referred to an _algebraically specified_ type. See something like [this](http://www.cs.unc.edu/~stotts/723/adt.html), or for more details: Martin Wirsing: Algebraic Specification. Handbook of Theoretical Computer Science, Volume B: Formal Models and Semantics (B) 1990: 675-788. This meant that the word "Algebraic" in "Algebraic data type" doesn't refer to the calculational aspect, but the fact that these types usually are similar to the syntactic data types used in algebraic specifications.
Used to work there. Didn't like it. A lot of people with a cargo-cult mentality about tools, and who reach for the Java hammer for every nail. Surprising levels of incompetence, often from acquihires. Food and "perks" (like video games) feel like a clever trick to try and make you make your time, social connections, and your very existence revolve around work, so they can extract more Java code from you for their not-so-great salaries. A lot of American corporate culture that was quite jarring to me, for example social events run by work after hours where you're expected to attend. Very opaque upper management. Most offices outside of MTV aren't so well-appointed. A lot of projects at Google are excessively boring just like any other coding job, and yet Google are happy to perpetuate the myth that people there work on "hard problems". 20% time is dead. They destroyed morale with bad management and promotion decisions. High-stakes performance review system, that can really damage an employee's career options in the company. Google's product strategy is now a lot more centralised, but still manages to schizophrenically jump from angle to angle as if they can't sort out what the hell they're supposed to be doing anymore. 
But are not Haskell inductive types initial in a suitable category? you can of course construct the map which is evidence of initiality: newtype Mu f = Wrap {unwrap :: f (Mu f)} bang :: Functor f =&gt; (f a -&gt; a) -&gt; Mu f -&gt; a bang f = go where go = f . fmap go . unwrap of course, this might not be total, but it doesn't have to be f . (fmap (bang f)) = f . (fmap (let go = f . fmap go . unwrap in g)) = let go = f . fmap go . unwrap in f . fmap go = let go = f . fmap go . unwrap in f . fmap go . id = let go = f . fmap go . unwrap in f . fmap go . unwrap . Wrap = let go = f . fmap go . unwrap in go . Wrap = (let go = f . fmap go . unwrap in go ) . Wrap = bang f . Wrap which is true in Haskell plus eta (so, Haskell without `seq`) right? The hard part is showing these maps to be unique suppose f :: f a -&gt; a g :: Mu f -&gt; a s.t g . Wrap = f . fmap g then g = g . id = g . Wrap . unwrap = (f . fmap g) . unwrap and then g = let go = g in go -- use fact above = let go = f . fmap g . unwrap in go --only suspect bit, depends on some sort of fixedpoint induction = let go = f . fmap go . unwrap in go = bang f no? IMO, it is indicative that the encoding of the initial algebra into the polymorphic lambda calculus Mu f = forall a. (f a -&gt; a) -&gt; a corresponds to what Haskell does as soon as that lambda calculus is extended with non termination (and is lazy, of course). 
The issue is not the length of the name, but the length of the type annotation
I can see three solutions in the existing Haskell libraries. First, expressing the problem using lists rather than an associative operator... msum :: MonadPlus m =&gt; [m a] -&gt; m a msum :: (Foldable t, MonadPlus m) =&gt; t (m a) -&gt; m a asum :: (Foldable t, Alternative f) =&gt; t (f a) -&gt; f a For `[Maybe a]`, `msum` picks out the first `Just`, returning `Nothing` if the list is empty or only contains `Nothing` elements. The `msum` version depends on the `mplus` for `Maybe`, and the `asum` version depends on the `&lt;|&gt;` for `Maybe`, so the second solution is one of two different spellings for the operator you're looking for... mplus :: m a -&gt; m a -&gt; m a -- (Alternative m, Monad m) =&gt; MonadPlus m (&lt;|&gt;) :: f a -&gt; f a -&gt; f a -- Applicative f =&gt; Alternative f These solutions so far have have the same basic problem - they're very generalized, none of the types even mention `Maybe`, so they don't concretely express the `Maybe`-based intent. Personally, I particularly like `&lt;|&gt;`, but if that generality bothers you, there's another spelling for a `Maybe`-only version in the Scrap-Your-Boilerplate library - `orElse` (Data.Generics.Aliases). orElse :: Maybe a -&gt; Maybe a -&gt; Maybe a Described as "Left-biased choice on maybes". I would vote to have that function with that spelling available without needing the SYB package. I would also make an analogy to another language - Icon - that I once learned a bit of. Icon had built-in backtracking, a little bit like Prolog, but the overall style was imperative rather than declarative/logic. It expressed "alternation" using the `else` keyword - if the left sub-expression succeeded that was the result, otherwise it used the right sub-expression result - associativity picking out the first successful result. Other than implementing failure as a kind of exception instead of using a `Maybe` type, it was basically this same `orElse`. The problem with Icon IMO was that it took this failure-and-alternation concept too far. For example relative operators didn't return boolean results, they were assertions that either failed or returned one of the arguments - and I could never remember which argument. I would have much preferred an `if c then x` that either failed or returned `x` - something like... justIf :: Bool -&gt; a -&gt; Maybe a justIf c x = if c then (Just x) else Nothing Oddly enough, I couldn't find `Bool -&gt; a -&gt; Maybe a` or `a -&gt; Bool -&gt; Maybe a` in Hoogle - it seems like an obvious function to provide, especially in a non-strict language where the expression providing `x` will never be evaluated if the `Bool` evaluates to `False`. Then again, the full `if` expression isn't really a problem. 
Actually this doesn't prove anything to do with lists, they don't meet the conditions required
What department, if you don't mind my asking? And what is "20% time?"
iinm, and /u/andrejbauer can correct me on this, in infinity CPOs, initial and final objects (can) coincide
Wow, I've had a much better experience than that. I think a lot depends on your team and what they do.
My thought on generalizing to `Monoid` is that it might make sense to write... (Just x) `onlyIf` c [1,2,3] `onlyIf` c (putStrLn "hi") `onlyIf` c But it seems a bit odd when you can also write... Nothing `onlyIf` c -- gives Nothing even if c is False [] `onlyIf` c -- gives [] even if c is False A different spelling than `onlyIf` might solve that. **EDIT** - A not-really-serious spelling suggestion... (*?) :: Monoid m =&gt; m -&gt; Bool -&gt; m (*?) x c = if c then x else mempty The `*` suggests multiplication, while the `?` indicates the `Bool` argument goes that side, so it's multiplication of a `Monoid` value by a boolean indicating "zero" or "one". It makes sense to multiply zero by zero, so it doesn't have the issue above. Then again, it's yet another cryptic operator. 
I've been at Google for ~18 months, doing applied deep learning in research. For me, it's an incredible place to work, but that's mostly because my teammates are smart and my projects are interesting. But, it's not a great place if you crave functional programming; the primary sanctioned languages are C++, Java, Go, and Python. There's a dedicated group of internal Haskellers trying to improve things, but they have a long road ahead.
Why do people use GHC if JHC is better? Extensions?
Sorry about that, that's why I wanted this discussion :) thank you!
The only approach I've found working more or less consistently is using wchar_t IO throughout. I've not tried native console functions yet.
I suppose that works too. Just when I hear about nested ifs guards leap to the front of my mind, and until today I hadn't realized they could be used with non-function values.
The variables should only be one letter when there isn't anything more appropriate. For example, the composition operator should be defined with single letter variables. `f . g = \x -&gt; f (g x)` tells you everything you know about the arguments, it is some f and some g, both of which are functions, which the type has already told you. There is nothing not covered.
As a tip I found the Atom editor, with the haskell plugins and ghc-mod installed is very nice. You can hover over code and see the type. While this doesn't help you understand, it is a lot faster than trying to work out types of 'inner' parts of a function in your head. You also get instant feedback about the bits that wont compile so no nasty surprises when you do a :r or cabal build.
OK, probably it need to stabilizes in some part but this will probably happen (or so I hope).
Haskell has a bunch of "new" idioms that take a while getting used to. Partial application, function composition, monadic DSLs, folds, applicative lifting, case analysis through functions and more. Before you learn these idioms/patterns, they will seem very foreign. Once you have learned them, they will be really easy to read. I don't have any quick and easy solution for you but to assure you that you are not going crazy. What you are experiencing is learning a whole new way of looking at programming which will take time and effort, and that it does get better. Much better.
You can use PackageImports to let you do `import "text" Data.Text (unpack)`, though I believe it's considered somewhat brittle to later re-organizing packages.
How is it any different than using an object in Java when all you know is that it implements some interface?
Do you have an example? In my experience Haskell code reads very clearly once I'm familiar with the patterns used.
[This Earley parser library](https://github.com/ollef/Earley), for example. I know how the Earley algorithm works and how all the used typeclasses work. But I don't have a clue how this code accomplishes any parsing.
I'm going to be either infuriatingly unhelpful or give you something awesome to hang on to, so hear me out. I am a mathematician and spent most of my programming life with C and C++, started learning Haskell about three years ago and basically stayed at the stage you describe for quite a while. What helped me was change in approach, I don't even think of Haskell as programming language at this point, it's just something that makes maths more interactive. In essence, I am embracing von Neumann's "You don't understand mathematics, you just get used to it" as a pretty sane approach to Haskell. Maybe it's a generally looked down upon mistake, but worked for me and indeed allowed me to go past small bare-bones projects. Resources that helped a lot: * [Haskell Road to Logic, Maths and Programming (free PDF)](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.137.9312&amp;rep=rep1&amp;type=pdf). Might get a bit boring, since it assumes no familiarity with Haskell, but it's a pretty good read if you won't stop at 'Haskell is the member of Lisp family' like some of the people who judged it and keep going. * Category Theory (for example [these lectures](https://www.youtube.com/watch?v=BF6kHD1DAeU&amp;list=PLGCr8P_YncjVjwAxrifKgcQYtbZ3zuPlb)) and its generalizations. ---- To quickly close possible complaint vectors: No, I'm not reckless or dumb and hardly your typical example of "research scientist == sloppy coder". It's just an addition to the meat of my work, not the focus. On few occasions where my C code required audit and external review before running it, I got nothing but praise. I just don't have time in my life to twiddle with each little nuance of language like Haskell, this approach is the best of two worlds for someone like me. Surprisingly well in fact. ---- EDIT: By the way, you might try and switch gears and go for [Idris](http://docs.idris-lang.org/en/latest/tutorial/introduction.html), at least for a short while. Depending on your general way of thinking about problems, you will either hate it or love it. I'm in the second camp, but again: mathematician ;).
Agreed that a lot depends on where you are in the company. It's unfortunate that anyone had this experience. Pieces of it, sure, but it's sad one person ran into all of those things in one experience.
Agreed 100%. The thing that annoys me though is that languages (including Haskell) are designed to *hide* the type from me, which is by far the most important thing. I wish there were something like in Agda or Idris where when you're building a function, you have a screen full of everything in local scope showing available objects and their types.
Spending that 20% time with my family at home will most benefit Google.
It is pretty confusing - many of the internal types have 5 or 6 type variables. When you have over two you're usually starting to climb complexity mountain.
I know someone who had that experience, and worse. Google has become a big city with a lot of neighborhoods, good ones and bad ones. It's clearly not the universally amazing place to work it used to be.
Theoretically, you're right. But for this topic, some Haskellers would want hear specifically what other Haskellers have to say. So it's relevant to this channel in a subtle way.
Nice article, could you expand a bit on what you use this for?
I worked on a very high profile, but now defunct user-facing product, as an engineer. Most of my work was in the data model on the back-end. I believe that the experience of other areas of the company (infrastructure dev, or those that do the AI stuff for search or ads) is a bit better, but working on user products is pretty hellish IMO. I also think it might be better if you're Californian, can adapt to the way Californians tend to behave (a kind of forced friendliness), or work in the much better MTV campus.
"You don't understand it; you just get used to it" has certainly been a useful approach for me. It makes it less scary for me to dive into the deep end of a library and just combine things that type check until I feel like I have an idea of how the library is used.
Yeah, there's a dgoiH in every large software company I think...
I could go on with more bad things, but I would come too close to casting personal aspersions on a public forum. For what it's worth, about 5% of the work I did was actually quite interesting, although most of that work I took on was just because I was desperate for a distraction. Also, as I was working on a doomed project, the management was a lot more stressed than I think they may be in other parts of the company. Regardless, I didn't hang around long enough to find out if I'd like another team better. I went headlong into academia and haven't looked back since. I did have an amusing conversation with Rob Pike though, in which I lost all intellectual respect for him.
This is somewhat of a pet peeve of mine surrounding the Haskell community. We have a language that has the *potential* to produce extremely readable code. We even have idioms that encourage the use of DSLs and combinators like Free Monads and lightweight type definitions it's just people never seem to go far enough. It's possible to write a large chunk your code (especially your business logic) so it's a DSL that reads like English and each part is all at the [same level of abstraction](http://codurance.com/2015/01/27/balanced-abstraction-principle/). Parsec code is often a great example of this DSL style - [this random example from RWH is pretty readable](http://book.realworldhaskell.org/read/using-parsec.html). But then you look at what people actually write and it's a jumble of large methods, wildly different layers of abstraction and abbreviations, all written using do notation with so many monad transformers it's basically in IO. Just as an example I'm going to pick on [`Stack.Docker`](https://github.com/commercialhaskell/stack/blob/master/src/Stack/Docker.hs) but this is hardly the only case like this.
Another interesting law for exponentiation is a^0 = 1: all functions from the empty set are equal, and as a consequence all terms in an incoherent context are equal. Note that the correspondence between type isomorphisms and cardinality only goes so far. In types you have the isomorphism 0^a * a = 0, which is not correct as a numeric equation (Edit: this example is wrong, see below). This mismatch is pointed out in [Remarks on Isomorphisms in Typed Lambda Calculi with Empty and Sum Types](http://www.cl.cam.ac.uk/~mpf23/papers/Types/shortremarks.pdf) by Marcelo Fiore, Roberto Di Cosmo and Vincent Balat, 2002. For a more recent and thorough investigation of the relation between type isomorphism and numerical equalities, see [ Axioms and Decidability for Type Isomorphism in the Presence of Sums](http://arxiv.org/abs/1401.2567) by Danko Ilik, 2014.
GHC has typed holes?
No need to hunt through imports to find where something comes from. Just load the module in ghci and :i will tell you where it imported a given symbol from. This doesn't eliminate some of the benefits of avoiding open imports, like avoiding breakage when a new minor version of a library exports a new symbol that conflicts with a symbol in your code.
This can be done in dependently typed language for a closed universe of polynomial types. It's quite fun actually.
How is a^0 * a = 0 an isomorphism in types? (Void -&gt; a, a) has |a| inhabitants, right?
In that case, how is that not correct as a numeric equation? Looks correct to me. For all nonzero a, 0^a = 0, which cancels the multiplication. If it's zero, well, 0^0 is not defined (but you might call it 1, as there's only one mapping from the empty set to the empty set), but the term a could cancel the multiplication anyway. Do you mean that 0^a * a = a in types, but 0^a * a = 0 in numerics?
There's an interesting result in the theory of probability, which says that if a random variable is distributed with a suitable power law*, which could be a good model for the time between my blog posts, then the expected value of the variable, given that it exceeds some threshold T, is about 2T. Given that, I would expect to see the next post in something like two and a half years. *more precisely, CDF(x) = 1 - x ^ -2
So it turns out I got this example wrong (this is what happens when you rely on memory without thinking clearly). In the paper, there is a surprising counter-example (attributed to Alex Simpson) but it is not this one, it is a case where an equality is true numerically but not as an isomorphism between types: `1 = 0^a + 0^(0^a)`. I have a sensation of deja vu about this blog post and, I'm afraid, the mistake I made above. The blog post is old so it may already have been posted here in the past -- I think I read it when it went out. Did we already have this conversation?
Oh cool! This is the problem with a series like this in a blog - it's sometimes hard to go back and edit the earlier posts to link forward to the later ones.
I also think I've seen this blog post before, but I haven't talked about this with you before 
(For anyone wondering, this is a rhetorical question. GHC has typed holes.)
I would post a real example, but I'm on the train atm! Disregard the tutorial example then, the real world example I gave is still horrible. 
The void -&gt; a doesn't seem to translate to Haskell. Due to laziness there are 'a' such functions. If the function where strict, then I have no idea what that one function would be.
But that doesn't help you if you have a finished function/program. I use the type lookup in ghc-mod a lot, but it's kind of annoying to wait several seconds for the result. **edit:** and this doesn't work if the program has an error and fails to compile. 
Isn't this the kind of optimization [JHC](http://repetae.net/computer/jhc/) is exploring (via whole program analysis)?
&gt;Haskell code reads very clearly once I'm familiar with the patterns used Of course it does. Everything that you are familiar with seems clear—that's tautological. The question is, how well does the idiomatic way that Haskell is written (e.g. very terse variable names, many highly specialised symbolic operators, very deep stacks of very generic abstractions) afford the process of *becoming familiar* with a new set of patterns? Say in a new library that you haven't used before. I'm very familiar with function composition, folds, and case analysis/pattern matching from other languages (Scheme, Prolog). I'm very comfortable with partial application—in my view, ubiquitous use of partial application is one of the least celebrated features of programming in Haskell but is one of the main things that makes Haskell code much cleaner than many alternatives. But then there are those deep, deep stacks of very, very general abstractions. It does seem (I read a blog post somewhere that said so explicitly) that Haskell programmers are more interested in finding the very most general, very most abstract way to express whatever they are doing than in doing the thing. That's fine as an intellectual adventure but can make life very hard for someone who isn't in your research group but would like to re-use your library. 
That was mine too. I think he does the same with binary trees in part two, by the way!
I got the Atom Haskell packages to work (in OS X) but found that the feedback loop is quite slow. My preferred alternative now is running ghcid in a separate Terminal window for quick type checking after save.. Even though it lacks jumping to the errors..
Isn't there an exponential amount of tests needed to determine that in such a black-box fashion? That is, if `A` exports `a` and `B` exports `b` that uses `a`, but nothing uses `b` it will take repeated pruning to get down to the minimal amount of exports. I guess you're suggesting this is done per commit, so things don't sneak in. I'd like something that would be applicable to existing code bases.
Oh, I stupidly didn't realize my intent was ambiguous. I meant it to be read as "also try", not "this is the wrong place." 
One thing I really like about subtyping that I haven't yet figured out how to do in Haskell/OCaml is sub-ADTs. For example, in Scala I can write: abstract class X case object A extends X abstract class Y extends X case object B extends Y case object C extends Y Which effectively introduces two type unions, `X = A | B | C` and `Y = B | C`, and makes `Y &lt;: X`. This is super-neat, as you can then specify that some functions only take `Y` instead of the whole of `X`. You can kind-of do this with row polymorphism (for unions, not records) or polymorphic variants, but it's rather inconvenient in languages I tried. Of course, the best alternative would be to have proper type unions, but those are not algebraic and thus require subtying as well. The other neat thing you could do (in Scala, but not in OCaml) is add additional properties to case classes (and even more additional properties just for `Y`, not for `X`)
Finding the most general way of doing something promotes true code reuse. Just solving your specific problem ignoring slight variations in the problem will result in a library that can only be used to solve *your* specific problem, but isn't capable of solving my very similar but slightly different problem. Finding a highly general library capable of doing what you want done is, all else equal, better than finding a very specialised library that doesn't quite do what you need it to, so you need to re-implement it anyway. It doesn't matter that the specialised library is easy to understand if it does the wrong thing anyway. With that said, of course libraries should include documentation on how they are used, ideally with a few different examples. I think it might also be important to point out that OP asked about how to grok the implementation of a library, not the API. (Which I assume OP is already familiar with.) You only need the API to be able to use the library.
&gt; Finding a highly general library capable of doing what you want done is, all else equal, better than finding a very specialised library that doesn't quite do what you need it to I agree, but it's the *all else equal* where we come unstuck. &gt;Finding the most general way of doing something promotes true code reuse No. It makes more kinds of re-use *possible*, but that's not the same as promoting it. In order for the re-use to actually happen, learning to use the library has to be quick and easy, enough quicker and enough easier than learning it is obviously much easier that building your own point solution two or three times. Many Haskell libraries don't do well on this front. &gt;You only need the API to be able to use the library Do you really believe that? That anyone can learn to use any library *only* by reference to its external interface? 
I'm not sure why we should be operating under the assumption that programming and maths should be different in this. I'm not assuming the opposite, I just would like to know why we should assume whatever we should assume.
That equation is true about types as well, depending on what you mean by true. You can't exhibit the isomorphism generically in `a` without call/cc, but for any given `a` of which you know whether it's inhabited or not, you can give an isomorphism.
&gt; can't be described as shallow in the hierarchy c:\Users\Michael\AppData\Local\stack Definitely not shallow. At least it's no longer c:\Documents and Settings\Michael\AppData\Local\stack
I have a [(slowly) ongoing project](http://haskellexists.blogspot.de/2015/03/a-very-first-step-towards-fragment.html) that faces the challenge of deciding which other declarations a given declaration actually uses. The biggest problem is how to deal with type class instances. Naively you need all instances for all combinations of types and classes that your declaration transitively mentions. You then need all declarations and instances for the types and classes that these instances mention and so on until fixpoint. In my current experiments this is quite a lot.
To elaborate: unfortunately, library design in Haskell, *particularly* where types are concerned, follows the tradition of mathematical papers, rather than software engineering projects. The folly of this is obvious to me. Not only the context is different, but I would argue that the centuries-long trend towards single-letter names and insane ligatures for operators is a plague that most mathematicians and computer scientists are simply immunized from through extensive suffering of such. I understand this opinion is controversial, but I have seen few convincing arguments to the contrary.
&gt; Finding the most general way of doing something promotes true code reuse. It also encourages people to ignore the library. There's a real trade off here: if you make your types more complex to support functionality I don't need, you are making your library worse from my perspective. If this process is exaggerated to a sufficient degree, the payoff for learning the library may no longer justify the required effort.
They have different goals, for one thing. Mathematics and Computer Science clearly are very closely related—I'm sympathetic to the idea that much of Computer Science is actually a kind of applied maths. But *programming* is not computer science. I approach this question, and all questions about programming, from the point of view of a “software engineer”. What would it mean to be “engineering” software? There's lots of confusion in that area, but I think of it this way: * pure science—the goal is to *know*, which is met if the results are (eventually) enlightening. Money is spent on this in the same way that money is spent on art. * applied science—the goal is to *know how*, which is met if the results are (eventually) useful. Money is spent on this in a spirit of speculation, tending towards gambling. * engineering—the goal is to *change the material world for the better*, which is met if the results improve people's lives. Money is spent on this carefully and parsimoniously and in the confident expectation of a near term return. Engineering is what happens when applied science meets economics. I do some programming for pleasure (in Haskell, even), that is to say at my own expense; and some for money, that is to say at other people's expense. I really wouldn't pay much attention to Haskell advocates if it weren't for all the ones running around telling me that programming for money in any other language is folly verging on defrauding my customers. But, on closer examination, many of these claims seem to be founded on the idea that what's good for programming which happens to be done in the service of pure Computer Science is also, obviously, and *incontrovertibly*, beyond the possibility of question, good for programming done in the service of a paying customer. I don't find this to be the case.
Yes, it breaks parametricity because parametricity requires that any two types are indistinguishable. It does not however break HoTT's equality, because HoTT only requires that any two *isomorphic* types are indistinguishable. Of course there's the problem of actually computing whether a type is inhabited or not, but what I mean is an oracle that decides this would not break HoTT (afaik) whereas it would break parametricity.
Of course, but the effort required to re-implement an Earley parser is way, way larger than the effort required to understand its public API. When you not a Haskell beginner, the latter takes *at most* an hour or two. Reimplementing it (even specialised for only your needs) takes probably at least an evening or two. Again, let me remind you that OP is worrying about grokking the implementation, not learning to use the library. Grokking the implementation is not something you should have to do other than in special circumstances.
While I didn't use any Haskell in Google, Haskell indirectly got me in there. In 2009 I was recovering from working at a hedge fund and I finally had some time to learn Haskell, which Peaker told me I have to learn because it is the language closest to how programming ought to be.. For practicing my Haskell I participated in Google Code Jam, and then I won a t-shirt and got asked if I wanna come over for an interview. Did the interviews and the problems were much less difficult than the code jam problems. They offered me a job. I didn't think that a large american corp will hire someone like me with no formal education, but they were surprisingly open-minded. However when I came to talk to different tech-leads (managers) to get assigned to a team and project, one of them was extremely rude and confrontational regarding my lack of formal education, didn't report that incident to HR though because it was kinda my first day there.. There's some really nice things about Google: * The great majority of people are very nice and much more normal than folks I worked with in other tech companies. * They care enough about code quality that everything goes through code review and they have a good internal system for that! * No hassle around IT. You don't need to install your own computer or anything. * They have some great infrastructure which they use pretty much system wide. * It's actually a plus that they have a limited set of allowed languages as it encourages reuse etc. * The "perks". While I really don't need colorful bean bags and pilates balls around, and care much more for example about which source-control system I use, it does give a feeling that your well-being is important for the company. However kamatsu's points are very much true. One thing which bothered me after a while is that it seems like everyone tries to "game the system". What system? * The annual performance review ("perf"): You are more likely to get promoted if you made a monitoring dashboard for your system which you can describe in vague details and present screenshots of, than if you solved a bunch of crucial bugs and made the important systems keep working. So the important work gets assigned to the new hires and the visible work gets taken by those who already figured out how things work. * "Readability": Each commit in programming language X has to be written by or reviewed by a programmer who has attained "X-readability". You have to go through a special code review process to achieve it, and it requires new code of significant size. This "readability" can be hard to get if your work is maintaining an existing system. You may have spent a lot of time debugging hard bugs and optimizing performance bottle-necks (which are well worth your time according to the internal measurements system), but that does not provide a fresh code chunk large enough to be reviewed for "readability". The problem is that people want to achieve readability anyways to improve their chances in the performance review, so they will change the way they write their code to achieve it. Is there an existing library that can do X for you quite well? Simply avoid using it and write a bunch of unnecessary code instead, this way you can get the readability! Your project already uses library X to achieve something but there's a small problem with it? Don't fix the library, replace its usage with your own "better" solution and get "readability"! After 6 months and going through the perf process I decided that I want to do something more meaningful with my time, and talked to Peaker on when we sync up to start working on [our project inspired by "subtext"](http://lamdu.org/). We decided to transition to part time jobs in 6 months' time to start our project. I told my managers that I'll want to switch to part time in 6 months and as the moment came near they made it clear that Google doesn't have this option for me, so I left (as I told them I will) and my managers were very surprised that I did actually decide to leave Google, with the local office manager even being somewhat confrontational about it.. Disclaimer: my impression is from one year of work five years ago.. things may have changed since then
Sure, the Earley parser library may not be an example of this situation. I think complaints about code accessibility deserve a warmer reception, though. Code is written for people to read, so if people are having trouble reading it, telling them "the way it's written is for the best" isn't very encouraging.
Well, according to the standing OCaml tradition, I didn't use them a lot. However, as much as I did, the main issue is that you basically loose the benefit of type inference - to avoid overly-long types, you need to specify types (type aliases) everywhere. Basically, polymorphic variants offer no structure. I like structure, but would prefer it a bit more fine-grained than the non-polymorphic variants (in OCaml).
`-fdefer-type-errors`?
Sure, you can do *even better*! But that type is a pretty good start compared to most other languages.
Oh, sorry. I didn't even think it was created by humans instead manifesting itself out of pure functional programming energy. :P
I think the incompetence thing is pretty universal. And the greater your need, the more you hire, the more likely it is that you're going to hire someone who isn't right.
I need to hack my Hakyll to put a nice source link ... for now it's https://github.com/slpopejoy/tatterdemalion/blob/working/posts/Effectful02.lhs 
My reading was that it was still a little inconsistent (all kinds are inhabited), but the bigger worry that we might be able to destroy termination for type inference/checking is unfounded because reasons. So it's not as bad as it would be in Agda or whatever and it doesn't introduce any unsoundness that wouldn't have been there before.
I haven't experienced the "slowness in general" you mention. For me it works as fast as Sublime (maybe my machine is just fast). Also, the added benefit of seeing all the types (specialized at a use site) is tremendous and worth every slowdown... because it's way faster than flipping to GHCi and typing `:t` or `:i` etc. anyway.
While `ghcid` is great and superfast, there's nothing better than having the information overlaid over the source code. The time you waste trying to relate stuff from `ghcid` back to the source or get info on types/typed holes is huge in comparison. For me it's always instant.
&gt; Remember Arc? Right, me neither. I do. For me, it's one of the nicest Schemes because it lets you define CL-style macros instead of the "syntax-define" stuff that's in MIT Scheme and Racket. I don't think it's the language design that's the reason it isn't anywhere today (aside from HN and Hubski), but rather the lack of libraries. What problems with the design have you spotted?
Tbh, I have not read the paper in any detail. Thanks for sharing!
IMO this kind of naming would be ridiculous in any other language. Why must the code be unreadable and require a thesaurus to decrypt (in the form of some part of the docs). No other mainstream language community accepts this as reasonable. Why?
Though if you see `&amp;` in code, it's in scope and that means you can check it's type! `a -&gt; (a -&gt; b) -&gt; b` is fairly self-explainatory.
https://www.reddit.com/r/haskell/comments/3ijtej/any_tips_for_reading_haskell_code/cuhcqcr?context=3 That's the idea, anyway.
&gt; only type theoretic systems where there is any hope of proving a theorem like this internally I'm not so sure about this. In vanilla MLTT one certainly can't say "suppose type was in type" but you can say "suppose we had a resizing axiom that let us send elements of any old universe to elements of the base universe" and then derive essentially equivalent results.
Perhaps “a theorem like this” was too broad, and I should have said “This theorem” ;-)
Honestly, reading those two without prior knowledge of the library, the one that's not trying to be brief makes infinitely more sense. (Granted, you definitely overdid it) I can actually see what everything is at each stage without having to guess. The brevity is nothing compared to this in my eyes.
From my experience and in certain contexts, its more readable to stage certain tests as you know more, instead of floating them to the top. Sorry about that!
Well I have a decent PC. It's nothing special but it's decent. It *should* be able to run a text editor with no problem. But Atom and Leksah feel sluggish and heavy. Perhaps my standards (instant text, fast or instant menus) are too high? It's a real shame because I'd really like to be using Leksah. Speaking of Leksah, have you used it? Why do you use Atom when a specialized Haskell IDE exists??
I did say that. &gt; If you don't understand the Control.Lens single-character conventions, this may initially seem more readable, but if every type signature was a thousand miles long you'd quickly lose scope of it all. By knowing what `s` and `t` and `a` and `b` means you get all the benefits of legibility with none of the drawbacks of mile-long names.
Cool, didn't know about that! Will link it in the readme
&gt; I doubt that OP is struggling with functions like that The OP didn't mention understanding functions by reading the type signatures *at all*, thus /u/Denommus is right to point it out as a helpful tip. &gt; Quickly now, without looking it up in Hoogle, tell me what a function of that type does. Interesting puzzle! I'm going to guess it modifies the `MonadState`'s `s` state by using the first argument to focus on the `a` it contains and applying the second argument as a function to it. As well as being written back to the state, the result of the function application is also returned. I admit this took me a couple of minutes of thinking about what could possibly be a reasonable answer. How did I do? EDIT: By the way, this is how I worked it out: `Over p ((,) b) s s a b` looks suspiciously like it stands for some sort of optic. Supposing it's a lens that means it represents how you can focus on an `a` inside an `s` and change it to a `b`. The `s` we have is inside our `MonadState` and we're *returning* a `b` so the function must at least use its second argument to return a `b` by looking at the `a` inside the `s`. It would be a bit silly if it didn't also write the `b` back inside the `s`, so I guessed it does that too. EDIT of EDIT: I dedicate this post to /u/gelisam :)
His (at the time quite new) Go programming language.
Why not looking it up in Hoogle? If you have tools to help you understand something, why avoiding to use these tools? *That's* babyish.
Not sure if you're claming that statement about types is actually true. NB for anyone else interested: it's not!
&gt; Haskell advocates are fond of propaganda along these lines: keithb is fond of strawmanning along those lines. To be clear, Haskell advocates don't claim you can tell *exactly* what a function does from it's type (in *any* case) but we do claim that types are *excellent* documentation and that learning to read and understand them can massively help you understand a Haskell codebase. 
That you can work out what a function does just from it's type? I'm not claiming it—I know it's not true. I see it claimed often, and used as an example of why strong static typing is *GREAT!* 
What does dgoiH stand for?
Well, my original suggestion explicitly mentioned looking for stuff in Hoogle. Types are also useful for the tooling they generate. And by no means I wanted to imply you can know what a function does through the types. I said you can know what a parameter of the function is by looking at the types (instead of what people do in dynamically typed languages, which is look at the names).
I don't understand half of this. What is the `#` sign after a type? What's DPH?
I think you missed the *should only* part. I agree that there are cases where it's not enough. However, there are lots of cases where it is not only enough, but the correct choice as there really is nothing more to cover and a name in place of a math style variable is possibly even misleading. *That* was my point.
This already works today. I'm working up a rather thin wrapper around the guts of this inside of the `structs` package. Basically with it you can make up objects, talk about the types of slots/fields, where slots are references directly to other mutable objects and fields hold regular haskell values, and you can check the references in slots for equality with a defined `Nil` pointer. 
Awesome! It seems like this could be pretty useful to speed up the clause store in my little boolean constraint solver project.
&gt; unbo**x**ed fashion. IIRC it also requires the [`MagicHash`](https://downloads.haskell.org/~ghc/7.10.2/docs/html/users_guide/syntax-extns.html#magic-hash) extension.
"dedicated group of internal Haskellers". Taken from the parent post.
Between js minification and compression, and the fact that [some](http://lisperator.net/uglifyjs/) minifiers already do analysis of the js and discard some dead code, I think it might be a very small win.
GHC has an extension named `MagicHash`. All it does is allow you to use # as part of the name of an identifier. On its own, it signifies nothing. It is just a part of an identifier that you aren't allowed to use without turning on that extension. We get all excited about the fact that in Haskell all of our `Int`s and the like are actually closures that can compute a real answer when you ask them to, but eventually they need to store that answer in some form! Internally inside of GHC, there are a number of primitive operations that return back actual machine integers as a value of type `Int#`. `Int#` has kind `#`, which is different than the usual kind `*` you are used to. Things that live in `#` are different. You can't work polymorphically over `#`. Each one of them could take up a different amount of space in memory. An `Int#` is a real actual honest to god machine integer, not a data constructor. Then we work with something like data Int = I# Int# which says that the Int that you and I know and love is a wrapper around this machine integer. That wrapper is where all the thunk-evaluation craziness happens. Those wrappers are homogeneous enough that we can write code that is parametric in them. Now, GHC supplies a number of primitives for us that are used to make things go, that live in #. e.g. data STRef s a = STRef (MutVar# s a) what the heck is a MutVar#? It is a pointer to an object that lives on the heap, whose entire role is to hold onto a mutable pointer to whatever you change the reference to. MutVar# s a :: # means you can't quantify over them, they are always strict, unlifted, they can never be a thunk themselves. They may contain a reference to a thunk, however. And internally, IO and ST look like newtype IO a = IO (State# RealWorld -&gt; (# State# RealWorld, a #)) newtype ST s a = ST (State# s -&gt; (# State# s, a #)) `(# , #)` here is an 'unboxed tuple'. so when ghc returns one of these from a function it is actually returning two entries in the stack, rather than making up a `(,)` cell on the heap and throwing the two entries in it. (# x , y #) :: # means we can't quantify over these unboxed tuples however. (You need `UnboxedTuples` turned on to work with these and they make GHCi sad.) And `State# s` is a representation to the compiler that we're threading state through the program. This sort of token passing is why the compiler doesn't reorder all your `IO` and `ST` operations just like it can everything else. they are all still strung along the chain of `State# s`'s that you pass between them. State# :: * -&gt; # RealWorld :: * `RealWorld` is just an argument we pass to `State#` for its phantom argument when dealing with `IO`. `State# s a` takes 0 bytes to store, it doesn't have an actual representation. It serves purely as a typing trick to make the compiler thread our computations in order. When we write a function in IO, we're "passing along the real world as state" as a metaphor. It is a pretty leaky metaphor, but its the one we've got. The goal of the compiler is to do as many things as it can with these unboxed representations as possible as they are way more efficient than the stuff available to us in the surface language of Haskell. To this end it uses strictness analysis to try to figure out when you build such an `Int` wrapper just to remove it moments later, and avoids the intermediate step whenever it can. So, * `#` after a type doesn't mean anything on its own, except for the fact that things with that decoration tend to live in kind #. * You aren't allowed to quantify over types of kind #. Finally, DPH itself is just "Data Parallel Haskell". It is an ongoing research project into how to make Haskell do nested data parallelism efficiently. It was the thing that spurred the development of type families and data families. So pretty much every innovation that came with GHC 7 was an offshoot of that research. The project itself seems (from my outsider perspective) to have largely fizzled, but it spurred a ton of innovation inside of GHC.
That's... not finite. You can take the inverse of `mkList` easily. Define: enumerateFrom :: Int -&gt; FiniteList a -&gt; [a] enumerateFrom i l = let list = unList l $ i + 1 in if length list &gt; i then (list!!i):enumerateFrom (i+1) l else [] unmkList = enumerateFrom 0 There's probably a better way, though...
Dead code elimination in JS is much harder than in Haskell.
Will do as soon as I get a Windows machine to play with. Frankly I don't have a lot of experience with Windows, I'm more a Unix/Linux guy.
I'm probably stupid but what does `* :: *` mean ?
Then why is it on the AppStore with a price tag on it
Really good explanation. What was confusing me is the fact that `::` is symmetrical, therefore I expect the `::` relation to be also symetrical (`a::b &lt;=&gt; b :: a`) .I always thought `::` as a sort of *equality* whereas `::` is obviously more like `∈`.
Repost from the future: https://www.reddit.com/r/programming/comments/3il3me/will_it_memoize/
"Dedicated group of internal Haskellers".
To someone who has interest in both Go and Haskell, this is funny. 
Haste does exactly this. It's quite easy to do and makes a significant difference in code size (even with minification), so there's really no reason not to do it. Of course, for things that are just sitting around on your near-infinite hard drive as opposed to being sent across the Internet every time someone cleared their browser cache, it's probably not worth doing.
I strongly agree! I have undoubtedly spent a lot of time updating "dead" functions and getting them to compile after refactoring. I have used hpc to try and get an overview of this, but it it's very inconvenient.
Since Haskell's already inconsistent as a logic, why bother adding a predicative hierarchy with all the attendant complications? (Well, I can think of a bunch of reasons—but I'm the one asking...)
If we're careful about categories then there are several options. Indeed, in a category of domains, such as ω-cpos, we will have a coincidence of the initial algebra and final coalgebra, so `data T = ...` could be seen to define either initial algebras or final coalgebras. If I were to try to explain more carefully what I am getting at, it's probably that there is a difference between how ML and Haskell treat data. In ML a recursive datatype declaration is properly inductive. For instance, the *values* of type ``` type nat = Z | S of nat ``` actually do correspond to natural numbers. In Haskell ``` data Nat = Z | S Nat ``` contains things like `⊥` and `S (S (S ⊥))` and so it is highly misleading to call `Nat` the datatype of *natural numbers*. And there just isn't any way to define a type whose values correspond *precisely* to natural numbers, because the (discrete) natural numbers do not form a CPO. (In ML the solution is to view data as coming from *predomains*.)
You can check out the repository right now and play around with it. It works.
yeah, traverse is more general than "IO and mutation". I just meant that in Python, you don't have to traverse a print statement or a increments nf routine over a list, you just map it over. because you ignore all side effects (and types).
Yes. I dislike the naming intensely.
The best I can offer is an anecdote. When I found Haskell I thought "man, if monads are this cool, and category theory says that comonads are just their dual, then comonads must also be awesome!" and then I proceeded to spend the better part of a year trying to understand comonads better. Was it worth a year? Maybe not in terms of direct payout. I admittedly learned a lot about them though! Along the way this informed a lot of little bits of things I designed later. With lenses there is a 'comonad coalgebra' in one of the nice formulations. When we start talking about what we can do with a lens or a traversal then knowing that there is this other connection is nice. Along the way I was able to spot other connections like Moore machines in the finite state machine world can be seen as a form of comonad, which lets me talk about resumable folds, which gives a nice design for implementing HMAC like operations without having to name all the parts, and the environment comonad wound up being a nice trick to avoid monad transformers back when Scala was terrible at them... In other words, I can't point to any one thing where I could say "learning comonads saved me X days of work" but I can point to a lot of little things where having learned my way around comonads made me a better programmer and better able to spot how to design an API that feels "right". At a high enough level, Monoids, Applicatives, Monads and Comonads are all instances of the same pattern -- they are all monoid objects in a monoidal category. So if you pressed on with the category theory mumbo jumbo at some point you can reach that realization and be able to move results between them. But I admit that not everyone is as motivated by "abstract nonsense" as I am, so YMMV. For the most part the category theory is a wonderful tool for figuring out how to organize code in a maximally reusable manner. This is an awesome tool for library designers, but admittedly that risks the counter that one might make a case that as an application writer you don't need to worry about these things! I'd counter that particular straw man by pointing out that I write applications by building up a library for solving some entire domain, and then scribbling in a `main` function on top. The best recommendation I can offer you is to find something you find interesting and dig in deep about it. Learn everything you can about it, and *ask questions*. Talk to other people and get a sense of their understanding. Having a mentor or a teacher to help guide your exploration is an amazing tool. Once you have made some headway, you can feel free to walk away from whatever random Haskell/math subtopic you obsessed about when it becomes boring, but just make sure you are doing so because it became hard.
Well this is something I don't particularly enjoy doing as it breaks the workflow. I talked about this in the post. Also for the module to be loaded into GHCI it must be void of errors which might not be the case at the time.
&gt; At a high enough level, Monoids, Applicatives, Monads and Comonads are all instances of the same pattern -- they are all monoid objects in a monoidal category. I think this confirms that that "portion" of category theory that is most obviously applicable to programming is really just trying to unpack the different (sub-)aspects of being a cartesian closed category, and in particular what monoidal functors are, what closed functors are, etc. Along with that I think that adjoints are of course a key concept that plays out in many ways, and then when one puts it all together, how such concepts interact with the yoneda embedding -- and the properties of presheafs in general. One thing that happens when one starts studying category theory in general is that there are many important interesting avenues that don't really payout in an obvious way to programming. But all the above stuff does. In general I think that bartosz' guide also does a good job of picking out "the more relevant bits": http://bartoszmilewski.com/2014/10/28/category-theory-for-programmers-the-preface/ And I'd add to that that I have a newfound appreciation for Bird and de Moor's "The Algebra of Programming" whose introductory section is a whirlwind tour though a slightly different choice of categorical concepts.
Thanks!
For someone who's just getting comfortable at Haskell, this is pretty readable. Thanks for this explanation.
Actually, I would say to study Lambda Calculus. Seriously, Haskell is just Lambda Calculus + Syntactic Sugar + Type Classes. (I think it is System F version of Lambda Calculus.) Category Theory applies to Haskell through Lambda Calculus.
That "in addition to" is pretty rich.
It will be a journey for me to understand the full content of the blog post, but many thanks to /u/edwardkmett for providing so much context.
You don't really need much knowledge of category theory to program in Haskell. As long as you can understand: id :: cat a a (.) :: cat b c -&gt; cat a b -&gt; cat a c f . id == id . f == f (f . g) . h == f . (g . h) Categories are just things you compose, and then you get: (&lt;=&lt;) :: (Monad m) =&gt; (b -&gt; m c) -&gt; (a -&gt; m b) -&gt; (a -&gt; m c) -- id = return (=&lt;=) :: (Comonad w) =&gt; (w b -&gt; c) -&gt; (w a -&gt; b) -&gt; (w a -&gt; c) -- id = extract (&lt;.&gt;) :: (Applicative f) =&gt; f (b -&gt; c) -&gt; f (a -&gt; b) -&gt; f (a -&gt; c) -- id = pure id The Applicative operator isn't defined anywhere, but is just `liftA2 (.)`. Arrows aren't really used a lot except in a few libraries, and I've gotten comfortable with lenses with no formal category theory past what I've picked up on reddit/#haskell. My most productive work has just been writing code to solve whatever it is I'm working on, and gradually formalizing things when my intuition tells me there's something interesting worth exploring.
A lot of people have a misconception that the "foundation" math of Haskell is category theory. This is basically false (although you can make categories work in that role if you squint at them right). If anything, it's lambda calculi and type theory that form the foundation of Haskell.
You left me hanging there at the end by not providing some sort of profiling/benchmark comparison for the two. Given that the point of Quick Sort is to be *quick*, a comparison of runtime performance is pretty essential here.
The blog post started as an email to SPJ. I quickly reskinned it, formatted it and added a Maru joke and turned it into a blog post in the course of about a half hour. In the process I realized that I didn't give it enough of a preamble, hence the comment above. =)
Same here.
While a fun exercise, in general, you shouldn't respond to Jon Harrop's trolling. 
&gt; Dynamically switched events are now garbage collected! Woop woop! 
The key is strictness. State# s lives in #. You can't create a thunk for anything in #. It doesn't have an extra bottom. The only way to produce such a value is to call a function that produces it strictly, but if that function takes another argument of that kind then you have to call the function to get _that_ function's argument strictly, repeat ad nauseum. 
Thanks. This is something I've been thinking about how it's accomplished. I'm glad to hear it's that simple. =)
No other mainstream language supports anywhere near the abstraction level of Haskell. It is often difficult to give useful names to very abstract variables and parameters.
I think you have never tried to write any abstract code and faced the challenge of coming up with descriptive names for things that simply don't have much to say about them. In very specialized code you name a list e.g. "employees", in something like map you might still name it things like "list" and "elements" (which are not really any more useful than the single letter variants established by convention already) and in something like fmap you are struggling to say much at all with your descriptive names.
(Typo: "In Haskell, we have to popular parser combinator libraries:" should be "In Haskell, we have _two_ popular parser combinator libraries:") 
&gt; It just makes it hard to read On a line by line basis that might be true if you are used to e.g. Java. On the other hand spreading out what should be one or two lines over hundreds like Java or C++ or similar OO languages often do does not help readability either.
Yes, I that's a good way of putting it: Haskell has the wrong idea of *data*.
Why not? Although the way he phrases his criticism may not be agreeable, at least on the subject of performance he certainly makes good points, and several issues that he pointed out have been fixed in the past. Responding in a positive way like this blog post seems like exactly the right thing to do.
`type FiniteList a = (Int, [a])` is too restricted a notion. The elements of the inductive list type can be of *any finite* length. `(Int, [a])` says that they have to be of one specific length. For that, you could use a tuple too. `type FiniteList a = (Int, [a])` is too weak, but the intended invariant can be enforced. `data FiniteList a = List { unList :: Int -&gt; [a] }` would be the correct idea, but as /u/tailcalled showed, it's invariant can be circumvented. This discussion around the impossibility of specifying inductive types in Haskell reminds me of the theorem that a FOL theory cannot force its models to be finite. The two results might even be related.
Why did you write &gt; infList = FiniteCons 1 infList instead of &gt; infList = infList ?
Only if you use `seq` in your `mkList` (which is evil); otherwise, you can still pass in an infinite list, and only the first component in the tuple will be bottom.
Great work! It's a bit sad that the concurrency primitives in Haskell are not already an implementation of a standard concurrency type class. In Java for example, no code change would be required to use a library like this, because the community chooses to use interfaces.
I don't see why that matters to anyone who isn't dyslexic, and therefore would confuse a d for a p or something.
I responded to his troll about IntMap being slow and having no fast alternative as fast as f# map. He lied about the factors (2.5x when he said 10x) but the grain of truth was there. So I ended up writing a ghc patch that made my hash table bindings beat his f# ones :) Also killed another bird with that stone (that you needed a PhD to hack on ghc)
&gt;uuagc and something newer I don't remember Ruler?
Just to illustrate the point that the list will have infinite elements.
It won't have any elements, because it's ⊥.
I like the contrast between your answer and every other answer on this thread :) Since you already implemented it though, I'm inclined to take your word for it over that of the nay sayers..
Please include more type signatures for the functions you are introducing.
Exactly. If Haskell's performance was in the same bracket as Ruby or Perl, I'd be uninterested in criticism about Haskell's inability to beat F# or Ocaml in a benchmark. But Haskell's performance is known to be competitive with compiled imperative languages that have garbage collection. And in that bracket, we need to be honest about why our programs have the runtime characteristics that they have. Are our programs less efficient because of functional purity? Cool, lets be honest about that. I have no problem giving up some performance for referential transparency and better parallelism/concurrency support. But I do have a problem with garbage collection and laziness making my code harder to reason about when it comes to performance as that grinds against one of my main reasons for liking this languages so much - I can accurately reason about my code without having to run it in a debugger. Haskell can get me pretty far before I have to pull out the debugger, but I'd never leave the house without a profiler. I don't expect that to change either. I realize accurately reasoning about runtime performance is a trade-off Haskell makes in order to get the wins we love it for in other areas, but that means we need to be mature when we're talking about those weaknesses, because that's the handicap we're accepting as software engineers when we use Haskell for a project.
I'm very excited about this: &gt; Support for Overloaded Record Fields, allowing multiple uses of the same field name and a form of type-directed name resolution. I thought it was going to be part of 7.10, but I don't mind waiting until 7.12.
&gt; Support for Overloaded Record Fields, allowing multiple uses of the same field name and a form of type-directed name resolution. Hhhhhhnnnnggghhhhh yessssssss
The monad will be the same as the `Moment` monad that is already present in the `Reactive.Banana.Switch` module. It's essentially just a reader monad, `Moment a = Time -&gt; a`, and will be commutative. But yeah, I agree that `accumB` with a monadic type is a big price to pay. On the other hand, dynamic event switching will become easier. I think that the tradeoff is probably worth it, many people have expressed to me that the `AnyMoment` stuff is very complicated. I intend to explain the reasoning in more detail in an upcoming blog post.
&gt; A huge improvement to pattern matching (including much better coverage of GADTs) Will I finally be able to write haddock for GADT constructors?
I would rather say theorems are implicit in category theory. Namely, the hard part is creating the concept, and then theorems sort of just pop out of them. For example, whenever you show that something satisifies a universal construction, you are proving a theorem.
State# s has no runtime representation at all. !x still can 'make a thunk for x then immediate demand it. `seq` imparts no ordering, it merely declares the left hand side strict. The compiler is free to optimize x `seq` y `seq` z into y `seq` x `seq` z That'd be bad if y and z were computations that you wanted to have do things to the world.
can someone explain?
Java actually makes this sort of thing really easy because you can arbitrarily modify the bytecode at runtime: so even if you *do* have some code not using an interface, the testing framework can make it work. Which is really awesome for testing legacy code.
The word that caught my eye was not *quick*, it was the titular use of *optimal*. Without proof, one is reminded of the best line in the Simpsons movie, "So far!"
I thought we could already do that with DisambiguateRecordFields? whereas OverloadedRecordFields takes it a step further and would let us do data A = A { x :: Int } data B = B { x :: String } processAs :: [A] -&gt; [Int] processAs = fmap x processBs :: [B] -&gt; [String] processBs = fmap x Although the scope has changed a few times, so I might be a little behind the times.
There is a really high cost to the current interface that imo, the change justifies making `accum` functions monadic. Problems with the current approach: * Increased API size to work around the fact that `-XImpredicativeTypes` is broken (`FrameworksMoment`, `AnyMoment` types). * Higher-rank polymorphism is surprising - there are times when things like `execute (fmap (FrameworksMoment . foo) ev)` should work, but because of the parametricity the types don't infer. Then I have to write `execute (fmap (\x -&gt; FrameworksMoment (foo x)) ev)`. * I can't get `GeneralizedNewtypeDeriving` to work with `Moment t`. Maybe this is just my failing, but for some reason newtype deriving fails. This is a pain when I want to abstract over `reactive-banana` (as a concrete example, I am building a HTML generation library for GHCJS that is backed by `reactive-banana`). * Dynamic switching is extremely verbose. It requires a *lot* of plumbing just to do basic tasks. If you don't need dynamic switching then I can understand that this isn't such a compelling argument, but for those of us who do need it, `reactive-banana` is practically unusable.
Congratulations on the release, Heinrich! I'm going to celebrate with a smoothie too, but I'll be using the correct fruit.
DisambiguateRecordFields requires the data types be declared in separate modules.
You're absolutely right. I'm afraid I was a little pressed for time, so I didn't perform any benchmarks - perhaps in a future post!
Great! I checked this just the other day and they were still shipping 7.6.3, which was a big turn-off from using openSUSE. Good to know that someone is maintaining a GHC distribution in the repository.
All right, fine. But that still doesn't give you a finite list.
`processBs` should be `[B] -&gt; [String]`
Active patterns are pattern matching expressions with logic. For instance: let (|Even|Odd|) input = if input % 2 = 0 then Even else Odd // If an active pattern with |_| returns a None, it's not matched let (|IntLiteral|_|) str = match Int32.TryParse str with | (true, value) -&gt; Some value | _ -&gt; None match n with | Even -&gt; printfn "%i is even" n | Odd -&gt; printfn "%i" is odd" n match str with | IntLiteral n -&gt; "%i is a valid int literal" n | _ -&gt; "%s is an invalid int literal" str 
You can find some basic examples here: https://ghc.haskell.org/trac/ghc/wiki/DWARF 
If you have 10 abstract things in scope and there is no way to differentiate them, then there is definitively a problem, yes.
...somebody with time on their hands to implement it :-)
True...but I doubt that code would be very readable even with long variable names.
I have a much faster `IntMap` in progress in my transients project.
This also introduces row polymorphism: foo :: r {x :: Int} -&gt; Double
I'm excited to see the garbage collection work described by /u/simonmar in ["Optimising Garbage Collection Overhead in Sigma"](https://simonmar.github.io/posts/2015-07-28-optimising-garbage-collection-overhead-in-sigma.html) . [This landed](https://www.reddit.com/r/haskell/comments/3evhy8/optimising_garbage_collection_overhead_in_sigma/ctmlleh?context=3) in 7.12 . Worth a mention in the release highlights? Relevant: [D524](https://phabricator.haskell.org/D524) and [D1076](https://phabricator.haskell.org/D1076). 
Yes, fixed.
I hope /u/augustss is able to get the type sections in - they're really handy! I've been spoilt by them. 
&gt; If you don't need dynamic switching then I can understand that this isn't such a compelling argument, but for those of us who do need it, reactive-banana is practically unusable. I admit that I don't have much experience with `switch*`, `execute`, and `*Moment*`, partly because that part of the API looks a lot more complicated than the basic combinators. I wonder if it's possible to implement one API in terms of the other? This way, we could each use our favorite API.
I'm flattered!
I dunno the first half of his post had some good points. but I'm still happy to chill for a minute after touching a "root module" while my code recompiles and ghc finds all my bugs.
The mailing list discussion is [here](https://mail.haskell.org/pipermail/ghc-devs/2015-January/008049.html)
is that (%=)? (edit: NOPE) all I know is that MonadState lets me use it any monad transformer stack. and function arrows are an instance of profunctors. Over some Profunctor means it's lens, in which case I'm sure it's documented. which just proves your point that types can't always be trusted. even: head :: [a] -&gt; a is a filthy liar. but the docs are explicit about it's being a partial function. 
Its not always trivial. From the point of view of category theory its trivial, but the results in a field become somewhat larger.
Awesome! I look forward to it.
run it in the same Emacs process as you edit your code and use compilation-shell-minor-mode, for hyperlinks! ... if you use Emacs in a GUI, that is.
that "same level of abstraction post" is great. even easier in Haskell with functions and laziness and type inference. you can just hoist out any subexpression and give it a name! often with one new line.
the Purescript parser is RealWorld and "separates levels of abstraction", I think.
&gt; is hardly readable and totally unskimmable I find that much more readable, and certainly much more of a guide to what's going on. "skimmable" doesn't seem like a meritorious quality for code to have. I don't understand your complaint about partial application. 
&gt; Support for Type Signature Sections, allowing you to write (:: ty) as a shorthand for (\x -&gt; x :: ty). This sounds awesome, but I don't understand. Can anyone elaborate?
I think that's a good point: the distinction between the possibility of code reuse use and its facilitation
&gt; Backpack is chugging along; we have a new user-facing syntax which allows multiple modules to be defined a single file, and are hoping to release at least the ability to publish multiple "units" in a single Cabal file. Is that still happening? Haven't heard much on this cabal issue since July: https://github.com/haskell/cabal/issues/2716
/u/augustss describes the uses here: http://augustss.blogspot.com/2014/04/a-small-haskell-extension.html
Is this right? Doesn't that mean adding a new kind for rows?
I remain skeptical of the possibility of making a library more complex without making it more difficult for me to learn. I do think that the right abstractions can simultaneously make a library simpler and more general. Pipes is an excellent example of this. That's not the kind of library I had in mind when I made that comment, though.
[IT'S HAPPENING!](http://i.imgur.com/7drHiqr.gif)
FWIW, I'm missing `non` and `view[s]`, and I'd be in favour of merging microlens and microlens-ghc (or perhaps just re-exporting Lens.Micro from Lens.Micro.GHC to save me the extra import line). Would be very happy to have a functional lens library without so many dependencies.
That's kind of what FRP does. You could try looking at reactive-banana. 
at first I though "is that in lens"?
My first instinct would be to build something on top of channels. [Here's](https://github.com/eightyeight/haskell-simple-concurrency/blob/master/src/tutorial.md#channels) a super-quick example of channels, [here's](https://hackage.haskell.org/package/unagi-chan) a super-fast channel implementation with an API that's drop-in compatible with the channels in `base` (AFAIR), and [here's](http://chimera.labs.oreilly.com/books/1230000000929/ch07.html#sec_conc-logger) a super-in-depth tutorial that re-implements `base`'s `Chan` API (using `MVar`s) for educational purposes.
I'm not familiar with extensible variants, but I think datatypes a la carte is used a lot: https://m.reddit.com/r/haskell/comments/1s3oba/24_days_of_hackage_extensibleeffects/ http://dlaing.org/cofun/posts/free_and_cofree.html
"Optim_ised_", not "optimal".
Given that most other languages' primitive is an array rather than a linked list, maybe we should be implementing quicksort on Vector. Is there an efficient in-place quicksort for linked lists in imperative languages?
[I'm so happy.](http://i.imgur.com/9LO0dr3.gif)
Oh? That is handy. Hadn't seen that use of it.
[I can give a sample in Haskell.](http://pastebin.com/cMCkzRKn) One issue is I expect the callback to be called twice, but it doesn't seem to be called twice. Not sure why. [Also, this is what it looks like in Node.](http://pastebin.com/pHH0Thgg)
good links thank you!
That's very interesting! So it could be read as "Type check as this." Thank you!
I remember seeing a slide deck about how to do a *real* (for some version of real) Observer Pattern in Haskell, [A Function Approach to the Observer Pattern (PDF link)](http://wiki.comlab.ox.ac.uk/algprog/FrontPage?action=AttachFile&amp;do=get&amp;target=alvaro_garcia_20090529.pdf). What's cool about this presentation, is that it boils down the *meaning* of the observer pattern into analogous pattern in Haskell. I've played with it a bit, but I haven't ran across a situation where I've been able to use it for real. The presentation runs through several versions, but eventually settles on this API: type SubjectT a m b = StateT (a,[Observer a m]) m b newtype Observer a m = O{getO :: a -&gt; SubjectT a m ()} setSubject :: Monad m =&gt; a -&gt; SubjectT a m () getSubject :: Monad m =&gt; SubjectT a m a addObserver :: Monad m =&gt; Observer a m -&gt; SubjectT a m () notify :: Monad m =&gt; SubjectT a m () notify = StateT (\(a,os) -&gt; case os of [] -&gt; return ((),(a,os)) _ -&gt; notifyAux os (a,os)) notifyAux [] (a, os ) = return ((), (a, os )) notifyAux (o' : os') (a, os) = do init &lt;- notifyAux os′ (a,os) runStateT ((getO o′) a) $ snd init (sorry for any formatting glitches, the PDF didn't copy well)
`view` is in microlens-mtl, and I don't really know what to do about it – if it's in microlens, it's going to be unusable in `MonadReader`, and I very much don't want to lose that. `views l f` is just `view (l . to f)` or `f . view l` or `f &lt;$&gt; view l`, and I almost haven't seen it used, so *by default* I assumed there wasn't any point in including it (and `uses`). I can totally be convinced otherwise if you show some good examples or perhaps “look, people use `views` often, see here, here, and here”. I opened an issue for `non`, and I'll release a new version with it in a couple of days (I won't have internet tomorrow). By the way, feel free to open issues for everything you think is missing! (Pull requests are welcome too, because I don't really enjoy writing documentation. If you spot a typo, however, it's probably better to just tell me about it – fixing a typo and making a push is easier for me than merging a pull request.) &gt; re-exporting Lens.Micro from Lens.Micro.GHC to save me the extra import line Good idea, and consistent with what `Lens.Micro.Platform` does. Opened an issue for this too. 
[Self plug](http://hackage.haskell.org/package/fltkhs). And here are the [installation instructions](http://hackage.haskell.org/package/fltkhs-0.1.0.2/docs/Graphics-UI-FLTK-LowLevel-FLTKHS.html#g:3) and how to [get started](http://hackage.haskell.org/package/fltkhs-0.1.0.2/docs/Graphics-UI-FLTK-LowLevel-FLTKHS.html#g:6). Happy to address any issues.
But to be fair, parser :: ListLike input terminal =&gt; (forall nonTerminal. Grammar nonTerminal (Prod nonTerminal name terminal returnType)) -&gt; ST stateThread (input -&gt; ST stateThread (Result stateThread name input returnType)) ... is somewhat easier.
It looks like main function just spawns two threads and then fills the `MVar`. You're probably seeing the main process exit before the second thread has a chance to execute. If chucking a `getLine` at the end of the main function fixes it, you'll probably want to look at the `async` package for awaiting multiple concurrent computations. Keep in mind that `MVar` is a concurrency primitive, and if you only work with them, you'll have to do a lot of the heavy lifting yourself. Simon Marlow goes into more detail in his excellent book *Parallel and Concurrent Programming in Haskell*. It's available for free online.
I was able to get fltkhs installed, and run the demos without any issues or much fuss (linux)
Seems like using the mathematical variant [U+1D77A](http://www.fileformat.info/info/unicode/char/1d77a/index.htm) or [U+019B](http://www.fileformat.info/info/unicode/char/019b/index.htm) would be a decent alternative.
Yay! Thanks for taking to time to give feedback.
Huh, that's the same thing as F#'s and OCaml's pipe-forward! Cool, TIL.
My trick is it really helps if you know how to vocalise everything. For example, for `instance Monoid v =&gt; Monoid (Map k v) where ...` I vocalise it as 'Given `v` is an instance of `Monoid`, then `Map` of `k` and `v` is an instance of `Monoid` where ...'. For `fmap :: (a -&gt; b) -&gt; f a -&gt; f b`, I'd say '`fmap` has type, `a` to `b`, to `f` of `a`, to `f` of `b`'. And so on.
Basic documentation is here: https://blog.heroku.com/archives/2015/5/5/introducing_heroku_docker_release_build_deploy_heroku_apps_with_docker For haskell, just make sure that your application is built somewhere under `/app`. Your Dockerfile might look something like this: FROM heroku/cedar:14 # Stuff taken form the Dockerfile for the `haskell:7.10` base image. ######### ## ensure locale is set during build ENV LANG C.UTF-8 RUN echo 'deb http://ppa.launchpad.net/hvr/ghc/ubuntu trusty main' &gt; /etc/apt/sources.list.d/ghc.list &amp;&amp; \ apt-key adv --keyserver keyserver.ubuntu.com --recv-keys F6F88286 &amp;&amp; \ apt-get update &amp;&amp; \ apt-get install -y --no-install-recommends cabal-install-1.22 ghc-7.10.2 happy-1.19.5 alex-3.1.4 \ zlib1g-dev libtinfo-dev libsqlite3-0 libsqlite3-dev ca-certificates g++ &amp;&amp; \ rm -rf /var/lib/apt/lists/* ENV PATH /root/.cabal/bin:/opt/cabal/1.22/bin:/opt/ghc/7.10.2/bin:/opt/happy/1.19.5/bin:/opt/alex/3.1.4/bin:$PATH # Stuff from the `heroku docker:init` template: ############################## RUN useradd -d /app -m app USER app WORKDIR /app ENV HOME /app ENV PORT 3000 RUN mkdir -p /app/heroku RUN mkdir -p /app/src RUN mkdir -p /app/.profile.d # Build and install WORKDIR /app/src COPY LICENSE LICENSE COPY cabal.config cabal.config COPY project.cabal project.cabal RUN cabal install --only-dependencies COPY src src RUN cabal install; cp /app/.cabal/bin/project /app/project
I use fltk. I wrote the GUI part in C++, and expose a high level API in C, which I bind to via haskell. I think it's a good division, it keeps the icky GUI stuff isolated behind a nice API.
Awesome!!!! This is plenty to work off of. I really appreciate it. For anyone following along, further `docker-heroku` documentation [here](https://devcenter.heroku.com/articles/docker)
Abstract question: what happens when a new compiler version, with new idioms, becomes commonplace? More precisely, what happens to the previous idioms?
The reason the callback is not necessarily called twice is because your "event" isn't necessarily handled synchronously like it must be in JS. Immediately after writing to the MVar, the `main` function completes and the application exits, whatever state your other threads happen to be in.
Those are still classified as letters, and I guess the latter is used in some language
It looks like they are reusing `Symbol` and the type level strings. &gt; For example, given &gt; data T = MkT { x :: Int } &gt; GHC will generate &gt; instance HasField "x" T where &gt; type FieldType "x" T = Int &gt; getField _ (MkT x) = x &gt; instance FieldUpdate "x" T Int where &gt; type UpdatedRecordType "x" T Int = T &gt; setField _ (MkT _) x = MkT x [(link)](https://ghc.haskell.org/trac/ghc/wiki/Records/OverloadedRecordFields/Redesign#Part3:Polymorphismoverrecordfields)
Are you familiar with [combinator libraries](https://www.youtube.com/watch?v=85NwzB156Rg&amp;feature=youtu.be)? They allow you to describe larger and larger systems by combining smaller systems together using a handful of combinators, starting from primitive systems which cannot be further decomposed. Since there is only a handful of primitives and combinators, such libraries might *look* limited at a first glance, but there is an infinite number of ways to arrange those combinators into varied and arbitrarily complex systems. An FRP library is a combinator library in which each system produces a bunch of events. The way to create a bigger system is not by handling those events, but by combining systems together: for example, a primitive system producing an event each time there is a mouse click over a button can be combined with a system producing an event each time the button toggles between greyed out and enabled, in order to produce a system which produces an event each time the button is pressed. To hook a GUI library such as GTK with an FRP library, you'd probably define a primitive system which produces its events by imperatively reacting to the GTK callbacks. You'd then build your application logic by filtering and combining this primitive system in an application-specific way, until you end up with a large system whose events describe commands to be sent back to the GTK library, such as "disable this menu item" or "switch to this tab" or "create a new button". You'd then imperatively react to those events by forwarding the commands to GTK. So in order to interact with your imperative library, the innermost and outermost layers of your program would still be imperative, but the application logic in the middle would be described by a large, pure FRP event network.
The `.` operator is actually not a special syntactic element, so you can totally use `∘` instead. If you install the `base-unicode-symbols` package you get a predefined version of it.
itshappening.gif
An ephemeral hash table is likely to be faster than any tree-based structure though, when you don't need the persistence.
It is, but, when you do have to have persistence, or have to sprint from persistent use to persistent use with brief flurries of mutation, having the map is nice. The one in `transients` is an array-mapped trie with an active finger, so you pay based on distance, so the transient bits make it nice for doing things like `fromList` mutably, then freezing behind you.
I realised, and for some reason I thought a couple of the ones in the list linked were the same - but going back I see they're not.
Well done well done well done. Thank you thank you thank you. 
If I remember correctly, hackage takes a couple of days to build the package and documentation.
Great work, I'm so glad to see this. I think this will eliminate the vast majority of "cabal hell" situations because users will no longer need to worry about how previously installed packages affect whether new packages can be installed. 
&gt; man, if monads are this cool, and category theory says that **comonads are just their dual**, then comonads must also be awesome! Well, there must be some transform which allows us to get something awesome from a comonad, at least.
Am I the only one who actually prefers `\` to `λ` for anonymous functions? I think λ looks too much like a letter (because it is).
And 3.5 supports something similar in stable cpython. Same author even 
There are several sorting algorithms already implemented in `vector-algorithms` package, including [introsort](https://hackage.haskell.org/package/vector-algorithms-0.7.0.1/docs/Data-Vector-Algorithms-Intro.html) (beefed up version of quicksort).
Actually I am, but thanks for your remarks. My observation of *limited functionality* was from a *type class* I saw in a documentation, where certain elements (labels, edits, buttons, ...) were created by that *type class*'s functions. I can't recall which library it was though.
While I really like stack from what little I've seen of it, I'd like to know as well.
Sadly, I'm not a huge fan of web technologies. ;)
I can build a monad from any comonad. newtype Co w a = Co { runCo :: forall r. w (a -&gt; r) -&gt; r } instance Comonad w =&gt; Monad (Co w) Then Co of costate gives state, Co of coreader gives reader, Co of cowriter gives writer. With some modification this gives a monad transformer from every comonad as well. Sadly you can't go in the other direction and get a comonad from every monad.
I don't know. What do you mean by "constructive definition"? But since we're discussing this: A general solution would be to ditch the antiquated habit of using Greek for everything. Why use a Lambda for computations at all? It certainly doesn't suggest it. Why not something more immediately descriptive? I would thus like to take this opportunity to propose the Unicode truck symbol (🚚) instead, which expresses how inputs are transported into the little function-factories: \x -&gt; f x 🚚 x → 🏭 x 
No, but it's just yet another stuff to learn. I have far much more stuff that I want to learn than my time allows me to do. stack even though is the list of stuff I might HAVE to learn is not in the list of stuff I WANT to learn.
Not really, I'd say. The key things are: 1. The default source of packages are [Stackage](https://www.stackage.org) snapshots, though it is easy enough to add extra dependencies if you need something not in Stackage. 2. Libraries are not installed directly with an install command of some sort, but by adding them to the dependencies of a project you are working on (after you do that, they will be installed next time you build the project). 3. Given point 2, everything that you do must be done in the context of a project, with a .cabal file and a `stack.yaml` to specify dependencies and other options. (Not everything must be a module of the cabal package though - you can do throwaway experiments in the context of a project with commands such as `stack ghci` and `stack ghc`.)
&gt; loooong time Exactly. Stack might have pushed things a bit ... or not.
&gt; I want to write concurrent programs in an event based way. [...] I would like an Observer pattern [...] Are you sure? That sounds like a very error-prone approach. Concurrency (as opposed to parallelism) means that you don't just want to use multiple cores to make your code faster, you actually need the non-determinism of multiple interacting threads of control. And observers communicate via side-effects, which are hard to reason about in the presence of non-determinism because there are many different orderings of side-effecting computations to think about. By any chance, are you trying to accomplish something specific, in node you'd use concurrent observers, so you're asking about concurrent observers instead of asking about your specific situation (a situation with the overly-vague name "[the XY problem](http://xyproblem.info/)")? Haskell has many nice abstractions for concurrent, parallel, and event-based programming which might be much better suited to your specific situation than observers. &gt; [...] that lets me write success, error, and last callbacks, and that has an API that allows me to "trigger" an event, rather than using putMVar. Does anyone know of a library to do that? Success, error, and last? As in, an event stream whose next event is either yet another event, an event indicating that no more events will be received because we're done, or an event indicating that no more events will be received because there was an error? That sounds like a lot like [RX](http://reactivex.io/documentation/observable.html), for which there is apparently a Haskell implementation called [RxHaskell](https://hackage.haskell.org/package/RxHaskell-0.2). I haven't used either RX nor RxHaskell yet, but judging by its [`Event`](https://hackage.haskell.org/package/RxHaskell-0.2/docs/Signal.html#t:Event) type and its [`send`](https://hackage.haskell.org/package/RxHaskell-0.2/docs/Signal-Subscriber.html#v:send) command, it looks like it might be what you're asking for: data Event v Source = NextEvent v | ErrorEvent IOException | CompletedEvent send :: Scheduler s =&gt; Subscriber s v -&gt; Event v -&gt; SchedulerIO s ()
It also probably doesn't help that my most recent post to reddit is a post in /r/programmingcirclejerk about spj thinks haskell sucks. 
Will that work? The original justification was that `(:: T)` would desugar to `\x -&gt; x :: T`, which has type `T -&gt; T`. That rearranges to `(-&gt;) T T`, which when given to something expecting `proxy a` unifies `proxy ~ (-&gt;) T` and `a ~ T`. That kind of requires `T :: *` to be meaningful. Now that doesn't require that the same syntax can't be used for both, just that I hadn't heard that it would work in conjunction with PolyKinds. ExplicitTypeApplication might also replace the need for proxies, though I'm not 100% on that. It would have to be possible to write functions that can only be called with explicit type application to replace proxies, and I'm not sure that would be a good thing. `commute :: proxy a -&gt; proxy b -&gt; () :- ((a * b) ~ (b * a)` for instance is ambiguous without proxies, and I would hate for `commute :: () :- ((a * b) ~ (b * a))` to typecheck if it meant any time you try to use it you're informed of the ambiguity at the call site rather than the definition.
I'm not sure if I follow. All the examples you gave use *sudo*, i.e. require root access. Is there a way to install these *without* root privileges? That's the point of the simple binary distro's where one can install GHC at a user directory. RPMs are great, and thank you for your efforts; but a binary distribution would be even more awesome. 
Nice. Actually, I'm slightly sure. I'm used to NodeJS where when you're writing servers, you do want an event loop to handle requests when they come in, so naturally you have to use some concept of observables. In Node, almost everything is an Observable, and an event loop actually helps with reasoning about code. Communication between observers isn't too bad, but in Node you always can register new observers (event callbacks) on observables (event emitters). I was just wondering if Haskell had any library like that, especially something like [Express](http://expressjs.com/) that has a nice API to write webservers.
Has anyone worked in one of the non-swe roles? I interviewed for SRE (devops-y) once upon a time, ironically because someone found me on haskellers. It seemed more interesting than the typical "debug this toolbar button" style of big company software job.
I don't think so. Stack is a different approach, where they also take snapshots of the package world, and then provide isolation for each project. It's more like Ruby's bundles. FWIW, as an ops engineer, I think stack is pretty essential.
BTW as another caveat: the list you linked is buggy, in that some of the alternatives listed are not actually correct. E.g. the `-&lt;` alternative is not `↢`, but actually it’s `⤙`. (A patch which fixes this should be in GHC 7.12.) The Unicode codepoints should be correct, though.
Welp... I didn't notice the typo. It should've been 'constrictive' definition. 
I still don't understand why not just const = (a b → b) It is not ambiguous at all, the `\` is redundant. So, why?
Setting `HEROKU_NO_BUILD_DEPENDENCIES=0` will attempt to build on the deployment instance *which has failed for about 50% of the times that I've added a dependency*. Most significant codebases won't compile within Heroku's memory limits, let alone their time limits. Halcyon also seems to trigger more dependency rebuilds than necessary. I'm not sure why. While `HEROKU_NO_BUILD_DEPENDENCIES=0` doesn't save you from this problem. I agree that it is worth using. I used `=0` on my production instances and `=1` on my staging instances. This kept me from ever breaking the production server. And, since I always built on staging first, the dependencies were always cached when I deployed to production.
I see. Can I count on your support for my `🚚 x → 🏭 x` proposal that I shall be submitting to the Haskell committee shortly? :)
There lots of things I like about stack (especially the fact that you can install ghc as well and test and benchmarking seems much easier). However, I had some issue with compatibility (stack not being to find a plan of a project which obviously has one) which is one of the reason I'm sticking up with cabal at the moment.
You probably want to look at the continuation monad. Where Node has a pattern of passing the "callback" as a parameter, Haskell has a monad. What Node calls a "callback" we call a "continuation". Then we wrap a monad around the concept so that you just write code in what looks like a sequential way, and the monad handles the plumbing.
So I read the post and the linked wiki page, but hoping for a clearer answer to the following: this new Nix-likeness is meant to solve your problems if you _don't_ use sandboxes for everything, right? Because if you do, you're already getting a stable, non-destructive package environment?
It really isn't "cabal xor stack". I'm using both for different needs. They don't exclude each other nor there's any desire in the community to get rid of cabal-install. 
I'm not sure I agree. To play the devils advocate, do you think the "competition" could be pushing forward progress :) ?. I 100% agree on the compatibility aspect, though.
Tbh I couldn't remember so I rm -rfed a Haskell project, recloned from Github, and it wouldn't run `cabal test` without configuring.
The core distinction will probably always remain, do you prefer to run a PVP solver or are you happy with a curated set of packages. Both tools support either workflow, but cabal is based around using a solver and stack curation. For commercial production work, I'd hands down recommend stack, it's just that much harder to shoot yourself in the foot when curation is the default. I appreciate the disk space benefits of shared stackage snapshots, the quick compile time of new projects that are based on existing snapshots on your system, and the elimination of cognitive load of tracking loads of cabal sandboxes. I for one do not adhere to the philosophy that PVP solving is somehow inherently a superior solution. I like to be on the "bleeding edge", but not to the point of frustration. stack also makes it easy when you do want to take small steps outside curation. I'd rather stackage curators go through the pain of cabal hell for me, so I can concentrate on writing code. Stack has the additional distinction of being a new project, and support for things like docker integration. Feature wise, I would not expect them to converge to absolute parity. A effort like stack requires broad community adoption to really be successful, stackage needs strong curators and enough community buy-in for people to list their packages on stackage for it to be a comprehensive ecosystem. Fortunately from what I have seen, that is possible. To make a governmental analogy: cabal acts like semi-anarchy, stackage behaves like a republic managed by benevolent dictatorship.
nice, this sounds like a nice improvement!
I think it is not so much a problem for users to learn it as a tooling problem with tools using cabal under the hood (e.g. things like yesod-devel or leksah need to implement support for both if we have two tools).
Right, this is supposed to make it better in the absence of sandboxes. Which is useful, because sandboxes require a lot of rebuilding of deps since they're all isolated.
from the first link: &gt; As there will be single database after implementing views, we don't need to call it for every sandbox database. I'm not familiar with ghc's package system, but aren't there (currently) global, user, and sandbox dbs? if I understand the proposal, cabal install x in a sandbox will install x in the single package db, and then the view file in the sandbox will be updated. does that mean we have to sudo every command, or will there not be a global package db?
Vishal hasn't finished the cabal remove/upgrade parts of the GSOC, so you'll have to wait some more ;)
It takes a while to restore **.stack** from the cache. This seemed like a typical run: $ time git push heroku master Counting objects: 6, done. Delta compression using up to 4 threads. Compressing objects: 100% (6/6), done. Writing objects: 100% (6/6), 510 bytes | 0 bytes/s, done. Total 6 (delta 4), reused 0 (delta 0) remote: Compressing source files... done. remote: Building source: remote: remote: -----&gt; Fetching custom git buildpack... done remote: -----&gt; Haskell app detected remote: -----&gt; Exporting config vars remote: -----&gt; Restoring libgmp-6.0.0a files from cache remote: -----&gt; Restoring Stack remote: -----&gt; Restoring .stack remote: -----&gt; Stack remote: stack will use a locally installed GHC remote: For more information on paths, see 'stack path' and 'stack exec env' remote: To use this GHC and packages outside of a project, consider using: remote: stack ghc, stack ghci, stack runghc, or stack exec remote: simple-0.1.0.0: configure remote: Configuring simple-0.1.0.0... remote: simple-0.1.0.0: build remote: Preprocessing library simple-0.1.0.0... remote: [1 of 1] Compiling Lib ( src/Lib.hs, .stack-work/dist/x86_64-linux/Cabal-1.18.1.5/build/Lib.o ) remote: In-place registering simple-0.1.0.0... remote: Preprocessing executable 'simple-exe' for simple-0.1.0.0... remote: [1 of 1] Compiling Main ( app/Main.hs, .stack-work/dist/x86_64-linux/Cabal-1.18.1.5/build/simple-exe/simple-exe-tmp/Main.o ) remote: Linking .stack-work/dist/x86_64-linux/Cabal-1.18.1.5/build/simple-exe/simple-exe ... remote: simple-0.1.0.0: install remote: Installing library in remote: /tmp/build_8f34ee6273b7c87c29ce105bce696e08/.stack-work/install/x86_64-linux/lts-2.21/7.8.4/lib/x86_64-linux-ghc-7.8.4/simple-0.1.0.0 remote: Installing executable(s) in remote: /tmp/build_8f34ee6273b7c87c29ce105bce696e08/.stack-work/install/x86_64-linux/lts-2.21/7.8.4/bin remote: Registering simple-0.1.0.0... remote: Copying from /tmp/build_8f34ee6273b7c87c29ce105bce696e08/.stack-work/install/x86_64-linux/lts-2.21/7.8.4/bin/simple-exe to /app/.local/bin/simple-exe remote: remote: Copied executables to /app/.local/bin/: remote: - simple-exe remote: -----&gt; Caching .stack remote: -----&gt; Making GHC binaries available to Heroku command line remote: -----&gt; Discovering process types remote: Procfile declares types -&gt; web remote: remote: -----&gt; Compressing... done, 1.4MB remote: -----&gt; Launching... done, v7 remote: https://mysterious-wildwood-1381.herokuapp.com/ deployed to Heroku remote: remote: Verifying deploy.... done. To https://git.heroku.com/mysterious-wildwood-1381.git 7d18572..a790e21 master -&gt; master $ git push heroku master 0.17s user 0.39s system 0% cpu 5:09.61 total
a "republic managed by a benevolent dictatorship"... so which? FPComplete seems benevolent, and I think stack is a great tool. but longterm, we should think about how to solve the issue without centralized curation. or how to cheaply have a community-hosted stackage server (how much does it cost them now? or for testing, which demands running code, how to minimize rerunning tests to avoid draining server time. maybe for packages under SafeHaskell, they could be asked to run on users' computers. or even any package, with a direct user confirmation, since we're all running people TH during build time and more haskell during testing and using, anyway. 
The one thing I really don't like about `stack test` is that implicit `cabal install` step which starts downloading stuff. `cabal`'s UI gives me more control. If I understand the no-reinstall cabal right, we won't need anymore sandbox, so running the testsuite will merely be cabal install --dep --enable-tests cabal test And the `cabal install` is needed only the first time. Until `stack` gives me this separation of concerns I consider it inferior to `cabal` for my needs. YMMV of course.
I can't imagine a time where I'd want to run the tests, but *not* install any new dependencies. Given "You asked for X, which has A, D, and Y unmet dependencies," stack's answer is to meet those dependencies while cabal just lets you know what is up. It's always struck me as somewhat odd that I'd try to run some cabal command, which would error because `cabal configure` or `cabal install` needed to be run (you're cabal, just do it for me pls) 
I have published the [promised blog post](http://apfelmus.nfshost.com/blog/2015/08/29-frp-banana-api-redesign.html) on the API redesign for reactive-banana 1.0.
This may be helpful: https://github.com/commercialhaskell/stack/blob/master/GUIDE.md#comparison-to-other-tools The rest of the guide will hopefully provide some help understanding how stack can be used, and for making the transition if you decide to.
You obviously don't want the control, I do. Just like when I call some command in the shell, I don't want the OS to automatically download install the command for me, but rather tell me what's up, and lemme act accordingly. So just because *you* can't imagine wanting this, doesn't mean that there isn't somebody else who wants that. That's why we have `stack` vs `cabal` in the first place. The only way I see `stack` and `cabal` united in a single tool is by having user settings where you configure whether the new tool behaves more like `cabal` or more like `stack`.
The comparison is clearly opinionated though: &gt; If you're a new user who has no experience with other tools, you should start with stack. The defaults match modern best practices in Haskell development, [..] To "match modern **best** practices in Haskell development" is quite a claim... 
That's understandable. Would you mind relating a possible scenario where you'd run eg `cabal test`, get the error reporting not installed dependencies, and not immediately run `cabal install`? (not really sure why meaningful discussion is being downvoted on both sides here? anyone care to explain their motives?)
`--dep` is short for `--dependencies-only` which you'd need to do in order to run the tests.
&gt; instance Monoid a =&gt; Monoid (Event a) What will `mempty :: Event a` be? `never`? In that case, I would prefer that such an instance be `Semigroup a =&gt; Monoid (Event a)` instead, both to avoid potential confusion and to allow use of types that don't have an appropriate `mempty`. I'm also fine with the monadic rewrite. I appreciate your concern about downstream libraries having to deal with phantom types. Would it also be possible and appropriate to allow use of monads other than `IO` in the `Frameworks` functions, so that `liftIO` and (a renamed) `liftIOLater` would have different types? I'm concerned about the potential for confusion in GUI contexts between constructing the UI/view, which is usually monadic for non-semantic reasons, i.e. imperative frameworks, as in `threepenny-gui` as well, constructing the event network, and writing callback handlers. These should (potentially) have distinct types. (I realize this comment isn't very clear but I don't have time right now to flesh it out; please ask me to explain what I have in mind!)
When running Haskell on the JVM (using Frege, that is) then you can choose between Swing (support comes with the distribution), JavaFX (supported through the FregeFX project), and others like SWT (take the Frege eclipse plugin as an example). And since you asked for recommendation: I personally run the FregeFX project and use it to maintain the JavaFX version of the Frege REPL. It is still early days but so far it has served me well. In fact, I partly ran this effort as an experiment to find out whether UI programming in pure FP is practical. To my surprise, I found that it is. 
I switched to Atom from Sublime and it was really easy: the same experience, key combinations, workflow and "philosophy". I tried Leksah a few times and it's a completely different world. I'm missing stuff I use most often, e.g. go to anything ctrl+p, multiple cursors with ctrl+d, lightweight projects, etc. Maybe I should open a few enhancement requests to add them to Leksah. It's a pity indeed that I can't use it because of such small issues. 
Depends on your interviewer, I know people who have done their interviews in Haskell before.
Looks like your cabal file requires GHC 7.8, so you'll need to use LTS 2.22 instead. It's theoretically possible that we could special case some of the wired in packages like that to give better advice. Another option is to bump the upper bound on base in your cabal file. If you're not particularly interested in sticking with 7.8, that's what I'd recommend.
Yes
It's totally awful naming in that there's Cabal the library and cabal-install the tool. It confuses everything in these conversations. Stack directly relies on Cabal the library, but replaces the cabal-install tool.
So... ``cabal sandbox init`` ain't good enough for ya folk? This is awesome news, by the way. 
I tried with LTS 2.22 it failed suggesting the command `stack solver`. I tried and it failed too. However, as you suggested, `stack init --solver` worked.
&gt; For commercial production work, I'd hands down recommend stack Why? I was going to say just the opposite. Stack seems geared towards the new casual user trying Haskell at home, who needs to be able issue one simple command and have everything work, without caring too much about specific versions, specific features of dependencies that are required, etc. Whereas a commercial user wants a more powerful tool with more control over the details. As a commercial shop, we use exclusively cabal, and it works great for us. However, I am looking forward to trying out stack when the next HP makes it easier to switch between them. That's probably the most important thing to strive for to begin with - reducing the buy-in cost of using one tool or the other.
Even if GHC had better support for impredicativity, I would still be undecided whether I prefer the impredicative API — it does add a type parameter to any abstraction that library authors might want to build on top. But GHC's lack of support for impredicativity definitely kills it for me, yes. In my understanding, impredicativity does have semantic issues, and at one point, I felt that this does indeed spill into the syntactic realm. (I have totally forgotten the context, unfortunately.) In any case, I now have the impression that impredicative types are genuinely hard to support, it's not just a matter of "nodbody did it". I might be totally wrong on that, though.
If I understand correctly in 1.0 you plan to remove accumE and accumB and just have an accum function?
This obviously suggests at least some familiarity with the abstraction, such that you knew learning about it would provide a solution to a problem. With what you've learnt, how did you get to the point where you knew you needed to learn about some area of category theory?
Oh, no, the example was just to demonstrate that the new semantics introduces new relations between `accumE` and `accumB`. They will still stay separate functions. I do entertain the idea of changing the type of `accumB` to accept a whole list of events accumB :: a -&gt; [Event (a -&gt; a)] -&gt; Moment (Behavior a) Not entirely sure yet, though.
Yes. The fundamental purpose of Haskell Platform is to represent the standard way to install Haskell, whatever the consensus of the community wants that to be. Since there is now a lot of interest in stack - deservedly so - the next version of HP will will provide stack as an option along side cabal. We'll see where things go from there.
Node's designer said he considered using Haskell for Node at one point, but couldn't figure out how to make his idea work in Haskell. It turns out to be that he couldn't figure out how to make his idea work in Haskell because it's *already* how GHC does IO. There's nothing extra to do. No libraries needed. It just happens, and you get to use a concurrent model instead of all the callback spaghetti.
You haven't said what you are trying to do in your concurrent program, so it's hard to give you specific advise. But have you considered other designs, besides the two that you mentioned? The simplest idiomatic way of designing complex concurrency patterns in Haskell is STM. Have you read the ["Beautiful Concurrency" paper](http://research.microsoft.com/pubs/74063/beautiful.pdf)"?
To be honest, few of us are... but that's what most people expect those days for computer applications. :-s
Reproducible builds are possible with cabal, but they're the *default* with stack. So that's an appealing point for commercial users as well. Stack also makes it easier to rely on frozen snapshots (than using stackage with cabal), which is also friendlier if you're distributing package RPMs to your servers.
Interesting. So the name `seq` is really a misnomer.
The main complaints about sandboxes is compile time and disk space, which are solved by this.
&gt; Reproducible builds That is critical for releases, but not necessarily good during development cycles. So I'm not convinced you want that to be the default. It's really no problem to add a single `cabal freeze` line to your release script. &gt; frozen snapshots... distributing package RPMs to your servers That's interesting. So far, we've only needed to distribute RPMs for releases, not for reproducing Haskell build environments.
That's definitely true for releases, and cabal-install also does that. During development cycles, though, you want to move forward painlessly whenever possible. Once you get stuck on old versions, the cost of getting up-to-date grows quickly and it becomes a negative feedback loop. It took us a while to work out how to avoid that using cabal (after first finally digging ourselves out of the hole). I'm a little worried that we'd get stuck again with stack.
This is a page about stack, written by the author of the tool at FP Complete, on a github site created by FP Complete. It's understandable that FP Complete describes their own toolset in that way. It's a bit of a problem that this page is presented as if it were an unbiased "commercial Haskell" page, though.
This doesn't contradict what I wrote (namely that `configure` is mostly redundant since 1.18). Moreover, `configure`/`build` both work with the packages you've already available in your package db. If the testsuite requires a package you haven't yet installed `cabal test` will fail. You have to view `cabal test` as a variant of `cabal run` (which implies `cabal build`, which implies `configure`) tailored to running the testsuite.
No, but I might have used it, had it existed 8 years ago. I haven't had to change any UI code for many years though, so I'm not about to go redo it all now. And I think the hard division forced by a language barrier has been good in general.
Why not just try it?
edit: btw, thanks for the constructive reply. My only desire is to reduce barrier to entry into Haskell, and make everyone's life easier. These kind of conversations help people working on tooling to figure out what to make better. I personally think that having one central tool that solves most problems (and perhaps has a plugin system for solving more problems) makes a HUGE difference to the usability of a programming language. Lein made working with Clojure a breeze when I briefly used that language. &gt; I'd be interested in the use-cases you consider outside of the scope of cabal-install. Maybe they're easy to add to cabal proper or implement as an add-on tool. Stack features that I *assume* `cabal-install` doesn't want to do: - Manage GHC installations - Fancy-pants Docker stuff - built-in support for using curated package sets (of course you can use Stackage with Cabal, but it'd be nice if you didn't have to do the extra work) - specify/build multi-package projects in some decent way - support things like git repos @ specific commits for providing dependencies Stuff Stack *does* but `cabal-install` currently doesn't; maybe it could: - support multiple templates for `stack new` I could be wrong about this stuff; this is just my impression after dabbling around. I also may be missing some important things. &gt; By redundancy I assume you're referring to the issue of having to duplicate properties between target stanzas in the .cabal file? I was thinking of having to specify `other-modules` and `other-extensions` (if I have to write `{-# LANGUAGE #-}` pragmas in the source, why do I have to list them all in the `.cabal` file as well?). I don't want to have to list out `exposed-modules` here, either. I'd rather modules just be automatically included (or at *least* allow me to put "all" in these fields) and let me override it if I really want to. &gt; As for the non-standard syntax, is there even a canonical standard syntax for package meta-data which all current .cabal features can be mapped to? Syntax: By "standard syntax" I am just referring to the very basic syntax. `.cabal` files don't use JSON or YAML or even Haskell, but some other weird syntax that we have to learn. 
How about semigroups moving into base, as the first step of a "mini-AMP" that will eventually make `Monoid` a subclass of `Semigroup`? /u/edwardkmett *promised* that we'd have this for 7.12. :) EDIT: OK, so he didn't actually promise. He said in a [reddit comment](https://www.reddit.com/r/haskell/comments/39tumu/make_semigroup_a_superclass_of_monoid/cs7i3aq) that it was a tentative plan.
We're bringing it in, but we're likely not going to get the AMP-like warnings in time for 7.12. There are some subtle differences that make it harder.
That's still great progress, and I'm glad to hear about it! What will happen to `Data.List.NonEmpty`?
Or `reactive-radioactive-reactor` to go for the trifecta.
Yeah, me too. We'll see!
It tastes like a package manager.
It is more than just the API blow-up, it is also the problem that it causes a leak abstraction. Now *everything* abstracting over `reactive-banana` gains a `t` parameter, and I consider this a leaky abstraction. It also means that you have to play the same tricks as `reactive-banana` in order to support dynamic switching. An alternative is to parameterize everything by `behavior` and `event` types (which can be `Behavior t` or `AnyMoment Behavior`), but then you have to do a lot of type level programming just to move between these types in any reasonable fashion.
&gt; Stack features that I assume cabal-install doesn't want to do: &gt; &gt; Manage GHC installations Well, `cabal` had support via flags for switching between multiple installed `ghc`s (as well as non-GHC compilers) for ages (whereas Stack is tied to GHC). Until GHC's build-system is expressed a big `ghc.cabal`-based source package, I don't see how `cabal` could be able to execute `cabal install ghc`. &gt; Fancy-pants Docker stuff I know too little about Docker. I haven't had the need yet to use Docker as it doesn't seem to offer anything beyond bare LXC or `systemd-nspawn` that I was missing. &gt; built-in support for using curated package sets (of course you can use Stackage with Cabal, but it'd be nice if you didn't have to do the extra work) according to [this](http://www.well-typed.com/blog/2015/06/cabal-hackage-hacking-at-zurihac/) it's being worked on: *"People started work on supporting these natively in Cabal and Hackage. The idea is that proper integration will make them easier to use, more flexible and easier for people to make and distribute curated collections."* &gt; specify/build multi-package projects in some decent way Could you elaborate what you mean by this? &gt; support things like git repos @ specific commits for providing dependencies It was actually considered for `cabal` to support git locations for `cabal sandbox add-sources`, but [it turned out that `git submodule` does the job better](https://github.com/haskell/cabal/issues/2769#issuecomment-131599845). Once you start pinning Git commits as build deps, you should use the native Git tooling support rather than reinventing `git submodule`s in cabal. &gt; Stuff Stack does but cabal-install currently doesn't; maybe it could: &gt; &gt; support multiple templates for stack new Not sure about this, as I've never tried `stack new`. What's an example of such a template? &gt; By redundancy I assume you're referring to the issue of having to duplicate properties between target stanzas in the .cabal file? &gt; &gt; I was thinking of having to specify other-modules and other-extensions (if I have to write {-# LANGUAGE #-} pragmas in the source, why do I have to list them all in the .cabal file as well?). You don't *have* to list them, but they are going to play a more important role starting with Cabal 1.24 as they will be [provide dependency information (like `build-depends`) to the cabal solver](https://github.com/haskell/cabal/pull/2732). This allows use to not have to use the unsound `base`-version constraint hack to denote that we need a compiler with a certain language extension in the future. &gt; I don't want to have to list out exposed-modules here, either. I'd rather modules just be automatically included (or at least allow me to put "all" in these fields) and let me override it if I really want to. How would `cabal` detect which modules belong to which target stanza and which of those should be exposed by a library? &gt; As for the non-standard syntax, is there even a canonical standard syntax for package meta-data which all current .cabal features can be mapped to? &gt; &gt; Syntax: By "standard syntax" I am just referring to the very basic syntax. .cabal files don't use JSON or YAML or even Haskell, but some other weird syntax that we have to learn. Fair enough, although I'd argue that JSON would be tedious to edit by hand; YAML would be more desirable than JSON here. Also you'd need a way to represent `if`s and `flag()`s `impl(ghc &gt;= 7.0)` as well as version constraints (e.g. "`&gt;= 4.3 &amp;&amp; (&lt;4.4 || &gt;= 4.5) &amp;&amp; &lt; 4.8`") in the JSON data model. You'd still end up with a Cabal specific grammar/syntax on top of YAML/JSON. 
Minor clarification, you probably meant `default-extensions` as putting pragmas into `other-extensions` does not enable them during compilation. `other-extensions` is purely informational for Cabal and the solver to know which extensions your code needs and compare them against the feature-set of your current compiler.
&gt; Therefore, it can be slow if you need to do many time calculations quickly. In that case, use the thyme library, which has the same rich semantic representation of time as the time library, but restricts itself to the range of time resolutions that usually arise in real life and is therefore able to be far faster. It also uses a ton of memory, if you need to store many timestamps. `thyme` will also help here.
What was the doomed project? 
Curiosity about the use case from a stack developer and a potential option to give that functionality is downvoted as _not constructive_? Weird. 
&gt; One project I'm working on right now uses NonEmpty pervasively, so that commands come out of the "view layer" with a guarantee that the user has selected at least one item. Oh, you're saying that you have a use case for instance Semigroup a =&gt; Monoid (Event a) but that this instance would be forbidden once I introduce instance Monoid a =&gt; Monoid (Event a) ? Still, I would prefer to follow the current precedent. Once `Semigropu` moves into `base`, I'm happy to move as well. Is it possible for you to just add the instance instance Monoid (Event (NonEmpty a)) by hand?
Well, Bjarne's talk was about moving away from dynamic polymorphism (he is not the first one to talk about it - Sean Parent, Chandler Carruth and even Alexandrescu had similarly inspired talks). This Alexandrescu's talk is still about static polymorphism. He just calls it non-generic since it is not about named concepts, but rather compile-time checks (like unnamed concepts since the concepts themselves are not reused). I would argue that this is still generic programming. When you start using unnamed functions (lambdas), you are still not abandoning functional programming.
I think you need to put `TemplateHaskell` in `other-extensions` for newer GHC versions, or you may sometimes get weird linker errors. 
I just spent a good amount of time trying to do this as nice as possible. This is what I came up with: oneAmLocal :: IO LocalTime oneAmLocal = do tz &lt;- getCurrentTimeZone time &lt;- getCurrentTime let local = utcToLocalTime tz time return local { localTimeOfDay = TimeOfDay { todHour = 1, todMin = 0, todSec = 0 } } It was much harder to figure out how to do it than what the others in here said.
&gt; There doesn't seem to be a good tutorial on this library so if you have some patience I might get one up in the following hours. That would be totally awesome. The biggest problem with the time library is its very terse documentation.
Yeah, I have to admit that particular manipulation is not convenient at all with the standard `time` library, mostly because you mentioned local time. The `time` library is really designed for making all calculations on universal time, and then converting to/from local time only when you are interfacing with the user. If you wanted to do your calculation with UTC time and just the regular `time` library, it'd be something like λ&gt; utctime &lt;- getCurrentTime λ&gt; utctime { utctDayTime = 60*60 } 2015-08-30 01:00:00 UTC But if you want to do it with local time, you almost have to know lenses to make it convenient. If you know lenses, you can write your moment.js operation as λ&gt; getZonedTime &lt;&amp;&gt; time .~ midnight &lt;&amp;&gt; hours +~ 1 2015-08-30 01:00:00 CEST which is type-checked and everything, so if you misspell `hours` you'll get a compiler error. It's great, but if you don't know lenses, it's probably also an impenetrable mess... ~~Doing the same thing without lenses gets long-winded:~~ DON'T LOOK AT THIS λ&gt; zonedtime &lt;- getZonedTime λ&gt; utcToZonedTime (zonedTimeZone zonedtime) ((zonedTimeToUTC zonedtime) { utctDayTime = 60*60 }) 2015-08-30 03:00:00 CEST ~~because at this point we're converting `ZonedTime` (local time with timezone) to `UTCTime`, performing the calculation and then converting back again.~~
 TimeOfDay { todHour = 1, todMin = 0, todSec = 0 } is just midnight { todHour = 1 } and tz &lt;- getCurrentTimeZone time &lt;- getCurrentTime let local = utcToLocalTime tz time is just local &lt;- fmap zonedTimeToLocalTime getZonedTime :) (But I strongly recommend keeping to `ZonedTime` and not `LocalTime` because once `LocalTime` values start creeping into your program you are losing information that you might need later.)
This code looks more complex than it needs to mainly because you are forcing 1 AM in whatever happens to be set on the local computer as the current time zone. Which might have changed since 1 AM if the clock changed on that day, and you forgot to check for that. I assume you are doing all that because moment.js does that by default. My guess is that quoted moment.js code is also buggy unless you add a boatload of code to check for a clock change. The `time` library default of UTC is saner. (Plug: you can use my [timezone-series](http://hackage.haskell.org/package/timezone-series) library to get the actual 1 AM for that day, if it exists, regardless of clock changes.) It's hard to figure out because it requires either already being used to the library, or being used to the old Haskell terse documentation style, from the days when Haskell was only a research language, where type signatures are the main documentation.
 getZonedTime &lt;&amp;&gt; set time midnight &lt;&amp;&gt; over hours (+1) There, much more readable ;)
I feel the need to ask at this point... where are the _odd_ numbers?
&gt; And I think the hard division forced by a language barrier has been good in general. Very true. It can be tempting to put code into the GUI code that does not belong there otherwise. You always regret it later but it is hard to avoid, especially in projects with several programmers.
Ah, it turns out that microlens-platform is what I was looking for! Very nice work. 
Last I tried, getting gtk working on windows was simple enough in that I don't think gtk2hs should be considered a Linux only option.
I wrote it like that because the poster wrote &gt; (I'm a haskell newbie) so no *fmap*. I know that this can be simplified greatly in this case, but I wanted to write it in a more general way to make it easier to make modifications to it :)
I think your convert-to-utc-and-convert-back example is wrong, because you're still choosing 1AM UTC
I'm not very fond of the tone of your question -- in fact I was thinking of writing a message to this effect on the [similarly antagonistic](https://www.reddit.com/r/haskell/comments/3ite8n/noreinstall_cabal_a_project_to_move_cabal_to_a/cujlyw0) question you asked in the previous thread. Developping tools is difficult, has a *lot* of tedious work involved, and requires some sort of sense of sacrifice for the community. Pitting projects against each other as you do here are *not* a good way to motivate people to work on either of them. Worse, they can give both sides of the fight ring you organize the sentiment that you don't value their work because you would be just as happy to go for the competitor insteand -- which might be true, but still not something you should throw in the face of people contributing their time to attempt to improve the general situation. Whatever tools one end up using, I think one should be respectful of the amount of effort that was poured into the tooling ecosystem, the part the use and also the part you don't use, whether or not it is up to your personal tooling standard or has obvious deficiencies yet to fix. Whenever you write a message on a public channel such a r/haskell, just think that you are writing directly to the main contributors of the tools that your discuss, and ponder about whether some of what you say would be inconsiderate if said directly to their face.
I think they are used for development only. So ghc in HEAD is 7.11 now.
You are absolutely correct. That's a bother. :( I did have a different example there at first which did the right thing but was much longer. Then I thought I'd be clever and do the UTC conversion. That's what I get for trying to be clever.
&gt; Whenever you write a message on a public channel such a r/haskell[2] , just think that you are writing directly to the main contributors of the tools that your discuss, and ponder about whether some of what you say would be inconsiderate if said directly to their face. You are right, and I apologize if I have offended (or demotivated) anyone. My question was only , "who uses which ones and why". I thought after a few months after the release of cabal it would be interested to do a sort of survey to know if everybody as moved to stack or not. Now, English is not my first language and I might miss some subtleties of it which can lead to a tone which doesn't please everybody. For example I don't see any difference between "I'm not very found of the tone of your question" with "I don't like the tone of your question" so I might use them indifferently whereas apparently one is acceptable on public channel but the other not. Anyway we learn everyday ;-) 
Then you have ZonedTime tz local &lt;- getZonedTime ;)
It's up now at http://two-wrongs.com/haskell-time-library-tutorial ! 
This is succinct and informative and ideally should appear early in time's docs.