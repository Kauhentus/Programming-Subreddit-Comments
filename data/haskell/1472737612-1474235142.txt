I would love to see part of Wagon open-sourced. I am particularly interested in an electron specialized command line 'terminal'. I know about https://github.com/shockone/black-screen but it currently only supports OSX. Anyhow, I would be curious to poke into the Wagon source (even if some bits gets removed for whatever reason).
Try changing: foldl' f v (PS fp off len) = to foldl' f v = \(PS fp off len) -&gt; and see if that works better. If so, it's due to a change in how GHC treats inlining pragmas. In the old days ghc would inline things marked inline, whereas these days it will only inline them when it sees a "saturated" call. This behaviour turns out to be more predictable for rule rewriting etc. So in this example, `foldl` as written would have to be fully applied before it gets inlined, and in your example of `foldl' plus 0` it's not fully saturated, so no inlining. Where as with `foldl' plus 0 . map id` this gets rewritten to `\x -&gt; foldl' plus 0 (map id x)` which is now fully saturated and so it does inline. So my change above changes things so that it's counted as fully saturated when the first two arguments are given at a call site, and so your example should then inline ok. IIRC, this change in inlining behaviour lead to the definitions of List `foldr` and `foldl` being tweaked, but this was never done for `bytestring`. PR gratefully accepted :-) (so long as you provide benchmark results)
Asking is the expected and encouraged way to learn that sort of thing, but that expectation should be discoverable. Did you use https://ghc.haskell.org/trac/ghc/wiki/ReportABug while making your bug report? If so, would it have helped if that page ended with a copy of the "need help?" text from https://ghc.haskell.org/trac/ghc/wiki/Newcomers
The CDN we use in front of hackage means we don't get reliable download counts anymore. If you'd like to help fix that by analysing the CDN stats, that'd be great. Or perhaps we should just remove the download stats since they're not useful, and with mirrors it's essentially impossible to fix.
Hooray! I have the first edition. Is there a changelog/release notes ?
Aw! I didn't notice it and bought the print version.
Wagon is a great product. I'm sad to see that it's going away.
Since the other thread (which was earlier) is getting buried in favor of this one, I'll repost my comment here: --- &gt;Backpack sidesteps the issue by offloading the work of caching instantiations to the package manager .... (it's extremely not first-class.) What does this mean in practice though? I read it as the caching is being handled by cabal(-the-library?) (i.e. the package manager), which I'd assume you would need in any case where you'd want to compile your code? 
Perhaps it means you won't be able to use Backpack just by invoking `ghc`? You'd have to compile via `stack` or `cabal`? I wonder how `ghc-mod` support would work.
I generally don't write Literate Haskell because the module header stuff (language extensions, export list, and imports) are just code barf that has to be at the top. The first 50 lines of [this](https://github.com/tfausak/hairy/blob/8032b3b7fd555356dc53754d924a4b34e58e625b/library/Hairy.lhs) are basically useless, for example. 
What exactly is Emacs + Evil compared to Yin in VIM mode?
&gt; "you have the wrong base" and mentioned nothing about ghc. That's a great idea. I think a few other library versions are also strongly tied to the compiler, those would also benefit from errors mentioning GHC versions. Maybe even try to calculate if there would be build plans under other ghc versions? &gt; I had to completely scrub the old one I totally share the desire for a clean filesystem, but ghc is set up to allow concurrent versions. Only one gets the `ghc` symlink, but all programs' real names have a version, like `ghc-7.10.3`, and keeps libraries and docs and stuff in directories with versioned names, like `/usr/local/lib/ghc-8.0.1`. Cabal-instal isn't smart about choosing a version, but it can be told with `--with-ghc=` or a config file. It's poorly advertised, but the functionality is there (it is in the manual, but not at all emphasized). &gt; Even when I did get the write distro working, building accelerate failed on many different occasions, and I ended up typing something like 31 commands to do build my project because it was breaking halfway through. That sounds like a dependency/compilation problem. How did typing more commands help? Did it take a lot of work to set up a working configuration, or did you actually have to type a bunch of commands every time you wanted to recompile your project? The second would be scary.
This has a lot of potential. I think I'm more excited by a proposal like this than trying to get closer to dependent types.
Getting Configuring ghcjs-0.2.0.820160826... setup: Encountered missing dependencies: haskell-src-exts &gt;=1.16 &amp;&amp; &lt;1.17 &amp;&amp; ==1.17.1
&gt; code barf I think I just found my new favorite term. In all seriousness, though, you could just exclude explaining basic stuff. Or am I missing something?
It baffles me, with the prevalence of indentation-sensitive languages like Haskell, that reserving an indent for comments hasn't been reimplemented dozens of times in many languages. This isn't rocket science; early Lisp implementations saved a bit on each word for garbage collection. This is the same idea. GHC allows for a custom literate preprocessor, which is handy for ghci. Otherwise, it is just as easy to preprocess externally as part of one's build system. I adopt "**comments are flush, code is indented**" with the additional escape that flush dots by themselves delimit arbitrary comment blocks. I then use markdown in comments, so all my code can be read as html. I've been using this system for years, for all Haskell code. There's no overhead that I can perceive once this is set up, so it isn't a choice, it's just how I code in Haskell. One can write such a preprocessor in a dozen lines of code; it wouldn't surprise me if someone could come up with a Perl one-liner. The harder part is tweaking format coloring for one's editor, as those specification languages rival TeX in their arcane qualities. I loathe bird tracks, or for that matter any comment characters. It's this, or no comments, for me.
[removed]
The great thing about Hutton's and Bird's books are that they're about the language and algorithms, and not current snapshot of the implementation (toolchain and library ecosystem). This means, the books are less likely to be obsolete.
I thought the same thing.
I'm sorry, I made a typo, I meant to say _you can't_.
No it was also impossible. It didn't have much effect on stack/nix when stackage snapshots were removed.
These fields are kinda underdeveloped in the ecosystem. Yesterday people create an organization to develop it more: https://www.reddit.com/r/haskell/comments/50idkq/datahaskell_an_open_source_haskell_data_science/
In this case people want to write performant code without doing GC pauses at inconvenient times. ATS uses linear types to track memory management, it could work for Haskell.
I would love to collaborate on a port of XMOnad to libweston-desktop, because ... reasons ..., and me loving my XMonad config, though I'm not sure how much will be lost due to being unmappable to Wayland idioms.
And yet people ridicule me when I say I wouldn't integrate with a random web service unless paid for doing so and maintaining it. What you don't control is not something to build your house (program) on. If it's open source and hostable, at least you can operate an instance.
I use `allow-newer: true` could you add it? # cat stack-base.yaml flags: {} packages: [] system-ghc: false extra-deps: [] resolver: nightly-2016-08-26 allow-newer: true then I bootstrap ghcjs with `build.sh` stack --stack-yaml stack-base.yaml setup PATH=`stack path --bin-path --stack-yaml stack-base.yaml 2&gt;/dev/null`:$PATH echo SETUP date stack setup the `stack.yaml` is has the original snippet, your other stuff and ` allow-newer: true`
Nothing necessary. A background in rigorous mathematical reasoning like an entry level mathematical proofs course at a university would be nice. If you are comfortable with induction on the natural numbers nothing should be too difficult to work through. If you aren't comfortable, IIRC it is explained in the preliminary chapters of the book and you should just take extra time going through the beginning.
My question really comes down to why this must be tied to `Typeable`. It seems like you could easily implement something like what suggest on top of the proposed reflection machinery. What does generalizing it buy us that a library would not?
It looks useful for efficient library code, so you would probably benefit from it anyway.
Thanks. I'm a victim of bad math teachers in high school, so my love and appreciation for mathematics only surfaced after years of programming. That means, I need to learn much of it now.
I was not familiar with Jeff Polakow's deep encoding, which seems to be something different, but I believe the "usual" analogue for linear types in current Haskell is indexed monads (rather than monadic regions, which I believe are an encoding of region types, as per their name).
The level of insincerity in the advertisements at the beginning is almost comical.
I've actually been doing a lot of work to marry the rig style rep with types are calling convention style calculus. If you want a sync up some time , this is something I've actually spent a lot of the past year working on. Edit: there's a few design things I'd suggest tweaking in the proposed design to both more naturally embed all the various delicious linear logical connectives AND More naturally line up with the performance/memory lifetime implications of repeated boxing and unboxing. The types are calling conventions paper combined with a slightly cleaned up flavor of Conor's rig type theory make for a yummy and rich substrate for linear logic Edit: even ignoring the resource logic pieces, the right approach to linear logic is a wonderful boon to protocol engineering / coinductive descriptions of fun algorithms. In fact stuff like copattern matching is even better in the presence of linear logic :)
Will there be a hardcover edition?
Brace for silly questions, I'm not very knowledgeable. Are linear types in conflict with mutability? It would be really cool to be able to modify an object using a lens, and not have it allocate any memory. Changing `Just (x ::1 _)` to `Nothing` would require affine types though, so without that mutability would be of limited usefulness. Is that correct? And borrowing in Rust sounds to me like an "extension" of affine types. Is there a reason a similar thing couldn't be done with linear types? Besides, why linear and not affine? What are the advantages/disadvantages? I'm guessing linear means that they are never deallocated -- this sounds like a nice property, but isn't it too restrictive in practice? &gt; In a way similar to application, pattern matching multiplies the weights. &gt; &gt; Assume x ::π D and &gt; data D = C (y ::ρ B) case x of C y -&gt; _ &gt; In _ we have `x ::(π×ρ) B` Shouldn't this be `y ::(π×ρ) B`? And what if π=ω and ρ=ω? Do we get ω^2 ? I would expect `y ::max(π,ρ) B`
Ah ok fair enough. Sorry about the misunderstanding! 
Tough question. I don't think TaPL has any clear prerequisites apart from discrete math and proofs, and a certain degree of mathematical maturity. Assuming that, Michaelson's *Intro to FP through Lambda Calculus* might serve as a gentle introduction to some of the themes that are covered in TaPL in much greater detail. *Software Foundations* by Pierce and coauthors covers much of the stuff up to simply-typed lambda calculus in full formal glory of Coq. (As I, personally, kinda lack the aforementioned mathematical maturity, I find it much easier to work with proofs when I can disassemble them into small digestible chunks in CoqIDE, the price on that being that the proofs *are* formal rather than informal.)
(sorry for the delayed response) &gt; Which of the following are (i.e., evaluate to) values? Hmm, that's an interesting question! Thanks for bringing up those trickier examples, this is turning into a much more engaging debate than I expected. Let's see. So first of all, I should mention that I over-simplified when I used `⊥ ∪ A` as the denotational semantics of `A`: if the type has non-strict constructors, there are more bottoms than just the top level. For example, the denotational semantics of `Maybe B` is `⊥ ∪ Maybe (⊥ ∪ B)`. With that in mind, the denotational semantics of your three examples are: * ⊥ * Just ⊥ * const ⊥ Okay, so the three expressions have well-defined denotational semantics, but that doesn't bring us closer to the question of which ones are Haskell values. I said that `undefined` is not a value, but an expression which, when forced, fails to produce a value. So `Just undefined` must be an expression which, while it doesn't fail when forced to WHNF, does fail when forced to full normal form. Since the expression can fail, it doesn't seem like it can be a value, because my intuition for the word "value" is the inert result of evaluating an expression, not an active construct which can trigger behaviors such as throwing exceptions. The case of `const undefined` is even more interesting because a function is already inert, it can no longer be further evaluated. But clearly, calling this function (and forcing its result) causes an exception to be thrown. But is calling and then forcing really that different from taking the "tail" of `Just undefined` and then forcing it? And what about the infinite structure `[1..]`, should it be considered a value even though there is no way to make it inert, as there is always more of it to evaluate? Clearly I have two different intuitions for what a Haskell value should be, and "inert" only covers one meaning. So let's define narrower concepts: * A "denotational value" is the mathematical construction given as the denotational semantics of a Haskell expression. * A "total value" is a denotational value which contains no bottoms anywhere. An expression is said to "evaluate to" a value if its denotational value is total. * A "WHNF value" is the result of an expression after it has been evaluated to WHNF or further. This cannot be bottom, but it can contain thunks which would bottom out of forced. * An "operational value" is either a thunk or a WHNF value. So your example expressions all have denotational values, none evaluate to a total value, all but `undefined` evaluate to a WHNF value, and thunks containing those expressions would all be valid operational values. When people say that "there are no values of type T", they clearly mean total values, and when people say "bottom inhabits all types", they clearly mean operational values or maybe denotational values. Good! I now have a more precise model of Haskell's values, and your tricky examples no longer seem tricky. Thanks again for pushing me in that direction. Do you agree with my more precise model, or do you have more tricky examples which my new model doesn't cover?
Still showing as unreleased for me, amazon says it comes out Sept. 30th.
I have the first edition. Great book. I wish there were an update mechanism - advantage of eBooks. Edit: I bought the first edition as a physical book, but I see that O'Reilly Safari has the first edition as an eBook, so maybe in a few months they will also have the second edition.
Yes, fixed.
How is that not relevant to your question? 
Typo in unrestricted application typing rule: Γ ⊢ f :ρ S ­&gt; T should be Γ ⊢ f :ρ S ­-&gt; T
Some thoughts: * You wouldn't have to steal any syntax for `-o`, it could be imported as a regular type operator. * First proposal I've seen that changes the `::` syntax. * Idris provides `UniqueType` and `Type*` parameterised over uniqueness. Will `-o` have the kind `Type -&gt; Type -&gt; Type` or will it make use of [representation polymorphism](https://downloads.haskell.org/~ghc/master/users-guide/glasgow_exts.html#runtime-representation-polymorphism)? * &gt; [...] we do not have to define several versions of `map` to take advantage of the new feature. In general, taking advantage of linearity does entail combinatorial explosion of the code. I think this is in contrast to Idris. * [Embedding a Full Linear Lambda Calculus in Haskell](http://functorial.com/Embedding-a-Full-Linear-Lambda-Calculus-in-Haskell/linearlam.pdf) is mentioned in the references, could some of this logic be embedded into the Haskell types system (as it is in the paper) rather than in the compiler? 
They do have the problem, and they do note the problem. Its lessened to the extent that most packages don't introduce datatypes but just tend to use the native structures lying around in js (which ezyang's comment already noted). You _do_ get this problem with plugins needing to modify something introduced in another module. Node's peer dependencies tackle this, as discussed elsewhere on the thread: https://nodejs.org/en/blog/npm/peer-dependencies/ But they ended up having of course versioning and bounds again, but without any sort of good solver in the middle. So round in circles it goes :-)
My last point is discussed [here](https://ghc.haskell.org/trac/ghc/wiki/LinearTypes#Whynotencodedlineartypes), the points against are valid although *#1* and *#2* sound like an argument for [further overloaded syntax](http://docs.idris-lang.org/en/latest/tutorial/syntax.html#dsl-notation) (of lets and lambdas) and improved error messages :)
I don't think I can implement in terms of the proposed reflection machinery. There's no way to write `genTyCon` (or whatever) to generate type-witnesses for arbitrary types that are not necessarily instances of `Typeable`.
What notion of fusion are you referring to that's a black art? You may find the following papers of interest: * http://www.cs.nott.ac.uk/~pszgmh/fold.pdf * https://lambda.uta.edu/uta.pdf 
It's definitely a useful technique. It's common in Haskell to return an `Either`. This doesn't mean you'd never use exceptions though.
&gt; use `-°` which stays within **latin1** I don't think that's any better than `⊸`.
I agree. I think the only background necessary is the patience to read slowly, repeatedly, and carefuly.
On the last point -- I think the nice thing is that the existence of a typed embedding means that we have a model of how the compiler _should_ do things. But I appreciate the argument that unless we bake it into the compiler, the solver will be slow, the messages bad, and the syntax much less than ideal:-)
Also asking questions in IRC! I went through TAPL... hm. 9 years ago? Asking questions in IRC made it really easy to grok.
I am using Google Web Fonts for the website, so I wonder if it's some intermittent bug on their delivery end. I tried to find bug reports but I didn't see an.
I don't think it's a big deal in practice because, as you say, most people will use cabal-install/Stack to build their software. But it has an influence on the design (something people sometimes wonder when they see Backpack is why they have to muck about Cabal; why can't they just define a module inside their module and instantiate it right there).
The Cabal repository set up its Travis CI to build and upload Haddock docs. The relevant bits are https://github.com/haskell/cabal/blob/master/travis-deploy.sh and the openssl bits in https://github.com/haskell/cabal/blob/master/.travis.yml for decrypting the push key.
Where are the controls for turning back when entering the red line, do some corrective action and continue trough the green line? Once in the red line, there is no way to go to the green line again. For example, a createFile fails because the folder does not exist. Why not create the folder and then retry the operation? Exceptions allow this since the error handlers can redirect the execution to the green line again .. or to a third or fourth or fifth line. That is why exceptions are potential goto's, and break composability. neither exceptions and either work with multiple threads. Most programs have not one but many trains going trough. Another problem: if step 1 and 2 did things that must be undone when 3 fails? There is no way , except cluttering the red railway with non composable code of previous steps. The same happens with exceptions. Anything that can solve these problems? 
Also since we'll be meeting weekly in NYC, we may need help finding space—if you can host us, please let me know!
[*StackOverflow Documentation*](https://stackoverflow.com/documentation/haskell/topics) seems to provide an infrastructure for hosting and crowd-editing taggable threads of markdown-based explanations together with a voting system to control visibility. *[HaskAnything](http://haskanything.com/)* seems to have multiple tag categories (used hackage libraries, media format, content, authors) and is mainly focused on aggregating information from other sites, but also allows hosting markdown content directly on the site. It seems to be also planned to automatically integrate content from other sources, e.g. library tags from the hackage database. Another difference is, that contributions are handled via github pull requests, which can be either used directly via github or a web interface. That said, *HaskAnything* seems to be currently in the alpha phase, lacking implementation of some important features like a search. But the author /u/beerendlauwers seems nice and open to [contribution](https://github.com/beerendlauwers/HaskAnything/issues) :). The site is written using the Haskell static site generator [hakyll](https://hackage.haskell.org/package/hakyll).
well, if you break your functions correctly, you don't have much stuff in the ugly part and the rest is neatly wrapped around it as if it were some innocent pure function.
&gt; What notion of fusion are you referring to that's a black art? Stream fusing relies on re-write rules and when the re-write rules fail to fire stream fusion silently fails. This means you can be working on a small piece of code that performs acceptably and make one small change that causes fusion to fail causing performance to plummet. 
Yes. You have to extend beyond the `Either` type, but all of this can be made automatic. For example, you can have instead of just the error returned in `Left`, a constructor which also has the rest of monadic computation, so that it can be resumed when handled. If things need to be undone, you can decorate the success line with undo actions that are automatically rolled back on failure. The monad instance becomes more complicated, but the resulting code still looks just like a regular do block.
Do you have a reference? This sounds like an interesting problem. I'd like to learn more. Thanks.
That suggests that every function that accepts any other function should be structured this way, because you never know when someone will want to do this. Should, after all, does not mean is. I agree it would be the nicest solution though, as opposed to solving it with exceptions, which is like a hack IMO.
&gt;affine/linear and uniqueness types You can cast into a linear type from a non-linear type. Uniqueness guarantees the value is unique as-constructed, no casting. Linear types guarantee the value constructed as or casted into linearity has no _further_ references made to it. A good example is https://en.wikipedia.org/wiki/Clean_(programming_language)
&gt; My first impression of repa was that it's powerful but the developer ergonomics aren't there yet, particularly for interactive workflows. My wake up call was when I tried to print a matrix to the screen and realized I had to write a pretty printer from scratch to print one matrix row per line. I had a similar experience with Accelerate. Things that were monads had no monad instance, for instance. A lot of other Haskell features were missing too: no lenses, traversals, or monoids. It worked fine but there were times where I felt like the solution I went with was the second or third that I thought of. (I guess I should probably consider contributing now that I think about all that) And the only way to store matrices was 32-bit bitmaps which meant I had to use repa in an accelerate project. 
I imagine for functions that allocate many small intermediaries, the benefit could pay off in GC times pretty substantially. e.g.: Any time a function generates a list and then computes a function from it. Even if GHC's optimizations don't remove generating the spine of the list, linearity could (theoretically) be used to unmark the memory for GC for (nearly) free.
&gt; Accelerate and Repa and hmatrix (?) among others seem like a strong starting point for higher level tools. I had pretty good experiences with Accelerate, but they didn't feel like "data science" tools to me. It felt like you were juggling folds and sums. There was no way to get an average or a standard deviation the way you could in numpy. Accelerate and repa are also both experimental. On linux I've had a positive experience, though I had to run things as "sudo" which was a pain. But on windows the cuda code wouldn't run. Also there was [this](http://chimera.labs.oreilly.com/books/1230000000929/ch06.html#CO22-2) issue which I don't know how to fix and is also pretty subtle from a developer's perspective.
My understanding is that memory is never 'unmarked'. It is only marked to be kept, and everything not marked is just gone without any additional work because it wasn't explicitly kept.
We got flagged by Slack for having too much user income in a short period of time. All the invitations for Slack are made through email in the end. What I'm doing now is "batching" invitations, so no invitations stay "idle". I won't use your address for anything, just send invitation and delete the email :)
Yes, I was thinking about making a mailing list in Google Code (I think that is called like that). But I have no experience on that, so would need some help. 
Awesome!
That's a good idea. I think I'd organize it into a bunch of partial functions which produce `Nothing` (`head'` or `safeHead` or whatever) and a way to convert `Nothing` into `Left err`: (??) :: Maybe a -&gt; e -&gt; Either e a Nothing ?? err = Left err Just x ?? _ = Right x To me that just feels more flexible and modular than combining the two into one function, especially because there are a lot of times when just a `Maybe a` would do (ie you match on the result right away). This way the partial function is also agnostic of how errors are handled. (It does presuppose that all the "primitive" partial functions are simple enough that they only have one meaningful kind of failure case, but that seems like a reasonable design restriction.) The resulting code reads exactly the same way, but with `??` instead of `$`. The only difference is that the `??` is optional if you don't mind a `Maybe` instead of an `Either`. You could also generalize this a bit to use monad transformers if that makes more sense for your application.
I see! Sending the email now. :)
&gt;Only one gets the ghc symlink, but all programs' real names have a version, like ghc-7.10.3, and keeps libraries and docs and stuff in directories with versioned names, like /usr/local/lib/ghc-8.0.1. Cabal-instal isn't smart about choosing a version, but it can be told with --with-ghc= or a config file. It's poorly advertised, but the functionality is there (it is in the manual, but not at all emphasized). Fair enough. More documentation is always a good thing! Is there any reason cabal doesn't use a custom ghc automatically though? &gt;That sounds like a dependency/compilation problem. How did typing more commands help? Did it take a lot of work to set up a working configuration, or did you actually have to type a bunch of commands every time you wanted to recompile your project? Oh, it did work eventually. It's just that it broke halfway through repeatedly which meant that I had to keep an eye on it. Which was frustrating given that all of the packages I had to install manually (happy, alex, c2hs) were available on hackage. 
oh... :-)
Given the function signature, the function looks like it is probably similar to a traversal, which implies it should be using Applicative anyway
What is in the minimal platform installer? What are the benefits of starting with the minimal platform? I'm still new to Haskell, but past the beginner stage. I got started with Haskell before stack and didn't use the platform because it was outdated at the time. Given my background, I would personally not suggest the platform to beginners. But might change my mind if I understand the benefits better.
this is `catch` which works just fine with `Either`
&gt; What is in the minimal platform installer? Here is what you get: https://github.com/haskell/haskell-platform/issues/250#issuecomment-243206682 &gt; What are the benefits of starting with the minimal platform? You get all the tools above and no extra libraries that could confuse the constraint based solver. With the [nix style package support for cabal](http://blog.ezyang.com/2016/05/announcing-cabal-new-build-nix-style-local-builds/) those extra packages shouldn't be an issue anymore.
Damn, `&gt;` is Shift+`.` (at least with the US keyboard layout). I accept your offer.
1st Edition was excellent. Learn monads with even realizing it. 
I've been thinking about trying org-babel for literate programming. It works more like "real" literate programming and is even able to use the indentation you really wanted when splicing code fragments together. Has anybody done this with Haskell? How did it go? 
And wouldn't there be programs that subtract a constant named "o" suddenly story working? 
I can HasData ;-)
Pretty sure you have to go out of your way to put something you can mutate into a reference-counted smart pointer in Rust. `Rc&lt;RefCell&lt;T&gt;&gt;`/`Arc&lt;Mutex&lt;T&gt;&gt;` are the most obvious ways to get that. With the first one being strongly discouraged.
Exactly. That's why the manual refers to these things as "stolen syntax". (Other examples being the hashes in `magicHash#` or `#label`, the dollar sign in `$(splices)`, `!bang` patterns, etc.)
An example (and please, correct me if I'm wrong): With uniqueness types, because the value is unique as-constructed, the compiler knows there are no other references to it, so the compiler can mutate it freely. If I only have a linear type system, I can create a non-linear thing and multiple references to it, then cast one of those references to a linear type. I can't, however, mutate my linear reference to the thing, because I don't know that my reference is unique.
Not a Haskell newbie but I need an ELI5 too. 
&gt; The expression `map (\x -&gt; f x) y` always checks I understand why this is a desirable property if we want a reusable `map`, but I don't understand how it's desirable for a function of type `(a -o b) -&gt; [a] -o [b]`. If I'm reading this type correctly, this is saying that if `f` uses its input exactly once, then `map f` also uses its input list exactly once, in the sense that it traverses the list once and uses each element once. If I write `map (\x -&gt; (x, x))`, should I obtain a function of type `[a] -o [(a,a)]` which claims that it only uses its input once but easily allows me to duplicate it using `unzip`? Or should it fail to type check? Ideally we'd be able to express a more precise type for `map` saying that it preserves the linearity of `f`, like this: map :: forall π. (Π(x :π a). b) -&gt; Π(x :π [a]). [b] Of course, that implies a much more sophisticated type system than what's being proposed.
I agree that its important for the platform/toolchain/what-have-you to be very up-to-date with GHC. Making it so and keeping it so was part of the whole set of improvements that we've implemented over the past period, including the introduction of minimal, fixing the "build networks on windows" issue, and adding stack. Thus far we've done pretty well with it. For 7.10.3 we released the day of: https://mail.haskell.org/pipermail/haskell-cafe/2015-December/122432.html For 8.0.1 we were within a week: https://mail.haskell.org/pipermail/haskell-cafe/2016-May/124024.html One thing that would help a whole lot would be more volunteer testers, especially on OS X and Windows, or people willing to jump in and help automate that testing (or even set up CI). We can scare up the resources for this if we have volunteers to drive it. (edit: if you're willing to volunteer, please message me!)
I don't have a reference, but fusion is heavily dependent on inlining, and the inliner is notoriously cantankerous. Also, there are all sorts of compiler transformations that can potentially interfere with rewrite rules, and it can be quite difficult for a mere mortal to predict how things will go in any particular case.
This is really great, thank you Edward! Once this current string of work travel is over, I'll try to pitch in some. Is the source for this document available in pull-requestable form?
if you don't like that Applicative trick, there's always delimited continuations (though that'll look like even more of a hack in Haskell, since they're not first-class).
I'm quite happy with the applicativery trick. It doesn't work if you want to use *someone else's* function (that you can't change) that doesn't accept an applicative function.
It will molder in https://github.com/haskell/cabal/pull/3727 for a bit, and then get merged into HEAD.
Oh that's a shame. I guess I'll settle for the paperback version then, thanks for the reply :)
We recorded the meetup, and the video should be up in a day or two.
There's also a counterpoint to this to be considered: upgrading GHC _too quickly_. When a new major version of GHC is released, there are two problems: * Often until the second point release, it's relatively unstable * Many packages haven't been upgraded to be compatible with it yet I'd generally advise people to stick with the previous GHC release until at least the second point release. I know many in the community don't want to wait that long, as evidenced by how often bug reports come out against packages for incompatibility with prerelease GHC versions. But figuring out which GHC should be promoted on haskell.org is definitely not a simple question.
I like this idea. Of course, you're going to have to have heterogeneous folders like '/'. Hopefully you could specify the type of a folder by what things it can contain e.g. '[JPEG, PNG, GIF] and get programmatic access to that list of types. Maybe you can have "Folder type permissions" which (dis)allows whether a folder can become heterogeneous or stay homogeneous. For example, I don't really care what goes in '~/Downloads,' so it can just become a sin bin of files, but you sure as shit better not put a Shell script in '~/silly_gifs'. Unfortunately, the hard question you have to answer is how do you determine whether a file belongs to a given type?
Semi off topic: I'm assuming the plan is for new-build to superseed build, is there any timeline on when this is planned or is that still too far in the future?
Wow, I had no idea of the feature set within new-build. 2 useful little improvements jump out at me: * cabal.project files for multi project builds * No need to 'configure' before new-build 
That's great! I have been using `new-build` this week, and really, it's a game changer.
what type of file would a folder be?
Thanks, all your comments are legit. &gt; I am not sure that this is correct. I think at least for profiling it applies to all packages (and has to do so). I don't actually understand how this works in the implementation, but if you have package with profiling enabled, it will automatically make sure that its deps have profiling. So you should be able to just put profiling in the package stanza you're interested in. Allegedly. I haven't actually ever tried this codepath yet :/
&gt; Are there plans to use this information to free memory automatically? I suspect the bigger win will be using this information to reuse memory automatically -- turning the usual FP idiom of "create a new object and throw the old one away" into "mutate the object in-place, confident that the old one would never be used again because of the linear type system's guarantees".
Well the jessie branch froze like 2 years ago. You should at least consider using the debian backports repo (which does have ghc 7.10.3).
Very good point - I was a bit too enthusiastic about having the latest release. I do strongly think it should be up there as soon as it's available, but you're right, maybe it shouldn't be the promoted version for newcomers.
Agreed. Google Books are good for tech books. They know how to scale an image along with the page unlike Amazon.
Sad. Now more than ever, of your going to get a book it mine as well be hardcover. Otherwise just get electronic version.
Thank you for trying to resolve this without creating more flamewars! While there will probably still be people unhappy with this, I’m really happy with this decision.
yes, certainly the "standard prelude" is lacking. sorry ): sudo definitely shouldn't be required, I don't know what went wrong for you there \\: the kernel caching issue? yeah, it sucks. This seems to be less of an issue with the LLVM backends at least (nvcc is slooow).
In the presence of exceptions it seems likely to me that the use of IO in their examples falls apart, making this far more invasive than it at first appears.
So what would the point of this be? 
In #haskell I take it? 
I can't thank /u/ezyang enough for not only pushing forward cabal by implementing and fixing lots of stuff, but also taking on the less popular job of documenting it all! Somewhat unrelated, we were discussing whether to switch the documentation from markdown to rST/sphinx, since we had good experience with [GHC's Users Guide](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/). Any opinions on that?
That has the same problem than exceptions: it opens a third railroad. It does not go back to the original one. It can do whatever and this is the reason why it is like a goto and produce non composable code: the catch railroad has to carry out all the corrective actions of the previous steps and this means that each step with `catch` has to know about the whole computation.
This looks really awesome! Keep up the good work. Having two great buildtools to choose from is undeniably better than having no choice at all.
Btw, one noteworthy neat feature is the built-in search feature you see in action in GHC's current manual
I still don't understand why specialising functions written over MTL constraints is fundamentally difficult for GHC. You don't have to inline in order to specialise. If I were to annotate the point where I instantiated my generic app with my concrete monad stack to say "GHC, please specialise this and chase it all the way down the call graph specialising for this type parameter everywhere" (or perhaps if that annotation when in the definition not the instantiation), is there any technical reason why that would be difficult for GHC to action. -- shudder, I just used "action" as a verb of my own volition. If that would be possible, I can imagine further that some heuristics might possibly be applied to do it automatically. Monad stacks possibly "look like" things that could benefit from specialisation. Typically in these cases there is only one monad stack so there is no benefit to keeping the generic version around. The automatic specialisation however is something that might be more interesting than good.
My two pennies: 1) literate programming is actually a bad idea 2) The way we do it is bad too. There's some decent papers by Norman Ramsey that suggests that a "car manual" approach is vastly superior to the "novel" approach proposed by Knuth. It's easy to see the attraction, but the truth is it doesn't seem to work. The second observation is also from Ramsey: cross-language programming is the norm. Even if you're writing a "pure Haskell" project, you should really be doing the same to your stack/cabal files and anything else. But for the most part our tools don't support that. I'll admit the tools are pretty useful for blog posts and dissertations, but they're much less useful for the everyday job of ongoing development. Yours, a recovering literate programmer.
You can do this, if you drop in specializations at the definition site. When you need to work parametrically in any of your effects or don't control the source code that isn't an option. With INLINEABLE it is possible to do a little more but then there are phase separation issues causing interop pain between INLINE and INLINEABLE code. Also, keep in mind until recently almost none of the methods in the transformers library had any INLINE hints on them at all, so the compiler was particularly keen to just give up on anything fancy.
Thanks for all this help! Will look more into it! :D
Absolutely. That would be amazing! There is limitless possibility, and if you inspect from that vantage point, the current filesystem is primitive. Perhaps NixOS could implement this?
NixOs is sort of leaning in that direction but my understanding is that they are hamstrung by needing to interoperate with legacy software.
&gt; Under the current scheme it's not possible, for instance, to wrap 'map' or 'id' in Dynamic of Data.Dynamic. It is possible but you just have to massage things because quantification in Hindley-Milner is implicit: {-# LANGUAGE DeriveDataTypeable #-} {-# LANGUAGE RankNTypes #-} import Data.Dynamic data Map = Map { unMap :: forall a b. (a -&gt; b) -&gt; [a] -&gt; [b] } deriving Typeable dynamicMap = toDyn (Map map) example = unMap (fromDyn dynamicMap (error "Bad fromDyn")) (* 10) [1, 2, 3] -- &gt; example -- [10,20,30] 
I use Literate Haskell for a specification I've written for our query language (which is implemented in Scala) at my job. It works perfectly for that since it's primarily a technical document with the Haskell code being used as a precise operational semantics. For a normal Haskell library, however, I'd just use Haddock.
[removed]
One book that builds a very solid bridge from beginner to aspiring Haskell hacker is Richard Bird's "Thinking Functionally With Haskell." — http://www.cambridge.org/us/academic/subjects/computer-science/programming-languages-and-applied-logic/thinking-functionally-haskell 
I think it starts to get wrong at "independantly of technical merit" and it gets worse at "it makes me feel bad" It must be a cultural thing 
So it's better but it's bad for feelings so you'd want it out. Because you don't want to be seen as a baddie. Yup, that's cultural
My favorite http://haskellbook.com/
Just to make sure I understand your point: if these data are kept in memory longer necessary, they may only spoil the cache by degrading memory locality (as these data are never re-read, they should not trigger any cache line invalidation). In practice, does it really have much impact on performance ?
I really loved "Learn you a haskell for the greater good"! http://learnyouahaskell.com
&gt; If the action was not corrective and simply closed some resources, ... then it may no longer be possible to resume execution on the green railroad, since it still uses the closed resource!
&gt; A natural transformation takes some Functor f to a Functor f, it maps f-of-a to g-of-a, for all a, in a "natural way", which means that if you were to fmap before you apply the natural transformation or fmap after you apply the natural transformation you'll get the same answer. So if you fmap then transform, or transform then fmap, you'll get the same answer. I've been trying to describe natural transformations to myself and failing. This seems like a nice terse formulation. ~~(Although I had to change "g-of-a" to "g-of-b" ...)~~ EDIT reverted to Ed's original statement
It's just a username ;)
Second edition of Hutton , public since yesterday: http://www.cs.nott.ac.uk/~pszgmh/pih.html
You can write this even more tersely as: fmap k . nat = nat . fmap k ... if `nat` has type: nat :: forall a . F a -&gt; G a You can also think about it like this: -- +-- `nat` can only modify this part of the type -- | -- v x :: f a -- ^ -- | -- + `fmap k` can only modify this part of the type ... therefore `nat` and `fmap k` must commute since they never modify the same part of the type
Thanks!
The problem is that in order to specialise, GHC needs to have the definition of the original function available. In order to make sure this happens, you need to mark your function as `INLINABLE`. Andras' top level comment is spot-on from what I can see. I don't really understand the objection Edward raises in the sibling comment to this. I've tried quite hard to come up with some code that GHC fails to specialise well at `-O2` but have been unable to come up with a good example. 
As a Debian user, I'm pretty certain I'll end up with old software if I use my distro's repository. I'll try it anyway (and did with Haskell), but won't blame anybody if I lose some newer features. That said, it would be great to have a disclaimer that Haskell moves fast, and the Debian version isn't really useful for development, just for compiling the packaged (and compatible) software.
Very cool stuff!
When I select an option with manual downloads, the hash appears truncated. I can still copy the entire hash, but can not see it. Is that intentional?
&gt; Is there any reason cabal doesn't use a custom ghc automatically though? First, I don't think it's ever been suggested! (based on searching GitHub issues). I also think it currently doesn't have enough data about other compiler versions to suggest any you don't have installed - I think it learns everything about your chosen ghc version by running it (including the version number, from passing `--numeric-version`), or running related tools like ghc-pkg. Maybe it does hardcode which packages are wired to the compiler version? With those resolved, a lot of automation would be nice, but I wouldn't want it to default to going too far without prompting the user. Building under a compile version you haven't previously installed might be a much larger build than you expected, seeing `cabal install` succeed only to find `ghci` doesn't have the library would be confusing, etc. An offer to add a workable version to the local `cabal.config` and continue would be nice.
I certainly don't have issue with the Debian packages. As you say it's to be expected. What I was really getting at was how, especially on the Haskell Platform web site, it implies you will get a current version. I created a [pull request](https://github.com/haskell-infra/hl/pull/178) for the haskell.org site to add instructions for Debian via backports which does provide a reasonably modern version of GHC
This sounds like it might be a nice way to structure an executable as a library with a thin executable stub in order to allow test sections to access the library. All this while not making an exposed library. Hopefully this can integrate into cabal haddock so convenience library documentation can still be observed during development!
I've found markdown not expressive enough for docs, and switched to rST in my projects. I think asciidoc is even more expressive, but it looks too noisy without highlighting.
You may want to read the discussion [on the existing PR for splitting off the solver](https://github.com/haskell/cabal/pull/3222).
Why the video is unlisted? 
Thanks! Pulled and deployed!
I can't imagine why it would be a good idea, so I don't think so, no :-) If you have a proposal of how to fix the html to solve this, PRs welcome: https://github.com/haskell/haskell-platform/blob/master/website/templates/plan-a/download-os-sections.html.mu#L197
Use `vector-algorithms` in production code. It has noticeably gotten worse at inlining in almost every release since 7.2 or so. In particular look at all the sort functions. If they don't inline and specialize away all the way down to unboxing the comparison function you can lose an order of magnitude in performance.
Well, I would that consider that a nice speed-up !
Not that it's readable by any "working hardware designer" because it's stuck in an academic journal. Truly, where good ideas go to die. Arrgghh!
Stack would have to adopt exactly the same hash key algorithm as new-build, which seems unlikely.
The less organized the person the more they could benefit from structure the more they would benefit the less likely that they would use it. The only people who would use it do not need it at all
Sounds like a database. And type sounds like schema. These days I think that it's less important to constrain what the system can store and more important to know what the data means. Like a triple store.
I've thought about designing a database language with algebraic schemas and maybe even higher order schema functions like to make custom schemas for various uses or parametrized on types or values 
I suggest dropping the "Nix-style local builds" moniker eventually (maybe that's what you had in mind) and instead describe the background/inspiration/influences/related-work in the docs. "cached globally" - you might want to say "cached in ~/.cabal/sore" or something, lest the reader thinks this is using the global package database. 
Updated. I'm pretty bad with the naming business, but I think what will happen is that `cabal install/build` etc. will all be changed to used the new infrastructure, and then there won't be an extra moniker; it will just be cabal-install.
The problem with Real World Haskell is that it's so old now you need real world Haskell experience to know which parts are outdated and how to deal with those parts. 
Copy and paste that exact link into http://sci-hub.cc, and you'll get the PDF. I actually am happy now when I find springer links, since Sci Hub will so often do the job.
No, no, *no*. Where did you read this from? &gt; Affine Types are more powerful but harder to understand conceptually. Linear Types are easier to think about but are more limited in what you are allowed to do. Affine types means an object must be consumed *at most* once, linear types mean that it must be consumed *exactly* once. That's it. That's the whole thing. For example, after you `malloc()` some memory, both linear and affine types would prevent you from `free()`ing it twice, and linear types (but not affine types) would also prevent you from `free()`ing it zero times. One permits more and guarantees less, the other permits less and guarantees more. Neither is really harder to understand than the other. &gt; It's a stronger (ie less flexible) version of Rust's variable lifetimes, which encodes how long variables will live as an extra type parameter. Affine or linear types -- or to use the umbrella term: substructural types -- have nothing inherently to do with lifetimes (as present in types). Lifetimes are a particular variant of region types. The two are separate things and there's no reason you couldn't have one without the other. (In a Haskell context, you can think of region types as the `ST` monad, and linear types as similar to indexed monads.) Rust *combines* substructural types and region types, and exploits some synergies between the two, which is partly where "borrowing" as a concept arises from.
On mobile warning. In the section about cabal.project, you mention just listing the directory. I've been using cabal.project files and I've found there are a number of times where I need to include the name of the .cabal file in order to get new build to see the package and build it. I think the issue is that I often put the cabal.project file in the "top level" package (so that all my packages can be parallel in the directory structure). So then I end up with the path "./" in my project file. Sorry I haven't filed an issue on github yet. Anyway, using the full path up to and including the cabal file fixes it. Perhaps the worst part is that the current preview doesn't give any feedback when it fails to find the cabal file, it just doesn't bother to build the package.
The website provide a pretty good roundup of learning resources: https://www.haskell.org/documentation It doesn't make any recommendations amongst them, but browsing through it can give you an idea of the range of things, and which ones might work for you.
As far as I can tell, yes that is what's going on. I should try a minimal test case. I'm traveling for the next week and a half though.
I was pretty excited about this release because of ```Data.Sequence.adjust` ```, but when I tried it out on a sample problem, I got about a 4x slowdown. Repo: http://github.com/johntyree/brnfckr Run `stack build &amp;&amp; time stack exec brnfckr programs/bench.bf` Then edit stack.yaml to add ``` extra-deps: [containers-0.5.8.1] ``` and try again.
I don't think `.lhs` offers enough to be worth the minor troubles others have described. I do, however, think that org-mode at least tickles the edge of plausibility for every day development by bringing so much more to the table (in-document equations, figures, linking, folding, *other languages*). That said, in my own use of this style, I find that I haven't entirely figured out how to balance things so that I can write whatever I think to write, but still be able to navigate the code without being buried by rambling. Some amount of discipline lets you write a TL;DR comment with a "Would you like to learn more?" subsection that can be hidden by default. This is definitely not bad, but as with all documentation, you have to be vigilant to keep it from getting stale. Something I have not pursued but intend to is linking to commits with a short description of how some piece of code used to be. I do the latter on occasion and find it very valuable for preventing me from making a mistake twice, but I think linking to the actual old code could be very nice. And finally the killer problem: integrating with tooling. I can get `intero` to work by using `outorg`, but it is delicate and I have yet to write a post on how to do it.
That would be kind of nice. I like this style, I just never get to use it because I don't like the end user experience. It could in theory take the argument names for data types where there is one constructor and they all are distinct simple variable names as well.
You seem to be talking about an alternative world where the incentives, validation and reward systems that exist in academia today are completely changed. It is an interesting conversation, but it is about long-term changes. On the contrary, I am pointing out that *today* anyone can send an email to Mary Sheeran to ask if it would be possible to make the paper accessible on her personal webpage. If people had the discipline to do that whenever they fail to access a paper that is of interest to them, the problem of not being able to access a paper would disappear in practice -- it's already fairly uncommon. Most people put papers on their personal webpages in addition to having them in a journal or conference proceedings. Some, and I agree it would be good to have more, also have it on arXiv. I personally believe that it is part of each researcher's responsibility to make their papers available to all, and that if they are not doing it they are not doing a good job. Of course, when I contact someone to ask them to make a paper accessible, I insist on the interest for readers rather than this (slightly subjective) moral position.
Yeah, [the list is growing](http://stackoverflow.com/a/23733494/1139697). RemindMe! 6 hours "add levity polymorphism and new extensions"
I had a look at this paper and it's not really about monoidal (control) categories but rather on point-free presentations of equations in the style of category theory.
[removed]
I'm worried that if subtyping were introduced, the variance story would become super complicated or even make the whole type system unusable. Not sure if requiring polymorphism tackles the problem better. (It reminds me of the situation of level polymorphism in Agda; people just can't resist of doing it everywhere.)
That is because of "The constraints of the benchmark are that your 95th percentile round-trip time cannot exceed 250ms.". As you can see on Rust 10000 the 95th percentile is 227ms. So most likely the 12500 bench failed. 
Yes indeed; the *Millenium Falcon* did the Kleisli arrow using less than 12 parsecs.
This has been confirmed in the pull request. It's an incorrect benchmark. That's a bummer.
Oh man, that condescending tone won't look good when others realise this is an incorrect implementation that drops 98% of all messages. Even more disappointed by prominent community figures tweeting in an even more condescending tone: [here][1] and [here][2]. Here's hoping we all learn from this and question GHC RTS when it outperforms others with a naive implementation. [1]: https://twitter.com/ndm_haskell/status/771973259432697856 [2]: https://twitter.com/GabrielG439/status/771972827562016768
Actually just saw this comment. Realistically it took about a month to be comfortable. This was done by just reading stuff and writing code. Getting feedback from real humans can be a major benefit. If I could go back and give myself some advice on learning haskell, it would be this: Don't try to make it too complicated. You just need to focus on two things, writing simple functions that do a specific thing and writing basic types (records + algebraic datatypes go *very* far). Don't write your own typeclasses. 98% of the time you don't need a custom typeclass. To become productive, you will need to use typeclasses, just don't write a new one. You will need to know Functor, Applicative, and Monad to really cross the threshold. Here's the key thing: Don't learn them abstractly, learn them for specific types. For example...take a Maybe type and learn what Functor, Applicative, and Monad can do for that Maybe. Take a List type and learn what they can do for that. Then you can build up to something like Parser Combinators....which sounds very daunting, but is just using the same Functor, Applicative, and Monad patterns. [This talk on parsing stuff in haskell](https://www.youtube.com/watch?v=r_Enynu_TV0) got me to understand those typeclasses...but only after understanding them in the context of Maybe and List. 
Uh, do you seriously intend to question judgement of Simon Marlow? Moreover, RTS is not at fault here, even remotely -- the function used has the resultant behavior in its contract. The fault is with the benchmark employing that function.
I was sad to see this was inaccurate as well. Is the 98% figure correct?
This comment is a joke... right? (`Broadcast` "dropping" messages is not due to a bug in the RTS)
Yes it is a (not particularly good) joke, based on Facebook's motto ...
I did find one paper on the topic [here](http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=64F5A2A8FD777D18AFBD1A414650ADF8?doi=10.1.1.52.4840&amp;rep=rep1&amp;type=pdf). I didn't find anything about modelling neural networks with category theory, but at least from a Haskeller's perspective it seems like it wouldn't be worth it. The objects in Haskell are types, and you would have to have a type for every neuron in your approach. And I don't know enough about neuroscience so maybe these questions are naive: what would be the functors here? What would be the natural transformations? What would be the foldable and traversable objects? If there aren't any I don't think there's much to be gained from category theory. 
I think using this style of function composition really suits image processing pipelines; it's one of the things I liked most from elm. 
Ah, I got the joke, it just sounded like you meant the RTS is shoddily engineered :P
&gt; Stack doesn't work at all on tons of linux distros, BSDs or real unixes. Most people don't use BSD. Stack should work on linux and mac but it's kind of hard to get extensive testing done when there are so few users of slackware/manjaro/freeBSD
Ah, did you read the post before? It gives a bit of context. Yes, your `FixedLengthArray` would be right, but your `mkFLA` wouldn't work directly in Haskell because the `Int` you put in exists at the value level, not the type level. You'd probably write: mkFLA :: KnownNat l =&gt; [a] -&gt; Maybe (FixedLengthArray l a) which would return `Just` if the list is the proper length, or `Nothing` if not. Remember, with polymorphic functions, the *user* gets to decide what the values are instantiated as. Then you could write `mkFLA [1,2,3] :: Maybe (FixedLengthArray 3 Double)` if you wanted a 3-item list, or `FixedLengthArray 2` if you wanted a 2-item list, etc. You could also write replicate :: KnownNat l =&gt; a -&gt; FixedLengthArray l a Which would give you `l` of that item in an array, based on the `l` that the user asks for. Making fixed-length arrays where the type (length) varies at runtime is the tricker thing, and the article mentions two ways of doing them. The simplest way to implement would be continuation -style: fromVector :: Vector a -&gt; (forall l. KnownNat l =&gt; FixedLengthArray l a -&gt; r) -&gt; r Where you pass in a Vector, and you get a scope where you have `FixedLengthArray l a`, with the correct length in the type: foo = fromVector xs $ \(fla :: FixedLengthArray l Double) -&gt; ... and in the `...`, you have `l` in scope, which has your length. [hmatrix](http://dis.um.es/~alberto/hmatrix/static.html) implements things this way, and so does [linear](http://hackage.haskell.org/package/linear-1.20.5/docs/Linear-V.html) (with `reifyVector`), which has a fixed-length-vector type which is pretty much identical to your type here.
Yes, but there I lose the possibility of having multiple outputs. For example, with the sum-type structure I can have: output = [Value 1, Value2, ...] while in your case the result is fixed. I know that I would have to use an existential type for putting all that in a list, but I still do not know how to do that. I will update the question with more info!
&gt; I find the comments about stack traces hilarious, since they're still thinking in the format of "what was the state when everything went wrong?" rather that a more equational frame of mind where there was no state, only information flow. Yeah. And ideally if your "Either" returns an error, you should have made it return a string which actually tells you what went wrong.
thanks - maybe I was only holding for the possibility that the MVar could block (further broadcasts) until the listeners finished. But yes either way the MVar's semantics itself are consistent. (I think that would be a bounded channel of length=1)
I don't sense a condescending tone in those tweets. Thinking you're right and saying it doesn't mean you look down on people. But it's a shame the info was wrong.
Indeed. Calling out to a suitable C library is probably much less hassle in the long term.
The recording [is available now](https://www.youtube.com/watch?v=Qam6t9EN5SQ).
&gt; I think the lesson is just that benchmarking is trickier to do right than it seems. Also, that it is important to test that the thing do is actually the thing you want to do. 
I thought the Go client was validating replies, shouldn't have. Otherwise, how did they know the other implementations were behaving? ¯\\_(ツ)_/¯
(oh and i should note the Giry monad is the sort of granddaddy of categorical approaches to probability more generally: https://ncatlab.org/nlab/show/Giry+monad)
&gt; Do we get ω2 ? For cardinal multiplication of infinite numbers (×) = max.
The title of your post is unrelated to the content and as others have given good answers to the prior, I'll focus on the latter. Generally speaking, Neural networks are graphs with edge weights. Inference is done as a series of matrix multiplications (layers) with pointwise application of a non-linearity. The weights are simply entries in a matrix (or tensor) and do not change unless updated by some optimization process; the notion of non-determinism doesn't really apply there. You can get things which output probabilities by training softmax as the final layer with a loss such as cross entropy. If I wanted to apply category theory to cutting edge neural networks, I'd look at generative and variational encoders in terms of graphical models and what category theory has to say on graphical models (googling I see this: https://arxiv.org/pdf/1312.1445.pdf). I'd also try to look at what it means for a program to be smooth and differentiable in category theoretic language, looking for what connections could be made to areas such as topology. 
Omega is an ordinal though?
Pointing out a benchmark where Haskell performs well is not condescending in the least. Here's one of the tweets you linked: &gt; Awesome post on fast websockets (link: http://bitemyapp.com//posts/2016-09-03-websocket-shootout-haskell.html) bitemyapp.com//posts/2016-09… by @bitemyapp. An optimisation story of just letting GHC RTS do its thing. Not particularly condescending! Awkward misunderstandings happen, but this sort of tweet is absolutely nothing to be "disappointed" about.
There is also an issue here: https://github.com/jaspervdj/websockets/issues/124 I don't think anything is exceptionally wrong in `websockets`, but there are definitely some parts of the code that can be sped up a bit and improved in terms of memory usage.
I'd guess the odds of at least one of the other implementations also being wrong are pretty high. 
Hi /u/atc. Did you find a satisfactory solution to this? If not then feel free to ask me directly by [filing an issue](https://github.com/tomjaguarpaw/haskell-opaleye/issues) or by [emailing me](http://web.jaguarpaw.co.uk/%7Etom/contact/).
Yes I did thank you. Great API is Opaleye (difficult to get up and running though). If I find time I might write a tutorial to cover what I learnt as there were bits missing from those on github. Anyhow in to more Haskell:)
I don't know Haskell but I remember reading good reviews about "Learn You a Haskell for Great Good!" http://learnyouahaskell.com/chapters
List is not free, because you lose the tree structure that the Free monad would give you. State is not free, because you lose the ability to observe intermediate states. I don't know what your second sentence meant, however; can you clarify?
The function argument to `GetLine` will be changed by the `&gt;&gt;=` operation for `Free`. It is typical to choose the "default" value when it is isolated like this.
The paper define Free monad by data Term f a = Pure a | Impure (f (Term f a)) And a monad m is a Free monad if there're a functor f that make Term f x isomorphic to m x. And I cant find such a f (nor disprove the existence), hence the topic question.
Yes, it is. I shrugged at this conflation too. My guess is that the proposition author deems bringing aleph-nought to syntax an overkill. UPD: Referenced in the proposition paper by Conor McBride introduces 0, 1 and ω as literals of an algebraic structure and defines addition/multiplication over them.
I haven't seen this before, thanks for sharing! What do you use it for?
Nope, that's not what we are up to. (Pun intended.) In the `newtype State s a = State (a -&gt; (a, s))` definition, getting something and putting it right back is equal to `return ()` (assuming function extensionality), but in your implementation you get `Bind` for the former and `Pure` for the latter
Looking at the source, to me it looks like a listener can only ever actually reciever one broadcast. Is that really what's happening?
Faulty benchmark Gil Tene on properly benchmarking web latency: https://www.youtube.com/watch?v=lJ8ydIuPFeU 
Thanks for the answer! I didn't know about this trick with constraint kinds. So if I understood well, what I wanted to do is not possible. The problem is just interpreting some kind of XML, and depending on the block, using a different interpreter. But I wanted to know if it was possible to change the possible interpreters but just changing the ones that are imported, but not needing manually changing the main code. Well, I will have to go with the normal way :) Thanks!
&gt;What you've done is to show that by losing more structure, `Free MyStateF` becomes isomorphic to `State` Yes, but I believe the structure lost is an artifact of the encoding, not anything fundamental to free monads. There are [other encodings](http://comonad.com/reader/2015/categories-of-structures-in-haskell/) which may or may not have analogous additional structure. &gt;There seems to be something fundamentally different between `Tree` &lt;-&gt; `Free Pair` and `State` &lt;-&gt; `Free MyStateF`. The question is how. In the case of `Tree` &lt;-&gt; `Free Pair`, `Tree` happens to have a very similar structure to `Free`. Does that continue to be the case using the `Freer` formulation (what I have called `Free`)? Something worth pointing out either way is that `Pair` is an endofunctor on **Hask** in true `class Functor` style, but `MyStateF` isn't; rather, it's an endofunctor on the discrete category of Haskell types/functions (which is why you can't formulate `State` in terms of standard `Free`, which I think is what the OP was alluding to). EDIT: Compare the type of `gratis1` from the linked Comonad.Reader article to that of `runFree`. I bet that if you used the `Free1` encoding directly, you wouldn't see the same discarding of structure.
For something to be the free monad given a functor `f` then by definition, for all monads `m`, natural transformations from `f` to `m` must be in one to one correspondence with monad homomorphisms from `Free f` to `m`. Every monad homomorphism gives rise to a natural transformation, and every natural transformation gives rise to a monad homomorphism. Free -| Forget, where Forget is the functor from the category of monads over Hask to the category of functors over Hask, which simply forgets your monad is a monad, and only remembers the functor part. Despite the wall of equations, your `Free` is "too observable" for the monad laws so you have to start playing games where you say that "well, really you just quotient out all the stuff you don't like". The definition data Free f a = Pure a | Free (f (Free f a)) on the other hand, doesn't have that problem. It passes the monad laws with no craziness. natural transformations from `f` to the base functor for some other monad `m`, are in one to one correspondence with monad homomorphisms from `Free f` -&gt; `m`. So you can write a `(Functor f, Monad m) =&gt; (f ~&gt; m) -&gt; Free f ~&gt; m` and a combinator that takes `(Free f ~&gt; m) -&gt; f ~&gt; m` which acts as its inverse so long as the `Free f ~&gt; m` is actually a monad homomorphism. The key in the reasoning here is that everything is 'on the nose' equal. The structure isn't too fine grained and the laws hold exactly. The `Free StateF` you have is also too observable, even if you use the more correct Free. You can introspect on the number of Puts vs Gets, you write an 'interpreter' that adds one to the state every time it is `Put` and which violates the state laws. Your type is 'too big' as it admits homomorphisms from it that violate the state laws. So it isn't state. It is something finer grained. Again, you can play games where you hide the implementation details and move all the burden into limited interpreter functions, but by that same thinking State "is" Reader, as long as you don't use the extra bits, which tells you you're off the mark somewhat.
I'm a little confused. You've said that the defintion data Free f a = Pure a | Free (f (Free f a)) doesn't have the too-observable property that the formulation I gave (I'll call it `Freer` for disambiguation) has. But you've also said that `Free StateF` is too observable. Is that a result of the formulation of `Free` or the formulation of `StateF`? Or is it a result of the fact that the structure of `State` doesn't factor exactly into `Free` and some `f`? I would think that `Free` and `Freer` would suffer roughly the same issues with being too fine-grained; `Freer` is in some sense the `return`/`bind` syntax of a monad factored out into a datatype, while `Free` is the `unit`/`join` syntax factored out (relying on a bona-fide `Functor` instance for `fmap`). I'm not sure how mathematically rigorous that characterization is, though. If the problem is that `State` cannot factor exactly into `Free`/`Freer` and some `f`, is it still possible that there is another, less finely-grained expression of free monads into which it *would* exactly factor? I have high hopes for the `Free f a = Free { runFree :: forall g. Monad g =&gt; (f ~&gt; g) -&gt; g a }` formulation, but I haven't tried to work out the equalities.
I don't bother ordering my imports because I find it annoying enough having to jump to the top of my file to import stuff i'm using all the time. Having to add imports _in order_ would be even worse.
You can use [codex](https://hackage.haskell.org/package/codex) which generates all the tags for your current dependencies. In vim its just a matter of pressing Ctrl-] on a symbol.
It will be a very helpful tool
Can't he just look at the types and figure everything out? I thought Haskell didn't need documentation :\^)
&gt; If I find time I might write a tutorial to cover what I learnt as there were bits missing from those on github. If you file an issue with links to what is missing I can include it in the tutorial.
The issue you'll run into with StateF in the latter case where you use _that_ definition of `Free` is that nothing stops you from overly introspecting on the state of the `State` monad. You can write a natural transformation from StateF s -&gt; (,) (Sum Int): foo (Put _) = (,) (Sum 1) foo (Get _) = (,) (Sum 0) and pass it in to this `Free (StateF s)` as the interpreter. You'll get a monad homomorphism. The `Free` part is fine, but the result still isn't something you could obtain by using `State s`. It will extract information it isn't possible to extract from `State s`, so even with that `Free (StateF s)` everything is too fine grained. There is an embedding of `State s` into the obvious`Free (StateF s)` regardless of implementation of `Free`, and a retract for that embedding that forgets some of the information, leaving you just the interpretation as `State s`, but there is information lost -- the two definitions aren't isomorphic.
There is tooling, e.g stylish-haskell for that
I press a key and my imports are sorted and aligned. Emacs's haskell-mode has "align-imports" and "sort-imports" I hooked up a key to call both.
I usually have those two groups and alphabetize within the groups. 
Actually `round` will round to the nearest integer, or the even one if they are equidistant. http://hackage.haskell.org/package/base-4.9.0.0/docs/Prelude.html#v:round Have a look to `truncate`, `ceil` and `floor` for more control over the way the fractional part is trimmed.
I would love to have a more or less comprehensive "catalog" of monad morphisms, listing what groups of monads are known to be commutative, what kinds of embeddings there are, etc. 
https://en.wikipedia.org/wiki/Rounding#Round_half_to_even 
Really? What's the justification for that? Like OP, my understanding since childhood has been that `x.5` rounds up, because that creates a uniform grouping: `x.0` doesn't round at all, so `[x.0 .. x.499]` is equal in span to `[x.5 .. x.999]` to any non-infinite precision of additional 9's.
A blog engine with Git backend (similar to gitit, but more blogging-oriented)
but its not equal because you shouldnt count .0. It shouod start with .001. Therefore rounding .5 up or down is arbitrary
It's a common thing in the science community, done to avoid rounding bias. Normally, rounding to nearest gives an average round change of 0.05. This just balances it back to 0.
The [Haskell Wikibook](https://en.wikibooks.org/wiki/Haskell).
Read up on IEEE-754 rounding for the theoretical justifications behind the various rounding choices you can make for X.5 numbers. Haskell is doing round to half even rounding where even X rounds down and odd X rounds up, which is the IEE-754 default rounding mode. 
That would probably be nice. One issue with the sort of embeddings I'm tinkering with here, though, is that I can't prove anything interesting about them, because, as far as I can tell, `mtl` typeclasses come with no laws, so, theoretically, anything could happen on the receiving side of things. Maybe I should just start proving things, inventing laws along the way. 
In finance at least rounding rules are _very_ region specific.
That sounds pretty awesome, is the source code online and, if so, can you send me a link?
As others have explained, the choice is very deliberate. That said, Haskell should really have the rounding mode you are expecting. The mode that rounds .5 away from 0, since this is a commonly requested mode. 
I went to school in Germany and we learned that 0.5 rounds to 1. 
A similar style is provided by haskell-mode in emacs. There you can also do just the ordering without the indentation - for my taste the indentation part is very annoying. EDIT: Actually, I now see that `stylish` provides many *more* styling options than haskell-mode, set in the `stylish-haskell.yaml` configuration file. EDIT2: The import styling capability of `haskell-mode` appears not to be an integration of `stylish`, but rather a re-implementation of some of its functionality in Elisp.
Same in Russia.
What is normal? The way Haskell rounds is normal, it's what I learnt in school. It's also usually the default rounding for IEEE floating point. So it's pretty normal. I just wish Haskell also had the mode you consider "normal"
and Austria
Also Greece
So what countries outside of the US have round 0.5 = 0 in primary school then? So far everyone here is stating the same thing I learnt in the US that round 0.5 = 1. While I later found out that was "wrong" or rather biased, I'm curious what country, if any, teaches this.
Neat! Why might someone prefer this over [hindent](https://github.com/chrisdone/hindent), [stylish-haskell](https://github.com/jaspervdj/stylish-haskell), [hfmt](https://github.com/danstiner/hfmt), or [haskell-formatter](https://github.com/evolutics/haskell-formatter)? As far as I can tell, this is the only one that uses [ghc-exactprint](https://github.com/alanz/ghc-exactprint). Is that the main difference? 
At least, R agrees with Haskell ;-)
I have a comment on the [hindent thread](https://www.reddit.com/r/haskell/comments/5059gm/hindent_5_one_style_to_rule_them_all/) with a code sample that gets badly formatted. This same sample is used here as the first example and it's much better.
Lots of people are complaining about this because it's not what they expected, but nobody has put forth a case for why their expectations are better than what Haskell does. Anybody care to bite?
Fair enough. As I said in that thread, there's [an issue](https://github.com/chrisdone/hindent/issues/219) that, if fixed, would make hindent format that code a lot nicer. I just noticed a couple things in brittany's repo: 1. This project goal: "Be clever about aligning things horizontally." I think that's something hindent is explicitly trying to avoid. 2. [This comparison with hindent](https://github.com/lspitzner/brittany/blob/eead525c168644cb6f161b25508c53cdfe191ef9/Showcase.md). I see that brittany's output is nicer that hindent's (johan-tibell) output, but not by much. 
&gt; Name the other languages that do it. [Java?](https://dertompson.com/2009/04/25/default-rounding-mode-in-java/) [C, at least in some environments?](http://www.gnu.org/software/libc/manual/html_node/Rounding.html)
Yes, I agree that the Attoparsec parser in websockets should be replaced by a hand-rolled one. If you are willing to help out with that, you're more than welcome!
The point isn't just to come up with a well-defined answer that your math teacher will mark as correct. The point is about what you want to do with the numbers and what properties you want from the results in order to correctly solve whatever problem you're trying to solve. Arbitrary is doing things because it's what you were taught at primary school, not doing what you need to solve your problem, though of course any default is always a bit arbitrary (even the one that would make your primary school teacher happy) because there's no one problem it's specifically solving, and your primary school teacher is unlikely to ever see those results anyway. Basically, teachers teach what they teach because of curricula, which are because of history, politics etc and designed for people who will mostly grow up never needing to deal with serious math problems. The IEEE754 floating point standard was designed by real engineers who were trying to solve real engineering problems. Primary school is aimed at primary school children. "There is one correct way to do x and this is it" is almost always wrong, but it's an easy-to-teach starting point. In real life, there are often many different ways to handle a problem, none perfect, each making different trade-offs. This is certainly true for rounding schemes. That's a bit much for five-year-olds to cope with, though. A world where there's always "one correct way" may seem simpler, but in reality, it just means that when you can't use that "one correct way" because it doesn't solve your problem, you can't solve your problem at all - you may think you did because your building blocks were "good enough", but your pseudo-solution will fail, because the building blocks weren't quite the right shape to make the whole structure stable. Besides, the way I remember school, I was taught a number of different rounding schemes - just probably not at primary school. 
"[D]efault rounding for IEEE floating point" is sufficient to prove the point on its own. IEEE floating point is so pervasive it can be easy to forget it is not the only option for "something between integer and real".
It's possible, but I'm sure a version of `codex` compilet with GHC 7 can parse code for GHC 8.
I found that once I got used to the indenting, it became much easier to follow. It's like glancing over a table rather than reading a list of lines.
&gt; This project goal: "Be clever about aligning things horizontally." I think that's something hindet is explictly to avoid. I don't think that horizontal alignment always increases the readability. Sometimes I've the feeling that it's more about aesthetics, than anything else. Horizontal alignment increases the visibility of structure. Like in the pattern example of the announcement, it's easier to see the results of all patterns, the first and the second parameter, but IMHO somehow makes it harder to read each pattern by itself, because the eye has to jump more between the parts of a single pattern. Horizontal alignment helps for simple repetitive elements, where the eye might easily recognize errors, but the more complex it gets, the less this is possible and then alignment only makes it harder to just read the code. 
This is a cool idea! I like the amount of instruction this gives. If you want to cheat, you could just use [my wuss package](https://github.com/tfausak/wuss) :)
These seem little known: https://hackage.haskell.org/package/ieee-utils and https://hackage.haskell.org/package/ieee-utils-tempfix Perhaps these should be adopted more properly, or brought into base?
If you look on other posts in this topic, you'll see that one argument is that this changed alignment in other lines mess up with most diff tools and thus point changes in lines that haven't got a thing to do with the real modification...
Basically this. If a customer says "implement this algorithm", and that algorithm includes rounding numbers, then it's quite likely they will expect .5 to round up. Unless they have some other policy such as "always round in favor of the customer", in which case one wouldn't be calling the `round` function.
Here's the property I would expect. forall x. abs (round (x+1) - round x) == 1 This property does not hold for x = 0.5 under the "half rounds to even" scheme abs (round 1.5 - round 0.5) = abs (2 - 0) = 2
The examples are a bit out of date. Much has changed since.
This is indeed a nice architecture I didn't think about. I was going to go through with explicit dictionaries, but that is a nice option too, I will keep it on the hat, thanks!
Done!
It would be nice if, at some point, you could provide a repository with the project file for cabal new-build, so that the manual steps are reduced and make it easier to build for more users.
Do you have plans to fully format, and even rewrite lists and such to leading comma (not trailing comma) style, if for some reason the code has the elements separated with trailing commas?
Leading-comma is also nice in other cases like guards or comprehensions, e.g. [ f x | x &lt;- xs , f &lt;- fs ] Or in just any regular pattern match, equipped with a guard: case v of -- case 1 x | x == Frob , y /= 0 -&gt; ... -- case 2 x | x == Twiddle , z == True -&gt; ... -- case 3, fallthrough _ -&gt; ... 
That was quick! Released as [wuss-1.1.1](https://github.com/tfausak/wuss/releases/tag/1.1.1). Should be on Hackage soon. 
I agree that alignment can be overdone, and brittany probably does [in certain cases](https://github.com/lspitzner/brittany/commit/b03996e401cb9f67a094213dcaec9edad4e2e384#diff-186c0de764eb4ec6547852fc38e805a7R107) (the alignment of `DebugConfig`, `LayoutConfig`, `ForwardOptions`). But in [many](https://github.com/lspitzner/brittany/commit/b03996e401cb9f67a094213dcaec9edad4e2e384#diff-46dc65d140e24c1a54e3ead634926751R1304), [cases](https://github.com/lspitzner/brittany/commit/b03996e401cb9f67a094213dcaec9edad4e2e384#diff-46dc65d140e24c1a54e3ead634926751R159) i find it preferable. Also it already is possible to disable alignments via config (but a corresponding commandline flag is missing atm). (It would not be too much work to selectively disable more cases; i already [have some conditionals that currently prevent overflowing](https://github.com/lspitzner/brittany/blob/master/src/Language/Haskell/Brittany/Backend.hs#L438)
I think it would make it easier to integrate with other tools, if we didn't have to specify `-i FILE` but could `FILE` instead. For example vim's autoformat would be easier to point at brittany that way.
I think horizontal alignment can often look better, but I don't think it's worth the maintenance overhead. To quote [the Elm style guide](http://elm-lang.org/docs/style-guide#types): &gt; Changing the name Boolean ever will change the indentation on all subsequent lines. This leads to messy diffs and provides no concrete value.
&gt; $ python &gt; &gt;&gt;&gt; round(0.5) &gt; 1 Fixed in Python 3: $ python3 Python 3.4.2 (default, Oct 8 2014, 10:45:20) [GCC 4.9.1] on linux Type "help", "copyright", "credits" or "license" for more information. &gt;&gt;&gt; round(0.5) 0
I don't think these packages actually work in general. `fegetround` and `fesetround` are thread-local in a process, which means that using them in any given Haskell thread will, essentially, change the rounding mode of a random OS thread at will, and only lightweight threads which are put on that particular capability will see this change (and as you may get scheduled on and off a capability over and over, that'll be a really hard bug to find!) You could try to do something clever and assign a thread to every individual capability and perform this operation; but these days the runtime can scale capabilities dynamically, making this much trickier. I think to really fix this, you'd have to store the FPU environment inside the TSO of a particular lightweight Haskell thread, and set it upon context switch. But that will increase the cost of every single thread ever created, too.
Good point. There's probably something better / more complex that can be done with the API to make it safer. But the existence of thread-local processor state like this sort of raises the question of what sort of more integrated runtime support would be necessary to provide a simpler API. (One could use these in conjunction with `runInBoundThread` or the like at least...)
Right. This is one of the reasons why brittany formats longer type signatures in the way that it does, with `::` indented in the next line: Fewer changes when renaming the function. I still like alignment in other places. But if a majority prefers no alignment, changing the brittany default config should be a one-line diff. 
Both hindent and brittany look pretty good here. The placement of the first comment in the hindent version is the only part that I'm not happy with. 
Do you know if Athens has any Haskell meetups?
Right! I already noticed your fancy one-file test approach; I had thought about something similar before, too. Managing longer series of quasi-quoted blocks is an annoying overhead.
Not for me. I learned from xmonad how much more efficient I can be by tiling the screen with reasonable 80-column windows rather than squandering most of my valuable screen real-estate by allowing long lines. So this kind of wasteful import indentation causes massive line wrapping and makes my import list into an unreadable glob of spaghetti.
Apparently Sweden, England and Canada, so far. And even there, perhaps only in high school and not in the lower grades. So yes, less than I thought. But significantly, this list includes the home country of the original authors of the Haskell `round` function.
Brittany currently uses the delta-positions provided by ghc-exactprint to retain the relative position of any comment to the associated syntactic element. Horizontally these deltas are relative to the indentation level, so comments both move if the indentation-level is changed by reformatting (`do` on same line vs new line and such) and remain where they are (even if somewhere past the indentation level). [This particular comment](https://github.com/lspitzner/butcher/blob/4b7f8681e75b6a8c10bd5fed9ad33439ade9035e/src/UI/Butcher/Monadic/Flag.hs#L202) would be a good example, but apparently i somehow broke `forall` so I cannot claim that brittany acts as identity on that signature currently. (Yep, more testcases..)(the comment handling is indeed "correct" though in this instance.) I do some potentially fragile stuff to move comments while transforming trailing-comma lists to leading-comma and in some other special cases. All in all I will completely agree that comment handling is hard. Delta-positions may be a good approach in general, yet I know of at least two special cases where brittany has slightly "buggy" behaviour similar to the one in hindent which I (unintentionally) pointed out in the showcase.
It's. released. I have a copy. Arrived yesterday.
C# rounds to even with `Math.Round`. Java rounds to even with `Math.rint` (but not `Math.round`).
The free monad isomorphic to lists should be `(,) a` with `()` as unit if i remember correctly.
My advice would be to try really hard at first to ignore the names, and ignore all your category theory knowledge until you get up and running with the basics. Your confusion is sadly common and I don't know of great resources. I _can_ point you to principled stuff about the mathematical understanding of Haskell semantics, programming language semantics in general, etc. as well as stuff that explains how people tend to translate categorical stuff into an "internal" Haskell setting. But all that tends to presuppose some experience with Haskell as such. If you _do_ want to go the "other way" I think the way to start would be with the correspondence between cartesian closed categories and the lambda calculus, and use that to lever up to a good understanding of the typed lambda calculus as such (maybe via Barendregt -- this, for example or something else: http://ttic.uchicago.edu/~dreyer/course/papers/barendregt.pdf). Then you can look at how Moggi introduced monads to model the semantics of "side-effects" (https://core.ac.uk/download/pdf/21173011.pdf). And you can learn Applicatives via the original paper that introduces the connection to monoidal functors: http://www.staff.city.ac.uk/~ross/papers/Applicative.html If you use irc, i'd encourage you to hop on #haskell-lens on freenode where some more mathy types hang out and welcome semi-off-topic discussion on categorical aspects. That said -- on some particulars (assuming I can follow your issues correctly -- it is a bit densly presented) . Every applicative (monoidal functor) is of course a functor. But even in category theory generally, it is the rare functor that is monoidal (as in fact we don't even necessarily have two monoidal categories on either side to begin with). From a categorical standpoint, the bind operator is less common, but you should be able to "desugar" it into join to be more comfortable with it. Pure and Return should of course both just be called "unit". And it is _good_ to be precise in distinguish between F(a -&gt; b) and F a -&gt; F b, as in a general categorical setting they will only correspond when F is full and faithful. On example 2, it is just a restatement of the formula in the typeclassopedia given above: pure f &lt;*&gt; pure x = pure (f x) So you should understand that as saying that application commutes with a monoidal unit -- i.e. the functor is _closed_. In this informal setting, given an applicative functor f, a "non-effectful thing" of type `f a` is a thing of type `f a` which can be constructed by calling `pure a` for some value of type `a`. But I agree that the "intuitively" thing is not much more intuitive than the equation itself, and perhaps less so for some :-) 
By the way, there are probably lots of other resources to recommend, but I had success back in the day once of showing a mathematician "look, some discussions of monads in haskell aren't _entirely_ without reference to actual math" by referring him to the article on monads here: https://wiki.haskell.org/wikiupload/8/85/TMR-Issue13.pdf You may find it of some use -- can't promise anything of course!
Well, this is informative. I didn't imagine a Category Theorist would have trouble, as programmer do, in getting to grips with Haskell. I've never heard of an "Haskell for Category Theorists", I can only try to suggest material for programmers which may, or may not work. The ["Haskell Programming from first principles" book]( http://haskellbook.com/ ) is very detailed and at &gt;1000 pages it really goes through everything. It may be boring for you, or it may be necessary to puzzle out the differences between what you know and what Haskell is like. It's not free, though. Adding another language may confuse you further, but I generally suggest ["Purescript by Example"](https://leanpub.com/purescript/read) in learning Haskell, as well. Purescript is a slightly simpler language, due in part to not being lazy, and also because it cleaned up many odd bits of Haskell which are due to historical reasons and backwards compatibility. Good luck, keep up posted :)
You could probably anonymous the code couldn't you?
thank you so much. You are correct in that my post was a little unclear and terse. It's late at night and I'm typing on my phone. I will take the time to properly respond tomorrow. Regarding F(a-&gt;b) vs Fa-&gt; Fb, ... I just meant that a functor, applied to a morphism X-&gt; Y is just a morphism from FX to FY... 
Not really. Wikipedia says: &gt; The origin of the term bankers' rounding remains more obscure. If this rounding method was ever a standard in banking, the evidence has proved extremely difficult to find. To the contrary, section 2 of the European Commission report The Introduction of the Euro and the Rounding of Currency Amounts[22] suggests that there had previously been no standard approach to rounding in banking; and it specifies that "half-way" amounts should be rounded up.
Search Internet for *What Every Computer Scientist Should Know About Floating-Point Arithmetic*. 
You know much more category theory than I do -- I'm more of a geometric topologist -+ but I asked something similar about a year ago. The comments might be useful to you: https://m.reddit.com/r/haskell/comments/3drgl3/haskell_for_mathematicians/
And South Africa
It sounds like you are wanting a Haskell to C++ Foreign Function Interface(FFI). [Haskell doesn't have one](https://wiki.haskell.org/Cxx_foreign_function_interface). [However, we do have a C FFI which is likely good enough for your purposes](https://wiki.haskell.org/GHC/Using_the_FFI).
Rounding modes can vary. There are different ones used in different industries.
I really envy you for having such a solid mathematical background, with which learning HoTT would be much easier. However IMHO haskell has no too much to do with Category theory and stuff. Yes, there is functor, monad, monoid, etc; but it's really just some borrowing of concept and does not require that much math. Haskell is a programming language more for real world use than for research. But I agree that haskell is maybe the most math-student-friendly language, yet still capable of doing practical programming(and doing a better job than many other practical PLs). For learning HoTT, I think a dependently typed, consistent language, like coq/agda/etc, would help you get started on the traditional type theory part.
I'm not taking a side on this horizontal alignment, but as far as 'messy diffs' go: many diff tools (at least the ones I've used, and including `diff`) have settings for 'ignore whitespace differences' or 'ignore changes to whitespace length', and similar options.
Right now I'm leaning towards adding a Tz string parameter to each function all the way down to parse', changing my ZonedTime instance to LocalTime, then making a pattern match for LocalTime in parse', and finally adding the timezone and creating the zoned time there.
I recently added [this pull request](https://github.com/haskell-servant/servant/pull/583/files) to Servant which allows you to specify the format string a Data.Time type should be parsed/formatted with - I feel something similar could easily be used here. I've considering whether this would be generally useful as a separate library with support for it in Servant. Edit: Thought I should add an example: x :: FTime "%Y-%m-%d" Day is a Day which is parsed and printed in YYYY-MM-DD format. Since I've been looking at the Frames recently, maybe I could add this.
Render your document in Postscript, then convert the output to PDF using `pstopdf` from [poppler](https://poppler.freedesktop.org).
You really want the rounding mode to be part of the type, but there are performance problems. The old Alpha processors had different instructions for different rounding nodes, but on many modern x86 processors changing the rounding mode is really slow since it involves changing FPU state and requires emptying the pipeline. From what I understand, modern x86 processors are doing better in this respect. Being able to use different rounding modes is important, e.g., when doing interval arithmetic. 
To play the devil's advocate role: isn't the solutions too brittle and complex for the issue described in the motivation part? If "using one module per record is often impractical" (why? is it because of module==file?), then lets fix module system to make it practical to use one module per record. Or we can let user disambiguate record selector using record type name as a qualifier. The drawbacks of the `OverloadedRecordFields` (runtime and compile-time performance) are pretty major IMO, so we need a good reason to introduce it. Disambiguating records is not a good reason for me. (To be clear, the concern is about motivation part, not about the whole proposal. I really appreciate your work, thank you for it).
Yeah, Python 2 got it wrong, but they fixed that in Python 3. :)
I think the original author of the `round` function is Joe Fasel and he is American. He based his design a lot on what Scheme does. Both Joe and the Scheme designers are careful people interested in numerical computations, so it's unsurprising that they used the less biased `round`.
I know this doesn't give us polymorphic record updates, but does it allow first class setters by chance?
&gt; So you should think of F(a -&gt; b) as F of the object representing functions a -&gt; b, not F of one particular morphism a -&gt; b. To denote the later, we would say let (f :: a -&gt; b) in F(f). Right. F(a-&gt;b) is F of the hom-object, and &lt;*&gt; is somehow the operator that maps F(a-&gt;b) into the set Hom(Fa,Fb). pure is just the functor F --- why do we have to call it pure? --- in that pure a is F a So, if I have pure (I can apply the functor to objects), I should be able to apply it to the object (a-&gt;b). What is pure (a-&gt;b) and how does it differ from F(a-&gt;b)? ... you see, I'm still not fluent in these notations and I have the suspicion that there is something lurking in the background that I am misunderstanding conceptually. Another complication is the implicit use of the hom-tensor adjunction ('Currying'). If I have maps a-&gt;b-&gt;c and want to apply a functor F, should I get maps F(a,b) -&gt; Fc or maps Fa -&gt; (Fb -&gt; Fc) or even Fa -&gt; F(b-&gt;c)? I'm assuming, that all this dance with applicatives and &lt;*&gt; operators is there to untangle this for me. I'll keep reading.
&gt; Even an IEEE standard for it :) IEEE 754 is the standard for floating-point numbers in general, not specifically for banker's rounding, and it defines at least 5 rounding modes. round ties to even is just the default (because it's unbiased)
That's simply not true. Take a random number from 0 to 10 inclusive. Divide it by 2. Round it, and we should expect an even distribution across the numbers 0 to 5 in increments of 1/2. Let's run this in some python that works in 2 and 3: from random import randint print("Rounding 0.5: %d | rounding 1.5: %d" % (round(0.5), round(1.5))) total_samples = 100000 rounded = list(map(round, (randint(0,50)/2.0 for i in range(total_samples)))) print("total that are 2:", rounded.count(2)) print("total that are 3:", rounded.count(3)) We should expect that 2 and 3 come out the same number of times, right? Let's try python 2 first. Rounding 0.5: 1 | rounding 1.5: 2 ('total that are 2:', 3838) ('total that are 3:', 3793) Ok, looks pretty even, if there's a bias it's small. Now python 3: Rounding 0.5: 0 | rounding 1.5: 2 total that are 2: 5793 total that are 3: 1915 This should not be surprising as 1.5, 2 and 2.5 all round to 2, whereas you need to be right on the nose for 3 to stay as it is. It's a different type of bias, sure, and it will work better for combined averages, but let's not say flatly that the first "introduces bias" and imply the second does not. [edit - reread and made it a bit nicer]
I'm a numerical analyst, so I should probably take this opportunity to ask you questions about category theory, instead of trying to answer a question. But, if I have understood correctly, I think the answer to the question of why pure exists is more about polymorphism in Haskell than it is about category theory. The function pure is part of a type class that provides an interface that allows you to write functions that are polymorphic in the type of the applicative. As a silly example, since pure has the polymorphic type `pure :: Applicative f =&gt; a -&gt; f a`, if you decide to reimplement an fmap for applicatives you can get a function that works for any applicative: appMap :: Applicative f =&gt; (a -&gt; b) -&gt; f a -&gt; f b appMap f a = pure f &lt;*&gt; a If you use a particular F, you get a less general type: listMap :: (a -&gt; b) -&gt; [a] -&gt; [b] listMap f a = [f] &lt;*&gt; a and will have write that function over again if you want to have a function that applies to a different F.
I didn't say it doesn't work with cabal, if that's how you interpreted my comment, but I don't think you did. Looking at your patch, I see some issues with it: 1. use got git: protocol will not allow anonymous cloning of the repositories, unless stack is smart and rewrites it, or you have rewrite rules in ~/.gitconfig 2. the repetition/expansion of dependencies doesn't look too practical to me 3. who or what updates the revisions which are hard-coded?
If I can reduce it to something I can share, I'll do. But it really wasn't anything special. I'll see if I can reproduce the same behavior in already public code.
Hmm, what compelled you to write it in C and only provide a thin Haskell wrapper? These kinds of simple counting loops are easy enough to write quite efficiently in Haskell, and even optimize to be *very* efficient if your use case demands it. EDIT: It seems that for the stack use case a simple idiomatic Haskell implementation would be more than sufficient. And this way, you are adding an FFI dependency to stack. Wouldn't it be highly desirable to avoid that for a basic infrastructure tool?
Wow, I wish we had more of these discussions. Coming to Haskell from the opposite direction compared to the op I still find your post very interesting. Whenever CT vs. Haskell discussion arises, it quickly degenerates into "CT is irrelevant to Haskell because of seq and bottom etc.". I see how that point is important, but I'm annoyed by the fact that every discussion degenerates into that.
Out of curiosity, can `hint` mandate `-XSafe`?
Re “why in C”: since the algorithms themselves are imperative, I find it *much* easier to write them imperatively. In fact, I implemented the algorithms previously in Common Lisp, where I could actually mutate values, then in Closure, where I *had* to use `core.matrix` library that also allowed mutations. So I considered two options: 1) use ST and mutable vectors from `vector` package for distance vectors, 2) write the algorithms in C. 2 performed better and so I chose it. Someone just kindly suggested on GitHub that it I could use `alloca` that makes the C version even faster (25% faster then current version, I measured). I consider doing that (and adding a warning about length of input values), but I haven't decided yet. As for “FFI dependency”, well I have to first ask why FFI dependency is such a bad thing is your opinion (the code comes with the package, it's not a binding to some external binary file!). Note that there are *quite a few* packages with parts written in C Stack already depends on, looking at the `stack.cabal` file: `cryptohash` (hmm, why not `cryptonite`?), `zlib`, and I think there should be a couple more. So using `text-metrics` in Stack didn't make anything worse. 
&gt; If you want to anonymously clone those dependencies, you can do so however you like. Then you can change the location to something like ../ghc-exactprint (i.e., a local path instead of a Git remote). Right, but why did you go from a remote to a local and not https:// or the anonymous git:// protocol?
Ezyang already has a [backpack patch for cabal-install](https://github.com/haskell/cabal/pull/3672). 
Ah I see part of the issue. You need to lear to separate types and terms. Types are the objects in `Set`, and terms are the elements of sets. `F(a)` means to use the functor `F` to find an object in `Set` for `a`. `pure a`, however, is a *term*, and so is `a` in this context. `pure` is not "just the functor."`pure` is a morphism of type `a -&gt; F a`. Functors don't necessitate the need of such a morphism. `pure (a -&gt; b)` isn't a real line of code. `pure` is a term, and `a -&gt; b` is a type. You can say `pure f`, where `f` is a particular morphism `a -&gt; b` (and thus a term), which will have the type `F(a -&gt; b)`. In terms of currying, let's say we have a morphism `g :: A -&gt; B -&gt; C`. `F(g)` doesn't make sense, because `g` is a term, not a type. `pure g` will be a term of type `F(A -&gt; B -&gt; C)`. `pure g &lt;*&gt; a` will have type `F(B -&gt; C)`. Ultimately, just read up on the distinction between types and terms. There are "dependently typed" languages (like Idris) that make the distinction very fuzzy, but Haskell isn't one of these languages. The barrier between types and terms is nigh impenetrable. And it's important to understand that terms are elements of sets, and that the set they are an element of is called its type.
Absolutely. A full-featured faithful PDF library would be great, and rendering via such a library is much better. But in general, although PDF has supplanted PS, PDF is an arcane format, and solid library support for it on various platforms has been very slow to develop. Whereas PS happens to be a nice little programming language, very well specified and documented. So in practice - if you just want to render content and not use various PDF-only dynamic features - it is often fastest and easiest to go via PS, even if that inherently creates certain awkwardness. Up to you - it's just another option to keep in mind. EDIT: I once implemented a solution involving many gigabytes of PDF content this way. There are issues, but speed is not one of them.
I never quite understood this. I know there's all these problems around mangling, but RCpp seems to do the job seamlessly: http://www.rcpp.org/ How are other languages able to pull this off?
Ha, very interesting. So I was wrong on both points.
Not in this proposal, as it covers accessors only, but the plan is to do something similar for setters as well. The idea is roughly to generate pseudo-instances of a class like this: class SetField (x :: Symbol) s t b | t -&gt; b, s b -&gt; t where setField :: b -&gt; s -&gt; t This could then be used to interpret overloaded labels as (various flavours of) lenses (possibly using a combinator).
What are those expressions?
Maybe it would be good to start discussing possible topics for the Hackathon? 
For typo correction you usually don't need to compute the full levenshtein distance, you can cut off distance computation at a certain bound (usually you don't care about the distance as long as it is higher than 3-4 and thus the candidate isn't a plausible typo-fix). Exposing an API that lets you just a "cutoff" parameter can save a lot of computation time. I think that your approach to get speed is not focusing on the parts that matter most. Implementing an inefficient API in C may get you faster code than that the same inefficient API in Haskell, but writing Haskell code would let you more easily provide more general implementations, for example working on collections with random-access that are not `Text`. You could also (in either C or Haskell) avoid the unnecessary code duplication of implementing the same logic twice (supporting transpositions or not, etc. could be a parameter).
Ah, very nice, thanks for all your work!
Why are the hashes inside input elements? That seems to be the cause of the problem, but I imagine there is a reason for it, so just removing the inputs may create another problem.
Indeed, I'll add ability to specify cut-off distance in the next release. &gt; I think that your approach to get speed is not focusing on the parts that matter most Generality does not matter at all for my use case, if I make it polymorphic I'll lose a lot in terms of speed. 
Thanks for the link, I'll benchmark that code and compare.
In self project I use wkhtmltopdf http://wkhtmltopdf.org, you just generates html with html-blaze and convert it with former utility. 
Here's a roundabout way of getting what you want: I wrote a Haskell binding to the [FLTK port of Webkit](https://github.com/deech/webkitfltkhs) and it includes a function called [`executeJS`](https://htmlpreview.github.io/?https://raw.githubusercontent.com/deech/webkitfltkhs/master/doc/webkitfltkhs-0.0.0.1/Graphics-UI-FLTK-LowLevel-Webkit.html#g:10) which takes a `ByteString` containing arbitrary JS and hands it over to WebKit. Unfortunately there is no direct way of getting the result back since it doesn't return anything. However there is a [`getValue`](https://htmlpreview.github.io/?https://raw.githubusercontent.com/deech/webkitfltkhs/master/doc/webkitfltkhs-0.0.0.1/Graphics-UI-FLTK-LowLevel-Webkit.html#g:10) function that takes coordinates to a DOM element. So maybe you could embed the JS in a dummy DOM element and read it back? Like I said it's awkward. Also it is possible to run the Webkit widget invisibly in the background using [`webviewNewWithoutGUI`](https://htmlpreview.github.io/?https://raw.githubusercontent.com/deech/webkitfltkhs/master/doc/webkitfltkhs-0.0.0.1/Graphics-UI-FLTK-LowLevel-Webkit.html#g:8) so you now have production grade JS engine running in your app and your user never has to see the shenanigans. If this is a popular use case I can talk to the author of the Webkit port and ask about exposing something a little nicer to work with. 
C++ has its warts, but it also has a huge ecosystem for all kinds of numerics / scientific / graphics / high performance applications. post-C++11, C++ is going in a Haskell-y direction. Both Haskell and C++ have been around for a long time. RCpp was mostly a one person project (granted, a very capable person). To me it doesn't add up that there's not a solution for this, but I suppose that just exemplifies how we need a bigger user base to tackle some of these hairier problems. edit: we're slightly off topic as the OP seems more interested in calling into Haskell from C++ than the other way around. Although in most scripting use cases you do want to script C++ functionality within the interpreter.
hrm even in python you probably want to make this a function. Assuming you do that, you can keep it scripty-ish with the constants as top-level definitions and turning the while loop into a recursive function on delta_t that accumulates results into a list that gets printed. 
there are some things that are hard to do in Haskell - like going on ARM/Android/IOS is not mature at all, also if you have requirements for a real-time-system the garbage collection might be a show stopper. If you have to do some interop with java I also would not recommend haskell. But other than that - just use it and maybe read http://www.haskellforall.com/2016/02/state-of-haskell-ecosystem-february.html
[removed]
Because of the associativity of the `-&gt;` operator in types, `a -&gt; b -&gt; c` is really `a -&gt; (b -&gt; c)`. So just as `fmap (g :: a -&gt; d)` (i.e. fmap of a function named g of type `a -&gt; d` will yield a function `f a -&gt; f d`, we can substitute `(b -&gt; c)` for `d` and see that `fmap (g :: a -&gt; b -&gt; c)` will yield a function `f a -&gt; f (b -&gt; c)`. You can experiment yourself in ghci to see such things: Prelude&gt; :t fmap (undefined :: a -&gt; b -&gt; c) fmap (undefined :: a -&gt; b -&gt; c) :: Functor f =&gt; f a -&gt; f (b -&gt; c) (here we're using `undefined` as a polymorphic placeholder value that we can then ascribe a given type to, just to observe what happens in the type calculations).
Oh yes, this is a very different question, on which I think the work and way forward is a lot more scattered and partial and less clear :-) Cubical theories take us a certain degree there (https://www.math.ias.edu/~amortberg/papers/cubicaltt.pdf and also its successor https://arxiv.org/abs/1604.08873). But they still draw (forgive my hand-waving here) an important distinction between application (which though it has higher-dimensional ramifications occurs at base on the set-level) and higher-dimensional substitution (transport along paths/equalities/homotopies, etc). So we wind up with models in (oo,1) categories, not full oo-categories -- i.e. we've completed the groupoid structure induced by equality, but don't have a higher directed path structure. Moving the bar in the other direction takes you to forms of directed type theories, which have been developed in correspondence to directed algebraic topology. As with full infinity category theory in general (beyond the (oo,1) case), this stuff scares and confuses me to no end. That said, here are a few references: https://www.cs.cmu.edu/~rwh/papers/2dtt/mfps.pdf and https://people.cs.kuleuven.be/~dominique.devriese/ThesisAndreasNuyts.pdf There may be another way to treat an `n-lambda-calculus` that don't follow this path but instead operate with some sort of quotienting of omega-categories or the like -- something highly speculatively along these lines: http://drops.dagstuhl.de/opus/volltexte/2015/5166/pdf/21.pdf Anway, this is all pretty far afield from getting up and running with the basics, I suppose :-)
You can use the GHC API to test if a module is safe (`isModuleTrusted`) before adding it to the interactive context. I couldn't figure out how to get it to _not_ print warnings to stdout, though. GHC API sucks.
Please do report your findings, I'm pretty interested in this as well.
Here is a slight variation on your python code which shows you how you might think about this in Haskell. Keep in mind that a Python iterator is in some ways similar to a Haskell list. Another hint: the enumerator `gen_vals` should remind you of the Haskell function `iterate`. import math import sys R = 6.371e6 # Average radius of Earth (m) M = 5.97237e24 # Mass of Earth (kg) rho = 5.514e3 # Average density of Earth (kg/m3) G = 6.674e-11 # Newton's Gravitational Constant (Nm2/kg2) k = 4.0 * G * rho * math.pi / 3 delta_t = float( sys.argv[1] ) def next_vals(t, a, v, x): next_a = -k * x next_v = v + a * delta_t next_x = x + next_v * delta_t next_t = t + delta_t return (next_t, next_a, next_v, next_x) def gen_vals(t0, a0, v0, x0): t, a, v, x = t0, a0, v0, x0 while x &gt; 0: t, a, v, x = next_vals(t, a, v, x) yield (t, a, v, x) for t, a, v, x in gen_vals(0.0, -k * R, 0.0, R): print t, a, v, x 
My particular use case is NLP. I'll need to do a lot of API connecting, etc. My general reason for asking this question is that I've heard a lot of people say that some things that seem really obvious in other languages are sometimes really hard to penetrate in Haskell, the classic example being Monads. Another reason is that I'm considering using Haskell for a hackathon and don't want to get in a situation halfway through where I don't know how to do such-and-such this properly in Haskell.
`levenshtein` is C, `levenshtein2` is the code from that blog post, adapted to work with `Text`, number after slash is length of inputs: benchmarking levenshtein/5 time 59.80 ns (59.67 ns .. 60.00 ns) benchmarking levenshtein/10 time 161.6 ns (161.3 ns .. 162.1 ns) benchmarking levenshtein/20 time 605.5 ns (604.4 ns .. 606.6 ns) benchmarking levenshtein/40 time 2.555 μs (2.550 μs .. 2.560 μs) benchmarking levenshtein/80 time 10.03 μs (10.02 μs .. 10.05 μs) benchmarking levenshtein/160 time 39.61 μs (39.51 μs .. 39.72 μs) benchmarking levenshtein2/5 time 333.3 ns (332.4 ns .. 334.2 ns) benchmarking levenshtein2/10 time 943.1 ns (940.8 ns .. 945.4 ns) benchmarking levenshtein2/20 time 3.174 μs (3.169 μs .. 3.180 μs) benchmarking levenshtein2/40 time 13.20 μs (13.11 μs .. 13.32 μs) benchmarking levenshtein2/80 time 72.76 μs (72.24 μs .. 73.31 μs) benchmarking levenshtein2/160 time 1.215 ms (1.195 ms .. 1.234 ms) And I think it's pretty impressive performance for that recursive algorithm. 
Delay and force are so slow that you can probably empty the FP pipeline instead. 
Considering the benchmarks you added, I'm inclined to agree with you for now.
Monad transformers, especially `mtl`
How good is it ? I'm generating html (using lucid) to be printed but only Firefox is correct (ie the printed page looks similar to the screen). Chrome and Edge gives strange result.
The nice thing about implementing it in Haskell is that it allows `levenshtein :: Eq a =&gt; [a] -&gt; [a] -&gt; Double`. This is afaik how hoogle implements its type search.
Can we nominate people on their behalf?
It is good at all, I used wkhtmltopdf about 3 years in many problems when i need to generate PDF and have no problems ever. It uses WebKit engine and it is pretty similar to Firefox Gecko.
There are a lot of good answers already so I will just add the following, since it was a big help to me in connecting the two: Skolemize everything. This gives one a sense of what witness functions need to exist, or be constructed to establish some kind of entity from category theory in the context of a functional program.
Hi all, I'm the one who started this. This is **very much a work in progress**. It is not done, and I was too busy to work on it during my Masters thesis. **Contributions are always welcome!** PRs and such can be made [here](https://github.com/learnyouanelm/learnyouanelm.github.io) Basically, I got tired of having people directed to LYAH on the Elm mailing list to learn FP, since a large part of it is devoted to laziness and typeclasses, neither of which apply to Elm, and are likely to confuse people who are coming to FP from the JS world. One of the biggest issues right now is the mix between in-browser and REPL instructions. Ideally, the entire book will be run in-browser, so that people can try Elm without installing it. But, the in-browser editor isn't great for querying the types of values. So, some parts use the REPL.
Cool! I've used the hamming distance in my work so seeing it supported "natively" is really convenient!
&gt; I've heard a lot of people say that some things that seem really obvious in other languages are sometimes really hard to penetrate in Haskell, the classic example being Monads. You don't really *have* to use monads for anything except IO in the beginning. I think aside from this Haskell was quite accessible to me when I was learning. As for using it at a hackathon, it depends how long you have to learn/prepare :) 
Probably not for a hackathon though. 
The online exercises are a great addition. They're currently tuned for an assessed course at UPenn, but a variation in which the code can be checked automatically would IMO propel this course to the next level for those wanting to learn Haskell independently. I think you'd need to provide more structure in the exercises to accomplish this, as you can't just compare `Picture`s for equality.
Fixed, but it might take a minute for the Jekyll to re-render
I would update hpdf to use Double and see if things just work. Also post a bug report on hpdf bringing up these issues.
I often feel like the success of a language or paradigm depends on having different types of learning material out there too. Keep up the good work! I look forward to reading it more in detail.
stackpack?
I have a question related to this. I'm working on my own [time library](http://hackage.haskell.org/package/chronos) right now, and for getting the system clock, using the ffi is required. I will eventually need to use my library with ghcjs. Is there some documented IFDEF magic to change this out with the javascript ffi when compiling with ghcjs? I know that GHCJS itself includes shims for just about everything in the haskell platform, but has anyone done something like this in a library itself.
I want to know more about side effects. In ClojureScript we can handle side effects explicitly but not that hard. It's a bit hard in Elm. So I want to learn more about that.
If you're developing a time library, you should take a look at [NodaTime](http://nodatime.org/) for inspiration. It nicely removes some of the little gotchas that come with most time libraries.
I've got 3 days to prepare. 
Ah. I wouldn't say Haskell is any worse than learning any other language in 3 days, but at the same time I probably wouldn't pick up clojure in three days either. Do you have any experience with functional programming?
&gt; I don't yet know which parts of these constructions I am to think of as 'effectful' Try reading "Notions of Computations and Monads" by Eugenio Moggi.
I have some with JavaScript. I've used libraries like Ramda and Cyclejs. But I know how different the two languages are. I've decided I'll use it for some hackhaton/side project in the future for sure, I really love the language, but for now I don't think I have enough time to learn it properly enough. 
Ah okay. Your plan sounds good then.
Doesn't directly answer OP. ---- There is a field `maxSize` (of the type of arguments [`Args`](https://hackage.haskell.org/package/QuickCheck-2.9.1/docs/Test-QuickCheck-Test.html#t:Args)) worth looking at. Its default value (100) limits the maximum size of — say — lists that *QuickCheck* will generate (via the `Arbitrary a =&gt; Arbitrary [a]` [instance generator](https://gist.github.com/Icelandjack/5afdaa32f41adf3204ef9025d9da2a70#instance-generator)): prop_lessThan100 :: [()] -&gt; Bool prop_lessThan100 xs = length xs &lt; 100 &gt;&gt;&gt; quickCheck prop_lessThan100 +++ OK, passed 100 tests. &gt;&gt;&gt; quickCheckWith stdArgs { maxSize = 200 } prop_lessThan100 *** Failed! Falsifiable (after 64 tests and 2 shrinks): [(),(),(),(),(),(),(),(),(),... Increasing it can help. ---- Unrelated: in case someone comes here searching for how to increase the number of tests, it is indicated by the `maxSuccess` field of [`Args`](https://hackage.haskell.org/package/QuickCheck-2.9.1/docs/Test-QuickCheck-Test.html#t:Args): &gt;&gt;&gt; quickCheck (\a -&gt; a == a) +++ OK, passed 100 tests. &gt;&gt;&gt; quickCheckWith stdArgs { maxSuccess = 5000 } (\a -&gt; a == a) +++ OK, passed 5000 tests. See more at the [*QuickCheck* documentation](https://hackage.haskell.org/package/QuickCheck-2.8.2/docs/Test-QuickCheck.html).
Next time, just bring up the Fast and Loose Reasoning is Morally Correct paper.
I wish I knew enough about compilers and programming languages to understand how it is possible to compile something from one language intended to be written by humans to another language intended to be written by humans which is then interpreted and executed and not only have it work but work reliably and actually be reasonably fast. Would this be possible with something like PHP as the target language? If not, is there something special about Javascript that makes this work great which PHP lacks? I ask it this way because PHP is my canonical horrible language. Apologies to any PHP fans out there (although it seems unlikely such people would be reading /r/haskell). I also can't believe that Javascript works as well as it does in general. I remember back in the mid 90's being annoyed at how they called it Javascript as if it had anything to do with Java. I also remember how it was pretty much used only for annoying popups/tooltips, and not much else. I lumped it in with PHP as a badly constructed toy language. Clearly it has gone far beyond that. But I still don't understand it. Can someone please suggest a book or other resource that will help me appreciate Javascript as a language and understand why it is so fast/powerful/etc? Is there some great philosophy or mathematical concept or PhD thesis behind it's design? Hopefully understanding these things will also help me to understand how it works well as a target language for Elm. And maybe Elm is finally the gateway to functional programming I've been looking for. I've bounced off all of scheme/lisp/elisp/haskell, mostly for lack of being able to get far enough into any of them to actually be able to apply it to a practical programming need I have (which has been how I've learned my imperative languages). Maybe getting quick practical results of some sort in a browser with some of my old web apps is the way to get functional. I viewed the source for the drag-and-drop example at elm-lang.org/try and chuckled at the thought of anyone who tried to view the source of an unbeknownst-to-them Elm program and actually learn something about how it works. :)
Wasn't involved with this project, but I'm quite pleased, because if there is one thing Markdown is really bad at, it's semantic hyperlinking.
The Int parameter indicates which bit to set. Bits are numbered starting from 0, i.e. `setBit b i` is the same as `b | (1&lt;&lt;i)` in C.
I think an opinionated list (along the lines of http://haskelliseasy.readthedocs.io/) is needed somewhere to help out new users trying to navigate the ecosystem. For example, some of those datavis bindings are pretty much abandoned and bit-rotted. Alternatively, a more automated approach could be to surface some reverse dependency and github stats.
It's a wiki, so anybody can help out! Which ones in particular are the bitrotted libraries? &gt; surface some reverse dependency and github stats. We don't have such automation in place at the moment
&gt; lets fix module system to make it practical to use one module per record I've often thought this would be a good direction for an improvement to the record system. Instead of mandating '1 file == 1 module', allow nested modules in a single file. Then you could do something like: module MyPackage where data Foo = Foo { id :: Int } module Sub1 where data Bar = Bar { id :: String } module Sub2 where data Baz = Baz { id :: Int } -- Here you'd either import MyPackage.Sub1, or maybe be -- able be able to use it directly as Sub1.id. I haven't thought this through at all, but it seems like it maps to having separate files quite easily. The only hard thing would be the compiler finding your modules, since there's no longer an easy mapping to files...
I'm working on &lt;https://guide.aelve.com&gt; which could host at least some of those things (“Machine Learning Libraries”, “Data Visualisation Libraries”, etc). I'm not sure whether the format would fit you, but at least look at it. As a bonus, once I implement Github/Hackage statistics, all packages would have that info displayed for them automatically.
English, attended English school: we did not learn "round to nearest even".
...and its sidekick Backage
st*ack* b*ack* p*ack* -&gt; st-b-p-ack so, stabpack.
I would say that you cannot handle side effects explicitly in ClojureScript, right?
I haven't read this book, but reading through the reviews, it may not be what you are looking for, as it doesn't seem to teach neither data science nor Haskell: &gt; I thought this book would explain algorithms. It doesn't. It simply points to numerous libraries that already implement them. And: &gt; This book is not for a programmer that is unfamiliar with Haskell. You may make your own mind by looking at the accompanying code on [Github](https://github.com/BinRoot/Haskell-Data-Analysis-Cookbook).
`#%~~.pack`
&gt; I wish I knew enough about compilers and programming languages to understand how it is possible to compile something from one language intended to be written by humans to another language intended to be written by humans which is then interpreted and executed and not only have it work but work reliably and actually be reasonably fast. At their core, JS and Elm are very similar. They *look* very different, in terms of syntax, but they have similar features. Functions can be recursive, you can pass functions as arguments, define anonymous functions that capture their environment, and so on. JS has a lot more than that (mutable variables, arrays, objects), but all the core features of Elm are there. So the translation is probably one of the least interesting things about Elm. The big difference is in Types, but it's easy to compile from a typed langauge to an untyped one. The other way around is trickier. &gt; I lumped it in with PHP as a badly constructed toy language. Clearly it has gone far beyond that. Elm exists because many people think JavaScript is a badly constructed toy language, that happened to become standard for the web. JS is the target, but once web-assembly hits, Elm will likely avoid JS as much as it can. &gt; And maybe Elm is finally the gateway to functional programming I've been looking for. Absolutely! In many cases, it sacrifices verbosity for simplicity, so code tends to be longer, but more newcomer friendly. And it has far fewer features than Haskell or OCaml or F#, so it's a better introduction. If you're interested in learning, I'd start with the [guide](http://guide.elm-lang.org/), thought the first few chapters of the book linked here might also be helpful. &gt; I viewed the source for the drag-and-drop example at elm-lang.org/try and chuckled at the thought of anyone who tried to view the source of an unbeknownst-to-them Elm program and actually learn something about how it works. :) It's very foreign if you're coming from C-style languages, but once you know the syntax and the basics, it's actually quite informative. It's very *declarative*: your code looks more like a specification of your program, than a set of instructions for a computer.
The Cookbook gives you examples how to do some common(basic) things related to handling data. I do not think that going for beginning book as well is a good choice.
Hmm. Maybe. But I probably will forget where this is when we get around to trying it, if it is not on Hackage. In yesod itself, you run `yesod init` to get a scaffolded site. So in that sense, the scaffolded site is indeed "on Hackage". UPDATE: The `yesod init` command now says: yesod: The init command has been removed. Please use 'stack new' instead Since we are not using stack, I'm not sure how that works. But still, I'm sure it's a similar idea.
I read it and I can state that it is garbage. You will not learn neither Haskell nor data analysis through this book. they just use some too particular examples and the code is not that OK either. To be fair, I recall I caught one bug or two in there, the code worked for the scenario they had but would not work in the wild, if just a field in the data changed. Will need to look for the notes to recheck. Anyway, I'd recommend you just skim through the book if you have no other resource materials. There should be others so it is best to just ignore this book. Sadly, it is garbage.
Thanks for all of the replies. If you want to email CVs to biglambda (at symbol) gmail (period) com
Hey this is actually a really important point, you should raise this as an issue in Stacks github page. It's sort of similar to how you can't pull a single file off of Github.
Hi Artyom /u/peargreen , what's Aelve? It looks very much like a directory as well, but does it focus on something in particular or one can discuss just about any library on it?
Regarding "Example 1": For applicative functors, see Section 7 of the original paper http://www.staff.city.ac.uk/~ross/papers/Applicative.pdf Not every functor (in the category theoretic sense that you're used to) is applicative in this sense; it's a bunch of extra structure. One thing that might be confusing is that the type (a-&gt;b) is really an exponential object, and not every endofunctor distributes over the exponential in the way that &lt;*&gt; says it does, or has a natural transformation from the identity in the way that pure says it does. return is the unit of a monad, and &gt;&gt;= is Kleisi composition (there are other ways to do the monad type class where you have the multiplication ("join") as primitive instead)
Thanks I'm familiar with Haskell just not data science.
 This is a great answer, thanks for taking the time to write it up. This really helped my brain get at least part of it GetLineF :: (String -&gt; a) -&gt; TeletypeF a fmap in this actions case would be like getLine in prelude where it is function composition of a "post processing step". The GADT makes this relationship more apparent. Any action in the DSL that can pass a value "down the line" needs to have the same type signature a "append" style function can then be applied and pass the value , right? 
Aelve is just a Github org for some of my projects that I don't want to associate with my name exclusively. The guide is intended to be a solution to the “I want to do X with Haskell, what do I choose, aargh, the Hackage is so big” problem. It doesn't focus on anything in particular, yep.
I have it. It is shockingly bad.
Making library which works for both GHC and GHCJS is quite easy. First, you can enable CPP extension and use C preprocessor in code: #if defined(ghcjs_HOST_OS) ... -- code only for GHCJS #endif Second, it's handy to place different versions of code (for GHCJS and for non-GHCJS) into different .hs files, and include them conditionally in your .cabal file, like this: library exposed-modules: ModuleForBoth if impl(ghcjs) exposed-modules: ModuleForJs else exposed-modules: ModuleForNonJs 
Thanks! That helps a lot.
Thanks for the great reply. Web Assembly! I have never heard of it but have definitely thought that a virtual machine which can be targeted with assembly generated by the language of choice would be a very nice way to go. Glad to hear it is finally happening! Now I have a lot of stuff to read up on. Thanks again!
Thanks, I was on mobile
&gt; https://guide.aelve.com Yessssssss, more Haskell information sources
Most programming languages frame it as a [benevolent dictator for life](https://en.wikipedia.org/wiki/Benevolent_dictator_for_life) ;)
I just tried it in my emacs. It displays text on the screen fine, but ligatures don't work. Personally, I didn't really like it. For whatever reason, `Droid sans mono` is the only font that looks good for me in emacs. Everything else looks improperly sized or blurry.
Here's the most recent thread on the subject https://mail.haskell.org/pipermail/ghc-devs/2015-January/007986.html Long story short, Manuel Chakravarty, primary author of the vectorization code, doesn't have the resources to work on it at the moment. The code is still inside master, so we're making sure it builds, but not necessarily running perf tests on it or even running it.
As I understand it, it makes them very easy to copy and select for pasting into a terminal, which is their primary purpose.
&gt; For the human brain, sequences like -&gt;, &lt;= or := are single logical tokens, even if they take two or three characters on the screen. Your eye spends a non-zero amount of energy to scan, parse and join multiple characters into a single logical one. Ideally, all programming languages should be designed with full-fledged Unicode symbols for operators, but that’s not the case yet. By this argument we should be using a character system like Chinese rather than an alphabet. I don't buy it. I'm going to keep writing words out of letters and symbols out of discrete characters, it's just simpler and easier to follow.
I was digging through some old GHC tickets and it looks like its not even mentioned. Is LLVM handling it? This seems like something the scientific computing guys that just formed need to look into.
2 days now, lol. For a hackathon, you want to be working in ghci almost the entire time. Practice that. Hoogle, intero and type holes are your best friends for rapid dev. 
Hmm. I looked and it turns out that complaint is closed: https://github.com/commercialhaskell/stack/issues/201 However running stack --help doesn't give you this so I will say it's not documented as well as other features of stack. 
I believe you may get some automatic vectorisation from the LLVM backend, although I've not tried it myself. Certainly numerics performs better with LLVM.
Which one do you prefer? They aren't as complete in the glyphs department, so I don't use them, to be honest.
I use it with Elm and OCaml too and it's convinient.
Do you know any `Data Science`? I think Haskell has a potential to overcome python. The problem I see, that people who could write a library in their free time will not be consumers of it. So the problem is there is a lack of "mock usage". If there was a high-level specification e.g. how to do a particular task, i.e. what input, output someone would write an implementation... just for fun..
Yes lack of ligatures is half the reason its annoying. You could use it though with prettify-symbols-mode in spite of emacs not having ligatures. I use pragmata pro (paid font) and this setup: https://github.com/mitchty/dotfiles/blob/master/emacs.org#haskell-mode Last update I ever saw on emacs ligature support: http://lists.gnu.org/archive/html/emacs-devel/2015-01/msg00024.html
FiraCode (and Hasklig) *are* monospaced. They have double (or sometimes triple) width ligatures, so that some adjacent characters like -&gt; render as one character, but it's still double width.
I will just leave it here https://github.com/haskell-learning-group/haskell-learning-group
Try #haskell again! Many of us love giving in-depth answers to curious people with interesting questions. In my 6ish years with Haskell, #haskell has provided me with lots of mentors and guidance. You might also spark a relationship with someone interested in more personal mentorship, as I have in the past. Feel free to ping me personally as well (same nick). I've seen people spend literally hours walking through concepts out of nothing more than a desire to be helpful, so please don't underestimate it as a resource.
Just assuming that someone would like to have a go and implement this paper in Haskell? What are the necessary steps? Some which I can see: * implement symbolic integration (big task :) ) Then how to use it? Should it go via TH or some EDSL? IT would be TH if we integrate via [Maxima](http://maxima.sourceforge.net/)
I will continue to give #haskell a shot. It's slightly more awkward to solicit longer term professional relationships on the channel, however, because the channel isn't really meant for that - which I understand. 
This question is a little hard to answer because there is less of a strong sense of what is "canonical" in Haskell than in python. For example, I might do the above script in this way which keeps the quick-and-dirty script-y feel of it: import System.Environment (getArgs) r = 6.371e6 -- Average radius of Earth (m) m = 5.97237e24 -- Mass of Earth (kg) rho = 5.514e3 -- Average density of Earth (kg/m3) g = 6.674e-11 -- Newton's Gravitational Constant (Nm2/kg2) k = 4.0 * g * rho * pi / 3 step delta_t (t, a, x, v) = let a' = -k * x v' = v + a' * delta_t x' = x + v' * delta_t t' = t + delta_t in (t', a', x', v') main = do args &lt;- getArgs let delta_t = read $ (args !! 0) :: Double steps = takeWhile (\(_, _, x, _) -&gt; x &gt; 0) $ iterate (step delta_t) (0.0, -k * r, r, 0.0) mapM_ print steps Or maybe I would create a wrapper around the state of the simulation: {-# LANGUAGE RecordWildCards #-} import System.Environment (getArgs) r = 6.371e6 -- Average radius of Earth (m) m = 5.97237e24 -- Mass of Earth (kg) rho = 5.514e3 -- Average density of Earth (kg/m3) g = 6.674e-11 -- Newton's Gravitational Constant (Nm2/kg2) k = 4.0 * g * rho * pi / 3 step :: Double -&gt; SimulationState -&gt; SimulationState step delta_t SimulationState{..} = let a' = -k * x v' = v + a' * delta_t x' = x + v' * delta_t t' = t + delta_t in SimulationState { t=t', a=a', v=v', x=x' } main = do args &lt;- getArgs let delta_t = read $ (args !! 0) :: Double initial = SimulationState 0.0 (-k * r) 0.0 r steps = takeWhile (\SimulationState{..} -&gt; x &gt; 0) $ iterate (step delta_t) initial mapM_ (\SimulationState{..} -&gt; print (t, a, x, v)) steps data SimulationState = SimulationState { t :: Double, a :: Double, v :: Double, x :: Double } If I really felt fancy, I would probably use a package like [dimensional](https://hackage.haskell.org/package/dimensional) to make sure that I didn't get the units wrong when multiplying things, and use a package like [lens](https://hackage.haskell.org/package/lens) to make the updating feel less awkward. In practice, what would probably happen is I would start with something super quick and dirty like the first snippet I posted. Then as the size of the file grew, I would add the `SimulationState` stuff. Then if things got more and more complicated, so that I was more worried about dimensional errors, and the updating syntax got too unweildy, I would refactor again to add in `dimensional` and `lens`. But that's the nice thing about Haskell! The type safety and statelessness makes refactoring relatively painless, so I'm free to refactor more and more as needed.
Any good idea can sound ridiculous if taken to its logical extreme. The key is to find a balance.
Yes, but you can approach people there (like me!) directly too. Did I mention that you should come talk to me on IRC?
Great point. I've added an install option
It's an awesome font in general. I use it for everything (code related, that is!)
This actually was implemented in Haskell as the [Hakaru](https://github.com/hakaru-dev/hakaru/) system. We used Maple for the symbolic integration. We compiled the programs into Maple expressions which were then simplified. If anyone is interested in porting the symbolic algebra code to Maxima/Sympy/etc I am happy to assist.
The definition of the `UnaryApp` constructor should take a `UnaryOp`
Agreed. I won a copy somewhere and it's full of random (not very enlightening) code snippets.
There's a number of architectural things in ghc that need some tlc for simd to be properly first class. One subtle point is how to handle platform specific intrinsics as well as how to make simd shuffles first class. Just blindly exposing llvm operations doesn't quite work out well :(
the modified version works like a charm! Now I can set the dimensions like let rect = PDFRect 0 0 (21.0/2.54*72.0) (29.7/2.54*72.0) and Adobe Acrobat says that the values are exactly Page Size: 210 x 297 mm thanks again for the patch. I hope the pull-request will be merged soon
Would you mind elaborating on why `OverloadedRecordFields` is an inappropriate name? Apart from the fact that it has meant various different things over the 3+ years we've spent figuring out the design for it...
I think this is because on Windows, file paths are defined as utf-16 rather than bytes.
&gt; This not an academic concern, since Windows file paths are encoded in UTF-16, while modern Linux uses UTF-8. What would make it "academic"?
If we want to be really precise, POSIX doesn't specify what the encoding of paths are; and there is a convention that in a modern Linux environment, these byte strings will be treated as UTF-8 encoded. I reworded to clarify the statement.
I reworded it as "this distinction matters in practice"
Yep. Essentially, the interpretation of a ByteString would depend on what encoding you decided to pick, and the ByteString you'd get out from, e.g., encoding a String would vary depending on the OS you were on.
I would like to know in detail in which ways backpack solves the string problem in better ways than the ListLike class: https://hackage.haskell.org/package/ListLike I can imagine some advantages but I would like to learn it from /u/ezyang or some other expert.
Hello yitz, thank you for the clarifications. I am trying to understand what the objection is here, so I can edit the post accordingly. Are you objecting to: 1. The proposed signature that an abstract FilePath should have (i.e., block of code in signature FilePath where) 2. The idea that there might be some implementation of FilePath as Unicode encoded strings 3. The idea that there might be some implementation of FilePath as ByteString encoded strings 4. The suggestion that when implementing this, a ByteString version of FilePath might be added? 5. The explanatory text? (I agree that the situation is complex, but I am still not sure what nuance the text is missing.) EDIT: I rewrote the section taking into this convo (and the subthread).
&gt; UTF-8 encoded Or, as some UI layers do, according to some locale setting. Which in some countries, including some extremely populated ones, might very well be some other encoding.
&gt; The explanatory text? (I agree that the situation is complex, but I am still not sure what nuance the text is missing.) To me it is unclear what the problem is with file paths. The file manipulation functions are platform-specific, so having FilePath be an abstract type (that is also platform specific) already solves this problem, without requiring backpack. It should never have been possible to manipulate file paths with string or list manipulation functions in the first place, and that is the problem that needs to be addressed.
Well, I hope that the locale functions return the right thing on OS X, for the sake of base's current decoder :) (I think it does; UTF-8-MAC right?)
I recently went through and updated that post to have more up-to-date examples. But I think the examples in this proposal are better, so yes, I suppose so. (EDIT: Added a pointer on the top of the blog post.)
That's only in UI layers. Darwin allows any bytes as a path, including non-UTF-8, like any standard Unix.
&gt; supersede precede succeed English can be mean, sometimes.
I re-read the current version of your post and I have no objection whatsoever at this point. It's great! I am sometimes very vocal here about people being too zealous about UTF-8. But it is not my intention to swing the pendulum too far in the other direction, either. To be more concrete, you might want to go back to something like this compromise: "...while in modern Linux environments the encoding is dictated by the locale, often UTF-8." But note that on Unix, the UI-layer encoding doesn't always go by the encoding specified in the environment as it does for the shell. I heard that some GUI libraries (qt?) use UTF-8 universally. I'm guessing that some others use a different settings mechanism other than the environment. So this text isn't completely correct either. There is a limit to how much effort it is worth investing in an off-topic detail.
I did read it a tiny bit more, and what **I** would do is: Have the database representation as generic as possible (exactly like you did). I don't know much about opaleye, but if it let you write arbitrary translation between SQL and Haskell datatype, have a Haskell type like that: data Tenant = GNew NewTenant | GInactive InactiveTenant | GActive ActiveTenant You can export `Tenant(..)`. Now the `NewTenant`, `InactiveTenant` and `ActiveTenant` are distinct types, with distinct fields. You don't export the constructors, just the type names. Instead, you can provide lenses for fields that can be updated freely, and effectful functions for the other types of transitions.
Btw, please adhere to XDG specs on paths. I.e. XDG_CONFIG_HOME convention. [1]: https://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html
see https://www.reddit.com/r/haskell/comments/4i40t2/demonstration_of_how_to_backpackify/ 
The question is: "How do I model my domain such that the compiler catches invalid states as compile-time errors?" First, you don't have a single type `Tenant`. You have three types, like /u/bartavelle said. The rest of the answer flows from this. Whenever you think "I have a class that has two types" or "I have a thing that has multiple states," stop, and break the states into new datatypes. So you *do* have some functions that are generic across two kinds of `Tenant`. You can use a typeclass to model this. A much simpler way is to just create a sum type: data InactiveOrActive = Active ActiveTenant | Inactive InactiveTenant So now your functions requiring one of these types of tenant is guaranteed not to ever take a `NewTenant`. Naturally, you'll have common fields that you'll want common access to. Fortunately, you can use `makeFields` in the `lens` package to generate them. Given: data ActiveTenant = AT { activeTenantName :: Text } data InactiveTenant = IT { inactiveTenantName :: Text } makeFields ''ActiveTenant makeFields ''InactiveTenant this gives you a class `HasName s a` and two instances, `HasName ActiveTenant Text` and `HasName InactiveTenant Text` which lets you write `someTenant ^. name` and get the name out of it. A *lot* of this sort of thing is just designing tables for relational algebra or SQL to a decent normal form. For instance, you might have a bunch of shared fields between your tenants, like `name`, `address`, `age`, etc. You *can* write the boilerplate and put each field on each model, but why, when you can just data TenantProfile = ... data ActiveTenant = ActiveTenant { activeTenantProfile :: TenantProfile, ... } -- make them fields someTenant ^. profile . name someTenant ^. profile . address --- Now, the next bit about *protecting* the data is interesting. All Haskell data is immutable, so you know that `foo :: ActiveTenant -&gt; Int` doesn't alter the `ActiveTenant` you passed in. What you really care about is data consistency at the database level. And for that, you just write your tables and constraints in a way that prevents you from saving invalid states.
HFS, the underlying filesystem, is _case-insensitive_ by default, which is why it requires a specific encoding, UTF-8, with a specific normalisation scheme, NFD. It's likely the new APFS will _not_ support this, as the programming world has learnt to appreciate just how complicated text can be when it has to work for six billion people.
also agree it is very bad, save your money
Wow, the OS seems to matter a lot: Haskell is in second-to-last place in the OS X and Windows test, and in first place on Linux!
Yes, NFD. Glitch in the matrix!
I was wrong ;) HFS+ filenames are NFD Unicode. They are stored in UTF-16. I'm not sure what exactly happens when you pass invalid UTF-8 to the filesystem layer, but if try to create a file with a name that is a precomposed string, and then read its name back, you should get a decomposed string, Can you verify this?
&gt; Instead, you can provide lenses for fields that can be updated freely, and effectful functions for the other types of transitions. Is this what you mean? data TenantForm = {...} -- all fields EXCEPT key, createdAt, updatedAt, status data TenantNew = {...} -- all fields EXCEPT status. Export only getters for key, createdAt, updatedAt data TenantActive = {...} -- same as TenantNew data TenantInactive = {...} -- same as TenantActive -- Effectful function for tenant creation createTenant :: TenantForm -&gt; App (TenantNew) -- Effectful function for marking a tenant as active activateTenant :: TenantNew -&gt; ActivationKey -&gt; App (Either ActivationError TenantActive) -- Effectful function for making a tenant inactive deactivateTenant :: TenantActive -&gt; App (Either DeactivationError TenantInactive) Question: How do I write a `deactivateTenant` function that works with, both, `TenantActive` and `TenantNew`? This *may* work for the current case, but what happens when I need to model an `Order`, which has a `paymentStatus`, `orderStatus` and `shipmentStatus`? Will this approach scale, or will it end-up with a combinatorial explosion of data-types?
Hakaru can be compiled into Haskell code.
I ran into an [issue](https://github.com/commercialhaskell/stack/issues/2022) when I tried to get stack to generate .prof files with the +RTS flags. It seems like the stack/ghc on windows swallows the +RTS flags meant for the application! This bug likely means the windows code never even got the -N flag and wasn't even parallelized. I don't have a Mac box to test it on, but I suspect the same is happening. I've [submitted an issue](https://github.com/atemerev/skynet/issues/74) about the issue for the project. EDIT: I've found a fix, but haven't ran the benchmarks to see the difference.
&gt; Separate record-types for an active tenant, inactive tenant, and new tenant? This is definitely worth considering. &gt; Or, something to do with GADTs? Or something to do with Phantom types? ... a type-class based approach ... ? IMHO definitely not. Stay away. These things will only make your life more complicated for very little gain.
You seem to be going about this backwards. What you really need to be doing IMHO is designing the tables and relationships in your database. Then *regardless* of how you represent them in Haskell you are guaranteed that you cannot break the invariants you have enforced. EDIT: I see this is what /u/ephrion is suggesting
&gt; Question: How do I write a deactivateTenant function that works with, both, TenantActive and TenantNew? The answer would be typeclasses, although I wouldn't use them and just write two functions. &gt; This may work for the current case, but what happens when I need to model an Order, which has a paymentStatus, orderStatus and shipmentStatus? Will this approach scale, or will it end-up with a combinatorial explosion of data-types? If you put the "variant fields" in the status fields, it should scale. Something like: data Order = Order { _commonStuff :: CommonStuff, _orderPayment :: PaymentStatus } data PaymentStatus = UnPaid | Paid { _psPaymentInfo :: PaymentInfo, _psShip :: ShipmentStatus } 
(Basically you model such dependencies with something like `(Step1, Maybe (Step2, Maybe (Step3, ...)))` )
Yep, that's exactly what happened. I used one of the classic normalization distinguishing characters: 1E9B LATIN SMALL LETTER LONG S WITH DOT ABOVE NFC=1E9B (UTF8=E1 BA 9B) NFD=017F 0307 (UTF8=C5 BF CC 87) NFKC=1E61 (UTF8=E1 B9 A1) NFKD=0073 0307 (UFT8=73 CC 87) When I create a file with the UTF8-encoded NFC version, it shows up as the UTF8-encoded NFD version. When I create it with the UTF8-encoded NFKC version, it shows up as the UTF8-encoded NFKD version. However, as I noted before, invalid UTF-8 bytes are accepted and used without change. No idea how those are stored in HFS+. EDIT: Actually, that test really doesn't prove anything. It is well known that the UI layer (in this case the shell) performs NFD normalization. So we can't see from this what is happening at the FS layer.
There will be standard functions to convert opaque `FilePath` to and from human-readable text. A bit more awkward, but not *hard*.
OK, so I think your problem is a bit simpler than the one you've stated here, and it's "how to represent at compile time the invariants that have been specified in your database". 
You'd just put an instance FromString String in your signature and overloaded strings would work as you want.
Lack of a case-insensitive APFS is currently listed by Apple as a [temporary limitation for the developer preview](https://developer.apple.com/library/prerelease/content/documentation/FileManagement/Conceptual/APFS_Guide/UsingtheAppleFileSystem/UsingtheAppleFileSystem.html#//apple_ref/doc/uid/TP40016999-CH4-SW1). We'll see. As for NFD normalization, I haven't seen any hint that Apple is planning any change in the platform-standard NFD normalized UTF-8 for file paths in UI applications. Whether that also happens at the FS level for APFS is less important.
I guess I need to add I am inspired by: https://www.reddit.com/r/haskell/comments/51ka1m/zurihac_2016_powerful_elegant_web_applications/ For all those folks who tried GHCJS before or wondered what is the current its state. Or those who tried to follow [spock tutorial](https://www.spock.li/tutorial/) but it had some missing bits. EDIT: If you know how to add `closure-compiler` I would be happy to hear DEMOS * ghc-8.0.1: https://spock8.kio.sx * ghc-7.10.3: https://spock.kio.sx 
Why is rpar cheating?
Interesting! I don't know how KindSignatures and DataKinds realy work. What are the benefits over just defining different types for TenantNew, TenantInactive and TenantInactive ? I can see that then the type parameter is more restricted. Is there something else? Maybe you have some links to provide on these extensions?
While I can see why type classes could be painful, I thought GADTs could be a good idea. Can you elaborate why you think it is better to stay away ? 
You'll need to check the console to see the api call. I will make it more fancy once I get home :)
Fair enough, I edited my comment. The library intentionally eschews supporting leap seconds because that's challenging and isn't necessarily relevant to many time-related applications.
GHC uses pretty specialized for a bytestring-based string type (it's FastString actually, but these don't really need to be interned.) pretty only offers a String interface. See also https://ghc.haskell.org/trac/ghc/ticket/10735
maybe not hard but still harder than necessary. I still don't understand the motivation between an abstract FilePath.
The theory of GADTs is not well understood and they have type inference problems. Unless the structure of one's GADTs is extremely regular one is probably just type-level cargo culting without much idea of what's actually going on.
I suppose that's right! That's a clever bit of that design that I hadn't considered. 
According to Apple guidelines for filesystem code authors, it's the FS code that does the normalization, not the UI. https://developer.apple.com/library/mac/qa/qa1173/_index.html The UI cannot do that because the normalization standard is specific to the filesystem.
If you want to use the status only as a field and if the tenant share the same fields you can also try. (You need some extensions like KindSignature) data Status = Active | New | Whatever data Tenant (status :: Status) = Tenant { the_common_fields } type ActiveTenant = Tenant Active type NewTenant = Tenant New 
I used to worry about corner cases like "what if I meet a filename which is not utf-8", but as soon as I learned about roundtrip encoding I considered the problem solved. Does this marginal case really need such huge efforts?
Let's consider an example, say "runParser :: String -&gt; Parser a -&gt; Maybe a". In type-class land, the signature would be something like "runParser :: (StringLike str, ParserLike parser) =&gt; str -&gt; parser a -&gt; Maybe a". Our first challenge is to make sure `runParser` gets specialized, even if it is too big for GHC to consider inlining it profitable. This can be done using the SPECIALIZE pragma; in fact, as long as there is exactly one module with a SPECIALIZE pragma, there will only be one copy of the code. (Of course, if multiple modules SPECIALIZE, there will be duplicates.) Our second challenge is to make sure any functions `runParser` calls, which are generic and have a type class constraint, also get replaced with their specialized versions. So these also have to have SPECIALIZE pragmas. In fact, the entire transitive closure of all functions that runParser calls, save for the actual type class methods, need to be given SPECIALIZE pragmas. What would happen if you omit the SPECIALIZE pragma? Well, in that case GHC will only specialize if it decides it's profitable to inline a function. Which it won't, if the function is too large. Do you trust users to do all the legwork necessary to ensure specialization takes place? I don't! I imagine most uses of type classes you see today where the constraint is propagated across a long function call chain don't actually get specialized by GHC. And that's OK, it's not performance critical. But for Backpack, we wanted to take code people wrote *today*, and say, "your performance is not going to get *any* worse."
Well, at the very least, you might consider using a packed filename representation rather than a linked list of cons cells. ;)
&gt; To clarify: I do not wish to write Haskell like this, but my pseudocode emphasises that a functor is a single object (concept) which consists of two mappings. In Haskell "mapObject" exists implicitly in the form of value constructors (like Just). data Const c a = Const c instance Functor (Const c) where ... There is no function `a -&gt; Const c a`.
The _objects_ in the Hask category are types. So your `mapObject` function is really a function on types. In that sense, it might look roughly like this: mapObject :: * -&gt; * mapObject = Maybe Additionally, the _target_ category of Haskell functors is also Hask. So you're really describing a functor from Hask to (well, a subset of) Hask (that is, an endofunctor).
Fair enough 
Simpler types? I don't know, really. I had seen their preliminary, fully-embedded, version at http://indiana.edu/~ppaml/HakaruTutorial.html , don't know why they chose to hide Haskell from sight.
Thank you, I definitely will 
&gt; That said, I'm inclined to keep the name as-is, on the basis that it is fairly widely recognized already, so changing it now would likely cause more confusion than it would prevent. I think you're probably wrong about this, I suspect that most people who haven't read the RFC think the work on overloaded record fields is using the current record syntax. Especially since the `DuplicateRecordFields` extension *does* use that syntax.
Really? Huh. Works fine on atom. I've finally found something Emacs can't do! 
I think Maybe's Kleisli category is likely much more useful
You went too far in the other direction. You need to give only the target object for each source object, but then you need to give a target morphism for each element from the set of morphisms between any two objects.
Well spotted, there are some cache issues while building those 2 things in one place... which is interesting...
An example of types not preventing a bug!? Quick, to the dynamic-type-mobile!
You can probably monkey-patch it on the way...
It is not unclear. Docs for the broadcast function: `broadcast b x changes the state of the broadcast b to "broadcasting x".` It's not a channel. It's a single cell.
Thanks. Do you know where it ranks now?
Last I knew people were reporting similar performance to the Go version.
So if I'm reading the docs right it is possible that when I call broadcast b "x" &gt;&gt; broadcast b "y" my thread may only get `"y"` (because it might take some time after it's woken up to get to read the value), but when I call signal b "x" &gt;&gt; signal b "y" my thread will always get `"x"` and `"y"`? If I do this: silence b &gt;&gt; broadcast b "x" &gt;&gt; broadcast b "y" &gt;&gt; silence b is it guaranteed that my thread will get at least `"y"` or can it happen that it'll be to slow and get nothing? A handful of examples explaining the semantics would be very useful.
This is a cool idea and I've been following the weeklys for a while, but just have a look at the rust weekly: https://this-week-in-rust.org/blog/2016/07/12/this-week-in-rust-138/ It seems much better than the Haskell one.
That's some highly compressed source code. 
Thanks for the feedback! I agree that Rust's newsletter is a lot better. So far Haskell Weekly is basically just the "News &amp; Blog Posts" section of This Week in Rust. I am open to suggestions and contributions, but so far there have been none. You can tweet [@HaskellWeekly](https://twitter.com/haskellweekly), email info@haskellweekly.news, or open an issue at [haskellweekly/haskellweekly.github.io](https://github.com/haskellweekly/haskellweekly.github.io). 
Actually, nowhere do I argue for or against using static typing in the post. All I'm saying is that types alone don't provide a meaningful specification, and the ecosystem plays a major role when it comes to productivity. edit: only on /r/haskell could this statement be possibly controversial
You can do this with refinement and/or dependent typing. See [liquid haskell](http://goto.ucsd.edu/~rjhala/liquid/haskell/blog/blog/2013/01/01/refinement-types-101.lhs/)
I'm very wary of lazy IO. I'm going to take a look at unagi-chan.
Then you end up with stuff like [this](http://goto.ucsd.edu/~rjhala/liquid/haskell/blog/blog/2013/07/29/putting-things-in-order.lhs/). I'd much rather maintain tests than that.
That's what Haskell people are all about, that's why GHC will [delete your code if it doesn't type check](https://www.reddit.com/r/haskell/comments/kmk6q/the_best_ghc_bug_ever/) 
The halting problem says you can't provide a "yes/no" answer. It doesn't say you can't provide a "yes/no/unsure" answer. And it turns out that in the real world, "no" and "unsure" are the same thing. "This doesn't obviously terminate, it's probably a bug" works well over 99% of the time. After all, if an algorithm doesn't obviously terminate, it's probably either wrong or deserving of a reference to a correctness proof in literature. So no, you can't just wave away proofs about code with "halting problem", because that ignores the reality of what code people actually care about. 
He can't, he is desperate to prove that stalangs and dylangs are on equal grounds for the sake of his zealotry.
&gt; And it turns out that in the real world, "no" and "unsure" are the same thing. Which eliminates a large class of programs. This is where the mindset of static and dynamic typing diverges. When you restrict yourself to a subset of code that the static type system can prove, you're often forced to solve problems in convoluted ways. Last I checked working in a language like Coq isn't terribly productive.
&gt; "instance Functor ..." means "let there be such object that maps objects and morphisms of category Hask to category Maybe". In Haskell (and mostly in GHC) there's also an implicit uniqueness, not just existence.
Thank you for your valuable input.
&gt;What's good with types is that your tests can focus on specification only instead of also checking code consistency. That's precisely what my tests focus on when I work with Clojure. I only need to check the input data at the edges. Something you have to do in a static language as well. The way I develop applications is always iterative. Let's say I'm pulling some data from a database, formatting it, and presenting it to the user. I'd write a function to get the data. I'd run it in the REPL and see the data. I'd write the function to transform it, run it, and so on. At no point do I have to consider more than the function I just wrote and the function I'm going to write next. I always know what the state of the data is. This equally applies to modifying existing code. I get the application in the state I need, then I work on a new feature. I'm never guessing what the data is when I'm working with it. It's simply a different approach that gets you to the same place in the end. Different people prefer different things. I'm not sure why that's so hard to accept.
I suspect the function of strictness annotations to runtime is non-linear, and that reasoning backwards from a profile to exact annotations is very hard. So the search space is 2^n for n possible annotations, maybe reduced to 2^m with profiling where m &lt;&lt; n, but still too large.
One would think that in order to judge who's more knowledgable, you have to be versed in the topic yourself. Are you suggesting that alantrying is committing a fallacy known as the argument from authority?
Calm down. You haven't *only* seen responses from $OTHER_PERSON... but lots of people have responded to you in this thread -- some of whom **are** experts (not me). The thing is: I *don't* think of myself as an **EXPERT** in, say, Particle Physics, but as it turns out I'm pretty good at determining what is and what isn't bullshit when it comes to claims of **QUANTUM HEALING** (and things of that nature). Don't get me wrong... experts can be fooled too, but... sometimes it *doesn't* take an expert to disprove an idea. So no, not an appeal to authority, but if many knowledgeable people tell you you're wrong... you *probably* are -- and should rethink. Not *definitely*, just *probably*. (But as I mentioned in another comment... we really do have *very* scant empirical evidence either way, so YMMV.)
&gt; my point is that the properties you can prove reasonably aren't usually terribly interesting [citation needed]
&gt;Don't get me wrong... experts can be fooled too, but... sometimes it doesn't take an expert to disprove an idea. So what idea are we disproving here exactly. The statement that you can't have one Turing complete program guarantee that another finishes? &gt;So no, not an appeal to authority, but if many knowledgeable people tell you you're wrong... you probably are -- and should rethink. Not definitely, just probably. Except I've been in the same headspace as these knowledgable people about 7 years ago when I used Haskell myself, then I briefly moved to Scala. I used to think all the same things these people are telling me. I've since spent about 6 years working in Clojure and I simply don't miss types working in it. Having been on both sides of the argument, I really do think it's personal preference at this point.
The blog goes wrong when it asks, "So, how did the fact that messages were being dropped slip by completely undetected?" The answer is that this program was written by a human, and that type systems do not catch every conceivable bug immediately. And IMO that's about the end of what there is to say on the matter. Instead the blog analyzes the internals of unagi-chan and type system "escape hatches" and blames these for the problem. Even though unagi-chan wasn't being used in the buggy code.
Slides are [here](https://github.com/meiersi/HaskellerZ/raw/master/meetups/20160722-ZuriHac2016_Bas_van_Dijk_FP-at-LumiGuide/Bas_van_Dijk-FP_at_LumiGuide-ZuriHac2016.pptx).
(Goddammit, can't be bothered to do the whole reddit-quoting-thing. I hope what I'm doing here sort of works anyway.) &gt; I simply said that most type systems don't provide sufficient specification to tell that the code does what was intended. I used a sort function as an example of that. Well it seems tautological that no language can prove that a **specification** is correct. After all, that's why we have specifications. Again, I feel you're moving the goal posts and trying to discuss something that I didn't actually contest, because now suddenly we're talking about your blog post whereas I was only speaking in the context of this particular thread on reddit. I also understand (believe me!) the urge to do this, but this is so far off from what "alantryning" was talking about that we should probably just stop this thread here. (Unless I've got my threads mixed up, but I don't *think* I did, so...)
The do notation desugars using `&gt;&gt;&gt;` to compose arrows, and `&gt;&gt;&gt;` is defined in terms of `.` which, specifically for Kleisli arrows, is (Kleisli f) . (Kleisli g) = Kleisli (\b -&gt; g b &gt;&gt;= f) 
AFAIK, that's true. No one has done a long-term study comparing TCO of two code bases targeting the same functionality but with the different development styles. Short term studies have repeatedly showed the advantages of formalization first.
That's pretty insane for a language that's at the 30,000 foot level like Haskell. Well done, team.
We usually win I/O heavy benchmarks though. I think this is down to implementation at a couple levels in the dep hierarchy.
Perhaps they don't provide a *complete* specification, but nothing can do that. Types do, however, provide large *portions* of specification for free, which is valuable.
&gt; Other approaches appear to provide the same value and it's not at all clear that static typing is the most efficient approach. I think that claim needs further substantiation. Think about the `null` problem in Java. Having a type system in place to guard against `null` through `Maybe` is a solution that's incredibly easy to work with, and catches the problem in all cases. Without types to guarantee a solution to the `null` problem, no amount of unit testing can guarantee that the rest of your code never calls that function with `null`. The type system automatically excludes large classes of errors, using only a line or two of code. Unit tests merely test what you can think of / what you remember / what you spend the effort to write. Types ensure that certain classes of tests are always in place no matter what; something the human programmer can otherwise easily forget, or implement incorrectly. And types do this more elegantly, without requiring the programmer to tediously implement large, verbose, type-like tests time and time again; instead you just write one line of type signature, and get lots of guarantees for free. Not only have you inserted countless tests automatically this way, you have also provided a rudimentary means of documentation (I know, types are no substitute for docs, but they do help). And yes, there are escape hatches to the type system. But in general, these are frowned upon and avoided unless it can be clearly proven that the escape is unobservable to the outside world.
I've worked with Java for over a decade, so I'm quite familiar with the `null` problem. Once I moved to Clojure, a dynamic language, this problem became far less common. Having higher order functions that handle edge cases internally goes a long way addressing it. Again, it's a different approach that gives you similar benefits in practice. You don't have to preach the benefits of the type system to me. I've used Haskell for about year when I got into FP. It's a nice language, but I didn't find that it lived up to the hype. I've been using Clojure for the past 6 years now professionally, and I simply don't see the problems that you're describing. Different people see different paint points, and prefer working in different ways. I completely understand why static typing appeals to some people. However, other people prefer different approaches, and as far as I can tell there is no clear benefits to one approach over the other. If you like static typing and it fits your mindset, then you should absolutely use it. However, if you're going to make general claims about its benefits, then I would like to see some empirical evidence to back those up. Considering how long static typing has been around, it's a bit of an elephant in the room that nobody has managed to show clear benefits of this approach. 
&gt; it's a bit of an elephant in the room that nobody has managed to show clear benefits of this approach. My comment elaborated on just a few of the benefits of static typing, and you didn't refute any of them. I respect that you're trying to say this is a matter of opinion, but you're not making a very strong case.
[removed]
Incidentally, I did write a [post](http://yogthos.net/posts/2015-11-28-TheSkyIsNotFalling.html) on that exact topic earlier. Also, since I've been downvoted here I can't really post to /r/haskell effectively having to wait 8 min between posts, precluding any real discussion.
I don't think that at all. I just don't think that this approach is more effective at providing a specification than other methods such as testing. 
I think my sarcasm usage here was a bit high, sorry for confusion. To clarify: * I agree with your original comment * Clearly in a benchmarking case like this, the client should act as some level of test suite. To some extent, the original 98% drop rate was a valid implementation since it passed the benchmark client. * The blog post is trying to say "types don't prove anything, tests do." But this example really shows that a tests don't prove everything correct, in exactly the same way we all know types don't prove everything correct
&gt;I guess I'll just wait for Haskell to take over the world of software development. Since it's clearly so much more efficient than what everybody else is doing, that should take no time at all I imagine. I didn't say it was or wasn't. I was actually implying that sometimes proofs are more economical than tests. If we took as a given that Haskell were 100x more efficient than other languages, it would still have huge hurdles to clear before unseating any of the top 5. This holds true even if it used strict evaluation like more devs are familiar with. This also holds true even if Haskell were 100x more effective, used strict evaluation, and used words more pervasively than symbols.
This is a fantastic question. Hakaru makes heavy use of program transformations and to implement them can requires some serious metaprogramming capabilities. To get the flexibility we needed requires a fairly deep EDSL. Also as /u/carette said we still have a combinator library to building Hakaru programs within Haskell. We also wanted to make a language that could appeal to Machine Learning people coming from non-FP backgrounds and language provides an opportunity to provide a more familiar syntax.
&gt; I only need to check the input data at the edges I understand now. We are not saying that you don't need to check the edges with Haskell, you do. What we are saying is usually, dynamic languages forces you to write (much) more than than. Let's say you write 10 tests. We are not saying we should write 0 instead of 10, but only 10 instead of 50. It just look like you are not writting enought tests ;-)
 Nope. This does not work. The `allow-newer: true` has no bearing on the compilation of the compiler, does it? $ cat stack-base.yaml flags: {} packages: [] system-ghc: false extra-deps: [] resolver: nightly-2016-08-26 allow-newer: true compiler: ghcjs-0.2.0.820160826_ghc-8.0.1 compiler-check: match-exact setup-info: ghcjs: source: ghcjs-0.2.0.820160826_ghc-8.0.1: url: "http://tolysz.org/ghcjs/ghc-8.0-2016-08-26-nightly-2016-08-26-820160826.tar.gz" sha1: "2ecd8523cafe22b3033726363b4acc5356117970" $ cat build.sh #!/bin/bash stack --stack-yaml stack-base.yaml setup PATH=`stack path --bin-path --stack-yaml stack-base.yaml 2&gt;/dev/null`:$PATH echo SETUP date stack setup 
Some of the specifics in that post are out of date, though, with respect to hoogle 5.0---there's no 'hoogle convert' command any more, for instance.
Mate I just lost half of last weekend to running and analyzing that benchmark in 3 different languages. I understand just fine how it works and what the numbers are.
If you mean by that that GHC is competitive in latency constrained problems, then I am glad to have been wrong! EDIT: I understood you meant that GHC generally "wins" I/O heavy benchmarks. My statement wasn't to say you were wrong, only that I thought this was true for throughput. Given that you can't control the length of the stop-the-world GC passes, I didn't think GHC was generally competitive when latency is the most important factor.
You h2 can be rewritten as: ``` h2 = proc x -&gt; do _ &lt;- double -&lt; x; a &lt;- plusminus -&lt; x; returnA &lt;- a ``` In fact, you are loosing the result of double.
&gt; the Haskell implementation failed to deliver messages reliably, dropping 98% of the messages it received. What's interesting is that this is exactly the kind of behavior we would expect Haskell type system to prevent from happening We would? Err ... why?
Suit yourself. But the point is that types can (and do) provide incredibly advanced guarantees. You want to check them with tests. Others want to use proof assistants and other advanced tools. But your claim that types are somehow inherently insufficient is simply not true. Granted, you may not like them. So feel free to chose from the myriad un(i)typed languages. ;)
I'm sorry you're being downvoted. I think people understand it to mean "I don't agree" but that's not what it means. Even though I don't agree with you I have been upvoting you steadily. :)
Right...I don't think a single (informed) Haskeller would ever believe that. Clearly the author has a very different (incorrect) understanding of the type system than we do.
props for using regular Haskell that you might expect someone to actually write, not obscure unreadable crap but this once again shows why Go is very popular...Go never has outliers. i've not seen many benchmarks where Go clobbers everyone else, but it always does very well and never has weird corner cases or gotchas. you really can just sit down, start coding intermediate-grade source, and expect good results.
The context of the discussion is that the type system failed to prevent a real world problem from occurring at runtime. This is exactly the kind of problems that happen in real applications and cause a lot of headaches. You test your app with small loads and things look great, then you put it in production with heavy usage and things start falling over. Stress testing the application would have quickly uncovered the issue however. I'm not sure how you can claim that the type system is sufficient in the context of a concrete example of that not being the case.
Think about the reading side. If you listen in a loop, there is a period where the thread isn't listening and will miss any "messages". I agree some examples would be useful. I can't think of a case where I'd want to use it.
thank you. I think I will take a look at coq or agda.... +1 for maths jargon ;) 
[Found it](http://www.hahastop.com/pictures/Poop_Mobile.jpg).
&gt; F(g) doesn't make sense, because g is a term, not a type. I think this is part of my confusion. Functors take an object to an object (eg a Set to a Group F: Set -&gt; Grp) and it makes sense to ask what is the action of F on some set S F(S) but not what is F(x) for an element of the set S; However(!), functors are also maps on hom-sets, that is to say the same functor F gives me a set-map from hom(S,S') --&gt; hom(F(S),F(S')) and it makes sense to ask for F(g) where g is an element of the set hom(S,S'). When I think of hom(S,S') as just a set, then F(hom(S,S')) is just a group, but in the context of maps, F(hom(S,S')) is a subset of hom(F(S),F(S')) and indeed F induces a set map. &gt; F(g) doesn't make sense, because g is a term, not a type. F(g) does make sense because g is a map, not just some element of some set :)
I think it's a question of what the functor does on hom-sets. Do we think of them as hom-sets or just as sets. Any functor F: C--&gt;C' induces set-maps F_{X,Y}: Hom(X,Y) --&gt; Hom(F(X),F(Y)) X -&gt; Y --&gt; F X -&gt; F Y (?) no matter what the categories C and C' are. If we are in the situation where C and C' are sets (like in Haskell) we can apply the functor F to the set hom(X,Y) viewed as an object of C. F: Set --&gt; Set Hom(X,Y) --&gt; F Hom(X,Y) X -&gt; Y --&gt; F(X -&gt; Y)
Note: This will only give you search access to the base libraries. To include other libraries, for example `lens`, you would run: nix-shell -p "haskellPackages.ghcWithHoogle (pkgs: with pkgs; [lens])" \ --command "hoogle server --local -p 8687" This is actually just what my Docker image is doing, except that the list of packages is 250 long, and I've already done the work of building them and preparing the Hoogle database inside the image.
Yea I suppose that was unclear. `F(g)` makes sense if you treat `g: A -&gt; B -&gt; C` as `g: A -&gt; X` where `X = B -&gt; C`. But this is just a weird consequence of the category of Sets. In this view, `F(g)` will be a morphism `F(g) :: F(A) -&gt; F(B -&gt; C)` (and thus a term of the type `F(A) -&gt; F(B -&gt; C)`, though that's beside the point). This is all much more confusing due to the relationship between morphisms and hom sets in the category of Sets. Every morphism in the category of sets is not only a morphism, but also an element of a homset (aka: a term of a function type). That's why values and functions are defined with the same syntax: functions and values are both just terms. f :: A -&gt; B -&gt; C f = ... x :: A x = ... The unfortunate consequence of all of this is that curried functions get very awkward to reason about in terms of category theory. A curried function `f :: A -&gt; B -&gt; C` could be notated as: `f :: A -&gt; HomSet B C` to denote that it's a morphism from the set `A` to a homset. A homset quite obviously represents an exponential object with the product object `(_, _)`, imposing the existence of a morphism: `eval :: (HomSet a b, a) -&gt; b`. In terms of a functor `F`, `F(x)` where `x` is a term is what I meant doesn't make sense. Functors don't map arbitrary terms. Functors map morphisms and types, and thinking about things as terms doesn't play nicely with this. To bring this all together with the notion of `Applicative`, recall that alternative definition of `Applicative` I defined earlier. class Functor f =&gt; Applicative f where pure :: a -&gt; f a (&lt;**&gt;) :: (f a, f b) -&gt; f (a, b) Applicative (endo)functors (of Hask/Set) are (lax) monoidal functors (with a strength). The monoidal functor `F :: C -&gt; D` implies an isomorphism between `F a • F b &lt;-&gt; F (a ∘ b)` for monoidal category operators `• :: DxD -&gt; D` and `∘ :: CxC -&gt; C`, half represented by the `(&lt;**&gt;)` operator, and other-half represented by Set being a cool category and giving it to us for free for any functor. And the strength is basically represented by `pure`. So, to tie it all together, `pure` is a term of type `HomSet a (F a)`, or a morphism `a -&gt; F a`, whichever you prefer. A functor does not require this morphism to exist, but strength does. And the normal notion of applicatives can be recovered like this: (&lt;*&gt;) :: Applicative f =&gt; f (a -&gt; b) -&gt; f a -&gt; f b x &lt;*&gt; y = fmap eval ((&lt;**&gt;) (x, y)) So hopefully now you can see how `f (a -&gt; b)` represents `f (HomSet a b)`, not `f(g)` for some morphism `g`. Throwing the terms `x` and `y` into a tuple with `(x, y)` means we can use the monoidal functor's `(&lt;**&gt;)` operator to create a `f (HomSet a b, a)`. Then, `fmap eval` lifts the morphism `eval :: (HomSet a b, a) -&gt; b` into `f eval :: f (HomSet a b, a) -&gt; f b`, which can be applied to `(&lt;**&gt;) (x, y)` to reduce down to `f b`. Finally, we can imagine using this with a curried function `g :: x -&gt; a -&gt; b` by using `fmap g` to lift `g` into `fmap g :: f x -&gt; f (HomSet a b)`, which can then be plugged into `(&lt;*&gt;)`. In practice, this pattern uses `(&lt;$&gt;)` as an operator for `fmap` and it looks like this: g :: A -&gt; B -&gt; C g = ... x :: Applicative f =&gt; f A -&gt; f B -&gt; f C x a b = g &lt;$&gt; a &lt;*&gt; b
I'm really not sure how to add local libraries. I think it would require a few steps: 1. Copy your source tree into the Docker image by adding a `COPY` command to the `Dockerfile`. 2. Create a custom `config.nix` file to locate your source tree within the image, and also `COPY` this in. 3. Add the package name to the list of packages referenced both in the `Dockerfile` and in `package-list`. 4. Rebuild the Docker image. Note that building the Docker image takes a really long time, so it wouldn't be economical to frequently update the package set within it. You could find a way to game Docker's caching so that updating a local package isn't so bad (do everything related to the local package at the end of the `Dockerfile`), but it will still take some patience.
I'm not sure I'd call GHCi and `runghc` "non-standard" tools. I use Python every day and don't feel it's tooling as significantly better than Haskell's when it comes to interpreting code. There are cases where it's handy but in general the difference is minimal. And our apps often take several seconds to start up anyway because the interpreter has a lot to initialize.
Are you putting GADT extensions in a cabal file or on top of the module? What version of hlint are you using?
yes i put GADT into cabal file and in LANGUAGE pragma. I believe I am using 1.9.21 but I am not sure (cannot check from this computer). I am running hlint as `hlint src/` so maybe it has no chance of picking up the extension from cabal file
&gt; The halting problem states that we can never be sure if a function will finish executing or not. That's sloppy wording, at best. It states that there is no single oracle that can tell, for *any* given function, if it will finish executing or not. Total functions are actually a fairly large class of functions, and we know all of them terminate. Personally, I think `main` or whatever you call it should be total. I think 2^128 primops should be enough for anybody. ;)
Yes, I did. Not a great choice on my part, but there isn't really any ambiguity in what I wrote, since types and values have separate name spaces in Haskell. Every variable to the right of `::` in the type declaration appMap :: Applicative f =&gt; (a -&gt; b) -&gt; f a -&gt; f b is a type variable. I don't know if it helps to inidicate the very limited scope of type variables, but there is implicit universal quantification in Haskell type declarations that can be explicitly added, with the right pragma provided to GHC. The above is really equivalent to appMap :: forall a b f. Applicative f =&gt; (a -&gt; b) -&gt; f a -&gt; f b which is the type of functions that can be applied to any types a and b and to any applicative `f`, taking a function `(a -&gt; b)` and an `f a` to an `f b`. In contrast, the definition appMap f a = pure f &lt;*&gt; a is at the value level and can't possibly refer to the same f as in the type level expression, because of the separate name spaces. The value of `f` in the right hand side of the definition is bound by a pattern match on the left hand side of the equation. If I had been smarter I would have written appMap :: Applicative f =&gt; (a -&gt; b) -&gt; f a -&gt; f b appMap g a = pure g &lt;*&gt; a for my example. But you will find that using `f` for "functor" at the type level and `f` for "function" at the value level is common. It's can be confusing, but only if you don't look at the rigid distinction between type expressions and value expressions. Your implementation of `appMap` mixes types and values and is not correct. At the type level, `F` is a type variable representing an applicative. But the variable `F` is not even in scope in the value level definition of `appMap` because it does not appear anywhere on the left hand side `appMap g a`. (it could perhaps be a data constructor in scope if you defined it somewhere else in a larger block of code, but it would bear no relation to the type variable `F` in the type declaration for appMap.) 
I think this is the file that doesn't pass when I try it: https://github.com/d3sformal/bacon-core/blob/master/src/Algorithm/Pdr/State.hs but it might be one of the other files in that repo... can't remember
I think you're reading a little too much into what I said. I don't have anything against Haskell, I used it years ago, it's a nice language. The vitriol seems to be mostly directed at me here for suggesting that the type system alone does not provide sufficient guarantees in practice. Apparently, that's a very controversial thing to say on r/haskell.
I find this question confusing. Can you rephrase it?
The funniest thing about this post is that there are actually typed solutions to the problem, such as using ocsigen's shared reactive signals or shared channels. 
If I only deny myself programs that might execute 2^(128)+1 primops, I'm only denying myself programs that I will never see complete anyway. By the time modern processors execute that many instructions, the Sun will be but a brown dwarf.
Yes, I do. :)
Enjoy. :)
I'll rephrase that as: Since `mapMorphism (a -&gt; b)` must be `mapObject a -&gt; mapObject b`, you don't need to define mapMorphism for the Maybe functor, mapObject is enough.
I think a lot of the downvotes come from a lack of justification in many of his comments. In this particular thread, I've tried to keep things focused and make it more objective, but in plenty of the other comment threads on this article, he's been pretty hostile and hasn't been justifying his positions.
this is more than code for a quantum oscillator, it looks like the skeleton of a little scientific computing library.
It's a bug in Haskell-src-exts - I simplified it and raised it upstream as https://github.com/haskell-suite/haskell-src-exts/issues/329. Seems haskell-src-exts is missing the case for both context and record syntax in a GADT. Running over everything in bacon-core, it's only one file that fails to parse - everything else will still be fully checked.
No goalposts have been moved. We have the testing approach to correctness [exemplified by SQLite](https://www.sqlite.org/testing.html) and the typed approach exemplified by compcert. They are both doing non-trivial things with formal languages.
Added to [HaskAnything](http://haskanything.com/content/presentation/presentation-functional-programming-at-lumiguide.html), thanks for the slide link. Could you click on the "Edit On Github" button at the top and check if there's any more tags or libraries that could also be added?
We have a Slack channel for that, but at this point auto-invites don't work. Could you send a mail at the datahaskell.org address? thank you!
Knot tying tricks are something you can do once at the end of building a structure. Attempting to map over it would destroy sharing information. You can use tricks like Andy Gill's `data-reify` to recover sharing as you rebuild the graph, but really that's just converting to a map-based representation and back. If you expect you'll need such operations, it might be best just to stay there, and 'tie the knot' all at once at the end.
I want to shout out to [SubHask](https://github.com/mikeizbicki/subhask), which is probably the most ambitious of all base replacements I've ever seen. While it's primary goal is to make numerical computations fast and elegant in Haskell for machine learning purposes, SubHask aims to subsume packages like classy-prelude, numeric-prelude, algebra, and even lens.
Why don't they make it part of foundation?
I don't think this has anything todo with Spock, but more about how you can benefit of type-level programming in Haskell. In this particular case we have a GADT data type: data HVect (ts :: [*]) where HNil :: HVect '[] (:&amp;:) :: !t -&gt; !(HVect ts) -&gt; HVect (t ': ts) The goal of this data type is to allow heterogeneous lists on value level and track at type level the types of the values. For example: HNil :: HVect '[] means that we have an empty list at value level, and an empty list of types at type level. True :&amp;: HNil :: HVect '[Bool] Indicates that we have the value `True` at the value level, and the type level will know that at position 1 we have a value of type `Bool` True :&amp;: "Foo" :&amp;: HNil :: HVect '[Bool, String] Is `True` and `"Foo"` in position 1 and 2 of our list with the types `Bool` and `String`. Like the value level `(:)` operator for lists, the corresponding operator for type level lists is `(':)`. The `[]` (and `'[]`) notation for that is just syntactic sugar. Thus we could also write True :&amp;: "Foo" :&amp;: HNil :: HVect (Bool ': String ': '[]) 
What do you mean with ASCII strings? AFAIU String is defined to be UTF-8
&gt; Not happy about ASCII strings not sure what you mean about this - the strings are UTF8. Personally I feel this might be a mistake; the general wisdom I've heard is to use UTF8 for storage of text, but something nicer for processing (UTF16 or UTF32, probably in whatever endianness is best for your system).
Different goals, and SubHask seems much more advanced and experimental. Foundation is also still evolving, but the focus is on changes that are much less open to debate—like a coherent design for strings or vectors. Maybe ideas proven in SubHask can be useful for `foundation`.
[Kanye concurs!](http://i.imgur.com/vX70KGu.jpg)
If you had tried writing quick sort in Coq or Agda you wouldn't write that answer.
Downvoting poor argument seems appropriate. Static types aren't enough and we know that, but having a correct conclusion doesn't justify examples with zero accuracy. Or maybe it does in a pub, not in debate club.
Thanks for explaining. A question though, why not use tuples instead and have HNil :: () (:&amp;:) :: a -&gt; b -&gt; (a,b) head :: (a,b) -&gt; a and so on?
[*Stream Fusion on Haskell Unicode Strings*](https://www.cs.ox.ac.uk/files/4455/paper.pdf) discusses [`Text`](https://hackage.haskell.org/package/text-1.2.2.1/docs/Data-Text.html)'s decision to use UTF-16 internally
`HVect`s are very similar to tuples, with the main difference that they can have a variable length, are more flexible and easier to write general case functions for. Without defining a type class instance for all the tuples in different sizes it will be hard to write a function that accepts an N-tuple in Haskell and does something useful with that. In our case we use the `HVect` to do currying with N-tuples. This means we can convert a function fun :: HVect '[a, b, c, d, e] -&gt; x into fun :: a -&gt; b -&gt; c -&gt; d -&gt; e -&gt; x but not only for `HVect '[a, b, c, d, e]` but forall `HVect xs` where `xs` is a type level list of types. And we can define these currying steps in a single function rather than having to define a function for each different HVect. (see http://hackage.haskell.org/package/hvect-0.3.1.0/docs/Data-HVect.html#v:curry for example). By the way, we can also turn any function fun :: a -&gt; b -&gt; ... -&gt; x into fun :: HVect '[a, b, ...] -&gt; x
Funny how nobody here disagrees with the conclusions in my article, except maybe [this guy](https://www.reddit.com/r/haskell/comments/51seqn/static_typing_vs_websockets/d7g2zqc). Yet, there's thread with 150 comments of people sounding genuinely upset by it.
Thanks for telling me about that
Very helpful, thanks.
Where are you seeing that? There's only one identifier starting with "H" in the entire package, according to the Hackage index: https://hackage.haskell.org/package/foundation-0.0.1/docs/doc-index-H.html Am I going mad?
Which is explicitly partial, so I'm still not sure what /u/ephrion and /u/joeyh meant.
I'm confused, is this course free? The main page states that you have to buy it, but OPs link takes you straight to a section of the course. Is Reddit whitelisted or something?
I'm not sure either, but in the blog post there is a reference to head :: collection -&gt; Element collection although I can't find it in the source code 
The blog post seems to have differing definitions. The blog post references a `head :: collection -&gt; Element collection` function, which doesn't appear to be present in the actual library. I'm still unhappy with `fromPartial`. It's the worst part of `Maybe` with the worst parts of partial functions. Instead of a `Nothing` value which behaves fine, you have a *pure* exception. No good, no good at all.
Agreed. EDIT: It's not clear to me that those functions are exposed though.
[removed]
I think arguing about which encoding is the true encoding, is not very useful. It's clear to me that there's no perfect representation, and they all have disadvantages. I wonder if in the future, we might investigate to go in the same direction as python (I think they are just choosing the most compact representation related to the data). I could imagine having: ``` data String = StringUTF8 ... | StringUTF16 .. | StringUTF32 ``` Obviously if we do happen to go in this direction it will be based on benchmarks (cpu and memory) to make sure we're not shooting ourselves in the foot. For now we just chose an encoding and make sure it goes in the same ballpark as text performance wise. (also I want to add that there's no such plan now)
The first one happens to just be our first upload to hackage that someone decided it would be a good idea to announce to reddit. I had the plan to write some of the motivation behind foundation before doing an announcement originally; now it's done.
What's an example of (a)?
Should foldPaginator use the `acc` param in the non-leaf case?
Strictly I think you're right. A `[] a` can be lifted into `StateT s [] a`, simply by the type of the `lift` function: https://hackage.haskell.org/package/transformers-0.4.2.0/docs/Control-Monad-Trans-Class.html But a function returning a `[] a` could also be said to be _lifted_ into a function returning `StateT s [] a`. The language is used a bit loosely in the article, I'm not sure I'd word it like that.
I'm interested by this bit: &gt; It is also trying to be more than a library: A common place for the community to improve things and define new things Isn't *hackage* a common place for the community to improve things and define new things? I don't understand the apparent desire to build a separate parallel community around one package. It seems to me that would make it less likely to succeed in achieving much use in the Haskell community in general. But I could be wrong.
Hey! I'm on mobile right now, but you are probably right -- will fix it tomorrow morning, than you!
This is a perennial point of discussion on the team. The big thing we use Maple for is performing certain mathematical transformations, especially ones dealing with integrals. We could, in principle, switch to some other symbolic computer algebra system— but we'd want one which is at least as smart as Maple (wrt., the sorts of things we use it for). We could also, in principle, write our own system in Haskell (which would be very nice since it would actually be typed, and would reduce impedance mismatches of translating back and forth between AST representations)— but that's a *lot* of work, and we are but a very small team. Also that sort of work, while interesting, isn't really in the focus of our research. There are a bunch of things you'd lose. The two big ones are: the ability to find closed-form solutions to integrals (aka "exact inference"), and the ability to detect and make use of conjugacies. Both of these are crucial to making the compiled/optimized programs fast. As things are written, you can always avoid the dependency by simply not calling the transformations that shell out to Maple; just don't expect much more than a glorified sampler :)
Well, maybe that should stop and make you think for a minute: * Did my readers come to the conclusion that I was intending to convey? * Did they come to conclusions that I was not intending to convey? * Did my writing convey the tone that I was intending? As I've already told you, I think you said more than just "types alone don't provide a meaningful specification, and the ecosystem plays a major role when it comes to productivity". Your article also said (paraphrasing and summarizing) 'static types aren't worthwhile, only tests are'. I believe it's this latter conclusion that most here disagree with.
I'm happy to be wrong in this case :D
You don't get much community involvement in something most of the community is unaware of, and discovery on Hackage is abysmal. Putting more things in one package has downsides for modularity and fluidity, but it's also easier to notice simply because it overlaps with more interests and thus more people have reason to talk about it.
Thank you so much, rodneythellama. I just re-read Lethalman's blog post, and was about to start looking into this, but you've given me a head start. Look for an updated image coming later this week.
&gt; I'm not sure hint can do that [Yes, it can](https://github.com/gelisam/hint-demo#readme).
That also makes sense. It's possible "downvote" is just not well defined. 
Honestly, I see your point. There's a duality going on.
Co-author of dimensional here. We are working on features for dimensional vectors and dimensional linear algebra. PM if we can rope you in to contributing.
I'm pretty sure I was very clear in what I was saying, and I even provided a summary at the end. &gt;Your article also said (paraphrasing and summarizing) 'static types aren't worthwhile, only tests are'. Nowhere do I say anything even close to that. What I say repeatedly is that types alone are not sufficient, and you need tests even if you're working in a typed language. Please stop trying to put words in my mouth here.
FWIW julia deals with ASCII and UTF in a very interesting way (http://docs.julialang.org/en/release-0.5/manual/strings)
Foundation looks interesting, but I don't see myself buying into this new parallel ecosystem just because I want to use the more uniform String type. You also don't buy a new car just because you want to use a more featureful car radio. Consequently, it's rather unlikely I will use Foundation for anything serious which also means I won't be able to provide any useful feedback. I strongly disagree with your reasoning as I would notice Foundation, but not because it's one big single package, nor because it is on Hackage, but rather because people would tweet about it or announce it or talk about here on Reddit. 
If anyone want to contribute things to make it foundation friendly, we're definitely going to look at it, but none of the current contributors use GHCJS as far as I know.
I don't buy into the parallel world stuff either, but i still want their arrays and strings. I think you can easily chop and choose. 
It's clear to me that there is a perfect encoding, and furthermore it is UTF8...
Very cool! What else are you planning to build?
Very cool project! Is there more information available about the algorithm for determining whether there is or isn't a bike present?
Depending on how much time I can take out of work, next I guess would either be electromagnetism, angular momentum (classical and quantum), basic quantum computation (and tensors), or general relativity.
Since it's proprietary software the only thing I can share is that it's basically a stereo matching algorithm.
I stand corrected, I hadn't realized the client was external. I still wonder about the question, but then it's less relevant here. Yes Go has types but.
I think the current feeling is that UTF-8 as a representation for Text would be superior, given that UTF-16 imposes a lot of costs when the rest of the ecosystem is often using UTF-8, and furthermore there tends to be a great deal of ASCII even when you're dealing with text in multi-byte alphabets, e.g. HTTP headers, JSON syntax and suchlike. 
 data Widget = Widget { render :: DisplayRegion -&gt; Image, growHorizontal :: Bool, growVertical :: Bool, primaryAttribute :: Attr, withAttribute :: Attr -&gt; Widget } reminded me of something like struct functions { int (*blah); float (*blah2); }; A vtable in C can change its behavior unexpectedly during runtime, but a haskell datatype doesn't have that problem. Still, when I see a data type that acts like an expanded typeclass constraint, I can still feel some of the unease I felt from a vtable in C. I'm trying to decide whether the unease I feel is unjustified. Also, I agreed with &gt; “Understanding” a data structure is a lot easier than “understanding” a function. I'd probably try writing a recursive data type that describes a rendering recipe instead of a rendering function although a sum type could become difficult to manage as it grows.
Probably a stupid question, but why isn't it possible if it's marked as pinned ?
&gt; if you're working on ASCII So there is no multi-byte sequence.
Thank you, I learned something today.
I've always wondered if putting strictness annotations doesn't just push the issue onto the heap instead of the stack.
One problem with Luke's post is that the `Widget` type can contain anything whatsoever. You don't have any guarantees about what's inside unless you don't actually expose the Widget type directly to the user, and wrap it up in something more safe. With an existential and a class, there are only n implementations of the class, with only n combinations of method implementations, for n types. Each type has one canonical set of code that applies to it, which you can look up in your haddocks.
The problem isn't *on* the stack per se, it's just that a large stack usage means that you are evaluating a long chain of thunks, which is probably a space leak.
&gt; A vtable in C can change its behavior unexpectedly during runtime, but a haskell datatype doesn't have that problem. Still, when I see a data type that acts like an expanded typeclass constraint, I can still feel some of the unease I felt from a vtable in C. I'm trying to decide whether the unease I feel is unjustified. Once you remove the mutability of the vtable most of the problems dissapear. However, there still remains the problem as described nicely by /u/chrisdoner [here](https://www.reddit.com/r/haskell/comments/5284ss/data_structure_and_functions_shouldnt_be_found/d7i4jsi). Though this presents a problem for docs and maintaing a headspace around instances you also gain the ability to define two instances that make sense (ex. sum and product) and pass them around manually. I'd be interested to see a GHC extension that allowed a `data class` construct which barred what might be called "orphan values" 
If base moves String in a Backpack ;-), could Foundation provide the Backpack? EDIT: IOW and less tongue-in-cheek, I'm hinting about using foundation as an alternative implementation for the String type given a version of base where String is abstracted using Backpack.
Haskell with full meta programming is a *significantly* different language from actual Haskell. 
Can you explain why? 
I'm pretty sure this doesn't make sense in Haskell. The types already contain lots of functions, namely the constructors and the field names. There was a time when I tried to do this, but I have moved away from it. My suspicion is that whatever division minimizes exports and imports is best.
Can you go into a little more detail? One of my favorite things about TH vs other metaprogramming I've done (Ruby, Java reflection-ish-things) is that the AST is type checked, and then the compiled output is type checked.
/u/Platz I finally had the chance to review the post and I have realised you don't even need the initial accumulator value at all, as you can always used the one provided by `PaginatorLeaf`. This way we can rewrite our `foldPaginator` like this: foldPaginator :: (Monad m, Monoid a) =&gt; ForwardPaginator m i a -&gt; Maybe i -&gt; m a foldPaginator (PaginatorLeaf items) _ = return items foldPaginator (PaginatorFetch cont) tkn = do (t', acc', res) &lt;- cont tkn case res of leaf@(PaginatorLeaf _) -&gt; foldPaginator leaf Nothing nextFetch -&gt; (`mappend` acc') &lt;$&gt; foldPaginator nextFetch t' To demostrate its use, let's define a `sumPaginator` (using `Data.Monoid.Sum`): sumPaginator :: Int -&gt; ForwardPaginator Identity Int (Sum Int) sumPaginator !n = go 0 where go i = PaginatorFetch $ \_ -&gt; case n &gt; i of True -&gt; let next = i + 1 in next `seq` return (Just $ next, Sum next, go next) False -&gt; return (Nothing, Sum 0, PaginatorLeaf $ Sum 0) Now we can use `foldPaginator` like this: ghci&gt; runIdentity $ foldPaginator (sumPaginator 10) Nothing Sum {getSum = 55} 
In the case of **foldl'**, the strictness annotation does prevent creating a bunch of thunks on the heap (which, when evaluated, would blow the stack).
The complaint IIRC is that the AST is checked *after* running the TH splice, i.e. you can't guarantee that you will generate a well-typed expression. Recent GHCs address this with a typed Template Haskell, but I don't have experience using it. 
&gt; I'd probably try writing a recursive data type that describes a rendering recipe instead of a rendering function although a sum type could become difficult to manage as it grows. This is generally known as building a DSL or EDSL. It is an exceptionally tidy coding style. However, it can still be very useful (and not at all bad) to actually put functions even in that DSL that you have made. Depending on what you are doing, it can legitimately be the best way of solving the problem. 
Ouch it turns out to be almost the same as the other post by /u/tomejaguar , only less content.
Yeah I did, thanks.
 countCond cond = length . filter cond Would you mind expanding a little on what this does?
It selects all the elements from the list that satisfy `cond` and then counts the length of that selection.
IMHO you shouldn't mix unrelated things - but one thing being data and the other thing being a function is a weak reason to call them unrelated. There are often much stronger reasons that override that. I'd look first at whether they're part of the same abstraction or not (and up to what level of abstraction). For one thing, this is Haskell - functions are first class and therefore *are* data. Thinking of functions as ordinary data that just happen to have the property of being functions (just as other ordinary data might have the property of being numbers or whatever) is a key part of the functional mindset. In the case of implementing a complex data structure, the functions in any manual vtables are part of the works - part of the same (low-level) abstraction as the rest of the data structure building blocks. So binding them together is IMO fine. Using an existention type to do the job instead, BTW, is really the same thing - there's still vtables and functions as part of the data structure, it's just managed by the compiler. The title of the article is "Why OO Sucks by Joe Armstrong". That's not a title that makes we think "oh look - a balanced, objective article". &gt; When I was first introduced to the idea of OOP I was skeptical but didn’t know why - it just felt “wrong”. After its introduction OOP became very popular (I will explain why later) and criticising OOP was rather like “swearing in church”. OOness became something that every respectable language just had to have. This is true, and a very bad thing, and sadly not quite in the past. OOP isn't the only programming bandwagon - there have been many - but that's no defence. Joe Armstrong was reacting to advocates of a falsely claimed silver bullet, but those are exactly the circumstances when most people are likely to go too far the other way. Personally, I've found myself arguing quite extremist points that I strongly disagree with myself, purely because of the polarizing effect of arguing a particular side (sometimes for pedanting details of a quite distantly related discussion). OOP is sometimes useful, not always for the "official" reasons, but is no more a silver bullet than any other claimed silver bullet (and I include functional programming in that). &gt; Data structures just are. They don’t do anything. They are intrinsically declarative. “Understanding” a data structure is a lot easier than “understanding” a function. A key idea of functional programming is that this applies to (at least pure) functions too. They are intrinsically declarative. Functions are easy to understand by "equational reasoning". If, as some claim, Haskell is pure *even including* `IO` etc, the implication is even code that includes I/O, mutation etc (with some blame-it-on-the-runtime provisos) is intrinsically declarative and subject to equational reasoning. &gt; Functions are understood as black boxes that transform inputs to outputs A function that takes two arguments isn't seen as fundamentally different to a function that takes one argument. Why treat a function that takes zero arguments as fundamentally different? The empty set is still a set, the empty sequence is still a sequence, a function that takes an empty sequence of arguments is still a function - but in Haskell there's no distinct concept of a nullary function - a nullary function that returns an `Int` is the exact same thing as an `Int`. Especially given that laziness means values might be unforced, unevaluated, and might even evaluate to bottom. &gt; Everything has to be an object Only in "pure" OOP. C++ really triggered the OOP obsession when it was "C with objects", but C++ is multi-paradigm, and modern C++ focusses much more on generic programming that OOP, just using classes and objects as one of the tools to get things done. [Here's an ancient article](http://www.drdobbs.com/cpp/how-non-member-functions-improve-encapsu/184401197) about how using non-member functions improves encapsulation - a counter-example of one of the purist OOP principles, perfectly fine if you use OOP as a tool where appropriate but aren't a religious extremist about it. The inventor of Smalltalk coined the term "object oriented", but didn't invent the idea - he copied and adapted ideas from Simula. C++ also copied from Simula, not from Smalltalk. Simula, so far as I can tell, was an extended dialect of Algol and thus - like C++ - not pure OOP. &gt; In an OOPL data type definitions are spread out all over the place They can be - so what? Keep the things together that should be together. If you think that means all data definitions **always** go in one place and all function definitions **always** go somewhere else, that's a lie you're telling yourself to avoid thinking about how things really relate to each other. And I seriously doubt that's really what OP means - I bet he means that (at worst) within one particular module. Well, one view of what a class is (due to IIRC the Eiffel designer) is that it's a module with some extras (ie that it's also a type that can be instantiated, an interface to be implemented and extended by subtypes, etc). &gt; Objects have private state ... State is the root of all evil. In particular functions with side effects should be avoided. Some more extremism. Actually, in an argument I had once about whether the Haskell type system is referentially transparent (I still say no by the definition of referential transparency I know) and the only admission I got that I had a point was when the person I was arguing with said something like "that definition of referential transparency was found not to be that useful so I'm using a different one" (which wasn't provided). Well, one other more general form of referential transparency is used in a book "Elements of Programming" (Stepanov, McJones). They don't call it referential transparency, they call it "regularity", give a formal definition, and use the properties from that definition to apply equational reasoning to generic code that includes I/O and mutation of memory. There are other related generalizations of referential transparency. One that we routinely use in Haskell - the immutability invariant isn't the only way to get referential transparency, the unshared invariant (only this code can see or access the data) is another. `Monad` provides (among other things) a way to work with "objects" that we cannot directly access, cannot even name, so cannot share - like the "world" in that explanation of `IO`. So the only changes we can observe are those we cause, at least within the confines of relevant expressions. Obviously, the object might already have been shared before the "monadic context" was entered - particularly in the case of `IO`, the world is shared by entities outside our program). 
Take a look at the diagram on this page: http://www.staff.city.ac.uk/~ross/papers/FingerTree.html You can see that the second digit in the spine (red) has 4 links, while the some of the "ends" (blue) can have one link up. Basically, you use a uniform structure for the digits. In the end, it's a consequence of using 2-3 trees specifically. If we used 1-3 instead of 1-4 it would be a binary tree, so this is the simplest possible structure that is still better than a binary tree. ^(Sidenote: I went to high school with that blog's author. Cool to see everyone ending up at Haskell somehow)
Shouldn't one of the `minimum`s be a `maximum` (and which one)?
Yeah, that was my mistake. I noticed it a few minutes after posting this haha.
Let me rephrase the question for a bit: why do digit support up to 4 elm? Sorry for the confusion, but I think in the blog I read author just use affix to denote digit, where I assume you use affix to denote blue link
Because you're using the same structure for every single digit - both the ones in the spine and the ones at the ends. Since the ones in the spine use 4 links, we just use 4 (also sorry for confusing nodes/digits in my original post; I fixed it)
The `Widget` in https://github.com/jtdaugherty/brick/blob/0151c159d5b85ad422208d22d09befd979431c04/src/Brick/Types.hs#L131-L138 essentially _is_ a typeclass; it's just encoded using plain old functions instead of typeclass constraints, using the style shown in /u/Tekmo's [blog post](http://www.haskellforall.com/2012/05/scrap-your-type-classes.html) about it. I think what Joe Armstrong means by that stricture is, data structures and _behaviours_ shouldn't be tied together. That's quite right, because later you might need to add a new behaviour to an existing data structure, or make a new data structure conform to an existing behaviour (e.g. make your new `Widget` a `Drawable`). This is exactly the problem that typeclasses solve--decoupling the data from the behaviour so they can be mixed-and-matched later. So that's what you're seeing in this `Widget` type.
Unfortunately, the ideas you're bringing together are just... all over the place. They are using the same words in different ways, so it's hard to make much sense of questions about why they conflict. I can throw out a few extra ideas, though. One is that first-class functions matter a lot. A lot of thinking about "object oriented" programming in the sense of C++ and Java is built around the assumptions of those languages that functions are a kind of *structural* property of the program, that there's a fixed number of them, each written by a programmer before the program is compiled, and tied to the lines of code that define it. This is a very limiting idea, and these mainstream OO languages have propagated it for many years. (It's worth noting that this is in no way an essential part of OO programming: Smalltalk did *not* have this limitation, nor do many more dynamic OO languages, and even C++ and Java are moving away from it, though with some ugly wrinkles due to backward compatibility). So with this in mind, one possible meaning of the complaint about mixing types and functions is that functions are more properly associated with *values* rather than types. When it comes to whether functions ought to be mixed with other *values*, the answer should be: yes, of course. After all, functions are data, just like anything else. Indeed, there are isomorphisms between function types and non-function types: `forall a. a -&gt; a -&gt; a` is (if you ignore `seq` anyway) isomorphic to `Bool`, even though one is a function type, and the other is obviously not. So it would make little sense to argue that function types should somehow be kept apart from non-function types.
Oh, you're right I missed that. Yes I'd also be worried about the cost of having a sum type, and if the benchmarks don't show a difference then I would suspect the benchmarks :)
tl;dr ?
There is something on [GitHub](https://github.com/ekmett/propagators).
Indeed, a concise summary of how Data.Text can be improved would be useful.
The name is cool, but going to be a bit unsearchable though. You'll get Isaac Asimov's books, thousands of tutorials on makeup and zurb-foundation. 
Others have already posted reasonable solutions, but here is mine. I think it shows off the regular structure a bit better: import Data.Function (on) feedback :: [Card] -&gt; [Card] -&gt; (Int, Int, Int, Int, Int) feedback guess ans = (a, b, c, d, e) where a = countElemBy ((==) `on` id) guess ans c = countElemBy ((==) `on` rank) guess ans e = countElemBy ((==) `on` suit) guess ans b = count (&lt; minimum (map rank guess)) (map rank ans) d = count (&gt; maximum (map rank guess)) (map rank ans) count :: (a -&gt; Bool) -&gt; [a] -&gt; Int count p xs = length $ filter p xs -- Like ``count (`elem` ys) xs`` with a custom equality test. countElemBy :: (a -&gt; b -&gt; Bool) -&gt; [a] -&gt; [b] -&gt; Int countElemBy p xs ys = count (flip any ys . p) xs Note that ``(==) `on` id`` is the same as `(==)`. I only wrote it that way for symmetry.
The best line from the presentation: &gt; Computer scientists commonly choose models which have bottoms, but prefer them topless.
Why isn't ghc-core directly the answer to your question?
I really think that the whole approach is not terribly scientific to be honest. You can't single out a particular language feature, assume that it makes a big impact on code quality, and then fish for evidence to support that assumption. A proper approach would be to sample a large number of projects written in different languages. Perhaps something like large open source projects on GitHub. The metrics could be how many bugs are reported per projects, how quickly they're fixed, how often features are added, and so on. Then if trends are found where particular languages show statistically significant results in some categories, you could make a hypothesis as to why that is. Of course, doing this is difficult and it's hard to control for different variables, but it seems like those kinds of studies would be the way to actually start understanding what language features have real measurable impact in practice.
From my point of view, hackage is a bazaar of mostly single maintainer packages, which are likely to appear/disappear at any moment. Some of those packages only exists because some maintainers disappear, some because the maintainer of X is too busy to integrate further things so you get X-Y. We want something more self-serving with foundation, where we have more maintainers, and more people able to influence design. Having more things in one place, means we're more likely to have a consistent {release cycle, design, benchmarks suite, tests suite}, and more people able to fill for each other when we inevitably become unable to handle maintainership (too busy, not interested anymore, etc.) In my opinion, it's hard to build the language foundation on a bazaar (types like string, vector, ...). base is a good example of a non-bazaar design. Sadly, base stopped evolving and helping define what's standard a lot time ago.
The sum type is not that costly it seems, but you need to break abstractions and specialize each functions to the elements of the sum. for example, this would be slow, because unsafeIndex would pattern match on the sum every time: break pred = loop 0 where loop i = if pred (unsafeIndex i) then ... unsafeIndex vec = case vec of Addr -&gt; ... ByteArray -&gt; ... Whereas this would be really fast: break pred vec = case vec of Addr ptr -&gt; loopPtr ptr 0 ByteArray ba -&gt; loopNative ba 0 where loopNative ba i = if pred (primByteArrayIndex ba i) then ... loopAddr ptr i = if pred (primAddrIndex ptr i) then ... if the sum type is handled really early, it is very cheap. It forces code duplication and breaking abstractions, but ultimately I think it's worth it having the capability of this type. We have one benchmarks with bytestring so far, and it is really promising. happy to have someone audit if the benchmark is wrong or not :) This is basically doing a break (breakElem in the case of foundation), very close to the beginning, in the middle, and at the end of the bytestring. (the first benchmark hint at the greater cost of the sum type): break/#word8-start foundation =&gt; time 15.59 ns (15.34 ns .. 15.90 ns) bytestring =&gt; time 14.84 ns (14.51 ns .. 15.17 ns) break/#word8-middle foundation =&gt; time 108.6 ns (107.2 ns .. 110.1 ns) bytestring =&gt; time 135.8 ns (132.7 ns .. 138.3 ns) break/#word8-end foundation =&gt; time 197.8 ns (195.0 ns .. 201.7 ns) bytestring =&gt; time 251.8 ns (248.5 ns .. 255.5 ns) 
Uriel was probably talking specifically about c in that case.
Doesn't Agda have a recursion checker that ensures any recursive call you do only occurs on a substructure of the original data structure?
Thanks, added :)
If types would be just an ordinary sets we can apply functions to (`filter`, `split`, `inclusion`, `exclusion`), wouldn't we get **dependant types** for free? (Probably with some other cool features I don't see right now.)
They are very similar, but types are restricted in fundamental ways. For example you cannot form the union of any two types, say, `Bool` and `Char`. Of course there is `Either`, but that gives you *another* type.
Answering to your title: `import`, `=`.
Oh thanks a lot!
They aren't empty. They both contain `undefined`, and in fact, they have different members since `(undefined :: X)` isn't a member of `Y`.
A number of Haskell data types aren't sets, for example: data D = MkD (D -&gt; Bool) where the `MkD` constructor is a bijection between `D` and its power set, which isn't possible in set theory, so `D` can't be modeled as a set. 
Yes, but requiring calls to substructures is very restrictive. For instance quicksort recurses on the results of partitioning the input, which aren't substructures of the inputs.
&gt; all your types now have an infinite amount of inhabitants. ?
Amusingly I have more of this stuff working nicely in C++ than Haskell.
Are their denotations distinct? Seems to me that any attempt to evaluate such a thunk to get a constructor will get you the same thing.
&gt; Template Haskell is frowned upon Isn't template Haskell used in many popular libraries like Lens, Free, Yesod, etc? 
When the distinction between "empty types have no inhabitants" and "empty types are only inhabited by ⊥" came up in an [earlier thread](https://www.reddit.com/r/haskell/comments/4zm2gc/notes_on/d75o4wt), I realized that the source of confusion is that there are at least four different kinds of values one might want to talk about in Haskell, and we're rarely explicit about which one we have in mind. In this case `X` and `Y` have zero "total values", one "denotational value" (⊥), and an infinite number of "operational values" (every expression of the proper type evaluating to ⊥). And while `Either Void a` has the same number of total values as `a`, it has more denotational values since ⊥ and `Left ⊥` are distinct. The notion of evaluating a thunk to get a constructor refers to yet another kind of values, "WHNF values", in which forcing `Left undefined` gets you a constructor holding a thunk whereas forcing `undefined` causes an exception to be thrown before you can get your result. And the notion of values "distinguishable by catch" seems to give rise to a fifth kind I haven't thought about yet :)
They all evaluate to the same total value, to the same denotational value, and to the same WHNF value. So yes, most of the time, we do want to consider those the same value. It is only their operational values which are different, because 2, a thunk computing `3 - 1` and a thunk computing `4 - 2` are certainly represented using different bit patterns in memory. And it is useful to think of those as values when we think operationally, as in "I compute the length of `xs = [] + [3 - 1]` and the head of `xs` now holds the thunk `&lt;3 - 1&gt;`, I compute its sum and now the head holds 2".
I think there is an infinite number of inhabitants. a = MkD (const True) b = MkD (const False) c = MkD (\(MkD f) -&gt; f a) d = MkD (\(MkD f) -&gt; f b) e = MkD (\(MkD f) -&gt; f c) ...
&gt; I think it's really bad policy to refer to build/foldr fusion as 'stream fusion'. Absolutely agree. In my thesis I tried to consistently use the terminology of build/fold or unfoldr/unbuild or stream fusion. And yes build/fold and unfoldr/unbuild are duals (not just in a way, but formally), and stream fusion is a slight variation on unfoldr/unbuild (mainly the addition of Skip for practical reasons).
Thanks for the response and for the vocabulary clarification concerning build/foldr fusion and stream fusion. Your response hints that `build`/`foldr` fusion doesn't suffer from the same performance problem from successive appends. Is this correct?
Ah, I had not realized that stream/unstream was a variation on unfoldr/unbuild. I hadn't come across any examples of unfoldr/unbuild other that the [haskell wiki page on short cut fusion](https://wiki.haskell.org/Correctness_of_short_cut_fusion). That's good to know. Thanks.
Also, I've tried to make the language in the post more accurate.
This is great! This is exactly how we hoped `servant` would be used and extended. Two small questions. Why does the GithubEvent HasServer instance use `addAuthCheck` (and with a 404 rather than 400)? Was it just that none of the alternative were more appropriate? We're open to increasing the number of fields in Delayed (sticking to the order [here](https://github.com/for-GET/http-decision-diagram)), but didn't want to do that without a concrete reason first, so if you feel like there's a check missing, let us know! Why does the second example have the more verbose: type WebhookApi = "repo1" :&gt; ( GitHubEvent '[ 'WebhookPingEvent ] :&gt; GitHubSignedReqBody' 'Repo1 '[JSON] Object :&gt; Post '[JSON] () :&lt;|&gt; GitHubEvent '[ 'WebhookWildcardEvent ] :&gt; GitHubSignedReqBody' 'Repo1 '[JSON] Object :&gt; Post '[JSON] () ) ... repo1ping :: RepoWebhookEvent -&gt; Object -&gt; Handler () repo1ping _ _ = liftIO $ putStrLn "got ping on repo1!" repo1any :: RepoWebhookEvent -&gt; Object -&gt; Handler () repo1any e _ = liftIO $ putStrLn $ "got event on repo 1: " ++ show e Instead of: type WebhookApi = "repo1" :&gt; GitHubEvent '[ 'WebhookWildcardEvent ] :&gt; GitHubSignedReqBody' 'Repo1 '[JSON] Object :&gt; Post '[JSON] () ... repo1 :: RepoWebhookEvent -&gt; Object -&gt; Handler () repo1 WebhookPingEven _ = liftIO $ putStrLn "got ping on repo1!" repo1 e _ = liftIO $ putStrLn $ "got event on repo 1: " ++ show e These are equivalent, no?
Check out the intro chapter in my thesis. It gives a bit of the history.
Now that you mention it, crew may be flexible enough that I can get it to work just by specifying the right commands. I'll check it out and report back. 
I agree that most people don't know what the final extension will look like (myself included!). I just doubt that changing the name will help very much. ;-)
Here's some good use of category theory applied to Haskell. In this case monad homomorphisms: https://www.youtube.com/watch?v=YTaNkWjd-ac 
Here's some good use of category theory applied to Haskell. In this case monad homomorphisms: https://www.youtube.com/watch?v=YTaNkWjd-ac 
All of them but `a` and `b` have the issue that they quickly end up in infinite loops when provided something other than `a` or `b` as input. 
Thanks for the feedback! I'm loving using servant so far :) For the `HasServer` instance of `GitHubEvent`, the only appropriate functions for adding the checks would be `addAuthCheck` and `addBodyCheck`, since I need to provide the matched event to the handler function; looking back, I think `addBodyCheck` would be more appropriate. The reason I used 404 instead of 400 was that I was thinking of the `X-Github-Event` header as performing some kind of subrouting, since all events are sent to the same URL by GitHub with the only indication that different events occur being transferred via that header. From that perspective, if there is no route that can handle the specified event, then no handler was found; hence, I chose 404. I'm no HTTP expert though, so I'm willing to be convinced that 400 is more appropriate if that's the case. As for the second example, you're absolutely right. It's much simpler to just match on the event type in the same handler than to write two handlers and use the dispatching logic built-in to `GitHubEvent`. I will amend the example to do that. Thanks again for the suggestions!
Another option which I personally like is data Void = Void { magic :: forall a. a } This requires `RankNTypes`. The nice thing is that it allows you to implement methods accepting a `Void`, e.g. in its `Show` instance, like this: instance Show Void where show = magic
That gets right to the heart of it
I find the Haskell variant saner, though. It's one thing Go did right compared to C family languages. Edit: just noticed I confused threads. Ignore this :)
ghc's build/foldr fusion tries to handle appends with augment https://github.com/ghc/ghc/blob/master/libraries/base/GHC/Base.hs#L835 https://github.com/ghc/ghc/blob/master/libraries/base/GHC/Base.hs#L957 Thus if I compile main = print $ sum $ [17] ++ [27] ++ [32::Int] with `-O2` and `-ddump-rule-firings` I see that Rule fired: fold/build fires three times and Rule fired: augment/build fires twice. In this particular case ghc manages to reduce the sum to a constant since we just see `$wshowSignedInt 0# 76# ([] @ Char) }` in the core. 
Yep. Spelling it out a bit more (not type checked), maybe something like: data Cat f a b where Id :: Cat f a a -- Maybe your left-to-right order is better here... Compose :: f b c -&gt; Cat f a b -&gt; Cat f a c instance Category (Cat c) where id = Id Id . x = x (Compose f x) . y = Compose f (x . y) liftCat :: f a b -&gt; Cat f a b liftCat = flip Compose Id lowerCat :: Category f =&gt; Cat f a b -&gt; f a b lowerCat Id = id lowerCat (Compose f g) = f . lowerCat g hoistCat :: (forall x y. f x y -&gt; g x y) -&gt; Cat f a b -&gt; Cat g a b hoistCat eta Id = Id hoistCat eta (Compose f g) = Compose (eta f) (hoistCat eta g) -- ...
Thanks for the help, it forms a category instance Category (FreeCat cat) where id :: (FreeCat cat) a a id = Nil (.) :: (FreeCat cat) b c -&gt; (FreeCat cat) a b -&gt; (FreeCat cat) a c xs . Nil = xs xs . Cons y ys = Cons y (xs . ys) 
As an example I'll model the free category of a graph from Awodey's book: data Vertex = A | B | C | D data Graph :: Cat Vertex where X :: Graph C A Z :: Graph A B U :: Graph A D Y :: Graph D B deriving instance Show (Graph a b) We can define a `Show (Cat graph a b)` using [`ForallT`](https://hackage.haskell.org/package/constraints-0.8/docs/Data-Constraint-Forall.html#t:ForallT) import Data.Constraint import Data.Constraint.Forall -- forall xx yy. Show (Graph xx yy) instShow :: ForallT Show graph :- Show (graph xx yy) instShow = instT @_ @_ @_ @Show -- instance (forall xx yy. Show (graph xx yy)) =&gt; Show (TList graph a b) instance ForallT Show graph =&gt; Show (TList graph a b) where show Nil = "" show (Cons (x :: graph x y) Nil) = show x \\ instShow @graph @x @y show (Cons (x :: graph x y) xs) = show x ++ " -&gt; " ++ show xs \\ instShow @graph @x @y And start composing x :: (TList Graph) C A x = liftCat X z :: (TList Graph) A B z = liftCat Z u :: (TList Graph) A D u = liftCat U y :: (TList Graph) D B y = liftCat Y &gt;&gt;&gt; y . u . x X -&gt; U -&gt; Y &gt;&gt;&gt; z . x X -&gt; Z ---- Is there a `fold...` version? type f ~&gt; g = forall a. f a -&gt; g a foldMap :: Monoid m =&gt; (a -&gt; m) -&gt; ([a] -&gt; m) foldFree :: Monad m =&gt; (f ~&gt; m) -&gt; (Free f ~&gt; m)
The particular encoding you've given here is often called a "Thrist" for "threaded list". This has the same problem that the `[]` vs Free `Monoid` construction has. It is only the free monoid for finite lists or ones where all the infinite recursion occurs on the right, you can't re-associate infinite left recursion through (.), and also it has pretty awful asymptotic performance for concatenation ... the main thing you do with it! A better finite form is to use reflection without remorse. This fixes the asymptotics of (.) at the cost of big fat constants. If you're only going to consume the result once, then a better infinite form is to finally encode newtype Free k p a b = Free { runFree :: forall q. k q =&gt; (forall c d. p c d -&gt; q c d) -&gt; q a b } Now `Free Category` can be the free category, `Free Semigroupoid` etc. Sadly the lack of |- in our constraint system makes the common shared `Free` type at kind * -&gt; * -&gt; * like this less usable than implementing it piecemeal for each abstraction. This latter construction is truly a 'free' form in the funny DCPO setting that is hask where you can meaningfully have infinite chains of (.)'s. This is the same way that a similar Free Monoid construction newtype Free p a = Free { runFree :: forall r. p r =&gt; (a -&gt; r) -&gt; r } borrows the Monoid from `r` to do its work. See http://comonad.com/reader/2015/free-monoids-in-haskell/ for more motivation.
[removed]
This one is one of my favorite definitions, and if it wasn't for a desire for a fully Haskell 98 package when I shipped `void` originally it is likely where I would have gone. You can `newtype` it though, which helps out some. The quantifier is inside.
You answered my question about `fold...` type f ~&gt; g = forall a. f a -&gt; g a type f ~~&gt; g = forall a b. f a b -&gt; g a b foldMap :: Monoid m =&gt; (a -&gt; m) -&gt; ([a] -&gt; m) foldFree :: Monad m =&gt; (a ~&gt; m) -&gt; (Free a ~&gt; m) foldCat :: Category m =&gt; (a ~~&gt; m) -&gt; (Free Category a ~~&gt; m) foldCat f (Free cat) = cat f ---- Can you elaborate how `|-` (implication constraints?) would help? Will we eventually unify all the free constructions under a single name?
You showed me the Monoid case at Compose Conference NYC 2016 when I asked you about Free Category, but this encoding doesn't work for [my purpose](https://github.com/gelisam/category-syntax#readme). I couldn't articulate why back then, but now I can: this representation allows me to instantiate the free category to the category of my choice, but it doesn't allow me to pattern-match on some constructors to see what the normalized expression looks like, like I can with `[]`. For my purpose, I'm looking at a finite piece of code, so I don't care about the infinite case. To clarify, since I'm manipulating code, I'm starting with an `Exp` AST which would have a non-lawful Category instance if I defined it: since `Then` is a constructor, `Then e1 (Then e2 e3) != Then (Then e1 e2) e3`. I want to simplify the code, for example by rewriting `Then e Id` to `e`. For Category, this is the only simplification and it's simple to do by hand, but with more structure (see my top-level comment) the simplifications quickly become much more difficult. By normalizing `Exp` to a free category, the task should become trivial, because `Then e Id` and `e` should both normalize to the same value. Unfortunately this is not the case with `Free Category`, as they normalize to two different functions which yield back `Then e Id` and `e` when I instantiate `r` to my non-lawful `Exp`. Obviously, the problem is that I should only instantiate `r` with lawful instances, but then I'm back where I started: I need to instantiate `r` with some concrete data type `FreeCategory` which has a lawful Category instance and which doesn't drop any other information. That is, I need a *concrete* free category datatype I can pattern-match on, like `TList`, not an abstract one like `Free Category`.
I think these two can be used together - one shows how to represent the current permissions available at the type level, the other how to require at the type level particular permissions in order to access the db.
This kind of comes down to the same thing. The two `undefined`s are different because types are proposed first and not constructed from their inhabitants
Glad to hear that :)
Let's go to define instances, shall we? instance Semigroupoid (Free Semigroupoid) instance Category (Free Category) so far so good. instance Arrow (Free Arrow) now needs instance Category (Free Arrow) so now I have to define it pointwise for every single subtype of category. This is inherently a FlexibleInstances situation, as the head isn't of the form (Free p), so inference will suck. And I have cases where I wind up defining class hierarchies 15 levels deep. I'm not signing up for Emacs Pinky here. =) Better would be instance (p |- Semigroupoid) =&gt; Semigroupoid (Free p) instance (p |- Category) =&gt; Category (Free p) instance (p |- Arrow) =&gt; Arrow (Free p) Now I pay for the individual dimensions I'm extending in. Inference would work perfectly as everything is a normal non-Flexible instance, and all is right with the world. This would require a |- in the Constraint kind as a base case and inductively lifted from a kind k it inhabits to kinds `i -&gt; k` to allow it arguments. (Such categories are also thin, and it is an admissable addition to Constraint to make it closed.)
separate permission - I imagined a single permission type such as `EditDocument` but different constructors for this type representing the different ways to get to this permission. Functions `a -&gt; permission`... interesting. I will definitely think about this further.
Hey, thanks for giving my code some publicity. :)
Well, it looks like it sort of works. If I set up crew to act as stack, and put a .crew containing: stack: cmd: nix-shell --run intero appears to behave correctly (it looks like the only command intero uses is stack ghci, which this translates to nix-shell --run ghci). However, this completely breaks stack for any other use, I can't immediately see how to map stack build into something else like raw stack build or cabal build. That may not be that big of a problem since my goal here was to be able to use use nix without stack, though I could achieve the same goal without crew by just using a simple shell script to translate stack ghci into nix-shell --run ghci
I think, preferably, you should use `()` instead of `Void`. `Void` has no valid values, which means that any product type it 'infects' also has no values. For example, `(a, Void)` is semantically unpopulated, even though haskell lets you create values like `(1, undefined)`. This is if you're using `Void` to remove parts in the way I think you are.
[removed]
I use both! For instance: &gt; data Exp v a la le b &gt; &gt; = Lam la b (Exp v a la le b) &gt; &gt; | Var v b &gt; &gt; | App a (Exp v a la le b) (Exp v a la le b) &gt; &gt; | Let le b (Exp v a la le b) (Exp v a la le b) &gt; &gt; type SimpleExp b = Exp () () () () b &gt; &gt; lambdaficate :: SimpleExp b -&gt; Exp v a la Void b &gt; &gt; simplify :: Exp v a la le b -&gt; SimpleExp b &gt; I can implement simplify by turning every v, a, la, le into (), which is convenient. Then I can implement lambdaficate by turning all of the Let instances into a Lam then an App. Similar tricks are used in conduit and many other libraries that want to have a very general type but refine it to subsets of its constructors in certain situations or after certain operations. If you really want to get into it you can export pattern synonyms and hide this whole thing, too, which is an ideal situation because now you have this sort of "chopping" type of subtyping but its ugly guts totally hidden from the user of the library.
Ah nice. Yes, using void is appropriate with a sum type.
I was also considering "[coassociate](https://hackage.haskell.org/package/category-extras-0.53.5/docs/Control-Category-Associative.html#v:coassociate)", would that be a good middle ground?
Hi, just a quick note-- if anyone's curious about seeing a real world example of a game built in this way, I have a simple example here: https://github.com/asivitz/Hickory/blob/master/Example/Shooter/shooter.hs At some point soon I'll publish a larger, more complex example.
Very good article. However there stuffs that I don't understand and I would appreciate if someone could clarify. What I don't grasp is how the input is passed to the model. GUI programming, GUI toolkits and Javascript are single thread and it means that you can't read a input and wait to it arrive like. So the standard way is to pass a callback function that will be called by the Event Loop (not visible and exposed to the programmer) when the input arrive. I think this code wouldn't be possible in most GUI toolkit even in Javascript. Model appState = newAppState(); // Create the initial program state while(true) { foreach (Input input in getInputs()) { appState = update(appState, input); // Update the program state for each new input } renderScreen(appState); // Update the view based on the new program state } update :: Model -&gt; Input -&gt; Model The event loop is not exposed to the library user, therefore you cannot do: click = getClick.button getClick () if click then window.text = "Hell worl"; else () Actually it is done in this way: button.Onclick ( fun click -&gt; window.Text = "hello world") I don't understand how this approach gets rid or abstract away the callbacks. The RxJS for example works by composing callbacks. In the RxJs observables are are the events that a callback function subscribe to. The only way I can see this to work is: button.OnClick (fun click -&gt; appState = update(appState, Click click) RenderView (appState)) window.mouseMove (func x y -&gt; appState = update(appState, MousePos x y) RenderView (appState)) I guess it woud be better to merge renderView with the update function. GUI programming is hard because you have multiple asynchronous inputs, callbacks and multiple updates in each callback. I might be better to make a big update function called from each callback passing a event argument as message with algebraic data types as the article shows. 
You have a couple of options here. The first is to asynchronously collect the inputs, and then once per frame process all the pending inputs at once. That's the approach I take. You may need to write some code to insert callbacks that collects the input into some mutable variable, or my library handles that for a desktop GLFW based game The second is to run the update function for each input as you get it, instead of folding over a list of them. I think that's more in line with the approach that React/Flux takes, with its dispatcher system. (This looks like what you're getting at toward the end of your message.) You can 'merge' renderView with the update function for some top level update routine, but at some point underneath you can still have a pure update function as the core of your program.
`associateR` and `associateL`?
technically you could do without `import` considering that all the imports are in a specific place and in a specific format. It make the detection of where it end more complicated, but definitely possible and I think unambiguous.
[bookkeeper](https://hackage.haskell.org/package/bookkeeper) records. The same idea can probably be applied to any type-level record library. About Opaleye, I assume not out of the box. Although it should be possible (but I'm guessing here) to write an adapter for type-level records -&gt; Opaleye table definitions.
You may try `-fno-ghci-sandbox`. 
You could try and write out the `DeepSeq` instances for `Block`, `Entity` and `Chunk` yourself instead of using the default instance based on `Generic`. Code looks great, good job!
How does it compare performance-wise with Rust or C++? If your implementation is already there you might have already hit the limit.
The term model (where a `T` type is mapped to the countable set of all `t` terms such that `t :: T`) doesn't tell us anything interesting about the structure and properties of types. We can interpret `-&gt;` more informatively, but if we don't interpret it as the set-theoretic function space, then we don't interpret types as sets either (but rather complete partial orders or something else). I think it's justified to say that `D` doesn't have a sensible set-theoretic interpretation.
Great idea, thanks. This shaved ~100ms off each tick.
`ghci` uses the `-threaded` runtime by default, while compiling code does not use `-threaded` unless you tell it to. I've had that cause issues in the past--never segfaulting, though.
Also I do think that you shouldn't force everything all the time. I don't really know if the updateChunks does something with all its data ...
Interesting. I've done something similar (in effect) with OAuth scopes in the past, where some context (Monad) needs to be parameterised over the set of scopes the operations within the context require. I used type families / type-level sets and it worked out reasonably well, but type inference suffered quite horribly.
It doesn't do anything with the data at all. The point of forcing is to artificially be close to the examples in the blog where everything gets processed each tick. Not forcing anything ends up with a super lazy runs-in-0ms program, which defeats the whole purpose of doing this.
Well as far as I can tell your data type XXX has only one "constructor" (Yo). Yo only allows Nats as kind arguments and thus you cannot create an XXX n with n instantiated to something else than a Nat... Am i missing something here? Edit: wich is what the Compiler tells you when you follow cosmicus advice
Or `LANGUAGE StrictData`.
Would you be better off with `Text` rather than `String`? I believe `rnf` on a list walks the entire list. And `String` is `[Char]`. Also, did you try unboxing the strict fields? There's a flag/pragma for that.
This is my preferred definition except for using data instead of newtype. 
Oh, good point about String. I didn't even consider that rnf would walk over those. Unboxing the strict fields was on my list of things to do for the "Faster" version.
I am using GHC 7.10.3 so these are not available to me. Upgrading is an option of course, but I wanted to wait a tad longer.
There are several things that don't seem to be correct in your implementation, compared to "faster C#": * chunks hold a list of blockIds, not a list of blocks (edit: I now see this is an optimization, sorry!) * `updateEntityPosition` doesn't update entity positions * The time measurement system isn't very robust Here is a version I quickly wrote that seems to be faster, and doesn't require concatenation of containers, so it works with a `Vector` : https://gist.github.com/bartavelle/c1aaf8a47158132ee12caf42449f9066 Perhaps the most noteworthy part is how it traverses the chunk vector to update it, and create new ones. It doesn't reproduce the exact benchmark though, but uses criterion to compute how long it takes to load the world and run 100 loops. 
The problem with the non-lawful instance is that I cannot use `Free Category` to normalize `Exp`. The reason I want a `FreeAssociativeCategory` etc. is to normalize `Exp`, not to obtain a lawful version of `Exp`. The reason I want to normalize `Exp` is to make it easier to detect whether I can simplify the expression to use only operations from a lower level. The reason I want to simplify is because it's easier to generate the extra `dup`, `swap` and `associate` operations assuming that all those operations are allowed than to generate them assuming only `associate` operations are allowed, then if that fails, assuming `swap` is allowed as well, etc. And the reason I want to generate operations from the lowest possible level is so that category-syntax works for all of those levels, not just Cartesian.
I do have -threaded in my GHC options in my cabal file, and when I fire up GHCI, it says: &gt; The following options are incompatible with GHCi and have not been passed to it: -threaded Are you saying this message comes up because GHCI uses this option by default?
 I'm working on authentication for servant right now, but hadn't until now really thought about how to do authorization. But I think the answer is yes. Supposing we can infer the type for the 'minimal' `prf` such that `Set prf` has the authorization to perform action `x`, for any `x`, then we could declare a class: class HasAuth val m prf where hasAuth :: val -&gt; m (Maybe prf) Which would try to show that `val` (e.g,, a `User`) has the permission to do the action, and return `Nothing` if it failed. What this means in practice is that you'd use `bookkeeper-permissions` normally in your handlers, declare some instances such as: instance HasAuth User IO Admin where hasAuth u = &lt;lookup in DB if user is admin&gt; And the framework would throw a 403 (Forbidden) if the user is not an admin. It'd be quite nifty. One concern, though, is that `bookkeeper` doesn't yet scale well to larger datatypes (compilation times become terrible). Nickolay Kudasov came up with some ideas about how to re-write it, but we'd have to do that first. 
If this works then the problem is likely something related to thread local storage.
I think `PolymorphicComponents` is weaker than `RankNTypes` but strong enough for this example.
Thank you for the suggestion. I was unsure how to produce a track trace for a crash in GHCI, so I did some googling and found [this post by Simon Marlow](https://simonmar.github.io/posts/2016-02-12-Stack-traces-in-GHCi.html). Because I am using GHC 8.0.1, I thought I would give it a shot. While attempting to use the suggestions in the post, I tried the following and it worked fine (no segfault): $ stack ghci --library-profiling --ghci-options="-fexternal-interpreter -prof" The following GHC options are incompatible with GHCi and have not been passed to it: -threaded Using main module: 1. Package `simpleservantblog' component exe:simpleservantblog-exe with main-is file: /Users/erewok/projects/haskell/simpleservantblog/app/Main.hs Configuring GHCi with the following packages: simpleservantblog GHCi, version 8.0.1: http://www.haskell.org/ghc/ :? for help Loaded GHCi configuration from /Users/erewok/.ghci [ compiling the files...] Ok, modules loaded: Api, Config, Api.Post, Api.User, Html.Home, Models.Post, Models.Author. [8 of 8] Compiling Main Ok, modules loaded: Api, Config, Api.Post, Api.User, Html.Home, Models.Post, Models.Author, Main. Loaded GHCi configuration from /private/tmp/ghci31931/ghci-script gchi&gt; :set -XOverloadedStrings gchi&gt; import Database.PostgreSQL.Simple gchi&gt; let connstr = "host=localhost port=5432 user=myusername password=mypasswd dbname=local" gchi&gt; conn &lt;- connectPostgreSQL connstr gchi&gt; Leaving GHCi. I can keep attempting to generate a stack trace if you think it would be useful. I am afraid my problem is specific to my computer, but if you would like me to try to generate a stack trace and submit a ticket, I would be happy to attempt any suggestions on how to do so.
It seems that you do not understand meaning of `do`. You use it in form of do let a = b a where `a` is pure expression. What you want is let a = b in a or a where a = b for function like `updateChunks`. Types with fixed size are `Int`, `Word`, `Int32`... in module `Data.Int` and `Data.Word`. Class `Bounded` will tell you exact range eg. `maxBound :: Int`.
Before I landed my first Haskell job I was doing part-time Java programming (I was a student), and I was using breakpoints and the debugger extensively for exploration, e.g. learning a new code base. So I understand how you're feeling. In Haskell what I do is: 1. I try to get familiar with the types first. Use a tag generator (I use `fast-tags`) and an editor that understands tags. When you see an use of a type, just jump to the definition and try to understand what it is. 2. Use `Debug.Trace.trace` when studying the control flow, e.g. in what order things get run. A problem with (2) is that sometimes you don't have Show instances for things you want to take a peek. In these cases if you're lucky you can just add some `deriving (Show)` lines. If that doesn't work then you have a bigger problem. Try to print printable parts of the types etc. It won't be as easy as using breakpoints and debugger, but you can make pretty good progress this way. I figured GHC this way, and I now have 93 commits (6,459++ / 3,216--). So it certainly works.
I’ve seen your `associate`/`disassociate` named `assocl` (“associate leftward”) and `assocr` (“associate rightward”) respectively: assocl :: (a `f` (b `f` c)) `k` ((a `f` b) `f` c) assocr :: ((a `f` b) `f` c) `k` (a `f` (b `f` c)) Where `f` could be `(,)`, `Either`, or something more exotic. 
We've started getting the videos back from the Compose :: Melbourne event at the end of August!
To be fair, it's perfectly legal Haskell to use do for non-Monadic things. It's just unusual 
That was one serious optimization. I've gone from 700ms to about 70ms per tick now.
Write some helper routines to make using the repl better. Print out a data structure (maybe in dot format so that you can get as graphviz it), and put some basic queries in as functions
I think this is what is known as the [Operational](http://apfelmus.nfshost.com/articles/operational-monad.html) monad. As you noticed, it's very closely related to Free: [here is a discussion](http://stackoverflow.com/questions/14263363/is-operational-really-isomorphic-to-a-free-monad).
In GHCI you can get a list of debugging commands by typing ":help" and hitting enter. These include commands for step through debugging. These all require the code you're debugging to be loaded in interpreted mode as the debugger steps over anything it's loaded from a compiled module.
@bartavelle I believe there was a space leak in that nloop function that was doing the 100 iterations. Here's a fourth version that fixes that leak and also unboxes the innermost vector of Entity. It speeds up several X and fixes the GC problem (well, for single threaded), yielding &lt;1% time in GC at -A20M: https://github.com/rrnewton/garbage/blob/8ce097a5a657fd5f7fbc7f5408aef49fe916fc40/src/NaiveGame3.hs This is still an out-of-place version, but it gets 0.96ms/tick on my machine (Intel E5-2699 v3 @ 2.30GHz). I run it with: stack build &amp;&amp; stack exec garbage -- +RTS -s -A20M -RTS It's less than 2X off of the C++ version which is pretty good for out-of-place. The next things I would try would be to make it in-place or use Compact Normal Form to keep the big live data out of the GC'd heap. Oh, and of course there's parallelism to consider. Repa or Accelerate would be a good choice for this kind of bulk data parallelism. I like this "Data Oriented Design" moniker and that's exactly what we're doing by unboxing and paying attention to the representations.
[I removed that slow Generic-based NFData](https://github.com/rrnewton/garbage/commit/ae6ccb3e8e4a623a4d0858159bcfb3d93879f30c), but it didn't make as big of a difference to init time as I expected. There must be something wrong with that initial rnf still.
I sort of want the propagator code to be as fast as humanly possible. In the Boston Haskell talk I go into a lot of the nitty gritty of what has to happen to make them really fast. https://www.youtube.com/watch?v=DyPzPeOPgUE I think anything architected on machines is going to pay far too much for it to be remotely competitive, and machines doesn't really address the whole pub/sub nature of listening to a lvar/cell.
Very nice. Given datakinds the trick to make things well-typed is "obvious", but it's quite nice seeing it fleshed out. I have a few nitpicks though: To retain the correct number of bottoms, `SummedNil` needs to be strict in its argument (or else `Summed '[]` needs to be a newtype rather than a data). Similar for the cons instance too, assuming you want coalesced sums (which you almost surely do if you care about the theory). I'd rename `Elsewhere` to `There`. IMO, the word "elsewhere" leaves open the possibility of something being in an entirely unknown/unspecified place, hence it is more like `Nothing` (or, `Everything` if we consider the dual of `Maybe`) to my mind. Finally, why aren't `Injectable` and `Outjectable` (ugh) the same class? Putting them together typechecks just fine in GHC 7.10.1
I'm curious, was that the unboxing or the String to Text?
Also, since the only thing going on with the `Summable` class is the associated data type, you can clean the code up a lot by removing the class and moving the family to the top level. And now that it's at the top level, since we have all the cases covered, we'd like to statically enforce that the family is closed. The way to write "closed data families" (à la closed type families) is with a GADT. Thus, the complete file becomes (with some renaming): {-# LANGUAGE MultiParamTypeClasses , FlexibleInstances , GADTs , DataKinds , KindSignatures , TypeOperators , DeriveFunctor , StandaloneDeriving #-} data FSum :: [* -&gt; *] -&gt; * -&gt; * where Here :: !(f a) -&gt; FSum (f ': fs) a There :: !(FSum fs a) -&gt; FSum (f ': fs) a deriving instance (Functor f, Functor (FSum fs)) =&gt; Functor (FSum (f ': fs)) class (f :: * -&gt; *) :&lt;: (fs :: [* -&gt; *]) where inj :: f a -&gt; FSum fs a outj :: FSum fs a -&gt; Maybe (f a) instance Functor f =&gt; f :&lt;: (f ': fs) where inj = Here outj (Here fa) = Just fa outj (There _) = Nothing -- N.B., the parentheses around the context are required here, else -- we get a parsing error due to the pragma. instance {-# OVERLAPPABLE #-} (f :&lt;: fs) =&gt; f :&lt;: (g ': fs) where inj = There . inj outj (Here _ ) = Nothing outj (There fa) = outj fa It'd be nice to also close off the `(:&lt;:)` class, rather than in principle allowing other instances to exist (even if they can't actually be written). But alas I know of no way to do that, since that would mean internalizing the test for whether `f ~ g`, handling the two possibilities by cases, but ensuring that those case expressions are optimized away via type inference. There may be some way to do it, but I'm not sure how.
If you had a comprehensive set of primitives, would you not be able to build more complex widgets on top of the primitives? Java byte code is like that.
String to Text only.
I skimmed the other posts but it seems something strange is going on with the setup. I haven't read everything but here are a few comments. The `Zap` type class is better known as the day convolution. It also seems strange to me that you have `Interrupt (Story a) (Story b) (b -&gt; z)` rather than `Interrupt (Story a) (Story b) (a -&gt; b -&gt; z)`. After all, why would you not map the continuation into the second parameter in your version? It then seems that you could set things up as a stack of free monads. After all the `FreeT` monad transformer is referred to as the "resumption" monad which allows effects to be interleaved. Setting up the a la carte machinery seems overkill. I was thinking something like.. `type Story a = FreeT Interrupt (Free ChangeF) a`. I couldn't carry out all the details as the new type of `interrupt` seems to be omitted. 
Thanks for the detailed response! Another nice part of the paper is explaining how making foldr "return a function" is the key to express foldl in terms of foldr and, more generally, to handle "inherited attributes" across your tree-like datatype. 
You encode roles, not users.
You are right, nicely done!
I post the issue here as well, so the wider community is free to comment. I feel that Stackage Nightly is failing to deliver its promise in providing a collection of recent packages building together.
Turned out that soon there will be LTS-7, and after that the Nightly will be cleaned up of blocking packages.
You need to step once more, to enter evaluation of the expression. That brings the `where` clause into scope. Here is an annotated sample debugger session for your code: *MyModule&gt; :step go 5 Stopped at MyModule.hs:(2,1)-(5,13) _result :: a = _ [MyModule.hs:(2,1)-(5,13)] *MyModule&gt; b &lt;interactive&gt;:24:1: Not in scope: ‘b’ Right, you have not yet entered the expression, so `b` is indeed not in scope yet. [MyModule.hs:(2,1)-(5,13)] *MyModule&gt; :step Stopped at MyModule.hs:3:5-9 _result :: a = _ b :: a = _ OK now `b` is in scope. It has the same type as the result. [MyModule.hs:3:5-9] *MyModule&gt; b &lt;interactive&gt;:26:1: No instance for (Show a) arising from a use of ‘print’ Cannot resolve unknown runtime type ‘a’ Use :print or :force to determine these types Relevant bindings include it :: a (bound at &lt;interactive&gt;:26:1) Note: there are several potential instances: instance Show Double -- Defined in ‘GHC.Float’ instance Show Float -- Defined in ‘GHC.Float’ instance (Integral a, Show a) =&gt; Show (GHC.Real.Ratio a) -- Defined in ‘GHC.Real’ ...plus 24 others In a stmt of an interactive GHCi command: print it Well, `b` is in scope, but it's still an unevaluated thunk. There is no `Show` instance for unevaluated thunks. So to examine `b` we use some other debugger commands: [MyModule.hs:3:5-9] *MyModule&gt; :print b b = (_t1::a) The `:print` command shows a symbol's current value, without forcing it. In this case, we can only see that `b` is still an unevaluated thunk. [MyModule.hs:3:5-9] *MyModule&gt; :force b b = 6 We can examine the value of `b` by forcing it. The cost is that now the evaluation order is different than it would have been outside the debugger. [MyModule.hs:3:5-9] *MyModule&gt; :step 7 Now we're done. I want to be able to peek back at the previous scope and see what value `b` was evaluated to. But currently there is no way to do that.
&gt; Ah, my train of thought there was to build a new sequence that didn't include chunks that were beyond the player distance, update the remaining chunks as it processed, and then use the difference in size between the result and the original sequence to determine how many new chunks to build. The `dropWhile` route would work if chunks were ordered by decreasing distance from the player though!
Very exciting - can't wait for the others - especially the cloud haskell talk (Finlay Thompson)
Aggressive pruning seems OK. The worse that can happen is that the maintainer will have to add his package back when he has time to work on the issue.
Let me elaborate slightly: * The Stackage team discussed internally about 2 weeks ago, and decided that cutting the LTS 7 release before a pruning was a better course of action given how much breakage was going to ensue * Getting a successful LTS 7 took longer than expected. In 20/20 hindsight, we should have put out an announcement two weeks ago about this plan * We're still refining the exact process by which we cut LTS releases, so feedback from the community is definitely welcome * If anyone's interested in joining the Stackage curator team and getting an even stronger voice in how we do things, just send me a PM/email/whatever, all are welcome to join the team. [Further information](https://github.com/fpco/stackage/blob/master/CURATORS.md) * The LTS 7.0 release is synchronizing docs to S3 right now, and then it will be available. It's the first LTS based on GHC 8 * I've gone through and done the aggressive pruning the linked issue describes. Please take a look at https://github.com/fpco/stackage/pull/1873/files. If one of your packages has been removed, please fix up whatever bounds need to be fixed and send us a PR to get it back into nightly. As always, thanks to the community for providing so many packages to build. You're making the Amazon build servers proud.
Another question: do you have plan to have concurrent LTS-6 updates, even when LTS-7 is cut?
Very cool! I had a suspicion that all of this could be cleaned up, but wasn't entirely sure what it would look like. Thanks for the kind words and the tips :)
Yes, we're planning on keeping both running for a while. Personally, I don't want to move fully to GHC 8 until 8.0.2 is out. I think there's room to make some policies around what GHC point releases are considered stable enough to make the sole maintained LTS release. But I think my views are more skewed towards stability/older releases than others.
&gt; A problem with (2) is that sometimes you don't have Show instances for things you want to take a peek. In these cases if you're lucky you can just add some deriving (Show) lines. If that doesn't work then you have a bigger problem. Try to print printable parts of the types etc. You might be interested in [present](https://github.com/chrisdone/present) which effectively lets you `import Present` and then use `$(presentShow [t|Maybe Int|])` which will generate a printer of type `Maybe Int -&gt; String`. I have a little bit of trouble with higher-kinded types (like ReaderT) which is a hurdle I still need to jump, but for most other types it can print _anything_. See [here](https://github.com/chrisdone/present#usage) for examples. This version of the lib is not yet on hackage, but there's a script in the repo that'll install it into your _global_ package environment (it has no dependencies other than base), which allows it to always be available for import within a GHC version, works with stack.
The idea is that the first parameter in `Interrupt` gets interrupted (by the second) before it completes, so there is no `a` to give to the continuation. I have to admit, that I didn't even think about using a `FreeT` approach for this. It's not immediately obvious to me how I'd make it work, but I'll try to hash it out later this afternoon. Thanks for the suggestion!
This looks very useful indeed! I'm not sure what you mean when you say your implementation makes sense. Can you also give some examples where `split` is inconsistent? Bonus points for raising an issue in their repository directly. I'd rather take a look case by case since I am not very familiar with `split`. `divvy` segfaults (it shouldn't) because of no bounds checking. I think `wordsBy` and `linesBy` should be corrected to match `lines` and `words`, because of their names. This will likely lead to 100% matching the `split` library. Here are the differences pointed out by your tests. Now that I think about it, `(Test.QuickCheck.===)` rather than `(==)` would have helped here (I recovered these outputs by hand before thinking of that). - `endBy "a" ""` - `vector-split`: `[""]` - `split`: `[]` - `wordsBy (const True) "aa"` - `vector-split`: `[""]` - `split`: `[]` - `linesBy (const False) ""` - `vector-split`: `[""]` - `split`: `[]` The current `linesBy` failure is actually bug in the test suite (comparing `L.linesBy` (from `split`) with `V.wordsBy`). But fixing it reveals the difference above. Actually, looking at the implementation of `wordsBy`, for instance, the presence of `dropBlanks` points to a bug, since the result did not drop all blanks. Indeed, the intermediate `dropInnerBlanks` does nothing because it matches blanks enclosed by delimiters, but the `dropDelims` that happens before in `wordsBy` removes them. `split (onSublist "") "a"` generates an infinite list (as opposed to the `split` library). One might argue that it makes some sense, but this is a rather undesirable behavior because accidental infinite loops are annoying. I don't have a preference for what the exact result should be (that is to say whether it should match `split`'s `["", "", "a"]` or not), but I find it fair to require finiteness. I think such corner cases are worth being documented in any case (in both `vector-split` and `split`), but I think the exact behavior of each function is in fact documented in `split`, though indirectly, via the equivalence they give. `wordsBy` is equivalent to `split . dropBlanks . dropDelims . whenElt`, `dropBlanks` indicates that there are no empty words in the output. --- Test output for anyone who would want to take a closer look but too lazy to set up the package themselves (like I usually am): tests chunksOf: OK (0.03s) +++ OK, passed 100 tests. splitPlaces: OK (0.04s) +++ OK, passed 100 tests. splitPlacesBlanks: OK (0.07s) +++ OK, passed 100 tests. chop: OK (0.03s) +++ OK, passed 100 tests. chopGroup: OK (0.03s) +++ OK, passed 100 tests. divvy: FAIL *** Failed! Falsifiable (after 5 tests and 6 shrinks): Positive {getPositive = 1} Positive {getPositive = 2} [0.0,0.0,0.0] Use --quickcheck-replay '4 TFGenR 00000A2C0BB4214F000000000EE6B280000000000000E12D00000000C6A98B40 0 992 10 0' to reproduce. splitOn: OK (0.02s) +++ OK, passed 100 tests. splitOneOf: OK (0.03s) +++ OK, passed 100 tests. splitWhen: OK (0.09s) +++ OK, passed 100 tests. endBy: FAIL *** Failed! Falsifiable (after 1 test and 1 shrink): NonEmpty {getNonEmpty = "a"} "" Use --quickcheck-replay '0 TFGenR 00000A2C0BB4214F000000000EE6B280000000000000E12D00000000C6A98B40 0 512 10 0' to reproduce. endByOneOf: FAIL *** Failed! Falsifiable (after 1 test and 1 shrink): NonEmpty {getNonEmpty = "a"} "" Use --quickcheck-replay '0 TFGenR 00000A2C0BB4214F000000000EE6B280000000000000E12D00000000C6A98B40 0 1024 11 0' to reproduce. wordsBy: FAIL *** Failed! Falsifiable (after 6 tests and 6 shrinks): {_-&gt;True} "aa" Use --quickcheck-replay '5 TFGenR 00000A2C0BB4214F000000000EE6B280000000000000E12D00000000C6A98B40 0 129024 17 0' to reproduce. linesBy: FAIL *** Failed! Falsifiable (after 5 tests and 6 shrinks): {_-&gt;True} "a" Use --quickcheck-replay '4 TFGenR 00000A2C0BB4214F000000000EE6B280000000000000E12D00000000C6A98B40 0 126976 17 0' to reproduce. basic strategies oneOf: OK (0.06s) +++ OK, passed 100 tests. onSublist: FAIL *** Failed! Falsifiable (after 2 tests and 1 shrink): "" "a" Use --quickcheck-replay '1 TFGenR 00000A2C0BB4214F000000000EE6B280000000000000E12D00000000C6A98B40 0 49152 16 0' to reproduce. 6 out of 15 tests failed (0.15s) 
&gt; `bimap Nil Nil` should still be `Nil`, right? No, I do not normalize `bimap Nil Nil` to `Nil`, because those have different types: I only use `Nil` (which I call `NilAtom`) on atoms, and I use `bimap Nil Nil` on pairs of atoms. Here are all the gory details: data FreeBiCat k a b where NilAtom :: FreeBiCat k ('Atom a) ('Atom a) NilPair :: FreeBiCat k a1 b1 -&gt; FreeBiCat k a2 b2 -&gt; FreeBiCat k ('Pair a1 a2) ('Pair b1 b2) ConsAtom :: k ('Atom a) b -&gt; FreeBiCat k b c -&gt; FreeBiCat k ('Atom a) c ConsPair :: FreeBiCat k a1 b1 -&gt; FreeBiCat k a2 b2 -&gt; k ('Pair b1 b2) c -&gt; FreeBiCat k c d -&gt; FreeBiCat k ('Pair a1 a2) d class Nil a where nil :: FreeBiCat k a a instance Nil ('Atom a) where nil = NilAtom instance (Nil a1, Nil a2) =&gt; Nil ('Pair a1 a2) where nil = NilPair nil nil then :: FreeBiCat k a b -&gt; FreeBiCat k b c -&gt; FreeBiCat k a c then NilAtom ks' = ks' then (NilPair ks1 ks2) (NilPair ks1' ks2') = NilPair (then ks1 ks1') (then ks2 ks2') then (NilPair ks1 ks2) (ConsPair ks1' ks2' k' ks') = ConsPair (then ks1 ks1') (then ks2 ks2') k' ks' then (ConsAtom k ks) ks' = ConsAtom k (then ks ks') then (ConsPair ks1 ks2 k ks) ks' = ConsPair ks1 ks2 k (then ks ks') bimap :: FreeBiCat k a1 b1 -&gt; FreeBiCat k a2 b2 -&gt; FreeBiCat k ('Pair a1 a2) ('Pair b1 b2) bimap = NilPair So while the expressions bimap NilAtom NilAtom :: FreeBiCat k ('Pair ('Atom a) ('Atom b)) ('Pair ('Atom a) ('Atom b)) NilAtom :: FreeBiCat k ('Atom a) ('Atom a) do not and cannot normalize to the same value because they have different types, the expressions bimap nil nil :: FreeBiCat k ('Pair ('Atom a) ('Atom b)) ('Pair ('Atom a) ('Atom b)) nil :: FreeBiCat k ('Pair ('Atom a) ('Atom b)) ('Pair ('Atom a) ('Atom b)) do normalize to the same value, namely `NilPair Nil Nil`. &gt; And how do you plan to handle `bimap (bimap f g) h` Without any difficulty: NilPair (NilPair f g) h :: FreeBiCat k ('Pair ('Pair a1 a2) a3) ('Pair ('Pair b1 b2) b3)
Who would want to attend a conference that would exclude people for a statement like that anyway?
Maybe it's just me, but isn't the application of human judgment here a good thing? Like, if you *really* want the latest of everything, you should just use the dep solver (assuming that people have been keeping the bounds on packages up to date?)
I understand that the tests as they currently stand are mostly there to point out the differences with `split` rather than as actual correctness test. &gt; `split (onSublist "") "a"` should result in `["","","a"]` as per the `split` documentation. Are you referring to the what the result in `vector-split` should be? I agree that it is unintuitive, the second one you propose `["a","","b","","c"]` looks somewhat better to me as well, but I don't think there should be something really intuitive about "splitting on the empty separator". Unless it makes some other function downstream behave nicely (unlikely), I'd just stick with whatever comes naturally from the implementation. I can't see how it is inconsistent with the documentation of `split`. There is a sentence specifically about passing an empty list to `onSublist`, with a redirection to `chunksOf 1` and `map (:[])`, which are more likely to be what a user is actually looking for here. I'm probably overestimating what can be done with `Splitter` as a simple function. I'll take a closer look later. Good work!
&gt; Quicksort first divides a large array into two smaller sub-arrays: the low elements and the high elements. Quicksort can then recursively sort the sub-arrays. So, the recursive calls still use a smaller set of the original input. So, the restriction is not violated.
From my side; I try my best :) resolver: lts-7.0 compiler: ghcjs-0.2.0.9007000_ghc-8.0.1 compiler-check: match-exact setup-info: ghcjs: source: ghcjs-0.2.0.9007000_ghc-8.0.1: url: "http://tolysz.org/ghcjs/ghc-8.0-2016-09-14-lts-7.0-9007000.tar.gz" sha1: e81dd1c00d8b04bc0569fbc4f3d691845c000317 
&gt;But I think my views are more skewed towards stability/older releases than others. For me this is a good thing.
IME, because Haskell is so good at refactoring (rich static types really help here), I always implement the code "ignoring" performance, then benchmark and tweak. Even some parts I'm "writing twice", I still get more features / programmer hour with Haskell than I do with Java or C. I find that if my first pass isn't "fast enough" in Python or Perl, the refactoring can't be done until I have a fairly complete test suite -- too much unintentional breakage slips through the compiler.
Yes, literally the whole point of Stackage is to provide a more stable alternative to "just get the latest of everything (within solver constraints)" by selectively holding back certain packages. That said, we do hope that nightly images are timely and do not lag too far behind the latest and greatest. It's hard to find the right balance between "kick everyone out of stackage nightly builds until they support the new version of Aeson" and "hold Aeson back on stackage nightly builds until everyone supports it." We have tended to err on the side of the latter; perhaps we should steer a little more towards the former.
What are you referring to by human judgement? To me it seems like the issue is that some people are unable to update their packages in the expected time frame and are thereby blocking all of `stackage` from upgrading a package. This seems more like a lack of human resources than human judgement. In the end some tradeoff has to be made between pruning packages which when driven too far (not saying that this change will do so) makes nightly useless because there are too few packages in it and giving package maintainers more time and thereby making nightly less useful if you want to just have a consistent set of up2date dependencies.
I'm not sure if that's what the nightly promise is. I mean, it's certainly something we want it to be. But the way I think of it is that nightlies are a preparation for the next major LTS release. By making nightly builds, we discover issues and form constraints incrementally over time so that by the time we want to make a new LTS release, we don't have to do all that work all at once. It's way too much work to do all at once; the amortization of this preparation is absolutely necessary. We already made the mistake of releasing LTS 4 with a version of Aeson that was "too new". Even though it was months old, it caused regressions. We booted out some packages in order to accommodate the new Aeson release, but what we should have done in this case was realize that some packages weren't upgrading yet for good reason. I strongly encourage stack users to learn how to use "extra-deps" to selectively pull in newer versions of libraries into your project. It's often the case that you can do this safely. If nightly isn't current enough for you, don't be afraid to override the package versions in this way.
Could someone TL;DR this? (or should it be TL;DW?)
As an Haskell hobbyist I have always been quite frustrated and disappointed by the indentation mode from `haskell-mode` in `emacs` (I don't think it is much better in other editors). I have since learned that it is a difficult problem so I am living with it. IMHO if there is one single feature, the `haskell-mode` should strive to improve it would be just this one ;-) That said, I don't quite see how customization of some `indentation` mode would be such a good idea. If you want to enforce a certain style, a formatter (the one that works best for you) is the right tool for the job. Many efforts are currently put to improve `hindent` and cie. It looks promising.
That's not the actual restriction, usually. Of course on paper that gives a valid termination proof, but at least Agda only accepts recursive calls on lists, it's not smart enough to figure that the results of partitioning are smaller. I don't say this to knock down Agda, I say it because I've actually used it in anger. I'm less experienced with Coq but details are similar.
I'm about 46min in. Haskell compiles to Core, a very small fully explicit *typed* language directly derived from System F which can be typechecked very efficiently. The various compiler passes are then Core to Core. This makes it possible to make sure that they are type preserving thus having a vastly better way of discovering bugs than having to chase down segfaults. Some examples of passes are let inlining (in a general sense : definitions are lets so this can act across module and package boundaries), beta reductions, or rewrite rules. These rewrite rules can be user supplied (developers can get GHC to take advantage their domain-specific expertise) or even compiler generated (if the compiler type-specialises and heavily optimises a polymorphic function, registering a rewrite rule can ensure that *all* uses of the function at that type will be replaced by the more efficient version). Edit: case-case transformations are described too (pushing an external case into the branches of an inner one may lead to extra optimisation opportunities). suggested reading: [Secrets of the GHC inliner (pdf)](http://www.msr-waypoint.net/pubs/67490/inline.pdf) but more generally [the reading list on ghc's wiki](https://ghc.haskell.org/trac/ghc/wiki/ReadingList)
Have you seen http://okmij.org/ftp/Haskell/extensible/extensible-a-la-carte.html? (Also: freer monad)
&gt; you should just use the dep solver It's a hit and miss with the solver. I can point you to countless examples of bug reports on GitHub where cabal gave up on calculating a build plan, even though a valid plan existed. Even if the solver worked better, the amount of busy-work you need to waste managing version bounds for the solver where even the slightest inaccuracy causes this house of cards to collapse make the whole solver concept appear rather ridiculous. Stack and Stackage have already proven that there's a superior alternative to solving. Just imagine if all that busy-work wasted on version bound maintenance would be invested into Stackage curation instead! 
Is there an indendation mode in `haskell-mode`? I always had to install an extra package to get indentation at all.
As a fellow Emacs user, what Haskell packages do you use? Separating the good packages from all the bad/old/unmaintained ones has proven hard.
Using them from the REPL can be used as a less time-consuming but also less productive way of doing this as well.
Not really, at least not if you are not willing to accept lazy IO and all its problems. There is a reason why we have `pipes`, `conduit`, … Note that I am in no way saying that Haskell is unsuitable, I just think it’s worth pointing out that lazyness doesn’t help for a lot of stream processing usecases.
I think this is true of using the abstractions in general
I like this post and I hope more people start working with data in haskell, but I should point out pandas can quite easily do streaming read without loading the full dataset into memory (just have to set the chunksize argument in pd.read_csv()). Nits aside, I'd love to see a full-fledged pandas and numpy analogues for haskell. The really nice thing about data science in python is that all the major libraries interoperate very nicely. It's easy to go from a dataframe to a numpy array to a tensorflow tensor. I hope a similar sort of healthy ecosystem starts to emerge for haskell :-)
I don't want to strictly enforce a style, only make it easier for someone to write things that are in the correct style (and also allow some flexibility in that style). I don't think `hindent` is usable. 
&gt; At the very least, though, our editor should be able to indent properly. Agreed. But it does not do this. Not automatically. &gt; I would be very surprised if nobody out there has a good setup in emacs that indents Haskell the right way. My Emacs indents Haskell the right way, using `indent-relative` and `tab-to-tab-stop` along with manual futzing. There is no automatic Emacs indenter that is going to satisfy the Tibbe style guide. An alternative is Vim. It is much better at manual indenting than Emacs. However it is much worse at automatic indenting. But Emacs automatic indenting is only good if you have a good mode for your language. Haskell does not have such a mode. All the automatic Emacs indenters are so bad that the one thing I always miss about Vim is speedy manual indenting. &gt; Putting a small amount of time into haskell-mode is reasonable. Based on the fact that this has been a known issue for years and no one has fixed it, I doubt it would be a small amount of time. &gt; Giving up on style is also not a reasonable option. OK. &gt; I think all of these suggestions are kinda ridiculous. Sorry! OK. I have been grappling with this for years and if you spend some time looking at this subreddit you will see this issue has come up time and again. IMHO these are your options whether they are ridiculous or not. Except I forgot option 5: if you don't like these options, go use something like C++, where there is a fantastic Emacs mode that handles all this stuff, not to mention better IDE support. But since you are here asking this question, it seems you really know these options are not ridiculous and you are hoping someone will give you a better answer. I just know from personal experience that you are not going to get a better answer on this. Sorry.
I am also quite surprised that so few professional haskellers are helping to make it better: https://github.com/haskell/haskell-mode/graphs/contributors Maybe elisp is that bad ;-)
Sigh, yes, Haskell is not a great language for "accidentally" getting good performance. Rust is probably a good bet in that department, but even more reliable are DSLs and special code generation frameworks that put a lot more constraints on programs to keep them more performant. I think this "Data Oriented Design" thing has the right idea, and Haskell code written for performance is in the same boat. It's all about what shape data takes in memory. Avoiding laziness is one thing, but avoiding excessive boxing/GC is another. GHC has an aggressive simplifier, so it can often inline and eliminate a lot of intermediate junk, which is great. But it is *not* good at automagically changing your data representations. If you ask for a Data.Set you get a Data.Set. I'd love to see more optimizations that radically affect data layout in the future, but we're not really there yet. Restricted (embedded) languages like Accelerate have super simple data models comparatively, so they can take a much more active hand in laying out your data for you. Still, I think this simple minecraft-like benchmark is actually a nice example where you only have to do the "low level fiddling" for the hottest data structure in the innermost loop (the Entity Vector). The rest of the code is nice, pure, and natural! 
I'm not sure about optimizations, but the newtype exists in at least `reducers`: http://hackage.haskell.org/package/reducers-3.12.1/docs/Data-Semigroup-Union.html ∴ ghci -package reducers -package containers Prelude&gt; import Data.Map as M Prelude M&gt; let xs = M.fromList [(1, "a")] Prelude M&gt; let ys = M.fromList [(1, "b")] Prelude M&gt; import Data.Semigroup.Union Prelude M&gt; import Data.Semigroup Prelude M Data.Semigroup.Union&gt; getUnionWith (UnionWith xs &lt;&gt; UnionWith ys) fromList [(1,"ab")] 
This seems like a new issue for me. I just ran into it for the first time just under a week ago.
An interesting choice of user name, /u/StackSucks
Cool thanks!
Same here. Didn't even upgrade stack or my project's lts
pinging /u/StackLover
You can pass args to `ghc` using `--`, although you still get a bunch of noise. $ stack exec ghc -- --version Run from outside a project, using implicit global project config Using resolver: lts-5.18 from implicit global project's config file: /Users/chris/.stack/global-project/stack.yaml The Glorious Glasgow Haskell Compilation System, version 7.10.3
Stackage exerts *editorial* judgment on the packages that it distributes. Its decisions range from policy decisions regarding when a package should be booted from the index, decisions of what to include in an LTS, and decisions about what packages are "too big to fail" and can block an LTS release. You say the problem is that "people are unable to update their packages in the expected time frame". But this is precisely the problem that Stackage exerts its editorial authority to resolve. It's unrealistic to say that, "Well, Stackage really doesn't do anything, because in an ideal world, everyone would upgrade their packages quickly so Stackage doesn't really have to do any work." It's unreasonable to think you would ever be in such a steady state; this is only perhaps achievable for a much smaller set of packages with shared maintainership.
There are such things as [derivatives of a datatype](http://strictlypositive.org/calculus/). Here the "one-hole context" comes with an element already sitting in focus. But this point of view can inform our implementation: {-# LANGUAGE RecordWildCards #-} module AllContexts where data Context a = Context { before :: [a] , focus :: a , after :: [a] } deriving Show fromContext :: Context a -&gt; [a] fromContext Context{..} = reverse before ++ focus : after allContexts :: [a] -&gt; [Context a] allContexts = go [] where go _ [] = [] go bef (x : xs) = Context bef x xs : go (x : bef) xs onFocus :: (a -&gt; a) -&gt; Context a -&gt; Context a onFocus f c = c { focus = f (focus c) } applyMap :: (a -&gt; a) -&gt; [a] -&gt; [[a]] applyMap f = map (fromContext . onFocus f) . allContexts Edit: `allContexts` can be refactored in a way that makes the intention more explicit: it's the `unfoldr` of moving the focus one step to the right when starting with the focus at the leftmost position. That's a bit lengthier but the combinators can be useful in other situations too. import Data.List (...) inspect :: [a] -&gt; Maybe (a, [a]) inspect [] = Nothing inspect (x : xs) = Just (x, xs) mkContext :: [a] -&gt; Maybe (Context a) mkContext = fmap (uncurry $ Context []) . inspect moveRight :: Context a -&gt; Maybe (Context a) moveRight Context{..} = uncurry (Context (focus : before)) &lt;$&gt; inspect after allContexts :: [a] -&gt; [Context a] allContexts = unfoldr (fmap (\ s -&gt; (s, moveRight s))) . mkContext 
I'd recommend the following order: Type system in Haskell (including type descriptions for functions) -&gt; Type classes and Instances -&gt; Functors -&gt; Monads. Don't move to the next topic until you fully get one. Try some examples. Don't read those analogies like "monads are like plunger" or similar. All this should be enough to understand monads. 
I actually wish Emacs would (in general, not just for Haskell) do dumber indents. Just copy the previous non-blank line's indentation always (autotrimming EOL spaces when I press enter multiple times), down to the exact mix of spaces and tabs, and let me handle it when I want to change indent levels.
This is great! This kind of use case is a key reason why I don't use Haskell at work. Some comments: * The benefits of type annotation and checking makes sense for production data science code/analyses. They make sense less so for exploration and on-the-fly scripting, especially e.g. if your data table has lots of columns. In the absence of automated object-relational mapping (a la F# type providers) I think it is important to have a weakly-typed option (or even default). Yes, in the example provided there was no need to "declare or name any record type ahead of time"; but I think a useful API for this kind of thing would also enable frictionless exploration without any (manual) type annotation. Often I want to load and examine data tables in R/Python *before* having a precise sense of how they're layed out. * It's an interesting choice to stay entirely within the `conduit` universe when there are existing "collections" and "record"-y interfaces to perform similar tasks (specifically: monad comprehensions -- cf. LINQ -- and lenses). * This is a matter of opinion, but I think the `tidyverse` packages in R -- `dplyr`, `tidyr`, etc. -- should be considered the gold standard for nice, functional-feeling relational data manipulation APIs, rather than pandas. The suite of functions available compose nicely, are well-named, and map exquisitely to common use cases. If I were better at Haskell, I'd try my hand at porting bits and pieces of these. * I don't know what some of these GHC extensions do. That's not a problem in and of itself, but I do think that ideally a library like this would be very accessible and dependency-light.
To me this is just an example of how central curation breaks down. I am hopeful that Cabal improvements help make the dependency-solver workflow more usable. Even then, there is the problem of the dependency-solver workflow depending too much on version information manually entered into `.cabal` files--and there is no agreement or even much discussion on what the information in the `.cabal` file means. I would hope things eventually advance to a point where some Amazon build farm could automatically test a whole bunch of build plans, without reliance on central curation or error-prone information from `.cabal` files, but I've done no work in this area so I don't know if it's even possible.
You don't even need the `--package`. `stack ghci mtl` also works, too
&gt; At this point, I do not have a Haskell project I wish to build This is pretty much the crux of the issue. Most of `stack`'s value proposition is for project management
I think you have read more (negative) into Richard's comment than I did. It seemed noncombustible to me.
On [this page](http://www.cs.uoregon.edu/research/summerschool/summer12/curriculum.html) there's a series of lectures by John Hughes called "Monads and All That". What it covers is broadly consistent with aleksejslvanovs advice - certainly no vague misleading metaphors. It goes a bit beyond monads too. Don't worry about the scary all-this-abstract-weirdness-that-I-didn't-know-programming-languages-and-compilers-could-involve stuff on that page - this particular series is at a level an experienced non-Haskell programmer should have few problems with. However, I'm not sure it goes back to the level of explaining typeclasses - I think so, but I don't remember. So... The [next years page](http://www.cs.uoregon.edu/research/summerschool/summer13/curriculum.html) includes a series of lectures by Simon Peyton Jones called "Adventures with types in Haskell". Although it covers some compiler internals and more recent Haskell features in the second lecture onwards, the first lecture is again pretty introductory and gives IIRC a very good explanation of typeclasses. Both SPJ and JH are extremely good teachers, and I strongly recommend watching both sets of lectures. There's also a set of lectures by Erik Meijer on the [Microsoft Channel 9 site Haskell tag](https://channel9.msdn.com/Tags/haskell) - click through to the second and third pages - which are a slower but also excellent introduction to Haskell generally - including typeclasses (lecture 3) and monads (lecture 9). Also, look at SPJs [publications here](https://www.microsoft.com/en-us/research/people/simonpj/?sort=area#publications). There's a document "Tackling the Awkward Squad" (2001) that gives basic explanations of monadic I/O, concurrency, exceptions and the C foreign function interface. It does use at least one metaphor (the world-passing model for IO) but I find that one quite helpful, and it's not entirely a metaphor - the compiler even has a token representing the world in the intermediate language. The danger, of course, that very much applied in my case is seeing something too similar to that in all monads, not just the IO-like ones or even just the ones with some concept of a state. I finally got over that hurdle when I figured out the monad interface to `Maybe` and how to use it. One thing all these have in common - they're all a bit dated. Those concepts haven't changed, but Haskell has changed relatively recently, particularly to have more generic operations by default (rather than concrete list operations). There has also been some slight rearrangement of the hierarchy of library typeclasses - some things that should always have been related, but weren't for historical reasons, now are. IMO that's not a big deal, but [Hoogle](https://www.haskell.org/hoogle/) and it's links to the GHC and library manuals are useful up-to-date reference sources. 
Apologies on the `Did not find...` business, that's entirely my fault. It was a crappy patch trying to fix a complicated and obscure problem, and those messages should never have been more than debug-level output. For the curious: When Hackage introduced the concept of cabal-file revisions, it introduced a brand new failure mode into snapshots and freeze files: a combination of packages that we know worked historically and have been using can now be marked as invalid by an upstream edit. At first, I thought this situation would never occur in reality, but I was proven wrong on a few occasions. So the solution: make Stackage snapshots more reproducible. Previously, we'd say "LTS 6.5 contains mtl version 2.2.1." But this doesn't tell us which _revision_ of mtl.cabal had been used, so we'd just take the most recent revision and hope for the best. If someone maliciously (or unintentionally) edited the .cabal file to say `base &lt; 2`, we'd be hosed. So Stackage snapshots now include the Git blob SHA of each .cabal file they use. We could theoretically have included the revision number, but (1) a SHA provides cryptographic evidence that we have the right thing, and (2) it's (seemingly) much easier to grab a SHA out of a Git repository than it is to stream through a tarball that has multiple copies of the same filename and grab the right one. It's certainly more performant. The flaw: I'd forgotten that we do shallow clones of the Git repository that has all of the cabal files, so with later revisions occurring, it's possible that someone would end up missing some of the blob SHAs. There are two solutions to that: 1. Short term: switch to a deep clone, so we get all of the blobs 2. Long term: add the commit SHA to the Stackage snapshot too, so that we know what commit to fetch to have all of the blobs locally (1) is on master now, and (2) is being tracked at: https://github.com/commercialhaskell/stack/issues/2487
You can contact Hackage Trustees, their policy descibes what can be done and how: https://github.com/haskell-infra/hackage-trustees/blob/master/policy.md
It might be not clear for everyone, but Stackage curators *do not ever edit* packages. So in the unfortunate situation, when even a single package maintainer becomes unresponsive, and their package is blocking a big part of Stackage; there are only bad options: cut the collection directly, or stay with old versions for that part (or make a request to Hackage Trustees to do something). So one could say that "Stackage really doesn't do anything, just pings other people to take action", that's valuable though too, but Stackage curators aren't Debian maintainers (where latter make patches to make stuff work together).
If you do want to hide the `a`, the extension you're looking for is ExistentialQuantification, and here's how it would look: data LeafNode = forall a. MyTypeClass a =&gt; LeafNode { leafMeta :: LeafMetaData , things :: [a] } side note: some might say this is an instance of the "existential typeclass antipattern"; here's the most cited post on the topic: https://lukepalmer.wordpress.com/2010/01/24/haskell-antipattern-existential-typeclass/ The gist of it is that you don't know anything about an existentially quantified, class-constrained type other than what the class instance says, so instead you should just use a record that contains all of that information.
I tried this when I was first learning and it left me with a weird, incomplete, frankenunderstanding of what was going on. Learning went much faster after I read through the Typeclassopedia, making sure I understood every sentence in every paragraph before moving forward.
What about hindent?
&gt; When Hackage introduced the concept of cabal-file revisions, it introduced a brand new failure mode into snapshots and freeze files Just for the record, thanks to the integration of hackage-security into cabal we can now finally close this gap in cabal 2.0 as now we can declare the index state as well when operating in `cabal freeze` mode. This is comparable to Stack's Git-based index, albeit with different characteristics (which are more convenient&amp;efficient for cabal's access patterns (which benefit from fast random O(1) access) to the pkg index than a Git store would be; I assume that for Stack a Git store could be the respective more optimal representation instead).
I actually use Stack-based scripts quite a bit, and documented the workflow here: https://haskell-lang.org/tutorial/stack-script I _really_ like being able to use it in blog posts to give copy-paste examples that I know will work for the user, regardless of which version of packages they have installed already and how far in the future they run it. Two recent examples: * https://www.fpcomplete.com/blog/2016/09/practical-haskell-simple-file-mirror-1 * https://www.fpcomplete.com/blog/2016/08/bitrot-free-scripts
I haven't yet watched the talk, but perhaps this is the thing being referred to? https://github.com/osa1/StrictCore
Now that I think about it, I had a similar experience. I do also remember thinking that I'd been approaching it was wrong, and I still feel that way. I think there *is* a right way of teaching this stuff, but it requires comfort with the lower abstractions before getting involved in monads. *What is a monad?* is probably already the wrong question to ask when learning monads, because the rules seem completely arbitrary from so high up. The "why" of monads was lost on me for at least a few weeks of struggling with the type system, but I honestly don't think it needed to have been that way (monad tutorial fallacy notwithstanding).
Using GADTs is another way of achieving this: Pragma to activate: {-# LANGUAGE GADTs #-} Example: data Showable where Showable :: (Show a) =&gt; a -&gt; Showable I think this is a lot nicer then using ExistentialQuantification directly, but as mentioned before, you might want to think about the existential anti-pattern before going down that road. 
&gt; Bitter complaints on haskell-cafe and deliberately inflammatory postings to reddit don't do anything to improve the world. Personally I don't think Richard's post was deliberately inflamatory. Quite the contrary. &gt; Stack is an open source project, if there is something about it that you don't like, you can change it to work the way you want. RIchard is one of the main players behind the progress being made towards Dependent Types in Haskell. Personally I'd much rather have him working on that rather than build tool issues. 
This is great. Thanks for the blog post!
Is there a stack command to grab/show the latest `.cabal` for a given package at specific snapshot?
No, but I _think_ (not certain) that `stack unpack` will do it.
It’s a mixture of different things. `@` is from `TypeApplications` and `("fl_date" := Day, "tail_num":= String)` is a type. "fl_date" is a typelevel `Symbol` and `:=` is a constructor defined in the library.
I guess, someone will have to actually do it: https://github.com/commercialhaskell/stack/issues/1391 EDIT: `stack unpack mtl-2.2.1` gives the right revision... it needs to have the full version
I think this is extend on a comonad for zippers,or close to it.
1. This is really cool! 2. If Chris hadn't packed his OSX metadata ( 'ontime.csv', '__MACOSX/', '__MACOSX/._ontime.csv'), then this would be: pandas.read_csv('http://chrisdone.com/ontime.csv.zip', chunksize=2**16) I spend a lot of time in pandas etc. and the common stuff is really easy, fast and optimized. The lack of types does bite often though so maybe ghc 8 featues will allow for a nicer interface (I've tried tackling a pandas or dply-like API in Haskell several times but it's always 5-10x as many things to type).
TL;DR: `Either` is a pretty cool monad =P In all seriousness, I strongly agree with the opening thesis: Exceptions make it hard to reason about code. So I really don't like seeing their proliferation throughout `IO` code. I was working with a websocket library yesterday, and the library notified you of the connection closing by throwing an exception! In Haskell's `IO` exceptions, there is no type level mechanism to notify me of this fact. Ultimately, I wish more erroring code would return `Either`. Plus, with `m (Either ...)` it's easy for me to lift into `MonadError ...` with `either throwError return =&lt;&lt; ...`.
You're welcome and thank you for implementing it :)
That's true, but even then my experience has been that stack can be useful. For instance, with the Accelerate CUDA backend I couldn't install the package via cabal without it being a pain. I can't imagine installing lots of packages to experiment with unless it was via cabal sandboxes or stack. Otherwise you're bound to get a problem somewhere.
Apparently docker isn't a great solution for sandboxing (see interview with Jess Frazelle from around 25mins in): http://risky.biz/RB424 http://security.stackexchange.com/questions/107850/docker-as-a-sandbox-for-untrusted-code Essentially if you can escape the container it doesn't really matter what limits have been set.
Do you also enforce a disk space quota? even in a docker container someone could cause temporary issues by writing lots. Very nice and simple though, good job! Maybe it should begin populated with the "hello world" example as the first thing I did was assume it was more repl like and add a top level expression
Do we have a guarantee that a malicious source string can't hack the compiler though?
Nice! I've tried to build this a couple of times this year but I failed because of my abysmal LaTeX skills.
Disk quota is not yet enforced, but I am working on it! I changed the initial page to show HelloWorld example instead of blank page. Thanks for the feedback! 
nice, now we could see the the value for fib 1000 without any problem. You might want to add `overflow: scroll;` for the `.panel-body` class.
My understanding: it's not really recursion in the formal sense, but self-application as seen in the Y combinator. It gets the job done all the same, but they're not the same construct; it's the difference between: Y = λf. (λx. f (x x))(λx. f (x x)) and fix f = f (fix f) 
Can someone explain this? I have trouble even parsing the type `Not`, since when usually people say Not, they mean negation. I would expect the type to reflect that, but it seems like it's something else. ~~Also the type of `honest` seems incorrect. Should be `honest :: False` I think. Which makes me wonder how the `absurd` function would even typecheck.~~ Nvm, got that part.
Not X is the same as X -&gt; False in propositional logic.
&gt; Rather than using inference like Frames does for CSV's though, I could just query the types from the database itself and maintain a mapping of database types to Haskell types. Using that mapping I would like to generate the above type. Essentially the biggest difference is instead of using inference like Frames does for CSV's I would be specifying the types of things. You might be interested in what I do with [sql-fragment](http://maxigit.github.io/sql-fragment/Database-SQLFragment.html) (link to the [doc](http://maxigit.github.io/sql-fragment/Database-SQLFragment.html). I use phantom type to combine SQL in a type safe way. It generates *fragments* (fields definitions) automaticaly from the database, and can construct HList records with or without label automatically from the query (code [there](https://github.com/maxigit/sql-fragment-mysql-simple/blob/master/src/Database/SQLFragment/MySQL/Simple/Record.hs)). The idea could be easily translated to `vinyl`. In facte I could even probably generate the SQL query from the type ...
If you spell out the given definition of `Not` (`Not p = p -&gt; False`), it means, "A proposition p is false iff assuming that it is true implies a contradiction". That's fairly close to what negation means in classical logic, though there are important differences.
In Haskell TH can run any code, thus not really... it could `ifdown` :)
&gt; I have trouble even parsing the type Not, since when usually people say Not, they mean negation. I would expect the type to reflect that, but it seems like it's something else. Note that `False` has no inhabitants, so if we ever "get hold of" a `False` value, well, we have a contradiction. So, if we have, for some `a`, a function `a -&gt; False`, that's perfectly fine _if_ we, under no circumstances, can have a value of type `a`. Thus, if we think of the types as propositions, that `a` would have to be a false. So `data Liar = Liar (Not Liar)`, in English, would be something like saying "the Liar ADT contains proof that it is false". honest :: Not Liar honest = \l@(Liar p) -&gt; p l `honest` can be read as "Liar is false", and all it does is take the proof of "Liar is false" (`p`) out of `l` and applies it to the containing value (`l`). But, of course, if we ever have a `Liar`, that's a big circular contradiction (just like the sentence "this sentence is false"), so what we would hope is that the type system would prevent us from ever creating a `Liar`. absurd :: False absurd = honest $ Liar honest Now looking at `absurd`: `Liar honest` is like saying "it is true that Liar is false", and equipped with that proof, we can show that "it is false (that it is true that Liar is false)" (`honest $ Liar honest`), and around and around we go, with no complaints from the type system.
Semantically, yes. Syntactically, no. From the source (emphasis mine): &gt; This post shows how to use the Liar paradox to prove that that Haskell is inconsistent *without using recursive terms*. The key take away is that it was possible to construct the paradox without _recursive terms_ (that's leaving aside recursion in the _types_). You'll note that there's no point where, at the term level, we explicitly self-reference (as in `let x = x in x`). Some might find that intriguing, as I found it interesting that, while the lambda calculus does not support any means of self-reference (or said another way, it does not support recursive terms), it's still possible to write (semantically) recursive functions via self-application (using a Fixed-Point Combinator).
Thanks for the help, I'll take a look at sql-fragment after work.
It's worth noting that this is related to [Oleg's more subtle proofs of inconsistency](http://okmij.org/ftp/Haskell/impredicativity-bites.html) where, as I understand it, he uses GHC's type equality constraints to "close the loop" and get recursive types much like the `Liar` type in this post.
By pointwise you mean like this: instance Semigroup (Free0 Semigroup a) where (&lt;&gt;) :: Free0 Semigroup a -&gt; Free0 Semigroup a -&gt; Free0 Semigroup a Free0 f &lt;&gt; Free0 g = Free0 (\h -&gt; f h &lt;&gt; g h) instance Semigroup (Free0 Monoid a) where (&lt;&gt;) :: Free0 Monoid a -&gt; Free0 Monoid a -&gt; Free0 Monoid a Free0 f &lt;&gt; Free0 g = Free0 (\h -&gt; f h `mappend` g h) instance Semigroupoid (Free2 Semigroupoid f) where o :: (Free2 Semigroupoid f) b c -&gt; (Free2 Semigroupoid f) a b -&gt; (Free2 Semigroupoid f) a c Free2 f `o` Free2 g = Free2 (\k -&gt; f k `o` g k) instance Semigroupoid (Free2 Category f) where o :: (Free2 Category f) b c -&gt; (Free2 Category f) a b -&gt; (Free2 Category f) a c Free2 f `o` Free2 g = Free2 (\k -&gt; f k . g k) given newtype Free0 k p = Free0 (forall q. k q =&gt; (p -&gt; q) -&gt; q) ... newtype Free2 k p a b = Free2 (forall q. k q =&gt; (p ~~&gt; q) -&gt; q a b) This is the best motivation I've seen for implication constraints so far (please share if you have more!) Very exciting answer and a much better motivator than data Exists c where Exists :: c a =&gt; a -&gt; Exists c instance c |- Show =&gt; Show (Exists c) where show (Exists a) = show a I need to sit down with some hot cocoa and try to wrap my brain around the typing of `|-`
Nice, I'm taking a look now!
This is a bit theoretical :) how do you know that there is no hidden exploit in any of images you see while browsing web. So yes one could DoS any server running a compiler. ;) And if we push how would I know there is no special microcode inside my cpu :) As for the project I am not sure `docker` provides a guarantee that a child process can not access the outside one.
With the current state of `ImpredicativeTypes`, probably looking sharply at the source code is sufficient to make it inconsistent.
Also: http://tryplayg.herokuapp.com/try/pascaltriangle.hs/edit
I do not understand much of what you said, but I hope you are right that better Type systems could lead to faster computations. But I have a hard time believing that. All of my numerical computations just have doubles as a type (I'm talking about C++ and MATLAB here). The way I see it, better Type systems only make the code less error prone (e.g. as in Haskell, different custom declared types for complex numbers, matrices etc.) and can automatically enforce some invariants that the underlying problem has. Can these fancy type systems do any better, I mean performance wise?
Wadler's papers, where Moggi's construct was first applied to programming languages, are very accessible. The first was "The essence of functional programming", but if you have trouble reading a postscript file, start with "Monads for functional programming", which is in pdf format and is a very similar paper to the first,slightly earlier, paper. Abstract. The use of monads to structure functional programs is described. Monads provide a convenient framework for simulating effects found in other languages, such as global state, exception handling, output, or non-determinism. Three case studies are looked at in detail: how monads ease the modification of a simple evaluator; how monads act as the basis of a datatype of arrays subject to in-place update; and how monads can be used to build parsers. http://homepages.inf.ed.ac.uk/wadler/topics/monads.html 
Normal recursion in Haskell makes it inconsistent. Simplest example is: foo :: Anything foo = foo This post is showing that it is inconsistent even without using normal recursion. At least to me this was quite surprising the first time I saw it, just like the Y-combinator was.
You should really change it to not be root.
Thanks.
Thanks.
I know it looks like nitpicking :) but I really don't think it's theoretical. Compilers aren't battle tested for this scenario (and rightfully so), and even if they were, it would be much harder to secure a compiler compared to an image viewer. The consequences might look trivial for a service like this (the attacker can just annoy you, or at worst mine bitcoins on your server), but if you were hosting this service for your users' private code, then you'd have to make sure that an attacker can't access other users' code, or even worse, modify it. You're right, docker gives no guarantees about security whatsoever, so I think it's a really non-trivial problem to host a compiler service.
Done! Now it runs as a non-root user.
Yes, I agree that Docker is not secure enough for this purpose. But I would like to experiment with it more to see how far I can go with this approach. Thanks!
You're right. I've yet to find a way to indent/space `f :: forall a. Foo a =&gt; a -&gt; a` that I like . . .
Needs differ. If I'm trying out a new design for profunctor-based lenses or a fair non-determinism monad, I usually don't need any packages at all.
It's fun, and that's all that matters ;)
&gt; But it is not good at automagically changing your data representations. If you ask for a Data.Set you get a Data.Set. Interesting, are there any languages that do this? If for example you program against the collection interface in Java, do they statically or even dynamically (based on access patterns) swap out the implementation for you? Is there a word for this concept (so i can find research)?
Obligatory 'no we are not the Strats team that /u/dons runs' goes here. It's a bit of a rider to our hiring posts. ;)
Thanks for the information. I will try ulimit to limit the maximum number of processes that a container can create. 
I understand the concern, and adding an extra layer of security can't hurt, right? But I think it's ultimately detrimental to system security to do things without good reason. Doing X because "you should always do X" is counter to actual security, and can provide a false sense of security or at worst introduce a new exploit. (The latter unlikely in this case, but eh.) 
better yet, could give emscripten a whirl. Perhaps you could run GHC on the browser rather than on the server.
Just so that the answer to this is mirrored on Reddit, not just the Github repo... I'm currently busy with a big cleanup / rewrite of IHaskell so I won't be able to get to making it GHC 8 compatible for a bit, but when the cleanup is done it should be GHC 7.10 and GHC 8 compatible (but not earlier GHC versions). If anyone wants to send a patch to make current IHaskell work with GHC 8, there's an open half-done PR for it and I'll gladly merge it if anyone finishes it up.
Don't fear the monad by Brian Beckman from Youtube: https://www.youtube.com/watch?v=ZhuHCtR3xq8
Limiting attack surface is a plausible security strategy. In this case someone might have found a way to make the compiler create a binary breaking out of the container. With root the things that could have been done are worse than a less privileged user. Though you are right that just this step doesn't make the whole thing secure just yet.
I disagree, I use stack to create new projects to play with ideas nearly daily and really appreciate the way that specifying precisely the dependencies for the code gets my thoughts in order. `stack new my-project simple` is all you need to play with new ideas, and know you'll be able to play with them again next year.
But is it actually easier to break out of the container as root in said container? I feel like that is the unanswered question. Maybe that's common knowledge and I'm just making a fool of myself. 
&gt; unboxed recursive types How do you do those? 
In Haskell it's /=not !=. I'm also pretty sure there's at least one library that provides the ligature syntax for that operator, possibly by now Prelude does too.
[`base-unicode-symbols`](https://hackage.haskell.org/package/base-unicode-symbols) is probably the most popular unicode operator library for `base`
Can you give me minimal examples of viewport and whatever render-time state information that can only be made available through monad? I can't imagine those examples.
What about "let infinity = S infinity"
Maybe someone needs a big-endian machine?
The tooling got much better in a short amount of time. When I began hacking in Haskell, I would spend significantly more time fighting with editor plugins, Cabal and other such things than actually writing lines of code. Maybe two years have passed, and everything has changed. Even learning Haskell is more easy now. The community has grown, the language has more visibility. These are great times :)
I've been playing with ligature fonts recently. Fira Code has a ligature for /= though it looks like /=. Its ligature for != is ≠ though. I was tempted to just declare `(!=) = (/=)`.
What does the /= ligature look like in hasklig? I'm using Fira Code. EDIT: /= looks nicer in Hasklig but most other things I prefer in Fira Code.
2nd that. And thanks!
Agreed! 
But this is not unboxed recursion, you just wrote that the values of data Nat = Z | S Nat are finite Z terminated sequences of S tags and I contradicted it with a well defined thing. Are you talking about totally redefining what a data declaration is in Haskell to make it unboxed implicitly?
We've never released Stack binaries for PowerPC and I don't think anyone has ever actually asked for them, so must be rare. GHC does, however, still release Linux PowerPC bindists, which is why I mentioned it. The problem in 'store' is a general one for RISC architectures that don't support unaligned memory access.
FYI: We've discovered a regression that may cause problems in build environments like Travis CI. If you have a version of `git` &lt; 1.8.2 (released in early 2013), `stack build` will fail with an error like `fatal: error in object: unshallow 05a8ba81a30b775e3aab8269642317345708bcfa` when it tries to update the package index on first startup. See https://github.com/commercialhaskell/stack/issues/2602 for more details and workarounds.
`stack hoogle` fails for me with this (Windows 10) Run from outside a project, using implicit global project config Using resolver: lts-6.17 from implicit global project's config file: C:\stack\global-project\stack.yaml Hoogle isn't installed or is too old. Automatically installing (use --no-setup to disable) ... Populated index cache. Minimum version is hoogle-5.0. Found acceptable hoogle-5.0.4 in your index, installing it. Run from outside a project, using implicit global project config Using resolver: lts-6.17 from implicit global project's config file: C:\stack\global-project\stack.yaml While constructing the build plan, the following exceptions were encountered: In the dependencies for hoogle-5.0.4: haskell-src-exts-1.17.1 must match &gt;=1.18 &amp;&amp; &lt;1.19 (latest applicable is 1.18.2) Plan construction failed.
&gt; When can we expect this in GHC? :) Take a look at Richard's [Dependent types in Haskell: Progress Report](https://typesandkinds.wordpress.com/2016/07/24/dependent-types-in-haskell-progress-report/).
I'm talking about a dependently typed language with precise data layout specification, which I did a small amount of research on. It's not Haskell. If the language is total, `let infinity = S infinity` doesn't type check. If it's partial, `let infinity = S infinity` goes into loop. `data Nat = Z | S Nat` in my language behaves like `data Nat = Z | S !Nat` in Haskell, but the memory representation is very different. In Haskell the value `S (S (S Z)))` is a pointer to a `Nat` object that itself transitively contains three more pointers, in each `S` node. It's a singly linked list without fields. In my language, the value `S (S (S Z)))` contains no pointers, and the value itself is not a pointer either. It's a contiguous memory chunk where `S` and `Z` could be tag words or bytes. If we put a flat `Nat` inside a linked list, the list would also have to have variable-sized fields in order to accommodate the `Nat`-s. If we pass a `Nat` to a function, it's pushed to the stack as it is, and the called function can determine its size by walking it until hitting `Z` (a bit like a variadic C function). Of course, there's little apparent use to flat `Nat`-s, so my hypothetical language would let us define `data Nat = Z | S &amp;Nat` where `&amp;Nat` is a reference to a `Nat`. In Rust, for comparison,`&amp;Nat` recursion is mandatory because there are no variably-sized types aside from a constrained set of "unsized types", which are AFAIK all based on arrays. 
Stack has made Haskell development a breeze. Thanks everyone who's involved.
`stack --nix` users should definitely upgrade: it's the only release that will work, due to breaking changes in Nixpkgs. stack-1.2 works around those changes, with a few goodies thrown in.
The OP design the PragmataPro font so he has to make some decision (well I guess you can do different variants in one font but something has to be the default).
Take a look at the code for the [*generics-sop*](https://hackage.haskell.org/package/generics-sop-0.2.2.0), [`AllF`](https://hackage.haskell.org/package/generics-sop-0.2.2.0/docs/Generics-SOP-Constraint.html#t:AllF) in particular. There is also a similar type [`NS`](https://hackage.haskell.org/package/generics-sop-0.2.2.0/docs/Generics-SOP-NS.html#t:NS) — which takes a functor and list of arguments data NS :: (k -&gt; Type) -&gt; [k] -&gt; Type where Z :: f x -&gt; NS f (x:xs) S :: NS f xs -&gt; NS f (x:xs) compare that to — which takes a list of functors and a single argument data FSum :: [k -&gt; Type] -&gt; k -&gt; Type where Here :: f a -&gt; FSum (f:fs) a There :: FSum fs a -&gt; FSum (f:fs) a so the differences are NS f [a, b, c, d] = f a + f b + f c + f d FSum [f, g, h,i] a = f a + g a + h a + i a Some of the instances of `NS` use [`Compose`](https://hackage.haskell.org/package/generics-sop-0.2.2.0/docs/Generics-SOP-Constraint.html#Compose) which is a [*constraint synonym*](https://gist.github.com/Icelandjack/5afdaa32f41adf3204ef9025d9da2a70#constraint-synonym-encoding) like a `type Compose f g a = f (g a)` that you can partially apply. I will use the definition `type (f · g) a = f (g a)` for clarity: deriving instance All (Show · f) xs =&gt; Show (NS f xs) deriving instance All (Eq · f) xs =&gt; Eq (NS f xs) So an `Eq` instance for `NS f [a, b, c, d]` requires All (Eq · f) [a, b, c, d] = ((Eq · f) a, (Eq · f) b, (Eq · f) c, (Eq · f) d) = (Eq (f a), Eq (f b), Eq (f c), Eq (f d)) (using [*constraints*](https://hackage.haskell.org/package/constraints-0.8/docs/Data-Constraint.html) we can show this by an inhabitant of) Sub Dict :: (Eq (f a), Eq (f b), Eq (f c), Eq (f d)) :- Eq (NS f [a, b, c, d]) See also [*Type Classes and Constraints*](https://gist.github.com/Icelandjack/5afdaa32f41adf3204ef9025d9da2a70).
You may want to investigate the source for [contained.af](https://contained.af/) \[0]. It's made by one of the people I consider most knowledgeable about securing such things. \[0]: https://github.com/jfrazelle/contained.af
I'm a little leary of this, because it is not obvious where the default stack.yaml is or which resolver I was defaulting to without knowing more about stack. In fact, mine is in a different place than his was. .stack/global/stack.yaml. I would be less leary if the default stack.yaml was just .stack/stack.yaml, or .stack/global.yaml, preferably with a comment explaining what it was and when it would be used. Or even just .stack.yaml on unix. Edit: I just noticed that when stack generates a new global stack.yaml it actually does have comments in it. Maybe none of this is necessary.
See also http://codepad.org/ (it runs on a custom sandboxing tool built in Haskell) and also https://cloud.sagemath.com/ which gives ghc and a bunch of other tools in many languages, in the browser, with a full environment, for free.
Could you use [hint](http://hackage.haskell.org/package/hint) and GHCJS to create a JS version of GHCi instead?
Yeah. Honestly, that's kind of a bad example, though, because that's equivalent to: data Showable = Showable { show_ :: String, showsPrec_ :: Int -&gt; ShowS, showListN_ :: Int -&gt; ShowS } makeShowable :: (Show a) =&gt; a -&gt; Showable makeShowable a = Showable (show a) (\p -&gt; showsPrec p a) (\n -&gt; showList $ replicate n a) The reason it's equivalent is because you can't do `\(Showable a) (Showable b) -&gt; showList [a, b]`, so the only values that can be passed to `showList` is any number of `a`s.
If you're happy creating two configuration files and a directory structure to try out a quick, single-module idea, that's great. I find it cumbersome.
Thank you so much, that was indeed the problem. Slowing down the program is no issue for me as I am currently only interested in the memory usage. Thanks again.
Yes. It exposes a lot more code. You can attack the Linux kernel if you are not properly sandboxed, which would allow you to completely thwart all defenses on the host system by simply using some kernel vulnerability. Your network container and lifetime guarantee are now gone (they may as well have never existed once you're inside the kernel and it has let you execute your payload) The root user will have many more capabilities available to it to perform attacks like this, since it will have higher privileges and capabilities, and be exposed to more code, even if the "outer" system is enforcing some level of control -- i.e. it will still, at some level, have unfettered system call access, and `uid == 0` will expose certain code paths throughout the stack that may not be hardened to an active attacker (the code may have been originally written under the assumption that if you have root anyway, it's game over, so who cares about vulnerabilities? So it might not have been extensively reviewed. This didn't account for the matryoshka-doll style containerization we have today.) In fact, that example is not theoretical, it is real. User namespaces in Linux are a relatively new feature that allow root users inside containers to be mapped to non-uid users outside of containers. Docker uses this. It might even be what this deployment by OP is using, to map the root user into a non-root user "outside". The problem with this is precisely what I said: User namespaces expose a substantial amount of new code as attack surface. Previously that code was only accessible for root anyway, but now it has to operate assuming namespaces might be involved, meaning that "root only code" is no longer root only code. Think about systems like iptables and nftables -- sure, the TCP/IP stack is hardened, but the actual *user interface* between the kernel and userland was previously root-only to do things like set up rules, using `ioctl`s to some device (or writes to some `sysfs` thing or whatever). However, under a user namespace, if you're in a container as root, why shouldn't you be able to use nftables on your own, namespaced network devices? And there's the problem: nftables is something *in the trusted host kernel* that is now exposed to untrusted systems, due to the fact they "have `root`", even if it's a lie. [Overlayfs is another example](http://www.halfdog.net/Security/2015/UserNamespaceOverlayfsSetuidWriteExec/), but nftables probably is another good thing to be suspect of I'd guess, because it's relatively new. There's been a string of user namespace vulnerabilities over the past years IIRC... Systems like grsecurity and even some Linux distributions have stopped from shipping user namespaces by default and probably won't change for a very long time, for these reasons. In general an unprivileged user could also do some kind of kernel attack from inside a container, of course. They'll just have less room to do so.
I love talks by Simon. In fact I like them so much, that I can tolerate the overabundant use of Comic Sans.
The recent AnandTech reviews of POWER8 show that it is actually more powerful than equivalent Intel hardware and costs less, simultaneously. Seriously impressive stuff for what seemed like a dead architecture only a couple of years ago.
Thanks, I learned a lot now! 
I am sorry. I am not experienced in template. I use c++ as a better c for its library. Use ada for embedded system. I love haskell for it is so good for mathematics...
He doesn’t seem to use the `ImpredicativeTypes` extension in that article. I think that extension basically just relaxes the constraints on when you have to pack and unpack types. `Rank2Types` is enough to let you express some impredicative polymorphism.
This is great, thanks for sharing! I'm really trying to grasp the intuition behind products and sums and their use in FP languages. Is the product construct the definition of composition? Is the sum construct, function application (or maybe type constructors)? 
No, not really--I was looking into this for a friend's project, and then got nerd-sniped once I ran into the IO issue. But I'll take a look at some other libraries.
Usually when treating FP categorically, product is a pair `a*b = (a,b)`, and sum is the `Either` type `a + b = Either a b`.
Would you say product and sum are foundations of category theory, if so does that mean that pair and Either form a foundation for FP language as well? They seem more trivial at the FP level. 
Thanks for the suggestion. I ended up setting up emacs + haskell-mode + hashtags, and then applied this technique(not that I had a choice, coz it's the only way I think, :) ). And I have already made my first contributions to Cabal, and it's merged in.
That's not the case if you want to write an operation on two instances of it; the existential makes it impossible to align the types of two values.
Ah cool. Studying these constructs is very enlightening into how exactly math gets transformed into code. Thanks.
So does the explicit dictionary in the article. You lose information about the type of the thing the that dictionary represents just as quickly, since the dictionary factors it out. Unless your talking about something else. Like, `data X a = X a` instead of `data X = forall a. C a =&gt; X a`. But that's a different thing than what I'm talking about.
Products and Coproducts (aka sums) are foundational concepts in category theory. But I wouldn't say they're the foundations so much as "entryways". In particular, they generalize to limits (resp colimits), and even further to ends (resp coends). There are a few avenues to build up "all fundamental concepts" from various starting points -- including via (co)limits, adjoints, and kan extensions. The usual path is to start with (co)limits, which are generalized products/coproducts, and build from there. In this approach, the avenue is to look at products and coproducts as instances of initial/terminal constructions. One thing to try is to find example categories that are less set-like than the programming language context (for example the category of graphs, or of abelian groups) and look at how limit constructions do or don't play out in those settings.
Good job! I'm glad to be helpful. Haskell has its advantages when it comes to learning a new code base. For example, pure functions are usually easier to understand and we try to use them as much as possible. Monads limit what can be done (in terms of side effects) so you can reason about what can be happening in a piece of code more easily etc. In the end, while tooling is much, much worse than Java, I think we compansate for it by having a much better language. Good luck with your future contributions ;-) 
"stack" (with "cabal") will handle downloading packages and building your project with a consistent set of dependencies. You may want to go through stack's guide: https://docs.haskellstack.org/en/stable/GUIDE
&gt; [...] the definition of composition The term "composition" is uselessly vague, despite how much pomp and circumstance it gets in FP circles. Virtually every construction you see in FP is some kind of "composition". Products and sums allow you to take two types and compose them to get a new type. The `+` operator allows you to take two numbers and compose a new number from them. The `(.)` operator lets you take two functions and compose them to get a new one. So don't think "Is this composition or not?" Think "What does this kind of composition do?" Products allow you to talk about tuples. In more general categories, they give you thinks like cartesean products (joins, if you think in terms of databases) the "and" conjunction in logic, and the intersection (or meet of a lattice or partial order). Sums give you disjoint unions, both for types and for sets. It corresponds to "or" in logic. 
Also, attoparsec and similar parser combinator libraries are often not ideal for parsing binary file formats. The classic libraries for this are [binary](http://hackage.haskell.org/package/binary) and [cereal](http://hackage.haskell.org/package/cereal), and there are some newer libraries that look nice.
Why not use an existing haskell git library to not reinvent the wheel ? [git](https://hackage.haskell.org/package/git) [gitlib](https://hackage.haskell.org/package/gitlib) and there's couple others
I should probably link to the `pipes-zlib` abomination /u/happy4crazy is trying to improve on: https://github.com/vaibhavsagar/duffer/blob/master/src/Duffer/Pack/Streaming.hs
I'm not familiar with either your library or the specific binary format you are parsing - I'm sure your library is great! Typically, though, binary formats consist mostly of fixed sequences of fixed size fields, or variable length fields where you are provided with a byte count. So you don't need the power of parser combinator libraries for dealing with complex syntax. Instead, the binary parsing libraries provide convenient combinators for dealing with things like endianness, and standard binary encodings for various sizes of integers and IEEE floating point numbers.
Cool! The speaker talk about combinators which are function which only refers to their arguments. It looks like "super" pure functions. I was wondering, is there a way to know if a function is a combinator from it's signature , is it related to *parametricity*?
Not sure if any of the parsing libraries support access to the byte count (or input position) by default. If not, you have two options: 1. Thread the byte count around in the parser results, so that your parsers return `(a, ByteCount)` where they currently return `a`. 2. When using a parser monad transformer, use `ParserT (StateT ByteCount IO)` instead of `ParserT IO`, or just use an `IORef` since you're already in `IO` anyway. The second option is arguably cleaner, but doesn't work with Attoparsec. Both are annoying because you'll have to reimplement many of the combinators provided by the libraries. Luckily, that's not usually too difficult.
Speaking as a lay person, I found "Category theory in context" to be practically impenetrable from about the first page. The beginnings of "Category Theory for Computing Science" is giving me hope, false though it may be, that I have a shot at understanding what comes later. Thanks for linking to the updated version.
Note to downvoters - If I am wrong, and that very well may be the case, please explain why and dispel the doubts. Making my post invisible will have the opposite effect.
Oh I did not know this! This is good to know. I am a stack noob. Stack is really very well-designed! 
The problem seems to have been an old `stack` tool. Always try to upgrade if something fails!
Then it's either the K combinator from SKI calculus, or the flipped K combinator. It is possible to return a combinator from a function.
This has me a little uncertain. A combinator can have free variables as long as those free variables are themselves combinators? Then aren't non-combinators actually pretty rare? The only things that can be proven to be free variables would be native functions like `putStrLn` or literals like `0`. Anything else either calls down to one of these eventually, or doesn't. Those that don't thus only call other combinators (if you treat type class instances as dictionaries that are explicitly passed, and expand the dictionary entries to separate parameters) which makes them combinators. This would seem to encompass a huge percentage of Haskell functions. Edit: I suppose this is to say: if everything is church encoded, then everything is a combinator under this definition, except for things that eventually call native functions.
Why do you say that the state of the paradigm is poor? It is one of the oldest programming paradigms, and there has been a lot of interesting new work on it in the past few years. Perhaps more code has been created in concatenative languages than in any other class of programming language, if you count automatically generated code. Because Postscript happens to be a concatenative language. :)
There could be more awareness and more practical language options. Things are getting better, though. Yes, people are doing interesting work, but there are few of us and we’re all kinda doing our own things, not collaborating.
I did a bit of research and found http://kittenlang.org, which looks pretty sweet. It's obviously heavily influenced by Haskell, which I like. Edit: omg it's you hahaha
Elaborating, `Applicative` for `Either'` doesn't stop when it encounters a single error, it collects all the errors and combines them using [`&lt;&gt;`](https://hackage.haskell.org/package/base-4.9.0.0/docs/Data-Semigroup.html#v:-60--62-): data Either' err a = Left' err | Right' a deriving Functor instance Semigroup err =&gt; Applicative (Either' err) where pure :: a -&gt; Either' err a pure = Right' (&lt;*&gt;) :: Either' err (a -&gt; b) -&gt; (Either' err a -&gt; Either' err b) Left' e1 &lt;*&gt; Left' e2 = Left' (e1 &lt;&gt; e2) Left' e1 &lt;*&gt; _ = Left' e1 _ &lt;*&gt; Left' e2 = Left' e2 Right' f &lt;*&gt; Right' a = Right' (f a) instance Semigroup err =&gt; Concurrently (Either' err) (Either err) where runConcurrently :: Either' err a -&gt; Either err a runConcurrently = \case Left' x -&gt; Left x Right' x -&gt; Right x runSequentially :: Either err a -&gt; Either' err a runSequentially = \case Left x -&gt; Left' x Right x -&gt; Right' x ---- See [`Data.Either.Validation`](https://hackage.haskell.org/package/either-4.4.1.1/docs/Data-Either-Validation.html) or the [*validation* package](https://hackage.haskell.org/package/validation).
if I were modelling this I would either use GADTs, if I wanted to show-off. Else, I'd hide the original constructor in a module and export a smart constructor with the type class constraints. what do other haskell-heads think of these approaches?
Given that `binary` doesn't have to deal with backtracking, that makes sense.
I think we chatted about this in your last thread about Fraxl. I agree with your conclusions. Another basic example of the same phenomenon but having nothing to do with concurrency is `[]` vs `ZipList`, where the latter uses `zipWith` for its `liftA2`. [Here](https://tangledw3b.wordpress.com/2015/02/12/applicatives-monads-and-concurrency/)'s a blog post I wrote about the same subject.
Oh, it looks like they added that in 0.7. Interesting. So much for the nice asymptotic guarantee.
Yes, it a very interesting approach.
If the bool is true, then your function is the `const` combinator. If false, then it's `flip const`. Since the bool is immutable (the function is not performing some sort of mutable dereference which would require IO) it's just an "implementation detail"; this function behaves like a combinator even if it doesn't fit the description of "not referencing anything except it's arguments".
To be honest, I know next to nothing about category theory (basically what gets covered in the first 5 pages of any CT text book) but I can follow, more or less, what Ed says in his talks. I'd even say that knowing more CT wouldn't make me understand more.
Stand next to Edward Kmett and show any sign of life. You'll probably have to learn a lot of other stuff in the process too, unfortunately. IRC works too.
The last couple of phone calls with danharaj have been all about graphics. =)
Another example is [`FailingWriter`](http://comonad.com/reader/2012/abstracting-with-applicatives/) which is very similar to the `Validation` type type FailingWriter' w a = Compose (Writer w) Maybe a which returns the entire trace even though it fails say' :: Monoid w =&gt; w -&gt; FailingWriter' w () say' w = Compose (Just &lt;$&gt; tell w) die' :: Monoid w =&gt; FailingWriter' w () die' = Compose (pure Nothing) &gt;&gt;&gt; say' ["Birth"] *&gt; say' ["Life"] *&gt; die' *&gt; say' ["Afterlife"] Compose (WriterT (Identity (Nothing,["Birth","Life","Afterlife"]))) It has no `Monad` instance! Compare this to `MaybeT (Writer _)` where death is more final type FailingWriter w = MaybeT (Writer w) say :: w -&gt; FailingWriter w () say a = MaybeT (Writer (Just (), a)) die :: Monoid w =&gt; FailingWriter w () die = MaybeT (pure Nothing) &gt;&gt;&gt; say ["Birth"] *&gt; say ["Life"] *&gt; die *&gt; say ["Afterlife"] MaybeT (WriterT (Identity (Nothing,["Birth","Life"])))
Bartoszmilewski.com I'm attending the irl class for this and it's pretty good
Haha, well, thank you nonetheless. :D The site is getting pretty old because I’m busy and lazy. You should check out the GitHub repo for new stuff. 
In C++ to boot. What a scam.
There is a way to do exactly what you would like. Monads are Functors, so you can use `fmap` (or its infix alias `&lt;$&gt;`) to do what you want: do nameMapping &lt;- Map.fromList &lt;$&gt; mapM sigPair collectedFuncs return $ transform (replaceName nameMapping) ts or even flip transform ts . replaceName . Map.fromList &lt;$&gt; mapM sigPair collectedFuncs Overusing pointfree form can make obfuscate things, but in this particular case it nicely captures "piping the result of an effectful compuation through an effect-free chain of functions".
Re syntax: implement MonadZip for your type, and enable the MonadComprehensions and ParallelListComprehensions extensions, perhaps?
Open an issue on the tracker.
Edward is crazy, so you don't want to go there :) and anyway, I feel (as a professional mathematician) that CT is overhyped. Go learn some homotopy theory instead, it's almost the same, just better :)) Anyway, Mac Lane's "Homology" is not too bad, it's rather dated and very terse, but not bad. He also wrote a book titled "Categories for the Working Mathematician", might be relevant. And there is the HOTT book, again, not exactly category theory, but (relatively) readable, and may serve you better.
I stood next to Edward Kmett a few times, and I can assure you that I became significantly smarter as a result.
He chose C++ so he wouldn't have to deal with binding certain C++ libraries to another language. Also I think people forget that before he became the white wizard of Haskell that he was Sauron of C++.
I really enjoy watching those videos and constantly refresh the youtube channel on Thursday's looking for the new material from a previous Wednesday. The best parts are the little side comments or observations which a mathematician or book might not say but help tie the material together for non mathematicians.