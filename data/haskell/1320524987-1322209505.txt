GHC 7.0.2 (as included in current HP) needs the Mac OS X 10.5 SDK. If you have Xcode 3.x installed on 10.7, you're good to go. If you have Xcode 4.x installed, then you need to have a saved version of the 10.5 SDK and placed it somewhere. If you have, then you can fix the end of the GHC invocation script as follows: sdk="/Xcode323/SDKs/MacOSX10.5.sdk/usr" exec "$executablename" -B"$topdir" -I"$sdk/include" -L"$sdk/lib" -pgmc "$pgmgcc" -optc"-I$sdk/include" -pgma "$pgmgcc" -pgml "$pgmgcc" -pgmP "$pgmgcc -E -undef -traditional" ${1+"$@"} (Script is located at: /Library/Frameworks/GHC.framework/Versions/7.0.2-i386/usr/bin/ghc-7.0.2) 
It sounds like you may want to take the same approach you mentioned in your C++ code, but in a functional way. I'd recommend taking a look at some of Bird's functional pearls such as [1] or look in his book [2] for a great variety, as he often takes the approach of: "okay, lets start with the obvious and (often) intractable approach of generating all possible graphs, and then filter that list." Then he goes one step at a time to generate the solutions more efficiently by taking advantage of the properties of the problem until he often arrives at quite efficient (and elegant) solutions. Haskell's laziness can be a great benefit here as Flarelocke mentioned: you only generate functions as your filter evaluation function needs them. So the essence of your program might be: `filter graphPass efficientGenerateGraph`. I also recommend you look at the Inductive graph library [3] as it is a much more mature graph library as evidenced by Ivan's wonderful supporting libraries such as graphalyze [4], graphviz, . It took me a bit of a learning curve to wrap my head around it, but once you get it (just ignore all the monadic interfaces at first for instance) then it is a joy to work with. To see some 'real' code that uses fgl then you can look at my code which does some computational biology inference/modeling using it. I've tried to document the code, but I could have done a better job. [5] [1] - http://www.cs.tufts.edu/~nr/cs257/archive/richard-bird/sudoku.pdf [2] - http://www.amazon.com/Pearls-Functional-Algorithm-Design-Richard/dp/0521513383 [3] - http://hackage.haskell.org/package/fgl [4] - http://hackage.haskell.org/package/Graphalyze-0.11.0.0 [5] - https://bitbucket.org/binarybana/grn-pathways
Is it possible to translate this into Haskell ? I know that it is stupid question, because Haskell already has monadic I/O. But do I have to learn Lisp/Scheme to learn Haskell monadic I/O ? Is it not possible to have a toy monadic I/O implementation in Haskell, for the newbies ? 
Haskell is not an ideal fit if you want to write 'largely' GUI programs. There is no (language) reason why Haskell couldn't be great for GUIs, but great GUI support is a result of "industrialization", it needs *time* dedicated people rather than just dedication in the sense of being *keen*. My suspicion is such dedication needs to be a paying job or at least a paying contract, for instance when Clean was a commercial product it had much better GUI support than Haskell (albeit Windows and pre-OSX Mac only). Using F# is a good alternative if GUI matters to you.
One can also make a modular application, with backend written in Haskell and packed as a library, and frontend written in, say, F#.
That sounds interesting. I'll have to look into how to do that.
It is not realy about getting the problem solved, its more that I need to come uppon a real use for language feature, to have the feeling that I realy understood what the possiblities and use cases of that lang feature are... TypeFamilies are still open at the moment on my list ;-) So what I realy liked here where two things: * a data type with a signature like Address IPv4 * the way to write the class implementation and say f.e.: the type Address IPv4 = Word32 
Even if so, does it sound like a bad idea to add these INLINE pragmas?
Keep in mind that this report is over 3 years old.
Another approach is to use web programming to get a rich interface. Haskell has several excellent web frameworks, and UI programming in general does seem to be heading in that direction. You can deploy the app as if it were a traditional GUI program by bundling it with its own light-weight Warp server; to the user, it looks like any other EXE that opens a GUI window. You do need to know something about web programming beyond just the frameworks, though.
In the case of Haskell/F#, the best approach by far would be to implement a message passing protocol between the front end and back end, say over the network loopback interface. AFAIK, nobody's done a .NET/Haskell foreign function interface, so in-process interop would be a lot of groundwork before you can start on the application (not to mention needing in-depth knowledge of GHC and .NET, and that both have relatively heavy-weight runtime systems that would likely be largely isolated from each other anyway) Also, if you are familiar with GTK or WxWidgets, there are Haskell bindings to them, but as Stephen pointed out, they aren't exactly a joy to use from Haskell. They certainly don't feel like a "native Haskell" API. There are numerous other GUI libraries that have been done over the years. Here's a [list](http://www.haskell.org/haskellwiki/Applications_and_libraries/GUI_libraries), many of which have bitrotted into oblivion.
I can see one problem with it. Maybe it's just a caveat for the implementation, though. If a compiler can express class-specific [rewrite rules](http://www.haskell.org/haskellwiki/GHC/Using_rules), then you can't really determine whether a class constraint is used or not, if that class only has rewrite rules for functions in its parent class. Sure, it's not possible now, but perhaps it becomes a feature in the future.
I haven't figured it out completely yet, but the problem occurs because your type `c` doesn't appear in the signature of your functions. So GHC sees some type (Word32), and knows that it corresponds to some type family application (T c0), but it can't figure out the actual type `c0` because, as the error says, type families are not injective. You can solve this by adding an argument containing `c`. You can just add a `c`, but since it's not used, I think a proxy is nicer: {-# LANGUAGE TypeFamilies #-} {-# LANGUAGE FlexibleContexts #-} {-# LANGUAGE FlexibleInstances #-} import Data.Bits import Data.Word data Proxy c = Proxy class (Bits (T c)) =&gt; C c where type T c :: * f1 :: Proxy c -&gt; (T c, T c) -&gt; (T c, T c) f1 _ (a, b) = let x = a .&amp;. b y = x .|. complement b in (x, y) f2 :: Proxy c -&gt; (T c, T c) -&gt; T c f2 p = fst . f1 p data D instance C D where type T D = Word32 
In this case c can be its own proxy: {-# LANGUAGE TypeFamilies #-} {-# LANGUAGE FlexibleContexts #-} {-# LANGUAGE FlexibleInstances #-} import Data.Bits import Data.Word class (Bits (T c)) =&gt; C c where type T c :: * f1 :: c -&gt; (T c, T c) -&gt; (T c, T c) f1 _ (a, b) = let x = a .&amp;. b y = x .|. complement b in (x, y) f2 :: c -&gt; (T c, T c) -&gt; T c f2 p = fst . f1 p data D = D instance C D where type T D = Word32
Good catch!
Replying to myself: Another option is to use a data family, since those *are* injective: {-# LANGUAGE TypeFamilies #-} {-# LANGUAGE FlexibleContexts #-} {-# LANGUAGE FlexibleInstances #-} {-# LANGUAGE GeneralizedNewtypeDeriving #-} import Data.Bits import Data.Word class (Bits (T c)) =&gt; C c where data T c :: * f1 :: (T c, T c) -&gt; (T c, T c) f1 (a, b) = let x = a .&amp;. b y = x .|. complement b in (x, y) f2 :: (T c, T c) -&gt; T c f2 = fst . f1 data D instance C D where newtype T D = TD Word32 deriving (Show, Eq, Num, Bits) 
It is possible to translate it, I guess. Look at unsafePerformIO, which is basically the function `run-io` from this blog post.
Yeah, I'm doing exactly this at the moment. I'm writing my GUI in Java and the algorithmic core in Haskell. I tried and failed to get a GUI going with the Haskell GTK bindings. Haskell's GUI libraries are really holding it back.
It would be nice to be able to warn about all not-as-general-as-possible types (with some way to define specialized values to override it). Consider this buggy definition: sort :: Ord a =&gt; [a] -&gt; [a] sort [] = [] sort (p:xs) = sort [x | x &lt;- xs, x &lt; p] ++ sort [x | x &lt;- xs, x &gt;= p] (Note it forgets to insert the pivot itself). The most general type inferred by GHC here is correctly: sort :: Ord a1 =&gt; [a1] -&gt; [a] This could be a great hint that this implementation is buggy. But the more specialized type is silently accepted, despite the intent to document the most general type here. 
It's a good tutorial, it's just a shame it's actually a long tutorial about how to write HaskellDB code using Chris' private libraries (see repeated code shared via hpaste in the footnotes). If Chris had only waited to get his stuff packaged and shared first, this would be perfect. Nontheless, good motivation to check out HDB.
But you often put type declarations that are not as general as they could be on purpose, and getting warnings for those would be annoying.
Yeah, some way to specify the type should match the most general possible (or vice versa) would resolve that.
The problem is that the type `c` does not appear in the class functions' type declarations. So why not make it so? class Bits c =&gt; C c where f1 :: (c, c) -&gt; (c, c) f2 :: (c, c) -&gt; c Then declare instances for each kind of thing you want this for; e.g. instance C Word32 where ... Later, you may define a type family whose range includes `Word32` if you like: type family Address a type instance Address IPv4 = Word32 so that you may write pretty types like: foo :: IPv4 -&gt; Address IPv4 -&gt; IPv4 This solves the problem with the code you pasted. Now, the other question is then: well, what good are type families, then? The answer is that sometimes, you want the type of one argument to a function to depend on the type of another argument to the function. The canonical example is inserting into a collection; you want the type of the thing you're inserting to depend on the type of the collection you're inserting into. As another concrete example, I have written a class like this: class Edit m where type Value m apply :: m -&gt; Value m -&gt; Value m As instances, you might write something like: instance Edit (Maybe a) where type Value (Maybe a) = a apply Nothing old = old apply (Just new) old = new instance (Edit a, Edit b) =&gt; Edit (a, b) where type Value (a, b) = (Value a, Value b) apply (ma, mb) (a, b) = (apply ma a, apply mb b) (Potentially quite interesting question: what do you think the data type representing edits to an `Either a b` value should look like? What about edits to a `[a]`?) Some sample uses: &gt; apply (Just (3,4)) (5,6) (3,4) &gt; apply (Just 3, Nothing) (5,6) (3,6) &gt; :t apply (Just 3, Nothing) apply (Just 3, Nothing) :: Num a =&gt; Value (Maybe a, Maybe a1) -&gt; Value (Maybe a, Maybe a1) &gt; :kind! Value (Maybe Int, Maybe Int) -&gt; Value (Maybe Int, Maybe Int) Value (Maybe Int, Maybe Int) -&gt; Value (Maybe Int, Maybe Int) :: * = (Int, Int) -&gt; (Int, Int) (That last one only works in recent GHCs.)
Do you think that such a warning would often point to a bug? I cannot think of another example than the one you have posted right now.
I don't know if it is frequent (though there are definitely more cases). I think the main benefit may perhaps be discovering "hidden generality". I see Haskell users often write code without realizing that it is in fact more general than they think, as they unnecessarily restrict the types.
Oftentimes, you want to limit the types, because you won't be using that function in a more generalized setting anyway, and stricter types make things clearer.
That was great!
Try something like ZeroMQ. No need to invent you own protocol. 
Excellent write-up. HaskellDB has a lot of potential, but I think that widespread adoption of it is hindered by the lack of up-to-date tutorials. Another good HaskellDB tutorial is [Example on using HaskellDB](http://users.utu.fi/machra/posts/2011-07-15-haskelldb.html) by Mats Rauhala.
This could work for me, I think. Do you know of anyone who's done that before (executable with bundled server to make a "local" Haskell web app)? 
I would be *very* interested in a short tutorial on how to do this.
+1 for this approach definitely. That's what we've done at work (with zeromq and protocol buffers in the middle). We've achieved pretty seemless integration between C#, C++, python and web components. and it's a breeze to integrate or swap stuff out. No haskell in there yet, but I've had a play and it's just as easy as the other languages.
My general recommendations: * Read LYAH, RWH, and Typeclassopedia * Hop on #haskell irc chat. Ask questions. Use hpaste. * Also ask questions on StackOverflow (tag them as Haskell) * Start conversations such as this one on /r/haskell When it comes to IO: * use `&lt;-` in a do block to perform an IO action, and assign the result to a variable * use `let` in a do block to "perform" a pure function and assign the result to a variable When it comes to random number generation, the general pattern is: foo :: IO Result foo = do gen &lt;- getStdGen let (result, gen') = random gen (res2, gen'') = random gen' putStdGen gen'' return $ combineSomehow result res2 If you want `foo` to be pure, then make `gen` a parameter, and bundle the new generator with the result: foo' :: StdGen -&gt; (Result, StdGen) foo' gen = let (res, gen') = random gen (res2, gen'') = random gen' in (combineSomehow res res2, gen'') -- then, equivalently to before foo :: IO result foo = do gen &lt;- getStdGen let (res, gen') = foo gen putStdGen gen' return res When it comes to passing state and/or generators, it's just something where you have to dive in and get used to it.
That's actually the exact description of how the yesod web-framework is used by some of its adepts.
I know that people have done this and deployed real apps in the wild. I haven't done it myself. Seems that you would set up the web app and Warp in the usual way, except on a high port. The only real issue I can see is working out how to get the browser window to launch, but many Windows apps do that. I suppose if you want it to be really polished there are going to be a few more details to work out, like making sure the app notices if the browser window gets closed suddenly.
Maybe that kind of thing would fit better in hlint than in the compiler.
All of the major web frameworks (Snap, Yesod, Happstack) include a bundled server. There's also the [wai-handler-webkit](http://hackage.haskell.org/package/wai-handler-webkit) package which uses the QtWebKit control. I haven't used it before but this is the kind of thing I would want if doing a desktop web app, it saves you from having to test on multiple browsers.
The last time I checked, the ZeroMQ bindings [don't work](https://github.com/twittner/zeromq-haskell/issues/8) properly on Windows. ZeroMQ tries to do fancy non-blocking stuff like GHC, so writing bindings is a bit more tricky than normal.
Given your doing F# GUIs I'm assuming you're on Windows, if so you could also try [Hs2lib](http://mistuke.wordpress.com/2011/07/01/hs2lib-automating-compilation-of-dynamic-libs/). It can build a standalone Haskell DLL with your functions exported using a C API. It also generates the C# required to call the C API.
Yeah, I've examined ZeroMQ, and I'd really like to play with it. Unfortunately I'm not sure what to do with it at the moment... my current projects don't have much message passing involved beyond what is already prescribed by various standards. 
[hledger-web](http://hackage.haskell.org/package/hledger-web) is one example. It does not use, but you could, things like [wai-handler-webkit](http://hackage.haskell.org/package/wai-handler-webkit) that give the web app a more desktop-app-like lifecycle.
Sometimes you discover that the type is so general that you should hoogle it instead of coding it.
i had success with this - it delivered exactly what was promised
When I got more serious about Haskell, I removed the platform and installed GHC from scratch, then installed cabal and cabal-install manually. It was very easy on ubuntu. 
Good point, did not think of that.
This isn't a translation of that post, but you may be interested in [my blog post](http://r6.ca/blog/20110520T220201Z.html).
This one isn't aimed at newbies, but it's doing something neat: http://www.cis.upenn.edu/~stevez/papers/abstracts.html#LZ07 There are definitely simpler examples, but I don't have the links handy.
Indeed, I'm pretty sure that [store is a comonad](http://hackage.haskell.org/packages/archive/comonads-fd/2.0.1/doc/html/Control-Comonad-Store-Lazy.html#g:2). :)
Can you elaborate how the type: sort :: Ord a1 =&gt; [a1] -&gt; [a] is more general (and correct?) than: sort :: Ord a =&gt; [a] -&gt; [a] given your definition of sort? The way I'm reading it I don't see how you could infer that it is anything other than the latter, so I must be missing something in my mental type-checker.
Is the [DBDirect](http://hackage.haskell.org/packages/archive/haskelldb/2.1.1/doc/html/Database-HaskellDB-DBDirect.html) command functional? It seems like a killer feature for people who want to build a Haskell app on top of an existing db.
Thanks I will take a look.
Ah...neat. Thank you.
Thank you. I will take a look.
whats wrong with "sticking with f#"? f# is a decent language that seems to actually address pain points of many haskell coders. as far as i can tell, its a fine language. certainly if you are writing windows gui code, any .net lang will be a solid choice ignore the "advice" below to write "back end" code in haskell and then stitch it to f# code with some protocol or queue. thats idiotic
Nothing's wrong with sticking with F#. But I was asking about learning Haskell, and overcoming some obstacles that I've had in the past trying to do so.
this is a terrible idea. by jamming haskell behind a protocol and queueing system, you neuter haskell's two meaningful wins: its type system and lazy evaluation. not to mention terrible performance. zeromq is not an ffi
It's certainly more general, since the latter type is just the first with the additional constraint `a1 ~ a`. That it's correct is the key point. Note that the purported `sort` function here returns an empty list in one case, and in the other, it concatenates two lists that are themselves the result of recursive calls to `sort`. In neither case is the type of the output list related to the type of the input list! In fact, this function just always returns the empty list. A *correct* sort function would have inserted `p` into the list, and it's *that* that would have added the constraint that the type of `p`, which an element of the input list, is the same as the type of the elements in the output list.
Ooh, that's an idea: make a tool that automatically hoogles for code that matches the types of your functions, and try to find a function among the results that matches what you actually do. Might be annoying when hacking on a library that is already indexed by hoogle though :)
I foresee this getting a bit annoying in conjunction with constraint synonyms, if it's on by default or with `-Wall`. In many cases, the whole point of a constraint synonym will be to bundle up a common set of assumptions behind an abstraction, and the fact that not all of those assumptions come into play in any given symbol won't matter.
Interesting... very subtle.
awful fonts, just awful
I currently use hlint after the code is somewhat tested&amp;complete -- so it wouldn't help me find bugs. If hlint suggests more than just style improvements, I guess people would start using it before testing their code.
If you want to make a simple Blackjack game, you might consider using the [Gloss library](http://hackage.haskell.org/package/gloss). This library has many functions for easily drawing images, text and vector graphics in a window. It even includes a [nifty function for making a game](http://hackage.haskell.org/packages/archive/gloss/1.5.0.2/doc/html/Graphics-Gloss-Interface-Game.html), which will allow you to handle the input events. Just look at the type signature for the function *gameInWindow* and work on implementing everything it needs. Be sure to check out [Gloss examples](http://hackage.haskell.org/package/gloss-examples) as well as the [Gloss home page](http://gloss.ouroborus.net/).
The reason why GUI programming is not ideal in Haskell is this: All the major GUI libraries are designed on OO principles, and the Haskell bindings for them are thin wrappers. These OO-style libraries force you into using callbacks, and this pushes you towards mutable variables and away from programming in a functional style. Mostly the problem is not so bad in practice, because Haskell is excellent for imperative programming, and the GUI doesn't account for most of the program. But it's not as good as programming functionally. A functional GUI would be a completely different - and much better - design, and I have seen designs like this. It's just that as far as I know, an industrial strength one doesn't exist. Wrapping a functional layer around an existing GUI library is a little tricky, because all that internal state confounds your efforts.
Hugs (http://cvs.haskell.org/Hugs/pages/downloading.htm) seems to provide those files. Try installing it and running your program with it.
sh*t - Someone is actually using HaskellDB. I was part of the student group that did an overhaul of HaskellDB back in 2k3, definatly did not even conisder that someone could still be using it, especially in a production environment. Really really cool surprise!
Translating code from a new language to one you know is a great way to learn the new language! But don't focus on the syntax! You can't win, as the old language often is not designed to do things in the way of the new language, so it will only be a frustrating experience. Make a translation that is clean and understandable. (Don't put code in strings, ever!) Focus on how stuff works. But see translating code as a learning experience only. Don't expect to end up with a usable way to write JavaScript.
 1) Web backends (with Erlang) 2) Compilers 3) Hardware communications (RS485 / GPIO), soft realtime/embedded :) 4) Misc. tools (also OCaml) 
awesome! My use case for this not satisfied by cabal-dev (to my knowledge) is building all the Yesod repos at head (Yesod is split into 4 repos and many packages). Someone tried to do this with cabal-dev (because of conflicts with regular cabal), but that means that some repos are going to have duplicate installs of all the dependencies, and he had to manually cabal-dev source the various repos - he gave up pretty quickly. With this tool, myself and contributors that don't want to muck up their current install can cd to where the Yesod repos are and compile inside a virtual env. Or for an app specific Yesod install clone the Yesod repos to where their app is and use a virthualenv for their app. Either due to the Reddit title, the silent nature of h in this word, or my stupidity, I thought this used the same 'virtualenv' name as python. Can we get a full hs in the name and perhaps shorten the exe name at the same time? How about hsenv or virtualhs? Why do I need to type:`source .virthualenv/bin/activate` why not `hsenv activate` Is there a command to tell me what my env is? I would display that in my shell prompt. 
Awesome, thank you. I've been wanting something like this for a long time. I wrote my own ugly hacks around cabal-dev so that I can get autocompletion etc. in vim, but this would finally deprecate it :)
thank you, kind sir!
&gt; awesome! thanks! &gt; the silent nature of h in this word I'm not a native speaker, but I pronounce it with a lisp. anyway, you're the second person to dislike the current name (and I thought it was funny), so if more people request it - I'll change it (I like the hsenv one). &gt; Why do I need to type:source .virthualenv/bin/activate why not hsenv activate because you are modifying your current shell process (or emacs process). thanks to this, it's trivial to use it in emacs/any other editor, otherwise you would have to start your editor from inside the virtual environment. if you really want something like 'hsenv activate' it's a trivial script (you can place it in .virthualenv/bin/script.sh): #!/bin/bash source .virthualenv/bin/activate exec bash &gt; Is there a command to tell me what my env is? I would display that in my shell prompt. there's already environment name displayed in your promp (at least it should be), e.g. "(yesod)user@machine:path". you can change the name during creation of the environment, if you don't like the default (current dir name), just use "virthualenv --name=foo". if you want the path, it is in the content of VIRTHUALENV env variable.
Seems very cool! My one worry is about the name: will I remember where the H goes? Was it virthualenv or virtuhalenv or virtualhenv? Also, I struggled to comment on your blog because the comment form and its error messages are in Polish (?) -- it seems all your posts are in English so perhaps you should switch to the English localization?
&gt; My one worry is about the name: will I remember where the H goes? Was it virthualenv or virtuhalenv or virtualhenv? ok, by popular demand, 0.3 will probably use hsenv name. &gt; Also, I struggled to comment on your blog because the comment form and its error messages are in Polish (?) -- it seems all your posts are in English so perhaps you should switch to the English localization? I've switched all settings I could find to english, sorry about that, didn't know it used polish settings. thanks for info. 
The feature that made it possible for me to sanely use Haskell on real projects was the ability to grab a package off of Hackage, untar it into my project directory, make the necessary changes, and then tell cabal-dev about this new modified package, at which point it uses it at least somewhat properly. Though I'm not 100% satisfied with its behavior as I continued to make various changes, I also couldn't quite characterize what it was doing anyhow, so it's hard to even complain. Is there a good solution for this problem with this system?
&gt; I've switched all settings I could find to english, sorry about that, didn't know it used polish settings. thanks for info. Cool, seems to be fixed.
cabal unpack; &lt;edit&gt;; cabal install should do what you want I hope.
I second this if you view this as a learning project. Gloss is an awesome environment for learning Haskell. If you're looking for something you can use outside of a learning project, though, you'll eventually need to move on from Gloss. For example, there is no clear way at all (barring unsafePerformIO hackery) to do any kind of I/O from a Gloss application, except for at startup. So it's wonderful for creating demos and playing around, but it doesn't scale to more real-world stuff. This isn't a mistake in Gloss: it's just part of the tradeoff, and part of why it's so nice for the simple things.
hsenv FTW! :) Really, having both .virtualenvs and .virthualenv, VIRTUALENV and VIRTHUALENV with a barely visible 'h' in the middle is a little too much fun.
I think that the worst part of Xtend is Java itself :)
I never liked Java, but this just looks worse.
Thanks for this Andy. The papers you and your colleagues at Kansas have been putting out recently (Kansas Lava and Chalkboard) have been very helpful to me for designing embedded DSLs. It will be nice to see the Kansas Lava code. 
Contravariant stuff...
Meh, its just a python-ized java with some type inference. I suspect its C++-auto style type inference over wholesale, haskell-like type inference though.
I believe that you can call Haskell from C (and presumably then from anything that has an FFI that can call C). It might get ugly quickly, but you could write your GUI in the language with which you're most familiar, and then have it talk to a purely functional core of Haskell.
Great! Also reminds me a little bit of ruby's rvm (http://beginrescueend.com/rvm/). A couple of questions/ideas: * How would one use this to deploy a project to something like a shared server? * Could we create per-directory .hsenv files to make the switch automatic when one navigates to the project folder? (This is how rvm works) * Could we have (one day, perhaps) a way to map from env names to (compiler version, installed cabal libraries) instead of just cabal libraries? This would allow things like: projectXenv -&gt; (ghc-7.0.2, snap-0.6) projectYenv -&gt; (ghc-6.12.3, snap-0.5) Thanks again for the contribution!!
This maybe http://hpaste.org/53691 They mention that they omitted monad comprehensions in updating ParseLib.hs from Gofer to Haskell; that would be interesting to see, now that they are back.
Its worth noting that rvm is fairly complicated and invasive: in response [https://github.com/sstephenson/rbenv](rbenv) was created. If you add on [rbenv-gemset](https://github.com/jamis/rbenv-gemset) you have the rvm/virtualenv functionality. I think I would be happy enough if I could put something in my shell to spot a .hsenv on shell load 
Type inference is coming to mean "we read the type of the right side, which is fully and trivially specified, and make it so you don't have to simply repeat it on the left side". I wish the programming community would come up with some other name for that, "type elision" or something, because inference means something much more powerful. I don't even think of it as a language "feature", it's just a syntax thing. Making you say a thing only once instead of twice is hardly such a breakthrough that it's worth a full screen of braggadocio. Edit: In particular, I'm just counting down the days until the first time I see something to the effect of "Haskell has type inference? Big deal, I've got that in C#." I give it about six months.
Maybe one day, but for now I think it's just too late. Type Inference means nothing more or less than you can omit types in LHS expressions if its completely obvious from the RHS. I like type elision though.
I thinks it it more like scala.
I did some testing on Mac OS X. I successfully installed it from github using `cabal install` and then ran `virthualenv` in the git clone and successfully built virthualenv (again using `cabal install`) inside my virtual environment.
I know. I looked at it and, for some reason, was immediately disappointed that Xtend was not Haskell? I work on quite a bit of Java and thought when reading about Xtend that the website would tell me that Java was over and I did not have to program in Java anymore. Xtend looks like strongly-typed cut-and-paste. 
Thanks sjoerd. These are words of wisdom. Sometimes I lose sight of the fact that this is a learning experiment over anything production ready. Lord knows the code I write while learning never leave my local machine!
The idea is to simply use `cabal install` while you are inside of the virtual environment. Do everything as normal, except use `cabal` instead of `cabal-dev`.
How does this compare to [hub](http://justhub.org/alpha/)?
+1 for hsenv.
Told you so!
All these new languages: scala, kotlin, xtend are "safely different". By safely i mean not scary to java developers. They should feel at home. That is also why i regard all of them as irrelevant. Nothing to see here, move on people. 
I disagree that it would neuter the type system. And I'm not sure that laziness is such a meaningful "win". And I wouldn't necessarily suggest ZeroMQ for this task. And a message passing protocol doesn't necessarily need to follow ffi/rpc models of communication.
Hey All, Recently wrote this as a generalization of a bunch of patterns I had written for internal projects at http://bu.mp . Hopefully it makes the whole "cutting edge" of network programming in Haskell more accessible to newer Haskell programmers. I'd appreciate any feedback you might have, thanks!
hub only works on Fedora and I think helps you get official distro packages of the Haskell platform. Different GHCs are already isolated by cabal. This is for *nix and lets you isolate cabal packages per-project. 
This looks great! If you really want people to get excited, post some benchmarks. the haddocks didn't build :( You could link to the source file from the github README, and in your cabal file you should link to the github source.
nice! if this performs well, i can see lots of uses for this. i normally run to perl's AnyEvent or Net::Server otherwise for generic servers
Great, looking forward to integration of logging, configuration, monitor.
I am not an expert on Haskell regex libraries, but did you try looking at pcre-light or regex-pcre from Hackage? If still no luck, stackoverflow might give you more answers. :)
&gt; How would one use this to deploy a project to something like a shared server? it's not a deployment tool - that's cabal's job. Once you have your code ready, just create a similar virtual environment on the production server and install all your cabal packages there. &gt; Could we create per-directory .hsenv files to make the switch automatic when one navigates to the project folder? (This is how rvm works) This would require a global cabal/ghc/ghci wrapper that would check for virtual environments in the current dir and activate them before calling real cabal/ghc/ghci. I'm really sceptical about installing new binaries that override haskell tools for every use. But it should be very easy to do so, so if more people request it, I might write a new package with such functionality. &gt; Could we have (one day, perhaps) a way to map from env names to (compiler version, installed cabal libraries) instead of just cabal libraries? I don't know if I understand this correctly but it's currently possible to have many virtual environments with different ghc versions, just use ghc from external tarball.
&gt; I think I would be happy enough if I could put something in my shell to spot a .hsenv on shell load it should be easy, just alias cd to run a function that checks for .virthualenv/ existance. and remember to disable it once you already activated an environment (to not switch those by merely walking dir tree)
thanks! could you also try using external ghc (e.g.http://www.haskell.org/ghc/dist/7.2.1/ghc-7.2.1-i386-apple-darwin.tar.bz2 ), I was informed that ghc for mac is also distributed as tarballs
this seems correct.
hub could be made to work with binaries from haskell.org, I suppose. anyway, I have no access to a machine where I could unpack the src rpms, and can therefore not check whether hub is a simple posix shell script. if you make it trivial and just a shell script, and in the process lose the dependency on ghc machinery, this could be a lot better and similar to the other languages' virt-env tools.
thanks for releasing this!
Currently, once you have compiled virthualenv binary, then only thing needed to use it is cabal-install binary. This could go away in a future version, by using cabal-install's bootstrap script. then you could use virthualenv on a system without any haskell tools
They call lambdas closures.
&gt;I wish the programming community would come up with some other name for that, "type elision" or something Wikipedia refers to this as **type reconstruction**.
See [stackoverflow](http://stackoverflow.com/questions/7882512/pcre-in-haskell-what-where-how/7883033#7883033) (a better place for straight q &amp; a).
Would personally suggest http-enumerator (you don't have to understand enumerators to use it!) and aeson.
Great idea for a first project! This will require you to come up with sensible data types for representing Reddit information, and of course understand IO/side effects to actually make it do something. I don't imagine there's a huge amount of pure functions for the Reddit data, so you'll probably be living a lot in an IO monad - if you come from an imperative background however you might find this quite comfortable. In terms of libraries, I like Aeson for working with JSON (though it might take you a bit to get your head around how parsing works), and http-enumerator is nice if you want a really simple way to fetch data, just don't be too scared about the fact that it's based on enumerators :) Might also be a good idea to teach yourself about general Haskell tool chain things. Maybe you can write some tests with HUnit, use HLint to check your code quality, Cabal to package it all up, hpc if you decide to play with code coverage, haddock to write your documentation. Don't forget to play with Hoogle and Hayoo! too, while you're finding which functions to call. Then you can go a bit further and try and make a `reddit` command for the CLI, which will let you learn a few more cool things - `recentPosts &gt;&gt;= mapM showPost` seems quite lovely to me :)
aeson is great. Never tried http-enumerator, but it's great that you don't have to understand them to use them as enumerators make my brain go cross-eyed.
That's exactly what I was looking for! Thank you. (I'm learning enumerators anyway.)
Can you post your Haskell implementation?
Oops, I meant to mention that in the text. Sadly, I accidentally deleted all my work by typing `rm *.hs` when I meant to type `rm *.hi`. But my first solution was just a straightforward list-based solution. Maybe I should try to recreate it anyway...
Ok, I've updated my post with a trivial list-based solution that takes an extremely long time to run. If I drop the upper bound from 999999 to just 9999 then it takes 1.5 seconds. Trying 99999 takes 48 seconds (indicating it's not a linear relationship). I haven't had the patience to run it on 999999.
Well, the problem with that implementation seems to be that it computes "collatz n" in full, all the way down to 1, for each item in the list. I imagine most imperative solutions would involve maintaining an array (or similar) of values you've already worked out, so you don't need to calculate "collatz n" again when you're trying to work out "collatz (2*n)". You can do something similar in Haskell, but note that lists aren't the ideal data type, as you'll need random access.
The brute-force C solution I linked uses no memoization whatsoever. It just computes the length of each sequence, from scratch. And I've already tried memoizing my `collatz` function by using [Data.MemoTrie](http://hackage.haskell.org/packages/archive/MemoTrie/0.4.5/doc/html/Data-MemoTrie.html), which helps a bit but it's still *far* too slow (and uses a ton of memory).
The experience of working through Iteratees and understanding them is very rewarding. I strongly recommend you try it. The easiest way is to write an iteratee. Just be careful to write the types statically as you go (typically iteratees are an exercise in where statements, make sure to add types so your error messages are more helpful!) That said, it's http-enumerator is great even if you don't get iteratees. :)
Well, I tried the solution you gave, and it runs in less than 7 seconds on the full problem size. Did you compile `ghc --make -O problem14.hs`? I think you are running the code interpreted, as ghci takes a bit less than 1.5 seconds for 9999 on my computer.
Data.MemoTrie looks like overkill for this task... I think my solution used Data.Array and/or Data.Map, and a little laziness trickery. I only used what came with GHC, anyway. Another possible cause of slowness -- are you compiling with optimisations ("ghc -O", or "ghc -O2" if you don't mind it potentially taking a while)? As far as I know, that's the only way people write haskell that competes with C for speed.
I also tried using Data.Array (but only memoizing up to 1 million), and that didn't seem to perform any better. I was not using `-O2`, and in fact the last time I tried to compile one of my prob14 attempts I got a stack overflow error (no idea why), but you're right, it makes a huge difference. Running this on the unmemozied version (but tweaked to calculate lengths instead of lists) with `-O2` takes 4.254 seconds. That's *significantly* better, but still an order of magnitude slower than the C version. 
Yes, I do feel a bit of an idiot now. I was foolishly expecting `runghc` to actually do compilation behind the scenes, instead of interpretation, and the one time I did try compiling I ended up with a stack overflow error (not really sure why, and it was on a different implementation than the one I gave here). But running `ghc -O2` on my solution (tweaked to calculate just lengths, not lists) takes 4.254s on this machine.
Mostly I'd say: fix up that Cabal file a bit! The unconstrained dependencies are never good. Enumerator will have possibly breaking interfaces in `0.5`, so your dependency there should look like `enumerator &gt;= 0.4 &amp;&amp; &lt; 5`. The `network` dependency should also be tweaked to specify `network &gt;= 2.3` since that was the version which included the functionality of `Network.ByteString`. Small things, but otherwise looks really cool!
I'm guessing that the C program doesn't actually build the list? Here's some Haskell code that doesn't: collatz :: Int -&gt; (Int, Int) collatz m = coll 1 m where coll r 1 = (m, r) coll r n = if even n then coll (r+1) (n `quot` 2) else coll (r+1) (3*n+1) main = do print $ fst $ maximumBy (compare `on` snd) [collatz n | n &lt;- [1..999999]] It takes 2.5s on my machine.
It looks interesting, but you would need to add some stuff before it would be useful for general purpose network servers. From the comments, it seems like you can't close the connection. Also, it writes to an unbounded TChan as far as I could determine, and would be vulnerable to a memory DoS.
You probably got the stack error because you didn't compile -O. Among other things, -O turns on the strictness analyzer, which is a practical necessity in Haskell.
Project Euler is great for making you question everything about computing.
Oh? I didn't realize that. I was also a bit suspicious of the following output: ld: warning: could not create compact unwind for _ffi_call_unix64: does not use RBP or RSP based frame ld: warning: PIE disabled. Absolute addressing (perhaps -mdynamic-no-pic) not allowed in code signed PIE, but used in ___gmpn_modexact_1c_odd from /Library/Frameworks/GHC.framework/Versions/7.0.3-x86_64/usr/lib/ghc-7.0.3/integer-gmp-0.2.0.3/libHSinteger-gmp-0.2.0.3.a(mode1o.o). To fix this warning, don't compile with -mdynamic-no-pic or link with -Wl,-no_pie but compiling the current solution gives that same output and actually works.
My tweaked version doesn't build the list either, and ends up taking 4.2s on my machine if I actually use the dam `-O2` flag instead of acting like an idiot and expecting `runghc` to have reasonable performance. I'm actually a bit surprised, yours takes 2.7s on my machine, even though it's not significantly different than my tweaked version. I modified mine slightly to have a clear tail-call in `collatz` and now it takes 3.8s, but that's still a full second more than yours. **Edit**: Down to 3.2s with an explicit `:: Int -&gt; Int` annotation on `collatz`.
This is an issue that showed up on newer releases of Mac OS X. It's harmless and will be fixed in the next GHC release.
Glad to hear it. Speaking of GHC releases, is the [Haskell Platform](http://hackage.haskell.org/platform/) still the appropriate way to get Haskell? I'm only asking because the current release was April 2011, and the page still claims the next release will be July 2011, but that's long since passed.
It's a very good habit to use a version control system (git, mercurial, darcs) even when doing small exercises like these. Not only is `rm *.hs` no longer a problem, but you can save your changes incrementally, and revisit old versions of your code without having to keep all of them hanging around in your working directory.
I usually do, but I was feeling lazy with these since it's such throwaway code. And it wouldn't have helped in this case anyway, since I wouldn't have committed it to the repo until I had a working solution.
Here is a solution that runs in 0.7 seconds on my machine when compiled with -O2. The idea is to not build the list but only count elements and to memoize the result of that using Data.MemoCombinators (the integral memoizer to be specific). import Data.MemoCombinators import Data.List import Data.Ord solve :: Int -&gt; Int solve n = snd . maximumBy (comparing fst) $ zip (map memc [1..n]) [1..n] where memc = integral f f 1 = 1 f n | even n = 1 + f (n `div` 2) | otherwise = 1 + f (3*n + 1) main = print (solve 999999) 
Yeah, that's not anything to get too worked up about. PIE is position-independent executable, which is more important for building shared objects. That way the OS can map different virtual addresses for the shared object into a single physical memory address, allowing one physical copy of the shared object to be shared across multiple processes. Strictly speaking you don't need position independent code to do that, but then the shared object has to be located at the same virtual address in each process, which complicates things. This differs from relocatable code. Relocatable code includes instructions to the loader about how to modify the code for any given base address. Basically, position independent code &gt; relocatable code &gt; position-dependent, non-relocatable code. 
That took 11.355s on my machine when compiled with `-O2`.
right, I need cabal-install for building virtualenv. if virthualenv starts building cabal-install for the sandboxes it will have to handle versioning incompatibilities and other issues for the GHC version in use for the sandbox. isn't this a problem you'd better avoid?
check this script: http://darcs.haskell.org/cabal-install/bootstrap.sh cabal-install can bootstrap itself using only ghc. afair that script is a little broken (doesn't work on all ghc versions), but if cabal devs fix it a bit it should be really easy. or I could write my own version (based on that script).
Yea, my bad, left out a 9, it is actually slower...
I have installet GHC through the Haskell platform, the way you usually install GHC, tools and libs on Mac OS X and I would rather not tamper with my install :)
&gt; What do you guys think? I think I have no idea what you're proposing, concretely. But if you've got a good idea, by all means implement it and brag about it here.
Keep your uncommitted work open in an editor, so if the file is deleted, you can just re-save it.
In the future, use version control so it's easy to recover all your files. In the future, use makefiles so you don't accidentally delete the wrong files.
You mean `enumerator &gt;= 0.4 &amp;&amp; &lt; 0.5`. If you set `&lt; 5`, that means something very different! &gt; The network dependency should also be tweaked to specify network &gt;= 2.3 since that was the version which included the functionality of Network.ByteString. Depending on how he's using sockets (I didn't look too closely), the `network-enumerator` package might be useful.
Done in 0.1.1, thanks.
Done, thanks for the feedback. Edit: (Not the benchmarks yet, though. Still tuning and tweaking the pipeline.)
If this was a real project, sure.
I actually did just that, but the work I had open in an editor was an attempt to use IORefs to implement a more imperative version that's closer to the C version. It didn't perform any better, and was very ugly, so I didn't want to keep that, though now I'm curious how it would have performed if I actually compiled the damn thing.
Yes, this is indeed really helpful! I've integrated it into 0.1.2 and removed a bit of socket-to-handle ugliness.
It is. I was a bit delayed this time around. We'll sort it out.
can't tell if sarcasm... wait, definitely sarcasm.
I suspect that version still allocates more than the C version as maximumBy probably doesn't fuse, being a left fold.
Are you using quot?
Is your time less valuable when working on a hobby?
Using div should give faster code, but I'm not sure ghc realizes that div with a power of two can be replaced by an arithmetic shift (quot can't for signed types).
There is [this](https://bitbucket.org/paradoxiology/hreddit/overview), although I know nothing about the Reddit API, so I don't know if it's relevant.
No, but if I don't expect to ever want to see this code again after it runs once, why bother using version control?
Glad to hear it. I was just afraid the project had grown stale, but if not then all's well.
You should always keep your code. There's no reason to ever lose anything now that storage is so cheap and version control is so easy. If nothing else, it gives you something to look back upon so you can be aware of how far you've come.
Just augment your idea of saving with version control. You don't have to put it on github, but so often I lose my undo stack and have to type everything again after I try and fail to make a significant change. Especially with a project where you are comparing speeds, a fork for each version you want to compare may seem excessive, but there is no reason not to do that.
I guess the author intent to use it in intranet, where DoS should not be a problem.
Concretely, I'm digging up the source for gcjni. I've got the source for haskell-jvm-bridge. I will attempt to make Haskell callable from within Java again. Once that happens the Java binders (is this what they're called?) should be no different from regular Java calls so using WindowBuilder shouldn't be any different. This will be a stressful, but fun, project for me. I'm still fuzzy on the deeper internals of programming, like linkers and FFI's, but I'm looking forward to the challenge. Don't hold your breath, it will be a long while before I produce anything usable. 
I am unable to distinguish between what you are describing and what we already have, in practice. The complexity in the bindings is a direct reflection of the complexity of the underlying libraries, and adding a CORBA layer is hardly different than anything else that is used. The problem Haskell has with GUIs isn't a lack of cross-language-ness. The endless and high-quality bindings for every combination of practical toolkit and imperative/OO language of significance proves that. Haskell's problem is the horrific impedance mismatch between the huge, huge morasses of imperative code that constitute these toolkits and Haskell's way of doing things, and the fact that replicating even a fraction of any of these toolkits rapidly gets into the thousands of manhours to do even a crappy half-assed job. And even if we stipulate those thousands of man-hours of labor being available, I don't think it's at all clear that anybody knows the best way to structure a pure-Haskell GUI solution. Proposals continue to stream in, and proofs of concept that work for a couple of wrapped text widgets and some basic layout may or may not scale to something the size of GTK as a whole. The fact that we have bindings in hand just goes to show that's not the actual problem. If that's where the problem was, nobody would be complaining, that's pretty much a solved problem.
I just treat all of Euler as one project; certainly a number of the problems are related, so code reuse cam be convenient.
Even with the binding tools currently available to Haskell programmers it's still an immense amount of work to make C bindings. To bind other languages is even more work and usually means using C as an intermediary. What if using a library written in another language was as simple as importing it? I'll grant that the result wouldn't be very haskelly but I think that many would like to be able to use libraries that are written in C++ and Java despite their imperativeness but never will because it's too much work to make bindings. So I guess I didn't make explicit that I wanted an automated solution that could create raw bindings for large libraries. As for creating something haskelly I think that one should create whatever API is nicely declarative and then compile that to sequenced code. The tool should help, though this part probably would require more involvement than the raw binding stage. I'm still vague on how this would work. So I guess I've taken on a larger task than I thought. To create an automated binding tool that can handle large libraries and then to show a nice, declarative GUI API and how to compile it to an imperative language.
This. -O has changed my life. Hell it should be automatic.
Here's a direct translation of the C program. I didn't spend any time making it look pretty: {-# LANGUAGE BangPatterns #-} module Main (main) where import Data.Bits main :: IO () main = putStrLn $ "longest: " ++ show longest ++ " (" ++ show terms ++ ")" where IP longest terms = euler14 data IntPair = IP {-# UNPACK #-} !Int {-# UNPACK #-} !Int euler14 :: IntPair euler14 = go 1 0 0 where go :: Int -&gt; Int -&gt; Int -&gt; IntPair go i !longest !terms | i &lt;= 1000000 = while i 1 longest terms | otherwise = IP longest terms where while :: Int -&gt; Int -&gt; Int -&gt; Int -&gt; IntPair while 1 !_ !longest !terms = go (i+1) longest terms while j thisTerms longest terms = let thisTerms' = thisTerms + 1 IP terms' longest' = if thisTerms &gt; terms then IP thisTerms i else IP terms longest j' = if j .&amp;. 1 == 0 then j `quot` 2 else 3 * j + 1 in while j' thisTerms' longest' terms' Writing this is a bit trickier than necessary. The main issue is that several low-level functions (such as `div` and `mod`) sacrifices performance in order to be "safe" (e.g. by introducing extra branches to check for out-of-bounds use). I disagree with this design decision. It makes Haskell look slow for no particularly good reason. $ time ./Euler14 longest: 837799 (524) real 0m0.444s user 0m0.442s sys 0m0.002 
Wow, that's very instructive. Thank you. **Edit:** Hrm. I just copied that into a file and ran `ghc -O2 prob14.tibbe.hs` and I got a *ton* of linker warnings that look like ld: warning: could not create compact unwind for .LFB3: non-standard register 5 being saved in prolog ld: warning: text reloc in _s11H_info to _Main_IP_con_info ld: warning: text reloc in _s11I_info to _s11H_info (overall there were 14842 warnings about text reloc) and when I ran the resulting program, it wasn't fast. In fact, I terminated it after a minute with no output. Interestingly, it sat at 1.0MB of RSIZE (while taking 100% CPU), which I've never seen a haskell program do before. Any idea what went wrong?
Having written some bindings using Haskell's FFI, I have to disagree what writing C bindings is 'still an immense amount of work'. This is mostly due to the fact that C is also largely driven by functions and that struct 'wrapping' is so easy thanks to the Storable type class. Usually, writing bindings for, say, Ruby or Python is much more work, since people expect bindings to provide an object-oriented interface. Wrapping a C++ interface, obviously, is harder. Given that binding C is fairly easy, and that there is at least one very decent GUI library written in plain C (Gtk+), binding a GUI toolkit is not the problem. As the grandparent says, the more interesting problem is solving the impedance mismatch. Current work (e.g. in FRP) attempts to find ways to reshape GUI programming in a functional manner, rather than treating it as something imperative.
Haskell requires today a lot of self confidence and drill. Official docs are not optimal and there are not many examples to learn from. What i found usefull was book Real World Haskell. But i decided that i will mostly focus on snap/yesod and web related stuff in haskell so i don't know much about going into GUI. I know one thing, going into gui stuff with functional language is hardcore :) 
I think I would prefer a message warning the user that the program may be slow.
This is against the universal rule.
See also [this recent reddit post](http://www.reddit.com/r/haskell/comments/m53w1/why_is_haskell_so_slow_on_project_euler_14/).
not only that, reddit normalizes the counts with fake downvotes thus it's meaningless
Is that the sort of thing that -fvia-c would pick up on?
Need I remind you, Mr. Marlow: [http://www.reddit.com/help/reddiquette](http://www.reddit.com/help/reddiquette)
No, because ghc has already decided it needs a function call to do the div. If the C compiler could see the callee it's conceivable that it could do it, but I doubt it. Also, -fvia-c no longer does anything in ghc 7.2.1.
Hmm, at first I would have said that sounds silly and advocated the change. But we take reasonable compilation speeds for granted. Simon feels that it would be a real mistake to make `-O1` the default. I think I'll vote for leaving things as they are for now. &gt; I think I would prefer a message warning the user that the program may be slow. I don't think that message would help much. Beginners probably wouldn't see it most of the time. Most often it would just look silly and add noise. The best would be if the GHC team were able to find some other setting, between `-O0` and `-O`, that would get more of the important optimizations without slowing down compilation too much.
The problem with conducting the poll this way is that if too many people downvote, the poll will disappear from /r/haskell. Better to post a comment for people to upvote/downvote, and possibly better (given the relatively small size of this community) to have people just post their opinion in a comment directly, and count them. So instead of downvoting, I'll say here that "beginners who aren't aware of -O's existence complain that their programs are slow" doesn't seem like a good reason to change the GHC defaults. People who are worried about performance will probably have to go through a lot more work than turning on a compiler flag. The downsides you mentioned seem significant enough to me to keep -O0 the default. There are also other concerns; for instance, just today someone in #haskell mentioned something about a bug having to do with CSE that only came up when using -O.
oops, I didn't know that! Sorry.
upvote this comment if you think GHC should use -O by default.
upvote this comment if you think GHC should continue to use -O0 as the default (no optimisation, fast compilation).
Why shouldn't the default be -O2 ? Is it only a, compilation time issue ? Or does turning on optimization introduce some other issues ? Previously my program was running incredibly slow and people suggested I use -O2. It went from runs infinitely to runs in under 5 Secs. If that is the difference that -O0 and -O2 makes, then irrespective of how much time it takes to compile shouldn't it be set to -O2 ? In my humble opinion a platform or language should not be chosen on how much time it takes to compile. Consider C++, on some files the compiler takes hours. This is without optimization. I think it is reasonable to expect people to wait for sometime for Haskell Platform to compile and cabal-installs to take a reasonable amount of time. But it is a one time process, is it that big of an issue ? Disclaimer: I am a newbie to Haskell and might not understand all the issues involved.
If you are building from cabal as part of your development process, then can't the optimization flags be changed locally to -O0 only for the app being developed ? Then it is set to -O2 by default for the rest of the world ?
Hmm, a shame this isn't on hackage.
Bad practice to reply to one's own post. But just in case any other n00b like me has the same doubt. I learnt that Haskell's -O2 is like a C compiler's -O3, it might not actually give a speed-up and sometimes it might even slow down the program. So Haskell C (C++) compiler -O2 -O3 -O1 -O2 -O -O1 -O0 -O0 Ok, that is good to know. Thanks. EDIT: Stupid formatting.
Alright, fair enough. I will still attempt to revive gcjni or haskell-jvm-bridge since I think people would like that. I will also attempt to contribute to one of the declarative API's, perhaps using a better interface for Gimp as my target. If I use Gtk as the target of the declarative API then Glade should be sufficient as a GUI designer.
I was also referring to that script. most issues with ghc compat are pure version string bounds. If I only have to fetch GHC and a virthualenv binary to bootstrap anything including cabal-install all in the sandbox, this will be great as a way to have simple sandboxes which are GHC version specific. How and where would you store the info which bootstrap versioning changes (cabal-install) you'd need? Check GHC version and fetch some meta-data from github with current info or embed it in the virthualenv binary?
&gt; If I only have to fetch GHC and a virthualenv binary to bootstrap anything including cabal-install all in the sandbox if it works, you'll only need virthualenv, ghc won't be needed at all. I think the right approach would be to download the most recent copy of cabal-install and bootstrap that. Then pay extra attention to deps in that script, so it builds on wide selection of ghc versions.
where's the 'i don't care, gimme a debugger' option?
&gt; There are also other concerns; for instance, just today someone in #haskell mentioned something about a bug having to do with CSE that only came up when using -O. The advantage of this is that bugs like this would be found more quickly if -O was the default.
&gt; Beginners probably wouldn't see it most of the time. Why do you think that? I am sure I would notice it. My compilations usually do not output anything else.
Only count upvotes, that is why there are two questions and not only one.
You can edit your comments to add stuff you forgot.
It's hidden away in the ghci debugger.
How does four years pass since the introduction of TV's? I could have swore that they happened sometime in the last year. Time quit disappearing on me.
Actually, with GHC, `-O` = `-O1`. &gt; -O Enable default optimisation (level 1) &gt; &gt; -On Set optimisation level n 
these are called catamorphisms.
I thought catamorphisms were more general than that - for instance, aren't `foldl` and `const` catamorphic?
I don't think foldl is a catamorphism (you cannot express foldr using foldl). I also cannot think of a datatype that would raise to catamorphism with const's type.
I think this is the same Mac OS X warning I commented on above. If so, it's harmless and fixed in newer GHC releases. It's entirely possible that my implementation contains a bug. I threw it together in 5 minutes do demonstrate what an apples-to-apples comparison looks like. It does run fine on my OS X 10.7.2 machine however: $ ghc -O2 Euler14.hs [1 of 1] Compiling Main ( Euler14.hs, Euler14.o ) Linking Euler14 ... $ time ./Euler14 longest: 837799 (524) real 0m0.444s user 0m0.442s sys 0m0.002s 
Too many people take it for granted that GHC (or any compiler, for that matter) will optimize the code by itself, without needing to tell it needs to optimize. To me, only enabling optimization by default makes sense. Developers know what they are doing (or they should), so they will explicitly disable optimization if they need to. Someone, please think of the users!
I'm currently working on a library called [reactive-banana][1], which is based on functional reactive programming and aims to make GUI programming declarative. The nice thing about reactive-banana is that you can easily hook it into any GUI framework, you only need a few lines of glue code. Here a few [examples using wxHaskell][2]. So, the only remaining thing to fix is the state of Haskell GUI libraries. Personally, I like wxHaskell because its API is very well designed. Any effort to improve its backend is well spent. [1]: http://www.haskell.org/haskellwiki/Reactive-banana [2]: http://www.haskell.org/haskellwiki/Reactive-banana/Examples
that would be in a proper poll, in which those questions are separate comments not in a "vote up if..." thread. notice here the comments were added later and the original title remains.
Informal poll is informal.
It would be nice if you can [list](http://haskell.org/cabal/users-guide/#source-repositories) the source repository in the cabal file, as in: cabal-version: &gt;=1.6 source-repository head Type: git Location: git://github.com/jamwt/haskell-scalable-server.git
&gt; A lot of popular data structures (Data.Map, Data.Set, etc) don't expose their constructors, which is why I suppose we don't see more of this. Note that most of these data types are instances of [Foldable](http://hackage.haskell.org/packages/archive/base/latest/doc/html/Data-Foldable.html) which is not as general as a dedicated fold but is sufficient for most use cases.
BTW, I recently added a nice new feature to `aeson`: generic default implementations of `toJSON` and `parseJSON`. Use it like this: {-# LANGUAGE DeriveGeneric #-} import Data.Aeson import GHC.Generics (Generic) import Data.ByteString.Lazy as BL import Data.Attoparsec.Lazy data D a = Nullary | Unary Int | Product String Char a | Record { testOne :: Int , testTwo :: Bool , testThree :: D a } deriving (Generic, Show) instance ToJSON a =&gt; ToJSON (D a) -- look: no need to define your own toJSON method! instance FromJSON a =&gt; FromJSON (D a) -- look: no need to define your own parseJSON method! Now if you have some `D`: someD :: D (D Int) someD = Record { testOne = 1 , testTwo = True , testThree = Product "Hello World!" 'x' (Unary 3) } you can encode it: &gt; let bs = encode someD &gt; BL.putStrLn bs {"Record":{"testOne":1,"testThree":{"Product":["Hello World!","x",{"Unary":3}]},"testTwo":true}} and parse a bytestring to a `D`: &gt; let Done _ v = parse json bs; Success d = fromJSON v in print (d :: D (D Int)) Record {testOne = 1, testTwo = True, testThree = Product "Hello World!" 'x' (Unary 3)} The performance of the generic `toJSON` and `parseJSON` is (almost) as fast as handwritten code (parsing big sums is still slower but we're working on that). 
how about petitioning the authors of various introductory texts to just mention -O? and "-prof -auto-all" and other useful flags.
Whatever these are (I'm unconvinced they're anything other than standard catamorphisms, but maybe it's down to your choice of examples), "cast" is definitely the wrong name.
Note for anyone using Reddit Enhancement Suite or similar: The up/downvote totals for comments are heavily fudged (but the total score isn't); you won't get any useful information out of them. Edit: Actually, I think the two post solution is a really bad idea, since it's easy to subvert with downvotes; it would be best to concentrate all voting on a single comment. (Perhaps it would be better just to set up a poll on one of the endless sites for those and submit it as a link...)
foldl is a catamorphism of sorts, but it's not natural. The (natural) right fold is the catamorphism for cons lists. The (natural) left fold would be the catamorphism for snoc lists (i.e., `data SnocList a = Nil | Snoc (SnocList a) a`). However, it turns out that cons lists and snoc lists are isomorphic (modulo bottoms), so we can define foldl on cons lists and foldr on snoc lists. But implicitly we're changing the representation from one variety of list to the other before doing the fold. This is why foldl on cons lists and foldr on snoc lists aren't natural, and hence aren't properly considered catamorphisms (though they're close enough that most people won't care). It is important to bear the "modulo bottoms" in mind here. This is the reason why foldl (on cons lists) is spine strict and does not work for infinite lists. Similarly, this is why we cannot define a foldr (on cons lists) that works for infinite lists by using only foldl (on cons lists). It is only when we restrict ourselves to finite length lists that there is truly an isomorphism between cons lists and snoc lists. If we allow infinite spines then the natural catamorphism for each variety of list will necessarily be more powerful than the fold which goes the other direction.
No, it shouldn't slow things down at runtime. It'll slow down compile time significantly though, possibly without giving much benefit for it.
And it's worth noting that catamorphisms aren't sufficiently powerful to capture all the sorts of recursion we'd like. Or in the OP's terminology, there are some ways of "casting" which require more power than catamorphisms can offer. In particular, we want at least paramorphisms. The difference between cata and para is that with catamorphisms on recursion sites we only get the result of recursion, whereas with paramorphisms we get both the result of recursion and the original value. So a paramorphism for lists would be `para_list :: (a -&gt; ([a],b) -&gt; b) -&gt; b -&gt; [a] -&gt; b`. To get an example of why we want paramorphisms, consider how you would implement the factorial function using `cata_nat :: a -&gt; (a-&gt;a) -&gt; Nat -&gt; a`. For another example, consider what it would take to make catamorphisms well-typed in dependently typed languages.
IMHO, the general design principle is that optimizations which can improve the time-complexity class of a program should be enabled by default. I'm not really sure if this is true of everything that is enabled by -O.
I just wanted to be supportive of GHC, so I upvoted both options.
Why oh why doesn't reddit have polls built in?
There he wasn't even compiling...
Do warp and snap support http pipelining? Pipelining is how apt-get makes downloading 50 small files nearly as fast as downloading 1 big one. Pipelining is neato. 
Cool. I did play around with reactive-banana a little bit but then the usual happened and I got distracted. I'm also interested in iteratee and TVs. I want to make something that has an interface complexity comparable to what you find in Eclipse, Blender, or Photoshop but done in a concise, declarative style. TVs sound very interesting because I think visual languages are good for GUIs, animations, and games. It would be nice to have something like a visual command line for managing windows and systems. I was trying to get phooey to work but I will look at reactive-banana too.
It seems to me to be a developer decision to use optimisation. I primarily use a netbook at home and compilation times are long enough without optimisation.
You appear to be talking about free structures, at least on some level
What's the name of the case expansion? list :: (a -&gt; [a] -&gt; b) -&gt; b -&gt; [a] -&gt; b list f = para_list (\a -&gt; f a . fst) For non-recursive types (Maybe, Either), this is the same as their fold, of course.
Might as well go with -O0 if -O2 isn't an option. O1 sounds like a half measure in my (ignorant) opinion.
I've thought about this to some depth. Essentially, this is Church encoding (http://en.wikipedia.org/wiki/Church_encoding). For example, you can turn any data structure into a function. There is a specific process, I've realized, that can do this. For example, I'll show you how to transform Maybe values into functions. Take the definition of Maybe: data Maybe a = Nothing | Just a First, for each case, Nothing and Just, we create parameters: \nothing -&gt; \just -&gt; ? Now, to encode the value "Nothing," we simply return the Nothing parameter: \nothing just -&gt; nothing To encode the value "Just 5," we return our "just" parameter applied to "5." \nothing just -&gt; just 5 This would be the "maybe" function, more or less. For example: maybe 0 (+ 1) (Just 5) = Just 5 0 (+ 1) = (\nothing just -&gt; just 5) 0 (+ 1) = 6 maybe 0 (+ 1) Nothing = Nothing 0 (+ 1) = (\nothing just -&gt; nothing) 0 (+ 1) = 0 Slightly more complex, take a list: data List a = Cons a (List a) | Nil And a value (Cons 1 (Cons 2 (Cons 3 Nil))). That would be: \cons nil -&gt; cons 1 (cons 2 (cons 3 nil)) Which is basically a fold, ex. a sum: [1, 2, 3] (+) 0 = 6 This is interesting with booleans: True = \true false -&gt; true False = \true false -&gt; false if True then 1 else 2 = True 1 2 = (\true false -&gt; true) 1 2 = 1 if False then 1 else 2 = False 1 2 = (\true false -&gt; false) 1 2 = 2 Tuples: (1, 2) = \f -&gt; f 1 2 uncurry (+) (1, 2) = (1, 2) (+) = (\f -&gt; f 1 2) (+) = 3 Natural numbers: 3 = \succ zero -&gt; succ (succ (succ zero)) fpow (* 2) 3 1 = 3 (* 2) 1 = (\zero succ -&gt; succ (succ (succ zero))) (* 2) 1 = 8 It would be interesting if this was built into the language. We wouldn't need to write a lot of catamorphisms ourselves.
It can (in theory, at least) be *more* efficient than C, but it won't if you are coding it wrong. How? Well, it's easier to optimize because it's more pure. I'm sorry to confirm stereotypes, I don't know much about the practical things about the performance, so I can't say if it *is* as fast.
This doesn't mean you will no longer be able to turn off optimization, it only means whether optimization will be on by default.
I'm not sure what you mean by Haskell not being as efficient as C. Could you be a bit more specific, in which areas you perceive Haskell being less efficient?
I haven't gotten around to it yet.
but if you keep optimizing c, how will haskell catch up?!
Can it really be as (or more) efficient then C? Lazy evaluation is a non-insignificant overhead. Strictness analysis can alleviate some of the costs, but it can't detect every case and needs to be conservative when it doubt in order to not affect program semantics. 
I don't know the areas, I'm looking for them. I found [this](http://www.haskell.org/haskellwiki/Introduction#When_C_is_better) to partly answer my question. So lack of memory control can sometimes be a source of inefficiency. Edit: formatting
You're a narcissistic egomaniac, you know that right?
There's an interesting question lurking here. But first, we should be clear that ghc-compiled Haskell is roughly as efficient as C when using the exact same algorithms on the exact same datastructures. The issue is that even when we think we're using the same algorithms, we're usually not using the same datastructures, due to pervasive boxing as required for laziness. Due to the additional layer of indirection, the typical target for well-written, idiomatic Haskell code tends to be around 2 times C. The real question is to what degree we can offset laziness and the attendent costs of indirection while remaining idiomatic. Some of that work involves improvements to compilers -- improving strictness analysis, etc. Other parts of that work involve developing approaches that reveal potential optimizations to the compiler while keeping interfaces idiomatic -- this is actually a give-and-take between compiler flags and library writers. Still another part involves adding more primitives to libraries that expose things like efficient matrix operations with a pure interface, etc... 
In other words, the problem is laziness?
"easier to optimize" somewhat makes the assumption that you as a developer has done an inefficient algorithm that can be optimized by the compiler. But isn't it that given time, competence (and a large dev team) you are probably smarter than the optimizer and can produce a better algorithm? I don't know the nature of haskell optimizations yet though... Edit: Well except for laziness etc. If that counts as optimizations (in this context).
Lazy evaluation can also make it faster (by ignoring the code that isn't needed), and it allows for another set of optimizations. As I said, I'm sorry to confirm stereotypes, but I don't know a lot about the practical side.
The IO monad allows you to be as low-level as C, if I understand it correctly.
Law of diminishing returns?
Happstack does.
Folds coincide iff (f, e) is a monoid homomorphism on lists, IIRC.
On the single-core processors we (still) have today, Haskell could, in theory, be faster than C, because it gives the compiler more information to optimize with. Whether laziness can be optimized out in general, I don't know, but Haskell could potentially offset that in other areas. It will take a while yet for this potential to be fully realized, however. The meaning of "more efficient" changes in a world where multi-core and novel CPU architectures (GPUs, FPGAs, etc) are the norm. C suffers from being tied to one type of CPU architecture. Will novel architectures become the important factor before Haskell approaches C on single-core?
I C what you did there.
Let's first assume that the team focuses on writing a fast program. Well then they can pretty much use the compiler as an assembler and therefor win by a [strategy stealing argument](http://en.wikipedia.org/wiki/Strategy-stealing_argument). Now, lets assume the team has to compromise between performance, safety, extensibility and/or other factors. I would then think that Haskell wins, just because performance can be achieved along with powerful abstractions. 
Given a reasonable idea and two otherwise equivalent groups of artists using spray paint and charcoal respectively, which team will probably produce the most expensive work of art? Why?
The problem with using laziness as a way to speed up programs is that when you write things to be fast in C, you don't write the bits you don't need. Laziness allows us as programmers to write clearer and more elegant code, and hope that it performs well. With C, you don't get that choice, so you never write in that way. Personally I feel that using laziness as a way to speed up programs isn't a great idea, and we shouldn't be saying things like "Lazy evaluation can also make it faster (by ignoring the code that isn't needed)". By all means go on about how laziness improves *programmer* efficiency, but not program efficiency.
Lazy evaluation only adds overhead to things that are lazy. Strict arguments and fields don't have any such overhead.
The issue is one of expressiveness. For example, earlier versions of C didn't let you express that two pointers don't alias and wouldn't generate optimal assembly code in some cases because of this. So *in theory* the program text that contains the most information should be the best for the compiler.
Well yes, if we want to use the same operator for both folds and still get the same answer then the operator must be associative (in general). It's really helpful if `e` is an identity element for the operator, though strictly speaking it only has to be a commuting element (so that we can push it from one end of the list to the other). However, even given a non-associative operator, we can do the work of a left fold with foldr and the work of a right fold with foldl--- provided that the list has finite length. That's what I meant by isomorphism: that they have the same formal power and work on isomorphic structures/types/objects. They need not coincide in denotation for that to be the case, we merely need to be able to convert the operator and element used in the one fold into an operator and element for the other fold.
I'm not sure if those have a special name; it's just the case analysis made into a first-class function instead of being restricted to special syntax. Logically they're the (non-recursive) elimination rule for each type.
&gt; On the single-core processors we (still) have today, Haskell could, in theory, be faster than C, because it gives the compiler more information to optimize with. What do single-core processors have to with giving the compiler "more information to optimize with"? 
No, the IO monad has nothing to do with being "as low level as C," it's about isolating stateful code from pure code.
There's also the issue of Garbage Collection. Typical C style involves a lot of manual malloc/free calls, and GC can compete with that. But C code can be written with a nearly-0 dynamic allocation style, where allocations are passed as arguments. I think the latter style is very hard to compete with in a GC setting.
Haskell is a GC'd language, so not quite.
The problem is that compatibility with the C model is a litmus test for any "novel CPU architectures".
I don't really notice compilation times.. I think by the time compilation times begin to matter, you've put enough effort into the project that remembering to specify the optimizations manually is a no-brainer.
I've posted this question on Stack Overflow: [Can compiler optimizations, like ghc -O2, change the order (time or storage) of a program?](http://stackoverflow.com/questions/7635805/can-compiler-optimizations-like-ghc-o2-change-the-order-time-or-storage-of). As a Haskell developer, what should I expect from compiler optimizations? It's already hard to predict the actual order of a Haskell program, now we have to think what will happen when going from -O0 to -O2. How do they deal with that in C/C++? That said, I don't really have an opinion on the default value.
upvote this comment if you think Simon Marlow should spend the following 2 years making GHC -O2 compilation as fast as -O0 is today
there's also the stack, which requires no allocation/free; that's imho one of the things that makes c so hard to beat.
I guess Cabal doesn't set -O? Anyway this discussion is making me wonder if we should use -O1 or -O2 for binary packages in Fedora perhaps? Do any of the other distros do that now?
"we should be clear that ghc-compiled Haskell is roughly as efficient as C when using the exact same algorithms on the exact same data structures." have there been any tests done to verify this assertion? most examples I've seen are quite a bit slower (except for concurrency), though they may have been idiomatic.
Be serious man, there ain't so many of us browsing this to even think that there'd be users who don't visit the comments page as well. And anyways, this voting is just as a broad overview of the developers opinion, and not an important factor in the decision.
There should be a small and fast set of optimizations that are most responsible for speed gains. If those exist, a -O0.5 would be useful as default.
-O3 or go home
For the 2x number, the benchmarks game (or shootout, as it is sometimes called) bears this out. For the matching performance w/r/t the exact same structures/algorithms, you can take a look at some of the work done on vector and vector-algorithms, etc, and various questions on SO w/r/t using them efficiently. edit: here's an example on SO of a straightforward Haskell program just about matching (and beating on at least one machine) a straightforward C program: http://stackoverflow.com/questions/6964392/speed-comparison-with-project-euler-c-vs-python-vs-erlang-vs-haskell/6964760#6964760
Granted, GHC's allocations resemble stack "allocations" (`alloca`) more than they resemble `malloc`.
To be fair, the IO monad is about isolating impure code from pure code, and virtually everything low-level that C code does involves impurity to one degree or another, since it typically relies on mutation and pointers and etc. So if you want access to the tools to do these sorts of things that involve mutating definite locations in memory and controlling allocation and etc. then you will necessarily be in IO.
Use something like the Pilot DSL, which compiles to C and gives you very low-level control and you can probably beat C. ;-)
How often do you get a situation where the number of evaluation steps is actually less than with strict evaluation, and where the overhead involved in building and accessing thunks isn't greater than any extra overhead in evaluating unnecessary steps with strict evaluation? Not often. The main advantage of laziness is that it defers evaluation, and allows us to do cool stuff with infinite lists and the likes. It doesn't often offer a speed optimization by avoiding unnecessary computation, although intuitively you might think so.
How about const :: a -&gt; () -&gt; a, that sounds like a catamorphism for ()., albeit a pretty degenerate one at that.
I suspect the argument was something along the lines of "*even* on the single-core processors haskell could beat C eventually, but on upcoming parallel architectures, it's gonna win much earlier"
GHC will allocate on the stack if it can prove that the pointer won't escape. Unfortunately this is a purely intraprocedural escape analysis, so there is quite a lot it will unnecessarily heap allocate. Even when GHC allocates on the stack, it is not as efficient as C alloca because GHC checks for stack overflow.
you can encode every paramorphism using catamorphisms, using the trick from subtraction on church numerals. I've coded some of this and other morphisms in pure lambda calculus here: https://github.com/Paczesiowa/refunctionalization/blob/master/refunctionalization.scm
&gt; The problem with using optimization as a way to speed up programs is that when you write things to be fast in assembly, you don't write the bits you don't need. Optimization allows us as programmers to write clearer and more elegant code, and hope that it performs well. With assembly, you don't get that choice, so you never write in that way. &gt; Personally I feel that using optimization as a way to speed up programs isn't a great idea, and we shouldn't be saying things like "Optimization can also make it faster (by ignoring the code that isn't needed)". By all means go on about how optimization improves programmer efficiency, but not program efficiency. The problem with this argument is that we have good enough optimizers that C code regularly outperforms assembly code. Even though we *could* write assembly that is as good as or better than the C code, we don't. Haskell's optimizer isn't good enough to optimize beyond C *now*, but I see no reason why it couldn't be improved to do so in the future.
Cabal does use -O, and that makes a lot of sense: the most common use case for cabal is to build some software once (a library or application) and then use it many times. When using Cabal for development, the recommended thing to do is set -O0. 
more (working) examples: https://github.com/Paczesiowa/refunctionalization/blob/master/refunctionalization.scm
See you const. Raise you magic :: Void -&gt; a.
Usually, general-purpose programming languages aren't efficient or inefficient, their implementations are. There are exceptions, but Haskell isn't one of them. The main issue with Haskell is that if you take performance-critical C code, directly transliterate it into Haskell (using the IO monad, Storable, etc.) and then compile the Haskell program with GHC it will often be slightly slower than the compiled C program. Laziness doesn't come into play at all here. This is clearly a compiler problem - there is no reason that a Haskell compiler shouldn't be able to perform low-level optimisations as well as a C compiler. This is being worked on but perhaps not as actively as some would like. As to your clarification, I would bet on the Haskell team. After all, they can play tricks like [this](http://www.cse.unsw.edu.au/~chak/papers/KCCSB08.html) or just quickly implement a DSL for their particular problem domain. Also, parallelising thing is much much easier in Haskell than it is in C.
One could argue that anything to do with parallelism and concurrency isn't really compatible with the C model. Or, more generally, with imperative programming.
As I said, I don't know how much it affects the speed, but it could, in theory, increase the speed of some programs.
And the unsafe code from the safe code.
I didn't imply I wouldn't be able to turn it off. The thing is that I don't want to turn it off for all packages installed with cabal that don't need them. I prefer that the wise developer uploading it to Hackage turn it on if necessary. I think that when you turn it on and try to upload it to Hackage there is a warning that it might be slow for some people, that warning makes sense to me. Anyway I understand perfectly why turn it on by default has some appeal, but it looks like premature optimization, like saying to the compiler "please waste time trying to optimize the program I didn't even try to write to be fast, didn't benchmarked, profiled or perfectly know I don't care because I will run it once in a while on a limited data set."
I find it interesting that you mention boxing but not immutability. I suppose the pervasive use of persistent data structures does have an effect on performance.
Google searches for Pilot DSL and Haskell Pilot DSL returned nothing. Can you be a little more specific ? EDIT: Nevermind I think you meant Copilot DSL. Found it.
Couldn't you pair up the result and the argument, and split off the result at the end, like this: fac = snd . cata_nat (1,1) (\(x,n) -&gt; (x+1, n*x)) Or is that cheating? (Or even the whole idea of a paramorphism?)
This is part of a general reliance on partiality by the standard library: head, tail, init, last, (!!), div, read, the Get monad and many more... We have very nice tools to work with Maybe values (their standard class instances) so I don't think it would be that horrible to have all of those functions return Maybe-wrapped values. When you can prove the value is a Just from those functions, you should be able to justify yourself via: unjust :: String -&gt; Maybe a -&gt; a Where the String is an argument that should explain the proof of why the value can't be Nothing.
Why not unsafeHead, unsafeTail, ...
unjust should alse accept PersonalInfo and Address params, just in case person who will use your code is a maniac serial killer.
i believe there have been projects to develop replacement preludes, i wonder what it would take for one to get traction.
My argument was mainly about laziness, not optimisations in general.
Does it really matter? As long as it efficient enough for the problem you are trying to solve?
Aren't most replacement preludes quite specialised? There's either preludes containing masses of type classes for algebraic structures, or preludes where everything is a type class, etc. I'm not aware of any serious effort at an alternative, usable, general purpose prelude. Is there one? It would be interesting to design a new prelude from the ground up.
But laziness is just another optimization. Every piece of pure, strict code can be interpreted lazily and do the same or less amount of work. In fact, C optimizers already do lazy optimizations; if I have `x=*y+3; x=5` the (properly optimized) generated code will never call dereference y or add 3, since it's value is never needed.
I filed immutable vs mutable under "different algorithms" and didn't deal with it for that reason. 
right, virthualenv can get ghc as cabal and virthualenv will be statically linked. there is no "most recent copy of cabal-install" which is compatible with "most recent copy of ghc", sadly. it always requires changing cabal-install.cabal and bootstrap.sh. I"m using the modular-solver branch of cabal and it greatly improves cabal. It is able to see that you will end up with a dep problem and doesn't even try to build which prevents issues you might only see after building 90%. we might want to support installing the modular-solver branch via virthualenv (optional).
That's a fair point. I think I read something into tehz' comment that wasn't there.
Ah, thanks, that makes sense. Not sure I agree with the statement, but I'm happy to understand it now. :)
This can probably be changed in future Haskell standards, but someone would need to [drive the process](http://hackage.haskell.org/trac/haskell-prime/wiki/WikiStart#GetInvolved).
&gt; there is no "most recent copy of cabal-install" which is compatible with "most recent copy of ghc", sadly. it always requires changing cabal-install.cabal and bootstrap.sh. that's why I think there should be one. after releasing new ghc versions, cabal-install should be bumped to work with it.
this is the kind of crap that haskell prime should have addressed. unfortunately it seems like most of what haskell prime accomplished was to codify extensions that ghc already supports. 
i saw this with full knowledge that as someone who uses haskell extensively but has never committed to ghc or the haskell platform I'm kinda part of the problem but still, i stand by my flamebait
When I want to know more about the result of the parse, I use reads So the functionality you want is available, read is just a convenience function, from my point of view.
Haskell prime is a rolling process that in the main involves the core language. There's a rolling libraries process that's been started to be developed along with it. And part of that involves plans for a libraries-centric "big bang".
How is that so? You can do plenty of unsafe stuff and just wrap it in `unsafePerformIO`.
You shouldn't use unsafePerformIO unless you are sure that it's safe.
They are both sort of correct. Julienne is wrong that RAND_MAX % N == 0 is the condition that must be met in order for rand() % N to not introduce some error in the uniform distribution of the resulting set of integers. But it is correct that the method does introduce a small systematic error for some values of RAND_MAX and N. It's possible that such an error would cause a sequence to fail some of the statistical tests that are normally employed, but for most practical values of RAND_MAX and N, it probably will not. Consider a contrived scenario, when RAND_MAX == 3 and we want an integer between [0,2]. We will have the following mappings, using rand() % 3 [ 0 =&gt; 0, 1 =&gt; 1, 2 =&gt; 2, 3 =&gt; 0 ] and so zero will be twice as likely as one and two. When discussing a similar method (using multiplication to convert an a uniform number between [0, 1) into an integer within [0, k)), Knuth writes (p. 120 of Vol. 2): &gt; This method gives each integer with nearly equal probability. There is a slight error because the computer word size is finite (see exercise 2); but the error is quite negligible if *k* is small, for example if *k*/*m* &lt; 1/10000. When boost::random wants to generate a uniform integer within range [0, n] from an underlying generator with range [0, m] with n &lt; m, it does (essentially): int bucket = (m + 1) / (n + 1) for (;;) { cand = rand() / bucket; if (cand &lt;= n) return cand; } In our above example, bucket will be 1 and this will call rand() until it gets a value 2 or less. The general effect of dividing by (m + 1) / (n + 1) and retrying if the result is greater than n is that we have split up underlying sequence into n+1 equal sized buckets of bucket, and there are (possibly) some candidates at the high end that we will throw away when we encounter them. So for example, imagine m == 255, n == 9. Bucket will be 25 and we will throw away any underlying generated number that is &gt;= 250. All the values [0, 24] map to 0, [25, 49] map to 1, [50, 74] map to 2, etc.
What about `div`?
Fair enough. I seem to recall, however, that there are settings in which that approach isn't possible. I'd have to go and figure out where those are though.
That's the whole idea. Paramorphisms arise from the fact that we can do two catamorphisms at once and pair their results homomorphically: one which is just the identity (passing the constructors in), and the second which "does the work". The trick being, of course, that with paramorphisms we can achieve this more efficiently since we don't have to spend the time rebuilding the structure we're taking apart. Also, the situation is a bit more severe in the dependently typed setting since we'd want to prove that the structure we're building up is in fact identical to the one being taken apart. To do that we'd have to (a) do yet another fold to prove the equality, or (b) build up the equality alongside the copy of the structure and the seed value for the recursion. So again, efficiency from fusing things.
Because they're perfectly safe. They fail when they're given certain arguments, but in Haskell, unsafe functions can actually break Haskell if used improperly. Using `head` improperly will screw up your program if you don't catch the error, but the Haskell runtime will be fine.
What's wrong with `div`?
I thought there was a least one or two such projects with a more pragmatic approach but I don't really know. Hoping someone more knowledgeable will jump in here. Maybe blog posts like this pointing out specific issues with the default prelude are just what is needed to get things going..
That's dead code elimination, not laziness.
If your code has a bug, someone will hunt you down and kill you? That seems rather... unjust.
With cabal at development time use: cabal configure -O0 There's no need to add conditional stuff to the .cabal file.
I haven't looked at your code yet, but are you compiling your haskell using -O or -O2?
 Prelude&gt; div 1 0 *** Exception: divide by zero
Dragons be there!
Stack overflow always strikes me as a much better forum for this type of discussion (not that I'm advising you to move now, just a suggestion for the future).
Right, sorry!
That's not a problem with div, it's a problem with math, although it does technically make `div` a partial function. I'd argue that the proper behavior *is* to do that and not to make `div` return a `Maybe Int`, though.
I'm using -O2
I agree. You're not the one who said "I'd prefer Prelude had no partial functions" :)
I wish there were a way to import a module that clashes with the prelude without having to add extra noise to every single file. The noise can be gotten down to just "import SafePrelude" if you build with -XNoImplicitPrelude, but it still seems crufty to need to always remember that (including when using ghci).
How are you determining the Python implementation is faster? I just did a quick benchmark here. For E=500k E=1M, Python takes 13s while Haskell 12s. With N=1M E=2M, both Python and Haskell take 28s.
Well, obviously, returning Maybe is too limited. We really want something more like: read :: (Read a, Monad m) =&gt; String -&gt; m a read str = case reads str of [(a,"")] -&gt; return a _ -&gt; fail $ "Failed to read " ++ str So that we can have: read "1" :: Maybe Int read "1" :: Either String Int But then we remember that 'fail' is evil and doesn't belong in Monad. What we really need is MonadFail, http://www.haskell.org/haskellwiki/Functor_hierarchy_proposal Though, that does not really stop us from using fail now. Except that it is evil. But maybe less evil than calling errror. Would be nice if the above function was added to Text.Read as readM. Not sure if anyone has proposed it yet. I have seen it defined multiple times though.. Also. In theory, there could be some types for which read never fails. So, we could still have a non-failing version of, read :: (AlwaysRead a) =&gt; String -&gt; a. Which only contains types which can never fail to be read. But, in practice, I don't think there are many useful types like that.
Try something like git ([A good intro](http://www-cs-students.stanford.edu/~blynn/gitmagic/)), you can very easily and quickly keep *local* commits. That way you can commit any time you have some bit of code to run, not just every time you have a working solution. 
&gt; When you can prove the value is a Just from those functions, you should be able to justify yourself via: &gt; unjust :: String -&gt; Maybe a -&gt; a &gt; Where the String is an argument that should explain the proof of why the value can't be Nothing. Might I interest you in some Agda, sir? IsJust : ∀ {A} → Maybe A → Set IsJust nothing = ⊥ IsJust (just _) = ⊤ fromJust : ∀ {A} → (ma : Maybe A) → IsJust ma → A fromJust nothing () fromJust (just x) tt = x
I love that I'm being evangelized on a version-control system that I've used for years (and even submitted patches to). :)
Similarly here: $ ghc --make -O2 randomgraph.hs -o randomgraph $ time ./randomgraph 500000 1000000 aa.txt real 0m8.094s user 0m7.681s sys 0m0.172s $ time python randomgraph.py 500000 1000000 aa.txt real 0m9.652s user 0m9.172s sys 0m0.245s
EDIT: and I see these two programs get different outputs. Not a good start for comparison. Rather bunk. For 500k x 1M: * Original Haskell (GHC 7.2.1, x86-64): 6.7 seconds * Original Python: 7.5 seconds * Haskell using HashMap (from unordered-containers): 5.6 seconds * Using one of the HashSet's or mersenne instead of StdGen didn't change performance. I suspect that at this point most of your time is going to your string-driven IO. Move to Text or Bytestrings instead of building strings and writing those out. This will be an immense time saver.
I did some very basic optimizations here: http://hpaste.org/53878 Improves performance by about 17%: $ time ./Original 500000 1000000 out real 0m4.375s user 0m4.310s sys 0m0.070s $ time ./Improved 500000 1000000 out real 0m3.636s user 0m3.580s sys 0m0.050s The main issue is that System.Random is dog slow. I recommend using something like the mwc-random package (which requires that you change generateGraph to run in ST). 
`safeDiv :: Integral a =&gt; a -&gt; PosNat -&gt; a` class Integral a where toPos :: a -&gt; Maybe PosNat
The `Network.CGI.Protocol` module has a [maybeRead](http://hackage.haskell.org/packages/archive/cgi/latest/doc/html/Network-CGI-Protocol.html#v:maybeRead) function.
++ for Nat/PosNat in the standard libraries. Still, that's a rather major undertaking and restructuring of the numeric typeclass hierarchy (which I completely support, mind you!) for a very innocuous partiality.
&gt;including when using ghci http://www.haskell.org/ghc/docs/7.2.1/html/users_guide/ghci-dot-files.html
Because those don't encourage the author to justify his reliance on partiality. Partiality is a bad thing, and sometimes it is needed. When it is, it needs to be explicitly justified.
Some functions used to work that way. The consensus these days, though, is that you should return the `Maybe`/`Either` in that case, and not the polymorphic `(forall m. MonadFail m =&gt; m a)`. 
The type inference algorithm has exponential run-time in the worst case. Apparently, you have hit a worst case. Remove a few `id` and behold the exponential slowdown. I believe that type inference is PSPACE-complete or something like that; I don't remember the right complexity class. EDIT: Oops! As [taejo][1] points out, it's not the type inference algorithm because Prelude&gt; :type id id id id id id id id id id id id id id id id id id id id id id id () works just fine. [1]: http://www.reddit.com/r/haskell/comments/m7uph/whats_going_on_with_id_id_id_id_0/c2yt513 
But it's `id`, how does it manage to be the worst case? Is it because it's unsure what kind of `Num` `0` is, precisely? But still, it's a one-argument function, what gives?
I'm kinda a Haskell newb, so feel free to contradict me. Keep in mind what you're doing btw: Function application is left associative. So: id id id 0 is the same as: (((id id) id) id) 0 If I put the parenthesis this way, I noticed the exact same problem you have. It goes slow as hell once I get to about 20 id's. If however, you do something more sane, such as: (id . id . id) 0 or id (id (id 0)) It goes fast and the problem disappears even with 20 id's.
If you must... appEndo (mconcat $ map Endo $ replicate 25 id) 0
About half of the time is spent printing: COST CENTRE MODULE %time %alloc printGraph Main 51.2 52.9 generateGraph Main 48.8 47.1 
Nothing to do with Num: the same happens with `()`. As krappie points out, it's not `id (id (id 0))` but `((id id) id) 0`. It's not necessary to apply it to a final (non-function) argument: `id id id ...` does the same (ghci won't like the type of this, use `id id id ... id \`seq\` ()`). But it doesn't seem to be type-checker either: asking ghci for the types of these expressions is fast.
I just tried compiling main = putStrLn $ show $ id id id id id id id id id id id id id id id id id id id id id id id 0 and noticed a big memory spike during compilation but the binary runs fast. But with -O0 compilation is fast and doesn't cause a memory spike. So whatever it is, it's an optimisation.
As far as I know, GHCi doesn't do any optimization. BTW, in case you didn't know about it, `print = putStrLn . show`
Yes, removing 0 results in it inferring the type immediately and complaining that `a0 -&gt; a0` doesn't have an appropriate instance of `Show`. So what the hell could possibly be going on here? How could it achieve an exponential explosion when applying a linearly nested single-argument function? Btw, trying to do `let idd = id id ... id` in ghci also barfs.
"ghc --make" uses -O0 (oh-zero) by default. -O is actually short for -O1.
why is there a copy of cabal (the lib) in ghc, but not cabal-install? does anybody now why it was unbundled (IIRC)?
That would be `id (id (id (...)))`, not what OP wrote.
Alternative: Alias "mv ~/.Trash" so that "rm" doesn't actually delete files.
Ewww.
 id (id (id (id (id 0)))) is not more sane than: ((((id id) id) id) id) 0 Both are equally insane to actually write, and demonstrate different things.
Asynchronous exceptions are not “unsafe.” Partial functions simply lead to bugs which can be hard to track down, they don’t compromise the integrity of the run-time system. I would be against watering down “unsafe.” It should be reserved for the truly scary functions.
?
Would upvote twice. Haskell is designed for robustness and type safety; it's not only stupid, it's antithetical for Prelude.read to raise an error instead of returning Nothing.
As `godofpumpkins` said, GHC allocations more resemble `alloca` style allocations than `malloc` style ones. GHC just bumps a pointer and returns the value it pointed to, after checking for overflow. There's no complex algorithm to carve up a heap and give you a chunk. That makes allocation ridiculously fast in the common case, basically as fast as a stack allocation, and it's one example of why the "GC vs no-GC" argument is not quite so simple.
Hrm, my home computer (that I got the error on) is running ghc 7.0.4, which I think I installed via [homebrew](http://mxcl.github.com/homebrew/). On my other machine, which used the package installer for the Haskell Platform, your solution runs beautifully. Guess I need to blow away my haskell installation on my home machine and re-install the platform.
The FFI case he cites isn't very interesting in the Haskell case at least, since with `Foreign.C` and the FFI you can *pretty much* break type safety as much as you want by casting pointers around arbitrarily. The `eval` problem is more interesting, in that it is something of a trouble in some ways, but I don't think it's insurmountable. Right now GHC itself is limited to monomorphic types for loading the GHC API - this could be solved with a polymorphic `Dynamic` but it's quite a bit of work (there's relevant stuff on the GHC wiki, maybe I can find it.) If you really need polymorphic eval, then `hs-plugins` will do you better, but it has its own problems (brittle, works on a 'needs to work' basis, uses a direct linker interface - `loadObj` - that leaks memory uncontrollably, etc.) Finally I'm not sure his example is so convincing. You could merely parse the file and use `reads` on particular lines/words in order to give back an appropriate numeric value, could you not? `reads` will appropriately bail if it's not parseable and you just continue on. The fact that his 'proof obligation/discharge' for his example is some sort of predicate merely called `number?` actually makes me kind of worry, in fact - what kind of number are you reading? Naturals, regular integers, floating point, BigInts? That's really the crucially important thing, and considering you have to treat every value the same for arithmetic to make sense, I don't know exactly what you lose in the `reads` case.
agreed. the high level of expertise and general skill in the haskell community seems to preclude haskell prime from every making fundamental changes...most of the community is aware of the warts and has internalized the processes for coding around them. lesser coders would have thrown in the towel and demanded a global fix 
I would guess the rightmost id gets a type a -&gt; a then the next rightmost has type (a -&gt; a) -&gt; (a -&gt; a) and the next ((a -&gt; a) -&gt; (a -&gt; a)) -&gt; ((a -&gt; a) -&gt; (a -&gt; a)) and so on. I guess types are not kept in minimal DAGs.
Parameter unboxing also acts as a poor man's escape analysis. Given: f :: (Int, Int) -&gt; ... f (x, y) = ... g = f (1, 2) The allocation of the pair can be avoided and the components of the pair passed in registers or on the stack. 
There was an example in a recent Repa paper where the performance would have been equal if GHC used vector instructions, like GCC does. No one have gotten around to add those to GHC yet (but someone is working at it right now I believe). 
really? i thought prelude was included in haskell prime because its defined in the report.
Actually, making shoddy programming practices impossible or at least inconvenient is one of the central themes of Haskell. So you think anyone who believes language warts should be globally fixed instead of relying on communal work around knowledge is a lesser coder? #flamebait
Yes, I assumed you knew that. I just wanted to see your full argument.
It seems so funny to me now that a younger version of myself thought Haskell would make me a more productive programmer. When trying to figure out how the "id" function works requires a long and complicated thread, you know you're never going to get anything done.
I'm tempted if nobody chimes in. A very conservative partiality-removal pass wouldn't be hard. Though I'd find it very hard to resist a crack at resolving map vs. fmap vs. &lt;$&gt; vs. mapM all ultimately being the same thing.... but then, that's what kills this process. Everyone who takes the task on wants to fix "everything wrong with Prelude", for suitably different values of "everything". Maybe what's needed is a metaproposal that it is the sense of the community that Prelude needs to be fixed up, and it is acceptable for it to be a slow process with many steps instead of one big bang that arrives at The Solution all at once. And yes, that has its own set of costs, but I can't help but notice how much of every interesting Haskell tutorial like LYAH must sit there and explain why all these silly things are in there....
Right, without DAGs type checking is doubly exponential. With DAGs it's just exponential. 
You have a nice example: http://www.haskell.org/pipermail/haskell-cafe/2009-October/067760.html
One thing to remember is that 0 has type Num a =&gt; a. Maybe there's added complication from the typeclass. Using 'a' instead of 0 might work.
*Actually, making shoddy programming practices impossible or at least inconvenient is one of the central themes of Haskell.* inconvenient? maybe you didn't read the wiki page on how to blow your foot off with a simple fold 
Does it work with a let binding instead of where?
I'm not sure how big of an impact this is in general, but C provides fined-grain explicit control over memory layout and whether or not structures are stored inline (i.e. unboxed) in other structures. In Haskell, my understanding is that it's possible to give the compiler hints about what it can unbox, but it's really just a hint and there's a lot that can't ever be unboxed, such as types with more than a single constructor. I've always thought that limitation is a little silly -- in C, we can define a union, and know that it's sizeof() is always the size of its biggest member. I don't see why ghc couldn't do the same. It also seems to me (who doesn't understand much of ghc internals) like it should be possible to store a pointer to an unevaluated thunk in the space set aside for a lazy, boxed value, and then store it out as a boxed value once it's evaluated.
 {-# LANGUAGE ScopedTypeVariables #-}
see this thread for discussion: http://haskell.1045720.n5.nabble.com/Proposal-Change-to-library-proposal-process-tp3319350p3331653.html
Four yeses! That's a record!
No idea, but using a let binding for this would be horribly difficult to read given the nesting.
Awesome. Does it just match the type variable names?
That shouldn't have happened if you wrote explicit type annotations, like any good programmer should :)
Naturally. That's why languages favored by "lesser programmers" like Java have ceded to popular demand and eliminated null pointers and partial functions entirely! For similar reasons, python and ruby prevent developers from monkeypatching classes at runtime.
How do you square this with the observation that :t id id id id id id id id id id id id id id id id id id id id id id id id id 0 terminates instantly (or as near as doesn't matter)?
Upvote because you actually *do* make a good point. This is a thread about a function that does nothing, applied to itself over and over, and it's got dozens of comments. We've got to be able to laugh at ourselves, right? *But* it is possible to learn something from this thread about how a Haskell compiler works. How many things in the world seemed useless when they were discovered, but turned out to be really powerful?
Seems criterion has bad upper version boundaries for the `statistics` package... :-/ Registering statistics-0.10.0.0... Configuring criterion-0.5.1.0... Building criterion-0.5.1.0... Preprocessing library criterion-0.5.1.0... Criterion.hs:43:8: Could not find module `Statistics.KernelDensity' Use -v to see a list of the files searched for. cabal: Error: some packages failed to install: criterion-0.5.1.0 failed during the building phase. The exception was: ExitFailure 1 
Yes. But only those with explicit declarations using the `forall` keyword.
Oh nice. I tried using `forall` in the hopes that it would do exactly this. Thanks!
Why not `pairThem x = fmap ((,) x)`?
I thought `quot` should give faster code. Did for me in a project euler problem.
Aliasing rm does sound like a bad idea.
Hey! `id` doesn't do *nothing*, it just does nothing to its argument. It's a perfectly respectable function.
[This is what the core looks like for just a few ids](http://codepad.org/2YrUqs1E), you can see the exponential growth, and adding a few more I got over 50.000 lines of this.
OK, that makes sense.
This is really nothing to do with `id`, it's more to do with how the type checker does its job. It's also something the majority of us have *never* encountered, nor would anyone actually write anything that looks like the above. It's an interesting topic though, because it allows people to learn that little bit more about the internals. Oh, and this is really nothing to do with Haskell either, it's a GHC question.
&gt; The existence of unicorns one of the first natural questions regarding the topology of universes. Very deep
http://i.imgur.com/B785s.png
See also [this post on the GHC bugs list](http://www.haskell.org/pipermail/glasgow-haskell-bugs/2003-July/002316.html) from 2003. 
Or `fmap (x,)` if you enable TupleSections.
Using `quot` gives faster code, unless you happen to be dividing with a constant power of 2.
Good stuff.
Btw, does the "BuildWrapper" backend support everything in the IDE that was supported previously with Scion?
Yes, nested `id` is exponential in the same way as nesting pairs within pairs. However, the types are computed rather quickly so it's not inference that's causing the problem; and the types are erased after Core and before assembly, so it's not passing huge type terms around that's the problem (in compiled code). Not clear why exactly this should have the problems it does.
Yes it does matter. In the real world I rarely have binary problems to solve there's always a tradeoff. If I make this go faster than we can save money by using fewer servers, or we can get the result to our client faster, or I get to keep my job because there's a C++ programmer who'd like to replace me.
&gt;which team will probably produce the most expensive work of art? Why? The spray painters, of course - because of the cost of removing it.
Excellent advice, thank you! Quick question, though: I have two different HashSet implementations from separate packages, one from unordered-containers and one from hashmap. How do I specify which package to get the Data.HashSet module from?
Yes, there is no loss of functionality, as far as EclipseFP is concerned.
It sometimes looks a bit [magical](http://stackoverflow.com/questions/7408911/haskell-magical-code-whats-going-on-here), yes.
My lab partner ran into a similar problem, and I used the same solution. It makes me wonder why it wasn't in Haskell 98 in the first place, because they do scope in class definitions.
I don't. The only difference in types is defaulting, and that's not an issue when comparing these two, which show the same performance difference :t \x -&gt; id id id id id id id id id id id id id id id id id id id id id id id id id x let f = \x -&gt; id id id id id id id id id id id id id id id id id id id id id id id id id x
Ooh, that's a neat one. Though it does make me a bit afraid my brain is broken, as I figured out what the `id` trick was doing the moment I saw it. That's gotta be a sign there's something wrong with me.
You should publish *A native implementation of the Hindley–Milner type inference algorithm in human brains*. The main reason I missed it, is because I missed the part where `id` was rescoped.
The first thing I saw while reading it was `(\id -&gt; .......... ) id`. My brain said "Why's he doing that? Must be trying to restrict the type of `id`. Lets see where it's being used... `id undefined`, ah he's trying to extract a type without having a value, keep looking... `id $! decode pc`, yep he wants the type of `decode pc`." Stupid brian. Go learn something practical!
I'm unfamiliar with profiling in GHC. How did you compile/run my program to get this data?
You can use Package Imports: {-# LANGUAGE PackageImports #-} import "hashmap" Data.HashSet as S
I don't think it's really defaulting. If you put a `:t` in front of the second one, it also terminates instantly. I think it's not the top-level type _inference_ that's the problem -- the types are stored in small DAGs for that part -- but in the code generation, which blows the small DAG up into an exponentially sized AST for core. (See e.g. [this comment](http://www.reddit.com/r/haskell/comments/m7uph/whats_going_on_with_id_id_id_id_0/c2yycc6).)
I agree w/ your assessment. 1/2 the time I feel that Mark wants to sell you his language with fairly weak arguments like this. Other times he seems more "pure" in his intentions. I do find the Sequent Calculus typing fundamental to Qi/Shen pretty fascinating though... 
&gt; I don't think it's really defaulting. The point of the example with lambda is to show that it's *not* defaulting - no defaulting there, and still the same explosion.
Ｇｏｔ ｉｔ， ｔｈａｎｋｓ．
Obligatory related question: when will the Haskell Platform be updated to have a newer GHC than 7.0.3? I recall rumors of a December release, though I'm not sure where that rumor came from.
The required `scion-browser` doesn't build with `process-1.1.0.0` (the one from GHC 7.2.1) due to the new `new_group` field of `CreateProcess` (quick fix: `cabal unpack scion-browser`; add a `False` argument to the offending `CreateProcess` constructor call). For forwards-compatibility, I'd suggest using `proc` or `shell` from `System.Process` and then record-updating the remaining fields, instead of using the constructor directly.
Last time I tried EclipseFP, all of the interesting menu options were simply grayed out :-( Also, it made a friendly suggestion of building hoogle indexes for me, and then didn't follow through. This was about 1 month ago. Is it worth retrying?
I kind of wonder where this `⊥ /= ⊥` rule is implemented, and whether it is really desirable or not.
That doesn't introduce you to applicative functors.
Does this version includes the ConstraintKinds extension?
The next GHC to appear in HP will be 7.4.1 I expect.
I just installed eclipse and the eclipsefp plugin to try it out. I followed the instructions on the main website, but I have no syntax highlighting in the haskell perspective (in files named with the .hs extension). I did a bit of searching and found that I'm supposed to have n+1 scion-browser sessions open where n is the number of haskell projects, but I only have one running ever. Any ideas on how to fix this?
My guess would be that the deriving code generates for each constructor C a case branch of the form C == C = True and adds a final catch-all clause _ == _ = False Since an empty data type doesn't have any constructors it ends up being just the _ == _ = False Restricted to the "proper" elements of you type it is actually reflexive. Unfortunately bottom breaks a lot of things (monad laws, for example).
No that will be in 7.4
Seems like you are right. If you try this with a non-empty data declaration (such as `data Z = Z`), `main` diverges. I would assume that this is caused by a `Z == Z = True` case that forces the value of `g`.
I echo this. I installed Haskell (trying both the Platform and just starting with GHC + cabal-install) on a new system this week. It was a pain with some packages requiring 7.2, others only building on 7.0. Haddock is the most prominent offender -- the latest version 2.9.4 only builds on GHC 7.2, so you need to know enough to request 2.9.2. Considering the Platform was supposed to be just "grab and go," that suggests someone is asleep at the switch.
A big thanks to GHC HQ for doing this release.
http://www.haskell.org/haskellwiki/Template_Haskell
&gt; reflexivity not guaranteed in pathological cases ftfy :)
I don't think that kind of skill is impractical. For one, you must be a invaluable partner for pair-programming. ;-)
this is why i don't recommend haskell anymore. how would anyone know to presume that the "standard" random number generator is "dog slow", and to know to seek out mwc-random? this is nearly as bad as having a default String type that we are also told not to use TIME TO START OVER
I like the way you think, this makes sense :)
No, there is only 1 scion-browser session running. There used to be n+1 scion sessions, but now there's no scion anymore. Check Eclipse error view and consoles to see for errors, and post on the EclipseFP forum.
Well, we tried to get it to work with GHC 7.2, but really the supported environment is the Haskell Platform. Thanks for the hint about the fix!
Well the underlying architecture has changed a lot, so it is worth retrying. But not knowing what the problem was, I can't guarantee things will get better
You should ensure the 'hoogle' package is installed in your system before trying again, and try in the latest version of Eclipse if you can.
...and when can we expect GHC 7.4.1 (or any RC of that) btw?
I had both...
You can use `-ddump-deriv` to see the generated derivations: $ ghc --make eqEmptyDataDecls.hs -fforce-recomp -ddump-deriv [1 of 1] Compiling Main ( eqEmptyDataDecls.hs, eqEmptyDataDecls.o ) ==================== Derived instances ==================== Derived instances instance GHC.Classes.Eq Main.Z where { GHC.Classes.== _ _ = GHC.Types.False GHC.Classes./= a_ahA b_ahB = GHC.Classes.not ((GHC.Classes.==) a_ahA b_ahB) } 
This is actually a good point and you see it often. Like the standard example of sorting or working with strings using ++ which looks clean and logical, and then you are told you should never do that. These are things that scare people off from using the language; you take the trouble of learning FP (mostly coming from imp programming langs) and then you are confronted with the fact that mostly everything which *looks* good should not be used in real life. I agree; all effort should be put in fixing the language from those strange things. I want to use ++ because it looks better and logical.
I think (==) should be strict in both arguments. Then in this example will diverge as it should.
I'm still stuck compiling with `LD-Options: -w` on OS X because the platform is four months late. Frustrating that the website claims it's "comprehensive", "robust", and "cutting edge", yet no one can manage to even update a page with a vaguely accurate release date after they've missed by four months.
The curious thing is though, that you forced the auto-derivation of `Eq`, since GHC wouldn't let you define data X deriving Eq emitting the following error: Can't make a derived instance of `Eq X': `X' must have at least one data constructor Possible fix: use a standalone deriving declaration instead In the data type declaration for `X' 
I think this syntactical restriction was introduced as of GHC 7.
Yep, I saw it and read the docs and I saw a lot of overlap with Scala especially with regard to * eliminating unnecessary type declarations through "type inference" (arguably not at Haskell strength) * replacing getters and setters with properties * all-methods-are-functions-are-rvalues * first-class functions (sorta, lambda expressions dressed up as "closures") * optional semicolons ...and now I'm looking forward to test driving it a bit.
Since the About section is currently empty, here's [DSH on hackage](http://hackage.haskell.org/package/DSH), which is what (I assume) this is using.
I've updated the website, and a discussion about a revised release timetable, given the changes to GHC, is underway on the HP dev list.
See this libraries proposal from 2008: http://www.haskell.org/pipermail/libraries/2008-February/009202.html
After installation the plugin asks me for installing missing “helper executables” buildwrapper and scion-browser. When I answer “install” after quite a long output to console I finally get: cabal: Error: some packages failed to install: derive-2.5.4 depends on haskell-src-exts-1.11.1 which failed to install. haskeline-0.6.4.5 depends on terminfo-0.3.2.3 which failed to install. haskell-src-exts-1.11.1 failed during the final install step. The exception was: ExitFailure 1 scion-browser-0.1.3.1 depends on terminfo-0.3.2.3 which failed to install. terminfo-0.3.2.3 failed during the final install step. The exception was: ExitFailure 1 And the next run of Eclipse starts with the same dialog. I tried to install those applications manually through cabal using system-wide mode (--global) and succeed! The executables are in the PATH now (accessible right from the terminal). But plugin still says that they are missing. Any suggestion what to do?
Go to Preferences -&gt; Haskell -&gt; Helper executables and enter the path to the executables there (or use the autodetect button). Since they were built outside of EclipseFP you need to tell EclipseFP where they are. I suppose we could search for them in the path and not ask the user if they're found.
Relevant: http://www.well-typed.com/blog/48
Thanks a lot! It works!
Just in case anyone cares: I installed the plugin, added a new Xtend class to a maven-based project and * the POM dependency editors stopped working; * Eclipse hung, badly. I had to kill it from the outside. TL;DR: My first experience with Xtend was not positive.
Simon Peyton-Jones said recently in a GHC status report that we should expect it "by Christmas". Not sure if that's changed.
This would make sense. 7.2 has, from the beginning, been explained as a sort of feature preview release. I use it because of SafeHaskell, but it would be odd to base an HP release off of it.
tl;dr: rant about something is definitely broken in its current state :( But still somewhat in the process is unfortunately totally broken, when installing packages via hackage is such a pain and leads to even more problems, since loads of packages seem to depend on 7.2, but 7.2 is more of a "feature preview release" not for "normal" users. One usually wants the whole ecosystem (ghc, HP and hackage) to function (almost) correctly, but as it stands now, this is not the case. If I have the current HP (using 7.0 at the moment), I want to be sure only packages for this release will be installed/downloaded correctly by cabal without going through too much pain. It's fine if other developers want to (lets say) "play" with 7.2 (actually me too), but as it stands right now the whole ecosystem is a pain to use for some users since ghc 7.2 . Despite known problems (we all may have had experienced) with cabal, the current state feels like a step backwards for many users: either go "stable" and use HP, or use newest/latest/bug fixed/improved/coolest libraries from hackage and install 7.2 by yourself. That's why some users might become frustrated and we see "frequent" HP update requests. Going with 7.2 myself I feel like it doesn't affect me much, but I understand for others this is a major problem meaning something is definitely broken in the haskell ecosystem :(
When I type in the .hs files, the symbols appear after ~1sec lag. In the progress bar there is "Synchronizing editor content for project" text. Turning automatic build off does not help. I did not have this issue with an older EclipseFP version. Ubuntu 11.04, Eclipse 3.5.2, GHC 7.0.3
Kudos on avoiding the usual IO and import vulnerabilities, but trying out the Mueval test suite, looks like simple DoS attacks work.
Just uploaded version 0.1.3.2 of scion-browser which should build on 7.2. Enjoy!
I'm afraid this is a consequence of the new architecture. There is no ideal solution here. Using a long running process takes up a lot of memory, so we moved to running an executable, and some operations may be a bit slower, especially on big modules. I found that in practice (I use EclipseFP on my other Haskell projects, of course) it wasn't a problem on a reasonably good machine, but that's something we still can improve I think.
Further Information for you!
Qi is fascinating and brilliant in and of itself. I'm not sure why Dr. Mark Tarver feels the need to compete with equally brilliant but different type-systems like Haskell and ML. For example, one of the neat things about the Qi type system is that it seems to be possible to introspect on the type-checker. Here's an [example](https://groups.google.com/d/msg/qilang/9sx_pTH6nys/818MCP_MC60J) in which Dr. Mark Tarver shows how to have the type-checker output all the types of an object. 
That would certainly be an awesome Christmas present. :)
Thanks Don (and HP devs). We all appreciate that.
Compile with `-prof -auto-all` and run with `+RTS -p`. This will generate a .prof text file in the same directory as the binary.
You can also pass the `-hide-package` flag to GHC.
I would recommend that you use the unordered-containers package. Milan Straka (the creator of hashmap) and I agree to continue development of hash maps in the unordered-containers package.
We'll use the Haskell Platform to promote the better package. We're working on it. I'm also working on an in-depth performance tutorial which will, among other things, talk about what libraries to use and functions with bad performance characteristics.
(Tip: Try to break your message up in paragraphs, it's really hard to read and thus hard to understand how to help you out). Are you saying that packages are now broken with 7.0.4? I haven't noticed. Could you please list what's broken with what GHC version please so we can fix the packages?
I'm surprised we haven't seen more on this. Haskell is extremely well suited for implementations on unconventional hardware, since the compiler is profoundly able to understand the semantics of the program, and since the program is profoundly prevented from making assumptions about the underlying hardware.
My module is about one thousand lines long. This executable can be run when there is no user activity for a certain time or asynchronously without blocking the UI (not sure how this fits into your architecture). Also I want to note one small bug. When I edit a file and undo the changes, it is still marked as modified. edit: on small modules the speed is reasonable
A fellow Haskeller at SC11 - greetings! I've been using Haskell in the HPC world for a while, although not for writing HPC applications directly. I've mostly been interested in Haskell as a language for building DSLs that generate code in lower level languages like C or Fortran. I've been following the parallel Haskell work, but haven't done anything serious with it yet - mostly in lurker mode on the mailing list at the moment. Functional programming in the HPC realm has been around almost as long as HPC itself -- stretching back to the old dataflow languages, things like multilisp and *lisp, and the Sisal project. Shoot me a message if you want to meet at the conference and chat about Haskell and HPC!
The executable runs in a job so the UI stays responsive, but Eclipse waits for that job to finish before it can save the file, which is why you notice it. I'll see if the concurrency can be improved! Thanks for the feedback!
I'm in similar shoes. I'm using Haskell for what can be loosely labeled as HPC: small in HPC world, big in normal world (meaning small clusters). However I'm only using Haskell for coordination, and later on I plan to use DSLs to generate C or CUDA code.
You can use a language pragma, that way you get the extension when compiling and when in ghci: {-# LANGUAGE NoImplicitPrelude #-}
In particular we're working with a new startup that is working on HPC in Haskell. Both low level SIMD vector stuff and also distributed/cluster parallelism. I hope to blog about both in the not too distant future...
There was a long discussion about that as a subthread of dons' [proposal](http://www.haskell.org/pipermail/libraries/2008-February/009202.html) to add `maybeRead` to the base libraries. (See also dons' post below.) There was a lot of bikeshedding about this. But I think the upshot of the discussion was that you really don't need all of that machinery. Since `Maybe` is the universal example example of this, you actually lose no generally by using `Maybe`. It is trivial to use `maybeRead` with any scheme you decide to use. So by the K.I.S.S. principle, I think that is the best way to go.
Let's revive that proposal.
Yes, but `reads` is usually inconvenient to use as a safe version, too. We need another convenience function for that.
Now that the library proposal process has been relaxed, the first step would be just get it into a library that is supported by GHC and other popular compilers. Once that is an established fact, it can be codified in a subsequent version of the standard.
If Reddit guarantees certain forms of purity, it's probably one of the cases where (un)safePerformIO can be used.
It would be good if ghci were faster, but it's speed does not disturb me. When I need speed I compile.
This is true, but in this case, a so remarkable difference in times is annoying me a bit. 
With some effort the interpreted code could be sped up, but I would much rather have that effort spent on speeding up compiled code.
Yes, if I have to choose between one of the two, I will obviously choose to invest time and efforts improving compiled code, but it will be nice to have a fast interpreter to play around with. I mean, the compilation is the secret weapon of Haskell, a fast interpreter could be push it even higher in the global adoption, imho :)
Sure, everything should be faster. But given the constraints of having not very many GHC developers, I don't think it should be a priority. One possible approach might be to use LLVM's JIT stuff behind the scenes for GHCi, but I don't think any of the LLVM code is currently wired to be able to do that. The usual approach to this stuff in open source projects is to wait for someone with an interest in seeing X happen to implement X, for the benefit of everyone else.
Uhh, SBCL is a compiler, of this I'm sure, and a pretty good one at that. I don't know how the repl works. Are you sure it isn't compiling on the fly before you run it? Many Lisp and Scheme repls work this way, especially if they have a fast-enough incremental compiler. (As a historical aside, Luca Cardelli pioneered this approach in his ML compiler for the VAX. There is a school of thought that the initial focus of a compiler should be on generating code quickly, and then directing future work on generating quick code without sacrificing compilation speed.) What I'd really appreciate in GHCi is a command like :rc that will recompile and then reload the object file, instead of exiting out of the interpreter, recompiling, and then loading the interpreter again.
Actually, it seems the Scala version uses `Int` (which I assume is not a BigInt), changing his code to the following cuts down the runtime to about 30% compared to his Integer+mod version in GHCI for me: solver :: Maybe Int solver = find divPred [20,40..] divPred :: Int -&gt; Bool divPred x = all (\y -&gt; x `rem` y == 0) [20,19..2] main :: IO () main = do putStrLn $ show solver
Thanks for the tip :)
Again, Haskell community is smart and clever as always :) Thanks to everyone for the replies. Also, check the blog post comments for the :set -fobject-code flag :)
Yeah. I actually agree with that. For most of the monads that you want to use read in, you don't really get anything exciting by call 'fail' instead of just returning Nothing. Mostly you just get more to think about and more complex type errors.
I guess the trick is :set -fobject-code, which I didn't know about. Though I wonder if this can be used to recompile and reload from a .cabal file. 
I've noticed in the past that GHCi is shockingly slow, at least in some cases. It's annoyed me a few times, but I don't see it as a big deal: after all, I don't usually use Haskell as a scripting language anyway. I was wondering if this was due to some inherent difficulty in interpreting Haskell, or just lack of priority for optimizing GHCi. Some of the responses here seem to suggest the latter. Still, given the awesomeness of the GHC developers, I'd expect the interpreter to be faster than this, even without a lot of optimization effort.
GHCi is known to be 20-30x slower than compiled code (which fits here). The two main reasons are: (1) it runs fewer high-level optimisations (i.e., it's closer to -O0) and (2) it's not meant to be fast in the first place. SBCL actually always compiles functions first before executing them. Scale runs on the JVM, so it can take advantage of its JIT. GHCi's interpreter design is also rather old-fashioned (stack-based instead of register-based). If you put in some effort you could get a factor of 5-10x over compiled code (assuming the same high-level optimisations). This is what I got from a microbenchmark comparing the techniques used in the LuaJIT 2 interpreter with hand-optimised assembly. The key design features are: (1) the interpreter is written in hand-optimised assembly, and (2) it's register-based, which increases the useful-work per instruction ratio. 
The main question here seems to be why GHCi is much slower than GHC whereas the scala interpreter is similar speed to the compiler. The answer is that Scala doesn't actually have a separate interpreter; it has a REPL (an interactive Read Eval Print Loop environment) but this is implemented by compiling to a temporary Java class file and loading that into the JVM. This makes especially good sense for Scala as both the scalac and javac compilers don't really do much optimization; they produce naive JVM bytecode and rely on the (highly advanced) Java JIT compiler built into the runtime to make it go fast. On the other hand, GHCi has a separate byte-code based interpreter which it uses by default. As others have said you can use the -fobject-code flag to make it work more similarly to the Scala REPL.
Axel has been a good and sensible maintainer for many years. Much credit is due.
See the new [SIMD vector work](http://www.reddit.com/r/haskell/comments/mbpoi/new_project_to_support_simd_vector_instructions/).
Sure, that doesn't stop us from doing a beta/tech-preview of haskell-platform too! There is clearly demand fot it. :)
I wish I knew more about these sorts of things so I would know what sort of questions to ask, it seems really interesting. Just some naive questions though: The linked page is clear about the goal and even links to a plan of implementation, but what sort of a speed increase would this likely provide? I have the impression that SSE were (assembly?) instructions that made operations on vectors faster but it's all pretty vague in my head. Would the speed increase be by a factor or a magnitude? Would the it only be noticeable on large vectors? By vector does that mean only unboxed arrays, or boxed arrays, or even strings (I'm guessing not)? And while speed in a programming language is always good, is there anyone out there in particular who is *really* eager for this kind of thing? I'm guessing they'd be doing some serious number crunching but I want to know what for?
See the paper "Efficient Parallel Stencil Convolution in Haskell", http://www.cse.unsw.edu.au/~benl/papers/stencil/stencil-icfp2011-sub.pdf . It mentions the lack of SIMD as one of the top reasons why they could not get equal performance to OpenCV.
The maximum speed up you could ever attain is the CPU vector length. This is because instead of processing 1 element of an array at a time, you could do N. That said, the CPU vectors are all pretty short: 2, 4, 8, depending on the type and if you're using SSE or AVX. For example SSE has vectors of 4 single precision floating point, or 2 double precision. AVX has vectors twice as long. So, in summary if your algorithms spend almost all their time working in a mostly-linear fashion on big arrays then potentially you can get it to go nearly 2x, 4x or 8x faster (depending on the type of vector and processor).
Vector instructions are normal instructions that operate on n elements on parallel. So, with 4-wide vectors, you can expect best-case speedup of 4 times. The cost is that you have to manage your data in aligned chunks of vector length. This is usually either trivial, or hard enough to make using vectors pointless, depending on your load. AVX 2, coming with Intel Haswell processors sometime in 2013, has gather instructions which should make managing vector data much easier. 
So are these new primops useful for everyday types and not just DPH; i.e. would it help accelerate Text?
&gt; I have the impression that SSE were (assembly?) instructions that made operations on vectors faster but it's all pretty vague in my head. Yes. Most current CPU architectures (x86, arm, sparc, ppc, mips) have extensions which add extra instructions and registers for handling relatively short vectors. The vector registers are usually for 2, 4 or 8 integers or floating point types. The operations on these vectors are things like addition or multiplication which are done element-wise. That allows 2,4 or 8 scalar operations to be done in one go which can be faster. It can be faster mainly because of the extra instruction-level parallelism, but also because it reduces the amount of work spent on instruction decoding compared to doing real work (*). They are extensions because the base CPU architectures were designed before people had decided that SIMD vectors were a good idea for ordinary general-purpose CPUs. So for example the x86 architecture has no less than three different vector extensions. There's the old MMX extension, the current SSE (versions 1-4) and the very new AVX extension. The progress has been towards bigger vectors, supporting both integer, single and double precision floating point, and a more complete set of operations. Having a better range of operations makes it more practical for compilers to generate vector instructions whereas earlier (especially with MMX) programmers typically used assembly directly (or really low level builtin functions in C). (*) For example, on old Cray vector computers the vector registers are up to 64 elements long. It doesn't have 64 operational units on the CPU. Instead the long vectors allow the CPU to do pretty much optimal pipelining and to overlap memory fetches/stores with arithmetic operations.
Good point :)
Depends. If Text is internally utf-8 encoded string of bytes, no. If Text is instead a string of 32-bit Unicode codepoints, then yes. Also, not all computation benefits from vector extensions -- notably, anything that would require nested jumps would not get any speedups.
Probably not much. And I don't think it's the UTF8 vs 32 issue so much as the kinds of operations you do on text.
The main example given is laziness. I would agree laziness can make things hard to reason about, but this has nothing to do with a sufficiently smart compiler. Laziness in and of itself is hard to reason about, regardless of how smart the compiler is.
 import Data.List (foldl1') main = print $ foldl1' lcm [2 .. 20] Problem? ;] (I know it’s not the point, sorry.)
&gt; Why don't we use an equals sign? Shenanigans. (More precisely, createEntry x has side effects.) I found “shenanigans” to be the more accurate answer here.
Hoogle requires a few command-line utilites including tar and wget to generate the database. Are these in your path?
Is there an article that tells me how to read Python like Haskell?
He wasn't using laziness the concept as the example. The example was the way the compiler will *sometimes* but not always optimize the laziness away. The net result being that sometimes laziness will bite you and sometimes it won't. This makes laziness much much more difficult to reason about than if the compiler just kept everything strictly lazy.
How to translate Python into idiomatic Haskell? Might be interesting.
Yeah.. I'll reinstall sometime soon and see how it goes.
Well there's this: http://hackage.haskell.org/package/berp :)
Fortunately, GHC can dump the Core output and expose what became strict and what didn't, etc. Unfortunately, Core is much less readable than it could be.
Heh sorry about that :) cheers for the code I use then!
I have a hard time agreeing that the derivation of (==) should insert "seq" here. I think the right answer is that deriving Eq on empty data types should fail. EDIT: or, potentially, I could agree with the suggestion in the bug report, which is that the derived instance would be instance Eq Z where _ == _ = error "Void datatype in (==)"
How does it make it more difficult? If you are willing to go strictly lazy, this will never make it worse than that... I guess what I am saying is, the fact that you have less control over laziness is not due to a super smart compiler. Its due to the fact that the language is lazy by default, and requires adding strictness annotations to the code. The fact that the compiler can do it for you sometimes is irrelevant in my opinion.
I think you just need to import haskell
Not really a problem. Python has much fewer combinators etc and it has a BDFL that curates the standard libraries and what becomes standard practice, making sure that it's all simple and that "there is one obvious way to do things".
There is this one for *writing* instead of reading: http://markshroyer.com/docs/pointfree/latest/overview.html 
Actually you're wrong that utf-8 operations cannot be sped up with SIMD. Look here: http://porg.es/blog/ridiculous-utf-8-character-counting
That's a hand-coded optimization -- there is no way a compiler will ever convert strlen to that automatically. Or do any automatic vectorization on utf-8 data.
This is awesome! I personally think happstack is already the _simplest_ of all the main Haskell web frameworks, but I can see how this may benefit newbies. More people should be using happstack. It is, in my opinion, the best Haskell web framework there is yet.
This fantastic solution has already been posted among my blog comments, but thanks anyway :)
I see. So if OP's proposal was implemented would these guys have benefited from SSE operations with a flag at compile time or would they have had to add throughout the code? From the sound of things it would be reasonable to expect benefits just by a compiler flag.
Thanks, that's all new to me.
Of course. But I mean that implementors may manually take advantage of SIMD and apply it to speeding up text libraries too.
- Memory alignment - Alignment of the stack: The wiki argues that alignment of the stack is unattractive. The standard x86-64 ABI always maintains a 16-byte aligned stack, so there is existence proof that this was not unattractive for the main ABI. For heap alignment, I am all with you in not aligning. There could be an option to align when the user asks for it. There is a cost in aligning which is that the garbage collector will trigger more often, and it also reduces the effective size of the cache. If the extra cache line that is brought in for an unaligned read contains other useful data that is going to be read anyways, it isn't much of a loss. I actually think that the whole ghc runtime would do with less aligning. The only overhead of unaligned reads nowadays (post Nehalem) is cache line crossing, but only when it actually happens. It used to be that the logic involved in the unaligned sse instructions would cost extra cycles. That isn't true anymore AFAIK.
I like it very much. Would it be migrated to wai/warp ?
However, ghc has quite a few options that make it much more readable: -dsuppress-module-prefixes -dsuppress-uniques -dsuppress-coercions
We're not planning on doing any automatic vectorisation, except for DPH. So you're not going to get a benefit just with a compiler flag like -msse / -mavx. You also have to be using a library that takes advantage of the primitive vector types and operations (e.g. some future version of the vector lib) or use those low level functions yourself.
&gt; The wiki argues that alignment of the stack is unattractive. The standard x86-64 ABI always maintains a 16-byte aligned stack, so there is existence proof that this was not unattractive for the main ABI. It's not that we think stack alignment is a bad idea. It's just more implementation work and it turns out we can do without it initially without causing performance problems. The same rationale applies for the heap. Note that if/when we do heap alignment, we'll certainly not align everything, just the heap objects that require the higher alignment, otherwise yes we'd be wasting loads of memory. But it's that conditional alignment that adds to the imlementation effort and another reason to postpone doing it since we don't need it for the big array use cases we're targeting initially.
Here are my settings: http://lbolla.info/blog/2011/10/15/haskell-and-vim/
I think that it is a great news. As a novice Haskeller, I'm often overwhelmed by the complexity of web frameworks like HappStack (senior), Snap, or Yesod. I will definetely give it a try! Thanks!
This is fantastic work, thanks! Hope it comes to fruition.
&gt; strictly lazy Pun intented? 
I think it's actually nicer to `seq` both empty data types. That way, you get the error from where it was constructed instead of the one from the `Eq` instance. The former is probably more informative (since these values should never be constructed).
Does it focus on type safety as much as Yesod does? For example, guaranteeing type-safe routes/URLs?
Yes. Unless we come up an even better option. ;)
yup. web-routes was originally developed for use with Happstack. When Yesod came along Michael and I worked together to refactor it into a type-safe url library that could be used with any Haskell web framework. You can read the section about web-routes in the Happstack Crash Course here: http://www.happstack.com/docs/crashcourse/WebRoutes.html#web-routes Yesod users generally use the web-routes-quasi extension to automatically generate the route types from the routing table using some quasi-quoting magic. That should work with Happstack as well, though I have not had a chance to document it yet. 
Unfortunately, new haskellers will likely be overwhelmed by any sufficiently capable Haskell web framework...at least at first. Most of them do pretty much the same thing. Of course, simplifying things is always a good idea. That was the main thing we did with Snap. But the problem of building websites just has a lot of inherent complexity for which there's really no silver bullet.
Here's a version of an old standard: data Expr a where BoolExpr :: Bool -&gt; Expr Bool NumExpr :: Double -&gt; Expr Double GTExpr :: Expr Double -&gt; Expr Double -&gt; Expr Bool AndExpr :: Expr Bool -&gt; Expr Bool -&gt; Expr Bool This lets you represent "1 &gt; 2 AND 3 &lt; 5" but not "1 &gt; True"
I'd be mildly inclined to drop AndExpr in favour of a polymorphic CondExpr :: Expr Bool -&gt; Expr t -&gt; Expr t -&gt; Expr t to show that you get some improvement over just defining two mutually inductive simple types ExprBool and ExprDouble. Of course, the essence of GADTishness is just data Equal :: * -&gt; * -&gt; * where Refl :: Equal x x giving the ability to trade in evidence that two types coincide.
I agree that `Refl` is the nub of the issue, but to a novice programmer, its use is much less immediately obvious.
Example construction is difficult. If someone were to ask for a single example of a notion which is easy enough to be a good introduction, complex enough to be impressive, and incisive enough to make clear what makes that notion necessary, that would often not be a realistic expectation. Perhaps I'm mistaken, but I interpreted the OP's question as targeting the third of those goals.
Perhaps not a simple implementation, but a simple concept: You can define [a type-garaunteed red-black-tree](http://github.com/yairchu/red-black-tree/blob/master/RedBlackTree.hs). data Red data Black data Tree a where Tree :: Node Black n a -&gt; Tree a -- Node types are tagged by their subtree's Black-degree (number of Black nodes per path). data Node t n a where -- A degree 0 Black node must be an empty tree. Nil :: Node Black Zero a BlackNode :: NodeH t0 t1 n a -&gt; Node Black (Succ n) a RedNode :: NodeH Black Black n a -&gt; Node Red n a data NodeH l r n a = NodeH (Node l n a) a (Node r n a) 
You are exactly correct. I don't need an introduction or an interesting use case. I just want the bare minimum of what a GADT is needed for that can't be implemented without it, even if the example is trivial or useless.
I appreciate the effort, but I already am familiar with the use case for expressions and this wasn't exactly what I was looking for. I just wanted the simplest example, even if it's trivial or abstract, of something that can be done with GADTs that can't be done with Haskell98. You don't need to motivate the application of them, because I already know several, including the one you just gave. I just wanted something that would make it really clear to me what conditions absolutely necessitate the use of GADTs so I know how to effectively use them to solve new problems.
The Refl example is exactly the kind of thing I was looking for.
"Without GADTs, but with what else?" remains an important question. But let's be clear about this much: if you have your usual nonuniform recursion (nested types) and you have existential types, then Equal will get you the rest of the way to GADTs. As in data Expr a = BoolExpr Bool (Equal Bool a) | ... using equality to insist that the parameter (a, here) is equal to the thing you actually want. Of course, you need equality at any kind, but you can always make a type parametrised by a given kind: data MyKindProxy (x :: &lt;mykind&gt;) = MyKindWitness Let's say &lt;mykind&gt; is * -&gt; *. Now you can use equality on * to say things like (Equal (MyKindProxy f) (MyKindProxy [])). So if anything's the unique encoding capability of GADTs, it's that there Equal! And how else will you encode equality? Specifically, you'll need stuff like injective :: Equal [x] [y] -&gt; Equal x y injective Refl = Refl That's not going to play so nicely if you just have data Leibniz x y = Subst (forall f. f x -&gt; f y) unless you have some wild type-level function magic to construct a suitable f. The cool thing that GADTs bring (just like Coquand's dependent pattern matching, from 1992) is unification of types in each specific case, instantiating the type variables involved in the problem. Making that happen by alternative means is somewhat problematic.
I don't think GADTs are good to solve new problems. They make it harder to write wrong code, at the expense of making it more descriptive, and, in my little experience, more verbose. I think phantom types are a previous step to get their usefulness.
Keep in mind, you only need a very minimal language to program anything. The real questions you should be asking are: 1) What problems do GADTs make *easier* to solve? 2) Does their usefulness outweigh the technological debt to employ them?
Great explanation! I assume you're familiar with this, by the way: http://hackage.haskell.org/packages/archive/eq/0.3.4.1/doc/html/Data-Eq-Type.html
There's no fundamental reason why GHCi has to execute slowly, but there are several explanations. Partly it has to do with trading off compilation speed against execution time, so we don't do many optimisations in GHCi. Partly it has to do with supporting the debugger - each breakpoint has to check a flag, and extra safe points are inserted. Partly the byte code instruction set is brain-dead in the name of simplicity (see nominolo's answer), and partly it's just because nobody has put any effort into optimising it.
Thanks. This is very helpful. So basically, (Equal Bool a) is like a type signature for "a" (like "a :: Bool"), except that you can embed it in the data constructor signature, exactly analogous to the more general form of GADTs where you would just specify it in the constructor's signature. So the below (pseudo-code?) would be sort of equivalent: data MyType (Equal Bool a) = T data MyType a = T :: MyType Bool
I hadn't seen that particular instance, and I'm struggling to cope with := as the propositional equality (no! it's not symmetric! no! that's more like assignment!). Using type functions to get injectivitity is only a new(ish) trick to Haskell. Us type theorists were doing it before... etc
There was a suggestion in some presentation/discussion about optimizing haskell programs that a pragma like VIRTUAL should be introduced to mark some (intermediate) datastructure as something you expect to get optimized away, and I guess you'd get a warning if it doesn't..
I really like Ruby's Sinatra. For example, I can say get('/adder') { (params['a'].to_i + params['b'].to_i).to_s } and voila, localhost:8000/adder?a=1&amp;b=2 returns "3" Is there any analogue in Haskell to simplicity? Ruby feels a bit string-mungey to me.
Suddenly, what Oleg has been doing over the years is becoming a lot more understandable! This post made a lot of things I half-understood (and half-filed-under-deep-magic) click in a big way. Thank you!
And actually you can use GHC's type equality constraints to do Refl without the overhead of actual data in memory: data Expr a where BoolExpr :: Bool -&gt; Expr Bool NumExpr :: Double -&gt; Expr Double GTExpr :: Expr Double -&gt; Expr Double -&gt; Expr Bool AndExpr :: Expr Bool -&gt; Expr Bool -&gt; Expr Bool is equivalent to data Expr a = (a ~ Bool) =&gt; BoolExpr Bool | (a ~ Double) =&gt; NumExpr Double | (a ~ Bool) =&gt; GTExpr (Expr Double) (Expr Double) | (a ~ Bool) =&gt; AndExpr (Expr Bool) (Expr Bool) And Equal is just data Equal a b = (a ~ b) =&gt; Refl 
&gt; I'd be mildly inclined to drop AndExpr in favour of a polymorphic &gt; &gt; CondExpr :: Expr Bool -&gt; Expr t -&gt; Expr t -&gt; Expr t Wouldn't ApExpr :: Expr (a -&gt; b) -&gt; Expr a -&gt; Expr b with maybe a ConstExpr :: a -&gt; String {- for printing :) -} -&gt; Expr a thrown in, be a lot more powerful? Come to think of it, the String in `ConstExpr` could be quantified over with the new Constraint Kinds extension and you basically have an expression language for any purpose. Time to get my hands on a recent GHC and go play with GADTs and CKs! Up to now, I've mainly used GADTs to make up for the fact that I always forget the syntax for existential quantification...
To be pedantic, there should be _no examples_ that cannot be encoded without GADTs. You can always transform a GADT into a finally [sic] tagless representation through a class if you have Rank N types. This is how the ocaml community worked around their lack of GADTs for the longest time. That said, such a representation can be much more awkward to work with.
'finally tagless' suddenly makes more sense to me! Thanks for this insight!
I am super interested in HPC with Haskell as well. In fact I am learning Haskell as I intend to use it for parallel processing further down in my career. Right now I am just going through the ropes of learning Haskell, but this is where I hope to be going for in research when I work on a phd.
Thanks. I'm beginning to understand now that the fundamental application of GADTs is just to serve as an equality witness to a type variable, either through the GADT directly: BoolExpr :: Bool -&gt; Expr Bool ... or through Equal (which acts like an additional parallel type signature): BoolExpr :: Bool -&gt; Expr (Equal a Bool) ... or through GHC's type equality constraints: Bool Expr :: (a ~ Bool) =&gt; Bool -&gt; Expr a
Also note that finally tagless is really an initial encoding, hence the sic in my original comment. However, this is perhaps a rather pedantic distinction as final and initial tend to coincide in Haskell, and initially tagless isn't a good pun. =)
Type theory for proofs before it was cool. You PL hipster ;)
A simple Haskell version might look a bit like: do method GET dir "adder" a &lt;- lookRead "a" b &lt;- lookRead "b" ok (a + b) of course, there are ways we could shorten it. We could define: get :: String -&gt; ServerPart () get d = method GET &gt;&gt; dir d int :: String -&gt; ServerPart Int int name = lookRead name And use applicatives for something more like: get "adder" &gt;&gt; (+) &lt;$&gt; int "a" &lt;*&gt; int "b" though, it is hard to claim that is easier to follow. We could write it like this instead: do get "adder" a &lt;- int "a" b &lt;- int "b" ok (a + b) The difficult part is defining what 'simple' is, who it is simple to, and when it is important. It is easy to make simple examples short. We simply add more primitives until we can express simple ideas using only a few built-in functions. But then we may end up with too many functions in our API doing too many simple things. Then it becomes hard to remember the API, and you get less done. And those simple things may not help us when we need to do things that are more complex. (I'm not saying Sinatra does that.. I have no idea. I am just saying, we definitely don't want to do that to Happstack). So, let's break this down a bit more. We will look at the first example I posted, and your Sinatra code. (I am going to make some guesses about how Sinatra works here, so I could be wrong). Sinatra starts with get. Happstack starts with, method GET. clearly the Sintra solution is shorter (one token instead of two). Though, we can easily define a shortcut in Haskell as shown above. The advantage of the current solution is that we can easily match on multiple methods. We could write: do method [GET, HEAD, POST] If we wanted to match on any of those three request methods. Not sure what the Sinatra solution to that is. Additionally, we can supply an arbitrary function that looks at the request method and returns true or false: do method $ \mtd -&gt; &lt;do something here and return True or False&gt; So, we have a lot of flexibility there by using two tokens instead of one. At the very least, 'method GET' is not significantly more taxing to understand than 'get'. Next we match on the path In Sinatra we have: ("/adder") In Happstack we have: dir "adder" So, once again, we have twice as many tokens as Sinatra. If we wanted, we could define 'method' to take two arguments instead of one so we could write: method GET "adder" But, that prohibits certain useful patterns. Let's imagine we have an HTML form. We want to show the form for a GET request and process the submitted data for a POST request. So, we might want to do something like this: dir "form" &gt;&gt; formHandler There we only match on the path "/form". Inside formHandler we would check for POST vs GET and do something different. So, obviously, being able to check for the request method and the path in different places and different orders is useful. (At least, it is something I often use). And, once again, while slightly more verbose, the Happstack version does not really seem any more complex.. Next we have the fetching and conversion of the query parameters. In Sinatra we have: params['a'].to_i In Happstack we have: a &lt;- lookRead "a" The differences here are largely syntactic. In Haskell, we do not have to explicitly state what type to parse the value as, because type-inference can figure that out for us. In Happstack, we are not restricted to only reading certain primitive values. We can add a FromReqURI instance for any type and it will then automatically work with lookRead. I assume you can do something similar in Sinatra by adding new to_foo methods to params? Finally, we have adding the values together and converting the result to a response. In Sinatra we have: (params['a'].to_i + params['b'].to_i).to_s In Happstack we have: ok (a + b) In Sinatra, we do not need to name the intermediate values. We just add them and then call to_s to convert the result to a String. In Happstack we add the Ints and call 'ok' to explicitly set the response code to 200. We could actually write: return (a + b) And it would implicitly be set to 200. But I like to be explicit. Plus 'ok' is shorter. The Int value will automatically be converted to a Response via the ToMessage class. So.. I think the big take away here is that this example too short to really show any real significant differences between the frameworks. We can see that for certain things ruby has a more concise syntax. But there is no reason to believe that will be an important deciding factor when we start to look at more complex real world examples. 
...does this mean, that Facebook will be Haskell powered soon? :-)
The second entry would be better as BoolExpr' :: Bool -&gt; Equal a Bool -&gt; Expr a The first and third ones are fine (and identical, actually; `(a ~ Bool) =&gt; Bool -&gt; Expr a` is the same as `Bool -&gt; Expr Bool`)
&gt; that cannot be implemented without them Last time I checked Haskell was Turing complete so...yeah. Features like GADTs (or any language feature, really) aren't for something that "cannot be implemented without them". They are for making certain tasks much nicer.
One example that I don't now if there is a solution without using GADT. I can create a tree to represent a composition of binary and unary functions like: data Tree a = Bin (a -&gt; a -&gt; a) (Tree a) (Tree a) | Un (a -&gt; a) (Tree a) | V And convert this tree to a function with: treeToFunc :: Tree a -&gt; a -&gt; a treeToFunc (Bin f l r) = f &lt;$&gt; treeToFunc l &lt;*&gt; treeToFunc r treeToFunc (Un f u) = f.(treeToFunc u) treeToFunc V = id The only problem with this solution is that you can only use functions of one type. If you want to have arbitrary functions in the tree and assure that the types of the tree nodes type check, you need to use GADT: data PTree a b where PBin :: (a' -&gt; b' -&gt; c) -&gt; PTree a a' -&gt; PTree a b' -&gt; PTree a c PUn :: (a' -&gt; b') -&gt; PTree a a' -&gt; PTree a b' PV :: (a -&gt; b') -&gt; PTree a b' With this you can create a function that convert this polymorphic tree to a function: ptreeToFunc :: PTree a b -&gt; a -&gt; b ptreeToFunc (PBin f l r) = f &lt;$&gt; ptreeToFunc l &lt;*&gt; ptreeToFunc r ptreeToFunc (PUn f u) = f.(ptreeToFunc u) ptreeToFunc (PV f) = f 
Congrats!
+1
enjoy php 5.2 (aka hiphop)
Maybe they'll write a Haskell EDSL that generates PHP to avoid having to code directly in PHP... ;-)
Iiuc, this is not the endorsed solution in happstack-lite, right?
Well, there is already [this](https://github.com/facebook/lex-pass).
Besides the ability to write certain programs, and the ease of doing so, another important issue is the trustworthiness of these programs (translating mostly to type-safety). Employing language features is "technical debt" in the general case, as many projects have the luxury of assuming their developers know the language they're using.
I'm told that the "[Finally Tagless](http://www.cs.rutgers.edu/~ccshan/tagless/jfp.pdf)" paper shows hows to simulate GADTs using typeclasses.
Well, I think everyone should use web-routes. And it does work with happstack-lite. But, we do not introduce it in happstack-lite. web-routes usage usually involves monad transformers, template haskell, and/or advanced parsing combinators. Seems like a bit much to throw at novices first thing.. web-routes was created, not just for the sake of type-safe urls, but also as the foundation of component system (aka, something like yesod sub-sites or snap's snaplets). happstack-authenticate, and other upcoming libraries are based around web-routes. So, developers will ultimately be exposed to, and encouraged to use web-routes. The dir and path combinators do have their uses, even in web-routes based applications. So, it is not as if people are learning something useless if they learn about those first. But, I am open to discussion. Do you think that web-routes would actually make it easier for people to get started? 
My somewhat limited understanding is that GADTs have more to do with Curry-Howard isomorphism (which dictates that types must reduce down to a single theorem) than it does to Turing completeness (which is essentially typeless). It is not at all difficult to push Haskell's type system into corners where it doesn't know how to resolve the types. GADTs offer a clean solution to an otherwise problematic corner case.
I'm not sure if these can be represented by another method, but I think a [type-threaded list](http://hackage.haskell.org/packages/archive/thrist/0.2/doc/html/Data-Thrist.html) is probably a canonical example. EDIT: for example the following is a "thrist" of tuples: Cons (1,'a') (Cons ('b', True) Nil) :: Thrist (,) Int Bool notice that only the "right- and left-most" types are visible 
I'm told that as well, though I've never followed up on it. Of course, one major difference is that GADTs define closed type families (in the general sense of the term TF, not Haskell's version per se) whereas typeclasses define open type families. So if you're using GADTs in a way that you require closedness, I'm not sure if finally-tagless will capture that case. I think I'll check it out and see if I can verify this over the break next week.
You can do this without GADTs for any family of function types which (1) is parameterized by a finite set of type variables, and (2) has a finite bound on the arity. Just pass all the variables as type parameters, and then 'mono'-morphize the constructors to use all specific combinations of variables you want to allow. It won't be pretty, but it's doable. If you need infinitely many variables, need unbounded arity, or need it to be pretty/uniform, then I think you will need type equality constraints and hence GADTs.
Is Facebook going to start using more Haskell internally?
The methods of the typeclass correspond to constructors of the data type, and instances to algebras over it. It's not clear to me it's the same as a GADT, but it should provide a closed representation.
because of hiring one developer??
It is merely the Church encoding of GADTs. If we have a GADT: data Foo i where Bar :: Foo Int Baz :: Char -&gt; Foo String Quux :: Foo a -&gt; Foo b -&gt; Foo (a, b) Then the fold is: foldFoo :: forall (f :: * -&gt; *). f Int -&gt; (Char -&gt; f String) -&gt; (forall a b. f a -&gt; f b -&gt; f (a, b)) -&gt; Foo i -&gt; f i The Church encoding of `Foo` is gotten by eliminating the `Foo i` from that signature, and using it as the implementation of the data type. The paper you referenced just packages the first three lines in a type class: class FooD f where bar :: f Int baz :: Char -&gt; f String quux :: f a -&gt; f b -&gt; f (a, b) and then uses `forall (f :: * -&gt; *). FooD f =&gt; f i`. It uses a type class (in Haskell) instead of a record or my expanded type above. I don't think this is any different than the observation (going back to Girard, at least) that System F has encodings of inductive types, and that includes GADTs if you up things to F_ω (such that you can actually have higher kinded types). (And the calculus of constructions has weak encodings of inductive families and the higher inductive types the homotopy folk are interested in).
Isn't this just using existential types? data PTree in out = forall t1 t2. PBin (t1 -&gt; t2 -&gt; out) (PTree in t1) (PTree in t2) | forall t. PUn (t -&gt; out) (PTree in t) | PV (in -&gt; out) 
These should be able to represented using just existential types. The key power that GADTs bring over existential types is the ability to constrain the type of the constructed value beyond universally quanitified type variables. For example, `data X e where MkPair :: a -&gt; b -&gt; X (a,b)`, or equivalently, using type equality constraints and existential types instead of GADTs, `data X e = forall a b. (e ~ (a,b)) =&gt; MkPair a b`.
Actually, even though Oleg makes an excellent job advertising the "Finally Tagless" approach, the basic technique of using (something like) church encodings of GADTs using typeclasses has been presented before. What Finally Tagless shows is a nice application (tagless interpreters with HOAS) of those previously reported techniques. The "Generics for the Masses" paper by Ralf Hinze is to my knowledge, the first time that this technique has been used (see his the Journal of Functional Programming version of this paper for a very nice description). In another paper (Typecase: a design pattern for type-indexed functions), this pattern of simulating GADTs with type classes is described in more detail and several different applications of it are presented. See: http://ropas.snu.ac.kr/%7Ebruno/papers/Typecase.pdf There's also "Extensible and Modular Generics for the Masses", which is where it is shown, for the first time, that if you use these type class encodings, you get extensibility (which you don't have in regular GADTs). 
See PequalsNP's link. Facebook has been internally using Haskell for quite some time, not to great extent though as it seems. 
Your data type is less general than mine with GADT. You can only use unary functions of type (t -&gt; out), and terminal leafs of type (in -&gt; out). With the GADT tree you can have: bin1 :: a -&gt; b -&gt; output un1 :: d -&gt; a un2 :: e -&gt; b v1 :: input -&gt; d v2 :: input -&gt; e And create a tree of type *PTree input output* with: let pt = PBin bin1 (PUn un1 (PV v1)) (PUn un2 (PV v2))
Maybe *bos* is equivalent to more than *one developer*...? :-)
I think that for someone "coming from Haskell", yesod's quasi-quoted web routes seem mysterious and complicated. But for someone coming from a clean slate or from another language, I think that the quasi quoted routes are no less foreign than anything else and are actually easier to handle than mysterious combinators.
I hear what you're saying. However, there are relatively few shops open minded enough to consider using Haskell in production. The ones that are willing to consider it are also probably smart enough to understand the tradeoffs between the bare-metal optimisation and the programmer efficiencies to be gained by using a *much* higher level language. (but maybe that's what you're trying to say anyway!)
Ah yes, that was the correspondence. So yeah, since typeclasses have a closed number of methods, that makes them analogous to GADTs with a closed number of constructors; the openness of instances is just the openness of being able to define many different case analyses. I can never seem to keep finally-tagless rooted in my brain. Perhaps it's a problem of being "too simple" and so I keep missing that it's a novel idea in the first place. Anyways, I think I'll still go and re-read the papers over the break.
&gt; It's not clear to me it's the same as a GADT, As I said, I'll have to re-read the papers, but I'm pretty sure they are (at least close enough). In particular, since we're encoding things by their eliminations then we're either doing Church encoding or Scott encoding. The main difference is that Scott doesn't incorporate the recursive principle, but in Haskell it's trivial to add that in since we allow arbitrary recursion. Since Church encodings and Scott encodings + fix are isomorphic to ADTs, the only thing we have to worry about is adding GADTs' type equality constraints. But since we're presenting the elimination of data constructors, we're the ones in control... data E :: * -&gt; * where Lam :: (a-&gt;b) -&gt; E (a-&gt;b) App :: E (a-&gt;b) -&gt; E a -&gt; E b class EAlgebra (e :: * -&gt; *) where lam :: (a-&gt;b) -&gt; e (a-&gt;b) app :: e (a-&gt;b) -&gt; e a -&gt; e b It doesn't matter to us that `e` is parametric, because we (the class writers) get to choose the parameters and more importantly we get to change our minds as we recurse. Any term which is abstract in `e` must be constructed entirely using the type class methods and parametric polymorphic functions, thus it is isomorphic to terms constructed by the data constructors of `E` and parametric polymorphic functions. The GADTy-ness doesn't change anything from the usual Church/Scott encoding technique. The only thing I can think of off-hand that might break this is once we start moving to constructors with arguments whose types use the type being constructed in non-strictly positive positions (e.g., `foo :: (e a -&gt; b) -&gt; e b`). Of course, those are problematic even in the GADT presentation.
Not quite (if I'm understanding your pseudocode correctly). `Equal` is like `(==)` except at the type level. Thus `(Equal Bool a)` says that the *type* variable `a` is equal to `Bool`. Whereas when we say `a :: Bool` we're saying that the *value* variable `a` belongs to the type `Bool`. So with `(Equal Bool a)` we know that `a` is in fact the atom `Bool`, or some type-level expression which reduces to that atom; whereas with `a :: Bool` we know that `a` is the atom `True`, the atom `False`, or some expression which reduces to those atoms. Just to make the picture complete, we can ask what sorts of values will have type `(Equal Bool a)`. By Curry--Howard, the values will be the proofs of the fact that `a` is `Bool`.
&gt; and since the program is profoundly prevented from making assumptions about the underlying hardware. That almost sounds like the exact opposite of what you would want when writing code for HPC.
This gets back to the issue of how smart the compiler can be, and how well it can understand the semantics of the application program. As a simple example consider how easy it is to make the __map__ operation parallel, because the compiler and library know that the calling program cannot depend on what order the element mappings are computed in.
Turing Completeness only talks about one aspect of what programming languages do: computation. Other aspects not covered: FFI capabilities, performance capabilities, type safety. So language features allowing the latter can allow things that Turing Completeness alone does not achieve.
Does this means that Repa would automatically get automatic vectorisation support, as it uses vectors to store its data?
SBCL compiles everything and always.
Is there a video of the presentation?
there should be eventually
&gt; The only thing I can think of off-hand that might break this is once we start moving to constructors with arguments whose types use the type being constructed in non-strictly positive positions (e.g., foo :: (e a -&gt; b) -&gt; e b). Of course, those are problematic even in the GADT presentation. That's one sort of thing I was worrying about. The other is implementing operations that return values in the GADT. It seems to be possible, but excessively difficult. With the GADT it's easy to write data AnyE = forall a . AnyE (E a) appL :: E a -&gt; Maybe AnyE appL (App l r) = Just (AnyE l) appL _ = Nothing GADT values can be represented by a definition like newtype E' a = E' (forall e . EAlgebra e =&gt; e a) but you also have to reconstruct the value data AnyE' = forall a . AnyE' (E' a) data AppLAlg a = AppLAlg (E' a) (Maybe AnyE') instance EAlgebra AppLAlg where lam b = AppLAlg (E' (lam b)) Nothing app (AppLAlg (E' l) _) (AppLAlg (E' r) _) = AppLAlg (E' (app l r)) (Just (AnyE' (E' l))) appL' :: E' a -&gt; Maybe AnyE' appL' (E' e) = case e of AppLAlg _ r -&gt; r
Does anyone ever present Haskell "framework" with more complicated example than blog site? SQL/ORM parts show their weaknesses when you have 20+ tables, stored procedures and so on. Am I right to assume that in "real" case I should just go back to HDBC or HaskellDB? Generating binding data types from DB is also a must have.
There's a good reason to use a simple example in an hour long demo: there isn't time for anything else. And I think your assumption is not correct. I have sites with dozens of tables, and it runs without issue. Also, did you read the slides? We do generate binding data types. Unless I'm misunderstanding you.
IMO not the fact that 'Thrist' is a GADT is interesting, but the possibility to instantiate it (the first parameter) with a GADT. Above you use (,) which is an ADT, obtaining a category of relations. When you need a domain specific language that requires sequencing, you can specify a GADT to define the language's vocabulary and work out specific types for it's "verbs", so you can constrain the usage of your "verbs" according to certain semantic boundary conditions. After constructing your thrist, you can either directly execute it (in a monad) or translate it to some other (untyped?) representation and execute it there.
You need to compose the two functions: b = (== 0) . m When you want to apply f to a value, then apply g to that result, you write g . f The (== 0) is a "section": A curried binary operator application to one value, giving a function that takes the other argument.
Thanks, that's what I'm looking for :) But now I have a new problem, a need to reverse the parameters in (mod), because (mod 2 8) is different from (mod 8 2). Is this even possible? *EDIT1:* now removeMultiples [1,2,3,4,5,6,7,8,9] returns [1,2] *EDIT2:* Solved :D, my solution is: removeMultiples :: Int -&gt; [Int] -&gt; [Int] removeMultiples d l = let rmod a b = (mod b a) m = rmod d b = (/= 0) . m in filter b l
http://imgur.com/x3PYP su[bp]scripts work in proofgeneral, so it's probably easy to add them to the mix.
Yeah, I get your point now. I was conflating (::) with (==) at the type level, which is not the correct analogy.
GHC even supports reading in a lot of Haskell's syntax as unicode, and then there's a package on hackage that gives you the prelude in unicode goodness (badness?). The only issue is lambdas, which can't be given the lambda symbol because it's a regular letter in the greek alphabet, and could be used as an identifier.
Can't we make the lambda symbol an exception? Using lambda as an identifier in Haskell code would be very confusing (probably even for a Greek).
I'm not sure exactly where Unicode support is right now, but at least a fair amount of these features are supported in source code itself by ghc (I tried writing in this manner in some code I played with). Greek, arrows, forall, comparisons and I read later in a feature request, superscripts/subscripts,as well as unicode combinators like ∘ should at least work .. It's not any more complicated to type them in either, say with a compose key and a couple of extra key combinations.
I don't like any sort of prefix for lambdas. What is wrong with just `x -&gt; x + 1`. Yeah it can require unbounded lookahead, but that's not really a problem for parsers these days... if C# and Scala can do it, Haskell can too. :)
Reay? It seems weird to te peope to avoid using certain common etters (in their anguage) in their variabe and function names because of a historica typographica accident. Sure, most peope, even from non-Engish-speaking countries, write Engish code, but I'm not sure we shoud activey discourage them from writing in their own anguage. A more paatabe option might be to compicate the parser so that we can use ambdas in both paces.
also backslash is usually hard to reach on most keyboards.
I'm happy enough with emacs/yi doing font prettification on regular text source.
Sure is. Look into flip: flip :: (a -&gt; b -&gt; c) -&gt; b -&gt; a -&gt; c So, what you're looking for is flip mod 8 :: Integral a =&gt; a -&gt; a flip mod 8 $ k = k `mod` 8
I agree with godofpumpkins, but note that there are several lambda symbols in Unicode other than the lower case Greek letter. There are several mathematical ones, though they're apparently all either bold, italic, sans or combinations thereof. Compare: 𝛌𝜆𝝀𝝺𝞴 vs λ for the Greek one.
I've encountered some libraries on hackage that use greek symbols in types, so ghc error messages have weird squiggly letters that I don't normally see on my terminal. This does not make it easier to reason about a type error message, IME.
It was more of general remark, after reading the slides I looked up persistent on Hackage and docs related to it. Yes, I've noticed that this presentation mentions many aspects of the framework so time for DB part was limited. I meant something like HaskellDB generator: takes connection string and generates Haskell code for entities defined in DB by querying information schema. Maybe that could be even done with $(generateEntitiesPgsql "conn_string")? "it runs without issue" - sure, it may for you. I meant that people can have DB design based on stored procedures and ORM that doesn't support stored procs/funcs and requires writing manual SQL everywhere is practically useless. There are lots of differences between programming languages and ISO SQL for instance time interval is usually represented using seconds in programming languages and ISO SQL has several different modes like months (different interval length in seconds depending on between which months it is). SQL also has CREATE DOMAIN and/or CREATE TYPE (and many more) which most ORMs get wrong.
The [benefits](http://s1.directupload.net/images/111117/7wu38jhm.png) of Unicode...
The presentation was recorded by the conference, but I don't know what their schedule is for releasing videos (or even if they will release it at all). If there is interest, I could give the talk again as a webinar/screencast. I'm not available for the next few weeks (traveling back home/catching up on work), but if you want to, be in touch and we can try and make this happen.
I wonder if there is anything similar in Vim.
There's Haskell Cuteness from vim. See: http://www.vim.org/scripts/script.php?script_id=2603
Thanks, my new solution is: removeMultiples :: Int -&gt; [Int] -&gt; [Int] removeMultiples d l = let m = (flip mod) d b = (/= 0) . m in filter b l
You want to use (\`mod\` d) here. removeMultiples :: Int -&gt; [Int] -&gt; [Int] removeMultiples d l = filter ((/= 0) . (`mod` d)) l Which can be shortened further like so: removeMultiples d = filter ((/= 0) . (`mod` d)) The rest of this is just a stunt, really: removeMultiples = \d -&gt; filter ((/= 0) . (`mod` d)) removeMultiples = filter . \d -&gt; ((/= 0) . (`mod` d)) removeMultiples = filter . \d -&gt; ((.) (/= 0) (`mod` d)) removeMultiples = filter . (((.) (/= 0)) . \d -&gt; (`mod` d)) removeMultiples = filter . (((/= 0) .) . \d -&gt; (`mod` d)) removeMultiples = filter . (((/= 0) .) . flip mod) removeMultiples = filter . ((/= 0) .) . flip mod 
Honestly, I don't understand compsci-type people's obsession with the lambda. Ok, we have history, it's nice and everything, but as a notation, lambda is horrible. Not speaking about how valuable a letter the greek lambda is in our limited mathematical alphabet. Personally I much prefer Haskell's backslash for lambda than an actual lambda!
Especially considering the lambda notation was a historical accident anyway. Typographically, I think the backslash is hideous, but I also don't think we should prevent people from using lambdas in identifiers. I wonder how feasible it would be to complicate the parser to support (unambiguously) a lambda in both positions.
[I insist](http://alaska-kamtchatka.blogspot.com/2010/04/properly-bound.html): the arguments to `bind` should be in applicative order.
&gt; Plain ASCII is beautiful Right: [The Methodology and an Application to Fight against Unicode Attacks](http://cups.cs.cmu.edu/soups/2006/proceedings/p91_fu.pdf)
By the way, you can use `mod` (or any other function) as an infix operator by surrounding it with backticks, like so: 100 `mod` 8 == 4 The reason I bring this up is because you could write `flip mod d` more clearly as (`mod` d) and then your `b` becomes (/= 0) . (`mod` d) which I think is quite natural.
This is actually probably better than my reply. Have an upboat.
≡ typically means congruence rather than equality 
I think you'll find that this works just as well with the existential version. Note the values of the recursive elements in PUn and PBin: forall t. PUn (t -&gt; out) (PTree in t) forall t1 t2. PBin (t1 -&gt; t2 -&gt; out) (PTree in t1) (PTree in t2) If you define this data type in a file using `{-# LANGUAGE ExistentialQuantification #-}`, load it in GHCi, and type `:t PUn`, you should see PUn :: forall in out t. (t -&gt; out) -&gt; PTree in t -&gt; PTree in out which is exactly the same type as the data constructor you gave in the GADT. Keep in mind that 'in' and 'out' are type variables, they are different for each instance of PTree you see!
https://github.com/frerich/unicode-haskell
&gt; P.S. Why don't we store the lexical tokens in a file, and have the IDE figure out how to represent it? That's what [Haskell-conceal](http://www.vim.org/scripts/script.php?script_id=3200) does in vim.
&gt; I know I can solve the 'removeMultiples' an other way. But I like to know how to add the 2 functions (mod) and (==) together to create a new function Int -&gt; Bool that I can use with (filter). I may be off-topic (and that is a matter of style), but it's probably simpler if you define the predicate directly (`let p x = x \`mod\` d == 0`).
I was using it to mean 'equivalent' as (==) is generally a test for structural equality (as in the case of lists) rather than a stricter test for syntactic or physical equality.
There's a [vimscript](https://github.com/Twinside/vim-haskellConceal) for that which uses Vim 7.3's `conceal` feature, which I find quite handy. It doesn't touch the original code, so the code remains portable (which is the big problem of modifying code source-level.) You see the unicode versions on every line but the line you're currently editing. There's a big problem though: it sometimes screws up indentation. I'm willing to look past it though.
*Edit: Nevermind. :D*
In Agda, ≡ is used for propositional equality. Even in latex, ≡ is \equiv, while ≅ is \cong.
...but `foo x \y -&gt; z` is already not valid syntax.
Hmm. I would've sworn I've seen examples used like that.. weird. :( Oh, maybe I'm thinking of things with infixes?? yeah, so like with &gt;&gt;= you often just write xs &gt;&gt;= \x -&gt; y
You can use =&lt;&lt; instead of &gt;&gt;= if you want to, but most of the time it is more readable using &gt;&gt;=.
Thanks. You sound kind of busy though and egreg says there will be so it might be better to wait.
This is great, except when I press spacebar at times, it becomes laggy. I don't know if this is true for everyone. I prefer https://github.com/Twinside/vim-haskellConceal suggested by adimit.
I don't think there's a problem. I didn't notice until "paatabe". ;-) But seriously, you don't have ban lambda completely, only the the identifier consisting of exactly one lambda. But anyhow, dcoutts has a better solution, and 𝜆 looks better on my screen than λ.
Ah, the myth of non-textual editors :) I'd love to see that one day too.
[IDN homograph attack (en.WP)](http://en.wikipedia.org/wiki/IDN_homograph_attack): &gt; The **internationalized domain name (IDN) homograph attack** is a way a malicious party may deceive computer users about what remote system they are communicating with, by exploiting the fact that many different characters look alike, (i.e., they are homographs, hence the term for the attack). For example, a person frequenting citibank.com may be lured to click the link [сitibank.com] (punycode: xn--itibank-xjg.com/) where the Latin C is replaced with the Cyrillic С.
The statements about closures should be made using inclusive subset, ⊆, not strict subset, ⊂.
⊂ doesn't *have* to mean strict subset. Depends on the author.
Guy Steele's Fortress language has a pretty printing mode that uses nice mathematical symbols like this. That seems a nice compromise way to be able to typeset really nice looking program text but still deal mostly with ASCII when you are typing/editing.
Well, this one didn't specify…
The metaphor doesn't make sense. What is the point in tagging a value with a useless box containing machinery I don't need? Remove the unconditionally present display at the exterior and you might get somewhere.
In Persistent you declare the schema and migrations can be performed automatically. This is a superset of what you have in mind for generating entities. I don't see what would stop stored procedures from working, although I don't know that anyone has tried them. Persistent does not support custom SQL types, which sounds like a deal-breaker for you.
My beef is only with `bind`; `&gt;&gt;=` is perfect just as it is.
Ah good the URL works now for me - nice slides!
But the two xs are different syntactic elements. One is a formal parameter to create a binding and the other is a variable. "Escaping" one with \ or λ makes perfect sense to me.
I guess you missed his point here..
I use a custom literate preprocessor, supported by GHC, that can handle this sort of thing, and I've gone back and forth on λ. Backslash is actually prettier here, λ is just being a prisoner to a slavish convention. John Nash once published a paper using every Greek letter, and he was quite proud of this feat. I used bits of this paper on the dorm windows in "A Beautiful Mind", and 0 &lt; π &lt; 1 ended up on a widely used publicity shot. This drove the painfully conformist types predictably mad, everyone knows π = 3.14159, right? λ is a letter for a reason. Many reasons, everyone has a different one. Leave it that way!
You are partially right. First, I really mixed the 'in' and 'out' variables contexts. But your data type still do not guarantee the right types in the tree nodes. You can create something like: bin1 :: a -&gt; b -&gt; output un1 :: d -&gt; b un2 :: e -&gt; a v1 :: input -&gt; r v2 :: input -&gt; s let pt = PBin bin1 (PUn un1 (PV v1)) (PUn un2 (PV v2)) Because of this you can't write a function ptreeToFunc (otherwise you could create a function that didn't type checks!).
http://hpaste.org/54221 PTree.hs:45:31: Couldn't match expected type `D' against inferred type `R' In the first argument of `PV', namely `v1'' In the second argument of `PUn', namely `(PV v1')' In the second argument of `PBin', namely `(PUn un1 (PV v1'))' (after commenting out incorrect thing) Prelude&gt; :l PTree.hs [1 of 1] Compiling PTree ( PTree.hs, interpreted ) Ok, modules loaded: PTree. *PTree&gt; ptreeToFunc (PBin bin1 (PUn un1 (PV v1)) (PUn un2 (PV v2))) () 0
Yes, my mistake, I defined: forall t1 t2. PUn (t1 -&gt; t2) (PTree i t1) when tried your code and this caused the confusion. Looks like your solution using only existential type achieves the same goal as mine!
I avoid using Haskell packages which use Unicode syntax and Greek letters for type parameters. I realize that this position isn't rational and a lot of the contributions are quite good. And who am I to bitch about the hard work others have decided to share with me for free? I should get over it.
but otto_s is right, \subset is (almost) just as common as \subseteq for inclusive subset, especially in analysis. In this case, \subsetneq is sometimes used to state noninclusive subset.
Very neat. &gt; If we imagine that ⊂ is a special kind of implication, the similarity with the monad laws is clear. Shouldn't it be "monad operations" instead of "monad laws"?
What is bind if it's not &gt;&gt;=? Maybe you'd be satisfied if Monad was like Eq in that it could be recursively defined with minimal complete definition requiring either &gt;&gt;= or =&lt;&lt;.
Man, I'm new to functional languages in general, let alone monads. That explanation went soaring above my head. 
The metaphor really doesn't make sense. Watch how he stops using the metaphor immediately as he gets to concrete examples like Maybe, List, State. That's because their semantics doesn't fit the metaphor.
Isn't leksah doing some of this?
Yup, but I think it only changes the way certain character sequences are displayed. So the files saved are plain ASCII. Out of the box it renders (at least) '\' as 'λ', '-&gt;' and '=&gt;' as nicer-looking arrows, '.' as a centered dot and the identifiers 'alpha' and 'beta' as their greek equivalents (maybe even '++' as a single operator). I seem to recall that all this is configurable, but I haven't played with that feature.
Conversely, what is `&gt;&gt;=` if not `bind`? The former is notation, syntactic sugar, after all.
Sorry I don't follow. Firstly, how is asking whether &gt;&gt;= &amp; bind the same thing any different when asked the other way around? And secondly, while do notation is syntactic sugar for &gt;&gt;=, &gt;&gt;= is not sugar for any thing and is required to define a monad instance.
Surely you're right and I'm wrong. I was under the mistaken impression that `Monad` literally used `bind` as primitive and `&gt;&gt;=` as m &gt;&gt;= k = bind k m whereas I now realize that there's no `bind` in Haskell except for the name of `&gt;&gt;=`. Sorry for the confusion.
I think emacs already does a lot of this. A friend of mine uses it and was talking it up. Frankly, I can't stand it. Unicode symbols are likely to be found prettier only by people who came from the math world. The only one of your examples that I like is the subscript one. But I strongly dislike the underlying x_1 notation. You should be able to do exactly the same thing without requiring the underscore, which would make the non-prettified code would look vastly better IMO. 
I've made a terrible, terrible mistake :/
I am more of a fan of the "monads as computation" analogy. The basic structure of a monad is that 1. You can kickstart a new computation with "return" 2. You can add an extra step to a previous computation with "&gt;&gt;=", aka "bind" The really neat tricks come from what you *cannot do*. If you notice, in general, there are no ways to 1. Merge two monads or inject a monadic value into an ongoing monadic computation (the only way to build up new monads takes either 0 arguments (return) or 1 monadic argument (bind)) 2. Get values trapped in the monad back outside. (The only way to access a monadic value is passing a funtion to be run inside the monad, via bind) Some monads, like lists and Maybe, exploit particular properties of themselves to allow you 1 and 2, but others, like IO *don't*. So via 1 we can deduce that every IO value can only be constructed via a linear *ordered sequence* of steps. Via 2 we know that the only way to "run" an IO value is to pass it to the unique and special "main" function. This, coupled with 1 means we can secretly use mutable sequential state behind the scenes because the sequentiality means we can use the same "order of computation" assumptions from traditional programming. --------- Of course, this might all initially seem like just a convoluted way to do something you could already do before without trouble. It was only after I witnessed the callback hell of asynchronous Javascript programming (a situation where it is *not* possible to fallback on traditional synchronous and sequential code) that I really groked the purpose of monads (carefully hidden inside the Promise APIs popping up in many JS libraries).
I agree that the underlying subscript syntax is fairly ugly. This notation would also work for alphabetical subscripts, so I figured it made sense to be consistent.This is also the syntax used in LaTeX for subscripts. I'm not sure what the right trade off is here though. Do you know of a way to get the prettier syntax *and* accommodate non-numeric subscripts *and* be consistent? This was the best I could do.
This is a [cross post from Stack Overflow](http://stackoverflow.com/questions/8184196/how-to-avoid-converting-among-different-string-types-in-haskell-using-snapfra). It's usually better to just post the link to SO rather than duplicate the discussion here.
The problem is that it [requires unbounded lookahead for programmers](http://arcanesentiment.blogspot.com/2010/09/is-infix-lambda-hard-to-read.html), too.
How is this relevant to code?
I really dislike that approach to explaining them, because it gives people really wrong ideas. Your properties 1 and 2 are not properties of the monad, because for one thing "properties of the monad" isn't a very sensible thing to say, because it implies that there is some other thing "monad" that can have "properties" when in fact the "monad" _is_ the property of being able to conform to the monad interface. Then you then have to back out of the explanation and explain why it's not really true because it actually depends on the underlying data structure and its properties. I don't see the pedagogical advantage of putting a false property declaration based on certain special cases on something, then explaining why it's false as if the falseness is somehow an "exception". The ability of a data structure to be monadic does not in fact imply that the structure is a burrito or spacesuit. Burritos and spacesuits may have the property of being monadic, but things that are not burritos or spacesuits do too. Such as Maybe or List. IO doesn't let you extract values of itself because the IO datastructure doesn't give you that power, entirely independently of the fact it also has the monad property. (Monads were never burritos or spacesuits, and it's not a mystery to me why people would get the idea that they're horribly mysterious things when they were pointed at such bad, bad explanations of them.)
Do you have any example of bad indentation? I use http://www.vim.org/scripts/script.php?script_id=1968 And it seems to works as expected. Or is it only a display issue? 
Argggg. Madness.
OK that's it. I've had enough. I am writing my own blog post on monads. I'll name it "Monads are like Post Office"
Yet another incorrect monad tutorial using the incorrect container/"box" metaphor. Making more bad tutorials *promotes* fear of monads.
&gt; While the vector and bytestring should in theory yield the same performance since they’re both arrays, the bytestring get a slight boost for O and O2. This might be due only to the overloadedStrings extension which allow compilation times representation of the packed array. Does the `OverloadedStrings` extension really allow to pack the string at compile-time? I was under the impression that OverloadedStrings was just syntactic sugar for converting string-literals by inserting calls to `fromString` which would get evaluated at runtime-time...
I don't think it's the extension itself that is doing it, but through the fromString, i think it allows ghc to optimise it as an Addr# primitive. you can look at the output of a module through ghc-core, an unpack list would look like lots of closure pointing to each other in the assembly: V_hex240_closure: .quad ghczmprim_GHCziTypes_ZC_static_info .quad V_zuzu_closure+1 .quad V_hex241_closure+2 .quad 1 whereas with a bytestring with overloadedStrings would looks like a sequence of byte in the assembly: .section .rodata .align 8 c128_str: .byte 195 .byte ... .byte ... ...
Module A implements "funｓ" and module B implements "funs". You read the documentation and even the implementation of funｓ(), because it's an important and security-relevant part of the code. In B you use only the safe and trivial foo() function, the compiler is smart and there are no name clashes, so we import A and B. But damn, you missed the small difference and call the evil "funs()".
&gt; but since we know we are doing safe indexing we can replace the indexing operator (!) by the unsafeIndex operation. Since the Haskell Char datatype has a range much greater for than a single byte you are in fact not doing safe indexing. Trying to look up a unicode character would go out of range of the vector.
If you're using malicious libraries like that, your code is insecure to start with. If you're implying it's an [Underhanded C Contest](http://underhanded.xcott.com/)-esque code review risk, it is not difficult to write a program that highlights such abuses. Browser vendors have this problem, for instance.
There's a rewrite rule for unpackCString# that's supposed to make constructing a ByteString from a literal string an O(1) operation. However, I think it's broken in recent GHC versions (there's a bug on Trac for this issue). 
i mentioned that i'm looking only at ASCII "Char", and for conveniency i used Char where it should be Word8. &gt; The Benchmark itself is using Char for conveniency, where it should use Word8 to be accurate and safe. 
&gt; If you're using malicious Doesn't have to be malicious, similar functionality but broken (according to the specification: without authentication, doesn't scale, different return values...) implementation is enough. &gt; libraries like that Most libraries are of bad quality, that's the problem and if a chinese, russian or whatever programmer uses the wrong ~~ｓ~~ s isn't unimaginable. What if you want to call a function named in an uncommon range of Unicode? Take the first symbol which looks similar?
Must have missed that somehow when reading. My apologies.
The need to reconstruct the value is the cost of doing business with Church encodings. But it's always possible, so it doesn't affect formal power. That's part of the reason I'm such a fan of Scott encodings. Because Scott uses the case elimination rather than the catamorphism elimination of Church, it's easy to just unpack one level without performing the identity traversal on the rest of the term. The downside is that Scott encodings don't come for free in System F, they require a fixpoint operator in order to define the catamorphism.
&gt;Doesn't have to be malicious, similar functionality but broken (according to the specification: without authentication, doesn't scale, different return values...) implementation is enough. So we have an action (a value of type `IO ()` is not a function) named funs and an action named funｓ that both don't take any inputs at all (so they're either of extremely limited flexibility, or are using global state, which is rare in Haskell...), and do the *same thing* but one is insecure? And both are in scope? This is incredibly contrived! Full-width characters like ｓ are only used in Japanese texts, anyway; they're for compatibility with Shift-JIS. So the whole name would be full-width, if anything, and that would be very noticeable (but it'd also be a bug in the library to use a name like that as opposed to the non-full-width characters...). &gt;if a chinese, russian or whatever programmer uses the wrong ~~ｓ~~ s isn't unimaginable. You haven't provided a compelling example. This single-full-width-character mistake could only happen if someone were typing in names by, I don't know, googling the names of codepoints and copying them in one by one. There are far easier, more plausible, and more serious errors one can make. Anyway, with Haskell mixing up two values will almost always give an error at compile time, since it's unlikely that the types match. And I don't know of a single bug that has arisen because Haskell lets you name a value `unsafeLaunchCafés` (which it has since 1998).
It's possible to reconstruct the term in general. I had some worries that getting the types right for that sort of thing would be difficult if you were trying to model a GADT without using them, but using a newtype of a value universally quantified over instances seems to faithfully represent the GADT values.
&gt; that both don't take any inputs I haven't used Haskell types or syntax. () only means it's not a nullary function. &gt; I don't know, googling the names of codepoints Or googling the type and find the wrong function, read the wrong manual or the right one on paper and use the wrong characters... &gt; There are far easier, more plausible, and more serious errors one can make. And? Anything that can go wrong generally does go wrong sooner or later. Maybe someone manipulates your code and vcs and you miss the difference. &gt; since it's unlikely that the types match Yeah, two functions - server version (scales, but slower on small problems, and maybe named by funny guy with ｓ for "big server") and embedded version (small footprint, but doesn't scale) and I doubt that any type without semantics for server/embedded will protect your application.
In a [recent mailing list thread](http://www.mail-archive.com/glasgow-haskell-users@haskell.org/msg20523.html), Simon says: &gt; In GHC, big cases are done as tables (if dense) or trees (if sparse). So GHC itself is presumably constructing a lookup table behind the scenes. Given that, though, it would be surprising to me if, as you write, "when there’s too many cases, the compilation slow down massively and the runtime slow down too". While I don't know about compilation times, in terms of runtime performance it shouldn't matter whether you're indexing into an array of 5 elements or 5000 elements, should it? Or maybe it just causes more cache misses, which leads to the degraded performance? (Note: I haven't done any tests myself.)
But the escaping argument makes less sense when you allow multiple argument lambdas: `\x y -&gt; x + y`.
I know this isn't what you asked, but you're darn right compile time suffers! This is why I changed crypto-api's CPoly module (which had ~10K top level / pattern matched definitions for a single function) to using an array. Many users of memory-constrained machines (and there are plenty of them, it turns out) complained about needing 1.5GB to compile crypto-api. Even users of more powerful machines complained about how long it took.
((.)$(.)) The owl agrees.
I don't see a principled reason why it couldn't. The I/O manager might need some additional tuning but I don't see why we couldn't reach those numbers. He already get in the ballpark of 50,000 requests/second with heavy weight protocols like HTTP.
Ahh, yes. Don't know why I didn't think about alphabetic subscripts. I just know that there's absolutely no way I'd change to a less readable coding style just so the code will look a little different for a few people using a fringe editor.
Only if you [studied at Oxford](http://www.willamette.edu/~fruehr/haskell/evolution.html).
GCing might become a major problem, but I've never done anything like that, so I can only speculate. If they go to such length to prevent wasting cycles, it certainly sounds like it could become a major factor, though...
After reading about Disruptor I asked myself the same question. I did some tests, which are available here: https://github.com/Tener/disruptor-hs Preliminary results are interesting. The fastest queue as of now is located in Test.ManyQueue module (see here: https://github.com/Tener/disruptor-hs/blob/master/Test/ManyQueue.hs). In 1P1C test it achieves up to 40M msg/sec, in 2P2C it goes up to 100-120M msg/sec. Disruptor achieves 50M-130M msg/sec, depending on the mode it operates at. (All tests done on Linux raptor 3.1.0-4-ARCH #1 SMP PREEMPT Mon Nov 7 22:47:18 CET 2011 x86_64 Intel(R) Core(TM) i7 CPU 870 @ 2.93GHz GenuineIntel GNU/Linux) I didn't yet fully understand the way Disruptor works and how it's authors measure the performance, so the above numbers may not be fully accurate. They do however suggest, that Haskell has good starting point. Perhaps with some improvements made to GHC we could match Disruptor speed. Such improvements would likely benefit many users.
How can you do case eliminations of GADTs using a tagless encoding, though? For example, the case eliminator for E has type caseE :: forall x r. (forall a b. (x ~ (a-&gt;b)) =&gt; (a -&gt; b) -&gt; r) -&gt; (forall a. E (a -&gt; x) -&gt; E a -&gt; r) -&gt; E x -&gt; r It's the `(x ~ (a-&gt;b))` bit that seems difficult to encode. I guess you can encode it as a function: data Leibniz a b = TEq { eqFwd :: forall f. f a -&gt; f b , eqRev :: forall f. f b -&gt; f a } caseE :: forall x r. (forall a b. Leibniz x (a -&gt; b) -&gt; (a -&gt; b) -&gt; r) -&gt; (forall a. E (a -&gt; x) -&gt; E a -&gt; r) -&gt; E x -&gt; r
...wouldn't a similiar rewrite rule be possible for `Data.Vector.Unboxed.fromList` as well?
Avoid allocation at all cost, and the GC will never run. Difficult, but possible.
But could you still call that Haskell? :)
I wonder what the backstory here is. As Mailrank was partially funded with outside investor's help (according to bos's Strange Loop talk), I doubt they walked away from Mailrank just because Facebook offered them nice salaries. I suspect either Facebook may have bought out the investors or Mailrank was having cashflow problems. Just some idle speculation. Whatever the case may be, best of luck in your new endeavors! 
There was discussion 5 months ago http://hackage.haskell.org/trac/ghc/ticket/5218
I was referring more to the style of programming you are forced to use. 
Alas, I couldn't find any official announcement yet... However, the [notable changes since 0.9 wikipage](http://www.haskell.org/haskellwiki/Xmonad/Notable_changes_since_0.9) might be useful to see what's new
http://hdiff.luite.com/cgit/xmonad/commit?id=0.10
Well, you almost certainly have to give up laziness, but win you're after that kind performance, you shouldn't be attempting to compute anything at all you don't need to really.
An on-the-fly (concurrent) collector for the major heap would given low pause times at the cost of throughput. It'd be a chunk of work to implement though.
Fantastic. The bugfix for frozen launcher is finally here!
Nice use of laziness! You seem to be using lists as streams (you never pattern match on the empty list). Does performance increase when you use an explicit stream structure? As in: data MQueue a = Cons {-# UNPACK #-} !(MVar a) (MQueue a) newMQueue :: Int -&gt; IO (MQueue a) newMQueue 0 = error "Can't create empty MQueue!" newMQueue size = do lst &lt;- replicateM size newEmptyMVar let fromList [] = fromList lst fromList (mv:mvs) = Cons mv (fromList mvs) return $ fromList lst writeMQueue :: MQueue a -&gt; a -&gt; IO (MQueue a) writeMQueue (Cons mv mvs) x = do putMVar mv x return mvs {-# INLINE writeMQueue #-} readMQueue :: MQueue a -&gt; IO (MQueue a, a) readMQueue (Cons mv mvs) = do x &lt;- takeMVar mv return (mvs, x) {-# INLINE readMQueue #-} 
I would've put enumerators under "right".
Yep, I agree with that.
Looking at the diff, it seems like there is a lot more `Control.Arrow` usage.
You are right, I modified the post to hopefully add some clarity. That being said, enumerators (or maybe just how Yesod uses them) are not without pain, particularly that they complicate exception handling in a monad stack.
vav on #xmonad put up the headlines: http://hpaste.org/54271
Me, too, thanks for all the great works! I recommend Andy Stewart, who is an active developer of fancy "manatee", for a new maintainer. Hope that we will have another great maintainer! 
Probably this: http://hackage.haskell.org/packages/archive/base/latest/doc/html/Control-Arrow.html#v:-60--43--62-
I was hoping it would fix needing to use Xinerama for multi-display / switch to xrandr
One especially fun construct (don't know if it's mentioned here) is the one for Hamblin semantics, which is literally just the list applicative. Another is Shan's modification of that, which is a perverse mix of an applicative and a monad: app :: [a -&gt; [b]] -&gt; [a] -&gt; [b] app fs xs = [y | f &lt;- fs, x &lt;- xs, y &lt;- f x] 
It is just mappend defined in ManageHook.hs: (&lt;+&gt;) :: Monoid m =&gt; m -&gt; m -&gt; m (&lt;+&gt;) = mappend
Can you fire up that snippet in ghci and check its type signature?
Personally I prefer `(++) = mappend`. Doesn't break anything in my projects, and makes use of the many different string types a lot nicer.
I'm using xrandr with multihead without any trouble with 0.9.2.
In the last 3 minutes, nobody has posted "hoogle is your friend" ;-} http://haskell.org/hoogle/?hoogle=%3C%2B%3E
*tl;dr: Acknowledgement that Yesod gets right what it intends to get right, and whining about how crap the state of web dev is despite superior technologies having existed for decades, bitch and moan.* For me, Yesod (and Happstack and to some degree Snap) optimizes a way of working in the web dev world that has been the same for a long time. We’re still writing HTML, CSS and JavaScript, manually. So for me, what Yesod gets right is the usual web dev pattern, and in a Haskelly way in the sense that it does more correctness checking. I would be happier if Haskellers were innovating in web dev with more advanced systems like WPF, FRP, CLIM, and Cells. HTML, CSS and JavaScript as used typically are a pile of crap, and they breed the strangest plethora of frameworks and ‘best practices’. It reminds me of other cases where people are working with inferior technologies but they don't really know it, concretely, like back when people were using ASM and FORTRAN for large systems and coming up with all sorts of practices to cope with the fact their tool was just insufficient to handle the complexity. Then there are these steps that were made by Böhm and Jacopini, Dijkstra (“Go To considered harmful…”) to establish structured programming as actually a good idea. Now it's all obvious but back then it was a new idea. I think the same is waiting to happen in web dev, as it has started to already in desktop application development. It seems inevitable we'll reinvent, the web browser has taken a long time to get a rendering canvas and built-in video player, and a moderately fast scripting language, something desktops have had for decades and decades. The software industry is so messed up. It's 2011! It's embarrassing… Anyway, I have some ideas for WPF, CLIM and Cells, and I know plenty of people working on FRP for the web. I'm still dabbling with the types and dabbling with ghcjs at this stage. I'm not saying it's obvious how to implement this stuff, but it is obvious that the current standard is utter crap. The way people talk about web dev at the moment makes me think nobody realises this.
I always read things like [this](http://www.haskell.org/haskellwiki/Xmonad/Frequently_asked_questions#Multi_head_and_workspaces_.28desktops.29) which basically tell you to use Xinerama. I've never gotten it to work correctly without it, but using Xinerama with Catalyst prevents you from using multiple GPUs with OpenCL.
 class ArrowZero a =&gt; ArrowPlus a where (&lt;+&gt;) :: a b c -&gt; a b c -&gt; a b c
I think that's way too harsh. You are right that the current crop of popular web frameworks stay close to traditional approaches, but until we have the new approaches we all want, are we to totally give up on the web? The Haskell community is full of people looking for better ways of doing things, but this attitude -- that motivates many to pick up Haskell in the first place -- may be part of why we have a poor history of, for example, GUI development. It is deeply disappointing when a newcomer asks on #haskell how to build a GUI app, and they receive scattered suggestions to try an FRP framework along with warnings that it probably won't work. I would rather Haskell at least had a reasonable, pragmatic GUI story such as Racket's before trying out exotic new approaches. The big web frameworks we have today should be seen as incubators for the new approaches we all want to see. By testing these new approaches in the crucible of real, production web development, we will learn what works and how it fits -- or doesn't fit -- with existing practices. Perhaps this piecemeal approach to progress is less likely to overturn all the things you don't like about current web development in one fell swoop, but it means that professional web developers can use Haskell today. Not only can they be productive, they can also gain experience with Haskelly templating systems and novel approaches to modularity (e.g. snaplets). I am optimistic that our community will produce great alternatives to writing HTML/CSS/JS, and I'm thrilled that those new technologies will be able to slot in to production-ready web stacks.
I may be dense, but where does it suggest xinerama instead of xrandr?
Under problems it only really mentions Xinerama. Also in various other places I've only seen mention of it and never Xrandr
You're right, its not Control.Arrow related, its defined in Xmonad's [monoid typeclass](http://hackage.haskell.org/packages/archive/xmonad/0.10/doc/html/XMonad-ManageHook.html#v:-60--43--62-). 
I was meaning to send this to the mailing list at some point, but here's as good a place as any. I really don't like Persistent and think it's not something that a lot more effort should be spent on. In a nutshell, it's an Object-Relational Manager, _when you have no objects_. The OO world is coming around to the idea that ORM is an antipattern even in their world, and trying to drag it into Haskell is dragging it even further from where it _might_ make sense. There are some nifty little aspects about it, but ORMs are a tarpit. They suck up endless amounts of effort trying to bridge that OO/Database gap, which is a bizarre way to take a Haskell library when that isn't even the relevant gap, and the Database/Functional gap is probably somewhat smaller. I'd rather see a good query wrapper like LINQ, then a combinator library built on top of that, or something. ("So do it yourself." I lack the time, and while this would be an interesting and worthy project I've got others higher on my personal priority scale. To be honest my goal here isn't to tear down or demand that somebody build me something, my real goal is to try to help some people avoid the aforementioned tarpit that ORMs inevitably become. (I _have_ written one in Perl, so this I do have some experience with.) They get to be not-fun.) I _love_ Hamlet. Love, love, love it. It's what template languages should have been for HTML since day 1. Templates should _always_ have been conceived as parsed representations of HTML that always knew where in the HTML they were so they could automatically and correctly apply the correct level of encoding, instead of dead strings being concatenated together. Much pain could have been avoided.
There's Xinerama the extension and Xinerama the library. Xinerama the library is the only way to access information about multiple screens in X, and xmonad uses it. Xinerama the extension has mostly been replaced by RandR, but the library interface to the information provided by RandR is still called Xinerama. Hope this clears some things up.
Not easily possible unless you add a ``Show a`` constraint to ``f``. If you add that, you can do something like: import Debug.Trace f x | trace (show x) False = undefined f x | p x = ...
Keep in mind that it is an ordinary function, so its definition can vary by context.
I don't think this is a debugging question, in which case this is a bad idea...
One option would be to construct an infinite list containing the successive applications of `g`. This gives you a bit more introspection power. Check out [iterate](http://hackage.haskell.org/packages/archive/base/latest/doc/html/Prelude.html#v:iterate).
Monoid is defined in base, not xmonad. xmonad just defines a more convenient name for one of the class methods.
It doesn't seem very similar to Puppet or Chef (they poll for changes and then apply them). It's very similar to [mass](http://lerp.com/~sic/mass/) and some of the stuff listed on that page under Similar programs. Nice that it's coded in Haskell, I'll have to check it out next week.
thank you so much. I'm pretty sure I can use this. Even if I can't it's a very interesting function.
Well, I don't know about it, but it works absolutely fine with xrandr so far. Maybe it's just less complicated to setup?
 printInput :: Show a =&gt; (a -&gt; b) -&gt; a -&gt; IO b printInput f x = do print x return (f x) f :: a -&gt; a f = ... debugF :: a -&gt; IO a debugF = printInput f
 Isn't this just the reason why God gave us monads? -- In this case `IO`: fio :: Int -&gt; IO Int; fpure :: Int -&gt; Int fio x = print x &gt;&gt; if x &gt; 10 then (fio &lt;=&lt; gio) x else return x fpure x = if x &gt; 10 then (fpure . gpure) x else x gio :: Int -&gt; IO Int; gpure :: Int -&gt; Int gio x = return (x - 1) gpure x = x - 1 -- *Main&gt; fpure 12 -- 10 -- *Main&gt; fio 12 -- 12 -- 11 -- 10 -- 10 See the immortal ["You could have invented monads!](http://blog.sigfpe.com/2006/08/you-could-have-invented-monads-and.html) ; the discussion of `Writer` starts from a 'pure' analogue of your problem, and then shows how it inevitably gives rise to a 'monadic' way of putting expressions together.
Yes, [google code search](http://www.google.com/codesearch#search/&amp;q=%5C%3C%5C%2B%5C%3E%20lang:%5Ehaskell$&amp;type=cs ) gives 1,116 uses -- as a parser combinator, a logical connective, etc. etc. It looks like maybe the most commonly used `&lt;+&gt;` is the one from `Text.PrettyPrint`: &gt; let hello = text "hello"; world = text "world" &gt; hello &lt;+&gt; world hello world &gt; hello &lt;&gt; world -- &lt;&gt; is the mappend for Doc helloworld
How old is xmonad? Why doesn't it have a 1.0 release? I've been using it for a few months and everything seems mature enough.
Almost 5 years old. The numbers mean nothing, the just go up over time.
http://okmij.org/ftp/Computation/monads.html#ExtensibleDS
Let's be slightly careful about terminology, because there are two interesting questions here that have separate (but both interesting) answers. Question one is: can we write a compiler without using the Monad type class? Question two is: can we write a compiler without using the IO type constructor? For question one, the answer is a resounding _yes_. But writing the code would be enormously tedious. There are all kinds of design patterns that we use from day to day that can be wrapped up in convenient data types with convenient accessor functions that together have the shape declared by the Monad type class. And, once we have the Monad type class, there's dozens of convenient functions that we can reuse between all the different Monad instances: replicateM, mapM, when, guard, etc. We *could* write any given program without this convenient code reuse... but it would be *annoying*. For question two, the answer depends on what you mean by "write a compiler". For sure, very nearly all of the computation done in a compiler can be done purely. In fact, I'm fairly certain that all the compilers out there right now follow roughly this pattern: 1. Do some IO to read in command line arguments, file contents, library interfaces, etc. 2. Run a long, complicated, but ultimately pure computation that turns all the information gathered in step one into assembly code. 3. Do some IO to write the generated assembly out to a file. All the interesting and complicated parts of the compiler are in step two; steps one and three can be complicated, but only in the "there are a lot of details" sense, not in the "there are difficult theorems about correctness to prove" sense. And *this*, I think, is the point: because step two can (and should!) be pure, we can do all kinds of reasoning about it without worrying about the craziness of IO. And, indeed, I would not really hesitate to call the code implementing step two "the compiler" -- the rest is just support machinery. So there's my two cents: it's possible but annoying to write a compiler without the convenient Monad interface, and it's possible (and even common practice) to write the interesting bits of a compiler without IO.
[Bootstrapping](http://en.wikipedia.org/wiki/Bootstrapping_(compilers\) ) and [Futamura projections](http://en.wikipedia.org/wiki/Partial_evaluation#Futamura_projections) may be related to what you are asking and of interest.
I've looked at the [parsing packages available on Hackage](http://hackage.haskell.org/packages/archive/pkg-list.html#cat:parsing) recently and noticed that there are roughly four interfaces available: * Monadic (binary, cereal, kangaroo, mtlparse, polyparse, RefSerialize, uulib) * Applicative (appar, attoparsec, incremental-parser, parsec, parsimony, trifecta, uu-parsinglib), also supporting an monadic interface * Grammatical (ChristmasTree, derp, grammar-combinators, Grempa) * Arrows (PArrows) Other packages in the parsing category are extensions of these libraries, are applications of these libraries (and belong somewhere else), are simple parsers for specific formats (idem dito) or are relinquished or irrelevant. You say you don't want a monadic interface yet. I then strongly suggest to look into the packages that have an applicative interface, that look more like attribute grammars when used: bools = True &lt;$ string "True" &lt;|&gt; False &lt;$ string "False" tuple l r = (\_ a _ b _ -&gt; (a, b)) &lt;$&gt; string "(" &lt;*&gt; l &lt;*&gt; string "," &lt;*&gt; r &lt;*&gt; string ")" tuple' l r = (\a _ b -&gt; (a, b)) &lt;$&gt; string "(" *&gt; l &lt;*&gt; string "," &lt;*&gt; r &lt;* string ")" *thanks, notfancy :$ **I now see that my answer is a bit out of scope here, but anyway: one could use applicative functors to implement parser libraries, and people have done that. No monads involved, as far as I know.
I've had exactly the same idea, only I didn't implement it before. Why? Well, the list itself isn't a problem here. Each thread should have their own immutable copy so there are no collisions with accessing the list. Consequently the whole thing scales pretty well and streams are not really needed here. I did implement your approach given the code and there is no significant difference in performance. Anyway, here it is: https://github.com/Tener/disruptor-hs/blob/master/Test/ManyQueueStream.hs
Don't you mean `\a _ b -&gt; (a, b)` in `tuple'`?
I don't think it's feasible to make a fully extendable compiler.
Any reasons why?
Well, it may be possible in theory, but the complexity would be far too great compared to the gain. You have to remember that there are many kinds of languages, and some are very incompatible. It's not really a haskell problem as much as a practical problem. If you really want to make a very extendable language, it might be a good idea to use S-expressions and macros ala lisp.
Do you mean extend the language in a macro sort of way? Template Haskell can do that. If you want a more flexible system you could try a Lisp.
Slides: http://www.cs.nott.ac.uk/~gmh/contractive.ppt Paper: http://www.cs.nott.ac.uk/~gmh/contractive.pdf
No, this is more about designing my own compiler from scratch.
You might want to check out the work on [Honu](http://www.cs.utah.edu/~rafkind/papers/honu11.pdf) by Jon Rafkind and Matthew Flatt. &gt;Honu is a new language that contains a system for extending its syntax with an interface built on concrete syntax. Honu combines an existing hygienic macro system with a novel use of a precedence parser to achieve a syntax that is algol-like while maintaining the power of s-expression based macros. We demon- strate how to build the parser and connect it to the underlying macro system.
If you want some really extensible syntax, you should look into the mixfix syntax of Agda. There are some things it can't do (like Haskell's `do`-notation), but it can handle quite a lot. Even if you don't want to allow users to define new mixfix forms, it could be helpful to have in the compiler itself.
Is this your first compiler? If you know scheme, or don't mind learning, there are plenty of sources where you can learn to write a scheme compiler. I think most CS students write their first compiler for scheme anyway. Once you've done that you'd have a pretty good understanding of how scheme implements its [syntatic extensions: pdf warning](http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;ved=0CBwQFjAA&amp;url=http%3A%2F%2Fcs.au.dk%2F~hosc%2Flocal%2FLaSC-5-4-pp295-326.pdf&amp;ei=vWHJTuOhKcXl0QHojLkQ&amp;usg=AFQjCNE4fBTjE_4dVi7eBdRBA4N36ihw0w&amp;sig2=1b-mGSv0xzHNYKcCSA15yg) and you'd know if you could map it over into haskell somehow or not.
There is no intended connection with other uses of the term "generating function" (see 27:45 in the video).
The answer is: yes.
The Utrecht Haskell compiler has been designed to be extensible in as many directions as possible. It might be worth looking at the Attribute Grammar system they use (a Haskell pre-processor that allows the modular definition of abstract syntax and syntax tree traversals): http://www.cs.uu.nl/wiki/HUT/AttributeGrammarSystem Arthur Baars has done quite a bit of work in this area, designing a system of syntax macros to make compilers extensible, making it even easer to define domain-specific languages in Haskell. His thesis gives a good overview of work in this area: http://igitur-archive.library.uu.nl/dissertations/2009-1202-200125/baars.pdf
For the record, I didn't take your post as a criticism at all, or at least not an unfair one. As it stands now, I want Yesod to really *not* be revolutionary in it's approach to things, and that means staying very close to the existing MVC HTML/CSS/JS approach. The main goal in Yesod is to do these things better than the alternatives due to the strengths of Haskell. But that doesn't mean I think it's a perfect solution. I think we *should* experiment with other approaches, and Yesod has been designed in a (hopefully) modular enough way to allow this experimentation to happen. For example, I think widgets are great, and we can build a lot of much more sophisticated stuff on top of them. I think we'll have a chance to do more of this after we've made our 1.0 release and gotten the first edition of the book out the door. I'm hoping to hear ideas from others as to how they would approach these higher facilities.
I've gone through writing mini compilers like this before. I'm looking at doing a larger project now.
Google on PEG, Scannerless parsers, MetaLUA, Dylan, Factor, Lisp.
This looks quite intriguing. I like the representation in the second part of paper. The coalgebra representation seems to be some kind of Moore automaton running on an infinite stream. This is neat! However, for defining the example streams (nats and fibs) the state transition functions (tfibs and tnats) actually only use the last state, the input from the stream is ignored. I guess it is not necessary to have this input from the stream since this input is in fact the result of the output function (hnats resp. hfibs) for the current state. EDIT: Thinking about it, it looks like one can simplify things then. Combine the state transition function t and the output function h into a single function of type s -&gt; (a,s) that transforms the state and produces an output. But what is then the difference between streams generated by coalgebras and streams generated by an equation of the form stream s = h s : stream (t s) where h :: s -&gt; a and t :: s -&gt; s are the output and state transition functions, respectively.
In case anyone was confused about why this is here, it's actually referring to "productive" in the sense of well-defined codata.
Just google "programming language with extensible syntax"... * Maude * Katahdin * Whatever this guy cooked up: http://www.cs.colorado.edu/department/publications/theses/docs/bs/erik_silkensen.pdf (googling his name gets you [the code](http://code.google.com/p/extensible-syntax/)) It's been done... and it's not that hard to find.
dmwit has some good interpretations of your question. But here's another one, which renders the question nonsensical. If you mean that you wish to exclude monads entirely from the compiler, then that means you're going to have to exclude `(a-&gt;)` from the compiler, along with other monads. Of course, once you exclude functions from `a` there isn't a whole lot you can do, what with removing functions from the lambda calculus and all. So, on some level, the question is nonsensical. Monads are everywhere. In fact, monads are at the core of the idea of substitution; so if you're using lambda calculus, or unification in logic programming, or even just giving proofs by replacing equals with equals, then you're using monads. So trying to banish monads entirely is a futile endeavor. On the other hand, as dmwit points out, there are other ways of interpreting your question which have less vacuous (more interesting?) answers. But you have to be careful with what you're really asking.
[This paper](http://red.cs.nott.ac.uk/~led//papers/led_towards_tfp11.pdf) may be of interest to you.
Just put me off a little bit. I know it's not founded on anything, but software without a whole number makes me feel like it'll be full of bugs and crash all the time.
It's a Javascript syntax highlighter, [here's the theme](http://alexgorbatchev.com/SyntaxHighlighter/manual/themes/midnight.html) it's using
So what I need is "fadeToGrey" color scheme? But these is no such vim theme.
So [write one](http://code.sweyla.com/articles/vim-color-schemes-how-to/). The hex codes are in the [css](http://alexgorbatchev.com.s3.amazonaws.com/pub/sh/3.0.83/styles/shThemeFadeToGrey.css) file for the theme.
&gt; I really don't like Persistent ....In a nutshell, it's an Object-Relational Manager, when you have no objects.... ORMs are a tarpit. They suck up endless amounts of effort trying to bridge that OO/Database gap Trying to throw Persistent in the ORM bin when it doesn't even meet the definition just seems foolish. Can you take any of the concrete ORM criticism and apply it to Persistent? Not having to deal with OOP's inheritance, methods, encapsulation, mutable reference types etc and just using a simple value type record matches up quite nicely with a table row while the quasi parser handles all of the criticism regarding ORMs amount of coding.
Also OMeta. In particular see [this PhD thesis (PDF)](http://www.vpri.org/pdf/tr2008003_experimenting.pdf)
Let me clarify this somewhat more for someone who is "still learning Haskell here". First: clarify what you mean by "Monad". In Haskell-speak, a "Monad" is anything that includes some concept of "sequencing" or "binding" or "replacing this part with that other part", plus a concept of "wrap this otherwise-pure value into the Monad". Optionally, have some concept of "fail". Note that this is ***very*** general. Second: realize that because "Monad" is ***very*** general, functions themselves form a Monad: instance Monad (x -&gt;) where -- actually the syntax accepted by H'98 is ((-&gt;) x) f &gt;&gt;= g = \x -&gt; g (f x) x {- in words: a function of x-&gt;a, called f, can be bound to a function a -&gt; x -&gt; b, called g, by creating a new function x -&gt; b, which calls f with x, then calls g with the result from (f x), followed by the x we got in the first place. -} return a = \_ -&gt; a {-in words: to wrap a pure value into the function monad, just create a function which ignores its input, and always returns that pure value. -} Third: Since functions form a Monad, no, you can't write ***anything*** without Monads. Fourth: What, exactly, *are* you asking about? i.e. it is more likely that dmwit's reformulation of your questions is better, but is it? And which one are you really asking about?
i think he uses a theme that's similar to vim's wombat color scheme: http://dengmao.wordpress.com/2007/01/22/vim-color-scheme-wombat/
&gt; Can you take any of the concrete ORM criticism and apply it to Persistent? Why, yes, yes I can. By _far_ the biggest problem that ORMs have is that "one row in the database" is not isomorphic to "one thing", and quite a lot of the tarpit I mentioned is trying to deal with that fact. Persistent has that problem, except that being very young it also doesn't actually _have_ any of the tarpit implemented that ORMs use to try to work around that, with nothing but a trace of join capability. And an example of what a real ORM can do that Persistent can not is to define an override on a particular property that is capable of preserving some invariant by using OO. Persistent hands you its set of invariants, and you may not touch them or build on them. "inheritance, methods, encapsulation" are what make the one row = one thing idea at least _partially_ work, because you can work around the resulting problems by throwing enough code goop at the problem. Stripping the opportunities for code goop away does have the advantage of accurately showing how false an idea that one row = one thing... but that doesn't help Persistent.
I think cab can do recursive removal of packages.
That may be the good query wrapper, I don't think it would be the combinator library I'd be looking for (which would _handwave handwave_ something for web frameworks, and result in some sort of combinators that map DB to Haskell datatypes but don't have a particular 1-to-1 mapping with your DB). I'm not sure because it seems like I should have heard more about this library if it does what it says it does. Does anyone have concrete experience with it?
Shorter: "Monad" is an adjective, not a noun. You can't really sensibly "remove" an adjective from something. (Whatever the result of that operation is, you will end up with a result that is also no longer than "something".)
I wouldn't worry about number of lines of code: your code includes import statements, types, and whitespace, none of which are in the haskellwiki solution. Both you and HW assume the answer is at least 100000, which is not obvious to me. The HW program does not check for substituting the digit 0 and assumes that the digit 1 does not appear outside of the digits being substituted, neither assumption of which seems justified without knowing the answer. (So in this key respect -- accuracy -- your program is much better than the one on haskellwiki.) Both you and HW assume that the smallest size 8 prime value is exactly size 8, and not possibly size 9 or 10 (although that is arguably up to interpretation of the problem statement). It took me a while to realize why "filter (\l -&gt; length l `mod` 3 == 0)" is justified.... However it is still not clear to me what the second half of that filter, "notElem x (map toInteger l)", and the expression "filter (haveDuplicates 3 . numToList)" in findPrimeFamily8 are for. In terms of efficiency, it looks like there is a lot of duplicated computation. I assume isPrime is a particularly time consuming operation (you could profile your program and see how much of your execution time is spent in isPrime), so it might be good to memoize it. Concretely, build (lazily) an array of whether the first 10 million or however many integers are prime or not, and write a function isPrimeMemo that, given x, looks up x in the array if x is in range and otherwise calls isPrime x. Then, isPrime will be called at most one time each on every integer less than 10 million. Additionally, your code tests any particular family of numbers (e.g., 133***3 as * varies) many times over -- e.g., the expression "whitemask n [3, 4, 5]" is evaluated once for every (prime, anyhow) n of the form 1330003, 1330013, 1330023, ..., 1339993. However each of those values n gives the same family of numbers (i.e., the output of whitemask is the same) so you should be able to save a substantial amount of time by only evaluating each family one time. Hopefully you find this helpful.
Thanks. This was helpful.
Here's my solution, kind of goofy, written a few years ago when first learning Haskell. You may be interested to look at it because it's fast though, trying to avoid unnecessary work. (It also doesn't contain the assumptions mentioned by iqtestsmeannothing.) I have since found much better prime number generators, so dropping in a better prime generator would be an easy speed-up. -- Project Euler prob 51: Consider families of primes, such as *57 or 56**3 -- where a family of primes is produced from substituting the same digit for -- each occurrence of * (e.g., 56003, 56113, 56333, 56443, 56663, 56773, and -- 56993 comprise the 7-prime family with smallest prime 56003.) -- Find smallest prime which by replacing part of the # (not necessarily -- adjacent digits) with the same digit, is part of an 8-prime value family. import Data.Char(chr,ord) -- Analysis: considering prime sequence, only substitute larger digits, only -- perform substitution on 0, 1, or 2, and keep track of how many failures -- are allowed. Once max # failures exceeded, can skip rest of substitutions -- and move to next prime in the sequence. -- -- from prob 50 -- fast prime from prob37, modified to remove p &lt; 5 test, remove 3 case, add 1 isPrime 1 = False isPrime 2 = True isPrime p = p `mod` 2 /= 0 &amp;&amp; isPrimeR 3 p isPrimeR f p | f*f &gt; p = True | p `mod` f == 0 = False | otherwise = isPrimeR (f+2) p primeSeq = (2:[n | n &lt;- [3,5..], isPrime n]) mapAllAToB s a b = map (\x -&gt; if x == a then b else x) s prob51 = tryPrimeR primeSeq tryPrimeR (p:ps) = let str = show p in if '0' `elem` str &amp;&amp; variantR '0' '1' str 2 0 &gt;= 7 then p else if '1' `elem` str &amp;&amp; variantR '1' '2' str 1 0 &gt;= 7 then p else if '2' `elem` str &amp;&amp; variantR '2' '3' str 0 0 &gt;= 7 then p else tryPrimeR ps -- return count of primes that result from substituting n into s in place of d -- rf is remaining number of failures allowed variantR d n s rf res = if rf &lt; 0 || (ord n &gt; 48+9) then res else let cand = read $ mapAllAToB s d n in if isPrime cand then variantR d (chr $(ord n+1)) s rf (res+1) else variantR d (chr $ (ord n+1)) s (rf-1) res main = putStrLn $ show prob51 
Is the video located anywhere else the link loads a blank page for me currently.
Well, inspiraton.swela.com is 404.
Wow, link with 2 mistakes - it's awesome!
No, it's not. Strings should me formatted with yellow, not with green.
For simple examples such as nats and fibs, we don't require the next input value when defining the stream using cfix. However, for more sophisticated examples, such as defining the conversion functions fromtree, fromgen and fromrgen on page 16 of the paper, we do require both the state and the input value. As regards the connection with the traditional co-algebraic approach to defining streams, in the concluding section we note that each of the new fixed point operators for streams that we define can in fact be defined in terms of the "unfold" operator for streams. There are a number of interesting aspects to these encodings, e.g. defining cfix using unfold is related to the notion of corecursive algebras. Exploring such connections is one of our topics for further work.
You can also look into Typed Transformation of Typed Abstract Syntax, a paper which uses type-level programming to extend the language they're working with in many ways. The paper is a hard and very interesting read, you might get some good ideas out of it.
It's a direct download link to the video file. Perhaps your browser is blocking it for some reason?
&gt;For simple examples such as nats and fibs, we don't require the next input value when defining the stream using cfix. However, for more sophisticated examples, such as defining the conversion functions fromtree, fromgen and fromrgen on page 16 of the paper, we do require both the state and the input value. But the input value is not really required right? It is convenient but you could just as well maintain the last output in the state. &gt;As regards the connection with the traditional co-algebraic approach to defining streams, in the concluding section we note that each of the new fixed point operators for streams that we define can in fact be defined in terms of the "unfold" operator for streams. There are a number of interesting aspects to these encodings, e.g. defining cfix using unfold is related to the notion of corecursive algebras. Exploring such connections is one of our topics for further work. So it is the case that streams defined as fixed points of contractive functions can be equivalently characterised as streams defined by equations of the form stream s = h s : stream (t s) Is that right? 
I tried to write about this in an [old blog post](http://martijn.van.steenbergen.nl/journal/2009/11/12/gadts-in-haskell-98/). I didn't know this is how ocaml did it though!
Futamura projections: Interesting. Would be interesting to write something like that some day. It *sounds* like it doesn't have to be much harder than a toy compiler. Sounds like one of those things where there are a bunch of people making it a competition trying to make it as small as possible - for the fun of it. Well, I expect it to have practical applications as well. Bootstrapping: Yup, knew of that one.
Not quite: they can be characterised by equations of the form stream s = let x = h s in x : stream (t s x) where the let clause is used to ensure that the value x is computed once and then shared twice, once as the first output of the stream, and once as an input the transition function t. Of course, if we don't care about sharing, the above definition can be simplified by recomputing the value h s within the definition of t. As noted above, this is related to Capretta et al's work on corecursive algebras.
This is safer still if you have f :: (Show a, Monad m) =&gt; (a -&gt; m b) -&gt; a -&gt; m a An example application of f: f' :: Show a =&gt; a -&gt; IO a f' = f print The advantage is you show to any caller that you won't launch the nukes. Or whatever.
What's the relation between this and your [previous information gathering script](http://www.reddit.com/r/haskell/comments/lz26e/please_help_me_gather_information_about_haskell/)? Was it the precursor to this? And if I sent information for the last one, do you also want me to run this one?
&gt; I wouldn't worry about number of lines of code: your code includes import statements, types, and whitespace, none of which are in the haskellwiki solution. It's the strangest thing about all HW PE solutions. &gt; I assume isPrime is a particularly time consuming operation (you could profile your program and see how much of your execution time is spent in isPrime), so it might be good to memoize it. How can I do that? &gt; Hopefully you find this helpful. Yes it is. Thanks.
Suppose you have a non-recursive function you want to memoize over some range. A quick way to do that might be something like this (untested code): import Data.Array (Ix, array, range, inRange, (!)) memo :: Ix i =&gt; (i -&gt; a) -&gt; (i, i) -&gt; (i -&gt; a) memo f r = let a = array r [(i, f i) | i &lt;- range r] in \x -&gt; if (inRange r x) then (a ! x) else (f x) isPrimeMemo :: Integer -&gt; Bool isPrimeMemo = memo isPrime (1, 10 * 1000 * 1000) More general techniques, in particular for handling recursive functions correctly (you would not want to use the above code to memoize a naive fibonacci function) can be found at the links from the [haskell wiki article](http://haskell.org/haskellwiki/Memoization).
My previous script helped me understand what people had on their systems, and where it was all stored. It was the information I needed to develop this script. This script is the actual uninstaller. (Though to be clear, it doesn't uninstall anything, just report unless you give it an explicit option to.) I was looking for a few people to run it so I could see how it performed on real people's configurations - since I only have a few Macs to test it on. Running it with the `test` argument won't actually deleting anything, and will output what the uninstaller would do, when called with each possible GHC version it finds on your system. If you'd be willing, please do run it and send me the result. I had about 75 responses to the last request! **Thank you all.** I've gotten about a dozen to this request so far. Another dozen would be great. (I don't think I need as many responses to this one, as I'm just checking the script is acting sanely.) 
Ok, sent. Thanks for working on this!
When I was working on implementing the compiler in [SPJ's functional languages book](http://research.microsoft.com/en-us/um/people/simonpj/papers/pj-lester-book/), I implemented partial ordering on infix operators. So instead of saying "+ is left associative with precedence 5, / is right associative with precedence 6", you say "* is left associative and has higher precedence than + and equal precedence as /". The compiler would fill in the partial ordering between all operators based on the program's specification, and operations that had ambiguous precedence resulted in compile failure. I'm not sure why Haskell doesn't do it this way, it seems much more expressive and solves the problem of 'should this be precedence 3 or 4? I really want 3.5...' or "I really want this operator to have lower (or higher) precedence than everything else"
Sent. Thanks :)
Can you show a minimal example where your problems with associated type synonyms show? I personally prefer them to functional dependencies, and would like to see that solution work properly :)
My thoughts, mostly trivial suggestions, and subjective matters of taste. -- instead of take 10 numbers = take 10 [0..] -- I'd just use the usual list notation numbers = [0..9] -- instead of matching on a list where [putted,unputted] = (\(x,y) -&gt; [init x,y]) $ ... -- I'd use tuples where (putted, unputted) = (\(x,y) -&gt; (init x,y)) $ ... -- instead of filter (\x -&gt; numLength x == numLength num ) -- I'd pull numLength num into a separate expression filter (\x -&gt; numLength x == len) where len = numLength num -- (not sure how smart the compiler is on this, -- though I'm pretty sure -O2 would optimize it similarly) -- instead of (\(_,x) -&gt; x == elem) -- I'd do ((== elem) . snd) -- this expression got rather long filter (\l -&gt; length l `mod` 3 == 0 &amp;&amp; notElem x (map toInteger l)) $ ... -- I'd factor the tests out filter (\l -&gt; foo l &amp;&amp; bar l) $ ... where foo = (== 0) . (`mod` 3) . length bar = notElem x . map toInteger -- (exercise for the reader to give foo &amp; bar more appropriate names) -- this scares me head . head . (head . blah) . blahblah
There was no way his: class Metric m where type Point m distance :: Point m -&gt; Point m -&gt; Double could work, given that `distance` did not mention `m` anywhere.
That, combined with the fact that `Point m` is not guaranteed to be injective.
That's what I meant. Without knowing the `m` or that `Point` is injective, you don't know what instance you want. Same issue with the fundeps further down the article. I'm not sure how he was able to use the class in the state he described.
Indeed, there is no way to limit `distance` into a proper instance. Isn't there a way to detect this? `hlint` keeps awfully quiet.
&gt; I thought the use of monads may make the program "unpure" to some extent. I think it was [sigfpe](http://blog.sigfpe.com/) who said it best: Monads cannot give you anything you don't already have. This follows from the fact that we have to implement the `Monad` instance somehow. Monads don't *do* anything, they're just a way of *talking* about the things you can already do. In particular, he's referring to the `IO` monad and the fact that it doesn't really add anything in terms of formal power; we can always just use the foreign function interface to call impure functions. The point of `IO` isn't to do anything extra, it's just to clarify what exactly it is that we were doing in the first place. Because Haskell is lazy, the aforementioned use of the FFI would (naively) result in unpredictable programs. We can get rid of the unpredictability of evaluation order by using a continuation passing transformation in order to explicitly order all the beta reductions in a line so that they must be evaluated in that specific order. This CPT is *exactly* what the `IO` monad is doing, and is the only thing the `IO` monad is doing. We could do the CPT manually and avoid using the `IO` monad, but then we'd have a bunch of plumbing cruft, and we'd have to explain the CPS to anyone who looks at our code. The bind and return operators of the `IO` monad are just a way of hiding all that plumbing so that we can focus on the parts of the code that really matter. Monads never give you any new power; they're only about abstracting away the details of something you could've done manually.
The point of having monadic I/O is that the type of the function you've defined there **guarantees** that no I/O can happen as part of evaluating a call to it. If you want to print a value, you have to write something completely different—an IO action. Yes, this is something very difficult to wrap one's head around. But basically, every well-written executable Haskell program can be split into two parts: * A set of pure functions and constants that does the heavy lifting. None of these can do any I/O. * A set of impure IO actions that handles all the I/O parts, passes I/O dependent values to the pure code, and writes out their results as I/O output. The latter is allowed to call into the former, but not vice-versa.
I have a ticket on the GHC trac asking them to warn about situations like these, but it hasn't seen much action :/
You could really flip it though, right? class Point p where type Metric p
Kudos to Bryan for a great library, and kudos to Felipe Lessa for doing the Text port and maintaining it in the separate library [attoparsec-text](http://hackage.haskell.org/package/attoparsec-text) until now. Will John and Felipe now fold together [attoparsec-enumerator](http://hackage.haskell.org/package/attoparsec-enumerator) and [attoparsec-text-enumerator](http://hackage.haskell.org/package/attoparsec-text-enumerator) analogously, or keep them separate? 
Whoops, thanks for pointing that out. It's a typo in the article which propagated through copy and pasting, not actually an issue with the original code. I've now fixed it.
What would be the point? At this stage you've just recreated the original much simpler metric space type class but made it more complicated.
Hm. On trying to create one, I cannot. It's possible I was experiencing a more mundane issue which was masquerading as a type families issue. I'll have another go later.
Perhaps because you didn't read the actual description and just looked at the type signatures? Typical Haskell programmer. ;-) As per other comment, it was just a typo. The whole point of second class was to be explicitly passing a metric value around, so it would be pretty silly if distance weren't actually referring to m. 
I believe this is used to implement prompts that contain the monad themselves, for instance functions like `mplus`. Personally, I prefer my [operational](http://hackage.haskell.org/package/operational) package since it's a bit simpler to understand.
This problem (TypeFamilies + qualified imports) **might** be related: {-# LANGUAGE TypeFamilies #-} module TestA where class ClassA a where type TypeA a functionA :: a -&gt; TypeA a -- cut here -- {-# LANGUAGE TypeFamilies #-} module TestB where import qualified TestA as A data DataB = DataB data DataB' = DataB' instance A.ClassA DataB where type TypeA DataB = DataB' -- but A.TypeA works functionA DataB = DataB' [GHC Ticket #5417](http://hackage.haskell.org/trac/ghc/ticket/5417) seems related to the above.
Hmm, I would have agreed with you. But of the three common uses mentioned in this thread, the one that the OP probably has in mind isn't found in hoogle. Namely, xmonad's use of `&lt;+&gt;` as an alias for `mappend`. Now that `&lt;&gt;` has made it into `Data.Monoid` itself, I suppose the use of `&lt;+&gt;` for that in xmonad will be gradually deprecated.
Hmm. I don't *think* that was the issue I was having. I wonder if it was something as stupid and simple as missing a LANGUAGE declaration at the top of my test suite. Hold on while I investigate... Edit: Nope. Seems to work fine without.
Of course it is perfectly normal that a C++ programmer [would write nested structs with member functions](http://code.google.com/p/inv/wiki/CurrentState) ;) It's an interesting idea, but it's still far from ready.
Cool project! Of course, the premises are thoroughly misguided: &gt; Our first goal is to reduce vendor lockup because of requirement to use a specific programming language X. &gt; &gt; Our second goal is to please minds of those programmers and managers who believe (mistakenly or not) in supremacy of native C and C++ programs over garbage-collected and JITted bloatware of these days. &gt; &gt; Our third goal is to attack language conservatism of mainstream development teams by reducing risks of using immature compilers such as our own in real-world commercial projects. but the blame for that probably falls on "the managers", not the project authors.
I will definitely take a look at it.
Why is it far from ready?
Just look at the example and then ["Our “Elevator Pitch”"](http://code.google.com/p/inv/). I really doubt the generated source code would be taken as "that's what a C++ programmer would usually do". It would need quite some tweaking to make this look natural. Also, the type inferencing is apparently broken, according to issues.
Micro-examples: http://code.google.com/p/inv/source/browse/#svn%2Ftrunk%2Fspl2%2Fhn_tests I've seen worse. 
What is up with that HN language? Who in their right mind thinks that _if (eq 0 (mod x (incr divisor))) (eq 0 1) (eq 0 0) is easier to read/write/understand than return 0 == x % divisor + 1 ? 0 == 1 : 0 == 0; And I am ignoring the precedence error in the generated code.
Specific micro-example: http://code.google.com/p/inv/source/browse/trunk/spl2/hn_tests/const.cpp Lots of artificial clutter right there. Another specific micro-example: http://code.google.com/p/inv/source/browse/trunk/spl2/hn_tests/euler48.cpp Builds a lot of extra data that a human programmer would never think of.
Another solution to this problem is to use to have a Binary instance on a view that doesn't contain the metric. type Metric a = a -&gt; a -&gt; Double create :: Metric a -&gt; MetricSearchIndex a toData :: MetricSearchIndex a -&gt; MSIData a fromData :: Metric a -&gt; MSIData a -&gt; MetricSearchIndex a instance Binary (MSIData a) But the type classes are probably nicer.
You've, so to speak, flipped the fundep. So now you can write (Point p, Arbitrary p) =&gt; with no problem...
&gt; I'm not sure how he was able to use the class in the state he described. He added another `m` argument to the type of `distance`. He could probably just as easily have used the associated types version if he pulled the same trick: class Metric m where type Point m distance :: m -&gt; Point m -&gt; Point m -&gt; Double
Me, just because it actually has a clear precedence order that allows you to evaluate it without having to read what the operators are. Personally, I'd prefer return (0 == (x % (divisor + 1))) ? (0 == 1) : (0 == 0); over both, though.
Yes but you've forced each type p to only have one valid metric type for it. I guess that's a *little* better than forcing you to attach relevant data to every instance, but it still means you have to newtype a lot to get interestingly different metrics.
&gt; Lots of artificial clutter right there. it's called design patterns &gt; Builds a lot of extra data that a human programmer would never think of. but an enterprise one would. 
Well, if you have a huge legacy codebase in C++, that's shouldn't reflect poorly on current managers, nor is it necessarily reflective of poor choices by former managers.
&gt; it's called design patterns Design patterns as applied by a CS freshman who had to meet a deadline though he never showed up in class, so he quickly looked at the slides and lecture notes and copy pasted all the things. Or, in this case, as applied by a simple program. I'm not saying that this software is unsophisticated. I'm saying that, in its current state, it isn't capable of creating code that looks like it was churned out by a seasoned programmer that likes to write readable code. &gt; but an enterprise one would. Sure, if you want to survive in an enterprise, you have to have the highest SLOC rate, and what is better to reach your quota than inventing senseless indirections?
Another approach would be to use rational numbers for the precedence levels, since they have the property that there is always another rational in between any other pair of rationals.
Oh, that wasn't there before :)
Hah okay, sorry about that!
From the [Manifesto](http://code.google.com/p/inv/wiki/Manifesto) &gt; Our first goal is to reduce vendor lockup... A list of examples ensues, ending with &gt; A license agreement demands certain language and no overhead caused by translation layers :)
Anyone who has a tiny bit of Lisp experience can read the former *extremely* easily. The latter, however, is a jumble of precedence nonsense that requires a lot more brainpower to parse.
Then I'm going to add my [effects](http://hackage.haskell.org/package/effects) package, also very similar to operational and MonadPrompt. It has an advantage that it is easier to mix several effects, and a disadvantage that it is brand new.
Might as well remove the redundant use of the conditional operator while we're at it. return x % (divisor + 1) != 0;
While I admit that I find the lispy code a bit easier to read than the c-style code (I'm kinda a newbie and have largely avoided C/C++), I'm really failing to see why either one would have return statements that are clearly supposed to be binary true/false, instead choosing to simply have much more human readable true/false. Basically, why would I, in both options, choose to write a boolean statement which always returns a specific value, since a human reader would have to parse the code and translate it simply into "true/false"? Why not just say true/false or whatever the straight-out boolean statement is for the language? 0==1 is obviously false and 0==0 is obviously true, but only a machine wouldn't care about the fact that you need to mentally parse those statements in to the equivalent.... right?
If I'm reading the API docs correctly, there shouldn't be much merging required. I think I can just change the type signature of `iterParser` to iterParser :: Monad m =&gt; Parser a b -&gt; Iteratee a m b and it'll work. First I want to let attoparsec_0.10 settle a bit (three point releases in an hour?), and Thanksgiving is this weekend, so I'll probably get to it next week.
As far as I know off the top of my head: * Polymorphic kinds and promotion of data types to kinds! * Constraint kinds and constraint language abstraction * Defaults for associated types * GHCi can now accept ALL top-level declarations, including class definitions/instances, data type definitions, etc. This is a huge complaint for many newcomers who expect the REPL to be able to take any input. * New constraint solver for the type checker, meaning it should be faster and more memory efficient * Improvements to LLVM backend, and a registered ARM backend (LLVM 3.0 only) * A bunch of improvements to profiling infrastructure, meaning ~~programs compiled with profiling info should be much faster~~ profiling results are more accurate. * Many, many Data Parallel Haskell improvements. Should be much more robust now from what I've read. * Eventlog improvements, for things like looking at the spark pool, etc (already supported by theadscope 2.0) * Safe Haskell redesign and a new implementation * Data.Bits improvement, with unsafe shifting functions (so wrapping isn't checked,) and a new 'popCnt' function that can optionally use SSE 4.2 `popcnt` (Johan wanted these for his HAMT implementation, which benefit greatly from a very fast population count.) * Many other smaller things I can't remember. I think those are all the really major highlights. This doesn't even account for all the library improvements, which I'm sure there are many (a TON of bytestring patches just hit, bumping it to a `0.10` version, based on a new `Builder` monoid; template haskell can now have a 'dependency' on an external file, so changing it results in recompilation automatically, etc.) A super exciting release! 
One library improvement I'm looking forward to is the `deepseq` library bundled with GHC 7.4, and that most of the other GHC-bundled libraries (including the new `bytestring` version) will provide `NFData` instances out of the box... no more `NFData` orphanage =)
That IS another good improvement, and criterion users everywhere rejoiced!
Try this: https://gist.github.com/1388165 You could use one of the library monads for this work, but I just coded a simple, special purpose one, just for your task.
Thank you. I'll have to pay more attention to it later when I'm back home. I was somewhere along those lines but couldn't wrap the computation in a monad. My "monadic" composition looked something of sorts: orders = fst $ ([], ants) ~&gt;&gt; gather ~&gt;&gt; scout
The main improvment with the profiling changes is that profiles are more accurate. Also, you can get a pretty useful stack trace for exceptions with the "+RTS -xc" flag in a program compiled for profiling now. This flag was always there, but it wasn't very useful before - now it works much better. I should write a blog post about this sometime...
If a year ago you'd told me I could ever be excited about a new compiler version I'd have never believed it. Also I'd heard that monad comprehensions were coming back, is that part of 7.4?
7.2 http://www.haskell.org/ghc/docs/7.2.1/html/users_guide/release-7-2-1.html
&gt; ... including the new bytestring version ... Unfortunately I was [a bit to late](http://www.haskell.org/pipermail/libraries/2011-November/017046.html) sending in the `NFData` patches for `bytestring`. So I don't think they will be in the `bytestring` release that gets bundled with GHC 7.4.
It could also use more documentation. Even a seemingly-gratuitous amount, being how it has a pretty bad case of TMTVA[1] syndrome. Might be a few minutes of concentrated staring could disentangle the meaning of the various type signatures, but I don't feel like putting in the effort. The code example from the home page gives me a faint inkling of what the library does and how, but a similar example with explanation should ideally go below every type and function. (Just my two cents, of course, it's your library and thanks for providing it. But if you want people to use it, I think the above would help. Also blog posts, etc.) [1] Too Many Type Variables, Aaaagh!
Thanks! BTW it would also be nice to add a `Monad` to the API that abstracts over the threading of the queue: {-# LANGUAGE GeneralizedNewtypeDeriving #-} newtype MQ s m a = MQ {unMQ :: StateT (MQueue s) m a} deriving (Functor, Applicative, Monad, MonadTrans, MonadIO) runMQ :: MQ s m a -&gt; MQueue s -&gt; m (a, MQueue s) runMQ = runStateT . unMQ writeQ :: MonadIO m =&gt; a -&gt; MQ a m () writeQ x = MQ $ StateT $ \mq -&gt; do mq' &lt;- liftIO $ writeMQueue mq x return ((), mq') readQ :: MonadIO m =&gt; MQ s m s readQ = MQ $ StateT $ liftIO . readMQueue 
please do!
Note that `Logging e` is simply the `State [e]` monad. This doesn't actually let you use some existing function as `(~&gt;&gt;)`, so I'm not sure it's what you're looking for.
it must have gotten changed. this is the wombat that i have and it's literally like the LYAH one http://pastie.org/2910289
Well... this would mean that by the time the GHC 7.4-based Haskell Platform gets released, it will be shipping with an already outdated `bytestring` package. I guess this might cause some annoyances for HP-users and/or for the library authors wanting to make use of the new `bytestring-0.10` features.
I remember reading your Haskell workshop slides about various profiling components and thought that's what it concerned. Thanks for clarification!
[Part 2: Fun with type functions](http://channel9.msdn.com/posts/MDCC-TechTalk-Fun-with-type-functions)
&gt; * Polymorphic kinds and promotion of data types to kinds! Is that actually implemented yet? AFAIR it's only partially implemented yet.
Yes, the patch was included into HEAD as of last week. Anything in HEAD at the time of branching (or before the branch) goes into the release. Obligatory GHCi session: 15:26:29 a@link ~ ./ghc-head/bin/ghci GHCi, version 7.3.20111121: http://www.haskell.org/ghc/ :? for help Loading package ghc-prim ... linking ... done. Loading package integer-gmp ... linking ... done. Loading package base ... linking ... done. Prelude&gt; :set -XPolyKinds Prelude&gt; data P = Z | S P Prelude&gt; :i P data P = Z | S P -- Defined at &lt;interactive&gt;:3:6 Prelude&gt; :k P P :: * Prelude&gt; :k Z Z :: P Prelude&gt; :k S S :: P -&gt; P Prelude&gt; :k (S Z) (S Z) :: P Prelude&gt; :t (S Z) (S Z) :: P Prelude&gt; Also check out some of the tests [in the testsuite](https://github.com/ghc/testsuite/tree/master/tests/polykinds).
Makes it seem less academic?
Agreed. Operating systems often have mascots, but not programming languages. Having a readily recognizable prefix/suffix (Haskell has hs) and a recognizable, simple symbol (Haskell's overlapped bind-lambda `&gt;&gt;=\`) is enough.
Haskell's thriving academic subculture is part of what distinguishes it from other languages. I'd like to think a lot of us are fleeing towards the academic part of Haskell, not from it.
As long as it is the banana slug. http://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Two_Banana_Slugs.jpg/200px-Two_Banana_Slugs.jpg 
From what I read on the mailing list, 'master' and the '7.4' branch are still in kind of lockstep, and developers are discouraged from checking in disruptive changes to the master branch. So there is still good hope for infusion of good stuff into 7.4.
My votes: * Mascot: [Lambdabot](http://haskell.org/haskellwiki/Image:Lambdabot.png) (could do without the drool) * Symbol: [the Lambind](http://haskell.org/wikistatic/haskellwiki_logo.png) (I think this is generally accepted)
&gt; Do you want the Haskell programming language to have a mascot? I was feeling sort of "yes and no" about this, so I checked both. :)
Go-lang has a hamster as a mascot, Python has two stylized pythons. OCaml has a camel. Thats what first coming on my mind.
I wasn't sure if OP wanted to write a monad, or just use one. In either case, I supplied a simple one because I think it helps demystify how these things work. It seemed to me that `(~&gt;&gt;)` was OP's attempt to do something like `(&gt;&gt;=)` because they didn't know how to use it directly. `(~&gt;&gt;)` is somewhat awkward, and I supplied the more natural `doOrders` that both works with OP's `[Ants] -&gt; [Orders]` functions, and with the monad for collecting the results.
Poor Javascript - it has no mascots or logos and the official name is as hip as a skin disease. I guess that is why the community overcompensates by also including the JS suffix on every other library name.
Your picture is lacking a monocle.
golang has a GOpher.
Right, just saying that just based on `(~&gt;&gt;)`, it's not at all clear that a monad *is* really the best solution; but if it is, its `(&gt;&gt;=)` should probably resemble `(~&gt;&gt;)`.
But we already have a [mascot](http://i.imgur.com/a6KNZ.jpg)?
What about the Hercules Beetle on the cover of the O'Reilly book?
Haskell has so many mind-bending concepts and ideas that it seems somehow appropriate, that I can choose both Yes and No as my answer.
More like a hascot, amirite?
Call me a dork, but I like the GOpher despite my love for with Haskell and CL.
Are you sure it isn't a rabbit?
How about a scruffy-looking chap in a tweed jacket and bowtie? Put a lambda on the bowtie for extra mascotness. On second thought, it should be an ascot, not a bowtie. You could call it a monadic ascot -- or "mascot" for short! Yes, a mascot in a mascot! Make the ascot look warm and fuzzy for extra laughs. Oh, and he should be eating a burrito. Round it all off with a catch-phrase involving tying-the-knot, and that there is pure mascot gold.
`Maybe Mascot`?
Closed after less than seven hours? Seriously?
You may try the steps at http://www.haskell.org/haskellwiki/Hoogle#GHCi_Integration to get Hoogle integrated in your GHCi prompt.
Java has that black and white triangley thing. I think his (its?) name is Duke?
I thought [Monica Monad](http://www.haskell.org/haskellwiki/Image:Monica_monad_falconnl.png) is our mascot? She's a warm and fuzzy thing.
Isn't the result of the function wrong, if I apply a ant-&gt;order function to the (order,ant) tuple shouldn't I get (order,order) as a result?
Btw, in one of the slides there was a curious lambda abstraction syntax: member negate [increment, \x.0-x, negate ] ...but there doesn't seem to be any support for parsing that `\x.0-x` in GHC...
The function is of type `[Ant] -&gt; [Order]`, and not for every `Ant` will there be an `Order`. This function I wrote acts a glue between multiple functions of type `[Ant] -&gt; [Order]`, so I have in example: orders = fst $ ([], ants) ~&gt;&gt; gather ~&gt;&gt; scout Where `gather` might for some ants create orders, and for the remaining ants have the `scout` function create orders. Effectively letting me have a chain of functions that are applied in a reductive manner on the ants passed in by the priority in which orders should be generated. Hope that clarifies it. I will be tinkering with it in order to have it under a monadic form, where my scope is it to use it as: orders &lt;- return ants &gt;&gt;= gather &gt;&gt;= scout
I vote for [Gallotia](http://en.wikipedia.org/wiki/Gallotia) **simon**yi.
 cabal install hoogle &gt; :!hoogle liftM
Yea, a stylized hercules beetle would be awesome in my opinion: small, but yet strong and elegant!
I also have a soft spot for "blambda." Depending on how you read it, it's either very excited, (blambda! like an emeril catchphrase) or very blase (blambda, schlambda... as long as the darn thing compiles...).
Maybe ask in the /r/xmonad subreddit?
Muchos gracias! Also, any idea what the problem above might be?
Excellent, thanks! One problem though, when I sudo cabal install hoogle, it fails on a dependency: &gt;hoogle-4.2.7 depends on haskell-src-exts-1.11.1 which failed to install. Requires happy &gt;= 1.17... Trying to install that now... So, happy 1.18.1 installs with no problem, but attempting to then 'sudo cabal install haskell-src-exts-1.11.1' fails with the same error &gt;Configuring haskell-src-exts-1.11.1... &gt;setup: The program happy version &gt;=1.17 is required but it could not be found. Strangely, haskell-src-exts does not appear to require happy, from its hackage page: http://hackage.haskell.org/package/haskell-src-exts But seems it thinks it needs it anyway. Any idea what's going on, what the fix might be?
I think it is just meant as `\x -&gt; 0 - x`. Lambda abstraction is often written with a dot, see for example the [wikipedia page on lambda calculus](https://en.wikipedia.org/wiki/Lambda_calculus).
A user install of happy will only put the binary in a local directory. You either need to add that directory to your path, or to move the happy binary to a more central location such as /usr/bin
I believe that's plan9.
look at permute xs = [ [ (fst y) :z | ... (fst y):z is a list. you wrap it into two more lists, so the result of permute would have to be something like [[[Int]]], not [[Int]]. If you get rid of the additional wrapping, it works: permute xs = [ (fst y): z | y &lt;- select xs, z &lt;- (permute (snd y)) ]
You can try to install the tools only for your user, instead of using "sudo" before each cabal install. Then, add ~/.cabal/bin to your $PATH and everything you install for your user will be available. So, I recommend you to try "cabal install hoogle", without sudo, and see if that works.
Hey thanks, The GHCI error wasn't really helpful for me. And I forgot that I could access the 'y' after its generator function.
thanks should have tried that before posting here :)
Well, anyway, dzen2 should be started by xmonad. My xmonad.hs looks something like this: main = do statusbar &lt;- spawnPipe "dzen2 [options]" xmonad defaultConfig { ... logHook = logHook' statusbar, ... } The logHook will pipe data into dzen2 to update it.
The ghci error message becomes actually more helpful when you remove the type signature for permute and let ghci try to infer the type.
Ah, same thing I had to do with the cabal binary, thanks. Soft linked it to /usr/loca/bin, along with soft links to ghc, ghci, cabal, and everything else. Thanks.
Thanks, I did try 'cabal install hoogle' but was getting 'permission denied' errors on a few things that prevented it. Soft linking every Haskell binary into /usr/local/bin seems to be best solution so far. Thanks for your help!
Solving Prolog problems with Haskell? [Escape from Zurg: An Exercise in Logic Programming](http://web.engr.oregonstate.edu/~erwig/papers/Zurg_JFP04.pdf)
Just Mascot.
On second thought, it should be a lamb. Duh!
What's the difference between this and cabal-dev? Is it that it's not a sandbox?