what is this i dont even.
Thank you very much, I'll check it out!
Really look forward seeing the finished book coming!
For a very low level discussion of this type, you might be interested in http://www.well-typed.com/blog/2014/06/understanding-the-realworld/ .
Okay, best shouldn't be the enemy of better. Even an encouraging "Improve this doc on GitHub!" link would probably have a worthwhile positive impact. I mean, sure, if you can figure out how to do a more direct embedding, let's do that too :)
I don't think so. Especially not if you can generate the symbolic representation of the integral you want to calculate. You might need to do sampling to solve it, but my intuition is that for simple analytic functions you might be able to solve the integrals generated.
People have mentioned Hakaru, though I personally think [this paper](http://dl.acm.org/citation.cfm?doid=2804302.2804317) looks even more promising. There is code for their system available iirc, though I haven't gotten around to trying to get it to actually work. They are trying to provide the most general treatment you can of probability distribution as monad, leading to all sorts of craziness with composition and threading code into a probability distribution, writing your own sampling methods, etc.
In all honesty, I didn't learn about any of those topics from web pages. I joined the #haskell irc channel on freenode, I just paid attention to anything mildly interesting in information-sponge mode. That said.. https://ocharles.org.uk/blog/guest-posts/2014-12-18-rank-n-types.html https://ocharles.org.uk/blog/posts/2014-12-12-type-families.html https://ocharles.org.uk/blog/posts/2014-12-13-multi-param-type-classes.html https://ocharles.org.uk/blog/posts/2014-12-14-functional-dependencies.html
Check out Quantrix. Some good ideas on reference language and gui there. Jvm based. Used to be cheap but then they got bought and shkrelied the license fee. Lost touch after that. It was able to export to excel files and I was able to build complex proforma financial models with scenario analysis in a few hundred lines of code. Vastly easier to debug them. Great project you have chosen. I hope you will let us know if the loeb function proves useful. 
Rather here, thanks! Huh, that's interesting. Thanks for telling me.
Brilliant! This is the kind of feedback I was looking for. &gt;``` &gt;terminalBounds = maybe (0,0) (T.width &amp;&amp;&amp; T.height) &lt;$&gt; T.size &gt;``` &gt; &gt;using Control.Arrow in base. &amp;&amp;&amp; is called 'fanout' - it takes a value and passes it to two functions, collecting the results of each in a tuple. Yes! Thank you! &gt;But temporarily looks like a pretty common behaviour. Turns out there's a library function that will help! Bracket does most of the work that temporary does, so we just need to tweak it to be just right. &gt; &gt;temp get set to act = bracket (get stdin) (\restore -&gt; set stdin restore) (\_ -&gt; set stdin to &gt;&gt; act) &gt; &gt;catchInput act = temp hGetEcho hSetEcho False $ temp hGetBuffering hSetBuffering NoBuffering $ act Hmm, I'd seen bracket being used in a StackOverflow answer, but it seemed quite confusing to me at the time. This looks much nicer. So, if I'm understanding right, `(get stdin)` is an IO action that would get the stdin handle, `(\restore -&gt; set stdin restore)` is a function that would take in a handle and return an action that sets stdin to that handle, and `(\_ -&gt; set stdin to &gt;&gt; act)` is a function that returns an action that performs the action given to the `temporarily` function after setting `stdin` to `to`. That last bit, I don't understand. We're setting `stdin` to `to`. Also, I just noticed that `get`, `set` and `to` are all values we pass in to the function, and I'm not sure why we need to do that. Can't we just use standard library functions to `get stdin` or whatever? Thanks so much for the feedback, and sorry for the late response.
&gt;You'll be joining a team of 4 MIT dropouts (among them, one owned a multimillion-dollar Bitcoin mine in high school, two were USA Math Olympiad winners, and one made ~$300k on stat-arb trading in high school). I can't compete with that (and anyway, just got a good offer for a Ruby job recently), and I'm not planning to apply, but that is a major perk if I had the opportunity to apply right now. I have so many questions for you. Man, why did I do nothing cool in high school?
[removed]
Speaking of getting the most out of hardware with Haskell, have you seen this? https://begriffs.com/posts/2015-06-28-haskell-to-hardware.html
There is a generic way to do it but it is tricky: https://www.itu.dk/~sestoft/pebook/pebook.html. Also a great post by Dan Piponi on the subject: http://blog.sigfpe.com/2009/05/three-projections-of-doctor-futamura.html. Oh and also: http://codon.com/compilers-for-free
The problem of teaching monads seems to be similar to the problems of teaching group theory and other algebraic structures. Algebraic structures usually make their laws talk about behaviors that are nice to work with a pen and paper (or in a text editor). This may seem like arbitrary “pencil pushing” to someone new to these structures. So lets look at fairly simple algebraic structure like a group. In Haskell notation they could be defined like so class Group a where (*) :: a -&gt; a -&gt; a identity :: a inverse :: a -&gt; a with the Group laws (x * y) * z = x * (y * z) x * identity = x identity * x = x x * inverse x = identity inverse x * x = identity Well these laws are just “on paper behaviors”. If we see “x * inverse x” in our code, we could just immediately replace it with “identity” because thats the behavior of how it works on paper. But because we know how the groups behave on paper, we can show some other statements must be true. Such as x = inverse (inverse x) foldr (*) identity xs = foldl (*) identity xs and quite a lot more. Groups end up doing a very good job talking about solving Rubik’s Cubes, counting problems, but they can also talk about things like addition over the reals, multiplication over the reals, “clock arithmetic”, and many other items. But we’ve run into a problem. It’s no longer good enough to become familiar with just one example case. For groups we can’t say, “it’s like adding”, because solving a Rubik’s Cube is very different to adding. Instead groups starts by defining how they behave when we write them on paper, then finding out more truths which can be used to solve many problems. A Monad is also an algebraic structure, which is defined as class Monad m where (&gt;&gt;=) :: m a -&gt; (a -&gt; m b) -&gt; m b return :: a -&gt; m a with the Monad Laws return x &gt;&gt;= f = f x m &gt;&gt;= return = m (m &gt;&gt;= f) &gt;&gt;= g = m &gt;&gt;= (\x -&gt; f x &gt;&gt;= g) So,we have given some on paper behavior. From these we can show some other statements that also must also be true. One function that we know makes sense is, sequence :: [m a] -&gt; m [a] — example case sequence [mx, my, mz] = do x &lt;- mx y &lt;- my z &lt;- mz return [x, y, z] and another important function is join :: m (m a) -&gt; m a join mmx = do mx &lt;- mmx x &lt;- mx return x But, if you haven’t used monads or seen several algebraic structures, it may be hard to buy into the fact that “pencil pushing rules” could produce something so powerful and useful. But because so few people have used monads before they start using Haskell, they are confused by them. So, it might be better to ignore “what a monad is” at first, and learn how to use a handful of specific cases. There are so many cases they can use, Reader, Writer, Parsec, List, IO, State, Either, ect. So perhaps it would be better to start with examples, and tell them just learn how to use them with them with the do notation. Show them things like how State (&gt;&gt;=) works like the word “then”, show that the Lists monadic interface is useful for when your dealing with non-determinism, and so on. Talk about how join and sequence work with each of these and how to read the statements above to make it easier to read for the next new monadic interface. This way its more like learning arithmetic before talking about group or ring theory, instead of just throwing them into group or ring theory. Sorry this became what it is. It started out more inline with talking about the monad section was talking about. But, at this point I’ve put too much work in to this to just delete it. TL;DR - Post rambling about my ideas on why monads are hard to teach. Which are basically, to a newbie, the monad definition and laws look like “just pencil-pushing”.
Alright, thanks for the help!
&gt; When to use foldl , foldr or foldl' is difficult for a seasoned Haskell developer to know. It is impossible for a novice! Is it really, though? Maybe it would help novices to explain the functions in pseudocode, like this: foldl (&lt;+&gt;) a [x1, ..., xn] = ( ... ((a &lt;+&gt; x1) &lt;+&gt; x2) &lt;+&gt; x3) ... ) &lt;+&gt; xn foldr (&lt;+&gt;) end [x1, ..., xn] = x1 &lt;+&gt; (x2 &lt;+&gt; (x3 &lt;+&gt; ( ... (xn &lt;+&gt; end) ... ) And then explain that `foldl'` gives you really a strict version of the first line, as long as you make sure that `(&lt;+&gt;)` is strict too. I don't think this is conceptually much more difficult than the intricacies of common OOP languages or Node.js.
I worry it'd be deeply confusing for the period when we don't cover all the commands, as you'd have a mix of the two without being fully aware of which ones worked smoothly together. Once we cover everything then that's an option for a staged switchover.
It is generic. Like &lt;foo&gt;gate for a scandal. &lt;foo&gt;hell represents programmer pain points. I think DDL Hell was among the first common uses (back to 16bit windows). Basically -- it isn't a definition of magnitude but of type. For example Fangate was obviously not at the level of Watergate, but both are of type "scandal".
 The only way to combine monads is with monad transformers, which are themselves monads. Isn't it misleading to say that `Transformer` is also a monad? It has a different kind and is more like used to produce "decorated" monads. Also I don't think that having to understand what you do (learn a bit of category theory) before you start doing it is a bad thing :). I am actually in favour of anything that makes programming less accessible in the sense that lot less junk software sees the light of day. Today everyone can produce poor-quality software (see google play store). --- Btw. thanks for the pointers to `Prince`. Wish there were a similar tool (preferably open source and in haskell) that would take markdown+css and produce presentations.
Why not?
I think Pandoc can do slides.
I think you are right, but you need to supply a latex template (or preamble...) and I'd really like to avoid latex at all cost (at least as a user, it may be built-in).
Meh. You can definitely make transformers that aren't monads when applied with a monad. But that would wholly defeat their purpose. In fact, IIRC, it would be logically incorrect to write an instance of `MonadTrans` that isn't a monad given any monad. I believe `MonadTrans` states laws that the type must construct a monad given a monad, and that `lift (return a) = return a`. So breaking those laws of `MonadTrans` would be just as bad as breaking monad laws or functor laws. Thus, a transformer *must* be a monad, given a monad.
See here for why this doesn't work: http://h2.jaguarpaw.co.uk/posts/strictness-in-types/
You are right.
That is definitely part of the problem with laziness.
Yes, that's part of my point. Teaching anything this abstract is difficult. So 1) teach it using examples and explain why these things are relevant. 2) Construct things in such a way that the Haskell novice doesn't need monads so early in their Haskell journey. 1 is difficult but 2 is okay provided you're writing the language/ecosystem from scratch.
Yes, I got this incorrect and forgot to go back and fix it. Monad transformers, once sufficiently parameterised, are monads. So form my slides, WritierT is not a monad but WriterT [String] Get is.
I never contradicted that. I said that `Transformer` is not a `Monad` because only after it being applied to a `Monad` it becomes `Monad`.
You at least need to break the intermediate html into pages which I do not know how to do (did not try, did not expect it to work) using the default template, or does pandoc just automagically assume where to insert page breaks?
lift really should be a monad morphism, so it should preserve monadic structure: lift . return = return lift . (f &lt;=&lt; g) = (lift . f) &lt;=&lt; (lift . g)
Some monad transformers are, in fact, monads, although they are still not Monads. type a ~&gt; b = forall x. a x -&gt; b x return :: m ~&gt; t m (=&lt;&lt;) :: (m ~&gt; t n) -&gt; (t m ~&gt; t n)
That's why the tutorials usually tell you to use foldl'. And GHC (with -02) will most of the time compile `foldl` and `foldl'` to the same core code.
Nice work Carter! Is there any reason *not* to use this? Is it a candidate for replacing the existing `ST` implementation?
Luckily, [HTML can be turned into a PDF](http://jspdf.com/) quite trivially these days :) 
&gt; People will not take the time opt in. They never do. You are right, take processing a database result for example. 15 years ago, when I was doing C++, the access to the database was low level but we had something called "cursor" which allowed you to process result of a query one by one. 10 years later, using Ruby (ActiveRecord), you just get a list or results. It's great but doesn't scale at all. Their answer (at that time) to laziness was : use "pagination" or "by_batch". It never really worked and was really awkward.
&gt;What are some of the cases where you really want laziness? Do you mean strictness ?
You're right. It's more like purgatory since you eventually get out. There's only about two types. Compile-time-purgatory and configure-my-machine-purgatory. Hell should be reserved only for error- messages-I-don't-understand-hell. Cmon folks let's be specific. 
I hesitate to say it's a full replacement for ST if only because with ST computations you needn't worry about different libraries having different errors for their operations. Which is a problem / challenge any Library using EitherT/ExceptT faces. I have some ideas for extensible explicit errors without pain that I still need to play with, but those ideas will likely only work for &gt;=8.0 Haskell code, whereas monad-STE looks to run fine for ghc 7.4- ghc 8.0rc4, so monad-STE should be pretty stable (user prefs in error Modelling aside). For errors where you want to abort to the top level of the pure computation, STE might make an interesting replacement for an Either or Exept based monad stack, and now have the opportunity to use mutable data structures in a manner private to the computation. It's also worth noting that with STE we now have a monad aside IO that has both a PrimMonad and MonadThrow instance (I personally think the STE monad throw instance is kinda ugly, but it's the only generic one possible). So that does create an interesting new opportunity for reuse, as long as you're careful about what notion of unsafety is in play. Which does suggest i need to tweak my internal STException data type a teeny bit so that certain unsafe but useful patterns that would be unsafe become safe. I'll do that later today, got to get back to the day's work :)
/u/dcoutts already gave a good answer I might just add that index-freezing is one possible answer to the frequently voiced criticism against the cabal-solver workflow of suffering from "having your dependencies change out from under your feet". Index-freezing is simply a different trade-off compared to the extreme of freezing all versions (which is obviously useful to have at your disposal in your toolbox as well!) 
Can you tell us more about your course - the kinds of things you've done so far, what people are liking so far, what people are grumbling about, etc... ? The more information you can provide, the better the advice that you'll get from here.
Also thanks! I presume you Also like how wide a ghc range it seems to build on? :)
Trying to get it all working, some things that kinda tripped me up a bit already: - The GHC bindist does not complain about missing llvm, libgmp etc. during configure/install - LLVM 3.5 is recommended on the GHC download page, but it actually produces broken binaries (very confusing error message) and has to be replaced with a 3.5.2 from the LLVM page (it's not available as a package in jessie) - The cabal-install that's available in jessie is too old to work at all with GHC 7.10.3 - Just getting the latest cabal-install from git and running ./bootstrap doesn't work (just works for releases) So, just waiting for cabal to build at the moment...
Gruffness welcome, I understand what a pre-coffee question on Reddit can do to a guy ;) Having not played with the internals of the package yet, just looked at it: are you considering something like an `unsafeCoerce` and some instance of `Exception` like `data MyInternalExceptionDoNotExpose = Evil ()`? If that's far too vague and/or nonsensical, I can try playing with it myself also, just curious if we're heading towards the same thing.
Ah yes I see, the dependency on `exceptions` makes that not possible. Well, good luck with the further work. This is a great contribution - thanks! 
&gt; What are some of the cases where you really want laziness? Anything where you take a prefix of an infinite list in Haskell to more easily define something, e.g. take n $ repeat x to create a repeated list of n repetitions of x or zip [1..] xs to add a numeric index to each element of a list. These kinds of things often end up looking much less readable in strict settings. There is also the problem that you can't easily define new control structures in strict languages without wrapping everything in lambdas.
No evil required. Just an extra sum and some shims for the officially sane way to map between the two 
There is no dep on exceptions in a fundamental way. I merely provide the monadThrow Look at throwSTE , it's got no exceptions constraint. Same with the other monad instances. 
Agreed. Although you could argue that the real culprit here is making instances of `IsString` where `fromString` is partial. I don't think it was ever intended for `fromString` to be partial.
I look forward to seeing it. Keep posting updates please!
In my opinion, this is a point *for* laziness. `foldr` is never the right choice in a strict language, and `foldl` always is. This is somewhat unfortunate as `foldr` is *the* list catamorphism and is somewhat more elegant. Through laziness, the *worst* choice becomes (very often) the best choice. This isn't accomplished by making `foldl'` any worse, but by making `foldr` better. Are there programs that are semantically correct in strict languages but not in lazy languages?
If you're trying to develop deeper understanding of monads, try picking up different monads and learning them instead. For example, start with Maybe, List, Reader, Writer etc. and see what problem they're trying to solve. I dunno what'll work for you but something clicked for me as I saw different monads and how a couple of functions (return and bind) are used similarly in all of them to implement different concepts.
So how hard would it be to add uniqueness typing to Haskell/GHC? Everyone who loved monads could still use them to pass around the environment token/etc., and the regular folks could use it in the straightforward way. Or does anyone have thoughts on Clean?
I'm talking about internal cyclic module dependencies, not external dependencies. In my early Haskell projects I tried to organize my modules in a way that seemed to fit the domain. But as the app grew I kept running into situations like module A needing something that was in module B, and module B needing something that was in module A. With a cycle of length 2 like this it's not too hard to figure out how to fix it, but often the cycle was much longer and it was not at all obvious how to break the cycle. I believe GHC can be made to work with cyclic dependencies, but I've never done it and my understanding is that in general it is not recommended. Using the pattern I described above I almost never run into cyclic dependency problems any more, and when I do they're always easy to fix.
I implicitly quantified with forall. In general transformer need not be monad...right?
This does give you compile time errors (though perhaps not as many as we would want). It behaves exactly like polymorphic type inference always does. For example, if we write anyIsString = "type|error" what type is it? Well assuming -XOverloadedStrings it is `IsString a =&gt; a`. Now we add another definition. str :: String str = anyIsString now what type is anyIsString? `String` because that's what type inference demands. Ok, lets add one more. reg :: Regex reg = anyIsString Type error. That's a compile time check if I've ever seen one. In general fromString should not be getting evaluated at runtime. It's a pure function, and virtually always is applied to literals (which by definition can't depend on any external inputs), so assuming GHC sees the opportunity for optimization it will run fromString and inline the result at compile time. I'm not any sort of expert on Haskell optimizations, but there is something wrong with my understanding of GHC if anything in the post is going on at run time. This is definitely the part of the hack that I'm least confident of though. Could you provide an example of OverloadedStrings causing a runtime explosion? I can't think of one that would work with the type system. (What exactly do you mean by runtime syntax error? I was not aware that the parser was even in the runtime system.) Of course it is still a hack, which is why I wish there was an -XCustomLiterals extension that could do it more cleanly. Edit: I'm totally wrong about the compile time optimization. Because laziness.
&gt; We need a compile-time mechanism to overload literals. Does `QuasiQuotes` qualify?
Oh, that's my mistake. I assumed that because the mkRegex function returns a Regex (rather than a more honest Maybe Regex or Either SomeErrorType Regex) it always returned a well formed output. If you look at the URI example I gave, it addresses exactly that issue. If we had the -XCustomLiterals extension, there would be a way to have compile time type errors. Of course TH can get the job done. It is strictly more powerful than type directed parsing, but sometimes you *don't want* more power. Static typing eliminates valid programs. It's nice to have hygienic macros, even though they are *less powerful* than unhygienic ones.
Anything type directed parsing can do QuasiQuotes can also do. QuasiQuotes has the issues of dropping lots of safety guarantees, being much more difficult to work with, and breaking mutual recursion.
Good example, mutually recursive (and sometimes even directly recursive) values (not functions).
&gt; not until i force you to force them to explain HAH! I am *unmatched*!
If you're interested in more of the math of Category Theory, then the book Category Theory for the Sciences by David Spivek is great.
Nearly every struct has members called sType, pNext and flags. I think using the library would be super awkward if you had to namespace these yourself. Hopefully in a couple of months nobody will care about this. The Strict language extension was nice too, although not strictly necessary as the package is machine generated.
Does it generate unsafe FFI bindings when it is safe to do? That can improve performance.
I actually wondered the same the about loops during the talk, but I didn't get a chance to ask. I too would appreciate an answer to this question.
Good point! It's because we started with a finance use case
That's a good question. The answer is "yes" as it generates no unsafe bindings because in general it's not ever safe to do it :) See here for a more in depth answer: https://github.com/expipiplus1/vulkan/issues/4#issuecomment-190140810
The problem is that you still need to declare what sort of QuasiQuoter you want (a regex, or uri, or sql or whatever). This is fine for larger DSLs, but type directed parsing really shines when it comes to tiny little DSLs that you want to be able to inline cleanly. You also don't get out of the fact that TH breaks mutual recursion. Such a function is basically just a way to add some safety to TH, which is an admirable goal, but you loose the extra benefit of type inference which type directed parsing gets you.
What means strict ?
`stack exec` being faster is the best part. They say milliseconds is a ton of time for a computer - it applies to some fast-paced programmers too.
&gt; This is pretty much as low level as you can get, and doesn't do any of the nice high level stuff Vulkano seems to be doing. Is anyone working on that? Or are there any plans to do that?
The name (probably) relates to "DLL hell" from Windows. DLL hell occured when a two programs that depended on the same DLL that was installed in a central location but required different versions. The "Cabal hell" problems of the past are largely the same problem, hence the name.
&gt; People will not take the time opt in. They never do. They take the time to opt in to explicit `IO` rather than using `unsafePerformIO` everywhere. Why wouldn't they opt in to explicit laziness?
regex-applicative does something like this: https://hackage.haskell.org/package/regex-applicative-0.3.2.1/docs/Text-Regex-Applicative.html#v:string
The github user corngood is working on something. I've not looked at it closely for a while https://github.com/corngood/vulkan-hl Most of my thoughts are down in https://github.com/expipiplus1/vulkan/issues/7 but I've not written any code in that direction yet.
Sorta. Open unions are doable using type level lists and GADTs {-# LANGUAGE DataKinds, GADTs #-} data Union (r :: [*]) where UNow :: a -&gt; Union (a ': r) UNext :: Union r -&gt; Union (a ': r) In the same vein, tuples can be imitated using type level lists. data HList (r :: [*]) where HNil :: HList '[] HCons :: a -&gt; HList r -&gt; HList (a ': r) type () = HList '[] type (a, b) = HList '[a, b] type (a, b, c) = HList '[a, b, c] ... I kinda wish Haskell had syntactic support for unions `(a | b | c)` like it does for tuples. And honestly, I wish such unions, as well as tuples, would be backed by a type level list rather than having hard coded instances and functions and what not. It's a far-off dream, and it would require some compiler hacks / primitives. But it would be nice to be able to easily write code that deals with arbitrarily sized tuples and unions, and still be able to use `(a,b)` and `(a|b)` syntax, all without losing performance.
Well, yes, I think it's pattern matching in both. I must say, I don't understand your code up there. Are you trying to force Haskell to analyze the types like a dynamic language would?
I think the mental overhead of deciding, when you write a function, whether it should be strict or lazy in its arguments, might be prohibitive. What if you could specify something like "this function preserves laziness/strictness", i.e. if its parameters are not evaluated it is lazy, and if its parameters are evaluated it is strict? 
&gt; I think the mental overhead of deciding, when you write a function, whether it should be strict or lazy in its arguments, might be prohibitive. Just like the mental overhead of deciding whether a function does `IO`? &gt; What if you could specify something like "this function preserves laziness/strictness", i.e. if its parameters are not evaluated it is lazy, and if its parameters are evaluated it is strict? What does that mean?
In my opinion, the mixing of strict and lazy in the same function is what makes reasoning about code in a non-strict language difficult. Examples: * I accidentally called a lazy function in the middle of a strict function * Special case of previous bullet: I used `$` instead of `$!` * My stuff is in WHNF, but is not deepseq'd. Whoops! * I used a "strict" monad transformer (like Control.Monad.Trans.Strict.WriterT) that isn't really evaluating everything immediately. * I used `fmap` on a Strict Map, but I forgot that `fmap` on Maps is actually lazy. I think what /u/PaulBone is saying is that in a strict language (where `() -&gt; a` is the only way to have laziness), you can tell from the type signatures that everything is deepseq'd all the time.
http://hackage.haskell.org/package/bytes-0.15.2/docs/Data-Bytes-Get.html
Short version, never use foldl.
I am confused about this point. I'm assuming that when they say "`f` is strict", they mean that `f` is strict in its argument, i.e. `f` is of the form f x = x `seq` ... Can someone explain why `foldl'` on a strict `f` and `foldl'` on a non-strict `f` would behave differently? 
Haskell uses non-strict evaluation (which is related, but not technically the same thing, as lazy evaluation). You can read more about it [here](https://en.wikibooks.org/wiki/Haskell/Laziness).
&gt; however my position is that a language should be strict by default with optional laziness. My experience is that languages that purport to offer this fail to offer enough laziness in their libraries for it to matter. I've simply yet to see anybody actually pull this off. e.g. Even combinators like (&amp;&amp;) start taking on strange types if you want them to permit short circuiting evaluation, it no longer gets to be a binary function on a common domain, but rather has to take a value and a lazy value and give back a value. You wind up needing to ensure that your strict language has proper tail call optimization or the moment you _can_ talk about something like Traversable it'll blow the stack for non-trivial examples.
With laziness I can make it handle the finite cases correctly and simply structure it so that it continues on to handle the infinite cases as well. I don't have to annotate it with 'WorksInfinitely' vs. 'WorksFinitely' it just works. 99% of the time users don't care and just having engineered it the common way handles both the finite and infinite cases. In a setting where I have to explicitly tag laziness you're stick picking for every single thing all the time which semantics you mean, so that 'continuous' extension catches a hitch.
Moreover, you can simply write the grammar for your parser in Haskell, with mutually e.g. recursive calls between the statement and expression parsers, etc. vs. dancing around trying to structure it in blocks suitable for calls to `defer`.
Cleaner parsers, understanding just what work your `where` clause will do if you have cycles in it, shortcircuiting (&amp;&amp;), writing control structures as combinators in the language, all the classic infinite list examples, extending traversals to infinite cases, etc. The moment `f _|_ = _|_` for all f bites you, you have to look around and come up with some`defer`-style hack and hope you can refactor the code to suit.
was thrilled to see this land and even more so to hear edward is still working on backpack, which is targeting a release in ghc 8.2. so exciting!
This doesn't work as written. https://hackage.haskell.org/package/parallel-3.2.1.0/docs/src/Control-Parallel-Strategies.html#Eval gets pretty close to your intent.
Two consequence of laziness that people don't talk about much: * Haskell's `if p then x else y` desugars to ifThenElse True x _ = x ifThenElse False _ y = y Because Haskell is lazy by default, it only evaluates the branch `x` or `y` that is needed. If we didn't have laziness, what's the right way to implement if-then-else? Would we have to make if-then-else a built-in special control structure? * Top-level declarations in your module aren't evaluated until they are needed. I wonder if strict languages (e.g. PureScript) have these desirable features, and if so how they implemented it.
With impurity I can make it handle the pure cases correctly and simply structure it so that it continues to handle the impure cases as well. I don't have to annotate it with `IO` it just works. 99% of the time the users don't care and just having engineered it the common way handles both the pure and impure cases.
"`f` is strict" means that forcing `f x` to WHNF forces `x` to WHNF, and I guess that's extensionally equivalent to `f` being of the form f x = x `seq` ... I have no idea why this is relevant to `foldl'` though.
Because it's *harder* to write a working program with `unsafePerformIO` half of the time. That's like asking why people "opt-in" to using types rather than passing `void *` everywhere in C.
Is x `seq` f x `seq` ... === f x `seq` ... another equivalent way to describe this property? 
I really like the analogy you draw between learnign pointers or recursion and monads. Maybe monads are hard to learn because they are so much their own unique thing. (BTW my favorite monad metaphor is the 'programmable semicolon', due to Don Stewart.)
Yes, and exactly the same way it's harder to write a working program where laziness is not explicit in the type system. [By the way, `unsafePerformIO` was a bit of a red herring. Really I'm talking about languages where `IO` is not reflected in the type system] 
If then else can remain strict in a strict language without a problem, because you can hide both the then/else arguments behind a suspension.
Sure, but when I want to write my own short-circuiting operator, then what? =)
No video sorry. We need to get a new camera at the meetup where this was presented since the current one cuts out at 20 mins :(
There's talk of an AnonymousSums extension here https://ghc.haskell.org/trac/ghc/wiki/UnpackedSumTypes
Thanks, that helps a lot. Does PureScript actually do this? i.e., does their if-then-else evaluate both branches? It makes no sense to strictly evaluate both branches of an if statement. But I guess this is the only way to implement lazy if-then-else in a strict-by-default language, other than creating a magical optimization that makes if-then-else lazy. (This hack would be referentially transparent, though, so it's not a terrible idea.) EDIT: By "magical optimization that makes if-then-else lazy", I really mean "magical optimization that makes if-then-else not evaluate the branch that isn't returned, if it's the case that the branch that isn't returned value has not already been evaluated".
I agree. Furthermore I claim that laziness is an effect and should be treated as such. It turns out that evaluation order of laziness-as-effect has huge consequences, the archetypeal example being given by `foldl` vs `foldl'`. This is why we need to see laziness reflected in the type system (and probably given a monad and comonad instance).
&gt; Normally, I just think 'ahh, that's just the domain he works in/has a hobby in' I just have a lot of hobbies. =) More seriously, I did give a talk about my particular school of black magic here: https://yow.eventer.com/yow-2014-1222/stop-treading-water-learning-to-learn-by-edward-kmett-1750
After thinking about this, I can't think of a satisfying way to design such a hack. You could make a hack that skip evaluations of whatever is in the "then" and "else" parts of the if statement (essentially substituting, under the hood, `if x expensiveA expensiveB` it with `if x (\() -&gt; expensiveA) (\() -&gt; expensiveB)`). But this could still lead to more evaluation than the programmer wants---for example, if `expensiveA` is actually a value that was bound in a `let` binding right before the if-statement, then that `expensiveA` is evaluated even though it's not needed. We could come up with a sufficiently smart compiler that does as much optimization as possible, but this still feels like a hack, and it would make the code harder for the programmer to reason about. (The operational semantics are basically impossible to determine unless you know exactly what kinds of optimizations the compiler is smart enough to make.) Wow, thinking about this has made me realize how much I enjoy laziness. I didn't fully grasp earlier that, in a strict language like PureScript, every `let` binding corresponds to an evaluation. I really hate worrying about superfluous computation (when I'm writing Scala or Python), and I didn't earlier make the connection that I would have to deal with this if I used PureScript etc.
It is a good paper. https://www.youtube.com/watch?v=hI0ajVy2xEk is the talk that went it. The requirement that all Conditionals wind up on the left of a monadic bind rather than the right is somewhat galling though. (section 4, paragraph 2) I'd rather have a separate type for specifying conditionals, because that fails the "is it a monad?" test. Also, extending this monad with an explicit constructor for handling `(&lt;*&gt;)` is quite important in practice for sampling from it, if you use MCMC-based techniques, because you can always do Gibbs sampling style moves for each side of the `(&lt;*&gt;)` independently. This makes a _massive_ difference in the performance of large models, as for independent draws we can Gibbs sample without any concerns about correlation. If each side of the `(&lt;*&gt;)` has a 10% acceptance probability, then without this you only accept the new state with probability 1%, whereas with, each side 'moves' with probability 10%. When you have 50 such `(&lt;*&gt;)`'s... well, then the naive implementation never moves in my lifetime and the independence-based one computes a "good" new dependent sample roughly every 500 Gibbs steps, and can make at least some progress during each step. The price difference is exponential vs. multiplicative.
 f :: Either A B -&gt; C
Oh. Then that's a purely semantic issue? Not one that really matters.
No one is talking about popularity here. I'm saying that you don't need laziness to be able to represent control flow with functions. Python's `collections`, `itertools` and `functools` modules can be used to piece together all kinds of clunky crap that way. It won't be easy to read or particular performant, but that is more about what the language has decided to optimize for than anything else.
I mean, would approximating an integral be count as sampling. Sampling usually means you throw in a bunch of random number seeds and take an average. Integration means you [approximate](https://en.wikipedia.org/wiki/Numerical_integration) the area under a curve using simpler shapes, roughly. For my definition of the type, the uniform distribution on `[a,b]` would correspond to integrating a function from `a` to `b` and then dividing by `b - a`. As for symbolic integration, that may be trickier. (Maybe you could use taylor series?) The problem is that if you say, `fmap` conway's base 13 function or something, you'll have problems.
My advise is: "You don't find monads, monads finds you" Check out this post that one of my teachers gave me when i was facing the same existance dilemma you are in. https://byorgey.wordpress.com/2009/01/12/abstraction-intuition-and-the-monad-tutorial-fallacy/ I NEEDED soo hard to master monads at the time. Why? I don't know. Perhaps because it was a topic I didn't find easy to understand/apply in everyday coding. Eventually i got pissed off. F*ck monads, f*uck functors, IO, do notation, I didn't needed them in my life before, nor would I in the future. Time passed by and finally it hitted me while showering. I bursted a loud laugh. It was sooo clear now! Realizing something by yourself will make a strong bond with it, create a crystal clear conception and make it easy to pass your wisdom to others. I trully recommend the "enlightenment" experience, so I'm only gonna say: Monads are like burritos. To speed up your travel of knowledge, I'd say to take a look and try to go thru this excercise guide the same teacher made. http://mumuki.io/guides/137-funcional-guia-monadas The downside is it's in spanish, but I guess that just using google translator would be enough. OT: We (the people who made Mumuki and myself, among others) teach functional programming at UTN FRBA (argentine university) using Haskell. Mumuki is a tool we (and the students, of course) found really handy to use. In there you will find excercises that covers most of what we consider Must-Know Haskell knowledge. The guide on monads is kind of bonus, but if you feel you need to overview previous topics, feel free to use it/ask. Cheers!
Yes, and Haskell is good at that. I don't recommend changing Haskell or even avoiding it. But being aware that laziness has it's issues. So if you're choosing Haskell go in knowing that these problems can occur.
Yes, that's something I was trying to show with foldl, foldr and foldl'. 1. Users must understand the choice between the folds. 2. Users must understand the characteristics of their code. 3. The characteristics of the code may change as the program is developed.
Quite the contrary, it's forcing the type checker to evaluate expressions like the runtime. The `'` *promotes* a value into a type, and a Proxy will take a type of any kind and wrap it so that a value of that type can be passed around in ordinary functions (sounds pretty much like `Val` doesn't it?). A `type family` is basically a type-level function, and a `class` does dynamic dispatching based on types (think values depending on on types).
Yeaah I know that works, I just wish we had some sugar for it ya feel
I mean a lot of his code is pretty intuitive. Like the law stuff is basically just math. Plus I'm pretty sure his comment is aimed at people who know Haskell to at least some degree. Because to have much of an opinion on Haskell you have to at least somewhat know it.
&gt;you don't need laziness to be able to represent control flow with functions. ...and of course there is also the vector family of languages (APL/J/K) while not lazy and not potentially infinite (and the Python [look-alikes](http://docs.scipy.org/doc/numpy-1.10.0/neps/deferred-ufunc-evaluation.html)). &gt;It won't be easy to read or particular performant ...what do you think of Common Lisp's [Series](https://www.cs.cmu.edu/Groups/AI/html/cltl/clm/node348.html#SECTION003410000000000000000)? 
cabal hell upgraded to cabal heck -- and soon cabal heaven.
They are strict languages that have very lightweight notation for blocks/thunks. And the flow-control constructs that users create look just a clean as the ones provided by the language. * [more detail on "if"](http://wiki.tcl.tk/1042) and [new control structures in TCL](http://wiki.tcl.tk/685). * [more details on :ifThen in Smalltalk](http://pozorvlak.livejournal.com/94558.html)
You don't need to use the "seq" function if the language is strict. This is an example of how to create a short circuiting function in a **strict** language.
I'm new to Haskell and I absolutely love laziness. There's something satisfying about setting a value and having no impact, then calling the value and having my memory usage jump by more than a gigabyte. 
That's pretty cool! Thanks.
[removed]
The most direct path from a simple interpreter to one with a pretty kick-ass JIT, if not a full compiler, is to do something like pycket: http://lambda-the-ultimate.org/node/5152 Sadly this require implementing the thing in rpython, so YMMV.
This is exactly how I thought it'd be done
Well, symbolically thinking, you can get everything in that example with just some numeric instances for (Prob Real); the question then becomes, given, for example, `(uniform 0 1 + uniform 0 1)^2` how do you generate the PDF/CDF for the resulting distribution? If you represent distributions by their characteristic function then `x + y` becomes `\t -&gt; dist_x t * dist_y t`, right? What about `sin x` or `x*y` then? Also there is the question of representing dependence with referential transparency, if `dist_x = uniform 0 1` then `dist_x*dist_x` corresponds to sampling that distribution twice, which needs to be distinguished from sampling it once and multiplying the result... is there a good way to think about that?
I would love to understand the meaning of monad law for example
so that mean that `foldl'`evaluate all is arguments as long as `&lt;+&gt;` does ?
I don't understand this; the following seems to work OK (along with the required Functor and Applicative instances): data Box x = Box x deriving Show unwrap :: Box a -&gt; a unwrap (Box x) = x instance Monad Box where return x = Box x Box y &gt;&gt;= f = f y
iterators aren't really composable either. You'd need something like pervasive[1] coroutines, or iteratee-style (e.g. conduit, pipes, etc.) I/O for that. [1] By which I explicitly *don't* mean explicit async/yield as in e.g. Python or C#. It really needs to *pervade* the whole language and I/O stack for it to be good enough to get composability at a level worth talking about.
Your definition is the definition of &amp;&amp; in a lazy language. In a strict language arguments are evaluated before a call, so 'b' would already be evaluated before you get to running the 'if' - you need it to be built-in to the compiler to get short-circuiting.
Have a look at PureScript's record syntax. It allows: ``` f :: { a :: A, b :: B } -&gt; C ``` All record types with exactly the same fields (and field names) are equal in the eyes of the type system. type A = String type B = String type R = { a :: A, b :: B } bar :: R bar = { a: "foo", b: "bar" } foo :: { a :: A, b :: A} -&gt; String foo _ = "that argument is acceptable" foo { a: "some", b: "record" } -- OK foo bar -- OK foo { a: "a", b: "b", c: "c" } -- type error foo { a: 4, b: "b" } -- type error It also allows any type which has _at least_ the given fields. The third call to `foo` above would be valid if `foo`'s signature was: foo :: forall r. { a :: A, b :: B | r } -&gt; String This is statically checked; it's possible in a static language, but the runtime of PureScript allows record field access by name, while the runtime of GHC Haskell is byte offset in a memory structure, so the polymorphic variant of `foo` would be an interesting implementation challenge.
Not Haskell, but shapeless Coproducts in Scala are perfect for this. 
One point I'd add is that index-freezing lets you "retroactively" freeze if you didn't freeze and didn't put in bounds on some local project and everything suddenly went *whoosh*. At that point, instead of just having to figure things out from scratch, you could choose to have index-frozen to approx the last time everything was working, and from there construct at least one known-good set of bounds. I imagine it will also be useful for other sorts of differential diagnosis where one may want to examine "the state of the world as of time _t_".
http://nikita-volkov.github.io/refined/
&gt; `foldr` is never the right choice in a strict language, and `foldl` always is No. `foldr` may or may not be the right choice for a strict *datatype*. It has nothing to do with whether the *language* (i.e. function application) is lazy or not.
&gt; Haskell's if p then x else y desugars to ... Does it? Where is this `ifThenElse` defined? 
Just wrap laziness behind a syntactically clean construct, for example `{}` let a = {expensiveA ...} b = {expensiveB ...} in if x a b 
On my phone so I'll be brief, but you want to wrap your action with liftIO from the transformers package.
I mean the monad laws writed in this post
Laziness would have helped me if they hadn't just forced the entire value. It seems like you're completely ignoring functions that return container types.
I just mean compared to code written in the way that the compiler/interpreter implementers envisioned.
Well the reason it's not performant is not because the technique is bad, it's because no one uses the language that way and there hasn't been any effort to optimize it. There certainly could be, if anyone cared, which they apparently don't.
Did you manage to end up in cabal hell by using stack ?? Only you can make it ! :))) (I know I abuse hell..)
You would still expect some compile-time guarantees. Haskell may soon grow a termination checker as we add more dependent type features. After that test passes your literal could still hang your program for a very long time, so maybe you would check that too somehow. Even then, I would see all that as a fancy extension. The usual expectation when you see a literal is syntax checked at compile time.
I'm so glad I haven't seen a screen like that for nearly a year. 
What about not enabling it? If it had simple and clean opt-in / opt-out mechanism it shouldnt be a problem
Under that proposal, you'd still have to do case analysis, using the following constructors: &gt; Each n-ary-sum type constructor comes with n data constructors, with systematically-derived names, thus: &gt; &gt; data (||) a b c = (_||) a &gt; | (|_|) b &gt; | (||_) c So is it again just a question of syntax, that you'd prefer to write f :: (A | B) -&gt; C f (a|) = ... f (|b) = ... instead of the more verbose but otherwise completely equivalent Either version? f :: Either A B -&gt; C f (Left a) = ... f (Right b) = ... Because the reason it's called a "tagged" union is that there are "tags" identifying which alternative you're talking about. In Haskell, the tags are the constructors, and case analysis is how you use the tags to distinguish between the left `Int` and the right `Int` in `(Int | Int)`. You do want to distinguish between the left and right `Int`, don't you? Because some languages also have untagged unions which don't. &gt; I would really like it to be a pretty direct instantiation of a coproduct category `Either` *is* the coproduct, just as much as `(|)` would be. Remember that universal properties are only described up to isomorphism: * `Either A B` is isomorphic to `(A | B)` * `Either (Either A B) C` is isomorphic to `((A | B) | C)`, and also to `Either A (Either B C)`, to `(A | (B | C))`, and to `(A | B | C)`. * `f ||| g` is isomorphic to `either f g`, and both are idiomatic ways to combine functions `f :: A -&gt; X` and `g :: B -&gt; X` into a function of type `Either A B -&gt; X`. I don't see an equivalent notation in the proposal for unpacked sums, but if there were it would be isomorphic to those two.
I mostly did it because it was pretty easy to do, and I figured that if I can get it to build on that wide a range, it's a good proxy for how build able / stable it may perhaps be in the face of an evolving compiler :)
Very interesting! I haven't looked deep enough into the Tidal website yet but it seems like this is only built for living coding - it uses its own main loop etc. Is there any support for integrating it with other programs, e.g., games? So instead of playing it through a synthesizer live, do the synthesizing and encode everything in a plain data structure that can then be converted to standard music files to be played by any device/program. 
This seems more like the general issue with Stackage lagging behind. In this case it's lagging behind 2 years: There doesn't seem to be any Stackage snapshot featuring a newer release than `transformers-0.4.2` from 2014... So I'm not surprised that Stack doesn't support newer versions `transformers` as those simply don't exist in the Stackage universe yet... but you'll surely be able to explain why this is great ;-)
While this sounds nice in theory, what if the opt-in/out mechanism ever broke and now Emacs is sending data to some remote server that I have little to no visibility into?
Based on the reaction to /u/tekmo's comment, it should have to ask permission. Which should mean there is some way of disabling the analytics in the future. I've just recently commented on the issue with a suggestion of enabling / disabling the analytics per project. Depending on the data collected, I would be fine with having it enabled on my OS projects, but may be uncomfortable having it enabled on my non-OS projects.
Read the f'in source. It's shipped as source code. It's not even big. You can read and check for yourself. 
Read the f'in source. It's shipped as source code. It's not even big. You can read and check for yourself. 
Sure, here is why : if we did not have stackage you'd never know there was a compatibility problem between transformer-0.5 and the rest of the set in the first place. But you'll surely be able to explain how closing your eyes and putting the burden on the unsuspecting user instead of having it on the table with the brightest men like you aware of it is great ;) 
Things seem a bit confused there. The featured item is Spicule, but the only clickable item under "Pre-Order Now" is one of the remaining USB keys from Peak Cut. Any ideas? Great trailer btw!
&gt; I just feel it is time to replace it with something cleaner. For slides in particular, there are several HTML5 solutions, but of course they all produce HTML output, not PDF. The thing is though that for actual modern presentations this might be preferable because it allows for all those fancy transitions and embedded media content. PDF is somewhat limiting in this respect. Some of those solutions are also usable via pandoc. I suppose that if you wanted you could probably generate PDFs from there, although I have never tried that.
Yeah, that would work I guess. The thing I am struggling on is pattern matching an undefined 'List1'. It's easy to say; Plus[ Plus[a,b],Plus[c,d]] =Plus[a,b,c,d] But what happens when we have longer lists, I don't want to pattern match each case.
Not many people have the vista to understand what's going on at this level. Thanks for laying that out.
There are several options. First, you can do a binary plus instead, which makes your problem disappear. Or you can combine conditionals with patterns, say for example data Expr a = Oper a | Plus [ Expr a ] deriving (Show, Eq) simplify expr = case expr of Plus list | all isPlus list -&gt; Plus (concat $ map unPlus list) ... isPlus :: Expr a -&gt; Bool isPlus expr = case expr of { Plus {} -&gt; True ; _ -&gt; False } unPlus :: Expr a -&gt; [Expr a] unPlus expr = case expr of { Plus xs -&gt; xs ; _ -&gt; error "..." } In general, look up (datatype-) generic programming, it is very helpful for this type for programs. I think the [uniplate paper](http://ndmitchell.com/downloads/paper-uniform_boilerplate_and_list_processing-30_sep_2007.pdf) is quite readable compared to some other things google gives me right now.
 (\()-&gt;...) is very ugly, but I used it because Haskellers know what it means. Of course such a language would denote the same structure with a prettier construct.
See, you say your not paranoid...
Thanks for the feedback. How am I not following the PVP? You mean that I lack one version number-part?
&gt; I'm don't know if I'll manage to get to try it with GHC8 in the very immediate future, though. That RPi will be 'deployed in the field' and I'll be separated from it for a while. Next time I'm around I'll plug in a new SD card and see how it goes... Sure, there's no hurry but I'd be curious to know it goes so feel free to let me know (or ping me if there are any other questions). &gt; How's the state of GHC8 and library compatibility - can I realistically expect that application like stack or my web app with tons of dependencies can be build with the nightly? At this point most of the issues are seem to be sorted. Even earlier in the release cycle most of the issues that I encountered were fix with `cabal install --allow-newer`.
Sounds good, I'll take you up on that offer. By the time I'll next mess with the RPi I'll likely have migrated the application to GHC8 anyway. Thankfully I can just pop in a different SD Card and try. 
I'm always skeptical of Arch because of the rolling release concept. What if Arch goes to GHC8 and my code isn't ready? The biggest issue was anyway not getting the packages, but the various bugs still in current Cabal/GHC that needed special flags to circumvent. Hopefully GHC8 fixes most of this 😅 Now if they could only make an RPi with 2GB…
Thanks! I'll make a post in the bug, not sure the guide is quite polished / general enough for the stack documentation 😀
&gt; if we did not have stackage you'd never know there was a compatibility problem between transformer-0.5 and the rest of the set in the first place. Now I'm confused... what's the argument here exactly? Why wouldn't I notice this incompatibility of `transformers-0.5` with other packages otherwise?
I don't wish to demean cabal, but I'm not sure when I'd prefer to use cabal over stack and vice versa, anyone wanna educate a newb?
[removed]
If you can explain how to represent strictness as a typed effect in a lazy language I would be very interested to see it!
The first one is rather artificial. `reverse` is a good example, as is `last = foldl (flip const) (error "Empty list")`.
It's not like he argues in good faith or his talent is not recognized already
1. Yes, and that's normal. Different ways to carry out the recursion, different behaviours 2. Yes, and I don't see in what occasion that would be a bad thing. Except if you mean that these characteristics could be reflected in the types, which indeed would be a good thing. 3. Maybe you'd need to be more specific. Do you have an example in mind?
As I understand it the intention behind gathering statistics is to see how haskell-mode is used in the wild to be able to improve it. So from my perspective analytics are not a bad thing per se, but the process of capturing them must be opt-in and transparent.
Yeah, it makes sense that different people would have different qualms. For me, it's unacceptable to send *real time* data to a *third party*. Sending aggregated data to a first party would not make the hairs on my neck stand up. This is why things like debian popcon are not as controversial (even though it's often cited in defense of Google Analytics -- hey, Debian does this too!) I.e. if haskell-mode sent your 10 most commonly used features every month directly to a server owned by the authors, it would feel entirely different for me. 
No, it's just such an odd target is all! It was meant more as a humorous quip rather than a dig or anything. :)
I cross-compile to Pi instead of trying to compile on the little board. Cross-compilation has its own set of issues, but at least one can try a build in a reasonable amount of time. I'll try to post a guide at some point.
I'd love to do that, but from what I understand the blocking issue is Template Haskell? My code uses some TH, and even if I got rid of that there's probably a dozen or so dependencies that would need it. In any case, be sure to post on r/Haskell if you write something, I'd be very interested.
Oh right. I'd forgotten I carefully avoided TH stuff. Harder to do if you're explicitly writing a web service: have to find a framework with really simple requirements.
IIRC I had over 100 dependencies in total, happens so quickly with Haskell. Building on a 1GB machine is really painful, would very much like to avoid that. I briefly looked into emulation with QEMU, but that didn't seem feasible either.
Really hope this goes well for you. Reckon you should crosspost to programming as well (maybe you already have). Props on promoting Haskell. As someone who has spent a lot of time making music using computer programs (and in the traditional ways, too... for over 20 years as a hobbyist), I find the idea of programming to create music just so counter-intuitive. I know a lot of people love it (myself included). It just seems like a terrible way to do it (not saying what you're doing is terrible, it just often makes me wonder if there aren't better ways to do it available... but perhaps that's what you're trying to do with your project). update: I posted it to /programming for you.
Thanks for the xpost, I'm afraid they're pretty conservative over on /r/programming tho! Writing code to make music works well for me, but I've been doing it for 16 years so can't really think about making it another way. There are certainly drawbacks, but also clear advantages. Last weekend we did an event with around 200 people, and a lot of them going pretty crazy, so I can say that it can work.
The overall performance of the compiler has definitely gone down over the last few releases. It's hard to quantify exactly because it's normally one of those "Oh, things just got a bit slower" here and there, things. But you can easily notice it just looking at things like Travis CI build times for different versions of the compiler for some package, which (noise withstanding) is often pretty consistently slower for most projects, release-to-release. Go try GHC 7.6.4 and it is noticeably faster than any of the more recent releases, IMO. Pandoc has an especially large dependency chain and is going to take a while to compile anyway. But there's no reason we can't improve what we have already. We just don't have benchmarks that are characteristic of such large, common builds that are really soul-sucking.
Yes, `Generic` is definitely tricky and there are some explosions here that are difficult to resolve. But this case is at least known, and people are noodling on how to solve it.
Thank you. `fltkhs` is actually a really good example, I think, that we should seriously consider adopting. I remember we chatted at Compose about how it took unreasonably long to compile, and you managed to make some of the pain go away. It's advanced, real world, and has a lot of features, but dependency wise it's very small and it clearly stresses some advanced aspects of the compiler. I think it's close to a perfect fit.
I am not so sure that you could make it easy and convenient to abstract away writing new control construct in a strict language without introducing an entirely new language construct like Lisp does with its macro system (which essentially does not evaluate its parameters before calling the macro code either). Maybe nobody cares in those languages because they all think it is impossible to do since they lack experience with a language that does make it easy.
I think haskell-src-exts used to compile so much faster in 7.6.
Any language that has "the implementation is the specification" semantics is an odd target language, you are not alone in that feeling.
I agree about 2. In fact, I think a monad-less introduction to IO in Haskell is not only possible but healthy. We start out by taking about how IO is managed through "IO actions" and the IO type and only later talk about how this generalizes to monads. This would go a long way to dispel the idea that IO is some weird magic and the concomitant misunderstandings of monads in general. You don't need to understand the full abstraction to start using do-notation an IO to write interesting things. The abstraction can come once you've got the hang of it. I'm not convinced the language needs to change much to support this, although better (customizable?) error messages would go a long way. The real impediment is how "Haskell" and "monads" have become conflated in the folklore.
That law means exactly what it says: return x &gt;&gt;= f Must be completely identical (at least visibly) to: f x For example: return 5 &gt;&gt;= (: []) Gives you `[5]` And: (: []) 5 gives you `[5]` `(: [])` is a function that puts its parameter inside a one element list.
I think it's a bit of a mistake to think about programming music in terms of efficiency, necessarily. Rather than finding a language to express an idea in an efficient manner, I think it's more about finding one that lets you work with an idea in an explorative manner. In terms of TidalCycles I mean one that allows you to combine patterns in a wide range of ways, to explore interference patterns beyond your imagination. Actually Victor says the same thing in that essay - "create by reacting". It's not really about power either.. You don't need many words and ways to combine them before you have an explosion of possibilities to explore.. So minimalism works well. Tidal is a EDSL so the whole of Haskell is available, but in live performance I tend to have only a few seconds to make each change, so there's no time or headspace to do anything far-reaching. I enjoy Bret Victor's talks very much, but I still think text is fantastically expressive, and shouldn't be discounted as somehow lesser than imagery or geometry. His demos that I've seen have all been about how to describe geometry with geometry, which of course works well in a rigged demo. I am interested in the idea of conceptual space, though, and have experimented with spatial syntax ( http://slab.org/colourful-texture/ ).. I think he's going in a really interesting direction and I wished I had some of his vision! I don't play the piano I'm afraid, so can't follow your example, but think I get the gist.. Tidal is pretty good exploring symmetry in terms of both time and value. I made TidalCycles to make improvised techno though, and it works well for me -- I think I can work as an equal to live percussionists, for example Yee-King (who you might know, of as an AFX fan..): https://www.youtube.com/watch?v=uAq4BAbvRS4 Note that isn't a performance of music, but an improvisation, making music up as we play it. 
I agree, it is possible and good to teach IO in that way. And as you say it's not perfect due to things like error messages.
1. Only for semantic reasons: associativity should be the only concern. 2. Generally make it as easy as possible to understand the code's characteristics, giving information in types is one option (which is typically how laziness is handled within a strict language). 3. If you have a call to foldr (or whatever) and the definition of the higher order value changes, possibly in a separate module, then it's laziness changes and this may change which fold should be used. But the developer may not notice because these things may be in different modules.
He reads a lot of books in a lot of different domains.
When thinking about the `Box` example, I was thinking of a category of recipes or physical actions (in which stuff can't be duplicated, for example). You can't just conjure up boxes.
&gt; Does this version improve the recompilation checker? I've been using stack a bit lately and I often have to use --force-dirty and stack clean. I'm not aware of any changes in this regard. But I've recently opened [#2063](https://github.com/commercialhaskell/stack/issues/2063) which is somewhat related. What were the cases where you expected recompilation but it didn't happen? Maybe an enabled [`rebuild-ghc-options`](http://docs.haskellstack.org/en/stable/yaml_configuration/#rebuild-ghc-options) would help in your case? &gt; Does this version allow you to run stack concurrently (in different directories) without risking a corruption of a shared stack work tree? `stack` has some locking mechanisms to protect common state. I routinely run builds in one project while installing a tool somewhere else.
Don't feel dumb, this stuff confuses most people at first: That one is similar: (m &gt;&gt;= f) &gt;&gt;= g is the same as: m &gt;&gt;= (\x -&gt; f x &gt;&gt;= g) You can also think of the lambda as `(&gt;&gt;= g) . f` or in other words applying f first and then bind (`&gt;&gt;=`) it on g. Which I admit is probably still somewhat confusing, maybe an example will help: ([1] &gt;&gt;= (: [2])) &gt;&gt;= (: [3]) [1, 2] &gt;&gt;= (: [3]) [1, 3, 2, 3] is the same as [1] &gt;&gt;= (\x -&gt; (: [2]) x &gt;&gt;= (: [3])) [1] &gt;&gt;= (\x -&gt; [x, 2] &gt;&gt;= (: [3])) [1] &gt;&gt;= (\x -&gt; [x, 3, 2, 3]) [1, 3, 2, 3] This law is the associative law of Monads, and is somewhat equivalent to the addition version (`(x + y) + z = x + (y + z)`) it just has to be modified for the types to line up. As: `m &gt;&gt;= (f &gt;&gt;= g)` doesn't typecheck except in rare circumstances such as when `m` is a function.
I want to second this and say I've had success with showing pieces to my co-workers with no previous Haskell experience who then went on to buy the book themselves.
This is true, but their current focus was (and still is afaik) eliminating differences from Haskell.
Might I suggest JavaScript? I realize that the project isn't really at any stage where it makes much difference, but having both the client side (browsers) *and* the server side (node.js or just regular runtime) down is a game-changer. (Still impatently waiting for GHCJS to be Ready For Production(TM))
What time!?
One problem I have been noticing more and more with GC'd languages is that GC only accounts for RAM. Most languages don't give you a good or easy way to hook arbitrary resources into the reclamation system. Finalizers, if they are even provided, come with tons of caveats and are run at unpredictable times, potentially in the middle of another IO operation. What this means for disk IO in Haskell is that the usual persistent data structures are a lot harder to implement if the backing store is on-disk instead of in-memory, as the simplest way to do it involves implementing your own GC of the backing store. &gt; What would be the best way to retrieve/store blocks of memory from/to disk to/from RAM? What would be the best way to append binary data to a file? The ForeignPtr API allows allocating arbitrary mutable blocks of RAM. I think the name is a bit of a misnomer, as it's used very heavily in Data.ByteString which is mostly used as a pure Haskell library. Using ByteString is probably the easiest way, although if you're not concerned about portability it's not much harder to use arbitrary ForeignPtrs and call out to the regular C APIs for file access via ForeignFunctionInterface. (If you are concerned about portability, just be careful about which C APIs you use)
When working with Python you also need to remember that simply looking at a value might change it. Passing the same iterator into two methods is almost certainly a bug. Mutable iterators are the quantum mechanics of programming.
I think this is the slowest single package to compile that is regularly used. Good example test case of having loads of definitions. 
Oh, I see what you're saying, now. You mean some function in a library wants a list even though you have an iterator. I think in a language like that, most people would adopt a convention to write functions over iterators, to allow it to work in far more scenarios. That just seems the most polite.
I see if there's anything I can do ;-) It seems Haskell + ARM is a very new development, actually looked like it's basically not working from the Google results I dug up before trying. I just wanted to write this up to show it's a total mess but actually doable. This issue certainly wasn't on my radar till I had the need to deploy something in a household without a PC that runs 24/7 (strange, I know!). I'll see how GHC8 changes things, it seems several of the bugs I encountered are already fixed...
Yes, that's precisely the efficiency I'm talking about - efficiency of explorative power expression. Geometry is an alternative way to express truth. Let's call algebra the contrasting one. Algebra is, then, essentially, textual language, but textual language that can modify its own semantics to a degree. However, they don't *have* to be contrasting. A good way to express truth lies somewhere within both of them and a meta language of sorts that allows one to build abstract and concrete definitions (a la haskell), but also to quickly use those definitions as a language/DSL/problem-oriented-language to make meanings within. A conversation between geometry as "use : work on content / semantics", and algebra as "design : work on context / syntax", perhaps? I wasn't trying to discount text, per se. I find brett victor's observations about language quite interesting, and in particular his expressions about building a language in which to *experiment*. The efficiency I was talking about, for example, might be the ability to express a pattern on a waveform level, and then take that abstract pattern and apply it to the time relationship of the notes (as a loop so to speak). One can't do that in traditional musical notation. Traditional musical notation is a geometric graph designed for "use", not for "design", so I would like it to be able to express changes in its own syntax, so you can do things like "set up abstract definitions of patterns", etc and then apply them to different things. It's just unimplemented pipedreams I've been dreaming of and thinking about for a very long time, but it's quite interesting. There have been various inroads people have made into designing such things, but they're mostly routed in the limitations we impose on pen/paper requirements. The computer need not impose such limitations, and can build less static languages. Runtime languages would be pretty nice to have... and doing "live music programming" is a great example of this... I wonder if Idris would be a better language to implement things in because of dependent types?
I had a feeling that I was focussing on the word efficiency there, but maybe 'liveness' is a less contentious word for this. I like to think of this distinction as analogue vs digital, continuous vs discrete, or language vs prosody.. I think these are all getting at the same fundamental dualism. I think Paivio's dual coding theory is a good way to think about how this works in perception. I think what you say applies to Euterpea, which I think is constrained by traditional music notation and the keyboard oriented MIDI spec. I don't read traditional music notation and so I don't think it constrains me, although TidalCycles is influenced a bit by Bernard Bel's Bol Processor, so there's a link with the cyclic structures of time in Indian Classical Music. TidalCycles works at 'control rate' although this can get into 'audio rate' via granular synthesis, and you can switch to its synthesis engine in SuperCollider and modify it live there. I'd definitely encourage you to follow this line of thought!
&gt; If some other person, a beginner or an intermediate user, has a problem with a particular set of packages, what is his way of having someone coming to help him or reproducing the problem ? ---- *If there's something strange, in your install-plans. Who ya gonna call? [Hackage-Trustees!](https://github.com/haskell-infra/hackage-trustees)* *If there's something weird and it don't look good. Who ya gonna call? [Hackage-Trustees!](https://github.com/haskell-infra/hackage-trustees)* *I ain't afraid of no Cabal Hell* *I ain't afraid of no Cabal Hell* ---- SCNR :-)
This is just an example, I will be using trees to perform symbolic manipulation with other operators. 
I'm an occasional user of -hb, and the lack of thread safety has probably not affected me since most of my applications are not multithreaded. I'm sad to see it go, but it might not be worth the effort to keep. 
This is quite cool, but it all seems extremely clunky: Having to shatter your projects into many files like this, and encode their wiring in a cabal file seems really awkward. 
And most people won't be able to trust it anymore once any such thing exists, especially in the code editor. Debian's popcon can also get similar data by tracking downloads, so it's not the same kind of personal info.
a compiler
https://github.com/ezyang/ghc/blob/ghc-backpack/testsuite/tests/backpack/should_compile/bkp01.bkp unit p where signature H where data T x :: Bool module A where import H data A = MkA T y = x unit q where include p unit h where module H where data T = T x = True unit r where include h include q
I haven't really thought about how well compile-time checking of something like !a would work, I'm just puzzled by the idea of not doing anything being an effect. :)
An important part of this in my opinion is that `bindIO` and `returnIO`, which are where the magic lies, should have their own names, rather than being hidden behind the monadic interface.
Couldn't this profiling option be disabled when used with the threaded runtime? Fixing it sounds like a low-priority but nice-to-have fix.
Any language I don't want to write seems like a very reasonable target for a compiler, to me. 
&gt;I think in a language like that, most people would adopt a convention to write functions over iterators, to allow it to work in far more scenarios. That just seems the most polite. Right, but they don't. That's what im saying. 
[removed]
Do you suggest this for all deps in the .cabal file, or only for the deps of the library? build-depends: base &gt;= 4.3 &amp;&amp; &lt; 5 , parsec &gt;= 3.1.2 &amp;&amp; &lt; 4 , containers &gt;= 0.5 , unordered-containers &gt;= 0.2 , vector &gt;= 0.10 , aeson &gt;= 0.8 , text &gt;= 1.0 &amp;&amp; &lt; 2 , time -any , old-locale -any To this: build-depends: base &gt;= 4.3 &amp;&amp; &lt; 5 , parsec &gt;= 3.1.2 &amp;&amp; &lt; 4 , containers &gt;= 0.5 &amp;&amp; &lt; 0.6 , unordered-containers &gt;= 0.2 &amp;&amp; &lt; 0.3 , vector &gt;= 0.10 &amp;&amp; &lt; 0.12 , aeson &gt;= 0.8 &amp;&amp; &lt; 0.12 , text &gt;= 1.0 &amp;&amp; &lt; 2 , time -any , old-locale -any Yes I tested with aeson 0.10.0.0, 0.11.0.0 and 0.11.2.0 :) I did not test all version steps with the other deps (only for aeson, because I'm aware there was some problem). For the other deps I simply put the upper bound on "less then (&lt;) the next API breaking version number from what my lib currently builds with on stackage nightly". I do not know what to do with time and old-locale's bounds. Frankly I do not know anymore why I put `-any` there :) What I do not understand is that I'm testing that my libs tests still work; but I do not test that any "leaked API" from my dependencies to did not change. Could you explain how the upper bounds solve that problem?
Ok, so this seems useful. The paper is very informative, I have not found that before. 
&gt; not sure how that would work with 2^N configurations possible each with at most a single user behind it It probably wouldn't. But that's a theoretical limit we're several orders of magnitudes away from. The effective configuration space has far more structure than the abstract 2^N configuration space. In fact, if this 2^N figure was true, you wouldn't be able to install almost anything without relying on Stackage, as the likelihood of hitting a working install-plan would be practically negligible, like finding a needle in a haystack. The data I gather via http://matrix.hackage.haskell.org/ also show a huge amount valid install-plans. Moreover, I'd expect to see way more issues filed at the Hackage Trustee issue tracker, but things seem to work fairly well... unless there's no `cabal` user left anymore, and everyone's only using Stack... :-) Finally, Stackage in some sense doesn't really fix the problem at the origin (NB: this doesn't make Stackage less valuable for users!). Stackage merely shields the user from inaccurate meta-data by placing another layer of meta-data on top. You still need to contact the upstream authors and make sure the meta-data is fixed retroactively on Hackage. Fixing up Hackage via occasional `.cabal`-edits is quite manageable for the Hackage Trustees, especially if package authors follow the Hackage upload guidelines. 
Just glancing through the preview, it would be great if you could fix page 6 where it refers to `main` as a function a few times.
Thanks, I didn't know the `mmap` function, really looks interesting, I'll take a look to it. though you are probably right about the overhead; actually I'm not looking to do it extremely efficient (I'd not be able anyway), just efficient enough to be useful. I think I'll just try with a combination of `hPut`, `hGet` and `hSetPosn` (from `GHC.IO.Handle`) to for the block part. 
Sorry. I didn't type check that. Try this: data Expr a = Oper a | Plus [ Expr a ] deriving (Show, Eq) extractPlus :: Expr a -&gt; Maybe [Expr a] extractPlus e = case e of Plus e' -&gt; Just e' Oper _ -&gt; Nothing optimizeTopLevel :: Expr a -&gt; Expr a optimizeTopLevel e = case e of Oper _ -&gt; e Plus exprs -&gt; case mapM extractPlus exprs of Nothing -&gt; e Just exprListList -&gt; Plus (concat exprListList)
I would be wary of using so much unicode in the book. Novices should probably be seeing Haskell code as it usually is in the wild, not the fancy unicode symbols.
All those subjects have been hashed many times over. But I believe even when you have the problem "fixed at the origin", having a global clock will still have tremendous value for both users and library authors. Your fix will make it easier to produce this layer, and it will still make sense. Those version represent a contract on both end (producer/consumer), just like library numbers within hackage. different contracts, different numbers. If you had a more expressive type system, I suspect they'd still be there (at even more levels !) except it would not be numbers. In the end it boils down to economy of scale : without a reference point, you can't have them. Now what you say is the reference point should the head of hackage convoluted in time with the install history of the unsupecting user, and that is less that 2^N.... great point of reference for everyone..
Yes that works. It is also limited by this; Plus / \ Plus [a] / \ [b, c] 
`main` is, in fact, a function. So I think that's good.
Refactoring only, no behavior change. (The generated code may be slightly different, but that's only because I was able to eliminate an explicit record that was passed around for direct references, so the optimizer would do better.)
Looks like a nice book, but $10 (suggested $15) seems steep to me. I know nobody's getting rich on that kind of money, but even with the sample section the closed form represents a risk that the rest of the book trails off. I would honestly probably pay $3-$5 sight (mostly) unseen, or $10-15 if I got to see the whole thing and decided it was good. The amateurish cover art and design is not helping. The title isn't helping either. I was super unclear what the book was trying to do until I read the article about it. Not a lawyer, but without permission the subtitle is probably Lanham Act infringement of *The Little Lisper.* Cool idea, but has a ways to go.
[Here's](http://imgur.com/MTJJYN0) my cover / title idea. Probably still amateur, but seems more plausible to me. The 1905 Coolidge painting should be in the public domain.
Regardless of legal things, I feel like _morally_ the "little" theme is something that is in the hands of the originators. The authors may want to call it something like "(A Haskell Book in the tradition of the Little series)" just to be clear that it isn't done as part of that series itself.
Is there a reason to use the vanilla runtime? What do I lose out on if I compile with `-threaded` no matter what?
Exactly. Especially for the string types, there should be a library of signatures representing them. Even further, given your naming convention, there should be `string-like-bytestring` which exports a module named `StringLike` so you can just `build-depends: string-like-bytestring` and no wiring is necessary--that's mixin linking! (BTW, on the naming front, I am kind of liking `Str` as the "generic string name" as opposed to `StringLike` which is long and clunky.) &gt; I expect backpack to include a tool to support automatically generating them. Based on personal experience (having implemented a tool for automatically generating signatures), I actually think it is not a good idea to generate them. For example, a client of ByteString may use functions from Data.ByteString and Data.ByteString.Char8, when really they logically just wanted the functions from Data.ByteString.Char8. An autogenerator can't see this intent and will just generate an hsig file for each one, and you'll end up having to manually edit it anyway. And the manual signature process is not that bad anyway; unlike hs-boot files they rarely change (except when you want to add something new, which you have to do manually anyway.) &gt; EDIT: Btw, do you know how languages with richer modules would do this? Mixin modules are an old idea but the design space is extremely underexplored in real life. In ML, you would have to functorize your application over the string data type, and then apply the functor to a few of the known types and let users use those (generally a bad idea to ask users to instantiate, since functors are generative.) &gt; I've heard of "first-class" modules; does that mean a record that you can literally import? First-class modules are modules that can be represented at the value level; so you literally have a variable x representing a module, and then you can project out types and values from it, e.g. `x.T` as opposed to `M.T`. &gt; Would the wiring then be a normal program, or is there still a separate language for defining/instantiating signatures? Remember that in ML, there is the core language and then there module language. This distinction doesn't go away when you have first class modules; there is just a new core language construct which lets you "unlift" a module expression into a core level expression. ML first-class modules can be a bit clunky to use in practice. You can read more about it here: https://realworldocaml.org/v1/en/html/first-class-modules.html To completely erase the distinction, you need a design like 1ML, where all language features are seen as a "mode of use" of modules.
I am very glad that GHC supports this file format (for testing), but this format is very unsuitable for real software. The big problem is that, at scale, you don't actually want all your modules in one file; it thwarts recompilation avoidance. There's an additional problem that when the component structure is in a file like this, Cabal has to do more work to figure out all the components it wants to build (although in principle this is something that could be fixed with sufficient engineering.)
On quick inspection it already looks quite nice to me. Parsec is of course another great example of a package to try this on (not saying _you_ should of course :-P).
I probably should. Doing these examples helps me wring bugs in the implementation anyway.
This allows backpack to be an implementation detail of tagstream-conduit, which is cool, it cleans up the implementation nicely I think. Is it possible to have this package also export the `Entities` module with "holes" that can be filled by users of the library with their own Text types? Is that on the roadmap?
Yes. To be specific, you can write a package whose main library has holes, and then clients of that package can fill it in later. (Unfortunately, cabal-install doesn't know how to deal with this case yet; it's one of my big ticket items for the implementation.) Last year, I suggested that we should add the capability to export multiple libraries from a single package https://www.reddit.com/r/haskell/comments/3cu5nu/feedback_requested_for_supporting_multiple/ but I got a very chilly response. So this is not on the roadmap: if you want a library to be externally visible, give it its own package.
Yep, that's right. (This is a DIFFERENCE from the original Backpack paper, every module is a "module factory" unto itself, based on the set of signatures it transitively depended on.)
`main` is not a function (how do you apply it to an argument? a function type has the form `a -&gt; b`). Its type is `IO t` according to the standard, also called an action or IO action. **Edit**: Similarly, putStrLn :: String -&gt; IO () is a function that returns an action, and putStrLn "abc" is an action.
don't know, i always compile `-threaded` and run with `-N`
Could you write up how you do it? I couldn't get anything at all working, didn't find any working tutorial.
To be concrete: -- string-like-bytestring.cabal ... library build-depends: bytestring reexported-modules: StringLike.ByteString as StringLike where `bytestring` exports `StringLike.ByteString`, which would have a `type Str = ByteString` declaration (if valid). Is that all? That's clean. (Personally, I *hate* non-word names (like `Str`, or `StringSig`) since I hear the code in my head when I think about it (`Strrrr`), and they're harder to dictate ("spell es-tee-ar"). But I agree that `StringLike` is awkward. Maybe `AnyString` or `AString`, like `SomeException`? (`AString` also puns on "Abstract", as /u/sinyesdo/ suggests). Or `Textual`, an adjective for what is shared by the nouns `String` and `Text`? Or just `String`, except it clashes with Prelude... Anyways, reading `Str` is still much better than reading redundant `.Text` and `.ByteString` modules :-) 
I like Textual. Is it common in languages with modules to name them with adjectives?
New suggestion: String**ish**. (If there was ever a time to popularize the use of the **ish** suffix this is it!)
I would even be wary about the card suits, honestly. I don't know how to type a heart off the top of my head, and the point of most programming books is to follow along at home.
Since it's an abstract type and some associated operations, could we get away with calling it StringTheory?
There's `CharacterSequence`. The natural abbreviation is `CharSeq`, which is pronounceable, but probably sounds too much like `Parsec` (or "car sick").
 a If Emacs becomes spyware I'm upgrading to ed, the most secure of all the editors with the most consistent error reporting. . w ? q ? ^C ? Help! ? 
Ah, I see the confusion. Technically monadic actions are functions. If you look at the type of a monad you'll find an arrow hidden in it. It's just that the argument is supplied by the monadic context rather than being explicit. That's what the bind operator is about. If evaluating the same expression in different contexts gives different answers, there must be a function there somewhere, because referential transparency. 
[Alternatively](http://imgur.com/AfRFusK)...
I see what you did there. [This](http://imgur.com/STPjAS9) is the best I could do with it. Sorry the PNG is a little large, but it's a pretty small and delicate image feature.
`STRING` (i.e. upper-case)? It looks cool. This naming scheme works for any type we want to signature-ize. Also, it lets people know that Haskell is SERIOUS about modularity. Windows filepaths are case insensitive, possibly relevant. 
Shouldn't be a problem; you probably aren't going to have both STRING.hs and String.hs in the same project.
Isn't it trivial? newtype Strict a = Strict a instance Applicative Strict where pure = Strict Strict f &lt;*&gt; Strict x = Strict (f $! x) and then ban `seq`, `($!)` and BangPatterns so that all strict operations have to go through Strict, thereby ensuring that any use of the strictness effect is tracked in the types. Of course, an expression of type `Strict a` might still inefficiently accumulate thunks if the applicative expression is built lazily, using `foldl` for example: foldlStrict :: Strict (a -&gt; b -&gt; a) -&gt; a -&gt; [b] -&gt; Strict a foldlStrict f x = foldl (f &lt;*&gt;) (pure x) That's morally okay though, because we're considering strictness to be an effect and we're considering laziness to be pure. So a computation which mixes both strict and lazy evaluation should be marked Strict, and it is. Tracking laziness as an effect and treating strictness as pure, as you were suggesting earlier, sounds much more useful but also much harder!
Not sure, but I know there are part of the library ecosystem that don't work correctly without out (eg there's something in the conduit stack that requires threads and hands without them).
If you have a pi image, you should be able to mount it directly and boot. There are a ton of resources on raspbian + qemu out there. https://wiki.debian.org/RaspberryPi/qemu-user-static http://paulscott.co.za/blog/full-raspberry-pi-raspbian-emulation-with-qemu/ https://www.raspberrypi.org/forums/viewtopic.php?f=29&amp;t=37386 You should be able to set your VM resources as big as you like and be good to go. Which part didn't work for you?
&gt; I have never found the arguments against laziness to be compelling. People who want Haskell to be strict confuse me. You might as well say that C shouldn't have pointers, or Rust shouldn't have linear types, or Lisp should have more syntax. Haskell is The Non-Strict Language. What would be the point of strict Haskell when OCaml, Scala, and F# already exist? This is not even to mention that huge swaths of basic Haskell functionality -- e.g., tons of monadic code, maybe most of it -- simply wouldn't work in a strict language.
If you want to build a persistence log, just batch the writes with the bytestring file append. You can easily persist up to a million events per second that way.
I'd love to see a way to add comments to function documentation on Hackage, a'la php.net. There is a lot of small pieces of information that could be contributed by passersby. And later that feedback could be integrated directly in the documentation itself.
My thoughts too. And if you did, you could put them in different `hs-source-dirs`.
&gt; Tracking laziness as an effect and treating strictness as pure, as you were suggesting earlier, sounds much more useful but also much harder! Strange. I thought an explicit memoizing `Thunk` data constructor sounds very easy.
Hmm, I think this is not really what I wanted. I wanted a type that lets me see when a variable is evaluated to WHNF.
Try something like this: doSomethingWithTime :: UTCTime -&gt; String doSomethingWithTime = undefined -- your code here main :: IO () main = do time &lt;- getCurrentTime let str = doSomethingWithTime time putStrLn str You can extract the value when you're working in the `IO` monad, then give it to a pure function (here `doSomethingWithTime`), which doesn't need to care whether it originally came from `IO` or not.
Thanks! I guess the mistake I made was trying to do it on OS X, instead of Linux? I was also confused that the tutorials suggested I needed to supply a special kernel (why couldn't an emulator use the actual kernel?). Then, I read in several places that the whole setup refused to work if you increased the amount of RAM over the default, which would be the main point for me. Maybe I should've just used my Ubuntu VM and would've had more success...
ByteString is not a *text*.
I've been doing this in Python lately. It is hard. Good error messages are really hard to put together. PHP's Composer tool actually does a good job of this, although it isn't as flexible as it could be. Of course, both of these have been done imperatively. It would be nice to see a good implementation that doesn't just use a giant mutable map. I might be rambling here... I just woke up.
To be honest, I'm not sure what's up with the kernel thing or how qemu works at a low level. I have followed tutorials like these and booted raspbian off of an image file, tweaked some configuration stuff, and then written it out to a card and had it all work. If you're an irc user, I'd recommend poking around the various ARM distro channels on freenode. There are bound to be people who do this regularly.
Are you fat?
How long did this take you?
It took much longer to write it down than to figure it out.
I'm honestly surprised by how snappy the RPi3 feels for most stuff. Even building with 4 cores is not too terrible. Things only turn ugly when it runs out of RAM and starts swapping to its measly 100MB swap on an SD card. The worst thing about the Pi are the 1GB and SD storage. The Odroid C2 is a similar machine with 2GB and much faster eMMC storage, but as so often with these cheap Chinese ARM boxes the OS support seems to be lacking. I can only hope the RPi4 will make a similar move to 2GB &amp; eMMC.
Of all the lessons to learn from the Java community, "require a giant, magical IDE," might just be the worst possible one.
Ooh, I like that idea! Something like this, except the compiler would put in the bang patterns automatically on all inputs of type `WHNF something`? {-# LANGUAGE BangPatterns #-} import Debug.Trace data WHNF a = WHNF !a deriving (Show, Eq) -- | -- &gt;&gt;&gt; foo (WHNF (42 + trace "10" 10)) (WHNF (42 + trace "20" 20)) -- 10 -- 20 -- WHNF 53 foo :: WHNF Int -&gt; WHNF Int -&gt; WHNF Int foo !(WHNF x) !(WHNF y) = WHNF (x + 1) Hmm, but how about polymorphic functions, would the compiler generate versions with and without bang patterns and then choose the correct one at the call sites? Or maybe `WHNF a` would have kind `#`, thereby preventing ordinary lazy polymorphic functions from being called on them?
Wow. We received 37 applications in total. Sifting through will be a fairly big job. I'll follow up with a process for mentor review soon.
also have a look at the functions in Data.Time.Format
What are the qualities that are shared between all the `StringLike` types? I'd find a name that suits those qualities, not a name derived from the CS tradition of calling an array of bytes a 'string'. (I thought of Textual; but that's not quite accurate since that's only really Text.)
A while back I had success with using a 500m swapfile when building stack. Otherwise I ran out of memory. Haven't tried that in several months though. Also it took 10-15 hours to compile. 
Got it all working, even without swap! ;-) Next time I'll have a swap partition on an USB HDD, though. https://github.com/blitzcode/hue-dashboard#raspberry-pi
Yeah it doesn't work with --allow-newer because either needs to be updated for ghc-8.0. https://github.com/ekmett/either/issues/44 So no stack doesn't build with ghc-8.0, at least not when you don't have stack already because you're trying to bootstrap stack itself.
Anyone know if this does realtime audio? Back when I originally looked at Euterpea this was mentioned as a longterm goal, but at that time the program was limited to non-realtime generation, ie generating wavs for playback by vlc or the like. 
This is a bad solution, and will eventually lead to incorrect results, since the compiler will assume that getCurrentTime is a pure function and thus might cache the result of it ---&gt; the same time is returned twice, even if the function is called at different times! Even worse, this behaviour is highly dependent on compiler optimizations and thus a nightmare to debug. 
Could I ask where you got that cool spreadsheet widget? I need something like that. Is your front-end written in Haskell?
Yes, the ByteString.Char8 view on bytestring is compatible with Text.
I guess I should have included the joke tag...
Or rather, the side effects are consistent across all runs of the program with input given. 
Well, it doesn't say it ONLY computes a mathematical function. I'm guessing that here 'computes a' refers only to what the algorithm outputs at the end.
BTW, I just [submitted this pr to `cereal`](https://github.com/GaloisInc/cereal/pull/54), which will hopefully help the overall benchmark suite, but it doesn't fix that one module I'm afraid. That trick won't work on `aeson` anymore, since it's already using a version that has it applied, but there is hopefully more I can do to alleviate it!
Check out http://haskellbook.com/, the book is not quite done yet, but it is very nearly there, and should provide the introduction you're looking for. It is the best introduction to the language. I'm not sure about the job market though. I don't work in Haskell.
Thanks for the resource!
Good article : D.
I highly recommend [*The Haskell School of Expression*](http://www.amazon.com/Haskell-School-Expression-Functional-Programming/dp/0521644089) by the late great Paul Hudak. Also you should learn as much as you can about Lambda Calculus in general like for example [this paper](http://ftp.cs.ru.nl/CompMath.Found/lambda.pdf). After that you should learn as much as you can about types, [*Types and Programming Languages*](http://www.amazon.com/Types-Programming-Languages-MIT-Press/dp/0262162091) is really important for that. Finally don't skip the important fundamental texts, mainly [*Structure and Interpretation of Computer Programs*](http://www.amazon.com/Structure-Interpretation-Computer-Programs-Engineering/dp/0262510871) and the [original video lectures by the authors](http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-001-structure-and-interpretation-of-computer-programs-spring-2005/video-lectures/) (about the nerdiest thing you will ever watch ;)
I'm referring to the positioning. Four haskell jobs near you right out of college!
great book, good explanations/examples, awesome exercises - my experience so far
The problem is that the `Exp` can be any expression of any complexity. Once it is spliced in, it then only runs at run time. For example, in this case, if you turn the call to the regex parser into a quasiquoter in a simple-minded way, all you are doing is splicing in a call to the same regex parser which will run at run time as before. We need some way to guarantee that the resulting `Exp` that we splice in is something that would qualify as a literal. A literal is binary data that we store in the compiled executable and just load (not parse or do any other computation) at run time. So the `Exp` would need to be built entirely out of `LitE`, `ConE`, and `AppE` constructors. To accomplish that for this regex parser, for example, we would need to have some way to "run" the parser at compile time with a given input string and harvest the resulting value as a literal-format `Exp`. Keep in mind also that any TH-based solution can easily cause drastic increases in compilation time even for trivial cases without very careful design. It would be much, much easier to build in hard-coded support for `Text` and `ByteString` literals specifically, without going through all the TH complexity.
it's not haskell, but i'm a big fan of [how to design programs](http://www.htdp.org/) as a way to learn the functional way of thinking. the lessons you learn there will carry over directly to whatever functional language you learn.
`SomeString` :)
In emacs: [ctrl-x]8[enter]black heart suit[enter] But it's black, not red. There are no Unicode characters for red suit symbols. So this is worse than just using Unicode. It's cheating. Huh. I thought no one ever cheats at Texas Hold'em.
wrong imgur link 
Month in Haskell Mode April 2016 had 378 views by 320 unique visitors since it was published.
There aren't necessarily a lot of jobs requiring it (I haven't looked so I don't know), but learning Haskell will give you a different perspective on your imperative programming that will help massively with problem solving later. It will also look good on your CV, (university computer science departments like it too if you want to go down the academic study route) and give you a something to talk about at interviews. I learned Haskell in the first year at uni, and I have friends from the course who do a lot of stuff with F# and Clojure who found the Haskell grounding gave them a head start there. 
e.g. setClipboard :: String -&gt; IO () getClipboard :: IO String reverse :: String -&gt; String `setClipboard` is deterministic: it always outputs unit. But, it's impure: effecting the world, changing a shared global mutable state, which can be observed not only by different threads in your haskell program, but by different processes on the same machine running totally different programming languages, and even by the user). Conversely, `getClipboard` is non-deterministic: it takes no input but can output something different on every call. However, it's not too unsafe to randomly invoke it thousands of times per second; you'd have to try hard to observe the effect it has (maybe OS logs, or delays). That is, `unsafePerformIO (getClipboard &gt;&gt; return ()) :: ()` probably won't cause any problems (or solve any lol). `reverse`, of course, always gives the same input for the same output, *and* causes no effects i.e. it's pure.
There is a very interesting papper that defines the concept of referential transparency, definiteness and unfoldability. It is mentioned in the following summarizing email: http://www.informatik.uni-kiel.de/~curry/listarchive/0856.html Haskell and other pure functional languages apparently have all three of these properties and are deterministic. But apparently a non-deterministic language can be referentially transparent, but not have finiteness or unfoldability. The Wikipedia article on referential transparency ([1]) says "[...], a referentially transparent expression is therefore deterministic", but given that it is marked with both "cites no sources" and "needs help from an expert" maybe needs correction (anybody here an expert?). Also the Wikipedia article does not give a formal definition of referential transparency. So there are very subtle differences and one should be very careful which terminology is being used. I myself am not an expert, so please correct me if I am wrong. [1]: https://en.wikipedia.org/wiki/Referential_transparency
PS: "Only depends on its input. That is, given the same input, it will always return the same output." What does that formally mean? 
I don't use CGI, I just `putStr` data and let JavaScript working with it. How can I access the GET variables without using a big new framework like Scotty?
means you can substitute a result by its call on an argument, always.
The PHP back end is cool because, believe it or not, PHP is still the easiest thing to make just work (tm) on the web, especially with the abundance of public uni ftp sites and shared hosting.
There is no such thing as 'GET variables' in HTTP. Or CGI. Perhaps you mean accessing the query string? In that case use `getEnv "QUERY_STRING"`and parse the string with `parseQuery` from https://hackage.haskell.org/package/http-types-0.9/docs/Network-HTTP-Types-URI.html.
Additionally, there's a [WAI handler for CGI](https://www.stackage.org/haddock/lts-5.15/wai-extra-3.0.15.1/Network-Wai-Handler-CGI.html), so theoretically any WAI application (such as a Scotty app) can be run as a normal CGI application. However, if there are high startup costs, CGI is a bad choice (FastCGI or SCGI would be better).
That's an excellent observation! In Haskell we often model side effects by implementing them out of pure functions, and then whether the resulting code is considered pure or impure indeed depends on our point of view. For example, we often use State to describe imperative computations which can read and write to a single anonymous mutable variable. State code looks like this: onDownButton :: State Int () onDownButton = do s &lt;- get if s &gt; 0 then put (s - 1) else return () This is clearly an imperative algorithm: it first reads the current anonymous state and puts it in `s`, then it checks if `s` is greater than zero, and if it is, it writes `s - 1` back to the anonymous state. Reading and writing to a mutable state is considered a side-effect, so this code is impure. By contrast, the expression `s - 1` is pure, since it causes no side-effects and always produces the same result given the same value for `s`, exactly like a purely-mathematical expression. However, the above computation is only impure from one particular point of view, the one we should take while reading and writing State code in order to understand *what* it does: it reads this, then it writes that, and so on. There is another point of view, used to understand *how* it does what it does, and from that point of view State code is pure. A computation of type `State s a` uses a mutable state of type `s` in order to compute a value of type `a`, and is implemented as a function of this type: s -&gt; (a, s) `get` and `set` are both implemented as such functions, and the imperative code above desugars to one too: get :: State s s get = State (\s -&gt; (s, s)) set :: s -&gt; State s () set s = State (\_ -&gt; ((), s)) onDownButton :: State s () onDownButton = State (\s -&gt; if s &gt; 0 then ((), s-1) else ((), s)) Finally, there is a third perspective in which we ignore the what and the how and only look at the externally-visible behaviour. For example, Haskell has a version of State called ST which is implemented using real mutations, not pure functions, and yet it is considered pure from the external point of view because those mutations are only a performance optimization, they don't allow ST computations to perform anything which a pure function couldn't do. Finally, to answer your question about determinism: I think determinism and non-determinism are specializations of the concepts of purity and impurity, specifically for side-effects which involve randomness. So from the point of view of what a computation does, a computation which queries a random number generator in order to produce its result would be non-deterministic, from the point of view of how it does it, a computation which is given a seed as input and uses pseudo-randomness to compute unpredictable values from it would be considered deterministic because those values are always the same given the same seed, and from the external point of view, a [Monte-Carlo algorithm](https://en.m.wikipedia.org/wiki/Monte_Carlo_algorithm) would be non-deterministic and a Las-Vegas algorithm would be deterministic, regardless of whether the randomness comes from a pseudo-random algorithm, /dev/random, a quantum source, or whatever else.
Right. Let's call it "evaluating `getClipboard` is pure, 'executing' `getClipboard` is impure". replicateM_ 2 $ do contents &lt;- getClipboard -- ^ ^ -- | | -- | the same *action* both times -- | -- possibly a different *value* each time -- print contents This insight really blew my mind, coming from Python to Haskell. With purity (not even laziness is necessary), you can easily build up "actions" (i.e. things of type `... -&gt; IO _`), without executing them, like printer = Map.fromList [("a", putStrLn "a"),("b", putStrLn "b"),] But if you wrote printer = dict(a=(lambda: print("a")), b=(lambda: print("b"))) in Python, it will immediately print, and you can't then later trigger the printing with printer["a"] you would have to wrap them in a thunk: printer = dict(a=print("a"), b=print("b")) and explicitly execute the thunks printer["a"]() But what I meant was that you can reason about `reverse` (even though it's an un-showable function) much more easily than you can reason about `setClipboard`/`getClipboard`. Both expressions are pure as experessions, but you can't* (for example) quick-check that \x -&gt; do setClipboard x y &lt;- getClipboard return (x == y) as easily as you can quick-check that \xs -&gt; (reverse . reverse) xs == xs \* (At least, not without spamming the system clipboard; and what if `setClipboard` is non-blocking, and you have to insert a lengthy delay for its effect to register?) This is because, to work with `setClipboard`/`getClipboard`, we need to run `IO` (and use `&gt;&gt;=`), where "work with" means run pure haskell functions upon (like quick-check does). But to work with `reverse`, we just do ("using `id`").
I haven't checked, but I guess it's the `non` that makes it do this.
I though the `non` applies to the lookup into the HashMap and triggers when that returns Nothing. What about putting `HS.empty` into the HashMap causes it to delete the entire key? Why is that not a valid value to set?
&gt; 1 LoC [...] &amp; [...] %~ [...] That's a cute story. ;-)
I used &lt;&amp;&gt; for the first time today, not sure I like where I'm going with all this either ;-)
&gt; applies to the lookup into the HashMap and triggers when that returns Nothing &gt; non (HS.empty) `non` works in both directions. So, if you modify the value to be HS.empty, the entry will be removed (as if you had set it to Nothing).
Hm, I guess you're right! I just find this really odd. If I had a map of Strings, would [] remove the key? What about Ints, would the key be removed if I put a 0 in it? So, how do I write a lens that can deal with the fact that the lookup into the HashMap might fail, but also support the idea of having an empty HashSet as the value in the map?
I'd love to, which lens could I use instead?
Will memory usage also be profiled and improved?
Honestly, what I actually want is: 1. The ability to decompose type-level Symbols at compile-time, break them down into lists of Chars. 2. A typeclass that will turn "string" into fromString (Proxy :: Proxy "string"). This would enable compile-time parsing of strings into, for instance, regexes.
&gt; HM.fromList [("user1", HS.fromList [15, 10])] &amp; ix "user1" %~ HS.delete 10 Isn't `at` used for deleting? ghci&gt; HM.fromList [("user1", HS.fromList [15, 10])] &amp; ix "user1" . at 10 .~ Nothing fromList [("user1",fromList [15])] ghci&gt; HM.fromList [("user1", HS.fromList [15, 10])] &amp; ix "user1" . at 15 .~ Nothing fromList [("user1",fromList [10])] ghci&gt; HM.fromList [("user1", HS.fromList [15, 10])] &amp; ix "user1" . at 1000 .~ Nothing fromList [("user1",fromList [10,15])] 
Thanks! So, should I use `ix` when setting and `at` + `non` when getting to deal with the Maybe when indexing a container?
Creating a space leak by using a tuple as an accumulator. Creating a space leak by using the (lazy or strict) Writer monad.
Re error: I sometimes use error or undefined when some sort of notImplemented keyword would be better, where compiling with them in the code would generate a warning.
`ghc` can warn you about incomplete pattern matches, but it doesn't do that by default: you have to pass `-fwarn-incomplete-patterns` to turn it on, or `-Wall` (which I normally use).
Accidentally recursive code
`-Wall` sounds like a good idea in general, I guess.
Potential solution: something like [the `undefined` provided in ClassyPrelude](https://hackage.haskell.org/package/classy-prelude-0.12.7/docs/ClassyPrelude.html#v:undefined)
Man how has no one recommend Learn You a Haskell for Great good? Or Real World Haskell? I highly recommend both. Both are free and high quality. They're how I got started and I learned the rest by just trying to write programs and googling solutions to my issues. Lastly the Haskell IRC Chanel is a great resource
Yep, thanks for spotting the typo.
And we'll make the bugs pay for it!
&gt; Why is this happening [...]? Lens is complicated, so let's split your one liner into smaller pieces to make sure we understand how everything works. {-# LANGUAGE FlexibleContexts, RankNTypes #-} import Control.Lens import Data.Map as Map import Data.Set as Set import Test.DocTest initialValue :: Map String (Set Int) initialValue = Map.fromList [("user1", Set.fromList [15])] atUser1 :: Lens' (Map String (Set Int)) (Maybe (Set Int)) atUser1 = at "user1" nonEmpty :: Lens' (Maybe (Set Int)) (Set Int) nonEmpty = non Set.empty myLens :: Lens' (Map String (Set Int)) (Set Int) myLens = atUser1 . nonEmpty delete15 :: Lens' s (Set Int) -&gt; s -&gt; s delete15 l = l %~ Set.delete 15 -- | -- &gt;&gt;&gt; finalValue -- fromList [] finalValue :: Map String (Set Int) finalValue = delete15 myLens initialValue So we have `atUser`, a lens focusing on a particular key of the map, and the value it is focusing on has type `Maybe (Set Int)`. Why not `Set Int`? Because the key might not be present in the map, so `view`ing the value might fail. What happens if you use that lens to `set` the focused value to `Nothing`? The key is deleted. Next, we have `nonEmpty`, a lens focusing on the `Just` case of the `Maybe (Set Int)`. But wait, how is this a lens? What happens if we try to `view` a `Nothing`? It will return the `Set.empty` you gave as an argument to `non`. For consistency, if you `set` the focused value to `Set.empty`, the container will become `Nothing`. By now it should be quite clear what's happening: `Set.delete 15` sets the focused value to `Set.empty`, which causes the `Maybe (Set Int)` to become `Nothing`, which causes the key to be deleted. &gt; This still seems really counter intuitive and unproductive, what an interesting choice, I wonder why it's done this way... Sorry for being so blunt, but lens did not make an "interesting choice" here, you did. The lens library gave you a vast assortment of tools allowing you to specify exactly what you want to happen, you used those tools to request a particular behaviour, and that's the behaviour which lens gave you. The question is not *why* lens behaved the way you asked it to, but rather *how* to specify the behaviour you want. Still, why does lens provide the `non` lens? Taking a clue from its name, `non empty` allows you to specify that you only want to store non-empty values in this container. For example, if you're using `Map k (Set k)` to store edges between nodes of type `k`, and you're building this map incrementally, one obvious way to do it is to start with an empty map and then to add edges one at a time, inserting new keys as necessary. Equivalently, you could also decide to start with a map associating each node to an empty set. Inserting new keys is just an optimization in which you consider missing keys to be equivalent to keys mapping to empty sets. Since not storing the key at all is the most space-efficient of those two equivalent representations, it makes a lot of sense to use the invariant that if a key is present in the map, it is always associated with a non-empty set, and thus after removing the last edge associated with a key, the key should be removed from the map. `non` allows you to specify that this is the representation you want, and lens obliges by adding and removing keys as necessary. &gt; how do I make it stop? Well, which behaviour would you like instead? I get that you want the end result to be a map with a key pointing to an empty set, but why? If that's all you want, you don't need lenses to do it: -- | -- &gt;&gt;&gt; finalValue' -- fromList [("user1",fromList [])] finalValue' :: Map String (Set Int) finalValue' = Map.adjust (Set.delete 15) "user1" initialValue Clearly, you're trying to figure out how to do it using lens. And so you need to learn that a lens is not just a different syntax for writing code like the above, it's a way to express the relationship between a part and the whole, in a way which allows you to access and modify the whole by specifying how you want to access and modify the part and leaving the rest alone. Since the key is not guaranteed to be in the map, you can't access the `Set Int` without assuming some kind of default value, and the "relationship between a part and the whole" way of saying that there is a default value is to say that a missing key is equivalent to a key pointing to a particular value. Since you're unhappy that lens used this equivalence to reduce memory usage, you must be thinking of a different relationship. Maybe you don't want a "lens", a relationship between a part and a whole in which the part is always guaranteed to be both modifiable and viewable, but rather a "setter", a relationship between a part and a whole in which the part can be modified if present but isn't viewable because the part might be missing? -- | -- &gt;&gt;&gt; over mySetter (Set.delete 15) initialValue -- fromList [("user1",fromList [])] mySetter :: Setter' (Map String (Set Int)) (Set Int) mySetter = ix "user1" 
Is that `Data.Text.Text` or `Data.Text.Lazy.Text`? ;) I think picking the wrong version of a data structure (strict versus lazy) is too easy to do in Haskell. It's often hard to even know which one you want. 
Thanks a lot for this, very helpful! My example was simplified from what I actually tried to do. In my actual code the HashSet was nested in a record in the map. This of course made it extra confusing, keys in a map disappearing because the last item from a set was removed two layers down.
Yeah, I just forgot that `At` has an instance for sets.
Yeah, you want a strict datatype: data Pair a b = Pair !a !b Tuples are like: data (,) a b = (,) a b 
"How to prove it" by Velleman is handy to come up to speed with the skills you're expected to have for "Types and Programming Languages" - although if you're comfortable with proving things / induction you may not need it. Software Foundations is also a good entry point for TAPL. With that said, if you're just starting out and had the choice between reading TAPL or just writing a lot of Haskell for a year, I'd recommend the latter. 
Using `foldl`, at all. That should be removed from the Prelude
I would just sit down and read TAPL, its going to give you the "why" of the type system in Haskell and put it in a context with other programming languages.
&gt; error yeah! we sometimes are not sure which packages we shoud use !
"accidentally." You mean like accidentally infinitely recursive, right?
I really like that direction of investigation. I think there is a lot left to distill into actual compiler (or even hardware) technology in the research around the Geometry of Interaction, Deep Inference, Game Semantics and Rewrite Systems. Generally are operational accounts of logic fascinating (and IMHO sorely needed).
Can someone explain this in layman's terms?
Is it true that you are actually supposed to use foldr on lists constructed by recursive use of the `(:)` operator? For example: sum [1..10] == foldr (+) 0 (fix (\next a -&gt; if a&gt;=10 then [] else a : next (a+1)) 1) As I understand, this can be optimized to a loop that avoids using the stack entirely, but this is not true for `foldl` used on lists constructed by recursive use of `(++)`. For example: sum [1..10] == foldl (+) 0 (fix (\next a -&gt; if a&gt;=10 then [] else [a] ++ next (a+1)) 1) Or am have I misunderstood?
I think using `foldr`, `map`, or `concatMap` can solve that problem in nearly all situations. You probably shouldn't pattern match on a list, or use `head` or `tail` when there are so many better ways to map and fold over a list.
Note that it isn't laziness that causes this, it's just that ML languages distinguish let and letrec. 
It means it is easy to be confused by Haskell documentation because 99% of it is nothing but category theory jargon.
I'm actually surprised to see so many notable people mention this. The Haskell way of doing it seems like the much more natural way to do it in my eyes. I had no idea people expected anything different. (Besides the obvious case of a new convert who is still used to how they did it imperatively.)
Only the tuple constructor is forced, its innards are still thunks. A strict data type forces its innards as well when evaluated
When I read some ML/OCaml, I thought "let rec" was dumb. Why do I have to tell this to the compiler? Then I got bit by an accidental loop. And another. And another. I no longer think "let rec" is dumb...
Here's an example: getCurrentTime &gt;&gt;= return.(formatTime defaultTimeLocale "%D %S") &gt;&gt;= putStrLn.show
I don't know. Lets try translating the bits first: zygo = "joint" histo = "tissue" (they probably mean etym?) morph = "form" -ic = "pertaining to" pre = "before" pro = "forward" morph = "form (again)" -ism = "practice of" So I guess _zygohistomorphic prepromorphisms_ is the practice of doing some kind of joining, with history, within some _form_, before moving forward to some other _form_? Edit [way off](https://www.reddit.com/r/programming/comments/6ml1y/slug/c04ako5)
What causes these loops? Are you trying to shadow definitions?
Usually in my work with big or many data structures, the difference between those two in practice is measurable but not much. There's a much larger difference in choosing e.g. HashMap or IntMap over any Map. So that one I wouldn't necessarily call a big deal. 
Or ticks' added to variables making it hard to distinguish them.
Shadowing is usually the culprit (a slap on the wrist and enabling `-Wall` is the cure for that). But they can arise accidentally. 
You can recover this distinction in Haskell with bang patterns: Prelude&gt; let !x = x in x &lt;interactive&gt;:4:5: Recursive bang-pattern or unboxed-tuple bindings aren't allowed: !x = x In the expression: let !x = x in x In an equation for ‘it’: it = let !x = x in x 
&gt; let x = 1 + x in x I think it is logical that an expression of the form `x = 1 + x` is nonsensical. It basically says that `x` equals itself plus one, which doesn't make sense.
When your code depends on a list being non-empty, you should encode that invariant by using `NonEmpty` instead of `[]`.
I'm not sure. The early Lisps are mostly imperative languages, so it makes sense for them to "rebind" values that way. In a declarative, immutable language like Haskell, it makes less sense.
Then it sounds like it refers to unfoldability.
It's good to have a handle on both, even if you choose one primary one to use when leveraging _The BEAM_. Personally I like to write elixir, but understanding erlang lets me more easily leverage the libraries out there that are written purely in erlang, without any kind of elixir wrapper already existing. 
Looks like edwardk wrote an explanation quite a while ago: https://www.reddit.com/r/programming/comments/6ml1y/a_pretty_useful_haskell_snippet/c04ako5 “zygohistomorphic prepromorphism” is usually used as a joke, though. Keep that in mind :P As the master himself says, he doesn't see any practical use. That might have changed in the meantime, but if there are uses, there probably won't be many.
I'm not sure I understand the motivation here. &gt; one can do such things as create heterogenous dictionaries where each key is a `TypeRep` Do you mean "dictionary" here in the sense of a finite map? If so, I don't see why this can't be done in the proposal as it stands. &gt; construct exceptions more cleanly (i.e., without having to create new types) I'm not entirely sure how this would work in the current implementation of `Control.Exception`, which requires that exception types are instances of `Exception`. Namely this means that you'd need to provide some way to construct a `Show` instance for your new "type". On the whole I feel like what you proposing may be a reasonable proposal, but it is qualitatively *different* from `Typeable`. The fact that `Typeable` currently represents precisely one *type* is IMHO a strength which you'd lose under this proposal. For instance, Cloud Haskell depends critically upon this correspondence in serializing values; in expanding the domain of things represented by `TypeRep` you are adding a class of bugs that the type checker can't check. However, it seems like you could easily implement your generalization as a user library on top of the current `Typeable` proposal (the magic to ensure uniqueness might be tricky, but I believe it ought to be possible).
Indeed this falls under "compie-time performance".
Getting Haskell rejected by your boss because your boss misunderstands Haskell and its capabilities.
Good answer - but I wonder what you'd say to solve the 'actual relationship' the OP was having problem with i.e., a HashSet inside a Record
Fortunately, that isn't all that easy.
It might be useful to edit the post to include a better description of your original problem i.e. HashSet inside a Record - better for both readers and people who answer :)
I don't think it's losing power -- just explicit "rec" where you're being recursive.
`non []` and `non 0` would. But, you can have `non "x"` or `non 1` too. &gt; HM.fromList [("user1", "x")] &amp; at "user1" . non "" %~ (tail) fromList [] &gt; HM.fromList [("user1", "-x")] &amp; at "user1" . non "x" %~ (tail) fromList [] &gt; HM.fromList [("user1", "x")] &amp; at "user1" . non "x" %~ (tail) fromList [("user1","")] -- "" is not "x", so it stays 
For a few seconds I was certain it was a project to convert Haskell to SML.
Yeah, it wouldn't be worth changing because of all the breakage it would introduce, but I do wish that the `Text` type from `Data.Text.Lazy` were named `LazyText` instead. In fact, in most of my projects, I have type LText = Data.Text.Lazy.Text written somewhere.
Haven't failed me, you got my upvote.
Comparing cyclic structures (tying the knot) with `(==)` (or similar functions). This has caught me so many times than I'd like to admit, and I usually end up wasting a good amount of time scratching my head wondering why this thing just `&lt;&lt;loop&gt;&gt;`'s.
My answer wouldn't change. The reason the record was removed isn't because a field inside it happened to be a Set which happened to become empty; it's because *all* of the fields became empty, making the entire record equal to the value given to `non`. If this empty record is supposed to be the default value when the key is missing, then a missing field and a field pointing to an empty record should be equivalent, and so it should also make sense to remove the key when the record becomes empty. If the two are not equivalent, then a lens is not appropriate, but a setter might be.
I agree with your suggestions to find out about the lambda calculus and read SICP but for the majority of people writing some code should come well before reading TAPL. This person sounds like they would benefit more from a quick and practical introduction.
&gt; If you look at the type of a monad you'll find an arrow hidden in it. I agree that `main` is a function but your reason is a red herring. It's just a function with arity 0. The action/function distinction isn't a real one, just a way to indicate you're talking about IO.
Right, but I tend to "factor out" shared fields anyway. e.g. from the awesome `fsnotify` package, this type: data Event = Added FilePath UTCTime | Modified FilePath UTCTime | Removed FilePath UTCTime can have total accessors: eventPath :: Event -&gt; FilePath eventPath (Added path _) = path eventPath (Modified path _) = path eventPath (Removed path _) = path eventTime :: Event -&gt; UTCTime eventTime (Added _ timestamp) = timestamp eventTime (Modified _ timestamp) = timestamp eventTime (Removed _ timestamp) = timestamp but the "factored out" type: data Event = Event { eventChange :: Change , eventPath :: FilePath , eventTime :: UTCTime } data Change = Added | Modified | Removed deriving (Enum) distinguishes a sum type from a product type. Some benefits: * (1) clarity * (2) easier to "handle" the sum type, i.e. handle :: Event -&gt; ... handle = \case Event Added _ _ -&gt; ... Event Modified _ _ -&gt; ... Event Removed _ _ -&gt; ... versus: handle :: Event -&gt; ... handle = eventChange &gt;&gt;&gt; \case Added -&gt; ... Modified -&gt; ... Removed -&gt; ... * (3) easier to use `lens`: makeLenses ''Event makePrisms ''Change https://hackage.haskell.org/package/fsnotify-0.2.1/docs/System-FSNotify.html 
Why not `import qualified Data.Text.Lazy as L`? Then you have `L.Text` which seems close enough to `LText`.
Yep, exactly.
[SubHask](https://www.stackage.org/package/subhask) has a great solution to this: &gt; In SubHask, you can access the containers package by importing `SubHask.Compatibilty.Containers`. This module exports `Map` as a lazy map and `Map'` as a strict map. In general, the prime symbol on a type signifies that it is a strict variant of the unprimed type. 
It is too easy to include a bunch of random libraries and use them in the manner seen on a blog post or stackoverflow, instead of the way the author intended. Maybe standardization is the wrong term, but a Matsumoto level of clean library design and documentation would be nice. Especially unit tests. Not for correctness, but to see the author's intent on how a library should be used. There is no better documentation than software driving a library to put it through it's paces. 
True but it's still easier than actually defining the instance.
Why? 
s/a bit/completely/
I actually wrote it as a testament to how hard these things were to use in practice. =)
Good to know. I've needed `apo` for short-circuiting (if the key exists in the map, return the value you'd put there), but I wanted a `State` effect, and didn't know how. Maybe I want an `apoM` with `(a -&gt; m (Base t (Either t a)))`, using `(&gt;=&gt;)` over `(.)`? I gave up a few months ago. 
One of the things I love about PureScript is that the Prelude has to be explicitly imported. So using an alternative prelude (like mine, [Neon](https://github.com/tfausak/purescript-neon)) is just as easy as using the default. This is helped by the fact that the prelude isn't included with the compiler (it's just another package) and the syntax is (mostly) rebindable without any extensions. 
Well, we have `Functor`, `Foldable`, `Traversable` for a common recursive pattern. It could be nice to get these for all type-parameters and not just the last one. Automatic folds (unrelated to specific type parameters) could be nice but mutually recursive types would make that very hairy.
[](/rdwut) I see what you did there.
It sets a bad precedent to create a strict function with a "lazy-looking" name.
[removed]
One of the great things about Haskell is that "the way the author intended" isn't as important as it is in many other languages. Frequently I write code and then later on realise I could use it in so many more good ways than I at first intended.\* This is a power of Haskell making it easy to write generalised code, and not something to be stifled. ---- \* Example: An AI decision making algorithm. By swapping out `Maybe` for `Alternative` and trying the list instance, I could get a full enumeration of possible choices, rather than just the single one. Exact same algorithm. How cool isn't that?
IMO build plan files shouldn't be considered human readable. These should be generated by your "IDE", not created by fiddling around with a text file.
Yes, that's a perfectly logical way to think about it. However, practically all new or potential Haskellers are converts from imperative languages, and they are not going to think about it this way at first because they have been primed to think imperatively.
When ever you see a type you can think of the pieces as layers. `UTCTime` is a type, `IO UTCTime` is a `UTCTime` "wrapped" inside an `IO`. `IO` is a type that only "wraps" other types. Another such "wrapping" type is `Maybe`. `Maybe String` is a type that can be `Just "..."` or `Nothing`. But we can get access to both constructors of the `Maybe` type so we're able to write a function like this: fromMaybe x Nothing = x fromMaybe _ (Just y) = y And then we can get the value "out of the box" if it exists (and a default value if it doesn't). `IO`, however, does not give us access to its constuctors in this way (for some pretty practical reasons) so we can never get anything out of it the way we can with `Maybe` and others. But haskell gives us a way to operate on things inside their wrapping without taking them out. To take functions like `SomeType-&gt; SomePossiblyOtherType` and turn them into functions like `WrapType SomeType -&gt; WrapType SomePossiblyOtherType`. One such function is `fmap`. So if you had a function like: f :: UTCTime -&gt; String f t = "The time is: " ++ show t then you can apply `fmap` to it to turn it into a function with this type: `IO UTCTime -&gt; IO String`. The interesting thing here is fmap is general enough that it works this way on any "wrapped" type, whether they have ways to get values "out of the box" or not. For example, `fmap` will also work on `Maybe` types the same way. I would recommend reading [this](http://learnyouahaskell.com/functors-applicative-functors-and-monoids) to learn more about how all this works and get some more precise definitions once you start grasping the concepts.
I think [holes](https://wiki.haskell.org/GHC/Typed_holes) are probably what you want here.
Yeah, bad should be hard and good easy, that's what I mean too :) &gt; As simple as possible, but not simpler
OK, I'm going to play around a bit with `servant-mock`.
Guess it wasn't as easy as he thought =P
I don't think that was way off.
Sadly not, the renderer was mainly written in C++ and Python.
Well, at the end of the day the information has to come from somewhere. There are good reasons for it to be in a human-readable, diffable plaintext file.
I tried something like that but I think it was rejected because of the template haskell stage restriction, sadly.
Are there any remote positions?
I'm afraid we're not hiring for remote work at the moment.
As far as jobs go, Haskell is still a niche but there *are* options. The practical upshot of this is that if you want a Haskell job you can get one, but (with the exception of a handful of remote jobs) you will probably need to move to a "tech hub" like the Bay Area or NYC. Also, if you end up enjoying CS in college and want to continue on doing CS research and get a PhD, Haskell is almost definitely going to be an option for your research projects.
Thanks for the info. I already live a few miles away from a "tech hub" so hopefully in the future there will still be openings for functional programming.
Are you working with clash? Or what are you programming FPGA in?
Most of the FPGA development is done via CLaSH. We do have a handful of low level modules written in VHDL which aren't expressible in CLaSH.
Yes, that was a typo, thank you. :) Fixed modulo-isomorphism.
Thanks for this, is there a video of the actual talk?
&gt; **Example of stack integration** &gt; &gt; - Add liquidhaskell and dependencies to your stack.yaml file &gt; - Run stack exec liquid on each source file you want to test [liquidhaskell-cabal](https://github.com/spinda/liquidhaskell-cabal) is now available to help with this. If you give it a try, please let me know how it works out!
I've been waiting for this talk to be posted! Awesome job /u/deech :) I think FLTKHS is fertile ground for experimenting with GUI idioms in Haskell. The API is fairly small, and thanks to deech it is super easy to install. Basically, you can focus on GUI paradigms and how to structure GUIs in Haskell without worrying about if the damn thing will build tomorrow.
`-fdefer-typed-holes`
To start, `take 1` has a different type than `head`; also, an error is better than silently bad data due to invariant failure.
The Generalized Continuum Hypothesis. Haskell is independent of standard axioms.
Thanks for posting this, and bravo /u/deech ! Your efforts are appreciated. I hope help arrives to make this even better. And, perhaps some of the experience and techniques will transfer to wrapping other popular GUI toolkits.
People trained as programmers think of shadowing first. People trained as mathematicians think of recursive definitions first.
Actually, from what I can understand about ZHPs (not much), it's almost pretty good.
Even with multiple modules? i.e. `qqLiteral ` is in `A.hs`, `mkQQLiteral` is in `B.hs`, `uri = ...` is in `C.hs`, and `[uri|...|]` is in `D.hs`. Also, does it work if you inline `let function = varE name` into a distinct `mkQQLiteral` macro, rather than wrapping `qqLiteral`?
Here are the [slides](https://github.com/deech/fltkhs-compose-conference-2016-talk/blob/master/Talk.org) and the [library](http://hackage.haskell.org/package/fltkhs). The Mac issue mentioned in the talk has since been fixed. The compile time issues still exist. Thanks for watching!
OK, I've played around with `servant-mock` and figured out how to use it. Post 2 in the series will contain a somewhat lengthy blurb about it. Thanks for pointing it out!
I learned Haskell this year (I'm in High School as well). Most helpful for me was [Learn You a Haskell for Great Good](http://learnyouahaskell.com/chapters), read over the course of like 5 months
So I was looking through a preview before I bought it and it said mathematics could be an issue for High School. What level does this go into? Edit: Also, a lot of these are quite frankly, out of my cost range. 
What are some reasons one might choose one over the other?
Any chance we could get their code? (I always like more diagrams examples)
Are you able to sponsor visas for Americans? I have no idea how UK immigration works. 
No. This was an informal talk I gave for LeapYear over some pizza and beer
http://blog.sigfpe.com/2011/08/computing-errors-with-square-roots-of.html is also relevant.
I believe I have the money for SICP and The Haskell School of Expression. I've heard SICP is pretty controversial among programmers; why is that? Edit: I found them even cheaper used on "valorebooks" than used on amazon. Although "Types and Programming Languages" is still *really* expensive either way. 
Love how you described the process.
Shit no. It's way too ugly, and I'm too much of a Diagrams newb. :p
Ah yes, thank you for reminding me to put this one up there. Such is the trouble with manually managed websites. :)
It does have to come somewhere, but human readable is often an inconvenient constraint. If you can assume a tool will always modify the build configuration, a lot of restraints (e.g. "don't have all the configuration in one file") just fall away.
It doesn't store the length and iirc it uses a form of tree as Data Structure. So the only way to get the length is counting all nodes. (Since the tree doesn't have to be symetric) Edit:I went and checked and size is O(1) so it stores element count. So what do you mean by length run then?
Either way, I think it's `function` that causes the issue, because it's defined in a let binding in the same declaration. I was thinking I might be able to write a function that generates the whole quasiquoter declaration like this: mkQQLiteral :: String -&gt; Name -&gt; DecQ mkQQLiteral name fn = [d| $(varP (mkName name)) = QuasiQuoter {..} where [...] |] But I didn't quite manage it.
To simplify just Plus expression, you could do this: simplify :: Expr a -&gt; Expr a simplify (Plus xs) = Plus . concat $ map plusStep xs simplify exp = exp where plusStep (Plus xxs) = xxs plusStep exp = [exp]
I just read through the slides. All the work to get `setValue` to do the right thing really emphasizes to me that we need a better story for static type-based dispatch. (Simon PJ calls this the "power of the dot") All the OO languages have object namespaces that are instantly inferrable from their context; if you have `a :: A` then `a.field` is looked up on `A`, and can be an entirely different function from `b.field` (with `b::B`). This does complicate the type inference story slightly; we can't rely on the field name to disambiguate the function being called, which means we can't "reverse-engineer" the type of `a` from the rest of the arguments and the return type. I am curious how often this would come up in real code, though. Very often we know `a`'s type from some other use site, or the top level type signature. But honestly the alternative we have now leads to much worse problems. I don't think not having a known principal type for an un-annotated `f x = x.someField` is that big of a price to pay (I am not suggesting `f` dynamically dispatch based on its argument type; instead I would say that`f` is ambiguous and shouldn't compile without annotating a type for `x` somewhere)
We're part of the [UK Government's Autonomous Vehicles Program](https://www.gov.uk/government/news/driverless-cars-technology-receives-20-million-boost). Their primary focus is driving development of driverless cars. We're working with camera and other sensor data from driverless cars at the moment.
Very interesting post ! I find it especially refreshing that you focus on statistical metrics instead of hard error bounds. Another domain where statistical moments are used instead of intervals (or another abstract domain) in that of hardware design, where one tries to minimize hardware cost under some application-specific accuracy constraint. This constraint is usually given as a bound on noise (=error) *power*, which is simply defined as: Variance(error) + Mean(error)^2 Usually, noise power is estimated through simulations. A few analytical approaches exist, based more or less on the ideas described in your post, but they can be very imprecise for non-linear computations, leading to largely underestimated errors. The question I would like to ask you is the following: can you think of a way to compute bounds on *variance*, for example by bounding the error made when truncating the Taylor expansion ? 
I didn't realize that Lisp and ML were imperative languages!
This approach is a bit convoluted for your trees but the KURE library allows to simply write rewriting rules and applies them recursively so all redundant Plus-constructors get squashed: https://hackage.haskell.org/package/kure {-# LANGUAGE FlexibleInstances #-} {-# LANGUAGE MultiParamTypeClasses #-} {-# LANGUAGE LambdaCase #-} module ExpressionFolder where --I use the KURE library import Language.KURE --The given datatype data Expr a = Oper a | Mult [ Expr a ] | Plus [ Expr a ] deriving (Show, Eq) --This instance allows to apply rewriting rules recursively instance Walker () (Expr a) where allR r = rewrite $ \_ -&gt; \case (Oper o) -&gt; return $ Oper o (Mult xs) -&gt; Mult &lt;$&gt; mapM (applyR r ()) xs (Plus xs) -&gt; Plus &lt;$&gt; mapM (applyR r ()) xs --The type of a rewriting rule type RewriteR a = Rewrite () IO (Expr a) --This is the simple non-recursive rewriting rule combinePlus :: RewriteR a combinePlus = do Plus xs &lt;- idR let trms = concatMap (\x -&gt; case x of Plus ys -&gt; ys ; _ -&gt; [x]) xs return (Plus trms) --anybuR is a rewriting rule combinator which allows to apply the rewriting rule at every level of the tree combineAllPlus :: RewriteR a combineAllPlus = anybuR combinePlus --This function represents the actual rewriting function doCombineAllPlus :: Expr a -&gt; IO (Expr a) doCombineAllPlus = applyR combineAllPlus () --this is a test expression testExpr :: Expr Int testExpr = Plus [Mult [Oper 1,Oper 2], Plus [Oper 3, Plus [Oper 4, Mult [Oper 1]]]] --doCombineAllPlus testExpr = Plus [Mult [Oper 1,Oper 2],Oper 3,Oper 4,Mult [Oper 1]] 
Alright thank you for the help. I already checked my local library for all of the books, and unfortunately they don't carry any of them. 
I see. Do you have access to a State University?
It depends on who you ask, but Common Lisp (which has inspired many subsequent Lisp-likes) is normally written in an imperative style, with reassignments, mutations and other side effects abound.
Impure functional is still functional.
I love exploring the developments coming out of Evan and Elm.
The majority of breaking changes that people have considered, as far as I know, have been library-level changes, not language-level ones. For example, we recently adopted the Applicative Monad Proposal, which stated that all Monads need to also be Applicatives. This broke old code because all the existing Monads had to implement Applicative now. But this wasn't a language level change. Haskell the language remained exactly the same. It was the standard library (`base`) that changed. We can't change whether this proposal is implemented on a per-file basis, because that would mean different files depend on a different version of `base`, which isn't possible. I can't actually think of any proposals people have asked for that would break backwards compatibility that aren't library-level. And for those things that *are* language level, this is exactly what we have the `LANGUAGE` pragma for. A file can declare language extensions that it makes use of by writing `{-# LANGUAGE ExtensionName #-}` at the top of the file. And even still, most language extensions don't actually break backwards compatibility; they just aren't something the community has deemed correct to put in the standard language yet.
I think you misunderstood OP's point. He wants a pragma to have the *new* compiler correctly compile *old* code.
My only grief is (as always) that running the code through `ad` is going to hurt performance. So in (arguably not) general purpose library I would push for the instances to be hardwired in the implementation. 
Ok so this works, it removes the nested plus. I haven't tried all situations yet.
This seems like a regression to me. I'm no FRP expert, but this "new" stuff seems very not new to me.
Can someone explain this from a language-designer's, rather than a language-user's perspective...? I bet there are a bunch of scary words which Evan avoided mentioning to avoid frightening the latter, but which could help the former make some connections.
You could always use Taylor models. (Truncated Taylor series with an interval covering the truncated portion.) Picking how far off you truncate the series lets you trade off accuracy with performance, while netting a conservative answer (assuming you have support over rounding modes, which needs something like MPFR or crlibm).
As a distant onlooker with no experience in Elm or FRP, I agree. From this far off, it just looks worse
That seems like a good approach. In which way does it "not work" on `Mult`? Are you getting the error "`Non-exhaustive patterns in function snp`" and you're trying to understand what it means, or did you already figure out that you're supposed to implement the `Mult` case as well but you're not sure what to put on the right-hand side?
Unfortunately Elm is making less and less sense to me. I approached it after learning a bit of Haskell and I really like the applicative `Signal`. I can still look at the [old clock example](http://share-elm.com/sprout/5732140de4b070fd20dabd63) and think right away "aha, main is mapping the `clock` function to every second": `Signal.map clock (Time.every second)`. I look at this new example and am not sure what the main function does exactly. Furthermore where the hell is `Cmd` declared, and why is a `Msg` declared as a `Tick Time` and what the hell does that mean. The other thing that attracted to Elm was the declarative graphics API which you can easily sketch out shapes with. For example [drawing a red square is super easy](http://share-elm.com/sprout/57321769e4b070fd20dabd6d). This is really neat but hasn't developed much over the years I have been following the language. There is still no efficient way to make a rounded rectangle for instance. Instead the focus is now on writing HTML/CSS but I feel like if I am going to do that, I am better off with more mainstream front-end web tools. I am likely very biased since I learnt the old Elm and people are resistant to change. I also know the Elm devs are very hard at work trying to make things easier for beginners and all around making web-dev more fun. It just feels like the focus has shifted away from the things I liked about the language and I am no longer the target audience. That's not necessarily a bad thing though (well, it is for me but maybe not for Elm). 
I've been watching elm for awhile, and I keep expecting it to cruft or die, yet he keeps making strides forward with it. I think im out of excuses, time to pick it up.
My understanding is that the runtime system in the new Elm is broken into two pieces: the input part (Sub) and the output part (Cmd). Sub and Cmd together are the equivalent of Haskell's IO. Now, the runtime system also manages an implicit loop through a mandatory user-defined "update" function that must handle internal messages as well as inputs from Sub, and it must emit both internal state changes and outputs to Cmd. This is just how I understood it, but I may be wrong.
I think the problem was that Signal worked great for clocks, but scaled poorly to real Web apps. 
Part of the Elm philosophy is that input is most valued from those who are actively using the language. Don't knock it till you've tried it!
So the reason you're getting a "Non-exhaustive patterns" error is indeed that there are not enough patterns: you're not covering the `Mult` case. You already have code to match `Oper` and to leave it unchanged, and the code to ignore `Mult` expressions is exactly the same: snp (Mult xs) = [Mult xs] I suspect you know this already though, but that this code doesn't yield the behaviour you want. In order to proceed forward, do you have a concrete example input and expected output which this code doesn't handle the way you want?
Is that documented anywhere?
That repo looks awesome. Thanks!
...meh. If "functionalness" is a scale from 0 to 100, then side effects are further to the 0 end (although still far from it) than purity.
I agree. Web UIs have a better model and are more cross-platform than native ones, but we still want the desktop's privileges/efficiency/privacy. I hope `threepenny-gui` gets "Electron integration" (once we find out what we can and can't conveniently "integrate" into a cabal package). It seems easier to do this for Electron than for massive C++ libraries (see `gtk`) but /u/apfelmus and /u/jeff_weinstein can offer less vague advice. https://ocharles.org.uk/blog/posts/2013-12-07-24-days-of-hackage-threepenny-gui.html https://hackage.haskell.org/package/shake https://github.com/HeinrichApfelmus/threepenny-gui/issues/52 https://github.com/HeinrichApfelmus/threepenny-gui/issues/111 https://github.com/ndmitchell/shake/issues/304 http://hackage.haskell.org/package/gtk-0.14.2/src/
Agreed! Elm is a really important point in the design space. As much as I love Haskell, it's (in many but not all cases) the land of the proof of concept. Elm is optimizing for fast on boarding and productivity, the same stuff that Rails used to bring Ruby to the mainstream. I'm excited to see how it progresses.
Yeah, I'm not sure happy about how the Num instance worked out either, but I guess it's sort of a neat-story-better-than-nothing approach for now :| The Data.Correlated model I think works well to alleviate that, and is the only way to really purely trace the "context" of samples and how they are intercorrelated, I think. Thanks for the link with the references, I'll definitely be looking into those :)
My CDN is having a lot of trouble getting it together now, so if the link is down, there's a non-CSS'd version up at http://mstksg.github.io/inCode/entry/automatic-propagation-of-uncertainty-with-ad.html :)
Drawing a red square is still super easy in Elm 0.17: import Color exposing (..) import Collage exposing (..) import Element main = collage 400 400 [filled red &lt;| square 50] |&gt; Element.toHtml That's it. Unfortunately, Evan removed support for Collage in the Try Elm server, so you need to install Elm locally to compile the code above, but once you do, Collage is still there. 
This strikes me as a great step forward. A lot of what's happening in the 'reactive' space just seems to be a way of tying yourself knots by pushing too much logic into a wiring graph. Elm has never been about FRP for its own sake, and its strengths have come more from functional programming insights. This release pushes further in that direction, and achieves more simplicity by it. Bravo. (Also nice to see elm-html and start-app going into core. Previously you could be forgiven for thinking that Elm was "just for games." Now it's taking its rightful place as one of the best things in commercial frontend programming.) Confetti-balls all round. 
&gt; A lot of what's happening in the 'reactive' space just seems to be a way of tying yourself knots by pushing too much logic into a wiring graph. I believe part of the value that FRP brings is better composability than what you can get with The Elm Architecture. With Elm you have to separate things into three parts: Model, Update, and View. With this structure, if you want to add a widget to your application you have to add the widget's model to your application's model, the widget's view to your application's view, and the widget's update to your application's update. For stateless widgets this ends up being pretty easy, since they won't have a model or a view. But for stateful widgets, it becomes kind cumbersome. You essentially have to duplicate your app's structure in probably 3 different places. But FRP (at least with Reflex) allows widgets to maintain internal state that doesn't have to be added to a top-level application model. So you don't have to worry about the structure of your model, update, and view getting out of sync. This seems to me to be inherently more composable. But it's difficult to implement efficiently. Also, in my experience, you end up leaning on higher order FRP a fair amount. So it makes sense to me why Elm didn't converge on it. Thankfully though, we now have [Reflex](http://hackage.haskell.org/package/reflex), which I believe is the first implementation of FRP that solves these problems in a robust way and is usable for production apps.
Hey, Wagon CTO here. We do this thing! [You mentioned](https://www.reddit.com/r/haskell/comments/4ipah2/resources_for_electron_haskell/d30656r) you've seen the basics in [our talks](https://www.wagonhq.com/blog/engineering), so I'll stick to your questions: - How do you run the Haskell server inside the Electron app? - The compiled Haskell executable is included in our Electron app's resources directory. We copy it in before running [`electron-packager`](https://www.npmjs.com/package/electron-packager) / etc. - From Electron we call [`child_process.spawn`](https://nodejs.org/api/child_process.html#child_process_child_process_spawn_command_args_options) with the path to the Haskell executable. [`process.resourcePath`](https://github.com/electron/electron/blob/f58b3f853ea4761f828ea567e603cc280a64cf78/docs/api/process.md#process) helps construct that path. - The Haskell server's main method calls `Warp.run …` to bind `localhost` and a known port. The WAI application includes a status endpoint that returns `200` when it's running and healthy. - Electron polls `localhost:PORT/status` until it gets `200` or reaches a timeout. In case of timeout we show a sad message and exit. That doesn't happen much—thanks Warp/GHC/et al :) - How does cross-deployment work? - I take it you mean cross-platform deployment, e.g. Windows and OSX builds. Let me know if I'm wrong! - Packaging is automated on CI (CircleCI for OSX, AppVeyor for Windows). The packaged apps are pushed to S3 and accessed through CloudFront. - Our site redirects users through CloudFront to download the right version of the app. - We've implemented [auto-updates](https://github.com/electron/electron/blob/master/docs/api/auto-updater.md) on OSX and Windows to keep our users running fresh versions. These parts of Wagon are important—when they break the whole app breaks—so we're always looking for ways to make them simpler and more robust. We'd love to hear feedback / ideas / experiences from anyone that's tried this type of setup! Wagon is hiring—if this sounds like fun I bet you'd like it here :) [wagonhq.com/jobs](https://www.wagonhq.com/jobs) or just shoot me an email mike@wagonhq.com
Thank you! This is definitely a big help on the way - I'm currently in the midst of toying around with Electron and Yesod, but it means I needed to brush a bit up on both since I haven't used either for a while. Have you considered doing a more technical tutorial-esque writeup of your specific approach? It's things like this that I think would be a nice thing to have a quick skeleton or template for, since I think it would help drive interest to Haskell for things that perhaps weren't on peoples radars before. Hopefully I'll be able to provide that when I'm done toying around :) &gt;I take it you mean cross-platform deployment, e.g. Windows and OSX builds. Let me know if I'm wrong! Yeah, exactly what I meant! Think I accidentally smashed the words deployment and cross-platform together in my hurry :)
For some reason I feel like you quickly get stuck when looking for resources in Hakyll on more advanced things (not that this is super advanced), so I thought I'd try to at least share some of the little things that help along the way (and hopefully more advanced things later on).
... I couldn't resist: upvoted for the title.
Are you sure? &gt; cat Main.hs main = _ &gt; stack exec -- ghc --version The Glorious Glasgow Haskell Compilation System, version 7.10.3 &gt; stack exec ghc Main.hs [1 of 1] Compiling Main ( Main.hs, Main.o ) Main.hs:1:8: Found hole `_' with type: t Where: `t' is a rigid type variable bound by the inferred type of main :: t at Main.hs:1:1 Relevant bindings include main :: t (bound at Main.hs:1:1) In the expression: _ In an equation for `main': main = _
The funny thing is, I made that post for the sole purpose of making a joke: https://www.reddit.com/r/ProgrammerHumor/comments/4igxd3/difference_between_python_and_haskell/
Thank you.
[Complaining that comments are placed weirdly](https://wiki.haskell.org/Wadler's_Law)
Holes are also errors in GHCi. The wiki is, unsurprisingly, wrong. 
It's too easy to write partial functions accidentally (due to `head` et al., and because of record field accessors)
Just release them under [CRAPL](http://matt.might.net/articles/crapl/). :)
Oh. OutsideIn is actually the name of ghc's type inference framework (see [this](http://research.microsoft.com/apps/pubs/default.aspx?id=162516)), so the title is a bit misleading..
Thanks! seems like the perfect example to take a look.
What's the problem with Haskell numeric tower? (I use Haskell exclusively for compilers stuff..)
This is not really surprising. Elm never had monadic streams and it turned out not to be expressive enough. The Elm Architecture is quite nice but I still think there is a space to explore by higher order FRP like Reflex.
I think thatsthejoke.jpg
Yes, and until this happens, I don't see work on the tower being all that useful.
A big problem is that real IEEE floating point numbers do not have nice algebraic structure. However, they are the result of many decades of best practice, almost every feature is there for a reason. Furthermore, they are the only performant way to do large amounts of computation. No language has yet managed to formalise real floating point arithmetic ergonomically. I'd love to have one, but this is a much harder topic than it looks. Really need some solid research on the intersection of algebra and numerical analysis, two fields which are pretty much as different as can be.
Have you seen ["Encoding statistical independence, statically"](https://medium.com/@jaredtobin/encoding-statistical-independence-statically-ec6a714cf24a#.gv0hveagg) ? 
Also, the `numeric-prelude` could be made simpler, or at least enhanced with (many) examples. Or was it `SubHask`? 
Oh gosh, I remember that story. I can't tell if it's gone anywhere; the Phab changeset seems to only show a diff which adds the ability to use `hiding` with the new syntax.
This is a useful post, if only to document what turns me off and probably many others. It was almost entirely like reading my own experience, except that I was unable to get GHC to even build (with help from GHC devs).
I currently have a simple app looking "desktop-like" by spawning a web server and using chrome --app flag. It works quite OK and in fact my biggest concern is that I communicate through HTTP, which is insecure -- I don't validate that the party connecting to the server is in fact my client. So I'm interested in the RPC mechanism which I understand is provided by electron. Noone tried to integrate with that? Too much effort, or is there a bigger hurdle that I can't see?
I agree, same for me.
Has anything been written about how/why abs and signum are broken, and how they might be fixed? I'd be very interested to read that.
That was more or less what I had in mind, without knowing about Taylor models. Thanks for the pointer !
I think Evan is more preoccupied with having good UX and teaching materials than with originality for originality's sake. People tend to think of those as something you can bolt on - and sometimes you can (stack vs cabal), but Elm is exploring an alternative approach here.
Why do you think shorter import syntax is at the very deep end of the difficulty pool? I imagine you could implement it fully in the parser, without splashing over even into the renamer, if you wanted a first working version. 
Technically executing `reverse` is also not "pure" since the execution environment is usually not pure. Doing it will change the state of the computer, allocate memory, etc. A pure function can technically eat up all memory, not terminal, and/or throw exceptions like division by zero.
Do you remember what problem you had? After a git clone `./boot &amp;&amp; ./configure &amp;&amp; make` works fine here.
Yeah, if only mathematicians had developed something like [rings](https://en.wikipedia.org/wiki/Ring_%28mathematics%29) with simple laws and a whole host of theorems that can be used for algebraic transformations at compile time... Always bugged me that Haskell borrows so heavily from category theory but not from the simpler and more developed [algebra structures](https://en.wikipedia.org/wiki/Algebraic_structure). Example, in [a few compiler versions](https://prime.haskell.org/wiki/Libraries/Proposals/SemigroupMonoid), Semigroup will finally be a superclass on Monoid...
How deep does it barrow?
I've been contributing since early 2015, and my thoughts are almost exactly the opposite: - I don't mind the unusual style. Reading is no problem at all. When it comes to working on that code, at least in my experience, hacking on GHC has always been thinking 90% of the time and coding only 10% (ignoring the debugging part :-) ), so even if you have to modify code with that unusual style it's in practice not a big problem. When you add new code you don't have to follow that style. (that has another problem though, inconsistent style) - The arc tool is easy to use. Except for a few cases where it refused to update my patch and instead created a new patch, I have not had any problems at all. Once you get used to it, Phabricator interface is much better than Github interface IMO. - I really like NOTEs. Writing down a detailed comment and then referring to it in different places is very useful for e.g. searching for all the code that relates to a comment. I don't understand "requires tooling == bad" approach. Especially in this case, all you need is grep/ack/ag. - Build system is IMO awful. It works fine, after a fresh clone `./boot &amp;&amp; ./configure &amp;&amp; ./make` just works. But you can't: - Clean only stage2. - Clean only libraries. - Build only stage1/2 from the top level. - Build only the libraries. - Uninstall GHC. Hopefully with new Shake-based build system I can at least implement these myself. 
Syntax changes are difficult because a) Seemingly simple changes can have far reaching consequences that are difficult to fully understand until you have implemented it and played with it. b) Syntax is relatively easy to understand so there will be a lot of people with opinions and counter proposals In other words, [Wadler's Law](https://wiki.haskell.org/Wadler's_Law). The [trac ticket](https://ghc.haskell.org/trac/ghc/ticket/10478) has 23 comments by 7 commenters. YOu can see from that ticket that this is far from a trivial issue.
Let's start with distinguishing pure versus impure via referential transparency (which Haskell does), then we can model totality and linearity (like say Idris does) and more. `reverse` has never eaten all my memory, always terminates (on non-bottom input), and never throws exceptions (on non-bottom input). Is any function in any programming language included in your definition of purity? (Most pure functions I use are total anyway e.g. `listToMaybe` over `head`).
And now in `base`!
Maybe I'm dense, but I'm having a hard time figuring out what you mean here. &gt; I wrote an off-the-cuff response to some of the points, but it didn't really seem appropriate. Why wasn't it appropriate? &gt; I do respect Anthony a bit A bit? Just a bit? &gt; I thought it added some useful context. Okay, I read the thread. I see some bikeshedding, but also some whining on the part of Anthony, and I don't know if it's totally justified, or perhaps a slight overreaction. Please, Reddit, tell me what to think.
Haskell _barrows_ very deep in catagory theory!
what do you mean it leaves a bitter taste ?
Are the wxHaskell bindings actively maintained?
I can rrrroll with how deeply Haskell borrrows.
Wuuuaaaw, considering how much I've toyed with Yesod, I have no idea how I could forgot lucius -.-... Think I'll look into using that instead of relying on an external tool :)
Very nice set of articles! :D Currently reading through your [hakyll's functionField](http://beerendlauwers.be/posts/2015-09-21-hakylls-functionfield.html) which I wasn't actually aware of - I feel like I should be able to do some cool stuff with it.
I definitely think it would be possible, although I haven't actually played with GHCJS before. I'd imagine you'd compile it to your JS, and then you would just include that generated file in Electron. Does GHCJS also handle generating HTML for you (like Elm), or is it purely JS?
As someone relatively new to Haskell, using at it as the core of a major new project.... GHC is really the only game in town. The LLVM-based LHC compiler appears relatively dead, with little activity in recent months. UHC is, from what I can tell, an academic project not aimed at pro development. Leaving GHC. Which works pretty darned well. Has there ever been an attempt at a Haskell frontend for GCC? I've worked on GCC a bit. Which leads me to this: Much of what the OP is commenting about is simply the nature of larger free software projects. If anything, GHC sounds as if it's in a fairly good state. It does it's job well, and has an active community. That's a good thing. 
But the solution appears to be embracing callbacks. Are callbacks really the best we can do here?
Haskell source must be parsed not only by GHC but also by many of other tools out there, for example source code editors.
Windows is a terrible arch for software mainly targetted at the Linux/OSX world (Let's face it, Windows is not a first-class citizen when it comes to Haskell, as soon as a package depends on `unix` you can forget about it ;-( ). Maybe this will hopefully become all easier with [Win10's Linux subsys](https://blogs.msdn.microsoft.com/wsl/2016/04/22/windows-subsystem-for-linux-overview/) which provides a more natural environment to develop non-Java OSS projects.
&gt; In part this is due to a tendency to avoid changes that aren't strictly necessary so as to ease long-standing branches representing work of people who are not outsiders. Such a wise thing to do in any project. The way you align your do-blocks or if you use braces or not just does not matter, and it's up to a person to stop concerning themselves with it and start thinking about semantics instead of syntax, [stop counting balls and focus on gorilla](https://www.youtube.com/watch?v=vJG698U2Mvo).
I think Evan's great insight is that the Gloss programming model is useful not only for rendering graphics but actually for any kind of IO-bound application. Most Web apps are just glorified (and obfuscated) state machines. A Web app in Elm is just a simulationOf that renders HTML. Subscriptions and Commands provide the infrastructure for a general IO state machine, and ports are just a simulationOf that handles JavaScript data inputs and renders JavaScript code.
I don't believe so.
If only some of the most common numeric types were actually rings...
Exactly this! It's about making the barrier to entry as low as possible. I'm sure there is a ton of low-hanging fruits (documentation or what have you), that would be an easy starting point for new contributors, but the current process absolutely turns people away, myself included. I don't want to spend hours or days learning how to submit a small "insignificant" patch, when my alternative workflow takes seconds to minutes.
I think I was whiny. The process had dragged on for so long at that point that I didn't want the effort to just politely fizzle.
That is how I did implement it, and submitted the patch.
I made it clear from the first public utterance that the change would not affect any existing program. Through all the ordeal, that fact was never challenged.
I've actually used Elm a fair bit and I don't think it's a regression. The way you use Elm is almost unchanged. This just makes it easier to understand. The reality is that, when it had FRP and Signals, everyone still tended towards an application design that basically pushed those two things so far behind the scenes that it started becoming obvious they weren't that important. This change just represents official recognition of that fact and as a result it makes the whole ecosystem cleaner and easier to understand. It embraces what Elm actually became and needed to be when used for practical development vs. what it was imagined to be when theorizing about it.
I think some people are reading this as "Reasons GHC is Bad" which it was not intended as. I spent some time getting everything working and making a few changes to the compiler. Making the changes (the infamous ShortImports and the less well known LocalImports that let you scope an import to a let or where) were relatively easy! I didn't say anything about that other than to note that I found the recompile cycle fine, and was impressed that the compiler could admit changes so readily. The remarks about style and comments are clearly not critical issues, but if there is a style guide why are people upset with me for pointing out that it's not followed? The actual take away that I thought was clear is that I think it *would* be more closely followed if the contribution process were smoother. 
I tried to make it clear that inconsistency was the issue. It is far from a deal breaker, but the style switches add little bits of friction as you're tracing through the code. My point was not that varied style itself is a negative, but that I think its persistence is a symptom of the contribution process being too difficult for janitorial activities. Regarding Notes, as I said, they can be very helpful, but they are yet another place you might have to look to read about some design decision. That this information is spread among email threads, off the record discussions, wiki pages, trac tickets, and code comments is not ideal, but unifying those resources, or at least making them uniformly accessible, would take additional tooling. Perhaps my comments on Notes became harsher in editing, as I didn't intend to discount their value.
(I'm not associated with GHC development in any way.) Not quite what /u/bgamari was talking about, but my biggest concern with the proposal was adding "yet another import syntax" and the potential confusion that would cause. If there was something along the lines of "go fix" which could be counted on to auto-upgrade source, and it could be integrated sanely into Hackage, etc., then I'd have much less to worry about. I realize there wouldn't be actual *breakage* per se, but I think any new import syntax should have a clear plan for removing the *old* syntax (**very** eventually!) I can certainly understand the frustration, but I think /u/bgamari is right that you accidentally picked a very tricky area. (Not tricky technically, but *socially*.)
Yes, but it encourages absolutely **awful** git practices -- which is strange for a tool built on git. (See e.g. Linus' complaints.). Two of the worst IMO are: a) It encourages inconsequential drive-by PRs which are nothing but a drain because PR filers aren't willing to adhere to conventions, and b) GitHub Issues is absolutely **awful** for any moderately sized project if you don't have someone to manage it full-time. (I don't know if Trac is better, but...) Sometimes it isn't worth it.
I appreciate that. The new syntax was behind a pragma. Making it available behind a pragma seems like a necessary first step to any eventual change, so I don't know what could be done differently to effect a global switch over. Again, though, the point of my notes was the plethora of accounts, markup syntaxes, and social fora one needs to navigate to do these things. Compare this with most open source projects where you'd either just have a pull request that evolves over time in response to inline comments, or that plus an original issue report plus high-level discussion.
Not saying you don't :). Just providing some context and trying to explain why you might have met so much resistance (from *some* quarters). EDIT: grammers is bads
Welcome :)
yes I mean then I must hide the certificate in my app that I ship to the client and so on. It's getting quite complicated while electron is exactly meant to solve that issue without needing any HTTP server. I hope I find some time in the future to research the area.
I think hsqml is the closest right now. It's not comparable in the sense that you don't write your GUI in Haskell..
&gt; I made it clear from the first public utterance that the change would not affect any existing program. Sorry, I should have been more clear. I didn't mean to imply that your proposal would have changed existing programs. I added the parenthetical specifically to try to clear up this potential confusion. Instead I was just trying to point out various issues that one might encounter when trying to amend an existing grammar. If I recall correctly, this particular issue was one which your proposed grammar did well in handling, where some of the alternatives struggled a bit more.
&gt; Again, though, the point of my notes was the plethora of accounts syntaxes, and social fora one needs to navigate to do these things. Compare this with most open source projects where you'd either just have a pull request that evolves over time in response to inline comments, or that plus an original issue report plus high-level discussion. I think this is a valid complaint and I would love to hear ideas for reducing this burden. I've been looking at Rust's community procedures a bit today hoping for some inspiration. A few years ago I I personally participated for a while in the Rust community and thought their community infrastructure worked reasonably well. In particular their [machinery](https://github.com/rust-lang/rfcs) for dealing with proposed language changes is significantly more developed than ours; it may be that our lack of structure is to some extent to blame for the rather round-about evolution of your proposal. Unfortunately, the Rustaceans also have significantly more human-resources at their disposal, which their procedures seem to reflect (for instance, their RFC process provides a Shepard to each proposal, whose job it is to mediate interactions between the Rust language committee, the proposal author, and the community). As far as varied communication media is concerned the Rust community isn't doing *that* much better than we are. They previously had a mailing list (since retired), a Github Wiki, the /r/rust subreddit, a Discuss instance, the PRs of the `rust-rfcs` repo, and the `#rust` IRC channel. It may be, however, they they are using these channels more orthogonally (whereas, in our case GHC Trac tickets and Wiki pages can begin to look awfully similar to one another after a discussion has progressed for a while). Anyways, I thought I'd share these thoughts. If you do have any ideas on how we can improve the situation I'd love to chat. 
It's possible to keep Trac and just link to github instead of phabricator.
&gt; Why do you think that would be the case? Maybe because other languages either did the jump to github (ocaml recently) or have use primarily github for a while (rust, swift, ..), with great success ?
This is an interesting point. I think that if it is found to be a significant issue, it's very conceivable that an FRP system could provide you with a way to access all the state it controls to make it possible to save/restore everything.
We don't have reference equality because the run time system needs to be free to move things in the heap whenever it likes. Also, using reference equality to "optimise" Eq instances breaks supposedly valid properties we'd probably want for (most) Eq instances. E.g, if I go let a = fix (\x -&gt; 1:(map succ x)); b = fix (\x -&gt; 1:(map succ x)) in a == b GHC might optimise that to: let a = fix (\x -&gt; 1:(map succ x)) in a == a So then we get different results depending on whether the compiler optimised our code, which is horrible.
I've been talking to the Phabricator developers about just implementing `git push` in Phabricator directly, for this purpose. It would work more like Gerrit, where you push into a special remote that creates reviews. You shouldn't need anything else, then. I imagine it will hit about 90% of the mark since most people actually just dislike/become anxious Arcanist more than anything.
&gt;encourages inconsequential drive-by PRs which are nothing but a drain because PR filers aren't willing to adhere to conventions From a different angle, this in a different form has proved a problem for how we get feedback on the book we're working on. We're dramatically shrinking the pool of people who send us feedback for the next two books. This should not be taken lightly, close to half my time working on the book is processing poorly written feedback. I can only imagine code being worse in terms of effort ratio.
Hear hear. It's particularly frustrating because I've only known Anthony to be a very pleasant, constructive, and easy-going person.
Is your worry that people will run a server on the same port and sniff the traffic? The server itself is a binary, so it could do some kind of authentication to the client of sorts that only the server would know.
[removed]
no, the graph has always been static.
I was going to argue, but then your edit mentioned the argument I was about to bring up!
Are there some advantages of using a webserver for a desktop app? 
&gt; the run time system needs to be free to move things in the heap whenever it likes. Other language runtimes with moving GCs handle it fine, by restricting “whenever it likes” a bit. For example, C# has reference equality, Mono’s GC moves objects from the nursery to the major heap during a minor collection, and you can’t observe this fact from within C# without `unsafe` code. 
Haskell is not more successful than Swift. That's not to say that Swift's success can be attributed to being hosted on GitHub, though. 
Where can I find the [contributor magnet](https://www.youtube.com/watch?v=9z-uYMrWTCc) in GitHub? If I give you good money, can the contributor magnet be installed in Phabricator?
I've long desired consistent linting for things like Haskell style, etc. throughout GHC. I think it would be much more pleasurable to work on then, especially with mechanical reinforcement. But in practice people don't seem to care too much about it, and even active developers hate when linters complain at them. If people do call it out, it's generally an irregular complaint. Which does seem unfair. Even active developers hate it when I would pick on this because it seems irregular to point out *some* things and not others (so I sort of stopped). That's why you just have robots enforce it on you without remorse! In the past, the way we fixed this to ease merges was to only do one file at a time. Like when we removed tabs, we removed tabs from a file, and then made using tabs in that file an error. We could maybe do something similar. We simply whitelist what files we want to run the linter over, and over time as a file becomes lint-clean (which we'll just 'do' occasionally) we add it to the whitelist and it stays that way. It takes a while in terms of 'organic effort', but for tabs, `.lhs` files we eventually did a lot of it at once. If we could agree on consistent rules, I'd like to work on this sometime. &gt; (And personally I find SPJs style quite bizarre, especially the explicit braces with "do" notation.) I've actually come to like it. There are parts of the type checker that are a bit long, but this style actually comes out nicely when you have lots of alternatives or cases where you want to have many 'fail' cases in a very simple manner. Here's an example from `TcInteract.hs`: runTcPluginsWanted :: WantedConstraints -&gt; TcS (Bool, WantedConstraints) runTcPluginsWanted wc@(WC { wc_simple = simples1, wc_insol = insols1, wc_impl = implics1 }) | isEmptyBag simples1 = return (False, wc) | otherwise = do { plugins &lt;- getTcPlugins ; if null plugins then return (False, wc) else do { given &lt;- getInertGivens ; simples1 &lt;- zonkSimples simples1 -- Plugin requires zonked inputs ; let (wanted, derived) = partition isWantedCt (bagToList simples1) ; p &lt;- runTcPlugins plugins (given, derived, wanted) ; let (_, _, solved_wanted) = pluginSolvedCts p (_, unsolved_derived, unsolved_wanted) = pluginInputCts p new_wanted = pluginNewCts p ; mapM_ setEv solved_wanted ; return ( notNull (pluginNewCts p) , WC { wc_simple = listToBag new_wanted `andCts` listToBag unsolved_wanted `andCts` listToBag unsolved_derived , wc_insol = listToBag (pluginBadCts p) `andCts` insols1 , wc_impl = implics1 } ) } } where ... Note that with braces we don't have to live with block-level indentation rules, so it's completely OK to have the two `do` statements align column-wise, even though one of them is actually the 'else' arm of an 'if-then-else'. When you have code that needs to take lots of alternative cases like this (think like 5 of these blocks) or check particular invariants and take different code paths, it's actually quite a nice style, because you don't have to use "tricks" to keep column-bloat down, as you inevitably block-indent your code, over and over again. But I prefer consistency over any particular style, including this one. But it's not without rhyme or reason, I think!
The worry I always have to do with this approach is maintaining invariants outside of the type system. Taking the cart example, there's the implicit invariant that the `GeneralCart` isn't empty, isn't a singleton, and has more than one distinct fruit (the last of these, of course, subsumes the first two). EDIT: both /u/Syrak's [comment](https://www.reddit.com/r/haskell/comments/4iw96f/on_adhoc_datatypes/d320chd) and /u/Iceland_jack's [comment](https://www.reddit.com/r/haskell/comments/4iw96f/on_adhoc_datatypes/d32e6bx) contain approaches I would be more comfortable with.
One thing to note is that the `Eq` instances of `IORef` and `STRef` are a form of reference equality: * https://hackage.haskell.org/package/base-4.8.2.0/docs/Data-IORef.html * https://hackage.haskell.org/package/base-4.8.2.0/docs/Data-STRef.html If I understand things right, `ref1 == ref2` iff modifications made to one are visible when reading the other.
Because in the rather contrived scenario, the customer needs to buy at least 3 items in order to get a discount... I often find it's very hard to come up with examples that are: 1. Not too complex (you can't spend 10 paragraphs just explaining the example you're going to use or its background) 2. Not too trivial (because then the approach doesn't make sense) 3. Not too contrived (but I can't use the code where I actually apply these techniques because it's closed source) Does that make sense or did I totally misunderstand your question?
If you'd like to automatically compile Clay to CSS in Hakyll, the snippet I used in an old blog was clayCss :: Compiler (Item String) clayCss = getResourceString &gt;&gt;= withItemBody (unixFilter "runghc" []) with an accompanying match "css/*.hs" $ do route $ setExtension "css" compile clayCss in my `main`. This was in 2014 according to the last commit, but assuming Hakyll is still roughly where it was then, this (or something similar) should still work.
For others reading this, http://research.microsoft.com/en-us/um/people/simonpj/papers/ghc-shake/ghc-shake.pdf explains more about `shake`, which hadrian is based on, and why it works better than `make` for GHC.
&gt; Swift has interest because of the name behind it, but not much use yet. Yeah, I keep wondering what drove Apple to name their programming language after a popular singer... I'm not sure it attracts the intended audience... 
In practice I think most people basically have the same impression as you do, including active long term developers: - When the build system works: positive reviews, like "it's ok" or "mandatory". - When the build system doesn't work on some thing and you peek inside: "what", followed by "i'll just clean and rebuild" If you fall into #1, then it's actually fairly OK, yeah.
wxWidgets 3.1 has a Qt backend so it's a way forward for a native UI that has Haskell bindings and also supports thigs like HiDPI and Wayland.
Like what? Most tools just never work for me anyway. I was delightfully surprised that `haskell-mode`'s `etag`s worked, and then I learned they're produced by `ghci` (i.e. officially supported). (To be fair, `haddock` only breaks in minor ways, like comments interleaved with imports or on the same line as methods). Ideally, the GHC API would provide everything an editor or tool or even IDE needs (including parsing). That provides standardization (different editors and tools can reuse the same code) and correctness (the only way to parse everything `ghc`'s parser can parse is to use `ghc`'s parser). until then: https://github.com/haskell/haskell-ide-engine https://hackage.haskell.org/package/ghc-exactprint
Once you add reference equality to the language, then constructing a lambda now needs to carry with it an explicit location that can be checked for reference equality! So now you pretty much have to give up any compiler optimization that would lift arguments around, because it can change the lifetime and identity of those lambdas. Is a lambda that is constructed in a context that doesn't reference any of the surrounding environment 'fresh'? Do we lift it as high as possible and declare it made fresh there? This is a pretty big deal in scheme compilers, where R5RS explicitly states that lambdas carry a location. Compilers in that ecosystem have to do all sorts of control flow analysis to try to determine if a function might flow into a situation where its location is used before it can do any optimization. Data constructors in Haskell are so much tissue-paper. Used by the compiler and discarded. This means they don't even have to be constructed if the compiler can pair up their construction site and use site. Reference equality becomes even hokier in the presence of laziness. Evaluating a thunk forwards it to its "real" answer, changing its identity, or requiring the system behind the scenes to tuple up and pass around some extra token just for this functionality. Huge swathes of optimizations would become illegal to add an unignorable effect! Reference equality isn't free. You can build a little universe in Haskell that has it though: https://www.youtube.com/watch?list=PLnqUlCo055hVfNkQHP7z43r10yNo-mc7B&amp;time_continue=4&amp;v=I1FuZrDhOEk
I explicitly did acknowledge that side of the coin in multiple places in the article. I know we don't all agree on things, but you're not being ignored.
Yes, sorry, and I appreciate you pointing this out again that you did acknowledge that (in fact I think this article is quite well balanced, although I disagree with some things). I guess my problem is that you actually *did* address the problem in a more nuanced way, one that's lost when the issue is just squared back around "Github brings more people" and people are like "omg yes!" Obviously, that's what this subthread is about, but addressing the root problem is more difficult than that. I guess I'm as guilty of getting caught up in that as anyone else. But also, maybe I should just stay out of this thread. This isn't really meant to be a thread about debating points as much as it is an assessment, so it's probably best if I just end up staying out of it.
GHC performs optimizations that those compilers wouldn't dream of. Not only that but it really relies on those code transformations to emit performant code due to the presence of laziness. When was the last time you saw javac do a worker wrapper transform to convert a bunch of code using boxed ints into unboxed ints? It just doesn't happen. Scala basically takes the structure of your code as given, it doesn't do any of the let floating/lowering stuff we do in GHC, and these are precursors to much more important optimizations.
I don't understand why people are upvoting this. `There are reasons` by itself is not a useful argument. Are you referring to issues like /u/bitemyapp [has described them](https://www.reddit.com/r/haskell/comments/4isua9/ghc_development_outsidein/d31j8pm)?
Apart from you I'm not aware of a single person who wouldn't contribute to a github project on principle. Are you sure there are more people with the same attitude? And what are the reasons? Only that you'd have to open an account?
Note that that is more usafe than you'd think, as it can produce both false positives and false negatives. (At least, I remember Austin telling me that...)
How do you pattern match without exporting constructors.
I think you are arguing against a straw man. Nobody is arguing that we should acquire more contributors at all costs (i.e. dropping all code review standards and giving away the commit bit freely). We're only arguing that we should acquire more contributors at a moderate, reasonable cost (accepting small enhancements via Github pull requests). This is an excellent way for newcomers to contribute and test the waters before they are prepared to make a bigger commitment to the GHC contrib workflow for larger patches.
Might as well skip the intermediate datatype then.
The barrier to using Github is still lower. I can edit any file in my browser and auto-generate a pull request from the diff, which can be automatically run through CI. I've submitted fixes to other people's GIthub projects entirely from my phone. This is why Github is phenomenal for small fixes and enhancements (like improving documentation, updating the wiki, and fixing broken links).
Well, I published this more for you than anyone else so we could see where our perceived pain points overlap since we were talking about the wiki the other day. I certainly appreciate that you carry the torch for catering to regular contributors. It would be a disaster if you listened solely to people who think they might want to contribute in favor of those who actually do. Since I went through the full exercise, I hope the listing out of `email -&gt; trac -&gt; wiki -&gt; phab` can be a usefully concrete point of discussion about if the process can be improved. I much prefer the Issue/PR process on GH projects as it combines discussion and code in a pretty nice interface while still admitting normal git workflows (e.g. I have private remotes, public remotes, and do pulls over LAN). Ultimately, regular GHC devs commit code then deal with public backlash (e.g. the type of `$`), while outsiders have to run this gauntlet. There are various ways of smoothing out the differences between those experiences, and I earnestly hope that you can do so.
There is such a thin slither of js here you're better off just writing the js. If you did use GHCJS it would mostly be ffi calls. 
Absolutely agree - the point of including the web server is that you write most of your logic there, and then only interface to that with the JS. 
This one required more than just changing the parser, but, yes, it [exists](https://github.com/acowley/ghc/commit/bc94a316b1cc2e15b5abcfd9559a90f72e6969d1)!
Excuses me, I should make my question more clear, I am not asking to add references equality to the language so that you can compare any heap objects, eg. lambada... I'm asking the solution to this particular situation: you have a large data structure a, and both record b and c have a field point to a, how can i compare b and c without go through a? This is basically how react optimize virtual Dom rendering by using immutable data and references equality, the same story apply to the serialization and deserialization process, for example immutable.js list how to leverage sharing to make serialization/deserialization faster and more compact in their 2016 roadmap. I know there are libraries like RefSerialize, but I don't think we push enough on these.
I actually have! it's what inspired me to brush off this old project haha
I must admit I find the "lots of cases thing" unconvincing. The "dangling" else is pretty easy to miss, for example. I would think that it would generally be a better idea to put the non-control-flow bits of those blocks into either the where clause or separate them out into top-level functions. Anyway, getting off-topic, so I'll stop here :).
The point was to bring up that "wanting more contributors at some cost X" is not always necessarily clearly worth the cost X. The example was extreme merely to highlight this. It isn't clear to me that "X = allow things from GitHub" is justified, even though you think it is meager. It is also not clear if "X" in this case actually solves the root problem people have with the tool or process, as I've said before. The reality is, once we accept it *now*, we have to accept it *forever*. So we now have to maintain two major ways for people to contribute patches, and shuffle inbetween them. We now have to look in two different places for reviews, between two tools that have substantial differences in the review workflow. And, GitHub's is frankly worse (for example, compare [this page](https://phabricator.haskell.org/differential/) to [this one](https://github.com/commercialhaskell/stack/pulls) or [this one](https://github.com/haskell/cabal/pulls) and tell me which patches are immediately actionable. I'll wait for you to individually read all of the patches in the last two links.) We cannot allow any of GitHub's normal workflow. That is because we must manage the repositories with Haskell.org itself, as we enforce extra criteria on commits at push-time that are not possible with Github (and prove to be a constant source of issues for GHC developers). So there are substantially less features that work out of the box with the interface. We also cannot even necessarily *merge commits directly from remotes*, they must may need to be amended. That is because (as I've said elsewhere in this thread) the syntax for Trac and GitHub collides in an unfixable way and there's no likely future where this will change. That means that any 'GitHub references' must be scrubbed if they ever (correctly or erroneously) refer to the wrong tickets, etc. It is probably impossible to resolve this mechanically in a nice way due to the ambiguity. GitHub does not support, out of the box, the breadth of tools we want to do things like Continuous Integration. For example, I'm unaware of any place to get a perfectly usable FreeBSD build bot for GitHub commits like Travis CI, yet I'd like to add this, as well as ARM build machines, into our build matrix. Furthermore Phabricator has technical advantages in its design over Travis CI: it builds *every* commit across the whole matrix, as opposed to 'the latest ref that was `git push`ed', meaning it's much easier to pinpoint commits that introduce bugs or failures. I have been slow to do this, but with GitHub I will have to set up *another* completely out of band tool to provide the same level of coverage, and this is an area we want to consolidate, not split up. (I also plan to have Phabricator automatically report test failures directly into diffs and commits in an easy to navigate way in the UI, which is much nicer to looking at the 10,000 line logs in Travis-CI to see where things went badly. This will probably wait for Hadrian, though.) It also probably opens the flood gates on people wanting their favorite 'tools' or things supported like Reviewable.io or whatnot, since the basic GitHub review tools are so anemic. Which will just expand the complexity of contributing and me having to shuffle patches around even more. Now. It is questionable whether some small, drive by patches are actually worth this amount of effort. It is also not clear if accepting GitHub as another input method is actually the necessary fix to this problem of people feeling the development friction is too high for those tiny things. However, I am sympathetic to this argument in general - I'm just not sure the 'solution' is actually a good one. As I've talked with to multiple people, the real problem is, lots of people seem to dislike Arcanist. They want to `git push`, because that is effectively the lowest barrier to giving someone a patch, and that's what they want. You can log into Phab with your GitHub OAuth credentials in like 10 seconds, that's hardly a barrier. Most people can get over UI things, but people seem to be inherently anxious about `arc`. I'm fairly sure if we let people `git push` their patches in whatever magical way they desired, they wouldn't care as much and we would get most of the benefit of drive-by patches, with a fraction of the associated complexity. But, also: if you just give us some patch, *regardless of the method*, if it's not trivial, but something remotely complex, *it needs to go through the same process as everything else*. It doesn't matter if you `git push`ed the change or attached a `.patch` file to a Trac ticket, or struggled and triumphed over `arc diff`. Hell, some people *still* attach `.patch` files on Trac, to this day, but we still push it back if it's not totally clear. That means that even if you `git push`, you're still going to have to open an issue, file it, revise your patch, answer our questions, and possibly even still have it rejected. `git push` vs `arc diff` vs `.patch` files are like, 0.1% of the work involved. Fixing the *overall process* to streamline the remaining 99.9% of the work is obviously still very important. I think it is important and it would also have a large impact, just as much as improving drive-by patch submission (for example, the amount of reading you have to do to start with GHC is off the charts, and the GHC wiki is old and showing its age and limits IMO). But sure, if you're fixing something very obvious, typos, some really obvious build break, etc, you shouldn't need to do that. Just send a patch and we can apply. OK, I buy that. It's a weak spot of Phabricator and arcanist and people want to be able to do that. `git push` is the best in this actual scenario, because it's the small cases where `git push` is about 90% of all that's needed. But if *that's* the actual problem, we can lower the activation energy in other ways, without resorting to increasing levels of maintenance and complexity elsewhere, at the expense of our larger contributors. The point isn't just about adding contributors and value, but also about adding them in a sustainable way that tries to keep complexity manageable over time, and maximizes the actual *real* amount of work that people can achieve. All the stuff above to appease GitHub PRs or whatever? That's just busy work that could have been better spent doing work continuing to review more code or write more code. Sure, we shouldn't give the commit bit to everyone, that much is obvious. But while it seems obvious to you, it's not clear to me the sizable complexity increases outlined above are necessary, when we can achieve like, 90% of the desirable features at a much lower cost.
 foldCart :: r -- ^ Empty cart -&gt; (Fruit -&gt; r) -- ^ Single item -&gt; (Fruit -&gt; Int -&gt; r) -- ^ Bonus cart -&gt; (Cart -&gt; r) -- ^ General cart -&gt; Cart -&gt; r I much prefer the data type. When you write a `case` statement, you can reorder the branches however you like. When you call a function with positional arguments like this one, they must all go in a certain order. The case statement is much more readable, too.
Counterpoint: I see only ~2,200 pull requests for the entire GHC history in the link you sent me and four outstanding pull requests. Rust, a much younger project, has ~16,000 closed pull requests and 107 pull requests currently in progress. Clearly they have figured out a way to make Github-based workflows scale despite your criticisms of Github and I would like to understand what prevents us from learning from their development model.
You could use a more sophisticated representation that makes the implicit invariants impossible to violate, though probably it's not usually worth it. For example, since `GeneralCart` must have at least two distinct fruits... data CartView = EmptyCart | SingleCart Fruit | BonusCart Fruit Int | GeneralCart (Fruit, Int) (Fruit, Int) Cart That is, pull the first two items out of the list, so they absolutely must be present. When it's more convenient to see the full list it's easily reconstructed. Similarly, there's unsigned integer types such as `Word`, and if you know that the minimum value is e.g. 3, you can store the excess instead of the actual value. 
Day The mappings are found in the FromField documentation : http://haddock.stackage.org/lts-5.16/postgresql-simple-0.5.1.3/Database-PostgreSQL-Simple-FromField.html
I wanted to contribute some documentation fixes and improvements once. I cloned the repository and promptly gave up once I realised how terrible the process was.
Thank you very much.
Especially given [`GeneralizedNewtypeDeriving`](http://downloads.haskell.org/~ghc/8.0.1-rc4/docs/html/users_guide/glasgow_exts.html#ghc-flag--XGeneralizedNewtypeDeriving) {-# Language GeneralizedNewtypeDeriving #-} newtype Dollar = Dollar Int deriving (..., Num, ...) Where you can easily use numeric literals and operators for newly defined types ghci&gt; :set -XGeneralizedNewtypeDeriving ghci&gt; newtype Dollar = Dollar Int deriving (Show, Num) ghci&gt; 4 + 10 :: Dollar Dollar 14
That `getUser` looks like a great place to use another [ad-hoc data type pattern](https://byorgey.wordpress.com/2010/04/03/haskell-anti-pattern-incremental-ad-hoc-parameter-abstraction/): data UserOpts = UserOpts { onFailure :: RetryMode , sorted :: Sortedness , filtered :: Active } instance Default UserOpts where def = UserOpts Retry Sorted AllUsers getUser :: UserOpts -&gt; IO User
What was the primary demotivator in your case?
I made the changes and commited them on my clone, and then I went to submit the patch somewhere. I was told to download a bunch of tools, make a bunch of accounts, and do essentially _way too much work_ for some documentation improvements. So I just deleted the clone and gave up. Eventually some of those fixes were added by a core contributor later on.
I am in the middle of designing a web socket protocol and I am planning to depend on the statefulness of the protocol - the client 'subscribes' to things, however if the connection fails, the client reconnects and subscribes to things again. E.g. for a chat server, if you could just resend all the messages that were not ack'ed by the server upon connection reconnect. Elm doesn't seem to support such model.
You can even take make the scary looking `cart@(~((fruit, _) : _))` into a pattern, pattern LazyFirstFruit :: Fruit -&gt; Cart pattern LazyFirstFruit fruit &lt;- ~((fruit, _) : _) ... isBonusCart cart@(LazyFirstFruit fruit) = do Okay, this has gone far enough.
while we're at it... does a patch for integrating [`hpp`](http://hackage.haskell.org/package/hpp) into GHC exist as well? Been hoping we could get this into GHC 8.2 :-) 
Yeah, sure. So long as it's just a couple of clicks I wouldn't mind.
Records are one way of quickly whipping up a data type. See https://nikita-volkov.github.io/record/ for a list of issues. (`Lens` and friends solve one of these.)
 data CartHandler r = CartHandler { emptyCart :: r , singleCart :: Fruit -&gt; r , bonusCart :: Fruit -&gt; Int -&gt; r , generalCart :: Cart -&gt; r } foldCart :: CartHandler r -&gt; Cart -&gt; r Remember that a function `a -&gt; b -&gt; c -&gt; d -&gt; e` is isomorphic to `(a, b, c, d) -&gt; e`, and then you can make the tuple a record to give them names.
However that abstraction quickly fail when you start to use `(*)` because you need to multiply `Dollar` by `Dollar` which is wrong regarding dimensions ;)
+1 on NOTEs. They saved me so much time on what otherwise would have been roundtrips to SPJ when I was adding pattern synonyms.
I think writing a Haskell DSL that compiles to AHK could be a fast solution.
Oh certainly! Someone who only wants overloaded numbers (`fromInteger`) is strong-armed into a hodgepodge of methods that often make no sense for their types, that is clearly a design consideration.
Partly, but they don't support choice. The crux of the post was making a type to represent the limited set of cases for your invariants. 
If you want to have faster equality based on some notion of identity (which seems to be the case when you say two fields "point to" a), then you have to maintain this identity yourself. You can equip every "object" in your data with some unique number representing its identity, and use that for comparison. Haskell's normal equality is based on values, not identity. It is possible to use pointer equality to speed up regular equality in Haskell, but it's rather tricky since you want to maintain the same semantics. E.g., if you know that two objects are fully evaluated and cycle free (i.e., regular equality will not loop), then you can use pointer equality as a fast first test for equality.
http://dept.cs.williams.edu/~byorgey/pub/species-pearl.pdf works at the moment.
IMHO the cleanest solution to this would be to build your own heap/database with something like `Map ID LargeDataStructure` or `IntMap LargeDataStructure`. Reference equality that's built into the language itself is a bad idea, as other posters have explained already. To get the system you want you need to provide some notion of equality, for example by using primary keys like in relational databases. This gives you fast (non-)equality comparisons, but you'll have to do the bookkeeping yourself, for example making sure that the keys are unique, removing unused objects (GC, "dangling pointers"), et cetera.
Which is why we need either `*` to be removed from Num (or have a finer Num hierarchy) or to have a Monoid instance for Num (using 0 and `+`) (and you can even rebing `+` to `&lt;&gt;`.
I think the point of the OP is that is Ok (and we should do it more) to have "local" types. By local types I mean types which totally invisible and so don't need to be exported. If their is a need to export it, then it's just a normal type which doesn't fall into what was the OP talking about. Maybe is example is too contrived then,but it's not relevant IMO. 
Yes， it is another optimization on top of virtual dom， but you get the idea, immutable data guarantee reference equity means value equal, and react won't recreate whole virtual Dom tree, only those model has changed, this tricky thing is if you compare old model with new one using value equality, it will waste too much time, that's the whole immutable optimization about, but I don't know how to leverage this in purescript, it just don't work.
I'm guessing you take Add=Plus?
I guess for a localized, ad-hoc product type, you could just go without field names and thus bypass the problems listed in the article.
such value
Indeed! [Simon Marlow suggested the same thing when answering us on Stack Overflow.](http://stackoverflow.com/questions/36772017/reducing-garbage-collection-pause-time-in-a-haskell-program?noredirect=1#comment61127896_36772017) Unfortunately for us, the feature is too far in the future. I also confess I don't understand it very well. Simon also linked to that phabricator page, but it looks like a better source of information is this paper: http://ezyang.com/papers/ezyang15-cnf.pdf
I'm sorry to see this issue bite you, and as a fellow Haskell programmer I understand and to some point share your predicament. But this an old issue, and it is not even a Haskell issue. See for example the same problem happening in Java Virtual Machines (this presentation by the way was quite an eye-opener for me on the issue): https://www.youtube.com/watch?v=LJC_2agoLuE There are likely some workarounds. For example, maybe it is possible to remove large buffers from Haskell's GC control using the FFI API. Or wait for the sanctioned compact regions patch. 
Really nice! Thanks for bringing that link to my attention.
Previous versions of the book seem to be available online for free; one I found is at the author's university: https://svn.divms.uiowa.edu/repos/clc/projects/agda/book/book.pdf I've started reading (this older version of) the book and rewriting all the examples to use the Agda standard library's style (using propositions instead of `T`, etc.). It's a lot of fun because you know you'll be able to rewrite the examples and solve the exercises without running into situations like the old joke about Agda programmers changing the light bulb ("Are you kidding me? It takes two PhD's six months just to prove the socket and the bulb are wound in the same direction."), since they are both very beginner-friendly and built up only from previous parts of the book. Overall, I recommend it. 
I assume your referring to `Double` and `Float`? Are there other commonly used non-ring numeric types?
I think once you export the `CartView` constructors, together with an `isValidCartView` function, it's not really a local ad-hoc type anymore which means the point I was trying to make doesn't apply anymore.
Would it make sense to use a free monad? You could write one interpreter that generates the AHK scripts and another interpreter to run the bindings.
Yep, Andrey is actively working on the boring bits - they are coming!
I know I just saw slides (from /u/ezyang, I think) showing how immutability *does* help GC a lot. Maybe somebody knows what I'm talking about. [edit] NM, you link to it in the article! :D Thanks for the article; it has expanded my understanding of ghc internals just that one extra smidge more.
I think you mean [these slides](http://www.scs.stanford.edu/16wi-cs240h/slides/rts-lecture-annot.pdf). It's true that immutability helps. Specifically, it helps with promoting values to older generations more quickly. But I don't think that helps in our case; it just makes the bad G1 collections less frequent. And on the whole, the heap in Haskell has lots of mutable values, just like, say, Java.
I'd love to see your work though!
Perhaps you could sort-of fudge it by serializing fixed size blocks of messages to single ByteStrings except where they're at the leading or trailing end of the queue. You mentioned that message size didn't seem to affect it much, so using larger compound messages should be similar to decreasing your working set, no? EDIT: was working on some demo code when I learned the hard way that your benchmark is a bit too beefy for my laptop. EDIT2: Nope, not the slightest improvement in pause times. EDIT3: Hmm. I'm finding that changing the message size does actually change the max pause time. I'm also finding that using parallel GC improves max pause time but *only for smaller messages what on Earth is going on here.* ByteStrings are supposed to be pinned, their size shouldn't have anything to do with max GC pause, let alone change whether or not parallel GC helps!
I day job performance/scalability testing and this was frustrating for me to read because I can see how many person-hours you've put into this, yet I've seen these same misconceptions over and over and I don't understand why, in 2016, we have not gotten the word out that garbage collection pauses are linear in the size of the retained set. (And the total GC time, meaning sum of all pauses, is inversely proportional to the amount of free memory.) This needs to be taught in undergrad computer science, and early enough that even people who only take a minor learn it. Sooooooooo many people try to optimize by reducing the amount of garbage they create, but it's a hopeless strategy that defeats the benefits of garbage collection. The solutions I'm aware of are offload the dataset into a database or build a distributed system with multiple heaps or fast/slow heaps.
Yes, Double and Float are examples. And so are finite integral types that throw exception on overflow (which is what I think Haskell's Int should do).
&gt; LocalImports Which version of GHC is expected to support this? I did a few quick searches, but could not find any references to it.
I don't think incremental GC changes the fundamentals that I described, especially if you have a large long-lived working set.
Floating point numbers are said to break the algebraic laws because they are inexact. But could floating point numbers with proper handling of error bounds be able to pass many of the algebraic laws?
No version of GHC released by the GHC development team is expected to support this, but you can build a GHC-7.11 with it using the repo I linked. That's sort of the point of all this. We live with worse software than we should because of how little respect elements of the dev team have for outside contributors. As the tooling and standard libraries are taken over by other parties, it's entirely possible a derivative compiler could emerge, too. Should that occur, I expect we will see a flood of innovation. Until then, we'll just get periodic reminders of how many of the community's most prolific contributors stay away from the compiler whenever someone takes a deep breath and wades into it as with these notes.
I agree that multiplication is less foundational than addition, but in algebra for example addition feels much heavier than multiplication. That's why addition has a symbol but multiplication is just written as juxtaposition.
A TChan is an imperative-style linked list, with two working pointers, one to the write location (the end of the list) and one to the read location. It uses TVars rather than references, but the difference doesn't matter for this discussion. We rely on the garbage collector to clean up the prefix of the list after you read it. To duplicate a TChan we just make a duplicate with the same read/write locations as the old one (with cloneTChan) or with the read location set to the current write location (with dupTChan). You can spin off 50 workers with dup'ed TChans, and the only 'leaks' will come if the associated threads you have pumping them aren't able to keep up with the message supply, but this is precisely what it should be, as it represents an unbounded channel. While a TChan is alive it'll keep everything from the read-head forward in the list alive, but once it gets garbage collected it won't keep anything alive.
I think CNF is very unsuitable for incremental GC. But it should help a lot if there is data you know is long lived and shouldn't be traced; just stuff it in the CNF and you'll stop paying for it in major GCs.
If these don't scale to meet your needs you may want to have a look at https://hackage.haskell.org/package/unagi-chan which provides a similar API with much better concurrency properties.
&gt; Within a few hours we received a definitive answer from Simon Marlow, the main author of GHC’s runtime system, who confirmed that our hypothesis was correct and that currently there is no way to work around this in GHC. Does anyone know if working around this issue by abusing the C FFI to work with unmanaged memory helps? Basically, translate the Haskell messages into a more C-like representation for queuing, using `Foreign.Ptr`, `Foreign.Marshal.Alloc` and `Foreign.Storable`. The Haskell objects are then garbage so the GC doesn't copy them. Then, when extracting from the queue, reconstruct the Haskell object. Precisely two copies per message queued, rather than some unbounded number depending on the queue length and frequency of garbage collections. It means doing some manual memory management, but presumably only within a single module. This seems like it *should* work, but I've never done anything complex enough to worry about performance using these methods, and I imagine there's potential issues - for example if your messages have pointers to other Haskell data, that may mean `Foreign.StablePtr` issues, and I have no idea what impact that has on garbage collection costs. Actually, thinking about it, since `StablePtr` references a Haskell object that must not be moved or deleted by the garbage collector, it might worth checking the impact that has on its own (without using FFI memory). Presumably it prevents those copies, but maybe with other worse costs? 
Which languages allow you to set a max pause time for the GC? Does Java / HotSpot?
What particular feature of Parsec are you missing? For instance, the code you wrote above should Just Work™ if you can write an Alternative instance for your type, I think.
If I double the available memory, the GC will have twice as long before it has to cycle. So I will get double the time between each "stop-the-word" event. That makes sense. But what I don't understand is: If the GC runs only half as often, wouldn't it have twice as much work to do each time it does run?
Except you could also drop the whole client/server architecture and just compile all of your Haskell business logic with GHCJS. 
Thanks for linking a free version of the book. https://svn.divms.uiowa.edu/repos/clc/projects/agda/book/ The README [at the above link] says that the book you linked is only about half complete and is missing material. Do you know if that is accurate?
I mean that all you need for `many` to work is an `Alternative` instance. Maybe I don't understand what you're trying to do (or what functionality of Parsec you're trying to use). I definitely don't know how to make Parsec parse anything that isn't text, so aside from just pattern matching I'm not sure what a good solution for you would be.
As it happens, those instances already exist, as `Sum` and `Product`. getSum (Sum 1 &lt;&gt; Sum 2 &lt;&gt; Sum 3) = 6 getProduct (Product 1 &lt;&gt; Product 2 &lt;&gt; Product 3) = 6
It does :-) https://github.com/sboosali/workflow-osx/blob/master/sources/Workflow/OSX/Types.hs#L46 (Any custom monad, besides the "opaque" `IO`, could work. But `Free` saves you so much effort.) As /u/Darwin226 mentions, generating an AHK from a monad isn't possible. e.g. uppercase = fmap Char.toUpper c uppercaseClipboard = do c &lt;- getClipboard setClipboard (uppercase c) `uppercase` is a haskell function that must be run between the two actions. AHK statements can't be interleaved with haskell functions. Many workflows aren't even `Applicative`. e.g. transferText = do press "C-x" press "A-&lt;tab&gt;" press "C-v" could be represented as: transferText = ["C-x", "A-&lt;tab&gt;", "C-v"] without any loss of expressiveness. i.e. for `transferText`, not only (1) can't its actions take input from other actions (not a monad), but (2) those actions don't even return any output (not an applicative). `transferText` can be easily compiled to an `.ahk`. An example of an `Applicative` workflow might be: (,) &lt;$&gt; (getClipboard &lt;* delay 1000) &lt;*&gt; getClipboard But full `Monad`ic workflows are too important. e.g. the very simple -- | google's the current clipboard contents. googleClipboard = getClipboard &gt;&gt;= google -- (as above, calls `openURL`) Well, I guess `googleClipboard` is really only an `Arrow`. An example of a necessarily `monadic` workflow: pasteUnlessHuge = do c &lt;- getClipboard if (length c &lt; 1000000) then sendText c -- inserts text into the current application else return () -- does nothing, neither truncating the data nor force feeding the app too much of it Not only does the input to `sendText` depend on the output of `getClipboard`, but whether or not this effect is run depends on the output of that other effect (the "shape" of the continuation, where "continuation" is the block for which `c` is in scope i.e. below the `c &lt;- getClipboard `). (But even if arrows were enough, I'd likely stick with monads for the syntax (do notation over proc notation) and library support (no "free arrows" packages)). `IO` is a already monad. The benefit of the indirection (i.e. `(Workflow (), Workflow () -&gt; IO ())`, not just `IO ()`) is that you can provide any "monadic interpretation" you want: 1. Currently, I have WinAPI bindings 2. I might add calling shell scripts (and waiting on their result), if DOS (or whatever) has sufficient functionality (e.g. `open` can launch apps and open urls, but I don't know if it can access the clipboard, like `pbcopy` on OSX) 3. You could even forward the workflows to some random program that does all this from some networked API (which can respond to requests with non-unit responses). (In other words, the benefit of an interface is that it can have multiple implementations). 
That's right.
They might want to take a look at /u/ndmitchell 's https://hackage.haskell.org/package/nsis-0.3/docs/Development-NSIS.html for inspiration. It "cleans up the mess" of a tool with rich features and a poor scripting language. 
Well, that's not what I call an "instance".
&gt; Can Haskell adopt other languages (even scripting languages) and then let us symbolically turn the syntax of those languages into Haskell format? Yes! We can do so by creating an "Embedded Domain Specific Language" (EDSL). The DSL part means we're writing code in a specialized language which can only describe, say, mouse clicks, as opposed to a general purpose language which can describe everything from web apps to compilers. The "embedded" part means that instead of defining a whole new language with a custom syntax, a custom compiler, a custom everything, we'll just define a few Haskell functions which happen to describe mouse clicks. I'm not familiar with the syntax of AutoHotKey, so let's imagine a simpler language named AutoClick instead: &lt;move to 100,100&gt;&lt;click&gt;&lt;move to 200,200&gt;&lt;click&gt; So AutoClick has two commands: moving the mouse to a particular position, and clicking at the current position. An AutoClick program consists of a sequence of commands. Let's model this in Haskell: data Command = Move Int Int | Click type Program = [Command] And tada! I can now write AutoClick programs using Haskell syntax: myProgram :: Program myProgram = [Move 100 100, Click, Move 200 200, Click] In particular, we can use Haskell's abstraction facilities to define higher-level operations which don't exist in AutoClick. For example, we can define an operation to click at a particular position, and then use it to rewrite `myProgram` more succinctly: clickAt :: Int -&gt; Int -&gt; Program clickAt x y = [Move x y, Click] myProgram :: Program myProgram = clickAt 100 100 ++ clickAt 200 200 There are many more ways we could make to our EDSL easier to use, but a more important question is: now that we've described the clicks in Haskell, how do we actually perform the clicks? The answer is that we transpile our EDSL code into AutoClick code. Since our EDSL code is just an ordinary Haskell value, the transpilation process is just a function which produces a string. import Text.Printf transpileCommand :: Command -&gt; String transpileCommand (Move x y) = printf "&lt;move to %d,%d&gt;" x y transpileCommand Click = "&lt;click&gt;" transpileProgram :: Program -&gt; String transpileProgram = concatMap transpileCommand You can then save this string to a file and run it with AutoClick. If AutoClick has an API or a command-line interface, you could automate this step, allowing you to control the mouse directly from Haskell without having to manually go through AutoClick.
In Parsec you can use the [ParsecT](http://hackage.haskell.org/package/parsec-3.1.9/docs/Text-Parsec.html#t:ParsecT) monad transformer: `ParsecT s u m a`. `s` is the stream type (so in your example you have `ParsecT [Row] u m a`). Then you need need an instance of `Stream s m t` to use the parser. `t` is the token type (`Row`). So to parse a list of `Row`s, the instance `Stream [Row] m Row` should be defined - if you look at the [Stream](http://hackage.haskell.org/package/parsec-3.1.9/docs/Text-Parsec.html#t:Stream) class you can see that it exists :-) The documentation for [`tokenPrim`](http://hackage.haskell.org/package/parsec-3.1.9/docs/Text-Parsec.html#v:tokenPrim) should be helpful too
http://researcher.watson.ibm.com/researcher/view_group_subpage.php?id=175 250 micros isn't bad. The G1 collector also has a pause time goal, but that's ugh, a little more aspirational, where metronome can be deployed in hard realtime settings.
To a first approximation, for at least one GC (the lisp one). I don't know about the details of the GHC garbage collector, but it is *probably* something like that.
As /u/augustuss mentions, it changes the behavior: let x = [1..] in x == x -- bottom let x = [1..] in x `reallyUnsafePtrEquality#` x -- 1# (maybe? what if ghc inlines x? what if the copying garbage collection moves x?) I guess if you have a type like newtype Port = Port Int you aren't going to be intentionally declaring any bottoms (e.g. cycles, which can be useful in lists, like the eponymous `cycle ['a'..'z']`). So the short-circuiting `(==)` that violates referential transparency might be worth the risk. (But in this example, comparing `Int`s is fast enough, the runtime might even intern them itself). https://www.reddit.com/r/haskell/comments/4h36ou/haskell_programmer_seeks_forgiveness_for_using/d2nr3f8 http://stackoverflow.com/questions/9649500/what-advantages-do-stablenames-have-over-reallyunsafeptrequality-and-vice-vers 
I don't understand why there has to be any invariant. Clearly `CartView` represents a `Cart` but it is not an one-to-one correspondence (I think *view* in the names adds confusion). `CartView` contains some additional information - namely the way it should be printed. Consider `CartView` to be a part of printing interface. Suppose there was a function `printCartView :: CartView -&gt; IO ()`, such that `printCart2 = printCartView . cartView`. `printCartView` is not concerned about business logic, so it makes no assumptions about its argument. It will happily print any `GeneralCart`, no matter singleton or not. All in all, some carts admit more than one way of printing. `CartView` represents just that.
e.g. `newtype Port = Port Word16` literally the only method I want is `fromInteger`. 
It is.
ACM professional members can add annual access to *all* ACM Books for $29. For student members it is only $10/year! See the [blurb](http://books.acm.org/subscribe).
Exactly. As you run out of memory, the total time needed for GC until the program ends approaches infinity.
Yesod.Auth.Email is completely customizable. It doesn't sound too hard to customize it to skip all the password-related steps, e.g., have `setPasswordRoute` just redirect to `afterPasswordRoute`, etc. Have you tried that?
This doesn't sound very hard to implement. HMAC the timestamp and user, use that for the link, check in that endpoint that the timestamp is not too long ago, and set the cookie?
I think that would be awesome! I would definitely be much more inclined to contribute if such a workflow existed - it would also be a great balance that kept both sides happy I think.
You wouldn't have a blog on your work with Haskell performance &amp; scalability, would you? ;)
I have not tried anything yet; I was hoping to solicit feedback before starting the project. I'll try your suggestion, though. EDIT: https://github.com/yesodweb/yesod/issues/1222 is worrying for this, though.
this is a very interesting question. I don't believe the core question has been answered in the thread, related to optimising equality checks for large **immutable** data structures. even I'm not sure how the compiler/run-time will deal with something like let x=(large complicated tree DS) let y=x x==y will y be a copy of x, or will both "point" to the same "thing" in memory? will the run-time do unnecessary equality comparisons on each element/node of this DS?
Point taken. I should probably write something up. :)
Are the books available DRM-free (or at least in a PDF or something, if it must have DRM), or is it a web viewer of some kind? It's a really good deal, but dealing with web viewers is kind of a non-starter for me.
Yes, that's I am thinking about, I wish runtime can leverage immutable property to optimize some code without change semantic, but it seems impossible?
Also, if you want channels/queues which can only grow to bounded size or which can be closed, you may want to have a look at http://hackage.haskell.org/package/stm-chans
Why don't you just try to experiment with Github in some capacity? The worst that happens is that you'll find it out it doesn't work and you can abandon the experiment. The reason I suggest this is that you may find that you'll get a substantially larger pool of developers like Rust has if you can find some low barrier way for people to get their feet wet with GHC development.
It has a garbage collector so duh? Thee are companies out there who make millions on their custom low latency huge memory jvm for example. I say right the hard bits in rust and call from gcc. They seem like two peas in a pod. No GC low overhead more memory safe strict rust and gc'd lazy haskell. Rust could be the c to Haskell. 
I don't know, but you can call ACM at +1-212-626-0658 or e-mail acmbooks-info@acm.org The website also says, "ACM Books are published in multiple digital formats, including downloadable pdfs and reflowable ebook formats." I've read about this series in ACM Communications, but I haven't understood about the subscription until today. I'll have to look at the list of books already published to see if it is worthwhile for me. The Agda book sounds interesting, but I still have a lot of Haskell to learn first.
Note that while Haskell values are immutable, their representations in memory are *not*.
&gt; I day job performance/scalability testing and this was frustrating for me to read because I can see how many person-hours you've put into this, yet I've seen these same misconceptions over and over and I don't understand why, in 2016, we have not gotten the word out that garbage collection pauses are linear in the size of the retained set. [...] Sooooooooo many people try to optimize by reducing the amount of garbage they create, but it's a hopeless strategy that defeats the benefits of garbage collection. So earlier this year I was working on this situation in a Haskell program, so it's pretty fresh in my mind: 1. Profiling the benchmark suite showed enormous allocation and %GC time numbers. 2. Using the `-A` RTS option to increase the allocation area size made a dramatic improvement on run time and %GC time. I tried a few sizes and settled on `-A256m` (the default is 512KB). 3. The profiling in #1 attributed more than 95% of all the allocation to a handful of "inner loop" functions in the program, which were doing a lot of random access into unboxed `Vector`s, but with generic `Vector` class operations. Inspecting the Core revealed that the vectors were being accessed with heap-allocated `Int` indexes. Using `INLINABLE` pragmas and more aggressive optimization made all those heap-allocated `Int`s go away, which produced an even more dramatic improvement in run time, and eliminated the need. So I can't help but conclude: * The retained set was comparatively small, so the cost of a round of GC was small. * The program was nevertheless generating garbage at such a prodigious rate and filling up allocation area so often that the run time was dominated by GC. * Increasing allocation area size therefore reduced how often the GC was triggered without increasing the cost of each individual collection (since the retained set was still as small as before), leading to a dramatic run time improvement. * But the proper solution was **don't generate that much garbage**! (Or, at least, don't do it in your program's innermost loop.) 
For documentation changes these days, I tend to not even use git, but the inline github editor. The *full process* is &lt;30 seconds.
First [Scala goes native](http://www.scala-native.org/) and now Haskell makes the jump to the JVM. Super cool&amp;mdash;good luck with the project!
Have you tried auth0? See auth0/passwordless
This is exactly what I was thinking, although it is kind of a kludge. Instead of using a nice, simple, clean Haskell code with a familiar data structure, you have to create your own map-like data structure from scratch that basically operates within it's own block of memory. It would be nice if GHC gave us the ability to use a different kind of memory allocation scheme on a per-module basis. Ideally, it would be nice if we could set some compiler flag for "MessageBuffer.hs" to reserve a separate memory allocator and a separate garbage collection routine (like a mark-sweeper) that is only used by the code running in that module. Bonus points if each module's unique allocator could run independently in it's own thread without stopping the world for the remainder of the program. Of course, if another module uses an object from the isolated module, the whole object graph would have to be copied out of the isolated module. But if the only thing you retrieve from the isolated module is messages or integer values, then this isn't a problem.
This seems to be the only realistic solution at the time. But it is a shame that you can't just use nice, clean, pure Haskell code with a well known data structure, you have to reinvent the wheel and implement a Map data structure using mutable arrays just to avoid GC issues.
You can never implement a syntax change only in the parser, because you also want good error messages.
It depends on what you're optimizing for. In our case we were concerned with latency, where generating garbage is mostly a non-issue. But it sounds like you were optimizing for throughput (equivalently: reducing total %GC time), where generating garbage is indeed a problem.
You underestimate how creative people could be :) The idea is to allocate to one region, but create other one and evacuate objects from the former in background. You can do that incrementally, and swap regions at the end. You can build pretty sophisticated garbage collectors that way, e.g. the train algorithm.
Unfortunately I can't find the code. I described the idea in other reply in this thread.
As the post pretty much rules out GHC/Haskell as a solution to their problem, I was going to suggest Rust as well. You get the reliability of a strong static type system and the predictable latency of deterministic memory management.
ghcvm wants to support FULLHASKELL. i.e. ghc compiles haskell to stg, then ghcvm compiles the stg to java (or jvm bytecode or whatever). This means most (all?) language extensions work, and most (all?) non-C-ffi code works. Frege is a dialect of Haskell, supporting "almost all of Haskell2010" with intentional differences. e.g. `String` is abstract, and is the java `java.lang.String` primitive. The generated code is probably faster and more idiomatic. https://github.com/Frege/frege/wiki/Differences-between-Frege-and-Haskell 
That is very good news! is there some documentation around design ideas? I am very interested to know the plan to handle Java interop, TCO etc.
I have not thought of many different cases.
I found some documentation here: https://gist.github.com/rahulmutt/9bcab8c9b8bc3d99be26c11ae42d2795
https://hackage.haskell.org/package/inline-java seems to use the JNI. 
The price is ok for academic departments.... By the price, this book is not vetted for the masses :-( .... 
...so yeah, if you could just try a little bit harder avoiding success, that would be great!
Sorry for being daft, but what advantage does this have over compiling to native?
&gt; Btw, our current prototype solution is exactly "to remove large buffers from Haskell's GC control using the FFI API". We might post about that in future. :-) Is there no library on Hackage to do such a thing already? If not, it might be nice to release some sort of "offheap-containers" package.
The joke is parametrized over dependently types languages.
You're right. Guess I've only used "constant patterns" &gt;&gt;&gt; pattern HTTPS_PORT = 443 &gt;&gt;&gt; HTTPS_PORT 443 
Can you derive generics for GADTs?
&gt; Of course, if you wish to you can use this new type-level encoding at compile-time instead of at runtime. For example, Ben Gamari has defined a type family for determining whether a datatype is strict in all its fields by examining its generic Rep. This opens up a whole new world of meta programming if we can choose at compile time what "style" of types are permitted (didn't want to use the phrase "kinds of types") for particular instances at compile time. --- &gt; The fields of ExampleConstructor will have different DecidedStrictness depending on what flags are used to compile GHC This gives me interesting ideas, being able to decide which functions to execute based on the strictness of fields could mean that a given function could decide whether to run an optimized algorithm when compiled with `-O2` or a debug algorithm when no optimizations are enabled that prints out intermediate values, for example. Sounds like it could make for fragile code, though. It might be hard to predict what the code is going to do since this would be unintuitive behavior.
Very cool, thanks for all the work!
Unfortunately, no. That's still very much an open research problem. There have been attempts at generic encodings of subsets of GADTs. For example, the [`instant-generics`](http://hackage.haskell.org/package/instant-generics) package can encode a representation for GADTs that use existentially quantified type variables, provided they meet certain restrictions (you can't have quantified variables as arguments to a constructor, for instance) described in section 6 of [this paper](http://dreixel.net/research/pdf/gpid.pdf). The author [has admitted himself](https://ghc.haskell.org/trac/ghc/ticket/8560#comment:9) that he doesn't find this approach very elegant, however. [Another paper](http://www.cs.uu.nl/research/techreps/repo/CS-2009/2009-017.pdf) successfully encodes a wider range of GADTs, but its approach is quite different from the one used in GHC generics. I'm not sure how feasible it would be to use.
When using the phrase "derive generics for GADTs", one almost always means "GADTs with existential constraints", such as: data Nat = Z | S Nat data Vec n a where VNil :: Vec 'Z a VCons :: a -&gt; Vec n a -&gt; Vec ('S n) a deriving instance Generic (Vec n a) This has never been possible. If you try to compile this, GHC will loudly complain: • Can't make a derived instance of ‘Generic (Vec n a)’: VNil must be a vanilla data constructor, and VCons must be a vanilla data constructor • In the stand-alone deriving instance for ‘Generic (Vec n a)’ And for good reason: GHC generics doesn't give you a way to uniformly represent existential constraints!
From the operations perspective whatever Java does for its memory management is a huge pain. Requiring virtually every user to specify half a dozen parameters probably makes their task easier but it certainly doesn't get easier for the users of your software.
How do inodes help you avoid fragmentation? Isn't fragmentation a (non-)issue depending on your allocation scheme for data blocks in your files while inodes deal with metadata?
Sqlite is a huge pain (used from any language) if you want to access it concurrently (e.g. in a server from each request thread). Serializing access in some way takes away many of the advantages you try to gain here (locks make you wait again).
I’m not sure what you are asking, but let me elaborate. It is all about using the right tool for the job. If you want to have predictable performance and optimise for latency, then you must amortise the cost of memory management over the entire running time. If throughput is what you are after, a GC can perform very well and depending on the kind of program and the kind of GC, a GC can outperform deterministic memory management. For instance, if your program involves a lot of cyclic data structures, refcounting overhead (which is what you’d use in Rust) can be considerable. Apart from that there are cache effects of things like heap compaction. But if low latency is what you are after, minimising the _total_ time spent on memory management is not the most important thing. You want to have a low upper bound on every individual memory management operation. The obvious choice then, is to use a language that gives you control over memory. C does that, but it is also very tedious (manual memory management) and unsafe (a weak type system). C++ is somewhat better (if you use smart pointers and RAII correctly), but its type system doesn’t help you write robust software. Rust gives you automatic, deterministic, memory management, and a type system that ensures safety without sacrificing control. (The price you pay is that it is harder to write a program that compiles.) The effects of heap fragmentation depend on the allocator and the allocation patterns. In this particular case, if there is a reasonable upper bound on the size of a message, you could use a ring buffer of preallocated messages and that would completely eliminate memory issues if you can spare the RAM. If that wastes too much memory, a binned allocator will work well. A span-based allocator can perhaps save more memory, but you might indeed run into fragmentation for long-running applications, and the allocation time is less predictable. Rust uses jemalloc by default, but you can plug in a different allocator if you like, or you could write a custom specialised one. Aside from control over memory, Rust has `Option` and `Result` types which make it an excellent choice for writing reliable programs. True, it doesn’t have higher-kinded types and all of the advanced machinery, but it [does](https://doc.rust-lang.org/std/fs/struct.File.html#method.open) [force](https://doc.rust-lang.org/std/io/trait.Read.html#method.read_to_end) you to consider that opening a file or reading from it might fail, something that even Haskell [doesn’t do](https://hackage.haskell.org/package/base-4.8.2.0/docs/Prelude.html#v:readFile). I’ve found the Haskell principle “if it compiles it likely works” to apply to Rust a great deal too.
On the other hand, do you really want the entire crappy Java ecosystem with its overall poor library design dumped onto Haskell? Looking at Scala and similar languages it seems virtually impossible to reuse Java components without giving up a lot in the process.
Quick markup correction: you need to escape the asterisk in "(\*)".
Lots of languages live by the fact that shops standardize on JVM based languages. 
Cool! Out of all the haskell data visualization libraries, I've been especially happy with [the diagrams plots library](https://github.com/cchalmers/plots) - it has things like heatmaps in pure haskell, although there are a few rough edges still. I really like the scatter chart you've done, would your work fit well as a contribution to the plots library? I think they have one or two scatter charts built in already, so this may sit well alongside the other ones (or subsume them).
It's called Backpack. If you google "backpack haskell", I'm sure you can find loads of papers and things :).
&gt; Linus' complaints. Well, he has proven to be wrong imo. Github PR __are__ much more featurefull and practical that plain diff sent by mail. &gt; It encourages inconsequential drive-by PRs which are nothing but a drain because PR filers aren't willing to adhere to conventions, and b) GitHub Issues is absolutely awful for any moderately sized project if you don't have someone to manage it full-time. (I don't know if Trac is better, but...) no, that's wrong. Please, could you elaborate those "awful" git practices ? and anyway, lots of people handle big projects on github very well. Just take a look to convince yourself. Also, it's always possible to enforce strong policy ci, bots, saved replies (reply templates) ,etc. 
How near/far off is running hello world on the JVM ?
If I recall, you never made a proposal or went through a patch process for localimports because you were so burned on the shortimports extension? Apologies if my memories are way off base here...
&gt;The free software crowd obviously, since they care about freedom and github actively opposes that. are you trolling ? &gt; The free speech crowd again obviously since github actively opposes that. are you trolling ? &gt; We see github largely as a place for web hipster dummies to write buggy clones of several decade old unix utilities in javascript for a couple days and then spend months arguing about who is oppressing who because of variable names ...trolling ? oh, you forgot to mention that Abraham Lincoln and santa claus too avoid github because they think it cause cancer. ---- https://github.com/google https://github.com/microsoft ... Also, linus use github. what he told was about PR vs manually applying git diff. that's old stuff, PR are way better now, and one can use github without PR. 
yes, it does. if you go out of your little bubble, and join the big bubble that is github, you'll see it does. Also, I would be interested to know why you think that github is an hostile organization ?
&gt; Can the DecidedStrictness stuff be used to optimize NFData instances? It can. From [/u/bgamari's example](https://gist.github.com/bgamari/b67becc433026e80ba2b) mentioned in the blog, you can use type families to [dispatch at the type level](https://gist.github.com/bgamari/b67becc433026e80ba2b#file-deepstrict-hs-L41-L48) whether a datatype is strict in all its fields or not. From there, it's simple to [define a typeclass](https://gist.github.com/bgamari/b67becc433026e80ba2b#file-deepstrict-hs-L50-L57) with a method that either invokes `deepseq` or `seq` depending on whether the type is fully strict or not. There's also an [open issue](https://github.com/haskell/deepseq/issues/3) in `deepseq` related to this question.
I agree I was a bit hostile, sorry about that __but__ I really think what I told you. When you state that github don't bring anything more that using email + git patches, I have the impression you really refuse to see the real world. You don't give real arguments against github except that they are evil and you just question the fact there is lots of people on github ready to contribute. ----------- Regarding the fact that they delete repos and ban users, I never heard of that before. Do you have proof you could link ? I know they ban abusive users after a manual profile review if too many people complain, but any organisation (github or not) ban abusive users, so nothing special here.
&gt; no, that's wrong. Please, could you elaborate those "awful" git practices ? &gt; and anyway, lots of people handle big projects on github very well. Just take a look to convince yourself. &gt; Also, it's always possible to enforce strong policy ci, bots, saved replies (reply templates) ,etc. I think you may be missing something. I'm *on* GitHub and participate in a great number of projects. I don't *like* it, but I'm there... and it's *just* because of the network effects. It's *awful* compared to the tools I use for day-to-day professional development.
&gt; Access to a ton of high quality Java libraries (Apache, ...) instead of one man projects that were abandoned a year ago "High quality" Java libraries written for the kingdom of nouns. &gt;Ability to write high performance imperative code when needed without going down the rabbit hole that is C Highly tuned Haskell is faster than highly tuned Java, this is an anti-feature. &gt; No global record namespace Point. &gt; No String, Text and ByteString confusion, there's just String Text and ByteString serve different purposes, conflating them is just a terrible idea. &gt; Applicative is a superclass of Monad [Um...](https://hackage.haskell.org/package/base-4.8.2.0/docs/Control-Monad.html#t:Monad) As for disadvantages? There is only one active developer. It is missing a lot of the more recent and powerful GHC toys. Java interop is only even desirable for some Haskellers anyway, many (most?) wouldn't give anything up at all in exchange for it. That's one minor pro (that's getting fixed anyway) and a whole bunch of cons, some pretty major.
Your first advantage of Frege is written as: &gt; Access to a ton of high quality Java libraries (Apache, ...) instead of one man projects that were abandoned a year ago Every ecosystem has lots of abandoned projects. But those aren't the projects that you should look at when you characterize an ecosystem. GHC Haskell has lots of actively maintained projects, and I like to look at those instead of the abandoned ones. It has libraries for basic web server things (notably `wai` and `warp`) but nothing that is nearly as full-featured as apache. But to answer your actual question, I would say that I don't need the JVM. I don't need apache or any on the Java libraries. If I worked in a different field, maybe I would need them, but I haven't run into situations where I wanted them. Yeah, it's kind of annoying that some people use `String` when `Text` is preferable, but it's not worth abandoning all the GHC libraries. Also, GHC haskell has had `Applicative` as a superclass of `Monad` for over a year. Here are the [docs to prove it](http://hackage.haskell.org/package/base-4.8.2.0/docs/Control-Monad.html#t:Monad). So you might want to remove that point.
&gt; Highly tuned Haskell is faster than highly tuned Java, this is an anti-feature. I seriously doubt that. Some algorithms just work better written in an imperative form Especially matrix calculations, graph algorithms, etc... Things that just seem hard to represent without mutation. [Computer Language Benchmarks](http://benchmarksgame.alioth.debian.org/u64q/haskell.html) kind of reflects that. &gt; Text and ByteString serve different purposes, conflating them is just a terrible idea. [That doesn't matter if they are a pain to use](https://www.reddit.com/r/haskell/comments/4f47ou/why_does_haskell_in_your_opinion_suck/d25zq7t) &gt; There is only one active developer. &gt; It is missing a lot of the more recent and powerful GHC toys. &gt; Java interop is only even desirable for some Haskellers anyway, many (most?) wouldn't give anything up at all in exchange for it. Can't argue with those points! 
I've never seen plots - thanks for the link.
&gt; Do you really want the entire crappy C ecosystem with its overall poor library design dumped onto Haskell? Is this a trick question? In any case the answer is: No. If (GHC) Haskell's runtime interacted only with the kernel directly, I'd be quite happy, thankyouverymuch. Need I remind you of e.g. https://www.us-cert.gov/ncas/current-activity/2016/02/17/GNU-glibc-Vulnerability ? We *can* do better than this.
Do you have a link to the bibliography you are talking about? That seems interesting. I have had some problems with other solvers of that type, it is really difficult to get meaningful feedback about why it did not work. 
&gt; Some algorithms just work better written in an imperative form Especially matrix calculations, graph algorithms, etc... Things that just seem hard to represent without mutation. There are certainly algorithms where the fastest known approaches require mutation. This doesn't tell us that Java will be faster, however - Haskell does permit mutation, and when we're talking about "highly tuned" we've already left behind idiomatic code.
Not interested. It isn't appropriate to my current project, which are no JVM-based. Not that I wouldn't consider it for a different project.
Really glad to see this extended series. I love servant and hope it gets more traction. Excellent work!
I was talking from the perspective of a place which already has a big Java-based system running. Otherwise of course you wouldn't go near it, it's horrible!
I wanted to make a blog post very similar to this since I'm working on a self-contained project at work using Servant very closely to how you outlined in this blog series. I see niche has been filled, and much more comprehensively than I would have been able to write. Thanks for the contribution to the community! :)
I have to agree; this post sounds a lot more like "y'all are dumb for not using Frege" than an actual search for useful information.
Perhaps this is nitpicking, but it does bother me that Frege (the logician) was an anti-semite. I'm reminded of it every time I hear about the language. That has really killed my enthusiasm for the language. As fas as technical reasons go, I just don't normally have any reason to go near the JVM. The only thing I use with any regularity that runs on the JVM is minecraft. Making minecraft mods might be fun but I have yet to go down that rabbit hole.
Would love to see an episode with authentication and security.
For code you control, you can always just decide on a formatter tool and verify its use a code review time (preferably through an automated process). For code you don't control, you follow existing style or your commits don't get mainlined.
Isn't "I am strict in all my fields" just mean that my WHNF implies their *WH*NF? That doesn't seem to say anything about whether I need to apply deepseq...
An `IORef` holds a pointer to a `MutVar#` on the heap. The `MutVar#` points to the object in question. `MutVar#` lives in # and so cannot be `_|_`. When we mutate the `MutVar#` we mark it dirty for the garbage collector. GHC isn't currently capable of marking data constructors as dirty for GC. This is for many reasons, but in particular the data constructors get made and torn down whenever you pattern match on the constructor. So how do you preserve its "equality"? e.g. i open a data constructor that holds an `IORef`, then pack it up into another constructor. But then I can go back and mutate the original `IORef`, this is required to mutate the _other_ `IORef`. These semantics preclude unpacking an `IORef` fully into a data constructor as a direct pointer to the target object! If you unpack the `IORef`, then you go from a pointer to the IORef which points to the `MutVar#` which points to your data down to pointing to a `MutVar#` which points to your data. I gave a talk a while back about a library for building things in kind # that can point directly to other things in kind #. This gets rid of the last indirection, in exchange for having to work in another universe (`#`) where everything is strict. I get around this by putting a little wrapper (like `IORef`) around the mutable array types that deals with this impedence mismatch, and then relying on the inliner and optimizations to avoid continuously repacking the little imperative universe back up in a little wrapper over and over, and it optimizes it away for the same reason that GHC can optimize away the Int constructors and work with unboxed ints much of the time. https://www.youtube.com/watch?v=I1FuZrDhOEk
You can use reallyUnsafePtrEquality# to compare for reference equality and fall back on the (==) instance if it says no. Doing so is hazardous if your data structure contains any IEEE floating points, however, as NaN /= NaN, so reference equality does not imply (==) even just for types you can find in the Prelude, as (==) is not necessarily structural.
Haskell has perfectly fine imperative constructs, with mutable vectors, `IORef`s, the `ST` monad, etc.
- Java libraries are *not* a killer advantage. PureScript has *really* good FFI to JavaScript, and while it is useful, it's still a pain to write good bindings to external libraries that respect PureScript's immutability assumptions. Java has the same issue. All (or the vast majority of) Java libraries assume pervasive mutability and OOP. Java's type system permits `null` and subclassing, which Haskell's doesn't, and this has to be overcome. Meanwhile, Haskell libraries are already well tuned for the functional paradigm, and are pretty excellent overall. - Haskell does high performance code pretty well. The `IO` and `ST` monads allow you to work with imperative mutable structures if you need to. `repa` and `accelerate` are great for highly parallel numeric computing. [Parallel and Concurrent Programming in Haskell](http://chimera.labs.oreilly.com/books/1230000000929) covers a good bit of these techniques. You may also be assuming that functional=slow and imperative=fast, and indeed, this depends on the runtime. GHC is optimized for functional performance, and the JVM is optimized for OOP and imperative performance (it doesn't even have tail call optimization). - Haskell does not have a global record namespace -- it has a *module* record namespace, and that is being alleviated with `OverloadedRecordFields` soon. In practice, I rarely run into this problem, using either lenses or prefixed record labels. - `String`, `Text`, and `ByteString` fill different roles. That `String` is a linked list is unfortunate, but the problem would almost entirely be solved by *not* exporting the `type String = [Char]` alias. So, I don't find your "game changing advantages" to be terribly compelling. I'm actually a little amused that you, in your first point, complain that Haskell libraries are "one man projects that were abandoned a year ago" and are now asking us why we're *not* using a one man project. Perhaps we're worried that it'll be abandoned! After all, it happens so much to us :P
This isn't new. Back in the 1992-1996, when I worked on the largest Haskell program of the time other than GHC (the NLP system https://en.wikipedia.org/wiki/LOLITA) we had the same problem as its semantic network was a large, mainly unchanging datastructure. I and /u/paulcc used a C++ backend to store the data, and a specially designed crit-bit tree to keep track of the changes made to it. In that way, the GC only needed to traverse actually live data. If I were coding this, I would push the history data onto the C heap, and out of GC space, to keep GC usage low.
I'd rather have crappy Java where I can use some Haskell, then be forced by circumstance to just use only crappy Java.
Thank you for your kind words! I'm afraid there is considerable overlap with Servant's official tutorial, but I did go out of my way to make the series particularly aimed at Haskell beginners (explaining things like the how to work in the `EitherT` monad transformer, what orphan instances are, how to use `IORef`, Pandoc, etc.). And there is a bunch of content concerning what a mock backend is and why you might make one. Glad you enjoyed it!
Haskell can handle infinite data structures, so in one sense, it's *more* effective than C++! /s Generally, the algorithms and approaches that you adopt in C++ and Haskell will be so different that it doesn't make much sense to try and compare them directly. GHC produces pretty efficient code, and well-written Haskell tends to be somewhere between C and Java wrt performance.
You occasionally have to go to extra lengths in order to attain good performance. For example, this is the quicksort code that [Haskell.org spends several pages on as a demonstration](https://wiki.haskell.org/Introduction#Quicksort_in_Haskell) of Haskell's functional beauty: quicksort [] = [] quicksort (p:xs) = (quicksort lesser) ++ [p] ++ (quicksort greater) where lesser = filter (&lt; p) xs greater = filter (&gt;= p) xs While elegant, the problem, of course, is that — aside from not being "true" quicksort, which is inherently imperative, at least as originally defined — this implementation performs badly, and nobody would implement sorting like this, at least if they desired performance. So the [vector package](http://hackage.haskell.org/package/vector-algorithms-0.4/docs/src/Data-Vector-Algorithms-Intro.html#sort) implements sorting with mutable arrays to circumvent this problem, and it's possibly the fastest implementation around. It's able to do this without really "cheating" — it uses the `ST` monad, so it's still pure and safe from perspective of the caller — but it's certainly not simple, and I'm not sure I can call it elegant either, except in the sense that it's able to do this with the power of the tools that Haskell and various libraries gives you. (In case you're interested, [here](http://stackoverflow.com/questions/7717691/why-is-the-minimalist-example-haskell-quicksort-not-a-true-quicksort) is a discussion on Stack Overflow about alternative methods.)
Hint: on the line `filecontent &lt;- liftIO $ readFile fileName`, what does `readLine` refer to?
What is the error you are getting? For those of us not in front of a computer / ghci we need more than just the code. Posts like this don't generally get much traction around here. It also looks like you're probably shadowing readFile from Prelude in your server definition, which could cause some problems. I'm actually unclear as to why you're defining readFile there at all since you simply return it.
Great talk! Thanks for your work in the GUI field.
Read the current source code: http://hackage.haskell.org/package/base-4.8.2.0/docs/src/Data.OldList.html#sort Or the original uploaded source in the GHC ticket (which is the same thing, but perhaps with better context): https://ghc.haskell.org/trac/ghc/attachment/ticket/2143/SortBy.hs Basically, the builtin sort is a variation of merge sort. However, keep in mind that this sorting algorithm assumes that the *only information* we can derive from the elements is pairwise comparison. If we can extract more information from the elements, we can [sort efficiently in linear time](https://hackage.haskell.org/package/discrimination).
Thanks that was all I needed!
&gt; between C and Java wrt performance I would place Haskell and Java in the same ballpark. Both can beat "idiomatic" C if optimized. Haskell has the upper hand on memory usage, but has more variance with regards to performance of idiomatic code.
I think it's generally not a good idea to name variables the same as library functions, e.g. the 'readFile' endpoint function for your API and the Haskell Prelude.readFile this adds extra confusion, and makes it a bit harder for me to reason about the errors without trying to run the code myself. My advice so that debugging is easier would be to first restructure the code like this, along with renaming your 'readFile' function: server5 :: Server IOAPI1 server5 = readFileEndpoint where readFileEndpoint :: Maybe T.Text -&gt; Server IOAPI1 readFileEndpoint Nothing = return $ FileContent "" readFileEndpoint (Just fn) = do filecontent &lt;- liftIO $ readFile fn return (FileContent filecontent) After making those changes, recompile the program and see if you get a more sensible error. I think that calling the library function 'readFile' inside of your user defined 'readFile' may have been the problem. But I have not run this code to check. Goodluck! note: The library function 'readFile' will throw an error if a file with the filename of the argument does not exist. You can fix this with calling 'doesFileExist' from the System.Directory module to check if a file exists before calling 'readFile'.
I don't think I fully understand your question (because, as others said, the question wasn't clear), but I suspect you would find /u/Tekmo's [Foldl library](https://hackage.haskell.org/package/foldl-1.2.0) helpful.
Just glancing at your edit it looks like your Nothing branch returns a String rather than the empty FileContent that you likely meant based off the Just branch and API type.
Hi swingtheory, I recomplied the program: 386| server5 :: Server IOAPI1 387| server5 = readFileEndpoint 388| where 389| readFileEndpoint :: Maybe T.Text -&gt; Server IOAPI1 390| readFileEndpoint Nothing = return $ FileContent "" 391| readFileEndpoint (Just fn) = do 392| filecontent &lt;- liftIO $ readFile fn 393| return (FileContent filecontent) and got the new errors: Error:(387, 11) ghc: Couldn't match type `Maybe T.Text -&gt; transformers- 0.4.2.0:Control.Monad.Trans.Except.ExceptT ServantErr IO FileContent' with `transformers-0.4.2.0:Control.Monad.Trans.Except.ExceptT ServantErr IO FileContent' Expected type: Server IOAPI1 Actual type: Maybe T.Text -&gt; Server IOAPI1 In the expression: readFileEndpoint In an equation for `server5': server5 = readFileEndpoint where readFileEndpoint :: Maybe T.Text -&gt; Server IOAPI1 readFileEndpoint Nothing = return $ FileContent "" readFileEndpoint (Just fn) = do { filecontent &lt;- liftIO $ readFile fn; .... } Error:(390, 33) ghc: Couldn't match type `transformers-0.4.2.0:Control.Monad.Trans.Except.ExceptT ServantErr IO FileContent' with `FileContent' Expected type: Server IOAPI1 Actual type: Maybe T.Text -&gt; FileContent In the expression: return $ FileContent "" In an equation for `readFileEndpoint': readFileEndpoint Nothing = return $ FileContent "" In an equation for `server5': server5 = readFileEndpoint where readFileEndpoint :: Maybe T.Text -&gt; Server IOAPI1 readFileEndpoint Nothing = return $ FileContent "" readFileEndpoint (Just fn) = do { filecontent &lt;- liftIO $ readFile fn; .... } Error:(392, 13) ghc: Couldn't match type `transformers-0.4.2.0:Control.Monad.Trans.Except.ExceptT ServantErr IO FileContent' with `FileContent' Expected type: (Maybe T.Text -&gt; String) -&gt; (String -&gt; Maybe T.Text -&gt; FileContent) -&gt; Server IOAPI1 Actual type: (Maybe T.Text -&gt; String) -&gt; (String -&gt; Maybe T.Text -&gt; FileContent) -&gt; Maybe T.Text -&gt; FileContent In a stmt of a 'do' block: filecontent &lt;- liftIO $ readFile fn In the expression: do { filecontent &lt;- liftIO $ readFile fn; return (FileContent filecontent) } In an equation for `readFileEndpoint': readFileEndpoint (Just fn) = do { filecontent &lt;- liftIO $ readFile fn; return (FileContent filecontent) } Error:(392, 46) ghc: Couldn't match type `T.Text' with `[Char]' Expected type: FilePath Actual type: T.Text In the first argument of `readFile', namely `fn' In the second argument of `($)', namely `readFile fn'
Thanks for the response. 
What does this do?
It is often misinterpreted though. The goal is not to avoid success but avoid focusing on market adoption when considering language design and implementation. A better way to phrase it is: "Avoid, 'Success at any cost'". Meaning, do not prioritize adoption over research goals, features, expressive power or quality. 
Feel free to drop by on Gitter if you have further questions. The plan is to eventually shift the content of that proposal in the GHCVM Wiki so it's more accessible. Have just made the Gist public so that everyone can see it - I didn't realize that many people would be interested in reading it.
I realise this isn't a mac sub but a lot of developers use macs, so I figured there was a chance that a maintainer or someone open to becoming the maintainer might be subbed here. Also, haskell-stack doesn't seem to exist on ports at all.
Yeahh... I prefer a single command installing stack for me by far.
Well, Generic can be instantiated manually in this instance, like by doing: instance Generic (Vec 'Z a) where type Rep (Vec 'Z a) = D1 D1Vec (C1 C1_0Vec U1) from VNil = M1 (M1 U1) to (M1 (M1 U1)) = VNil instance Generic (Vec ('S n) a) where type Rep (Vec ('S n) a) = D1 D1Vec (C1 C1_1Vec (S1 NoSelector (Rec0 a) :*: S1 NoSelector (Rec0 (Vec n a))) from (VCons a as) = M1 (M1 (M1 (K1 a) :*: M1 (K1 as))) to (M1 (M1 (M1 (K1 a) :*: M1 (K1 as)))) = VCons a as Sure, it's not as simple as deriving it automatically, but it still lets you use the generics for deriving other classes.
Your comment about linear in the size of the retained set only applies to copy collectors, correct?
I can't tell if this is sarcastic or not. What you wrote looks more complicated. 
&gt; it has a module record namespace, and that is being alleviated with OverloadedRecordFields soon. In practice, I rarely run into this problem, using either lenses or prefixed record labels. I'm not sure how lenses help here. You still can't have the same field names even with lenses. But yes, once OverloadedRecordFields lands, this issue will be solved too. 
That "works", but only in limited circumstances when you know for sure where the `n` parameter to `Vec` is either a `Z` or `S`. In particular, this means that you can't use `DefaultSignatures`/`DeriveAnyClass` to auto-derive stuff for `Vec`. As an example, you can't auto-derive an instance for `GShow` from the [`generic-deriving`](http://hackage.haskell.org/package/generic-deriving-1.10.4.1/docs/Generics-Deriving-Show.html) library: import Generics.Deriving.Show instance GShow a =&gt; GShow (Vec n a) Since that assumes there's a `Generic (Vec n a)` instance exists. The closest you could get is something like: instance GShow a =&gt; GShow (Vec n a) where gshowsPrec p v@VNil = gshowsPrecdefault p v gshowsPrec p v@(VCons{}) = gshowsPrecdefault p v Also, it doesn't scale well to datatypes with differing constraints, e.g.: data ReadOrShow a where Read :: Read a =&gt; ReadOrShow a Show :: Show a =&gt; ReadOrShow a One could define a `Generic` instance like this: instance (Read a, Show a) =&gt; Generic (ReadOrShow a) where type Rep (ReadOrShow a) = D1 D1ReadOrShow (C1 C1_0ReadOrShow U1 :+: C1 C1_1ReadOrShow U1) from Read = M1 (L1 (M1 U1)) from Show = M1 (R1 (M1 U1)) to (M1 (L1 (M1 U1))) = Read to (M1 (R1 (M1 U1))) = Show But now you require all `Generic` users of `ReadOrShow` to provide both a `Read` and `Show` instance. This is a simple example, but for real-world GADTs, it may be unacceptable to accumulate all of a datatype's constraints at the top like this.
This looks neat! I really hadn't used `ghci` a lot before, but I just gave it a go and finally found stack's `--no-package-hiding` flag that allows me to load files from the test suite! This might just be the beginning of a new era of interactive development for me! :D
Well, stack ghci :mytestname should also work.
I am so surprised that nobody is using Frege on Android. I think the biggest problem is that you need to manually create the bindings to Java. The Android API is very complex, so there are many bindings to write...
Ha, another revelation! :)
http://hackage.haskell.org/package/lens-4.6/docs/Control-Lens-TH.html#v:makeFields 
Also depends on what you want, ghc optimises for throughput more than latency, for example.
Doesn't work for me, I'm in Ireland.
Whenever I need a constant array (e.g., for precomputed lookup tables) I do it manually. It's not too bad boilerplate-wise; but I'd love to see this just built into GHC
Oh wow, i wasn't aware. Still, it requires a user to follow a specific naming condition for it to work. Basically a kludge until a proper fix lands. 
Mac ports has more ports than brew (brew's strategy seems to be let every system get it's own package management system and use that, e.g. ocaml opam). I'm also not remotely comfortable with /usr/local being owned by a user account.
What the heck is up with the `() =&gt; Struct t =&gt; t s` types for the patterns?
Sure, but many of the packages that use c ffi have cross platform support. Certainly it will make the build process more complicated, but it seems worth it. Otherwise it will be much harder to reuse the existing packages
Interesting. Package management has always been a pain no matter the context. The downsides to Brew are a major reason not to use it on a production device. But just on my local development machine, the problems are pretty unimpactful. Luckily, OS X isn't used in production in such a way that Brew would be used in the production environment.
This isn't exactly what you are asking for, but I once made a package for statically generating lookup tables using Template Haskell. It works for any instance of `Storable`. I ended up not using it for the purpose I originally intended, and to my recollection the code it generates wasn't quite as fast as I was looking for, but it may be okay for you depending on what you are doing. http://hackage.haskell.org/package/lookup-tables
Agreed. I use only `makeLenses`(/`makePrisms`) myself. I've been learning `vinyl`. There's some boilerplate in proxies (e.g. `Proxy :: Proxy '("name", String)` and newtypes (e.g. `Compose Maybe Field`). But, you get very clean "first-class records", like: -- (inferred) :: FieldRec ['("x", Int),'("y", Int)] &gt;&gt;&gt; let p2 = x-: 1 :&amp; y-: 2 :&amp; RNil {x: 1, y: 2} -- (inferred) :: FieldRec ['("x", Int), '("y", Int), '("z", Int)] &gt;&gt;&gt; let p3 = x-: 1 :&amp; y-: 2 :&amp; z-: 3 :&amp; RNil {x: 1, y: 2, z: 3.0} &gt;&gt;&gt; type Point3D = ['("x", Int), '("y", Int), '("z", Int)] &gt;&gt;&gt; p3 :: FieldRec Point3D -- yes &gt;&gt;&gt; type Point2D = ['("x", Int), '("y", Int)] &gt;&gt;&gt; :kind! Point2D ⊆ Point3D -- yes, the constraint can be satisfied ... &gt;&gt;&gt; recAdd p2 (rcast p3) {x: 2, y: 4} &gt;&gt;&gt; rreplace (y-: 22 :&amp; x-: 11 :&amp; RNil) p3 {x: 11, y: 22, z: 3.0} where: -- "overloaded" labels, but "monomorphic" x = Proxy :: Proxy '("x", Int) y = Proxy :: Proxy '("y", Int) z = Proxy :: Proxy '("z", Double) -- just because records can be heterogenous (-:) :: proxy '(s,a) -&gt; a -&gt; ElField '(s,a) (-:) _ = Field https://hackage.haskell.org/package/vinyl-0.5.2/docs/Data-Vinyl-Tutorial-Overview.html https://hackage.haskell.org/package/vinyl-0.5.2/docs/Data-Vinyl-Derived.html#t:ElField 
Cool! One question. I wanted these at the type-level (and the package of the identifier is there too) to perform static validation of "exportable" identifiers. e.g. We want to generate a python object from a haskell record: data Point = Point { x :: Int, y :: Int } deriving (Generic, ToPython) class ToPython a where toPython :: (ToPython a) =&gt; proxy a -&gt; String &gt;&gt;&gt; putStrLn $ toPython (Proxy :: Proxy Point) Point = namedtuple('Point', 'x y') We can statically exclude sum types by not providing `(:+:)` instances. And before, we could only dynamically exclude invalid identifiers: if `x` is refactored to `x'` (with an apostrophe), it would still be valid haskell, but not python. However, I still can't write a type-level `IsPythonIdentifier :: Symbol -&gt; Constraint`. How can I compare `Symbol`s char-by-char? I only see: https://hackage.haskell.org/package/base-4.8.2.0/docs/GHC-TypeLits.html#t:CmpSymbol 
This is a good question. I never realized that I didn't have the latest and greatest; I just went with what `stack` gave me. Consequently, the blog doesn't cover the latest version. :(
Well it looks like a 2 liner if you constrain it to two lines. I would say it's four commands vs just one. Putting stack aside, I would much rather recommend the ghc for OS X installer you mentioned rather than your method, it's not very newcomer friendly. Also I've never had to use sudo with homebrew. 
&gt; How can I compare `Symbol`s char-by-char? Sadly, I don't believe such a thing is possible at the moment. `GHC.TypeLits` simply doesn't export a very useful API for manipulating `Symbol`s like one would for `String`s, as evidenced by the [multitude](https://ghc.haskell.org/trac/ghc/ticket/10776) of [open](https://ghc.haskell.org/trac/ghc/ticket/12049) GHC Trac [tickets](https://ghc.haskell.org/trac/ghc/ticket/11342) about the issue.
You have to tell the pattern what is provides vs. what it requires. I think the order of those two was swapped in 8.0 after I wrote this. 
Yeah, I was recently looking into Servant as well and got a bit confused since I had to use an older version of the documentation (seems it got updated and moved after 0.4). For all intents and purposes, I think to get the best reach, using the one that is on stackage is probably best, so I don't think its a problem you're using 0.4.x :) 
The direct IP approach did the trick for me (after clicking through the TLS warning).
Please consider a screen cap for the next lecture. Anyway, great talk!
Thanks! And also thanks for keeping it compatible with base 4.6/GHC 7.6! However, I'm somewhat worried [about the compile times](https://travis-ci.org/mrkkrp/megaparsec). How come GHC 7.10.X takes 600%-700% of GHC 7.6's time? I don't think that this is megaparsec's fault, but still, the time seems rather high for a (3+2) kLOC application.
&gt; I expect some people may not the intensive use of NonEmpty I like the intensive use of NonEmpty. When I ported some `[]`s to `NonEmpty`s, I generalized `(++)` to `(&lt;&gt;)` and `map` to `fmap`, and the only code that didn't type check was the code that shouldn't, and the only code that crashed was from the partial `IsList` instance (via `OverloadedLists`, which is fine). If some people don't like `NonEmpty`, it's pretty easy for them to use `toList`. 
I'm worried about this too. I honestly don't know why it takes so long on Travis (GHC 7.6 seems to be much faster than later versions).
[removed]
&gt; Cheap isn't free; if your memory usage is proportional to your dataset, and your dataset is growing, eventually the cost of getting more memory will bite you, but how much it bites you depends on how slowly your memory usage grows. No one is saying it's free.If time is money, your time to solve a problem is usually more expensive that just buying more memory or moving the problem to a cluster. Your other points may or may not apply to OP's posting.
Right now, we don't require a space between operators and non-operator identifiers, so the lensy `foo^.bar.baz.quux` is valid syntax as well as `five+three`. Implementing a prefix that's an operator character would be an annoying issue to resolve, potentially breaking a lot of old code. Then we're left with alphanumeric prefixes, which you can already do now just by defining your record fields whatever you want them to be. On the awkwardness of the naming, I feel that too. I usually "solve" it by using `RecordWildcards`: tax :: Product -&gt; Double tax Product{..} = price * 0.07 which to me reads and looks very naturally. You could also have `of = ($)` and write price `of` product but that might be too DSL-y for your tastes :)
This a good chunk of it, at least. Keep in mind that the OCaml implementation is not permissively licensed, so you can't look at it if you're making something that is. http://sat-solver.readthedocs.io/en/latest/references.html I also suggest checking out the composer project from the php community. It is readable, with effort, and gets good results.
I watched this video on /r/programming and explains that lazy functional programming is used in compiler to get realtime AST recompilation.
If you want operate on large data structures _correctly_ in *parallel* or *concurrently* I think Haskell is far more efficient.
Really liked this video. The sudden realization that landed on me when Anders described the architecture was that a compiler could basically be an FRP system that describes a program as a graph of modules, each module a behaviour, that is sampled when needed. I like that idea!
Great talk ! Thanks !
Lens (in this case) and RecordWildCards are only workarounds to the issue, which make Haskell a complex language. What happens when someone wants to do something like this? foo :: Product -&gt; Product -&gt; Double foo Product{..} Product{..} = price + price * 0.07 I think this would work better and is a lot clearer: foo :: Product -&gt; Product -&gt; Double foo prod1 prod2 = prod1.price + prod2.price * 0.07 This is clear too: foo :: Product -&gt; Product -&gt; Double foo prod1 prod2 = .price prod1 + .price prod2 * 0.07 I know that operators do not require the space (even if I do not write code like that), but maybe an exception could be made. Breaking compat is bad, but ruining the language experience for future users (and current users too) is worse.
Yes. FAT was stupid in lots of ways: it wrote blocks immediately without cache, it used first-fit allocation of free sectors, and it didn't operate with any sort of IO scheduler. All of these things together combined to make files fragment frequently and reads of fragmented files more painful. Current OS and filesystems usually will cache blocks, either individually or as a group, will usually leave space between files to allow for changes, and will attempt to allocate free sectors as close as possible to the other sectors in the same file. They also use effective IO schedulers which can interleave reads/writes as the disk spindle turns. The difference between GC and malloc memory management is that malloced blocks stay fixed in position. The heap is a virtual contiguous space, so doing a lot of allocations and releases of different spaces will leave a chain of different sized holes, and it's possible that future mallocs could be satisfied by the total free space, but not by any single available hole. A copying GC on the other hand compacts used space every time it collects. But this is in _virtual_ space. The virtual space available to a program is the size of addressable memory, which is typically much larger than physical memory. Unused pages of virtual space simply don't exist anywhere, so having a large number of holes in your program's memory virtual footprint doesn't alter its real memory footprint. The worst that might happen is the performance of malloc might decrease logarithmically in proportion to fragmentation.
Isn't his foldr actually foldl?
Thanks for the awesome programming. Have fun with whatever these other projects are!
Can somewhat explain what line-folds are?
than C++? why?
Yes I know about that. I can tell you which modlue takes 95% of compilation time. It's `Prim.hs` from test suite.
I saw my blog mentioned a few times. I also have series of videos from my Haskell class. There is a lecture on monads there too. https://www.youtube.com/playlist?list=PLbgaMIhjbmEm_51-HWv9BQUXcmHYtl4sw .
I've not used megaparsec before, but when it prints out the input string and says error at line X column Y, does it point to the spot like clang does? 
That is an interesting question. Haskell inherited much of its syntax from [Miranda](https://en.wikipedia.org/wiki/Miranda_(programming_language\)), including dot (`.`) for function composition. But Miranda used `$` for turning functions into infix operators, like Haskell's back-tick notation.
I don't know but dollar sign is used in shell scripts to reference arguments. http://linuxsig.org/files/bash_scripting.html
What other character would you use? I suspect this was mostly an arbitrary choice, given that the operator has little precedence (heh) in other languages.
PureScript's parser was written before megaparsec was out. Also, there's an issue to replace parsec with megaparsec :)
There is a big difference between a language like Scala, whose whole purpose is to make design compromises in the language itself for the sake of Java integration, and making Java libraries available within genuine Haskell.
For one, because `$` resembles wealth, but also because no work gets done unless you throw money at it. Hence you need money to give an incentive to a function to apply itself to its arguments!
It reduces the cost of using it and providing an instance since you don’t need to depend on transformers to use it , see https://ghc.haskell.org/trac/ghc/ticket/10773
Well, its an approximation, the real composition symbol looks like: "∘".
/r/shittyAskHaskell ;) (In the gist of /r/shittyAskScience)
Yes, sort of. The actual function composition symbol (Unicode RING OPERATOR) was not an option when Miranda was invented in 1983. Then Haskell inherited it from Miranda.
&gt; GHC 7.6 seems to be much faster than later versions https://downloads.haskell.org/~ghc/7.8.1/docs/html/users_guide/release-7-8-1.html &gt; The new code generator, after significant work by many individuals over the past several years, is now enabled by default. This is a complete rewrite of the STG to Cmm transformation. In general, your programs may get slightly faster. 
there is e.g. `http://hackage.haskell.org/package/universe` with `Universe` and `Finite`. `(Bounded a, Enum a)` may not be `Finite`, there aren't laws saying so. Also`[minBound..maxBound]` don't need to list /all/ values.
universe doesn't fit for the same reason [\[\] ist not the Free Monoid](http://comonad.com/reader/2015/free-monoids-in-haskell/). `foldType` doesn't imply finity. If `[minBound..maxBound]`doesn't list all values, that's a missing law :P.
It's the other way. :) &lt;$&gt; is the more recent operator, and was likely named after the existing $.
Using Unicode characters not on any keyboard for syntax would still be painful today.
Hopefully one day GHC will catch up to the commercial state of the art in this area.
It just about works for Agda, but that's a specialized use case
I'm still puzzled in how they program Agda in anything but Emacs.
In a similar vein, why aren't there Num instances for all the applicatives? They certainly would make thing easier in a lot of contexts.
&lt;| and |&gt; are, IMO, vastly superior, as they are mnemonics. And unlike &gt;&gt;&gt; and &lt;&lt;&lt;, they aren't too long compared to the present equivalent.
It would allow some correct code to be written more succinctly, but it would also allow some incorrect code to compile where currently it doesn't (consider `f -1`).
I don't know of anyone who doesn't. 
Come and have a look at the Haskell Operator Emporium! We have many kinds to cover all your function application needs! * The dollar, for traditional Haskellers: ($) :: (a -&gt; b) -&gt; a -&gt; b * Ditch the `(&amp;)`! Apply a function backwards now with the fancier: (₴) :: a -&gt; (a -&gt; b) -&gt; b * A cheaper but functionality-limited version for Haskellers who can't afford a full dollar: (¢) :: (a -&gt; a) -&gt; a -&gt; a * Apply a function, in very sophisticated ways: (£) ∷ (𝕒 → 𝕓) → 𝕒 → 𝕓 * We also have the Euro, Japanese Yen and Korean Won models! (€) :: (a -&gt; b) -&gt; a -&gt; b (¥) :: (a -&gt; b) -&gt; a -&gt; b (₩) :: (a -&gt; b) -&gt; a -&gt; b * **(NEW!)** Apply a function with bitcoins! (the first argument specifics the number of confirmations to wait before applying): (Ƀ) :: Int -&gt; (a -&gt; b) -&gt; a -&gt; b
They ARE all instances of Foldable.
You need to import Data.Foldable
Which also looks way better. I had my editor set up to render `.` like `∘` for a while, and it really made chains of composed functions easier to read because `∘` did a better job of visually grouping things together. I'm not sure exactly why this was the case, but it was probably because it was vertically centered unlike the normal `.` operator.
https://hackage.haskell.org/package/uniplate-1.6.12/docs/Data-Generics-Uniplate.html#g:4
I would recommend `mono-traversable` package for anyone who'd like to avoid a bunch of qualified imports to operate on lots of different data structures.
These libraries are a lot older than the Foldable typeclass (or at least, its widespread use and adoption into base). The special `foldl`s came first. They're probably still in there just for backward compatibility. The move to Foldable is relatively recent.
I would guess because it is an easy to type symbol that was not already used in Haskell.
Right. You need to hide foldl in your other imports as well. I prefer Ocaml's way of doing this, which is to implicitly shadow with the most recently imported binding.
That's unfortunate. Haskell code has a tendency to scale horizontally rather than vertically (like most languages), which makes code less readable, if the module name is in front of all functions.
The point of this post was to avoid having to write `Vector` in front of `foldl` because for me it is unambiguously clear what `foldl (+) 0 vector` means. Where `vector` is a vector of course.
[removed]
Until the real `base-4.9` docs are uploaded, you can view a candidate version [here](http://hackage.haskell.org/package/base-4.9.0.0/candidate/docs/Control-Monad-IO-Class.html).
The idea of putting `MonadBase` into `base` was proposed, but was rejected. Edward Kmett lists his reasons for not putting `MonadBase` into `base` [here](https://mail.haskell.org/pipermail/libraries/2015-July/026061.html).
There is a good write-up about [how to import modules properly](https://wiki.haskell.org/Import_modules_properly) it explains why you'd really want to write import Data.Foldable (foldl) import qualified Data.Vector as V I have to admit, I arrived into the same conclusions /hard way/, using the practice from beginning would saved from unnecessary work.
While it's true that `MonadIO` is often used in conjunction with monad transformers, you don't need to directly use `transformers` to write useful code with `MonadIO` constraints. That was [the motivation](https://mail.haskell.org/pipermail/libraries/2015-July/026008.html) for including `MonadIO` into `base`—eventually, we will want to put generic IO operations into `base`, and `MonadIO` provides a nice, Haskell 98 way to accomplish this. In addition, the `Q` datatype in `template-haskell` admits a useful `MonadIO` instance, and unless `MonadIO` lives in `base`, it's awkward to get the dependencies right for GHC dependencies like `template-haskell` and `transformers`.
&gt; you don't need to directly use `transformers` to write useful code with `MonadIO` constraints. I hadn't thought about it that way. That's very good. Being able to use the `IO` operations in `base` without calling `liftIO` would be really cool. As an aside, is there an alternate prelude that offers pre-lifted `IO` ops? 
[`lifted-base`](http://hackage.haskell.org/package/lifted-base) covers a good deal (but not all) of the `IO` operations in `base`.
Yep. I avoided it by not giving them signatures, but then that just became a warning, so uh.. I have a lot of work to do for 8.0.
Am I understand correctly that it's MPTCs and fundeps that don't belong in base, due to not yet being part of the standard? What about implicit params and overloaded labels? class IP (x :: Symbol) a | x -&gt; a where ip :: a class IsLabel (x :: Symbol) a where fromLabel :: Proxy# x -&gt; a
I answered your question :-)
Very cool! GHCJS is such an awesome project. Does anyone know if anything came of the comments at the end about supporting future development? If other people have plans in this direction, I would be interested in joining in.
Does anyone know of resources on designing this kind of compiler? Every book I've looked at falls into the dragon book category.
Hilariously, we should then use |&gt; as the normal function application. that would be great.
It isn't so much about `base`. `base` has lots of things that will probably never go into a Haskell Report. On the other hand, `MonadIO` is the sort of thing that you could envision a large part of the Haskell Report being lifted into. Whether we ever do this or not is a subject for discussion and debate, but it is a thing we could do within the confines of the language we have already standardized. On the other hand, `IsLabel` and `IP` there aren't likely to become part of a language standard any time in this decade. Heck, for that matter, nobody can tell me how you're supposed to compose labels of that sort. `foo.bar` seems entirely beyond the scope of what that sort of thing can do, making them a rather sharp step backwards from what we can express today using certain mainstream libraries.
ho-rewriting and compdata packages also provide rewriting frameworks, but can be complex
I popped in here to mention precisely this. The `instance MonadIO Q` question was what started this transition in the first place.
Too bad April 1st already came and went.
Part of this is historical. Until 0.8 or so I couldn't get Roman to supply even standard instances for things like `Monad` and the like. He was concerned about -any- potential source of overhead. I wrote a `vector-instances` package, just for my own sanity, and those eventually got merged back into `vector`. More recently, we started adding similar instances for working with the `Array` types in `primitive`. The `foldl` exports in the modules date from this time, but there is another concern at hand. `Data.Vector`'s `Vector` can be an instance of `Foldable`, and `Traversable`, but `Data.Vector.Storable` isn't eligible for the same treatment. So the question then becomes do we try to provide a consistent API between `Data.Vector` and the various `Data.Vector.Storable`, `Data.Vector.Primitive`, `Data.Vector.Unboxed` modules? Do we export genericized versions like we did with `Data.List`? Do we leave the API untouched? At the moment, since each of the modules is already used qualified, we left it untouched.
How many non javascript Haskell-to-Foo compilers are you including in your sample set? JavaScript is a very different case because you have no hope of calling C code, except perhaps from node. However, the primary platform we care about does not have FFI. GHCJS has a clever and useful hack which essentially says "I'm going to treat C FFI as if it's a javascript call". Then, if you're lucky, you can shim out that C function in javascript land in order to get the library working. GHCJS also has its own special FFI for invoking javascript functions, as an extension of the existing FFI syntax. &gt; It would be a shame to have a GHC-to-JVM compiler which does not provide full access to the wide world of Java libraries, and FFI is the natural way to do that. Why use up the FFI for JNI? I really hope that does not happen. I agree, it should have mechanisms for marshalling with Java and convenient mechanisms for invoking it. I strongly disagree with the idea that C FFI should be re-used for this. Inevitably the rules will need to be changed for Java regarding which types can be used in the FFI invocation, and the valid syntax for functions in the language being invoked. At this point, we cannot re-use C FFI syntax. I hope that we don't ditch C support out of some sort of urge to get syntactic re-use of our C FFI decls...
This sounds really exciting! I hope you find some good engineers.
`stack purgatory` is actually how I refer to stack problems, but that's mostly to be silly about it. 
For what it's worth, making your code point-free should never be a goal when writing Haskell. Writing readable code is usually the thing you should be strive for. Sometimes point-free code happens to be the more readable style, and sometimes it's not. Sometimes code that involves the letter 's' is more readable, sometimes it's not. Point-freeness and readability are rarely correlated. You should never "try to make things point-free"...just like you should never try to shoe-horn the usage of binary search trees, for example, into problems where they make no sense. Just putting this out there in case there are some misconceptions about the utility or purpose of point-free code.
You can use [this tool](http://pointfree.info/#input=i%20n%20%3D%20liftM%20%28%28%2C%29%20n%29%20.%20show%20.%20truncate%20%24%20n) to make a function point-free. Sometimes the result is readable. This time not so much.
What are the differences in semantics? I've only read about the strictness of the value.
[removed]
Perhaps what /u/twistier means is that, e.g. in this case: newtype Money = Money { getMoney :: Int } `Money` is an `Int` at run time but doesn't type check as an `Int`. Also, it allows you to create a new type-class instance, e.g.: instance Num Money where ... with your own custom implementation.
Not to mention (&gt;&gt;) and (&lt;&lt;) for composition. Elm and OCaml as well. Much clearer in my opinion!
Here's a start: Company|Language|Website|Sources :--|:--|:--|:-- JP Morgan |Haskell |https://www.jpmorgan.com/pages/jpmorgan |https://www.reddit.com/r/haskell/comments/3re0kp/jpmorgan_haskell_team_is_hiring/ Standard Chartered |Haskell |https://www.sc.com/en/ |https://twitter.com/donsbot/status/654630194519646208 Kapersky |Haskell |http://www.kaspersky.ru/ |https://www.reddit.com/r/haskell/comments/3phy61/haskell_job_in_moscow_kaspersky/ pusher |Haskell |https://pusher.com/ |https://news.ycombinator.com/item?id=10152809 https://www.reddit.com/r/haskell/comments/3mpip2/hask_anything_the_thread_where_you_ask_and_we_try/cvh270e Swift Navigation |Haskell |https://www.swiftnav.com/ |https://news.ycombinator.com/item?id=10152809 Front Row Education |Haskell |http://frontrow.workable.com/jobs/70963 |https://news.ycombinator.com/item?id=10152809 Linkqlo |Haskell |https://itunes.apple.com/us/app/linkqlo-style-fit-for-every/id957167717?mt=8 |http://functionaljobs.com/jobs/8859-full-stack-haskell-software-engineer-at-linkqlo-inc Wagon |Haskell |https://www.wagonhq.com/ |http://functionaljobs.com/jobs/8857-haskell-engineer-at-wagon Galois |Haskell |https://galois.com/ |http://functionaljobs.com/jobs/8856-software-engineer-researcher-at-galois-inc Silk (formerly TypLab) |Haskell |https://www.silk.co/ |http://industry.haskell.org/partners http://www.quora.com/What-startups-use-Haskell-for-production-work Better |Haskell |https://better.com/ |http://industry.haskell.org/partners Scrive |Haskell |http://scrive.com/en/ |http://www.quora.com/What-startups-use-Haskell-for-production-work Swap.com (formerly netcycler) |Haskell |https://www.swap.com/ |http://www.quora.com/What-startups-use-Haskell-for-production-work Janrain |Haskell |http://janrain.com/ |http://www.quora.com/What-startups-use-Haskell-for-production-work CircuitHub |Haskell, Elm |https://circuithub.com/ |http://www.quora.com/What-startups-use-Haskell-for-production-work Bump |Haskell |http://bu.mp/ |http://www.quora.com/What-startups-use-Haskell-for-production-work Mailrank |Haskell |http://www.mailrank.com/ |http://www.quora.com/What-startups-use-Haskell-for-production-work Chordify |Haskell |http://chordify.net/ |https://wiki.haskell.org/Haskell_in_industry#Haskell_in_Industry Fynder |Haskell |https://fynder.io/ |https://wiki.haskell.org/Haskell_in_industry#Haskell_in_Industry Glyde |Haskell |http://glyde.com/ |https://wiki.haskell.org/Haskell_in_industry#Haskell_in_Industry Hasura |Haskell |http://hasura.io/ |https://wiki.haskell.org/Haskell_in_industry#Haskell_in_Industry Hustler Turf Equipment |Haskell |http://www.hustlerturf.com/ |https://wiki.haskell.org/Haskell_in_industry#Haskell_in_Industry IMVU |Haskell |http://www.imvu.com/ |https://wiki.haskell.org/Haskell_in_industry#Haskell_in_Industry Soostone |Haskell |http://www.soostone.com/ |https://wiki.haskell.org/Haskell_in_industry#Haskell_in_Industry SumAll |Haskell |https://sumall.com/ |https://wiki.haskell.org/Haskell_in_industry#Haskell_in_Industry Prezi |Elm |https://prezi.com/ |http://elm-lang.org/blog/announce/elm-and-prezi Skedge.me |Haskell |http://www.skedge.me/ |https://www.reddit.com/r/haskell/comments/3ow8dr/skedgeme_is_hiring_remote_haskellers/ NoRedInk |Elm |https://www.noredink.com |https://twitter.com/rtfeldman/status/656238188961226752 GrassWire |Haskell |https://grasswire.com/ |https://www.reddit.com/r/haskell/comments/3s0ma7/fulltime_haskell_job_at_grasswire_vcbacked_startup/ 
Even though your solution is a lot similar from the one the tool had shown, I'm finding yours easier to grasp. P.S.: Loved the bonus. Can't understand anything, but it's nice.
Some people tend to use ``newtype`` to define data types then derive complicated class for the newtype (such as Functor, Applicative, Monad, MonadReader, MonadState, ...) . it is possible to do the same with ``data`` if ``data`` and ``newtype`` are interchangeable, however, in this case, ``newtype`` won't introduce any runtime performance penalty.
Tripshot (www.tripshot.com) is using Haskell for the backend. We're hiring two Haskell positions right now (remote okay): http://www.tripshot.com/#!blank/bil63 
It's hard to know how "heavily" a company uses Haskell unless one works for it. Clearly some folks (Simon Marlow &amp; crew) have used Haskell at Facebook, for something quite critical to the company. However (unless I've been living in a cave and missed the news) I don't think one would argue the whole company uses it heavily. That caveat aside, here are some companies known to have used Haskell for something: Microsoft Research which employs Simon Peyton Jones of GHC fame is part of a company. BlueSpec's hardware description language compiler was written by Lennart Augustsson in Haskell. FP Complete Well Typed. Companies that use Snap according to wikipedia: Racemetric (dead?), SooStone Inc (haskell job advertised), and Group Commerce Mailrank https://www.youtube.com/watch?v=ZR3Jirqk6W8 (since bought by Facebook) Erik Meijer's company applied-duality might. Also my tiny company ansemond.com has delivered one project in Haskell for a customer, although the bulk of our work has been in assembly / C / C++ / Objective-C / Python as desired by the client. I also implemented some things in Haskell for AMD, NSM and Cyrix as well as the afore-mentioned languages.
My company [LeapYear Technologies](http://leapyear.io) uses Haskell almost exclusively.
Here's a more general version of what you're asking: f x = (foo x . bar) x = foo x (bar x) (where, in your example, `foo = liftM . (,)` and `bar = show . truncate`). This is a pretty straightforward application of the S combinator `s x y z = x z (y z)` (a more general version of which is called `ap` in Haskell), which is super handy for making less line-noisy pointfree functions if you want one! So you can reduce this to just `f = ap foo bar`, or again in your case, `i = ap (liftM . (,)) (show . truncate)`. * * * ETA: Also, `ap` is also the same as `&lt;*&gt;` but restricted to monads instead of applicatives, in the same way that `liftM` is just `fmap` but restricted to monads instead of functors. It doesn't help with the line noise as much spelt that way, though!
I believe OP's point is that there's no reason a `data` declaration can't be optimized the same way when it has only one constructor, which would render `newtype` pointless.
This probably isn't addressing your specific question, but what made me 'get' `newtype` was understanding that it defines an isomorphism of types. Before I understood this working with things like monad transformers were completely foreign and bizarre but now they are (relatively) easy to reason about.
I work at Ambiata, we make extensive use of Haskell for everything imaginable (compilers, web, infrastructure, tooling, databases, automation ...) We've also got a smaller amount of Scala code that's written in a very FP style.
Hack is written in OCaml, and the question was FP shops (with an example of Jane Street, which is a heavy OCaml user).
&gt; coming from Ruby, two itches that Haskell still hasn't been able to scratch are: &gt; (a) Efficiency, especially memory-wise, and (b) Falling short of it's safety/purity promise This makes no sense to me
I was having trouble with the sentence structure (which, by all means, could just be my misunderstanding) but it seemed to imply that Ruby has these two things that Haskell doesn't.
It might help if you used an ADT to represent all possible commands (and possibly a parsing phase if those strings are coming from user input). When you do this you won't need a catch-all pattern at the end and can't forget to update this after adding a case. Using a string leverges the type system less and requires a catch-all pattern, so it's not as nice.
&gt; Therefore, I'm planning to look at Rust, which at a cursory glance, seems to have picked a lot of ideas from Haskell. Is this correct? Not really, Rust comes more directly from the ML line and [the original compiler was in OCaml](https://github.com/rust-lang/rust/tree/ef75860a0a72f79f97216f8aaa5b388d98da6480/src/boot), so most of the ideas which seem Haskell-inspired were more properly sourced in MLs (SML, OCaml), hence e.g. mandatory exhaustive matching of ADT, strictness, and lack of purity. AFAIK and according to [the influences section](http://doc.rust-lang.org/reference.html#appendix-influences), the Haskell inspiration is mostly in traits which are strongly inspired by typeclasses (though somewhat more limited at this point as Rust doesn't support HKT, or GHC's more esoteric features). 
It's a shame Barclays often never gets mentioned. It's been using Haskell in production since 2007 to support revenues that likely far exceed most of the companies on the above list. It currently has a team of 7 people writing Haskell full-time, with many others contributing.
Haskell and Rust both have: * Type/memory-safety * No nulls * Immutability by default * Expression-oriented syntax (`if`-`else` returns a value etc.) * Higher-order functions * Nominal types * Algebraic datatypes (sum types, tagged unions, "enums with data in em", ...) * Pattern matching * Generic functions and types (parametric polymorphism) * Type classes (traits) * Local type inference Only Haskell has: * Purity * Laziness * Automatic / implicit allocation * Higher-kinded types * Global type inference * Polymorphic recursion * Tail call elimination Only Rust has: * Strictness * Pervasively unboxed data * Built-in procedural control flow * Affine types ("move semantics") * Lifetimes (region types, borrowing) * Automatic destructors * Elimination of data races * A ban on orphan instances * Nested and recursive modules Glasgow Haskell and Rust both have: * Associated types * Macros (called Template Haskell, in Haskell's case) Only Glasgow Haskell has: * Higher-rank types * Kind-polymorphism, constraint kinds, GADTs, existential types, DataKinds, ... * The kitchen sink
Wow -- that's a pretty good list. Some questions: * Rust doesn't have tail-call elimination? How does it handle recursion then? * What's polymorphic recursion? * What are orphan instances? And how does Rust ban them? * What are affine types? * Doesn't Haskell optimize away boxed data? What happens to stuff like "data URL = String url"? In the compiled binary is it represented as a String or as something wrapping a String?
I interpret it as those were two things he was hoping to achieve by switching away from ruby, but was not able to. 
&gt; Macros (called Template Haskell, in Haskell's case) I don't know how flexible Template Haskell is, but Rust macros are relatively weak, I'm guessing an extensive match to TH would combine Rust macros and [syntax extensions](https://doc.rust-lang.org/book/compiler-plugins.html).
That's what I also think and do. However newtypes can be derived automatically with some extensions and it's easier to write `newtype` if you can and then modify it to `data` if needed than the opposite. I aggre that the compiler should be able to do it (apart from the "strictness" bit that I haven't really understood.
If it doesn't type check as an `Int` it is sematically not an `Int`. Your point that `Money` *is* an `Int` only stand for an optimisation point of view. 
Although this seems not to work.
There are observable differences in laziness between using data and newtype (data has a thunk to the inner type, newtype is literally that run time representation), which if you wanted to keep would not allow you to do the optimisation. 
This is actually distinct from higher-rank types and [even GHC doesn't have it](https://ghc.haskell.org/trac/ghc/ticket/2893). Rust only has it, and only *needs* it, for lifetimes, though, so I think of it as being subsumed under the "Lifetimes" bullet point. Haskell doesn't have to deal with lifetimes so it has no reason to miss higher-ranked lifetimes in constraints either. (Similarly, I'm pretty sure Rust does have polymorphic recursion *for lifetimes*.)
Some smaller shops: Conversant, College Vine, Karius, Sodality, Sentenai
Also, Haskell has STM and a very fast IO manager. Rust's concurrency is much more low level
Rust is not really a high-level language, though. I'm not sure a comparison would be fair. I love rust, for systems and other low-level stuff, but Haskell remains my go-to for quickly banging out apps
A data with one constructor that takes on *strict* argument could be optimised in this wauy (I think). Otherwise the "optimisations" would add strictness where the programmer didn't expect it.
Won't a data with a single, strict field behave the same?
Just a quick note that Rust doesn't have *guaranteed* tail-call elimination. It might seem like I'm splitting hairs, but it's important to realize that TCE is just and optimization that Rust can apply. It's just that, like most languages, it's not baked into the language. Normally only functional languages need TCE since they often don't have native loop constructs and so require recursion for list operations. 
No. If you try to extract a strict Int field from a type defined with data, you must evaluate it. 
I guess you could also make a trampoline if you really need recursion and want to guarantee you won't blow the stack?
How would haskell handle `foldl' (+) 0 [1..10000000]` without TCO?
I tried to get up and running with this because it is very close to my ideal side-project; I got stuck at the point where I tried to run Xen inside a VM, as recommended by the docs. If anyone has a reliable recipe for this I'd love to hear about it.
Embarrassing confession: I'm not really sure what a trampoline looks like in practice... [There is this.](https://crates.io/crates/stacker)
The docs and error messages are much better now. Id say the borrow checker is easier than learning monads (not specific common ones like maybe, either etc but the concept itself). At worst I'd say the borrow checker is on the same order as monads. 
&gt; I guess my takeaway is to not bother with newtype unless there is a very specific reason to use it instead of data. Odd, I'd assume you'd break the other way on that. `newtype` is more efficient than `data` containing a single strict member. I usea `newtype` whenever it makes sense, except when I really need an extra bottom to be included in the type. You get to do less memory allocation, get more efficient code and get access to GeneralizedNewtypeDeriving.
Building a new team using Haskell to tackle a number of projects related to DSLs, supply chain, machine learning. Actively hiring. 
on site or with remote people?
As others have said, it's about the readability. I think you get a feel for this when programming haskell. Some people use it for golfing or showing off, but for me making a function point free is about boiling down to the purest form of what is actually being done as well as making it potentially more general. For example your h function. We notice that `b` is at the end of the signature, as well as the function definition. So we could drop it from both positions: h :: Double -&gt; Char -&gt; (Double, Char) h a = (,) a Now `a` has moved to the last position so we can do the same again: h :: Double -&gt; Char -&gt; (Double, Char) h = (,) And the final step, in this case, is to decide: can h be more general (e.g. could it be `a -&gt; a -&gt; (a, a)`)? and if so, do we need h at all? In your above case, you didn't it need as `k` could be defined as one of the following: k = (,) n k x = (n,x)
&gt; it's horrible enough so people won't use it unless they really have too TH is actually very nice, and very powerful. Many excellent libraries use TH to great effect, and not because "they really have to". Of course, TH does have trade-offs, like anything else, so it's not always the right tool. One of the drawbacks is that there is a small learning curve; you do need to spend a bit of time looking at the documentation before you can read or write TH code. Unfortunately, some people reject TH without considering the trade-offs because someone told them that it is "horrible" and they never bothered to look at the documentation.
This doesn't compile for a number of reasons. So it's hard to see what your problem was and why `-Wall` did not solve it. Please post something a bit closer to your actual code.
Though I guess you don't simply want to rewrite part of a tree; what's the bigger picture, what problem are you tackling?
Thanks. But I had to [post a follow-up question](http://stackoverflow.com/q/37279101/477476) because I still can't get the hang of it for parsing more tree-ish things.
Also, the "become" keyword is reserved for a future implementation of guaranteed tail call elimination, but AFAIK the implementation hasn't been started yet.
I think that strictness is enough. No special TCO is required, since Haskell does not use stack, but just builds linked lists representing stack. 
There seems to be some interesting links in the readme at https://github.com/dotnet/roslyn 
So what are people referring to when they talk about TCO in Haskell?
My two biggest annoyances with TH are: * Spliced expressions need to be imported (not locally defined) * [Declaration order matters now](https://stackoverflow.com/questions/10857030/whats-so-bad-about-template-haskell#comment14143273_10857227)... or at least that's what I want to say, but I couldn't get the behavior to trigger in 5 minutes of trying. Is this still the case?
[Polymorphic recursion](http://twanvl.nl/blog/haskell/non-regular1) (at both value and *type* level.)
you are absolutely right ... it is a fold (kind of by definition). ```each ``` works fine here. Although the definition as a ```Getter``` works, it would maybe break some laws at another point? Having a ```Fold``` feels (sounds) much more right.
Right, my point is that you shouldn't need `minBound` or `maxBound`- anything nonsmelly you use it for would be able to use foldType at some higher level instead.
I would be remiss to not point out that anywhere you're working with generic types, you're working with higher kinded types to some extent. And you can encode "real" HKT using generic traits and associated types. It just super sucks eggs and is blatantly a hack. Especially when compared to Haskell.
Not sure why everyone is making it more complicated than it should be: i :: RealFrac a =&gt; a -&gt; [(a,Char)] i = fmap . (,) &lt;*&gt; show . truncate Explanation: h == (,) liftM == fmap (\n -&gt; let k = h n in liftM k) == fmap . h (\n -&gt; (fmap . h) n $ (show . truncate) n) == fmap . h &lt;*&gt; show . truncate Demo: ghci&gt; (putStr . show . i) 1234.0 [(1234.0,'1') , (1234.0,'2') , (1234.0,'3') , (1234.0,'4')] Note: some people have mentioned the fact that `(.) == fmap` for functions. I would use this identity very sparingly (if at all), because it tends to obfuscate your code.
("Higher-kinded types" is colloquially used to mean (equivalently) *abstraction over [...]*, *variables of [...]*, *first-class [...]*, ...)
Stack not working? Impossibru!
What, and needing to call an identity function to force the type-checker to unify types isn't first-class enough for you?! :P
 import Data.Generics.Uniplate.Operations data Expr a = Oper a | Mult [ Expr a ] | Plus [ Expr a ] deriving (Show, Eq) rep = transform f where f (Mult [Oper 'b',Oper 'a']) = Plus[Mult[Oper 'a', Oper 'b'], Oper '1'] f x = x Parse error on input f x = x, I cant find many examples on this. 
Well currently simply rewriting part of a tree is turning out to be a challenge, if you could give some pointers. 
The second point is soooo annoying.
`Getter` and `Traversal` are both `Lenslike`, differing only in the constraints that they impose on the underlying functor: Lens' a b = forall f. Functor f =&gt; Lenslike' f a b Getter a b = forall f. (Contravariant f, Functor f) =&gt; Lenslike' f a b Traversal' a b = forall f. (Applicative f, Functor f) =&gt; Lenslike' f a b So, `teamAges` can be defined as follows: teamAges :: (Applicative f, Contravariant f) =&gt; Lenslike' f Team Age teamAges = teamMembers . traverse . memberAge If you generalize `teamMembers` and `memberAge` to `Lens'` instead of `Getter`, then `teamAges` will become a simple `Traversal'`: teamMembers :: Lens' Team [Member] memberAge :: Lens' Member Age teamAges :: Traversal' Team Age teamAges = teamMembers . traverse . memberAge Regardless of whether you generalize, you can view the team's ages as a list: (^.. teamMembers . traverse . memberAge) :: Team -&gt; [Age]
Yes, denotationally.
Tbh after thinking about it, I don't think the idea that "Rust has higher-kinded types because `Vec` and `Option` are types of higher kind" is accurate even in a lawyerly hair-splitting sense. In Rust things like `Vec` and `Option` *don't even have a kind*. Just like in a language without higher-order functions (not even function pointers like C), functions don't even have a type. Just because we can imagine what it *would* be in a more powerful system, doesn't mean that it *is*.
Some companies verified as using Clojure: http://clojure.org/community/companies
Yes ok, I get a parse error on input 'case' from here; simplify = rewrite $ \ case 
I'm not saying Haskell doesn't have TCO (it has) but pretty much never uses it.
I've read the documentation and even manage to use it but it's really hard to write and to read and I still don't understand how people think it's acceptable to have to write AST manually instead of Haskell code with placeholder.
To answer your general question: To use a parameter more than once, you need to duplicate your parameter like: dup :: a -&gt; (a, a) Then you call two functions on each. Algebra of Programming could help you. https://www.amazon.ca/Algebra-Programming-Richard-Bird/dp/013507245X 
Sounds fascinating. That's going to be one hell of a job. Haskell is really only a tiny tip of the iceberg.
For one second I was pissed off. My contract for Target just ended after spending a year subtly hinting at moving from Python to Haskell. But my Target was a distributed data analysis and database system for astrophysics and medical research, not a grocery store. Too bad, my Target hasn't learned anything.
Good idea, we'll do that.
Today, I changed readKeyBinding = traverse readKeyChord . fmap (splitOn "-") . splitOn " " to readKeyBinding = traverse readKeyChord . fmap (splitOn "-") . splitOn " " It's very easy spread out "one-liners" over multiple lines (especially composition pipelines).
Thought I was on /r/shittyprogramming for a second.
Sounds a bit odd. To my knowledge data science in Haskell is not very mature. I thought Python or R are still the languages of choice there. And how is category theory needed to do anything? But good to hear that Haskell is used more and more in the industry.
welcome to the information age... (like it or not)
I wrote `enumerate` to list all values in a finite type (even polymorphic ones, once specialized), including the ones you mentioned: &gt; (a,b), other tuples, Either a b, Maybe a and even Set a Indeed, it's straightforward, just a chore. Thus, `Generic`s to automatically derive them, which you can do for your own types too: {-# LANGUAGE DeriveGeneric, DeriveAnyClass #-} import Enumerate data A a = A0 a | A1 (Maybe a) (Either a a) | A2 (a, a) | A3 (Set a) deriving (Show,Generic,Enumerable) calling `enumerated` and `cardinality`: -- A Void &gt;&gt;&gt; cardinality ([]::[A Void]) 1 &gt;&gt;&gt; enumerated :: [A Void] A3 (fromList []) -- A () &gt;&gt;&gt; cardinality ([]::[A ()]) 8 &gt;&gt;&gt; enumerated :: [A ()] A0 () A1 Nothing (Left ()) A1 Nothing (Right ()) A1 (Just ()) (Left ()) A1 (Just ()) (Right ()) A2 ((),()) A3 (fromList []) A3 (fromList [()]) -- A Bool &gt;&gt;&gt; cardinality ([]::[A Bool]) 22 &gt;&gt;&gt; enumerated :: [A Bool] A0 False A0 True A1 Nothing (Left False) A1 Nothing (Left True) A1 Nothing (Right False) A1 Nothing (Right True) A1 (Just False) (Left False) A1 (Just False) (Left True) A1 (Just False) (Right False) A1 (Just False) (Right True) A1 (Just True) (Left False) A1 (Just True) (Left True) A1 (Just True) (Right False) A1 (Just True) (Right True) A2 (False,False) A2 (False,True) A2 (True,False) A2 (True,True) A3 (fromList []) A3 (fromList [False]) A3 (fromList [False,True]) A3 (fromList [True]) -- A Ordering &gt;&gt;&gt; cardinality ([]::[A Ordering]) 44 &gt;&gt;&gt; enumerated :: [A Ordering] A0 LT A0 EQ A0 GT A1 Nothing (Left LT) A1 Nothing (Left EQ) A1 Nothing (Left GT) A1 Nothing (Right LT) A1 Nothing (Right EQ) A1 Nothing (Right GT) A1 (Just LT) (Left LT) A1 (Just LT) (Left EQ) A1 (Just LT) (Left GT) A1 (Just LT) (Right LT) A1 (Just LT) (Right EQ) A1 (Just LT) (Right GT) A1 (Just EQ) (Left LT) A1 (Just EQ) (Left EQ) A1 (Just EQ) (Left GT) A1 (Just EQ) (Right LT) A1 (Just EQ) (Right EQ) A1 (Just EQ) (Right GT) A1 (Just GT) (Left LT) A1 (Just GT) (Left EQ) A1 (Just GT) (Left GT) A1 (Just GT) (Right LT) A1 (Just GT) (Right EQ) A1 (Just GT) (Right GT) A2 (LT,LT) A2 (LT,EQ) A2 (LT,GT) A2 (EQ,LT) A2 (EQ,EQ) A2 (EQ,GT) A2 (GT,LT) A2 (GT,EQ) A2 (GT,GT) A3 (fromList []) A3 (fromList [LT]) A3 (fromList [LT,EQ]) A3 (fromList [LT,EQ,GT]) A3 (fromList [LT,GT]) A3 (fromList [EQ]) A3 (fromList [EQ,GT]) A3 (fromList [GT]) To be clear, the instance is `Enumerable`, not `Bounded` and `Enum`: class Enumerable a where enumerated :: [a] -- should coincide with `[minBound..maxBound]` when all three instances are provided cardinality :: proxy a -&gt; Natural -- constant-time But you can easily write your own orphans in three lines with the helpers in `Enumerate.Enum`: instance Bounded A where minBound = minBound_enumerable array_A maxBound = maxBound_enumerable array_A instance Enum A where toEnum = toEnum_enumerable array_A fromEnum = fromEnum_enumerable table_A -- CAF array_A :: Array Int A array_A = array_enumerable -- CAF table_A :: Map A Int table_A = table_enumerable links: https://hackage.haskell.org/package/enumerate-0.2.1 http://sboosali.github.io/documentation/enumerate/index.html http://sboosali.github.io/documentation/enumerate/Enumerate-Enum.html Another commenter notes that &gt; tuples must antidiagonalize, and sums must interleave which this package doesn't (it just enumerates them depth-first i.e. left-to-right recursive-descent, which is the simplest implementation); nor does it enumerate infinite types. If you want more sophisticated behavior, you can check out the "alternatives" section in the documentation. But it does provide an instance for every relevant instance in `base` I could find: Enumerable Bool Enumerable Char Enumerable Int8 Enumerable Int16 Enumerable Ordering Enumerable Word8 Enumerable Word16 Enumerable () Enumerable FormatAdjustment Enumerable FormatSign Enumerable Void Enumerable SpecConstrAnnotation Enumerable GiveGCStats Enumerable DoCostCentres Enumerable DoHeapProfile Enumerable DoTrace Enumerable ConsoleEvent Enumerable IOMode Enumerable NonTermination Enumerable NestedAtomically Enumerable CodingFailureMode Enumerable CIno Enumerable CMode Enumerable BlockedIndefinitelyOnMVar Enumerable BlockedIndefinitelyOnSTM Enumerable Deadlock Enumerable AllocationLimitExceeded Enumerable AsyncException Enumerable Newline Enumerable NewlineMode Enumerable IODeviceType Enumerable SeekMode Enumerable CodingProgress Enumerable BufferState Enumerable CChar Enumerable CSChar Enumerable CUChar Enumerable CShort Enumerable CUShort Enumerable CWchar Enumerable ArithException Enumerable All Enumerable Any Enumerable Associativity Enumerable GeneralCategory Enumerable a =&gt; Enumerable (Identity a) Enumerable a =&gt; Enumerable (Complex a) Enumerable a =&gt; Enumerable (Dual a) Enumerable (a -&gt; a) =&gt; Enumerable (Endo a) Enumerable a =&gt; Enumerable (Sum a) Enumerable a =&gt; Enumerable (Product a) Enumerable a =&gt; Enumerable (First a) Enumerable a =&gt; Enumerable (Last a) Enumerable a =&gt; Enumerable (Down a) Enumerable a =&gt; Enumerable (Maybe a) (Enumerable a, Ord a) =&gt; Enumerable (Set a) (Bounded a, Enum a) =&gt; Enumerable (WrappedBoundedEnum a) (Enumerable a, Enumerable b) =&gt; Enumerable (Either a b) Enumerable a, Enumerable b) =&gt; Enumerable (a, b) Enumerable a =&gt; Enumerable (Const a b) Enumerable (Proxy * a) Enumerable a, Enumerable b, Enumerable c) =&gt; Enumerable (a, b, c) Enumerable (f a) =&gt; Enumerable (Alt * f a) Coercible * a b =&gt; Enumerable (Coercion * a b) (~) * a b =&gt; Enumerable ((:~:) * a b) Enumerable (Rec * f ([] *)) (Enumerable (f a), Enumerable (Rec * f as)) =&gt; Enumerable (Rec * f ((:) * a as)) Enumerable a, Enumerable b, Enumerable c, Enumerable d) =&gt; Enumerable (a, b, c, d) Enumerable a, Enumerable b, Enumerable c, Enumerable d, Enumerable e) =&gt; Enumerable (a, b, c, d, e) Enumerable a, Enumerable b, Enumerable c, Enumerable d, Enumerable e, Enumerable f) =&gt; Enumerable (a, b, c, d, e, f) Enumerable a, Enumerable b, Enumerable c, Enumerable d, Enumerable e, Enumerable f, Enumerable g) =&gt; Enumerable (a, b, c, d, e, f, g) 
Does the example I gave use TCO?
surprisingly functional: persistent data structures (in the Okasaki sense, not the Codd sense), laziness, purity, zippers. 
Like I said I can only be very vague here but I think it stems from the datatypes a la carte paper http://www.cs.ru.nl/~W.Swierstra/Publications/DataTypesALaCarte.pdf It's a typeclass that takes care of figuring out the correct sequence of injections to route through your Coproducts. Very handwavy... sorry
Are you sure you are not looking at Free strictly from the library perspective here? One of the most [engaging talks](https://www.youtube.com/watch?v=rK53C-xyPWw) I've seen in the last couple of months is Chris Myrs talking about how Free allowed them to decouple independent parts of their system and minimize the cost for change. I'm sure you understand the technical implications a lot better than I do, but maybe Free's scope is more of a architectural / organizational one?
 {-# LANGUAGE LambdaCase #-}
I work at [Genetec](http://www.genetec.com/) and we have some teams doing F# http://clients.njoyn.com/CL2/XWEB/XWeb.asp?tbtoken=YF1QShwXCG97Z3RyTSQlCCBKcmREcCNacEhZUFx4E2AtX0oYUUYfAWR1dAkbURRQQHgqWA%3D%3D&amp;chk=dFlbQBJe&amp;Page=JobDetails&amp;Jobid=J0116-0683&amp;BRID=119900
I doubt they can find anyone that is competent in both haskell and data science....
[This](http://dlaing.org/cofun/) is one of the best resources I've seen on the subject. Goes into all the neat bits on using a free monad to structure a program, a cofree comonad to structure an interpreter, using pairings to formalize that a bit, and finally using the Data Types a la Carte mechanism to break it into separate bits. That's a good bit of the *how*, but you should understand the *why* too. Free monads aren't necessary for encapsulating the effects/commands your program has, you can do that just fine (in fact, MUCH more easily) with `mtl` style. A free monad approach is useful when you want to build an abstract syntax tree representing the computations, and then inspect that structure and do potentially multiple different things with it. `pipes` and `conduit` use a structure very similar to the free monad, and they use this to fuse multiple steps into single steps. `haxl` uses a free monad to batch data requests. What *problems* are you solving with `free`? Make sure it is the right tool for the job.