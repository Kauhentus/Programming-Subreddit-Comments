I'm sorry? I was under the impression I was using Cabal's actual parser.
Oh, whoops, sorry. Duh.
Have you tried http://hackage.haskell.org/package/Hs2lib :)
I tried 'show 1.2'. Nothing happened. :(
The supported Prelude functions are [listed here](http://hpaste.org/stepeval). It appears to just stop evaluating when an unknown function is reached. 'foo 1.2' gives the same result.
A somewhat limited Prelude. But a very cool application even so.
New link, http://mortenlysgaard.com
Massively multithreaded architectures are one way current researchers are approaching the problem of making an architecture that supports data flow. (Feo gave a talk about this which I documented here http://blog.ezyang.com/2010/07/graphs-not-grids/) One of the persistent problems plaguing non-Von Neumann architectures is that traditional CPUs have had many years of engineering effort to make them fast, and without this advantage alternative architectures tend to be very slow. This kind of sucks.
I would imagine that you only need to be able to emulate such systems in order to develop a theoretic model on which you can perform optimization. I would imagine that this will happen in 5-10 years as research technologies get better at translating directly in to production.
Researchers for unrelated things (I think the specific case I heard about was CPUs with capabilities built in) have done this: the problem is emulation is really, really slow, so you can't actually test anything reasonably large. I'm not sure the problem is fabrication so much as it is "elbow grease and minor theoretical results required to make something actually work fast."
My point is computers are increasingly able to produce the elbow grease and minor theoretical results on their own, leading to faster translation from new discovery to streamlined technology.
Just for understanding: What exactly does &gt; 65536 Symbols mean? That sounds like an awful amount of functions...
Hackage's `Control.Arrow.&gt;&gt;&gt;` would work as a library solution. The `|&gt;` is used by `Data.Sequence.|&gt;`
Solution: use Linux
My first guess is that there must be lots of internal functions being exported. First thing to check is that it's only symbols for exported (manually or extra unfoldings) functions from each module being exported from the DLL (and not all internal symbols in each .o). If that's not the problem then it does get harder. We'd have to do a visibility check starting from the unfoldings of the exported functions in the exposed modules, tracing through all possible internal functions that could be reached (recursively) via those unfoldings. It is then only that subset that need to be marked for DLL export. The same trick could be used on ELF platforms to reduce the number of exported symbols which would improve performance.
Ugh... I posted this sort of on a whim and now it's on my friggin twitter feed. I rather wish that there were a more low key way to discuss this.
Don't worry about it, it's a fun post. :)
It seems the tension between ML and Haskell is orthogonal to "worse is better".
I like the comment "You have to concede it's a bit funny that you're pointing to Haskell as an example of the 'worse-is-better' approach that leads to viral adoption of dirty solutions over the endless pursuit of perfection."
I think we can all agree that the right answer is "Haskell is better". ;-)
I'd be interested in this too... 
Probably not what you are looking for: Emily Mitchell has [described](http://neilmitchell.blogspot.com/2011/03/experience-report-functional.html) her experience with using Haskell for Paleoecology research. In particular, she compares it to R.
"Basic category theory for computer scientists" by Pierce is the only book on category theory that I've actually understood!
KU's FP group is working on a connection between R and lambda-bridge, a stream based RPC written in Haskell intended for connecting to FPGAs. http://ittc.ku.edu/csdl/fpg/Tools/LambdaBridge 
I dunno, let me finish evaluating the idea...
It's about 50% functions (XXX_info symbols) and 50% data (XXX_closure symbols). But yes, that is a lot of functions - and thats only the exported ones! Many, many more functions than this are generated by GHC but don't get exported. This is just the nature of Haskell code, where every thunk is its own little function :-)
I actually took some time to look at Core (here's a [cleaned-up version](https://gist.github.com/1063476)), and from the absence of 'RuleFired: SC:*' comments it looks like SpecConstr was not able to apply its magic tricks. I also find it hard to understand how it could remove those constructors. What did you expect SpecConstr to do here?
There's RClient, a Haskell package that allows you to call R from Haskell, using R's RServe interface (i.e. it's interprocess comms and a layer that marshalls between R representations and Haskell types), but I'm not aware of "RHask". Generally not a fan of munging languages into the same process... 
Multiple typeclass inheritance is something I found somehow missing to Haskell, but couldn't think of a clean way implementing those. Didn't know that ML offered these, maybe I should look into it.
ML offers something rather different that can sometimes be used for the same purpose. 
I vaguely recall someone mentioning they were considering rewrite of R in Haskell to improve performance woes?
I think we can all agree that the answer is "Haskell won". The OP seems to back-rationalize the fact that Haskell won to the notion that Haskell must be worse in a "worse is better" way, which is faulty logic. I won't agree that Haskell is better, though.
But you don't want 10's of 1000's of symbols per-library on ELF platforms either, it makes the dynamic linker slow. Think OpenOffice-like slow program startup.
There's no reason in principle that you can't have both type classes and ML style modules. There are several research papers on this, it's just never flowed down to an actual implementation of either language (to my knowledge). And this repeats the same misconception (if you ask me) that type classes are somehow a kludgy version of ML modules that are a little more practical. But this is false. They are, at their core, two different things that in practice can be used for some similar purposes. ML modules are for abstraction. They're good for writing down an interface, programming to that interface, and making abstract implementations of that interface. You can kind of do this with type classes, but it ends up being clunky, with newtypes floating around for selection of implementations and whatnot. Type classes are for type-directed name resolution, based on the fact that there must be a single choice of instance in scope for any particular type. Type signatures with class constraints are essentially _the_ correct types to give terms where the implementation is selected by the type (something like them anyway). ML modules pretty much don't provide this functionality at all. Instead you have to pass dictionaries/modules around yourself, which may not be that bad in some cases, but is a pain in others. So it's not, "type classes are a crappy version of modules that won out." They both have uses that the other isn't really good for. Maybe encoding modules with type classes is less of a pain than the reverse for the practicing programmer. But I sort of doubt that Haskell "winning" was solely based on type classes vs. modules anyway.
&gt; There are several research papers on this, it's just never flowed down to an actual implementation of either language (to my knowledge). Do you have any links? I'm curious. I'm well aware they tackle different problems, i'm just more familiar with the arguments that they can't work together than with evidence to the contrary.
What did Haskell win? Language adoptions, a contest of taste or something else? In terms of applications, F# and other ML derivatives probably blow it away. Isn't Bing written in F#? That's not to say that Haskell's design hasn't been influential (though it should be noted that ML preceded it by almost two decades).
There's [Modular Type Classes](http://www.mpi-sws.org/~dreyer/papers/mtc/main-long.pdf), which tries to make type classes sugar for modules. There's [ML Modules and Haskell Type Classes: a Constructive Comparison](http://www.cse.unsw.edu.au/~chak/papers/modules-classes.pdf), which shows how to go in the opposite direction. There's also [a longer thesis](http://www.stefanwehr.de/publications/Wehr2005.html). Ken Shan wrote [Higher Order Modules in F_w and Haskell](http://www.cs.rutgers.edu/~ccshan/xlate/xlate.pdf), which shows, at least, how to desugar modules to the same intermediate language that Haskell requires. And there's also [F-ing Modules](http://www.mpi-sws.org/~dreyer/papers/f-ing/main.pdf) in that area. These probably don't answer all the questions about how type classes and modules should interact. But two schemes for translating them into each other and two schemes for translating both into a common underlying language isn't a bad start. Edit: Oh, I almost forgot. Coq has type classes that desugar into dependent record passing (as I recall) in a similar way you might expect type classes encoded as modules to behave.
Thanks!
Mindshare, and, i believe, eventual use. You're right about F#, and i hadn't considered it (for various reasons), but i think it's doomed to be an ancillary language, a compromise. It's the sort of thing you allow to get the weird, geeky programmers in, but never really support. Haskell's community is stronger than anything in the ML world, and that means it's become the go-to language of choice for bored smart programmers who want to learn type-centric functional programming. Add in the concurrency research going on in Haskell, and it starts seeming like a practical investment, too. Add in the libraries at hackage (unfettered by the OO semantics of .NET), and you can actually get stuff done in Haskell, too. So, yeah, even including F#, i think Haskell's won.
I think that as we began to use typeclasses, it became more apparent that that they were a different animal compared to modules. I was an ML fanboy before I became a Haskell fanboy, so it *seemed* to me that typeclasses were a worse but better version of functors. For those not familiar with both... The way we often use typeclasses is when we want to specify how a function works relative to some other given specification, i.e. foo :: (SomeSpec a) =&gt; a -&gt; b ML's functors not only let you do this, but they also gave you a namespace in addition to it. i.e. module type SomeSpec = ... module Blah = functor (Spec :: SomeSpec) -&gt; foo :: Spec -&gt; b foo = ... module Foo = Blah(MySpec) It should be clear then, how you can use newtypes in conjunction with typeclasses to implement functors.
What does the `+RTS -s` output look like?
GC time is is 8.3% for the sequential version, increases to 19.5% when using `-N4`, but drops back down to 6.6% when using `-N4 -qg`. Running `ghc-tune` did not reveal much interesting.
Cool, but doesn't preserve sharing :-) http://hpaste.org/steps/48627?expr=let+n+%3D+product+%5B1..10%5D+in+n+%2B+n&amp;submit=Submit
I agree that parallel comrehensions for the Eval monad look quite neat!
re: namespacing I don't think there's any intrinsic reason type class instances must be global. Lexically scoped instances seem coherent to me.
Haskell's type classes support multiple inheritance. Unless you mean something different by this term?
Yeah, but only with newtype, which I find kind of ugly
I don't think we're on the same page. This is what I mean by "multiple inheritance": &gt; class (Eq a, Show a) =&gt; C a where ... Is this what you mean? Because that's pretty standard notion of multiple inheritance I think, and a common usage of type classes, but I'm not sure where newtype enters into it.
Wouldn't an arrow model be better here? class Arrow r =&gt; ArrowParallel r where (/***/) :: r a b -&gt; r c d -&gt; r (a, c) (b, d) (/&amp;&amp;&amp;/) :: r a b -&gt; r a d -&gt; r a (b, d) f /&amp;&amp;&amp;/ g = arr dup &gt;&gt;&gt; f /***/ g where dup a = (a,a) instance ArrowParallel (-&gt;) where f /***/ g = \ (x, y) -&gt; fx `par` gy `pseq` (fx, gy) where fx = f x ; gy = g y 
Late to the discussion, but relevant: http://stackoverflow.com/questions/5725324/expressing-long-chain-of-compositions-in-haskell
Consider a monad for tracking data dependencies. Clearly the "side effect" of tracking dependencies doesn't care about the order of them, just the dependencies. So it's commutative. Also, consider the reader monad. It doesn't matter whether you read now or later; the environment will be the same. For partial commutativity you can consider ST. If I write to a variable x, and also write to y, it doesn't matter which I do first so long as I do them both (assuming y /= x, and assuming you already have the values to write in hand so you're not reading from one to write it into the other). But clearly there are times where it's not commutative. Also the writer monad when the messages being written form a commutative monoid. (This is essentially the same as the data dependencies.) Or if the messages are only semi-commutative, then we get a semi-commutative monad. If you restrict yourself to finite lists/sets, then when thinking of it as the nondeterminism monad it's also commutative. The performance characteristics may change, but the end results won't (on a sufficiently large computer). If you allow infinite lists/sets then it's definitely not commutative unless you explicitly add commutativity with something like [logict](http://hackage.haskell.org/package/logict)
Heh, I sent that to a friend who works in paleontology and geology and is an avid R user and he had a rather negative reaction to the paper.
Oh, what did he say? He doesn't like Haskell?
It was the paleo content that upset him. For example, he didn't really get an idea of what she'd really done from reading the paper, he knows dozens of people working in computational paleoecology, it's not a particularly new field, and he himself has done some of it. 
Consider a simple state-providing arrow, AS, which allows for parallelization: newtype AS s a b = ASRd (s -&gt; a -&gt; b) | ASWr (s -&gt; a -&gt; (s, b)) runAS :: AS a b -&gt; s -&gt; a -&gt; (s, b) runAS (ASRd f) s a = (s, f s a) runAS (ASWr f) s a = f s a instance Category (AS s) where id = ASRd $ const id ASRd f . ASRd g = ASRd $ \s -&gt; f s . g s rf . rg = ASWr $ uncurry (runAS rf) . (runAS rg) instance Arrow (AS s) where arr = ASRd . const first (ASRd f) = ASRd $ \s -&gt; first (f s) first (ASWr f) = ASWr $ \s (a, thru) -&gt; (s', (b, thru)) where (s', b) = f s a It provides methods of reading and writing state: instance ArrowState (AS s) where accessState = ASRd changeState f = ASWr $ \s a -&gt; (f s a, a) We can then define `ArrowParallel` so that it automatically parallelizes read actions if possible. instance ArrowParallel (AS s) where f /***/ g | canParallelize f g = proc (x,y) -&gt; do ff &lt;- simplify f -&lt; () gf &lt;- simplify g -&lt; () let ffx = ff x gfx = gf y returnA -&lt; ffx `par` gfx `pseq` (ffx, gfx) | otherwise = f *** g where canParallelize (ASRd _) (ASRd _) = True canParallelize _ _ = False simplify (ASRd f) = ASRd $ \s () -&gt; f s Now that I think about, `canParallelize` and `simplify` might be better off as members of `ArrowParallel`. `canParallelize` determines if two input arrows are parallelizable, while `simplify` is a function of type `(r a b) -&gt; r () (a -&gt; b)` - it takes an input arrow and un-`arr`s it into a plain function (arrows which hide an IO underneath may want to use `unsafePerformIO`). 
I think he's thinking of multiple instantiation.
multiple inheritance of the same type class. For example for Num there is the Sum and Product newtype wrapper, because Numbers form a Monoid both over summation and multiplication
While I like Haskell and love that it gets more and more traction, I kind of feel this testimonials are too obviously exaggerated, or the people are unaware of what they are saying. When you have things like: &gt; Each feature took longer and longer to implement: development time was almost exponential in terms of required features. You kind of assume the fault is here the programmer, not the language. 
Even some more restrictive version of IO could be commutative if it places sufficient constraints on allowed updates - updates would have to be backed by some sort of eventually consistent protocol, such that they can be applied in any order to still get the same result - something like Google Wave, perhaps. 
No, of course they don't. Ben [already proved that](http://disciple.ouroborus.net/).
As someone who developed ASP web sites in the past, i disagree. It indeed quickly spins out of control. And once site grows, things take much more time to get done than they used to be. Especially when you have more than one developer on the team. But testimonials are kind of light on evidence and mostly are an emotional response. Btw i use yesod in production. 
I think the fact that it is a testimonial is sufficient to justify emotion. There can be little expectation that a testimonial be a dry technical document. 
Ah, so you want named instances. I believe this is the standard terminology used in the literature, and there are various proposals for named instances.
&gt; [Yesod] already has a much stronger *foundation* (Emphasis mine) Punny ;)
See this related discussion at stackoverflow: http://stackoverflow.com/q/5897845/371753
Yes, it's really awesome! In fact, I would go as far as dropping the `rseq` and `rpar` functions in favor of the new `mzip` structure.
Who's writing this?
See also the haskell-cafe ML [message](http://www.haskell.org/pipermail/haskell-cafe/2011-July/093831.html). And the whole [blog](http://ghcarm.wordpress.com/).
On further reflection I think there may be merit in considering a hypothetical possibility here. Let us assume that the programmer who wrote the testimonial is in fact a mediocre programmer. (Understand that I personally think no such thing.) Let us also assume that this programmer has had poor experiences due to challenges presented by other languages, and that the programmer's unfortunate lack of skill has made those challenges difficult to overcome. Given the glowing testimonial we may speculatively conclude that the programmers use of Yesod and Haskell has resulted in a positive experience and greater productivity. If all of this is true then perhaps this positive outcome may be mirrored in other less than stellar programmers who employ Haskell instead of their default go-to language. If I were the head of a department looking for a better way to get the most out of my programming staff I might be inclined to try/use Yesod and Haskell. This may mean violating the principle of avoiding success, but sacrifices of that type are not intolerable.
&gt; it's become the go-to language of choice for bored smart programmers Well, we're certainly bored! Time will tell if it was a smart choice. The investment required to learn Haskell is unlike any other language I've learned. So far, worth it. 
I found that the fastest way to learn for me was to get a copy of Types and Programming Languages by Benjamin Pierce and implementing the lambda calculi defined therein. It's the kind of problem that touches every significant aspect of Haskell and also shows you a lot of its raw power. Plus, actually studying the type systems that form the basics of Haskell's type system really helped me grok it more quickly.
A simple tiled map game. Like old final fantasy, or pokemon, or that XNA sample RPG. With a map and entities would be brilliant. Preferably with easy to understand explainations of where the performance problem points occur, and how to detect and avoid them.
Squarified tree map of a directory's files based on their size (with coloration based on user customizable file type collections). It'd be especially good if you could hook into inotify to make it update on the fly too. I feel this is a nice mix of user input and algorithms.
Just for kicks, I tried to estimate how many people that'd be useful advice for. I wound up with a pretty small number.
The hardest part in learning haskell is unlearning all the stuff you learned before and realizing that you are not just learning a new syntax over old concepts. Know that you are starting from scratch. The second hardest part is once you have learned the basic concepts, is how do I put everything together. I feel this is where most of the books and online materials fall a bit short. There is no book called "Large Scale Purely Functional Software Design" yet (such as [Large-Scale C++ Software Design](http://www.amazon.com/Large-Scale-Software-Design-John-Lakos/dp/0201633620)).
Indeed. Having motivation to implement (various!) lambda calculi implies an above-average sophistication with PL ideas. The greater majority need to start further upstream. Back to the OP: "When I want to make something useful, I have to stick everything into a "do" statement and then afterwards I just get stuck and don't know how to proceed." Well, how does he get stuck? Recall that Haskell is the world's finest imperative language.
One of my "if I one day get the time to get round to it" plans is to write an [XBattle](http://en.wikipedia.org/wiki/Xbattle) clone in FRP style.
I'm a little further along than the OP but I've considered doing just this on a smaller scale. There are a few LC tutorials that show how to implement basic logic and arithmetic in an untyped LC. I find working the examples and exercises on paper _extremely_ tedious. I'm curious about the simply typed LC etc. but I'm afraid my hand would fall off.
you must be trolling
being a beginning haskell coder is a wonderful time, it all looks so pure and revolutionary then you dig in and find out that the masters write "efficient" haskell (i.e. what you see on the shootout) by writing it like c...strictness annotations, lots of IO monad... then you dig in to string types and realize that in some ways haskell is a disaster. what other language do you know that makes you work so hard to grok STRINGS? are we using bytestring today or text? or lists of chars? anywho, enjoy the haskell honeymoon, its a fun time 
I think the hardest part for some is figuring out what adviceis really helpful. (Burritos, spacesuit, programmable semicolon vs. don't worry about monads for at least 3 weeks). For me, on the other hand, having e.g. pattern matching explained many different ways is helpful BTW the wiki page could use a few updates: new 3rd edition of Simon Thompson's book, which I'm sure is good based on reading his erlang text, and the guide to symbols/punctuation http://www.haskell.org/haskellwiki/Tutorials http://www.imada.sdu.dk/~rolf/Edu/DM509/E06/haskell-operatorer.pdf
My advice would be: learn as much as you can about the Haskell type system. This will create clarity and motivation in your programming process. By relying on the type system, you can prototype an entire program logically and understandably without writing a single algorithm. As a recent beginner, this is what changed everything for me.
Its really only a step removed from scheme in 48 hours or what-have-you...
&gt; then you dig in and find out that the masters write "efficient" haskell (i.e. what you see on the shootout) by writing it like c...strictness annotations, lots of IO monad... Yes, because the shootout is extremely representative of the real world, especially for those who write fannkuch every day over and over again for their job. You're also correct strictness annotations and the IO monad are clearly something only "master" haskell programmers could ever hope to comprehend and are the ONLY way to ever get efficient Haskell code, ever (oh, except when it's not. I also find the implication 'strictness annotations' make your code "C-like" and are *some sort of cheaing*, utterly, utterly laughable too.) Troll harder - it doesn't even sound like you're trying. I'll grant that performance is definitely important, but your points here are seriously laughable. &gt; then you dig in to string types and realize that in some ways haskell is a disaster. what other language do you know that makes you work so hard to grok STRINGS? are we using bytestring today or text? or lists of chars? I don't think anybody's claiming Haskell's perfect - indeed, the proliferation of String-like types (and their strict and lazy variants) is something many Haskellers find displeasing, including myself (among other things - bad `Num` hierarchy is always pointed out, but I'd also enjoy better records, and a little more generalization in the prelude, like Bool.) I think the 'solution' has mostly come down to "use text for actual strings of data as it respects Unicode, and use bytestring for binary-like data" and that's mostly it, practically speaking. These are only the rough conventions for usage however - and are not truly codified. It also does nothing to solve the many times necessary conversions between strict and lazy variants of some string type, different string types entirely, and the combinations thereof. This is nowhere near optimal, without a doubt. If you have a solution to this however, I think many people would be glad to hear it. Or, you know, you can continue to throw around rhetoric, whatever's easier.
Haskell is not a fine imperative language. The syntax is terrible. Javascript: var x = 5, y = ""; while(--x) { y += String(x); } Haskell x &lt;- newIORef 5 y &lt;- newIORef "" loop x y where loop x y = do modifyIORef x (-1) guard (x /= 0) $ do xv &lt;- readIORef x modifyIORef y (++ (show x)) loop x y Of course, some of this could be improved by having better names, more standard combinators, etc.
At first glance, this is too large an example. But if we focus just on character movement, it becomes an interesting exercise in animation.
I'll classify that under "board game examples". :-)
You wouldn't need a whole game :) I'm thinking of something like a 50x50 grid map with a 20x20 view portal on it. A character moving around under user control, causing the portal to scroll, and maybe 2 or 3 non user controlled characters moving randomly. The interactions between animation, multiple characters, user input and map scrolling would be very interesting using FRP.
 y &lt;- newIORef "" forM [1..5] $ modifyIORef y . flip (++) . show Using a mutable loop variable in this case is just perverse...
I think that there is are Bytestrings and Strings because sometimes the difference is pretty important, for example, when passing Unicode data into a MySQL database. I don't think text manipulation is inherently any more difficult in Haskell. Though, I think one failing of the community is that there are a lot of commonplace, basic functions that haven't been integrated into the on-line repository, which should have been by now, and therefore beginners end up reinventing a lot of wheels.
If you wanted to, it would be easy to write write a library to give you syntax closer to mainstream imperative languages. For instance if your lib had: var = newIORef class Truthy a where toBool :: a -&gt; Bool instance Truthy Int where toBool = (/= 0) class PlusEquals a b c | a b -&gt; c where (+=) :: a -&gt; b -&gt; c instance Show a =&gt; PlusEquals (IORef String) (IORef a) (IO ()) where vr += ar = readIORef ar &gt;&gt;= modifyIORef vr . flip (++) . show preDec v = modifyIORef v (subtract 1) &gt;&gt; readIORef v while c d = c &gt;&gt;= flip when (d &gt;&gt; while c d) . toBool You could then code your example as: x &lt;- var (5 :: Int) ; y &lt;- var "" while (preDec x) $ y += x 
The fact that computing things by a sequence of imperatives, such as variable updates, is 'perverse' in Haskell is exactly what makes it a poor imperative language.
or just y &lt;- newIORef . concatMap . show $ [4,3..1] but that's not so imperative any more
Really, it's perverse in any language, this is just more obvious in Haskell.
That seems to be a popular viewpoint among Haskellers, and I think it's ridiculous. The description of algorithms in terms of a sequence of steps acting on state variables goes back thousands of years. Lambda calculus doesn't make it obsolete.
Haskell is a fine language for doing imperative things. If doing things in "imperative style" means step-by-step emulation of how one would do it in an imperative language, then by definition Haskell's a loser. But if doing imperative things means managing and manipulating stateful computations, then I think there's a much better case to be made...
The longevity of an idea is not a good indicator of it's merit. Humorism, for instance was the dominant medical philosophy of hundreds of years. Using mutable loop variable is definitely perverse in sense three in Merriam-Webster of "marked by peevishness or petulance", as the ubiquity of off-by-one and similar errors testifies. On the broader point of whether imperative programming is perverse in the more general sense of being wrongheaded, if we didn't think that then we wouldn't be here. That's not to say that functional style made imperative style obsolete, but it seems to be a better approach for many applications.
Great advice. Mind if I ask what resources were most helpful to your learning process? 
what do you mean "throw around rhetoric"? you didn't even disprove my points, indeed as far as i can tell, you bolstered them. fixing strings is impossible without a time machine. haskell will have to live with the mess, seeing as use in hackage is all over the place and always will be and are the shootout benchmarks contrived? sure, but haskell is the only language on that list that absolutely avoids being idiomatic in favor of basically embedding a c-like language. doesn't that tell you *anything*???? if truly idiomatic code samples were put in place for these tests, haskell would probably fare about as well as python. and by the way i did not imply that strictness annotations are the basis of the imperative sublanguage used to score well in the shootout, i meant the express avoidance of purity. go reread some of the sample programs. i've read your comments around here before, you like to bluster, but on this one you're all hat and no cattle. and to be pedantic,, i use haskell all the time and contribute to hackage. its not like i'm new to the language
no mostly the differences are because every two weeks someone comes up with a better implementation, and with each iteration they get further off of idiomatic haskell. strings are indeed idiomatic, but their performance blows.
I'm not arguing against the idea that functional style is preferable to imperative in many applications. My point of view is twofold: 1. Imperative style is sometimes appropriate. 2. Haskell's support for imperative style is poor. I think point (1) is addressed empirically by the fact that nearly every Haskell programmer uses monad blocks. Maybe there is some future development of functional programming which will allow most applications to be written without imperative constructions, but really existing functional programming is insufficient. As for point (2), it's of course a matter of taste, but standard imperative languages offer a lot that Haskell lacks. * `f &lt;$&gt; x 1 &lt;*&gt; return (y 2)` is `f(x(1), y(2))` in imperative languages. * `if` can accept an effectful value in imperative languages * basic structured programming facilities - blocks, while and for loops - are built in in imperative languages, as well as more esoteric flow control like `break` and `goto`, which can occasionally be very useful. * Suppose you have three nested scopes of sequencing. Each has local state which it updates. In imperative languages, requesting the value of a variable `x` at the outer scope looks like this: `x`. In Haskell it looks like this: `x &lt;$&gt; lift (lift get)` (I always have to look up `get`, because I use arrows quite a bit - in an arrow you use `fetch`).
On point (1) I think we're in agreement. I have a couple objections to your second point. * In Haskell you can limit your imperative code to where it's actually appropriate. Having your code be a little more awkward when you're doing something truly imperative is better than having your code be really awkward because you're forced to do something imperatively where it's a poor fit. * It's really easy to define your own control flow. Yes, it's annoying that if can't accept IO Bools, but it's trivial to write ifteM, which does. * While there is a tax for using IORefs, but it pays off when you start working with more complex structures, MVars, Chans, STM, etc.
I think rather simple things that turn out to be not so simple, like a decent file dialogue, colour picker, and font chooser, are good examples.
But note that it's necessarily realtime.
What's a squarified tree map?
A character map and picker for (a subset of) Unicode. You could use this to pick characters you cannot enter easily and copy them to the system clipboard. Some simple puzzle game(s), like memory, lights out or minesweeper. An extensible scribble app. You could draw, say, lines and circles with it, and it should be possible to extend it to draw more shapes. Haskell code browser. At least a small prototype of one :) A simple chat server and client. A graphing app that would allow you to enter a function and have it draw the curve, and then drag the curve and modify the function. At least a small prototype of one :) 
http://en.wikipedia.org/wiki/Treemapping
Cover Flow http://en.wikipedia.org/wiki/Cover_Flow
augustss can embed anything in haskell... after two failed attempts at solving this without impredicative polymorphism (one with gadts and one with let) I think I understood what impredicative polymorphism is all about: it allows you to explicitly tell which type variables should be generalized (just like let-binding does), is this correct?
i will say this - yesod is very well documented. and michael seems to be incredibly responsive and engaged on the mailing lists. 
I don't get it.
It's not really about explicit generalization. It's about the status of quantified types in the overall type system. Or more generally, impredicativity is about self-referential definitions. In vanilla Haskell, we have the kind * which classifies types. Depending on how you set things up, though, it classifies only monotypes. So Int :: * Int -&gt; Int :: * And we can specify type schemas that contain variables ranging over `*`. So if: a, b :: * then: a -&gt; b :: * And there's always an outermost `forall a b ...` implicitly around those type schemas we write, if you want to look at things that way, but those aren't real Haskell types, so we don't have to actually address them. However, GHC adds higher-rank polymorphism, and first-class quantification. So at that point: forall a. a -&gt; a becomes a genuine type. So you have to ask what its kind is. GHC will tell you that the answer is `*`, but that is not exactly the truth by default. GHC is kind of inconsistent here, though. For instance, a more verbose version of the above is: forall (a :: *). a -&gt; a and you can indeed instantiate that type with `a = forall b. b -&gt; b`. If we were consistent about this, `*` would no longer be the kind of monotypes. But this means that the type `forall a. a -&gt; a` is quantifying over the kind of all types, which includes itself. So we have introduced a degree of self-reference. However, as I alluded to, GHC is lying by default. If you ask the kind of `Maybe`, GHC will tell you it is `* -&gt; *`. However, it will complain if you try to write the type: Maybe (forall a. a -&gt; a) This is because internally it has some distinction between monotypes (like `Int`) and polytypes (like `forall a. a -&gt; a`). And the `*` in `Maybe` is serious about it being the kind of monotypes, unless you use the flag for enabling impredicative types. For some reason, it doesn't care about impredicativity when only `(-&gt;)` is involved. Only for datatypes. Without the flag, though, you can think of there being two kinds, `*m` and `*p`. `Maybe` has kind `*m -&gt; *m`. And types involving `forall` have kind `*p`. The actual GHC system really even allows types in `*p` to quantify over `*p`, making it impredicative, it's just the kinds of datatypes that are more restrictive. And you can wrap a type of kind `*p` in a data/newtype to make a type of kind `*m`. So it's not so much a foundational concern (like it might be in a language for proving things) as a vehicle for making type checking/inference easier, reportedly.
If memory serves me, *foundation* is a rough translation of the Hebrew word (transliterated as) *yesod*.
Dolio's comment is correct as far as the im/predicativity goes, but yes, you're on the right track. One thing I find helps with these quantifiers is to think of things from a game-theoretic perspective where the caller and the callee are adversaries. If we have the type `foo :: forall a. X -&gt; IO a` then that means the caller has to choose the `a` (and the `x:X`), and then the callee only has to support that specific choice of `a`. Once the caller has chosen, it's fixed and can't change. However, with the type `foo :: X -&gt; IO (forall a. a)`, the callee has to return a value which works for every `a`. The caller gets to choose after they have the value in hand, which means they can choose one `a` and then change their mind and choose a different `a`. This is how augustss can use the variable as both an r-value and an l-value. This sort of reasoning also applies to -XRankNTypes. It is how the ST monad uses quantification to ensure that side effects can't escape, and also how list fusion ensures that the fusion is actually safe to do. The impredicativity issue is somewhat different. The problem here has to do with using a quantifier underneath a type constructor (other than the arrow, which is special), which raises the spectre of all sorts of in/consistency problems. The impredicativity isn't essential to the problem augustss is trying to solve, it's just a technical issue with the way he's trying to solve it.
See [this excerpt from the Yesod book](http://www.yesodweb.com/show/topic/179).
thanks, I'll read it a few more times, and maybe one day I'll get it.
Maybe data types represent some bottom level "leaf node" in GHC's representation? It seems to make sense. You have to make the impredicativity (i.e. self-referencing) stop somewhere right? And it also just doesn't seem to make sense to let Ints be represented by * -&gt; *. i.e. There is no value in abstracting it as such.
&gt; You have to make the impredicativity (i.e. self-referencing) stop somewhere right? Not really, in system F you have \*_m = \*_p, and the impredicativity doesn't "stop". The two strata are useful for inference but coherence (and type preservation) is still satisfied.
i'd love to see this article: "Twenty little programs that solve small daily problems that do not involve advanced haskell" seriously! the perl, python, ruby communities embrace solving mundane problems....would be nice to see haskell slum it a little and demonstrate utility for intermediate coders
If the new UTF-8 version might actually perform worse in some cases, why not keep the old UTF-16 version around, but rename it to Data.Text.UTF16, so people can use it if they want to?
Simplicity. Instead of "people can use it if they want to", the typical reaction is more likely to be "why do I have to care about this? what will go wrong if I choose one over the other?"
Isn't that something they should ask when it comes to unicode? :p
No, actually. The internal representation used by Text should be completely transparent except for performance, and the performance differences we're seeing (or expect to see) aren't big enough to warrant providing a gratuitous option. Keep in mind that providing that choice not only makes Haskell harder to use, but also means people will also occasionally need to swallow the performance hit of converting from one to the other because the libraries they use made different choices. So even if UTF-16 performs better in some cases, the overall performance picture isn't necessarily better when you provide that choice. So we've got: (a) harder to use, and (b) no guarantee of any performance improvement.
There are cases for it, though. When working with FFI you probably want to be able to pass arrays without having to re-encode them, and will need to use whatever encoding the target library expects. An alternative might be generic functions operating on `Text a`, and then the client application can choose whatever internal encoding they want.
Right... a huge part of the justification for considering a move to UTF-8 is precisely that external libraries, protocols, etc. are very likely to encode data in that way so there's the possibility of avoiding a conversion for free. If you happen to have a library that uses UTF-16 internally (does such a thing exist? Java uses UTF-16 internally, but not in a way that's likely to be visible in an FFI setting), then you may possibly get some advantage by treating untouched text blocks from the library as an opaque newtype wrapper around ByteString, with conversion to and from Text only when needed. As Jasper mentioned, stream fusion can actually still prevent making actual copies in many cases. Hypothetical scenarios with very unusual libraries and the FFI still fall way short, in my mind, of a justification for adding a type parameter to one of the more common types Haskell programmers ought to be using... especially when it's very rare that you'd ever care, and the cost when you do is fairly minimal.
&gt; If you happen to have a library that uses UTF-16 internally (does such a thing exist? Windows :)
OSX also uses UTF-16 internally, though it's not clear how relevant that is to the FFI. Does Windows use UTF-16 exclusively, er, externally i.e. where Haskell code would be hooking in? 
This seems like it could be a very useful GSOC project, but with benchmarks only in ASCII and Russian it is nearly useless. The whole purpose of this library is for *text*, written human language. I think we need to see good benchmarks in a much wider variety of languages before we can even start to think about making changes to `Data.Text`. In addition, I'd like to learn more about the effect of this change on the need for encoding/decoding in real applications. It seems to me that a switch to UTF-8 could be a big win in languages that are usually encoded that way to begin with nowadays, whereas in languages for which there is resistance to UTF-8 there could be an even bigger hit than what the benchmarks will show. (Aside from the issue of what encoding is used internally on various platforms, which is discussed elsewhere in this thread.)
&gt; The whole purpose of this library is for text, written human language. I think we need to see good benchmarks in a much wider variety of languages before we can even start to think about making changes to Data.Text. The `text` package has benchmarks for Chinese, Hebrew, ASCII, Russian, and Japanese. The average number of bytes used to encode a code point is what matters for performance, so we only need a few representative languages. &gt; In addition, I'd like to learn more about the effect of this change on the need for encoding/decoding in real applications. Please contribute some benchmarks! We have benchmarks for doing some typically tasks, like decoding HTML/XML, splitting strings into words and storing them in a dictionary, upper-casing text, etc. &gt; It seems to me that a switch to UTF-8 could be a big win in languages that are usually encoded that way to begin with nowadays, whereas in languages for which there is resistance to UTF-8 there could be an even bigger hit than what the benchmarks will show. Do you mean languages that are encoded as UTF-16 or something else entirely? The web is now about 50% UTF-8 and 20% ASCII, and the remaining 30% is shrinking pretty rapidly. 
Besides the runtime, it would also be interesting to see the memory usage. UTF-8 should have an advantage there in the case of English text.
Could the slowdown with 32-bit stores have been due to alignment issues?
I think this will be one of the main benefits.
Personally I'd split it into a simulation thread and a display thread (regardless of FRP or not); simulation handles multiple characters and user input, display thread handles animation and map scrolling. Possibly two interconnected FRP streams?
Can anyone delve into an explanation of why GHC did not unbox that? What optimizations are missing for that to happen?
Question for you, bos: &gt;Even in the slowest case, we can now decode upwards of 250MB of UTF-8 text per second, while for ASCII, we exceed 1.7GB per second! Looking at the graph preceding the paragraph before this one, it seems as though the numbers you're quoting (1.7GB throughput for ASCII, 250MB for Japanese) are the numbers associated with the hand-tuned x86 C, not the Haskell, which boasts rather more modest (but still impressively performant) numbers. Am I reading it incorrectly?
He has a large quantity of other benchmarks (that was much of the beginning of his summer of code project) but not much of the library transcoded to UTF-8 yet.
You're correct that the numbers are for the fast C decoder. I toyed with the idea of writing the faster decoder in Haskell instead, but it would have come out uglier (and probably slower) than the FFI-based version, which didn't make it feel very worthwhile.
I don't think so, as I observed it on pure ASCII input, where all the store instructions would necessarily have had properly aligned destination addresses.
`x2` through `x4` are not used in every branch of the subsequent code, and they are declared in a let block. So they must be lazily evaluated. To do better, GHC would probably have to decide to inline the definitions of them, but at least in the case of `x2` and `x3`, that would cause some code duplication, so GHC might not decide to do it. At least, my guess would be that it's along those lines.
I find it interesting how Neil's work is continually rediscovered by people, and how it must make enough of an impression for them to share. Perhaps this is because the tools never became part of mainstream Haskell (aside from hoogle), so they aren't part of normal conversation/lecture when learning/teaching Haskell. Who wants a GSOC to bring some of these tools more into the fold?
I would imagine more than a GSOC would be in order to get a version of Catch built up off a GHC toolchain rather than YHC.
The original thread on the [Haskell Libraries](http://www.haskell.org/mailman/listinfo/libraries) list is [here](http://www.haskell.org/pipermail/libraries/2011-July/016546.html).
I'm not too clued up on this stuff but does this mean we might be able to compile Haskell code to run on iPhones and (jailbroken) iPads? 
I wonder if it has the ability to (partially) verify exhaustivity for GADTs: given the type data T a where foo :: T Bool bar :: T Int the function f :: T Bool -&gt; Bool f foo = True is total...
I'd assume so. And Android, Meego, etc, as well. I can't wait; if I had more spare time I was looking into doing this.
Same with Super-O, but I can taunt people, can't I?
I don't recall YHC ever supporting GADTs.
No. 
There is a version of Barnes-Hut in the dph-examples package of dph. It has a graphical interface written with gloss, and there is a screen cast here: http://code.ouroborus.net/dph/video/nbody.mov
&gt; The text package has benchmarks for Chinese, Hebrew, ASCII, Russian, and Japanese. Great, that sounds like a much more representative cross-section. I hope that shows in future progress reports on the GSoC project. &gt; We have benchmarks for doing some typically tasks, like decoding HTML/XML, splitting strings into words and storing them in a dictionary, upper-casing text, etc. Also great! Again - let's see those before we make any changes to `Data.Text`. &gt; The web is now about 50% UTF-8 and 20% ASCII, and the remaining 30% is shrinking pretty rapidly. Sorry, I'm not convinced. The only source I have ever seen for numbers like that is on a UTF-8 evangelism site at Google. Whenever I have heard opinions expressed about this by actual developers in the Far East, they say that there is significant resistance against UTF-8 and even Unicode itself. Whatever the numbers are for the web, I'd like to hear the opinions of more developers in the Far East who spend their days working with CJK texts. That is a large and quickly growing proportion of software development, but still poorly represented in our community. If performance for non-UTF-8 encodings is important to them in their daily work, then it must be a significantly weighted factor in our benchmarks. Another point is that there is a huge amount of non-HTML content which is not UTF-8. That includes PDF, content in databases, and content in enterprise formats such as SGML and various proprietary formats. That stuff is not going away, and it's not moving to UTF-8. So we have to be careful not to make some small speedup for UTF-8 at the cost of a big slowdown for other encodings.
Just to make things clear: It should be easy to fix manual Typeable instances. It's just that if no action is taken, they will eventually break. So it's good to make sure that there is public awareness, and for the community to have awareness about the scope of the changes that need to be made. Also, we should clarify that AFAIK everyone agrees that this is a welcome improvement to Typeable. We are just trying to smooth the upgrade process so that it does not cause unexpected inconvenience to users of the various libraries on Hackage. 
Good to see that the Haskell School of Web Page &amp; PowerPoint Design is sticking to its roots.
That web page brings back memories of Frontpage that I never used and wished never existed.
I don't see why this statement would be at all controversial- and I'm speaking as an Ocaml fanboy here. The guys behind Haskell were very well aware of the "state of the art" in programming languages, especially the ML family of languages. Unlike some programming language designers I could name (Go, I'm looking at you). If they couldn't have significantly improved on what had gone before (Ocaml, SML, Miranda), they wouldn't have bothered to create Haskell at all.
I was wondering how this related to his older book Haskell School of Expression. From the Preface: &gt; This current book is a rewrite of The Haskell School of Expression with a focus on computer music, based on, and greatly improving upon, the ideas in Haskore and HasSound.
http://haskell.cs.yale.edu/?page_id=276 with PDF download
This is a cross post from /r/javascript. I was interested in any feedback/corrections that /r/haskell might have.
This is a massive simplification of what a functor is, and in many ways somewhat misleading. It implies that functors are just wrappers around individual values, or even wrappers at all, which is not true. The closest you could, in principle, get to in Javascript is a fuzzy description of data structures with mapping functions defined for them. JQuery objects might be inhabitants of some sort of functor type (assuming theat $.map obeys the functor laws) but they're really the simplest kind of functor you can imagine -- basically just the identity functor. The full array of things that functors actual do is more interesting.
Just as a second point, its the functor laws that make fmap what it is. Presumably you know this, but it's an important point so I want to stress it. Most container types have functions of the same signature, but which fail to satisfy the functor laws, and consequently aren't suitable for fmap. for instance, for lists: fakemap :: (a -&gt; b) -&gt; [a] -&gt; [b] fakemap f [] = [] fakemap f (x:xs) = [f x] Its failure to satisfy the functor laws is easy enough to show: fakemap id [1,2] is [1], not [1,2]. But it certainly has the right type signature. This isn't mentioned at all in the post, afaict, and thats a problem, because the JS world might get the impression that functors are just anything with methods of that type.
Cool.
In other words: functors must be "structure preserving"?
That's what the functor laws amount to, iinm, but the post didn't address this.
Completely accurate assessment. I had originally planned to include an explanation of the functor laws but its a long detour from trying to get people interested in them as a simple pattern, and more broadly interested in functional programming. In both cases, preservation of identity and composition, an entire side discussion would be necessary on what the composition and the id function do/are and then examples to clarify in Javascript. Again, I agree that it is a simplification of the Functor but thats intentional. Maybe after the applicative functor article I'll follow up with a more in depth explanation.
A slight quibble but class Functor functor where fmap :: (a -&gt; b) -&gt; functor(a) -&gt; functor(b) is in fact valid Haskell syntax.
thanks, updated!
Wow, timing couldn't be better for me. I've been wanting to make a another attempt at learning Haskell for a while, and music is my greatest passion in life so, yea...
You're right, it would be necessary to have such a side discussion, but without that discussion, you're not talking about functors, just some other type class. Haskell, of course, doesn't have real functors, because it can't enforce the functor laws, it just has conventions, but that doesn't mean you can ignore this aspect of what it means to be a functor.
That's like saying roads don't have speed limits because they're only enforced when there's a policeman around. There are other strong limits on the type of functors captured by the Haskell typeclass. See this discussion for example: http://stackoverflow.com/questions/3273373/are-all-haskell-functors-endofunctors
jQuery is not a Monad. jQuery is not a Functor.
I appreciate peering into the thought process of a developer. I have a question about how you use return, but will wait until the end of Part 2 in case it's answered there.
Actually you better use EMACS with ghc-mod. The modify, type-check cycle is more efficient with EMACS.
Maybe if you squint at it? I don't know jQuery, but from the post it looks like you might construe it as a functor that takes trivial elements as represented by their tag to the set of all elements with that tag in the current document.
jQuery varies its meaning based on what argument you feed it. It does some strong intensional analysis. It has no parametricity, and none of the functor laws come close to holding. It does different things if you give it tags themselves, multiple tags, css descriptors, arrays, etc. jQuery is magic, not a functor. So, I can't squint that hard without my eyes popping out. ;) jQuery is a wonderful combinator library that has revolutionized the web. Even Microsoft has acknowledged its superiority to their earlier solutions and started building on top of it. I'm not taking that away from it, but these posts that try to use it to analogize about Functors and Monads, etc. are just plain wrong. Combinators are a useful tool and can provide abstraction and domain-specific insight in their own right. I would rather celebrate jQuery for what it is than try to pretend it is something it is not.
I use this in my .vimrc: autocmd BufWritePost *.hs !ghc -c % This causes it to type check every time you save the file, which speeds up development a ton.
The jQuery $ is highly overloaded and magic, but I think the point was that this one thing he's used to, $("tag").map(f), reminded him of this other thing he's learning about, functors in Haskell. He says, "jQuery’s design leverages abstractions similar to Haskell’s better-known type classes." He never says that jQuery is a Functor. I think the guy deserves a break.
Of course there's a strong limit on functors -- they're necessarily endofunctors in the category Hask. But that's not the point, the point is that he's using functors as an example to the JQuery world, but this post is teaching people a misconception about what it means to be a functor. It's entirely plausible for someone to read this post, go away thinking they know about functors, and then get completely confused when they try to talk to someone who knows something about functors. Adding in the fact about the functor laws is easy enough -- it's one paragraph at most, and something you can brush off as so obvious in the case of $ that it doesn't require proving -- but it should be said *somewhere*.
So when do we start computing cohomology of rings?
It would be great to get an answer from the OP but in the meantime, you may want to look at http://book.realworldhaskell.org/read/monads.html under the section "The Maybe Monad": instance Monad Maybe where Just x &gt;&gt;= k = k x Nothing &gt;&gt;= _ = Nothing Just _ &gt;&gt; k = k Nothing &gt;&gt; _ = Nothing return x = Just x fail _ = Nothing So if you went through the screencasts and replaced all the `return`s with `Just`s, I think it would work just the same.
Major kudos for posting this, it's a fascinating watch and most people would be too embarrassed by the silly mistakes we all make to post something like this. At least for me, videos like this are very motivational/inspirational. This screencast is especially interesting when compared to a TDD-style screencast like http://vimeo.com/8445870 -- although I suspect his was prepared, so it's not really a fair comparison of the two methods. I don't have much to say in the way of tips/suggestions, but I think you should have laid out the top-down structure a bit more before actually implementing functions. I think you could have avoided some of the refactoring you did in the second video if you'd gotten all the types squared away first. You went all the way "to the bottom" of a single function and then switched gears into implementation, but you should have gone to the bottom of all the functions. Does that make sense? I think it would have improved the screencast if you had explained the algorithm and libraries you were going to use at the start. That is, mention that you were going to do character recognition via a simple overlay method, and that you were going to use the PGM library to handle the file IO. Anyway, thanks a bunch for making/posting this. I really enjoyed it. 
Thank you very much. Does anyone have other links to screencasts?
Thank you so much for your comments. I have been wanting to record a session like this for a long time now and finally pulled through. So glad that it was motivational for you. I was thinking a bit about data types first vs. mixing in high-level implementation details, but am not yet 100% sure how to do it better. Could you give an example? My idea was to implement it without knowing what algorithms/libraries I was going to use. I had prototyped a little before, but I think it might even work starting out blind.
Point taken. I was just responding to the "Haskell doesn't have real functors" point, not anything else.
jQuery in common usage is *close* to a monad, I think. The main problem is that its core functions hop between being map and concatMap based on the types of values their arguments return. This is, I think, a confusing flaw in jQuery. If the core traversal features were structured to respect the monad laws, I think it would be a much saner API actually. 
I couldn't have put it better. In general, I prefer using "return" because it is independent of the monad employed. Therefore function :: (Monad m) =&gt; a -&gt; m a function x = return x f = ((function 4) :: [Int]) g = ((function 'a') :: Maybe Char) will return the result in whatever monad is required in the context.
I completely agree with your point that I could set up a much more effective/efficient development environment. For one thing, I tend to be lazy in setting up my work environment (also because I only develop in the little spare time I have) and also I haven't been programming for months, so any nice automation and scripts weren't handy off the bat. Thanks for the suggestion, I'll give it a look, even though EMACS is obviously the devil's work from my rosy VIM glasses ;)
Well, in Haskell I think that's enough, because I think the polymorphic types guarantee that if fmap id = id, then fmap f . fmap g = fmap (f . g). But if we didn't have the static typing guarantees, I am not sure the second law would remain as redundant.
Well, Haskell *doesn't*. The code people writes does, but Haskell has no way of expressing functoriality, same as with monadality, which is why you always hear warnings about breaking the monad laws at your own risk. If you obey them, then you have a functor/monad/etc. but that's by conscious design not by necessity. A dependently language like Agda, on the other hand, has true functors and monads and so forth and can express the difference between merely being Mappable and being a Functor quite easily.
[Part1](http://entirelysubjective.com/wp-content/uploads/haskell-ocr-sessions-1.ogv.flv) and [Part2](http://entirelysubjective.com/wp-content/uploads/haskell-ocr-sessions-2.ogv.flv) for people willing to watch it offline.
hehe. this being my first screencast I accidentally set them up as autoplay. With 500 views and the videos at 150mb i must have caused 75gb traffic today ;) sorry for the poor sound, hope you enjoy the videos.
It's cool, man. We need moar of these. If the traffic causing you troubles, then just put it on vimeo and embed. They allow download too, it's good 'cos I usually watch at 1.1-1.3 of original speed. 
This is something we don't see everyday, since most screencasts miss the thought process part because they're prepared beforehand. Now we need some of these from the very experienced Haskell developers too!! \o/
i take your point, good sir, and I agree ;)
That'll probably have to be done in Agda. You can't do enough of the proofs in Haskell.
This was very nice :) Are there any more? Please continue this kind of thing; very educational!
You don't need Catch for this particular example; GHC already does this.
Function composition `(.)` is for situations where you don't want to name the argument. For instance, here a function that counts the number of lines in string example :: String -&gt; Int example = length . lines This is much nicer to write and think than exampleNotNice mystring = length (lines mystring) --- In contrast, function application `($)` is used for saving parentheses. For instance, the above program can be hooked to `stdin` and `stdout` as main = interact (show . length . lines) and you can remove the parentheses by writing main = interact $ show . length . lines That's because `($)` has a really low precedence, it binds less tightly than all the other operations. 
I don't feel like I've grasped them yet either. I use (.) to chain together functions, and ($) when I want everything on the right to be evaluated before It's put into a function. example: 1 + (sum ((map (*2)) [1..10])) (+1) . sum . (map (*2)) $ [1..10] This is convenient if your chain of functions on the left is long. Then you spare yourself of a lot of parentheses. Edit: thanks gtarget
($) cannot replace (.) in all cases because the associativity is different. They exist to reduce parenthesis in different situations, basically. You'll find that many higher order functions have significant overlap, and there are many ways to derive identical functions with slightly different syntax.
Consider the types. ($) :: (a -&gt; b) -&gt; a -&gt; b (.) :: (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c Or, equivalently, ($) :: (a -&gt; b) -&gt; (a -&gt; b) (.) :: (b -&gt; c) -&gt; (a -&gt; b) -&gt; (a -&gt; c) Thinking about this, the first operator, application, doesn't change anything at all. You can think of it as a replacement for parentheses, when you wish to apply a function to only one argument. The second, however, composes two functions to create a new one. So their intended use cases are quite different. As you say, however, you can use them to do similar things, and to a large extent, it is a question of the number of parentheses you wish to use. You are correct that (f . g) x == f $ g x But this is mainly because you can treat ($) as a declaration that all that comes after may be considered in parentheses, which is just a result of the operator precedence. If you want your latter suggested replacement to work, it should be written so: (f . ((+) a)) b Here, we have two functions, f and ((+) a), and we compose them. We then have to place parens around it in order to convince Haskell that we wish to apply that composed function, rather than compose f and ((+) a b). Equivalently, we could write: f . (+a) $ b This clearly delineates (again, thanks to operator precedence) that (f . (+a)) ought to be applied to b.
Fire up GHCi and look at the types: Prelude&gt; :t ($) ($) :: (a -&gt; b) -&gt; a -&gt; b Prelude&gt; :t (.) (.) :: (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c The easiest to remember is that (.) is used to make new functions from existing functions while ($) is used to *apply* functions to values. Consider the functions take 16 :: [a] -&gt; [a] drop 6 :: [a] -&gt; [a] You can use (.) to create a new function (drop 6 . take 16). This function has type [a] -&gt; [a]. You can apply this new function to an argument in the usual way: Prelude&gt; (drop 6 . take 16) "Hello functional world!" "functional" You can also use ($) to apply this new function to an argument: Prelude&gt; drop 6 . take 16 $ "Hello functional world!" "functional" Note how I didn't need parentheses this time. There are few cases in which you **can** interchange ($) and (.) freely (without also changing other things). (f $ g $ h $ x) == (f . g . h $ x) But for other cases: (f . g) /= (f $ g) A note on searching Haskell stuff: Use [hoogle](http://haskell.org/hoogle/) and [hayoo](http://holumbus.fh-wedel.de/hayoo/hayoo.html). They are specialised search engines for Haskell stuff.
The ($) function is just function application (like f x). The difference between (f x) and (f $ x) is that ($), as an infix operator, has a lower precedence than just a space. For a more detailed explanation: http://learnyouahaskell.com/higher-order-functions#function-application The section right after that is about function composition. You can't do (f . (+) a b) because (.) is for functions (a -&gt; b) and (b -&gt; c), so you would have to do (f . (+a)) b. So ($) is just a convenience so that you don't have to put parentheses around your function arguments. It can't replace (.) because sometimes you just want to compose functions without applying them. For example, blah = filter (all Char.isLower) . words -- &gt; blah "hello world HIEfdfs" -- ["hello","world"]
You are missing a ) on the 2nd line :D
Yea, the significant overlap with the higher order functions is what really confuses me.
Your last example really helps, thanks!
I did read the section, and it is slightly confusing. Especially with these two lines: &gt; Well, because $ is right-associative, f (g (z x)) is equal to f $ g $ z x and &gt; Function composition is right-associative, so we can compose many functions at a time. The expression &gt; f (g (z x)) is equivalent to (f . g . z) x in which the original expression is rewritten using both operators.
Wow, 2 replies that say that ((+) a) and (+ a) are equivalent. They are not! ((+) a) is equivalent to (a +). (+) does not have to be commutative.
Use `(.)` when you're doing function composition, and `($)` when you want the right hand side to be "evaluated" first.
The ($) operator is less like (.) and more like the application operator -- which many people don't think of as an operator because it is not represented by any symbol, but simply by putting two terms together with *nothing* in between: f x This is indeed an operator, applying the term f on the term x. It has the highest precedence of all Haskell operators and is left-associative, which sometimes requires us to write parenthesis to say what we really mean: f (g x) -- without parens it would mean (f g) x because of left-associativity or f (x + y) -- without parens it would be (f x) + y since application has higher precedence than + ($) is *exactly* the same operator, but with a very low precedence, and right-associative, which often comes in handy f $ g x -- this really means f (g x), since precedence of $ is lower than of application f $ g $ x -- and so does this, because $ is right associative and f $ x + y -- means f (x + y) since $ has lower precedence than + The (.) operator on the other hand is an operator on two functions, rather than a function and an argument. Compare their types: ($) :: (a -&gt; b) -&gt; a -&gt; b (.) :: (b -&gt; c) -&gt; (a -&gt; b) -&gt; (a -&gt; c) (.) is the well known composition circle from mathematics, and you use it to construct functions by "sequencing" two other functions, but without ever talking about the argument.
True. I was being sloppy. I even thought about correcting myself, but didn't take the time.
Allow me to throw my explanation in. First, understanding operator precedence is essential to understanding how `$` is useful. Understand why `f $ a + b` is the same as `f $ (a + b)`, and why `f . (+) a b` is the same as `f . ((+) a b)`, and things will become much clearer. The operator `$` has a lower precedence than `.`, which has a lower precedence than function application. (In the expression `f x`, you can imagine that there's an invisible "apply" operator between the `f` and the `x`, and this operator has the highest possible precedence.) That said, `$` and `.` do indeed have very similar use cases. The expression `f (g x)` could be written either as `f $ g x` or as `(f . g) x`; all three of these expressions do exactly the same thing. So, let's examine everything. The definition of `.` is the equation `(f . g) x = f (g x)`, so it should be clear that, in fact, `(f . g) x = f (g x)`. Now, because of operator precedence, `f . (+) a b` is the same as `f . ((+) a b)`. By the way `(+)` is defined, this is the same as `f . (a + b)`. If you were to apply one more argument `y`, you would get `(f . (a + b)) y = f ((a + b) y)`, which probably is not what we want. Let's look at `f $ a + b`, on the other hand. Because of operator precedence, `f $ a + b = f $ (a + b)`, and since the definition of `$` is `f $ x = f x`, we know that `f $ (a + b)` = `f (a + b)`. Similarly, `f $ g x = f $ (g x)` because of operator precedence, and `f $ (g x) = f (g x)` because of the definition of `$`. That's how everything works. What do you think?
Very clear, and many examples, thanks! I feel the LYAH section kind of glossed over the differences, and as I posted already on this post, used the exact same example to show both.
Thanks for the lovely feedback. I am planning to finish this project on screencast. I have a couple of old recordings, but they were done much less thoughtfully, so they are probably a pain to watch. I'll have a look at them to see if they are bearable. I'd love to record more, but I have only occasional bouts of programming addiction, so it might take me a little.
Notice that one example uses the word "equal" and the other uses the word "equivalent". This is significant. f $ g $ z x is really equal to f $ (g $ (z x)) because of associativity rules, meaning that the two are evaluated in exactly the same way (if you know parsing, they will generate the same AST). (We then say the latter is equal to f (g (z x)) since $ is application, but this is cheating a tiny bit.) However, (f . g . z) x is not *equal* to f (g (z x)) in the sense that evaluating those two terms will happen in different ways. They are however equivalent in the sense that they always evaluate to the same term in the end. Normal function application is left associative (see my other comment), which is partly why ($) is sometimes handy.
Two discussions on stack overflow: * http://stackoverflow.com/q/4876828/371753 * http://stackoverflow.com/q/940382/371753
Aaah, forgot about SO, thanks!
I actually found [hlint](http://community.haskell.org/~ndm/hlint/) really helpful to grasp `($)` and `(.)` Running it over my code and reading the suggestions trained my eyes to see how they can be used.
GetOptFu is my attempt at a dirt simple command line argument parser. Tutorial: http://www.yellosoft.us/getoptfu
Really? Can you explain how please? I'm afraid my GHC is on the fritz right now...
Just adding some intuition here, because others have delved into the differences in the types of (.) and ($). You can always convert a chain of $ applications like x = some $ nested $ funs $ arg with x = some . nested . funs $ arg Basically, this means that instead of nesting them all together with the invisible parens from ($), you glue the first three functions together into a single big function, and then apply the argument to the whole thing. You can't do something like x = some . nested . funs arg because this will apply arg just to funs, and then it won't be able to be composed because it will be a value instead of a function (roughly speaking. That value could still be a function, but it probably won't be the right type to fit into the composition). Only ($) has low enough precedence to wait until all of the functions are glued together before applying the argument. (.) has very low precedence as well, so normal function application always trumps it. Similarly, you can't "leave a $ hanging" when you want to leave out the argument to a function. For example, normally we would rewrite f arg = some $ nested $ funs arg into something like f = some . nested . funs in order to leave out the argument (this is called point-free style, and it makes it clearer what f is doing). But, let's say we want to rewrite this with ($). We might try something like: f = some $ nested $ funs $ But this won't parse. That's what I mean by "not leaving a $ hanging". ($) requires the argument. by contrast, (.) doesn't require an argument because it is concerned with gluing functions together into a bigger function, not actually doing the application Again, this is all roughly speaking. I've abused the use of the word "argument" a lot here for the sake of giving the intuition
You can also use ($) in some other circumstances that might not be obvious. For example (in ghci): zipWith ($) [sin, cos, tan] [1, 2, 3] 
***WANT***
You'll need a cabal file describing the package, and then if you want to put it on Hackage, you'll need an account there. Here's a tutorial which pretty much covers the basics: http://www.haskell.org/haskellwiki/How_to_write_a_Haskell_program If you get stuck, please don't hesitate to ask; that wiki page has been known to improve in response to feedback from folks on /r/haskell.
Never seen hayoo before. Any particular way this is an improvement over hoogle?
Hayoo searches all of hackage. Hoogle (the online version) only searches a small selection of packages. In practice I use both.
'cabal init' is a great way to start. 
Just remember 'cabal init --no-comments' unless you want to spend the next 20 minutes cleaning up the mess.
Many thanks for your suggestions. I have written a [follow-up message here](http://groups.google.com/group/haskell-cafe/msg/12a6551d0c351682).
TIL
First of all, I like Ur/Web and what it is trying to accomplish. What features does Ur/Web have that aren't in existing Haskell frameworks? Every single bullet point on the [main page](http://www.impredicative.com/ur/) is true of Yesod except for the client ajax bullet point which I don't understand. Yesod has not tried to do anything special client-side yet, but other haskellers have made some continuation based frameworks and modules. So I am unclear why there is a need to design a new language for the web- or I guess I am unclear as to what is actually made better by dependent types in Ur. Also I believe the choice to add xml as a first class citizen is a flaw- html/xml is for browsers and programs and possibly for designers, but I find it ridiculous to be typing in closing xml tags as a programmer. Again, I like Ur/Web and hope it succeeds, I am just hoping someone smarter than me can explain its utility vs. Yesod or other options.
Yesod is not even remotely close to Ur/Web. Haskell doesn't even have records, much less type-level record metaprogramming! You cannot really compare formlets to the record based form stuff in Ur, as they don't give anywhere near the type safety and flexibility. And the SQL support in Ur is *real* SQL, not some dumbed down ORM layer. Those bullet points might be true for Yesod, but in a much more limited way. The fact that Adam has managed to provide more type safety, flexibility while still requiring less code is a pretty good argument that creating a new language is the right way to go.
I dunno, I looked over the tutorial and it looks similar to a Yesod app. I don't see that there is actually less code for basic things. Can you show some example code where Ur forms give more type safety or flexibility (other than in presentation) than Yesod's forms or digestive-functors? Definitely the ability to use raw sql is very nice. The downside with Ur is there is only direct support for SQL, whereas one can use a very similar syntax in a "dumbed down ORM" with other non-sql backends like MongoDB. And of course the bigger picture is Haskell already has bindings to many other data stores. Being forced to shoe-horn every type of data into SQL isn't something I want to deal with anymore. Maybe Ur can have a Haskell FFI?
:set -Wall
&gt; the SQL support in Ur is real SQL I am using yesod, and i code exclusively raw sql in it. I do not use any ORM, dumbed down or not. So i do not really understand this "advantage". What am i missing ? 
Static type safety? Or do you have a system to guarantee the sql is well-typed and produces values of the types you think it does?
Do i understand correctly that you have to generate sql mappings first for that to work, akin to HaskellDb ? 
Well, all you do is specify the tables, and what columns they have. So I think a lot less work than haskelldb. But Ur/Web generates the schema for you, and I don't think there is any way (right now) to use an existing schema.
I wonder how this compares to the approach taken by PGOCaml - which is to check the SQL at compile time by using the database software. That means it has to compile on a machine with access to the correct schema, but that isn't too hard to do. 
There is also my own question: http://stackoverflow.com/questions/3030675/haskell-function-composition-and-function-application-idioms-correct-use
Here's an interesting fact: Prelude&gt; :t (map $) (map $) :: (a -&gt; b) -&gt; [a] -&gt; [b] Prelude&gt; :t map map :: (a -&gt; b) -&gt; [a] -&gt; [b] 
Reading that title while 1/4 drunk makes me feel 3/4 drunk.
Haskell's what?
Unfortunately, the process is not reversible.
But, but... functions aren't the only instance of Arrow!
It's different. GHC has its own calling convention, and uses a very different set of idioms than you would expect from a traditional library. I don't recommend looking at GHC-produced assembly unless you also have Core, STG and C-- at hand. Fortunately, these are pretty easy to produce during compilation.
What in the.....I don't even know how to....well, wait...I mean....WHAT?!?!?!?
Reversing binaries -- malware, in particular -- is a pretty big part of my job. C is pretty easy to reverse engineer. C++ is slightly more difficult because of virtual function dispatching, but still is pretty easy. I haven't spent as much time looking at GHC output (only dabbled out of curiosity) but I thought it was more difficult to see what the code was doing. Of course, like anything else it's got idioms it employs, and knowing these idioms speeds up the process. If I were in your position, I would advise that hoping for obfuscated bytecode is a pointless endeavor, but if someone wants an answer, GHC's output is more difficult to follow than gcc or MSVC. I'd also be quick to point out that malware authors, who are /very/ concerned with deploying obfuscated code, still can't hide what they're doing.
 BlogTitle:1:18: parse error on input `are' Failed, pages loaded: Reddit.Haskell
Sorry, but your assertions about what the compiler will do are wrong. GHC does not compute the result of a function at compile time (except for some type-level functions).
Well, it might compute a call at compile time if it decides to inline the function. Just like a C++ compiler might. 
You can download the PDF for free
[Relevant comic](http://ro-che.info/ccc/12.html).
GHC by default puts a crapload of (moderately human-readable) symbols into binaries, and stripping them will typically reduce the binary size (and reversibility) significantly. Once the symbols are gone, I think Haskell is mostly pretty reverser-proof right now because it looks like nothing most reversers are used to reading, and there's so much indirection for everything. People get frustrated with C++ because of the virtual function calls, and GHC's output has way more of those indirect function calls. Also, most tools that reversers use make assumptions about generated code (stack layouts, function preludes, etc.) that GHC doesn't follow. I don't think it'd be impossible for an experienced reverser to figure out, but they might run away screaming before they get down to understanding it. Also, the intersection between the binary analysis community and the people who have heard of Haskell (and especially those who use it regularly) is tiny, sadly. 
The rule is: If they have access to your binary, they will figure out what it does if they really want to.
jhc's assembly output might be easier to read than ghc's.
I really think if you don't want people reverse-engineering your code, the solution should be legal, not technical.
Thanks, I tried and it does work. Fun! I should integrate GADTs into my work more...
Whoah, small world. Reversing binaries -- malware, in particular, is a pretty big part of my job too :-P And I love Haskell! GHC moving to LLVM stuff aside, I think if I wanted to reverse GHC's normal platform-specific binaries, I would take a look at GHC's mangler perl script for those platforms it supports, and read up on the internal binary structures and calling syntax, to get a good delta from what is already well known (i.e. GCC-styled C binaries).
Is there a place to submit corrections/suggestions? It's possible this is addressed later in the book, but the `maxPitch` function on page 53 return `(C,0)` if all the pitches in the list are lower than `(C,0)`. A better approach might be to make `maxPitch :: [Pitch] -&gt; Maybe Pitch` and define `maxPitch [] = Nothing`. Edit: Nevermind, it's addressed a bit later when talking about different types of folds.
&gt; I would take a look at GHC's mangler perl script God be with you.
You have to be a bit careful about identifiers ending up in the binary, or that the business logic ends up in tables which do not actually need disassembly. (That's relevant to C++, too.) GHC does not use the hardware stack, so a stock disassembler/decompiler will be very confused. That and the different evaluation model are not major obstacles, merely deterrents, but it's not as trivial as decompiling Java. One thing that makes me wary of putting Haskell code in the field is the lack of crash dumps.
By lack of crash dumps do you mean the equivalent of a core file for a C program? That I could use that core file and gdb to diagnose a problem?
The mangler is gone since -fvia-c has been dropped.
Oh. I feel a little empty inside now.
&gt;If they have access to your binary In other words, don't distribute binaries; execute the code on your own servers if you are super worried about this. The only way to *truly* obfuscate your code is to make it so even the machine can't understand it; but that sort of defeats the purpose of programming.
Oh? I didn't know that had been dropped as a method for compiling. Definitely for the best, though.
Yeah, I decided I didn't like that either. I've already changed it so it asks you interactively at the end if you want explanitory comments (defaults to no). I hope that''ll make it useful for first time users who do want the comments but not annoying for the rest of us. I'm also drafting a new quickstart guide as part of a new Cabal user guide. http://code.haskell.org/~duncan/cabal/user-guide/developing-packages.html#quickstart Comments/feedback welcome on the quickstart part (the rest of it is still very much draft so best ignored).
The MiniDump functionality on Windows works just the same for Haskell binaries as it does for C binaries. Isn't the same true of core files in Linux?
If pressed, I probably wouldn't say that I thought I was the only one, but part of me kinda did believe that :) Are you going to be at DEFCON? We should talk shop.
Just to add to that, I've done a lot of reverse engineering in the past and am still fairly involved in at least a couple of related communities. What I've gathered from it is that most reversers are pretty mediocre, and do most of their reversing by extrapolating from bits and pieces of human-readable information left in programs. Even experienced reversers will be hugely relieved if the information is still in the program. For example, many companies (including certain fruity ones) obviously use macros that include the path, function name, and line number of an error message. So because of this, even if they strip binaries of symbols, the error messages (even ignoring the actual message, which itself is also hugely valuable) contain a treasure trove of information on the original structure of the program. We can then go into IDA and write a quick script to harvest the strings with a regex and name all functions with error calls automatically. Once you have names, things generally make a lot more sense. One of the reasons the jailbroken iOS hombrew development community is so active (to the point of several people actually making very good livings writing "tweaks" for it) is that it's really easy to reverse the vast majority of programs on iOS, due to their use of Objective-C. As a smalltalk-like messaging language that uses string-based messages (called selectors), most programs are basically forced to leave all their function (fine, method) names in their programs, because dispatch can be dynamic. Even better, due to highly dynamic nature of Objective-C (and some questionable early design decisions that nobody at all except for the reversers probably takes advantage of now), all these nicely named methods also contain type signatures (often fairly detailed, sometimes even including struct definitions). Again, the binary and runtime structure of Objective-C prevents any of this from being removed from the program, so it's more information for a reverser to incorporate into his knowledge about the program. They even have tools that dump complete and valid header files for any Objective-C-compiled binary, using this information. The approach employed by some clever obfuscators on Objective-C is to post-process (even just by hashing) method names to be garbage. The garbage method names will still be in the binary, but any human trying to read it will have nothing to rest on. Even with other strings and error messages left in the program, I suspect that simple step would kill 90% of the developers writing software for jailbroken iOS right now. I've experimented with obfuscating C with a few simple functional techniques, like doing pseudo-CPS on my functions (nested functions don't work on Apple's ARM compiler, but they have newfangled block things that do, and make it fairly pleasant to program in that style). The general reaction from many experienced reversers I know contained many expletives. Even the kinds of reversers who know what to do when there are no strings in a program generally haven't come across functional languages and concepts like the lazy thunks (almost no constant function branches/calls anywhere in the program) or CPS used by Haskell compilers. Anyway, I guess this long rant is to show you that there are two main categories of reversers in my experience, and the larger one would be mostly clueless if there were no strings in the program (and given the indirect representation of everything in Haskell, even strings might be hard to track back to their use location). The other one doesn't intersect much with the PL crowd (I've recently gotten a few of my reversing buddies into Haskell and they love it, so that may change), and is thus ignorant of how functional languages work and are compiled. **tl;dr**: I wouldn't worry too much about people reversing compiled Haskell for now. A few people can probably do it, but they're few and far between, and probably have better things to do.
I must admit I've never used the functionality, but the [GHC man page](http://linux.die.net/man/1/ghc) does seem to make mention of core files' existence.
Of course I'm going to DEFCON, Mr. Clone-Of-Me. I wouldn't mind that at all :-)
The core file is just a bit difficult to interpret, even for bugs which do not involve memory corruption or race conditions. There aren't any useful debugging symbols, either. That's the flip side of confusing disassemblers.
A cabal package? Or a repo of packages for haskell/*.hs? 
I remember that HackageDB 2.0 should make HackageDB "more social" and thus easier to find high-quality packages. Does anyone know the current state?
The approach you mention doesn't work for library functions meant to work properly for a variety of databases. Ur/Web supports type-checking such functions in isolation, which provides guarantees that they'll work correctly for any parameters.
Ur/Web has a C FFI which is fairly pleasant to use in implementing interfaces to other database systems.
The key advantage of Ur over Haskell is a more expressive type system which facilitates statically typed metaprogramming. For instance, consider how you might implement &lt;a href="http://www.impredicative.com/ur/demo/crud1.html"&gt;the online CRUD demo&lt;/a&gt; in Haskell. I think it would be much more painful, and would probably require overlapping instances.
I remember reading in Johan's post about what a good GSOC project is that this project was indeed completed but not deployed. By "this project" I mean the rewrite of hackage 1, not the social features.
At the very least it needs to be upgrade to Happstack 6 (I have a patch that does 95% of the work for that). It should also be migrated to acid-state (from happstack-state). That is pretty straight-forward and will make optional components a lot cleaner to work with. Both these tasks could be completed in a day -- they are not significant obstacles. Not sure what needs to be done feature-wise.
I don't think "what" is a problem, but "who".
**Abstract** We make monadic components more reusable and robust to changes by employing two new techniques for virtualizing the monad stack: the monad zipper and monad views. The monad zipper is a higher-order monad transformer that creates virtual monad stacks by ignoring particular layers in a concrete stack. Monad views provide a general framework for monad stack virtualization: they take the monad zipper one step further and integrate it with a wide range of other virtualizations. For instance, particular views allow restricted access to monads in the stack. Furthermore, monad views provide components with a call-by-reference-like mechanism for accessing particular layers of the monad stack. With our two new mechanisms, the monadic effects required by components no longer need to be literally reflected in the concrete monad stack. This makes these components more reusable and robust to changes. ** Authors ** Tom Schrijvers Bruno C. d. S. Oliveira **Publication Date** 2011 
Last I asked Gracenotes on #haskell, it was something like he was too lazy/busy to iron out the details necessary to switch the live server.
How about: https://github.com/dpp/LispHaskellIPad Quote: "A simple iPad app that demonstrates using Haskell in an iPad app. The Haskell app is a Lisp interpretter."
&gt; I have a patch that does 95% of the work for that Oh, please send it to me.
I'm doing some hacking on it at the moment so that people can use it as an in-house mirror server (getting live mirroring working properly). There's also plans to do some work on it at the upcoming [Hac_φ](http://www.haskell.org/haskellwiki/Hac_%CF%86). I believe they're going to concentrate on testing and the migration path.
After my one day seminar on Agda. Part I-II: complete rewrite, Part III is improved, Part IV: started. More to come: Complete part IV, and some insteresting stuff in Part V (Programming Practices in Agda) 
Localhost is not the best place to link to.
Thanks! The correct link is [http://pnyf.inf.elte.hu/fp/Overview_en.xml#agda](http://pnyf.inf.elte.hu/fp/Overview_en.xml#agda) I am wondering what to do now..
After my one day seminar on Agda. Part I-II: complete rewrite, Part III is improved, Part IV: started. More to come: Complete part IV, and some insteresting stuff in Part V (Programming Practices in Agda)
I sumitted it again with the right link.
How could you see my post? I cannot find it among new posts (neither this one nor the corrected one).
I caught it before it got buried for having a bad link :)
I don't see it :/
Looks great! Might it pay to mention that your Dyn type is effectively isomorphic to Unit?
Very nice! I think it'd be worth mentioning that your `Dyn` type is effectively isomorphic to a unit type. Also, in the section on coinduction you say `#` is data (in the "Phantom Element" section, not at the top), when it actually is what you say earlier on the page.
... still doesn't have automated uninstallation of packages. :{
Everything Jeremy Gibbons writes is really interesting. 
Related work on reasoning about monadic Haskell programs (not mentioned in the above paper): * http://www.diku.dk/hjemmesider/ansatte/stovring/papers/icfp07.pdf * http://www.iai.uni-bonn.de/~jv/icfp09.pdf
cabal-dev relies on cabal for all the heavy-lifting, so there are some things that are out-of-scope of cabal-dev; you could think of it as a UI layer that is specially crafted to cater to common development tasks (although sandboxing builds can be beneficial for application deployment as well). That said, it probably wouldn't be too difficult to also wrap ghc-pkg unregister. I'm not clear on the use case for this, though. (*edit:* and I'm not sure the cost-benefit is worth it, with `cabal-dev ghc-pkg`) Let's discuss it on the ticket you created. (if anyone else is interested: https://github.com/creswick/cabal-dev/issues/27 )
Resubmitted [this](http://www.reddit.com/r/haskell/comments/it67k/rewritten_agda_introduction) because the original got buried somehow as spam, I think. The odd URL is because reddit successfully recognizes that this link has already been submitted, and suggests I refer instead to the buried other submission. I can click "post anyway" but it ignores me.
Good remark :) I kind of knew that because one cannot pattern match on elements of Set. I should find a better (and still simple) example for existential types...
Thanks, godofpumpkins!
Summary: Part I-II: complete rewrite, Part III: improved, Part IV: started. More to come: Complete part IV, and some insteresting stuff in Part V (Programming Practices in Agda)
Good remarks :)
You could mention that Exists Nat (Vec A) ~ List A :)
Thank you! I really like the format of this. Very schematic and lots of examples :)
Nice! One of my excuses for not spending more time learning Agda is that none of the "tutorial" level material actually matches the current language implementation. I've been told the differences are minor, but when you're a newbie and you don't know what yellow means in Emacs or why you should care, how to remove it, etc. it can be a total derailment. Contrast this with competing languages Coq and Isabelle. Coq is easy to install and several books that are still relevant are written about Coq. I'd much rather learn Agda than Coq, but boot strapping Agda has proven much more "debugging" intensive. I hope your tutorial changes that.
On windows to use -fllvm you need to [download some binaries](http://hackage.haskell.org/trac/ghc/ticket/5170#comment:2). Not sure about targeting iOS, seems tangential.
I'm a fan of both systems (which are ultimately rather similar on the language side) so I have to ask: why would you much rather learn Agda?
...you can just message the mods to get it unspammed.
I have no idea why people like this... it's needlessly complicated, hard to use, and lacks few important features.
maybe use cab?
If you could reformulate this as concrete problems (why is it complicated and hard to use? which features are you missing?) your feedback would be useful to the cabal-dev developers.
Corrected! (Maybe you have to reload the pages)
News: I added the missing sections! (Maybe you have to reload the pages.) Well, it is far from ready but it is somewhat closed now. 
Added, thanks for the remark!
I'm writing an alternative to cabal-dev, I'll probably release something next week.
I am what I consider to be a practical programmer - working with objective C. Haskell seems close to something usable for this end (iOS), but I wonder what this is usable for.
zzing, there is a chance that we will have a good Agda2 compiler sooner than it happend with Haskell.
What do current Agda compilers compile to? Do they go through Haskell? If they do go through Haskell, do they not lose some of the optimization benefits -- throwing away knowledge of impossibilities that GHC later has to consider unnecessarily, even at runtime?
Have you looked at who they are? Have you heard from any of them in recent history? :)
OK, now I added more content! (Maybe you have to reload the pages)
Speaking only for myself, I use it because it solves serious problems I'd otherwise face in Haskell development. By using cabal-dev, I can be sure that building one project won't break my builds on something unrelated. I certainly don't find it needlessly complicated at all; it's actually all rather simple... really, replacing 'cabal' by 'cabal-dev' is all there is to it (though the -s option is also nice, if you're working with more than one related package)
I am glad to hear it, but I don't really understand what practical things Agda is useful for now. I love esoteric languages - especially when they become practical for work.
Agda has a really simple type system with only a few constructs and rules, GHC with its extensions is much more complex. If you use the GHC type system extensions, then learning Agda helps you to understand them - most of them are special cases in Agda.
Agda has now an [Epic](http://www.cs.st-andrews.ac.uk/~eb/epic.php) backend. Epic was developed exactly for these kind of languages, it has already a few intresting optimizations.
The original compiler through Haskell compiled by sticking an `unsafeCoerce` around every single expression (nested all the way down) so it was using Haskell as an untyped core, basically. The Epic backend looks interesting though. There's also a new javascript backend (yeah, weird, I know) and someone hacked that up to be a ruby backend too. Fun times.
Or you could just contribute to it and provide constructive feedback, instead of reinventing the wheel and providing unpleasant, useless feedback. Or if you really don't want to contribute to it, at least tell the authors what's wrong about it instead of just bitching. 
Better integration with Haskell (as far as I can tell, obviously I'm not counting extraction which seems to not work well in any system) and much nicer syntax.
"While there have been some fantastic contributions to wxHaskell over the past years from any people, the reality is that most of the work gets done by me" I don't know whether this would apply to Haskell projects too, but the guys from the D programming language experienced a rise in contributions by 10 times, after they moved to GitHub.
That sounds interesting, but the set looks like it gets complicated.
I'm sorry you've found it complicated - there *are* some rough edges with respect to sandboxing some of the cabal commands (`cabal-dev configure`, for example, has lead people to problems - `cabal-dev install` is usually what you want) If you have time to share details about what you don't like, I would be happy to see what we can do to make it better.
`cab uninstall` also invokes `ghc-pkg unregister`. Cab does do a nicer job of reporting transitive dependencies (and offers a way to remove them all), but I'm hesitant to incorporate another application for the feature -- checking for compatibility with external executables is Very Hard. A Cab library would make this much more palatable, since we would know that the API isn't likely to change after compilation... similarly, cabal-dev, and cabal-install library APIs would also be nice. Maybe someday :)
&gt; Or you could just contribute to it I'm working on something that serves the same purpose, uses similar tricks but is meant to be used in a different way ( https://patch-tag.com/r/Paczesiowa/virthualenv/home ), tools like cabal-dev are so easy to implement, that there's nothing wrong with reimplementing the wheel. &gt; instead of just bitching. I had a bad day and I let my inner douchebag come out and play. sorry.
I tried it with 2 of my projects: * one didn't work because of configure flag (seems fixed in 0.8) * the other didn't work because (I think that's the reason) it's parsec and cabal-dev ghci always picked global parsec. what I really hate is that cabal-dev is an application with multiple submodes and multiple command-line switches - it is complicated! this ( https://patch-tag.com/r/Paczesiowa/virthualenv/home ) will serve the same purpose as my (desired) usage of cabal-dev with a single command without any switches. another thing, that makes this unusable for me, is this: from what I understand the flow is supposed to look like this: edit code in an editor -&gt; cabal build -&gt; fix type errors -&gt; cabal-dev install -&gt; cabal-dev ghci -&gt; play with the code. it's just unacceptable for me, when I'm working on the code in emacs, I want to edit code, hit C-c C-l (or something like that, my fingers remember that) and fix-errors/play with the code immediately.
That's a good point. By the way, [reactive-banana](http://www.haskell.org/haskellwiki/Reactive-banana), my FRP addition to wxHaskell (and other GUI libraries), is already in a [good shape](http://www.haskell.org/haskellwiki/Reactive-banana/Examples).
I think that you will find these particular objections all covered by the 0.8 release of cabal-dev, with the exception of the complexity of the codebase. The complexity stems from the reality that people have different needs, and a build tool has to support those needs. We have tried hard to make cabal-dev do the sensible thing by default, but to provide an escape hatch when our notion of sensible does not match yours. In particular, cabal-dev ghci no longer requires you to install the package before using ghci, and should have the precise behavior (selecting the appropriate packages) that cabal-dev install does. We did not run into these problems when we initially developed cabal-dev because we developed it to help with keeping builds on a build farm separate from each other (automation), so we didn't heavily test the interactive workflow.
Extraction does seem to be better handled by Agda, though as there is only one (well, technically infinity, but you get my point) universe of sets you cannot erase computationally irrelevant information as easily (though i believe that you can mark arguments to be erased in each case). As for the syntax, I believe that tactics more than make up for that, though I can't imagine why we couldn't have the best of both worlds in the future...
I can understand you, I feel the same. Complicated things exist in any system. Agda tries to solve a hard problem, so it is necessary complicated. On the other hand, I belive that anything can be explained better, and my explanation was not enough in this case.
&gt; though I can't imagine why we couldn't have the best of both worlds in the future... In Agda? In Coq? In something entirely new? Which current language seems to have the most promise in terms of growing into this ideal?
I wish I knew. It doesn't look like Agda is getting tactics anytime soon, but the Coq people are becoming more interested in having the powerful dependent elimination of Agda, and more powerful notations (though the current system isn't so bad, you can even define [recursive notations](http://coq.inria.fr/coq/doc/Reference-Manual015.html#@default773)). I'm not sure who the big shooter for dependently typed programming is going to be though. Epigram 2 is just starting to gain ground for instance...
It is interesting what I like and what use in the programming field. I dislike the idea of Python's dynamic typing because of what can happen at runtime with type errors and other similar issues. I love the syntax of Haskell, but find some of the stuff to be rather difficult because of the plethora of concept and lack of mainstream GUI libraries I can use on the mac and iOS. I end up using Objective C for its dynamic nature coupled with a good amount of static typing. I would really like to see something like haskell or even better work for iOS and similar platforms.
Hate to say it, but the first step might be to get it to build reliably. As of right now, it fails to install with 'cabal install' with some missing symbols in a C++ source file. I've tried installing it a half dozen or so times in the past, and every time, it's been failing with some build error or another.
So I'll take you up on that... I don't have a problem with cabal-dev, but one thing that would make it even easier is if it could either: 1. When -s is not specified, search in an inside-out for a parent directory with a child called cabal-dev before it decides there isn't one, or 2. Let me add a dot-file in the current directory with options like -s in it. (Or... maybe this already exists? I looked some time ago)
Just so I'm not leaving an unhelpful "there's a bug" comment, at the moment, it's src/cpp/eljartprov.cpp: In function ‘void PushProvider(wxArtProvider*)’: src/cpp/eljartprov.cpp:54:2: error: ‘Push’ is not a member of ‘wxArtProvider’ src/cpp/eljartprov.cpp: In function ‘bool PopProvider()’: src/cpp/eljartprov.cpp:63:9: error: ‘Pop’ is not a member of ‘wxArtProvider’ src/cpp/eljartprov.cpp: In function ‘bool RemoveProvider(wxArtProvider*)’: src/cpp/eljartprov.cpp:72:9: error: ‘Remove’ is not a member of ‘wxArtProvider’ But like I said, it's been something every time I try.
This is a serious question (from somebody being rather ignorant). What is the appeal of github? I mean, any time I click to a github link, I'm confronted with a set of source code files in a browser, without any further information. Hackage with the Haddock docs looks infinitely many times better and useful to my eyes, so I really would like to understand what's the drive behind all the github hype.
What's the reason for defining `_≡_` with x as an argument to `refl` as opposed to the more familiar definition? data _≡_ {A : Set} {x : A} : A → Set where refl : x ≡ x
Github and Hackage are completely different things. One is a place to put source controlled repositories and the other is a package database.
Github's appeal is that it's github -- it's reached critical mass. Edit: to elaborate, dvcs hosting sites have become a bit like social networking sites, so the value of the site increases with each new user. Eventually one of them gets enough people that its value is far higher than any of its competitors. Github's there, at least for now.
I definitely agree that I'd rather see a link to hackage than github when I'm finding out about a new library; especially since Hackage displays the link to the repository if it's there in the package description, while github projects either don't link to hackage or don't put the link in a standard place. As for the appeal of github, it's that it makes it easy for someone else to create their own copy of a project, make and test changes, and then request that the main project pull their changes. And for that matter, even if the original project doesn't pull your changes, you can tell other people about your own copy. Having your own local changes in a place that's easier to talk about and browse and discuss than a patch is convenient, and a lot of people are rather shy about writing project maintainers to ask them to pull changes behind the scenes, but are okay with just throwing up their change and sending a pull request.
Scroll down to the README. I often find github READMEs to be more useful than Haddocks. I would like the ability for hackage to show a README. Github is basically a great GUI for source code. But not just yours, everyones- which means your changes always have visibility even if they aren't taken in by the project maintainer, and the maintainer can look at any changes being made by others.
What would need to be added? What's wrong with [this](http://hackage.haskell.org/package/fclabels), for example? It is of course unusual to see that level of documentation on Hackage, but I don't think it's a technical problem with Hackage, it's a cultural problem. One that strikes me as slowly, but surely improving.
Hackage/Cabal need to support an external README markdown file is all. Michael and I tried to put in an example description of hamlet syntax but found it to be impossible because of the way things get parsed. Moreover, a cabal file is a very poor place to put a large piece of documentation.
"Yes it means there's no dynamic dispatch, but I don't miss it much." Actually, there is dynamic dispatch if you want it; it's called existential types. But it's used vastly less than in OO languages, because compile-time genericity provided by type classes is good enough most of the time.
I'd suggest a different approach. Not only is it a pain to put a lot of documentation in the package description; it's also probably not the kindest thing to do to your users. You want to answer two things in the package description: 1. What is this package? In a bit more words than the synopsis, what does it do, and when might I be interested in using it? 2. Where do I start looking for more information? Typically this is a top-level module containing documentation comments. By putting this all on the front page, you make it harder to find the other stuff the user might care about -- mainly the links to individual module documentation they are looking for. As a user, I'd rather see documentation in a top-level module than in the Cabal description field anyway.
tl;dr: Haskell doesn't have all the annoying stuff that other languages have.
There are two things here. The first is that I have seen two definition of _≡_ and I liked the second one better (I think there is no semantic difference, and no difference in usage): data _≡_ {A : Set} : A → A → Set where refl : {x : A} → x ≡ x The second is that I made the first parameter of refl explicit and I made some other parameters explicit in other examples too, just because of the following explanation (it is more obvious to see the types then). Anyway, implicit arguments don't change the semantics.
Here is the new link: - http://pnyf.inf.elte.hu/fp/Overview_en.xml?fuckyoureddit#agda At first, I read "Fuck your edit" and I thought "But I haven't edited anything!". 
That's great, but I agree with those saying that the .cabal file is not the right place to put this information into. I usually put this to the topmost module (actually it never occurred to me that I can put such a long description into the package description), which is unfortunately somewhat hidden from the user (they have to discover it). So basically I have to agree that a README with haddock markdown would be great.
Are you sure that you have a development version of wxWidgets 2.8 installed?
what he calls "fun" i call basic requirements for sanity, but yeah, that's about it.
Almafa, if i just want to use a package, i completely agree with you: HackageDB is a much better place. Sites like GitHub (Bitbucket etc might be as good) make it very easy to contribute code/documentation to projects, to explore projects, to tinker with projects and to keep in touch. I think any project can benefit from a very low barrier for contributions, especially projects who need some attention, like wxHaskell.
Many of these terms would be called "Design patterns" in other languages. It's often easier to talk about things by using well-understood names. Like "let's use a Facade" instead of "lets write something that hides how things down there are implemented". Other features just don't exist in main stream languages, so there is no wonder you don't have to deal with them there. Haskell has a much stronger background in Academics than most other languages. You'll find many people who are interested in the theoretical foundations, exploring and talking about those concepts. But don't get the wrong impression: you can successfully write Haskell code without using those terms or understanding where they come from: * List is a functor, but who cares: there is just this handy "map" function. * IO is a monad, but who cares: with "do" you can write I/O almost like you would do in an imperative language. * Currying... who cares: you just can create new functions by passing less arguments to other functions. 
Thanks again. Could you also submit my new post about [economic Haskell interpretation](http://pnyf.inf.elte.hu/fp/Economic_en.xml)? I can't see it after posted. I ask also the moderators about the situation.
I read "Learn You a Haskell for Great Good" as well as other books, but felt that one in particular made many of these concepts relatively easy to understand in just the fashion you meant. I think the OP just went to the wrong sources, because the Haskell community makes it somewhat hard to find simple explanations of their terms.
I don't see the problem, the Haskell concepts you mention are entirely natural to me. In contrast, other languages use technical mumbo jumbo that is really undecipherable. For instance, here a list from the [C++0x wikipedia page](http://en.wikipedia.org/wiki/C%2B%2B0x): EDIT: Removed Haskell terms from the list. * Initializer lists * Uniform initialization * Range-based for-loop * Alternative function syntax * Object construction improvement * Explicit virtual function overrides * Finalized classes and virtual functions * Null pointer constant * Strongly typed enumerations * Right angle bracket * Explicit conversion operators * Template aliases * Unrestricted unions * Identifiers with special meaning I mean, honestly, this is really too advanced for simple, everyday programmers familiar with functional languages. What the heck is "uniform initialization" and why would I care about it? 
&gt; Currying... who cares: you just can create new functions by passing less arguments to other functions. That's [partial application](http://www.haskell.org/haskellwiki/Partial_application), not currying. [Currying](http://www.haskell.org/haskellwiki/Currying) is the transformation of a function that accepts a tuple of n arguments to a function that accepts 1 argument and returns a function that accepts n - 1 arguments etc. curry :: ((a, b) -&gt; c) -&gt; a -&gt; b -&gt; c
"iterators or generators or whatever else that so many imperative languages have. That's all gone." To be replaced by Iteratees or Enumerators.
Beautiful and elegant example. Thank you.
Your attempt at facetiousness is rather hampered by haskelisms like ‘Type inference’ and ‘Lambda functions” appearing in your list. (Though, on that note, the term ‘Lambda function’ annoys me somewhat. ‘Anonymous function’ is rather more approachable.)
I believe he is referring to laziness here, which does replace many uses of iterators/generators.
thx. I will probably also post these videos on vimeo in the future. Didn't know about the speed control, great tip! Jonas
Learn you a Haskell should get an award for decrypticing those terms. I generally ask everyone to read that book, if nothing else. But its the only one of its type.
You are right, but the real world benefit would be partial application. In the same sense Monads and Functors are much more than i have presented it.
Um, except that some of those should not be on that list, because they're also terms used in Haskell.
Yes as I said, language design is complicated with lots of research behind it. You have looked at the wiki, but look at how community is writing about it. for eg [Useful New Features in C++0x](http://drdobbs.com/cpp/231002092) and look at the headings of topics being introduced: *Let the Compiler Figure Out Types, Extracting Type Information* : being used to describe type inference *Streamlining Iteration* : for Range-based-for-loop *Initializing Variables' Contents* : Discussing Initializer lists... And above that, in the summary the author actually sums it nicely by saying its absurd to understand everything in the spc. However, with haskell, its the opposite. People are expected to know each and every language feature by heart.
I understand what you are saying, that's how it should be. Btw, I don't think design patterns as a right term, maybe language patterns. I think design patterns are language agnostic tools, although tradational patterns are too too stateful to be used in functional programming.
you fall for the same illusion like everyone who turns away from a language because it sounds scary. and you seem to draw the same conclusion: that these things could be named differently. and consequently that they are made to discourage, in some form of academic elitism. the fact of the matter is, it is elitist in a certain way, but not the way you think. these words are the names that most accurately capture the concept being named. all the examples you give are either a) very important and thus deserve a proper name *and* being learned about, or b) not that important for your everyday code, no less deserving of a name, but you might as well ignore them for the time being. IMHO (YMMV!), category a) includes monads, currying, combinators and closures, with the rest in b). you say yourself that it took you only 15min to grasp currying, not surprisingly because it's not a complicated concept. but somehow the fact that it carries a rather arbitrary name puts you off. i say, don't be put off by names! so the elitism is this: haskell takes pride in it's concepts and dares to name them accurately. and in that it caters to the elite who will not be scared by unfamiliar names. so your illusion is this: that the names are there to scare those who don't know them, when in fact they are simply there and (at worst) scare those who are not willing to learn them. NB. none of these names come out of thin air. they are not arbitrary complications. they have origins and contexts. in fact it would in a way be a severe complication to name everything *differently* when these names are the most natural in their respective context. NB. your example about compile time polymorphism: "one name, many defns" is not the same thing. "compile time polymorphism" very clearly captures the concept. the chapter title you desire simply describes what you *do* in order to get it. while that may be good for a heading or something, it's not suitable for general intercourse, because it is bulky and imprecise. this is the very reason we make names for things. and that is why i say you should not refuse to learn new ones.
The message on the mailing list is from "Karel Gardas".
Function, variable, object, instance, method, member, argument, iterator... For someone new to programming all of these words must seem pretty daunting, but once you know them, you couldn't imagine talking about imperative/object-oriented programming in any other terms. The real problem is the lack of entry level materials that cover these topics well, one that LYAH has gone a long way towards filling. (moar plox bonus). Also often the best explanations of some topics are journal articles, which I can imagine being offputting to those outside academia.
To be honest I think we should all just be happy it's not usually referred to as Schönfinkelling.
That's really cool. Not sure how it works though, I thought Heroku was for web applications written in Ruby?
On another note, would it be possible to separate your reactive-banana and reactive-banana-wx into separate repositories?
I too would like to see many Haskell projects move to github/bitbucket
But I want to see a full overview in one place and not every package lends itself well to having one true top-level module.
Their new cedar application stack is much more flexible- you can potentially run almost anything. You can only *install* things for supported languages. But with Haskell you don't have to install anything if you staticly link.
Well, I had 2.6 installed rather than 2.8; but when I install 2.8, I get a new set of errors in C++ source files; a good 10 to 15 pages of them this time... the first few: In file included from /usr/include/wx-2.8/wx/generic/imaglist.h:16:0: 0, from /usr/include/wx-2.8/wx/imaglist.h:48, from src/include/wrapper.h:34, from src/cpp/apppath.cpp:1: /usr/include/wx-2.8/wx/icon.h:48:1: error: ‘wxIcon’ does not name a type /usr/include/wx-2.8/wx/icon.h:48:1: error: ‘wxIcon’ does not name a type /usr/include/wx-2.8/wx/icon.h:48:1: error: ISO C++ forbids declaration of ‘object’ with no type In file included from /usr/include/wx-2.8/wx/imaglist.h:48:0: 0, from src/include/wrapper.h:34, from src/cpp/apppath.cpp:1: /usr/include/wx-2.8/wx/generic/imaglist.h:39:5: error: ‘wxIcon’ does not name a type In file included from /usr/include/wx-2.8/wx/window.h:23:0: 0, from /usr/include/wx-2.8/wx/generic/splitter.h:15, from /usr/include/wx-2.8/wx/splitter.h:44, from src/include/wrapper.h:36, from src/cpp/apppath.cpp:1: /usr/include/wx-2.8/wx/cursor.h: In constructor ‘wxBusyCursorSuspender::wxBusyCursorSuspender()’: /usr/include/wx-2.8/wx/cursor.h:65:22: error: ‘wxIsBusy’ was not declared in this scope /usr/include/wx-2.8/wx/cursor.h:67:26: error: ‘wxBusyCursor’ has not been declared 
Oh! I just realized this, and it might help; apparently the normal version is building fine, and I'm only getting these errors when it goes back and builds the profiling version.
**I love haskell. But this story is all spinning, no truth.** &gt;Think about the whole thing with iterators or generators or whatever else that so many imperative languages have. That's all gone. No, they are not. Because you don't want to trash your memory, you better pick correctly between foldr and foldl. Pick the correct 'sequentiallity' if you will. &gt;Select vs. threads vs. coroutines is also gone, but that's just a ghc feature. All that flap about asynchronous events and event based programming? Gone. Instead you get a bunch of competing concurrent frameworks, that do not interact with the rest of API or library ecosystem. Same shit, different day. &gt;Think about that whole null pointer thing that all these imperative languages have. Null pointer exceptions? Gone! Sometimes a value really needs to be a 'maybe'. Then we do a fromJust because in a specific context we know it's always going to be "Just" right? Then we get a pattern failing. Just another word for a "null pointer exception". Which is just another word for a 'run-time typing error'. &gt;Think about that whole thing with reference vs. values. That's gone. Or think about the thing with identity vs. equality vs. deep equality, that's gone too. Yes, you only get deep equality. And only if you define it (semi-manually) yourself. That's not an improvement. Sometimes I want to refer to things by name. And now I have to use a library, like say a Dictionary, to get the same behavior. It's brilliant that we're being this explicit about introducing identity. But if there isn't any build in support for identity, .. then there is no standard either. What do we use when we _want_ identity? A string? An integer? From window-id's to connection-id's .. we just end up with a 'roll your own support and don't be compatible with each other'. That's not an improvement. &gt;All that stuff about copy constructors or equals methods or all the 'this.x = x' constructor garbage, all that boring boilerplate is gone. Until you actually have a need for identity. Then, oh shit. Roll your own. &gt;No more writing toString()s or __str__()s by hand, you can get a useful one for free. Reflection is a cool beast indeed, but not unique to haskell. And since it's compile-time reflection, not that flexible either. &gt;Type casts are gone too. Not true. "fromJust" .. haskell is full of type casts. And not all of them are safe either. It's just more explicit. The core language is powerfull enough that it doesn't need to be a build in construct. But converting something from type to the other, is like the bread and butter of Haskell. &gt; All that hairy generics stuff is vastly simplified without subtyping. There we agree. People called haskell's typing system complicated. Then they saw what happened if you want to have the same sort of generic code in other (statically typed) languages. The typing systems blew up. Literally. &gt;Think about all the mandatory type declarations those not-python languages have. That's gone. Or maybe think about how slow and single-threaded and hard to deploy and runtime-error-loving python is, that's gone too. True. Although that has nothing to do with the language. One of the most performance hating language definitions (Javascript) is quite performant these days. You're talking _implementations_ now &gt;That whole thing about control structures being built-in syntax and different from functions? That's gone. That's been done in other languages as well. But that's a good plus indeed. Ruby comes to mind, where many control structures are just library functions. As long as there is prelude that sets an actual standard (or default set, if you will), this is a good thing. Without a default set, it just means code would be even more unreadable for other eyes than your own. &gt; The whole statement vs. expression thing is gone too. No, it's not. Again, we do need statements. It's just another "roll your own". However, because monads get special syntax treatment (hmm, so much for the debate being over), the ecosystem has standardized on monads and people generally don't roll their own anymore. Except for all that 'arrow' stuff, off course. &gt;That lengthy edit, compile, wait, debug cycle is mostly gone too. It usually takes less than a second to reload a module and start running the code you wrote one second ago. Compared to statically typed languages? Yes. Don't get me wrong. I love Haskell. It's a language research playground. But there is a reason why some languages take off as well as they do, and other don't. **It's that .. most of the problems Haskell solves aren't that important. For 99% of all applications out there, the complexity and challenge is not in information transformation. It's in maintaining state, and having enough code conventions (either from discipline or language features) to allow good collaboration on large projects.** In both these cases, Haskell as a language has a strong 'not my problem, roll your own' attitude. Depending on the libraries you choose to use, it's a completely different programming language. And that is very very cool, that that's even possible: that the typing system and the core assumptions are so minimal and flexible this can be achieved. **But it's completely unpractical! It isn't that we don't need to maintain state, manage identity or deal with failing subsystems in concurrency. It's that "roll your own" simply disqualifies you from even entering the competition.** The fractured eco-system of Haskell, where we have a kakophony of language features hidden in libraries, and no obvious defaults .. it makes us end up with a situation where the libraries are as usefull together as completely different programming languages are. In the end, doing simple GUI stuff, or doing simple web-based stuff, or anything that does a lot of IO, and needs to maintain state &amp; identity ..is not easier in Haskell: it's harder. And not to mention, many of the boilerplates you claim are gone, are not gone as soon as you use the features that are associated with it. It's like replacing a meal with a kitchen. Sure, you can make anything you want. But it's no lego anymore. Try using the transactional state support in combination with a GUI library. And tell me, the code isn't exploding .. or that you have weird performance problems .. or that all the boilerplate is gone. **None of it was ever gone. It's just wasn't "part of the core language" anymore** 
And yet, slightly sad. Who wants to join me for a beer and schönfinkel on Friday?
That's this myth of laziness. That we somehow end up with not having to tell it do it in a specific order. Except, we do. We choose between foldr and foldl for example. And not just, because picking the wrong one, will blow up on an infinite list. One of them will use exponentionally more memory in some cases as well. In the end, Haskell isn't explicit about evaluation order .. but the people that write Haskell have to be. It's just all smoke and mirrors and a bunch of spinning. Are infinite lists cool? Hell, yes. Is composing iterators/generators on the fly with so little code? Hell, yes. Is this some sort of magic bullet? No. Can I stop thinking about evaluation order? Hell, no. Maybe it can be argued that some of the mis-uses of iterators and generators have better alternatives in Haskell. But they aren't gone. 
Unfortunately, some of that stuff is 'required' to actually do what you need to do. We have to deal with identity sometimes, we have to do IO. And it's cool that the language is flexible and powerfull enough that many of this features can be dealt with on the library-level. But at that point, the same issues come back. **So, i propose this tl;dr** Haskell delegates both the support and the associated headaches of crucial language features to its libraries, and then goes 'lalalala - i have zero problems' 
Here you go: http://www.cynic.net/tsac.html
You're missing the point. He's not arguing that you can stop thinking about operational properties. He's just arguing that you don't need an explicit iterator or generator construct -- which is true, since laziness can generally do the same job.
Fine, top-level module, I don't really care about that part. But somewhere packaged with the module itself is ideal. You get versioning (with the module), you're encouraged to maintain it as _part_ of the package rather than as an external entity (humans are human and every step you add to a process drops out participation X%, and there's nothing you can do about this except remove the steps), you're encouraged to fit into a certain style guide by the affordances of the Hackage format (which may need to be upgraded, but that's a separate question), including the ability to easily cross-reference and automatically include type signatures and all kinds of other Haskell-documentation-specific features that "mere" HTML doesn't have, etc. It also creates a situation in which it is clear that you really are documenting the package for _users_ of that package; the popular "document-by-academic paper" approach in the Haskell community isn't necessarily intrinsically wrong, but there is a serious tension between the needs of a user of the documentation and the needs of a reader of an academic paper, and I think both aspects suffer as a result due to the defocusing of the audience. Besides, I'm also ready to worry about the problems created by _excessive documentation_ when the problem actually exists. That is not the problem of the day.
You're missing the point on so many topics that I must assume you are deliberately trolling here and not at all speaking out of any personal experience. If not, please prove me wrong.
I put them in the same source directory because they are originally an offspring from the Haskell-Blackboard repository and also because I develop them simultaneously. What advantage do you see in putting them in separate repos? Or do you mean that you don't like the fact that many of the examples showcasing reactive-banana actually require reactive-banana-wx? 
Names matter. If I was asked to distill all the things I've learned in 20 years of programming down to a couple main points, this would be one of them. A clear, precisely defined name greatly improves one's ability to communicate. Because Haskell has a higher level of abstraction than C++, you will encounter more new concepts that need names. Hence the need for more jargon. Now, when choosing a name, which is better? Do you choose a familiar word that won't scare people away, or a scary word for which there are no preconceived notions? Familiar words come with existing meaning. If properly chosen, a familiar word can help people start to understand a concept the instant they hear the name. However, familiar names can be misleading. It's usually hard for people to separate a new concept from meanings they have already associated with the concept's name. Many of Haskell's higher-level concepts have no clear, concise names in plain English. What name would you use for the concept of a functor that doesn't sound scary? Transformable? Ick. Mutable? Too many other meanings. Sometimes you just need new words. It just so happens that the math guys already have words for this stuff. It would be crazy to not use them.
I don't think it works, that NonTerminating test terminates just fine (like regular lazy haskell)
It's still full of language specific terms, like * "The idiom of using a container's *begin* and *end members* to obtain *iterators*" * "because we don't have *initializer expressions* to supply types for date and time." The problem you perceive is one of familiarity, not an absolute one. I mean, at some point, you learned what an "iterator" is, and it's equally possible to learn what "type inference" is. Of course, there are way fewer book and tutorials for Haskell than for C++, but that's simply an effect of popularity. The Haskellwiki does have a [Glossary](http://www.haskell.org/haskellwiki/Category:Glossary), by the way. Though I have to admit that some descriptions could be a lot simpler, but it's a wiki, so anyone can edit. 
That's weird. Maybe it's using different gcc flags or something.
Well there's some semantic difference, though I'm not sure whether it has any significant ramifications. Namely whether the first A is a parameter or an index, which can be deeply significant in other cases. I was just curious whether this was one of the places with significant ramifications and I was unaware of it. Equality is a tricksy beast after all. The syntactic difference ---of making the `x` argument explicit in `refl`--- can be overcome by making the `(x:A)` non-implicit in either version (assuming Agda passes tycon arguments to the datacon like Coq does). I agree that in practice it tends to be more usable to have `x` explicit.
There are some exercises in Hindley and Seldin's [_Lambda-Calculus and Combinators: An Introduction_](http://www.cambridge.org/gb/knowledge/isbn/item1175709/?site_locale=en_GB).
Aw man, when I saw the title I was hoping someone had put some electronica in my Haskell. One of the livecoders perhaps. Alas
Not just partial application, also partial evaluation. Albeit the latter requires you to manually curry the function rather than using the `curry` combinator, but partial evaluation is an incredibly powerful technique.
Please please, it's Schönfinkelization!
As they are now clearly separate projects, I would find it more logical to have them on separate repositories. Might also help getting more contributors, as people wouldn't need to clone the blackboard application. Also searching would benefit from it. The few first times I searched for the repositories, I turned away when I saw the Haskell-Blackboard title.
I still think it was a design mistake to let the theorists name things.
&gt; It just so happens that the math guys already have words for this stuff. It would be crazy to not use them. Hell, I study mathematics and I still think they went overboard on naming things by their academic terms. Wouldn't it have been sufficient to name it something less intimidating and simply point out that it's the same as the mathematical term? 'Anonymous function' versus 'lambda function' for example.
Ah, you are talking about separating reactive-banana from Haskell-Blackboard, not about separating reactive-banana from reactive-banana-wx? (There are three .cabal files in the repo: Blackboard.cabal, reactive-banana.cabal and reactive-banana-wx.cabal. I'd like to keep the last two in the same repo, but the first can be split off.)
&gt; Indexed sets is the first subset of inductive families discussed here which cannot be defined in Haskell. Seems that it can be defined in Haskell. {-# LANGUAGE GADTs #-} data Zero = Zero data Succ x = Succ x data Fin x where FZero :: x -&gt; Fin (Succ x) FSucc :: Fin x -&gt; Fin (Succ x) ghci&gt; :t FSucc (FZero (Succ Zero)) FSucc (FZero (Succ Zero)) :: Fin (Succ (Succ (Succ Zero))) One difference is that in Haskell it also works for stuff other than [type-level] integers. ghci&gt; :t FZero "Hello" FZero "Hello" :: Fin (Succ [Char]) 
I was thinking techono music too.
Good enough for me :)
I was thinking techono music too.
Well, it's not exactly what you was hoping for, but maybe you will enjoy them (and they were made in haskell, at least the visuals) * [one](http://www.youtube.com/watch?v=BMuzdTFwV-A&amp;hd=1) * [two](http://www.youtube.com/watch?v=cQNcmYlVFcQ&amp;hd=1) * [three](http://www.youtube.com/watch?v=0hrWIWdQG4w&amp;hd=1)
Actually, most of the time it works just fine to ignore the evaluation order. It just now and then that you have to pay attention.
I'll see about doing that, then.
Thanks to laziness, even. In strict languages, there is always the nagging temptation to make sure that everything is tail recursive.
I agree with RalfN that laziness doesn't do the same job. Usually it's due to IO being in the mix and Lazy IO is a bad solution for that. But even without IO, lazyness means that it is hard to control the memory consumption of your programs, or at least I couldn't figure easy elegant way to [do](http://stackoverflow.com/questions/6614023/a-good-way-to-avoid-sharing) [that](http://stackoverflow.com/questions/6208006/any-way-to-create-the-unmemo-monad).
Design patterns are definitely not paradigm independent: http://stackoverflow.com/questions/327955/does-functional-programming-replace-gof-design-patterns That's why they are not so much language independent as it seems.
That's true. What I don't understand is that these days people always seem to link github project pages, which, in 98% of the cases, gives me exactly zero information about said project. In contrast, if it is a Haskell project and you link the Hackage page, it usually gives me some useful information about the project. And of course you can still put the github link into the "source-repository" field of the .cabal file if you insist on using it.
Then start some Haskell projects and use github or bitbucket. :) Seriously, people get really tired of being asked to change their tools just because someone else doesn't like them. The best way to create that kind of change is to be the person making that decision about useful projects.
Ok, I've changed the [repository name][1] and readme file. The Backboard folder is still present, I hope it doesn't hurt until I remove it at some later point. [1]: https://github.com/HeinrichApfelmus/reactive-banana
It's not like you can't do IO in Haskell. You just have to be more explicit about it.
Have I written commercial applications using Haskell? No. Have you? But i'm not trolling, and it's interesting to see that nobody counters any argument I made. "You must be trolling. How darely you speak ill of Haskell" I'm not speaking ill of haskell. But the idea that we suddenly don't have to worry about evaluation order, type castings or run-time errors .. is an illusion. And this illusion is driven by the fact that [error-handling, type-casting, equality, identity] .. that all this stuff isn't part of the core language. Are you really claiming that all of the arguments I made are invalid, and ifso, can you give me an argument for at least one of them. Because I honestly think the article is full of lies and spin. It paints a picture too pretty. Here's the shortest example of a lie I could find. &gt;Think about that whole thing with reference vs. values. That's gone. No, it's not. IORef [QED]
Some parts were over my head, but all in all a very good read. Thanks for the find ;)
Thank you again! Found your tutorial, part 1, via google. It helped a lot, since there is a huge lack of Snap tutorials out there. Cheers!
We don't delegate NULL pointer problems to libraries. We just don't have NULL pointer problems. Same with having to specify the types of every single variable, and many other things. I agree some of it is overboard -- iterators/generators are not really gone.
Well what is better : To have all this complexity all the time everywhere or to be able to add it in if you really need it and then just restricted to a part of your code ? ;)
I agree Haskell's lazy lists are far from solving the generator/iterator problem. I also agree that "statement vs. expression" thing isn't really much improved in Haskell than elsewhere. But on every other count, he's being reasonable. &gt; ... a bunch of competing concurrent frameworks, that do not interact with the rest of API or library ecosystem. Same shit, different day. What are you talking about? What concurrent frameworks? &gt; fromJust Sure, fromJust exists -- and sometimes people use it (why, oh why?). But the *default* is to avoid it, and so Haskell at the very least makes NULL pointer dereferences *far far* more rare. In my code, I virtually **never** use fromJust, so I just don't have that problem. Note that in most other languages, you just *can't* differentiate nullable from non-nullable at all. &gt; Yes, you only get deep equality Well, in other languages there's a *semantic* difference between deep equality and reference equality. In Haskell there isn't. &gt; And only if you define it (semi-manually) yourself Are you complaining about "deriving (Eq, Ord)"? &gt; That's not an improvement It's a major simplification of the core language semantics. It also gives you nicer conventions: If things are equal, you know the semantics are the same/equivalent and (given well-behaving Eq instances) the program should not change if you exchange them. &gt; Sometimes I want to refer to things by name. And now I have to use a library, like say a Dictionary, to get the same behavior This is not that frequent, though, so it's not worth messing the core language with. In other languages you have to worry about the kind of comparison you use *every single time*. &gt; we just end up with a 'roll your own support and don't be compatible with each other' What kind of compatibility do you want? We have uniqueness/supply monads for identities and IMO that's really all the compatibility you need. &gt; Until you actually have a need for identity. Then, oh shit. Roll your own What does identity have to do with "this.x = x" boilerplate? &gt; Reflection is a cool beast indeed, but not unique to haskell. And since it's compile-time reflection, not that flexible either You can't actually get the equivalent of an automatic Show instance in, say, Python. Mainly because of identity concerns. Haskell also supports runtime reflection with the Typeable/Data libraries. &gt; Not true. "fromJust" .. haskell is full of type casts If your code is full of "fromJust" then I am glad I don't have to use your code. &gt; But converting something from type to the other, is like the bread and butter of Haskell Converting values between different types (e.g: via fromIntegral) is not really what he means by "casts". "casts" are more like fromJust, where you're asserting a runtime property that cannot be checked. Good Haskell code has virtually none of this. &gt; True. Although that has nothing to do with the language. One of the most performance hating language definitions (Javascript) is quite performant these days Well, Javascript is fast relatively to other dynamic languages. But it's slow relatively to Haskell or other statically typed languages. And slowness is just one of many things he mentioned. The main point is "runtime-error-loving". Dynamicness means less runtime guarantees -- and that's not an implementation concern. &gt; That's been done in other languages as well. But that's a good plus indeed. Ruby comes to mind, where many control structures are just library functions Sure, but most mainstream languages don't. And Ruby is still quite limited w.r.t control structures compared with what Haskell can do. A polymorphic semicolon operator goes a long way. &gt; Except for all that 'arrow' stuff, off course. The Arrow stuff is hardly used. And for good reason. Arrows are equivalent in power to Category+Applicative, so people are realizing that they are not that interesting. &gt; It's that .. most of the problems Haskell solves aren't that important. I strongly disagree. For almost every bug I debug at work, where we don't use Haskell, I spend a bit of time thinking -- would this happen with a Haskell-like type system? In almost every case -- the answer is NO, Haskell's type system would have captured virtually all of our bugs at compile-time, rather than us spending multiple people's and machine days researching these avoidable mistakes. Debugging is one of the *major* time sinks of the entire project's timeline. So I would say Haskell is solving an *extremely* important problem. Not only that, but if I had a Haskell type system checking my code, I would be far more refactor-happy, and the code would more easily be improved. &gt; ... roll your own ... Haskell isn't really a "roll your own" language at all. The "Identity" problem is only partially solved by libraries -- but that's because there's not much benefit to a common "identity library". Other things are consolidating around very standard libraries. &gt; In the end, doing simple GUI stuff, or doing simple web-based stuff, or anything that does a lot of IO, and needs to maintain state &amp; identity ..is not easier in Haskell: it's harder. Have you tried the most popular libraries for each of these? How are they harder? &gt; And not to mention, many of the boilerplates you claim are gone, are not gone as soon as you use the features that are associated with it. Example? &gt; Try using the transactional state support in combination with a GUI library. And tell me, the code isn't exploding .. or that you have weird performance problems .. or that all the boilerplate is gone. I think the burden of proof here rests on you -- why do you think that there *would* be a problem? GUI programs aren't that performance intensive and STM performance is at least reasonable. There's no reason to think there would be a problem. Haskell has very little boilerplate IME. Again, can you show counter-examples? &gt; None of it was ever gone. It's just wasn't "part of the core language" anymore You're really just making a lot of unsubstantiated claims here.
Some of your points are valid, but I only want to argue one thing: Haskell delegates a lot of solutions to competing libraries and language extensions precisely because it wants to get it right before making it part of the standard. The philosophy was to "avoid success at all costs" so that the language could evolve in the right directions after careful consideration instead of following the fad-of-the-day.
Hint spawns ghci child processes? Is it possible to just use ghc as an in-process library to interpret things? Also, how is the `a` argument to `interpret` a witness of being a monotype? Couldn't it be a polytype argument? Why does it need at all? 
I call shenanigans. The names aren't intimidating, they're just names. Nobody avoids Haskell because of the jargon, they avoid it for other reasons and then make excuses afterwards. It's true that learning new jargon comes with the territory of learning anything new, and jargon bloat is a real problem. But "Anonymous function" is no less jargon-y, and not inherently any more helpful than "lambda", except to someone who's already very familiar with the concept yet somehow managed to never hear them called "lambdas". If learning something doesn't involve learning new jargon, it's a safe bet that either you're not actually learning anything, or (more insidiously) they've disguised their jargon by reappropriating common words to mean something else, in which case you've probably misunderstood several important things because of lack of clarity.
&gt; But i'm not trolling, and it's interesting to see that nobody counters any argument I made. "You must be trolling. How darely you speak ill of Haskell" You're too fundamentally confused to have made an argument. Most of what you said isn't even coherent enough to be wrong. And I'm sorry, but spouting incoherent nonsense and then declaring victory when people don't want to deal with it falls into what many would consider "trolling". &gt; Here's the shortest example of a lie I could find. &gt; &gt; &gt; Think about that whole thing with reference vs. values. That's gone. &gt; &gt; No, it's not. IORef &gt; &gt; [QED] There are so many things wrong with this it would take paragraphs to clarify, and your arrogant tone suggests you're not even interested in learning. I suspect you'll be all upset and take this personally, but too bad. Tough love. If you want people to respond to your arguments, get informed opinions first.
Finally, r/haskell starting to look like the rest of reddit!
&gt; There are so many things wrong with this it would take paragraphs to clarify, and your arrogant tone suggests you're not even interested in learning. Just in case RalfN thinks it's only camccann who feels this way: it isn't. You have to pick your battles. If someone comes into the #haskell IRC as fundamentally confused as RalfN appears to be, and shows willingness to learn, I'm generally quite happy to explain. Confidently exclaiming things like "QED" while spewing bullshit puts someone in the "it's not even worth arguing with you" bag, for me and many others I know.
Null pointers are implemented as a mandatory construct in many languages, where they behave as an implicit Maybe type, with "fromJust" applied automatically. fromJust can give you a return time error. Actually any imcomplete pattern can. Here's are some examples: http://community.haskell.org/~ndm/downloads/slides-detecting_pattern_match_failures_in_haskell-26_nov_2007.pdf 
The fact a partial function is injected *everywhere* makes quite a big of a difference. You *can* use Haskell without touching fromJust, it is completely practical. You cannot use other languages without such partiality.
Nobody counters your arguments? I think I have...
&gt;What are you talking about? What concurrent frameworks? http://www.haskell.org/haskellwiki/Applications_and_libraries/Concurrency_and_parallelism I count 14 of them, but some actually require a (forked) implemenation of GHC. Say, I have a theoretical audio library that uses on of them, and a GUI library that uses another set of them. Hell breaks loose. This again, reinforced the idea that Haskell is used more as a research playground .. but that it literally avoids success. It does not have a very productive ecosystem as a result. This isn't a bad thing. But it does mean the article is _wrong_. &gt;Sure, fromJust exists -- and sometimes people use it (why, oh why?). But the default is to avoid it, and so Haskell at the very least makes NULL pointer dereferences far far more rare. In my code, I virtually never use fromJust, so I just don't have that problem. Alright. Let's say i have a method that reads a file. Then it does something with the file. The thing is, it isn't improper to assume the file is there. And I agree, explicitely dealing with that situation yourself is better, than just using fromJust. But i was arguing against the arguments of the article. Not haskell. &gt;It's a major simplification of the core language semantics. Yes. Much like not doing a project is a major simplification compared to doing the project. We do actually need identity at times. If we have to simulate it, fine, but don't act like that suddenly solves the complexity of dealing with identity. It doesn't. It's like removing all math functions from a core language, and then claiming it no longer has divide-by-zero errors. And if people then complain that they need to do math, you just tell them to 'roll their own math support'. It avoids by the problem by not addressing the issue. &gt;This is not that frequent, though, so it's not worth messing the core language with. In other languages you have to worry about the kind of comparison you use every single time. Actually no. Some languages actually default to identity- instead of value- semantics. And Haskell does at least one counter example, where the default behavior is different (IORef). I don't see how we can suddenly stop worrying about identity. It is formal requirement in many situations. &gt;Converting values between different types (e.g: via fromIntegral) is not really what he means by "casts". "casts" are more like fromJust, where you're asserting a runtime property that cannot be checked. Good Haskell code has virtually none of this. Good Haskell code is a research paper, that doesn't actually with the outside world? Because as soon as you do, there will be uncertainty. Will the file be in the correct format or will it not. Haskell could theoretically even make things like fromJust illegal and we could still write any program in the world. We would just have to be forced to deal with the error condition. Perhaps throw a manual run time error? The thing is, the world is dynamic. And if the type system can't deal with it, we will just store this "dynamic type information" as values and manually throw errors. Essentially just reimplementing dynamic type checking. None of the issues related to dynamic type checking would go away, because they can't go away: they are inherent to the problem domain. &gt;What does identity have to do with "this.x = x" boilerplate? It's that one of the most common ways to manage dynamic identities would to use a dictionary/map. Things suddenly look very much the same to me. &gt;Dynamicness means less runtime guarantees -- and that's not an implementation concern. No, that's not. But comparing the performance of Haskell to Python seems unfair. Comparing the performance of Hugs to V8 seems more fair to me. Type erasure sure leads to faster code. But you can debate how important it is, that this happens at compile-time or launch-time. It's usefull to do static type checking at compile time, and while you're at it, why not apply type-erasure immediately! But it's not actually a valid argument (anymore) for performance. Given any javascript program, for example, those parts that could theoretically benefit from type-erasure, actually benefit from type-erasure in V8. And the parts that can't, use types for 'dynamic information' .. in Haskell you would be forced to encode it as such and you wouldn't get a magical performance advantage either. &gt;Sure, but most mainstream languages don't. And Ruby is still quite limited w.r.t control structures compared with what Haskell can do. A polymorphic semicolon operator goes a long way. Totally agree there. I do still think that control structures generally having a default 'common set' is a Good Thing, when collaborating, and more important. Haskell solves this by strongly promoting a Prelude .. other languages do this by making certain constructs 'build in'. &gt;The Arrow stuff is hardly used. And for good reason. Arrows are equivalent in power to Category+Applicative, so people are realizing that they are not that interesting. Actually, some GUI libraries use them. So trying to use those in combination with monads, gets kind of messy. And that situation can hardly be claimed to solve the 'expression/statement debate'. &gt;What kind of compatibility do you want? We have uniqueness/supply monads for identities and IMO that's really all the compatibility you need. Weird. Because I see many libraries 'support' identity using IORefs, integers, strings, custom datastructures, maps. &gt;For almost every bug I debug at work, where we don't use Haskell, I spend a bit of time thinking -- would this happen with a Haskell-like type system? In almost every case -- the answer is NO, Haskell's type system would have captured virtually all of our bugs at compile-time, rather than us spending multiple people's and machine days researching these avoidable mistakes. This is don't disagree with. At all. It's very obvious Haskell is targetted to a very different problem domain. Most bugs in common programs in common programming languages are about dealing with unexpected states, branching errors and generally just 'information proccessing'. The reason why so many people use those languages to succesfully build so much software is because that's not really the hard part at all, in the common domains. The hard part is managing state, maintainability of the code and collaboration. Haskell has good support for collaboration (with explicit scopes, and modules) .. maintainabiliy will require some disciplines ("please dont invent your own control structures") .. and it just sucks at managing state. A typical crud application with a database backend, really isn't simpler in Haskell. At all. But say, a compiler? Hell, yes, use Haskell. &gt;So I would say Haskell is solving an extremely important problem. Not only that, but if I had a Haskell type system checking my code, I would be far more refactor-happy, and the code would more easily be improved. They've tried to add Haskell type system to both Java and C#. Sort of. I think they've could have done a better job. And i'm sure as hell not claiming those languages are the fine wines of our world. But do you really think that for the projects of your company, that the equivalent Haskell code, would be as maintainable? As easy to write? I don't know what you guys are making .. so, it may just very well be the case. But none of that makes the 'claims' of the article any more true. Nor is Haskell this magical language that fits every, or even the most common, problem domains. &gt;I think the burden of proof here rests on you -- why do you think that there would be a problem? Because the GUI library uses a different abstraction for statements and concurrency as the STM. You have to convert from and to. The wild grow of alternative approaches is great for research, but it's a disaster for the ecosystem. Sure, a mono culture is also very dangerous in the long term, but it does allow for a lot of neat integration and assumptions about the working environment. Something like RoR wouldn't be half as productive, is there wasn't this assumption about using ActiveRecord made by half the libraries out there. Defaults are a Good Thing (tm). &gt;You're really just making a lot of unsubstantiated claims here. That's true for both me and the [OP's] article. 
&gt;You're too fundamentally confused to have made an argument. &gt;Most of what you said isn't even coherent enough to be wrong. &gt;There are so many things wrong with this it would take paragraphs to clarify, and your arrogant tone suggests you're not even interested in learning. Yes. _my_ arrogant tone. I see it now. &gt;I suspect you'll be all upset and take this personally Ah, assumptions. You are a wise man, and I am humbled by your skills to predict my emotional state. Here's a question though. If that is the outcome you expected, why bother trying to insult me? Or, perhaps its the outcome you would prefer? Do you prefer a world, where people that have an opinion different from yours, act like assholes? Would that validate a world view where you rather not debate any of your own opinions? It should make arrogantly dimissing statement of others much easier of course. Then again, what do you do, when it doesn't actually work? Well, let's find out! 
Don't worry! There will be enough fanboys expressing their love for Haskell, sugared coated in pretentions of intellectualism. Some days, I may even be one of them. But not today. Because the article just went to far for me. I like Haskell, but i don't agree with the majority of claims the article made. Obviously, i should have stated my case much more eloquent, much more erudite. Although I doubt I could top your level of subtlety. Then again, i don't think I want to. 
Totally agree. And that's great. &gt;You can use Haskell without touching fromJust, it is completely practical. Sure, but many times, we really do know for 'almost certain' that it will be a just. Unless an installation is corrupted, or a server is down or something. In those cases not using fromJust, means that I'm going to throw an error. It's a better situation. But it's not like the inherent problem is any different. That it magically goes away, like the article states. It's just a bit easier to not have it bite you in the ass.
&gt; [1] http://www.haskell.org/haskellwiki/Applications_and_libraries/Concurrency_and_parallelism &gt; I count 14 of them, but some actually require a (forked) implemenation of GHC. These 14 concurrency/parallelism tools are not incompatible competitors but part of the same eco-system, and built upon the same primitives. Some are parallelism tools for pure code, some are concurrent tools for IO, some are transactional concurrency. Since they are built on the same primitives, compatibility is easy. Do you have an example of a problematic incompatibility? &gt; Say, I have a theoretical audio library that uses on of them, and a GUI library that uses another set of them. Hell breaks loose. Not really, adapters between STM and IO are trivial, as with any of the other concurrency tools (especially those that have MonadIO instances!). &gt; This again, reinforced the idea that Haskell is used more as a research playground .. but that it literally avoids success. It does not have a very productive ecosystem as a result. You're going to have to do better than claim some theoretical problem exists. Show some code that's actually hard to reconcile. &gt; Alright. Let's say i have a method that reads a file. Then it does something with the file. The thing is, it isn't improper to assume the file is there. And I agree, explicitely dealing with that situation yourself is better, than just using fromJust. If you stick "fromJust" in your code, you're *explicitly* foregoing the safety that Haskell gives you, just as if you're using unsafeCoerce. It is bad practice, and it is ridiculous that the existence of these tools in Haskell makes you think that Haskell is just as unsafe as other languages which apply these tools **implicitly**. &gt; We do actually need identity at times. If we have to simulate it, fine, but don't act like that suddenly solves the complexity of dealing with identity. It doesn't. Identity is needed for a *very* small subset of code. In Haskell, only that small subset has to deal with it. In other languages *everything* has to deal with identity and aliasing. In this sense, it is a great simplification. In my Haskell projects, I barely have identities to deal with, and when I do, the explicit identity handling is *better* than the object identity slapped on every piece of data by other languages. &gt; It's like removing all math functions from a core language, and then claiming it no longer has divide-by-zero errors. And if people then complain that they need to do math, you just tell them to 'roll their own math support'. No, it's not like that. It would be like that if other languages involved division by zero errors in virtually every part of the language, whereas Haskell only involved them when actual division was involved. &gt; It avoids by the problem by not addressing the issue. It avoids it by removing the *meaning* of identity from the vast majority of code which does not have to care. &gt; Actually no. Some languages actually default to identity- instead of value- semantics. And Haskell does at least one counter example, where the default behavior is different (IORef). I'm not sure what you're trying to say here. Languages do default to identity, and have multiple types of comparisons between objects, and you have to be wary of aliasing issues, and x==y does not mean the two are interchangeable as it does in Haskell. It is definitely more complicated than in Haskell. &gt; I don't see how we can suddenly stop worrying about identity. It is formal requirement in many situations. Not so many in my experience. In OO programming, *every* single object has an identity that the semantics of the language actually expose. That is unnecessarily complicated. &gt; The thing is, the world is dynamic. And if the type system can't deal with it, we will just store this "dynamic type information" as values and manually throw errors. Essentially just reimplementing dynamic type checking. It's not "manually implementing dynamic type checking" because Haskell forces you to consider &amp; explicitly forfeit safety statically if you don't want to handle the errors. This brings more of the costs upfront, so Haskell can be more expensive at development time -- but it saves you from paying interest when these bugs become expensive. Also, if you're not writing quick&amp;dirty hacks, you better truly handle the error cases properly. There's really no one-size-fits-all error handling. Dying with a runtime "Type Error" exception is not an acceptable solution in most situations. &gt; None of the issues related to dynamic type checking would go away, because they can't go away: they are inherent to the problem domain. *Most of the issues* related to dynamic type checking **do** go away. You don't *have* to forfeit the safety. If you do -- you get a *whole* lot more certainty about the conditions under which your code will work or fail. This is a *huge* issue. &gt; It's that one of the most common ways to manage dynamic identities would to use a dictionary/map. Things suddenly look very much the same to me. Show me some code. I think you might be "Doing it wrong" here. &gt; No, that's not. But comparing the performance of Haskell to Python seems unfair. Comparing the performance of Hugs to V8 seems more fair to me. Why not compare the performance of PyPy to GHC? Do you really think the extra static information in static languages cannot translate to better optimizations? &gt; Type erasure sure leads to faster code. .... &gt; Given any javascript program, for example, those parts that could theoretically benefit from type-erasure, actually benefit from type-erasure in V8. I think you're confused. Type erasure has nothing to do with it. Knowing the types statically is the key here. Whether or not you forget what the types were or keep it around somewhere is irrelevant. In Javascript, you may sometime be able to infer/know what the type is statically. Sometimes you will not be able to. &gt; And the parts that can't, use types for 'dynamic information' .. in Haskell you would be forced to encode it as such and you wouldn't get a magical performance advantage either. In Haskell, the powerful type system has never left me wanting "dynamic typing" (or better described as uni-typing). Splitting comment as it is too big for Reddit (first time for everything!)
IME the problem magically went away with Haskell. I just didn't have any NULL dereference bugs anymore. If I had inserted a fromJust, I either could prove it wouldn't be Nothing (and actually used `fromMaybe (error "proof of why this cannot be Nothing")` [never use fromJust] or had updated my spec and expectation of the program for the handling of this particular case. With the added benefit that the crash now included an exact message, rather than a generic stack trace. Haskell took away the surprises -- everything about the handling of Nothing became statically known, even if my choice ended up being "just throw an error". With other languages, I am never sure my program is correct even w.r.t the non-exceptional NULL handling situations. I just have no guarantee and no idea.
&gt; Actually, some GUI libraries use them. So trying to use those in combination with monads, gets kind of messy. I think they're mostly being phased out. &gt; Weird. Because I see many libraries 'support' identity using IORefs, integers, strings, custom datastructures, maps. You seem to be lumping together apples, oranges and space ships in the same category here. IORefs establish mutable cells that have identity, but do not actually expose that identity (They don't have an Eq instance, for example). Integers and Strings are potential identities, whereas maps are namespaces to key by identities. Code that uses a Map with String/Int keys exists in virtually all languages, so implying that is some result of a hole in the language seems absurd. OTOH, viewing these keys as some sort of identities in all of these seems reasonable. I really don't see what the problem is. Do you think an OOP's object's identity is a replacement for Maps-of-Strings identities? &gt; ... Most bugs in common programs in common programming languages are about dealing with unexpected states, branching errors and generally just 'information proccessing'. I am not sure I agree that "most bugs" are about that at all. IME, bugs vary between wrong argument orders, forgetting to assign an object attribute, using the wrong units, in addition to all the categories you mentioned. &gt; The reason why so many people use those languages to succesfully build so much software is because that's not really the hard part at all, in the common domains. I think you're misinformed if you think people are generally successful with mainstream languages. The significant majority of software projects are failures. Those that "succeed" are also late, over-budget and under-featured. Quality is low. I know what we spend our time on, and it is *debugging, debugging, debugging*. &gt; The hard part is managing state, maintainability of the code and collaboration. Haskell has good support for collaboration (with explicit scopes, and modules) .. maintainabiliy will require some disciplines ("please dont invent your own control structures") .. and it just sucks at managing state. Haskell is *awesome* at managing state. Composing (state -&gt; state) functions gives you atomicity/transactionality for free. The power of SECs and other combinators is unparalleled. &gt; A typical crud application with a database backend, really isn't simpler in Haskell. At all. This is an unsubstantiated claim. &gt; They've tried to add Haskell type system to both Java and C#. Sort of. I think they've could have done a better job. And i'm sure as hell not claiming those languages are the fine wines of our world. &gt; But do you really think that for the projects of your company, that the equivalent Haskell code, would be as maintainable? As easy to write? I don't know what you guys are making .. so, it may just very well be the case. I think it would be. In our case, we do systems programming, so Haskell may be inappropriate for other reasons (or any other GC'd language, actually). But some of Haskell's type system features really could have been back-ported to C and greatly reduce our workload and increase our reliability. &gt; But none of that makes the 'claims' of the article any more true. Nor is Haskell this magical language that fits every, or even the most common, problem domains. I think Haskell is excellent for virtually all domains, except low-level systems programming. &gt; Because the GUI library uses a different abstraction for statements and concurrency as the STM. You have to convert from and to. The wild grow of alternative approaches is great for research, but it's a disaster for the ecosystem. I think the conversions are so trivial that it would barely even be annoying. &gt; Something like RoR wouldn't be half as productive, is there wasn't this assumption about using ActiveRecord made by half the libraries out there. &gt; Defaults are a Good Thing (tm). Haskell has One True Way to do most things. For those that it doesn't, it's still advancing the state of the art -- and typically interoperability is easy. 
&gt;You have to pick your battles In other words: insult, but not debate people that disagree with you. If you are not in this battle, what are your insults doing here? Try being a little honest about your motiviations. &gt;Confidently exclaiming things like "QED" while spewing bullshit How, is giving a counter example to a claim not a proof? Yet calling my statement bullshit, with nothing to back it up, is fine display of an intellectual authority. I honestly think the article is full of BS. I don't dislike Haskell, but i don't have many projects where it would be a good fit. And i'm pretty sure that you are not applying the same critical standard to the 'praise of haskell' as you apply to the 'critism of praise of haskell'. So guess what? Behind all that elitism, is just a fanboy applying the same kind of sheep-logic as every one else here. Off course, I can hardly call any of your claims "fundamentally confused", because beyond the insults, there aren't any claims. At all. So there is no risk in you sounding stupid. That doesn't actually make you smart though, but substituting arguments for insults, does make sound like a dick.
When already in a hole, you should usually stop digging. As expected, you're taking it as a personal attack and ignoring issues of substance. I'm not insulting you. I may not even disagree with you, in many cases. I'd be happy to debate, but--and this is an objective statement of fact--you are not sufficiently informed to express the arguments I think you're trying to make, insofar as I can guess at your meaning. I strongly encourage you to please stop embarrassing yourself and spend more time learning.
Take a look at http://hackage.haskell.org/packages/archive/base/latest/doc/html/Data-Maybe.html#v:fromMaybe (the Data.Maybe documentation) and pay some attention to the fromMaybe and maybe functions. You seem to be ignorant of these functions. They serve different purposes, but I've never once used fromJust when I have them around. 
&gt; How, is giving a counter example to a claim not a proof? Because it's not actually a counterexample. &gt; And i'm pretty sure that you are not applying the same critical standard to the 'praise of haskell' as you apply to the 'critism of praise of haskell'. Here, let's look at a comment I left a few days ago elsewhere: &gt; Haskell getting something right in how it describes numbers? That would be a first. *Sigh.* :[ Quit it with the persecution complex. Nobody here is a blind fanboy. I'm confident godofpumpkins can guess roughly what I'm sighing about there, and I'm pretty sure he has a laundry list of his own complaints about Haskell.
Another cross post from r/javascript from me. I appreciated the discussion here last time even if it seemed that most folks didn't like that I might be misleading people about the category theory behind Functors. If it's any consolation I've since presented on the topic and I took the time to direct people to the functor laws and spoke about them afterwards (30 minute talk).
No, I've done it all before. Look back (a few months) through my comments if you want refutations. I'm just tired of arrogant programmers and of writing the same arguments over and over again, especially with people who clearly have no interest in learning (because they seem to think they already know the answers).
I do want to note off the bat, that some parts of our discussion are moving away from 'are the claims of the article true' .. to 'is Haskell usefull for typical commercial applications'. I don't mind, but I think it's a harder discussion. &gt;Since they are built on the same primitives, compatibility is easy. Do you have an example of a problematic incompatibility? Alright. Imagine STM based "actors" that interact (and should be able to roll back) GUI states. Except the GUI library is using a message passing system for concurrency internally. If say, STM was an (almost mandatory) default, this wouldn't have happened, and the GUI library would expose its API as STM transactions. &gt;It is bad practice, and it is ridiculous that the existence of these tools in Haskell makes you think that Haskell is just as unsafe as other languages which apply these tools implicitly. I wasn't saying 'as bad'. I don't think you can even qualify that for 'all other programming languages' at all. But it's not a magic bullet. And more importantly the claims of the article that they suddenly don't exist any more isn't true. &gt;I'm not sure what you're trying to say here. Languages do default to identity, and have multiple types of comparisons between objects, and you have to be wary of aliasing issues, and x==y does not mean the two are interchangeable as it does in Haskell. It is definitely more complicated than in Haskell. Yes, it's more complicated. And perhaps it's better to complete the separate the two types of equality. But the article claimed that you have no identity in Haskell, suggesting all the problems of managing identity are gone. They are not. It's just two separate islands now. &gt;Identity is needed for a very small subset of code. In Haskell, only that small subset has to deal with it. In other languages everything has to deal with identity and aliasing. In this sense, it is a great simplification. In my Haskell projects, I barely have identities to deal with, and when I do, the explicit identity handling is better than the object identity slapped on every piece of data by other languages. Interesting. In my projects, about half of the code is about dealing with identity. Database-tables, database-records .. it's all identity based. Perhaps the domain of your projects is less common? &gt;It's not "manually implementing dynamic type checking" because Haskell forces you to consider &amp; explicitly forfeit safety statically if you don't want to handle the errors. It's not a choice to 'forfeit statical safety'. First of all even Haskell's type system isn't a turing machiene. Secondly, I don't have any garantuee about the world being statically safe. I can not expect specific database-columns, to exist in my database. Sure, I assume they do. And i sure as hell, am not going to consider it a user-error. But i'm going to find out at run-time! There isn't anything any language can do about that. In dynamically typed languages you can sometimes use the type to embed information that is calculated at run-time. In Haskell we are forced to embed such information into the datastructure. This is much better solution indeed. But the dynamic nature, the associated issues, aren't gone. They can't ever be gone. &gt;Show me some code. I think you might be "Doing it wrong" here. Don't have it at hand. But it was a Map of addresses. It has an actual function that makes it easier to set them up. Commonly, we would consider it a constructor. And i had functions to quickly get and set certain fields, that also did some validation. It's all boilerplate, and it's going to go away. Well, it takes less code in RoR to do the same though. &gt; Do you really think the extra static information in static languages cannot translate to better optimizations? Yes. I think it even goes back to the Smalltalk era, when they figured out that, as long as a static type system is decidable, you can infer it. In V8, they pretty much do just that, and whenever it is not inferrable, it would be a situation where the information would be encoded as data, rather than types in a statically typed language. &gt;Type erasure has nothing to do with it. Knowing the types statically is the key here Type erasure saves memory. It also, implies that you do static dispatching. You decide with function to call, based on the type, at compile-time. That is: you make the decision once. You are comparing it to the situation where that decision is made over and over again. Interestingly, certain JIT compilers, like V8, make sure they only make that decision once as well. &gt;In Javascript, you may sometime be able to infer/know what the type is statically. Sometimes you will not be able to. Yes. And in haskell that same algorithm would encode that same information, encoded as type in Javascript, as data in Haskell. &gt;In Haskell, the powerful type system has never left me wanting "dynamic typing" (or better described as uni-typing). Yes. Because of the polymorphism, many use-cases are covered. But here's a strong counter example of dynamic typing &amp; reflection at work. An ActiveRecord model connects to the database, looks at the table definition that is the plural of its class-name. It then creates methods for each field, that automatically convert the database-type to a sane local type. You could so something similar with Template Haskell, but not at run-time. Dynamic typing really isn't important. But in combination with reflection it does allow us to do usefull that aren't otherwise possible. Then again, because of the ability to express datatypes yourself, so nicely, I think this whole scenario could have solved quite differently and cleanly in Haskell. Indeed encoding the field names as proper strings (rather than types), seems a more sane approach. But then you can still get a bunch of error conditions at run-time that you need to deal with. The dynamic nature of the problem, doesn't go away. &gt;Splitting comment as it is too big for Reddit (first time for everything!) I've been there before. 
&gt;IORefs establish mutable cells that have identity, but do not actually expose that identity (They don't have an Eq instance, for example). Integers and Strings are potential identities, whereas maps are namespaces to key by identities. Yes. &gt;Code that uses a Map with String/Int keys exists in virtually all languages, so implying that is some result of a hole in the language seems absurd. OTOH, viewing these keys as some sort of identities in all of these seems reasonable. Yes. &gt;Do you think an OOP's object's identity is a replacement for Maps-of-Strings identities? There is not a problem unique to haskell here. I was just claiming that all the issues associated with managing identities, doesn't go away. It's still here. And there isn't even a defacto way to manage it. So any library exposing identities will likely not have 'compatible' identities at all. Object-identity in an OOP languages, the most likely candidate for interoperation between different libraries. &gt; Those that "succeed" are also late, over-budget and under-featured. Mine aren't late, over-budget or under-featured. What they are is "nothing special". And 99% executed in the context of this software, isn't mine at all. It's libraries, database engines, web-servers, etc. &gt;I know what we spend our time on, and it is debugging, debugging, debugging. I spent some time debugging. But much of that is almost mandatory and wouldn't go away if I was using Haskell. I still have to debug the HTML, i still have to debug all the database interactions. Where there are performance problems, and when to properly cache stuff. Well, why not do the experiment? Just take one days worth of code, and try to reimplement it in Haskell. I've tried it. And it wasn't a success. Mostly because of the ecosystem, but also because i felt there wasn't a clear 'One Way' to do things. The stuff that easy in other languages, like say maintaining state about a database model, just required more and uglier code in Haskell. &gt;I think Haskell is excellent for virtually all domains, except low-level systems programming. I assume you are using C. Interestingly, I think the reason you spent so much time debugging, isn't specifically related to the type system. I think when valid code looks more readable, that this also helps a lot. print "this is a recent comment" if this_comment.updated_at &lt; 2.days.ago If I just try to imagine the same code in C, .. yeah, hell yes, the chance of making mistakes is much higher. And maybe you guys are just doing more complicated stuff. But that really isn't the norm for many domains. It's also funny, that many people tend to say "i love haskell.. i think it's brilliant for everything, except for [my-domain]" &gt;Haskell has One True Way to do most things. For those that it doesn't, it's still advancing the state of the art -- and typically interoperability is easy. I didn't experience it that way at all. Perhaps thats my error. Perhaps the document is kind of sloppy in that regard, I don't know. 
&gt;because they seem to think they already know the answers I read an article. I think it's complete spin on reality. I don't actually dislike Haskell, but it's not a magic bullet for real intrinsinc challences in the computer science field. I've countered some of the claims of the article. Yet, you come along. And then you arrogantly dismiss my statements, because when you glance at it for a second, it looks like im just dissing Haskell. &gt;especially with people who clearly have no interest in learning (because they seem to think they already know the answers) Did i just sign up to be your student? If i'm wrong, feel free to make a fool out of my with actual arguments. But don't assume i'm here for you. I just want to get decent haskell news, so i can keep a small eye on it. And not this inflammatory misleading articles praising Haskell like a the new coming of a God. So, yes, I set out to tear it down. I will believe many of my counter arguments are valid. That the article is just misleading. If all the claims in the article would be true, you could literally deduce that when you write a javascript interpreter in haskell it would suddenly make Javascript type safe. The article acted like real CS problems, some of which are just intrinsinc to the problem that people try to use a programming language to solve. So, if you are like MrHaskell, why don't you go tear this crap down, instead of me?
Okay fine, I'll bite. &gt; That's this myth of laziness. If you're wondering why people (including me, further down this thread) are jumping on you, statements with a tone like this are it. &gt; That we somehow end up with not having to tell it do it in a specific order. Except, we do. We choose between foldr and foldl for example. And not just, because picking the wrong one, will blow up on an infinite list. One of them will use exponentionally more memory in some cases as well. In the end, Haskell isn't explicit about evaluation order .. but the people that write Haskell have to be. It's just all smoke and mirrors and a bunch of spinning. (wonderful attitude) `foldl` and `foldr` aren't actually deciding evaluation order. In fact, they are completely different functions unless your binary operation is associative. And some things make sense on an infinite list, while others do not. For example, what is the sum of an infinite list? What is the concatenation of an infinite list of lists? The former does not make sense on infinite lists, and the latter does, as it can be generated lazily. Which you use depends on the operation you're folding and its time complexity in each of its arguments. List concatenation is constant in the length of its second argument, and linear in the length of its first. So chances are you want to be associating right. This is basic computer science, and nobody in the Haskell community argues you don't need to be aware of that. It's quite distinct from evaluation order. What actually determines the evaluation order on `foldl` and `foldr` is still the strictness of the binary operation you're folding. `foldl'` puts more constraints on evaluation order. But in practice, the situation is nowhere near as bad as you make it out to be. Yes, in some cases you need to be aware of evaluation order, but in most cases you don't. Well, I'm not, and I write a lot of Haskell, and I don't run into issues. Anyway, you can call it the myth of laziness, but it really isn't. Laziness isn't a silver bullet, and many (even SPJ) feel ambivalent about it. But it certainly isn't a pile of "smoke and mirrors and a bunch of spinning". If you assume (without basis) that we're acting in bad faith ("spinning", come on!) we're going to think less of you. What laziness does buy you is the ability to not have to worry about order of evaluation. And that is a big win, when you want to compose operations cheaply or create custom control flow. I can write `replicate n x` as `take n (repeat x)` and it'll be no less efficient than something that explicitly recurses on the number it's given. I can count elements in a list with a fold over it, without worrying about the elements being computed unnecessarily. I can basically create arbitrarily many efficient combinations of basic functions without worrying (for the most part) about efficiency, because it'll generally be pretty good. I realize you have a monster "refutation" down below but I can't deal with going through it point-by-point right now. I did want to comment on a couple of your examples though: &gt; &gt; Think about that whole thing with reference vs. values. That's gone. &gt; No, it's not. IORef &gt; [QED] You actually tried to refute a point like that with a feature that most people almost never use for most of their code, except on the outer fringes of their programs? And then were arrogant enough to slap a "QED" on that? I have no words. The rest of the so-called refutations are about misunderstandings about type systems (decidable -&gt; inferrable) or strawmen about operations most people strive to avoid (like `fromJust`) or even just grasping at straws about installation problems or missing database columns and not being able to statically guarantee that won't happen. Types don't exist to guarantee nothing bad happens. The best we can do is to strive to minimize runtime errors with things we can guarantee to be absent. We're not language fanboys in here. A lot of us are PL nerds and have done a lot of coding in many other languages. I for one have probably spent more time staring at assembly language than most people here. In most of your posts you paint yourself (implicitly or explicitly) as some sort of voice of reason in a crowd of hysterical fanatics, and that pisses people off to no end. We know languages. We like Haskell. We are not "spinning" or creating "myths" or anything of the sort. We do understand that laziness has its downsides. This blog post was written to make a point, and not to examine all the pros and cons of Haskell, and I thought it worked quite well for that. I apologize for being harsher than I normally I am. Your unique combination of uninformed opinions and attitude problems really triggers me, for some reason.
&gt;As expected, you're taking it as a personal attack and ignoring issues of substance No, i wasn't taking it as a personal attack. I know you want me too, though. In denial, much? &gt; I'm not insulting you Are you sure? &gt;I strongly encourage you to please stop embarrassing yourself Oh, I am insulting myself. ;-) &gt;you are not sufficiently informed to express the arguments I think you're trying to make, insofar as I can guess at your meaning. Well, the people that claim to be suffienciently 'informed' are too busy playing the elitist authoritive card and arrogantly dismissing comments of others. So somebody had to call bullshit on the article. If you prefer, feel free to write a critique of the article based on your vast knowledge and life-time experience. Off course, in between of establishing yourself as somebody that only goes around calling other people idiots, without saying anything meaningfull yourself. It's funny. People vote you up, because of 'Haskell good, critique of haskell bad'. And you seem to mistake that as a validation that you did anything other than arrogantly dismissing things other say, without any subtance at all. So, far in our interaction. I'm the only one being constructive. 
jQuery is not an Applicative. Lets see an example of actual functors and applicatives in javascript to contrast. First, lets define a simple argument-at-a-time, under-application-only currying combinator: function curry2(f) { var that = this; return function(a) { return function(b) { return f.call(that,a,b); } } } Then we can define a Functor. function Functor(map) { this.map = curry2(map); } map is required to be able to take two arguments. the first should be any function from a -&gt; b, and in exchange it'll give f a -&gt; f b for some f specific to your particular Functor instance. A simple example of a Functor is one that takes an array, and gives back a new array, having mapped every element through a function. Our choice of 'f' is basically Array. We'll ignore anything exotic, and just pretend length is good enough to scan the elements. Mainly because I'm in a hurry. var array = new Functor( function(f,a) { var b=[]; for (var i=0;i&lt;a.length;i++) b[i] = f(a[i]); return b; } ); Now you can map just fine over arrays. If you give me an function from numbers to numbers, I can take an array of numbers, and give you an array of numbers. &gt; array.map(function(x) { return x + 1 })([1,2,3]) [2,3,4] But map is required to work for everything, not just HTMLElements, or whatever jQuery's designers decided to let it wrap today. Now lets define Applicative. function Applicative(pure, ap) { this.pure = pure; this.ap = curry2(ap); } Here pure takes any value of any type a, and generates an f a, for some f particular to the applicative. And ap takes an f (a -&gt; b), and an f a, and generates an f b. Every Applicative is a Functor, so we'll chain the prototypes. Applicative.prototype = new Functor( function(f,a) { return this.ap(this.pure(f))(a); } ); Now to demonstrate, we can make the reader applicative out of functions: var reader = new Applicative( function(a) { return function(e) { return a; } }, function(mf,ma) { return function(e) { return mf(e)(ma(e)); } } ); You can bolt the traditional reader combinators like ask, directly into reader. reader.ask = function(e) { return e; }; Notice we take pure values and return fresh new values, clearIds on the other hand is just mutating some HTMLElement. We can also define the state applicative. var state = new Applicative( function(a) { return function(s) { return [a,s]; }; }, function(mf,ma) { return function(s) { var t = mf(s); var u = ma(t[1]); return [t[0](u[0]),u[1]]; } } ); But when you look at jQuery. the $() isn't a polymorphic wrapper. It does a great deal of analysis on its arguments. jQuery is an amazing combinator library, but it isn't a functor, it isn't applicative, and it isn't a monad.
The article claimed: &gt;Think about the whole thing with iterators or generators or whatever else that so many imperative languages have. That's all gone. To back this up, it is claimed: &gt;I believe he is referring to laziness here, which does replace many uses of iterators/generators. Then I said: &gt;&gt; In the end, Haskell isn't explicit about evaluation order .. but the people that write Haskell have to be &gt;&gt; It's just all smoke and mirrors and a bunch of spinning. &gt;(wonderful attitude) I was referring to the article. But yes, in general, there is an order of evaluation, and we do have to take into account. I'm not saying it's bad or a nightmare. I'm saying that the claim 'that laziness means we do not have to write iterators/generators' is not true. And that that is part of a larger 'spin of reality' that we don't generally have to think about evaluation order. &gt;But in practice, the situation is nowhere near as bad as you make it out to be. Yes, in some cases you need to be aware of evaluation order, but in most cases you don't. Well, I'm not, and I write a lot of Haskell, and I don't run into issues. I don't make it out as 'really bad'. I make it out, as the truth. That, like in other programming langues, you do have to pick your algorithms carefully. (even if they seem mathematically equivalent) And that is a recurring 'propaghanda' point of Haskell, that is misleading. That is indeed smoke and mirrors. I'm not saying it's harder perse in Haskell, but the claim that you can just ignore evaluation order is just pure spin. &gt;You actually tried to refute a point like that with a feature that most people almost never use for most of their code, except on the outer fringes of their programs? And then were arrogant enough to slap a "QED" on that? I have no words. Since when is an IORef a fringe thing? I assumed I was the Haskell 'amateur' here. Only doing the typical tutorial like stuff for college.. like writing a small compiler. Not doing actual commercial stuff with it. But that's besides the point. You considering it a fringe thing, does not counter the fact that there are references in haskell. And it's just _one_ counter example. There are many other types of references in Haskell. Transactional variables in a STM for example. The claim is misleading. Not only does Haskell support many types of references, it better should because many real things we want to use a programming language for, require some concept of indentity and managing it. &gt;The rest of the so-called refutations are about misunderstandings about type systems (decidable -&gt; inferrable) I'm guess you are referring to the fact that that I claimed that dynamically types languages don't have a performance penalty beyond the initial JIT phase. Yes, just because it's decidable, doesn't mean it's inferrable. But no, the Haskell types are generally inferrable. And a compiler like V8 would optimize them the same way using type erasure, and inlining method-dispatches when encoded as Javascript. &gt; Types don't exist to guarantee nothing bad happens. The best we can do is to strive to minimize runtime errors with things we can guarantee to be absent. I agree. So, if the problem is: how do we deal with the situation when something bad happens, Haskell is not a magic bullet. Very much unlike what the article tries to suggest. A concrete real world example: Imagine we get a null-pointer exception in say Ruby, because some file, which we can generally assume exists, fails to load. In Haskell that problem doesn't go away. It just shows up differently then in a dynamically typed language.. it shows itself as a null-pointer exception there. But it's going to be some sort of run-time error condition in the Haskell alternative as well. Again the article is misleading us. &gt; In most of your posts you paint yourself (implicitly or explicitly) as some sort of voice of reason in a crowd of hysterical fanatics, and that pisses people off to no end Well, actually, just my replies to you &amp; Camccann. &gt;We are not "spinning" or creating "myths" or anything of the sort. No, this gets confusing. The article is spinning the truth. You are insulting, and when you were consistently called out on it, bothered to add some actual counter-arguments. (mixed in with more insults) &gt;This blog post was written to make a point, and not to examine all the pros and cons of Haskell, and I thought it worked quite well for that The point being that haskell does not have any of these 'problems'. Just because they show up differently. It is a misleading propaganda piece. &gt;I apologize for being harsher than I normally I am. Same here. &gt; Your unique combination of uninformed opinions and attitude problems really triggers me, for some reason. Your unqiue combination of arrogance and elitism triggers me. If you don't want to debate something, why bother insulting people? And it's quite annoying that, even when I consistently point out, i'm critizing the article, and in a one case this generic 'myth that laziness somehow magically picks the correct evaluation order' .. .. that somehow it is seen as a critisism of Haskell. How dare I bring it down to the level of other programming languages. 
&gt; If you prefer, feel free to write a critique of the article based on your vast knowledge and life-time experience. The article was well-intentioned fluff. Mostly accurate, but oversimplifying a lot of things and too wrapped up in enthusiasm to be insightful. I don't know why it was posted on reddit, to be honest. &gt; Off course, in between of establishing yourself as somebody that only goes around calling other people idiots, without saying anything meaningfull yourself. Actually, I spend [rather a lot of time helping people learn Haskell](http://stackoverflow.com/users/157360/camccann). What have *you* done? Feel free to provide evidence of your knowledge of Haskell. Ignoring the other nonsense because I still don't care about your bruised ego or ridiculous persecution complex.
To be clear I don't say anywhere in the entire post that jQuery is a Functor or an Applicative. Only that both Functors and Applicatives can inform interesting snippets of code when working with jQuery because there is a *resemblance*. Furthermore, while *I* fully appreciate your examples, attempting to reach a broader audience is not served by ignoring analogues that exist in libraries they use every day. 
If it makes you feel any better, Haskell tends to mislead people about the category theory behind functors as well. What, you mean there's more to life than covariant endofunctors from the entire ambient category onto limited varieties of subcategory? Good heavens!
&gt;The article was well-intentioned fluff. Mostly accurate, but oversimplifying a lot of things and too wrapped up in enthusiasm to be insightful Some claims of the article were just factually incorrect, others were misleading. And i just bothered to point it out. Maybe i wasn't formal enough to please you. But i wasn't set out to do so. &gt;What have you done? Feel free to provide evidence of your knowledge of Haskell. I've critiqued a 'well-intentional fluff' article. :-) The difference is. I don't need to play the authoritive card, because i'm using _arguments_. It's when you are being a dick, and arrogant dismissing statements of others, then it might be relevant. Although still a logical fallacy: http://en.wikipedia.org/wiki/Argument_from_authority While you are reading up on proper debating etiquette. You really ought to read this one as well: http://en.wikipedia.org/wiki/Ad_hominem &gt;Ignoring the other nonsense because I still don't care about your bruised ego or ridiculous persecution complex. Yes you do care. Why else, would you bother to keep insulting me? I don't think it's healthy for you to lie about this to yourself. You like this dick measuring competition in who-can-insult-the-other in the most creative way. Why else would you initiate such a thing, and then keep playing?
The problem is the resemblance, if any, is quite slight. The laws don't hold, the type is constrained, jQuery is doing intensional analysis on the type of the argument to $ so it isn't parametric, I can't return a fresh value in most cases only mutate the one you give me obviating any functional techniques at all, and (f a -&gt; f b) -&gt; f a -&gt; f b is very different from f (a -&gt; b) -&gt; f a -&gt; f b. The former can be trivially satisfied by the identity function for any type f! Functors are useful precisely because of those laws. They tell you not only what the functor's map operation is going to do, but also what it can't do. They are almost everywhere. Applicatives are useful because the laws let it capture a notion of context-freeness in a very general way. There are fewer of them but you can do more with them. Monads are useful because they enable context-sensitivity. There are even fewer, but you can do even more. I would be doing a disservice to remain silent while letting folks walk away thinking they understand applicatives or functors based on the analogy provided. I'm sorry for how negative my replies to the previous post and this one have sounded, but your last two posts have been the latest in a very long succession of posts purporting to connect jQuery to monads or related concepts that go back years. I've been grinding this ax for a while, my apologies for swinging it at your head. ;)
In the first place, I don't think generators on their own would solve your problem. In the second place, I actually answered that question properly. I don't see what's wrong with my solution? And I'd disagree about lazy IO being a bad solution for simple idiomatic exploration of a small state space. For "industrial strength" problems it might run into some tricky bits regarding resource usage, but even then I suspect that it could be handled straightforwardly.
The class constraint requires that we can look up the class instance, which fails for a polytype.
&gt; Have I written commercial applications using Haskell? No. Have you? Actually, `RalfN`, I think this `sfvisser` character *may* have had a go at it once or twice, see e.g. http://www.silkapp.com/ http://twitter.com/#!/sfvisser http://blog.silkapp.com/2009/09/why-we-use-haskell/ etc etc. 
Interesting link. The video doesn't load for me though, on youtube. Don't know why. Very local too. I'm pretty sure i've been to the same college and had the same courses about Haskell as they had. I had the same enthousiasm as well, in the past. They brainwash you well there. (at the UU) But my attempt to actually use Haskell after wards, kind of fell flat. Their usage seems a very limited domain as well: they do all the GUI stuff in Javascript and proccess streams of data out into XML. Haskell seems like a great fit for that particular purpose. I wasn't dissing Haskell though, I was dissing the article, and some of the magic bullet bullshit rethoric. Then again, for people from the same university, it's kind of weird to see them play the 'authority' card. He should have known better. And interestingly, he does point out some of the things I've pointed out as well. &gt;Because of laziness, performance problems can be harder to diagnose and fix. &gt; Since it doesn’t have a user base as large as some other languages, there are less libraries available. This last one is a bit misleading though. Many more people now Haskell than say Ruby. It is mandatory stuff at many colleges. Yet, somehow, the Haskell community isn't very productive in their ecosystem. This is very due to the whole 'avoid success at all costs' approach. 
&gt; I've critiqued a 'well-intentional fluff' article. :-) &gt; &gt; The difference is. I don't need to play the authoritive card, because i'm using arguments. Hahaha, wow. Are you *serious*? It's not "playing the authoritative card", it's *demonstrating relevant knowledge*. This all started because your "critique" demonstrated precisely that you *don't* understand Haskell enough to criticize the article meaningfully. You realize that citing an authoritative source *about the matters they're an authority on* is not in any way a logical fallacy, right? &gt; Why else, would you bother to keep insulting me? I haven't insulted you. I've made objectively true statements about your knowledge of Haskell, the quality of your arguments, and remarked on your poor attitude and unfounded belief that you're being unfairly persecuted. You're the only one making outright personal attacks here. And I particularly haven't made unrelated attacks on your character and claimed that that somehow undermines your argument on the actual subject, which is what ad hominem actually means, since you don't seem to understand debating terms *either*, good grief. You know what, I give up. Have fun making "arguments" without any knowledge to back them up. Have you considered a career in politics?
Both ghci and hint use the same GHC API as I know. I am not sure, but hint does not spawn a separate process. Starting an Interpreter action is slow because it has to load and bind several dynamic libraries, I think.
Maintainer in question here. I suspect you're doing something wrong. Did you compile this with the latest GHC head and give it the `-fplugin` argument? This feature is not available in 7.0 - you need to get 7.2, install the 'strict-ghc-plugin' package with `cabal install`, and *then* compile the test with something like `~/path/to/ghc-head -fplugin=Strict.Plugin`. Proof from my machine (using a 2 week old GHC, my later package db's seem to have fumbled somehow): 2:02:13 a@kratos tests master ~/ghc-head/bin/ghc-7.3.20110703 -fplugin=Strict.Plugin -fforce-recomp NonTerminating.hs [1 of 1] Compiling Main ( NonTerminating.hs, NonTerminating.o ) Loading package ghc-prim ... linking ... done. Loading package integer-gmp ... linking ... done. Loading package base ... linking ... done. Loading package ffi-1.0 ... linking ... done. Loading package array-0.3.0.2 ... linking ... done. Loading package containers-0.4.0.0 ... linking ... done. Loading package filepath-1.2.0.0 ... linking ... done. Loading package old-locale-1.0.0.2 ... linking ... done. Loading package old-time-1.0.0.6 ... linking ... done. Loading package unix-2.4.1.0 ... linking ... done. Loading package directory-1.1.0.0 ... linking ... done. Loading package pretty-1.0.2.0 ... linking ... done. Loading package process-1.0.1.4 ... linking ... done. Loading package Cabal-1.11.0 ... linking ... done. Loading package bytestring-0.9.2.0 ... linking ... done. Loading package binary-0.5.0.2 ... linking ... done. Loading package bin-package-db-0.0.0.0 ... linking ... done. Loading package hoopl-3.8.7.0 ... linking ... done. Loading package hpc-0.5.0.6 ... linking ... done. Loading package template-haskell ... linking ... done. Loading package ghc-7.3.20110703 ... linking ... done. Loading package syb-0.3.2 ... linking ... done. Loading package strict-ghc-plugin-0.1 ... linking ... done. Linking NonTerminating ... 2:02:16 a@kratos tests master ./NonTerminating Stack space overflow: current size 8388608 bytes. Use `+RTS -Ksize -RTS' to increase it. 2:02:18 a@kratos tests master In fact you shouldn't have even be able to compile this technically since the package depends on exports exposed only ghc &gt; ghc-7.2 (the `GhcPlugins` module,) but I speculate you were in the top-level directory and did something like `ghc tests/NonTerminating.hs` which would cause GHC to automatically search the CWD for hierarchical modules, and pull in the standalone `Annotation.hs` file (thus not attempting to compile `Strict.Pass` or `Strict.Plugin`.) Granted this could all be clearer in the readme.
Forgive me for being a nay-sayer, but shouldn't you be hacking on ddc, instead?
&gt; They don't have an Eq instance, for example Actually IORefs have it.
I expect this "feature" to be a bug and being closed quite quickly by Heroku. I wouldn't want any strange binaries from some place in my system...there's a reason why they do the compilation themselves...
I used ghc from 2 days ago, but I had no idea about that -fplugin arg (I used ghc --make).
Some major points: * A GUI that used IO threads and communicated with normal Chans would expose an IO API. That IO API would be very usable with results of STM transactions. I really don't see what problem you're alluding to here. * There is no way in hell that un-inferrable programs in Javascript would be translated to Haskell and encode all the runtime types "as data". When you translate to Haskell, dynamic typing disappears. Period. If not, you're doing it wrong. * I deal with databases too. My database keys are identities (that I would have to explicitly deal with in any language) and my explicit dealing with them is a tiny fraction of the code. Every other bit of code does not deal with any identity issues. In other languages, **everything** is complicated by identities. * Sure, Haskell may not solve all runtime problems yet, it isn't Agda. But it solves a whole lot of them. Every thing that is different about Haskell in this respect is a *huge* improvement. &gt; Yes. I think it even goes back to the Smalltalk era, when they figured out that, as long as a static type system is decidable, you can infer it. What does this even mean? Agda's type system is decidable, do you think you can forego all of its static types and have a compiler generate automatically all the assurances? I think that's absurd. &gt; But here's a strong counter example of dynamic typing &amp; reflection at work. Where? Why not use fclabels for all your boilerplate? &gt; You could so something similar with Template Haskell, but not at run-time. You are aware that the Data library can do reflection at runtime, right? But reflection at runtime is generally a bad idea. &gt; Dynamic typing really isn't important. But in combination with reflection it does allow us to do usefull that aren't otherwise possible. They are possible in Haskell. &gt; But then you can still get a bunch of error conditions at run-time that you need to deal with. The dynamic nature of the problem, doesn't go away. You aren't trying hard enough. Perhaps if you post some code, I can show you how the dynamism mostly goes away. 
&gt; There is not a problem unique to haskell here. I was just claiming that all the issues associated with managing identities, doesn't go away. It's still here. But he made it clear he was not talking about dict-of-string identities. He was talking about **object identities**. Those do go away. And you rarely have to think about them anymore. &gt; And there isn't even a defacto way to manage it. So any library exposing identities will likely not have 'compatible' identities at all. Object-identity in an OOP languages, the most likely candidate for interoperation between different libraries. There's no de-facto way to manage explicit identities in any language I know. Object identities are not very useful for the cases you would use explicit identities in Haskell. &gt; Mine aren't late, over-budget or under-featured. What they are is "nothing special". And 99% executed in the context of this software, isn't mine at all. It's libraries, database engines, web-servers, etc. Maybe you are taking on less ambitious projects, or maybe they account for a bigger budget. Either way, the data disagrees with "There's no hard problem here for Haskell to solve". &gt; I spent some time debugging. But much of that is almost mandatory and wouldn't go away if I was using Haskell. I still have to debug the HTML, i still have to debug all the database interactions. Where there are performance problems, and when to properly cache stuff. I don't do web programming. I do systems programming. We don't have to deal with the bugs in other big pieces of software interacting with our code. If there's a problem, it's our problem. And there are lots of very expensive problems, and they would generally mostly go away in a language like Haskell. &gt; Well, why not do the experiment? Just take one days worth of code, and try to reimplement it in Haskell. One day's worth of code is typically a small component in a big project. Making it interact with the rest of the project would be a big FFI-binding project. As I said, Haskell is not suited for the kinds of systems programming that we do. &gt; I've tried it. And it wasn't a success. Mostly because of the ecosystem, but also because i felt there wasn't a clear 'One Way' to do things. The stuff that easy in other languages, like say maintaining state about a database model, just required more and uglier code in Haskell. I think you're too quick to blame Haskell, when it's likely you're far more trained in these other languages than in Haskell. Maintaining state about a database model in Haskell takes less prettier code. &gt; I assume you are using C. Interestingly, I think the reason you spent so much time debugging, isn't specifically related to the type system. **But as I already said, I have empirically examined many of the bugs** and reached the conclusion that a type system would have caught them. And there you go, with 0 data, claiming I was wrong and it was not really the lack of type system's fault -- but readability. Readability would help avoid some of the bugs, perhaps, but **not the vast majority of them**. And yes, we do complicated stuff, with very harsh performance constraints. A more powerful type system would help us a lot. &gt; It's also funny, that many people tend to say "i love haskell.. i think it's brilliant for everything, except for [my-domain]" Who are these "many" people? I think you're repeatedly generalizing from "one" to "many" without justification. &gt; I didn't experience it that way at all. Perhaps thats my error. Perhaps the document is kind of sloppy in that regard, I don't know. Perhaps you've mistaken having different concurrency libraries that deal with *different* trade-offs as having incompatible ways to do things. Hell, you even lumped *parallelism* libraries with those -- and those are clearly not even in the same domain. Perhaps you've mistaken the ability to explicitly forfeit safety from NULL pointers in rare cases to still having NULL pointer unsafety in general. Perhaps you've mistaken having "AttributeErrors" all over the place with having *any* rare dynamic fault. For some reason when someone talks about the mostly-useless *object identities* being gone, you mention a completely different beast of explicit identities as still existing -- which is unrelated. You're really grasping at straws to try to find the rare exception to the OP's examples which is really virtually irrelevant.
Ah, and why can't it generate a polytype from an interpreted string?
Ah, thanks. I wonder if people use those similarly to identity-comparison of OOP languages. If many did so, then the OP would indeed by wrong about identities being gone, but there's so little use of IORefs in code in general (as opposed to use of objects in OOP languages, that is), that it's still safe to say that identity considerations are mostly gone.
Is this strict Haskell that needs everything to be compiled as strict, or can the strict parts interoperate with lazy Haskell?
&gt; &gt; You actually tried to refute a point like that with a feature that most people almost never use for most of their code, except on the outer fringes of their programs? And then were arrogant enough to slap a "QED" on that? I have no words. &gt; &gt; Since when is an IORef a fringe thing? I assumed I was the Haskell 'amateur' here. Only doing the typical tutorial like stuff for college.. like writing a small compiler. Not doing actual commercial stuff with it. &gt; &gt; But that's besides the point. You considering it a fringe thing, does not counter the fact that there are references in haskell. And it's just one counter example. There are many other types of references in Haskell. Transactional variables in a STM for example. I built a website (which is actually running in production today) written in 7000 lines of Haskell, and I never once used IORef or any of the "other types of references". I think that pretty much validates the points godofpumpkins made. 
You can annotate top-level functions with a `Strictify` data type that will cause the Core pass to analyze them. See the `NonTerminating` test in the `tests` directory. Currently you can only strictify at the granularity of function declarations, although I could probably implement it for a whole module at a time too. Anyway, the transformed code can easily co-exist with the lazy parts of haskell - the actual *transformation* at the core level in fact, is just replacing `let` (lazy) with `case` (strict) in the body of a binding with the annotation.
This was originally implemented by Max long ago way, way before DDC was ever even announced. It's mostly a feature to show off how to use the plugins infrastructure in GHC - a strictification pass is actually extremely easy (transform `let` to `case`) so it was a simple candidate. You can actually find details and formulations of this plugin in its *original* form back in a 2008 edition of The Monad.Reader - that's all the way back when max was originally doing this plugins work. I've just updated everything and tried to help get it merged. &gt; but shouldn't you be hacking on ddc, instead? Compiler plugins, and by association, this plugin, are completely orthogonal and have absolutely 0 to do with DDC, as far as I'm concerned. So no.
I coupled a `Makefile` with some of the other plugins so you could easily and correctly compile the tests, but not the strict-ghc-plugin repository it seems. Anyway later today I might add READMEs and to all the plugins and a Makefile. It could be clearer as to how you might use it.
anyway, how does it work? does it force args or result to be in rnf?
Judging by the excerpt, those exercises seem better than average. Thanks.
No - look at Strict.Pass [here](https://github.com/thoughtpolice/strict-ghc-plugin/blob/master/Strict/Pass.lhs). In Core, there are basically 2 ways to bind names to expressions - `let`, and `case`. `let` creates a thunk upon a binding, while `case` will evaluate the binding to it's outermost data constructor (WHNF.) The plugin is simple in that it simply systematically replaces all uses of `let` with a simple `case` equivalent. It also must transform the arguments of applications due to the fact `e1 e2` as a lambda term is equivalent to `let x = e2 in e1 x [where x is fresh]` - GHC can transform function applications into explicit thunk-introducting `let`s when generating STG code later in the pipeline, so the plugin must also account for any possible implicitly introduced thunks too. Also see Max's 2008 article about this functionality; it's in [The Monad.Reader Issue 12](http://www.haskell.org/wikiupload/f/f0/TMR-Issue12.pdf).
Why shouldn't strict evaluation be available in a much used an excellent compiler? Instead of just in a research compiler? I see no problem here. 
The fact that these names are scary is not a feature of Haskell, nor a feature of the names, but of your own psychology.
Because that strictness pass still doesn't give you effect types, mutability, lack of need for monads, and other goodies. Haskell isn't particularly *meant* to be a strict language. And, no, ddc isn't a research compiler. Well, at least it doesn't *want* to be one. Also, why shouldn't a magnificent competitor to Haskell be brought to production quality? Anyway, my question has been answered: It's about giving the plugin system a spin.
DDC is a very different language. In particular, I rather prefer working with Haskell's type system to DDC's type and effect system, DDC's do notation is intrinsically tied to fiddling around with mutation and effects. This is rarely the reason why I'm using a monad in the first place. GHC has a large number of other features that haven't been mixed with type and effect systems yet either, so it isn't clear what costs are associated with enriching the type system with a notion of effects. That isn't to say that some times I miss what DDC has to offer from an effect tracking standpoint, but it doesn't help me work on the stuff I want to work on. &gt; Also, why shouldn't a magnificent competitor to Haskell be brought to production quality? I welcome it to the functional programming language gene pool, but right now I think the burden of proof that the concerns it focuses upon are useful to focus upon is still on DDC.
Does the strictification happen before optimizations? It looks like a really useful thing to do when writing stupid number crunching code.
Yes. Look at [Strict.Plugin](https://github.com/thoughtpolice/strict-ghc-plugin/blob/master/Strict/Plugin.hs) - plugin installation functions are invoked with a list of compiler passes to be run over Core (previously constructed based on optimization settings, etc.) They insert their own compiler pass into the list and return it to the optimizer, which runs them. As you can see, this plugin makes sure the first thing in the pipeline is it's own `strictifyProgram` pass. &gt; It looks like a really useful thing to do when writing stupid number crunching code. You'd have to try yourself and see what happens honestly. :)
[See this thread instead](http://www.reddit.com/r/haskell/comments/itkii/rewritten_agda_introduction/) First I posted the introduction, but it was marked as spam so that another post was made instead.
Actually, dbpatterson was having troubles getting a Snap app working and opened up a support ticket in which they gave him suggestions to get his binary working. Its not like Heroku is a shared hosting provider- what they give you is supposed to be isolated and monitored, so the only downside for them is that unsupported language customers will probably be less satisfied with their service and might ask for more support.
so if foo is marked as strict, then whenever the result of foo x y z is demanded, it will compute x,y,z and the result completely? what about foo x y?
Yup. All of my own projects are on github, but I'm no stranger to sending patches by mail.
Hmmmm, interesting point!
Hopefully a bit of work will get done at hac-PDX this weekend: http://haskell.org/haskellwiki/HacPDX-II/Projects#Projects I'm not at all familiar with the codebase, but I see a lot of opportunity for a social hackage to be very useful - mainly to allow people to contribute documentation. I'm imagining per-function commenting / discussions / user provided examples / etc. Ideally, there'd be a system to assist the user in integrating user comments into the haddock, but perhaps just including provisions for removing integrated comments will be enough. (it makes sense to preserve comments across versions - not exactly trivial).
Existential types are decent at "simulating" (really they're very, very similar on a type/theory level) OOP's dynamic dispatch, but only for the single-dispatch case. Do you actually find yourself ever wanting open sum types (as have been properly done in some papers and Haskell extensions before) or are type-classes + existentials generally "good enough" to give all the dispatch functionality you want?
I'm not sure! Maybe it comes back to not being able not being able to marshall across the 'Dynamic' type if there's no Typeable instance, which there won't be for a polytype. Although I'm not sure where the Typeable dictionary gets magicked up from on the other side - this might fail at run-time if we don't type-check to a type with a valid class dictionary. The type witness means we fail at compile time if we thought we wanted a polytype, instead of beating our heads against the wall with run-time errors. It might be more productive to read the source rather then just guess about it, at this point :-)
You don't really get anything out of this though, besides idiomatic clarity (once you start reading '$' as 'apply'). You could just as easily have written: zipWith id [sin, cos, tan] [1,2,3] 
You're using an operator section; "(map $)" is not "map ($)". In fact, for any function f, (f $) is the same as just f.
I especially enjoyed un petit verre de verveine.
ah, merci!
&gt; It doesn't look like Agda is getting tactics anytime soon Actually, people are working on metaprogramming in Agda which should allow for tactics for Agda to be written in Agda no less.
Nice! Anyone know where to get clojure stickers?
The fact that it works for things other than type-level integers is sort of why it's *not* an indexed set.
:) The trouble is that I don't know how to completely implement all those Goodies yet. If I did then we would have already taken over, but until then there's a lot more work to be done...
Disciple doesn't eliminate the need for monads. It does try to help manage state, but State is only one instance of the general monad structure. It's true that the costs of a mixed type and effect system still need to be explored. Disciple also has region and closure typing, and we've only just started to scratch the surface of those. 
And tshirt!
It's indexed. But it's indexed by `*`, which is the case for all GADTs in GHC.
I remember the lambda calculus chapter of Types and Programming Languages is short but not bad.
I remember the lambda calculus chapter of Types and Programming Languages is short but not bad.
Yeah. For any function f is the same as (f $). For some of us slower kids that's neat. Humor us.
Will they stick immediately after they are applied? If so, can they really be genuine Haskell stickers? I am not a PLT expert so please help me here. 
Ordered - these will be on my office door soon!
I went ahead and ordered some too! Whoever tracks the orders: "What the hell is it with all these weird stickers going out?"
Ordered a set too, can't wait for them to come! EDIT: On a side note, thanks for introducing me to this site, seems like there's a lot of cool stuff on there!
I think they mean indexed by data, tho. The examples are all properly dependent types, and the description of Vec is as a set that has a parameter *and* an index, suggesting that the author clearly considers only the values to be indices.
Types can be indexed by `Set` in Agda, which is comparable to GADTs in Haskell. Indexes and parameters are distinguished by what side of the colon they comes on in the data declaration. For `Vec`, it's: data Vec (A : Set) : Nat -&gt; Set ... `A` is on the left of the colon, and thus a parameter. The `Nat` is on the right, and is thus an index. The difference is that parameters must be uniform in the result types of the constructors, while indexes need not be (and parameters are roughly in scope over the whole data declaration, while things that appear in indexes must be bound by the constructor if necessary). You can have parameters with types other than `Set`. The simplest example is probably a definition of the identity type: data I (A : Set) (x : A) : A -&gt; Set where refl : I A x x The `A` and the `x` are both parameters, but `I` also has an `A` index. An example of a type-valued index would be type reps: data TypeRep : Set -&gt; Set where TyNat : TypeRep Nat ...
Hey, you get no downvotes from me :) One thing to remember is that ($) is just 'id' with a restricted type (only works on functions), as an infix operator. (1 +) = ((+) 1) -- so (f $) = (($) f) = (id f) = f 
Interesting. It makes me have a "What does it all mean?" moment.
It has just occured to me that it's "Avoid (success at all costs)" and not "(avoid success) (at all costs)"! Now the quote finally makes sense!
Hey sclv, thanks for answering in SO! I'm kinda unhappy with the answers being not very simple. None of that is your fault, because probably the only answers for this aren't simple. When I find time I'll give a shot to implement all the stuff from my "List" package for lists represented as "unfolds" as you described, and see how happy I am with the code for that and consider if I'm in favor of ditching the straightforward representation of lists for those. Thx.
My experiments with GADTs tend to end with the conclusion that they work well when the type parameter is directly used, as in the standard typed interpreter example, and badly when the type parameter is a witness. You can actually see the seed of the problem in the linked example; while the sum type is closed, the type parameter isn't. e.g. data Expr data Patt data Type data AST a where Apply :: AST q -&gt; [AST q] -&gt; AST q Deconstruct :: String -&gt; [AST Patt] -&gt; AST Patt Iff :: AST Expr -&gt; AST Expr -&gt; AST Expr -&gt; AST Expr Var :: String -&gt; AST a and I would want to restrict `q` in `Apply` to `Expr` or `Type`. This can be encoded as Apply :: (ExprOrType q) =&gt; Expr q -&gt; etc. but that gets horrible really quickly, and GHC can't properly reason about it because type classes are open. On the bright side, GADTs actually handle this kind of recursive case, which I don't think can be said for variant types. One of [SHE](http://personal.cis.strath.ac.uk/~conor/pub/she/)'s main features seems to address this, but I haven't tried it, so I don't know what its limits are other than what the website says.
I link-danced like a fly against a hot summer window, looking for a succinct paragraph defining from scratch what dbus-core actually does. Add this paragraph to a few of the "home" candidates for this package? Don't need to know it's "there", it probably _IS_ there somewhere, but if I gave up before finding it, others will to, which could hinder adoption.
There are a lot of broken links :( The first link goes to an empty page, and the second has links to github pages which seem to have moved. I assume because it's in Haskell-Blackboard and that's now changed to reactive-banana.
Oh, indeed; thanks a lot for reporting this. Should be fixed now.
Ah, sorry about that. There's an introduction in the PDF, but I didn't copy it to the release notes page. * D-Bus is a cross-platform IPC protocol. It's heavily used on Linux, FreeBSD, and other open-source desktop platforms. [More info on D-Bus](http://www.freedesktop.org/wiki/Software/dbus) * dbus-core is a pure-Haskell implementation of D-Bus. It's better than bindings like the "DBus" package because it can provide a more idiomatic API.
Actually, I thought it meant the latter. See [this article](http://www.computerworld.com.au/article/261007/a-z_programming_languages_haskell/?pp=10) to read about the origin of the quote. The way I interpreted the article was to read it as saying that they couldn't be flexible with the language if it became popular and they had to worry about compatibility/legacy issues when designing new standards the way that Python does.
The LC chapter of Types and Programming Languages does indeed make this simple. It starts with an encoding of true and false, followed by an if-then-else construction, then other logic functions.
How do I port the following code from `DBus` to `dbus-core`? outputThroughDBus :: Connection -&gt; String -&gt; IO () outputThroughDBus dbus str = do msg &lt;- newSignal "/org/xmonad/Log" "org.xmonad.Log" "Update" addArgs msg [String str] send dbus msg 0 return ()
... oh, right, I forgot to commit signal emitters. Install dbus-core 0.9.1, then use: {-# LANGUAGE OverloadedStrings #-} import DBus.Client.Simple outputThroughDBus :: Client -&gt; String -&gt; IO () outputThroughDBus dbus str = emit dbus "/org/xmonad/Log" "org.xmonad.Log" "Update" [toVariant str]
Thanks! Finally, I can run Xmonad with `xmonad-log-appet` compiled with GHC7 w/o having to patch up `DBus-0.4`... :-) btw, I guess you rather meant `dbus-core` instead of `dbus-client`... :-)
Why are there so many articles in here named "&lt;Something I've never heard of&gt; in Haskell"? Why not make the title descriptive of the purpose or advantages of the thing being discussed, or the type of issues it helps resolve?
&gt; btw, I guess you rather meant dbus-core instead of dbus-client... :-) I am not a clever man.
Because that's not what the authors are interested in talking about. Just skip over them if you're not interested in them. It's OK. (I do not mean this sarcastically.)
I can't tell whether I'm interested in them!
It usually turns out that I am, but wouldn't know it from the title.
Awesome, we need more bindings like these to make Haskell more attractive for desktop applications. Good work man, have an upvote.
Mmm, that's not so good! Maybe if Johan ever figures out how to inline all instances of Maybe as NULL versus a pointer this will become a lot cheaper.
Well, the trouble with mathematical concepts is that they tend to solve lots of problems, and actually characterizing them is quite difficult. You can pick one characterization for the title, but really a post like this is trying to explore the various dimensions of a simple definition. You could post the definition, but I'm not sure that would help...
Nice, but unless you want a C&amp;D letter from Valve, you should probably name it something else.
I don't mean to sound condescending, but /r/haskell isn't really a "skim-the-title" subreddit, and for many of us, seeing a concept we're not familiar with in the title makes us want to read the article, so it's an effective way of getting readers if you're submitting. I remember in the early days of Reddit /r/programming was like this too. I submitted an article detailing the use of zygohistomorphisms without explaining what they were, and it got a reasonable number of upvotes for the time. In those days, people would see something they didn't yet understand and be excited to have the opportunity to learn more. /r/programming isn't like that anymore, but /r/haskell still is, and I hope it doesn't change anytime soon.
So after reading this (having never heard of profunctors before) I saw his `UpStar` example which takes a normal functor and turns it into a profunctor. I thought maybe I'd play with my favorite simple functor, `Maybe`, to try to see what the result was. So first we have that Upstar Maybe a b = UpStar (a -&gt; Maybe b) So, the function behind the constructor is a generic one that can fail. So what do `lmap` and `rmap` do? lmap :: (a' -&gt; a) -&gt; UpStar Maybe a b -&gt; UpStar Maybe a' b lmap k (UpStar f) = UpStar (f . k) rmap :: (b -&gt; b') -&gt; UpStar Maybe a b -&gt; UpStar Maybe a b' rmap k (UpStar f) = UpStar (fmap f . k) Well, it's pretty easy to see what's going on here. `lmap k` applies our function `k` to the action that can fail *before* the action is carried out -- that is, it applies `k` to the input of that function -- and `rmap k` applies the function `k` to the *output* action. So basically, if `maybeFunc :: a -&gt; Maybe b`, then you could write something like \x -&gt; a . b . c &lt;$&gt; maybeFunc (g . h . k $ x) as rmap (a . b . c) $ lmap (g . h . k) $ UpStar maybeFunc modulo the container, obviously. Note that the order of the `rmap` and `lmap` don't matter, which is kind of cool. The r and l kind of threw me, since I think of stuff in function composition order, whereas his notation is much more in dataflow order. But that's not a big deal. I think this specific example illustrates what sigfpe is talking about when he says that "the first argument of a profunctor describes how an element related to a type is sucked in, and the second describes what is spit out." Obviously for this simple kind of example this kind of plumbing is overkill, but it does seem like it could be a useful abstraction in other instances. If you think of a "profunctor" as a pipe where functions are applied to stuff before it gets sucked into the pipe (on the left) and after it gets sucked into the pipe (on the right), it seems like a lot of different situations would apply. It's pretty different from how I think of a functor, as a generic container that provides a means for me to modify its contents without taking things out of the container. Profunctors seem more dynamic, that they represent stuff moving and I can do stuff to what's moving either before it enters the profunctor or after it exits. Perhaps this intuition is too related to the specific example I took a few minutes to explore. Any comments?
That'd be cool!
I am still using wai-handler-devel for interactive development because as i remember there were problems with yesod-devel. Are those problems fixed ?
I believe this also requires the MissingH package for Data.List.Utils? Anyways, good start. The refresh behavior makes it pretty hard on the eyes. There must be a better way. Curses maybe?
&gt; The solution is actually very simple. There is now a yesodweb organization on Github. Instead of an individual repository for each package, it has "megarepositories" grouping together common packages I wish Cabal/Hackage would allow such "umbrella packages". Basically, similarly as we have hierarchical modules, we should have hierarchical packages. The fact that we have package sets (and a lot) like * haskelldb-flat * haskelldb-hdbc * haskelldb-hdbc-mysql * haskelldb-hdbc-odbc * haskelldb-hdbc-postgresql * haskelldb-hdbc-sqlite3 * haskelldb-hsql * haskelldb-hsql-mysql * haskelldb-hsql-odbc * haskelldb-hsql-postgresql * haskelldb-hsql-sqlite * haskelldb-hsql-sqlite3 is imho a very clear sign that some new way of organization is needed here. 
When I checked few months ago, yesod-devel was still kind of sketchy. If I remember correctly, its compilation loop was also significantly slower - which, in my opinion, defeated the purpose of it. I know wai-handler-devel doesn't support the scaffolded config directory. What did you do to get around that?
Actually it supports config folder, you just need to put it in cabal file: hs-source-dirs: ., config But i simply moved the source files out of it and just left the routes file in there. 
He should probably make a cabal file to encode those dependencies. I may make one for him if I'm feeling nice...
You will want to add a cabal file for your project so that others will be able to build it correctly. You will also need a cabal file if you plan to put it on hackage. Remember, if it's not on hackage it doesn't exist. Otherwise, good job!
Can you say more about what you mean? Do you just mean the presentation on the hackage website or something more fundamental?
Yes, it reminded me a lot of arrows. So I guess the question then is, what would be an example of a profunctor you might use in Haskell that is not an arrow? Not that having a use case makes any difference, it's cool either way, but I'm curious.
I tend to use the Maybe monad in those cases, given that in this context: a &gt;&gt;= b is equivalent to: case a of Just x -&gt; b x Nothing -&gt; Nothing The maybe function is another way I use to avoid having too many nested case expressions.
At least a cabal file. I'm less sure about when to put Haskell applications on Hackage. In this case, the program itself is not at all likely to be of any use in Haskell development and won't be a dependency of any other Haskell program. Target users (supposing it gets to the point of having any) won't be likely to have cabal-install, nor know how to use it. Basically, there aren't the same ironclad reasons to use Hackage that you would have for a library or development tool. I don't know if I'd put a package like that on Hackage if I built it.
Stop down voting, these are amazing.
I looked at the Google Books preview of TaPL. I was surprised at how readable it is. I wish the preview contained more of the "Mathematical Preliminaries" chapter so I'd know for sure before I drop $40 on a used copy. Maybe I just need to finish up Haskell Road to Logic Maths and Programming. I know sets, relations, and functions will be properly covered. 
I haven't thought about it very deeply, but I think something more fundamental than just presentation is needed. A very typical situation is that there is a core package, and there are extra features needing fancy dependencies, and also bridges to other packages. Now, one would of course want to minimize the number of dependencies, so the core package should ideally work without any unnecessary packages, and similarly, a single extra or bridge should work without without the others. This is key to achieve modularity. Traditionally, this was solved by compilation flags, which in practice looks like a pretty bad solution to me. So now the accepted ad-hoc solution on Hackage is to have seperate packages for all these, which is definitely better than compilation flags, however one would like to organize these packages into a tree instead of a flat list. Basically, one would like to * download and install all related packages by a single command (maybe refined by the option to install only those extras which has the necessary "big" deps. installed; for example if something has a Cairo and an OpenGL backend, and I have only OpenGL installed, then a sensible default would be to install only the OpenGL backend). * maintain the related packages in the same repository, and upload them to Hackage together. * build the related packages together and probably some more which eludes me at the moment.
In my opinion, putting an application on Hackage does make sense and there are several there from XMonad and Darcs to raincat. I didn't bother trying raincat until it was on Hackage. Having a cabal file allows distro maintainers to more easily package it for a distro and the Hackage RSS feed means they can more easily watch for updates. When it comes to software distribution you want to make it as easy for people to get it as possible. Hackage is just one more way people can easily try it out. Creating app bundles to install for the non-Haskell crowd can (and should) still be done for a game like this.
Second two are a matter for the tools like cabal-install to make that kind of thing convenient. For the first, it sounds like you're asking for a one package to be able to "recommend" rather than depend on another package. Then hdbc would "recommend" hdbc-odbc, hdbc-postgres, hdbc-sqlite etc. Then one policy is to always try to follow such recommendations when their dependencies can be satisfied.
&gt;Perhaps you formalists know handwaving over the details as the “Axiom of Choice,” no? I hearby invoke it. Admitting your lack of understanding of mathematics referenced in the Haskell community and then demonstrating it to belittle some of your readers is not a good strategy to get people to read your post.
His point would have been far better made if he did away with the pseudoformalistic nonsense. 
Agreed; it's very distracting and seems more than a little antagonistic. Although, I'm not sure what his point *is*; I'm sure #haskell would tell any self-identified newbie who asked about the Comonad.Reader that it's probably not the most appropriate reading for them at this point. Does he want warnings on the thing itself?
I tend to think that was meant to be more tongue-in-cheek than anything else. I wouldn't take it as a deliberate slight. &lt;troll&gt;and besides, the Axiom of Choice is dumb anyway, not sure I can argue with his version of it&lt;/troll&gt;
Intentional or not, it's distracting, and he certainly goes on about the Axiom of Choice a lot (and quite non sequiturally. Is that a word?)
The most significant dispute I can lodge against this--and the one place where I think he's deeply, seriously missing the point--is that rather a lot of the formalism and crazy abstraction that people engage actually *is* useful and practical. This is why things seem jumbled together, why chunks of arcane math will be sitting right next to something mind-numbingly normal like an XML parser. There's no clear dividing line that says "Over on this side, is all the weird math and formal proofs and general abstract nonsense, on that side is all the pragmatic day-to-day stuff to get things done". It's a smooth spectrum of applying more layers of abstract concepts to structure code. To support this assertion, I'll dig up [this old comment](http://www.reddit.com/r/haskell/comments/cs54i/how_would_you_write_du_in_haskell/c0uvqqo), from a post asking for how to implement a simple utility in a functional, elegant, and practical way. The solution given draws on *exactly* the sort of stuff that the OP here complains about; yet it's arguably the best and, in many ways, conceptually *simplest* version given. Other than that... I've said before that Haskell has a comparatively steep learning curve; but I feel that it's also steeper than it actually *needs* to be, and that more could be done to improve the situation. So I can't really dismiss the complaints here out of hand. This is someone who's clearly *actually trying* and, in fact, actually *uses* Haskell. It's easy to look at something like this and say "well, why didn't you just ____?", but... well, why *didn't* he? I don't think most people would choose to frustrate their own efforts in learning a language that they enjoy. I can say with absolute confidence that it **should have been easier for Darrin to learn Haskell**. It should be easier for him, and everyone, to keep learning! What could be done to make things better?
How long ago did you learn Haskell? I feel like nowadays the answer to almost every beginner question in #haskell or Stack Overflow or whatever is "Read this chapter of RWH" or "Read this section of LYAH" and people do a pretty good job of steering beginners in the right direction. So I guess a lot has changed since you started learning Haskell?
My standard advice is LYAH as a light intro or for people new to programming in general; RWH for demonstrating practical use or for people coming from years of using mainstream languages; and the Gentle Introduction for bragging rights or people whose other hobby is ritual self-flagellation.
I think it has gotten better. There is a haskell-beginners mailing list that does a very good job of answering basic questions. There's also the stackoverflow haskell tag which does a stellar job as well. One can browse the archives of these and learn all sorts of stuff. When I first started in haskell none of this stuff existed. When I got stuck, I stayed stuck. I too tried to sit in irc and it also didn't work. Had I the resources that exist now I would have figured this stuff out five times as fast.
sure, but we get the idea. just imagine it to be a plot device.
Am I not allowed to offer writing suggestions about a post so grating I stopped reading it the first time through?
I think you have the same problem with traditional programming languages, just in a different form. Whereas with Haskell you have type theory and academic papers, with Java/C# you have enterprise-style stuff and various layers of architectural abstraction. If you took a beginner C# programmer and introduced him to MVC/MVP/MVVM from the get-go, I think they'd be pretty scared as well. And yes the "formalism and crazy abstraction" comment applies here - when you use these patterns/abstractions in the right situations, they *are* "useful and practical". Perhaps the reason people run into funny abstractions in Haskell earlier because the "depth" of code required to need/use these abstractions is a lot smaller. See (e.g.) Oleg's examples, which often introduce a mind-bending concept in the space of half an email. By comparison, a simple OO-UI pattern example will be a zip file blob, and only really makes sense when you reach a certain level of complexity. Edit: Thinking again, perhaps another difference is: - in Haskell, you ask how to do something, and the answer is "it is *only* a... [zygoprepohistomorphism]", thereby reducing the problem to something else (perhaps more arcane) - in C#/Java, you ask how to do something, and the answer is "you should be using the [PullerSingletonJumpingFactory]", expanding the work you should do in order to reduce the complexity Ok, rambling thoughts now.
I think a useful learning tool might be for the community to compile a collection of programming techniques programmers commonly use (and for the most part, already have names for) in Haskell, varying from introductory ideas like "bottom-up design" to more advanced techniques like "using phantom types to guarantee isolation". Essentially, it would be a collection of design patterns. And then, (this is the important part) construct a directed graph that represents the order in which ideas should be learned in order to make sense. For instance (to give an obvious example), you should learn about "functors" before "applicative functors". This graph could serve as a reference for Haskell programmers when they encounter an unfamiliar idea. It would both serve as a tool to locate the background information required to understand the new idea, and perhaps also as a warning sign that the idea is over one's head, if it has a large tree of other unfamiliar concepts. In a way, the Haskell wiki is already somewhat set up this way, I'm just advocating for a more formal organizational scheme.
I can somewhat understand this .... I had the same feeling from time to time. But on the other side: I really like this "formalist"-view and I indeed want to know a lot more about it but it's rather hard to find the right sources for the "formalist-noob" - maybe you guys here can point me to the right literature. And no fear - I have no big lambda-calc. or cat. theory background and I'm no CSler but I know my math ;)
Well, UpStar and DownStar can be applied to things that are not monads and comonads, the result then fails to be an Arrow but is a Profunctor. And while you can compose the profunctors, you don't get something that acts like an arrow.
I've uploaded 3 packages to hackage. [profunctors](http://hackage.haskell.org/package/profunctors) provides the basic Haskell 98 core definition for the Profunctor class. I adopted sigfpe's naming here. [profunctor-extras](http://hackage.haskell.org/package/profunctor-extras) provides profunctor composition and a category (well, semigroupoid) for the [collage (aka cograph) of a profunctor](http://ncatlab.org/nlab/show/cograph+of+a+profunctor). [representable-profunctors](http://hackage.haskell.org/package/representable-profunctors) which provides the indexed (co)representation of profunctors. Basically the ability to say that a given profunctor is isomorphic to the UpStar or DownStar of some functor, along with the fiddly bits of plumbing to show that the composition of representable profunctors is representable.
There's three different 'Haskell curricula' posted as replies to [this stackoverflow question](http://stackoverflow.com/questions/6003728).
I've had some experience with this as well, as I tend to seek out people to teach Haskell to. First of all, #haskell itself, last I looked, was one of the resources that you hope no one stumbles upon without a really strong math background and an interest in using the programming language for exploring abstractions. I literally try to keep people away from #haskell for at least a couple months of initial exposure to Haskell. Depending on when you asked... maybe. It's possible someone there would clarify the purpose of Comonad.Reader. But it's equally possible to get a very incorrect speech about how a lot of the stuff there is completely important to solving basic practical problems in Haskell. The problem is precisely that there is *not* a strong division between "practical" and "theoretical". Of course Haskell programmers routinely talk about monoids and such in practical contexts, and some of us even talk about comonads in practical contexts. There seems to be a problem with part of the community realizing how far that's going to fly over a new programmer's head if they don't have a math background. I'm also not sure about the answer... but I think if we think that's an important thing to solve, part of the solution will have to involve separating parts of the community more strongly.
&gt;it would be a collection of design patterns. Oh, yes please. This would help so much, but there's this idea floating around that functional programming in some way doesn't have patterns.
&gt; But, people like me need to be both steered to right learning resources, such as Learn You a Haskell or Real World Haskell, and steered away from the writings of Logicians and Type Theoreticians. Let me quote [myself](http://www.haskell.org/haskellwiki/What_a_Monad_is_not): &gt; Also, don't be surprised if you leave this page more confused than before. That just means that it has successfully destroyed your false assumptions, or that you've fallen for some horrible inside joke. Beware of [Zygohistomorphic prepromorphisms](http://www.haskell.org/haskellwiki/Zygohistomorphic_prepromorphisms). Go for [warm and fuzzy](http://ertes.de/articles/monads.html), instead. I found myself in a somewhat similar situation as the OP, in days before RWH and LYAH were around, but just refused to be intimidated by formalisms -- though I still treat them with a long stick made out of respect (unless they aren't constructivist). Having a background in SICP and having been on the lookout for something new and cool did the motivation part, and rather operational papers -- say, the STG paper -- brought my understanding up to par with the math geeks, at least as far as actually coding-related stuff is concerned. Things have definitely gotten better since then, and I still wouldn't want to miss any of the diverse specialisations present in the community. If it's obvious to everyone that *someone* will outsmart you in *some* way then we have a sure way to keep a productive environment. Oleg himself, of course, couldn't even be bothered to write harmful posts.
You say: &gt;The solution given draws on exactly the sort of stuff that the OP here complains about; yet it's arguably the best and, in many ways, conceptually simplest version given. And maybe so. But for a learner the solution which is “best” and “conceptually simplest” in the eyes of an expert is not the best learning tool. I think there's a cultural problem. A lot of Haskellers seem to just _love_ that they live in a world where this sort of thing (taken from the conversation around your [1]) &gt;it's a straightforward lifting of the non-monadic version. You can almost get it from g_hylo by using the identity comonad, it's distributivity law, the identity natural transformation, and using T.sequence as the monad's distributivity law. is a perfectly natural comment to make. To anyone outside that culture (who just wants to, say, read each line in a file and so something with it, or whatever) this can look like deliberate obscurantism. And the appearance of the word “straightforward” can seem mocking. A metaphor: I'm studying German (on and off). I'm reading the easy-reader version of Heidi (intended readership, children about a fifth my age) and finding it a struggle. That book as exactly zero literary merit. It will be a long time before I can read and appreciate the poetry of Rilke, great and good as it undoubtedly is. I understand that the kinds of abstraction that Haskell affords is _great fun_, and maybe very useful in the long run but beginners don't need that. They need concrete examples. They need directly applicable advice. They need _less good_ but _more easily understood_ approaches to learn with. * added the metaphor, copyedit 
I understand complaints about lack of introductory materials or about the difficulty of finding them, but isn't it odd to complain about lack of warnings to steer you away from stuff? Aren't people usually the best judges of what interests and is understandable for them? Would putting "if this blog post is about something you don't care about or has too much category theory for your taste, don't spend too much time reading it" at the top of every blog post solve the problem?
That sounds about right: more practically-oriented materials help, not some weird warnings to steer you away from things you wouldn't be interested in (which are a strange request since usually no-one knows what you are interested in but you).
Is there a place where I can read someone talk about why they think functional programming has no patterns? I can't imagine someone thinking that; doyou they think functional programmer approach every problem differently and never reuse insight they developed from a different problem?
I think that there are some other differences as well. First, Haskell is undergoing some very fundamental transformations. The technical wing of the Haskell community has done amazing things with bytestrings/text/enumeratees/&amp;c., and several of the libraries are emerging as the Right Way to structure common IO and processing tasks. And yet, our tutorials still start everyone with strings and lists. So there's a second learning curve. Even if a new user finds helpful tutorials and understands the language basics, they're usually being trained on libraries that no one takes seriously. The cool kids and evangelists are in a completely different sandbox. This is just a transitional thing, and over time the new libraries will mature and socialize. But for now, I think we should be careful about the latent assumption that only expert users are allowed to have performance expectations. Tutorial-grade C# or Java isn't amazing, but the strings/streams/arrays that they introduce you to are the fast yet feature-poor parts of the language. That's a comparatively safe place to start a new coder, because "where can I find this other feature?" is typically an easier question to answer than "why is my code slow?"
I wish I wasn't trying to write this comment on my phone, because I really want to give a more detailed answer than this. I think i speak for more people than myself when I say that I'm not just into it because the math sounds cool coming out of my mouth, because i enjoy intellectual conversation, because i like the mental challenge, or because i like some subjective notion of elegance that comes from it all. To me, these are just very nice tools that help me get the problem solved. It's very nice that this is one of those rare communities that has managed to transcend the usual human blockades against learning new things; I think we are better programmers and better communicators because of it. Attempting to limit our vocabulary is the same kind of thing as trying to rename monoid to appendable; it only serves to make things more vague and less precise. If somebody asks what a term means, we are usually more than happy to explain, but please (not just you who I am replying to but everybody reading this), don't make us hide it in some "advanced" corner of the haskell community. We want useful vocabulary to spread. The philosophy is, in some sense, part of the "haskell package", and removing it would severely limit further progress, both academically and practically.
I didn't have a strong math background (and still dont) when i was a beginner, but #haskell was hands down the best resource I found. Everybody was very considerate to my beginner mind when teaching me, and i got some exposure to what was to come for me in the meantime. If I didn't get the opportunity to see that so many people are so productive with haskell, I'm not sure how long I would have stuck with it.
&gt; I think we are better programmers Possibly, although the economic and long-term technical evidence for this is lacking. &gt;and better communicators because of it. I'd say definitely not. I find the Haskell community at large _keen_ to communicate but actually quite bad at it. I'm not asking you to limit your vocabulary, nor to hide in some corner. I'm suggesting that you may communicate better (if that's your goal) by adjusting the lanague you use to be better fit to the people you are trying to communicate with. This shouldn't be controversial. EDIT: fucking markdown
As the author says, a lot of this is just a matter of reading somebody else's conversation. Of course I'm not suggesting that we should intentionally try to talk over peoples' heads.
Here are three: 1. ["In a functional language one does not need design patterns because the language is likely so high level, you end up programming in concepts that eliminate design patterns all together."](http://www.defmacro.org/ramblings/fp.html) 1. ["One could even say, that D[esign]P[atterns] are not needed in functional programming -- there is no itch which DP is cure for."](http://programmers.stackexchange.com/questions/89273/where-are-all-the-functional-programming-design-patterns) 1. ["'Design Patterns' are a phenomenon of traditional object-oriented programming languages, not languages like Scheme."](http://groups.google.com/group/comp.lang.scheme/msg/254206024d61d3ed) And these people are wrong, mainly because they have misinterpreted [this thing](http://norvig.com/design-patterns/).
Amen. I figured out on my own that I needed to avoid worrying too much about understanding Monads as an abstraction. What I needed to do was understand functors then applicatives, then monads, then arrows. That I figured out somehow. LYAH provided me with the worked examples. It used to be that when I thought about a problem I would see some tail recursive function. Now I think about maybe wrapping something up in a functor and throwing a fold at it. I'm still at the caterpillar stage in this mode of thinking but it's a huge improvement. So, thanks LYAH! Things are getting better.
http://www.haskell.org/haskellwiki/Tutorials The above (#2 when googling "haskell tutorial") , would have steered him to Learn You, and tutorials by Daume and Barsky, etc http://www.haskell.org/haskellwiki/Books_and_tutorials links LYAH, Hutton book, some other good intro's (needs 1 or 2 updates, e.g. Simon Thompson's 3rd ed book)
Thanks. I couldn't have said it better. What's different about the Haskell community is that there are a _lot_ of these conversations going on. I would never want that hidden away. Those conversations should not be hidden. That work makes Haskell great. But to the n00bs of the more traditional software engineering bent I would say, be aware that you can't expect to absorb the full breadth of the Haskell community because it is extremely diverse. So if you are reading LYAH and don't understand something, get help. On the other hand, if you are reading about nutri-endo-zygo-morpho-fugo-nintendo-propisms, and you don't get it, just don't worry about it yet. So my only complaint is that each n00b has to figure that out on their own and things would be better if they didn't. I'm not proposing a solution though. 
Yes. Yes, you are.
I "want" to write "Category Theory for Practical Haskellers", which would walk through category theory, talk about all the fun morphisms and such, and then accompany it with practically-oriented examples, except that I'm really not strong enough on either the Haskell or category theory side to pull it off yet. But it's a good idea. One of the things that is frustrating to me (and I don't mean this as a criticism of the content but a report on my frustration) is that this content is full of all these words that I have only a vague and hazy understanding of, but when expressed in Haskell they may only have a bare handful of terms, making it clear that there's a pretty tight bound on how complicated this concept can _really_ be. You're bounded on how complicated you can get with two types and three functions of two arguments using those types, but without at least one concrete example it's often hard to wrap my head around anything; I don't even know where to start. Like I said, no complaint intended, write what you want to write, but if you are someone who has the chops to pull off what I'm talking about and want to write an interesting article, something that _builds up_ to "almost get[ting] it from g_hylo by using the identity comonad, it's distributivity law, the identity natural transformation, and using T.sequence as the monad's distributivity law." with examples along the way of each of the steps and what they are good for would make for a fascinating article in its own right. (But, if done correctly, a necessarily-lengthy article. If it's short, it's still not exampled enough.) There's an interesting opportunity to get a lot of programmers interested in category theory, but I think it's going to take a bit more than transliterating a category book into Haskell... but not necessarily impractically more. I envision something that looks a lot like a followup to LYAH.
Some very smart people are advancing Haskell. That is great. But, that is for the top 1%. Unfortunately, the Internet is so flooded with whizbang talk that people are equating Haskell with "you have to be a math guru". We non-gurus need only the "basic" Haskell features. We recognize the value of the type system, pure functions, referential transparency, and most of the functions in the Prelude and List. But, I would like the following: 1. The guru-world and the basic-world of Haskell should have separate online communities. 2. The guru-world needs to ferret through the new advanced concepts and promote a few ingenious understandable abstractions (like List hiding Monad inside) that provide function without understanding category theory or abstract algebra. If this doesn't happen, I predict that another language/platform will steal all the basic-world Haskell people away. One such candidate might be ClojureScript?
Assuming you are behind the sedentia cdsmithus of blog fame, I like your writing. I find that even though your stuff is challenging, I can make sense of a lot of it. I found #haskell helpful for getting answers to questions. I definitely couldn't understand a lot of the conversations but people there are very tolerant of dumb questions. Contrast with say #ruby, which is downright hostile. When you say you steer people away from it, is that based on people having bad experiences with it? For whatever reason, I never found the "advanced" conversation there off-putting or distracting. 
&gt; We want useful vocabulary to spread. That will not happen. Haskell already has a grip on a large share of the top-1% programmers who are abstract-math geniuses. Monoids and category theory probably can never be understood by average programmers. I say this using myself as an example. I am a top-1% programmer with some background in simple abstract math and I am having a very lengthy mental absorption process for this stuff. The other programmers I work with will never understand; they are trapped in simple programming or fad-of-the-day programming. Therefore, we need a separate Haskell community for "smart people who aren't math gurus"--people who partially understand things like lambda functions and closures but not the whole world beyond that. A world where people can discuss a problem without being flooded with answers like "oh, just use a zygo-morpho-propism". If a programmer walks in a room and does not understand half the things said, maybe the room holds an exclusive, closed club. We need a better entry path into Haskell--a path that fully recognizes that many people will never go beyond "Haskell beginner" stage but will be very successful at that stage. I propose two separate forums: Haskell and Advanced Haskell.
And they are not just different but wildly inconsistent with one another. Are Arrows "worth learning" or something you "should know" or "potentially fascinating but wholly unnecessary"?
&gt; When you say you steer people away from it, is that based on people having bad experiences with it? Yes. Not that people are rude, or unhelpful when asked specific questions. Rather, new Haskellers can show up looking for a community where they might fit in, and they tend to leave very depressed and discouraged, or on some occasions outright hostile toward what they see as an out of touch group of Mensa members that have nothing to say about normal software issues. Heck, for that matter, I don't feel like I fit in with #haskell much of the time, and while I'm not an academic as a career, I do have a strong mathematics background involving several published papers in ring theory. I know other people, as well -- long-time members of the Haskell community, at least one of whom wrote a master's thesis on type theory -- who participate in some of the web framework mailing lists, IRC channels, etc... but avoid #haskell for this reason. I'm not saying I think this is all a bad thing... it's just the way things are. Most of the time, I like this level of connection between people with different perspectives, and I think it adds a lot to the community. But I do get worried when, for example, there are proposals to add a couple dozen new type classes to the numeric hierarchy with names that only mathematics students will recognize and that solve problems most people don't have, and they get treated as reasonable possibilities, or when it suddenly seems like the community consensus is that we should choose a package for lenses based on comonad identities. Or when someone from outside of Haskell specifically asks on their blog about practical uses of monads, and the answers are again all about more layers of abstractions. It's great that Haskell takes abstractions from, e.g., algebra and category theory, and incorporates them *when* we have very strong evidence of their applying to a large class of ordinary programming problems... but we can't forget to make that case in practical terms, and to be conservative about how important it is to get the latest neat correspondences from category theory incorporated into common use and advocated to the community at large.
I think a lot of people misunderstand the object of their misunderstanding and the reason for it. The general concepts of the Haskell language are pretty simple and most people get them individually. The fact that you can combine them to express very complex abstractions is not because Haskell is hard or has a steep learning curve, it is because people use it to do things that are hard and have a steep learning curve. So it's normal not to understand all articles or blogs that use Haskell as a means to express these concepts. Throw in a little math phobia in there and haskell seems like the insurmountable mountain it is not. I'm confident the same concepts expressed in C++ would be beyond comprehension for mere mortals. The only thing scary about functors, applicative functors, monoids and monads are their name. If you look at the code they are not that complex. They are powerful but not that complex. Probably the biggest hurdle I think is understanding the type system. Resources like LYAH and typeclassopedia do a great job of explaining how all those types are defined and used to do everyday programming tasks.
Or maybe too many programmers got too used to being able to pick up yet another OO language in a matter of days and be productive with it that they take it for granted that should be the case with all languages and complain when they come across unfamiliar terms. Then again, the number of C++/Java programmers I've seen learning C# from a 300 page book... Clever way of avoiding work? What else could it be? Surely they're not actually learning anything from it? What is there to be said about C# that needs 300 pages?
I can't believe I'm saying this but I'm sympathetic in principle to the more complex numeric hierarchy. But it's for practical reasons. I like to make systems that only contain features needed to solve the current problem. It results in a system with less accidental complexity. So if I spotted an abstraction in my program that would benefit from proper + and * but not /, (would that be a ring? or a group? I'll quit guessing now...) I'd want to use the smaller typeclass. But maybe that's not what's being proposed. 
the issue isn't that readers of /r/haskell may or may not have a hard time learning haskell...we're preaching to the choir. the issue is that everyone else either can't or won't pick up the language. yes, sadly it matters how many other people use a tool. programming languages live or die on adoption effects, and after waiting four years for haskell to get wider adoption, i'm getting ready to throw in the towel. i once spent hours talking it up to coworkers, suggesting it as a solution to error-prone duck-typed hacks as well as a way forward for parallelism and concurrency, but i never got back more than a polite nod back. and i still don't know anyone i see face-to-face on a daily basis who uses it. 
What makes you think you can't understand monoids? It's just a generic word for types that have an associative binary operation and an identity element. There's actually nothing more to it than that, nothing at all. Lots of things are monoidal: appending on lists, prepending on lists, function composition, addition, multiplication, etc. It's such a tremendously common pattern that it deserves its own name, and frankly I think if you think you can't understand it you're probably working yourself up into a tizzy about the name. For all those other people who don't want to learn terminology that you referred to, perhaps Haskell isn't for them. No one said Haskell needs to be the next Java. Avoid success at all costs is our motto. You either want to learn or you don't. The ones who don't want to learn should feel free to not bitch at those that do. I'm sick of how anti-knowledge the programming community has become. Keep that stuff in /r/programming. I most certainly do not welcome attempts to cleave Haskell into "Haskell" and "Advanced Haskell". Even thinking about it makes me mad.
i think you are blaming the victim here. i think maybe it is time to accept that haskell has some very strange abstractions that are imposed upon it due to some fundamental design issues that some would suggest are mistakes. it may be that the final conclusion is that a purely functional, lazily evaluated language makes no sense at all. certainly this appears to be the opinion of ML wonk robert harper. even for advanced haskell coders, it is practically a black art to reason about time and space performance due to lazy evaluation, and it also makes reasoning about parallel programs difficult. indeed, i think the bigger problem is how you rationalize idiomatic haskell with the haskell you see submitted to the language shootout. and yes, people care about the shootout. people talk about haskell being high performance, yet the solutions on the shootout are totally outside of the realm of idiomatic haskell, indeed its more like c embedded in a haskell parseable manner. but look at the samples for other languages...they are essentially idiomatic. i would suggest that the haskell shootout contributions are a giant fraud, ginned up to convince people that haskell competes with java. i would suggest that idiomatic haskell would perform approximately in line with python3
while i agree with you about the need for programmers to appreciate some of the essential values of the theory behind languages, its a nonstarter. the truth is the bulk of programmers do not care and never will care.
or maybe haskell is broken by design in that simple concepts need such lengthy instruction.
*What makes you think you can't understand monoids?* what makes you think people care about understanding monoids?
I just want to say: personally, my learning experience in Haskell floundered until I realized I need to get /away/ from tutorials like "Learn You a Haskell" or "Real World Haskell" and start listening to the logicians and the type theoreticians. This is because the essence of Haskell is logic, math, and types -- not producing lowest-common-denominator code that can be quickly written by any hack script-kiddie. Being still a relative newbie, I'd say I've been most especially helped by Davie's "An introduction to functional programming systems". Davie did a great job of explaining all the concepts thoroughly and methodically.
Of course! There's no other way to explain it. We don't need no stinkin maths. Or science. They require too much thinking and lengthy explanations for the simplest of concepts. Something there's not right! Therefore all science is stupid. Perfect!
I agree. Part of the problem is that people are not used to having to do this to the same extent with other programming languages, which have shallower learning curves, so their "not *for* me" detector is not properly tuned.
haskell isn't "science" (the fact that you use this term in this context implies you do not understand it). and it isn't math. its a programming language. your reply suggests that haskell is unassailable and irrefutable, and it is only the coders who are broken for not understanding it. i was once in the haskell cult, so i know where these replies come from. i actually commend robert harper for being essentially the lone wolf who is willing to point out haskell's flaws. it takes balls to stare down a cult, but he's right, the language has fundamental flaws
why not dive into agda and coq then? if you really want to get high-minded, you're probably wasting your time with haskell.
No, what I said does not imply that and the fact that you can't see the similarity between what you said and my reply implies something else :)
Right, so that's a ring... and `Num` already has `+` and `*` but not `/`. It's `Fractional` that adds `/` to the mix. Of course, you can come up with other examples: suppose, for example, you have a semiring (no `negate` or `(-)`), or a non-unital ring (no `fromInteger`), or a ring where signs aren't meaningful (no `abs` and `signum`; though those can always be defined as `id` and `const 1`, so I'm even less sympathetic there), or a dozen or so other variants... In the end, you have a choice between a small, manageable number of type classes and some partial instances; or literally dozens and dozens of type classes (integral domains, fields, euclidean domains, principal ideal domains, and on and on...) and no partial instances. To people who are interested in correct abstractions at all costs -- such as mathematicians -- having all that terminology is essential. But the extra complexity it adds to the language definitely has a severe cost, which there's an unfortunate tendency to ignore. We definitely need multiple numeric classes, of course, and your example already exists. It's just a question of whether to add every definable abstraction, or instead try to compromise on a few that capture the common ideas, and then resign ourselves to some partial instances when things don't fit cleanly.
What people *usually* mean by that is that they're defining "design patterns" as "generic structures that *can't be described directly* in the language". Things that can be described directly are packaged as libraries or such, which makes them (or so the definition goes) something other than "patterns". What it's really about isn't functional programming so much as powerful metaprogramming facilities. The reason it gets associated with functional programming is because it basically started with smug Lisp fans sneering at Java programmers. In actual practice there's no clear dividing line, but it's absurd to say there are no "design patterns" in Haskell, even if you take the Lisp-style approach and declare that anything you can possibly accomplish with mountains of Template Haskell and Oleg-level type hackery is by definition not a design pattern. Phantom types are probably the simplest, most clear-cut example of something appropriately called a design pattern.
&gt; Monoids and category theory probably can never be understood by average programmers. The average programmer *already* understands monoids. They're as simple as "1 + 1 = 2" and I mean that *very* literally. What they don't understand is that there's an abstract concept that covers many similar things, and that the abstraction can be considered independently of specific examples. The problem is not the difficulty of the concepts. The problem is a mixture of making things sound harder than they are, mental barriers to absorbing abstract concepts directly, and unfamiliarity of the terms used. Much like the infamous monad^[0] tutorials, attempts at explaining the concept can easily end up just making it more complicated out of some misguided notion that it needs to be easier. **[0]:** Actually, monoids are just a generalization of monads from composition of endofunctors to a binary operation on an arbitrary set. What's the problem? *Ha, ha.* Yeah, not helping, I know...
Good catch on my bad example. I like that Haskell isn't static. I'm willing to write a program, then upgrade GHC, then have to update my program too. As long as that process is allowed to continue then people can continue add to the Numeric hierarchy, conservatively as you advocate.
awesome. lets hear about haskell and the scientific method
You're in the wrong subreddit, go back to /r/programming.
Monoids were obviously a bad example here. But the general point should be listened to, I think. Just substitute "comonad" or "Yoneda" instead of "monoid". Haskell being at the forefront of a lot of programming language research means there will be people who are trying out new abstractions, and advocating for their use... but we do have a bit of a problem where too little time can pass between some core Haskell members thinking about a new abstraction, and that being widely circulated among the community and represented as the "right" way to solve certain problems; often before there is any serious long-term experience doing things that way.
&gt; And maybe so. But for a learner the solution which is “best” and “conceptually simplest” in the eyes of an expert is not the best learning tool. Yes! This was exactly my point, in fact. I'm familiar enough with the concepts used to understand what that solution does and not be bothered by the terms, but I still have to think about it a moment and I wouldn't be able to just sit down and write a similar solution off the top of my head. I wouldn't dream of suggesting that it's relevant to a beginner (at least not the way it's presented there). The most I would say is "don't worry about it yet, come back later and it might start to make sense". But the fact remains that it's not in *any way* frivolous or obfuscatory--for someone who's internalized these higher-level abstractions thoroughly enough, that's what "just wants to do ____" actually looks like. Let me give another example. This time it's my own code, but hopefully this won't come across as egregious self-promotion or anything. :] On Stack Overflow, a question was posed of how to incorporate something in a reasonably elegant way into a program using lazy IO, where (as it turns out) the desired addition is intrinsically incompatible with the standard lazy IO functions. The questioner seemed to be neither a total beginner nor an expert, so I felt the best approach was to [demonstrate a rough solution using regular strict IO](http://stackoverflow.com/questions/6668716/haskell-lazy-bytestring-read-write-progress-function/6669453#6669453), but with small composable functions and abstract combinators, rather than suggesting a hackish workaround using lazy IO or pointing them at an iteratee library. If you read the answer, compare my code to the program winterkoninkje posted in the old thread. It feels (to me, at least) that the underlying concept is somehow similar, but where I wrote a quick and dirty example using a single-purpose loop combinator, he used an existing abstract recursion scheme and a few very simple functions. So on one hand, I feel strongly that the design in my answer is the most approachable and straightforward way for a novice-to-intermediate Haskell programmer to accomplish that task; but on the other hand, I strongly suspect that my example is reaching clumsily toward the sort of arcane abstraction that apparently discourages people. This is why I think it's important to neither shield beginners from such material, nor brush it aside as irrelevant abstraction golfing. I'd rather find ways to convince people that there's simply a long, rewarding progression of useful abstractions that *can* be learned, that people who've traveled far on that road happily mingle with those just setting out, that they don't need to study the *next thing* before using the things they *already know*, and most importantly that they won't be looked down on for failing to somehow know everything already.
One source of needless confusion is people saying "we can solve Problem X with Abstraction Y" when what they *really* mean is "here's a solution to Problem X, this solution is an instance of Abstraction Y, here's how to apply general results to the specific case". In cases like that, people who just want to solve Problem X don't need to learn or care about the abstraction at all. Nobody needs to understand monads to use `concatMap`, do they? The abstractions are only broadly relevant once solutions to multiple, unrelated problems are found to share them. `Monoid` is the awkward, boggling extreme of this, where it describes so many solutions that we have to jump through extra hoops just to specify which of the bajillion possible instances for `Int` we want this time.
&gt; What I needed to do was understand functors then applicatives, then monads, then arrows. Your phrasing here is interesting, because `Arrow` does not in any way follow conceptually as the "next step" after `Monad`. If anything it would follow `Applicative` and even then only in a very confusing way. The common tendency to talk about `Arrow` as if it's part of a natural progression following those other three is subtly misleading and, I think, contributes in a small way to making it harder for newcomers to learn. (Note that I'm not criticizing you in any way here--just observing a common pattern that you seem to have picked up somewhere along the way.)
but thats exactly what we are talking about, getting the type of people in /r/programming into /r/haskell
That's not what I was talking about, I would rather those people stay in /r/programming and bury themselves in Java, XML, and whatever the next enterprisey-technology du jour is. I do not see any pressing need to turn Haskell into a fad. I do not see any need to get into yet another troll discussion with someone who doesn't "get" Haskell and doesn't want to put the effort into learning it. Refusing to understand what a monoid is when you're learning Haskell is like learning C and refusing to understand the difference between the stack and the heap, or C++ and refusing to understand what pass by value semantics are, or Perl and refusing to understand what a regular expression is, or whatever. I don't want to have to drag you kicking and screaming to a concept. You either show up wanting to learn because it turns you on or you go back to doing the minimum you need to keep your industry job. There are lots of programming languages out there. There are very, very few jobs that require that you know Haskell. There is therefore no "practical" reason to learn it. So if practicality is all that motivates you, then you know where the door is. Complaining about "monads and monoids and functors and laziness" in a Haskell forum is like posting in `comp.lang.lisp` and bitching about parentheses.
Vaguely group-like things have one operation, and are distinguished from each other by the presence of identity and inverse elements and the rules for what expressions are equivalent. Vaguely ring-like things have two operations that interact in certain ways, with both operations (considered independently) also forming a group-like structure. Vaguely module-like things have two different types of elements, one often being built on top of the other in some fashion, each type (considered independently) being one of the above structures, and some concept of "multiplying" a value of each type together. Conversion from numeric literals, conversion to a string representation, and comparison operations are completely orthogonal. The above lumps together lots of things with extremely important differences and would make any self-respecting mathematician cry, but it's about the bare minimum necessary to separate things that naturally have very different type signatures from a Haskell standpoint. `Num` is just horrible in all kinds of ways, both theoretically and practically. Ugh.
Nice username.
&gt; In the end, you have a choice between a small, manageable number of type classes and some partial instances; or literally dozens and dozens of type classes (integral domains, fields, euclidean domains, principal ideal domains, and on and on...) and no partial instances. Or the third choice, which is to fix the inflexibilities of the type class system so that it's no longer necessary to commit irrevocably to either. Not sure if that's likely to happen any time soon, though. :T
It's more that they just *forgot*. Non-programmers have at least as much trouble learning a mainstream language as programmers who know a mainstream language do learning Haskell.
My biggest problem was that I thought monads were my problem. My actual problem was the type system. I would see Parsec String u String and have absolutely no clue what that really meant. I had trouble differentiating data and newtype. I had trouble with kinds when I wanted to make a type class. I had no understanding whatsoever of transformers, which makes it pretty hard to do anything worthwhile in this language.
No one has explained to me yet what agda and coq will provide for me that Haskell doesn't. But once I have fully mastered Haskell I might take another look.
I think this is pretty much what the original blog post is trying to get at, but your comment hammers the point home a lot more succinctly.
We are talking about two categories of people attempting to be Haskell programmers: 1. people who can quickly meld their brains into every new abstraction, and 2. people who want to use the basic functions of Haskell and slowly learn the advanced stuff. You seem to be dismissing people in category 2 because "they aren't trying hard enough". And, that was essentially the implication of my earlier comments: Are the smartest, most advanced super-genius Haskellers willing to let "normal genius programmers" use their language? If so, other than a few tutorials and awkward reddit chains (like this), where is our place to learn and chat? Remember there are no other languages with so good a type system and enforcement of pure functions and laziness. From the beginner's perspective, monads and functors and arrows are things to learn in the future.
That is the sad direction we are on. We have here, Haskell, a great language with great basic features that pretty much eliminate 95% of popular programming difficulties and defects. But, esoterica is overrunning every discussion of the language. Abstract math gurus are putting up barriers and scaring off the unwelcome masses who might create "lowest-common-denominator code" with their language. Is this for real? People have written fabulous tutorials and frameworks and libraries. Are we really going to beat down newcomers who don't understand cofomonoidads and aren't using them in every 100-line web app? It is time for self-examination. We need to agree to take the most abstract features and place them a little further out of the sight of newcomers. People should know they are there, but people should not feel like they have to fight for the right to use "basic Haskell".
What you don't understand is that there is no one in group 1. If there are, it's a very small number, and not the least bit representative of successful Haskell programmers. When I started playing with Haskell in 1999, there were no "monad tutorials", there was no Haskell subreddit, no #haskell IRC channel (at least I don't think there was), and I'd never played with functional programming before. Haskell had no fast implementations, there were no libraries to speak of, and virtually no one seemed to be using it. I think I may have read the "Gentle introduction to Haskell", which is known for not being gentle. Why did I keep playing with such a useless language? Because the abstractions and the type system *taught* me to look at things in a way I never had before, and the rush of it completely overwhelmed me. Why do you want to learn Haskell? Because you've heard it's the next Python, or the next Ruby, or the next whatever? Because you've heard that the smart kids use it? Seriously, what's your motivation? Because what makes the Haskell community great is not that it's filled with "super-genius Haskellers", but rather that it's filled with people who are in awe of the paradigm shift, who are excited about the way Haskell encourages them to think. That doesn't mean those people *get it* right away. Just like most everyone else, I struggled with monads for a while before I got it. I struggled with applicative functors, and I struggled with comonads, and I struggled with arrows. But I wanted to struggle, because that struggle taught me something. And that struggle was *fun*. Are you here to learn or to bitch that all your imperative, object-oriented habits don't translate 1-1 into Haskell the way they do in virtually every other language? Are you one of those people who thinks all there is to learning a language is learning what a `while` loop is called in that language, and how you make an `if` statement, and whether you use braces or `begin` and `end`? Believe it or not, learning takes effort. Either put in the effort, or don't. But don't come into a Haskell forum with the attitude of "it's up to all of you to convince me that something I don't understand and have made no real effort to understand is worth my time." Because if you aren't already clear that learning something new is nearly *always* worth your time, then Haskell isn't for you.
Actually, I think I really want umbrella packages, and as a logical corollary of this, packages organized into a tree instead of flat list. I haven't thought about what are a the consequences of such a package structure. In any case, cabal-install and the other tools would need some metadata to do the above, too, and it seems to me reasonable to reuse the existing package structure for that. The recommendation stuff would be just a convenient extra.
Where do you get the impression that using "basic Haskell" is discouraged? I've seen several people who've made major contributions to the community (authors of large libraries on Hackage, for instance) say that they've never read even some of the most low-level research papers, don't really have a feel for things like `Applicative` or `Arrow`, etc. They're just using average-level Haskell to get things done, and not letting the existence of arcane math stop them. Are people being scared off? Or are they scaring themselves unnecessarily?
Wait. So, no one is in group 1 and group 2 is filled with lazy, hacking, bitching programmer wannabes? I think I might be trying to make a different point than the one you are concluding from my comments. Either you are in group 1 or you have worked long and hard learning advanced Haskell. Either way, I commend you. As for me, I already know "basic Haskell" and have for about five years. I have successfully used it in a pure-functional way for a few small projects and it has been fabulous. I know I didn't use all the cool abstractions that I could have (same as when I showed an APL program to a developer of the APL compiler). But, what I know so far works. It is enough to be worthwhile. I don't need more *right now*. I don't need to be convinced about "advanced Haskell". I know it exists, is impressive, and is valuable. But, many of us do not have the graduate-level mathematical background to go much farther, except one little bit at a time. And, time has to be rationed, when there are so many software technologies to follow. 
There is no such thing as a PullerSingletonJumpingFactory.
Sorry this is late. I hope someone still notices it. This post, like just about any other beginner's explanation of `par` and `pseq`, raises some obvious questions: 1. Why x \`par\` (y \`pseq\` x + y) and not (x \`par\` y) \`pseq\` x + y ? Don't we want to ensure that *both* `x` and `y` are evaluated before we compute their sum? 2. What is the difference between `seq` and `pseq`, and why do we need this new combinator? For both we are told they mean that the first argument should be evaluated before the second
The progression is sort of natural. As you get better at Haskell, you begin to use the type system to prevent bugs. For example, you might wrap a simple list type in a `newtype` and call it `OrderedList`, to make sure that you never inadvertently mix up ordered lists and normal lists. Or you might use phantom types and type classes to define restricted composition rules for families of types. Or you might use existentials à la `ST` to prevent separately generated instances of a type from becoming mixed up. The natural progression is 1) fear the type system, it barfs at me and I don't get it; 2) respect the type system, it seems to catch a lot of stupid stuff; 3) use the type system, if I think about it a little I can harness it to catch pretty non-trivial bugs in my code; 4) abuse the type system, use fundeps and undecidable instances to create possibly very complex type-level hackery to check invariants at compile time. By the time you get to 4, you really are starting to see that the type system isn't as flexible as you'd like. Your `OrderedList` newtype, for example, forces you to do a lot of wrapping and unwrapping and doesn't actually guarantee that what's behind your `OrderedList` constructor is actually ordered. Your division operator happily divides by zero. `fromIntegral` starts seeming like only a step up from `unsafePerformIO`. Then you start seeing what dependent type systems can do. You read [Why Dependent Types Matter](http://citeseer.ist.psu.edu/viewdoc/download;jsessionid=BE6E20FB538BD8A935598C9BD05B1066?doi=10.1.1.106.8190&amp;rep=rep1&amp;type=pdf) and you pretty much blow a load in your pants. Currently, languages with dependent type systems are pretty abstract and mostly used by people playing with PL theory or mathematicians using them as theorem provers, so if you think the Haskell community is math oriented looking into these languages will probably be pretty intimidating. But it's a fun ride.
Think about the logical essence of Haskell that you enjoy. What is the logical value of `error` or `undefined`? Or a function that goes into an infinite loop without doing anything useful? The logic guides the types to show you what your program does, and that it does it correctly. The existence of `undefined` means that, at any point in the program, you have the ability to wave your hands and tell the compiler "trust me on this one". Sane programmers don't abuse this, but it's still there. You say that your recursive function will never go into an infinite loop? Can you prove it? Ehhh, *waves hands* it's fine, trust me. Agda and Coq are about writing programs without always having "just trust me" as an axiom everywhere.
&gt;But, many of us do not have the graduate-level mathematical background to go much farther, except one little bit at a time. This is exactly the myth I'm calling out.
I'd just like to add a general request here that people don't just sit in IRC. Please ask questions, even seemingly basic ones -- there are friendly people in the IRC channel who *will* do their best to help you. :)
I think it does kind of make sense to learn Arrow after those other abstractions, but only because it's far more infrequently used, and its advantages and disadvantages are easier to understand once you understand at least Monad. You can make the comparison that in x &gt;&gt;= f, because f is a function, it becomes impossible to scrutinize it in any way (to potentially enable some runtime optimisation), prior to running x. But in f &gt;&gt;&gt; g, both f and g are members of a datatype that you have control over and potentially some analysis can be done to computations before executing them. (Unfortunately, the excessive use of arr in arrows generated by proc/do notation generally messes this up quite a bit, but we're working on that problem here at iPwn Studios. :) It's a little harder to motivate the Arrow interface without that realisation, because it generally is not as expressive as Monad, in the case where both apply. On a side note, I've started to think lately that Arrow should *not* generalise monad, and that the Kleisli categories for most monads should not be instances of Arrow, because they fail the "interchange law" that (f &gt;&gt;&gt; g) \*\*\* (h &gt;&gt;&gt; k) = (f \*\*\* h) &gt;&gt;&gt; (g \*\*\* k) This law basically ensures that the pretty arrow diagrams people draw actually make sense without lots of extra boxes in them which most people leave out. It agrees with what I think most people's intuition about Arrows is, because it's what lets simple topological moves on those arrow diagrams make sense. (Like sliding two boxes in different disconnected parts of the circuit past each other.) Without this law, it's sort of like trying to design a circuit where how you layout your components on the circuit diagram is allowed to drastically affect their behaviour. However, it actually isn't an Arrow law (according to Hughes' paper), and means that Arrow is no longer a generalisation of Monad. However, it is a property enjoyed by monoidal categories (bifunctoriality of ***) and more generally 2-categories (where it's called the interchange law). It ensures a certain lack of global state, only allowing "local" stateful behaviour (like FRP accumulators, for instance). The Kleisli Arrow for a State monad for example, will not satisfy this law, because the order in which g and h execute will be swapped.
Can I substitute 'AbstractSingletonProxyFactoryBean' in for 'monoid'?
Thanks. I am already to the point where I love Haskell's type system -- especially because I love the constructor syntax, and the TS has made program prototyping so much more fun (and effective) than I have experienced in any other language. But I don't think my mathematical and theoretical knowledge is advanced enough yet for me to move on. So I've book-marked that link.
&gt; But, what I know so far works. It is enough to be worthwhile. I don't need more right now. You're right, you don't. And that's fine, really! I don't mean that in any sort of condescending way. You're getting things done, and in the end *that's the whole point*. &gt; But, many of us do not have the graduate-level mathematical background to go much farther, except one little bit at a time. And, time has to be rationed, when there are so many software technologies to follow. Really, though. You don't need that kind of background. I may not be an expert but I can follow along with pretty much all the "advanced Haskell" stuff, even if I couldn't have invented it myself. My education background consists of an undergraduate CS program taught by people who wrote C programs in C++, and the only math I learned was some Calculus that I forgot a couple years later. You don't need to be a genius with an advanced degree, honest. But yes, it takes time. I've been using Haskell for about a year and a half now, and I've spent a *lot* of time learning as much new stuff as I could. What I (and others) object to is the idea that the advanced stuff needs to be hidden or marginalized, or that it's out of reach for most people. It's not out of reach, it's just not something everyone needs to learn immediately. Also, I'm sorry for the negative reaction you're getting. Your tone does come across as a bit accusatory, but your concerns are real even if I disagree with your suggestions on how to improve matters.
Well... I would never teach someone about Iteratees before teaching them about lists. Even when you have Iteratees and the Text library and so on, you're still going to want lists. Maybe not as many lists of Char, but lists of other values certainly. Also, the plain String type, while it might not be fast enough for enterprise web applications with 10k connections per second and so on, is easy to work with and good enough for the small programs that beginners are likely to start off with. It's best not to optimise things prematurely. My job right now is writing a game in Haskell for iPhones, and while there are some ByteStrings in our code, we don't have any Iteratees/Enumerators or such. (We do have a fancy FRP system though, so I guess that makes up for it.) There are still plenty of plain old lists, and a handful of Strings here and there (pretty much just for debugging output).
Sure, and such programmers probably don't care enough about the quality of their code or the language they're writing in to bother with Haskell in the first place. So what? One can make a decent assumption that anyone programming in Haskell actually cares enough about the general approach of how programming is done that the theory will interest them. I see nothing wrong with assuming that, and I see nothing wrong with limiting Haskell users to that set of people.
Hm, your remarks on `Arrow` are very interesting. Seems to help clarify a few intuitive qualms I've had with it that I've not been able to clearly express so far. Anyway, I agree completely on the best order to learn the concepts; my concern is when the phrasing of such a recommendation implies a conceptual progression that just doesn't exist. The interrelations between the `Functor` and `Arrow` families of type classes are not at all obvious or helpful to anyone who doesn't already understand both.
One solution is to get the word out to beginners that if they want answers, they shouldn't just sit there silently waiting for other people to hopefully answer their question. The people in this community will generally go a long way to help beginners to understand things when they ask questions, but if you just lurk and try to absorb everything from the mailing lists or IRC channel, you have to be prepared to not understand a lot of it for a long time. That said, when I was starting out (back around 2001-ish), I lurked for a while without saying much, and learned a good bit, but I have a lot of patience, and in retrospect, I probably should have asked more questions anyway.
Unfortunately, my impression is that most of the literature on the formalist-leaning perspective tends to assume the relevant background, then applies it to Haskell. I will say that the Curry-Howard correspondence is the linchpin of lots of stuff. Are you at all familiar with it? How comfortable are you with symbolic logic?
Actually, I find Agda's (and Coq's) type system much easier to grasp. The type-level hackery in Haskell is mind-bending and often requires one to turn many ghc extensions on. It feels very fragile and insecure. I tried Agda a few months ago and now I'm at peace :-) The type system feels consistent and powerful; writing code that is correct by design is natural. TL;DR: Lots of stuff beyond Haskell 98 confused me; tried Agda and reached nirvana.
Add to your list, "Type Theory for Practical Haskellers." I would buy it without blinking twice.
If I didn't care about the quality of my code, I would use perl. BaZing!
I get direct and straight-forward answers in #haskell all the time. Try going there and asking a very basic question.
Apart from making a good point, excellent and very funny essay. Well done.
I think beginners really need projects before answers. I remember having tried, as a beginner, to follow what was going on in haskell-cafe threads: it felt like I was trying to drink from a firehose (someone recently even mentioned #haskell felt like this too), and there was no LYAH or RWH back then (these would have helped me a lot). Watching all those random concepts did help me know where I was standing, but I really started learning Haskell after creating a concrete project and forcing me to get it done. A good strategy would be directing beginners to try to reimplement in Haskell something they already did in another language.
I don't want the type of people in /r/programming in /r/haskell. I want the type of people in /r/haskell in /r/programming.
Most people agree that both 1) Haskell is unusually difficult to pick up (not terribly difficult, just unusually difficult) 2) The Haskell community is very friendly (almost eerily so) I often wonder if there is a causal relationship of 1 leading to 2.
&gt; This is why I think it's important to neither shield beginners from such material, nor brush it aside as irrelevant abstraction golfing. Sure... but on the other hand, sometimes it's okay to admit that it's abstraction golfing. Research communities do it all the time. People pick research topics because they stumble upon a surprising "aha" moment or a neat correspondence between things that didn't seem related. At that point it *is* just playing with abstractions. Then they go out to justify why that realization was worth having. Sometimes the result is obviously of immense practical importance, and sometimes it's of no importance at all; but far more often, you find some places where it looks to be a bit useful, and you try to develop them to make the argument that it is. You take on a bit of a role of the advocate that your work is useful. That's a very familiar position for anyone who's discovered or created something new, and then wondered why others don't jump on board. It's not a dishonest role in the least, but it is definitely biased, and should be recognized that way. I think it's important, aside from everything else, to acknowledge that as an active research community, the Haskell world has more than its fair share of these advocates for their "aha" ideas.
&gt; The only thing scary about functors, applicative functors, monoids and monads are their name. If you look at the code they are not that complex. That is true, but I think the high levels of *abstraction* is what is mentally painful. I'm comfortable with all of the above now, but I can recall the brain hitting a wall trying to understand some Applicative instance for lenses or whatnot. Like one level of abstraction too much for the moment. Most other languages aren't capable of building these layers of abstractions like one can do in haskell; once you've learned the syntax you're basically done.
Troll too obvious...
Arrows should just be forgotten: they are basically Applicative + Category + a couple of laws, expressed in a cumbersome way.. EDIT: laws, not rules. Formatting.
Learning new abstractions, and new ways to abstract, is always hard. Harder than people realize. I first learned procedural programming (pascal, C, fortran), then switched to OO. It was hard. Way harder than learning a new procedural program would be. But once you get your brain wrapped around objects, it's much easier to learn other OO languages. You don't have to learn whole new ways of thinking about problems- maybe a new trick here or there, but all the standard GoF patterns get applied in more or less the same way in all OO languages. It really is little more than picking up a new syntax.
Yeah, but like, half of everything written in Haskell uses -fglasgow-exts which contains some decidedly *not* simple language features.
I agree with camccann; my personal proposal is to allow the methods of a superclass to be defined in a subclass instance; and to allow subclasses to give default implementations for superclass methods. Then, you could just define a single typeclass instance with basically the same methods as you do now, and get the sound typeclass hierarchy above.
I find #haskell very hard to keep up with considering how popular it is now; but not so long ago, I found it very friendly and helpful. Sure, there are a lot of high-level discussions going on in there, but surely people don't try and read every single thing said? I find that the individual discussions usually suit the level of the person being helped quite well.
++ I find the arrow *combinators* very helpful, but the underlying module seems confusing and unnatural.
And how many hours does your "Hello, world!" program take to run? :)
A bit humbling too, for those of us who'd like to see wider adoption. The kicker is, I know math researchers (who are Haskell fans) who don't know a great deal of CS (and thus don't really understand the type theory stuff, and definitely don't care for the performance issues), and CS researchy folks who likewise don't understand the category theory. It would probably go a long ways to making the language less intimidating if everyone knew this. If it doesn't bother the "real nerds", it shouldn't bother you either. This goes back to his point about creative tension between open-minded industry hackers, and the different folks who are using it as a research language. I don't think we've fully internalized this. Personally, I feel it's a good thing there's math stuff floating around. In a strange way, it gives me hope *because* I don't understand it. 
I always wished there was some sort of class group. So pretend there are four operations add, subtract, negate, and transpose. Then there is a Number class group which is composed of the classes Add, Subtract and Negate, and a Matrix class group which is Add, Subtract and Transpose. If you write code you could type a variable as "Number a =&gt;" or a "Matrix m =&gt;" and cut down on the complexity of the type signature. Then you could have an insane number of classes in the codebase and things would still remain easy to understand. I'm sure there's simple reason why this would not work, but it would be nice.
I'll give an example. When i got to the part in Real World Haskell about **folds**, I was left really confused. I got the sense that the authors were taking the approach of going into a lot of theory "first" because they knew beginners wouldn't get it. But then, they realized they don't have enough space to adequately discuss the theory, so they just skimmed it. Thus after reading a few pages, me the beginner, was thoroughly confused. I finally understood them. All it took was a one sentence explanation! See the description box [here](http://zvon.org/other/haskell/Outputprelude/foldl_f.html). I feel that all they had to do was give a one sentence explanation like this, give some examples, and then move on to the next topic. I haven't gotten to the part about monads. But I highly suspect it will be the same thing. A lot of stripped down theory, that will only serve to further confuse me.
This was proposed (alongside other more advanced features) under the name "constraint synonyms" in a recent paper, at http://www.cl.cam.ac.uk/~dao29/publ/constraint-families.pdf and I think it's an excellent idea. I suppose it probably hasn't yet been implemented yet in a way that's suitable for long-term inclusion in GHC. It would be awesome if it were -- either as just constraint synonyms, or with the full constraint families features. This wouldn't completely solve the problem, though, as it would make simpler abstractions potentially dependent on very technical and incomprehensible tangles of classes, and make Haddock-generated documentation well-nigh unreadable if use in an extreme way.
its not dumb if you're in the sphere copyin' business
That is pretty much what I was hoping for. Too bad there's no way I could ever hope to implement something like that myself. But it was a pretty good read. Thanks.
Really? There was a pretty good Haskell compiler available just a few months after the Haskell 1.0 definition appeared. 
I think in a way design patterns are looked down on by some because of their association with object-oriented programming via the "gang of four" book. My perspective is a little strange in that I haven't read that book, but I have read "A Timeless Way of Building" and "A Pattern Language" by Christopher Alexander, which inspired the GOF book. The design pattern idea originally came not from computer programming but from architecture. A design pattern is really just: a) a conflict that arises in a certain context and b) a solution that resolves that conflict. You then give it a name and share it with your peers. This is such a familiar idea that it seems like there isn't anything to it, but it really is a nice way to organize your thoughts when encountering some challenging design task and communicating a design with others. Patterns can also be organized in interesting ways. For instance, in architecture, a common problem is to be midway into designing a floorplan and realize there isn't any space for a garage, or a bathroom, or a staircase. Alexander found that if he built a directed acyclic graph out of the patterns he had cataloged, with very general, top-level patterns at the root, and the more specific patterns that only affect one particular element of the design at the leaves, and that if he applied the patterns in that order rather than trying to apply all of them equally at all times, he was able to design houses with much less backtracking. I think as programmers, we're all familiar with having to start over because our original design didn't work, and I suspect that the same approach of applying design patterns in a certain order would mean not having to start over so often. I also suspect that we all do this to some extent already by instinct. Bottom-up design as a foundational principle is just our way of protecting ourselves against design rigididty that would force us to start over. However, I also think it would be constructive to organize design patterns by the order in which they ought to be learned, as I suggested in the parent post. Perhaps both orderings are the same, I'm not sure.
Never heard of it (Curry-Howard) - of course Curry rings some bells, but linchpin says me nothing. Symbolic logic should be no big problem - I did not dig into logic to far but you have to know some of it for more mundane mathematics (like topology and stuff) I guess ;) I really enjoyed this: http://de.reddit.com/r/compsci/comments/j1v6k/category_theory_introduction_using_ml/ Thought maybe there would be more stuff like this out there
Yes, Alexander puts a lot of emphasis on pattern _languages_ and that organising principle does get talked about at the patterns conferences. Folks get quite excited when it looks as if such a language is emerging. Patterns and patterns languages are both a literary form and an knowledge management tool—GoF isn't regarded particularly highly as an example of either in the pattern community. The sad thing is that the great “success” of GoF simultaneously indicates how hungry people in the industry are for material like that and sets a bad precedent for what it should look like. In my experience as a pattern user and a published pattern author the order in which patterns in a particular domain should be learned (about) and the order in which they appear in a breadth-first traversal of the language are not the same. 
&gt;Are people being scared off? Or are they scaring themselves unnecessarily? Both. And the Haskell community can help with both. But the problems are social, not technical. So they won't be fixed by more and better explanations of technicalities. Language communities are shaped by their earliest members and the influence lasts a long, long time. The Haskell community has been shaped by academic computer science researchers, a specialised sub-set of those, even. That bakes in a certain set of assumptions about what programming is _for_. I recall talking to Peyton-Jones a few years ago about the mismatch between what industry practitioners think a reasonable programming environment looks like and what the Haskell community thinks a programming environment looks like. His response: just use EMACS. He was only joking a little bit. But I think this example goes to the heart of it. For a PL or CS researcher a language like Haskell (and development of same) is an end it itself, for working programmers in industry the language they use might be more interesting or less interesting but it is always a means to an end. So tools and affordances and culture matter a lot. Some Haskellers seem to think that mainstream commercial programmers are somehow conservative sticks-in-the-mud, resistant to obviously brilliant new ideas. Not so. “working stiff” programmers are committed to making a living for a long time from writing code all day every day and they jump at any opportunity to make that a more pleasant experience. But they do that in a context that I don't think the Haskell community understands well, nor, often, respects. And it shows. If there is a feeling in the Haskell community that they want the language to be successful in industry then they have to learn a bit more respect for they way that working programmers think about their tools. Consider this: it took years for some large organisations (banks, say) to approve the change from Java 1.4 to 5 for their mainstream production code. They take that long because of their approach to operational risk. In the crazy world of the Quants Haskell does have a tiny bit of traction (much less than the fanboys would have you believe)—but quant teams in banks are typically well–known by the mainstream programmers who use their works as erratic, unreliable deliverers of code than often doesn't work. I've seen quant teams make multiple drops of non-working code with the same egregious defect not fixed in several different imaginative ways in the same day. So, not a good place to start with a story regarding “reasoning about correctness” If you want to be successful with the mainstream folks there needs to be a stable, well deliniated core language (_not_ something that needs an ever changing set of flags for non–standard compiler extensions) that can be picked up relatively easily by smart and capable but not grad-school brilliant people and used to gain obvious big wins _this week_. And the Haskell community would have to respect the idea that such a thing is _more valuable to industry_ than the latest excitement about partially overlapping existential blah blah blah. Even if that turns out to be useful. 
For the records: http://www.reddit.com/r/haskell/comments/iz2ew/the_great_yesod_reorganization_of_2011/
The link is much more accessible conceptually than a lot of other documents on dependent types, I really recommend it. Specifically, it works through an example of a dependently typed merge sort, in which the analog of my `OrderedList` example is, rather than a simple `newtype` wrapping the way it would be in Haskell, actually a type that can only hold sorted lists. The type system is powerful enough to say, "this is a list in which the head of each sublist is less than or equal to the head of that sublist's tail." So instead of forcing you to make sure that a list is ordered before you wrap it in an `OrderedList`, the type system will reject any cast where it can't prove that the list satisfies the invariant. As a result of the type of their merge sort, you effectively have a *proof* that merge sort produces a sorted list. It's really pretty mind-blowing. Other things dependent types can do: vector types with the type system enforcing bounds checks. We all know the use case: you're writing some C, you have an array, you're constantly reading and writing to the array. Putting bounds checks everywhere is computationally expensive and you *know* it's redundant in most cases. A dependent type system can check when it really *is* provably redundant and will barf at you if it can't prove it (which probably means you need to rethink your algorithm, or put in runtime assertions). What about dealing with bounded integral values properly? In Haskell, we implicitly assume that `Int` is the same as `Integer`. I mean, we know it's not, and that `Int` has a defined range, but how often do you put in max int checks and min int checks? Probably almost never, because it's very rarely necessary. But what if the type system were powerful enough to barf when you can't safely cast? Sign overflow vulnerabilities have resulted in privilege escalation before, and yet very few programmers think about them seriously. What about subsets of the natural numbers? Sure, we have signed and unsigned, but what about distinct from zero (for division, for example?) Or even only? Or odd only? Or prime only? Or what about handling the fuzzy precision of floating point numbers? We all pretend like addition is associative, but with floats it isn't. When is it safe to assume associativity (subject to a defined "acceptable" loss of precision?) It's amazing the amount of stuff that can be checked at compile time. So what's the downside? Well, Hindley Milner type inference is decideable. Haskell actually implements a superset of HM, but basically, in most cases, the compiler can infer types very quickly and easily. Dependent type systems need more help. Have you ever used the ST monad? You know how sometimes, you need to annotate the type because GHC (or whatever) can't figure it out? It's not that your code is wrong, it's that sometimes the type system can't deduce the correct type for a term. Well, this problem is multiplied somewhat with dependent type systems, unfortunately. A lot more annotation is necessary. However, the annotation serves, as in Haskell, a documentation purpose. The type annotations form a sketch of a proof that what you're saying is true, and the programming language fills in the missing steps with its inference engine. This is why DT PLs are used as theorem provers.
Everyone uses LANGUAGE pragmas these days... Rank n types and GADTs are new concepts, but most other extensions aren't particularly mind bending...
One of the things I love about Haskell is that it can actually sustain and mechanize abstractions that would otherwise never leave the backs of the envelopes they were scribbled on. As one of the people who tinkers about at the bonkers end and occasionally blogs or ships to haskell-cafe, I'm no stranger to Warnock's dilemma. I'm not fazed by it either, but perhaps I should be. I'm worried by the thought of doing actual damage by alienating learners, and I'd like to be a decent citizen about it. But how to manage situations like the following..? (1) the obvious conflict between academic paper-writing (where background recapitulation competes for space with innovation and necessarily loses) and the need for accessible learning materials; (2) a naïve question provokes an interesting but tangential discussion (perhaps because the helpful person who answered with the conventional wisdom inadvertently showed why the conventional wisdom might need a bit of sprucing up); (3) a concept or technique is imported from another discipline, along with a bunch of peculiar terminology, taking people out of their comfort zone. All of these situations have the potential to damage the self-confidence of learners and to generate new bits of learning curve with somewhat alpine gradients. But they're also signs of a community that is open-minded and making progress. We have a really good community process here of grinding down those alpine peaks at rather better than a glacial pace. I think, however, that our resources are not so well editorialised (except by chat): there's just a welter of stuff, but not much of a model of a person encountering that stuff. I wonder whether we need some sort of (rough, dynamic) curriculum dag to which we could tie content. That might help learners map out the territory and at least see the route from where they are to where some random eruption of mad-but-good is kicking off. I really think we'd lose a lot if the community became more segregated. I like the café-ness of haskell-cafe: it's a bunch of disparate conversations in a public space, sometimes with learners, sometimes amongst specialists, with no guarantee that everything will be of interest (or even comprehensible) to everyone, but where earwigging is always welcome. If we could manage slightly better learning-orientated metadata, we might help people to the gold in them thar hills.
I can recommend [An Introduction to Lambda Calculi for Computer Scientists](http://www.collegepublications.co.uk/computing/?00002). You know the series, if you're reading "The Haskell Road...", and it's very reasonably priced. Lots of exercises.
Agda's actually pretty good at not having you annotate types any more than Haskell makes you. It's amazing how much like Haskell most Agda code can be (or alternately, all the things we think type inference is giving us that it isn't). Oddly enough, Agda doesn't even have a type annotation operator, so we can't just annotate an arbitrary expression (although sometimes we have to do something similar by passing implicit arguments explicitly). I'd say the major downside to all the static guarantees the fancier type system can give you is that the crazier your types get, the more work you usually need to do to convince the typechecker that your values fit. For example, with statically-sized vectors, you might (from some other expression) have ended up with `Vec Bool (n + 0)`, and you want to slap that into a goal of type `Vec Bool n`. Although it seems obvious to us, Agda does not actually know that the 0 natural is a left identity for addition, so `n + 0` won't unify with `n`. You can bring that knowledge into scope with a suitable proof, but it can be tedious. Note that `Vec Bool (0 + n)` is actually equivalent in the typechecker to `Vec Bool n`, because the inductive definition of natural addition defines `0 + n = n`. These annoying proofs can be automated for certain classes of problems. For example, we have an automated semiring equation prover that will prove that for all `a`, `b`, and `c`, `a + b + 2 * (4 + c * b)` is equal to `a + b * (1 + 2 * c) + 8`. But in most cases, you're still stuck with writing some fairly annoying proofs. Anyway, my basic point is that it's pretty rare for "it" to prove anything for you at all, except for reducing basic definitions to a normal form. If you write a division operator that disallows 0 in the denominator, you're going to need to have a proof lying around that your denominator is not 0, every time you call the division operator. I still think it's a worthwhile pursuit to learn one of these dependently-typed languages, but sadly you don't get much for free. I'm interested in integrating automated theorem provers for specific theories into such a language, but I don't think that's been done anywhere yet, and sadly many theories we'd want it to solve are undecidable :(
As pigworker would probably say if he saw this post, it's not just about checking and policing, either. We know that djinn can take a subset of Haskell types and write terms that satisfy them. Sometimes the type uniquely determines the implementation, so you can get the computer to write `callCC` for you. But, I think the pitch goes: fancier types mean that more information is available for the system to use in this manner. So, conceivably, we could shift away from a programmer writing implementations, and a compiler checking some invariants on them, to a programmer writing a specification, and directing the system toward the desired implementation with much more automation. How far along this path one can go is questionable. But for instance, agda can already write djinn-level programs for you in the interactive mode, and it has type-directed interactive editing that makes writing Haskell drudgery by comparison (some of it could be done for Haskell, but it hasn't been). And this is probably the tip of the iceberg: an emacs mode with a few compiler hooks.
Actually, the last few chapters of that are probably a good starting point, starting at ch. 7 and in particular ch. 10. It demonstrates connections between certain categorical constructions and both type systems and logical systems. The Curry-Howard stuff is the same concept, only jumping mostly between the latter two without necessarily going via category theory. Essentially, when you see something like `A -&gt; B` in Haskell, you can obviously read it as "the type of functions from A to B", but you can also think of it as an arrow in a category whose objects are Haskell types, or as material implication in a corresponding logic, i.e. "A implies B". Going the other way, consider the statement "(a and b) implies c", which is equivalent to the statement "a implies (b implies c)". Translating this equivalence to types says we can convert between `(a, b) -&gt; c` and `a -&gt; (b -&gt; c)`, and the implementation of functions to perform that conversion maps closely to a proof of the logical equivalence. Obviously it gets a lot more complicated than that, though! Does that give you some idea about how formal reasoning might translate to and from a program in Haskell?
Rank-n types are not new concepts. Girard's paper on System F is from 1971. Reynolds' is from 74. Doing decent inference in conjunction with rank-n types is somewhat newer: early-mid 90s for MLF, for instance. Inductive families were coined in the late 90s, and they were re-invented as GADTs/first-class-phantom-types/etc. in the early 2000s (I think), so those are kind of new, I guess (only a little over/under a decade, depending on what you count).
Sorry, I said "new concepts", whereas I meant "new to the imperative programmer".
Looks similar to [cabal-dev](https://github.com/creswick/cabal-dev).
It solves exactly the same problem.
 -- |Primary function to calculate CUV for account/portfolio for single month calcMonthCUV :: PerfObject -&gt; ISODateInt -&gt; ISODateInt -&gt; IO [((Int, Int), CUV)] Why does this function return an IO action, instead of just a tuple? 
Ok that was easy. I spent a while back looking for a similar functionality and no one recommended this if I remember correctly, spent a couple days chasing wild ideas down rabbit holes and gave up. Thanks
Because it relies on getMonthCUV to get the previous month's CUV from the database, which is in the IO monad.
This article worked on me. I had no plans to learn Haskell anytime soon. A couple days after reading this I started thinking Haskell might be a good fit for some personal projects. Now I'm halfway through Learn You a Haskell.
Well stange as things be sometimes I happen to listen to a podcast (http://www.computersciencepodcast.com/download-mp3/6/6-scc.mp3) yesterday (yeah somewhat old) that talked about those stuff ;) And yes I think I might have a bit of a clue of what they/you are talking about - thanks
what a ridiculous reply. so now people who aren't interested in type theory are necessarily unconcerned with code quality. and you found six other people to upvote this i'm starting to wonder if the critics are right - that haskell is indeed a cult
i had problems understanding why the entire prelude had to be reimplemented for monads, then i realized this was haskell's fault, not mine.
Since "getMonthCUV" relies only on the arguments "prevMonthEnd" and "perfObj", what about passing the value of "getMonthCUV" as another agrument to "calcMonthCUV"?
I haven't read it all so I don't know the details, but I think in this case the naming is confusing. If calcCUV is a pure function I'd expect calcMonthCUV to be a pure function too. Then you would have a separate function that reads the DB, feeds the input into that function and returns the value (like Lenny222 suggests).
this should get much more visibility (like pep8 in python), because I've complained about it for a long time and yet I haven't seen this link.
Yes! Haskell can do that! I'll probably release it this weekend (all that's left is emacs integration and readme file (and some more testing)). https://github.com/Paczesiowa/virthualenv if you want to use it today, mini intro is in description field of https://github.com/Paczesiowa/virthualenv/blob/master/virthualenv.cabal
Is this in response to this thread, [file splitter with enumerator package](http://www.haskell.org/pipermail/haskell-cafe/2011-July/094193.html)? If not, what's the context? Oleg's posting is interesting in its own right, but I'm wondering what prompted it.
Thanks, Chris. I've added a proposal to http://www.reddit.com/r/haskell_proposals/comments/j33ub/write_a_haskell_source_code_formatter/
"answer"? This "formatting" throws out all comments. This is why Haskell community sucks - no one bothers to make something really usable, monad fapping all the way.
http://www.haskell.org/pipermail/haskell-cafe/2011-July/093639.html
The "Language.Haskell.Exts.Comments" module seems to support source code comments.
Well, so you have to write your own formatter anyway. I am trying to make IntelliJ IDEA plugin for Haskell and was infuriated when I learned that there is nothing close to working code formatter, seems I have to write it from scratch. Practically all languages (including the horrible C++) have code formatters. Haskell does not. Why?
When you compare apples with oranges sometimes the apple peel tastes better than the orange pith. I can understand the Python code in one glance and see the multiple changes necessary but the Haskell code is still opaque until I run through statements trying to understand what they do. All right I started using Python in 1997 and Haskell reminds me of PERL with all of its strange syntax and side effects and I only have been trying to understand it for a year. I had fun writing a thirty-line APL program in the mid-1970s. Haskell also reminds me of http://jsoftware.com/
After someone has written one, there will be one.
Haskell has a much more flexible syntax and people from different backgrounds write code differently. While you could prescribe some things, there are often places where either choice would be fine and it's up to the author to make a judgement call. This makes it hard to automate these things. Just as an example: foo x = map (foo x) something where foo a b = useBoth a b foo x = map (foo x) something where foo a b = useBoth a b foo x = map (foo x) something where foo a b = useBoth a b The Haskell refactorer HaRe actually has a code formatting functionality that even tries to integrat with the existing code style. It's probably not easily usable as a standalone formatter, and it only supports Haskell'98.
&gt; This makes it hard to automate these things But [Haskell is for hard problems!](http://cdsmith.wordpress.com/2011/03/13/haskells-niche-hard-problems/)
Jesus H. Christ will you stop shitting in the damned thread you pinhead? What the fuck is your point? Haskell doesn't have a source formatter whilst C++ does? I'm going to go out on a limb and suggest the reason for that is because somebody has written one for C++ and nobody has written one for Haskell. HENCE WHY THIS THREAD EXISTS IN THE FIRST PLACE!
Then why not you clever Haskell guys take time to write one instead of playing with zygohistomorphic prepromorphisms?
Yes, indeed when you compare apples and oranges you get bizarre results. Like, for instance, when you compare a programming language that you have at best a passing familiarity with to one in which you've got 14 years coding experience.
Sure, Haskell is famous for its side effects.
I also found this explanation from the thread extremely clear: http://www.haskell.org/pipermail/haskell-cafe/2011-July/093648.html
/me silently hands dpm_edinburgh a fine cup of Valerian tea and joppux a teddy bear.
You're just encouraging it to continue.
To be fair to the OP, Oleg does write some code that only a mother could love.
No, engaging anti-Haskell trolls with real discussion as if they're interested in evaluating the merits of the language encourages it to continue. There's a difference between "welcoming community" and "easy trolling target". Every Haskell thread on this site is inundated with trolls who are engaged with multi-paragraph missives as if they've got something new or interesting to say. It drags the comments off topic and makes it hard to actually understand where the real discussion lies, even with the voting system.
Only if that mother is a dark wizard.
Did you try #haskell? I get all my legit build environment advice from there.
Your responses so far don't indicate that you're truly interested in a useful discussion. So this is going to be my last response. Just like with any open source project, things get written if someone feels compelled enough to write the code in order to fix a problem they encountered. Apparently, so far nobody wanted an automatic code formatter for Haskell badly enough to implement one. Getting "infuriated" by that or complaining that the whole community "sucks" just because someone tried to be helpful and gave a solution that turned out to have a flaw certainly won't fix it. You have a few simple options: * Try to write one yourself. Look around whether you can use existing tools to minimize your effort. * If you don't want to implement it yourself (or feel it's above your current level of proficiency with Haskell) try to motivate other people to help you with that. I don't think insulting people will help with that, though. * Decide that the problem isn't that important to you and leave it be.
I have more experience with haskell than python and I still prefer python's snippet
I think that until a moderator decides to take action (which we can encourage, of course), it's in our best interest to not lose our cool. Losing our cool is, after all, what trolls are looking for.
I guess for similar reasons that Python doesn't have one (or at least didn't have one for a long time).
There are a lot more use of `Double` and specifically accumulations of `Doubles` then makes me comfortable for financial applications. Granted this appears to be only for producing summery data, and maybe it isn't going to be significantly off for this range of data. But still, I don't know how much error will accumulate, and I fear the author doesn't either. But I'm the type of person who spends 10 seconds to compute confidence intervals of probability distributions using `Rational` rather than spending a fraction of a second to do the same computation using `Double`.
Sigh. This is exactly why I prefer being on Stack Overflow. Stuff is expected to actually be *useful*, and irrelevant flames and trolling *will* be purged by moderators with no pretense of valuing "discussion". This whole thread starting from joppux's first comment and all child comments is not really constructive in any way and deleting the whole thing would be an improvement.
One moderator last posted on reddit 12 days ago, another 25 days ago and the last hasn't posted in /r/haskell for donkey's years. We need more moderators, and we need mods who aren't afraid of deleting off-topic trolling nonsense.
That's a good point. However, we only display performance summaries to clients with one decimal point. In addition, I have a lot of unit tests around the code, and compare the results of the haskell functions against my own computations and the error accumulation is within acceptable limits.
How does your thing differ from cabal-dev?
You're right, this could be named better, and Lenny222 is also correct about passing the value of getMonthCUV as an argument. Unfortunately, totalReturn is also in the IO monad, and it is also in the calcMonthCUV function. To be honest, I really struggled with keeping the pure and impure parts of the code separate as the application has to pull a lot of data from the database under different conditions. I'll have to go back and see if I can do a better job of it.
Yeah, I've been wondering about that. It's kinda weird not having dons everywhere all the time, and I don't recall ever seeing the other two mods being active here. I make pretty heavy use of the community moderation tools on Stack Overflow and I've come to really, really appreciate the benefits of the strict approach they take.
* easier to learn - one command (without any args) to learn, if you know python's virtualenv it will be natural * easier to use - if your project needs "-fswitch" to build, you just build like you build any other package since you learned haskell, not worrying if cabal-dev supports this particular command or looking for it in its manual (just cabal configure -fswitch) * you use "ghci MyModule.hs" just like you used to do for so many years, no installation needed * instead of using ghci through terminal you can fire up ghci inside emacs to use the currently edited file (I work that way, not editing in my editor and going to a different window hitting :r, or maybe even :l with a different filepath to see if it compiles) * you can use different version of ghc inside your sandbox (e.g. nightly builds) while it does have two features not found in cabal-dev (external ghc and emacs integration) I created this because I fell in love with python's virtualenv - I skimmed its www for 30s and I knew everything I needed to know to use it. I spent 30min trying to use cabal-dev with two of my projects (only to learn that it was impossible) - that's not the way I imagine such a simple project to work.
Because they don't want to. You want it, so you make it. They want to play with zygohistomorphic prepromorphisms, so they do that.
FYI, [PythonTidy](http://pypi.python.org/pypi/PythonTidy)
Are you referring to the whole thread? Because Oleg doesn't mention yield at all in the linked post.
I'm never on IRC these days, but either there or here or somewhere, it would be nice to start a meta-discussion on more mods for the haskell reddit/a more activist moderation policy. I'd gladly support you for mod :-)
I'm not sure Oleg quite picked up on Python's generator support? Python can indeed essentially transliterate the entire iteratee-based approach into itself, complete with composition and local state. Of course, it's Pythonic local state, so, external mutation, variable sharing, etc. is still in full effect, so it's got the usual imperative pitfalls, but it isn't true that Python necessarily requires major modifications in multiple places in the program to add something like a "last five lines" generator or something. Going back to the original question, I do have to say that iteratees as they stand now are more complicated to use than Python iterators, because you have to very explicitly manage their state, and strong typing requires you to manage your endpoints and understand more of the mechanics of that than the Python code. There's more visible-to-the-user exposed mechanics in Haskell. I'm not currently sure if this is fundamental or accidental though.
The main reason is the .NET interface. There is a lot of value in still working with a functional language when you're already committed to that platform. Haskell is just not as well suited to participating in code environments that are built around imperative features, so while Haskell can be made to interact nicely with various environments (C and C++, and .NET too, though I think the project is mostly abandoned from lack of interest), it never really participates in them in the same sense that F# is a first-class player in the .NET landscape. Incidentally, I think you're misinformed on some of the details. In particular, F# is much more closely related to ML than Haskell, and although Simon Peyton-Jones is amazing and there's a unity of spirit among functional programming experts in Microsoft Research, he's not the main contributor to F#.
Scala isn't Haskell on the JVM. It is designed to work with Java in such a way that it isn't awkward. This allows you to use Scala where the JVM platform is required. Because of the way the type system is designed, and because it's a strict language, it interfaces a lot easier. Sometimes you have to be on a specific platform so you can interface with the existing libraries, because you have an existing codebase or because you foresee a better future for the platform. The reason why I choose Scala over Haskell sometimes is because I want to use the vast amount of existing Java libraries.
Scala isn't Haskell on the JVM. It is designed to work with Java in such a way that it isn't awkward. This allows you to use Scala where the JVM platform is required. Because of the way the type system is designed, and because it's a strict language, it interfaces a lot easier. Sometimes you have to be on a specific platform so you can interface with the existing libraries, because you have an existing codebase or because you foresee a better future for the platform. The reason why I choose Scala over Haskell sometimes is because I want to use the vast amount of existing Java libraries.
F# is open source, the compiler and libraries are available under the Apache 2.0 License. 
this seems (?) like a thinly-veiled slam on f#. i would suggest that the heretical statement is true: apart from the other benefits of using f# (namely deployment in .net/mono stacks), f# is a better-designed language that provides the key benefits of haskell without some of the flaws. first and foremost, f# is eagerly evaluated, which eliminates the foundational fault with haskell and all of the quackery that results. also, f# seems to be open source
"I'd write some Elisp to have it work on Haskell buffers on save." Hmm, this might be a convenient vector into the various unicode characters Haskell source can use. If I can type backslash at any time and have it cleaned up into a lambda character on save automatically, I might consider actually using some of those characters, whereas if I actually have to type them, it's always going to not happen.
You can add me as mod if you wish. I'm around a fair bit. I won't let it go to my head I promise. (*secretly readies crown and staff*)
Did you duplicate post? I see another with 13 up votes submitted 45 minutes ago by you...
Your title is about F# but later you slam Scala too. Scala makes no claim to be Haskell. It is much better understood as the successor to Java, with numerous functional (Haskell like) features as well as numerous Java improvements (rather C# like) included. It provides a very practical way to move forward from Java without abandoning Java legacy libraries and stateful programming.
Lambdas don't actually work, do they? It's a lowercase letter, not a symbol. I know some editors display \ as λ but that's only in the editor window, it's not saved that way. Unless GHC added λ to the unicode syntax extension and I haven't heard about it yet, at least.
For context, this was no doubt prompted by the [trolling and troll-feeding going on here](http://www.reddit.com/r/haskell/comments/j31f4/is_there_a_haskell_code_formatter/).
If you *are* interested in code quality, there are lots of reasons you ought to be interested in type theory. Types are assertions about the behaviour of a program which are effectively machine checkable from the source. Having a decent type system removes the need for a wide variety of tests, because it becomes impossible to make certain large classes of mistakes. It allows programmers to specify the reasonable ways that the parts of their library can be fit together. They also provide a kind of partial but enforced documentation. They can provide the users of your library with guarantees about what the code which they are about to add to their program can and cannot do. If you don't think that sort of thing is interesting, I have to question how much you really care about the quality of your code.
I may have, it's 502-it-went-through, right? maybe I was zealous. cf, irony.
I didn't read very deeply, but this looks a bit like rvm, though perhaps less complicated (given the sharding of python is somewhat less than ruby). Am I more or less correct?
The main contributor of F# is Don Syme.
IRC isn't really ideal for such discussions. Pretty much the only sane options are haskell-cafe and right here on /r/haskell, and I see little value in clogging -cafe with reddit housekeeping issues unless it's as a general call for community input with the intent of getting more activity here. [jfredett's recent post](http://www.reddit.com/r/haskell/comments/j3ct6/moderation_in_this_subreddit/) is probably as good a place as any to start the conversation, if you have stuff in mind. It would be useful to know what people actually want--I'm obviously an advocate of more aggressive moderation based on relevance and constructive contribution, not just getting rid of only the *really* obvious trolls.
Question that's not answered in the cabal file... does this have an equivalent to `cabal-dev add-source`? Can it maintain a shared private package database for multiple packages in a manner similar to `cabal-dev -s`? If it can do those things, then the multiple GHC versions looks tempting as a reason to use this over cabal-dev for buildbots.
Sorry, I knew GHC supported "unicode" but it looks like you are correct about that particular glyph. But that was a bad example, then; for the other supported Unicode I'll never use them without some sort of auto-replacement support, and I like it intergrated into a process that is generally intended to prettify code more than any other alternative I've come up with. Generally saving shouldn't change the file, and I hate autocorrect of any kind so I'm not going to let my editor autocorrect -&gt; into an arrow. However, if it's part of a generalized source-code prettifier, and I-the-user am choosing to run that on every save of my own free will, it all adds up to something sensible to use for me.
That and the fact that the nice .hu (can't remember the URL) Agda tutorial stuff was blackholed (spam-filtered?) for several days so the author pinged me and asked me to post it for him.
Well, yeah. I didn't mean to say it was that post *alone*, though I guess it sounded that way. More a "last straw" sort of thing after a general impression of not enough moderator activity.
I mention it only because it's a source of constant, minor disappointment for me. :] We don't say "backslash calculus" or "backslash abstraction" after all! Incidentally, as a low-tech approach for the not-very-smart editor I use, SciTE has an "abbreviations" file that specifies substitutions to be applied behind the cursor when a keyboard shortcut is used. Other simplistic code editors may have something similar, and I've found it relatively usable for those times when I really need to do something incredibly useful like defining a natural numbers type as `newtype ℕ = Nat Integer`.
If you do, consider changing the spelling ? That one's going to be very hard to remember.
I was also a huge fan of virtualenv. What's going to be the "right" way to use the haskell platform with virtualenv?
The only slam on F# is its .NET-ness. I see it as Haskell for Microsoft programmers, and I see no benefit for using a nonmultiplatform language. If F# is eagerly evaluated, then it loses one of the best features of Haskell: lazy evaluation. This allows for incredible features like infinite lists that work well with declarative programming.
F# is essentially an ML dialect. It's closer to OCaml.NET than anything else. It's strict, impure, and lacks many useful features found in Haskell (as well as some found in OCaml). By the way, please don't feed the trolls.
Why couldn't you put "virtualenv" in the title instead of "this"?
then we will have to disagree, because its been obvious to me for years that lazy evaluation is why you can't reason about time and space performance in haskell programs, and why performant haskell is usually written in the dialect that looks like a subset of c (go look at the shootout entries). its also why we need bandaids like iteratees...which essentially fixes the major performance issues with lazy io
He also contacted me via modmail, it just took me some time in getting to it.
I don't think a reddit like this has any place being 'democratic'. It has a very specific purpose to serve.
Even so, I prefer to keep dictatorial actions to a minimum, just on the grounds that better decisions get made in a group of rational people. I'm mostly cautious due to my experience w/ /r/skeptic. 
I do think it's important to distinguish between something more open-ended like /r/skeptic where the goal is certain kinds of discussion, as opposed to something like /r/haskell that's about a specific, well-defined topic. I think this is what danharaj meant, as well. It's a lot easier to say "this does not provide useful information related to Haskell" and be done with it.
If posts and comments are going to be deleted due to "inappropriateness" I recommend defining in the description of /r/haskell (on the right) what exactly is considered inappropriate (similar to how /r/worldnews does it) so people cannot (reasonably) claim to be surprised when their posts are deleted. The moderators should start with their own definition of "inappropriate" and incorporate user feedback from discussions in case they come up.
&gt; If F# is eagerly evaluated, then it loses one of the best features of Haskell: lazy evaluation. You still have lazyness where you want it. &gt; This allows for incredible features like infinite lists that work well with declarative programming. Of course infinite sequences are not a problem.
In case anybody is interested, i repost it to haskell reddit. ----------------------------- I've managed to simulate iteratee with python's generator, still missing many features though, e.g. EOF, composability, plenty of combinators. Actually, i start to appreciate the design of iteratee, haskell is really good at composing things together, iteratee don't need any special syntax, just functions composed together. the code is here (http://pastebin.com/drKW8jMN) in case yor are interested. for example, you can compose generators like this: ''' +- lineno --------+ | | line --+- grepw ------+--- (lineno, line, context lines) | | +- context lines -+ ''' gen = split( lineno(), grepw(word), lastn(5) ) 
Is there any changelog available?
&gt; I'm mostly *cautious* due to my experience w/ */r/skeptic*. (emhasis mine) Go figure ;)
I think most of us at /r/haskell trust you and the new moderators; moderation is one of those activities where the phrase "it's easier to ask forgiveness than permission" fairly applies.
As a very brief bit of introduction, I'd like to mention that the majority of my activity in the Haskell community has been on [Stack Overflow](http://stackoverflow.com/). I sorta figure that anyone else who frequents it recognizes me by now, but I know there's a fair amount of non-overlap between the various places Haskell users interact, and SO tends to get mostly newcomers plus a few veterans, so I expect there's some folks here who've not seen me that much. My user profile on Stack Overflow can be found [here](http://stackoverflow.com/users/157360), but probably more informative would be a [rough indication of how active I've been regarding Haskell](http://stackoverflow.com/tags/haskell/topusers), or perhaps more directly relevant are some of [my posts on Meta Stack Overflow](http://meta.stackoverflow.com/questions/98507/should-community-flag-serial-voting/98525#98525) [about moderation](http://meta.stackoverflow.com/questions/98455/cool-down-on-flagging-posts-in-the-pre-comment-feature-era/98486#98486) [and community involvement](http://meta.stackoverflow.com/questions/99537/how-to-restrain-improper-not-a-real-question-votes/99539#99539). I'm occasionally active on haskell-cafe and am semi-regularly on #haskell as well. 
Is this going to have the [class context equality constraints](http://www.reddit.com/r/haskell/comments/i6ftr/superclass_equalities_are_now_supported/) yet? Or is that still coming later? The snapshot build 7.1.something I'm using has them, but I can't recall how GHC's version numbering works.
Here's the actual announcement: http://permalink.gmane.org/gmane.comp.lang.haskell.glasgow.user/20371
Maybe not the best place but you can find quite a lot info here: [http://hackage.haskell.org/trac/ghc/browser/docs/users_guide/7.2.1-notes.xml?rev=3a1023ad77cf1193865d423f4a243d8e1b807bb1](http://hackage.haskell.org/trac/ghc/browser/docs/users_guide/7.2.1-notes.xml?rev=3a1023ad77cf1193865d423f4a243d8e1b807bb1)
what would a haskell reddit be without lazy mods? (sorry... had to...)
Alas, a few of the fundamental platform packages (which many other packages depend on) don't build yet (e.g. `network-2.3.0.4`, `HTTP-4000.1.1`, `text-0.11.1.5`) :-/
How is it on lion?
'tis the plan. I'm going to get something up for community review shortly. But it will lean more on the "heavy moderation" side than /r/skeptic, certainly.
I agree. I just have a sort of fear (in the sense of "fear of God") of the hivemind. We will likely lean on the "heavy moderation" side, especially with the obvious trollposts. Fortunately, most of the content here is usually pretty on topic, it's just a few outliers that happen to be loud.
&gt; Do me a favor and upvote this a bit Wait, shouldn't I be downvoting you for asking for upvotes? Now I'm confused.
I don't mind asking on self.posts, since there's no karma to gain, I mostly asked because there is a lack of "sticky" option in reddit. Otherwise, I totally agree. but then again, I just downvote everyone, because I'm full of hate.
&gt; does this have an equivalent to cabal-dev add-source? yes it can be used for developing multiple packages: you create a virtual environment (doesn't matter where), you activate it and then you can go to the dir with your dependent project (e.g. library, may be patched/unreleased on hackage), cabal install it, then go to another directory (e.g. with your app that depends on the said library) and cabal install it - cabal will detect that the dependent library is already installed and will not install it again &gt; Can it maintain a shared private package database for multiple packages in a manner similar to cabal-dev -s? pkg db in a virtual environment works for every operation once the env is activate - you can install inside as many packages as you want.
what would you suggest? currently it clearly reminds of python's virtualenv and there's an 'h' inside. I think it's the only pronounceable (though with a lisp) insertion of 'h' inside of "virtualenv"
to be honest I don't know what are the reasons behind haskell-platform (not counting things like those packages are blessed so they are supposed to maintain some kind of quality). I don't think anybody distributes Haskell code that is not cabalized and iirc it's not possible to just mention "haskell-platform-2011.2" as a cabal dep. All I care about is ghc and cabal-install (if you have those - you can do everything). in 0.2 version, you get a GHC (system of from a tarball) inside your virtualenv and you use cabal-install from your regular system (e.g. from haskell-platform from ubuntu repos). this works suprisingly well (using my system version of cabal-install-0.10.2 I could use ghc-6..7.3 inside virtual environment without problems. future versions of virthualenv may not even require cabal-install, and will bootstrap it inside virtual environment.
Manuel Chakravarty [addressed this](http://haskell.org/pipermail/glasgow-haskell-users/2011-July/020639.html) in the announcement thread: &gt; The RC unfortunately doesn't build on Lion (OS X 10.7). It needs two patches I recently pushed to the master branch (and suggested to be merged into stable). They are the following patches: &gt; &gt;eb01af6ba964fe74375e461723b83597ef97155d (On OS X, use gcc-4.2 with Xcode 4 and up) &gt; &gt; 30ccc9f39dd2cf1ad14e6116778aa1fd94526c19 (On OS X x86_64, use "-Wl,-no_pie" and "-Wl,-no_compact_unwind" to avoid linker warnings) 
Well, don't worry too much. The mods may need to be strict occasionally, but will never be eager.
Yes it will have them. In almost all cases, all the features from a development snapshot GHC X.(Y-1) you try will be included in GHC X.Y
Haven't tried, but you may be able to work around by configuring with --with-gcc=/usr/bin/gcc-4.2
(Original author of the plugin here) It means that any thunks created in the body of foo will be evaluated immediately. It does not *necessarily* make the function strict in its arguments. If you compile all your code with the strict-plugin then all arguments you pass will be evaluated *anyway*, so there is no need for functions to force their arguments.
You do a great job on SO, and I've also noticed some of your posts on MSO. I'm sure you'll do well on /r/haskell.
Hooray!
Why not just let the community decide whether a comment is appropriate? That's what the voting buttons are for, right?
Why does ghc seem to be very fickle about mac operating systems?
Frankly, I don't see why interventions like `joppux`'s need to be eliminated. He or she began idiotically - or perhaps in a style appropriate for another chunk of the idiotic redditland - but was expressing exasperation in attempting what I assume would be a worthwhile project. Moreover he or she had clearly studied the preceding text, and attempted to use Haskell.Src.Exts in the recommended way, before being exasperated by the comment cancellation business. And in fact a number of decent points come out before it is decided `joppux` was simply a "troll" -- this was palpable nonsense in view of the above. I suppose there is little hope of collectively adopting a sensible plan for approaching interventions like his, but I don't think I agree with `camccann` that it would have been better if the whole thing had been eliminated; it would have been better if people had better instincts how to handle it. 
**Point #1**: The initial comment was unnecessarily antagonistic in making a valid point, then added gratuitous insults at the end. This is not in any way constructive, because the useful information contained in the comment was suppressed by the comment being (legitimately) downvoted heavily. Later comments from said user contained even less useful information and more useless antagonism, which makes me even less inclined to give the initial comment any benefit of the doubt. I really, *really* don't think it's at all unreasonable to expect at least a *bit* of maturity on the part of people participating here. This is not a "style", and certainly not "appropriate" anywhere. It's just being obnoxious. **Point #2**: Essentially what nominolo said in a later comment: &gt; Your responses so far don't indicate that you're truly interested in a useful discussion. This is basically what I would call "trolling". Arguments where both sides aren't participating constructively and in good faith aren't useful or productive. The result is inevitably a very poor signal/noise ratio and excessively large comment threads. /r/haskell is a public space and the only purpose of having comments at all is to enable discussion of the subject of the post, in a way that's valuable to other people reading. **Point #3:** &gt; it would have been better if people had better instincts how to handle it. My experience has been that, in relatively open communities of any nontrivial size, this is absolutely and completely unworkable. It only takes a few people to have poor instincts in a particular situation to create massive amounts of useless noise, and once that sort of nonsense begins it's more likely for additional people to jump in. Empirically speaking, people feed trolls, and that's really all there is to say on the matter. It's not so much about specific problem threads as it is keeping a clean house. Having non-constructive threads like that encourages more threads filled with irrelevant arguments to develop. This is also why I see no reason to tolerate pointless antagonism. Earlier threads set the style and expectations for newer threads, and enforcing a standard of quality helps discourage content of sub-standard quality from being posted in the first place. I also strongly prefer to purge poor content with an explanation of why, rather than punishing the people putting it there. As far as I can tell joppux isn't consistently a problem; just this one thread. Letting threads like that continue provides positive reinforcement for antisocial behavior, making it more likely that eventually the only viable option is removing a user entirely, and that's not a good result for anybody.
Because, sadly, moderation is not that easy, as the post camccann linked shows, trolls are fed, regardless of downvotes, and it is my firm belief that such posts are not benign, but actively detrimental. The content of those troll posts are equivalent to "syntactic noise" -- and like syntactic noise, they should be removed. The question is "what is the benchmark for noise" -- my philosophy of moderation is curation, not censorship, so there is nothing to fear in the "thought-control" sense (I say this, because this argument is most often linked to the notion of censorship in subreddits). A post is forthcoming with the guidelines I, and the other mods, intend to follow. I intend to give the community-at-large a chance to review those guidelines, and then they will be posted in the sidebar.
(setq haskell-font-lock-symbols 'unicode)
They're switching the default compiler to 'llvm-gcc' (so 'gcc' is actually a symlink to 'llvm-gcc') as of OS X Lion, which doesn't have the features GHC needs. So you need to explicitly give it the real gcc (gcc-4.2) to compile correctly. The compact unwind warnings are because libffi doesn't understand compact EH for Mach-O object files. You can ignore it with the linker flag. The PIE warning is because now all executables on Lion are by default PIE (this is a requirement for things like ASLR.) Again just ignored with a flag.
There already is a haskell mode for emacs.
Why does it need to have an 'h' inside?
I was under the impression, that haskell package naming went the kde way.
\* cry \* Looks like this applies to previous versions of GHC as well... Okay, that decides it I guess. The Mac users in my Haskell class starting in two weeks will be using vmware and Windows or Linux.
I don't mean that your view of the matter is simply wrong, just that different views even of a distasteful thread like this one can both be rational. What for me is decisive is that `joppux` has genuine experience, was interested in the topic because it bore on his or her so far painful experience making a Haskell plugin for a widely used IDE, actually experimented with chrisdone's module, etc. The exasperation was genuine. This has nothing in common with the likes of J. Harrop. Harrop by the way is an out and out troll, moved by malice on the basis of minute Haskell experience, yet the tempests he stirs up are sometimes (sometimes!) fruitful and interesting. I am only counseling temperance, as I think an atmosphere of censorship is also ugly, and someone's feeling that he or she is being unjustly silenced is quite dangerous. It would be very bad if the moderator -- note the name -- is ever himself moved by anger or rage. 
This part shouldn't be terribly surprising. Compare the `Applicative` instance from the article: instance Applicative ((:~&gt;) a) where pure x = arr (const x) f &lt;*&gt; x = arr (uncurry (flip ($))) . first x . arr swap . first f . arr dup ...with the following instance, which has been gently anonymized: instance Applicative (Arr a) where pure x = Arr (const x) f &lt;*&gt; x = Arr $ \t -&gt; runArr f t $ runArr x t It should be obvious that this is doing the same thing as the first, except that it's taking a shortcut by exploiting the internal representation of the `Arr` type. In fact, `Arr` is clearly nothing more than a wrapper around a function, so this is just a trivial case of the above instance. Why is that interesting? Because the `Arr` type above, it so happens, is in fact the `Applicative` instance for `Reader`. Discarding the wrapper gives the instance for just `((-&gt;) a)` alone, the definitions for which correspond to certain other well-known functions: instance Applicative ((-&gt;) a) where pure = \x _ -&gt; x -- K combinator (&lt;*&gt;) = \f g x -&gt; f x (g x) -- S combinator So, another way of looking at the results in the article is that **the `Applicative` instance for `Reader` generalizes to any `Arrow` instance, not just `(-&gt;)`.** The same is *not* true for `Reader` as a `Monad`, however, despite the two being equivalent for `(-&gt;)`.
 newtype MyMonad a = MyMonad (StateT MyState (ReaderT Options (ErrorT MyException (WriterT [String] IO))) a) deriving (Functor, Monad, MonadReader Options, MonadState MyState, MonadError MyException, MonadWriter [String]) Ouch.
I don't dispute that the exasperation was genuine. The issue is whether or not this is expressed in a constructive way. Bashing a package because of some feature it lacks is one thing; going on to insult the "Haskell community" as a whole is another. A post like this: &gt; What, seriously? You call this "formatting"? It throws out all the comments, which means it's pretty much unusable garbage in practice. Does anyone have something that actually works? ...would be grating, but pretty clearly on the acceptable side of the line, as long as the response (after someone points out that it does, actually, have a mode that preserves comments) wasn't to simply move the goalposts and resume complaining about other, non-specific things. &gt; I am only counseling temperance, as I think an atmosphere of censorship is also ugly, and someone's feeling that he or she is being unjustly silenced is quite dangerous. There's a very big difference between censorship and expecting a degree of civility and I think it's extremely disingenuous to conflate them. Actually, jfredett expressed this pretty clearly in another comment on this post, so I'll just quote that: &gt; The content of those troll posts are equivalent to "syntactic noise" -- and like syntactic noise, they should be removed. The question is "what is the benchmark for noise" -- my philosophy of moderation is curation, not censorship, so there is nothing to fear in the "thought-control" sense (I say this, because this argument is most often linked to the notion of censorship in subreddits). 
This has already gone too far, I suppose, but l am just communicating with you: I of course agree that `joppux`'s exasperation was not expressed in a constructive way, he or she was childish, and presumably is a child, but it was possible to have the attitude of barsoap, which was my own: &gt; /me silently hands [X] a fine cup of Valerian tea and joppux a teddy bear. It was more productive when people were developing the powers of `Haskell.Src.Exts` to which `joppux` needed only to add some definite views about ideal formatting. One might have pointed to some of the available dogmatic but sensible coding style manuals abroad, e.g. tibbe's https://github.com/tibbe/haskell-style-guide (Myself, I go for the occasional wild stylistic outburst like [this hpaste](http://hpaste.org/49634), via Planet Haskell.) There is every reason to think that `joppux` could have been what is called a valuable member of the community, especially with an small access of maturity and a feeling for the ideals of the community. 
For what it's worth, I (another new moderator) mostly agree with what barsoap did there. Ideally, everyone would react that way, but the thing about trolling is that some people are more hot-headed than others, and will "feed the troll". These conversations aren't even necessarily trollish, but they usually devolve to useless name-calling on both sides and help nobody. So while I'd prefer everyone show restraint and avoid getting pissed off, I think needlessly flame-baiting by insulting an entire community is worth moderation. If nothing else, maybe we could start with just a reply from a moderator saying "be more constructive or any further comments of yours on this thread will be removed; same to anyone who responds non-constructively to this (but well thought-out rebuttals will be kept for posterity)". I don't know. I'm generally pretty hands-off, but some things just seem inappropriate here.
The style guide we use for the Snap Framework came from tibbe's. I wrote a [little utility](https://github.com/mightybyte/hstyle) to help me with some of the style checking for Snap. It only does the simplest of the checks, but if someone wants to add to the code, go right ahead. I'd love to see something more extensive. The line wrapping problem in particular is a difficult one that I'd love to see someone take on.
I recently read an [article that implements file IO based on arrows that capture the accessed files](http://blog.downstairspeople.org/2010/06/14/a-brutal-introduction-to-arrows/). Porting this to Category and Applicative would be an interesting example to back up the claim that Arrows are not necessary.
The web page is unreadable on an iPad. It insists it is a very wide page (so you have to scroll left and right to read every line) instead of letting the browser figure out how it should be displayed.
Right... in fact I chose a different theme for the blog just to get it wide enough to not scroll horizontally on most computer screens. The culprit is the long lines in the proofs of the Applicative axioms. I could make them shorter, but I would need to wrap them more and they would then become far less readable on ordinary computer screens.
NSFW ads on the linked webpage.
There are ads? Sounds like you'd got something going on client-side if you are seeing ads on the page. I'd suggest a malware scan.
...not that they're terribly readable anyway. I'm not sure if having the whole thing written out is adding much to the post; perhaps it would better to show a couple sample lines and link to another page with the full proof?
Yeah, I don't see any ads either. That malware scan is probably a good idea.
what's wrong with that? I've used this monad stack many times (with different MyState/Options/MyException types of course)
I get this exception in my application whenever there is a non-exhaustive pattern match. I don't think I use `forkIO` or `MVar` anywhere, though it is a very large and complex app (10s of thousands LOC Haskell) so perhaps there is some concurrency buried somewhere that I forgot about. The front-end is Neil Mitchell's cmdargs. Could that be the source of it?
For one, it's pretty redundant -- mostly because IO is a sin bin of everything you can think of. For another, it indicates bad design; you shouldn't just lump everything into one huge monad stack, instead of factoring out different parts of the program to use the appropriate monad for their situations. If you use a "global monad stack" like this, you're saying "every single function in my program needs all of this". And that's just not going to be true.
I use cabal-dev with multiple [yackage](http://hackage.haskell.org/package/yackage) servers. That is a very powerful way of including - or not including - various groups of various versions of various local packages in various virtual environments on various different machines. Setting that up in cabal-dev was a bit complicated though. More complicated than it seems that it should be. I have a number of shell scripts I use to set up, tear down, and manage the cabal-dev environment in the presence of yackage servers. Will it be easier to support that kind of workflow in virthualenv?
Did you mean to say that the Applicative instance for Reader generalizes to any *Arrow* instance?
I am happy to see them written out. Having a link to them on a different page would solve the formatting issues. But please make sure that the other page will not be in a place that will become a dead link some day. This post will probably become a go-to reference link.
Or perhaps try a different ISP.
&gt; For one, it's pretty redundant -- mostly because IO is a sin bin of everything you can think of. how would I go about using IO for ReaderT Options/passing options around? the only way that I know of is unsafePerformIO, IORef, and NOINLINE - I choose not to use it. anyway, I prefer to use pure things and use IO only to do, well IO. &gt; For another, it indicates bad design; you shouldn't just lump everything into one huge monad stack, instead of factoring out different parts of the program to use the appropriate monad for their situations. are you thinking about using different monad stacks throughout the program? I'd have to use lifts, runSomethingT everywhere, it wouldn't be very composable. maybe lumping every function to work in MyMonad isn't very good to express its effects - it's not that hard to fix - if I remove all the type sigs it would infer only needed effects (like MonadReader Options m + MonadIO m) &gt; If you use a "global monad stack" like this, you're saying "every single function in my program needs all of this". And that's just not going to be true. I have a function "runProcess env prog args input", that uses every part of that monad stack (it uses state to know how to indent the logging msgs, writer to record them, error for exceptions and reader to know what to log where. and IO for actually running the process) and almost every other function needs to run some process (the whole virthualenv started as a 100 line bash script, that evolved to 1k haskell script of much better quality.)
what do you mean by multiple yackage servers? from yackage synopsis I understand that you get a (single?) hackage substitute that you put into cabal config file. it's possible to use it with virthualenv - just put that line into cabal config file inside virtual environment (.virthualenv/cabal/config)
Ah, I see the new version of cabal-dev supports this nicely. Great! Will this work easily in virthualenv also?
cabal-dev works just like cabal, except you add the -dev suffix so that you know you are addressing the sandbox environment rather than the global environment. Also, cabal-dev wraps up all of the sandbox-related data in a single place - the cabal-dev subdirectory - so you can mentally switch between the global and sandbox environments without getting confused. That also makes it easy to customize the sandbox, write scripts that use it, make sure to run the right version of a compiled binary, etc. Same with `cabal-dev ghci` vs. `ghci` - I think that's a nice feature of cabal-dev. I wouldn't want to type `ghci` and not know what environment I'm getting. I would expect to be able to use different ghc installations with cabal-dev the same way as I would with cabal, though I haven't tried it. If it doesn't work yet, I'm sure they'll add that feature if there's a need. Same for all cabal flags and commands. I think the latest cabal-dev has pretty wide coverage. In short - I think the paradigm of cabal-dev vs. cabal to address sandbox vs. global for everything, and all the sandbox data in the cabal-dev subdirectory, is a very simple, powerful and convenient way of working. How does that translate into virthualenv terms? I wouldn't know how to get started. The emacs integration sounds like a nice feature. I personally don't work that way, though.
You can run as many yackage servers as you want, on different ports, or on different machines. They are lightweight. That was also how you had to do it in the older version of cabal-dev, by adding lines manually to the sandbox cabal.config. I have some shell scripts to help with that kind of thing. They're pretty messy. The new version of cabal-dev supports this via the `--extra-config-file` flag.
Okay, so interpreting the example there in terms of this post, what you get is that you can write an Applicative instance for that type. That's not terribly surprising... the type was data IORWA a b = IORWA [FilePath] (a -&gt; IO b) The Applicative instance is instance Applicative (IORWA a) where pure x = IORWA [] (const (return x)) (IORWA as f) &lt;*&gt; (IORWA bs x) = IORWA (as ++ bs) (\t -&gt; f t &lt;*&gt; x t) Where the inner `&lt;*&gt;` is from the Applicative instance for IO. You can check that this is the same instance that the article gives in terms of the arrow terms. It is. Therefore, you know this is a good Applicative instance... that is, all of the axioms hold... assuming that it was a good Arrow instance.
Er, yes, probably. That would seem to make more sense. Clearly it doesn't work for all `Category` instances.
I would think the Haskell Wiki would be a very reasonable place to put this sort of information, myself.
Okay, I wanted to see example programs comparing Arrow and Applicative notation for the `IORWA` type so I tried to come up with some myself. As a warm-up, copying a file only requires the `Category` instance: copy :: FilePath -&gt; FilePath -&gt; IORWA () () copy from to = writeFileA to . readFileA from So let's instead try to implement something like the Unix command $ cat file1 ... fileN &gt; file Again, we can compose it from reading and writing actions: cat :: [FilePath] -&gt; FilePath -&gt; IORWA () () cat ins out = writeFiles out . readFiles ins Reading files is quite straightforward with either `Arrow` or `Applicative`: readFilesArr :: [FilePath] -&gt; IORWA () [String] readFilesArr [] = arr (const []) readFilesArr (f:fs) = (readFileA f &amp;&amp;&amp; readFilesArr fs) &gt;&gt;&gt; arr (uncurry (:)) readFilesApp :: [FilePath] -&gt; IORWA () [String] readFilesApp [] = pure [] readFilesApp (f:fs) = (:) &lt;$&gt; readFileA f &lt;*&gt; readFilesApp fs The `Applicative` version is slightly shorter. An action writing a list of strings to a file can be implemented using `(|||)` from the `ArrowChoice` class: writeFilesArr :: FilePath -&gt; IORWA [String] () writeFilesArr file = arr viewList &gt;&gt;&gt; (id ||| ((writeFileA file *** writeFilesArr file) &gt;&gt;&gt; arr (const ()))) where viewList [] = Left () viewList (x:xs) = Right (x,xs) I have no idea how implement a corresponding action using the `Applicative` instance. But maybe this is cheating because I used the `ArrowChoice` class for the `Arrow` version, so I tried something simpler that does not require `ArrowChoice`: writing exactly two strings to a file: write2Arr :: FilePath -&gt; IORWA (String,String) () write2Arr file = writeFileA file *** writeFileA file &gt;&gt;&gt; arr (const ()) Again I don't know how to write this using `Applicative` because it requires to combine two actions of type `IORWA String ()` into one of type `IORWA (String,String) ()`. This does not seem possible without additional combinators that change the first type parameter which is universally quantified in the Applicative instance. In general, I wonder how to implement programs where the first type parameter changes. Maybe you already have some ideas about it. I'm looking forward to your next post! edit: [complete code](https://gist.github.com/1116845) on Github.
I find this work quite interesting, but I have to question its claimed significance. &gt; If we can do this, then Arrow becomes not particularly interesting, since it’s just an overly complicated way of expressing computations with Category and Applicative. This is what we’re trying to show. I would say rather that the Arrow typeclass becomes redundant; but that doesn't make it uninteresting. As an analogy, `ErrorT e StateT s` is just the composition of two transformers - `ErrorT` and `StateT`. But it is a particularly interesting combination of transformers, because it allows certain patterns of backtracking computation. If you regularly used `ErrorT` and `StateT` alone, you'd still have more to learn about using `ErrorT StateT`. I think the same situation exists with `Arrow`, except much moreso. If you study how to use `Applicative`, and then study how to use `Category`, you'll probably have no idea what `Arrow` is good for or how to use it. In fact I doubt you could get very far studying `Category` on its own, since there are rather few interesting `Category` instances whose usefulness doesn't derive from their also being `Arrow` instances. This is all stated relative to my own experience and interests - I use Haskell to implement things, but am not deeply involved in the study of Haskell itself (e.g. the type system, the categorical correspondences between certain typeclasses, etc.) Perhaps to someone whose main interest is the theoretical content of `Arrow`, this proof does render `Arrow` "uninteresting".
Right, so you need Category and Applicative, both. instance Category IORWA instance Applicative (IORWA a) Then you get `(&gt;&gt;&gt;)` (which is just `flip (.)`) from the Category instance, and you can build `(***)` from the Arrow primitives, `arr` and `first`, which we've already implemented. After some simplification: f *** g = (,) &lt;$&gt; f . (fst &lt;$&gt; id) &lt;*&gt; g . (snd &lt;$&gt; id) So you can write the term you want as: write2Arr file = pure () . ((,) &lt;$&gt; writeFileA file . (fst &lt;$&gt; id) &lt;*&gt; writeFileA file . (snd &lt;$&gt; id)) You don't actually care about the output, though, so you can simplify a bit more: write2Arr file = const (const ()) &lt;$&gt; writeFileA file . (fst &lt;$&gt; id) &lt;*&gt; writeFileA file . (snd &lt;$&gt; id) Okay, that's a tad longer in characters, but simpler. You can actually use an auxiliary Applicative function to do better, if the result of `write2Arr` is `()`: write2Arr file = writeFileA file . (fst &lt;$&gt; id) &lt;* writeFileA file . (snd &lt;$&gt; id) None is comparable to the brevity of your Arrow version, mainly because Arrows are built to work with tuple types and have primitives for it, while Applicative handles that as just yet another kind of type to work with.
Interesting. &gt; you can build `(***)` from the Arrow primitives, `arr` and `first`, which we've already implemented. You are probably referring to your second post about getting `Arrow` from `Applicative` which I am looking forward to reading.. Do you have ideas for extensions of `Category` and `Applicative` that correspond to `ArrowChoice` or other `Arrow` extensions?
&gt; there are rather few interesting Category instances whose usefulness doesn't derive from their also being Arrow instances. I dispute this on the basis that the `Arrow` instances are not, typically, inherently interesting either. Both `Category` and `Arrow` describe abstractions over composition; `Category` alone only lets you stick things together end-to-end, whereas `Arrow` and related type classes allow looping, splitting, merging, and whatnot as well. All of this is completely useless on its own, it's what the compositions mean for specific instances that's useful. Note that your analogy to monad transformers cites *specific* instances; a closer analogy would be to say that `Functor` isn't interesting other than where it leads to `Applicative`. In some ways, `Arrow` is actually less interesting, because it demands the ability to lift arbitrary functions, ensuring that `Arrow` instances are basically "functions with extra machinery bolted on", rather than anything completely different. Consider an EDSL with a very limited selection of primitives and the ability to construct function expressions of some sort; a `Category` instance would probably be straightforward, but supporting `arr` would undermine the whole thing. 
Oh, you're right, those terms are defined in the upcoming second post, but here they are for reference. arr f = pure f &lt;*&gt; id = f &lt;$&gt; id first f = pure (,) &lt;*&gt; f . arr fst &lt;*&gt; arr snd 
That `arr` requires only `Category` and `Functor` is another one of those entirely unsurprising results, I note. More interesting is `first` and the associated laws.
not an answer but slightly related: http://www.haskell.org/haskellwiki/If-then-else
What's wrong with `if foo then bar else qux`?
well... I like functions! :D I can pass `iff` to combinators, I can use it in folds, maps, and whatnot, I can lift it to a monad... I like this (ok, it's a stupid example and I couldn't quickly think of a better one): fmap ((ffi 1 0).(&gt;5)) [1..10] where ffi = flip.flip iff better than this: [if (k &gt; 5) then 1 else 0 | k &lt;- [1..10]] It's a stupid example, but you can see that `iff` is just more flexible and useful. 
It may have some uses, but I think the second version is way more readable.
The Data.Bool.HT module in the utility-ht package has that (if'): http://hackage.haskell.org/packages/archive/utility-ht/0.0.1/doc/html/Data-Bool-HT.html
Ugly special syntax that doesn't gel with the rest of the language; awkward indentation rules; can't be used in a higher-order manner; inconsistent with other eliminators (`foldr`, etc.).
I like cap (consequent, alternative, predicate), which looks like this: cap c a p = iff p c a and is *very* handy for monadic code. Some shorthand for \x -&gt; case x of ... would be even better, right now I end up having lots of functions named f and g.
It even has `(?:)` foo ?: (bar, qux)
Way more readable.
It should be called 'bool', to follow the pattern of 'maybe' and 'either'.
To emphasize jbpriestley's point that import of `Control.Category` is almost unknown, *except* in support of `Control.Arrow` note that the maybe not-so-reliable http://www.google.com/codesearch#search/&amp;q=import%5C%20Control%5C.Arrow%20lang:haskell&amp;type=cs finds 911 modules importing `Control.Arrow`; http://www.google.com/codesearch#search/&amp;q=import%5C%20Control%5C.Category%20lang:haskell&amp;type=cs finds 130 imports almost all of them with `Control.Arrow`. The independent imports of `Control.Category` seem to be almost entirely in speculative theoretical modules (bless them), whereas imports of `Control.Arrow`, are followed by library-specific instances. -- This is why we don't see except in connection with `a -&gt; b` and `(a,c) -&gt; (b,c)` and so on; their merit seems to be bound up with the character of specific libraries. (In case you think they belong somehow to the past, note that e.g `hakyll` has been rewritten with an 'arrow interface' just recently; I assume this decision was founded on actual features of the library though it has been giving me some conceptual trouble lately :)) cdsmithus' suggestion that "But Category and Applicative are very simple, easy, and intuitive" is certainly true of `Applicative,` but in fact there is no evidence about `Category`, since it is simply unknown in actual or substantive code; I assume its existence is simply an outgrowth of `Control.Arrow`. The `(.)` and `id` he uses in his proofs have almost never been seen in substantive modules. 
For whatever reason, I find the two operators convenient: infix 1 ??&lt; (??&lt;) :: Bool -&gt; (a, a) -&gt; a (??&lt;) True = fst (??&lt;) False = snd infix 1 ??/ (??/) :: Bool -&gt; (a -&gt; a) -&gt; (a -&gt; a) (??/) True = id (??/) False = const id The first is the ternary operator acting on a pair. The second abbreviates a pattern that comes up all the time in some code that I write where a substitution of values is predicated on a condition. "If b then replace with function f, else leave unchanged." Of course, this is just a matter taste. I think different people have different inclinations towards the intuitiveness of different syntaxes. I think having an 'iff' function as you have written is useful as well since it better integrates into the intuitions of the functional syntax, able to be partially applied and composed. 
If you participate in obfuscation contests, please tell me so I can bet on you :P
&gt; There's a very big difference between censorship and expecting a degree of civility Your comment prompted me to [google for civility and censorship](http://www.google.com/search?q=civility+censorship). Apparently, different people draw the line differently between these two concepts (some draw it not at all). Asserting a "big difference" without explaining it may be perceived by some as "thought control" ;)
Huh. I can't remember the last time I used an Arrow, but I use Category with fclabels somewhat frequently. Incidentally, those category morphisms are definitively *not* arrows; there's no way to lift a function into a lens.
How many of those modules import Control.Arrow for any reason other than using its constants with the (-&gt;) instance? I guess there's probably no easy way to figure that out programmatically. My experience is that when I import Control.Arrow, it's because I want to use `(&amp;&amp;&amp;)` or `(***)` on pure functions.
Hummm! This is nice syntax!
It's more readable, but less flexible and harder to generalize. My personal rule is: if you have to choose between functions and special syntax with keywords, you should probably use functions. Functions can be composed, usually generalize to some known pattern, can be passed around to higher order combinators... Special syntax and keywords are static features of the language, with fixed syntax and rigid usability. What makes then readable, makes them limited and rigid.
Let me be clear: I'd never write something like this code above in a real code. It's a poor example that can be done in many simpler, more readable ways, even without abandoning the point free style. My point is that iff can be passed to maps, folds,etc... and can be composed with functions that return Bool. You can break up a pattern from that though: (ffi.pred) cond1 cond2 where pred is a predicate function that returns a Bool, and cond1 and cond2 are values to be returned for the different results. You can now map this over any functor, or mapM it in a monad. You can make this pattern part of a fold... 
Eh. That google search only seems to turn up vapid political pundits wringing their hands over nothing of substance. That's not "thought control" because you can't control what doesn't exist. And the difference is "have whatever opinions you like, but don't be an asshole about it". It's *really* not that complicated. Yes, it can be problematic when talking about controversial issues where people may get offended over ideas alone, but /r/haskell is about a very specific, technical topic and anything that controversial is probably egregiously off-topic anyway. But of course, keep in mind that I'm not going to be acting unilaterally here, and it's pretty clear that I have the strongest views about quality of content, so I doubt any actual guidelines--which remain to be determined-- will be as strict as I might prefer.
This pretty much matches my own use. I also tend to simply import `Control.Arrow` by default, even when I don't end up using anything other than composition (e.g., with fclabels, as mentioned in your other comment), and thus only needed `Category` anyway. On the handful of occasions where I've actually used `Arrow` proper, `arr` often seems to be more of a nuisance than anything else. In most cases, I would much rather have an extended `Category` class with tuple support and no lifting.
&gt; In most cases, I would much rather have an extended Category class with tuple support and no lifting. Arrow currently relies so heavily on lifting arbitrary functions to handle all the plumbing, the set of constants would have to grow rather quickly to get tuples without lifting. I suppose you'd need constants for lifted versions of swap (`\(x,y) -&gt; (y,x)`), dup (`\x -&gt; (x,x)`), fst (`\(x,y) -&gt; x`), and assoc (`\((x,y),z) -&gt; (x,(y,z))`), in any case. And I don't want to think about what the axioms would look like!
Efficiency concerns aside, I believe `fst`, `snd,` and `(&amp;&amp;&amp;)` would suffice. In practice you probably don't really want to implement `assoc` as `fst . fst &amp;&amp;&amp; (snd . fst &amp;&amp;&amp; snd)`.
Shall we rename `foldr` to `list` while we're at it? :]
`if foo then bar else qux` doesn't lend itself to currying
I'm more than happy to have you do the job. Thanks! =)
There was a proposal for such a shorthand. IIRC the syntax was : case of pat -&gt; expr pat -&gt; expr 
I plan to implement a feature to not ship default cabal config, but use the one from user, only overriding some settings. when it's implemented, using --extra-config-file instead of ~/.cabal/config will be trivial. what I don't understand about multiple yackage servers is this - do you still want to use single server per environment (that's easy) or do you want to create single environment using multiple servers?
If you're a bit clever, you can replicate the syntax of the ternary operator too -- in a functional way. :) For a first pass, consider (?) b x y = if b then x else y Now we can write: True ? 1 $ 2 Since `:` is taken, it's difficult to replicate things directly (also, the types are funny too). But that's pretty nifty, I think. 
Ah, right... I wasn't thinking of `(&amp;&amp;&amp;)` as a primitive, but rather as a term implemented in terms of arr. Adding that as a primitive would go a long way to replacing the arr-based plumbing.
Yeah. Keeping in mind that `id` is already a primitive, any reordering of tuples can be done with a branch-and-project approach, parallel composition can be done by branching with distinct projections, duplication branching with just `id`, etc. The axioms would mostly amount to specifying that any combinations of `id` and the above tuple-wrangling which have the same type are equivalent, e.g. `id` = `fst &amp;&amp;&amp; snd`.
I've just written a blog post about the conditional choice operator: http://zenzike.com/posts/2011-08-01-the-conditional-choice-operator it's not a library, but your question reminded of this.
I actually like this for didactic reasons more than syntax reasons. In the end, `\x -&gt; (case/if) x ...` isn't _that_ big a deal (though given the terseness of Haskell in general that does stick out more than it would in more verbose languages), but I think that teaching someone Haskell and being able to introduce these as yet more functions instead of as special syntax constructs would go a ways towards continuing to hammer in the functional nature of Haskell. I would consider the syntax exception of letting case functions put their parameter right after the word "case" to be an acceptable "quirk". Another entry on my Haskell 3000 list.
Once you're at it, you can unify case, pattern-matching and lambda abstractions entirely: fac = λ 0 -&gt; 1 n -&gt; n * fac (n-1)
There's always [bool-extras](http://hackage.haskell.org/package/bool-extras), which has the arguments in the 'right' order (false case, true case, bool).
Multiple servers. (One of them is Hackage itself, of course.) That is accomplished simply by adding multiple `remote-repo:` lines to the cabal config. That is fully supported by cabal. Shouldn't be too hard for virthualenv either, once you are supporting something like `--extra-config-file`.
I also like: ifM :: Bool -&gt; a -&gt; Maybe a ifM b a | b = Just a | otherwise = Nothing Useful in the Maybe monad and elsewhere: x' &lt;- ifM cond1 x _ &lt;- ifM cond2 () 
I think you might be able to generalize that, no? Any MonadPlus ought to satisfy that, eg: ifZ :: MonadPlus m =&gt; Bool -&gt; a -&gt; m a ifZ b a = if b then return a else return mzero And really, it's any Monoid with an identity element -- eg, and `mzero`. MonadPlus just happens to have the word with it. 
I think you might be able to generalize that, no? Any MonadPlus ought to satisfy that, eg: ifZ :: MonadPlus m =&gt; Bool -&gt; a -&gt; m a ifZ b a = if b then return a else return mzero And really, it's any Monoid with an identity element -- eg, and `mzero`. MonadPlus just happens to have the word with it. EDIT: I thought about this some more. Maybe it would be useful -- or at least curious -- to define class WithIdentity m where mID :: m a such that instance MonadPlus m =&gt; WithIdentity m where mID = mzero and so on. This class (which is poorly named, I think) encapsulated the notion of a "Default" value like `Maybe` does, but it a more generic way. An interesting tack on this idea is a structure with multiple "default" values -- perhaps the correct term is multiple "distinguished" values -- perhaps an unknown but finite number of such values. I could see that as the basis of a Monadic Exception system with a bit more cleanliness than `Either`. Further, I imagine that such a structure might have some interesting mathematical properties. I have an image in my head for what such a space might look like topologically, but I'm not versed enough to know how to express it. In any case, it's interesting. 
Tasty!
That's my favorite part about Haskell, in Perl, you golf to get the shortest program possible, in Haskell, you golf to get the most absurdly generalized version possible. It's like Perl-golf, but more mathy.
I understand why the bool comes last, but why the false case before the true case?
http://hackage.haskell.org/packages/archive/DeepArrow/0.3.3/doc/html/Control-Arrow-DeepArrow.html ^^ Set of methods growing to get tuples without lifting. It subclasses Arrow, but is supposed to imagine "arr" just isn't there.
Because: &gt; :i Bool data Bool = False | True -- Defined in GHC.Bool It's the order in which the constructors are defined. Again, by analogy to `maybe` and `either`.
It just means you don't use any arrowized libraries but you do use what seems to be the only "category"-ized library. It surprises me that the writers don't recognize that this thread is de facto Arrow trolling.
I wouldn't mind. :)
That makes sense, thanks for the answer.
I certainly didn't intend any trolling. I am writing this in an attempt to better place Arrow in relationship to other concepts and thus understand it. Category is one of those concepts, because regardless of anything else, it's at least really simple and obvious.
http://www.haskell.org/ghc/dist/stable/docs/html/users_guide/release-7-2-1.html
I've fixed `network` now. I will make a release shortly, as soon as I've addressed some new warnings introduced in 7.2. Edit: Done! http://hackage.haskell.org/package/network
I think MonadPlus/Monoid just ought to be split into a Zero class and a Monoid/MonadPlus subclass. Perhaps even the associative operation deserves its own class.
`maybe` and `either` are not recursive. Maybe `list` ought to be: `(a -&gt; [a] -&gt; r) -&gt; r -&gt; [a] -&gt; r` and not the recursive variant? 
`foldr` is reversed: λ&gt; :i [] data [] a = [] | a : [a] -- Defined in GHC.Types λ&gt; :t foldr foldr :: (a -&gt; b -&gt; b) -&gt; b -&gt; [a] -&gt; b 
I'm always for making typeclasses smaller, they're so absurdly composable. I'd really just like to have Type Class Aliases the most...
Yes, they're exactly asking "which constructor". In fact, they convert the data type to its [Church encoding](http://en.wikipedia.org/wiki/Church_encoding), which is essentially reifying a pattern match as the continuations of each case. Using GADT syntax makes this easier to see: data Maybe a where Nothing :: Maybe a Just :: a -&gt; Maybe a Each case of a pattern match is effectively a function that takes the same arguments as the constructor, but produces some unspecified type `r` for the final result: matchNothing :: r matchJust :: a -&gt; r A complete exhaustive match obviously needs to handle each case: matchMaybe :: r -&gt; (a -&gt; r) -&gt; r Functions of this type are therefore isomorphic to values of type `Maybe a`. A conversion function would be straightforward to write: churchMaybe :: Maybe a -&gt; (r -&gt; (a -&gt; r) -&gt; r) This is precisely `maybe` with a different argument order; the same applies for `either`. Product types work similarly: data (a, b) where (,) :: a -&gt; b -&gt; (a, b) matchPair :: a -&gt; b -&gt; r churchPair :: (a, b) -&gt; (a -&gt; b -&gt; r) When dealing with recursive types, however, there are two options: Note that the type signatures for the Church encoded form are obtained by replacing the data type with an unspecified result in the type signature of each constructor. Recursive types are those that appear as arguments to their own constructors; leaving those as is gives the [Mogensen–Scott encoding](http://en.wikipedia.org/wiki/Mogensen%E2%80%93Scott_encoding), while replacing them everywhere gives recursive Church encoding: scottList :: [a] -&gt; (r -&gt; (a -&gt; [a] -&gt; r) -&gt; r) churchList :: [a] -&gt; (r -&gt; (a -&gt; r -&gt; r) -&gt; r) The latter (i.e., `foldr`) is strictly more general, subsuming every operation possible on lists as well as being well-typed without general recursion (due to being obviously terminating on finite lists); as above, the Church encoding is isomorphic to the corresponding data type. The former style is rather more convenient in many cases, but relies on other means of recursion to do anything useful. So it really is true that `maybe`, `either`, &amp;c. are in some sense a degenerate case of generalized `fold`ing.
I wonder how readability goes as nested conditions are added.
I recognise what you say about Quants delivering egregiously broken code. Sometimes it doesn't even compile, let alone work. As a Haskeller though, I'd say that we have been brought in to try to tame this nonsense. If Quants are going to write bad code, then we'd better give them tools that either point out (or automatically eliminate) the majority of their bugs before it goes any further than our team. Ironically, we have found more eager adoption of Haskell by some of the traders rather than the Quants. Being untrained in programming but mathematically literate, and well-versed in pure Excel sheets, they often find Haskell easier to understand than the alternatives available to them (e.g. VBA).
"Doctor, it hurts when I do this..."
Oh, that is lovely. I sort of wish the brackets were partly asymmetric, though--the expression as a whole is certainly not symmetric, since swapping the values requires negating the boolean.
Most of those laws are false when you take `undefined` into account...
It's still morally sound?
Worth noting, syntax-wise, that `&lt;|` and `|&gt;` are already defined in a couple places, like [Data.Sequence](http://haskell.org/ghc/docs/6.12.3/html/libraries/containers-0.3.0.0/Data-Sequence.html#v%3A%3C|).
That's encouraging. I really wish that Excel itself (ie not VBA) were taken more seriously as a reactive, direct manipulation, functional, data driven programming environment. Spreadsheets are "alive" in almost the sense that Smalltalk and Lisp images are alive (and relational databases are supposed to be), which can be a beautiful thing. 
yes.
Yep, I did angst a bit over that. I considered `&lt;?` and `?&gt;`, but I thought for the sake of the post I'd stick to syntax that's closer to the original (which is rendered in UTP as triangles, and in CSP as a `&lt;` with a vertical bar going down the middle).
That's... pretty odd. Here are some experiments to try: run without `-threaded`, run with a ridiculously large nursery size, grep for MVar, grep for forkIO, grep for these in the libraries you use...
I'm curious, just for the purpose of staying up-to-date on all the ways of solving this problem... what do you gain with multiple yackage servers, versus using `cabal-dev add-source` prior to your build? I can imagine some reasons: for example, you do have to run the right combination of `cabal-dev add-source` commands each time you initialize a new sandbox, which I suppose could be more frequently than you do a yackage upload. Is that the problem? Or is there something else?
You used "there" wrong twice in the same sentence.
Hmm… compare x y = LT &lt;| x &lt; y |&gt; GT &lt;| x &gt; y |&gt; EQ 
I believe Bryan filed a bug about this and it has since been fixed. I guess it'll be in the next RC.
Well, there's one on a central server for versions that are published within our company but not to Hackage. Then there is at least one on each physical machine, for controlling which "work in progress" versions of some packages get used in other packages being worked on on that machine. There can be a yackage server representing a branch that cuts across several packages. Etc. Yes, of course much of that could be accomplished by careful use of version numbers and cabal files. But that is a horribly complex and error-prone way to work. It's like the difference between using CVS or Microsoft SourceSafe and managing branches manually with version numbers, vs. using a modern system like darcs or git. Yackage uploads are far more convenient than initializing a new sandbox. It's instantaneous. I have a "cabal-forget" script to make sure a certain package is reloaded from yackage. I do those two constantly during a development cycle, maybe every few minutes. Whereas for every new sandbox you need to rebuild all the dependencies, which can take a long time. Not to mention all the packages you need to add-source, though that could also be scripted.
Goddamned homophones
Implementation without `Maybe` type: infixr 0 &lt;|, |&gt; (&lt;|) :: a -&gt; (a -&gt; a) -&gt; a x &lt;| f = f x (|&gt;) :: Bool -&gt; a -&gt; a -&gt; a False |&gt; y = const y True |&gt; _ = id 
Or something like this? infixl 0 &lt;?&gt;, &lt;:&gt; (&lt;?&gt;) :: Bool -&gt; a -&gt; Maybe a True &lt;?&gt; x = Just x False &lt;?&gt; _ = Nothing (&lt;:&gt;) :: Maybe a -&gt; a -&gt; a Nothing &lt;:&gt; y = y Just x &lt;:&gt; _ = x 
You may already know this, but this is essentially equivalent to saying that the category in question has all products, with the product of a and b being (a,b). exists fst :: (a,b) ~&gt; a exists snd :: (a,b) ~&gt; b forall f :: x ~&gt; a, g :: x ~&gt; b, exists unique f &amp;&amp;&amp; g :: x ~&gt; (a, b) such that fst . (f &amp;&amp;&amp; g) = f and snd . (f &amp;&amp;&amp; g) = g I don't know that all the stuff you've specified is strictly equivalent to this, but it's probably the closest categorically motivated version. class (Category (~&gt;)) =&gt; HasProducts (~&gt;) where fst :: (a, b) ~&gt; a snd :: (a, b) ~&gt; b (&amp;&amp;&amp;) :: (x ~&gt; a) -&gt; (x ~&gt; b) -&gt; (x ~&gt; (a,b)) It's actually a little off, because the product for any category is required to be the Hask product (and, ignoring the fact that it's technically not a product at all), but that's pretty common due to Haskell's limitations. Edit: Actually, it occurs to me that a lot of arrows may not actually have products correctly for this. For instance, `(a, b)` is not a product of a and b in `Kleisli (State s)`, because there are two choices for the order of effects, and `fst . id &amp;&amp;&amp; put v /= id`. So I guess this isn't really a `HasProducts` class for most Arrows.
Is the haskell if/then syntax better for cases where we have to avoid undefined? I saw a similar comment in the post but so far no one has given an example.
No. Those laws don't apply to if/then/else any more than they apply to this operator. But that doesn't make them true laws.
I sort of knew that. I was trying for a categorically motivated version but didn't bother to look up half-remembered concepts. Pretty sure mine isn't equivalent though; I think I left the order of effects ambiguous in the same way you noted in your edit. In keeping with the spirit of the post, though, `Applicative` actually has the same ambiguity; it's only by convention that effects are ordered left-to-right by `(&lt;*&gt;)`. Consider `Maybe` as a functor to a category "Maybe Hask". Using its `Applicative` instance to map tuples in Hask to tuples in Maybe Hask, and given the arrows `f = fmap id` and `g = const Nothing`, `fst . f &amp;&amp;&amp; g` =/= `id`. So `(,)` isn't a product in Maybe Hask (which is fine, because `Applicative` doesn't actually claim that it is). So, a category with actual products is a significantly more restrictive notion than the monoidal structure given by `Applicative` and (I think) `Arrow` as well, though I believe it's more restrictive than `Applicative` to some extent.
Maybe it's [lazyConsume](http://hackage.haskell.org/packages/archive/xml-enumerator/0.3.4/doc/html/src/Text-XML-Enumerator-Document.html#renderLBS).
BRAVO! I am excited to see how this goes. I have a 13 year old and will try it on him!
I'm also teaching some 10-13 year-olds programming in Haskell, though I'm calling it a "video game workshop" and loosely basing it on bootstrap, so I'll be followed your blog! One reason I chose Haskell is that children will be applying for jobs in five years' time, not next week, so they need to learn the ways of the future, not the antiquated and ultimately doomed methods used in industry today.
Do you think that purely-functional programming will dominate the industry in 5 years?
Sound like a real challenge but man, if you pull this off, well done. There's a lot of people saying Haskell is really hard to understand as well as being the most natural easiest to understand. Not much middle ground though but hopefully you can show Haskell as a very understandable language.
As for me, I don't think that. But I also don't care. When teaching children at that age, it's doing them a disservice to try to plan out their careers for them and start in on the job training. It's far more important that functional programming is better preparation for mathematics and logic, and that's very hard to deny.
I'm not worried about the Haskell, honestly. If something is too hard in Haskell, I just won't teach it. It's not on my list of goals for students to leave with a complete understanding of the programming language. Gloss has the very nice advantage of sticking to the purely functional subset, and for the most part avoiding type classes... so it's a lot closer to a Scheme-ish environment than many other alternatives for graphics programming.
Give me a break. Although I agree fully that these kids will enjoy Haskell more than, say, C++ or Java; the same kids will not benefit more by being able to write "Haskell" on their resume than by writing "C++, Java."
Eh, Haskell is more likely to be relevant 10 years from now than Java is. All else aside, Haskell will probably still see use in programming language research, whereas Java is heading fast toward being the next generation's COBOL--a stagnant tool for dead-end careers (albeit well-paid ones, since all the new kids won't want to touch it with a ten-foot pole, and there'll still be mountains of terrible legacy code written in it). The kids would benefit from having on their resume whatever language will be most popular at the time, which probably won't be any of the above.
&gt; There's a lot of people saying Haskell is really hard to understand as well as being the most natural easiest to understand. When people say Haskell is hard to understand, they're right. But what they usually forget is that *programming* is hard to understand. Anyone who thinks imperative programming is somehow intuitive has forgotten how many newcomers struggle with concepts like assigning a value to the target of a reference. Not to mention that most programmers never actually do learn to understand real OOP.
I suspect functional programming is easier to learn for someone completely new to programming when compared to ease of learning say Java or even Python. I also suspect that more people would be able to enhance their lives, careers, and help others around them more if they were introduced to functional programming. Arguably Haskell's type system gives too much rope. The compiler's error messages can be unfriendly because of it. But that's a prevalent problem for every language commonly used in first programming classes, if recent research is to be believed. The way students normally address errors is to repeatedly mutate their code more or less randomly until their current problem goes away. For all the "complexity" of the Haskell type system, I suspect there are fewer ways for a student to get things wrong and fewer methods to employ and learn for figuring out where his or her thought process went wrong. Put another way, I'm familiar and very practiced at visualizing a cpu traversing my code when I'm trying to fix problems. When I'm programming Haskell I spend a lot more time just thinking about the types of the functions I'm applying. Thinking about types is easier, and pure functions tend to behave exactly as advertised. Put yet another way, I'd get a lot more done working Haskell's way if I was equally practiced at the two approaches. My suspicion is that this would translate well to beginning programmers as well. I hope we get to hear more about the development of this course. Don't hold back. 
&gt; Haskell is more likely to be relevant 10 years from now than Java is. The problem with Haskell, or any pure functional language, is its requirements on (1) a mathematical mind and (2) a somewhat deep understanding of how Haskell works underneath. Without (1), the code will look worse than procedural; without (2), the code will space-leak in ways you would think the Big Bang was just a small trickle. Both requirements gives Haskell a steep barrier of adoption. What made Java so successful in the beginning was its determination to simplify the complex C++, to make it easy for even kids to start hacking out games. I just don't see Haskell making it easy. Try explaining to kids how Arrow works. 
First, best of luck with this effort! Second, I think Haskell has a lot going for it in this context. You will have much less resistance on some topics because you are not displacing ingrained behavior. However, one area where it is difficult to teach superficial Haskell is with error messages. If a student is executing sequences of imperative commands, and their program crashes due to something a type system would catch, they can often print out values to trace execution and see a specific, concrete, nonsensical value. When the student compiles a Haskell program and gets an error message from the type checker, they will be dealing with something much more abstract. We can, and do, argue that careful construction of a specification in the form of types and comments is tremendously important, and one of the most important skills we aim to teach. However, the error messages from the type checker will need to be explicitly addressed as part of in-class instruction. Forcing this issue may be a good thing, but it is something that you will have to handle differently than people using Python or Racket. My take is that you can handle it, and I'm looking forward to updates on your progress.
&gt; Try explaining to kids how Arrow works. The key is to *not* explain to kids how Arrow works. Heck, I've been programming in Haskell for years, and I only use the Arrow module because some of its combinators are useful when specialized to work on functions. Trying to introduce Arrow to children is like explaining the Spring framework to children; they won't have the context to understand it, and they don't need it anyway. On the other hand, you're right that Haskell requires mathematical thinking. That's one of the key reasons that I decided on it for this task!
Error messages are a big concern, yes. I'm explicitly planning to be there and available for a lot of the programming time for exactly that reason. I don't want to leave people working out what could possibly be meant by "No instance for Num [Char]" on their own!
I'm really referring to the language and vocabulary you use to help the students understand the error messages. Maybe you don't want to get too much into type theory, but there is a real risk that by using friendlier terms to help understand specific errors you end up presenting to the students an ill-conceived re-invention of basic type theory. I really don't know what the best way to handle this is. Hopefully there won't be any really insane-sounding messages from the type checker, but this will require considerable effort on your part to construct exercises that assiduously avoid the deep end of the pool.
One thing working in your favor is that you are explicitly not obligated to teach them the entire language, or to make sure they learn all the practical, real-world uses. You can ignore or replace anything that would be too complicated to introduce. For example, it looks like Gloss uses `Float` for almost everything; you could replace large chunks of the standard libraries with every function using fractional numbers specialized to `Float`. You could also decide that treating strings as simple character lists is a distraction and add a `newtype String = String [Char]` ADT. I'm not a big fan of that kind of simplifying when it comes to teaching people who intend to be programmers or when intending to teach Haskell itself in any depth (seriously, why do we need `map` specialized to lists?) but I think in this case it might have some merit.
Ah, in this implementation, `(|&gt;) p` is just the [Church encoding](http://en.wikipedia.org/wiki/Church_encoding#Church_booleans) of `not p`! Nice.
Have you considered using Helium instead?
In what ways do you think Java makes it easier to structure code and understand operational behavior? Please explain *without recourse to any prior knowledge of programming*, since we're talking about kids who've never seen any of it before, and they're easily confused by the difference between having a clone of an object and having multiple references to it. Then try explaining Design Patterns to those kids.
No, I haven't. What's the interoperability like? Would I have to abandon the gloss package? If so, that would be a dead end; everything I've got planned right now is heavily tied to gloss.
have you considered Ocaml? I've often wondered which would be a better choice for teaching a young person. I agree though that if you are teaching someone with no prior programming experience, functional programming could be an excellent way to start. 