I wrote a small library recently for writing IRC bots using Pipes: https://github.com/co-dan/pipes-irc The code is fairly small
No, I think he means 'with School of Haskell' and 'with FP Haskell Center'. He is not referring to the algorithmic aspect (whose beauty is wholly due to Haskell), but to the presentational aspect of this blog post, with the running code, the possibility of immediate feedback and exploration on the web page, the inline creation of diagrams, ... It seems appropriate to say that this is accomplished by the School of Haskell and the underlying technology stack, not by Haskell per se. (Well, that's what I think he means. And I think it is appropriate.)
Remove m=1 from the url to see the code snippets.
Why not access the file directly from the source tree? Use the location TH function to get a path to the current source file, then modify it to get a path to ../schema.txt or whatever. Are there any gotchas with this method?
I wrote a chess program some time ago, using bitboard representations - and for that it uses bit search absolutely everywhere for iterating the results of some bitwise operation. Made a huge difference to use the built in bsf, and bsf was massively faster than lookup tables or binary operations. (this was before gcc had such a builtin I suspect)
Thanks Edward, this is a truly excellent article. I have read much about comonads but I have not yet reached the point where I can "see" them when I write code. This tutorial has been very helpful.
Oh, darn. That's what I get when chiming in when afk for a few days! :-) 
I haven't explored that possibility, to be honest. Using a pre-build hook jumped immediately to mind, so it was the path I took :)
I'm surprised the tail recursion optimization would make such a big difference. The original code uses the stack to reverse the list, which risks a stack overflow, but I wouldn't think it would be much slower.
Sweet, good work. Certainly I haven't given performance that much attention in postgresql-simple yet, as it is faster than the alternatives and fast enough for my purposes. But there should be some low-hanging fruit there for anybody who wants to take a crack at it. The big thing I'd like to accomplish in the near-ish future is to change the blocking low-level bindings from using blocking C calls to non-blocking C calls and the IO manager. But while that might improve performance, I'm doing that for other reasons too.
Seems clear to me that files accessed from TH should be treated as source file, not data files, so getDataDir is the wrong tool. In some cases, this might prevent code reuse between TH and runtime code... but sharing the location of data files isn't safe anyway. You might have runtime code that depends on installed packages while you are changing and building packages in development. I certainly don't expect running 'cabal configure' to potentially break a previously working installed package, which is the result you get here.
I've been playing around with making some games in haskell and the largest downfall I've found is that there are no good sound libraries. I am currently trying to write a higher level wrapper for PortAudio as the SDL-Mixer library has it's own issues: 100% IO blocking, can't query how long a sound has been playing or what chunk it's using (similar), has both sounds and music, some functions only work on music not sounds and there's no type to dictate what will only work on each, no real callback system for when sounds are done playing, etc.
Can't say. Time is much more important than money. Not many people have asked about open sourcing and there are many more important (more frequently requested) features that I need to implement first.
Sure, if OP keeps delivering, then in one month or so a link to his blog would be reddit-newsworthy. Right now it's just vapour...
Hey, you made something. The worst that can happen is someone tells you why it's wrong. This fpcomplete complete thing is starting to become a great platform for tutorials. I hope that we can one day rival the wealth of teaching material the tails community had when I first started writing rails apps.
The prints were difficult to understand, primarily. =] I was building a stack trace by hand, meaning it took several minutes for each iteration just to extract the information from logs. (I could have built tooling around the logs, thinking about it now: if I logged enter/exit for each function I could automatically get a stack trace.) I didn't think to use ThreadScope. I will definitely try that the next time I need to debug a concurrent Haskell program.
The nice thing about Haskell is that we don't have to take it at face value. Most of us are here because we see a lot of unrealized potential in Haskell. I can't say the same for Python.
My solution to the exercise: instance Comonad (Pretext s) where extract (Pretext f) = runIdentity $ f Identity duplicate p@(Pretext f) = Pretext $ \x -&gt; fmap (const p) (f x)
I'd encourage you to check the laws for your duplicate. They should fail.
As another interesting insight into thinking about `Pretext`, consider the two dominant style of lens: 1.) `data-lens` style: `(b -&gt; Store a b)`, which uses a `Store` `Comonad`-coalgebra. 2.) `lens` style: `forall f. Functor f =&gt; (a -&gt; f a) -&gt; b -&gt; f b` If we accept that `Pretext` is just `Store` in `experiment` form, then when you take `Pretext` `Comonad`-coalgebra: `b -&gt; Pretext a b`, rip off a newtype and play with `flip` a connection between the two lens styles should become apparent! 
Compose I forgot to try. I'll look there next.
Exciting! I also want to understand more how the Bazaar generalization works.
With `Bazaar`, some of your reasoning starts to fall about how you have to use the function exactly once to get the `f` for the result. Then we can use `pure` and `&lt;*&gt;` to deal with 0 or more results instead of just one.
(bsf the x86 instruction that is) The code is scattered with hundreds of loops like this: bits = someTest(); while(int i = bsf(bits)) { bits ^= (1 &lt;&lt; i); } 
Perhaps it would be worth creating a free private repo on Gitlab and recruiting some small help? 
I'm not totally convinced either that getDataDir is the correct way, because, as you pointed out, is meant to be used at run time. I'm also having a play with /u/longlivedeath 's solution to see if I can remove completely the Setup.hs. Something I don't understand though, is how this can break the previously installed package; would you mind expanding on that? :)
This can also be used to work around test-framework's incompatibility with `QuickCheck-2.6` and `base-4.7.0` ;)
How about this one? Inspired by your mentioning `Compose` in another comment here: duplicate (Pretext f) = getCompose $ f (Compose . fmap wrap . wrap) -- wrap :: s -&gt; Pretext s s where wrap x = Pretext ($ x) 
That's it!
Spine-lazy doesn't really make sense for Data.Map, because the spine of a balanced tree is changed very often, to the point that you gain no useful laziness out of it. For instance, you can't have Data.Map of infinite size.
Now that everyone has worked through working versions, we can see that we can use experiment to do everything: {-# LANGUAGE DeriveFunctor #-} {-# LANGUAGE Rank2Types #-} {-# LANGUAGE FlexibleInstances #-} {-# LANGUAGE MultiParamTypeClasses #-} import Control.Applicative import Control.Comonad import Control.Comonad.Store import Data.Functor.Compose import Data.Functor.Identity newtype Pretext s a = Pretext { runPretext :: forall f. Functor f =&gt; (s -&gt; f s) -&gt; f a } deriving Functor instance Comonad (Pretext s) where extract = runIdentity . experiment Identity duplicate = getCompose . experiment (Compose . fmap sell . sell) -- extract (Pretext k) = runIdentity $ k Identity -- duplicate (Pretext k) = getCompose $ k (Compose . fmap sell . sell) sell :: a -&gt; Pretext a a sell a = Pretext ($a) instance ComonadStore s (Pretext s) where experiment f (Pretext k) = k f pos = getConst . experiment Const peek s = runIdentity . experiment (Identity . const s) peeks f = runIdentity . experiment (Identity . f) -- pos (Pretext k) = getConst $ k Const -- peek s (Pretext k) = runIdentity $ k (Identity . const s) -- peeks f (Pretext k) = runIdentity $ k (Identity . f) pretextToStore :: Pretext s a -&gt; Store s a pretextToStore = experiment (store id) storeToPretext :: Store s a -&gt; Pretext s a storeToPretext w = Pretext (`experiment` w)
Simpler derivation of `guess`: guess :: Pretext s a -&gt; Store s a guess (Pretext m) = m $ flip Store id
That is the derivation I use, although the `Store` in `comonad-transformers` (and automata post) has the arguments in the other order to avoid the flip.
Good news! I tried it and I was wrong. GHC has our backs here. Hack.hs:6:1: Can't make a derived instance of `Show Secret': The data constructors of `Secret' are not all in scope so you cannot derive an instance for it In the stand-alone deriving instance for `Show Secret' It refuses it even without `-XSafe`.
I love that formulation! Thanks for showing it. I just like to mention to people, who often seem unaware, that there's a beautiful closed-form equation for Fibonacci.
I nearly wrote that one in as well (or, as I found it, `m exact`) but it seemed to fly in the face of my whole "you can pick apart a pretext in two independent steps" argument. I also kept trying to get the `experiment . guess = id` proof to go through and I thought the separation would be useful. It's definitely the nicest statement, though.
Following principles (in this case laziness) when they serve us is good, but as someone said: "a foolish consistency is the hobgoblin of little minds"; if too much laziness is causing us to write incorrect programs, we should use less laziness. This is empirically the case with the old `Data.Map` API.
The only advantage I can think of is if you also want a separate function that is not overloaded.
I don't like it because it introduces a new name for the exact same thing.
Personally, I am not a fan of making `Show` instances that don't `Show` valid code. It seems short sighted to me. If you ever have a `Maybe LispError` or have to print some structure that has them embedded, then the `Show` you get for the composite structure is ruined. By making `showError` instead, you have a principled way to get a `String` out of the `LispError`, and could retroactively decide later to let `Show` do the right thing. =)
You might want to look into [type families](http://www.haskell.org/haskellwiki/GHC/Type_families) (in particular, associated types) to help inference along (loosely speaking). class Setlike t where type Elem t search :: Elem t -&gt; t -&gt; Bool ... instance Ord a =&gt; Setlike (Set a) where type Elem (Set a) = a search = S.member ... Also, note that [this](http://www.haskell.org/haskellwiki/GHC/Type_families#An_associated_type_synonym_example) section of the type families page implements something very much like what you're trying to do.
Requiring Show instances to be valid code is pretty restrictive. They *do* need to be composable, though, which being valid code guarantees.
My experience is that every time someone has written a `Show` instance that isn't valid code, I've wound up with it inside of a data structure and had my debugging abilities crippled by it. Your mileage may vary, but it seems it always happens to me.
There is no predominant style on this issue. I've seen both styles in major libraries. There is a slight performance consideration in that you can sometimes get better performance by first defining a specialized version and defining rewrite rules in terms of the specialized version. I find this works much more reliably than rewrite rules written in terms of type class operations. However, even when you do that you still don't have to export the specialized version and you can keep it hidden and internal.
http://hackage.haskell.org/packages/archive/monoid-subclasses/0.1/doc/html/Data-Monoid-Cancellative.html Contains classes to abstract the "difference" function.
But how is that possible? I'm copying the resources in my pre-build hook parsing the package version from the cabal manifest, and due to the fact each package version has its own folder, everything will work (tested simply bumping the version and configuring again). What am I missing? 
The general rule I follow is that `read . show == id`. As long as your `Show` and `Read` instances satisfy this constraint, `show` can output whatever format you like.
Certainly you don't bump the version number any time you make a change. I'd typically bump the version number right before a release, meaning that if I were making a change to a data file that needed to be in sync with the code change, then the package would constantly be broken during development. (Granted you're using hsenv, so it would only be broken within that specific sandbox.)
Couldn't agree more. We also have pretty printing libraries for a standardized way of doing printing. One future thing that'll become more relevant is defining printers for editors (with exploration and lazy loading and such).
That seemed to be exactly what I needed. Thanks a bunch!
I'm curios if people here have in mind a list of generic patterns, maybe Fst, Snd ? Head, Tail? but in those cases there isn't that much to gain. So anybody?
There are many uses if they get fully implemented. 
Can the keyword "pattern" be dropped without adding ambiguity? 
At first glance, there might be a confusion with top level pattern bindings.
Only in classes.
I've been using pattern synonyms for years, now. ([SHE](https://personal.cis.strath.ac.uk/conor.mcbride/pub/she/patsy.html) has them.) My key motivation has been to build datatypes in kit form, reusing components which have already been equipped with useful structure. For example, your basic sums-of-products functors can be built from this kit. newtype K a x = K a newtype I x = I x newtype (:+:) f g x = Sum (Either (f x) (g x)) newtype (:*:) f g x = Prod (f x, g x) and then you can make recursive datatypes via newtype Fix f = In (f (Fix f)) e.g., type Tree = Fix (K () :+: (I :*: I)) and you can get useful generic operations cheaply because the functors in the kit are all `Traversable`, admit a partial `zip` operation, etc. You can define friendly constructors for use in expressions leaf :: Tree leaf = In (Sum (Left (K ()))) node :: Tree -&gt; Tree -&gt; Tree node l r = In (Sum (Right (Prod (I l, I r)))) but any `Tree`-specific pattern matching code you write will be wide and obscure. Turning these definitions into pattern synonyms means you can have both readable type-specific programs and handy generics without marshalling your data between views. That's why I'll always have and use pattern synonyms, even if they don't become part of the official language. I like exploiting compositionality of datatype construction and the consequent re-use of structure, but I still want to be able to read non-generic code.
Great, thanks.
I alaways recommend [this](http://shuklan.com/haskell/) when someone asks
That's only chapter 1 till 10.
the web (archive) does not forget... http://web.archive.org/web/20130612082552/http://learnyouahaskell.com/
The web page loads just fine now. Thanks for the alternatives though.
I'd be interested in a more detailed write up
Didn't think about them, thanks.
s/roles/rolls/
1. I didn't catch how acidstate works with a big data. As I understand all acidstate db is loaded to memory. Do you take it in account? Does your app load only relative (to query) part of db in memory or you just "open" all acidstate db for every query? In short can you explain us how your app works with acidstate? 2. Have you looked at graph db, orienddb, for example?
[workflow](http://hackage.haskell.org/package/Workflow): A monad for transparent logging and recovery of execution state. Allow to invert back the inversions of control that complicates the programming of long workflows such are typical in the enterprise using BPM and web services. It permits the resume of execution after a soft or hard error or failure. It has been used in Cloud Haskell to migrate the execution of a process from a node to another. [mflow](http://hackage.haskell.org/package/MFlow): A Web framework for programming an entire web navigation as if it would were a console application, while still being REST compliant and the navigation safe at compilation time. The back button and bookmarked pages simply works thanks to the backtracking mechanism used for the synchronization between the browser and the server. It has other unique characteristics not available in other frameworks. 
There are a [few](http://xkcd.com/927/) :)
Sometimes it is useful to have a form of the function that has a more restrictive type, as a form of documentation. If you see a use of showError, you know immediately that it takes a LispError; if you see a use of show, you must examine the surrounding context to determine the type of data it is taking. This is not so much an issue with show, but I find myself needing the extra documentation sometimes when using fmap and pure.
Oh, I skimmed the page and didn't realize this worked with view patterns. I was just thinking this looked boring because it didn't replace view patterns with something I consider more sane, but in fact it enhances them to make them more sane.
Very nice. It is a pity that it has no alternate way to serve monitoring pages. That makes it impossible to use for web applications in Heroku for example, where there is only one port available.
s/ijustwanted/toplay/c
n
This is amazingly cool!
Yes, and it should be in base, and ghci should use that by default, and fall back to Show if an instance does not exist.
What an interesting idea! The bulk of the game lives inside a monad that has a MonadRandom instance, and that's really it. It shouldn't be much work to abstract the Frame consumer/Wire stepper out to have an SDL or JS interface - and I may make this my next focus. Gives me an excuse to install GHCJS ;)
I have read a little about FRP, but not in depth. Could someone please explain in general terms how interaction with the user is handled? It seems to me that interaction with the external world is somewhat at odds witch a static world of predetermined evolution functions (which is my idea of FRP).
I totally agree with the `IORef` solution for holding finalizers. This is the only one which can reliably recover from asynchronous exceptions.
The problem is that is a lot of round trips for an interactive application.
Excellent post. I've been trying to better understand the theory and practice of Haskell's category-theory based classes and this post should help a lot with the comonad portion. Trying to look at Monads, Comonads, and Arrows all at once and see how they are facets if the same general concept is really headache inducing.
Quite so, but this is not actually a problem in practice. The latest development version uses WebSockets for communication and can run a small drum machine over the internet without a hitch (literally). (Well, there are hitches on a UK to DE connection, but that's to be expected and the rhythm is still steady.)
The most impressive thing I've ever seen on the general topic is [ λrex](http://arxiv.org/abs/1102.3730.pdf), which is so to speak the de Brujin version of λx, the I guess now canonical explicit substitution calculus: Fully local beta-reduction^1 *and* no alpha conversion pain. ^1 provided your terms keep track of their free variables, which can be done by mere bitsets.
I agree with your assessment completely, which is why John and I decided to start working on both a library to capture some common patterns, as well as more thorough documentation. I hope that within a few months we'll have written enough that you can use our tutorials as the comprehensive guide you describe. To hopefully alleviate some of your fears, a lot of the scariness of exceptions IMO dissolves once you get a more thorough grasp of how they work. So hopefully a few months from now they won't be intimidating. And for the record: partial functions *are* scary, and I think they should be avoided as much as humanly possible.
I mostly use `bound` to just take away the pain of dealing with names correctly for alpha equivalence and accidental capture. Getting beta too would be neat, but I think I'd have to sacrifice a _lot_ of ease of use. The benefit of the current approach is if you want the free variables is you can just ask an expression using common combinators, and it'll tell you. =) &gt; Foldable.toList (lam "x" (Var "x" `App` Var "y")) ["y"] Monads are _about_ substitution, so my primary goal was to be able to use monads _for_ substitution.
That sounds exciting! I'm really looking forward to that.
I wonder how Kmett's [exceptions](http://hackage.haskell.org/package/exceptions) fits into this picture.
I believe the idea is that user input is also modelled using time-varying values, or streams of events. For example, "asteroids" might use a time-varying value `spacebar_pressed`, and a web application might, rather than listening for individual clicks, instead model the input as a "stream of click events".
I love `wackelkontaktM`.
Wow, this is a great resource!
It's basically an Excel sheet with formulas with cell references. You then plug it into the IO monad which interacts with the value-cells while the sheet is being kept up to date.
&gt; An instance of Bound is required to form a left module over all monads. I have a feeling this observation is more significant than just a coincidence of formalisms. If I have to conjecture it has to do with the interaction of a logical theory (in this case, the trivial theory) and binding forms.
If it is of interest, I'm working on a SpaceWar clone using Elm.
Substitution is indeed subtle, and I've had bugs in my "naive" substitution function. But I think I have one that works. :)
1. The acidstate db is held in memory and indexed using IxSet. According to [acid-state's website](http://acid-state.seize.it/) they discourage reading structures from the disk dynamically. 2. To be honest I am not really into database tech and just made this on a whim in an afternoon.
As I understand from Edward, `exceptions` isn't intended to be a solution to runtime exceptions, though that's not fully reflected in the documentation yet. I was personally confused by that, since providing `mask`ing functionality *does* seem to relate directly to runtime exceptions, so I definitely don't feel qualified to provide an explanation of how to properly use that library. The issue that I found was the following snippet: import Control.Monad.Catch import Control.Monad.IO.Class main :: IO () main = do res &lt;- runCatchT $ action `finally` cleanup print res where action = do liftIO $ putStrLn "This is some action" error "Some runtime exception" return () cleanup = liftIO $ putStrLn "Some cleanup action" If you run that, the output will be: This is some action foo.hs: Some runtime exception Note that the cleanup function is never run, despite the usage of `finally`.
With extensions, the [type system is Turing complete](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.22.2636), so you can calculate it that way. Alternatively, you can use Template Haskell, which is probably more straightfoward.
If I'm not mistaken, though, both of those would require me to rewrite the program specifically to enable compile-time calculation. I was wondering if there was a way to avoid that and just have any and all calculations that could be performed at compile time done.
the compiler is supposed to compile fast enough, if it does these things for you it might take too much time anyways, this is a job for super-compilation, still a research topic http://en.wikipedia.org/wiki/Partial_evaluation http://research.microsoft.com/en-us/um/people/simonpj/papers/supercompilation/design-space.pdf
Interesting. I would like to see a continuous display of the result of evaluating the expression.
something something halting problem
What does the halting problem have to do with it? I don't really know how Haskell compiles things too well, but I'm thinking something like this would be like 90% of the solution... haskellCode generateCode(haskellExpression exp) { if (isKnownAtCompileTime(exp)) { return yieldLiteral(eval(exp)); } else { return generateMachineCode(exp); } }
&gt; isKnownAtCompileTime(exp) Implementing this function amounts to solving the halting problem.
You could just try to evaluate it at compile time.... If it evaluates, it's obviously known at compile time. It really doesn't seem that hard. I mean, I could just do if exp == "2 + 3" then return true; else return false; And then just start generalising out from there.
The OP doesn't want to change the source code. Doing it with TH would (at least the way I can see it being done) involve writing a TH function that generates an optimized fib version or such. Or, you can of course try to use TH to implement a supercompiler pass in your own program rather than waiting for GHC to include one. Good luck...
&gt; it seems simple but isn't :) 
&gt; You could just try to evaluate it at compile time.... If it evaluates, it's obviously known at compile time. Sorry, but it's obvious you don't understand the halting problem. I suggest you read the Wikipedia article I linked earlier.
Repeating saying that it should be simple doesn't make it so. :-) Based on your recipe, why don't you try solving the Halting problem now, once and for all. Shouldn't be complicated, right? Just try some evaluation of the program in question at compile time. Starting with: "if (cond) then halt else halt" obviously halts; now just generalise out from there.
&gt; I'm just going to quickly throw together an equivalent version of this program in C++, but I'd be amazed if modern C++ compilers didn't perform this optimisation, at least for so simple a program as the one in this post. Nor do Clang or GCC, even if you explicitly mark the function as `constexpr`. Just like in Haskell, you can rewrite it using metaprogramming to guarantee it will be evaluated at compile-time. (If you then make it impossible to evaluate at compile time, the compiler will be free to tell you off.)
To be fair, you could have a timeout. If it does not halt, fail after a specified time. I am not saying this is a good idea at all, but it could be done without solving the halting problem.
Have you seen my EDIT? I write "if (collatz-sequence-is-finite-for-this-input-I-just-got-from-the-user)". Now, if the Collatz conjecture is true, then always the first branch of the if-then-else is taken. Otherwise, we can't know (without knowing the user input) which branch will be taken. 
Sure. That would make it into a semi-decision procedure, which is always possible. But the suggestion was that *deciding* the question, not just *semi-deciding* it, should be simple.
Well, that depends on whether the Collatz conjecture is true. :-) Or maybe I misunderstand what you mean.
The short answer is 'not currently, although most simple stuff will be handled properly for you'. For the long answer, well, better that you work it out for yourself. But consider this minor change to `main` in your example: main = print $ fib (-1) There's plenty of free beta reductions to be done here, so let me know what your hypothetical compiler will do.
I you are worried that my "collatz-sequence-is-finite-for-this-input-I-just-got-from-the-user" may not terminate for some input, in case the Collatz conjecture is wrong: replace Collatz conjecture by Goldbach conjecture. Then, for every fixed user input I can in finite time decide whether the property is true or not. Depending on that, I switch between the two branches of the if-then-else. If the Goldbach conjecture is true, then I could know, at compile time, that always exp1 will be evaluated (which may just be a constant). But if the compiler does not know that the Goldbach conjecture is true, it needs to assume that sometimes exp2 will be evaluated (which my just be another user input). So the compiler will be unable to tell that the whole if-then-else always evaluates to a constant, even though that is actually the case.
try writing what you are saying in any *actual* language (of your choice), and you will know I am saying Partial evaluation only does some of the work at compile time, which it would normally at run time. There is no magic. 
I'm fully aware of "Partial evaluation only does some of the work at compile time, which it would normally at run time. There is no magic." I'm not sure why you tell me, or what it has to do with the point we were discussing?
"try writing what you are saying in any actual language (of your choice), and you will know I am saying" I don't think so. :-(
&gt; Is there any way to force Haskell to compile this program so that the resultant binary essentially has only the equivalent of print 354224848179261915075? Yes, via Template Haskell. Place everything in a splice evaluated by the compiler, and it will be guaranteed to be computed at compile time. module B (fib) where fib :: Integer -&gt; Integer fib 1 = 1 fib 2 = 1 fib n = (fib (n - 1)) + (fib (n - 2)) and {-# LANGUAGE TemplateHaskell #-} import B main = print $( let x = fib 10 in [| x |] ) Which results in the core: Main.main3 = GHC.Integer.Type.S# 55 Your compile times will be worse. This is really the official way to do it. Anything else (relying on inlining) is only an approximation.
Here: main = do n &lt;- readLn m &lt;- readLn print (if goldbach n then 42 else m) goldbach n = any (==n) [u+v | u &lt;- primesLessThan n, v &lt;- primesLessThan n] primesLessThan n = ... left as an exercise ... So: If the Goldbach conjecture is correct, this program will always output 42, so it could be optimized to main = print 42. If the Goldbach conjecture is wrong, this optimization is not possible. We were discussing whether the compiler can always know whether to do such optimization. I'm not sure what you think I should realize now that I have implemented my example in my language of choice.
Ah, so you are content then that the compiler does it exactly up to n=100. Good for you. The next programmer may want to have it work up to n=101. The compiler writer doesn't know which is the critical n for every programmer. If they would want to prepare for *every* n, they would risk non-termination at compile time. Beyond that, I'm sorry I have to tell you read the literature. The keywords are "partial evaluation" and "binding time analysis", and as mentioned above, "super-compilation" (which is one particular partial evaluation technique). 
You can perform binding time analysis, and use the result for partial evaluation. Partial evaluation has similar time/space trade offs as inlining. Some programs can greatly be sped up with this technique, but: compilations time rises, binaries can grow in size, the memory foot print of your application grows. What if the structure you're evaluating at compile time is large, or even infinite? Where to stop? Lots of trade offs here, hard to get 'just right' for any term in any Haskell program.
&gt; I actually very strongly doubt that your hunch about "modern C++ compilers will do it" is even remotely correct. You are correct - MSVC 11 compiled to basically the same thing as Haskell did. &gt; The issue is that what you think "seems quite trivial" I don't *think* that this is trivial. It *is* trivial. I look at my fibonacci program above and I can immediately say that `fib 100` is capable of being determined at compile time. It took me like one second. I didn't even have to get out paper and pen, or run tests, or anything. I look at the definition of the program and I can *immediately* say that it is determinable at compile time. Either I'm the worlds' most brilliant human being, or determining that my fibonacci program is determinable at compile time is not actually that hard.
This can be simply done, if you run the compiled code once, compute the value and then persist this value somewhere. That's simply caching, isn't it? Those Cache-Files should be distributable as well.
&gt; Doing it with TH would (at least the way I can see it being done) involve writing a TH function that generates an optimized fib version or such You can lift pretty much any regular Haskell program to be a compile-time program by using staging, though. No need for code generation as such. http://www.reddit.com/r/haskell/comments/1kpu1h/can_haskell_programs_be_compiled_such_that_the/cbrfq4t 
Nobody said it's difficult for a compiler, with some reasonable binding time analysis built in (see the literature), to detect for your specific fibonacci program that fib 100 is static. Why, then, does GHC not employ such a binding time analysis and simply do what you want? sfvisser's comment further above says why: trade-offs, extremely difficult to get right for arbitrary programs.
Yes, yes. You don't need to generate a specialized version of the function, you can just call it at compile time and inject the resulting constant back into the program. But in any case, I think the OP doesn't want that. They want that the compiler automatically sees which are the expressions that should be executed at compile time, and then do it. So that's the opposite of any TH annotations. 
&gt; What's hard is doing it automatically and in a way so that it gives the correct answer ("expression is constant/static" or "not so") for every program. I'm not optimising every program. I'm optimising this fibonacci program, and apparently I'm able to trivially beat the combined efforts of all of the writers of both the GHC and the MSVC compilers. Given effectively the same source code, GHC, MSVC, Clang and GCC all generate a program that naively crunches through all of the fibonacci numbers at run time. However, I (and probably virtually all other human beings) am able to trivially recognise that `fib 100` is in fact determinable at compile time, and thus I would generate a program that simply returns that number. How is it that I am capable of outperforming the combined efforts of all of those compilers? Am I the first person to realise that `f x = x + 2; g = f 3` can be replaced simply with `g = 5` at compile time?
Let me ask the opposite question: Why should any compiler writers spend effort on investing into techniques to improve "stupid" programs that are equivalent to "print a-big-constant"? That's not the class of programs that programmers and compiler writers are interested in. For any non-trivial class of program the approach you suggest is *much* more complicated than you make it look. It's naive to assume that just because the thing is obvious for your fib-program it should work in general. You are saying you are not optimising every program. Okay. You can propose a patch to GHC that deals with exactly your fibonacci program. I guess it's not going to be accepted into GHC HEAD, though. Beyond that, I'm not sure what you want.
A compiler could do this for pure, structurally recursive functions on compile-time literals, such as `fib` and `+`. Such functions always halt in the presence of strict evaluation; however, Haskell’s non-strict evaluation complicates things: `length` can fail to halt, despite being structurally recursive. Do you ascribe a different semantics to the compile-time language than to the runtime language? With Template Haskell, you can do this explicitly. I think that it is saner in the long run for the programmer to instruct the compiler about what it ought to do—the compiler can then make *suggestions* to the programmer about what they might actually want to do.
&gt; If they would want to prepare for every n, they would risk non-termination at compile time. Why would I want to prepare for every n? T `n` required is explicitly stated in the source. Obviously, we only have to work out up to `n = 100` because the source code we're trying to generate a program from only includes the line `print $ fib 100`. This is not a general case. This is a very specific case. The compiler has missed a very trivial optimisation that even a novice programmer can spot and, according to the responses to this post, there is apparently no way to make it *stop* missing that obvious optimisation short of having a human being explicitly tell the compiler that there is an optimisation opportunity here.
&gt; There will always be some inputs which are possible to calculate at compile time, but your program can't determine that. Yet I determined it. I would bet that even novice programmers could have determined that `fib 100` was determinable at compile time. What magic algorithm are us humans using to determine that? Why can the compiler not replicate the process that humans use to determine whether something is determinable at compile time?
The compiler would do whatever the program would do. The compiler is just executing `fib (-1)` a single time, and then generating a program with `print $ &lt;result&gt;`. My guess is that the compiler would crash, or hang, or something. I'm not terribly familiar with GHC.
I mean, sure, I could do it manually. The whole point of using languages of a higher level than assembly is to reduce the amount of stuff that has to be done manually. I don't have to manually go through the source code files and replace every instance of `2 + 3` with `5`. Similarly, I shouldn't have to go through and manually replace all instances of `fib n` with whatever number it actually represents.
What do you count as compile time calculations? Reductions under lambda?
The compiler writers don't know your source code (and that it containes the line "n=100") when they write the compiler. So *they* (as I said) would have to prepare for every n. And they don't know the source code of your fib-function either, when they write the compiler. So they cannot even know that it will terminate for n=101, or any other input. So they have to assume that it may diverge, which makes it risky for them to do such optimizations without big caution. At least if they care that their compiler terminates. About your second paragraph, I can only repeat what I said: you are naively assuming that since you have spotted the optimisation opportunity for one program, it is easy to do so. For what class of programs? For fibonacci programs? Who would care about such an optimisation? For "all similar programs"? Can you define this class of "all similar programs"? If not, then you don't know what you want. And it is not true that the responses to your post say it is impossible, that compilers are simply too stupid. Several people have pointed you to "partial evaluation" and "super-compilation". I suggest you read at least one of the relevant papers from the MSR Cambridge guys, or by Neil Mitchell, or by Peter Jonsson, and then come back if you have any questions beyond "Why are all compilers so stupid?" 
So what should the compiler do if you, eg, leave out the "fib 2 = 1" line?
Would you evaluate loop = loop at compile time or not?
because the compiler doesn't know what we are trying to do also, the example you gave is simple, if you make it more complicated I am sure we won't be able to figure out easily how much should be pre-computed
Expecting the compiler to do this on its own is unreasonable, but I wish there were a pragma to say compute-this-at-compile-time. 
&gt;What magic algorithm are us humans using to determine that? A heuristic one, and one which is *not* complete. A compiler can replicate any number of human heuristics to improve partial compilation, but no amount of heuristics will ever allow it to determine every possible expression. This is a *proven* fact about any Turing Machine equivalent model of computation. It doesn't matter how much you think it should be easy, there is a proof that it's impossible, and it's simple enough that it makes the property obvious. What *may* be possible is solving the problem for all "reasonable" programs for some useful definition of reasonable, and indeed there is a lot of active research into these topics.
Well, dons' "official solution": &gt; $( let x = fib 10 in [| x |] ) is more or less that. Sure, could be packaged into a pragma for easier use, in particular not having to come up with a fresh variable name. 
You just wont accept it, will you? Fib(100) is easy, because we can prove that it will terminate with math, but a mathematical proof is not coding - proving this for the general case of coding is impossible. For every program you produce that you claim will solve it, I can use it to create a program that it fails to produce the correct output. This fundamental fact, known as the halting problem, means we can at best make a sound, but incomplete, algorithm determining this. The compiler guys *could* do this for the fib case, or they could move on the more important things than try to optimize for all special cases you and others come up with.
Depending on the semantics of the language, that is likely to be optimized out due to static analysis though.
That's a rather poor "solution". It requires TH, so it requires fib to be in different module. 
Ah, I didn't even know that one cannot TH-evaluate functions from the current module. Makes some sense, though. :-) 
&gt; A heuristic one Not at all. `fib 1` and `fib 2` are constants. `fib (n &gt; 2)` is totally dependent upon lower values of `fib`. Therefore, `fib` is determinable at compile time for all n &gt; 0. Alternatively, I could test it by simply trying it. Just keep on trying to evaluate `fib 100` until I either get to: * a constant value * something not determinable at compile-time (such as IO, or a complicated function I can't be bothered crunching through, or whatever) * an expression I've already seen, in which case report an error
One thing I found great about cabal 1.16 (currently available) is using multiple cores to build different packages simultaneously: 1. run `cabal install cabal cabal-install`, `cabal install cabal-dev` to update `cabal`, `cabal-install` and `cabal-dev`. 2. add line `jobs: 4` into your `~/.cabal/config` 3. enjoy! (install something via `cabal-dev install foo`). 
&gt; or a complicated function I can't be bothered crunching through So your procedure isn't complete.
I think it's a great improvement in terms of moving cabal-dev functionality into cabal-install. Plus, permanent `add-source` for me is a big improvement (no need to list all local dependencies every time). This API is much cleaner.
Yes that's the goal, but the result will be music so the display will mainly be auditory :) But with visualisation along each path, where it makes sense.
Alternatively, you can inline the definition of fib into the let: $( let fib 1 = 1 ; fib 2 = 1 fib n = fib (n-1) + fib (n-2) x = fib 10 in [| x |] ) ...avoiding the need for a separate module at the cost of not being able to refer to fib from outside the splice.
With Cabal HEAD (IIRC, this is not in 1.16) you can use `jobs: $ncpus` to automatically set the number of parallel jobs to the number of cores. This is now the default.
The new `add-source` behaviour is very handy - changed add-source dependencies are reinstalled automatically. I (the author) use this all the time when working on Cabal.
You kind of ask for some calculation preprocessor. A Preprocessor is known to be used in C or C++. Look here @ 4.11.4 http://www.haskell.org/ghc/docs/7.0-latest/html/users_guide/options-phases.html The -F option allows you to use a custom preprocessor.
&gt; The recommended method of bootstrapping the Git version of the cabal tool is by using cabal-dev. Assuming you already have a previous version of cabal installed: Is this correct? The code snippet below doesn't use cabal-dev, it uses cabal itself. That said I really can't wait for the new Cabal to come out. With this and `cabal repl` the new version is going to be one monster upgrade for my productivity.
I guess I am still to "novice" to really use this but thanks - will try to understand this.
I have only used Netwire. Elm or Elerea is something I need to learn, and maybe my next FRP project will be with one of them and I can write about their differences.
Thanks, fixed.
Is there any way to configure it to use `--haddock-hyperlink-source` by default yet? That would make my day.
Let's say that you're working on a program (for me, it's `cabal-install`) that depends on a WIP version of a library (Cabal, in this example). You create a sandbox for this program and `add-source` the library dependency: $ cd /path/to/cabal-install $ cabal sandbox init $ cabal sandbox add-source ../Cabal $ cabal install --only-dependencies Installing Cabal-1.17.0... You build your program with `cabal build`, as usual: $ cabal build Building... Then you make some changes to Cabal and again run `cabal build` in the `cabal-install` directory. The `cabal` tool notices that the add-source dependency has changed and reinstalls it automatically: $ $EDITOR ../Cabal/some/File.hs $ cabal build Some add-source dependencies have changed. They will be reinstalled. Reinstalling Cabal-1.17.0... Building... With `cabal-dev` you'd have to reinstall the modified dependency manually, compiling it from scratch each time.
Could Cabal detect when I'm missing profiling versions of my dependencies and install them for me?
What about extra resources (`data-files`) and executables? Are they properly sandboxed as well? I've had a lot of trouble with `cabal-dev` because this kind of sand was always leaking out of the box. For instance, the binaries from a sandboxed wxHaskell installation were being written to the main `.cabal/bin` dir.
Any time `hailstone` is called with a value determinable at compile time. If it doesn't terminate at compile time, it certainly won't terminate at run time. Obviously, it's better to pick up the bug at compile time than leaving it in for users to trip over.
I'll hijack this topic for a related question: My Cabal situation is terrible and I want to start from scratch with some kind of sandboxing. How do I remove cabal completely including installed packages? (Ubuntu 12.04, cabal-install version 0.14.0 from the package manager.)
That is largely why I left C++. I was sick of having to use all kinds of compiler trickery just to get things to evaluate at compile-time rather than run-time. I was under the impression that the big benefit of pure functions was that aggressive optimisations were easily possible, as the whole thorny issue of stateful computations is essentially avoided. However, I put together this `fib` example where it would seem to me completely obvious that the compiler ought to detect and optimise the heck out of the code, yet instead it just naively churns through it all at run-time. Such a let-down!
This looks like exactly what I'm looking for! Will this still work if there are things like IO computations etc. in module B?
&gt; Any time hailstone is called with a value determinable at compile time. You can't determine that short of executing everything. You _also_ can't tell the difference between a non-terminating calculation, and one that will terminate in 2 seconds. How will the programmer have to wait to get an executable? You can't tell that. So forget about doing anything intensive compute wise with that compiler - you've just prevented the programmer having any control over when things are run. Oh, and just because there is an expression written in code does _not_ mean that it will be evaluated at runtime - even in a strict language. So your argument doesn't necessarily hold.
Yet there isn't a way to just turn off the trade-off for a final release build, where execution speed is paramount and compilation time is irrelevant?
I'm not sure what "reductions under lambda" mean. I simply mean that any and all calculations that can be performed at compile-time and then somehow cached or stored in order to reduce the amount of calculations that must be done at run-time.
I really want an IOSafe -- equivalent to IO in every way, except no exceptions.
&gt; No, I don't think it's possible to determine whether every single possible function is determinable. Well, for much of the discussion here you have been giving the impression that you indeed think that it is possible to determine that for every function. (And that you know how to do it.) About: &gt; That doesn't mean that we should never check if any are determinable. No, it doesn't mean that. And nobody takes it to mean that. People have been telling you that techniques which just approximate "staticness", and use such approximate information for program opimization exist, and how they are called, and how to find more information about them.
Who said that execution speed vs. compilation time is the only trade-off to take care of? Nobody, as far as I can see. The post by sfvisser I pointed you to mentioned several other trade-offs. So your question singling out execution spees vs. compilation time seems irrelevant.
&gt; Well, for much of the discussion here you have been giving the impression that you indeed think that it is possible to determine that for every function. (And that you know how to do it.) Ah! That would explain some of the responses I've been getting. The Template Haskell method seems to be something very close to what I want. I'll have to look into that more, but it seems like the "okay, final release build time, let's do the most uber-fast, super-optimised version possible for the users" thing I was hoping to find.
typos: arrows :: [Type] -&gt; Type -&gt; Type arrows = foldr Arrow should be arrows :: Type -&gt; [Type] -&gt; Type arrows = foldr Arrow and pattern LazySecond a b ~ (a, ~b) should be pattern LazySecond a b = (a, ~b)
It's really an instance of the usual "I can do anything by calling a library in one line" case. The Erlang solution uses a lot of existing infrastructure. The Haskell solution has a lot of things like this: joinGroup :: Group -&gt; IO (Gate,Gate) joinGroup (MkGroup n tv) = atomically (do { (n_left, g1, g2) &lt;- readTVar tv ; check (n_left &gt; 0) ; writeTVar tv (n_left-1, g1, g2) ; return (g1,g2) }) and there's several other "infrastructure" functions like that. It's a legitimate observation that Erlang ships with some useful primitives that Haskell did not have at the time (the paper with the Santa problem predates Cloud Haskell by a lot, and I'm not sure of the exact current status of Cloud Haskell), but I'm not convinced this says anything useful about STM. It's easy to imagine all that just being in a library somewhere. With a reasonable loading of Haskell code into libraries (basically move the Data, Group, and utility chunks into libraries, leaving only the top chunk), I'd say the two solutions come out essentially identical in length.
What you are saying is that I don't really have to remove the old packages lying around, as long as I from this point on only use sandboxed environments?
&gt; If a calculation reaches a point where it is executing the same function on the same arguments, that's obviously an infinite loop. How much memory will it take to keep track of all that? &gt; After a couple of million recursions, perhaps just deem that function as not terminating and stop optimising it I have code where one iteration takes 2 hours. (Granted, that particular code now read from input, but it initially didn't, having the precise problem spec coded in). Is 4 million hours a reasonable time to wait for compilation? Your arguing that something is sometimes easy, therefore it should always be done. That doesn't make sense - it's not the easy cases that are the limiting factor, it's the _hard_ ones, and you're wanting to handwave past those. It's especially difficult in this case, because you can't determine the difficulty before attempting anything, so you can't even pre-sort into groups where it's only worth attempting certain optimisations in some cases (as happens with some other optimisations). 
Maybe, as long as they do not interfere with the installation of the sandboxing tools? But I do systematic wipes at each haskell platform release, so that's not what I'm directly suggesting. :)
After reading over the rest of your comments, you appear to be unwilling to accept anyone's answers to your question, despite them being firmly grounded in decades of very smart people trying to wring every last bit of performance out of compilers. You are either rapidly approaching or at the point where the only answer to be given is "If it's so easy, go try it then." Though, bear in mind, your solution needs to not just work on the Fib sequence... it needs to work on _all_ code. When you modify the compiler with your optimization, all the rest of all Haskell code in existence needs to continue working, and preferably with no slowdowns resulting from your "optimization". Nobody cares if you produce an optimization if it also breaks code, or slows down other more common cases. This is part of your problem, by the way. You're hyper-focused on the compiler solving the Fib problem, but I don't think you're quite getting that whatever "solution" you use has to work on _all_ code, not just your trivial little program. Neither compilers nor compiler authors get to pick the programs they will compile. Reading over the entire thread to date, this has been alluded to, but I'm not sure it has quite been made as clear as it should be for you. Your input to your putative optimization procedure is not your program... it's _all_ Haskell programs.
This is a Haskell re-write of an assembler/disassembler that I initially wrote in F# and C#. In comparison to the F#/C# version, I think the Haskell version is very clean and elegant. This is probably a combination of: * The hindsight that comes from re-writing a program from scratch and learning from all the mistakes in the original. * The fact that the F#/C# version was my first attempt at not only writing an assembler, but also areas of programming which I was previously unfamiliar such as parsing, building and processing ASTs, etc. * The benefits of Haskell, itself. As a general note on Haskell, I really love Monads and also the separation between pure functions and IO. I find that the added strictness forces me to reason more fully and thoughtfully about what *level of responsibility* a given function should have, and that this results in a more elegant design fairly organically. In any case, I would be very appreciative of feedback on the source code, documentation, usage, or anything else. This is particularly true since I'm still learning Haskell, so improvements specific to Haskell idioms or conventions would be especially great.
Supercompilation was an active research topic for Haskell for a while.
I guess what part of my question is: how do you safely remove all the installed packages?
Why choose :: [(STM a, a -&gt; IO ())] -&gt; IO () choose choices = do { to_do &lt;- atomically (foldr1 orElse stm_actions) ; to_do } where stm_actions :: [STM (IO ())] stm_actions = [ do { val &lt;- guard; return (rhs val) } | (guard, rhs) &lt;- choices ] instead of choose :: [(STM a, a -&gt; IO ())] -&gt; IO () choose = join . atomically . foldr1 orElse . map (\(x, y) -&gt; fmap y x) ? -edit- omg λ: main Loading package array-0.4.0.1 ... linking ... done. Loading package deepseq-1.3.0.1 ... linking ... done. Loading package old-locale-1.0.0.5 ... linking ... done. Loading package time-1.4.0.1 ... linking ... done. Loading package stm-2.4.2 ... linking ... done. Loading package random-1.0.1.1 ... linking ... done. ---------- Ho! Ho! Ho! let's deliver toys Reindeer 1 delivering toys Reindeer R2RRRRRRe eeeeeeidiiiiiinennnnnndlddddddeieeeeeeeveeeeeererrrrrr r 3i456789 n dgdddddde eeeeeeltllllllioiiiiiivyvvvvvveseeeeeer rrrrrriiiiiiinnnnnnnggggggg tttttttoooooooyyyyyyysssssss ---------- Ho! Ho! Ho! let's meet in my study **Let's not.**
The entire group infrastructure seems to be moved to ADTs in [this 34 SLOC version](http://www.haskell.org/haskellwiki/Santa). I think O'Keefe was trying to use a screwdriver as a hammer.
There are pure functions with arguments known at compile time that do not terminate. `sum [1..]` comes to mind.
For instance, would you expect `f x = replicate 5 x` to be transformed to `f x = [x,x,x,x,x]` at compile time? This is a reduction under lambda. In general reduction under lambda requires you to compute with open terms, whereas runtime evaluation is always with closed terms (in some sense).
&gt; isPureFunctionWithArgumentsKnownAtCompileTime This isn't constrained enough to rule out things that will break. What you're getting at, though, is a deterministic subset of the language - which *necessarily* wouldn't include unconstrained recursion, and so dramatically limits its usefulness. &gt; Better for it to take the time at compile-time, then! That's missing the point - the key point is it **might never finish** - if you have a bug, the compiler won't be able to tell you, it'll just run forever. The negative effects harm even people not using the feature - *especially* people not using it. Even assuming the program has no bugs, that's based on a whole lot of assumptions, such as that the computer it's compiled on is more powerful than any machine (or machines) the software will run on. If you're building folding@home, you may know all the proteins you want to fold, but you certainly don't want to fold them all on your laptop every time you try to compile. Also, that penalty will be felt by everyone who compiles it, and, unless it's cached, it'll be felt every time they recompile. If it is cached, they still feel it any time they make a change or the cache expires. All for a value that, in an average-case run of the application, might not even get used (it's not unusual to hard-code edge cases). All the while, for cases where this would really be most useful, the value could trivially be pre-generated either by a separate program, or with some templating. Lots of people have commented on this thread, some of them notable individuals in the community who've contributed to the compiler themselves, all of whom are telling you that they know it to be not so simple. Surely you've at least considered the possibility that it's in fact not that simple?
If you want this to be a good comparison, I'd be really interested to how a third rewrite in F#/C# would compare. That way the (huge) advantage of hindsight and experience aren't working in Haskell's favor, but indeed against it.
1. rm -r ~/.ghc 2. read [SICP](http://www.vex.net/~trebla/haskell/sicp.xhtml)
Why not lean on School of Haskell's infrastructure? That way everyone will benefit from it.
It's a pity that he doesn't know too much about OO programming, or at least carries the misconceptions that many people carry. If you really dive into the work of Kay et al., you'll see that OO is roughly (very roughly ;-)) an abstraction on top of functional. Smalltalk uses blocks (anonymous functions, often implemented as closures) to do even the simplest stuff like if/then statements. It's a bit of a "meh" article, except for the part that, indeed, kids need to learn how a computer works and how to program it (rather than how to switch it on and start IE/Word/...).
I'm a mostly-happy user of plain-old-`cabal`-without-sandboxes (386 user packages installed at last count), but this `add-source` is very appealing when I'm trying to track down a bug in a dependency of my project. (Or add a feature to a dependency, etc.)
I had this project in mind for a while now, but never managed finish it. This is mostly because my code was never good enough for me to be happy with it .. so much IO operations because of mutable vector operations(to model fast memory write/read operations), I/O operations which made my code look like an OCaml code instead of Haskell. .. in the end I wasn't satisfied with the program and decided to leave it. I learned one thing though .. DCPU-16 is very simple, yet an implementation of it covers some realistic programming problems, and most importantly it's fun. So I recommend implementing it to someone wanting to learn a language(we have lots of DCPU-16 programs published on various websites, most of them have online interpreter so you can compare results etc.). Thanks for sharing this. 
Nice to hear from you. I may be misunderstanding your comment, but Saturn is actually an assembler/disassembler for the DCPU-16, not an emulator. I *have* in fact written a complete emulator for the DCPU-16 including peripherals (I can post the github link if there's interest), but it's written in C++11 and makes use of Boost and SDL. Using the assembler/disassembler with the emulator together is a lot of fun.
This is a very good (and fair) point. In the original F# version (it's almost entirely F#, I just used C# to get NDesk.Options for the CLI), I was tempted to use mutable state and objects heavily. The Haskell version (which is more feature-full, by the way) instead tends to localize mutability more (though the state monad) and is also more modular. I'm sure I could replicate some of the lessons learned from Saturn to the F# version, but certain things would be hard to translate (the Maybe monad, the Either monad, etc). Another difference between the two is that the evaluation semantics for labels and constant definitions are very weird. There was no pre-processor, so things got complicated when I tried to add constants, conditional blocks, etc. It works, but it's not pretty. I learned my lesson, and kept things simple with Saturn.
[Reformatted a little](http://lpaste.net/92006).
I actually wrote the prototype emulator (just the processor, not the peripherals) in Haskell before the C++ version, for what it's worth. :)
It's true that having code which performs a long, intensive calculation means that you'll have long compile times where you're wondering if the compilation will ever terminate. However, you're really just punting the problem. You might have a fast compile time, but you then have the exact same concerns when you run the program. The only thing moving the calculation out of the compiler has given me is that I'm wondering whether "example.exe" is an infinite loop as opposed to wondering whether "ghc exeample.hs" has an infinite loop. As a side note, adding a compiler pass that performs no optimization and prints whether the code compiles eliminates the question of whether it's a syntax error or if the code is just trapped in optimization. As for your comment about the Ackermann function, that's the single best argument *for* this kind of optimization. Let's imagine that performing the optimization that calculates the "ack 4 4" at compile time takes adds a week to the compile time. That's certainly a pain. However, now imagine that your code needs the value of "ack 4 4" and needs to be run a hundred billion times before you get your paycheck. Unless you can perform the calculation in a six microseconds, you're better off caching the calculation once at compile time than forcing the value to be recalculated every time the code is run. The more complicated the math is, the more important that it's performed at compile time so that you don't waste time recalculating the same value every time you run the program.
I seriously don't get this attitude. Just because an optimisation doesn't work in *every* case doesn't mean that it should *never* be don. If it can be detected, why *not* perform the optimisation? Why does it matter so much that the optimisation won't work in *other* cases? If it really is so horribly, horribly slow to simply detect whether such an optimisation can be applied, fine, make it a compiler flag that can just be set for final release builds. In any case, it seems that there *is* a proper way to accomplish this - Template Haskell apparently does precisely what I asked. Apparently, I was correct to continue questioning the answers I was getting.
Yet I'm not asking for the general case. It is *sometimes* possible to determine what can be evaluated at compile time. For example, my `fib` program can be evaluated at compile time for n &gt; 0. I'm simply asking whether there is a way to force whatever can be detected to be computable at compile time to then actually be computed at compile time. Apparently there is such a way, with Template Haskell.
I wouldn't say error-prone. `constexpr` functions only allow a specific (functional-like) syntax. `constexpr` functions can be called both at compile-time and run-time. If you use a non-constexpr function in a situation where the answer must be known at compile-time (constexpr), you will get a compiler error.
Apparently, you haven't understood the answers you were getting concerning Template Haskell. TH will *not* go and detect expressions that can be computed at compile time. It will *only* compute things at compile time that you tell it to compute at compile time. So *you* have to decide that "fib 100" is the thing that should be computed at compile time. TH is not "a flag I set and then the compiler detects that fib 100 is static, so it computes it at compile time". 
There are big differences between simple cases like "2 + 3 computed to 5 at compile time" or "i++ vs. --i" on the one hand, and "fib 100" on the other hand. The former two cases do not involve recursion. That's why simple inlining, constant folding, strength reduction, whatever, is enough. Most compilers will do some form of those. The case with "fib 100" is different, because a user-written, recursive function is involved. I'm afraid that by conflating these two cases, you hinder yourself understanding the issues.
No. You haven't understood what Template Haskell does for you. It doesn't do what you ascribe to it here in your post.
`cabal-dev` lets you do something similar by sharing the sandbox between the two projects. For instance, if project `B` depends on project `A`, I can do $ cabal-dev install A/ B/ and `B` will be able to reference `A` in its dependencies. `A` does not get recompiled unless it needs to be. Can I do this with the new `cabal` sandboxing (if I create the sandbox in the parent directory) or do I have to use the `add-source` method?
But why would anyone ever use the function `isKnownAtCompileTime` in their source code? Surely it's an internal compiler's function; it's not available to me as I write my source code. So Halting Problem does not apply. Right?
&gt; Your compile times will be worse I should hope so. The whole point is to move work from run time to compile time. The flipside is that your run time will be better.
If the function exists in the compiler, you can write an equivalent one in Haskell and pass it into itself in whatever internal representation it takes, to form a similar construction to the one above. I'm not saying this is a sensible thing to do, but the point is to show that there exist programs for which the property cannot be decided. However, as it seems this discussion has been at cross purposes, I'll emphasise that no one is saying we can't have a very useful *partial* solution to this problem. Merely, that a general solution is definitely impossible.
For a number of reasons. 1) I had the essential parts of the site done before School of Haskell first opened, so moving there would have had relatively small benefit for me. For everyone else, it should be the same where the content is hosted. It is open anyways. 2) SoH is inflexible. It runs someone elses code on someone elses server. When I had the idea about adding multiple choice questions to the material,it took me a day and they were online. I have plenty of other ideas, so I need something that I can change. 3) SoH is for tutorials, not courses. My project is not a single tutorial, but part of a larger whole, consisting of exercises, lecture videos, etc, which are not really hostable on SoH. As a further example, I don't think there is any way to track which students did which exercises on SoH, nor do I think FP-complete would have any interest to add such features. 4) SoH content license signs over too many rights for my liking. For single tutorials this doesn't really matter, but for something that takes weeks to write, they become an issue for me. 
Yes, though, necessarily they'll just happen at compile time instead.
False negatives are fine. The optimization can give up after a timeout.
Failing to halt is not that big a deal. Just use a timeout.
Everyone mentions the compiler hanging problem. Just use a timeout.
A timeout (which gives up on the optimization) solves that consideration.
The idea behind a supercompiler is that it would sort of just start running your program at compile time, and then insert "residual" code whenever it can't get any further (e.g. IO). There was research on this going on for a while, and sometimes it led to really fast programs, e.g this example where it beat C by uncovering a way to encode state in the program counter instead of having an explicit register for it http://neilmitchell.blogspot.com/2007/07/making-haskell-faster-than-c.html For a while, there was talk of similar things making it into GHC, but then I never heard of it again. Does anyone know what happened to that?
Right. The most insightful comment on this post so far.
&gt; You might have a fast compile time, but you then have the exact same concerns when you run the program. I perhaps made the argument better in my other comment (or maybe not), but the point I was trying to get across was that you generally don't want this behavior (checking every function to see if it terminates) to happen at compile time. It's generally okay to wait for a program to run to completion (or potentially run forever) for some inputs when actually getting answers. If the program takes a long time to run, we generally don't want to wait for that when we're compiling. &gt; As for your comment about the Ackermann function I just threw that in there as an example of something where it's not as immediately apparent as Fibonacci that it terminates, since a lot of the arguments were based on perceived simplicity and obviousness. Arguably, maybe a non-terminating one that wasn't obviously so for all inputs would have been a better example, but I figured any heavily recursive function would get the point across. &gt; imagine that your code needs the value of "ack 4 4" and needs to be run a hundred billion times before you get your paycheck. I'm sure you're just playing devil's advocate here, but we're not talking about running this optimization once for a specific piece of code (as with templating), we're talking about running it on every piece of code - we don't have enough information to make those assumptions. Any given piece of code might be an edge case that rarely gets run, or programmer/compiler time might be dramatically more expensive than run-time user/machine time - if we want to be really thorough about it, even for the individual case, we'd need to talk about amortized cost in terms of those variables. But certainly, if all the above assumptions apply for some specific piece of code, template away.
Everybody mentions failing to halt in compilation as a huge problem. Of course a simple timeout fixes that, but beyond that: ghc already has WONTFIX bugs that cause exactly this, and it's not considered a big deal. Namely, encoding the y combinator via infinite newtypes causes the inliner to infinitely replace the y combinator with itself.
How long should the timeout be? The more something would benefit, the longer the timeout would need to be to detect it. How much time gets lost to running until the timeout (resulting in no optimization) on code that's either too complex or non-terminating? At what point is it no longer an improvement over templating?
I'm well aware of this possibility. :-)
I actually agree with the OP that a simple attempt to constant fold, even through recursive code, with a timeout is an optimization that could be useful. It may not justify its development costs or the extra complexity in the compiler's UI (enable/disable an extra optimization). I was quite disheartened that people threw the halting problem at OP, because the optimization can be avoided when it doesn't halt. And because there are already cases where GHC doesn't halt (for valid Haskell98 programs). 
The problem is they are more than slightly neutered. You can't abstract over the workflow, because there is no notion of higher kinded types. This means you have to defer type checking to the site where it gets inlined... but that means you can't write functions that are recursive that are parametric in the choice of Monad. To put this in perspective, it makes `Traversable` an unthinkable abstraction in F#.
The halting problem came up because the OP was suggesting that they could always *decide* where constant folding is possible. And the OP was quite insisting on it, so the halting problem came up again and again. About pragmatic attempts, giving up completeness but doing some actual good for runtime performance: people have pointed out partial evaluation etc. But OP insists to solve the whole problem, once and for all, because it's so simple...
I think it's a matter of mis-communication. I think OP meant the "whole problem" of his example, and not all possible programs.
 $ rm -rf ~/.cabal ~/.ghc $ git clone git://github.com/haskell/cabal.git /path/to/cabal $ cd /path/to/cabal/Cabal $ runhaskell Setup.hs configure $ runhaskell Setup.hs build $ runhaskell Setup.hs install $ cd ../cabal-install $ sh bootstrap.sh
This solution has a number of disadvantages compared to `add-source` (`A` is always unnecessarily reinstalled, you have to manually list all dependencies, you're using `install` instead of `build`). But sure, you can use shared sandboxes, if that's what you want: $ cabal sandbox init $ cd A $ cabal sandbox init --sandbox ../.cabal-sandbox $ cd B $ cabal sandbox init --sandbox ../.cabal-sandbox $ cabal install ../A/ . If I were you, though, I'd just do `add-source A` since that's exactly the case this feature was designed for. Let the computer do the hard work of dependency management for you!
Yes, they are. I'll look at the wxHaskell issue again, but IIRC it was caused by them hardcoding `~/.cabal/bin` in the `Setup.hs` script.
Uh, so, why is this in /r/haskell when it doesn't even support Haskell? Edit: Looking at their github, they already have some Haskell exercises in the making!
Yep, it already supports haskell, as the title of this post indicates as well! I feel as though this is an area that has been lacking in terms of learning resources. The learning curve gets steep at a point. There's understanding basics, understanding how a language works without having practice writing a lot of code, understanding the tools to maintain code, understanding libraries and frameworks, and all sorts of stuff *on top* of all the practice it takes to become a good hacker/coder. 
I've shipped `exceptions` 0.3, making this change.
Why not post the actual Exercism link?
Heh, I guess I missed that part of the title...
I'm a haskell noob... but does this work with memoisation?
Template Haskell is just executing the ``fib`` and splicing the result in at compile-time. There isn't any clever detection or static analysis going, it's just shifted the computation from run-time to compile-time. 
I see you have fixed 'subst' in the 'naive substitution' code. I strongly suspected the bug was left in as an exercise; I was going to report it yesterday, but reddit was down ;-)
Moreover, this should paralellize well, so you could (in principle) throw huge amounts of computation at it when you will be more constrained at runtime.
I honestly can't tell whether that was meant as praise or snark.
What the OP asked for isn't reasonable. What the OP wants, I would surmise, is as close as is practicable an approximation of that, which is certainly reasonable for certain values of "practicable".
&gt; 2-3 tries Aha, a new data structure I haven't come across! Can you write about 2-3 tries next?
Hah!
They won't happen if they have nothing to do with the function from module B that is called in module A (i.e., if they just appear in other functions in module B, unrelated to fib in the example). [Obviously. But I'm not sure if obvious is obvious enough here.]
Maybe it is.
Here you go: http://ghc.haskell.org/trac/ghc/wiki/Commentary
Speaking of combinators that can't go wrong: Jean-Philippe Bernardy just emailed me a link to his [Names For Free](http://www.cse.chalmers.se/~bernardy/NamesForFree.pdf). In there they also use the polymorphic recursion trick, although they use the `f (Maybe a)` version from the first half of Bird and Paterson, not the `f (Maybe (f a))` version that drives `bound`. One combinator that they have that `bound` doesn't offer would morally translate to: unpack :: Scope () f a -&gt; (forall b'. b' -&gt; f (Var b' a) -&gt; r) -&gt; r unpack e k = k () (fromScope k) which gives you a way to talk about the `bound` variable, but protects it from manipulation through the existential context. They have a few other combinators that may make sense to appropriate as well, but they, sadly, need to lean on `IncoherentInstances`, which generally makes me queasy.
see conor's kleisli arrows of outrageous fortune, which uses indexed monads to track effects precisely: https://personal.cis.strath.ac.uk/conor.mcbride/Kleisli.pdf
I don't have an answer, but it seems to me that with dependent types you could actually have monads guaranteed to follow the monad laws. That said, I doubt anything too amazing comes up, since dependent languages don't actually introduce a new model of computation ie other than the type system, dependent languages are pretty standard functional languages. 
no, I can't (write such function). The paradox can only be constructed *in principle* in languages which run their own source code. Haskell can't, and doesn't. The real problem is combinatorial explosion in forward chaining because/when we don't have good heuristics. I.e. we need to identify the *advantageous* base cases, and advantageous derivation steps, and generally we can't know which they are. So we expand the knowledge boundary in all directions, i.e. we drown in useless irrelevant knowledge. That is the true problem (I forgot how it's called, formally). In case of `fib`, base cases and derivation step is all we know, so forward chaining goes only in one direction, and solves `fib` super-compilation trivially. It can still timeout of course (theoretically). In which case it might switch to trying to generate/deduce some recurrence relations for `fib` and become truly bogged down in it. But halting problem has got nothing to do with it. 
&gt; 1.) MonadCatchIO-transformers ceased to be implementable with GHC 7.7, and we were making heavy use of it in Control.Exception.Lens to allow exceptions to be caught and handled over transformer stacks. Interesting... why not?
This idea reminds me of scala's [extractors](http://www.scala-lang.org/old/node/112).
For the whole compilation? For each expression?
Reduction under lambda means exactly that: given some term `(\x -&gt; e)` we perform reductions in `e`. Notably, Haskell *does not* perform reductions under lambda[1]. In principle there's nothing stopping us from doing this; but in practice it would make things very slow because we'd have to perform evaluation of open terms[2], which basically requires manipulating and interpreting ASTs at runtime. Of course, it's worth pointing out that according to theory it would be better if we actually did perform evaluations under lambda. For example, by not doing so we lose the Church--Rosser property, therefore order of evaluation does alter the results[3]. But then, outside of the theorem-proving community, almost no languages perform evaluation of open terms, so this performance hack is ubiquitous even if theoretically inelegant. [1] E.g., if we define `f = (\x -&gt; (x, 1+1))` there's no way to reduce `f` to `(\x -&gt; (x,2))`. Therefore, that `1+1` will be evaluated separately every time `f` is called--- assuming the optimizer doesn't float it out (because it doesn't actually rely on `x`) or evaluate it at compile time (via constant propagation). [2] I.e., terms which have free variables in them (namely variables bound by they enclosing lambda). [3] Of course, we only lack CR in the syntactic sense. The extensional behavior of terms is still observably CR.
Here's one analogy to lift some confusion: Think of synchronous exceptions like "under construction" signs on the road. You don't expect to run into them often, but once you know where they are, you can alter your trip accordingly. Or return home. Yet until the problem is resolved, every time you go down that road, you know the path will be blocked. Asynchronous exceptions are like blowing a tire. You never know when it's going to happen, and it's always a surprise. Even still, you'd like to have some kind of plan (example: slow down, pull over) for when it inevitably happens. In this picture, "masking" an asynchronous exception is like installing a positive pressure system that keeps the tire sufficiently pressurized until the car stops moving, giving you a chance to find a safe place to stop. A partial function, on the other hand, is a break pedal that never works when you're in fifth gear. It always fails under the same conditions, is never going to be resolved, and you'll find yourself constantly wishing your car was designed better.
You can use some higher kinds with fsharp-typeclasses (including all of the standard ones in prelude, applicative, traversable etc) and you even get type safety but comes with the downsides of 1) really weird types that don't mean anything to the user 2) functions can take tens of seconds to typecheck. Edit: Clarity
Nicely done, cool!
The issue is you can't tell *when* it is appropriate to apply the optimisation - as others have said finding out if arbitrary code can be safely calculated at compile time is the same as the halting problem, which cannot be solved. 
You don't *have to* use role annotations. In fact, most of the time GHC will deduce those roles for you.
Given that you seem to know plenty about theory, I feel I must be misunderstanding your point. I'm assuming you're not saying Haskell isn't subject to the halting problem? Obviously the practical goal of super-compilation is a useful partial solution, but I feel that the fact that a total solution is impossible is definitely relevant.
Here’s a quick and dirty code review. You prefix many functions in `Parse` with `_`, which suppresses “defined but not used” warnings under `-Wall`. Is that intentional? It can hide dead code. In your `friendly` instances, you can reduce repetition of the function name by using `case` instead of function patterns. `opCode` and `register` from `Decode` are similar, and you can also factor out the `Just` by using a guard for the `Nothing` case. `Label.lastGlobal` being strict on a `String` isn’t very useful, as it’s only strict in the outermost cons cell. Same goes for `active` and `initial` from `Section`. Instead of `String` you might prefer `Text`, which is a packed UTF-16 text type. More generally, it would be safe to make more of your data structures strict. 
Following up /u/sclv, another example of monad that one could call indexed is a monad indexed by a monoid. If `(A : Set, empty : A, _&lt;&gt;_ : A → A → A)` is a monoid, a monad indexed by `A`, `M : A → Set → Set`, is: -- Functor map : ∀ {a b i} → (a → b) → (M i a → M i b) -- Applicative pure : ∀ {a} → a → M empty a apply : ∀ {a b i j} → M i (a → b) → M j a → M (i &lt;&gt; j) b -- Monad bind : ∀ {a b i j} → (a → M i b) → M j a → M (i &lt;&gt; j) join : ∀ {a i j} → M j (M i a) → M (i &lt;&gt; j) a _&gt;=&gt;_ : ∀ {a b i j} → (b → M j c) → (a → M i b) → (a → M (j &lt;&gt; i) c) satisfying the usual laws. Notice how the monoid laws kind of force `&gt;=&gt;` to be associative and `pure` to be its identity (it's not enough, though). Every normal monad is a monad indexed by the monoid `(Unit, (), λx y → ())` A concrete example of this is `Vect : Nat → Set → Set`, the type of lists of fixed length, where the monoid is `Nat` with multiplication and 1. Usually the length index gets in the way of implementing `List`'s monad with `Vect` because the length can change, but with monoid-indexed monads you can: -- map is the same as lists pure : ∀ {a} → a → Vect 1 a -- must return a singleton pure a = [a] join : ∀ {a i j} → Vect j (Vect i a) → Vect (i * j) a join [] = [] -- i * 0 = 0 join (xs::xss) = xs ++ join xss -- i * (j + 1) = i + (i * j) But all of this and /u/sclv's can be done "comfortably" to some degree in Haskell with DataKinds and GADTs, also now we have two different kinds of monads, neither of which include the other. Can we do better? Can we unify the two indexed monads into one with an unHaskellable concept? We might try to generalize from monoid-indexed monads, what's the structure with associativity and identity that's more general than a monoid? A category, of course. So, say `C` is a category, that is, it's a record with: Ob : Set Hom : Ob → Ob → Set id : ∀ a → Hom a a _∘_ : ∀ {a b c} → Hom b c → Hom a b → Hom a c) where `_∘_` is associative and `id` is `_∘_`'s identity. A monad indexed by C is: -- Functor map : ∀ {a b i j f} → (a → b) → (M f a → M f b) -- Applicative pure : ∀ {a i} → a → M (id i) a apply : ∀ {a b i j k} {f : Hom i j} {g : Hom j k} → M f (a → b) → M g a → M (g ∘ f) b -- Monad bind : ∀ {a b i j k} {f : Hom i j} {g : Hom j k} → (a → M f b) → M g a → M (g ∘ f) join : ∀ {a i j k} {f : Hom i j} {g : Hom j k} → M g (M f a) → M (g ∘ f) a _&gt;=&gt;_ : ∀ {a b i j k} {f : Hom i j} {g : Hom j k} → (b → M g c) → (a → M f b) → (a → M (g ∘ f) c) That's quite a mouthful, but it's just a copy-paste of the monoid-indexed one with the types fixed to make the composition well-typed. As the saying goes, "a monoid is just a category with one object", so a monad indexed by a monoid A is a monad indexed by the category `(Unit, λx y → A, const empty, _&lt;&gt;_)`. Indexed monads are monads indexed by the category `(Set, (_,_), diag, comp)` where `diag i = (i, i)` and `comp (j, k) (i, j) = (i, k)`. Dammit, all the code makes this post look *way* longer than it is. EDIT: Welp, I dumped this and forgot to make it relevant to computation/effects. Monadic regions form a monoid-indexed monad, where given an initial set of "region ids" S, the monoid is the ~~powerset~~ type of supersets of S, union and S itself as identity. I don't know any category-indexed monad that's not either monoid-indexed or (_,_)-indexed, but it's nice that it generalizes both.
it might be a good idea to mention the danger in using fail, because most monads use `error`. e.g. the real-world haskell book tells &gt; Beware of fail &gt; Many Monad instances don't override the default implementation of fail that we show here, so in those monads, fail uses error. &gt; Calling error is usually highly undesirable, since it throws an exception that callers either cannot catch or will not expect. &gt; Even if you know that right now you're executing in a monad that has fail do &gt; something more sensible, we still recommend avoiding it. It's far too easy to &gt; cause yourself a problem later when you refactor your code and forget that a &gt; previously safe use of fail might be dangerous in its new context.
I've always been a fan of having fun using Haskell with Project Euler tasks. :D. It might not be as important for Haskell learners as they tend to have a more mathematical background (directly or indirectly), but Project Euler forces you to understand a problem and implement an algorithm solving it. Of course it won't help implementing simple data shuffling applications (web apps) that much, but it will teach discipline in programming the computer to do things. of course that's no disagreement with what tekmo said.
I haven't read this paper, but I know Edwin has a nice effects implementation in Idris. (Plus, it's borderline practical, which is always fun). 
Not sure if it's worth it - such cases should be very rare. And the `cabal` tool is supposed to be portable (at least to OS X &amp; Windows).
&gt; How long should the timeout be? The timeout should have a reasonable default, say half minute, and be overridable by the user. (I'm not advocating the OP's suggestion, but the same issue appears with wildly undecidable type systems, and I think this should be the default solution for that) 
Re: your last paragraph, shouldn't the identity be the empty set, i.e. no effects?
Looking forward to the official release. In the meantime, a lot of people have had good results just using the git repo - which by now has gone beyond what will be the 1.18 release. If you want something a little more stable, you can also use the "`cabal-install-1.18`" branch.
That's what we used to do. But now I really believe that the sandboxing built in to this new version of cabal is better. No need for cabal-dev or hsenv anymore.
Yes, and if you need it, you can still get the old behavior by using `--snapshot` to avoid rebuilds of the remote source.
I have a working development version of `ghc-server`-based `ghc-parmake`, providing real speedups. Next is polishing it for a release and then integrating with Cabal.
*Thank you* for not requiring write access to my github repositories, or any access to my private repositories. Lots of places seem to do this, unfortunately.
If you want to see more indexed data types goodness, you might want to try some dependently typed programming! Really, dependent types are all about adorning types with information, use cases abound. [Idris](http://www.idris-lang.org/) is a good start: it's a dependently typed programming language that strives to be more practical as a programming language rather than theorem prover like Coq or Agda, and the friendly crowd at #idris on irc.freenode.net would be more than happy to help you make your first steps into the dependently typed world.
I might merge master into the cabal-install-1.18 branch before we make the release actually. I might have cut the branch a bit too early, missing some nice fixes that went onto the master branch.
Just completed the last set. I can show anyone who is also finished the challenges. Just tell me your github name and I will add you as a collaborator to my private repo.
Using Ajhc Haskell Compiler. http://ajhc.metasepi.org/
&gt; Indexed monads are monads indexed by the category (Set, (_,_), diag, comp) where diag i = (i, i) and comp (j, k) (i, j) = (i, k). I propose we call this the category of dominos.
I have been using 1.17 for months now and it has parallel compilation at the dependency/package level. Major speedup for big fresh installs. Scanning the notes quickly, I was a little surprised to see no mention of it other than the jobs config parameter 
the post discusses space leaks using the example of a (supposedly pretty bad) space leak in `reactive-banana`. i'll just ask here, is there a public commit of the fix somewhere? i did not find it on apfelmus' github repository.
Thanks for going through the code and for the feedback! &gt; You prefix many functions in Parse with _, which suppresses “defined but not used” warnings under -Wall. Is that intentional? It can hide dead code. I didn't realize that. I'll remove the underscores. &gt; In your friendly instances, you can reduce repetition of the function name by using case instead of function patterns. opCode and register from Decode are similar, and you can also factor out the Just by using a guard for the Nothing case. I just looked up guard, and I can see what you mean. &gt; Label.lastGlobal being strict on a String isn’t very useful, as it’s only strict in the outermost cons cell. Same goes for active and initial from Section. Instead of String you might prefer Text, which is a packed UTF-16 text type. I was following conventions from [here](https://github.com/tibbe/haskell-style-guide/blob/master/haskell-style.md) about strictness in records. I understand the benefits of Text, but if I wanted to stick with String, would it be preferable to remove strictness from the String and [a] fields? &gt; More generally, it would be safe to make more of your data structures strict. If possible, could you expand on this point?
This was already in 1.16.
Why the downvotes? OP want to know whether recursion (being so prelevant in Haskell) really eats up the stack all too quickly (for example while traversing a very deep tree). What's wrong with that question?
If mbed is [this thing](http://en.wikipedia.org/wiki/Mbed_microcontroller) then isn't 64kb of memory too little for Haskell? At least for ghc I was under the impression that Haskell typically users a **lot** of memory, even if it garbage collects it almost immediately afterwards. Cool stuff though, it seems like a step towards Haskell on lighter embedded systems.
This is looking great! Sandboxes (even though they're not going to work for my main use case), repl, run and get are all things I've wanted in the past. Thanks to all those involved!
`block` and `unblock` were rightfully removed. 
Praise.
I already know some Idris, but I didn't know that there's also a IRC channel for ir :) In any case, my problem now is that once I know Idris/Agda, I don't know where to find information about the kind of structures (such monads indexed with monoids) that seem to be in the wild and/or sort of folklore, but with no article describing them.
The regex library's use of fail, along with Either being an orphan instance, [tripped me up](https://groups.google.com/forum/#!msg/haskell-cafe/mHR-h6tF3og/p3P1HFxjc8EJ) for hours. At least now the Monad instance for Either is no longer an orphan; this must have changed recently. For some time I have been using [explicit-exception](http://hackage.haskell.org/package/explicit-exception) just to avoid the Either orphan instance, out of fear that it would bite me again in some strange way.
_All_ optimizations already work by first detecting whether they can be applied. That's a vital part of the optimization. You can't handwave that part away when you claim something can be optimized; you need to provide the decision routine too, or you don't have an optimization, just an unmotivated AST tree manipulation. In your head, you've got a munged up combination of a trivially useless detection routine ("it's fib! what's so hard about that?!") and a magically potent one that you're not willing to specify but is vaguely floating about in your head anyhow. People are trying to walk you through nailing down the second one enough to discover it's impossible to manifest in the real world, and you are steadfastly refusing to nail it down. Until you do, until you take the step of starting to manifest vague thoughts in concrete implementation, you're going to continue to be confused. I have found this is a very important skill in programming in general. I often feel half my job is reaching into Platonic space, where everything is easy and happy and perfect, and yanking something out of it into the real world. Platonic ideals don't like that. They kick and they scream every step of the way as you expose them to the blow torch of the Real. Sometimes I even start to feel bad for the buggers as their face melts away Indiana-Jones-style and their innocence is lost. What comes out of the end of the process is uglier and more complicated than what you saw in Platonic space. It's always harder than you hoped. And sometimes you discover that you can't embed it in the Real at all. But you're still better off in the end than if you had just grubbed about piling muddy goo on top of gooey mud while merely dreaming about how wonderful Platonic space could be, which is the way most programmers approach problems. Don't be content to float around in Platonic space; learn how to manifest the ideas in the Real. (At this point, my attraction to Haskell should be obvious; it's still in the Real, irreducibly so, but it's more Platonic-friendly than the vast majority languages.)
This is a really helpful writeup. One day I would like to see a "strict" Haskell with an explicit Thunk datatype. That would be really interesting, I think, especially since Thunk would be a monad! [EDIT: the key to this proposal is that `Thunk` would be an explicit datatype. Means of making Haskell more strict, either existing or proposed, are nice but still lacking explicitness, which I really see as the key.]
Yeah, it is a really good exercise for solving challenging algorithmic problems.
It's in the `develop` branch. [Comment on the bug tracker](https://github.com/HeinrichApfelmus/reactive-banana/issues/52#ref-commit-833898f).
Ah, interesting. I must have missed it then. I guess I had the old cabal config without the jobs parameter. Thanks!
I don't know how it works, exactly, but that sounds like it could be a problem. The guy who wrote it (Guatavo) likes to talk about it over email but even with his help I couldn't get some stuff to work which pushed me to use Haskell (and I've never looked back). 
I know it's not related to that. It's rather the lack of `hSetBuffering stdout LineBuffering` in `main`, and the use of `putStr` instead of `putStrLn` isn't helping either. I guess that buffering mode is implicit when the program is properly compiled.
The second most popular answer mentions fmap and using liftFoo1 rather than liftFoo2 as an fmap instance. fmap is a special case of lifting for lifting/promoting single argument functions. From what I understand, in a more general form of lifting, where it doesn't have to be implemented separately for each arity, is applicative functors. 
This sounds awesome! 
Will haskell-mode support cabal sandboxes when it is released? 
Ah yeah cross compilation. I can't wait for that. That is just what I needed to make greater use at my company.
I *really* enjoy these series. I'm not a total functional noob and can understand the idea behind monads. The problem with other tutorials including 'Learn you a Haskell' so far was that they taught me basic functional stuff all along, which I'm already quite fluent with. This is far more interesting, because it mixes learning Haskell with some parsing theory, thus giving me the feeling of doing an actual project rather than reading a language reference.
Maybe it will be useful for Haskell newbies as me.
Buffering mode depends on how it's called actually. From GHCi it's `NoBuffering`, from a terminal its `LineBuffering` and invoked from another process it's `BlockBuffering`. At least on Ubuntu/OSX where I keep having to fix that stuff.
Indeed, there is already an "evaluation strategy" monad, which also deals with parallel evaluation: http://hackage.haskell.org/packages/archive/parallel/3.2.0.3/doc/html/Control-Parallel-Strategies.html#t:Eval
What happens when someone killThreads a safe IO thread?
I doubt you could have a thread in IOSafe. Only operations that have no exceptions would be in IOSafe. In this way they would be like pure code, so async exceptions can still do their thing (just like they would in pure code). The idea is to be able to tell when you have an IO action that does not need to be exception-protected.
These are all totally valid reasons. I wasn't challenging you so much as wondering what led you to your decision. Sounds good to me.
Interesting. I don't have any experience working in F# so I didn't know. I'll keep this in mind the next time someone suggests using it as a Haskell replacement.
As others have already said: very good article!
You may want to look at profiling: http://www.haskell.org/haskellwiki/How_to_profile_a_Haskell_program
The vast majority of my code really can't be written there. e.g. try porting almost any of the old category-extras. I'm not sure what that says about me or F#, but there is an impedence mismatch. ;)
Wouldn't Thunk be more natural as a comonad? Edit: I think it's a bimonad, that is, both a monad and a comonad where the following properties hold: duplicate . return = return . return extract . join = extract . extract extract . return = id I still expect that the comonad part would be more useful, though.
Yes, you would need some way to embed SafeIO back into IO in order to actually run the actions :) I mean, I can build such a `SafeIO` on top of the free monad, or whatever, but I can't import from the FFI and mark it free, etc. Not easily.
Are you sure about this? I'm testing this out having a function with type myIOThing :: IO () in a separate module. foo = $(let x = myIOThing in [| x |]) It will produce this error: No instance for (Language.Haskell.TH.Syntax.Lift (IO ())) arising from a use of `x' Possible fix: add an instance declaration for (Language.Haskell.TH.Syntax.Lift (IO ())) In the Template Haskell quotation [| x |] In the expression: [| x |] In the expression: let x = myIOThing in [| x |] Moving myIOThing inside the brackets will compile but not run at compile time. Making this work would be handy for running tests at compile time. edit: I found a way around this problem for my particular use case by not using IO. I can use `trace` for printing when I run tests instead. It would be useful to be able to run IO at compile time however.
Or a Haskell with strict function application and lazy constructor fields by default? Or, even better, a Haskell with lazy function application and strict fields? I really wonder how that would play out.
I want to add a {-# LANGUAGE Strict #-} that would make both function application and fields strict with optional laziness by annotating functions arguments/fields with `~`. This would allow you to play with strictness by default on a per-module level.
Something like https://gist.github.com/singpolyma/6300661 might actually be sufficient. It's an idea I come back to every so often.
Yes, but exercism forces you to work on them in a specific order, and you don't get to work on the next one until the right person has marked your solution as good (currently, anyway). I ported all 28 of the exercises from Clojure to Haskell, and there are others that I might get to in the next few days.
I think you'd see a lot more value in an explicit Thunk datatype because then you could determine "strictness" from function signatures. I guess you'd also have a `ThunkT` transformer given by `ThunkT m a = m (Thunk a)` and that would allow you to define strict versions of `Maybe` and all your other favourites, and use them lazily through the standard functor, applicative and monad combinators. 
Explicit strictness/laziness has benefits, but might get in your way. It's a trade off. Maybe put them in some observable effect system, orthogonal to types.
&gt; Explicit strictness/laziness has benefits, but might get in your way. It may indeed. That's why I'd like such a language: so I could see what the tradeoffs are!
It would be nice to parameterize the ~. Duplicating data structures just for different strictness annotations is akin to duplicating them for differing types. Then a spine/value-strict list is `ListT Identity a`. A spine/value-lazy list is `ListT Thunk (Thunk a)`. That could be nice.
Cool, thanks!
Apparently it is called a rectangular bandoid: https://github.com/ekmett/semigroupoids/commit/607f3263fdaf8d61561f723cb7e923cdee345433 But dominos is a much better name!
This is awesome, thank you!
I second Crandom. You're doing some fantastic work longlivedeath - keep it up! We all appreciate it.
&gt; if I wanted to stick with String, would it be preferable to remove strictness from the String and [a] fields? Yes. Strictness on a list doesn’t buy you much, because it doesn’t make the head *or* the tail strict, only the first cons: data A a = A ![a] A undefined `seq` () -- *** Exception: Prelude.undefined A [undefined] `seq` () -- () A (():undefined) `seq` () -- () &gt; &gt; More generally, it would be safe to make more of your data structures strict. &gt; If possible, could you expand on this point? `Location`, `Statement`, and `Expression` all have fields which are lazy but don’t need to be. 
Have you seen Disciple?
OCaml has this. I hate it. It's awkward to use, so nobody really uses it. 
Actually, seq doesn't specify the actual order to evaluate the arguments. It only promises that by the time you get the result it has evaluated the first argument. 
You're only in the latter group once, then you're in the former for the rest of your life.
Thanks for the explanation!
Hm, I wonder about this: &gt; value-strict = if the container is in WHNF, then each value stored in the container is in WHNF &gt; &gt; The plain data type `data Tree a = Bin (Tree a) (Tree a) | Leaf a` has... the value-strict variant &gt; &gt; data Tree a = Bin (Tree a) (Tree a) | Leaf !a Can't we construct things like `Bin thunk thunk`, which is in WHNF but has not yet forced all (any of) the values?
Right. x `fakeSeq` y = y `seq` x `seq` y is the same as `seq` denotationally, but if you use it to define `foldl'` the result will be a space leak. GHC also provides `pseq`, which is supposed to guarantee the operational semantics one might expect from `seq` (which has only denotational guarantees). I've never been quite sure why using regular `seq` seems to always work out when defining stuff like `foldl'`.
Thunk, like Eval and Async I think of more as "modalities" than monads or comonads as such, even though they have properties of both. In a pure language, they're basically denotationally identity (modulo some trickiness with bottoms), and in a pure total language they're absolutely denotationally identity. So not only do they have some nice properties, the have "all the nice properties" and commute freely with just about anything. (well, async is over IO, but we can sort of squint and ignore that). It strikes me as interesting, maybe, to throw these all into the mix at once, examine how they interact, and generate some sort of "calculus of modalities" or "modalculus" to explore what that sort of granular control of evaluation would overall make possible. There was a paper with a fine-grained concurrency library a few years ago I think, but I can't remember it for the life of me...
But I'm saying it's *not* value-strict, because you can evaluate it to WHNF without evaluating the values to WHNF.
I think the Haskell community would be more motivated to make it usable (as we're used to pervasive laziness) as well as more capable of making it usable. Already there have been suggestions of how to parameterize common types by their strictness behavior. This isn't directly possible in ML, since there type constructors can only have parameters of kind `*`.
*Edit*: Here are the slides -- http://gbaz.github.io/slides/hurt-statictyping-07-2013.pdf Finally, next wed., August 28, we're going to have some amazing talks. The first will involve Haskell and _robots_! Like johnny five, or the terminator, or, uh, a roomba. The second will be on Agda. If you're in the general NY area, it'll be well worth checking out: http://www.meetup.com/NY-Haskell/events/133966332/
It makes much more sense to me now. Thank you!
duplicate would be a specialization of return, and join would be a specialization of fmap extract.
NO DISASSEMBLE! (Sorry, had to)
&gt; Are you sure about this? Absolutely. I've run full parsers reading outside files, generating very very large swathes of code at compile time. &gt; foo = $(let x = myIOThing in [| x |]) It isn't quite that simple. You can embed IO actions in any Quasi monad, of which Q, the monad that runs template haskell is an instance, via Language.Haskell.TH.runIO -- that gives you a Q action that runs the effect. so you can use that in the splice. what you tried to do was splice in a TH-time IO action as an expression. that is different. It is usually used to take a number or something that is known at compile time and store it for use later at runtime. Instead you should just sequence the actions you want to take and get `Q Exp`, `Q Dec`, etc. some of those actions will be IO tasks lifted with runIO, that is all.
Interesting. This reminds me of a small experiment I did a while ago to run Haskell on bare metal x86. However I took a much easier approach: link a minimalist libc with C code produced by ajhc and apply some kludge on jhc-rts until it works. ;)
Thanks for the answer. I might take a look at it if you're open to pull requests? Apparently you can do anything with pipes. You just have to be able to get sufficient neurons pointing in the right direction, as task which mostly escapes me at the moment :-)
We (LexiFi) use it but we have added some syntactic sugar (lazy let bindings). It's not *that* bad.
I think you are right that the example for value strictness is not actually value strict in the sense defined in the post. Anyway, I find the post quite interesting. Especially, the "reverse strictness" properties look intriguing. 
&gt; Thunk, like Eval and Async I think of more as "modalities" than monads or comonads as such, even though they have properties of both. Perhaps, but you would probably primarily use comonad combinators when working with them. &gt; commute freely with just about anything Denotationally, maybe, but the way they commute is operationally complicated.
:)
Great! If so, please try _JHC_JGC_NAIVEGC flag to get small footprint. http://ajhc.metasepi.org/manual.html#special-defines-to-set-cflags
We already have web servers like goliath or tornado (e.g warp), so I'm not certain any contribution in that area would be useful to the haskell community. As the GHC IO manager is already using event-based, asynchronous IO under the hood, it requires much less substantial effort to get highly scalable servers in Haskell, particularly if you use a nice streaming IO library like pipes or conduit.
Well, I never used neither Yesod nor Snap, but [this StackOverflow answer](http://stackoverflow.com/a/5650715/635669) says (in a very hidden way) it is possible. Anyway, I would post this question in other sources, as the [StackOverflow](http://stackoverflow.com/) and [#haskell IRC channel](http://ircbrowse.net/browse/haskell).
Not easily. The Snap monad uses Iteratees under the hood; Warp uses conduit. Two different approaches to streaming bytestrings. You'd need to runSnap to get an iteratee, grab the bytestring, turn it into a Builder and wrap that in a Response, and hand that off to Warp. Possible? Sure, but you'd get the worse of the two models' speeds, and that's if all goes well. It might help if we knew why you're interested in doing this.
Somewhat related, support for "cabal repl" is being added already: https://github.com/haskell/haskell-mode/issues/195 But there's not much to do IMHO to support native cabal sandboxes, as once it's setup, you just use the normal "cabal" commands as if you would w/o an active sandbox. 
also noteworthy: you can select which build target's environment you want to visit via `cabal repl`; so if you have a testsuite named `test-foo` you run `cabal repl test-foo` and the GHCi session is setup with `test-foo`'s packages and include paths.
Your time would likely be better spent learning the existing `snap` / `yesod` / `warp` toolchain for doing web development in Haskell. They demonstrate many useful abstractions for dealing with web content, and do a _lot_ of stuff for you that is hard to overestimate. They expose lots of powerful functionality already, and there is only one `Comonad` in the entire web ecosystem that the end user is likely to trip over consciously -- `Snaplet` configurations happen to form one. Understanding `Comonad` isn't necessary to being a good Haskell programmer. However, it was very useful to me in terms of evolving into the rather strange kind of Haskell programmer I am. I started playing with comonads because nobody else was playing with them in Haskell at the time. There'd been a couple of academic papers by Tarmo Uustalu, and Dave Menendez had written up a small library. That understanding has since metastasized into the `lens` library, which now helps power and speed up at least one of the web frameworks. Who knew?
I can see how comonad combinators might be useful A) if you are in a fully strict language _without_ a `Thunk` and B) if you are only working with `Async`, but even then I don't think the power of `extend` over `fmap` is very worthwhile, 95% of the time? Anyway since you can write `extend` and `duplicate` in terms of the monad operations in this case, the only "comonad combinator" as such is arguably `extract`, so the entire question of "are you using monad or comonad operations" becomes sort of silly. They wind up being the same!
I didn't mean to exclude it, it simply escaped my mind. I tend to view the trade-off between them being that 1.) `Yesod` will give you a lot of support in building a web site. It is more or less Rails ported to the Haskell ecosystem. Strong defaults, lots of quasiquoters to make domain specific parts of the task less painful. The type errors you get are sometimes rough, mostly because they encourage you to fly without a net signature wise, and then use a lot of overloading to put things together. With something like the #yesod channel at your back you can be quite productive though. It has also contributed a lot of machinery for things like `conduit` to the ecosystem. 2.) The snap framework avoids the quasi-quoters that give Yesod its distinctive feel. It also focuses rather heavily on users being able to generate new `Snaplet` components that provide reusable bits of functionality, that you can drop into a site, which maintain their own state through a Lens into the larger system state. 3.) Happstack was first on the scene as HAppS. It is ultimately probably the most flexible solution. You can build almost anything you want with it, but you need to understand the entire solution from soup to nuts. It is more of a family of components you can string together to build whatever kind of web server you want. My understanding is that is evolving to become easier to use. The trade-off to me comes own to quasi-quoters vs. existing scaffolding for reuse vs. flexibility, but even there it is a bit convoluted, because lots of the pieces interoperate, and the other solutions are pretty flexible too. In the end I'm personally prone to choose between `snap` and `yesod` based on what I want to talk to, and tend towards `snap` since it depends on more of my "ecosystem" already, so the abstractions integrate well with my code.
&gt;Does Snap support WAI Nope.
Oh! That's what happens when someone palpitates in a subject he doesn't understand. Thank you for the clarification. :)
Nice. We're exactly on the same wavelength :)
That's a very good point. EDIT: I have fixed the main text. It appears that space invariants of an abstract data type depend a lot on a the methods that are available. For instance, if the only operation you are allowed to perform is insert :: Path -&gt; a -&gt; Tree a -&gt; Tree a insert p x tree = x `seq` ... then all the values will be in WHNF when the container is in WHNF, even though the container does not need to be spine-strict. When you are allowed to use the `Bin` constructor directly, then you are right, the data type will not be value-strict.
I freely admit that my statement merely reflected my experiences with the different frameworks. `stepkut` and `sclv` have been trying to convince me it has changed for some time. ;) So please take this as just slightly dated data point.
The link gives me a 404 right now - temporary?
Apparently it's called [Pulsar](https://bitbucket.org/jehaberk/pulsar) now
As an example, a lazy list would have to be like `ListT` (the done-right version). I think most would agree that this is a very awkward style, especially if it's pervasive.
I renamed Saturn to Pulsar, since there was an existing project related to the DCPU-16 named Saturn (an emulator). The new link is https://bitbucket.org/jehaberk/pulsar.
http://www.reddit.com/r/haskell/comments/1kqfr8/saturn_a_dcpu16_assemblerdisassembler_written_in/cbt14z8 Sorry about that.
Can you tell us why do you need this? Unless you intend to build an app with like millions strong userbase there's not much difference speed-wise between warp and snap.
It was a fun talk, and the slides were a fun read. 
Pattern matching on it would be awkward, but I tend not to do that much, preferring `map`, `foldr` and friends. Still I'd like such a language to be able to see exactly *how* awkward it is and see if some clever people can come up with a way of making it less awkward.
Then why Snap? Just pick Yesod. And if Yesod is too heavy for your specific case, i think they recently added an option for light apps. Just barebones web server without any templates, sessions, database backends etc.
I like the idea of snap monad, which is a state of request and response wrapped in Iteratee Io, I think it is really neat. I haven't learned yesod yet because I heard it is a heavy framework, but maybe you are right, I should give it a try.
scotty seems to be a nice lightweight web framework, it is such a shame to get abandoned. 
I think I have their first album.
snap-core is to the Snap Framework as wai is to the Yesod Framework. If you like the Snap monad, then you should use snap-core. wai is not much more than a Request and Response data type, and since only one of the three main Haskell web frameworks uses it, it's not really accurate to consider it a standard.
Lemmih, the author of the [current SDL binding](http://hackage.haskell.org/package/SDL), will be working on it next week during [ZuriHac2013](http://www.haskell.org/haskellwiki/ZuriHac2013).
The type of `return` doesn't look very useful (except, obviously, when used along with `(&gt;&gt;=)`), since the argument must already be evaluated. More useful would be thunk :: (() -&gt; a) -&gt; Thunk a Just seeing it makes me shudder. :( This makes sense, though, since `return` is supposed to be benign, and in this case the effect is laziness itself.
WAI was intended to be like wsgi in python, but with no adoption it obviously didn't achieve that.
The code size doesn't matter as much as the amount of RAM required, since micros typically have much more flash than RAM. Also, the amount of RAM usage is hard to predict, so typically heap allocation is avoided and RAM space is statically allocated.
You can't pattern match against 4.1 because you're not requiring the type to be equality comparable (class Eq). Here's the fix: lucky' :: (Fractional n, Eq n) =&gt; n -&gt; String This is what the error message is trying to say, but it will take some getting used to in order to understand what they mean. But it's not a good idea to compare equality of floating point numbers anyway. If you leave out the type declaration, you can ask GHCi what the type is by saying :type lucky' And it will respond: lucky' :: (Eq n, Fractional n) =&gt; n -&gt; String 
Pattern matching on floating point requires `Eq` (it's also not reccomended for the same reason that equality checks on floating point are not reccomended). `Eq` is required on all `Num` in the official Haskell spec, but GHC violates this in the standard libraries.
Thanks for explaining it to me. Still trying to get used to this language!
Thanks for the reply. Why does GHC violate this?
In order to compare `n` to 4 (or 4.1), Haskell needs the type of `n` to be some type that supports equality testing. In order words, the type of `n` needs to have a corresponding instance of the Eq class and this needs to be reflected in the type signature of your function. In the first case, there is already an implicit requirement for an Eq nstance for the type of n. According to the [docs](http://hackage.haskell.org/packages/archive/haskell98/latest/doc/html/Prelude.html#t:Integral), to declare a typeclass instance for Integral you also need that type to have a typeclass instance for Real. Similarly, Real requires Ord (ordering) and Or requires Eq. On the other hand, the Fractional type class does *not* have Eq as an indirect dependence so it is possible that someone could define a new type with a Fractional instance but *not* implement equality testing for that type. The fix to this is suggested by the error message: you need to specify in your type signature that your function only works for the Fractional types that *also* support equality testing: lucky' :: (Fractional n, Eq n) =&gt; n -&gt; String The reason the code works if you omit the type signature is that the Eq constraint you left out then gets inferred by the compiler. You can verify this inferred type by defining your code in ghci and asking for its type via the ":t" command: λ&gt; let lucky' n = case n of 4.1 -&gt; "Lucky"; _ -&gt; "Not Lucky" λ&gt; :t lucky' lucky' :: (Eq a, Fractional a) =&gt; a -&gt; [Char] Finally, as an aside, returning strings like that is OK for an example but in a real program you would probably want to return a more strongly typed ADT instead data Luck = Lucky | NotLucky
&gt; My code compiles properly when I comment out the 4th line. Maybe you're not aware of this, but you can load the code in ghci and see the inferred type.
nah, I just took way too long to explain a simple concept :)
`return` is useful when you have an already evaluated argument that you want to pass to a function which expects a thunk. As you point out, the equivalent is true in for any monad. Having to write `thunk \() -&gt; exp` would indeed be messy. A new special form would probably be required.
Because supposedly there are some representations of numbers which don't have a good instance of `Eq`. One strange example is making `(Num b) =&gt; a -&gt; b` an instance of Num. fromInteger = const . fromInteger f + g = \x -&gt; f x + g x -- etc
I'm sorry, I couldn't understand. Could you try dumbing it down for me?
Because there really is no reason why Num should require Eq in the first place. It's perfectly possible to implement Num for types where defining Eq is not feasible. Num makes sense for certain types of functionish data types like wires from netwire. It's not possible to implement Eq for them. This Eq =&gt; Num constraint is overly restricting for no real reason. 
This used to work prior to [7.4.1. Section 1.7.3. Libraries]( http://www.haskell.org/ghc/docs/7.4.1/html/users_guide/release-7-4-1.html), first point * The Num class no longer has Eq or Show superclasses. A number of other classes and functions have therefore gained explicit Eq or Show constraints, rather than relying on a Num constraint to provide them. Both Integral and Fractional depend on Num, so at that time they also had the Eq constraint. You can navigate the information by using `:info Type`. (I'll cut the irrelevant parts out) :info Integral class (Real a, Enum a) =&gt; Integral a where :info Real class (Num a, Ord a) =&gt; Real a where :info Fractional class Num a =&gt; Fractional a where 
Sometimes you want to pretend that a function that returns a number is itself a number. For example: sqrt :: Double -&gt; Double sqrt = ... -- predefined square :: Double -&gt; Double square x = x * x Sometimes as a convenience you might want to "add" these two functions together without supplying them with an argument just yet: sqrt + square :: Double -&gt; Double This would get translated to: \x -&gt; sqrt x + square x Note that I don't necessarily approve of doing that, but that's what he was referring to.
People like to make the type of "functions that result in numbers" numbers as well. There's a really direct "pointwise" way of defining the instance instance Num b =&gt; Num (a -&gt; b) where f + g = \x -&gt; f x + g x -- (f + g)(x) = f(x) + g(x) abs f = \x -&gt; abs (f x) -- |f|(x) = |f(x)| ... but while instances like this are perfectly good ideas of "numbers", they cannot have machine decidable equality. There's no way to determine whether `(f == g)` faster than trying it out on every possible input... which will take unbounded amounts of time for any infinite input domain. instance Eq b =&gt; Eq (a -&gt; b) where f == g = forAll (\x -&gt; f x == g x) -- how can you define `forAll`? So, despite the standard, common believe now is that `Eq` shouldn't be a superclass of `Num`.
Yeah, OCaml has a `lazy` keyword for this purpose. It just does the same thing, though.
Another example is real numbers, they cannot be compared for equality, but they are numbers.
Woah you have blown my mind. I did not realise `seq` did force evaluation order.
It's an odd form of programming, but quite amusing, and a great DSL if you want to think of it that way. Given the corresponding Num, Fractional, and Floating instances, the following code works like a charm: data Circle = Circle { radius :: Double, center :: (Double, Double) } diameter :: Circle -&gt; Double diameter = 2 * radius circumference :: Circle -&gt; Double circumference = diameter * pi area :: Circle -&gt; Double area = pi * radius^2 
The post is 9 days old. No one will see your comment. (:
Could you elaborate a little on how snaplets form a comonad?
Thank you for these comments. Indeed, you are right, unfortunately these are the errors in the article. The editor of the magazine, Edward Z. Yand, kindly agreed to update PDF with the fixes and it is available via the same URL.
Because the spec codified a bad idea, and to change the spec we first need to have an implementation that shows the right way. 
I copied and pasted that number from the `io-streams` code to compare using the same chunk size at the time I wrote that up. However, I'm pretty sure that it's not a typo even in the `io-streams` code base. Whenever you see a number slightly smaller than a power of two it's because they are factoring in the overhead of some data structure so that it fits within whatever page. You'll see this in the internal code for `ByteString`s too where the default chunk size is some power of two minus the overhead of the `ByteString` data type.
Worked great for a small project I did. Based on Ruby's Sinatra.
Use `:i &lt;identifier&gt;` to do this.
Some nitpicks: * said higher kinded when he meant higher ranked (when presenting ST) * missed the point of ST. It's all about the pure interface. The single thread is a weird way to describe it and doesn't convey the usefulness. 
This. A thousand times, this.
Note `seq` does not imply ordering. `pseq`, however, does. 
Yeah this was a great talk. NY-Haskell tends to be a really good event.
Just to clarify, role annotations are never necessary to ensure type safety. gcross's comment does not talk about type safety, per se, but many of the responses do -- what we're talking about here is correct vs. incorrect runtime behavior, not type safety.
Ah, now I see it! Thanks. The modular handling of state afforded by Snaplets is one of Snap's strong points, IMHO.
You are absolutely right, and given that I myself made that very mistake in a later comment I appreciate the correction. :-)
I actually *really* like it since I just start thinking of pointwise definitions of functions. The problem is that since functions are everywhere it's pretty easy to get confused as to which `Num` instance you're really using. Conal Elliot's Beautiful Differentiation paper makes really incredible use of it as well.
lowercase `:i`.
This is a great concept, and I'd love to see it take off. At the moment the process is a bit unclear (i.e. who can see my submission? How will I be notified?) but I'm sure I'll get the hang of it.
Nope. Fractional requires only Num. Integral requires Enum and Real, Real requires Num and Ord, and Ord requires Eq. However, all the basic types that implement Fractional also implement Eq.
&gt; Fractional requires only Num. In Haskell '98 and 2010, Num is a subclass of Eq (but not in GHC).
Does `2 * radius` work because integer literals implicitly have an `fromIntegral` to them, and `fromIntegral = const` with the corresponding `Num` instance? That's really cool. I fairly recently invented this `Num` instance by myself to solve a particular problem I was having, but I never realised it could be this useful. But what about `pi :: Double` presumably?
Well, I just tried and while it indeed ran with a smaller memory footprint, it makes GC pauses a lot more noticeable.
That name is already taken: http://omega.googlecode.com/files/Thrist-draft-2011-11-20.pdf :-)
All this is wonderfully written up by Altenkirch et al. http://cs.ioc.ee/~james/papers/AssistedMonads2.pdf
That's the free category.
Pfft, real numbers work like you'd expect them to. λ 0.99999999999999999 == 1.0 True Were you thinking about the problems of comparing IEEE floats/doubles, or did you have something else in mind?
How does it work with `pi`?
Remember also to check [Haskell News](http://haskellnews.org/grouped). :-p
A good starting point might be [ocharles article] (http://ocharles.org.uk/blog/posts/2013-08-18-asteroids-in-netwire.html), [reddit discussion] (http://www.reddit.com/r/haskell/comments/1kmes7/building_an_asteroids_clone_in_haskell_using/) You should also probably read some introductions to FRP, like the [netwire tutorial] (http://hackage.haskell.org/packages/archive/netwire/4.0.5/doc/html/Control-Wire.html). There are a lot of different FRP-frameworks you can use, netwire is one, [reactive-banana] (http://www.haskell.org/haskellwiki/FRP_explanation_using_reactive-banana) is another.
`pi` is part of the [`Floating` typeclass](http://hackage.haskell.org/packages/archive/base/3.0.3.0/doc/html/GHC-Float.html#t%3AFloating).
Oh. Nice one!
btw, what is the original source/context of "Brian's Observation" about full-time idiots on a team?
Cuboid is using OpenGL (GLUT) and Yampa, a 3D puzzle game around 250-300 LOC utiliazing FRP. https://github.com/pedromartins/cuboid Here's a 2D pong game written using Helm game engine and Elerea which is the FRP library that Helm utiliazes. https://github.com/klrr/pong What I did was I took inspiration from the Elm examples. Games are architectured with a "model" part, that is all the game objects represented as data and signals. Then the "update" section, which is completely pure and is stepping the state each frame. The last part is the "view" part which handles the rendering of all game objects.
[Elerea](http://hackage.haskell.org/package/elerea) is another great FRP library. Benefit is that it's very simple, it got a SDL module and there's a game engine called "Helm" which utiliazes it.
On the test at the end, it tells me that `g . f . h . j $ x` should be ticked: that's wrong. I think it was just a typo, because the text explains perfectly why it *shouldn't* be ticked(`g . f != f . g`).
[Here](http://jshaskell.blogspot.tw/2012/11/breakout-improved-and-with-netwire.html) is a full example of implementing a breakout game using [netwire](http://hackage.haskell.org/package/netwire).
No, I'm talking about real numbers, not floating point numbers. Equality for real numbers is undecidable.
The Haskell numeric classes might be a bit confusing, but a lot of thought (Joe Fasel's thoughts) went into them.
I might be wrong about this but I think Elm includes Arrowized FRP which it calls "Automatons". http://elm-lang.org/docs/Automaton.elm Also, OP might want to check out Frag, the 3d shooter written in Yampa. http://www.haskell.org/haskellwiki/Frag http://www.cse.unsw.edu.au/~pls/thesis/munc-thesis.pdf
I agree.
Yet (constructive) real numbers are perfectly possible and useful in Haskell.
(To the GP:) This is actually quite natural and you will see `f + g`, `f * g`, and so on everywhere in mathematics. It only seems weird in Haskell because we have a standard typeclass called `Num` instead of `Field`, `LinearSpace`, and `Algebra`.
Now I really hope Haskell has idiom bracket syntax so that: `[| sqrt + square |] :: Double -&gt; Double`.
GHC: removing Eq and Show superclasses was pretty recent, Feb 2012 http://www.reddit.com/r/haskell/comments/p886g/ghc_741_is_out_heres_the_release_notes/ http://haskell.1045720.n5.nabble.com/Proposal-Remove-Show-and-Eq-superclasses-of-Num-td4808712.html
GND just re-appropriates an existing instance for use with a new type. Adding instances of associated types is something else, as internally, these instances are not actually tied into class instances. But, especially with roles around, it may be possible to do what you want. Submit a feature request -- this wouldn't be hard to implement, I think.
I was researching Automatons today and what they mean. Was left with confusion. But yeh Automatons seem to form an Arrow instance indeed.
Thanks. That's indeed a mistake!
You're right that I'm partially playing devil's advocate here. However, I was also venting some frustration because the code I'm working on right now is in this exact situation. It's doing a couple of quick calculations based on some VERY slow calculations that are constant. The code has slowly been growing uglier as I've been hard coding in values to try and get reasonable speed. I'm probably going to wind up templating away a lot, but I would have killed for a compiler switch to calculate the slow function at compile time.
Ticket created: http://ghc.haskell.org/trac/ghc/ticket/8165
You can also check out [Dungeons of Wor](http://hackage.haskell.org/package/dow), which is a reasonably complex game implemented in Elerea. Or for something even more interesting, have a look at [Stunts](http://hackage.haskell.org/package/stunts), which combines Elerea with OpenGL and Bullet physics.
&gt; snap-core is to the Snap Framework as wai is to the Yesod Framework. Kind of true. `snap-core` is a lot more then just an interface between a server and an app, which is essentially what is the purpose of `wai`, `wsgi`, `rack`, etc. Since `snap-core` is doing a lot more it is not likely to be picked up by other app frameworks in a way that `wsgi` is for Python and `rack` is for Ruby. I believe I've seen a discussion on why `snap`'s not using `wai`. But I forgot the conclusion (if there was any). To me it is clear that having a `wai` that is broadly embrased by a community is a good thing.
Yesod and Scotty use it. Others are free to use it. There was no clear lib like `wsgi` and `rack` in Haskell land when `wai` was created, that I'm aware of.
I think most programmers can be called Brian for this. And not just programmers. Certainly I've thought I could see idiots everywhere quite often. And occasionally, that wasn't because I was being an idiot. 
Consider the type Synonym type SF a b = Signal a -&gt; Signal b You can make SF an instance of Arrow (if Elm had typeclasses)
I forget who on IRC pointed to the "Modules over Monads and Initial Semantics" paper. I finally gave it a quick skim and it feels _very_ deep, and somewhat inevitable. Its definitely filed in my "tackle seriously, eventually" pile. http://web.math.unifi.it/users/maggesi/ic-final/ic.pdf
That was me :P
&gt; You are right, roles do not fix this issue. But they also don't introduce this problem. I agree; my criticism is more about the syntax that they chose to use. Personally I think that what they should have done was to go with /u/fas2's suggestion {-# ROLE R #-} because the syntax doesn't cause older compilers to break. &gt; And I think that if a newtype introduces its own Ord instance then the runtime behaviour of the Set functions should be seen as correct wrt the Ord instance provided. But the point is that the `Set` functions use the wrong `Ord` instance by default when the newtype uses GND unless you supply a role annotation.
As with all things, there is a version of this included with the lens distribution. In this case two of them. ;) https://github.com/ekmett/lens/blob/master/examples/Brainfuck.hs https://github.com/ekmett/lens/blob/master/examples/BrainfuckFinal.hs The former is a somewhat well factored version, with a parser, interpreter, etc. The latter shows that with final encoding of the semantics, many of the former steps can be fused together. Both versions gratuitously use a zipper into the state.
Thanks for the interesting read, then.
I like how the name have the same structure as "Learn you a Haskell for great good"
I would be happy to be proven wrong, but my intuition tells me that approach would not work well enough for something like a multiplayer game with FRP front-end and some kind of central-coordination server. I think the latency would be visible/sense-able even for a single player game and a multiplayer game, which is what I'm interested in, would make matters far worse. This is why pretty much every modern game has a full functioning physics engine inside, with tricks like position interpolation, latency estimation and all kinds of other goodies in the case of multiplayer games. This is as opposed a once-tried notion of shell front-ends that simply relay info to the server and wait to hear back what to do. The gameplay quality suffers too much. I should say that I have tried a few of your examples and came away a bit disappointed due to latency. I could feel the lag. Having said that, you mention below that Websockets made a real difference and I am looking forward to giving that a try!
&gt; As with all things, there is a version of this included with the lens distribution Quote of the week!
And "Write Yourself a Scheme in 48 hours"
Granted, the latency without WebSockets was 500ms or something, because that's the interval in which the client polled the server. It's markedly shorter with WebSockets. As for games, unless you're writing an AAA title, chances are that the browser/server concoction is fast enough for quite some time. I stopped worrying about performance a while ago and it works pretty well.
Nice! Was looking for exercises, will read this after breakfast. :-)
Here's a wonderful solution for this problem: http://martijn.van.steenbergen.nl/journal/2010/06/24/generically-adding-position-information-to-a-datatype/
My approach is always just to extract the functor by replacing recursion with a type variable and then close it with an explicit fix. I virtually never write ASTs with direct type recursion anymore..
What's more, this style of annotation produces a comonad. I don't know if it's a particularly useful property, but maybe it is.
Exercise idea: * Change the definition for `BrainfuckCommand` so that `LoopL` and `LoopR` are replaced with `Loop [BrainfuckCommand]`. * Write a new `parseBrainfuck` function that parses `Loop`s correctly. * Use this refactoring to get rid of the `bfSource2Tape` and `seekLoop` functions entirely.
I've seen it suggested before. I suspect the reason *comparing* got into the libraries, but not *equating*, is that the more general *on* was invented in between, after which the need for specific versions strongly diminished.
This is the approach I often take. You get a lot of generic operations (e.g. fold) for free.
so comparing wouldn't get included, were it to be proposed now. i still think it reads pretty nice and would be great in base :D.
The general rule of thumb is that if we can define a new combinator out of directly applying two simpler ones, we tend to shy away from adding the name for the composition as it eats up precious namespace, and discourages users from learning the more composable units of functionality. This used to be known as a the Fairbairn threshold, after Jon Fairbairn who proposed it on the libraries mailing list several years ago, but I seem to be the only one who still uses the term. ;) Now this is of course only one concern among several, and linguistically chunking is useful. That said, for a somewhat more amusing take on this stance, I offer the following short story by [Borges](http://en.wikipedia.org/wiki/Jorge_Luis_Borges), (emphasis mine). &gt; [Funes the Memorius](http://en.wikipedia.org/wiki/Funes_the_Memorious) by Borges &gt; &gt; The voice of Funes, out of the darkness, continued. He told me that toward 1886 he had devised a new system of enumeration and that in a very few days he had gone before twenty-four thousand. He had not written it down, for what he once meditated would not be erased. The first stimulus to his work, I believe, had been his discontent with the fact that “thirty-three Uruguayans” required two symbols and three words, rather than a single word and a single symbol. Later he applied his extravagant principle to the other numbers. In place of seven thousand thirteen, he would say (for example) Máximo Perez; in place of seven thousand fourteen, The Train; other numbers were Luis Melián Lafinur, Olimar, Brimstone, Clubs, The Whale, Gas, The Cauldron, Napoleon, Agustín de Vedia. In lieu of five hundred, he would say nine. Each word had a particular sign, a species of mark; the last were very complicated… . I attempted to explain that this rhapsody of unconnected terms was precisely the contrary of a system of enumeration. I said that to say three hundred and sixty-five was to say three hundreds, six tens, five units: an analysis which does not exist in such numbers as The Negro Timoteo or The Flesh Blanket. Funes did not understand me, or did not wish to understand me. &gt; &gt; Locke, in the seventeenth century, postulated (and rejected) an impossible idiom in which each individual object, each stone, each bird and branch had an individual name; Funes had once projected an analogous idiom, but he had renounced it as being too general, too ambiguous. In effect, Funes not only remembered every leaf on every tree of every wood, but even every one of the times he had perceived or imagined it. He determined to reduce all of his past experience to some seventy thousand recollections, which he would later define numerically. Two considerations dissuaded him: the thought that the task was interminable and the thought that it was useless. He knew that at the hour of his death he would scarcely have finished classifying even all the memories of his childhood. &gt; &gt; The two projects I have indicated (an infinite vocabulary for the natural series of numbers, and a usable mental catalogue of all the images of memory) are lacking in sense, but they reveal a certain stammering greatness. They allow us to make out dimly, or to infer, the dizzying world of Funes. He was, let us not forget, almost incapable of general, platonic ideas. It was not only difficult for him to understand that the generic term dog embraced so many unlike specimens of differing sizes and different forms; he was disturbed by the fact that a dog at three-fourteen (seen in profile) should have the same name as the dog at three-fifteen (seen from the front). His own face in the mirror, his own hands, surprised him on every occasion. Swift writes that the emperor of Lilliput could discern the movement of the minute hand; Funes could continuously make out the tranquil advances of corruption, of caries, of fatigue. He noted the progress of death, of moisture. He was the solitary and lucid spectator of a multiform world which was instantaneously and almost intolerably exact. Babylon, London, and New York have overawed the imagination of men with their ferocious splendour; no one, in those populous towers or upon those surging avenues, has felt the heat and pressure of a reality as indefatigable as that which day and night converged upon the unfortunate Ireneo in his humble South American farmhouse. It was very difficult for him to sleep. To sleep is to be abstracted from the world; Funes, on his back in his cot, in the shadows, imagined every crevice and every moulding of the various houses which surrounded him. (I repeat, the least important of his recollections was more minutely precise and more lively than our perception of a physical pleasure or a physical torment.) Toward the east, in a section which was not yet cut into blocks of homes, there were some new unknown houses. Funes imagined them black, compact, made of a single obscurity; he would turn his face in this direction in order to sleep. He would also imagine himself at the bottom of the river, being rocked and annihilated by the current. &gt; &gt; Without effort, he had learned English, French, Portuguese, Latin. I suspect, nevertheless, that he was not very capable of thought. _To think is to forget a difference, to generalize, to abstract._ In the overly replete world of Funes there were nothing but details, almost contiguous details. &gt; &gt; The equivocal clarity of dawn penetrated along the earthen patio. &gt; &gt; Then it was that I saw the face of the voice which had spoken all through the night. Ireneo was nineteen years old; he had been born in 1868; he seemed as monumental as bronze, more ancient than Egypt, anterior to the prophecies and the pyramids. It occurred to me that each one of my words (each one of my gestures) would live on in his implacable memory; I was benumbed by the fear of multiplying superfluous gestures. &gt; &gt; Ireneo Funes died in 1889, of a pulmonary congestion.
I love Borges. I swear I once read an essay comparing Clojure's concept of identity to Funes the Memorious, but I could never find it again.
Last I checked, there weren't even 99 solutions. :/
The problem with coding is you usually get the solution or the problem. I couldn't find the hints, but I did find solutions so you can double check your work.
Lens is Haskell's emacs.
You can replace it with the cofree comonad over a base functor data Declaration a = Class SuperTypes [a] -- ^ A class declaration. | Interface SuperTypes [a] -- ^ An abstract interface, or Objective-C protocol definition. | Mixin Type [a] -- ^ A mixin, or Objective-C category declaration. | Property UnderlyingType -- ^ A property declaration in a class. | Enumeration UnderlyingType [a] -- ^ An enumeration. | Constant UnderlyingType -- ^ A constant within an enumeration or class, or outside of any scope. | Function ResultTypes [a] -- ^ A function. | ClassMethod ResultTypes [a] -- ^ A class method or static member function. | InstanceMethod ResultTypes [a] -- ^ An instance method or member function. | Parameter UnderlyingType -- ^ The parameter to a function or method. | TypeAlias Type -- ^ A redeclaration of a type under a different name. deriving (Eq, Ord, Show) data Cofree f a = a :&lt; f (Cofree f a) newtype Mu f = In { out :: f (Mu f) } Now `Cofree Declaration Identifier` replaces your old `Declaration` and `Mu Declaration` is a `Declaration` without the annotation. Moreover you can swap out for other annotations as you go. [I talked about this on my blog back in 2009](http://comonad.com/reader/2009/incremental-folds/). 
All it needs now is a getter/setter implementation.
The guards get desugared in the order of their placement. [ (a,b) | a &lt;- as, b &lt;- bs, p a b ] has the same asymptotics as [ (a,b) | a &lt;- as, b &lt;- filter (p a) bs ] By placing the filters where they are in your original list comprehension, it appears you've already done this optimization. It is clearer if you look at the list comprehension: [ (a,b,c,d) | a &lt;- as, b &lt;- bs, p a b, c &lt;- cs, q a c, d &lt;- ds, r a d ] = do a &lt;- as b &lt;- bs guard (p a b) c &lt;- cs guard (q a c) d &lt;- ds guard (r a d) return (a,b,c,d) 
It's a fantastic reference for Clojure's sense of identity! It also suggests that we, the Haskell community, could make more of an effort to embrace obscure (and not-so-obscure) literature so that we are not pigeonholed as only leveraging obscure mathematics to drive our designs.
[Here it is](http://chrisdone.com/data-extra/Data-Eq-Extra.html).
But in `[ (a,b,c,d) | a &lt;- as, b &lt;- bs, p a b, c &lt;- cs, q a c, d &lt;- ds, r a d ]` you have to do the equivalent of `filter (q a) cs` a total `length bs` times per `a`, and you have to do `filter (r a) ds` a total of `length bs * length cs` times per `a`. In `[ (a,b,c,d) | bs' &lt;- p a ``check`` bs, cs' &lt;- q a ``check`` cs, ds' &lt;- r a ``check`` ds, b &lt;- bs', c &lt;- cs', d &lt;- ds' ]` each of those filters only happens once.
Yes, I found the solutions too, but I'd like to be able to get a hint without checking the solution. For the problems I get stuck on, you know?
This would be something that could work amazingly well in the School of Haskell with interactive exploration of the solutions.
The comonad is somewhat useful if you want to generate the annotation via an f-algebra automatically.
We should use a shorter name. Something like `on (==)` would do. :)
But `fs &lt;*&gt; []` doesn't short-circuit, that's the optimization introduced in the last step.
Now I'm following rampion's reasoning. I retract my objections completely. =)
Sounds interesting. Does that mean you do the following: data Expr e = Appl e e | Func Ident e | Var Ident and write an evaluation function eval :: Fix Expr -&gt; Fix Expr ..... I guess I convinced myself by writing this, but I'll leave it here nevertheless. :)
&gt; Kind of true. snap-core is a lot more then just an interface between a server and an app, which is essentially what is the purpose of wai, wsgi, rack, etc. Ok, I'll grant that the snap-core interface includes more than the wai interface. But I don't really want to get into a debate about whether the extra things included in snap-core mean that it's not an interface. &gt; I believe I've seen a discussion on why snap's not using wai. But I forgot the conclusion (if there was any). It's because the parties involved couldn't reach agreement on what the API should look like. &gt; To me it is clear that having a wai that is broadly embrased by a community is a good thing. I think this point is over-hyped. First of all, it ignores the paradox of choice the costs of having more granular packages. Second, even if the paradox of choice were not a problem, there just aren't many web server choices out there. And if there were, would being able to switch to a different one really matter much for your application? If the new Snap server ends up being faster than warp, would it really make a huge difference to switch to it? Probably not. But if the new server was 100x times as fast and it absolutely made a significant difference for you, then there would be no benefit to having a choice. And this assumes that the hypothetical 100x faster server even used the same interface. The most likely scenario would be that the 100x speed increase was achieved because the new server changed from using conduits (a dependency of wai) to some new package, tubes, which provided the quantum leap in cylindrical infrastructure that made the 100x improvement possible. So it wouldn't be a drop-in replacement anyway because the new server would be different enough that it could not conform to the wai API. 
I was unaware of this. I had to check it out for myself: ghci&gt; [] &lt;*&gt; undefined [] ghci&gt; repeat undefined &lt;*&gt; [] ^C (the second statement just looped infinitely, so I had to CTRL-C). Good to know! 
Not if you use `on` infix-ly. :) λ&gt; let isLongerThan = (&gt;) `on` length in "(==) `on`" `isLongerThan` "equating" True 
Good ideas! Adding an atomic "loop" instruction to `BrainfuckSource` is pretty much an optimization already when you evaluate it, so I think it'll make a good addition to the open-ended part.
The Scheme tutorial was kind of what I was aimig at, just with a much easier language.
Meh, character length is irrelevant. A function like `equating` produces fewer nodes in the AST, which means is simpler than `on (==)`. I like it, and would use it.
We're mostly going for 'jQuery' ;)
heh. sorry about it.
 class Fun a b where fun :: a -&gt; b instance (Eq b, Fun a b) =&gt; Eq a where x == y = fun x == fun y
Shouldn't we fix that then? Apparently it is useful. Or will that break things?
&gt; Ok, I'll grant that the snap-core interface includes more than the wai interface. But I don't really want to get into a debate about whether the extra things included in snap-core mean that it's not an interface. Its many interfaces in one package, cover so many things that it becomes hard to reuse outside of snap.. &gt; It's because the parties involved couldn't reach agreement on what the API should look like. Fair enough. Those things happen. &gt; there just aren't many web server choices out there. Well, not yet. In the beginning there weren't many servers in Ruby-land.. FastCGI was the only option. But this changed, and might change for Haskell too. Maybe some webserver in C, or an nginx extension.. No idea what the future will bring. But to me the standard on a server API will only allow for more choice. &gt; And if there were, would being able to switch to a different one really matter much for your application? If the new Snap server ends up being faster than warp, would it really make a huge difference to switch to it? Probably not. But if the new server was 100x times as fast and it absolutely made a significant difference for you, then there would be no benefit to having a choice. And this assumes that the hypothetical 100x faster server even used the same interface. The most likely scenario would be that the 100x speed increase was achieved because the new server changed from using conduits (a dependency of wai) to some new package, tubes, which provided the quantum leap in cylindrical infrastructure that made the 100x improvement possible. So it wouldn't be a drop-in replacement anyway because the new server would be different enough that it could not conform to the wai API. You certainly have a point.
You can also use `(+) &lt;$&gt; sqrt &lt;*&gt; square`
Using that is going to be no fun at all.
I'm a fan of irony. :)
Agreed. It reads as one word, and is conveniently manipulatable. It's also shorter to say, and readily has completion support.
The irony of comparing _character_ length, if you're going to do it, is that people almost never take key presses into account: e q u a t i n g 1 2 3 4 5 6 7 8 o n SPC Shift 9 = = Shift 0 1 2 3 4 5 6 7 8 9 I can type `equating` twice as fast as I can type `on (==)`, just like `fmap` (4) vs `&lt;$&gt;` (6).
[This](http://imgur.com/OB3PsKq) makes no sense what so ever. All of them are correct, or the fact that they aren't ticked is correct (I asssume that this is what you mean)? To clear this up I think the incorrect answers, regardless of if they were ticked or not, should be marked incorrect (or not at all?) and only if an answer is correct should it say "(Correct)" when you click check. Furthermore, the tick boxes could be removed/disabled (a better choice so the student can still see what answer they picked for certain) once the answers are checked. Maybe add a "Reset Question" button? Also maybe a disclaimer that questions are multiple multiple choice would help too.
No, the laws don't always work out. Take, for example, `ZipList` (it doesn't have its own `Monoid` instance, but let's assume it has the same one as `[]`). Your definition of `(&gt;&gt;=)` ends up being the same as `concatMap`, which sounds fine, since that is consistent with `[]`'s `Monad` instance. However, your `return` is going to be the same as `ZipList`'s `pure`, which does not form a `Monad` along with `concatMap`.
Hmm, maybe? It would be inconsistent with `ap` (obviously, as the current definition is just `&lt;*&gt; = ap`) as `&gt;&gt;=` can't "know" that its second argument will always return `[]`. But changing `&lt;*&gt;` to [] &lt;*&gt; _ = [] _ &lt;*&gt; [] = [] fs &lt;*&gt; xs = ap fs xs seems fairly innocuous as these things go, as `ap` would force the same parts of the input in the same order... the short-circuiting `&lt;*&gt;` would really just exit earlier. Changing stuff like this is always a bit scary, though.
&gt; I can type equating twice as fast as I can type on (==), just like fmap (4) vs &lt;$&gt; (6). `fmap` ` f m a p ` (6) &lt;$&gt; SHIFT , $ . UNSHIFT (5) On my US keyboard.
Tim Williams gave an excellent talk about this several months ago at the London HUG. His slides can be found here [1] and one of the examples is annotating an AST with information using a cofree comonad with a cata/paramorphism. Here's a nice list of all the links from the talk [2] (I find the origami programming to be the most approachable). [1] http://www.timphilipwilliams.com/slides.html [2] https://gist.github.com/threedaymonk/5265258
There's also the problem that the type variable `a` is not bound in the instance head. Do you mean the currently forbidden instance (Applicative f, Foldable f, forall a. Monoid (f a)) =&gt; Monad f ? I'd love to be allowed to express such higher-order constraints, but it's not so obvious how to keep instance inference coherent if you have them. Worth thinking further about what might be ok and enough. Meanwhile, for the actual question, here's another simple counterexample. Choose any monoid on `Bool` and note that `Const Bool` satisfies the requirements. Then observe that `&gt;&gt;=` for `Const Bool` cannot possibly use its second argument and is thus bound to fail the laws. In situations like this, it can be handy to remember | Functor | Applicative | Monad -----------+---------+-------------+------- Const Void | yes | no | no Const Bool | yes | yes | no Const () | yes | yes | yes as a bunch of simple separating examples.
 instance Monad (Const ()) where pure _ = Const () _ &gt;&gt;= _ = Const () return a &gt;&gt;= f = f a = Const () m &gt;&gt;= return = return m = Const () (m &gt;&gt;= f) &gt;&gt;= g = m &gt;&gt;= (\x -&gt; f x &gt;&gt;= g) = Const () Basically, the laws are all correct because they are all equalities, and as there is only one value of type `Const () a` they must be equal.
I never use infix functions, but that's probably irrelevant to the discussion anyway. Addressing the chord issue: I've determined that runs of chords are a substantial factor in RSI, so I tend to count chords as separate key presses because they cost about as much or more for me. I.e. if you're counting fingers that are doing work: Shift , $ . 1 2 2 2 Consider the cost on your fingers for a tap vs a hold. I wouldn't say that the period here SHIFT , $ . UNSHIFT ^^^^^ comes for free. The opposite. Not only is it a prolonged chord, but one that requires one-handed chording, which means that one hand is doing twice the work, too. That sounds like over-complicating a simple thing… but to me, operators over-complicate simple words.
Yeah, I definitely understand. With my past experience, this may be more easily accomplished by working with someone else. Either a classmate or a tutor, if they are a free resource. If nothing else, you work with what you have and develop a new way of studying in which you still retain the information from the problem even if the answer was given to you. The whole situation is a bit more realistic than having hints for problems in real life. You don't waste time reinventing the wheel, and once you get past the feeling like your creativity and problem solving skills are being stifled, you realize there's so much more unknown still that you can use what's already there to get to the cutting edge and pioneer new horizons.
Not to mention that control+P completes `equating` but does not complete `on (==)`. :)
There's always `Alternative` instead of `Applicative`+`Monoid`.
Recently I found a problem with this workflow. I started using a more recent version of cabal-install than the one that comes with any version of GHC (which I highly recommend). When I prepend to the PATH, that clobbers my preferred version of the `cabal` command and reverts to the old version that came with GHC. For now, my solution is to put the `cabal` command I want by itself in a directory with an easy-to-type path, like `/usr/local/cabal`, and I prepend both that path and the HP path to `PATH`. A bit messy, but it works.
What you're probably looking for is the fact that the composition of an adjunction is a monad.
But Haskell is Turing-complete and you can write a Haskell interpreter in Haskell. So yeah, the halting problem still applies
The URL for this series has been updated to: https://www.fpcomplete.com/user/edwardk/revisiting-matrix-multiplication/ This group includes all of the parts. This part in particular is available from: https://www.fpcomplete.com/user/edwardk/revisiting-matrix-multiplication/part-1 
The URL for this series has been updated to: https://www.fpcomplete.com/user/edwardk/revisiting-matrix-multiplication/ This group includes all of the parts. This part in particular is available from: https://www.fpcomplete.com/user/edwardk/revisiting-matrix-multiplication/part-2 
The URL for this series has been updated to: https://www.fpcomplete.com/user/edwardk/revisiting-matrix-multiplication/ This group includes all of the parts. This part in particular is available from: https://www.fpcomplete.com/user/edwardk/revisiting-matrix-multiplication/part-3 
The URL for this series has been updated to: https://www.fpcomplete.com/user/edwardk/revisiting-matrix-multiplication/ This group includes all of the parts. This part in particular is available from: https://www.fpcomplete.com/user/edwardk/revisiting-matrix-multiplication/part-4 
The URL for this series has been updated to: https://www.fpcomplete.com/user/edwardk/revisiting-matrix-multiplication/ This group includes all of the parts. This part in particular is available from: https://www.fpcomplete.com/user/edwardk/revisiting-matrix-multiplication/part-5 
1. Many of the programs are highly numerical in nature, and therefore doesn't gain much by using a higher level language. 2. Quite a few of the Haskell implementations in the benchmark game use unidiomatic (and longer) code to squeeze out more performance. In Haskell, when the short and obvious solution isn't quick enough, you can often optimise for performance by helping the compiler out and writing a less obvious – and oftentimes longer – version.
I don't think at all that Haskell optimizes for length of code compared to Ruby/Python, but instead reduces assumptions. The Hs code is far less likely to be wrong during development, more likely to notice and consider edge cases, more likely to reach that level of quality quickly. None of those are captured in the shootout though. It's terseness always feels like a happy accident of high-level expression to me when it occurs.
It's a question of practice. I'm getting very good at typing `&lt;$&gt;`.
My love of efficient bit-twiddling hacks is a fetish I picked up in a bygone era as a c programmer. Here it is a matter of necessity. If I ignored those concerns then we wouldn't be any better off than if we'd just written the naive matrix-as-pair-of-intmaps that we can write with, say, my `linear` package. I will say that in general, the initial implementation was 2 orders of magnitude slower than the implementation that uses these heaps, and switching to a smarter bit twiddling hack that I'll describe in the next part to calculate most significant differences implicitly made a further factor of 2 improvement in wall-clock time.
Wow, so it seems that those things were quite important! As a reader, it's a little bit like hearing about how you're mining for gold, and then you go on to describe the different parts of the shovel, and somehow forget about the gold. Or maybe it was all about the shovel after all ;). Again, just as a reader I'm used to seeing something like a claim "We do X for performance, and here's a nice implementation" then eventually the implementation is justified using some performance measurements. But maybe I have just misunderstood the target/purpose of the article :)
well, no. it's a question of keyboard layout. 
and he does not adress the main point anyway. we -kind of- get new obscure vocabulary, which does not abstract that great. But i still think that a, equating still composes pretty ok, and b, that it is a very good name.
Well I appreciate the work you put in, it's quite informative!
I agree, but find frequently that users of scripting languages will refer to terseness in a different way. JSON parsing is my pet example right now where something like `v['key1']['key2'][3]` is a terse example of JSON access that hides many assumptions about the structure of the JSON blob. This is less code that makes you think more compared to a full `FromJSON` parse.
1. Yes 2. I prefer 'where' to let where-ever possible, so no. I think it's nice to have consistency between comprehensions and guards, though. `PatternGuards` are a nice addition here too that helps with that. 3. For a single guard, yes, but I think if you have multiple guards, a let will be local to a single guard, whereas a where will scope over all the guards. Besides the consistency I mentioned between guards and comprehensions, which is nice, I like that every name binding clause in Haskell can have a "where" to build intermediate computations from those bound variables. For `case` alternatives it is especially important because there is no other way to scope over all the guards.
&gt; I prefer 'where' to let where-ever possible That's interesting statement. Why?
Regarding 1 and 2, can you show me an example? --- Regarding 3, it seems you're right. import System.IO.Unsafe x a = case a of _ | m (), a -&gt; a | m (), not a -&gt; a where m = unsafePerformIO $ do putStrLn "." return (const True) m' _ = unsafePerformIO $ do putStrLn "." return True With `m` in the guards, `x True` and `x False` only show one stop. With `m'` in the guards, `x True` gives one and `x False` gives two stops. So I guess the `m` "thunk" is shared between the guards.
see http://learnyouahaskell.com/syntax-in-functions
I like the TIOOWTDI^1 ideal (as hard as it is to reach in Haskell). So I find it nicer to consistently use the same name binding syntax. Also, `let` syntax seems heavier, involving 2 tokens and more layout rules. `where` is just 1 token, with a simple layout after it. ^1 There is one obvious way to do it
1. https://github.com/Peaker/lamdu/blob/master/Lamdu/GUI/ExpressionEdit/HoleEdit/Results.hs#L273 2. The whole code-base. We use `let` in do blocks extensively (because otherwise you cannot address the previous do-bound variables).
1. So you use it rather to use bindings from the patterns, rather than to introduce extra bindings for the guards, right? Also, that example doesn't really seem to be much of a case for `where`-inside-alternatives, as it can be easily transformed to a `where`-inside-function-binding. Understandable, though, since you came up with the example so quickly :) 2. I use it a lot in `do`-blocks too. I know it's nice to have the same syntax in both guards and all comprehension-like constructs, but I think it gets really ugly in guards.
I would guess that benchmarks are unrealistic because they are small and very tightly focused. Code size is a very good predictor of development time and maintenance issues, mostly independently of language level - if a language lets you write less code to handle the same job, usually it will be quicker to develop and less buggy in that language. So far as I can tell with limited experience, Haskell does well for that. However, Haskell - like all languages - often needs more complex code to achieve the best possible performance, rather than just good-enough. It even has some laziness-managing issues that most languages don't have, and some boxed-type issues that C and C++ don't have. These issues can be worked around - strictness and unboxed types can be forced - but it needs some extra code. In normal applications, this kind of thing is limited to key parts of the code, so the extra code is a small proportion of the whole application. In benchmarks, things are I guess a bit different. 
Personally, in this case, I have no objection to adding it. My Borges quote was an attempt to articulate the general position of why we might not want to add every two and three operator combination to the standard library, which is admittedly a bit of a straw man argument.
Of course you can get rid of any particular example, but I'd find it annoying if I had to transform every pattern match to a function application just to allow where syntax.
Being terse often comes at the cost of the libraries you're using being very verbose because they are so general. All the boilerplate for type classes adds up to quite a bit in a number of packages. The tendency to decompose functions into many smaller functions can have this effect too, but the type level correctness that you gain evens it out in my opinion. This allows a lot of compile time checks in what you implement meaning your program won't blow up in your face after it's compiled 99% of the time. Most problems (segfaults, failed assertions) I've had have been my own incorrect assumptions when interacting with FFI. The fact that if it compiles (once you get it to ;) ) it will work is very nice and makes developing a lot easier. No more "why is this NaN/undefined?!" with the static type checker.
It's the top-down approach. This is how I like to read a program. The question is: What does this program do? * Sorts the people by rating OK, tell me more. * The sort is a stable sort on a list of people by a rating calculated by their score. OK, tell me more. * People are defined by a name and integer score. * The rating is defined by the person's position in the list and their score. OK, that's enough information. So when I read: program = sortBy (comparing rating) people where people = … rating = … I'm happy. When I read in the opposite order (bottom-up), where all the details come first, that don't mean anything to me, and I'm only given what the program actually does at the end, I become really impatient and annoyed at the author. `let`/`where` essentially sum up this philosophical difference. For me, the only use of `let` is for sub-expressions and in `do` notation.
I thought that about ST. I've never thought of it as a single thread.
&gt; For me, the only use of `let` is for sub-expressions and in do notation. And what do you think about `let` in guards?
Good point. I consider SHIFT to be low-overhead because I use it all of the time (when writing code and when writing prose). It seems you have a different view. If we were talking about CTRL or ALT or META, I'd say the cost is 3 or 4. ;P (I am a Vim user, not an Emacs user.)
`let` seems better for naming subexpressions: f x0 y0 x y = let dx = x0 - x dy = y0 - y in sqrt (dx ^ 2 + dy ^ 2) Or factoring repetition: f x0 y0 x y = sqrt $ (let dx = x0 - x in dx * dx) + (let dy = y0 - y in dy * dy) `where` is nice for making local definitions: f :: Double -&gt; Double -&gt; Double f x y = sqrt (dx ^ 2 + dy ^ 2) where dx, dy :: Double dx = delta x dy = delta y delta :: Measure Double -&gt; Double delta (Measure n0 n) = n0 - n Related but different concepts.
That's definitely true and sort of outlines the fundamental terseness challenge in Haskell for me: higher level components against greater encodings of the logic of your program.
I did a [case study](http://eyallotem.blogspot.co.il/2013/05/comparing-python-and-haskell.html) on a few Python programs that I transliterated into Haskell. Even without using Haskell idioms much, but rather (mostly) the same abstractions from the Python code, Haskell still managed to stay on par with same code length (95-110% of code size). At the same time, Haskell gave much better performance, and much better safety guarantees, too.
Do you have any guidance about good articles to read about what comonads are? 
&gt; scheme &gt; brainfuck &gt; much easier language -_-
Before someone complains, note the logarithmics only kick in ***after*** the `LazyLength` has been calculated: -- suppose n &lt;&lt; m as = replicate n () bs = replicate m () x = lazyLength as y = lazyLength bs minBound &lt; x -- O(1) x &lt; y -- O(n), forces x and the spine of as and the partial calculation of y and the spine of bs x &lt; y -- O(log n), reuses the precalculated parts of x and y y &lt; maxBound -- O(m - n + log n), forces the full calculation of y and the spine of bs y &lt; maxBound -- O(log m), reuses the precalculated parts of y I don't doubt someone else has come up with this before me, so if you know who, I'd be happy to learn. 
The literature on comonads tends to be dreadfully technical. Maybe I'll see about writing up something for the School of Haskell site based on the usual way I run through it with folks on #haskell.
Can you explain what the use of this is? I don't get it.
Do you really want a path to the running executable? Would cabal's data directory or the current directory or XDG paths or something do? Looking in the path of the running executable for stuff is usually the wrong approach on unix likes.
Suppose you have a collection of lists, and want to find the shortest one. You could calculate the `length` of each, but this would require forcing the spines of all the lists. Using `length` to do this will take O( n_1 + n_2 + ... + n_k ) where n_i is the length of the i'th list. If one of the lists is infinite, `length` will never complete. If instead, you use `lazyLength`, then you can find the minimum in O( k * min( n_1, n_2, ..., n_k ) ), and you can compare finite list lengths to infinite list lengths freely. Lists are a unary representation of their own length, so you don't **need** `LazyLength` for this. What it does provide, however, is an efficient way to memoize this lazy length calculation, so that the calculated length can be reused later at a cost of O( log n ) rather than the O(n) time it would to step through the unary representation again.
&gt; Do you really want a path to the running executable? Yes, because I actually want to run that executable in order to spawn worker processes.
That would be awesome :D
what's wrong with a classic fork?
Unless I have missed something, "classic fork" only works for Posix operating systems, which in particular does not include Windows. (Edit: minor change in wording)
oh. yes. that might be a problem if you need to support windows.
Because Basili and Perricone, 1984. Among other things.
&gt; If we were talking about CTRL or ALT or META, I'd say the cost is 3 or 4. ;P (I am a Vim user, not an Emacs user.) [I more or less agree.](http://chrisdone.com/posts/emacs-key-analysis) (I am an Emacs user, not a Vim user.)
Yes, it’s not the content of the code I was trying to convey, but the shape. Edited to flesh things out a bit.
Can you publish this to hackage?
Cool! Reminds me of a toy idea I've been kicking around. Say you have an expensive comparison function, and you want to do a sort. Aside from the expensive comparison you also have a quick pointer equality. Memoizing the comparison in the obvious way might not help much, but you could maintain a cache that understands the transitivity of the ordering relation. Maybe there is an obvious clever way to do this? I've been thinking about using an algorithm for maintaining the transitive closure of a dynamic graph.
I think it would be cool to have the editor show some faint indicator of some kind of lexical distance for variables. Parameters, locally bound variables, variables defined in the same module, etc. Sometimes it's not obvious where I will expect to find the definition of something. Should I Hoogle or just keep reading? Especially highlighting of symbols defined in the same module would be nice.
If you turn this into a lazy number type, with a Num instance, this could be useful for more than just lazy lengths.
If you can decompose your expensive comparison `f` into a cheap comparison `g` and an expensive transformation `h`: f a b == g (h a) (h b) Then you can state: sortBy f as == map snd . sortBy (g `on` fst) $ map (h &amp;&amp;&amp; id) as This way, you'll do the expensive transformation `h` O(|as|) times and the cheap comparison `h` O( |as| log |as| ) times as opposed to doing your expensive comparison `f` O( |as| log |as| ) times. There's a name for this kind of replacement, but it slips my mind at the moment.
After it's been thoroughly bikeshedded, sure :)
I'm always uncomfortable making `Num` instances for naturals, since there's no legitimate negation, and subtraction can overflow.
Schwartzian Transform.
Is it a Schwartzian transform?
Nice. Actually as sjoerd_visscher says, it's a general lazy natural implementation. http://www.reddit.com/r/haskell/comments/1l3c19/want_to_compare_lists_by_length_without/cbvj6ty 
I pinged ReinH to let him know. It looks like he just forgot to flag it public.
Ah, awesome for taking the initiative! Would it be possible to have this on iTunes as well?
refactor-bot says &gt; sortBy f == map snd . sortBy (g `on` fst) . map (h &amp;&amp;&amp; id) [FYI: I just like refactoring things. I'm not trying to make any kind of point :)] [edit: debugged refactor bot.]
This is cool. In the past I've done lazy comparison with: -- requires Data.Ord and Control.Monad compareLength :: [a] -&gt; [a] -&gt; Ordering compareLength = comparing void Clearly the `LazyLength` representation is more desirable for lists of nontrivial size.
Sometimes I really think there should be another language shootout game which focuses on writing idiomatic and readable code. Focusing on performance IMO just ends up with every submission trying to bend their language implementations into generating the optimal machine code for a given algorithm.
&gt;Others are free to use it And they don't. That's the point.
The first and last thirds of this podcast are pitched at a much more general audience. I confess I get a little lost in the weeds in the middle. :)
I tried a similar thing, but for a different reason. Caculating list length lazily can lead to a much simple solution to the "linear pretty-printing problem" (for a latest effort/summary, see Oleg's simple generator http://okmij.org/ftp/continuations/PPYield/#pp). And it worked. I was kind of surprised that nobody tried the "lazy natural" approach to this in-famous problem, at least not in the literature that I'm aware of. Using lazy comparison requires the least modification to the naive algorithm, but at the expense of obscuring the space/time complexity even further. Not an ideal solution, but nevertheless an interesting one.
You could construct a compatibility layer, though, using `fork()` where available and `system()` otherwise.
Oi! I had never even *looked* at Control.Exception.Lens before. That just got Lens included in all of my hairy IO code.
Absolutely! Just setting up the feed and such.
Lenses absolutely make more sense once you start experimenting with them.
Looks good. Thanks!
Glad everyone is enjoying it. It was a lot of fun to record. It'll be on iTunes soon!
It was still processing on YouTube's servers. I wasn't expecting anyone to post it to reddit until I woke up :P
It's waiting in some iTunes approval queue somewhere.
So make one. Edit: Why the downvote? Is *doing* so hard to imagine?
That looks really handy. Thanks!
jQuery for Haskell values is particularly compelling to me. I was literally sitting down last week in the REPL faced with obnoxious nested data structures, many of the nodes of which didn't even have field accessors, and I was thinking how I really need to write a jQuery-like thing so I can just drill down trivially like "first slot, second list item, third slot, the first field called `foo` in the whole tree, fourth slot" and be done.
Is this the same talk that's been doing the rounds, or is it a new one?
Direct link to youtube: [http://youtu.be/1PhArSujR_A](http://youtu.be/1PhArSujR_A). [The actual link is just an intro](http://wayback.archive.org/web/20130826032315/http://functionaltalks.org/2013/08/26/john-carmack-thoughts-on-haskell/).
Yeah, this is his quakecon 2013 talk
Cf. http://hackage.haskell.org/package/time-lens
ZOMG lenses!
So *that*'s what Chris was up to; a few weeks ago on Twitter he mentioned that the "Haskell From Scratch" videos would no longer come out every week because of another project he was working on.
Yes, I saw it, but by that time I was already done with mine (note to self: always do the research first). I sent the guy an email to see if he's interested in merging the two.
They use different lens libraries, so I'm not sure how mergable they are. But I am open to any ideas. (I am the guy, though unfortunately I haven't received any emails from you yet.)
Oops, sorry, I thought you were talking about http://hackage.haskell.org/package/lens-time. Which is also based on /u/edwardkmett's lens package and is very similar to mine, but a few weeks older and looking a bit unfinished. Your library seems to be much more mature. And yeah, it would be better to have only one such library, but I don't know whether it's possible with the different lenses (without too much hassle). But I am very much open to cooperation if there are good ideas how to unify these!
What's problem with something like O(N min(x1, x2, ..., xN))? shortest xss = shortest' xss 0 where shortest' xss n = case reduce xss of (False, rss) -&gt; shortest' rss (n + 1) _ -&gt; n reduce ([]:_) = (True, []) reduce [] = (False, []) reduce ((_:xs):xss) = case reduce xss of (False, rss) -&gt; (False, xs: rss) r -&gt; r Thanks!
Why not use genericLength and Natural from the numbers package?
I just wanted to drop by that as a Haskell newbie who does't quite understand the middle third of the talk, I think edwardk is cool as hell and very inspirational.
If you're like me, he inspired you to want to understand the middle third of the talk. :D
I'll have to give it some time! I can understand it, but only on a very, very cursory level.
Russel O'Connor's multiplate paper really helped me out here: http://arxiv.org/abs/1103.2841
Thanks, that looks very well written. I'll give it a shot tomorrow if I can! Edit: Second thanks. I have not been through all of it yet (I put it down to digest the concepts I've gotten so far) but I love the clear introduction to lenses and comonads!
I don' t think refactor-bot is right. h :: a -&gt; b first h :: (a,c) -&gt; (b,c) h &amp;&amp;&amp; id :: a -&gt; (b, a)
Fire up GHCI and play around with things! It helps!
I cut it all off at the beginning of the year. I started walking a lot and by the time I'd get to where I was going it was all over the place from the wind and I looked like a homeless person. Rather than fend off people offering me change, I decided it was time to let it go.
&gt; There's a name for this kind of replacement, but it slips my mind at the moment. As others've mentioned, in Perl it's called the Schwartzian Transform. In Lisp it's known as decorate-map-undecorate (or decorate-sort-undecorate,...)
That's similar to the version in [Data.List.Extras.LazyLength](http://hackage.haskell.org/packages/archive/list-extras/0.4.1.1/doc/html/Data-List-Extras-LazyLength.html)
One of the main problems with using lazy naturals (i.e., `data Nat = Zero | Suc Nat`) is that they're extremely expensive--- it's a unary encoding implemented by pointer chaining! That's why I implemented [Data.List.Extras.LazyLength](http://hackage.haskell.org/packages/archive/list-extras/0.4.1.1/doc/html/Data-List-Extras-LazyLength.html) way back when.
You're right! Refactor-bot was wrong. Oh well, at least the code looked nice :)
It looks great this way, just fyi!
So in the context of Syntax/Semantics duality the Galois connection operates on inclusion posets? A bigger theory includes smaller theories, etc?
Somehow, you look like Yaron Minsky.
OK, Apple just approved it.
This was a lot of work to prepare, and I learned a fair amount in the process. I hope I managed to convey some of the amazing things at the foundations of programming language semantics. To the extent that I didn't, the references I provide at the end of my slides are hopefully a very good set of places to start. Slides here: http://gbaz.github.io/slides/dsl-slides-07-2013.pdf And as always, I'd like to remind everyone in the NY area that the next NY Haskell meetup is next Wednesday, August 28. It will involve robots and theorem provers. http://www.meetup.com/NY-Haskell/events/133966332/ 
I like that description of the lens library as "jQuery for ADTs"
We need a better implementation of natural numbers then. :) But that said, if you're just comparing the lengths of two lists once, I doubt there's any gain here. Over stupid naturals, I mean. 
[I agree, for one time use, it's not necessary](http://www.reddit.com/r/haskell/comments/1l3c19/want_to_compare_lists_by_length_without/cbvff4h). The real use case is if you want to use and reuse a partially-evaluated length, say when maintaining a min-heap of lists.
Works great on OS X 10.8.4 with GHC 7.6.3.
I don't think there's any problem with this at all, if you're only processing xss once. (Well, one nitpick might be that `shortest'` isn't strict in n, so you'll build up a chain of thunks with `shortest' rss (n + 1)`, but that's easily fixed with bang patterns or a `seq`). The only advantage `LazyLength`has is when you need to reuse those lengths. For example, suppose you modified `shortest` to replace the shortest list in `xss` with a new list of unknown length. This would still take O(N min(x1, x2, ..., xN)). If you replaced the smallest list M times, then this would take O(N min(X_1) + N min(X_2) + .. + N min(X_M)) (where X_i is the the x1, x2, ... xN for the i'th iteration). If you instead calculated `LazyLength`s for each list, and compared them using those, then that would be faster: data Measured a = M { list :: [a], length :: LazyLength } data Zipper a = Z { left :: [a], focus :: a, right :: [a] } -- reversed list prefixes rInits :: [a] -&gt; [[a]] rInits = scanl (flip (:)) [] replaceShortest :: Measured a -&gt; [Measured a] -&gt; [Measured a] replaceShortest _ [] = [] replaceShortest p' ps = reverse ls ++ p' : rs where -- find the position of the shortest element Z ls _ rs = minimumBy (compare `on` (length . focus)) zs -- replace each element with a zipper pointing to that element zs :: [Z (Measured a)] zs = zipWith (\ls (r:rs) -&gt; Z ls r rs) (tail $ rInits ps) (init $ tails ps) -- this takes O( N min(X_1) ) ps1 = replaceShortest p0 ps0 -- this takes O( min(X_2) + N log(min(X_1)) + N(min(X_2) - min(X_1))) ps2 = replaceShortest p1 ps1 -- this takes O( min(X_3) + N log(min(X_2)) + N(min(X_3) - min(X_2)) ) ps3 = replaceShortest p2 ps2 -- ... -- this takes O( min(X_M) + N log(min(X_{M-1})) + N(min(X_M) - min(X_{M-1})) ) psM = replaceShortest pMminus1 psMminus1 So using `LazyLength`, the total run time is `O( N max( min(X_i) ) + min(X_2) + min(X_3) + ... + min(X_M) + N log(min(X_1)) + N log(min(X_2)) + ... + N log(min(X_{M-1})))`, better than before (and of course we could do even better if we used a heap). 
Nice. This is very much the tutorial I was planning on writing for lens-aeson. Now I'll have to write on on semi-indexing json instead. ;)
Well, I grant that this could be confusing. I added explicit messages that say "You are right/wrong. Correct answer is X" after pressing the check button. I'm not convinced that I should disable the checkboxes, since one use case for these is to work as "TA summoning signal". When there is red on students screen, we sit down and discuss why the ticks go where. And it is nice to be able to tick answers to be correct after they've been explained.
liyang's thyme package is really well thought out and addresses many of the issues with the standard time package. We had some lenses for time in the stock lens distribution, but I decided to retrench and focus on the things that we could do within the confines of the laws for the most part and decided to leave time as a task for outside libraries. Too many fiddly points in the design space and it was causing me to rapidly iterate somewhere in the 1.x-2.x era.
A random observation about lenses: So, the lens package package contains a bunch of combinators for working with lenses in non-monadic code, and a bunch of combinators for working with lenses in the context of the State monad. I'm going to talk about the latter. Some of the combinators are very handy for dealing with maps and sets without having to remember what the appropriate "lookup" or "update" function is, or remember what order the arguments are. For instance: addLike :: Uid -&gt; Id -&gt; S () addLike uid id = do dislike . at id . traverse %= (S.delete uid) like . at id . traverse %= (S.insert uid) This is (a simplified version of) the code to mark that a particular user (identified by Uid) likes a particular object (identified by Id, which is a more general type that can identify users, comments, tags, etc..). Basically, we remove that user from the set of users that dislike that thing, and add the user to the set of users that do like the thing. At the top level, I have a record with a bunch of maps. Some of these are called dislike, like, users, etc... If we look at the first line: dislike . at id . traverse %= (S.delete uid) That "at id" does the map lookup behind the scenes. The " . traverse" iirc handles the case where it might be Nothing, and then we apply a function to what it finds. The next line is similar, except we add the user to the set. The problem is, what should we do if either of those "at id . traverse"s fail? We shouldn't ever be looking up an Id for an object that doesn't exist, and if we do, we should stop what we're doing immediately and return an error. Instead, if the thing isn't found, that line is a no-op and we fail silently and proceed, even though our data structures were in an unexpected state. The "S" in the type signature is an alias for an EitherT State monad, which does have a graceful way to represent failure: we have the "left" method, which causes the whole computation to fail in a way that we can easily catch. Unfortunately, the lens combinators were defined in terms of the State monad, not an EitherT State monad stack. So, they aren't able to access the additional functionality that lets us fail the right way. I run into a somewhat similar problem with lookups: "x &lt;- use $ foo . at bar" where foo is the accessor for a Map or Set will return a Maybe value, which we have to pattern match against. If the appropriate combinators were designed to work with EitherT State, we could just return a plain value, and throw an exception if it wasn't found. Of course, I don't expect the lens library to be re-written around my particular use case. I'm sure there's plenty of places where EitherT State wouldn't be appropriate. I wouldn't want to force the lens package to take sides in the ErrorT vs EitherT debate. However, perhaps there's room for an EitherT State lens combinator add-on module or something. tl;dr: The lens package has some awesome combinators that make working with stuff inside a State monad a lot easier, but there are some cases where you really wish you could throw exceptions via EitherT State or some equivalent.
I think you may have encountered an idiot :) Fixed the typo. Thank you for letting me know. PS. We actually have cardboard TA-summoning signs in our classrooms. 
Very nice show, looking forward to listening to the next one. :-) I did not really understand anything Edward said, but well, I know basically zero math and have only used getters and setters in lens. xD
D'oh!
Yes, LazyLength would be useful in some cases. Thank you! :)
I still want a `reflection` tutorial! The semi-indexing JSON paper is readable enough ;) I mostly ask for reflection though because while I "get" the types, I haven't managed to build up a real intuition yet, but it seems to play an important part in the advanced Haskell'ers toolbox.
I'm only 10 minutes in at the moment, but this talk is fantastic! You have a fantastic speaking personality, and I look forward to enjoying the rest of this talk :)
White text on light pink background? Are you sure that's wise?
This is very strange to me. The applicative constraint sort of indicates to me why. You don't need this, you can always just use the applicative instance directly, and recover the constructor for your more general FooT (composed with your 'construct') fooT it st = Foo &lt;$&gt; it &lt;*&gt; st Worth noting that if you needed to use FooT for something other than applicative interpretation, I would perhaps see the utility, but I think parametricity prevents you doing anything too fancy. Your generate arbitrary Foo examples can just be done with `Foo &lt;$&gt; arbitrary &lt;*&gt; arbitrary`.
The deduction part only looks confusing with all those extra symbols. I wish good luck to your students to understand that.
Though there are only two extra symbols, I'd be grateful for better ideas.
Right. This is just Traversable, which can be derived.
I find "better idea" to be a vague concept :) I don't know the level of your students, or how to teach in general. But when I helped some of my friends Haskell, and form their type intuition I didn't go invent a new notation for that. I've ran through gradually harder compositions. I base my initial comment on my knowledge of Haskell, and that from that point of view it's confusing. If I where to use some visual cue, I'd rather use colours for emphasis.
Thanks, I will check it out!
Wow, this looks great, I will definitely check it out! It's a bit hard to find though...
In the preview of the video it looks like that, but when watching, instead it's a medium gray. I agree that a good amount of contrast is desirable, though.
There isn't any of that in the actual video, it's just a glitch somewhere between youtube's thumbnailer and the original ogg vorbis encoding.
Lens state is implemented against the MonadState interface, so it can be used in StateT, RWST, EitherT State .... whatever that has an instance for MonadState. You don't need to patternmatch on maybe, rather use the great `note` function from errors (assuming StateT Either monad): let baz x = use (at x) &gt;&gt;= lift . note "Not found" runStateT (baz "bar") mapWithBar Right (...) runStateT (baz "foobar") mapWithBar Left "Not found" Btw I know it is a matter of semantics, but I find StateT Either more useful than EitherT State
I am a professional programmer, a reasonably experienced Haskeller, and I have a B.S. in Discrete Mathematics. And it took me way too long to understand what this page was trying to communicate. Partly the fonts... the header fonts in particular are very hard to read. But beyond that, `(:t x) -&gt; (:t x+x)` just seems like a strange way to describe a type. And the use of deduction for syntax decomposition seemed crazy to me. Why not just use an AST? http://i.imgur.com/ObvdTfD.png (pardon the mspaint)
I've also found uses for records with fields enclosed in a higher-kind wrapper which led me to formulate a kind of Functor-like and Applicative-like type-classes for manipulating these records. My related question in StackOverflow is here: http://stackoverflow.com/questions/18052158/functors-and-applicatives-for-types-of-kind One thing to note is that you don't necessarily need `Foo` as a separate type as it is essentially isomorphic to `FooT Identity`.
Previously leaving this part vague turned has turned out really badly. Especially the later parts of the course were riddled with problems with the basic types. This is perhaps an overcompensation for that problem. You can perhaps teach this with plenty of examples, but I failed at that. The deduction-based syntax did work quite well in my previous course and for advanced students, so I decided to try it on an earlier level. That's not to say that I expect this monstrosity to succeed fully either and everyone will still be reading LYAH on the side. 
Take a good look at the 'finally tagless' method of writing interpreters -- and remember that type classes are essentially fancy sugar for special kinds of records. This is also related to some generic programming techniques (whose name escape me right now... but one of the SYB papers used this too).
Fair enough. I do want to showcase "advanced library design" for things like hyperloglog at some point, which abuses reflection to make it so you can have HyperLogLog $(5) as the type of a hyperloglog set where we want that many bits per bucket, and/or use reflection for the parameter to HyperLogLog to make it tunable at runtime .. and can use `HyperLogLog 5` directly on more modern GHCs, etc.
I heard that line and really wanted to write *something* as Prisms were quite opaque for me as well until exactly when they became simple. I'm finding every time I write something about lens it's quite a challenge however as all the pieces really fit together and improve one another. I end up writing so much hand waving in the goal of practicality.
 &gt;&gt;&gt; failover (element 3) (*2) [1,2] :: Maybe [Int] Nothing &gt;&gt;&gt; failover _Left (*2) (Right 4) :: Maybe (Either Int Int) Nothing &gt;&gt;&gt; failover _Right (*2) (Right 4) :: Maybe (Either Int Int) Just (Right 8) `failover` will try your `Traversal` and when it fails to match anything will give you back `Nothing`. As an aside, `at x . traverse` can be replaced with `ix`.
 &gt; "foo" ^? each 'f' Shouldn't this be Just 'f' ? Also _String :: Prism' Value String _Sting = prism' aString String The tutorial itself is pretty good and informative, thank you for writing it.
Fixed, thanks!
This is really nice! I would have thought the final form of the example would use `key` rather than `ix`, though. In any case, I hope people keep writing lens tutorials of all sorts. It's really nice to see things from different points of view as we all explore this new terrain.
Thanks! That was more a typo than an intentional choice—but it does go to show that they're really just synonyms so long as you're working in the specialized domain of JSON. Fixed!
ontopic: I personally am strongly against unnecessary dependencies, and have several packages which are similarly splitted. I wish there was more support for fine-grained package hierarchies, though... Somewhat offtopic note about FindBin: it seems that FindBin uses `argv[0]` to figure out the executable path, which is not really a safe thing to do. The package [executable-path](http://hackage.haskell.org/package/executable-path) probably works correctly in a larger set of circumstances, though the path of the running executable is not really a well-defined concept on many operating systems, and even when it happens to be well-defined, is impossible to obtain on some operating systems... disclaimer: I'm the author of executable-path.
I don't understand. What is "this"? I don't see how `Foo` or `FooT` can implement `Traversable`.
I support this proposal. 
I've just sent you a pull request to thyme. ;)
Listening to it at work, and getting excited to listen to it when I can put some thought into. Very cool.
This behavior is directly supported in my records library Vinyl! http://www.jonmsterling.com/posts/2013-04-06-vinyl-modern-records-for-haskell.html (It's available on Hackage: http://hackage.haskell.org/package/vinyl) Basically, Vinyl records are parameterized by an applicative functor, which you can instantiate as `Identity` to recover normal non-effecting record behavior. There's a function `run` which sploots a record `R f` into `f (R Identity)`, which is like what you're doing there. Not sure how pleased I am that I named the function `run`, but pull requests are welcome if you've a better name for it! I'm so glad to hear that someone else thought this was a neat idea.
this is really great and very creative! i really like the visuals of the colored arrows for some reason. are the backwards arrows at the end some of the glitches you were talking about? they could maybe represent reverb or feedback or something. :) i'm still new to haskell and have only gone through some of the basic/intermediate tutorials, but i have a question for you. could you point me in the direction of what modules/packages you used to do the drawing of your ui? i looked through what modules you were importing and couldn't, at least through a quick glance, quite tell what was being used. i would at some point like to start a project that does some visualizations with haskell, but i must admit that i'm quite intimidated by openGL and the like. did you learn from any interesting tutorials or projects?
Oh man, I read about Vinyl before and it seemed like magic. I'll dig in. Thanks!
My mistake, I've been reading too much lens code lately.
Cool! Feel free to get in touch if you have any questions! Delighted to help.
In general, "here's a big pile of BNF which I am going to explain with no context" is terrible pedagogic style. "Inflict" is sadly the right choice of words here.
I haven't thought a whole lot about EitherT State vs StateT Either, but it seems like the former would be a bit more convenient if you just want to ignore the EitherT functionality most of the time, but it's there when you need it. With StateT Either, it seems like the either would be more obtrusive. Note looks like a useful function to know, thanks.
If you have any questions, ask: I am one of the authors.
The difference is that the former retains your state updates even when you call `left`/`throwError`; the latter discards your state updates in this case.
I really like the idea. It would be cool to have something like this setup for *really* complex political situations like the current war in Syria or the last Lebanese war. However I skimmed through you source code and I have to say that it's not going to be easy to adapt your code, since you basically have all your templates hardcoded in 2k LOC file ;)
I know it's Scala and not Haskell, but I think the things taught will most certainly be transferable.
Noo, bikeshed subsequent releases! Iterate, iterate, iterate...
Yeah there's quite a few FRP libraries already: http://www.haskell.org/haskellwiki/Functional_Reactive_Programming
&gt; `$(5)` This is the most disgusting thing I've seen in a while. I like it :)
Yes those are the glitches.. Haven't got to the bottom of them yet. I'm using SDL for the graphics, chosen as the quickest way I could find find to get something working. It was a bit of a struggle to be honest, I just had to dig through the various sets of examples and piecemeal tutorials around. I haven't much in the way low level graphics before either, and the assumption seems to be that you have. Here's a couple of interesting links though: * http://www.renci.org/wp-content/pub/tutorials/BeautifulCode.pdf * http://projects.haskell.org/diagrams/ have fun!
I've thought that it would be pretty cool to model some regional conflicts. It'd be tricky to get reliable, complete data, though, unless you have people inside the region submitting data. At present, it's pretty US-centric. That seemed like a good starting point. The code was pretty much thrown together without much thought given to making it easily customizable for uses other than the main polink.org site. I don't think it would be terribly hard, though. Most of the templates wouldn't need any modification unless you want to modify the UI in significant ways or add support for other languages. The ones I can think of that would need some customization is the template for the front page and the help pages, which could easily be split off into their own files. One might also want to customize the set of allowable link types and tags. This first release is basically just to get the code out there. Making it easy for third parties to run their own site is just something I haven't gotten around to working on yet.
thanks for the response. and no, i haven't had any real experience with low level graphics. i have messed around with processing, but my stumbling block is that i'd rather learn to code in haskell than processing/java. so i get stuck trying to find something as nice as processing (in terms of the api it provides) in the haskell world, where it doesn't look like such a thing exists. thanks for the links. i have perused those before, as well as the [gloss package](http://gloss.ouroborus.net/), but it looks like that's it. i'm encouraged by you trudging through it though, so it looks like i should just get started doing the same. :) thanks again. i'll definitely keep an eye out for your work.
=) To make it work there is a `Num` instance for `template-haskell`'s `Type`, (there is also, irrelevantly, one for `Exp`) and then `instance Num a =&gt; Num (Q a)`, then I define instances for reflection that lift GHC TypeNats on modern GHCs and use a system of newtypes to get to the right instance on old GHCs.
This is because the implementation is incomplete in GHC 7.6. I'm told GHC 7.8 will actually support some arithmetic for type-level nats.
You can always get your instance by going through a normal list. instance SingI n =&gt; Traversable (FV n) where traverse f vec = listToVec &lt;$&gt; traverse f (vecToList vec) where listToVec l = FV $ (l !!) . fromIntegral 
It's not in HEAD yet though :( as far as I can tell. Does anyone know whether it is really slated for inclusion in 7.8?
I thought I recalled hearing that unification of type-level arithmetic expressions would be available in GHC 7.8 [citation needed]. That being said, I get the same error in my GHC master snapshot built a few days ago.
Last I heard was that Iavor is trying to get some version of the type-nats branched rebased and ready to merge into HEAD hopefully before the 7.8 cutoff.
Yes, Iavor is working on it and Simon/Dimitrios need to review the changes still. The freeze window is basically ICFP time, and Iavor has recently gotten back from holiday. AFAIK, he's working on getting things up to par right now. I expect it should be ready in time for 7.8.
The plan all along has been to get it into the 7.8 release, but the branch has not been merged into the compiler yet. I expect it will make it.
Just to clarify, the way Odersky et. al use the term "reactive programming" has little or nothing to do with FRP or similar purely functional approaches. This course is more about actors.
Out of curiosity, Isn't Meijer one of Haskell designers? (at least according to Wikipedia)
When he kept saying [D3D](https://www.youtube.com/watch?v=XWX4GUYGQXQ#t=33s).
I had the same thought which led me to post the video. I wanted to add text annotations, but there's a limit to my free time :)
Yes. At least he's a listed author of the Haskell Report. http://www.haskell.org/onlinereport/
Anything can be used to port haskell to JVM. Check out this haskell like language on jvm: [Frege](https://github.com/Frege/frege)
And having it run efficiently on the JVM is a different story.
 instance Functor (b, b -&gt; a) where fmap f (piece, hole) = (piece, f . hole) This is confusing, because it is invalid code. `(b, b -&gt; a)` has kind `*`, a `Functor` needs kind `* -&gt; *`. I believe you meant: instance Functor (Store b) where ...
I did say 'inflict' intentionally. I'm not sure on how to set the context properly, but I did try to add some examples to the beginning of the text to smooth the way. Does that help at all or would it just be better to discard the whole idea of using derivations for this topic?
Fair enough. I hope your project takes off :)
Unfortunately `Store` hasn't been introduced up until that point in the tutorial, so I'd also move the “Functors” section after “Comonads”.
Not only about actors, also about futures and observables (Rx). But it's true we will not be able to cover traditional FRP. 
On a somewhat related note, haskell-cafe archive on haskell.org seems to 404 for a good while (at least for me). This causes a lot of existing links to there to break.
Seems to be working for me.
I know. I still need to fix this (and looked into it some last week.) I'm sorry, but I've been busy battling other things including a new gig and several other server concerns. Mailman is also annoying as hell to deal with and has terrible documentation and overall I hate it a bunch because it's dumb and there I said it. Luckily this fire just resolved itself, I think (see update.) Seriously though, I'll be trying to get around to fixing this sometime soon, but it requires a bit of patience, debugging and hand-holding with python scripts and hundreds of megabytes of raw textual email. It's not really pretty (and frankly that's even *before* I can make sure I can restore the existing URLs - this is just to restore all the *lost* email, not even concerning myself with the URLs for the emails that are still there.)
Yes, it just came back online about 15 minutes ago. See updates.
Well, no worries and props for the quick respond. I look forward to any progress on bringing the archive back. :)
That guaranteed to preserve the number of items because of the `Traversable` laws, right? I think I'll use your solution while the new version comes around.
Ok, guess I'll have to wait more then.
Should this: set _1 :: a' -&gt; (a, b) -&gt; (a, b) be this? set _1 :: a -&gt; (a, b) -&gt; (a, b)
Isn't this the second or third time in a month it's gone down? Do we need to start looking to alternate sources of funding for the haskell.org server?
It isn't, no. I'll forward declare Store to make it so, though.
I wouldn't expect so. It's on a Hetzner dedicated host which have a reputation for being stable, many people I know in the Haskell community are using it including myself and it's been solid.
~~It actually should have been~~ set _1 :: a' -&gt; (a, b) -&gt; (a', b) ~~as the difference between `a'` and `a` comes around again later.~~ (Edit: nope you were totally right. My mistake)
Thanks! :)
Nope, it's just my fat fingers - it's the 9th. Sorry!
It just hasn't appeared that stable lately, and I wouldn't mind helping out to keep it stable. 
The other disruptions have mainly been for migrations, upgrades, and sysadmin work. Those will subside as we get more stable. 
By the way, if anyone considers themselves a mailman expert user, please get in touch. Skilled hands and advice dealing with the beast would be very appreciated.
They're all kind of closely related, aren't they? In each case you're talking about breaking apart a whole into a part and a hole.
It doesn't work reliably when building with -threaded
I'm currently a CS York undergraduate. Colin's a great lecturer and I was very glad to learn about his contributions to FP and his place in the Haskell world. I think I'll have to come down!
The difference is that a zipper gives O(1) update of that hole. Contrast that with a lens into a deeply nested data structure that is O(depth of the field in the data structure).
&gt;It's on a Hetzner dedicated host which have a reputation for being stable They do? I've never heard anything like that, but rather that they are dirt cheap and that their stability reflects that.
That's true—zippers have more information about how to move within a structure which lets them optimize so long as you're moving within the zipper. You can't use a zipper to dive into an element inside your functor, though.
Would love to go but it costs so much... Will there be videos? 
12 should work fine.
commenting for later science.
This was - as far as we can tell - some kind of abrupt power failure of some sort or hard reset on the server. To my knowledge, we have not ever suffered from this before anytime in *any* recent memory. As in, years. Or ever. Despite Hetzner's alleged claims of instability from others below, I have only known people who have been very happy with their service and I haven't ever heard too much against their dedicated instances. We were previously using Hetzner as well. It is entirely possible something randomly shit the bed on *our* side, and our machine had a kernel panic or something, but we're unfortunately still not sure (given all logs just 'cease' around the time it went offline, it looks a lot like some possible power anomaly.) Pricing/money **is not the problem here**. Yes, we could always use more money, and it would be cool if everything could forever have its own separate Hetzner box and a gagillion gigaRAMs. But as it stands, the sever is fairly powerful and even has room to do more. I hope you understand that despite what it may seem like, just throwing money and/or bodies at the problem *is not* sufficient in any way to 'solve' the issue on its own. It's entirely likely to create more management headaches (yes, I am 100% serious.) Finally, the servers have been somewhat in flux ever since the server move, but things are more stable recently. We are trying to modernise parts of the infrastructure to make it easy as maintain as possible in the long run. Upgrades, migrations, and stability fixes are part of this. The new team of admins has slowly been examining the servers and dealing with them, tweaking them, learning them. But it comes with *years* of legacy infrastructure *and very live data*. At the same time, we have new services we want to deploy thanks to the new hardware - and those need to be discussed and tested too. This certainly isn't my first Rodeo, but it's a big one. If something is broken, do say so: http://community.galois.com/mailman/listinfo/haskell-infrastructure. In the mean time, some things (like the mailing lists,) are a bit wacked - but that isn't so much a question of money or bodies as it is a question of completely subjecting yourself to crazy amounts of annoyingness and pain. (As a closing statement, mad props to the operations/sysadmin teams out in the world.)
Hey, not to worry. The content will be recorded and uploaded onto our website. Most of the people we talk to find the value comes with the conversations between the talks with fellow attendees and experts though! Theo
At least, everyone I've spoken about it to are very happy with the stability, haven't heard anything to the contrary.
There's been a bunch of movement and shifting of hands I think.
Would still need a good paradigm for accessing java-land functionality.
Do you mean, can Haskell be compiled to Scala, and thus by transitivity, compiled to the JVM? Perhaps.
I completely understand that it's a more complex problem than can be solved by just throwing money at it. I wasn't aware of the recent server migration, and was mostly just curious if there was anything more the community could do to help out. All the hard work from you guys is greatly appreciated =)
Jasmine is the least painful assembly I've ever written. Making an ffi wouldn't be too bad, probably just a clone of a C ffi.
Just before section "What did we gain?", it looks like: Lens b a = a -&gt; Store b a should be: Lens a b = a -&gt; Store b a If not, then something is wrong with the definition of `(&gt;-)`
I took his course on Declarative Programming -- they were THE most fascinating and interesting of my 3 year course. And then I became an imperative programmer. :-( Still waiting for the "revolution". ;-)
Simon will be talking about Haxl tomorrow at the [Zurich FP Afternoon](http://www.haskell.org/haskellwiki/ZuriHac2013#FP_Afternoon).
Pfft, *now* you give me `reflection`. But I want a pony! And a unicorn! Thanks a lot for writing this :)
Well, I think it's a little misleading to just say you "won't be able to cover it". You are covering approaches that are IMO fundamentally at odds with the purely functional model. :) I would characterize both actors and observables (if we are talking about the "Deprecating the Observable pattern" paper and related work) as being _all about side effects_. (And `Future` as implemented in Scala is not really purely functional either.) So FRP and other purely functional approaches are at the opposite end of the spectrum, not merely something you "aren't going to get to"!
 This is correct. There's a bit of a weird convention where `Lens a b` means a lens looking from `a` into `b`, but the `a` needs to be the last type parameter to `Store` since `Store` is a `Functor` over the whole, not the piece. Good catch.
FWIW the older and more portable method works something like this: - First build up the ability to reify/reflect numbers, represented at the type level similarly to the usual way, except in binary rather than unary for performance (`data Zero; data Succ n; data Twice n; ...`) - Then lists of numbers, in similar fashion - Then arbitrary Storables, represented as a list of numbers - Then arbitrary values, represented as a StablePtr to the value, which is Storable It's pretty genius.
[This was literally posted here yesterday](http://www.reddit.com/r/haskell/comments/1l4ph3/dsls_and_towers_of_abstraction_with_gershom/)
RC2 is out: cabal install http://johantibell.com/files/Cabal-1.18.0-rc2.tar.gz http://johantibell.com/files/cabal-install-1.18.0-rc2.tar.gz
I agree that's more clear. I clarified it slightly more—thanks!
We still have a slow path lying around in the code that more or less does that. It just generates the ability to reflect bytes, then 8 bytes at a time, then reflects a pointer as a set of up to 8 bytes, then generate them from a StablePtr. That bytewise version is about 10x faster than the original way. But in the end the new way is about 3 orders of magnitude faster than that. =)
My god, yesterday I needed to use cycles for a control graph algorithm... so I built a data structure where you have a position in the cycle, can move left/right and modify the thing at the current position, with those operations amortised O(1): type Size = Int type Pos = Int data Cycle a = Cycle !Size !Pos [a] a [a] left :: Cycle a -&gt; Cycle a right :: Cycle a -&gt; Cycle a getc :: Cycle a -&gt; a sizec :: Cycle a -&gt; Size all fine and dandy. Then I remember that I was going to learn about comonads so start reading the `comonad` package docs and what do you know my `convolution` function is almost the same as `extend`; also `extract` is just getting the value under the current pointer! So wonderfully: instance Comonad Cycle where extract = getc extend f c = mkcycle $ h c (sizec c) where h c 1 = [f c] h c i = f c : h (right c) (i-1) Now we can describe a function that adds the current value and the values of it's neighbours: f :: (Num a, Comonad w) =&gt; w a -&gt; a f c = extract (left c) + extract c + extract (right c) and we can just use `extend f` to do a kind of generic convolution on the cycle. Fantastic. Then, you can extend this to a finite toroidal grid by just using: newtype Grid a = Grid { unGrid :: Cycle (Cycle a) } and defining all the right instances again, as well an `up` and a `down` function. &lt;3 functional programming.
Nice! A few thoughts: First, You may find it interesting to see the PointedCycle comonad in: http://comonad.com/reader/2008/the-pointed-set-comonad/ There it only gives access to the right efficiently. Second, Your `left` and `right` can be replaced by just extending cokleisli actions, see below. Third, You could define `Cycle` as a `Store` comonad, or `EnvT`'d `Store` by using newtype Cycle s a = Cycle (s -&gt; a) s s deriving Functor instance Comonad (Cycle s) where extract (Cycle f _ s) = f s duplicate (Cycle f b s) = Cycle (Cycle f b) b s here the b parameter serves as your size. Now you can define a `ComonadStore` instance and get `peek`/`peeks`/`pos`, etc. You can also define `ComonadEnv` to get the ability to `ask` for the `b` in a generic way. left :: Cycle s a -&gt; a left (Cycle f b s) = f (mod (s - 1) b) -- equivalently: left w = peeks (\s -&gt; mod (s - 1) (ask w)) right :: Cycle s a -&gt; a right (Cycle f b s) = f (mod (s + 1) b) But if you use `b` as a pair of coordinates instead you can then just make Grid into Cycle (Int,Int) with similar combinators, and then do things like: blur :: Cycle (Int,Int) Double -&gt; Double blur img = (left img + up img + right img + down img) / 4 on various topologies. Alternately you can replace the 'b' parameter with just a function that normalizes your `s`. e.g. instead of storing the maximum size, store in it the function for taking (`` `mod` n``). With that: `World s = EnvT (s -&gt; s) (Store s)` or something similar, but if you want more exotic topologies, you may have to define it by local movements/charts and things get messy. The `Store` definition above isn't a panacea. As with the one I provided in the [cellular automata](https://www.fpcomplete.com/user/edwardk/cellular-automata) post it will slow down unless memoized. Another version of it that fits your usecase is to use something like the [`Pointer` comonad](http://hackage.haskell.org/packages/archive/comonad-extras/3.0.1/doc/html/Control-Comonad-Store-Pointer.html ), which is a special case of `Store`. Then you can use the access to the original array to get the bounds and wrap, stick, fail, etc., when calculating up/down/left/right
Heh, I spent about a week trying to add all the closed captioning to the new york lens video. I gave up about half way through, so it goes from being crystal clear captioning to autotranslated mess near the end. ;)
&gt; danharaj: arrows are just strong monads in the 2-category of profunctors. what's the problem? They... are?
I've heard legends of an end of Declarative Programming sword fight...
You could browse http://www.haskellers.com and use google/#haskell to encounter the person you think may mentor you.
arrows are just profunctors you can glue together `(.)` and manufacture out of whole cloth `(arr)` His statement is almost correct, but it neglects the rather strong additional requirement that you can also implement `second`, which makes them, well, just weird. (Unless you can somehow get that out of strong in a way that isn't obvious to me.)
For some reason I thought danharaj meant that arrows were ("strong") monoids in the category of endofunctors over the 2-category of profunctors.
&gt; Somewhat offtopic note about FindBin: it seems that FindBin uses argv[0] to figure out the executable path, which is not really a safe thing to do. The package executable-path[1] probably works correctly in a larger set of circumstances [...] Huh. If that's true, then it sounds like I should switch to `executable-path` then. :-) &gt; [...] though the path of the running executable is not really a well-defined concept on many operating systems, and even when it happens to be well-defined, is impossible to obtain on some operating systems. So in other words, it is a sufficiently finicky thing that I shouldn't make my entire library depend on it. Thanks, your comment was very helpful! :-)
Niceness is the killer feature of programming communities.
Well if you control how your executable is started, then you are probably safe with `argv[0]`. But if your user decides how to start your executable, then you can start to worry about symlinks and all kind of even more arcane stuff which I forgot...
wow - congratulations!
You made my day, thanks!
I'm familiar with those resources (and they are amazing!) but, I don't think it's a replacement for a personal tutor which is invaluable for some learners.
The thing about them is that some people on IRC are so helpful that they'll take an hour or more to really guide people interactively through concepts. If anything I think the format works better than a personal teacher just because it's fairly self-directed but you still can get as much guidance as you need if you get stalled when tinkering.
Such self-direction isn't great for all folks. An hour from someone here and someone else there *is* fantastic, don't get me wrong, but it isn't a replacement for those who need a schedule and someone to help them structure how to get to the level of knowledge they desire. I'm not saying the current resources aren't fantastic, but a one-size-fits-all mentality always hurts those who don't fit ]=
Fantastic. 
How would a programmer that doesn't know anything about iOS programming get started with this?
This is going to boost development so hard. Haskell really is doomed to succeed.
I thought the writing itself was okay, if a bit dense. But I do think the notation of (:t) and double angle brackets is confusing and unnecessary. If you desperately feel the need to introduce meta-syntax for talking about these things, maybe try descriptive words in a distinct font? E.g.: 1. *TypeOf(* `length "pandora"` *)* 2. *TypeOf(* *ValueOfType(* `[a] -&gt; Int` *)* *ValueOfType(* `[Char]` *)* *)* 3. *TypeOf(* *ValueOfType(* `[Char] -&gt; Int` *)* *ValueOfType(* `[Char]` *)* *)* 4. *TypeOf(* *ValueOfType(* `Int` *)* *)* 5. `Int` I agree with using an abstract syntax tree, though. It's even clearer that you're giving a technique, and not syntax. I find people have trouble juggling a syntax and a meta-syntax in their head at the same time.
Is TH supported?
Should reflect :: proxy s -&gt; a be reflect :: Proxy s -&gt; a ?
Awesome! 
I mentor a lot of people for free by e-mail. Just send me an e-mail if you have a question. My email username is Gabriel439 and is a gmail account.
Wow!
nope, not yet. This is something everyone wants for cross compilers in general, but requires a bit more systematic support for GHC to natively be a multitarget compiler. (because TH can run haskell libs, so libraries need to be in both the Build and Target object formats for TH to make sense. GHCJS will have some TH support, but more work is needed to generally support TH in any cross compiler sadly)
What's a profunctor? If `T a b` is covariant in `b` and contravariant in `a`, is that enough to make `T` a profunctor? And while I'm asking category theoretic questions, why is implementing `second` compulsory for arrows when implementing `right` is optional?
I see constant misinformation on types from imperative folk, so this is a lighter introduction to some of the problems they solve, with links to haskell learning resources. Feel free to share with the non-haskellers (I know I'm preaching to the choir here)
I think not, to emphasize that the "container type" `Proxy` is actually immaterial—all we need is the type information `s` and `Proxy` is just useful to somehow get it without actually having a value of `s`. So we can actually instantiate `Reifies s a` with any type that has a trailing `s` type parameter.
No. If you go through the rest of the tutorial, I touch on why the type of `reflect` is `proxy s -&gt; a` instead of `Proxy s -&gt; a`. The TL;DR is that `reflect` needs to work on any kind of argument, but we need to unify the quantified `s` type which appears. The 'outer' type constructor is otherwise irrelevant to `reflect`. This means, for example (given code from the article): O 1 :: O Int s reflect :: proxy s -&gt; a reflect (O 1) :: a -- 'proxy s' unifies with 'O Int s': s ~ s, proxy ~ O Int In general you could *also* be reflecting something of a type like the monoid types in the article: M "abc" :: M a s reflect :: proxy s -&gt; a reflect (M "abc") :: a -- unification: 'M a ~ proxy', s ~ s The `proxy` type unifies with anything, because it does not matter what it is.
How would a programmer that doesn't know anything about Haskell get started with this? What are some compelling reasons to?
I think it would have been a good point to also add the Maybe type, to show that when things are "nullable" we can cover that quite nicely. Whereas in the python code we would have to check at each step. bar :: Maybe Baz -&gt; Maybe Quux -&gt; Maybe Int bar baz quux = do x &lt;- doSomething baz y &lt;- doSomethingElse quux return (x + y) edit: after realizing that it may fail at any point, and doSomething and doSomethingElse may fail bar :: Maybe Baz -&gt; Maybe Quux -&gt; Maybe Int bar baz quux = do x &lt;- baz &gt;&gt;= doSomething y &lt;- quux &gt;&gt;= doSomethingElse return (x + y) 
What would a co-dementor even *do*? Give you extra souls?
Wow, cool! What is the current status with respect to Objective-C/Cocoa interoperability? I did see this project of Manuel Chakravarty a while ago: https://github.com/mchakravarty/language-c-inline
Can't wait to try this out :D
I think something like "Learn You a Haskell for Great Good!" and someone to email or chat with when you don't understand a concept is a good combo.
You can't. Yet. This looks like you will be able to create static libraries with your haskell code and link them to your Objective C application. You still need to know how to use XCode and Objective C and write your GUI in objectivive C. 
Well you only need `arr` and `first`. `second` has a default definition like so: second :: a b c -&gt; a (d,b) (d,c) second f = arr swap &gt;&gt;&gt; first f &gt;&gt;&gt; arr swap where swap :: (x,y) -&gt; (y,x) swap ~(x,y) = (y,x) You could of course define `first` in terms of `second` as well. 
Lol, I've started a game written in Haskell called SpaceTime!
I'll just repeat this... &gt;Such self-direction isn't great for all folks. An hour from someone here and someone else there is fantastic, don't get me wrong, but it isn't a replacement for those who need a schedule and someone to help them structure how to get to the level of knowledge they desire. &gt;I'm not saying the current resources aren't fantastic, but a one-size-fits-all mentality always hurts those who don't fit ]=
I can second this. One prominent member of the community whose name I will not share, so people don't start being concerned about this person's social life, spent a good deal of a saturday evening helping me understand the State monad. Then again.. why was *I* not out there? Ah well. On top of that, it is always entertaining to ask a question in #haskell that catches the interest of several people, and then watch the ensuing avalanche of help and golfing attempts. People there are always incredibly helpful and unbelievably patient, in spite of the fact that some of them seem to be wizards(/u/edwardkmett) and appear to be able to converse with each other entirely in category theory and lambda calculus. 
With names like `doSomething`, I'd expect that code to be monadic.
That's awesome! And I just learnt that, according to the internet, Haskell already builds on Android. This has made my day.
&gt; write your GUI in objectivive C. Or provide bindings that Haskell can hook into, am I right?
Yup! Now we just need someone to write a GUI dsl that has the right bindings and doesn't (for now ) need template Haskell. 
At first I read that title as "An intro to making code *with reasonable* types"...
Sounds like you have `directory` built against `bytestring-0.9.2.1` and `parsec` built against `bytestring-0.10.0.2`. (Or the other way around, but it must be this way because both `directory` and `bytestring-0.9.2.1` come with GHC 7.4.2.) In general, installing newer versions of packages that came with GHC is a Bad Idea. Probably what happened is you installed some third package that required `bytestring &gt;= 0.10`, so cabal installed `bytestring-0.10.0.2` for you, and then you later installed Parsec, and cabal built it against `bytestring-0.10.0.2`. The source tarballs of packages you've downloaded are saved by `cabal`, so you won't need to download them again, as long as the version you downloaded is compatible with the libraries that came with GHC 7.4.2. For example, you can find your Parsec source tarball at `~/.cabal/packages/hackage.haskell.org/parsec/3.1.3/parsec-3.1.3.tar.gz`. You can tell cabal to install from a tarball rather than from hackage by just specifying the tarball in place of the package name: cabal install ~/.cabal/packages/hackage.haskell.org/parsec/3.1.3/parsec-3.1.3.tar.gz Not sure under what circumstances cabal will automatically use the cached version, if you just say `cabal install parsec-3.1.3`. I recommend you `ghc-pkg unregister bytestring-0.10.0.2` and everything that depends on it. (`ghc-pkg unregister` will list what those packages are, and you have to remove them one-by-one, in the correct order. Bleh.) You should also `ghc-pkg unregister directory-1.2.0.1`, to save yourself hardship later. Then reinstall Parsec and whatever else you want, but make sure cabal doesn't try to reinstall `bytestring-0.10.0.2`. You can force it not to by specifying the version of `bytestring` to link Parsec against on the command line, like this: cabal install parsec-3.1.3 bytestring-0.9.2.1 but I think you can also write constraint: bytestring ==0.9.2.1 in your `~/.cabal/config`, so that cabal will use that version of `bytestring` for all future installs.
Thank you! I followed your instructions and now everything's working fine. I've head enough about Cabal Hell to be very wary of installing new versions of packages that came with GHC, so I've tried not to do it. I looked at all the packages listed by `ghc-pkg unregister` and their documentation, and it seems like Gloss is the culprit (requires `bytestring 0.10.*`). Since I was just having a look when I installed it, I've just left it alone. I added the `constraint` line to my `~/.cabal/config` before reinstalling the couple of affected packages that I need and the re-installations went smoothly. Thanks again for the assistance.
We can unfortunately not, from the type signature alone, determine that `bar` *will* return an `Int` – it could also return a bottom value. (The risk of that can be limited by choosing to use only total functions, though, but this is still not information conveyed by the type.)
Or bar baz quux = liftM2 (+) (doSomething baz) (doSomethingElse quux) if you like being all declarative-like.
Yes, yes. I wasn't really answering your question, but rather expressing my undying fanboyism for the Haskell community at large. That is all. *However*, I **personally** feel that sticking so vehemently to the need for a mentor is .. self-limiting. To me, that's practically the equivalent of admitting to one self that one is unable to actually learn on one's own. This being an ability that I personally take pride in -- being able to teach myself / learn to do almost anything, given enough time -- my pride would never allow me to make such a statement. You are sure you are not limiting yourself by *requiring* a mentor? It sounds as if you don't want to touch anything on your own. 
We'll have some more materials, Xcode integration guides, and demo projects up soon! I've been using a de-Template Haskell-ized https://github.com/jspahrsummers/ObjectiveHaskell to do all of my development thus far which I need to post as well, which provides a very workable bridge between the Objective-C and Haskell worlds.
That's a good point, although deliberately left out of the post. I've found that raising that idea too early (at least to people grounded in imperative languages) is counter productive. It leads to "oh so there's error and unsafePerformIO and undefined so haskell isn't any safer than &lt;favorite language&gt;!"
You pretty much nailed profunctor. It isn't obvious but there follow laws about mixing maps over the first argument and second from parametricity.
MORE CREDITS: Three connected projects concerned cross-compilation * **Registerised ARM support** added using David Terei's LLVM compiler back end with Stephen Blackheath doing an initial ARMv5 version and LLVM patch and Karel Gardas working on floating point support, ARMv7 compatibility and LLVM headaches. Ben Gamari did work on the runtime linker for ARM. * **General cross-compiling** with much work by Stephen Blackheath and Gabor Greif (though many others have worked on this). * **A cross-compiler for Apple iOS [IOS].** iOS-specific parts were mostly Stephen Blackheath with Luke Iannini on the Cabal patch, testing and supporting infrastructure, also with assistance and testing by Miëtek Bak and Jonathan Fischoff, and thanks to many others for testing; The iOS cross compiler was started back in 2009 by Stephen Blackheath with funding from Ryan Trinkle of iPwn Studios. Thanks to Ian Lynagh for making it easy for us with integration, makefile refactoring and patience, and to David Terei for LLVM assistance. And Maxwell Swadling for fixing the static linking up nicely.
Not much of a list yet. Anything pure Haskell will Just Work, as long as it doesn't use Template Haskell (this is on our to-do list). **network** works with a tiny tweak (move #include up to the top in Types.hsc). **OpenGL** works but requires a tiny patch and the right Objective-C hacking. This should get you started: --- orig/OpenGLRaw-1.3.0.0/OpenGLRaw.cabal 2013-08-30 08:41:42.000000000 +1200 +++ OpenGLRaw-1.3.0.0/OpenGLRaw.cabal 2013-08-30 08:23:31.000000000 +1200 @@ -242,14 +242,17 @@ extra-libraries: opengl32 else cpp-options: "-DCALLCONV=ccall" - if !os(darwin) &amp;&amp; flag(UseGlXGetProcAddress) + if !os(darwin) &amp;&amp; !os(ios) &amp;&amp; flag(UseGlXGetProcAddress) cc-options: "-DUSE_GLXGETPROCADDRESS" else cc-options: "-DUSE_DLSYM" if os(darwin) frameworks: OpenGL else - extra-libraries: GL + if os(ios) + frameworks: OpenGLES + else + extra-libraries: GL source-repository head type: git ...and a similar change to GLURaw's .cabal. Also there are Typeable instances in OpenGL - just remove the 'instance Typeable...' blocks from Graphics/Rendering/OpenGL/GL/Tensor.hs (this is a ghc-7.8 compatibility issue).
You missed &lt;/code&gt; type Lens a b = Coalg (Store b) a -- (thanks to Austin Seipp for pointing this out) +```` Which, all together, leads to the [fascinating](http://r6research.livejournal.com/23705.html) or [oblique] 
At the moment there isn't anything much better than doing it yourself with ForeignFunctionInterface, but I'm sure there's stuff in the works. I'll release an OpenGL skeleton some time soon - look for updates on http://ghc.haskell.org/trac/ghc/wiki/Building/CrossCompiling/iOS
The main compelling reason is Haskell just being an excellent language. A second compelling reason is that Haskell is a fairly good option for cross-platform development (now even better with GHC-iOS and GHCJS for web programming), except that AFAIK there are no truly cross-platform UI libraries. Hopefully people will start working on these kinds of things.
Spooky! Exactly yesterday I was just wondering what the current state was for this project. Also I'm excited to see what SpaceTime is going to be!
Then I would hold that the example is poorly constructed or the point poorly communicated, if one needs to resort to lies while trying to teach something.
 bar :: Baz -&gt; Quux -&gt; Int bar baz quux = doSomething baz + doSomethingElse quux &gt; We can see immediately that in comparison, Haskell code scales horizontally whereas Python scales vertically. Um. Unless you write the Haskell code like the Python code: bar :: Baz -&gt; Quux -&gt; Int bar baz quux = let x = doSomething baz y = doSomethingElse quux in x + y &gt; Given a `Baz` and a `Quux`, we can prove that the function `bar` will produce an `Int`. It can still throw an exception or go into an infinite loop, so all you can say is that *if* it returns a value, that value will be an `Int`.
I've been looking forward to using Esqueleto on an upcoming project. I'll definitely look back here when that time comes.
Looks cool, but I think it would be nicer to name `select` as `query` and alias `return` to `select`
The issue is not being able to explain the entirety of haskell's type system (or difficult concepts like bottom) in a short introductory post. I do think it's a fair point, and I'll add a footnote to clarify. But choosing what information can be presented and communicated in a short space (with links to better, more in-depth resources) is not the same as resorting to lies.
Oh, you definitely don't have to mention bottom or the halting problem or partial functions. Just say that we are to a greater degree certain that the function will return an `Int`, or that unless something spectacular happens, it will return an `Int`, or whatever. It's the blatant "lie" about using the types to "prove" that an `Int` will always be returned that made me react. I think presenting misinformation is lying regardless of the context. I'm with Feynman when he says that [tailoring the truth to make something easier to explain does more harm than good](http://www.wimp.com/feynmanterms/).
You don't have to lie or be vague, you can just say something like "the function will return nothing but an Int", which is the important thing. 
Excellent goal: "I want to write code that people would rather read and use than rewrite."
I use hsenv + cabal with great satisfaction. Recompiles world but works, and if I mess up just delete .hsenv and start again.
&gt; I've been using a de-Template Haskell-ized https://github.com/jspahrsummers/ObjectiveHaskell It does basic conversion between Haskell and Objective C data structures, and helps you call Objective C methods from Haskell and vice versa. I'll have it up asap. It does the trick for now, and makes the FFI *much* nicer.
oh that would be great
Sure. To be clear, I thought it was already sufficient to have a footnote to an article that explains what it means to prove the proposition and what bottom means in that context. I'm understanding from the feedback that it wasn't communicated clearly enough, so I'm going to update that part tonight. But even without the forthcoming clarification, the footnote is there specifically so readers can follow the link to learn more about it.
That's cool
We should also thank the folks at Apple for taking the time to backport the Clang patches Austin Seipp wrote into the pending Clang 3.3 Point release and thence into the Clang version in Xcode 5, without which something like GHC-iOS would be a more complicated effort. 
There are two videos going over Cuboid's (mentioned below) implementation in detail. I found them entertaining: http://www.youtube.com/watch?v=-IpE0CyHK7Q
Where exactly did the internet whisper such sweet tidings into your ear?
Yay! :D What great news to accompany the smellable copy of LYAH the postman brought me today.
Thank you very much to everybody involved. I know that to get here did take a lot of very tricky technical work. The future is in mobile, so this is a very important milestone for Haskell! If any of you are coming to ICFP, I think, you deserve lots of free beer :)
If you're going to be super-pedantic, that statement would never be true in any programming language: * You can have hardware failures in the middle of the computation which prevent it from returning anything * Another process could be inspecting our program's memory and reacting to observed changes, causing side effects during the evaluation of our function. Moreover, even if you ignore the obvious cheap shots at soundness it's still not a good idea to begin every blog post with an introduction to programming language theory or denotational semantics that would be necessary to craft a perfectly precise statement. Sometimes a blog post is just a blog post.
I love this derivational style of explaining things - starting out with a specific example and generalising out as far as we possibly can. Great article. 
Very interesting, I'm going to have to go through that slowly though to understand what's happening. Maybe actually properly learn some category theory :p (I was actually just thinking about sneaking back into lectures at the university I just graduated from to do the category theory course). Just a thought: I've been wanting to design a circuit simulator in Haskell for some time, where each component (and/or gates, flipflops etc) has inputs and outputs to others; the state of the the entire system at time step `t+1` depends on the state at `t` and there would be loads of internal cycles of wires between components. I think this could be written as a comonad (although the extract and extend methods may be a little long as they have to deal with cases for each component). I can see some difficulty with the non uniform shape of the component graph (it's going to be harder than a cycle or grid). What do you think? 
Don't forget [ghcjs](https://github.com/ghcjs/ghcjs).
You should also look at KU's sunroof. http://www.ittc.ku.edu/csdl/fpg/software/sunroof.html It works with GHC, and using monad rectification to turn JavaScript calls in Haskell into JavaScript.
You should at least take a look at elm. It's a haskell-like language designed for FRP and web dev: http://elm-lang.org/ 
I'd (mistakenly) thought it was dead. How does it stack with the others in terms of stability, speed, and FFI ease?
Thanks, I'll have a look into that.
You are fucking awesome!
Full GHC Haskell. ALL OF It. (except you get to ffi into JS rather than C, I think thats literally the only difference)/ GHCJS will be released in tandem with GHC 7.8. In fact, GHC 7.8 is getting certain patches that make it possible to support alternative backends like GHCJS. Basically GHCJS is really awesome engineering. And I (personally) really like the authors and have had the pleasure of many a fine IRC chat with them.
that's probably not a good idea.. show and read aren't meant as wire transfer formats..
I can't say much about other compilers but in GHCJS it works like this: in order to set up an interface with JavaScript you just set up FFI bindings, just like with C. A few examples are shown in this blog post: http://weblog.luite.com/wordpress/?p=14 Along with bindings to JQuery and Canvas there is a library for marshaling Haskell types into low-level GHCJS types: https://github.com/ghcjs/ghcjs-base/tree/master/GHCJS and you can base your bindings to whatever graphics library for JS you are using on those libraries
That I would. It looks like at the author has managed to write an activity using it, https://github.com/neurocyte/android-haskell-activity
That sounds pretty phenomenal. I'd overlooked it, I it was lacking in features when the articles/tutorials I'd read were written, but now it sounds like exactly what I'm looking for. Thanks!
json with aeson is also pretty easy. if you only want to read the jsons with haskell, you can just derive them automatically with th.
It's absolutely active, and has lots of cool stuff. Also, most of haskell libraries work out of the box. Be sure to use the prebuilt vagrant box.
Correct.
I'm perfectly happy with the way you wrote it. I think ignoring bottom when explaining this is the right thing.
I'm currently working on a HTML-based GUI library [threepenny-gui][1]. It's cross-platform in the sense that every platform has a web browser installed, but of course it's not native. If anything, I hope that the API design will be useful for native GUI libraries as well. [1]: https://github.com/HeinrichApfelmus/threepenny-gui
Yeah, I heard that some parts of Template Haskell are being re-written, such that it can be used on more platforms. What would enable this? Dynamically-linked libraries? Also, where/how can I check the status of that project?
You could do something similar with Data.Binary then compress it with gzip if you felt like it, and you'd save on bandwidth. Of course, given this is an online game, I can't imagine it using that much bandwidth. Data.Binary is faster than having to parse strings, though, and you can derive it automatically.
Yes! That's what I was waiting for. Thanks to you and all those involved.
Out of the bunch I've only used Fay so can't comment on rest. I find moving data between Fay-generated JS and "normal" JS can be a tad tricky - however I believe they have some plans to mitigate this issue. I would consider writing the performance intensive parts by hand in JS though. As much as the existing HS-&gt;JS compilers might do the job, I feel they will fail you when you need to be able to optimize the resulting JavaScript code which is probably going to be issue especially on lower end systems for the WebGL parts. (this is coming from a lot of experience in writing a variety of canvas based JS code which generally suffer from a variety of performance issues)
The Essence of Dataflow Programming: http://cs.ioc.ee/~tarmo/papers/essence.pdf talks about using comonadic semantics for this general style of thing. You can define how each wire is a function of itself and other wires at the previous timestep. Uustalu's presentation was for defining semantics for an interpreter, but the trick works just as well for an EDSL for the language itself: https://github.com/ekmett/comonad/blob/master/examples/History.hs may serve as a good companion to the original paper. Don't get hung up on the category theory in the paper, focus on the code. His `ComonadZip` is my `ComonadApply` in the `comonad` package.
All the difficult work has been done now. Android will be easy.
Interesting! Thanks for putting this online. Any chance the 'hot code swapping' stuff can be open-sourced?
GHCJS has managed to strike me as a very serious effort in this space and Luite is incredibly responsive. It can run some seriously non-trivial programs in the browser using diagrams and large swathes of the existing Haskell ecosystem.
Hi. I have a ticket to this but I can no longer make it. For unknowable reason SkillsMatter don't offer refunds, so would you be interested in a half price ticket?
in the slides (page 35) they mention fxl, the predecessor. an introduction to fxl can be found here: https://www.facebook.com/notes/facebook-engineering/fighting-spam-with-pure-functions/10151254986618920 
I'm no Java developer, so I don't what often means in Java. In Haskell, I use Maybe pretty often; maybe because I have not yet moved forward in designing differently my code, or switch to other idioms.
What operations do your `Var`s need to support? So far you have `unbounded` and `failed`: not very useful on their own. Your example of use at the end has setting a variable to a certain constrained range, but the client code must interact directly with the representation. Real client code would presumably require more operations. What are they? Why do you have separate `IntBounds` and `IntList` types, instead of a single `IntVar` type with `IntBounds` and `IntList` constructors?
Now I wonder if you happen to browse [/r/aww](http://www.reddit.com/r/aww/comments/1lcxo1/paws/). :D
Well, actually more declarative would be like bar (Just baz) (Just quux) = Just $ (doSomething baz) + (doSomethingElse quux) bar _ _ = Nothing edit: code aligning edit2: lol, I just realized my original code is wrong anyway. Fits me right for "coding" early in the morning
I laughed at this: · Goal: keep the code simple, use few extensions (and then on the next slide) {-# LANGUAGE ExistentialQuantification, TypeFamilies, GADTs, RankNTypes, ScopedTypeVariables, DeriveDataTypeable, StandaloneDeriving, MultiParamTypeClasses, FlexibleInstances #-} Really cool description of the project, though -- and interesting to see a real-world example of the gymnastics needed to hide the Applicative/Monad mistakes.
Mistakes?
Applicative is not a superclass of Monad. Additionally, many common functions (in this article, mapM and sequence, but there are others) have a Monad constraint when all they need is Applicative -- and they may behave better for many real-world monads when implemented in the way that produces an Applicative constraint instead of a Monad one.
I dislike reading code on a case-by-case basis. It is something about having to jump around while reading to figure out how it all aligns. If I can lift a function into the Maybe monad and treat it as a function operating on Maybe values I prefer doing that. But that's just my personal preference.
The end result will be the same using Applicative or Monad. Applicative allows for more optimization though (as noted in the slides) so it can be beneficial to solve these sort of problems primarily in Applicative and only drop into Monad when required. I've always wondered if GHC in the post-AMP world could ever (legally?) use the most general form possible when desugaring do-notation... without applicative-do. So something like this: foo = do x &lt;- op1 y &lt;- op2 pure $ A x y could end up compiled as: foo = A &lt;$&gt; op1 &lt;*&gt; op2 
Will those functions be fixed then as well? Once `Applicative` is a super class of `Monad` changing the class constraint shouldn't break any code right?
Sure I'm interested - is there an easy way to transfer the ticket?
I just want to mention that if you want a correct `ListT` that obeys the monad laws and streams you can use `pipes-4.0.0`, which is coming out this weekend. It provides a monomorphic `ListT` that is very easy to build and consume.
&gt; without applicative-do Yet what you describe is precisely applicative do. GHC just has to observe that none of the variables bound on the LHS of `&lt;-` appear on the RHS of the computation's `&lt;-`, and then it'll know that it can desugar to Applicative, instead of to Monad.
That's supppsedly how UU behaves as well, but it can result in very practical changes to bottom semantics depending on which interface you use.
Hilariously, I guess that a naive implementation of Nat in Haskell would be a good example of an "exotic" model.
&gt; they may behave better for many real-world monads when implemented in the way that produces an Applicative constraint instead of a Monad one I had to think a bit to work out how that could be: it's because it will often be possible to define `ap` in a more efficient way than `liftM2 ($)` [1] [1] or rather `liftM2 id` as it is implemented here: http://hackage.haskell.org/packages/archive/base/latest/doc/html/src/Control-Monad.html#ap 
Is there an entry in the user guide on this yet?
Note that Simon's patch to actually unload object code only works for static builds, where GHC uses its own home-grown linker. So if you want predictable memory usage for dynamic loading, your projects calling `loadObj/unloadObj` must be `-static`. Not sure if that's a deal breaker, definitely worth pointing out though. In general, 7.8 will use static by default (while simultaneously supporting `-dynamic` without rebuilds!) so this may not be a problem for you. It's also easier to deploy, so perhaps that is OK anyway.
It is phenomenal, but consider that you're running the ghc rts written in javascript in a browser. That's a lot of infrastructure just to get haskell. 
How's that a mistake rather than maybe irritating? It would be even more irritating if we arranged that Monad had every possible superclass as a constraint after the fashion of http://hackage.haskell.org/package/semigroupoids-1.2.6 or maybe something even more complicated that could register the not-quite-Functor hood of typical GADTs and so on.
I'd start with [this](http://elm-lang.org/learn/What-is-FRP.elm). Since you are interested in games, I'd watch [this video](http://www.ustream.tv/recorded/29330499) which explains FRP and builds up to creating [this Mario game](http://elm-lang.org/edit/examples/Intermediate/Mario.elm). From there, check out the [making Pong in Elm](http://elm-lang.org/blog/games-in-elm/part-0/Making-Pong.html) tutorial. Once FRP and signals sort of make sense, the [basic examples](http://elm-lang.org/examples/Basic.elm) in the React section should give more small examples to really make it click, and many of [the intermediate examples](http://elm-lang.org/examples/Intermediate.elm) will begin to cover using FRP for games. Also, we are always happy to help on [the email list](https://groups.google.com/forum/?fromgroups#!forum/elm-discuss), so feel free to come by if you have any questions! :)
For passing data between client and server, my friend started working on a thing called [typed-json](http://typed-json.org/) which aims to be a nice protocol for very different languages to speak to each other (Haskell, JS, Java, Elm, etc.) It is not ready for real use yet. I bring it up mainly to illustrate that talking between languages is a known issue that Elm folks are trying to work on :)
Not for real use is probably good enough for me. My types aren't overly complicated, basically just an ADT around game-state.
That's only really viewable as a mistake if using a narrow kind of hindsight that pays no attention to history. To wit: monads showed up in Haskell long before applicative functors. When applicatives were new and not yet widely used, it made little sense to change the typeclass hierarchy to accommodate them.
This could be enabler to build emacs like programmers editor configurable with haskell. 
if you look on the libraries list threads, and look up the AMP tickets on GHC, yes, measures are being taken to warn about breakages, and make the switch painfree and nice
you're mixing up two topics. 1) cross compiler support for TH 2) template haskell getting awesome new features. They are complementary awesome things, but not the same awesome thing. The best way to follow GHC development is to subscribe to all the relevant mailing lists, and read the documents on trac tickets and the associated trac wiki. More people helping out is welcome. The systematic solution is getting ghc to have good multi target support. That would subsume the TH problem in a nice way and make cross compilers just GHC with more targets enabled.
Don't forget the [GHCJS project](http://weblog.luite.com/wordpress/) that gives you Haskell running in a web page. The answer is that that would be very easy to do what you say. The only real issue is the lack of a decent cross-platform GUI - so you'd need to do the GUI for each app (like you said) - but a lot of people are talking about the need for such a thing, so I expect there will be work on it soon. (Let's make it really functional in its design and show those OO people how to do it right.) People have said that the success of emerging languages depends on them having a special niche, and just being really excellent (as Haskell is) is not enough by itself. Some have said "anything complex" is Haskell's niche, but is that marketable at this point? People are still blissfully unaware their projects are plagued with complexity, because they have nothing to compare it to, so they're not even looking for a solution. And complexity only kills in large projects, at which point it's too late to consider using a different language. I wouldn't be surprised if the ease of **cross-platform development** turns out to be the thing that really kicks Haskell off, and only later will people discover their development time has dropped as well. Today, companies still think it's normal to develop the same application three times (web [Javascript], Android [Java] and iOS [Objective-C]).
no, we just have a great collection of images at FB used for decorating code reviews.
That's exactly what I want. Maybe I'll implement it.
You might have the impression that this is nicer than it actually is :-) In 7.8 is basically what we had before + unloadObj() actually does something now.
heheh, not at all. I've been planning to do something with CMM + the ghc calling convention for my own dynamic code gen! https://gist.github.com/cartazio/5852610 in the pre 7.8 CMM convention this wee teeny bit of code would be // c minus minus code ghcShim{ W_ funptr = r1 ; jump funptr ; } so i'd be able to turn any funpointer that does the haskell calling convention but doesn't look at r1 into a normal haskell function value with a teeny indirected jump and maybe some unsafe coerces and template haskell in one single spot! I really should look at whats done in 7.8, but I think my strategy should allow dynamic code gen shims nicely (at least for my use case) in 7.6 and maybe even 7.4 and 7.2. Admittedly, in my use case, any given application has a fixed amount of runtime code gen, so theres no "space leak" issues from the code gen. 
It easily *could*. The process is simple, just as I described. The question, I suppose, is if it *will*.
Different post, but I didn't actually figure that out either. I think there needs to be an intermediate step based on reducing guess to something closer to `guess (Pretext m) = m exact`, but that didn't go along with my "compartmentalization by product" method, so I didn't explore it fully.
I suspect that traverse is slower than mapM, so it might not be the best thing to do to make mapM = traverse for everyone.
My impression is that the package ecosystem could be improved, so you might be able to find an existing package to improve on. I did this with [Data.MBox](http://hackage.haskell.org/packages/archive/mbox/0.2/doc/html/Data-MBox.html), porting it from String to Text because I needed better performance. Of course, find one you are interested in or need. Working from existing code and having the maintainer check/accept it was also a great learning experience and boost.
No overloaded strings? Seems like one of the more common extensions out there.
xmonad can be recompiled and does not lose its state. the same might happen with an editor without unloading code.
Any implementation in Haskell is exotic. 
I think those who keep recommending ocaml for real world applicability are living in some imaginary world. As for F# and Scala, the main reason to go with them would be if you have already invested a lot in a .NET or Java code base. I don't think I would recommend either if you are getting a fresh start. But at least they have a lot of support behind them. Really, I think Haskell itself is in a decent place already, and the main problem now is public perception. Or perhaps you could work to improve transitions from other languages and existing code. It's always good to have more or better libraries, tools, etc., of course. What do *you* find most painful about using Haskell? I think if you want to help then it would probably be best to do something you personally care about. 
Another alternative is [Forml](http://texodus.github.io/forml/). It's not Haskel, but it is written by a Haskeller who has made some pretty good arguments for using it: 1. It generates clean and fairly efficient code 2. Maybe having a separate language for client side code could be a good thing. Yes, I can totally see the appeal of sharing data structures between your client and server-side code. But to some extent JSON seems to make that less of a problem. I haven't adopted any of these yet, so I can't personally advocate any of them. But if you haven't seen Forml, it might be worth a look.
Allow me to shamelessly plug mollom.com for spam fighting :) -- which I think is allowed since it is you first use case. Downvote me if not. (disclaimer: I am friends with the company founders)
The overall campaign Galois (Haskell company) supported has about a day left, and seems pertinent given http://www.reddit.com/r/haskell/comments/zxmzv/how_to_exclude_women_from_your_technical/ last year.
 class Stored type where _getMap :: State VarStore (Map Int type) _setMap :: Map Int type -&gt; State VarStore () You can make instances for all representations and implement: newVar :: Stored type =&gt; State VarStore (Ref type) Which is the generic equivalent of you `addIntList`. Adding a new representation is as easy as putting it in the type and making an instance for it.
I'd suspect the opposite, for the same reason that `fmap` is usually faster than `liftM`.
No, I think you are right that working with multidimensional arrays in Haskell is a bit of a pain. `array` for "low level" multidimensional arrays and `repa` for "high level" multidimensional arrays *can't* be the end of the story. Both are quite painful in their own ways.
I'm surprised this even needs to be said. I mean, standard model theory doesn't specify anything interesting about the nature of the models. Usually you have to throw in extra constraints to make sure desirable properties hold. For instance, `()` is a perfectly good model for `Nat` that interprets `Zero` as `()` and `Suc` as the identity on `()`. But that aspect -- something called confusion, where you have two different constructors `C1` and `C2` such that `exists x0, ... xm, y0, ... yn. interp (C1 x0 ... xm) = interp (C2 y0 ... yn)` -- is often undesirable, so you add axioms against it. The other aspect is called junk, where there are elements unconstructed by constructors, and again you also explicitly add no-junk axioms against this. But maybe I'm missing something here.
Thanks for posting this! We're really proud to be part of what's turned out to be a great fundraising campaign for them.
&gt; However, wherever I look, Haskell is eschewed in favor of languages like O'Caml, F# and Scala when it comes to productive development environments. Many complain about the libraries, the steepness of the learning curve, the lack of examples and thorough documentation, and more. Really? It's 2013 here ;)
Mainly, I would recommend hanging out on #haskell and helping to teach other people who are learning.
It's like HJScript but with continuations and a threads abstraction.
DB/Web is really what my focus is on. I've loved what I've seen of Yesod, but I know that there must be so many gaps. I use rails day-to-day and the availability and diversity of libraries is staggering. A plan of mine is to create something like [spree](https://github.com/spree/spree) for Yesod, but quite honestly, I'm too new to Yesod to start yet.
Check out the `linear` package for multi-dimensional vectors. http://hackage.haskell.org/package/linear
We use Fay at FP Complete, it suits our needs. Pros: * Trivial to setup/work with (and for Yesod, especially with yesod-fay) * Compiles enough Haskell to get the job done * Fast enough to develop applications in * Trivial FFI for easy hacking * Produces small output * Predictable/easy to debug when things break * It scales; we have &gt;10k lines of Fay code across 60 modules Cons: * The thing it lacks is type-classes, which tests the limits of abstraction. * Doesn't know about fixity of operators. * Experiences name resolution/import problems sometimes. * Doesn't have source maps (yet, I wrote the hard part, just never made a patch). * The lack of type-classes incurs an overhead for automatic translation of types via the FFI (if you need it), which means larger code output that would otherwise not be there. I'm looking into Haste and a from-Core version of Fay called Fore privately, but no release plans. Given that no one else has really used the other compilers to build substantial applications like this, you'd be better off trying them out yourself, benchmarking, trying to write some demo apps. Regarding your problem, in Fay you'd just pass your object in easily: data Foo = Foo { foo :: Int, bar :: String } passToJavaScript :: Foo -&gt; Fay () passToJavaScript = "console.log(%1)" calling `passToJavaScript (Foo 1 "hello")` would pass `{instance:"Foo",foo:1,bar:"hello"}` to `console.log`.
I believe more tutorials and textbooks are the answer. Th easier it is to learn Haskell, the more presence Haskell has on bookshelves, the more attention Haskell will get. I used to think of textbooks as something I could never do, something other people did. But I found Learn You A Haskell, and was never the same. If you love Haskell, give it love back, by writing how people can actually apply it practically (Real World Haskell). Or Haskell for the Evil Genius. There are books like this for other languages as well--Learn You Some Erlang is a great read. Don't be afraid to have fun, or be quite dry. Write the tutorial you wish you had read years ago, and the community will be that much improved. Finally, release a package on Hackage whenever you can (see Hackage / genetics for a simple example). Perl's popularity is largely derived from CPAN's sheer size, and Node.js is seeing incredible growth due to the ease of submitting NPM packages.
&gt; I was wondering what kind of things I/we could try to do to make the adoption of Haskell easier for programmers from beginners to experts, from hobbyists to full-time professional coders. [Hackage2](http://www.reddit.com/r/haskell/comments/1iki8p/how_to_help_develop_hackage2/)!
Plus, we're finally getting around to fixing it.
Once Applicative is a superclass of Monad, we'll likely go through and make mapM etc, just be aliases for traverse and their kin with no stronger type signature. It solves the issue raised in Simon's slides and generally eases the transition.
I have an app written in Clojure (logic) and Java (Swing stuff). Of course it runs on the major desktop OSes, but that's about it. I've thought about going the Clojurescript route to get it on ios (with them opening up the Javascript Frameworks in ios7 this will be even easier), but if I could go native, then all the better. I don't mind doing native GUIs either. Could hit the desktop arena with a QT interface. Would only then have to do an ios version. I assume that eventually there will be an android target as well.
I would be happy to take a look. Can you please post an example that reproduces this? I think it is fair to say that the GHCJS Gen2 generator has had the following priorities: * Correctness (we are trying to match GHC) * JS FFI * JS Performance * JS Size * Compile time * Compile time memory usage But we are getting to the point where we are thinking more about the size of the JS output and compile time. Good example use cases will help us tune it.
You would be my hero forever. Well, you already are, but you'd earn a couple of oak leaf clusters for your hero medal.
Coming to Haskell as a software engineer rather than a mathematician, the most painful thing for me by far is the documentation. While some blogs are quite helpful to workaday programmers, they can't and shouldn't serve as a comprehensive source of documentation. haskell.org should fill that role, but for me, it's often less helpful than it could be. The writing style tends to be quite terse and tends towards the mathematical. Usage examples are rare or very brief. More examples would be extremely helpful. As would more verbose explanations. The more one uses Haskell, the easier it is to understand the terse docs. I can often figure out what I need to know just by reading the type signatures of library functions. But when I was just getting started, that approach didn't work for me. The docs made the language seem unwelcoming to beginners. I stuck with it, but I suspect others have tried and given up for this reason. More approachable documentation, geared towards the needs of working programmers, would probably help with popularizing the language.
It looks like they're essentially doing just graph operations, so they deal with integer keys and no text.
If you are interested in UI stuff, you can see if somebody is trying to write up SDL 2.0 bindings and help them.
The Vars need more operations. These operation depend on the type of the variable. So I was planning to have type classes that specify these operations. Something like this http://lpaste.net/92354 The reason they are separate types is because the implementation of the operation might get big. I thought it might be nice to specify them in separate modules. But it might be easier to use different type constructors.
That is great, thanks.
Are you serious? Do you have any idea who this is, what he's doing, and for whom?
You can find the proof in the Appendix C of [Functor is to Lens as Applicative is to Biplate: Introducing Multiplate](http://arxiv.org/abs/1103.2841v1).
I think Scala is an excellent multi-paradigm language. I also think there are reasons for choosing it other than being heavily invested in the JVM.
Decisions can be mistakes even when they're the best decision that could have been made at the time with the information available. I place no aspersions on anybody involved in any part of the process... but I'm glad it's changing.
If you're interested in GUI programming, I'm currently working on [threpenny-gui](https://github.com/HeinrichApfelmus/threepenny-gui), a small GUI library that uses the web browser as a display. New examples are always welcome. :-)
From a newbie's perspective and coming from a Software development background of consulting and product development, a lot of things you can learn about Haskell are very difficult to relate to actual advantages when doing "classical" Software dev, because many descriptions are quite happy to stay in the realm of abstractions and relationship to pure math. 
Join us: Programming Haskell (Facebook group) https://www.facebook.com/groups/programming.haskell/
Yep, the FFI is the only difference. GHCJS makes use of most of GHC optimizations (that are done at the Core level) and compiles STG to JavaScript. It also features a fully concurrent runtime (in JS) GHCJS has not been getting much love and there has not been much tutorials on using it. I am positive that this is going to change with 7.8 release :) 
Doesn't Fay also has its own runtime? I had that impression when I looked at it maybe half a year ago
If you used the prebuilt version from a few weeks ago you might be happy to hear that the latest builds are much faster. The optimizer used to be unacceptably slow, so I rewrote it, and also changed some internal representations from String to Text, for speed and memory usage (unfortunately there still seems to be a leak somewhere) We currently don't minify top-level names but prefix everything with `h$` or `h$$` so users can recognize and rename them with they own deployment scripts, most minifiers (except closure compiler in advanced mode) will not rename them automatically. Perhaps we should add more minification options to the compiler itself? By the way, there seems to be a bug in uglifyjs, choking on our `lib.js` file, which uses `arguments` at the top level (to get command line arguments if the file is run in jsshell).
Come on, OCaml is a nice language, it has it things and it is being used in The Real World(tm). The upcoming real world ocaml book also looks promising. 
If I understood correctly, the query operations are supposed to be "pure, sort of". Yeah, there is IO involved, but they don't trigger destructive updates on the backends. And that faciliates the move away from explicit concurrency. I think these kind of "pure, sort of" operations will crop up a lot in client-server scenarios and I wonder what patterns will develop to take advantage of them.
Given that you're planning for 4.0.0 to be the "last significant API change", maybe it would be worth doing a 3.99.99 preview release to get some final feedback on the API before freezing it?
In the monad's bind, the two sides are `m a` and `a -&gt; m b`. So you have run the left side to get the `a`, before you can apply the right hand side to it and then run that. The right side depends on the result of the left side. In Applicative, the two sides are `f (a -&gt; b)` and `f a`. We can run both sides independently, because neither depends on the other. So this lets us explore the whole expression and collect several data fetches to execute at once. Does that help?
I think there is a gaping hole where a Haskell book (aimed at absolute beginners but with a more "program-oriented" approach than LYAHs "expression oriented" approach) is missing and I am planning to do something about it. It's just that between my tutoring work and university, the work on the book is extremely sporadic (currently around 20 k words, I want to have roughly 150 k), and I don't want to make it public until it's done or nearly done.
What is the size/overhead of the RTS? What is the threading model mapped to js? Are js workers used?
Then what I will do is delay the release a week and make an announcement on `/r/haskell` to let more people preview it ahead of the release and offer feedback.
A GHC Android cross compiler won't be far away. GHC-iOS was the really hard part, but now all the hard work is done. I want an Android version of my application, so *someone* will do it quite soon (maybe me).
The RTS is more or less here: https://github.com/ghcjs/shims/tree/master/src , so currently ~100kB unminified, the biggest file (40kB) is a generated file with unicode character information (that's incomplete in JS). It depends on JSBN (for integer-gmp), some bits of the google closure library (for Map, Set, md5, 64 bit ints) and typed array emulation for older browsers. There's also a generated file with function application paths that's not included there (the size of that can be tuned, generating more specialized application paths can be faster, but you get more code) Threads are implemented as lightweight threads, scheduled in JavaScript from a main loop. Every thread has its own stack in a JS array (so you're not limited by stack limits imposed by JS engines, and StackOverflow can be a catchable exception), and arguments are passed through global variables (by far the fastest option in the things I've tried) The main loop regularly yields to let the browser run other things. Async IO in foreign functions suspends a Haskell thread, wakes it up when the result comes back. Blocked threads, or threads running a (pure or impure) computation can be interrupted by throwing async exceptions to them, for example throw System.Timeout.timeout when you find that calculating that big fibonacci number is taking a bit long. Threads scheduled with this model are always run in the background, JavaScript code expecting a result from a thread can only do so through a callback. We currently have an alternative option, synchronous threads, that get run to completion (when possible, if they don't do blocking operations) before the call that starts them (`h$runSync`) returns. Those are useful for event handling, and smooth animations, while still keeping the functionality of the full concurrent runtime in the rest of the program. We used to have a non-concurrent runtime as well, with a bit more compact generated code, where this property also holds, but with some additional limitations (JavaScript stack limits, no async exceptions), but that has fallen a bit behind after the concurrent runtime switched to more and more customizations from GHC 7.8 and JMacro. I plan to revive that soon though (not that much code needs to be changed from the concurrent one, and we can benefit from the cool FFI improvements and the new optimizer), perhaps after the busy weeks before ICFP. Web Workers are not used in the RTS because they don't share memory with the main thread. You could interact with them like you could with processes, for example by letting some Haskell thread wait for a result of a worker and then putting it in an MVar.
You have the same thing with yi. However, it does not work with hsenv, unfortunately (and that's a big limitation).
A while ago I found [a haskell port of Lazy Foo's definitive sdl tutorial](https://github.com/snkkid/LazyFooHaskell). That's the closest thing I know. EDIT: I forgot that the port only contains the code itself. For the tutorial itself, you'll need to cross-reference with [the original tutorial](http://lazyfoo.net/SDL_tutorials/). ANOTHER EDIT: I guess I've only just now read the full OP. I was responding to the title; but I don't know that the Lazy Foo' tutorials are very good as a beginner haskell tutorial (not the least because the actual text isn't written about haskell). As an SDL tutorial, though, they're pretty good.
Yes I am ;-) 
It has very few redeeming features for the real world. Libraries are pretty much nonexistent. Language progress is slow. The documentation for the language, compiler, standard libraries, and other libraries is terrible. There is little community to speak of. The language and culture are surprisingly hostile to abstraction, at least if you care about performance at all. It's more difficult to teach OCaml to programmers who only know imperative languages than most probably expect (I would argue that it's about as difficult as Haskell, just for different and less interesting reasons). There is little editor support or tooling. The REPL is probably the worst I have ever used in my life. Debugging and profiling are a nightmare, to the point that it's most effective to just guess what the problem is until you figure it out. There are a few necessary libraries for asynchronous I/O, due to the lack of runtime support for these things, and they tend to obscure debugging sessions even more by destroying stack traces. The worst 10% of OCaml's error messages are ten times worse than the worst 10% of GHC's error messages, and the best of each are merely about the same. It has no support for parallelism, and the maintainers are hostile to the very idea. The only redeeming features I can think of for real world applicability over Haskell are that you can naively port heavily imperative code to it, if that's really what you want to do, and that it's easier to write highly modular projects, thanks to its pretty awesome (in terms of power, not really in terms of ease) module system. I don't mean to insensitively rip into it as a language, but elevating it above Haskell, Scala, or F# for the real world is pretty silly.
OP didn't really have graphical PC games in mind, as far as I gathered from the submission. It sounds more like he was thinking about simple console games (guess the number, mastermind, a text adventure, a card game, a quiz – you get the point) which should be just as easy to create with Haskell as with Python.
I'm not going to push back on Scala or F# too hard, because I have not used either of them in the real world. Would you mind giving some reasons other than the JVM that Scala is good for the real world relative to Haskell? It could be educational for me and probably for other readers.
&gt; We will need a haskell equivalent to "pygame" before any such book could be successful. I'm trying! It's just going to take a while.
I wonder if STM monad uses (or could use) the same "free monad/applicative" approach. If `STM a` computation is composed using only &lt;*&gt; and &lt;$&gt;, then, you could know a priori what TVar's are read and written, so you can make STM transaction succeed without re-execution? 
I've heard /u/Tekmo mention on more than one occasion that he'd like to see pipes (particularly, pipes-concurrency) put to use for gaming.
Yes, yes and yes. Do you know what mollom does? And how? And for whom?
I was thinking the same thing although I haven't used pygame that much, the book I referenced as getting me into programming used it pretty heavily toward the end and pretty extensively in the 2nd book. I could see this really getting people to make the jump. Its hard to switch from C# or Python where you have so many libraries and code snippets on Stack Exchange (talking about the lowest common denominator of code hacking I belong to and not a developer who writes everything they need themselves)
Please Please do!
Yes lol you understood my post exactly (although a Haskell version of Pygame would be nice). Those console games are exactly what my python book had (I think this is pretty common in other programming books as well, but I'm not sure). I really hope someone in the community takes this up. I like the no-starch books(like the learn you a haskell and learn you a erlang) that usually seem to have the entire book free, but gives you the option of paying for a hard copy to support the author. Haskell is so cool from what I've seen so far, I just think it needs these type of resources rather than just the cut and dry manuals that exist in abundance. There are a lot of coders I know that write beautiful and efficient GUI applications everyday(not to sell to customers, but to use at work for VERY manual processes) that got their start by grabbing code chunks from online blogs and then fusing it all together with the syntax of the language being practically the only thing they knew of the language(I'm sure this is painful for some hear to read lol). Most of these people never read a full length academic style book that Haskell seems to love. I think the language would explode with popularity if we could just get enough people to get into it, and start posting code online...some good beginner style books would be a good start.
I'm already writing up a post about this. It's actually designed to not be very `pipes`-specific so a lot of the ideas can be reused by other libraries, too. A lot of my posts are inspired by game programming, so this is a subject near and dear to my heart.
&gt; It might look like 10 people uniformly want Markdown but it’s actually the case that 2 people want GitHub Markdown, 2 people want Pandoc Markdown, 2 people want original Markdown and so on and so on. From my experience (I chose and implemented support for Markdown as the doc comment format for [Dart](https://www.dartlang.org/articles/doc-comment-guidelines/)) it's actually the case that 10 people want GitHub markdown.
thanks drb226, I looked at a tutorial for pipes and don't fully understand. take' :: Int -&gt; Pipe a a IO () take' n = do replicateM_ n $ do x &lt;- await yield x lift $ putStrLn "You shall not pass!" it had the above snippet of code. I'm guessing it basically will let certain data in and print out the string "You shall not pass" if other data is input? http://hackage.haskell.org/packages/archive/pipes/2.1.0/doc/html/Control-Pipe-Tutorial.html
I couldn't agree more...not near enough examples. I almost need "The Complete Idiot's Guide to Haskell" lol
The thing is I disagree with a lot that's said in the post, but I also don't care enough to write any counterarguments. I hate writing in Haddock syntax, but I don't do it often enough for this to be a big issue for me. However, we may end up with people forking or making alternatives to Haddock.
Everything you just wrote is perfectly in line with the philosophy of the book!
How do you pronounce that?
In Haskell's case, most of the markdown tools use Pandoc markdown, for obvious reasons. I know I would prefer that to GitHub markdown.
Great, thanks!
Scala and F# still have a lot more of an "enterprise" vibe than Haskell. FP Complete is working hard to bring Haskell up to par in that regard, but I don't think we're there yet.
I really should reference your paper in these tutorials. It'd been so long since I read it that I'd forgotten that I'm basically just rehashing some of the arguments there in less general form. I'll add those references to both tutorials.
I don't particularly care if the format we get is markdown. I simply want some way to express the formatting. That could be embedded html fragments in callouts, embedded markdown under some markdown block incantation, pretty much anything! Right now I can't put a labeled hyperlink to a document in my documentation. I can't do any formatting whatsoever and it is quite frankly rather pathetic. Give me anything that lets me put attributes on images, do an embedded callout to a nicer markup, or even just format a link as something other than its naked text and I'll use it. Hell, I mean as it is I've tried horrible things like embedding data url's for svg documents and custom png's to try to get the formatting. I'm willing to bend over backwards if I can get the functionality of decent documentation content on the hackage website. Markdown is a means, not the ends. Getting the markdown notation doesn't matter. It'd be really nice in that it'd make it easier for folks to write documentation, but that is a side-goal to being able to express the documentation we want to write at all. [Edit: Fuuzetsu called to my attention that apparently &lt;http://example.com This&gt; is how you can write hyperlinks with text in them. That removes a lot of my concern. Sadly this isn't covered in the [haddock documentation](http://www.haskell.org/haddock/doc/html/ch03s08.html)]
Indeed, so much of the reasoning here seems to be based on the impossibility of fully leveraging someone's familiarity with markdown. That is a fine goal, but it is not the only goal. That would be the icing on the actually-usable-markup-support cake. The icing didn't come out right, so now there is no cake?
Well, I think I should point out that the tutorial is based on the out-dated 2.1.0 version. If you want to learn pipes, start with the soon-to-be-released [4.0](https://github.com/Gabriel439/Haskell-Pipes-Library) version. I'm not sure if an online version of 4.0 documentation exists somewhere, you can clone the repository and run `cabal haddock` to generate the documentation (which includes the tutorial) and start from there.
We are very close to being able to generate a strict wrapper with automatic transcoding of values, as far as I know [this PR](https://github.com/faylang/fay/pull/327) is the last piece for transcoding, and we can then just generate Strict.foo = function (arg) { return serialize(foo(deserialize(arg))); }. I think this will cover most use cases (we'll see!), and you always have the opportunity to do this manually if you want more control than Automatic transcoding gives you. 
This is by far the worst thing about Haskell. I can't count the number of libraries that I've tried to use but was thwarted by "documentation" that consists of nothing but type signatures and 0-1 brief sentences describing each function.
I just want to make sure that people are clear this isn't a retreat from the goals of the SoC. As per the proposal that fuuzetsu agreed to implement, the main features we were looking for were code cleanup, standardization, and flexibility: https://gist.github.com/Fuuzetsu/81253ba7d0c51ac88052 Where and how we'd start to support other styles of markup was deliberately left as an open question. Now that the refactoring work is largely done, its a question we can begin to ask more seriously, and hopefully things are now more extensible in a way that will let us start to try different ideas out. I tend to feel that a 'one stop shop' for haddock syntax reference (perhaps on a single haskellwiki page) would probably, in itself, be a big win.
In defence of OCaml (which we use at the company I work for): * For a REPL, try utop. * Editor integration for Emacs is pretty OK according to my colleagues. I use Merlin in Vim since some time, and that's OK'ish as well (comparable to what I get for Haskell with ghc-mod etc). * Profiling is getting better lately, e.g. using valgrind/kcachegrind. * Community &amp; tooling is enhancing quite a lot lately (e.g. thanks to OCamlPro) Not to say I like Haskell (as a language, community, ecosystem,...) more. Would love to have OCaml-style modules &amp; functor support in Haskell once in a while though :-) [edit: formatting]
He pronounced it very close to how you'd pronounce C.
And online version of haddock documentation is just so out of date :( I really hope someone can update the haddock web site very soon. I think it's important to keep the information on core tools up to date.
Thank you, the paper looks interesting. Is there an online version of the paper that includes the appendices? 
That helped a lot, thanks! Makes a lot of sense. I've also started reading your "Parallelism and Concurrency in Haskell" book- fantastic read!