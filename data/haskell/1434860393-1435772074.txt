It isn't "better" (define better) and actually worse in some aspects (certainly less efficient). It is just a different style, with potential advantages/disadvantages. The idea is to find, inside your function, some pattern that is generic enough to be given a global name. I've found this leads to more readable code (the last `separateParens` the easiest one to read) and reveals interesting abstractions that could be useful later (i.e., `insertAfter` and `insertBefore`). This practice changed a lot my workflow, but I think it improved my overall productivity and made Haskelling a lot easier. I'd swear naming things become one of the hardest parts of it now. (My internet is **bad** today, I hope it didn't go twice.)
If you need monadic testing [`Test.QuickCheck.Monadic`](https://hackage.haskell.org/package/QuickCheck-2.8.1/docs/Test-QuickCheck-Monadic.html) has good support for it. Simple use case is testing the result of `compileRunRead :: Exp -&gt; IO Value` against a pure test oracle `eval :: Exp -&gt; Value`: prop_eval :: Exp -&gt; Property prop_eval exp = monadicIO $ do result &lt;- run (compileRunRead exp) assert (result == eval exp) I recently added some examples to the documentation (focusing on `.Monadic`) to lower the barrier of entry, users shouldn't have to dig through papers to use libraries :)
The last separateParens function is much easier to read. 
Thanks for the post, I'd forgotten to post it on reddit!
Is it just me, or does the programming languages community have a slightly higher than average propunsity for word play? ;)
The artifact evaluation process only happens after a paper is accepted, it has no impact on acceptance and if the artifact fails its evaluation the paper is still accepted.
Looking through the repo, there are 3 different versions of the code in 3 different directories. What gives?
Using the extra package (http://hackage.haskell.org/package/extra) you can write: replace "(" "( " . replace ")" " )" Which is only one line, with no new auxiliaries, and I find clearer.
I meant "smaller" in the sense of complexity. GHC has quite a complicated build system, and when you change something, you can't simply hit :r in GHCi, because complex steps might have to be taken to get all dependencies compiled again.
I vaguely knew that Ed was working on this, but it wasn't until his Lambda Jam talk that I got some idea of how much speed he was aiming for (and has achieved). Apparently this isn't just O(n) - it has better constant factors than quicksort. The first half of the talk was on the well-principled basis that guided the API design. Most of the second half was on how he got the speed. Both halves were interesting, but the second half cannot be unseen :) Hopefully someone posts the video when it goes online.
I thoroughly enjoyed his Lambda Jam talk, very excited to see this released on Hackage. Quite awesome to have someone talk about category theory and custom primops all in one talk!
 isSorted = and . (zipWith (&lt;=) &lt;*&gt; tail) sort = head . filter isSorted . permutations EDIT: Just to clarify, this was meant to be funny. That is just about the worst way to implement `sort`; it's almost as slow as "random sort". It is meant to illustrate the point of /u/quchen, which is that some things are inherently complex in a way that makes it best not to implement them in a single line even though it's possible.
which is clever, but, as any regex-based conversion, hard to reason about in edge cases. e.g. what would be the result of: replace "z" "zz" "zzz" ?
(+), (-) etc. for standard types like Int are not lazy. You probably misunderstand how lazy evaluation works. A thunk for `x + y` is not a consequence of `(+)` being lazy, because by that time `(+)` hasn't even been called yet. It's the outer function that hasn't forced the result. 
That is often not true
`Applicative` comes with an instance for `(-&gt;) a`. It works like this: if `f` is a function of 2 variables and `g` is a function of 1 variable, then `(f &lt;*&gt; g) x = f x (g x)`. That's actually clear if you look at the type signature of `&lt;*&gt;` and think about what that would be if the applicative type is `(-&gt;) a`, i.e. functions whose input type is `a`. If you've ever looked at Haskell Curry's combinator calculus, `&lt;*&gt;` for this instance is the `S` combinator. EDIT: Changed "the untyped lambda calculus" to "Haskell Curry's combinator calculus".
I think that's the way most people program, except they use `where` or `let` block to store the local declarations (unless they get used somewhere else). You will rarely find single expressions spanning several lines. 
Is there any way to observe this? I opened ghci and tried a few things but couldn't really come up with anything. I suppose that there are no numbers you feed to the LHS or RHS of (+) that result it it ignoring its other argument.
 Prelude&gt; (undefined + undefined) `seq` () *** Exception: Prelude.undefined Prelude&gt; (undefined : undefined) `seq` () () 
And here is [the paper](http://www.diku.dk/hjemmesider/ansatte/henglein/papers/henglein2011a.pdf) (that you could actually understand)
I've seen of this library. Is it something largely used or better to stay away from ?
&gt; isn't basically all constructors lazy in their arguments Yes, assuming the argument isn't given a ! in the constructor definition. &gt; all functions strict in every argument they don't outright ignore No, not at all. `maybe` is lazy in its first two arguments but it doesn't "outright ignore" them. It does ignore them in certain cases, which may be closer to what you are getting at! Prelude&gt; maybe undefined print (Just ()) () Prelude&gt; maybe undefined print Nothing *** Exception: Prelude.undefined 
How did you end up with such an old version of cabal? Haskell Platform gives you one that *does* understand `sandbox`. There's an alternative installer called "GHC for OS X"; I don't use that, but I'm quite sure it also gives you a version of cabal that understands `sandbox`. Whatever non-standard thing you did to install your broken Haskell tool set, you should undo, install one of the standard installers, and start again. It should work fine then. No, do *not* use `--global` and do *not* use `sudo`. Do it exactly the way the `leksah` site says.
functions with irrefutable patterns are non-strict in the corresponding argument...
It's not arithmetic per se, but its application to large data structures. The canonical example is `foldl`, which causes a space leak: foldl _ acc [] = acc foldl f acc (x:xs) = foldl f (f acc x) xs This tail-recursive, as you can see, but `(f acc x)` doesn't immediately get evaluated (regardless of what f is. It could be undefined too). Only at the base case `foldl _ acc [] = acc` may the evaluation start, and by that time `acc` will be a massive thunk.
That's true! I can't believe I forgot about such a simple concept. I blame sleep deprivation!
This is a great example of the sorry state of testing IO in Haskell. In dynamic languages like python or JavaScript, and even in java using reflection, FakeState and its World instance is roughly one line of code. A stub, using reflection should be able to mimic any interface to the point that it is possible to query the stub regarding what arguments it was called with and similar. This does not require any boilerplate code in other languages. I think some stubbing library is needed for Haskell, possibly using TH or Generic.
Sure, this is roughly the definition of strictness: strict = never ignored, lazy = sometimes ignored.
There are three most widespread evaluation schemes: call-by-value, call-by-name and call-by-need. Call-by-value: all arguments to a function are evaluated once before the body of the function is evaluated, and the resulting values are substituted into the body. This is also called "strict evaluation" and is used in e.g. C and Java. Call-by-name: no argument is evaluated before the body of the function, but each time an argument is encountered in the body, it is evaluated. Thus some arguments may never get evaluated, while some may get evaluated more than once. I'm not sure where this scheme is used, but it is classified as "non-strict evaluation". Call-by-need: no argument is evaluated before the body of the function, but if an argument is encountered in the body at least once, it is evaluated once and once only. This is the default scheme used in Haskell. It is also called "non-strict", but it is distinct from call-by-name.
&gt; Sometime , you KNOW the partial function won't crash This line of reasoning does not take you to a happy place. I argued why [here](http://chrisdone.com/posts/boycott-head).
Well of course you can implement most Haskell language functions as a single line, but the lines can become very long.
Still on my wishlist to get Catch ported to GHC. 
How does this relate to the Omega(nlogn) barrier? Are the values assumed to be bounded?
Why not use something like fromMaybe hardcodedDefaultValue . msum $ [ defaultValueFromLocalConfig , defaultFromGlobalConfig ] in that case? Look ma, no partial functions!
Here is a quick layman summary of what I understood of the first part of the paper: Previous FRP implementations that leaked memory could be seen as specifying a function like this to combine behaviors and events: type Behavior a = Time -&gt; a type Event a = (Time, a) whenJust† :: Behavior (Maybe a) -&gt; Event () -&gt; Event a `whenJust†` can't forget the past because the event could be in the past. `Event ()` expands to `(Time, ())`, so really you could write: whenJust† :: Behavior (Maybe a) -&gt; Time -&gt; Event a but looking at the types, the result after applying the first argument is really just a behavior too: whenJust :: Behavior (Maybe a) -&gt; Behavior (Event a) That's pretty much where I got up to so far, I'm assuming the paper goes on to restrict `Behavior` so that you can only get the `Event a` out of it by sampling the behavior via `sample :: Behaviour a -&gt; Now a`. `Now` seems to be where current values and future events (possibly produced by I/O kicked off "now" but resulting in said future event via an `async` function) live.
Sure, but I'm curious about generalised folds vs catamorphisms specifically - throughout the video Wu talks about them as if they are different (X works for nearly everything except generalised fold) and the zoo of morphisms slide at 11:04 has them as different things. 
The linked article on this thread does a good job -- you can think of this as a "generic radix-style sort" where you trade off space (for the radix table) for time.
To go any faster than linear on input you need an index, right? Discrimination works on unindexed, unsorted data. So I don't think this will magically fix that -- it just captures well the case where you want a join-like thing on entirely unindexed data.
The way complexity for sorting is usually measured (with O(n log n)) is weird compared to most complexity measures. For sorting it is related to the number of input elements, whereas for a lot of complexity measures you instead measure the input size in bits. If you do input size in bits for sorting you'll get O(n). 
That's almost as efficient as the original sort: import qualified Data.PQueue.Min as PQ sort :: (Ord a) =&gt; [a] -&gt; [a] sort = PQ.toList . foldr PQ.insert PQ.empty main = print $ sort [1,3,2,8,7,9] 
ah, thank you for clarifying. it seems like these generalized folds give you a way to fold up non-regular datatypes. i'm not too familiar with the species, other than through [Twan Van Laarhoven](http://twanvl.nl/blog/haskell/non-regular1), but [this](http://staff.city.ac.uk/~ross/papers/gfold.html) paper (which i am reading just now) references several use cases for such non-regular datatypes
Where this comes to bite me (that is, where Haskell's restriction bothers me) is purely syntactic: Sometimes, I want to write something like `map show [1, 'a', "b"]` instead of `[show 1, show 'a', show "b"]`. I almost wish there were an `-XEphemeralHeterogeneousLists` extension (almost because it's definitely not worth the development effort) for this case.
What would it look like?
You can create an index in O(n log n) time so it seems that discrimination is only worthwhile in the cases where you need uncompromising simplicity.
TL;DR: That “barrier” assumes that `GT|EQ|LT` is all the information we can get from comparisons. And the essence of discrimination sorting/grouping is that we could get a little more.
The site is built with Scotty, postgresql-simple, blaze-html, and lots of other Haskell libraries.
Interesting. It is actually really hard to get `a+b` to get its own thunk under GHC now that I tried, regardless of the evaluation status of a and b. I don't think I succeeded.
They don't really affect things here. The list is just to get the data in. Behind the scenes it uses some fast array-like pads to do the bucket sorting. Putting them in vectors often means allocating something extra and from my tests almost always costs me in terms of performance, because I have to recurse at other list types, which means all new vectors all over again.
If you're doing this for a library to be released to Hackage, all of the standard discussions about PVP, upper bounds, etc, come into play. If on the other hand you're writing an application that isn't going to be shared, I think dependency pinning (via Stackage, `cabal freeze`, or equivalent), makes a lot more sense.
O(n log n) is still bigger than the O(n) above! I generally advocate for creating indices, but we occasionally get data into a process from several different datasources that you need to merge with no common index, and there discrimination can ensure that you have at least a linear time bound. Once I can get productive stable unordered discrimination working then it can even start spitting out results online.
It was a bit laborious to write, but the power-to-weight ratio of this infrastructure has been astounding. We haven't made any major changes to it in over a year. We have just one FakeState across the whole application that implements everything. We can, for instance, run SQL statements in pure tests and still thread tests across cores as though crosstalk were impossible. (because it is) The end result is that engineers working on new features don't directly interact with the definition of FakeState. They don't specify what to mock or how. They just write "runFakeWorld def myAction" and their tests are perfectly reliable and fast.
This is the first I had heard of this. This is what I was hoping for. Does this work with (-) and (*) as well? (I assume it doesn't happen for (/) because `b` could be 0)
Isn't there some way with a couple extensions to do something like map show ([1, 'a', "b"] :: [forall a. Show a =&gt; a]) ? I know that exact syntax doesn't work, but I thought I remembered seeing something like this.
How do you define 'arithmetic' and how do you know whether there is any 'arithmetic' somewhere inside a thunk without evaluating it? This'd destroy laziness everywhere. How would you go about evaluating ``` fibs = 1:1:zipWith (+) fibs (tail fibs) ``` ? I'm not even touching the problem of figuring out whether something may throw exceptions, go into a loop, or be very expensive to evaluate optimistically.
Ok, I misunderstood what you were suggesting. There is still a problem of forcing an expensive/non-terminating/exception-throwing operand that wasn't meant to be evaluated. But for the case when operands are evaluated, it works that way, as u/augustss explains.
I've tried (I thought `-XImpredicativeTypes` might be the key), but I haven't succeeded.
Yes :)
&gt; I'm not sure where this scheme is used, but it is classified as "non-strict evaluation". I guess you could say that GHC haskell is partially call-by-name: when inlining, I think it sometimes happens that ghc duplicates the computation of one expression if it's cheaper to compute than to store. 
What about -XRebindableSyntax? I can imagine perhaps allowing something like `[1, 'a', "b"] (each show)` at the expense of also having to write things like `[1,2,3] list` by changing cons into something that builds up a higher order function.
Maybe "avoid" is a strong term. It's a bunch of things, some unrelated, some unportable, some superseded by functionality in libraries like filepath, many still useful. It's fine to use, but most things in it could be found in a better form elsewhere. The only things not anywhere else are the kind of things I put in the extra library. 
I would still make those global functions local in a where clause unless they are needed in multiple places.
If you're considering it a syntactical convenience problem, the Lispy solution is e.g. {-# LANGUAGE TemplateHaskell #-} import Language.Haskell.TH hmap name xsE = do xs &lt;- fmap flatten xsE mapM (appE (varE name) . return) xs &gt;&gt;= return . ListE where flatten (AppE xs x) = flatten xs ++ [x] flatten x = [x] And then: λ&gt; $(hmap 'show [| 1 'a' "b" |]) ["1","'a'","\"b\""] 
Yes, you should! :D
This site looks really nice. Do you have guide on how to deploy it on the web? 
They're useful for binding to heavily OO APIs. I use type-level [heterogenous lists](https://github.com/deech/fltkhs/blob/master/src/Graphics/UI/FLTK/LowLevel/Dispatch.hs#L47) to give the user something that feels like polymorphic dispatch. Not sure how I could have achieved that otherwise.
There are still a number of optimizations I want to apply to this code. It could be much smarter about 'stopping' when it sees it is sorting an equivalence class with one element. (It needs to do so for the right asymptotics.) I've also begun working on a [promises](http://github.com/ekmett/promises) package that provides the 'lazy i-var' semantics i need to properly implement online stable unordered discrimination. Once I can get that worked out we'll have something that the origin discrimination papers do not: a way to work with data in a streaming fashion.
Thank you. I deploy it in a very simple way. I git pull on the server, run cabal build, and then restart the upstart job for the web server process.
Yes, it works for (+), (-), and (*) for Int (maybe Integer too). And it could work for all floating point arithmetic.
I doubt 99.5% my users will care. But if it helps the Haskell community, I'll do it.
Thank you. I think your advice is sound. I think I'll put up the info in a more discreet way. 
Maybe you misunderstood me? No dynamic parts exist in production.
Thank you. 
See, that's not a valid program, though. `map` is not merely a syntax extension: it has semantic meaning and semantic requirements. If this annoys you, I have good news: it becomes exponentially worse with dependent types!
Not exactly. With sufficient extensions, you can write data Box c = forall a. c a =&gt; Box a onBox :: (forall a. c a =&gt; a -&gt; b) -&gt; Box c -&gt; b onBox f (Box v) = f v map (onBox show) ([Box 1, Box 'a', Box "b"] :: [Box Show]) but by that time you're just repeating `Box` instead of repeating `show` and may as well just write [show 1, show 'a', show "b"] which is shorter anyway and doesn't need any fancy types or extensions.
Any reason why you build on the server? Why Not just copy over the binary?
cons x xs = x : xs Functions can be just as lazy as the other functions or constructors that they use. Constructor fields can be annotated with ! to make them strict.
Yep, that was by far the hardest part. And in fact that's where I gave up.
Many years ago I challenged SPJ at a conference regarding the sad state of Haskell tools. He smiled his matinée idol smile and told me to use EMACS. That wasn't good enough then and it's really not good enough now. If the haskell community really wants to make it on any meaningful scale with industrial practitioners then you need to provide slick tools which pretty much work out-of-the-box on *every* mainstream platform. It doesn't have to be as good as IntelliJ is today, but if it were, say, as good as the Lisp tools were twenty years ago that would be adequate. A couple of years ago I paid for my team to do a haskell training course (mainly for the intellectual exercise) and the best that the trainer, a professional haskell programmer, could do was SublimeText with some rats' nest of plugins. Practitioners don't think of programs as complex text needing a clever text editor. We shouldn't be forced to think of haskell programs that way because the only tools that work reliably everywhere are clever text editors.
A better toolchain would have told me that this was the problem, and good documentation would have let me work it out. And a friendly, helpful community member would refrain from inviting the inference that I'm just an idiot.
Imagine something like: $(mkSpy World) $(mkStub World) $(mkMock World) `mkSpy` would be the simplest case, it would create a `mkSpyWorld` function that returns an `WorldSpy` which is an instance of `World`. Also, for each function `foo` in the `World` typeclass, there would be a `spyOnFoo` function created: data SpyInfo = SpyInfo NumberOfCalls [CallInfo] spyOnFoo :: WorldSpy -&gt; SpyInfo by using `mkSpy` in tests, it would be easy to check that some `IO` function was called, how many times it was called, and with which arguments. The next level of support would be `mkStub`. This would create a `WorldStub` which is also an instance of `World`. In addition to the spying, this world would be stubbable. That is, it would be possible to specify what the functions in World would return. This could be done like in the article, but a stubbing API could implement sweeping generalization such as "all functions throw an error". All `Either` return types will return `Left mzero`. Stubbing APIs also typicaly contain matching APIs for matching arguments. Haskell is pretty good at this, so I'm not sure what that API would improve upon normal matching rules. The next level of support would be `mkMock`. This would create a `WorldMock` which is also an instance of `World`. This has all the benefits of the spy and the stub, but in addition, it integrates with the test framework. A Mock is a stub which also contains expectations, thus `assert` and possibly lifecycle management. A mock that is called with the wrong arguments will fail the test. An API for programming mocks would at least abstract over some `assert` functionality (regardless of test framework). This is easy to do in languages with duck typing, but should be doable in Haskell as well. All of this is pretty well known terminology and widely used in other programming languages such as java, python, and javascript.
&gt; I don't understand what you meant by that I meant, let's say you find a lower bound for the first package. When starting searching for the 2nd package, does it unbound the first package or does it uses the lower-bound already found ? I think you answered it .
It seems that in the definition 4.2 on page 11 (item (a)) there is a typo: it should be `map snd` instead of `map (map snd)`; otherwise I can't make sense of the definition.
ZuriHac video will be out soon! But not earlier than you, Ed, send me a signed video release form :P I am returning from long vacation now and I'll take care of it ASAP.
the first two approaches have analogues in tree searching - depth-first traversal or breadth-first traversal. I can think of it as "searching the result of an expression inside it's AST" the third scheme looks like breadth-first search in expression's abstract syntax DAG
It's worth mentioning that, at Imvu, we do not use stubs or spies in our Haskell. Instead, we offer fully functional fakes for every "World" capability. For instance, instead of using a replay mock to cause a mock database to respond to a particular SQL query to produce a particular result set, we offer a pure database that can actually run the query. Sensing results is done with an ordinary SELECT. It works incredibly well.
That's good to hear, but that also means that you only have functional tests, not unit tests. What does this mean? Let's say we have `base IO` as the basic `IO` functions at layer 1. Then on top of that we have various abstractions, lets call those layer 2. Then on top of that there is some other abstraction, let's call that layer 3. When testing layer 3, if all you look at is the inputs (except the World instance) and outputs for the function, then it is a functional test. If you look at how the function interacts with `World`, then you have a unit test. The problem is that your fake `World` is at layer 1, and your layer 3 function interacts only indirectly with `World` though layer 2, so when you look at how your function interacts with `World`, you depend on the implementation of all of those layer 2 functions. This makes the tests brittle, not in the way that they fail, but they fail when layer 2 is refactored. There are "external" dependencies in the tests. Better then is to define `WorldLayer2` which allows fake versions of higher level abstractions than what `World` alone can do. Then check how your layer 3 functions interact with the higher level IO functions in `WorldLayer2`. If you go down this path with unit tests, you will find that you can't really define _one true_ World fake, you need fakes that are tailored to the domain the function you are testing operates in. 
Would it be possible to auto-generate line 11 to 38? Isn't that really boilerplate given that MyEffects is known? 
Not big, unwieldy I'd rather say.
In practice, this isn't a problem for us. I think part of the reason why is that our application (an HTTP server) has a very broad but shallow abstraction stack. I think the other reason is that it just isn't frequently the case that we make a change to some "layer 2" that doesn't also change its public interface, in which case the "layer 3" code has to change anyway.
a well behaved `Show` instance has a coresponding `Read` instance where `read . show` aught to be `id` of course this is frequently not followed. polymorphic "stringify" functionality is frequently abused across language communities
This seems like an appropriate place to ask this question: What front-end systems are people using in a for-money context? There are a lot of promising approaches here, but most of them feel very experimental/immature. Who's using GHCJS in production? What are the pain-points after the initial setup? Who's using purescript for UI programming in production? What are the pain points?
I'm not alone!
I gave myself a little library of `flip`, `comp`, `ident` (`id` is taken), `fst` and `snd`, and probably 100 others for the Python I have to do for work.
Not in haskell, but in the OCaml world, [ocsigen](http://ocsigen.org/) is exactly what you are describing.
Ur/web is **extremely** impressive but also very immature. BazQux uses Ur/Web in production, but the author had to use shared libraries to mix Haskell and Ur/web, which is pretty janky.
Did you look into Purescript-React bindings?
It's perhaps worth noting that with `Data.Typeable` you can perform genuinely heterogeneous operations on list elements by stuffing everything into an existential constrained by `Typeable` and using `cast` or `eqT` to inspect the type of each element.
Isn't that a *homogeneous* type-level list, i.e. the elements (types) all have the same kind? It looks like you've implemented something closely related to `vinyl`/`HList`, which I think of as a wildly different beast than the heterogeneous list this post discusses. Values of these lists have fully determined static types, like tuples; nothing's hiding behind an existential.
Try (also) to make sure your project is added to some of the "Haskell in the wild" lists that are floating around the www. It might be more worthwhile to make noise there :)
If the language offers the feature it is going to be used in production by someone so I am glad Haskell doesn't offer this kind of feature.
Even though I agree it's inconvenient to not be able to do so, it looks like "local" syntaxic sugar to save some typing, instead of a real language limitation. Most of those "hardcode" heterogenous list would be better be a tuple. What you'll need then is something like map3:: (forall s. s -&gt; t) -&gt; (a, b, c) -&gt; (t, t, t). map3 f (a, b, c) = (f a, f b , f c). Unfortunatly, I don't think you can write this in Haskell at the moment.
You want Data.Map.insertWith
&gt; An oft-stated argument against static typing [citation needed]
But then I need to use Template Haskell in the module in question, which is a huge hammer and (annoyingly) prevents cyclic top-level definitions.
Well, if, as /u/acow [suggested](http://www.reddit.com/r/haskell/comments/3am0qa/existentials_and_the_heterogenous_list_fallacy/csdzxt6) `OverloadedLists` gets extended to handle heterogeneous lists, this stops being a problem.
There are many ways to approach the relationship between server and client in such a framework. Some systems are designed to feel like building an SPA but with magical access to running code on the server (basically just eliding the plumbing of your ajax APIs), others are designed to feel like writing your application on the server with the ability to magically execute code in the client (the callCC base or inspired approaches), yet others handle the separation and communication more explicitly. It's probably also worth learning from the experiments and endeavours in clojure(script) such as hoplon, coils and luminous. Also jmacro-rpc is rather interesting, see eg this video https://www.youtube.com/watch?v=rtfbQJGQj0Q (edited, thanks /u/gbaz1) 
I got the post downvoted, so I decided to delete it.
If that video link doesn't work, you can see it here: https://www.youtube.com/watch?v=rtfbQJGQj0Q And here's the package on hackage: https://hackage.haskell.org/package/jmacro-rpc
Pretty sure your site is powered by electricity. Just sayin'
I've not made any statements about haskell as a whole here, only and specifically about the tools.
Well, I saw a lot of people struggling from that issue when coming at Ubuntu for instance. I thought I was helping posting there. Guess I was wrong. :)
Take a look at Reflex as well: https://www.youtube.com/watch?v=mYvkcskJbc4
Not a book but here's two classes that are great: http://www.seas.upenn.edu/~cis194/ http://www.seas.upenn.edu/~cis552/schedule.html
Here's another way to do it. Maybe there's an even shorter way, but it doesn't immediately come to mind. Map.fromListWith (++) . map (key &amp;&amp;&amp; pure)
Really upset that I missed this. Were any of the other talks recorded?
All but two, and I'll be posting them as I edit them. The two missing ones are Ryan Orendorff and Greg Weber. (The rechargeable batteries for my wireless mic failed.)
And [WebSharper](http://websharper.com/) in the F# world.
[SlamData](http://slamdata.com) use PureScript for their frontend. They can probably comment on pain points, but I suspect one of the bigger ones right now is integrating with third-party components. PureScript FFI is pretty good at low-level integration, but JS components don't tend to fit too well into functional models of user interfaces.
A while back I wrote a way to do this in a more generic way (but it only works on typeclasses that have function that always take one param). https://gist.github.com/pjrt/3b2231e323f5a557ebea We did have a use for it, but not the way ppl usually see it (a heterogeneous list). The concept I was thinking was "a typeclass list".
"Powered by Calories"
Meteor is interesting in that it gives you a client-server shared data abstraction between mongo-DDP-minimongo. That architecture could be replicated anywhere, of course. I think there's a lot to be gained from "isomorphic" strongly typed applications in that types can be shared between client and server. This one-ups the "stringly-typed" matching system in place today. Haskell could also really shine with the client-based latency-compensation system Meteor has. That works by sharing server code on the client and having the client run it in "emulation mode" in order to predict the RCP's behavior on the minimongo cache. The client can then enact these changes to pre-empt the server which confirms it after some round-trip latency. If this doesn't scream "free monad", well...
I'm not living in the bay area, so I really appreciate that you are taking the time to edit and post the videos. I'll be looking forward to them!
I'm a Haskeller for a hobby and work in Java professionally. So I use heterogenous containers where every element implements some API often, ex. HashMap&lt;IndexEntry, SerializableRecord&gt;, where SerializableRecord is a Java interface or superclass. What exactly is a higher rank type, and what does it do to solve this problem in a different way than existential types?
No, irrefutable patterns are just patterns that can't be refuted, like `x` or `_` or `(a,b)`. 
At this point do you have a complete flow chart for answering criticisms to the type class mechanism?
When I first started out in Haskell I really thought this was something that was missing. But the lack of sub-type polymorphism (is that the term?) isn't necessarily useful as far as I can tell. For example in Java I might have `List&lt;SomethingFactoryAbstractorInterface&gt;` and I might want to perform some map over that list. The reason it works is because all of the concrete types in the list are mutable - so if I modify them via an interface, the object's representation in memory has mutated. But in Haskell if I could perform `map f someInterface` over a heterogenous list that implements the same type class I can't get anything useful out of it. I won't be able to properly recover any of the concrete types. The example in the blog post with show, what would be wrong with this? data Foo = Foo Int Char String deriving Show show Foo
Another option is a local binding to help you out: myWeirdList = 1 *: Just 3 *: Blue *: "banana" *: [] where a *: b = show a : b infixr 5 *: I think *: is only slightly worse than typing a comma, and defining functions as operators seems more the intended use of Haskell. To me this is clear to read, easy to type, and requires no extensions or Jedi type tricks.
I can't tell if its complete. I maintain an open universe assumption. =)
There's an IRC chatroom for reflex, and people there seem pretty helpful. #reflex-frp
One liners are so much easier to test and reason about. The more you can write like this while satisfying your performance requirements, the easier your life gets.
don't forget coffee
Just because they have different semantics doesn't make (a,b) refutable. I refer you to [Hudak](https://www.haskell.org/tutorial/patterns.html): &gt; Patterns such as formal parameters that never fail to match are said to be irrefutable, in contrast to refutable patterns which may fail to match. The ~ operator creates a lazy pattern, which _must_ be irrefutable, but (a,b) is also irrefutable.
I like using an IDE with type-inferred languages so that I can easily check the types of my various terms. Type inference is to save you typing and make the code read more straightforwardly, but without an interactive environment it actively obfuscates your code. In the context of a rich, interactive development experience, I think we could pull off subtyping. Even if type inference isn't total, you just need to nudge it in the right direction.
&gt; I think we could pull off subtyping. Certainly. The question is, do you want to? Languages that have subtyping are a giant pain to work with formally.
I am sure this is true, and I'm grateful that you are advocating this particular style of testing. My main theme is to show that there are holes in the Haskell eco-system around testing IO, not that you are doing anything wrong.
How much PL theory have you studied?
I'm acquainted with data/codata, FOL, modal type systems, MLTT, basic model theory.
I don't see how you can write this well behaved `Show` for any circular structure without including loop detection. Any magic reference, like what is used in common lisp is invalid according to the `Show` documentation, so your _well behaved_ `Show` instance would actually not be well behaved.
&gt;Not to mention, dealing with variance and bounds when you mix parametric polymorphism with subtype polymorphism is quite fiddly to do soundly (and several languages have gotten this wrong, such as Java and Dart). Is there some reading on this? I've only read the Kotlin paper. &gt;The other thing that I find difficult to work with in a language with subtyping is the subsumption rule makes elimination quite a bit less straightforward. I feel like if you pick a restrictive enough model of subtyping/a rich enough encoding for your terms, this really isn't a problem.
Oh, and last time I looked at split I didn't see linesBy/wordsBy, so I didn't think I was reimplementing as much as I did!
Interesting. Here `f` is strict because `f _|_ = _|_`.
Thanks, but I'm not trying to build GHC. Also, doing the programming that I actually want to do in a fedora VM is much easier than any of this. More annoying, but easier. Thanks again, though.
&gt; But in Haskell if I could perform map f someInterface over a heterogenous list that implements the same type class I can't get anything useful out of it. You get a list of whatever result type `f` has!
I think GHCJS is currently a more future proof choice than Fay. And as mentioned here by others: there is an editor called Yi of which the implementation relies very little on things outside Haskell, maybe it could be made to compile to JS with GHCJS?
Haskell: a better PHP.
This is more of an experiment for myself than anything else, and I'd really like to be doing more of "my own thing", rather than screwing with an existing codebase or forking an existing project. 
[RamdaJS](www.ramdajs.com) would probably integrate well ;)
It's got electrolytes. It's what ~~plants~~ servers crave!
(Shameless auto-advertising below) My book [Beginning Haskell](http://www.apress.com/9781430262503) includes several chapters geared towards learning how to develop a DSL. It includes quite some information about parsing, about how to create a data type which reflects the invariants on your DSL or how to give an interpretation of an AST (in order to do something or generate some code from your DSL). Look at the last few chapters.
Comments on the following would be appreciated. ---- &gt; An oft-stated argument against static typing is that heterogenous lists are unreasonably difficult to model. That's not an argument against static typing; it's an argument *for* dynamic typing. The point is not that static typing makes heterogenous lists difficult to model; rather, *lack of dynamic typing* makes them difficult to model. Static &amp; dynamic typing are not two sides of the same coin. They are not alternatives from which we must make exactly one choice. They are very different things, which happen to be used to solve common problems. Since they are so different, whether each is included in a PL is somewhat independent from whether the other is included. Some PLs (like ANS Forth) offer neither. Plenty of PLs offer one of the two. And there is no reason why a PL could not offer both. Now look at this: &gt; `data Py = forall a. Show a =&gt; Py a` &gt; `instance Show Py where show (Py s) = show s` And this: &gt; `λ&gt; [Py 1,Py 'a',Py "hello",Py (\x -&gt; x ** 2)]` &gt; `[1,'a',"hello",&lt;function&gt;]` Apparently Haskell is a PL that offers both static &amp; dynamic typing -- at least with the `ExistentialQuantification` extension enabled. That said, to make the type theorists happy, let's put it this way: Haskell offers both static typing and *tags*. ---- EDIT. A couple of notes. 1. If the above thoughts are sound, then it would seem that the article fails to achieve its purpose: showing that we can construct heterogenous lists using static typing. On the contrary, the given construction uses tags (dynamic typing); the heterogenous list is constructed essentially the same way as in the Python example. 2. Other commenters have focused on the question of whether difficulty in constructing homogenous lists *matters*. I am not addressing this issue at all.
What I like about typing `[1, Just 3, Blue, "banana"]` over your example is that my editor is very good at manipulating text inside various kinds of parentheses, including brackets. My editor does not understand that everything held together by `*:` is one unit.
One might be tempted to think that a function that loops is considered strict is a technical oddity. But it's in fact important for strictness analysis to work well. For example afac n a = if n == 0 then a else afac (n-1) (n * a) It's easy to see that this function is strict in `n` since it uses `n` in the comparison. But is it strict in `a`? In fact it is, because even if we never hit the base case it is strict in `a` due to the definition of strictness.
While it's not exactly a citation for oft-ness, the quoted passage in the article seems to be drawn from http://researchblogs.cs.bham.ac.uk/thelablunch/2015/06/types-computation-vs-interaction/
`(a,b)` is refutable by `⊥`
Not by default. By default, `__repr__` generates a string like: `&lt;object at address&gt;`. Note that most Python objects are mutable and have identity so they can't easily have this `read.show` identity which is easy in Haskell.
I really do not want to be rude, so I do apologize for coming of as such. I am not trying to say that total languages are bad or inexpressive - I don't think that is the case. &gt; The truth is that we can express all computations in total languages, and we can be conservatively accurate (but never entirely precise) about their finitary status. I still don't understand why, tho. Can we write that [evil function](https://mail.haskell.org/pipermail/haskell-cafe/2003-May/004343.html) in a total language? 
Oh, wow, this was close by. Wish I'd know about it! Will there be another next year?
I haven't used it yet, but there's [servant-purescript](https://github.com/anchor/servant-purescript).
Powered by the inexorable dissipation of the energy gradient between the Sun and surrounding space, ultimately culminating in the heat death of the universe.^TM
… or you can use [proper heterogeneous lists](http://hackage.haskell.org/package/HList) instead of the anti-pattern code in the article. 
&gt; Hot swapping of code is also a goal. Once a program is substituted by a new version, replaying from the log would recover the node state. If the program's behaviour is different wouldn't it diverge through replaying?
Yes, if it has a different sequence, not merely different. But possibly there are ways to transform the log in this case.
As /u/Saulzar correctly pointed out, reflex-dom is high-level, whereas ghcjs-dom is low level. ghcjs-dom is essentially a direct binding to the W3C DOM. reflex-dom, on the other hand, translates all of that into an FRP-based GUI framework. With reflex-dom, you can always invoke direct ghcjs-dom or even JavaScriptFFI commands if you need to, but it shouldn't be necessary 99% of the time.
Why won't I regret it? 
I'm aware of this post and didn't agree with it at the time it was posted. I don't see them equivalently. An existential carries with it capabilities, the record construct carries with it the results. They are different. The existential is open in the operations you are able to do, the record is closed. Consider: {-# LANGUAGE ExistentialQuantification, StandaloneDeriving #-} data Py = forall a. (Show a,Num a,Ord a) =&gt; Py a deriving instance Show Py λ&gt; let xs = [Py 1,Py 2.0] Construction is done. Now I can, or more importantly, a user of my API, can write: λ&gt; map (\(Py p) -&gt; show (if p&gt;0 then p*2 else p+1)) xs -- I use a number of methods ["2","4.0"] λ&gt; map (\(Py p) -&gt; Py (p-1)) xs -- the type has not changed, it is still undetermined [Py 0,Py 1.0] I.e. I freely choose the operations I'm going to apply. In the record encoding, you cannot express this because both the operations and the types need to be fully determined at the time of writing, you can only express the final step. Additionally, combined with Rank-N types, you can have in your API a function like `(forall a. (Show a, Ord a, Num a) =&gt; a -&gt; a)` to let people do transformations without ever knowing the actual type.
`case x of _ -&gt; Q` does return `Q` because `_` is irrefutable, `case x of (a,b) -&gt; P; _ -&gt; Q` does not for the very reason that `(a,b)` is *not* irrefutable and thus checked (resulting in `⊥` when `x = ⊥`). `case x of ~(a,b) -&gt; Q` returns `Q` as well.
I think you missed my point. (a,b) cannot fail to match. It can either match successfully, or the scrutinee can fail to evaluate. Neither constitutes a "pattern match failure" and therefore (a,b) is irrefutable. The definition of _irrefutable_ is that the pattern cannot fail. That is, the next pattern will not be tried in a `case` statement. Just because it forces the scrutinee to whnf does not make it refutable. 
[The Haskell School of Expression](http://www.cs.yale.edu/homes/hudak/SOE/) is good. Though it is dated w.r.t. libraries etc, as the blurb says "An underlying theme is the design and implementation of domain specific languages" so it might meet your needs. It is also a decent tutorial for beginners. 
Author here. Feedback always welcome. This is more of a "things that in hindsight are obvious" but I thought I would share. 
 asum [ something , defaultValueFromLocalConfiguration , defaultFromGlobalConfiguration ] &amp; fromMaybe hardcodedDefaultValue 
We can have a monad Partial :: * -&gt; * where `Partial v` is the type of possibly diverging computations with values (if they arrive) of type `v`. You are promised that a `Partial v` can always be interrupted by ctrl-C if you get bored, but you are not promised a return value. Taking `Nat` as somehow encoding programs, we can't have evalMagic :: Nat -&gt; Nat -&gt; Nat but we do get eval :: Nat -&gt; Nat -&gt; Partial Nat evil :: Nat -&gt; Partial Nat evil code = (1 +) &lt;$&gt; eval code code Now, let us assume that the code for `evil` is, without loss of generality, 999, i.e. that for all `x` eval 999 x = evil x We obtain evil 999 = (1 +) &lt;$&gt; eval 999 999 = (1 +) &lt;$&gt; evil 999 but that equation *has a solution*, namely that `evil 999` is the computation which you will eventually need to interrupt unless your patience also is coinductive. Everything is total, and nobody has overpromised. So, the negative result means just this: total languages inevitably underpromise. But when they make termination promises, they keep them.
A friend of mine once had to write a three-page essay as punishment, entitled "This is laziness". On each page, he wrote a single word: THIS IS LAZINESS. Or, in Haskell-form: this_is_laziness :: a this_is_laziness = this_is_laziness He was wise beyond his years.
Right. We don't want to share other peoples world views. Haskell FTW, everybody who doesn't share the enthusiasm for strongly typed programing languages pls GTFO.
I have very brief experience with Fay and Threepenny, but based on those I'd say go with Threepenny. For me, Fay was a nice idea but hampered by needing its own packages and only being a subset of Haskell. With Threepenny, you're actually writing Haskell and don't have to modify your build process, etc.
How ? you still need to provide the key and wrap the value in a list (or any monoid).
Previous reply deleted because I didn't read your comment properly. -------- OK, the `SerializableRecord` (if polymorphic) would be an existential type in Haskell - that `HashMap` is presumably a heterogenous container so you can't avoid that. But in OOP, because only each `SerializableRecord` instance knows which exact subtype it has, it also needs to implement all the functionality - potentially not on it's own, but it must be involved to resolve which actual subtype is involved. This means the set of operations that can be applied to `SerializableRecord` is mostly closed - determined by the interface to `SerializableRecord` itself. This relates somewhat to the "expression problem" - the set of `SerializableRecord` subtypes is open so the set of `SerializableRecord` operations is closed. You can use a pattern to make the set of `SerializableRecord` operations open (such as visitor) but you tend to end up with a closed set of subtypes and/or a lot of clunkiness. Ultimately you run into the multiple dispatch issue. Higher rank types allow you to pass a polymorphic function into your code that works for any `SerializableRecord`, providing a way to open the set of operations a bit, or at least to defer the responsibility for resolving them to the operation. It's really just a variant of the visitor pattern - you have a polymorphic operation instead of having to wrap operations in larger objects to get the polymorphism. It's not a magic solution to the expression problem, but it's an option for solving some problems in that space. You could, of course, also fairly directly translate the visitor pattern using an existential type for your operation object, so the actual operation(s) are accessed via a typeclass interface. Maybe that could be the right thing in some cases, but in general it's probably a writing-Java-in-Haskell smell. ----- **WARNING** - The visitor stuff here at best needs caution, with maybe a very narrow case where it's justified but I'm not sure. The visitor pattern is generally a bad idea in Haskell because you don't need a design pattern to have an open set of operations but a closed set of not-exactly-subtypes - that's what algebraic data types give you anyway. Oops. 
I assume you *would* put it in parentheses in most cases. The list sugar doesn't really seem to buy you all that much: [1, 2, 3, 4] -- is the same as... (1: 2: 3: 4: []) -- except that you can judiciously replace the (:) mapM_ putStrLn (1*: Just 3*: Blue*: "banana": []) -- note you can also cheat and you use different combiners for different elements The main complaint I would expect would be having to type out the [] manually. But if you have a long list and you want to change the way you're combining elements, it seems like the extra explicitness might be worth it. I was thinking about this more, and the other downside seems to be operator binding. This requires knowledge of associativity and precedence (including of the common operators in base etc.), and I've heard complaints that operators are hard to read. But in this case where the binding is local I feel like the latter concern is mitigated by the fact that the definition is right alongside the usage. The former concern is more pressing, but thinking about it more, infix functions are a part of the design of Haskell, so it might be worthwhile to understand, define, and use them when appropriate. If they showed up more often, maybe fewer people would have to lookup how to choose and specify precedence and associativity (for instance, I had to lookup the precedence of (:)). Edit: One example of a precedence gotcha is that: [1, 2, subtract 1 . (*2) $ 3] is not the same as: (1: 2: subtract 1 . (*2) $ 3: []) As the comma in the list sugar has magically subzero precedence, but (:) has precedence 5, while ($) has precedence 0. So first 3:[] is evaluated to [3], then (subtract 1 . (*2)) is applied to it, which is a type error. So you'd want to write: (1: 2: (subtract 1 . (*2) $ 3): []) but for instance: (1: 2: 3 - 2: []) is fine because (-) has precedence 6. So order of operations really can be a pain. It helps to know that you can inspect the precedence and associativity of an operator using :info in ghci, and that parentheses are always fine.
That version is (better|more explicitely) structured.
Author of threepenny here. Threepenny-GUI is still a bit on the experimental side, but people have already been using it to [write on-off GUI applications](https://wiki.haskell.org/Threepenny-gui#Gallery). The nice thing is that you can start right away without worrying much about installation. I certainly don't intend to let it die anytime soon. :-)
Have you tried [the Hoogle over at FP Complete](http://fpcomplete.com/hoogle)? It seems to do the right thing for all three examples you mentioned.
If you really need this, I would use Dyn and Typeable rather than existentials. Otherwise you're throwing out so much information....
What about modeling it like the following ? import Data.Map (Map) import qualified Data.Map as Map type Duration = Double type Cost = Double data ShiftReport = SRTotal Map (Employee, Day) (Duration, Cost) | SRIndividual Shift instance Monoid ShiftReport where mempty = SRTotal $ Map.fromList [] mappend (SRTotal c) (SRIndividual s@Shift) = -- pseudocode lookup (employee s, day s) c -&gt; update/insert (duration/cost tuple) I think it would be the most similar to the aggregation query you mentioned, unless I did something wrong :) **edit** and for this question &gt; knowing that once I've done that I'll need also to shrink it again and discard day to get the number of hours/cost per week You have multiple answers with my suggested model. The simplest option is to add a phantom type to `ShiftReport` and have two monoid implementations in that case. data DailyReport data WeeklyReport -- declaration changes to data ShiftReport a k v = SRTotal Map k v | SRIndividual Shift instance Monoid (ShiftReport DailyReport (Employee, Day) (Duration, Cost)) where ... instance Monoid (ShiftReport WeeklyReport Duration Cost) where ... **edit** you might not even need the phantom type, just parameterize the result "container" data ShiftReport m = SRTotal m | SRIndividual Shift instance Monoid (ShiftReport (Map (Employee, Day) (Duration, Cost))) where ... instance Monoid (ShiftReport (Duration, Cost)) where ...
Yep... I currently use that as an alternative. 
Looking at markup.rocks, I think I might end up choosing reflex + reflex-dom. In your opinion, are there any areas where reflex(-dom) falls short or any reasons not to use them? 
PureScript is awesome, but don't take my word for it, just check out our online demo (http://slamdata.com). We use purescript-halogen for the front-end and are working on improving the story for native component integration.
Ah, this makes me sad. I wondered why it wasn't working for me anymore. What changed?
IMO the version bounds in your cabal file should indicate a range of versions that you know [1] to work. When there's a dependency without bounds and I'm trying to add bounds, here's what I do. Presumably the code already builds. If not, then the first step is to get it building. When I'm trying to add a version bound for dependency foo, the first thing I do is find what version of foo the code is currently built with. If I'm using a sandbox, I do `cabal sandbox hc-pkg list | grep foo`. If I'm not using a sandbox, then I do `ghc-pkg list | grep foo`. One of these two commands will return something like this: foo-1.2.3.4 The [PVP](https://wiki.haskell.org/Package_versioning_policy) implies that if your package builds with foo-1.2.3.4, it should build with any foo-1.2.x.y where x &gt;= 3. There's certainly always a potential for mistakes that invalidate this rule, but you should assume it will apply until proven otherwise. This suggests the following version bound: foo &gt;= 1.2.3 &amp;&amp; &lt; 1.3 The PVP says that when a package adds new functions to its API it has to bump at least the c version component in a.b.c.d. This means that if your package does not use any of the things in foo's API that were new in 1.2.3, 1.2.2, or 1.2.1, then you can just use the lower bound `&gt;= 1.2`. In the ideal world, if you want to use that bound, you should test your package against foo-1.2.0.x first, but in most situations it's probably safe to use that bound. Some people in this situation like to use this syntax: foo == 1.2.* But as soon as foo-1.3 comes out, that `.*` notation will no longer work and you will probably want to bump the bound to this: foo &gt;= 1.2 &amp;&amp; &lt; 1.4 So I prefer to avoid the `.*` notation for consistency and use the dual bound syntax everywhere. [1] Yes, I know that ultimately the only way you can *know* is to actually build against that version. Here I mean "know with reasonably high likelihood", where we assume that most of the time package authors conform to the PVP and mistakes are the exception rather than the rule. This assumption works because cabal-install prefers more recent versions. So if there was a mistake that causes 1.2.3.5 to fail even though 1.2.3.4 succeeded, it's very likely that the author will release a fix as 1.2.3.6 that will supersede 1.2.3.5 in the vast majority of practical circumstances.
I think it's this [one](http://electron.atom.io/).
One of the authors here. The latest changes to Hayoo! haven't been changes to the core functionality. Instead, the problem with the wicked results is the indexer. Currently we crawl (!) the haddock html from http://hackage.haskell.org/. And since that changes quite frequently lately our indexer mixes things up while traversing the DOM. Only recently I discovered that /u/ndmitchell had implemented a Haddock output format which is more stable. But I haven't had time to implement a proper parser yet. But if someone wants to step up and do some Haskell programming in their spare time that would be a nice project with great community reward. One could almost completely reuse the existing indexer structure, but instead of crawling html one would crawl those haddock files. One could start with https://github.com/hunt-framework/hayoo/blob/signature/hayoo-hoogle-indexer/src/Hayoo/Index/Hoogle/Parser1.hs which is a first try on a parser for said format. As a partial solution: I will have a look on the indexers output and try to correct it. So hopefully we will have a recovered nice behaviour tonight. 
I'm confused. What is this demonstrating?
It seems like uses of existential quantification in Haskell are often called an antipattern and this assumption is usually not discussed further. I don't think it is the case that it is always an antipattern and I don't *think* that is what the antipattern article intended to say (I could be wrong on this particular point, but note that the title is "Haskell Antipattern: Existential *Typeclass*"). I think the antipattern happens when you attempt to force a OO-like API using a type class. There is a key difference between what is happening in the antipattern article and what is happening here: You need `AnyWidget` to pipe `Widget` types around but here, we add no difficulty to passing `Show` types around. In the antipattern, you also lose potentially useful information. Here, you keep exactly the information that you need (the fact that it is showable), no more, no less. It seems to me that the problem has more to do with the type class than the existential quantification itself. Existential quantification is used there to work around the issues caused by using a type class instead of just using a product type. On the topic of `HList`s, the size of an `HList` is fixed at compile time which adds additional restrictions. In this particular case, you also have to provide extra information beyond what you actually need: All you need to know here is that everything is showable. With an `HList`, you must give it all the specific types you will be using. I'm not saying `HList`s aren't useful, but I don't believe they are the right tool to use here.
&gt; what a cool community Isn't it, though. Between here, the IRC channel, and various mailing lists, a well-crafted question will reach the authors/maintainers of just about all major libraries (and the compilers and/or the language itself), and they're essentially all helpful and friendly. If you're new here, welcome!
the article makes such claims as &gt; It is hard to extend: new input and output facilities can only be added by changing the Input and Output types, and then changing the wrapper program. the code above provides a way to add new primitive event streams by adding them to the `registrars` record. code which uses these inputs must only be updated if it asks for the new field, in which case it just uses one of the provided lenses and propagates the constraint for the field's membership in the record of inputs
I believe Free Monads as such were first introduced in: M. Barr, Coequalizers and free triples. Math. Z. 116 (1970) 307–322. The same material is covered in "Toposes, Triples, and Theories" in section 9.4 (http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.394.2555&amp;rep=rep1&amp;type=pdf) I also think that understanding who invented the idea, when, and what a free monad "actually" is in the sense you're asking for won't help you. The construction of free monads in Haskell specifically is much simpler, more straightforward, requires none of this background, and can be picked up from any number of great articles and talks a quick google away.
An appropriate title and great blog post about Free Monads by /u/Tekmo: ["You Could Have Invented Free Monads"](http://www.haskellforall.com/2012/06/you-could-have-invented-free-monads.html) (I think the blogpost was later re-titled to: Why Free Monads Matter)
can concur, these uchicago students are a lucky bunch. 
No, you are not, browsing documentation at work became a pain. Sorry for jumping into the rant train, but I guess it's normal when people get frustrated by tools they love ;)
I'm on my phone, but the idea is: groupBy key as = foldr (\mp x -&gt; Map.insertWith (++) (key x) [x] mp) Map.empy as
Could you give a rough estimate on how much work this is? Depending on that I might be willing to give it a shot.
&gt; You don't necessarily need a shared base type, you only need that the elements share the parts of the interface that you actually use. I'm not sure whether you're talking about some other language or about Haskell here, but in Haskell there are a bunch of idiomatic ways to capture exactly that without existentials. See [Luke Palmer's *Haskell Antipattern: Existential Typeclass*](https://lukepalmer.wordpress.com/2010/01/24/haskell-antipattern-existential-typeclass/) article, for instance, although there are probably better and more recent explications.
A free monad in Haskell actually is data Free f a = Pure a | Free (f (Free f a)) ... plus the associated Functor, Applicative, and Monad instances. I'm not trying to be a smartass, that's all that really is for me. You can imagine it as a tree, a stream, a program, or whatever, but the definition is the only limiting factor.
This reminded me of this strip: http://www.gocomics.com/calvinandhobbes/1995/01/09/
 cata :: Functor f =&gt; (f x -&gt; x) -&gt; Mu f -&gt; x cata f = c where c = f . fmap c . out is a 'slightly generalized' fold in that it takes an arbitrary f-algebra and lets you tear down the least fixed point with it, whereas normally we just deal with `foldr :: (a -&gt; b -&gt; b) -&gt; b -&gt; [a] -&gt; b` but if we take the base functor of `[]` to be `data L a b = Nil | Cons a b` then we can see that `foldr` is just taking that `(L a)-algebra` apart into two functions. Going backwards: foldr :: (a -&gt; b -&gt; b) -&gt; b -&gt; [a] -&gt; b ~ (a -&gt; b -&gt; b, b) -&gt; [a] -&gt; b ~ ((a, b) -&gt; b, b) -&gt; [a] -&gt; b ~ ((a, b) -&gt; b, () -&gt; b) -&gt; [a] -&gt; b ~ (Either (a, b) () -&gt; b) -&gt; [a] -&gt; b ~ (L a b -&gt; b) -&gt; [a] -&gt; b On the other hand, in the 'recursion schemes from comonads' (rsfc's in the talk) world, it was discovered that you can build a fold that is a bit more stylized. gcata :: (Functor f, Comonad w) =&gt; (forall b. f (w b) -&gt; w (f b)) -- ^ a distributive law -&gt; (f (w a) -&gt; a) -- ^ a f-w-algebra -&gt; Mu f -- ^ fixed point -&gt; a gcata k g = g . extract . c where c = k . fmap (duplicate . fmap g . c) . out Here we use the distributive law to keep pushing the source of our comonad out of sight so that we never need to 'start' it. By carefully picking the comonad and the distributive law you can get interesting recursion schemes, including most of the "zoo of morphisms" described at the beginning of the talk. (You need to deal with a similar distributive trick to work with monads and anamorphisms to get all of them.) At some point I showed you can combine these distributive laws into distributive laws for known comonad transformers. These recursion schemes let you build bigger and bigger schemes. From this primordial stew the mythic "zygohistomorphic prepromorphism" was born -- just by smashing together distributive laws and asking "what does this mean!?" The talk spoke about how all of these generalized recursion schemes from comonads are instances of adjoint folds.
I like the semantics described here, although the need for `unsafePerformIO` is worrying. I'm always concerned that the compiler might move things around in an unexpected way. Events seem very much like IVars, so perhaps there is a way to implement them more directly. (Although these are apparently [prone to space leaks](http://blog.ezyang.com/2011/07/ivar-leaks/) as well.) In any case, it seems like you should be able to avoid updating an event's IORef once the final value is known. Perhaps something like this: memoEIO :: IO (M (Either (E a) a)) memoEIO einit = do r &lt;- newIORef (Left einit) return (usePrevE r) usePrevE :: IORef (Either (E a) a) -&gt; M (Either (E a) a) usePrevE r = E $ do res &lt;- liftIO (readIORef r) case res of Right _ -&gt; return res Left e -&gt; do res' &lt;- runE e liftIO (writeIORef r res') return res' 
Idiomatic Haskell doesn't use superclasses the way Java does. Normally, if we want to capture a finite universe of types within `SerializableRecord`, we'd do it the other way around, with a sum type: data SerializableRecord = UserRecord User | EntryRecord Entry | ... Which allows us to distinguish the "subtypes" `User`, `Entry`, etc. using normal pattern matching. Existentials (which are different than higher-rank types) provide another approach to this. We can make an existential type using two different syntaxes: data Ex = forall a. Ex a data Ex where Ex :: a -&gt; Ex In both cases, a value of type `Ex` can contain anything: `Ex "string"` and `Ex 3` and whatever else are fine. But the flip side is that a value of type `Ex` is basically useless to a consumer: you can't write a function of type `Ex -&gt; a` because that would explode the type system. Instead, you can add constraints to the existential; unwrapping the existential allows you to use the constraint. So with `data Showy= forall a. Show a =&gt; Showy a`, you can write `f (Showy a) = show a`, which has type `Showy -&gt; String`. Similarly with fancier things. Adding `Typeable` or `Dynamic` to the mix allows you to do more things. See the [GHC documentation on existential quantification](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/data-type-extensions.html#existential-quantification) for more general info. The usual question with existentials is: why am I storing an existential rather than the data exposed by the constraint? For instance, `[Showy]` adds nothing to `[String]`; the only thing you can do with a `Showy` is convert it to a `String`, so why not just store a list of strings? Remember, because of laziness you can populate a list with a bunch of potentially-expensive operations that yield strings without paying the cost until and unless the result is demanded. Higher-rank types allow inversion of control in polymorphism. This is potentially relevant to the present discussion, but I've run out of typing time.
I agree with /u/Roboguy2 that `HList` is a completely different beast. They're analogous to tuples, not lists, in that the length and element types of a any concrete value must be statically known.
&gt; I think there's a lot to be gained from "isomorphic" strongly typed applications in that types can be shared between client and server. This one-ups the "stringly-typed" matching system in place today. &gt; That's very interesting, when I heard claims about Meteor's latency-compensation, I had assumed that it was basically doing something rather trivial (if cumbersome to do by hand). That is much more (repeating myself) interesting. I would be very interested to see an FRP take on the client views bound to database live-queries idea.
The results from Hoogle 5 are now at http://hoogle.haskell.org and might give you better results than the old Hoogle. 
I created this but I haven't updated it in a while because I lack time. But would love to bring it up with the latest servant version this summer: https://github.com/arianvp/ghcjs-servant-client
The purescript-react wasn't around when I was doing this, so I can't comment. It does look a bit too opinionated and immature, though.
Yeah, if I need a web client for my current project, I will DEFINITELY use servant to generate the request functions. I'm very happy with servant so far.
I don't know much about reflex-dom, so I can't give a qualified answer on that. What I can say is that Threepenny tries to fill a particular niche: It's very easy to install, so you can start write a GUI application right away with little effort. It does some fancy things, like a little bit of FRP or some syntactic experiments, but the core focus is to be easy to use. It is "cheap", as the name implies, but it works, and it doesn't force you in a particular model — you can always use `IORef`s instead of FRP and it offers a JavaScript FFI that is almost as simple as fay's. In other words, it tries to satisft the "Give me a button on the screen" desire by whatever means possible — nothing more, but also nothing less.
You might like to look at the `interact` function: p :: [Int] -&gt; [[Int]] p = filterM (const[True, False]) main = interact (show . p . read) `interact` takes a function from Strings to Strings and turns it into a program which reads stdin, passes it to your function, and returns the output to stdout: $ echo "[1,2,3]" | runhaskell Main.hs [[1,2,3],[1,2],[1,3],[1],[2,3],[2],[3],[]] It can be very useful for wrapping pure functions for use on the console like this. To get the behavior you initially asked for, consider using the `lines` function to break stdin on line boundaries.
It's relatively easy to implement new plot types for the chart library. You just need to create a record with [this type](https://hackage.haskell.org/package/Chart-1.4/docs/Graphics-Rendering-Chart-Plot-Types.html) These don't need to be in the chart library itself - it's fine for applications to define their own plot types. 
It doesn't support them.
Directly, not really. Indirectly, yes. When you program in a language, it's generally best to follow the established idioms of that language. Functional programming in C tends to not work out so well. Mostly because the high-level programming you'd do in C would be better in a different language. On the other hand, newer languages often take inspiration from existing languages. If something works well in a language, then new languages might borrow that. As an example, Rust draws inspiration from Haskell and ML to a significant degree.
I only just found this - the default way of keeping the set of operations open for extension but closing the set of ~~types~~/~~classes~~/~~kinds~~/???s of values. I can't believe I was advocating a visitor-like pattern based on higher-rank types - the dangers of looking for a problem for a given solution rather than visa-versa, I guess. 
I generally do something like main = fmap (map read . lines) getContents &gt;&gt;= print . solve
I think it has made me write safer code in other languages by encouraging me to write pure functions and have fewer side effects. It has also made me think harder about types in my code: even in weakly typed languages, I tend to write code that is strongly typed, because it's easier to reason about.
I think so, yes. Java's reflection provides more information because `Class` objects contain more data, but it's basically the same things.
Java's `Class` can be queried at runtime, but in Haskell, type information is erased at runtime. This means that you can write more functions in Java with the signature `&lt;T&gt; T (T arg, Class&lt;T&gt; cls)` than there are functions of type `forall t. t -&gt; t` in Haskell (since the Java function can inspect the type argument). On the other hand, it's hard to prove anything about the Java version, whereas you know almost exactly what the Haskell function must do, given its type.
The wxAUI stuff looks nice. If someone wants to port Leksah to wxHaskell they are welcome to (as long as it does not loose any significant functionality). As a first step, it would be nice to have native looking file open and save dialogs and that should be easy to add without changing the rest of the code base over. Next step might be to make sure we have a way to manipulate the DOM of a wxWebView from Haskell. Basically we need the wx version of this https://github.com/gtk2hs/webkit/tree/master/Graphics/UI/Gtk/WebKit/DOM. It might seem daunting, but all this code is generated from the IDL files. So as long as someone can work out what is needed, we can update the generator and output all the Haskell wrappers from the IDL files using the same generator.
How about Data.Typeable (and Data.Dynamic)? The Typeable constraint is needed, but if that is present, the type can be observed in runtime, can't it?
Stack seems really nice. As it uses a specific stackage snapshot, does this mean I don't specify versions in my cabal files? Another question: Why have a separate config file for stack (stack.yaml). Can't we have custom fields in our cabal file?
&gt; As it uses a specific stackage snapshot, does this mean I don't specify versions in my cabal files? That's your call. If you're building a local project, the version constraints in a .cabal file will only have the ability to tell stack that it's got the wrong versions. There are still advantages to having version constraints in a .cabal: * It prevents `stack init` from choosing incorrect build plans * It can help you with testing on other snapshots (e.g. `stack test --resolver nightly-2015-06-23`) * If you want to support people using cabal-install to build your code, which is especially true if you're going to upload to Hackage &gt; Why have a separate config file for stack (stack.yaml). Can't we have custom fields in our cabal file? There are lots of reasons in favor of a separate file, but probably the simplest is that stack supports multi-package projects, meaning that a single stack.yaml can support 0, 1, or many .cabal files.
1) Basically, your indexer has to download all cabal files (http://hackage.haskell.org/packages/index.tar.gz) and all hoogle-haddock files (http://hackage.haskell.org/packages/hoogle.tar.gz) from hackage. 2) You would fit the cabal files into PackageInfo format (https://github.com/hunt-framework/hayoo/blob/master/hayooIndexer/src/Hayoo/PackageInfo.hs) and the hoogle files into FunctionInfo format (https://github.com/hunt-framework/hayoo/blob/master/hayooIndexer/src/Hayoo/FunctionInfo.hs). 3) You would then calculate the package rank (https://github.com/hunt-framework/hayoo/blob/master/hayooIndexer/src/Hayoo/PackageRank.hs) from the PackageInfo, specifically package names and their dependencies. 4) Additionally you would create 'ApiDocument's from your PackageInfos and FunctionInfos (https://github.com/hunt-framework/hunt/blob/master/hunt-searchengine/src/Hunt/ClientInterface.hs) wrap them up in insert commands and print that out in json. Which can be used as input to the searchengine. Parsing the cabal files can be done with the Cabal library, for the haddock format you would have to write a robust parser which gets you all the information for the 'FunctionInfo's you need. And of course we would guide you and provide answers to your questions. 
That's not an intersection type. What about intersection elimination?
How does this approach compare with Erlang, the gold standard in functional mobility?
Same test as above: * [first try: `vector`](http://hoogle.haskell.org/?hoogle=vector&amp;scope=set%3Astackage) * [second try: `Vector`](http://hoogle.haskell.org/?hoogle=Vector&amp;scope=set%3Astackage) Package I am searching for is on second place on the first try. Module I am searching for is on first place on second try. That's much better. Is Hoogle 5 a different version than the one run by *FP complete*? Or is the better result just a consequence of a narrower search scope?
The main point of the Haskell IO system is that it encourages you *not* to put things that do input and output into a "pure" function. A pure function is one that transforms data, like yours that describes how a set is transformed into its powerset. An IO function is one that describes an "action" that a computer performs, where it interacts with the outside world. The Haskell approach is that it is best not to mix those two aspects of your programs. The IO part of your program starts with the magical IO function called `main`. The action described by that function is run when your program is run. The pure parts of your program are functions whose types do not include IO. IO functions, like `main`, can call other IO functions and also pure functions. Pure functions only call other pure functions - to keep things pure. The type system helps you maintain that invariant by causing the compiler to report an error if you violate it by mistake. So rather than "making input possible" in your pure function, the natural way of accepting that input style would be to write a separate function - an IO function - to read the input list in the style that you want. Then, in `main`, you call that function, feed the result to your pure function to do the actual calculation (the pure part), and then print the final result. See the post of /u/Tekmo for a few good examples of how to write that input function. EDIT: In Haskell, "code golf" usually focuses on the pure part of the program. You then run the function at the GHCi prompt, or write a separate wrapper function that does the IO and doesn't count towards the golf score. But if you need to count also the IO as part of the golf score, there are techniques like using `interact`, `&gt;&gt;=`, etc. to make your IO code more terse, and you can inline your pure code inside the IO code.
&gt; MacPorts is the only thing that works for building GHC on OS X right now. Really? Why? Is it because of `libgmp` and/or `readline`?
I don't know erlang neither cloud haskell very well. This development is at the beginning and the erlang model is mature. The erlang model is intentionally low level, it tries to make the network and the communication costs visible. It focuses on reliability Mine tries to make the network transparent and emphasizes composability and high level programming. An example of composability is in the mapReduce snippet. Almost all cloud computing platforms work in inverted (reactive) mode. This is the only model that I know that operates in full de-inversion and yet is non blocking. This is the reason why it is composable. In contrast, cloud-haskell and Erlang uses blocking calls. Concerning inversion of control, this approach is more close to this: http://lampwww.epfl.ch/~odersky/papers/jmlc06.pdf Concerning mobility of software, It is more closer to Objectspace Voyager: http://www.inf.fu-berlin.de/lehre/WS99/VS/Misc/Voyager/API/doc/orb.pdf I will dedicate this software to Graham Glass, the genius behind Objectspace Voyager that developed a cloud platform 15 years ago that still is years ahead of anything else. But this is at an early state. Let´s see what happens. Anyway, I never dreamed to do this with so little code and so fast. That may mean that I´m in the right direction.
Great write-up, thanks!
Ok. Thanks! 
Thank you for the pointers, FRPNow and some further reading based on it (Yampa etc) was very enlightening. I updated the original posting with some further ideas.
I may go ahead and set up the open/save dialogs. Do you prefer I use plain wxHaskell or wxHaskell + reactive-banana-wx?
Even before switching to `stack` about two weeks ago, I've basically had very little worry about version constraints ever since I switched to using stackage lts (while I was still using `cabal-install`). I don't specify any constraints in my cabal file. I just let stackage lts pick them all. However, I will say that stackage lts is beginning to show the fact that it is a long term support package set, and this is where I find `stack` to help the most. If you're staying on GHC 7.8.4 for a little while longer, you can't use transformers-0.4 with stackage because none of the builds support it (even the nightlies). Similarly, the diagrams package is stuck on 1.2 (because 1.3 makes some breaking API changes). Fortunately, in the case of diagrams, I can just do this in my stack.yaml file: extra-deps: - 'diagrams-core-1.3.0.1' - 'diagrams-contrib-1.3.0.3' - 'diagrams-lib-1.3.0.1' - 'diagrams-svg-1.3.1.2' - 'diagrams-solve-0.1' - 'active-0.2.0.3' - 'force-layout-0.4.0.1' And now I everything works nicely (for diagrams at least. I don't know how to use the newer transformers). I find it to be a better way to use newer packages than the equivalent solution with a cabal-install sandbox.
I'm not sure functional programming, per se, really helps you that much. You're going to have an easier time writing Java in a safe way than VBA or lisp. Really the gains are more related to strong typing. Haskell just happens to be better at shifting errors to compile time because it also has many nice features like non-nullable values and warning if pattern matches are non-exhaustive. Pure functional programming, on the other hand, provides a way to avoid bugs to do with ending up in a bad state. I view it as an alternative to object oriented programming, which accomplishes roughly the same goal. However pure functional programming is possible more because of the type system than because of the functional paradigm. You can conceive of imperative languages that have this "purely functional" characteristic, and people have. To enforce a difference between pure and impure though, you need some thing to distinguish them and prevent impure code from being marked pure. In Haskell, the type system does this.
Neat! For the info of others I did a quick google for details and found the "pure functions" section here: http://dlang.org/function.html Seems like such a handy thing to have (and actually not too horrible to enforce) I'm surprised Rust didn't bother with it...
If you look at the section on "Coherence" in the linked Dunfield paper on unrestricted intersection types, you'll see that what he presents, while a step forward, is far from something suitable to implementation in any language at all, since the basic problems of inferring a usable, meaningful, and predictable behavior for those types still remain :-( The refinement typing story, as you discuss, is more tractable, and there's work on making LiquidHaskell much more usable and integrable into actual development. (And we have a GSoC project even that's about taking some small steps in that direction!)
So Electron does not help you packaging the Haskell part. As far as i understand, Electron will produce an installable app, while the server will be compiled with Ghc. What do you use in order to merge the two binaries?
I think you'll be happy to hear that [LTS 3.0 should be released soon](https://groups.google.com/d/msg/stackage/Ux7ideofwIA/U4gdzG9wS_oJ), which will add GHC 7.10, transformer 0.4, and a lot more (check out [the latest nightly](https://www.stackage.org/nightly) to see all ~1100 packages).
Even Fortran [has that.](https://en.wikipedia.org/wiki/Fortran_95_language_features#Pure_Procedures)
The LiquidHaskell research cites that line of work, but I don't know anyone specifically following the Xu approach.
Very good news. 7.10 for mainstream adoption! :)
thank you for the clarification. i am on my second day of digesting this paper, and i think what is the meat of it to me (as an api user) is this `Now` monad which is a lot like reactive banana's `Moment t` monad except that it does not require a `Frameworks t` constraint to perform io. i do admit it looks like a lot less ceremony to do dynamic event switching with io here, and that is an ergonomic win i am looking forward to, but i'm not sure the claim is true that you can not write these same events without this novel construction
I totally agree, and I'm very happy that LTS is not bleeding edge. I guess I wasn't clear about this in my original comment, but I'm trying to stay with GHC 7.8.4 until 7.10.2 comes out. Consequently, the nightly releases don't really help me at this point.
&gt; I guess maybe my "viable for commercial development" clause might still save me there. :P Apparently an advertising company primarily using D called Sociomantic had over [$100 million in revenue for 2013](https://www.sociomantic.com/dunnhumby-acquires-sociomantic/), so I think it's shown some commercial viability. Although Sociomantic were only using D1, I'll give the language the benefit of the doubt in assuming it hasn't regressed significantly over time.
Yes, I searched a bit and it does not seem to be followed. It's too bad as I found it quite natural and easy to learn and apply. Sure, many contracts are impossible to prove statically and they have to be deferred to run-time. However, they could be deferred to run-time only during testing and removed from the production builds, if performance is an issue. ~~I found LiquidHaskell to be much harder for me to wrap my head around.~~ EDIT: I take the last statement back. This paper does a great job of explaining how to use [LiquidHaskell in practice](https://git.softlab.ntua.gr/nvazou/softlab-thesis/raw/2b9957298c34e72b9f40644d70b559472b48f2c9/submitted.pdf).
Can anyone tell me how well `stack` plays with `emacs haskell-mode` error checking? By which I mean, will the error checker use my `stack` project's references automatically? I know I can check myself I just thought I'd ask in case anyone knew already.
Yes, so if you have a `Typeable` constraint, you don't get the guarantees that erasure would have given you.
It takes a lot more effort to richly type destructive updates in a way that can catch errors at compile-time as easily as you can with functional data flow. With arguments and results, having types verify you handle all the cases, and that everything checks out is a (at least partially) solved problem. Imperative programming adds another form of data flow -- through the current state of mutable variables and use of destructive updates. It is possible to give static types that help correctness, but it is far more difficult. The kind of static types in Java, when combined with destructive updates, is going to be inherently less safe than function application for data-flow.
I am excite!
Funny, I was looking for something very similar the other day, but with the signature `[(a, b)] -&gt; Map a [b]` -- basically a fromList that allowed duplicate keys. It surprised me that nothing existed. Your groupBy would do almost what I want, except that it wouldn't give the values in the shape I needed.
This probably depends on how you do error checking. Do you use the haskell-mode REPL or do you rely on ghc-mod?
I am not trying to be an ass, but I think the idea is 'get more people to stop using impure languages' or 'design more pure languages'. The glass half full half empty argument is the same argument over and over, given a max and a min you can always compare one relative to the other. It's not really the argument that counts, it's what people do after they listen to it. If I'm forced to write code alone, then purity is no problem. If I have to write code with other people without the future having already been written into the past, then we are all guessing what the other one is going to do, and in the midst of development, lots of things can still be up in the air over what is considered a side effect and what isn't, depending on the complexity of the project. 
Yeah, I just have this paranoid fear that imperative programmers will skim this article and say "welp, that saved me from having to bother with functional programming!".
&gt; welp, that saved me from having to bother with functional programming I'm not sure people with that attitude would be motivated to spend time learning it anyway.
I can't really comment with any kind of stance, after I started doing functional programming everything clicked together so much more elegantly. It just makes sense, but that's to me. I can't really look at it from the perspective of an imperative programmer, aside from wanting to get things built step by step, without always knowing confidently whether each step will work. I use that methodology for building some things still, so it's good for navigating some kinds of problem solving territory, but I can't really say whether one is better over the other, and who it's better for, if that question even has an answer definitively. I don't think anyone should have to learn anything, if it's hard to learn and hard to reason about for the individual, then maybe there is a good reason for that. Maybe it's just so counter-intuitive to how they know how to reason, that learning it would actually make them less effective at their craft. I can't really say I have opinions on it. I mean the biggest reason I like functional programming is because it's cool. It's cool how things fit into other things, and it's cool to abstract things and make new things and invent layers of things. But imperative programmers do that too to some degree (since I can still read their code), I just don't really know how it feels to build things that I consider cool or well designed in an imperative language. This is more the side of programming I think about, all the stuff that makes me make the decisions I do, without me always being aware of why I make those decisions. It was linked recently on hackernews or proggit about programming and superstition, and I don't like doing things out of superstition, so I don't really think it's fair to cover purely functional languages with a blanket of pristine aesthetic beauty and determine the question answered once and for all. Semi-apologies for the tangential deviation. 
I knew I forgot to mention something thanks gelisam!
I found this article largely unconvincing when it was published. It's one thing that one can use impurity to introduce more impurity, but people generally don't try to undermine the language they're working in, or they only try to do this when they're newbies coming from another language/paradigm. The same thing applies to Haskell, with `unsafePerformIO` etc. We could instead talk about sound principles of language design, and it's indeed plausible that shoehorning purity annotations into an otherwise wildly impure language might be just too unwieldy and not very practical. But that's only tangentially related to whether incrementally more functional code yields incremental maintainability and correctness, which I think is the case (see e. g. Carmack's [article](http://gamasutra.com/view/news/169296/Indepth_Functional_programming_in_C.php)). If I wanted to be a bit snarky here, I could construct an analogous argument with respect to Haskell's type system: Haskell's "mostly functional" style clearly doesn't work; since every type is inhabited by bottom, static guarantees don't actually guarantee anything, and the type system is useless as a tool for reasoning about programs. 
I'm curious about the various `flycheck` plugins including those which use `ghc`, `ghc-mod` and `hdevtools`. 
Windows users have a high tolerance for pain--in my anecdotal experience.
I think a lot of people don't understand Erik Meijer's target audience. He's not addressing people who desire to program using as much purity as possible. He's targeting people who have become complacent, thinking that their limited brand of purity is good enough for all practical purposes. These people often advise others against even more pure alternatives like Haskell because they believe that the extra purity is "not worth it".
I'm personally all for total programming too, BTW, it's just the exaggerated purist disposition that I disagree with. 
I dunno; they sound sufficiently *lazy* that they could be convinced!
So far stack has been playing really well with haskell-mode and flycheck, first I switched away from ghc-mod to /u/chrisdoners [haskell-process] (https://github.com/chrisdone/haskell-flycheck/blob/master/haskell-flycheck.el) flychecker. That way errors are pulled from the stack repl. My haskell-mode is also configured at follows, to also use ghci-ng ``` ((haskell-mode . ( (haskell-process-type . ghci) (haskell-process-path-ghci . "stack") (haskell-process-args-ghci . ("ghci" "--with-ghc=ghci-ng"))))) ```
Ours problems are indeed equivalent and I was expected also to find a version of your group by. I had your problem initially but I found that is often better to not modify the of the input `(a, b)` for performance reason. 
Caveat: I am liable to take this personally since I have put a lot of effort into bringing pure FP to imperative/OO programmers ([Effect](https://github.com/python-effect/effect), [multiple](https://speakerdeck.com/radix/purely-functional-programming-in-python-pure-fun) [talks](https://thestrangeloop.com/2015/side-effects-are-a-public-api.html) , [Sumtypes](https://github.com/radix/sumtypes)). I totally agree that we need to be bringing more people to *enforced* pure FP, but I very strongly think that bringing the pure FP to them is a step in the right direction. *edit: by "this", I meant Erik's rant against FP-in-imperative-languages, not your comment* I only take issue with Erik's rhetoric, not with his intent. And I think they're inconsistent. The introduction lambasts the middle ground, but then the rest of the article is all about how you should have strictly enforced pure FP. That's not an argument against the middle ground, it's completely on the side of enforced pure FP. I agree with bringing strictly enforced pure FP to everywhere. I don't agree with "The slightest implicit imperative effect erases all the benefits of purity, just as a single bacterium can infect a sterile wound". To kind of respond to what you actually said ;-) I find Erik's early rhetoric to be damaging to the advocacy of FP to the masses of programmers. In fact, Haskell has long been a source of great ideas introduced to other languages, and as that has continued, so has Haskell's own popularity.
I guess the only thing I have to say in response to this is that I disagree about there being no concrete and direct benefits to functional programming. Reasoning is, well, *possible* with pure FP.
Hence qualifiers like 'hodgepodge' and 'drowning'.
Julie and Chris's new Haskell book is actually starting off with lambda calculus in the first chapter. I think that's pretty cool. :) http://haskellbook.com/images/sample.pdf
groupBy burned me once already this week. I was expecting it to group all elements belonging to a key into a single list, but it only does this for contiguous elements. Running the list through sortBy first helped, but it wasn't obvious from the name or type signature.
No. Either is a sum type. Sum types and union types are different things. See the linked papers.
https://www.haskell.org/ It's the big logo in the middle, resized
Whoah, whoah, what? I'm super confused. I thought Either was a product type. Are you using a different definition of "sum type" than the common one in Haskell? 
I generally find Erik is inconsistent with his opinions on FP, it's hard to know where he stands from one month to the next.
My objection to the haskell type system is that it isn't nearly strong enough. Since I don't make my living by solving data structures and algorithms homework problems the constraints that it can express aren't super useful to me and the constraints that would be useful to me it can't express. And my objection to the rhetoric surrounding haskell's type system is that if the people spouting it really believed what they were saying they would immediately abandon haskell for a language with a stronger type system. Doubly annoying is that they don't recognise that the very reasonable reasons why they don't do all their work in, say, agda, are pretty much the same reasons that I don't do all my work in haskell.
It's just distressing to see people arguing over what the deal (puritywise) should be, globally, when the point is that the deal can be documented accurately and renegotiated locally.
I do a lot of work in Agda, but the reason I don't write executable software in Agda or Coq is because they are just massively impractical for that purpose, as they are. It would be great if we had a language with a powerful type system and strong support for software development. We don't have it. 
&gt; Reasoning is, well, possible with pure FP. In the very narrow sense of “reasoning” that people who make this claim seem to mean (what is that you think C++ programmers do? examine chicken entrails? roll dice? and yet the world is equipped with many working C++ programs) that's certainly a *difference*. Showing that the difference, and what you have to go through to get it, is a net *benefit* I have not seen done.
"Mostly functional" programming does indeed work very well, at least when it is a combination of: * Top level function ("main") that does nothing except call the other functions (or run the main loop) * Many small pure functions (95% of the code) * One or two small functions that do nothing except write (file, printer, network) * One or two small functions that do nothing except read * A few small functions that change properties of an object that is an input to the function. This is not an attempt at duplicating Haskell style purity. This is an attempt to build a many-featured system from understandable, testable, self-contained components. I've been "mostly functional" like this for ten years and it not only works, but is readable, maintainable, and performant.
Great news. Looking forward to trying it.
Would it be possible to provide a Debian repo ? (I am using a derived distro that has just moved from Ubuntu to Debian jessie). I would assume Debian packages would work for Ubuntu ?
s/reasoning/local reasoning/ and the statement makes more sense. In a pure context, you only have to consider the things "around" a chunk of code to know the entirety of how that code operates. In a context with unrestricted mutation, "spooky action at a distance" is possible and one needs to consider the state space of the _ensamble_ of code throughout the program.
/u/radix probably means [equational reasoning](http://www.haskellforall.com/2013/12/equational-reasoning.html)
Often you are your own "next engineer", though. A month after or so. If you need to remember what you did you are going to have trouble scaling your own code up. If you understand your old code by just reading it, that's at least a good sign.
&gt; I find anonymous voting a very cowardly invention. You do realize that is just inviting more downvotes, don't you?
Works great! :-) I like how it has a bit more "smartness". By giving a command "build" or "test" you state your intent and the tool "makes it happen". If that means downloading GHC, initializing sandbox, figuring out which stackage version, and building all deps (also for test target), it just does all that's necessary. Previously, I was using "stk sandbox" to have shared sandboxes but this is all automatic now. Also, a great feature is being able to configure many projects using a single `stack.yaml` file.
&gt; If you want to down vote my post, at least leave a comment. I was hoping that this comment would begin with "I'm not one of your downvoters, but &lt;insert constructive criticism here&gt;". Unfortunately, after reading your comment more carefully, I was compelled to downvote as well. I downvoted your comment because of your tone. I can see that you have an intelligent argument in there, some concrete examples of how to tame effects by being mostly functional but not purely functional. However, the way in which you expose your argument, by promoting your own accomplishments and belittling the accomplishments of others, discourages me from continuing the discussion. &gt; I find anonymous voting a very cowardly invention. Downvoting without leaving a comment is a good way to minimize the impact of trolls, as leaving a reply would go against the common wisdom, "do not feed the trolls". To avoid being mistaken for a troll in the future, please try to watch your tone more carefully.
And the award for least searchable name for a serious project goes to...
&gt; sounds cool but what's it mean? The title was of this post was a bit strange for a reason but no one has pointed out why. Perhaps no one noticed. I have updated the readme. I'll be making a how to screen cast once VirtualBox 5 is out.
Go.
Ahh yes. I default to using grep because the direct arg only works for an exact package name, while greg gives me partial matching 
Maybe "stack build tool"? I'm feeling lucky worked for me there.
&gt; Purely functional programming is a small subset of the code that can be created in a language like C. Can you provide an example of something that can be done in C that cannot be done with Haskell?
R.
I do. I was just snarking!
I think the stack here refers to your 'technology stack' as exemplified in abbreviations like the LAMP stack (linux/apache/mysql/php). What layers of technology do you have stacked on top of each other to create your solution? ...I just realised it's probably actually a reference to stackage, which was a pun on hackage, which was a pun on package, for which we definitely do not have a manager. Ah, software.
Yes, I know very well what /u/radix means by “reasoning”—now show under what circumstances this is a *net benefit* to the programmer, given what has to be done, and what given up, to gain that facility. Haskell advocates talk as if having this facility is obviously an unalloyed, all-conquoring good in all circumstances, without further discussion.
You can optimize C more easily. If you ignore performance then all nontrivial languages are equal, but performance matters in many practical cases. Personally I'm willing to trade a little performance to facilitate debugging (and therefore development time), which Haskell does for me.
Ah, OK. Yeah, `libiconv` can also be an issue.
sounds like you're really good at C, but do you know Haskell? the standard typeclasses, how to purify a module, dataflow with polymorphic data structures, etc? if you know the basics, and aren't impressed (I was), you should check out haskellforall.com . there's dozens of tutorials on advanced haskell features, well-motivated and clearly-explained. to take just one, stuff like Free Monads can make "separation of concerns" in complex programs easier, and thus increases readability and test ability. also, you can skim through this huge list of haskell features (http://dev.stephendiehl.com/hask/), with examples, and see if anything catches your eye. since you know so many other diverse programming languages, it shouldn't be take too long to establish an informed opinion.
You have to understand that Erik wants to shock people, so he changes what he says to be the most upsetting for his audience.
This looks very similar to [the denotational semantics of Sodium.](https://github.com/SodiumFRP/sodium/blob/master/denotational/Denotational%20Semantics%20of%20Sodium.pdf) Is that based on this?
Sounds interesting. It looks like this kind of distro will never consider systemd, so that's a plus.
This post has a lot of information. Except for the info on why you are on /r/haskell.
For small programs where you sort of find out what you want it to do as you write it, you have to constantly change your test cases as you change your code functionality. The more likely it is that the compiler can catch bugs, the less important test cases are. Maybe it's a question of definitions? I count everything from testing for a bug and fixing it as "debugging". The significant majority of my time is spent debugging for nontrivial programs. I haven't taken statistics on how much. There are many times when I program where there is an error which would be clearly labelled by a strict compiler.
&gt; How do I get extra build tools? &gt; stack will automatically install build tools required by your &gt; packages or their dependencies, in particular alex and happy. Nice. ~~I'm still a bit dubious about (seemingly) being forced to use stackage to use this. Last time I tried (about a month ago), I had trouble building packages that worked just fine with cabal sandboxes. I probably could have fixed them by using a stack.yaml with extra-deps, but that seems like a step in the wrong direction, in particular because it wouldn't (appear to) allow version ranges.~~ *update*: As I was typing this, I read the [FAQ](https://github.com/commercialhaskell/stack/wiki/FAQ). I'll update this comment if that turns out to give me what I want. *another update*: I was able to set the following in `~/.stack/stack.yaml`: package-indices: - name: Hackage download-prefix: http://hackage.haskell.org/packages/archive http: http://hackage.haskell.org/packages/archive/00-index.tar.gz By using hackage, I lose out on: - git-based indexes, which should update faster by only updating incrementally. - signed packages However, I (hope) I know don't have to bother with `extra-deps` in a local stack.yaml, and can simply use PVP to sort out dependency constraints. The reason I'm so adverse to `extra-deps` is that it feels much like a requirements.txt file in python with hard-coded version numbers, which means (presumably) that I lose out on the ability to stay in sync with non-breaking version changes upstream. *yet another update*: I still get pegged with an lts resolver when using `stack new`, despite having `resolver: ghc-7.10` in `~/.stack/stack.yaml`. ~~I really hope this is a configuration issue or bug, rather than vendor lock-in.~~ *~~last~~ update (I think)*: `resolver` is specified at the project level. Feels like I should be able to specify a global default though. Visiting the stack issues page... Using hackage is apparently a bad idea. Presumably, I could use both stackage and hackage, but looking at the default `hack.yaml`, it seems stackage overloads the name "Hackage". I suspect that can be over come, but given that stuff "just works" for me right now with cabal-install and was supposed to "just work" what hack, I'm not inclined to try. So really, there doesn't seem to be a clear migration path from cabal-isntall to stack unless you're willing to stay within their ecosystem (LTS/Nightly) + stackage, which doesn't seem to be (at the moment) as current as regular hackage. Back to cabal-install I go...
&gt; For small programs where you sort of find out what you want it to do as you write it, you have to constantly change your test cases as you change your code functionality. This is not my experience. I have worked on multi-year efforts accumulating many person-decades of programming effort with intensive automated testing as a primary quality and design tool. What I've seen is that there is a leading edge of tests that change frequently, and then less frequently and then become part of an ever-growing and largely static body of regression tests. &gt;I count everything from testing for a bug and fixing it as "debugging". Me too, although I prefer to talk about diagnosing a failure and fixing the underlying defect. And in the teams that I've seen that use intensive automated testing as a primary design and quality tool there just isn't very much of that. And nothing remotely like 75%.
If you get some unpleasant results, please report them. stack is designed to handle just about all Haskell dev use cases, so if it's giving you trouble, it's something we should try to address.
You still have to count the total effort spent writing and maintaining those tests, though. Otherwise you're just talking about completely different things and the comparison becomes totally invalid. IME, Haskell code tends to require a *lot* fewer tests than equivalent Java/Python/etc. code. Of course a big factor here is the absurdly high level of abstraction in Haskell -- which means that a lot of what is pure boilerplate in other languages is extrinsic to the "interesting" bits of the program in Haskell -- and so you can test the "interesting" bits without having to worry about all that incidental complexity. YMMV.
3/10 relevant hits when searching for that.
For those who, like me, had never heard of Jade before: [Jade](http://jade-lang.com/) is a template language (as in HTML templates, not Template Haskell) for node.js. (hmm, there are [many things](http://www.reddit.com/r/haskell/comments/3aun81/ghc_7101_alpine_packages/csg3r70) I've never heard of these days. I guess I only stay current on Haskell-related news...)
keithb. We get it. You are irritated by Haskell and you are irritated by Haskellers. However, this is a Haskell forum for people interested in Haskell so they are allowed to be somewhat informal when it comes to talking about Haskell here. I would like to cordially suggest you rethink your confrontational, negative, moany attitude. You are treading very close to trolling territory. 
&gt; of course this is frequently not followed I believe that's because there isn't a separate pretty-printing class (say Pretty) *in the base library*, which there should be, and `ghci` should default to that instead of Show when it exists (and should fall back to Show when it does not). (I see below that python does something similar)
&gt; Should it be something like `fromJust (Nothing::Maybe a)` then? That would be silly. `fromJust` is for those cases in which you know that the answer is guaranteed to be a `Just`, and you want to tell the compiler to trust you on this. &gt; Is `undefined` any different? The difference between calling `fromJust` on `Nothing`, using `undefined`, or calling `error` is that the error message will be different: &gt; fromJust Nothing *** Exception: Maybe.fromJust: Nothing &gt; undefined *** Exception: Prelude.undefined &gt; error "server replied with an unknown command" *** Exception: server replied with an unknown command In general, `error` is the better choice because it allows you to give a helpful error message. `undefined` is for those cases where you don't have a better error message than "never happens", and `fromJust` is for those cases where the error message would have been saying that you were fairly certain that you would receive a `Just` but you received a `Nothing` instead. &gt; [...] I don't have a meaningful value to return. The canonical return value in this case is `()`, not `undefined`. Remember, the Haskell equivalent of the C signature `void foo(int x);` is `Int -&gt; ()`, not `Int -&gt; Void`.
Give C some credit here. It clearly was the first major language using a one letter name.
After a quick look, I don't see any jade converter in hackage. You are probably better off adding a generation step in your build and just use node/go tools. Unless you want to write your own parser/generator.
But first you have to search for the correct search term.
It took longer than expected but check it out now. It should be much better now. 
This one too :) http://www.superbwallpapers.com/games/half-life-42772/
That was my archlinux/xmonad wallpaper for so very long.
In 1969? 3 users.
http://images.uncyclomedia.co/uncyclopedia/en/f/fb/Halflife-Haskell.jpg
I *love* this. It's made my experience with Haskell so much nicer. Unfortunately, there aren't any vim plugins that work with stack, so I need to have a cabal sandbox directory anyway. I'm hoping that'll get resolved soon. Otherwise I may have to learn emacs...
or in favor of pipes, depending on whom you ask :)
This isn't much of an argument. Haskell has a sizable and active industrial/commercial community; a community of "serious-minded practitioners." It has tooling, a killer compiler, and a pretty broad library collection. Sure, the "serious-minded practitioners" you have in mind may not have much use for it, but you haven't said who they are, why they don't, and what you, I, or anyone else should do about it. I really appreciated both sides of your conversation with /u/tomejaguar [elsewhere in this thread](https://www.reddit.com/r/haskell/comments/3aufaj/erik_meijer_the_curse_of_the_excluded_middle/csh01g4), by the way. As he says, you've clearly got constructive ideas; I'd like to hear them.
Ok, that's fair, the results are indeed meaningful.
I can also confirm that this is true.
No worries, so I talked to ahills a bit and part of the reason I just stuck my nose to the grindstone is I wanted apk packages enough. So the apks I posted work, though they're all built without aslr aka with -nopie. I might have a fix for the pie bits, ghc is compiling now. I'll test it later today and if its fine I'll go push up a new release version without the settings file hack. But if that works too then the last hitch that was brought up to getting ghc in should be gone. I also need to file a bug about PIC support in ghc if so. :)
Hipster Logo Generator: http://i.imgur.com/dUmL8KT.png
&gt; for which we definitely do not have a manager Full disclosure: I lol'd.
But vimscript is scary! ;) Is there already some discussion or effort on this that I can join in on? IIRC the reason the current vim plugins don't work is a dependency on hdevtools and ghc-mod, both of which are dependent on cabal. There's not much documentation on using ide-backend, so I'm not really sure where to start.
Do you mean users or developers? In my experience Windows users have a very low tolerance - if your software doesn't have a one click installer that sets you up completely, then you're dead before you even start.
I'll relate you what /u/theonlycosmonaut wanted to say, in reverse: &gt; for which we definitely do not have a manager Haskell does not have a package manager, cabal ain't one. &gt; hackage, which was a pun on package &gt; stackage, which was a pun on hackage
In practice with `stack`: I set up a XMonad configuration project and get up and running using `stack new` in just about less then 10 minutes. This was something I had just given up on ever maintaining using cabal (pre-sandbox admittedly). I think I had given up on ever maintaining it before sandboxes just came out. The processes of going through setting up the sandboxes was just that little too much against my resolve to actually change something to make me actually do it. Plus the thought of wasting all that disk space just because I wanted to maintain a couple files. With stack, I know I'm going to be able to reproduce my build in the future. There was no big compile setup because I choose a version of nightly which I have been running for the past week. Everything just works. We're very excited here at FP Complete to see the positive uptake with `stack`. I hope this makes it clear why we went ahead and built this instead of continuing to try to improve cabal.
Customary to ask the deprecator, I think :)
I'm surprised you didn't mention Haskell's "-XDeriveGeneric" extension. Which not unlike Java reflection, lets us convert a datatype to and from a generic representation and write generic code for any datatype. More details here: https://downloads.haskell.org/~ghc/7.8.4/docs/html/users_guide/generic-programming.html And a nice talk by Andres here: https://skillsmatter.com/skillscasts/3932-a-haskell-lecture-with-leading-expert-andres-loh Sure, Reflection in java allows more, such as dynamic binding and many other unsafe operations (even mutating internal symbol tables). But this additional power also makes things more complicated and less efficient (many runtime checks by class loaders, security managers etc) 
There doesn't seem to be any way of deducing the reason or even the date of deprecation from the Hackage material. Where is it discussed?
I really like the idea of using Haskell to write emacs extensions. As shown in this blog post, it is already possible to call Haskell from elisp. What I'm missing is the other direction: calling emacs functions from Haskell. Does someone know if this is already possible somehow?
Thanks for the info!
It uses [`Cabal`](https://github.com/haskell/cabal/blob/master/Cabal) the library but reimplements all the commands/UI in the executable [`cabal-install`](https://github.com/haskell/cabal/tree/master/cabal-install). It makes no sense to fork `cabal-install` as it would basically need to all be changed. The bulk of the logic in cabal the tool is in the `Cabal` library which stack *is* using.
do you manually move the xmonad executable to .xmonad folder? And how do you handle auto recompiles? ( Mod+Q) ?
This doesn't make it clear... You're pointing out what you like about `stack`, but this still doesn't spell it out why you built a new tool from scratch rather than improving `cabal`.
I think it's evidence that cabal shouldn't be cabal. 
I think I'm going to have to.
I suppose there is in the future, but we'll have to think about how to best set that up...so probably not tonight. We welcome suggestions / brainstorming about how we might best leverage this. Edit: I guess one approach could be to post questions to this reddit thread and we could try to have someone ready to ask the ones with the most upvotes. We'll see what we can do.
I'm very excited to try Stack out! I have all but stalled development on a couple of hobby projects because when I got a new computer cabal-install stopped building them locally and I couldn't be arsed to figure out which packages it gets the wrong versions of and so on. High hopes for Stack!
What is the use case of that?
&gt; I no longer fear haskell builds. This is exactly the reaction that `stack` is going for.
https://en.wikipedia.org/wiki/B_%28programming_language%29
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**B (programming language)**](https://en.wikipedia.org/wiki/B%20%28programming%20language%29): [](#sfw) --- &gt; &gt;__B__ is a programming language developed at [Bell Labs](https://en.wikipedia.org/wiki/Bell_Labs) circa 1969. It is the work of [Ken Thompson](https://en.wikipedia.org/wiki/Ken_Thompson_(computer_programmer\)) with [Dennis Ritchie](https://en.wikipedia.org/wiki/Dennis_Ritchie). &gt;B was derived from [BCPL](https://en.wikipedia.org/wiki/BCPL), and its name may be a contraction of BCPL. Thompson's coworker [Dennis Ritchie](https://en.wikipedia.org/wiki/Dennis_Ritchie) speculated that the name might be based on Bon, an earlier, but unrelated, programming language that Thompson designed for use on [Multics](https://en.wikipedia.org/wiki/Multics). &gt;B was designed for recursive, non-numeric, machine independent applications, such as system and language software. &gt; --- ^Relevant: [^Little ^b ^\(programming ^language)](https://en.wikipedia.org/wiki/Little_b_\(programming_language\)) ^| [^Programming ^Language ^for ^Business](https://en.wikipedia.org/wiki/Programming_Language_for_Business) ^| [^Ken ^Thompson](https://en.wikipedia.org/wiki/Ken_Thompson) ^| [^Martin ^Richards ^\(computer ^scientist)](https://en.wikipedia.org/wiki/Martin_Richards_\(computer_scientist\)) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cshjtpw) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cshjtpw)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](/r/autowikibot/wiki/index) ^| [^Mods](/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Call ^Me](/r/autowikibot/comments/1ux484/ask_wikibot/)
I'll be watching. Continue this in the future!
Thanks. Reading this now.
Thanks for the explanation!
looking forward to it
Link to the stream: http://original.livestream.com/pivotallabs
the questions is, my homework is about make assembly interpreter using haskell, where i'll store the commands in a list. i need tips, no the answer! for improve my study 
¡caballero configure! ¡caballero instale!
*Feels* meta though.
Tuples are basically fixed length heterogenous lists, right? I think with the right extensions (ones typically used for emulating dependent typing in Haskell) that can be handled, thought I'm not sure how nice they would be to work with. 
Some constructive criticism. Could you start the stream a few minutes before the actual start of the talk? I came here at 7pm and kept refreshing every few minutes, and still managed to miss the beginning. When you stopped the stream during the break, the livestream interface started to play the next video in the queue, which meant I again had to refresh every few minutes to see if the new stream was up or not. Leaving the video on but the sound off would have provided a better user experience.
Yep! In this context the one on the bottom is called, well, "bottom," and represents a number of things including nontermination. (The "/s" is code for "that was sarcasm. But that's okay.)
You can likely replace calls to "cabal" with "stack".
But you can do `stack install stack` too, so long as you put `stack` in `$HOME/.local/bin`. And that's recommended.
There are two main use cases for stack. * You want to write your own stack.yaml for a project * A stack.yaml is already written for the project you want to work on In the latter case, all they have to do is follow the download instructions and run `stack test` on the project. Stack's messages should guide the user from there if anything needs to be set up. In the former case, I don't know if any good "tutorials" exist yet. You can refer to [the readme](https://github.com/commercialhaskell/stack#readme), and read up on the [stack.yaml wiki page](https://github.com/commercialhaskell/stack/wiki/stack.yaml). Stack's messages again should help guide you in writing a stack.yaml for an already-cabalized project. If you'd like to use stack to start a brand new project, then try `stack new`, although I don't think this command is very polished yet. Generally, I'd link newbies to the readme for now. ---- There's also a third use case that stack wasn't quite designed for but can still handle quite well: * You want to use stack to experiment with arbitrary Haskell code In that case, create a new folder, and write this as your stack.yaml file: resolver: lts-2.15 Then you can `stack install $dep` for any given LTS Haskell dependency you use in your scripts, and `stack runghc $hsfile` for any given .hs file you've got. If you want anything more complicated than that, then you might as well create a project for it.
Also, in a totally unrelated and very personal note, my university is on a strike again. The government stopped paying our janitors, which caused the professors to start a strike, which caused the students to start a separated strike... and now my FB feed is flooded with memes about dropping out of calculus, etc. (Curiously, the janitors are still working.) I'd be surprised if that didn't happen every single year. Nobody takes things seriously here, it is all about party and soccer. Don't get me wrong, I love my people and playing soccer, but all that is frustrating when you want to take things seriously. I've been looking a lot lately into other directions/places to go. If anyone here has an insight/advice, please let me know...! 
Why? To have a user-local stack install that's the most up to date?
Yes. So you literally only need to run `stack install stack` to upgrade.
My first thought was, "That's a weird way to typeset 'LaTeX'."
Was there some expectation that `stack` should have been `cabal`?
I looked at hamlet, but it is a templator. Jade is more of a shorthand for html. Similar to Sass is for CSS. I saw questions about making a parser in Haskell for a Jade interpreter, but I can not find if that person was successful or not.
Can it be used to build GHCJS yet?
A more-straightforward way to write this that I can imagine (not something really allowed) would be without an extra constructor: type Py = forall a. Show a =&gt; a and have the list as: [forall a. Show a =&gt; a] Why isn't this possible in Haskell? Would it break something in the language if it is brought into the language?
I think my sentence parses, but if it was unclear: I just meant that coding in Haskell could be really fun all the time given the detailed feedback and massive refactorings that are possible, given the information statically provided its type system.
"Improving" cabal-install basically means throwing everything out and starting over. This is what the people at FPCo did, and at that point it makes sense to also create a new name. Mind you that "throwing everything out" from cabal-install is not as drastic as it sounds – "the good parts of Cabal" lie in the Cabal *library*, not the cabal-install program. Stack still uses the Cabal library.
`cabal install cabal-install` might very well be one of my favourite commands to type. There's something inherently fun in it that can't be matched with `stack install stack`. You will be missed, `cabal-install`.
why dont vim nor emacs count as proper editors?
After reading your posts a few times, I think you want the following: you have an .asm-file and you want a Haskell-program that runs it, like a virtual CPU. I have a few questions, though: * I don't understand what you mean by "binary numbers". From where do these binary numbers come? Do you have to parse a string of 0 and 1 (e.g. "10110011" -&gt; 179), or do you get a bitstream from somewhere? * Do you want to emulate the CPU's registers in Haskell, or do you want to write to the *actual* registers? * Do you emulate the stack and the RAM too? Also, you only need two's complement for subtraction. You can add two numbers by straightforward bitwise addition (+carry). 
I like it very much; it's well blended into the existing theme and it feels "modern", which I think it's the ultimate idea we would like to convey, to shake the label "academia" off our shoulders ;)
You might find the treatment of n-tuples in the `generics-sop` library of interest: http://hackage.haskell.org/package/generics-sop (see also the paper, linked from the Hackage page).
I didn't change anything. I must have missed inserting the ranking into the index the last time... Sorry about that.
The top symbol should've been ¬, then it would've been ∃⊥∀¬ :-)
I am trying stack at the moment, and it seems to install executables inside `~/.local/bin` by default. Two questions: 1) Why is this a sensible default? 2) How can I override it?
I think that some of these claims are not complete fair to Haskell: - You can work with Haskell in a great amount of environments. I think that EclipseFP or HaskForce (for IntelliJ) work really well. Right now I am using Atom and a handful of Haskell plug-ins, and I get all kinds of help from it (completion, type inspection...). - Of course there are several libraries doing the same thing! This happens in most languages: how many libraries are there for treating dates in Java? How many async libraries are there for Python/Ruby/Java? - While the literature on Haskell has been historically small, it is growing in the last years. I myself wrote Beginning Haskell, focused on explaining things such as project management with Cabal, designing using GADTs and other things. The book by Simon Marlow on parallel and concurrent programming, and Real World Haskell dive into the problems of laziness, and they are both free. Furthermore, there are several freely available on-line courses to get started with Haskell! - Even though Hackage lacks things such as looking at the newest packages, or some kind of community-based voting to highlight the best packages, I think it really works in terms of discoverability! My usual workflow is to go there, find some package with keywords of what I want to achieve, and usually I get one or two hits. Of course, you have packages such as `adjunctions` or `lifted-base` which are complicated to describe, but usually you pull those are requisites of other packages. I think the main difference with Ruby or NodeJS is that Haskell has grown as a programming language on itself, whereas the others became popular in a specific scenario with a specific set of libraries/frameworks (think of Ruby on Rails).
The "haskell is persecuting me!" complex is tired.
5 types of string, actually, 4 of which are useful. (`String`, lazy and strict `Text`, lazy and strict `Bytestring`). Other than `String`, this is pretty much the right decision, because dealing specifically with text and dealing with bytes of generic binary are two different use cases. Also, only one of these (`String`, the design mistake) is in `Prelude`.
&gt; I'd prefer that you remove the "Other downloads" part, in any case. We shouldn't present two conflicting alternatives. I disagree 110%. I want to be able to find all possibilities on the download page, independently of what the community decides is the best. What is best for some people maybe not the best for other people. Personally I disagree a lot of decisions the "community" did in the last few years. Quite possibly I'm not completely alone with that. Some other comments: I rather dislike all these "modern" webpages, especially for pages presenting *information*, kind of goes against the original information-dense concept of web. Also usability (questions for both the old and new versions): Does this page work without JavaScript? Does it works from a terminal browser like `links`? Does it work for a blind person? (compare say with this page: http://cr.yp.to/ - maybe it's ugly, but works for everyone and easy to navigate. Needless to say, I also liked the old wiki front better) 
`Bytestring` hardly qualifies as a string type in the sense people usually mean when they talk about strings in any language other than C.
&gt; It was recently slipped through inside what was advertised as a (much needed and very well done) restyling of the haskell.org site. That's inaccurate. I redesigned the whole site from scratch, including content and the look and feel. In fact, it was running on haskell-lang.org for some months before I was approached to have my site be the new haskell.org. The rest of your comment I agree with. I've always used plain old GHC since 2007. I'm out of the picture on installing Haskell, so I go off what other people's experiences say and there is enough negativity and technical complaints about HP that makes me question it. I want a single Right Choice as much as everyone else.
My two cents: I hope good discussion comes out of this, but I don't think that the "reddit subsection of the Haskell community" is necessarily a good representative pool of "Haskell users". Furthermore, I don't think that we'll eventually make a straight "up or down" choice of a winner here because this is a conflicted situation. What we have now is a messy compromise. What I hope we can get to is a less messy compromise. And we should bear in mind that this proposed redesign is _also_ tied to plans to reboot the Haskell Platform process into a more regular release cycle.
Is it too early to present stack as a default option? It seems to be the easiest way to get a full Haskell stack running on a system at the moment.
MinGHC should definitely be included alongside the platform. It's a choice. I switched to MinGHC simply because the platform was outdated. If the platform keeps up with MinGHC in terms of having a release for every single GHC release, then it will still remain relevant given all the improvements in cabal-install re: true isolation of sandboxes. I expect to see some convergence between the two. MinGHC has MSYS packaged in, which is important. Platform Users need to install it manually. Such distinctions should be highlighted on the downloads page. (like choose Platform if you want.... choose MinGHC if you want...). Finally, there's also the 'stack' tool which can install GHC for people. It is quite new etc. but at some point it could be offered as a choice too. 
I use immutable global data and pure functions in my C project. C is definitely not a functional language. Why do people insist on using either of these as evidence for the adoption of functional programming? C also allows me to easily use mutable data and non pure functions whereas functional programming makes this harder but still possible (monads). The argument for functional programming should be why programming with a subset of tools is superior to a full set of tools. Pure functions can and have been normal functions in all programming languages since the late 1950's. Functional advocates imply that Math like notation and concepts are inherently better than more traditional languages. I took APL in 1976 and loved it. It was totally unlike any other PL I had seen but it turned out to be useful in only limited problem domains. APL was very much about programming using Math. I have seen no arguments or empirical data to suggest that Math notation or concepts are better than the status quo. A PL is a very well defined description of the instructions a programmer wants executed. Why the need for Math syntax? This isn't a swipe at Math. I use anything from anywhere that benefits my programming and that includes Math. Lazy execution is another one. I understand what it means. I could program it if I wanted in C. I have never had the need for it. I have never heard any good argument why mainstream programs would be better off using it for all computations. I have much more if you are interested.
The tooling work we we did over the years on a shake based build tool (shake-install) was going to be renamed `hake`... until stack came out and _hopefully_ made it irrelevant. But I thought it was a good name and a bit clever: * Haskell Make * Hake is a fish similar to Haddock (the Haskell documentation tool) * 4 easy to type characters, alternating between hands * https://hake.io was available :-) 
&gt; My two cents: I hope good discussion comes out of this, but I don't think that the "reddit subsection of the Haskell community" is necessarily a good representative pool of "Haskell users". I agree, [let's make a survey and ask everyone](http://www.reddit.com/r/haskell/comments/3b1yuk/haskellinfrastructure_fwd_new_haskell_platform/csi9rtx). Then the good discussion can be based on data and shared knowledge of the issues at hand.
Emacs isn't modal.
I think php had wiki style annotations to their online documentation in the late 90's. It would be great if Haskell was less than 20 years late.
I'm not sure I understand. After all, the [jade homepage](http://jade-lang.com/) even calls Jade a templating engine. If you're just wanting to run some code through a preprocessor that outputs HTML, seems like you could just stick with Jade. If you're wanting to go all Haskell, seems you can either write a parser for Jade yourself or write a small one off to something like hamlet (mentioned above), or [lucid](https://hackage.haskell.org/package/lucid). This is what the Jade example would look like in Lucid (with a bit of help from [raw-strings-qq](http://hackage.haskell.org/package/raw-strings-qq)): {-# LANGUAGE OverloadedStrings #-} {-# LANGUAGE ExtendedDefaultRules #-} {-# LANGUAGE QuasiQuotes #-} import Lucid import Text.RawString.QQ index pageTitle youAreUsingJade = renderToFile "index.html" $ do doctypehtml_ `with` [lang_ "en"] $ do head_ $ do title_ pageTitle script_ [type_ "text/javascript"] [r|if (foo) { bar(1 + 5) } |] body_ $ do h1_ "Jade - node templating engine" div_ [id_ "container", class_ "col"] $ do if youAreUsingJade then p_ "You are amazing" else p_ "Get on it!" p_ [r|Jade is a terse and simple templating language with a strong focus on performance and powerful features. |] main = index "Jaee" True
I know right. I tried doing this [awhile back](https://groups.google.com/d/msg/haskell-pipes/3EmcPigBjik/H-1xB_t43A4J) and my brain melted trying to get it to typecheck.
I am aware of the existence of MyPy and PEP 0484. The title is more tongue in cheek, because that was like a thought(and code) experiment about a more functional python with types in an alternative dimension.
Or in hamlet (In a separate file. Alternatively, with a quasiquoter): $doctype 5 &lt;html lang="en"&gt; &lt;head&gt; &lt;title&gt;#{pageTitle} &lt;script type="text/javascript"&gt; if (foo) { bar(1 + 5) } &lt;body&gt; &lt;h1&gt;Jade - node template engine &lt;div id="container" class="col"&gt; $if youAreUsingJade &lt;p&gt;You are amazing $else &lt;p&gt;Get on it! &lt;p&gt; Jade is a terse and simple templating language with a strong focus on performance and powerful features. I rendered using this: {-# LANGUAGE OverloadedStrings #-} {-# LANGUAGE TemplateHaskell #-} import Text.Hamlet (hamletFile) import Text.Blaze.Html.Renderer.String (renderHtml) template pageTitle youAreUsingJade = $(hamletFile "template.hamlet") main = putStrLn $ renderHtml $ template ("Jade" :: String) True "" I'm sure it's not 100% right, but the output is corect (though for this I output to stdout instead of a file, which would be a trivial change).
BTW, the definition of "primes" that is used there is certainly a popular one. However, contrary to popular belief, this is not an implementation of Erastostenes' sieve. In fact, it boils down to a naive check of divisibility by all prime numbers smaller than the number which is checked. It indeed works, but it is probably not an algorithm one wants to use as a showcase for Haskell.
You say you get the best of both worlds, but refactoring support's a pretty weak element of current Haskell tooling. It could be great, but it isn't - yet.
&gt; Sure, the script works, but what would ghc say? I love that!
How is this different than if you used e.g. Java 7 and your ops team deployed in a Java 6 environment? Communication between teams to keep everyone on the same page in terms of version compatibility, etc. is something that _always_ needs to be taken into consideration.
https://github.com/commercialhaskell/stack/issues/342
I second this. Just tried it today, and beside the fact that `ghc-mod` doesn't know how to find the sandbox it was awesome. It solves all my problems *on Linux*.
Indeed. But maybe we need that story for ourselves, anyway? I guess "medium-level" Haskellers also need those explanations. Maybe even I myself need those explanations? So I think we should explain it anyway. We could say that a magic 1-click thing just works (tm), but it would be a fat lie in todays environment. It was almost true for the Platform when the Platform started, but it is not true anymore. Same thing, I don't think sandboxes are a magic solution, or even any kind of solution (I think sandboxes are treating the symptoms, instead of even trying to do anything about the underlying issues).
You still have to indicate what version of e.g. lts haskell your project is intended to build against or whatever, regardless. "Build against all the new stuff" isn't a legit thing to say to an ops team.
That's certainly true, but you just said Java, which is more like GHC than an LTS collection of a thousand user-contributed packages. The confusion with the sys admin could have been avoided by OP, but the confusion due to the downloads situation ("So then I have to explain...") is on the haskell.org website.
That's why there is the class `ByteArray` in vincenthz's `memory` which is heavily used in his new crypto library `cryptonite`.
Both are the types used by their modules and `xxxToBehavior` transform it to the abstraction (eg. `counter` need `Num a` but could be more complex). Anyway, your comment help me more to see my problem as "currying modules". Thank you! :) (I need practice it)
You're welcome!
Reading Reddit comments requires a backtracking parser. Unlike HTML5, where parsers must accept certain non-closed tags and insert closing tags as appropriate, Reddit markup requires parsers to accept non-opened tags and insert opening tags as appropriate.
no it isn't. the haskell.org website doesn't say "stop" and in fact it points at multiple resources. https://www.haskell.org/downloads The _ghc_ website says "stop" -- and indeed it is _not_ recommended to download the raw ghc compiler. Perhaps it would be better to fix that website to point to the haskell.org/downloads page rather than the platform page directly -- but that's a secondary concern. Regardless, if you have an ops team that doesn't know your language and setup, you need to point them to very specific steps of where to go for what, not just throw them to the google wolves to find their way.
&gt; The ghc website says "stop" -- and indeed it is not recommended to download the raw ghc compiler. Perhaps it would be better to fix that website to point to the haskell.org/downloads page rather than the platform page directly -- but that's a secondary concern. Actually the raw compiler has been consider superior to the platform for a long time now by many. Stack is the first option that might turn out better but it certainly isn't a secondary concern that the GHC download page uses scary language to point people in the wrong direction.
What you're describing as the platform isn't. If anything this is a failure of our documentation, or the result of you being provided bad information. In order: The platform is a blessed set of libraries, versions, and a compiler. It doesn't "pin" anything. It doesn't split libraries -- the global vs. user vs. sandbox distinction is baked into the standard way packages are handled in GHC (and cabal-the-library). There is a perfectly legit way to switch between versions of ghc, and you can have multiple platform ghc installs side by side. Cabal warns you to not install packages that step on one another's toes -- if you want to break your environment you have to override its warnings. Upgrading doesn't require wiping your own install -- as I noted, they can live side by side just fine, and there are even scripts to switch between them.
That why I said "almost"... Anyway, platform was less outdated those days and the package ecosystem was also a bit less fragile. So the general experience with the Platform was much better a few years ago than now (haven't tried the brand new one yet).
Sadly they all depend on ghc-mod which is very hard to get working properly, especially with 7.10 :( I'm not sure how it interacts with stack either.
You have confused the raw compiler, which is just the compiler, with a minimal distribution, that at least comes with the cabal install binary. The raw compiler is intended mainly as an upstream distribution source for binaries. For end-users, some distribution is recommended, be it minimal or otherwise.
No, it isn't unlikely, to the extent that authors write platform compatible packages, as very many do.
&gt; No proper editor/IDE (Vim/Emacs don't count) You also forgot that you need to be able to read and write, *that* takes a long time to learn. Mainstream programming language should be visual, like [scratch](https://scratch.mit.edu/projects/10015059/#editor) ;-).
I've built it in many variations in various settings before but never packaged it. e.g. http://lpaste.net/79582#line22 The laws are just the Traversable laws with a "natural transformation" substituted in.
Incidentally, this "Strongly-typed bound" script is one of my favorite pieces of Haskell code. I started a [video series](https://www.youtube.com/watch?v=7mUxp-xrcUw) on it a while ago. I filmed most of the second episode, but got stuck on the editing. Making videos is hard...
&gt; the haskell.org website doesn't say "stop" and in fact it points at multiple resources. https://www.haskell.org/downloads He's referring to [this page](https://www.haskell.org/ghc/download_ghc_7_10_1) which still says: &gt;Stop! &gt; &gt; For most users, we recommend installing the Haskell Platform instead of GHC. The current Haskell Platform release includes a recent GHC release as well as some other tools (such as cabal), and a larger set of libraries that are known to work together. How do we fix that?
cabal-install directly off Hackage has a bootstrap script that worked fine for me for years, certainly a lot better than the platform ever did. I suppose for platforms like Windows, which treat people who want to use a compiler like second class citizens, a distribution was necessary. For Linux the least troublesome route was the raw GHC compiler + cabal-install with bootstrap.sh script.
Thanks. =) FWIW- Nowadays I'd implement the `lam` smart constructor in there quite a bit differently. I'd borrow from http://comonad.com/reader/2014/fast-circular-substitution/ rather than use the `unsafePerformIO` mess I pasted there. (That isn't to say I'd throw over `bound` for fast circular substitution, but that if i wanted a pseudo-hoas style `lam`, I'd get it into `bound` using that approach.)
Glad I could clear things up -- my frustration has not been over different evaluations of `stack` -- it is just that I want to make sure people don't get the wrong idea about what is possible with various other tools already, albeit not necessarily with as "one button" of a user interface.
This is the idea behind Cartesian closed categories. You write what looks like a lambda calculus expression but these lambda expressions are "overloaded" to work with anything that implements the `CCC` type class, which would look something like this: -- The exact specifics of this type class may vary, but you get the idea class Category cat =&gt; CCC cat where apply :: cat (a -&gt; b, a) b curry :: cat (a, b) c -&gt; cat a (b -&gt; c) fst :: CCC (a, b) a snd :: CCC (a, b) b pair :: CCC a b -&gt; CCC a c -&gt; CCC a (b, c) instance CCC (-&gt;) where ... So if you wrote an expression like this one: example :: \a b -&gt; a The inferred type would be: example :: CCC c =&gt; c () (a -&gt; b -&gt; a) ... and it would desugar to operations using the `CCC` type class: example = curry (curry (snd . fst)) This desugaring process is a solved problem and there's a mechanical way to turn any lambda expression into something that implements the `CCC` type class. In fact, you're not the first one to pursue a practical implementation of this idea. Conal Elliot had a similar idea which he popularized in a series of blog posts: * http://conal.net/blog/posts/haskell-to-hardware-via-cccs * http://conal.net/blog/posts/overloading-lambda * http://conal.net/blog/posts/optimizing-cccs * http://conal.net/blog/posts/circuits-as-a-bicartesian-closed-category Really, all that's missing to implement your vision is an `OverloadedLambdas` language extension that generalizes all lambda expressions to instances of something like the above type class. This is also why people say that lambda calculus is the "internal language" of cartesian closed categories.
I know a lot about CCCs, but there are a lot of categories we might be interested in that aren't CCCs. For example, linear type theories are, as far as I know, only CC (though you can construct a CCC by using `!a -o b` as morphisms). You are right that a nice syntax for CCCs would definitely be included in this language, though.
*shouts from the bleachers* PI TYPES!
Have you seen [TextualMonoid](https://hackage.haskell.org/package/monoid-subclasses-0.4.1/docs/Data-Monoid-Textual.html)? ( ͡° ͜ʖ ͡°)
The duality of And and Or to Product and Sum types has been known for quite a long time.
"The title is more tongue and cheek, because that was like a thought(and code) experiment about a more functional python with types in an alternative dimension." Of course that can't work with the current Python semanthics : ). 
It would have more women programmers. (Is that too sappy and SJW-y?) EDIT: Oh, wait, you excluded that didn't you... alright, I'll give my second-best answer: Magic.
I think the import syntax wouldn't be that annoying if we wouldn't need to import so many modules.
Is there the possibility of that making it into the Prelude? What about cases where old code uses `++`?
I doubt it. The code is ridiculously overloaded and creates many new classes. But we can still just include the lib and use it! Also, this is just an alternative to mono-traversable, which is the main go-to for working with String-like data in a common interface.
`fix foldl` isn't even well-typed :(
Looks awesome! It definitely looks more modern. Only suggestion would be to add arch linux to the list of linux distros: sudo pacman -S ghc cabal-install haddock happy alex
Linear logic is the internal logic of a symmetric monoidal category, not a Cartesian one. You add remove general purpose contraction and weakening, which destroys 'fst' and 'snd' which weaken and (&amp;&amp;&amp;) which contracts. You then can choose to carefully reintroduce it through the (!) modality / comonad, but that is a separate step. The power of linear logic is that you can talk about types that _don't_ have (!)'s on them -- not that you can shove them on everything to make a CCC again.
The irony of asking for help with how Haskell Platform is presented is that a Haskeller got an artist to volunteer her services for a new page design for HP and Mark drove her off by being unresponsive. Now it'll be difficult to get her to have anything at all to do with the Haskell community unless it's with somebody that has been vouched for. I looped in the founder of TravisCI to help with http://community.galois.com/pipermail/haskell-infrastructure/2015-June/000896.html and Mark never replied to Mathias asking for a URL to the Haskell Platform build so he could bump the timeout for us. Why are we trying to dump more resources into a tool that hasn't worked well and that we constantly have to tell users not to use? Blaming library authors for not using ancient versions of libraries doesn't fix anything. The tools need to work with the ecosystem (ie, the humans), not the other way around.
Yes, but again, even if you add those modalities the power of linear logic is that you can have values where you don't have them applied on everything. Not every object has the (!) pre-applied. We've gained the power to talk about types that don't. While the category itself isn't Cartesian, the Cokleisli category of (!) is a CCC.
I Need to create a "memory" where it have 256 positions. ex: ["0000","0000","0000","0000","0000","0000",...] but i want to save this result on a new list. how can i do? memory :: Int -&gt; String-&gt; [String] memory n x | n &lt;= 0 = [] | otherwise = x : replicate (n-1) x
I specifically changed the wording in the code from "sieve" to "filterPrime" so people would stop bringing this up. Nobody is claiming it's a sieve. Please come up with an alternative example that checks all the boxes we want instead of presenting the objection everybody's known about for months.
I don't think you mean duality, I think you mean correspondence.
Relatedly, for those who haven't seen it, there are only 4 inhabitants of ∀ a b c . (b → b → c) → (a → b) → (a → a → c) You can reason it out for yourself, but it was also explained using the Yoneda lemma [here](https://www.reddit.com/r/haskell/comments/2bj7it/let_me_tell_you_about_the_types_of_data/cj6ipqu) by /u/pcapriotti. I'm still not 100% sure what it means in the algebra / calculus of algebraic data types, but I still think it's really cool :)
That is a fair point, thank you for making it. I suppose it was a misunderstanding, so, apologies for my hasty conclusion and outburst. I think this was less about this thread and more about a perceived pattern, in hindsight. I was writing less about mentioning prior art, and more about nobody actually discussing the thing which the OP wanted to share (presumably to get feedback).
Yep. [Here's the math.](https://en.wikipedia.org/wiki/Curry%E2%80%93Howard_correspondence) edit: [here's the relationship between many other types and logic](https://en.wikipedia.org/wiki/Curry%E2%80%93Howard_correspondence#General_formulation)
Optional laziness seems so much more appropriate. People speak of modularity, but I always seem to write my code with strict evaluation in mind since I may add in strictness annotations later.
cool :) "H" is for "heterogeneous"?
The misconception is that List's purpose is to be a linked list. It's actual purpose is to be like an iterator, and it is alright for that purpose.
it's a constructor for church pairs, but with some intermediate case analysis and some less polymorphism.
Now that Monad is strictly a subtype of Applicative, is there a need for return?
no, they are identical
FWIW, back when it was first invented, people were very explicit about distinguishing the tool vs the library/spec. It's just that, over the years, people have gotten lazy about / forgotten about that distinction. Granted, it doesn't help that the `cabal-install` package provides the `cabal` executable for commandline use of the `Cabal` package. Names are hard.
What about something like double bang patterns for deep strictness, single bang patterns for WHNF?
Doesn't this also exclude some total functions?
A simple one - Swap ":" and "::" :)
The bit that I'm not sure about is how to interpret the number of inhabitants of type when parametricity is in play. We know that a -&gt; b has `b^a` inhabitants, as long as we know what `a` and `b` are. There's some kind of leap from that to (b → b → c) → (a → b) → (a → a → c) having 4 inhabitants if we can't know anything about `a`, `b`, or `c` that I find interesting. Even ∀ a. a -&gt; a -&gt; a having 2 inhabitants instead of a^a^a inhabitants is kind of fascinating to me. I understand that Yoneda helps there, what I'd really like is some kind of interpretation. I was trying to puzzle out something with roots of unity before I got around to asking if anyone knew :)
it looks like there is already a branch that implements overriding the install path
I don't why you have this idea of history - but it isn't what happened at all.
Did you try `try`? I don't know if it does what you want because I haven't used it, but it would be my first attempt.
The problem is that pretty much every `IO` could potentially fail. My output buffer could be full, my disk could have bad sectors, the network could be down, and stray cosmic ray could flip a random bit of memory. You're dealing with "The World", and the world has an almost infinite range of possible things that could go wrong that I don't care about. There is (almost) no such thing as an IO without a failure case. So every single side-effecting function would then inherently become "IO Either a b", at which point you might as well roll that up into a monad transformer and call it "MyIO" and be back at square one, because if you squint that's effectively what IO already is. I want to encode expected failures as part of my type system, things that are part of my normal flow control. "File not found", "DNS name not resolved", things that I would reasonably expect to happen. Except in extremely low level code, things like "The disk controller failed" or "Somebody tripped over the network cable" are not expected conditions, and I don't ever care about dealing with them. They are out of band signals that I want to be able to catch at a very high level and provide a graceful "500 - Server Error" page instead of completely crashing. The problem, I suppose, comes down to the fact that every use-case has a different definition of what should be considered "Flow Control" and "Exceptional". 
It depends on how it's implemented. Most functions you normally use are still perfectly acceptable. The average programmer rarely needs more than straightforward structural recursion, which can easily be decided by the typechecker. More nuanced recursion schemes are problematic. When the recursion acts on some invariant that depends on the arguments, but explicit in their structure, you will have a hard time. The simplest example would be something like the gcd function, and the next simplest is mergesort. In the gcd, the recursion is on the fact that every subsequent argument is smaller in absolute value. (And if you pass in 0, this invariant breaks, and you get stuck in a loop). The mergesort recursion scheme relies on the fact that a "sublist" of a list is shorter in length. Lastly, there is the minor issue of general recursion. Almost no programs you'll write in software will *require* general recursion. But at least, you can't write an evaluator for the language you're working in. 
Maybe it makes more sense if you stop thinking *this must work for some type a* and instead think *this must work for __all__ types a*. The only inhabitant of `a -&gt; a` is `id`: since we don't know anything about the value we are given, there's nothing we can do to it other than return it as-is. It doesn't make sense to look at "*a^a*", as whatever *a* is has nothing to do with the *function* -- only with a *single, concrete application* of the function to some value of type *a*.
Because even if you don't download the platform, you should download a minimal distribution such as ghc-for-os-x or minghc, and not the raw compiler. As I explained three times in this thread. And the message itself hasn't been updated because it's been there for years and we've had nobody doing any work the ghc website for most of that time.
If you look at the other replies on this thread you'll see that we do now have a volunteer working on it, and we put out a call for help with it some months ago. And furthermore, that there is a proposal to change that text. You're kicking in an open door outside of the attitude you're bringing with you. The typical answer to "why hasn't X good thing been done" is "we run on volunteer steam and it wasn't noticed or it was but there was nobody with the spare cycles to do it, please volunteer to help." And the more stuff that gets thrown at volunteers about how things aren't perfect, and the more outraged and entitled sounding it is, then the more mental cycles those volunteers lose to dealing with it, and the less appreciated they feel, and the less rewarding that volunteering is, and the less likely more people want to help, because they see what what a world of irritation they're in for just for trying to help. (sorry for dumping all this in a response to you in particular, btw. i'm expressing a general frustration at the tone of these discussions.)
You can use `turtle`: {-# LANGUAGE OverloadedStrings #-} import Turtle retry :: Int -&gt; IO ExitCode -&gt; IO () retry 0 _ = return () retry n io = do c &lt;- io case c of ExitFailure -&gt; retry (n - 1) io _ -&gt; return () main = do retry 3 (proc "cmd1" ["arg1", "arg2"] empty) retry 3 (proc "cmd2" ["arg1", "arg2"] empty) One of the advantages of `turtle` is the [beginner-friendly tutorial](http://hackage.haskell.org/package/turtle-1.1.1/docs/Turtle-Tutorial.html)
It bothers me that we don't have a fully-backtracking-by-default parsing library (and `attoparsec` doesn't count, there are corner cases where it still does not backtrack)
But the users of your program might! &gt; You're dealing with "The World", and the world has an almost infinite range of possible things that could go wrong that I don't care about. 
Yeah, I understand all of that quite well. It looks like I'm epically failing to communicate my point :) Ironically, in the comment thread I linked to, I was struggling to point out that I was talking about the parametric type rather than the concrete types :) I'm just fascinated that Yoneda gives us something that we can use fairly mechanically with some (all?) of these kind of functions in the count-the-inhabitants-of-types space. Edit: Not sure if this will help clarify things or make it worse - in my above comment, by "struggling to interpret", I guess I meant "struggling to map from the types to the arithmetic on the inhabitants of types in an intuitive way". We _can_ use Yoneda, but I'm wondering if the arithmetic-on-inhabitants metaphor holds when we do it for some reason. 
i edited it to reflect parametric types a bit more. also it's not so much that yoneda gives us this, but rather than yoneda helps us reason about it at times. the proof you mention uses yoneda in a very particular way that relies entirely on the fact that the type has the form `forall a. (T -&gt; a) -&gt; F a`. yoneda tells us that this is equivalent to `F T`.
Types don't form a semifield. They form a semiring. Apparently, this is cited from a mistaken post [here](https://pavpanchekha.com/blog/zippers/derivative.html#fn.2).
That's really nice feedback, thanks! A good eye for design is always appreciated.
I realize this was on /r/programming and /r/compsci recently, but I figured a lot of people here may not have seen it on those subreddits. It's really a high quality list. I think we can all learn a thing or two from the resources there. Enjoy!
well, it does, but you have to understand that `forall` is not going to preserve the sizes underneath it, that's all. there's no intuitive mapping for `forall`.
I want a lisp-like syntax I know it might put some people off, but it's really cute if you get used to it
This is a pretty minor thing and isn't really Haskell-specific, but I'd like to be able to write something like "0 &lt; x &lt; 1" and have it de-shugar to "0 &lt; x &amp;&amp; x &lt; 1".
Actually, we do! http://code.haskell.org/~malcolm/polyparse/docs/ https://hackage.haskell.org/package/polyparse
so, idris?
The module systems of the ML languages aren't perfect, but compared to ML, Haskell doesn't feel like it has anything worth even being called a module system.
There are all kinds of things I want to say: - This is Haskell. Haskell focuses a lot on safety. Let's not throw it out just because there's some IO involved. - I didn't say anything about avoiding errors at runtime. I didn't mention totality. So you can still have `error` if you want. - I was just backing up my argument with some experience with both using and trying to write software that doesn't suck. Getting completely unhandled errors, as a user, sucks. 
What do you expect partial application would look like with the more conventional, uncurried syntax?
Unfortunately, this would probably require either promoting `Ord` methods to special syntax, which is hackish, or else doing something way too complex that will break type inference and other properties.
It's for "higher" or "higher-order".
Here's where an IDE would help.
Sml had them swapped before Haskell even existed 
[D:](https://www.youtube.com/watch?v=umDr0mPuyQc)
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Therac-25**](https://en.wikipedia.org/wiki/Therac-25): [](#sfw) --- &gt; &gt;The __Therac-25__ was a [radiation therapy machine](https://en.wikipedia.org/wiki/Radiation_therapy) produced by [Atomic Energy of Canada Limited](https://en.wikipedia.org/wiki/Atomic_Energy_of_Canada_Limited) (AECL) after the Therac-6 and Therac-20 units (the earlier units had been produced in partnership with CGR of [France](https://en.wikipedia.org/wiki/France)). &gt;It was involved in at least six accidents between 1985 and 1987, in which patients were given massive [overdoses of radiation](https://en.wikipedia.org/wiki/Radiation_poisoning). :425 Because of [concurrent programming errors](https://en.wikipedia.org/wiki/Race_condition), it sometimes gave its patients radiation doses that were thousands of times greater than normal, resulting in death or serious injury. These accidents highlighted the dangers of software [control](https://en.wikipedia.org/wiki/Control_system) of safety-critical systems, and they have become a standard case study in [health informatics](https://en.wikipedia.org/wiki/Health_informatics) and [software engineering](https://en.wikipedia.org/wiki/Software_engineering). &gt; --- ^Relevant: [^Nancy ^Leveson](https://en.wikipedia.org/wiki/Nancy_Leveson) ^| [^Software ^bug](https://en.wikipedia.org/wiki/Software_bug) ^| [^Race ^condition](https://en.wikipedia.org/wiki/Race_condition) ^| [^Theriac](https://en.wikipedia.org/wiki/Theriac) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+csj5wfw) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+csj5wfw)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](/r/autowikibot/wiki/index) ^| [^Mods](/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Call ^Me](/r/autowikibot/comments/1ux484/ask_wikibot/)
Are there any plans to stop the platform's pollution of the global package database with so many more packages than absolutely necessary? 
Whichever you choose, be sure to post the code snippet here! Or create your own Hakyll blog and host it on Github Pages ;)
If you're referring to the `error` function that simply blows everything up even in a pure function - In that case, yes I agree this should not be used. (Nor should blindly deconstructing `Right` for the same reason). &gt; The problem is that too many programs don't handle errors when they should. Putting it in the type system helps us keep track of whether we have handled all the errors, without making it unduly hard to write quick hacks that don't handle errors. Sure, but how far down the rabbit hole of type safety should you go? I could argue that corrupt ECC memory is a potential error and you should encode it in your type system. But short of kernel level code, I probably don't care, and cannot reasonably do anything with it, so there is no benefit whatsoever for me having to add plumbing for it. Functions should provide type-system guarantees for things that could reasonably be expected to occur, everything else is just noise that gets in the way of handling things that are important. The best I can ever do is to bubble them up and deal with in a panic handler whether the plumbing is implicit or explicit.
Some minor parsing changes. I would really love being able to roll my own mixfix operators, and it would be great if you could override the default assumption that alphabetic functions are prefix. This would allow some very nice DSL applications. And of course, dependent typing for mere mortals, by adding bread &amp; butter dependent types like fixed length lists to base, and by supplying ready-made theorems that make it easier to deal with type-level numerals, so that GHC would know, out of the box, that`(α + β) ~ ((α + 1) + (β - 1))`. Edit: I found out that there are some plugins available which will leverage the GHC 7.10 plugin system to teach GHC some facts about arithmetics. NB, I just tried [ghc-typelits-natnormalise](https://hackage.haskell.org/package/ghc-typelits-natnormalise), and it seems to work great thus far \o/
couldn't you achieve that with `manyTill`? btw why do you use `parsec` not `attoparsec`? I migrated recently because `parsec` is not maintained anymore (not that much anyway AFAIK) but started to regret because I can't give to my users anymore a precise location for a parse error (for me as a developer it isn't a problem, it's more for users of the app). Starting to think `attoparsec` for computer-generated input, `parsec` if the end-user types the input.
and these things can bee implemented in other languages?
a better record system, definitely. one like PureScript's or Elm's.
This. So much.
Yes. But it will much more verbose and hard in a language without static typing, type inference and first class functions and data immutability. You can simulate the option/ maybe type in Python with classes but it has an overhead and the language doesn't enforce this. Some languages similar to Haskell are OCaml and F, an OCaml dialect to the .NET platform. There is also Scala that is a FP language for the JVM, however it doesn't have type inference like Haskell. The main difference between them is the purity and laziness, Haskell is the only one pure and lazy. Another feature hard to replicate is the Haskell compact notation. * Here some languages with [option type](https://en.wikipedia.org/wiki/Option_type) 
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Option type**](https://en.wikipedia.org/wiki/Option%20type): [](#sfw) --- &gt;In [programming languages](https://en.wikipedia.org/wiki/Programming_language) (especially [functional programming](https://en.wikipedia.org/wiki/Functional_programming) languages) and [type theory](https://en.wikipedia.org/wiki/Type_theory), an __option type__ or __maybe type__ is a [polymorphic type](https://en.wikipedia.org/wiki/Parametric_polymorphism) that represents encapsulation of an optional value; e.g. it is used as the return type of functions which may or may not return a meaningful value when they are applied. It consists of either an empty constructor (called *None* or *Nothing*), or a constructor encapsulating the original data type A (written *Just* A or *Some* A). Outside of functional programming, these are known as [nullable types](https://en.wikipedia.org/wiki/Nullable_type). &gt; --- ^Relevant: [^Nullable ^type](https://en.wikipedia.org/wiki/Nullable_type) ^| [^Null ^pointer](https://en.wikipedia.org/wiki/Null_pointer) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+csj73zu) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+csj73zu)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](/r/autowikibot/wiki/index) ^| [^Mods](/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Call ^Me](/r/autowikibot/comments/1ux484/ask_wikibot/)
If you're the type of person who likes to watch videos instead of (or in addition to) reading articles, this video covers pretty much the same concepts: https://www.youtube.com/watch?v=YScIPA8RbVE (and coincidentally(?) has essentially the same title).
&gt;There is a ~~standard~~ library function called X which does this for you. Thanks for summing up my Haskell learning experience.
Or TH, which might be the most reasonable solution. Not sure.
What's wrong with record syntax?
Anything to ease the burden of overlapping names. Meaningful typeclasses, undisciplined typeclasses (we already have `Num`, don't we?), some other kind of overloading facility, or something else entirely -- I don't care -- I just hate having to type `Vector.length`. And while I'm fantasizing: * more flexible names * mixfix operators * make `:=` the assignment symbol, so the equality test can be named `(=)` * anything to make effectful programming less clunky -- no more `readTVar` and the like
That's still the wrong default. Making laziness explicit in the type with a `Lazy t` type seems far better to me.
Sounds like a line from a Robert Harper post.
When people complain about "exceptions in Haskell" they're generally talking about 2 and 4. You can't forbid 1 and 3 after all! They come for free as soon as you have sum types.
I think cdsmith is saying the existence of record syntax is a psychological impediment to getting something better.
Like what?
Oh c'mon, you can just put some parens ((putStrLn (((&lt;&gt;)) ("Hello, ") "world")))))) 
For example, different records using the same field name is a common request.
`if then else` syntax. It's not composable, it doesn't play well with do notation and nesting if's becomes an ugly mess. If should be a function in the prelude. *MultiWayIf* is a bit better, even though I rarely use it.
Is this even possible in the real world?
Would you happen to have a link to some documentation of these corner cases? I'm currently writing an attoparsec parser and getting some really strange results, which this might just provide an explanation for.
Mixfix support then?
The current page is less flashy but I think it better shows what kind of options you have when installing ghc. I would not recommend using Haskell Platform as the default option for windows, MinGHC has worked a lot better in my experience. 
I think people miss the point of the article. He is not claiming that imperative programming is inherently evil, just that it isn't the right model when dealing with *parallelism, concurrency* and *big-data*. And that a mostly functional model is still an imperative model with all the drawbacks of an imperative model.
To answer a couple of questions, I didn't use: * any of the other backtracking parser libraries because I was keeping my dependencies low. * `manyTill` because it doesn't keep the results of the both parsers, `untilP` does. TMK, there is no combinator in `parsec` that does this.
&gt; but none of the tensors you can use to turn your linear logic into a monoidal category will let you build a proper currying natural isomorphism when you go to define c^b = !b -o c! Which is why I talked about using a different category, constructed from the non-CCC: &gt; though you can construct a CCC by using !a -o b as morphisms On the other hand, the linear type theory is was talking about *is* Cartesian; the product is the additive conjunction.
Unboxing requires specialization, because losing the unified boxing, values of different types are laid out differently and behave differently. A single polymorphic function cannot (easily) handle values of different types. So you have to specialize the function for each set of types it was called at, each handling a particular set of unboxed types. This is the strategy all C++ compilers take, and I believe it is far superior to the boxing strategy used by GHC.
I'd like to throw in some more desiderata: 1. Don't use backtracking, instead explore all variants concurrently. Backtracking requires to keep the entire input in memory (also known as a memory leak). 2. Proper prioritized choice, not the kind of committed choice that PEGs have. 3. Parse context free grammars in polynomial (cubic) time, and deterministic grammars in linear time (and while we're at it: parse nondeterministic but regular grammars in linear time). 4. Support left recursion.
You'd [have to](http://stackoverflow.com/questions/10534744/static-types-polymorphism-and-specialization) give up polymorphic recursion, `ExistentialQuantification`, `RankNTypes`, and either type classes with polymorphic methods (such as `Functor` and `Monad`) or separate compilation. (You might want to take a glance at Rust.)
Add Nat &amp; Natural as unsigned counterparts to Int &amp; Integer. Change all indexing functions to take Nat such as (!!),(!) take, drop, takeWhile, dropWhile, replicate, etc... No more exceptions getting thrown!
Check out [uu-parsinglib](https://hackage.haskell.org/package/uu-parsinglib); it does the first two, at least on context-free grammars.
Well, you could have boxing for a small subset of the values that need those features. Even then, I think the boxes could be metadata describing the memory layout of the unboxed values which could still use far less indirection than Haskell boxes currently use. 
 (∈) :: Ord a =&gt; a -&gt; (a,a) -&gt; Bool x ∈ (a,b) = a &lt; x &amp;&amp; x &lt; b
Scala does the same. It is actually the correct mathematical syntax. 
Won't be good for much longer then - as that's moving to `io-streams`!
Thanks for the detailed reply. I designed my language/database for concurrency from the ground up. There are 3 ways to provide concurrency with the best method being the first. 1. Group your data and keep each group totally separate. Give each group it's own local memory. Allow each group to only run on a single thread (green or OS). 2. If a group of data is immutable then you can use it from multi threads without problem. Write as many pure functions as you can as they can be used without problem from anywhere. 3. Global resources need enclosed in a critical section (locks) to serialize access by multiple threads. Design your system by minimizing global resource access. I didn't invent these 3 points (got them from Joe Duffy, a senior PL developer at Microsoft) but I had already worked them out myself. Please note that #2 is exactly what FP does all the time even when not needed. The above 3 points can accommodate BOTH mutable and immutable data. If a program can be written in "pure" form, I have always done that in C (since 1975). If the data can be immutable, that is exactly what I make it. The difference is that most projects need mutable data and FP tries to make you jump though hoops to solve those problems. (Or just put all the mutable data in a database which isn't an argument for the superiority of FP but it can work.) The language I am creating with my C project allows all code to be written as if run by a single user on a single thread. There are no lock annotations, functions or commands. There are no instructions to allocate memory or de-allocate it. Persistence to the disk of any object is automatic. No open/read/write/close access to files. There is a persistent queue of messages and all Servers (my name for a macro scale group of data and functions) execute concurrently on any number of cores. SQL is implemented by translating the SQL into my language and then running it. All database tables or data live side by side with the code. I am not saying that you can't get concurrency by using FP and immutability. I am saying that my system is a real world example where you can have mutable data and automatic concurrency without any special coding at all. Eric implies otherwise. My language implements automatic memory management but I don't use GC algorithms. I don't globally track down live pointers nor do I have any memory collection pauses. I use a combination of 5 different memory managers and as many local memory managers as are needed. Memory is automatically de-fragmented using multiple methods. It can get a little confusing when I talk about my project because most of my comments have been from my C project point of view. The language I am creating is very different from C. I take away (like you mentioned for Haskell) many things other languages allow like threading, memory management, persistence to the disk, database connections, pointers and many other features of languages like C or Haskell. The language I am creating was never intended to compete with C++, C or Haskell. I agree that application programs should never be written in C, C++, Rust, Go etc. Tools and languages (like mine), however, can't be created in anything but these languages. Before I conclude, let me say how nice it was to get such a detailed response, much appreciated. In conclusion, languages like Haskell didn't come up with the idea of pure functions and immutable data. Both of these are available in ALL programming languages and have been since 1950's. If Haskell wants to win the debate on it's superiority, it will have to prove that all coding can and should use only these 2 techniques and forgo all others. Haskell debaters will have to prove that Math syntax and concepts are superior to all other PL constructs. I am not arguing that pure functions or immutable data isn't useful, it just isn't sufficient. Please note that I haven't argued that C is the best language for any type of programming. I have only showed examples from C that refute the points that Eric has made about the superiority of FP.
[`Early`](https://hackage.haskell.org/package/Earley)?
`if ... then ... else ...` is already just `case ... of True -&gt; ... ; False -&gt; ...`.
I like this idea. Provide people with simple information so they can make a decision! It also wouldn't hurt to put a label on one option saying 'we recommend most people do this', as long as the reasons are clear.
Unicode operators are a nice feature. But that only works for open intervals. [This](http://hackage.haskell.org/package/data-interval-1.2.0) was what I was talking about. It's not quite as nice syntactically but it's a bit more flexible. Ideally I'd like to write `x ∈ [0,5)` for half open intervals too, although most editors would complain about mismatched parentheses. Aside from the a `(∈) = member` function alias, the package above has pretty much everything I need when it comes to interval arithmetic.
To my knowledge the site isn't even in svn. And bear in mind this text isn't just on one file, it is at the top of the download page for each individual compiler release. I think it suffices to replace the text for just the current release as a start given that people apparently find this particularly troublesome. I'll make sure there's some followup here, but given the way this issue was raised I'm not very happy about this whole process.
Great post-- even though most of it was over my head, It's awesome to see Haskell being used in live features at Facebook!
That would indeed be a better approach to the topic, I agree.
Nice post! You say: &gt; We're careful to ensure that we don't change any code associated with persistent state in Sigma. Does this mean you don't change the data format of persistent state at all? Or do you have some way to migrate it? And is this just ensured by careful programming, or do you use types somehow to enforce this?
There's a clear boundary between the code that doesn't change and the code we can swap - they're in different repositories - and the code we swap doesn't have any persistent state. We typically restart the processes once a week or so to roll out new server code, and when we do that we can change any of the code, including the persistent state representation.
This is precisely the different in priorities between application-level code and systems-programming code
It took me way too long to see why there were more than 2 inhabitants (switch the 'a's or not). I'm so used to working linearly, I didn't think to duplicate the first or second argument. I wonder how the number of inhabitants change generally for sub-structural type systems? 
&gt; Well, you could have boxing for a small subset of the values that need those features. This is a solid goal and simple to state, but not necessarily so simple to specify. For example, consider what happens if you lay out the fields of polymorphic datatypes unboxed, like C++ does, and then quantify over the type variable. Now you no longer have a statically known size for the type, nor (in the general case) offsets for its members. (Rust's approach here is to make indirections explicit, together with the so-called "dynamically sized types" scheme - essentially it enforces that types without a statically known size *must* be behind an indirection.) &gt; Even then, I think the boxes could be metadata describing the memory layout of the unboxed values which could still use far less indirection than Haskell boxes currently use. I believe this is what's known as [intensional type analysis](https://www.cs.cmu.edu/~rwh/papers/intensional/popl95.pdf) (not to be confused with intensional type theory).
It depends. As a user, I want applications to provide information to me about what went wrong. If I'm using some application, whether GUI or command line or whatever, I want to know that the application couldn't save my file or make a network connection, and I want it to be presented to me in a useful way. If I'm writing a backend distributed system, I want to make sure that my program gracefully handles unavailability of remote services. So in some sense, it's an application concern. But it is also often possible to abstract away these concerns from core logic in an app. You can define your core logic in some monad other than IO (or even other things than a monad), and then have the IO layer do appropriate things with the errors.
Actually, I wonder if this can be counted as a language feature/bug. It is somehow represented in the created AST, but its practically not supposed to work in the original type system of the language. 
I am not familiar enough with the concept of a semiring to judge what the problem is but while we are at it, `Float` should not have an instance of `Eq` either.
&gt; It is actually the correct mathematical syntax. After Haskell lead me to read more and more math documents in recent years I am not convinced such a thing exists...at least not one more than 90% of all mathematicians can agree on for any given bit of notation.
You can't make things locally lazy. That would require a lazy version of every single function you plan to use in those lazy sections of your code. You can make things locally strict though simply by forcing evaluation. Thus laziness is the correct default.
I haven't thought about this, but, for the purposes of syntax design, I think it really is a question of how often partial application is used relative to full application. Hypothetically, if partial application is used infrequently, it could look really ugly and it wouldn't matter much. As soon as we start teaching mathematics to children using curried syntax from the start, then there will be no issue with using it in Haskell and other languages. Until then, partial application is the tail wagging the dog.
I believe mixfix refers to the way if/then/else are in between (or mixed with) their arguments.
I've discussed this here recently, and the takeaway ended up being that `Eq` doesn't actually have any laws, so the `Eq` instance for `Float` is technically valid.
And by help you mean work around the problem.
Something like Common Lisp's [eval-when](http://clhs.lisp.se/Body/s_eval_w.htm) would be nice, i.e. a construct to easily wrap an expression in to tell the compiler to evaluate it at compile time and replace it with the resulting value. This would be useful to allow smart constructors to reject constants in the code if they are syntactically invalid, to compute lookup tables,... I know you can probably already do so via Template Haskell but that always feels like a completely different language. I am thinking of a stream-lined construct that works for everyone, even those who don't know the details of Haskell's AST.
I don't get it. That ext. was the first thing to occur to me; /u/cdsmith , is that what you meant?
Where is that? You mean the `:` (also `|`) in set-builder notation?
Obj-C copied it from SmallTalk, yeah.
Can anyone involved speak to what sort of dev environment they use? Kinda curious what people being paid to write Haskell use.
what about Rust? it has immutable variables by default, statically typed, type inference but Im not sure about first class functions.
Blast! Sorry about that ._.
&gt; Huh? Not sure what you mean. I mean type notation: def f : Int =&gt; String = ... or val x : Int I mean why is it "the correct mathematical syntax"?
This project's development was done via Linux command line tooling. The developers were using their favorite flavor of emacs/vim and building using Facebook's build tool-chain.
I don't understand your reply, since your www.codeworld.info dialect primarily uses conventional function syntax, at least as far as I can tell.
Hmm. "Why doesn't SHE do that already?", I ask myself.
Smalltalk actually enjoyed some popularity in the 80s and early 90s, until Java came along claiming to be the same thing with curlier braces and turned OO into a nightmare of semantically-meaningful single inheritance. (Smalltalk used inheritance for code reuse, but it basically didn't have a type system at all, only the clever illusion of one.)
&gt; Haskell's FFI is designed to call C rather than C++, so calling C++ requires an intermediate C layer. In most cases, we were able to avoid the intermediate C layer by using a compile-time tool that demangles C++ function names so they can be called directly from Haskell. Tell us more! :) Is such tool open-sourced anywhere?
It's a simple bit of Haskell code that turns a C++ type into the mangled name, we call it from hsc2hs at compile-time. Open-sourcing it is on our roadmap, but I can't tell you exactly when we'll get to it (hopefully soon).
Because She still wants to be Haskell. 
Interesting. My experience is pretty close to that of Koen Claessen and Jean-Philippe Bernardy in [Efficient Parallel and Incremental Parsing of Practical Context-Free Languages](http://www.cse.chalmers.se/~bernardy/PP.pdf) that _with a good sparse matrix multiplication routine_ and having separated out Kleene stars, Valiant is n log^2 n in practice. So now I can do an initial parse in n log^2 n time in which I get to exploit parallelism and update it incrementally in ~ log^3 time. By working in a modified finger-tree you can get rid of all that sequence-algebra nonsense as well. Valiant's parser effectively glues together two partial parses through an intermediate terminal. That terminal rather drastically filters the space of 'live' parses down to something logarithmic'ish in practice. So you are only usually dealing with two logarithmic 'frontiers' you are zipping up, not all n states.
Right, but surely we shouldn't *prevent* the programmer from doing a straight equality check?
And replace `(&gt;&gt;=)` with `bind :: (a -&gt; m b) -&gt; m a -&gt; m b`, in keeping with the usual argument order for higher-order functions.
What do you mean by monomorphic? [This](https://en.wikipedia.org/wiki/Monomorphism)? Or do you mean that its type is of the form `S x S -&gt; S` for some `S`? If that's the case, that is [closure](https://en.wikipedia.org/wiki/Closure_(mathematics\)), which is one of the requirements of a monoid (and, more generally, semigroups and magmas). Also, abstracting generalizes things. As a result, commonalities are identified between different operations so I don't think it's necessarily a problem that + and * would be viewed as similar. It pretty strongly depends on what you mean by "the proper abstraction." If you want to differentiate them though, you could take into account that * has an [absorbing element](https://en.wikipedia.org/wiki/Absorbing_element) and + doesn't. Going the other way, + forms a (commutative) group (if negatives are included at least) but * doesn't.
Abelian group.
There are no entry level functional programming language jobs (yet?). What you have (in the U.S.) at least is: companies want a Senior Developer in some particular technology X, with an FP background too. My suggestion is to learn FP concepts from FP languages, then immediately drop that and focus on a marketable language, from which you can utterly school other programmers (and lead "OOP Architects") using the FP paradigm :D. But you won't get payed accordingly for that in the short term[1], trust me on this. [1] What happens is half of your colleagues ignore your work and pretend it doesn't exist, or that it's some weird style of non-OOP. The other half constantly try to learn your technique. This goes on for a few years, then magically you're delivering better systems incorporating those techniques and can talk about them in such a way as to demonstrate their value -and then you get blamed for destroying QA and maintenance jobs (if in a large company). I saw the writing on the wall as to the sluggishness and long-term frustration of such a career progression, and I left the industry to work at my own startup instead.
But plus on the naturals does *not* form a group. There are no inverses. Yet it still has this property. And we still call it "plus".
As long as we are talking about floating point only, why not?
Haven't seen that notation since my introductory Algebra course.
Ah, you're using sparse matrices, I was assuming dense matrices. With sparse matrices you would most likely indeed get good asymptotics for all practical grammars, depending on how many entries you end up computing that a left-to-right parser wouldn't need to compute. The constant factors are still an issue I think. Until we're going to get more than n=8 cores it's kind of difficult to get actual speedup out of parallel algorithms with worse constant factors :( For parallel parsing it could work better to chop up the input into `n` chunks and then parse those left to right with a kind of modified Earley and hope that most of the unneeded alternatives die quickly. The trouble with subdividing it until the end is that you end up doing a lot of unnecessary work. The substring `if(p){ x = 1+2; }` could be parsed in at least 3 ways. Maybe it's part of the actual program, maybe it's sitting in a comment, maybe it's sitting in a string literal. When you don't know the context you have to parse it in all 3 ways (or invent heuristics to avoid that). A left-to-right algorithm already knows whether it's part of the program, a comment, or a string literal, so it only needs to parse it in one way. For incremental parsing you would need to subdivide it at least almost until the end to get mileage out of it (perhaps a structural editor is the correct answer here...). It's certainly a more interesting approach than I initially thought because of sparse matrices! Can you perhaps do left to right parsing with the same approach by introducing one terminal at a time? (this is also nice for parsing data that is being streamed from the network or disk) Can you modify it to get some kind of monadic parsing back while retaining incrementality &amp; parallelism in practice (if the substring parsed under the bind is short)?
&gt; &gt; Obviously there's some distinction, since I can't just replace + with * in my &gt; code and expect coherent answers. What is that distinction? &gt; While you wouldn't get the answer you wanted, why wouldn't you get a coherent answer? The most obvious distinction I can think of is what sets arithmetic plus/multiply out of the domain of a boolean algebra, + does not distribute over *. In a boolean algebra, 1 + (2 * 3) would equal (1 + 2) * (1 + 3). So in this sense + obeys one fewer law than * does as * still distributes. I'm not sure if it makes sense to say that + should not encompass operations like "multiply" or "product". It's really hard to come up with things + has that * does not.
Here is [one issue](https://github.com/bos/attoparsec/issues/76)
Using `extern "C"` was not enough?
No particular syntax. My exposure to math is very basic compared to everyone here (think introductory Calculus-basic, certainly not type theory-lvl); and, where applicable, we always wrote about 'domains and co-domains', literally, instead of signifying in symbols that `f` 'maps' from one to the other. In fact, I haven't heard functions being called 'maps' since Intro Algebra where that exact notation was used (but more like `f: x |-&gt; y` where the `|-&gt;` is smaller and more arrowy).
I'm not sure `extern "C"` is enough when you have to deal with objects. Non-static methods always takes `this` as an hidden argument and I'm not sure it works so easily. More [here](https://wiki.haskell.org/CPlusPlus_from_Haskell).
`unsafe` is usually reserved for functions which are, in general, *unsound* in a type-theoretic sense, not just easy to misuse.
I'm new to Haskell myself, but I can point to some minor things that will make your code more 'Haskelly'. iterate n a = take n $ repeat a -- Oh... In fact: replicateM n a = sequence (iterate n a) -- e.g. takeAllLines = replicateM 9 takeOneLine -- needs Control.Monad [Hoogle](https://www.haskell.org/hoogle/?hoogle=Int+-%3E+a+-%3E+%5Ba%5D) is your friend for finding these little things. I frequently forget the name of iterate myself... Stylistically, two things. First, pattern matching in functions definitions [(quick examples)](http://learnyouahaskell.com/syntax-in-functions) is much more usual than case expressions. Second, *where* clauses are somewhat more usual for helper functions than let ... in, (I think) especially functions that don't use your parameters. Combine these two and your code will 'scan' much faster by good Haskellers, getting you better advice than mine :) Example: sumAllPosibilites s = Seq.foldrWithIndex sum 0 s where sum _ (FieldPossible n) c = Set.size n + c sum _ _ c = c Also, here, as you aren't using the index you could import Foldable and use foldr. Importing a module for one function seems to be more the rule than the exception in Haskell, especially for anything in the Typeclassopedia.
Are there any total languages you would recommend looking at?
I'm curious about "Facebook's build tool-chain," particularly the ways that it invokes ghc.
I've seen a few entry level FP jobs. That could be related to the meetup I'm active in / the associated meetups that I keep tabs on.
not sure what you want exactly, but: &gt; coproduct seems to have virtually none of these properties is not really true. coproducts are associative up to isomorphism (which is good enough for category theory), and if the category has an initial object (for example in an abelian category), than this is neutral with respect to the coproduct (again up to isomorphism).
Have you checked out [neovim](https://github.com/neovim/neovim)?
It's worth noting that that example is not real quicksort -- quicksort works in-place on an array, and uses mutation, whereas that algorithm is not in-place, it works on a list, and does not use mutation. (They do appear to acknowledge this, by referring to the C version they reproduce as "true" quicksort). See also [this stackoverflow question](http://stackoverflow.com/questions/7717691/why-is-the-minimalist-example-haskell-quicksort-not-a-true-quicksort). Having said that: while the Haskell implementations of "true" quicksort they give on that SO question are certainly not as pleasing as the "false" one, I would still hugely prefer maintaining one of those to the one in C. I can't actually think of an example where I'd prefer an OOP language to Haskell. Perhaps that's because I've drunk too much of the FP kool-aid, or because I just haven't used an OOP language for such a long time, or because I think OOP is a terrible idea.
A coproduct is what you want. "Conceptual Mathematics" by F. William Lawvere is one of the best introductions to category theory for those who haven't majored in mathematics. There's a section that, if memory serves, talks about universal mappings and obviously product/coproduct. At least read that bit and then work out the your list of types by treating each as a category. 
Hey, this is what I came up with, and it's rough, so bare with me. We can dictate this property in terms of a relation, &gt;=(n,m). We now have a straightforward way of putting it: &gt;=(n+m,n) on the naturals. So my thought was, is there something analogous for *? As it turns out, there is. Instead of "all greater natural numbers than n" you end up with "all natural numbers divisible by n". There might be a pretty deep rabbit hole to go down here, in particular why the rational numbers tie these two things together. However, interesting observation too. + also yields all the numbers divisible by n, while * does not get you the greater than relationship.
Hmm, I'm not sure I see the connection with categorical products and coproducts. What category would this be in? I'm thinking that the objects would be numbers but it isn't clear to me how the corresponding universal properties would fit in, if products or coproducts are identified with addition or multiplication (what would the projection/injection maps be?). I might be missing something though, since I don't have formal category theory education either. Maybe, a bit more indirectly, you could say something about a functor from a category with products/coproducts and a concept of size, say the category of finite sets, to a category such as the category of finite ordinals (or maybe the simplex category). A Cartesian product of two finite sets A and B would result in a set with a cardinality of |A|*|B| and the disjoint union of them gives you set with a cardinality of |A|+|B|. That's the first connection that came to mind, but it would only apply to natural numbers though.
I know how to make a game using FP. That's not the problem. The problem is making it performant. For instance, a well known optimization technique is to use a struct of arrays rather than an array of structs. This is impossible in Haskell without using IO everywhere.
Ah, fair enough then. I never got into a position where performance was causing much of a problem in a game I was working on, neither in Haskell nor any OO/imperative language, so I suppose I can't really add anything here. The closest I got to really worrying about performance was when some of my rendering code was very slow, but then I rewrote that particular bit in JavaScript using the FFI, and everything was OK again.
I'd sure like to hear more about that hot code loading. I didn't know we could do that. C++ mangling is another cool idea. I can work out how to do that I guess. But the hot code loading makes me want to go work for Facebook and quit my first day after I get my filthy hands on that technology.
I can. Anywhere that Lua is the best choice, Haskell is the worst. This isn't so much an FP vs OOP thing; Lua is as functional as you want it to be. It's a footprint thing. Shipping an embedded ghci is about 1000 times bigger than Lua. Also Matlab doesn't need to loose any sleep over Haskell, neither does Excell. Let's not forget R, Julia, SQL or Bash. Basically any system that sacrifices everything else to do one thing well is unbeatable in its domain.
How does the struct of arrays rather than array of structs optimization make things faster?
`Word`/`Natural` are already there. (And `Nat` for type-level ints.)
Oops! Yes, you're right. Fixed
Not if you need to mutate the state across threads or through FFI callbacks.
Most of the C++ code we need to call uses classes, so extern "C" doesn't work. With the mangler tool we can directly call C++ class methods from Haskell (you have to pass `this` explicitly in Haskell, of course).
I worded that poorly. You really just can't use arrays/vectors in general in Haskell if you need to make lots of minor changes in sequence.
These were the only things I would change after spending a few minutes reading your code. Don't just accept them as better, because these are some picky nits. 1. I would flip the order of the args to `withToken`. Typically, from what I have seen, `withX` functions have types `X -&gt; Thing -&gt; Thing`. This allows you to partially apply like `withX myX` to get a `Thing -&gt; Thing` function, and it is really easy to chain a bunch of those together if necessary. This is an example of the "the data structure goes last" rule (which is nothing hard-and-fast, keep in mind). 2. I think that your `case v of` statement in `handleResponse` can be shortened using a prism (you are already using Lens so why not). I don't know if aeson provides prisms for Value or not. 3. The only reason `modify` from `handleResponse` returns a `Maybe` is because you pass it to `alter`. However, you know it always returns a `Just`, and this forces you to use `fromJust` later. I would suggest removing `Just $` from the definition of `modify`, eliminating your usage of `fromJust`, and passing `(Just . modify)` to `alter` instead. 4. Right now, the first thing you fail to parse will crash everything and prevent (unrelated?) UUIDs from being handled. Is this what you want? I do not know. 5. The name of the function `force` makes me think that you are forcing the evaluation of something, like in the deepseq package, but this is not what you are doing. I would name it `fromSuccess`, but this is a REALLY small deal. Obviously, like you mentioned, using 4 string representations is quite a hassle, and the biggest improvement would probably come from eliminating some of them, but I don't have a good solution for that at the moment.
I think there should actually be two types: one for platform-specific (unportable) and a portable type. This is because there exist paths that are valid on one platform but not valid on another (e.g. Unix allows `:` but Windows does not). The portable type can be thought of as the intersection type of all platform-dependent paths.
I don't get it. Why not?
Facebook uses a fully self-contained build system, including the entire compiler tool chain, so that builds are fully reproducible regardless of the host system. Integrating GHC into this framework was non-trivial, but many of the changes we had to make to GHC were pushed back upstream - mainly things like making sure we propagate custom C compiler and linker flags everywhere in the GHC build, and making the GHC installation independent of its location in the filesystem. To build the packages (a subset of Stackage LTS), we use cabal-install to create a build plan, but do the actual building using our own set of tools on top of Cabal-the-library. The build system used for the project source code is another system entirely, and there we invoke GHC directly (no Cabal-the-library). We needed to integrate with a lot of C++ code and an existing build system, so it made sense to add Haskell support to that build system. However, it's been quite a lot of work, for example we only just got Template Haskell support working.
I'll definitely put it in a bit repo tomorrow, sorry about that.
It really depends on the company's needs and budget. Sometimes you need a hardcore specialist in the stack that you're using, and you need him asap. Sometimes you're happy with a hire who's not familiar with your technologies, but who just generally seems very capable of learning really fast. Maybe you have the time and and expertise to bring that person up to speed for the long term, so it's a worthwhile investment for the company. Sometimes you have 12 months left in the bank, and a hire becoming finally productive after 6 months will probably kill your company. Being sharp, disciplined, humble, interested in the craft and aggressively productive are all qualities that span beyond just Haskell. Having them should easily earn you a spot in most shops.
There was a recent [discussion about Haskell, the language](/r/haskell/comments/3b498l/if_you_could_change_one_thing_about_haskell_what), where we tried to find flaws. The language itself is superior, but there is much space for improvement in the environment. 
I'm not sure what you mean by using a "unix `FilePath`" in a windows system. Can you be more specific?
&gt; Would the `toFilePath` function be partial? That is, would `toFilePath` throw a runtime error if a badly formatted filepath was given as an argument? No. It was considered, but it would complciate things, as whether a filepath is valid may depend (beyond the current OS) on the current locale settings as well as the filesystem used (Linux supports dozens of filesystems) etc. So in the interest of KISS, the default conversion functions are pure and total. IOW, a `FilePath` doesn't encode any invariants regarding the validity of a filepath. &gt; Similarly, would Template Haskell functions (à la path) be added so that badly formatted filepaths can be detected at compile-time? Defining smart QuasiQuoters is definitely possibly, but simply not part of this proposal, as this proposal aims to be minimal (with the intent to become part of a future Haskell Report, for which TH/QQ is very likely out of reach) &gt; Another feature of path that I find useful is the use of phantom types for marking what kind of path it is, e.g., Path Rel Dir or Path Abs File). Is there a reason that the proposal decides against this? Yes, for simplicity. To quote what I already wrote on the mailinglist: &gt; Trying to redesign the FilePath type to also include dir/file distinction seemed too daunting, as there's quite some additional design-space area to explore (do drive-letters deserve a separate type? do we use DataKinds? What invariants can/shall be represented at the type-level? what errors are caught at the type-level, which are caught at runtime? etc...), parts of which may require type-system extensions, while just having a KISS-style opaque FilePath evades this.
http://gamedevelopment.tutsplus.com/articles/what-is-data-oriented-game-engine-design--cms-21052 The principle is to design your data (and their layout) by how data is transformed in the program, rather than by some other organizing principle^1 . So if you have entities with three attributes, and each one of those attributes are updated/transformed in bulk, then you might want to make an array for each of those attributes in isolation so that you can iterate over them with better cache locality. ^1 As opposed to the more common organizing principle, which is to tie things that are *logically connected* together, such as by making an entity record/struct with all the attributes of that thing.
GHC keeps track to a certain extent whether a type-synonym was used, e.g. λ:2&gt; let x = "foo" :: FilePath x :: FilePath λ:3&gt; x "foo" it :: FilePath λ:4&gt; x &lt;&gt; x "foofoo" it :: FilePath λ:5&gt; x ++ x "foofoo" it :: [Char] That's why I think that GHC could actually warn when it has to drop the type-synonym color. But I haven't looked into it in detail yet. That's why it's marked as a TODO item.
I'm in favour of this KISS approach; get the type opaque. That's a big enough upheaval by itself. Indeed, the typed path design space is still being explored; `path` is my synthesis from experience on work projects, others have and are trying varying approaches.
I think you might have misunderstood what I meant. With the approach that I meant you don't need any post-processing. Parameterize all your parser combinators by the nesting level of blockquotes in front. For example if you had a list parser: list = ... some parser code here ... You change it to this: list n = ... some changed parser code here ... This is supposed to parse a list under `n` nestings of blockquotes, e.g. `list 3`: &gt; &gt; &gt; * one &gt; &gt; &gt; * two &gt; &gt; &gt; * three &gt; &gt; &gt; * four Inside the parser you start every line with `n` times `&gt;`.
You're right I didn't follow you. Ok I get it now. I don't know, I think it won't work that well in practice. I have the feeling it'll get messy I think when the blockquote level changes and to find which level I should take into account at a newline, and I don't like the idea of polluting each parser with that. I also worked with an idea like that of providing the parsers with a transforming function (I was giving them `sepBy endOfLine` or `sepBy (endOfLine &gt;&gt; string "&gt; ")` to simplify), but it made everything messy and harder to follow and didn't even really work well. Still I'm keeping this in my mind, though it won't be my first choice right now.
Forth?
Remember Windows has absolute paths, drive relative paths, path relative paths and fully relative paths. Makes it a much more complex type system...
The obvious answer is the main distinguishing feature of OOP: subtyping. While you learn to "program around that" deficiency - just as you learn to program in objects in OOP -, modelling hierarchies is pretty bothersome and seldom done. Haskell and Java/C# work from diametrically opposed assumptions: the former sees data as simple, dumb, and transparent, and lets you manipulate it in every way you wish, the latter package data into complex classes that only let you access it via their methods. Both of these strategies are fine as long as you stay within their paradigm, but will feel more light straitjackets if you try to go against them. I'll give you two examples: one that favours Haskell and one that favours OOP. * String In Haskell, `String = [Char]`. You can freely manipulate strings like you do lists. You can take the head, the tail, intersperse some character, take a sublist, take every odd-numbered element, filter, drop, etc. Because the String-manipulating functions are not bound to the String type, you can also write new ones and use them wherever you like. You can also make String an instance of new classes (e.g. Monoid or HasSize). This extensibility coupled with simplicity of data types does wonders for data reuse. First: had data types like `[]` been closed, they could not have been made instances of `Monad` or `Applicative` - things that came along years after Haskell's inception. Second: dumb datatypes without coupled code are very versatile. Because you only have `data [] = [] | a : [a]` and not `EnterpriseArrayList`, `MyList2`, or `BufferedList`, each outfitted with whatever bits and pieces its writer deigned to give it, the type has a certain universality to it. It's simple and it can be widely used, without worrying about implementation details. In Java, `String` doesn't expose its internal representation (it's an array) and only allows you to access the contents through whatever functions the library's writers considered. Even if you could write a new function by using only String's public interface, you can't bundle it into the `String`-class. There's no `String.tail()` and that's that. C# has extension methods to alleviate this problem, but they're just a cheap hack for a fundamental deficiency. Likewise, you can't make an existing class implement new interfaces, even if you have the necessary data. Scala "solves" this problem with the *pimp-my-library-pattern*, wherein you create a new class that implements your desired interface and then create an implicit cast between the old class and your new one. * Taxonomy The standard example of inheritance is a taxonomy like `Animal &lt;- Mammal, Reptile`, `Mammal &lt;- Cat, Dog, Cow`, `Reptile &lt;- Snake, Crocodile`, `Dog &lt;- Labrador, Chihuaha, GoldenRetriever`. `Animal` has `makeNoise` and `needsFood`, which each particular animal overrides. In OOP, this scenario can be naturally and easily implemented: make abstract classes `Animal`, `Mammal`, and `Reptile`, and concrete classes `Cat`,`Dog``Cow`, etc., and have them inherit. Because all will be a subtype of `Animal`, you can put different animals into the same list/map/set with no problems and when you call their `makeNoise`-methods, each animal will make its particular noise again. One *can* encode all this in Haskell, but it's damn inconvenient and results in a truckload of boilerplate. First, you'll need a class for the methods: class IsAnimal a where makeNoise :: a -&gt; String needsFood :: a -&gt; Food Then you'll need existential types for *every* class: `SomeAnimal`, `SomeMammal`, `SomeReptile`, `SomeDog`, etc. data SomeAnimal = forall a. IsAnimal a =&gt; SomeAnimal a data SomeDog = forall a. IsAnimal a =&gt; SomeDog a ... If you want to give additional methods to dogs, you'll need to create another class class IsAnimal a =&gt; IsDog a where tailWaggingSpeed :: a -&gt; MetersPerSecond and have the corresponding existential class be data SomeDog = forall a. IsDog a =&gt; SomeDog a Then you'll need functions for each upcasting and downcasting operation, e.g.: someDogToSomeAnimal :: SomeDog -&gt; SomeAnimal someAnimalToSomeDog :: SomeAnimal -&gt; Maybe SomeDog For every concrete class, you'll have to create a regular data type and make it an instance of IsAnimal/IsDog/IsWhatever: data Dog = Dog {dogMakeNoise :: String, dogNeedsFood :: Food} instance Dog IsAnimal where makeNoise = dogMakeNoise needsFood = dogNeedsFood You can see what a nightmare this is. Even with Template Haskell, you can't abstract it away. Consequently, there are only two places where people tend to use this poor man's inheritance: exceptions and GUI widgets. 
Excuse my beginner question, but I think I am missing something. Why do you use code hot swapping instead of simply shutting down the running process and start the new version? If you have many instances running, there is no downtime.
And that's what [spacemacs](https://github.com/syl20bnr/spacemacs) is for! :)
Changing the blockquote level is easy. You define the blockquote combinator like this: blockquote n = markdown (n+1) &lt;$&gt; BlockQuote where `markdown` is your main parser. In words: "to parse a blockquote under nesting level `n`, parse markdown under nesting level `n+1` and wrap the result in a BlockQuote constructor". I think this is the simplest approach; handling this with post-processing seems like a (more complicated) hack to me, but YMMV.
Yes, the parsers that span multiple lines would need to be aware of this. It would not make them significantly more complicated. You just have a parser `start n` which parses `n` times `&gt;`, and insert that in the correct places. If you wanted you can even wrap everything in a reader monad to hide the `n` parameter, but I don't think that's even worth it. Of course it's a bit of complexity, but what's the alternative? Doing it in postprocessing or with raw string mangling seem significantly more complicated, fragile, and hacky to me. By the way, do you want to be able to support blockquotes inside a list? (as opposed to a list inside blockquotes)
I mean of course you _can_ write low level code in Haskell. You can do pretty much anything in any language. But you could also write a text editor in J, that doesn't mean it would be a good idea.
Looks very useful. Thanks for implementing it and sharing.
So true. Especially figuring out how to run ghci in multi-cabal file sandboxed project.
Nice. How would you feel about this and filepath being used as part of the proposed strongly typed FilePath? IIRC that has an IsString instance, though, which you point out can be a mistake, but using your path package under the hood should surely lead to useful benefits for everyone, without the need to wrap and unwrap the newtypes. I suppose the slight distance allows folk to choose between complete freedom and careful construction, but complete freedom isn't always what it's cracked up to be, as our dynamically typed friends sometimes find. (The only fly in the ointment might be that Abs and Rel could potentially cause name bumps if they came with base, whereas AbsPath and RelPath are clunkier.) Thinking out loud as you post to the Internet is perhaps not always the best plan, but I'm coming to think that you'll see benefits to keeping the types here separate from the proposed FilePath. I quite like the idea of being able to use some of filepath's combinators directly with your package, with automatic wrapping and unwrapping plus normalisation without me saying so explicitly. 
Hehe. Some folk have a talent for saying what I mean to say, but in a tenth of the words. Seems I need to switch from OO English to pure FP English. 
Entry level FP here (meaning &lt; 5 years real-world experience). I worked a variety of roles in mobile and web, with PHP/Java/Groovy/Obj-C/C++/C/JS/etc. I attended FP meetups and networked my butt off, eventually landing a gig writing Clojure due to my JVM experience and Haskell knowledge. The next step will be to move onto a Haskell role in a few years, but so far I'm pretty happy slinging LISP. I have some support from the team to introduce ```core.typed``` into our codebase. You're actually a bit of an anomaly since you've had F# exposure so early on, and at *more* than just one job. Whatever you did for the first two companies, do it again! If you're in a large metro area, make sure to attend tech meetups and apply similar strategies as for any software job. Keep track of companies that you'd want to work for and that use tools you're interested in. Tailor your resume, github, blog, and linkedin accordingly.
I was under the impression that 7.10.2 was "any day now"?
Thanks, martini was nice to use though. I might try something with the new web framework written in response to that criticism or just use the stdlib.
Why do people keep rejecting '/' as a delimiter on Windows? It has worked since the DOS days. 
Isn't hasql postgresql only? These seem to be MySQL benchmarks.
&gt; No. It was considered, but it would complciate things, as whether a filepath is valid may depend [...] the default conversion functions are pure and total I would consider at least distinguishing between valid and invalid inputs on a very basic level, e.g. do not allow empty strings to be converted to filepaths. This would result in eliminating a rather large class of bugs that can result from passing those empty filepaths to deletion or similar functions and I do not think the OS exists where empty strings are valid paths (well, technically they could be considered valid relative path components but still, I think this would be worth it).
Would a Haskell data type of mutable arrays accomplish the same cache locality?
How did you compile and run the Haskell code (which optimizations, threaded or non-threaded RTS, which -N setting,...)?
Looks like theres something wrong with the martini part? *Non-2xx or 3xx responses: 164388*
&gt; Bash is an excellent interactive shell language. I disagree. It has several major flaws that let it fall short of anything that could be considered excellent. The history handling is atrocious if you run several shells on the same user and system in parallel, it is missing many of the conveniences more modern shells have introduced (e.g. proper handling of recalling multi-line statements from history, not all condensed into one line),...
I'll be putting it up with on github shortly, but I only used -O2 and -threaded. I didn't try any +RTS N option.
I don't doubt you; you sound much more experienced than myself. What alternatives would you recommend? I use Linux (Ubuntu), if that matters.
If you don't call a threaded runtime with +RTS -N (or compile it in via -with-rtsopts) you still only get one capability as far as I remember, it is effectively the worst of both worlds, you get the overhead of the multi-thread-capable RTS but without actually using multiple CPU cores.
Well, personally I have been using [zsh](http://www.zsh.org/) for years. It has its own flaws but usually it works better than bash for interactive use. One of my coworkers uses [fish](http://fishshell.com/) and likes that one a lot too. If you regularly work on remote systems via SSH I would also recommend [tmux](http://tmux.github.io/) or [screen](https://www.gnu.org/software/screen/) (but if you are going to learn something new try tmux, it is the more active project) to be able to resume sessions if you get disconnected.
Our tool implements the Itanium ABI name mangling scheme, which (I believe) is used by gcc, clang, and the Intel compiler on x86-64. I'm sure someone will correct me if I'm wrong...
You've got me wondering what OO English and FP English even are, which is inconvenient because I'm not sure there's a meaningful answer to that...
I suggest you write some "how to" documents on a blog. Pick a Haskell library that interests you and explain it with examples. This will demonstrate your ability to solve problems and communicate. Trying to teach something is an excellent way to really learn the subject! Of course, the blog exposure will be a great way to get attention of potential employers.
That's a good observation. Is that behavior, i.e. when inferred types retain or lose a synonymous annotation, documented anywhere? 
I feel like having so many string representations is a big hassle when it comes to learning Haskell, and writing real apps with it. In an app I wrote that uses vty-ui, Aeson, and Parsec, I have to deal with `[Char]`, ByteString, Lazy ByteString, Text, and Lazy Text, because all the various libraries I use require these different representations in different places. Of course we should maintain the separation between ByteStrings and Text, but having to care about Lazy vs Strict and also dealing with `[Char]` is so frustrating, and I don't think there's any way to avoid it yet.
Ok you convinced me to try. Also, I coded the transformation way and it's messing with the AST in ways that I don't like. I'll try that too and report. Thanks for the tip!
At this point in my Haskell journey, what I'm missing is mentorship. I've got this big great soup of concepts inside my head, but I need some guidance on how to arrange it into well-designed programs.
Not all servers start up or shut down in trivial amounts of time. The costs of shutting down + starting up a Sigma server multiplied by the number of times we'd need to do it in a day make it a first-order problem.
Off the cuff, perhaps if you had a type class like class ImplicitlyCastable a b where cast :: a -&gt; b and a statement you could include at the top of your file which to turn on one implicit cast, such as allow ImplicitlyCastable [Char] Text which allowed it to transform your code and insert the extra calls to cast. This still has the well-typedness of Haskell, and you're being very explicit about it so you don't get hit with performance issues that you didn't explicitly ask for.
I haven't thought it this through, but taking a clue from the `filepath` package, what if there were 2 modules `GHC.FilePath.Posix` and `GHC.FilePath.Windows` each of which would define its own `FilePath` type. There would also be a `GHC.FilePath` module which would simple reexport everything from one of the two previous modules depending on the platform. So both `GHC.FilePath.*` would be available on all systems, the difference would be what `GHC.FilePath` reexports.
Probably because the difference between files and those is somehow irrelevant.
After some more reading, it does appear that Product (*) and Coproduct (+) are what I was looking for. As far as I understand, as soon as you show that you have initial / terminal objects, they immediately become commutative monoids. So yeah, if that's the case, then this is indeed exactly what I wanted.
Yeah, I'm pretty sure they're the same under the hood (or at least used to be)-- I'd opt for both I guess... I feel like each route can be more elegant depending on the circumstances. I'm quite aware (and glad) that the official language is conservative about adopting new extensions into the fold, and that answering questions like this are crucial to that process, but I use a number of these extensions so extensively that it's really just selfishness on my part so that I wouldn't have to do so much typing :)
 (&lt;/&gt;) :: Path b Dir -&gt; Path Rel t -&gt; Path b t YES!!!
I wonder how `\\servername\sharename\folder` or `C:\folder` is represented then, and whether `\folder` constitutes a `Path Abs _` or a `Path Rel _`, and whether `"\\servername" &lt;/&gt; "\sharename" &lt;/&gt; "\folder"` typechecks at all...
How do you accept either an absolute or a relative path in your program? After all, that's the point of relative paths. If I'm in `/etc`, I (the user) should be able to say both ls /etc/nginx and ls nginx 
ghc-mod is used in an editor, see for example http://www.mew.org/~kazu/proj/ghc-mod/en/emacs.html for the use inside emacs. There is the possibility to use it via vim, sublime-text and atom, I think. There may be others. What is your setup? ps. Unless you're trying to write a library which uses it, but I don't think that's the case.
You can use hlint to check your code.
Well what do you know, it does -- when quoted. TIL. Leave off the quotes and it tends to be interpreted as delimiting a switch, which is probably why this isn't common knowledge (it's certainly why I never considered the possibility). C:\&gt;dir windows/system32 Parameter format not correct - "system32". But `dir "windows/system32"` works, so hey.
The `vector` library does strict of arrays in pure code. It's far from impossible.
i think it gets messy, for instance in the parsing of paragraph. if I'm at a depth 1, I must get out if i reduce to depth 0. so "&gt; a\nb" must produce [BlockQuote [Paragraph "a"], Paragraph "b"] so inside parseParagraph I put that every carriage return must be followed by the blockquote start for the current level, otherwise refuse the parse it. So that we exit the current paragraph, drop out of the current parse level and try at a lower level. but then "&gt; a\n" doesn't even parse a blockquote anymore... this gets more complicated, more difficult to reason about i think. well, still playing with the raw parsing with transformation: https://github.com/emmanueltouzery/projectpad/tree/notes_parse_blockquotes_as_raw and your approach: https://github.com/emmanueltouzery/projectpad/tree/blockquotes_record_depth i think i'll go back to raw, trying to make the AST less ugly.
&gt; i think it gets messy, for instance in the parsing of paragraph. if I'm at a depth 1, I must get out if i reduce to depth 0. That should be completely automatic, similar to how if you're parsing arithmetic expressions you don't have to get out of anything if a parenthesis is closed. The rules are simply defined such that it matches parenthesized expressions, and what comes after the closing parenthesis is handled by a rule further up. In this case you define the rules so that they match a piece of markdown of the same depth. When the depth is less after that, that will just be the end of that piece and whatever comes after it will be handled by a rule further up. p.s. your links go to the main page of the repo.
I'm talking about system calls, not the terrible command interpreter. 
Your parse will automatically fail because for example a `list n` will only match lines starting with `n` times `&gt;`. The `blockquote n` parser will handle going deeper and shallower. I don't see the problem that you see :(
&gt; Isn't that how it's supposed to be? Do you mean∀x: ∀y: A. x=y. Yes that would be true, but the = would be at type A' not A. Whoops, yes. Anyway, no, that's not the way it's meant to be. &gt; I thought the idea of ∀ was that the argument is irrelevant, not the result? For example if you write a function that needs some integer argument to make the proof go you do ∀n:Int ... The idea of ∀ is that the result is a mere proposition, i.e. that all the proofs are equal. Essentially, ∏ and ∑ would be about types while ∀ and ∃ would be about mere propositions.
Call me a cynic, but the OOP obsession with taxonomy usually ends up like this: class Human extends FeatherlessBiped { @Override BroadFlatNail getNail(); } In any case, there is a much simpler and straightforward model of taxonomical hierarchies than OOP-style inheritance and subtyping: an IS-A relationship is simply an [**injective function**](https://en.wikipedia.org/wiki/Injective_function) between the two related types: data Rectangle a = Rectangle { height :: a, width :: a } data Square a = Square { side :: a } toRectangle :: Square a -&gt; Rectangle a toRectangle sq = Rectangle (side sq) (side sq) -- | A partial left inverse for 'toRectangle'. Law: -- -- &gt; toSquare (toRectangle (Square x)) == Just (Square x) toSquare :: Eq a =&gt; Rectangle a -&gt; Maybe (Square a) toSquare (Rectangle h w) | h == w = Just (Square h) | otherwise = Nothing 
If this proposal were implementing system-filepath, that would be great. But unfortunately, it sounds like it is not.
thanks for your advices, i think i understand all you're saying but i think the devil is in the details, and clearly right now i can reason much more easily about my solution, whatever that means. i never meant to spend so much time on this :-(
This bot had damn well better be written in Haskell.
Well I guess what I said is a complement then. Because in my opinion, every single content re-write faithfully reproduced - and clarified - what the previous haskell.org site was trying to communicate, except for that one.
&gt; Call me a cynic &gt; `Human extends FeatherlessBiped` Ohh, you *dog.*
Well, I know that it is used by editors, though you can run it also from command prompt. I need to see some examples where it is run by command prompt over some Haskell source files.
Then why not write OCaml/SML instead of Haskell?
The HoTT book. On page 118, they write: &gt; Definition 3.7.1. We define traditional logical notation using truncation as follows, where P and Q denote mere propositions (or families thereof): &gt; T :≡ 1 &gt; ⊥ :≡ 0 &gt; P ∧ Q :≡ P × Q &gt; P ⇒ Q :≡ P → Q &gt; P ⇔ Q :≡ P = Q &gt; ¬P :≡ P → 0 &gt; P ∨ Q :≡ ||P + Q|| &gt; ∀(x : A). P(x) :≡ ∏x:A. P(x) &gt; ∃(x : A). P(x) :≡ ||∑x:A. P(x)||
Networks? Biases? What does "ffnn" stand for? Why do you need `flip`? Why `foldl` and not `foldl'`?
A survey is not the right approach currently. The problem is that there is a huge amount of confusion and misinformation about cabal and Haskell Platform going around the net. It's suffering from the Google amplification effect. So if you create a survey at this point, at best the results will be based on confusion, and at worst you will contribute even more links to wrong information. Besides cabal and Haskell Platform, there is a lot of other great new stuff out there. What we need to do is to figure out how to put together what we have into an excellent default Haskell install across all platforms.
If I'm reading the blog post right, congrats to Greg on the new job! 
&gt; is cross-compiling Haskell projects with dependencies a mess? I could be missing something, but this has been my experience. I gave up on cross compiling and have been using docker containers for compiling to arm for instance.
Ok, so I tried to use it via command line, using the syntax here http://www.mew.org/~kazu/proj/ghc-mod/en/ghc-mod.html It seems to work for me (but I'm using linux). I tried the `ghc-mod type` command on a single file: [meditans@localhost snippets]$ ghc-mod type listMonad.hs Main 9 1 9 1 12 15 "[(Int, Char)]" Is `ghc-mod --help` working for you? 
Thank you. So ∀ is the same as ∏ then? I guess I have to delve into topos theory to really understand what winterkoninkje meant.
Only when P(x) is a mere proposition.
Fantastic! PS: Stack is great. I can only do haskell in the spare 20 mins or so I have every so often - I now actually build way more stuff as those 20 mins aren't taken up trying to get cabal-install install to do what I want. 
&gt; disjoint-set data structure Why do you think this doesn't work well in Haskell? Just because the traditional description of an algorithm or data structure happens to be in terms of imperative languages doesn't mean that something analogous couldn't be done in a purely functional language as well.
Hm, I would be surprised if she was talking about more than the type vs mere proposition stuff. Pinging /u/winterkoninkje.
[Kafka](http://kafka.apache.org/) is an interesting idea somewhere in this space, though it's not so much "immutable" as append-only...
&gt; is it even possible to get the semantics right without ending up with an overengineered set of types and requiring all sorts of Maybe/IO wrapped result types? Yes. The [system-filepath](http://hackage.haskell.org) library already does it. Unfortunately, that library is deprecated, but that is basically how it should be done.
The general motivation behind topos/logos theory is to discover the sorts of categories that give rise to particular logical constructions; that is, any sort of logic or programming language can be thought of as being the [internal language](http://ncatlab.org/nlab/show/internal+logic) of some sort of category, and the study of such internal languages is what topos/logos theory is all about. Sure, there are plenty of categories that don't show up on that little diagram I made, but that just underscores the point: for these other categories, what sort of "programming language" would you make out of them? We could always try to build an implementation of all of category theory and use that, but then we'd be working in the external language of mathematics, we wouldn't be working *in* the categories of interest. Ultimately we want/need both the internal and external perspectives, as they each shed light on different things. It's just, what the OP was talking about sounds more like trying to capture the internal languages of various categories.
See my edit, I linked a gist
&gt;Haskell is lazy, you can avoid typeclasses by "homogenizing" your list-of-animals True enough, but if you have multiple functions, you end up shuffling reified dictionaries around. This method is also impossible to use when you can't abstract away the type, e.g. class IsAnimal a =&gt; CanGrow a where nextGrowthStage :: a -&gt; a I've read that blog post a couple of times over the years and while the author is welcome to his opinion, it's just an opinion, not gospel truth. He shows that existential types can be eliminated in some cases - but they cannot be eliminated in all cases. More importantly, he gives no motivation for why they *should* be eliminated. 
The naturality of such hierarchies comes down to subjective taste. I agree that you don't *need* them, but many would argue that they are natural and easy solutions to certain problems. Some things, like taxonomies, algebraic structures, or exceptions really do form hierarchies. In my experience, people seldom try to implement a full-fledged OO hierarchy in Haskell - I'm with you on that. However, they often do re-implement parts of it. For example, when you don't need heterogeneous collections, but just common functionality + interface subtyping, you use typeclasses. If you only need heterogeneous collections, you use existential types. Let's take algebraic structures with one operation: class Algebraic a where op :: a -&gt; a -&gt; a class Algebraic a =&gt; Neutral a where one :: a class Algebraic a =&gt; Associative a class Algebraic a =&gt; Invertible a where inv :: a -&gt; a class Algebraic a =&gt; Commutative a class (Neutral a, Associative a) =&gt; Monoid a class (Monoid a, Invertible a) =&gt; Group a This is a hierarchy in all but name. You have attached to values (`op`, `one`, `inv`) and (multiple) inheritance. You don't have existential types because it's pretty unlikely that you'll need to store a bunch of them in the same collection. In fact, you'll seldom even need to store them as objects at all, so it's easier to attach them to values in their domain via the type class's hidden dictionary. Exceptions are a more complicated example. Unlike algebraic structures, you do need them to be explicit values, and because you often want to catch entire categories of exceptions, you do need existential types. In fact, I adapted my example from [Control.Exception](https://hackage.haskell.org/package/base-4.8.0.0/docs/Control-Exception.html). The only thing you don't need there is the type classes, because exceptions generally don't have specific methods attached to them. So yes: you seldom need the whole shebang I wrote there, but I think that it would be erroneous to conclude that it's therefore useless. Sometimes, you only need type classes (as is most often the case in Haskell). Sometimes, you only need the existentials. Sometimes, you only need up/downcasting à la someDogToSomeAnimal/someAnimalToSomeDog. It's rarely done because it's so cumbersome, but sometimes, it'd be really nice to have all three.
There's a simpler API that doesn't require type class boiler-plate: {-# LANGUAGE GeneralizedNewtypeDeriving #-} import Control.Applicative import Control.Monad.Trans.Maybe import Control.Monad.IO.Class import System.Environment newtype Env a = Env { unEnv :: MaybeT IO a } deriving (Functor, Applicative, Monad, MonadIO, Alternative, MonadPlus) getEnv :: Env a -&gt; IO (Maybe a) getEnv env = runMaybeT (unEnv env) env :: String -&gt; Env a env key = Env (MaybeT (lookupEnv key)) Then the user would just write: connectInfo :: Env ConnectInfo connectInfo = ConnectInfo &lt;$&gt; env "PG_HOST" &lt;*&gt; env "PG_PORT" &lt;*&gt; env "PG_USER" &lt;*&gt; env "PG_PASS" &lt;*&gt; env "PG_DB" You can also embed arbitrary `IO` actions since `Env` implements `MonadIO`.
Aha! So, I think you've possibly found a bug, but I also have a workaround for you. Ish. If it lines up with what you need. In `Control.Monad.Trans.Error`, there is (Functor m, Monad m, Error e) =&gt; Alternative (ErrorT e m) and in `Control.Monad.Trans.Except`, there is (Functor m, Monad m, Monoid e) =&gt; Alternative (ExceptT e m) So you'll need a `Monoid` instance for `Foo`. If you can live with using `ExceptT` instead of `Either`, you can use import Control.Applicative import Control.Monad.Trans.Except import Data.Functor.Identity import Data.Monoid data Foo = Foo instance Monoid Foo where mempty = Foo mappend _ _ = Foo newtype Bar a = Bar { unBar :: ExceptT Foo Identity a } deriving (Functor, Applicative, Monad, Alternative) and things might work out. At the very least, they should compile.
Simple example: imagine that your shell was like a `ghci` REPL, and if you wrote: &gt; f = (+ 1) ... that created an immutable file named `f` representing the function `(+ 1)`. Then if you wrote: &gt; x = 3 ... that creates an immutable file named `x` whose contents were the number 3. Then you could write: &gt; f x 4 ... and it would apply the function stored in the file named `f` to the value stored in the file named `x`
I used "OOP" in the sense of "Java-style OOP". &gt;Except that this isn't intrinsic to OOP, and was arguably a mistake to begin with No argument from me there. Look: I'm not some sort of Java-fan. As a matter of fact, I'm the first to say that inheritance is almost always useless and not worth it as a trade-off against type inference, and that the same goes for the endemic plague of in-place mutation of objects that leads to the [circle-ellipse problem](https://en.wikipedia.org/wiki/Circle-ellipse_problem) and makes a mockery of the idea of subtypes. It's just that Java-style subtyping does have *some* uses.
I understand and like the reasoning behind tracking a relative vs an absolute path, but the file vs directory distinction is, if you'll pardon me stating this baldly, writing cheques it can't cash. As far as I can infer, the only reason for this to exist is to prevent gluing a path component onto the end of a name that you think should not be traversable. Why do I think you've gone down a bad route with this part of the design? I can think of some unfortunate practical implications that make me very uncomfortable. * Now your type system misleads you into thinking that you statically know things about the association between names and the kinds of entities in the filesystem. In truth, it is possible and not even uncommon for a named thing to change type, perhaps even under your own control. Now I'm sure *you* as the author of this package are clever enough to know this, but faux invariants that can change behind the backs of the less experienced users of your package don't belong in a type system. I have seen innumerable bugs based on this category of faulty reasoning, so this isn't a theoretical hand-wavy concern. * Consider an implication of the inevitable desire to plumb this out to `IO`. An implementation of `getDirectoryContents` that's "type correct" must return a heterogeneously typed list, or a pair of lists typed as (things that are directories, things that are files). (And what happens with exotic things like symlinks or named pipes? At the very least, the choice of `File`/`Dir` naming leads us quickly to dissatisfaction here.) * Another implication of `IO` support will be that `getDirectoryContents` will have to `lstat` every entry before returning a result in order to categorise names correctly. This is obviously not desirable for performance reasons. The bar for making statically enforced promises about how the world works is a high one, and this aspect of the `path` design does not, in my estimation, meet that bar. So 50% applause, 50% disapproval from me. Not bad :-) (Guess who's spent a lot of his career messing with filesystems...)
&gt; using docker containers for compiling to arm Please, tell me more...
Does this end up, in the long run, as a way to specify the entirety of `stack.yml` on the command-line? I imagine if you start to use Haskell scripts often, you'll end up pulling in more packages (depending on specific versions), using different compiler versions, etc. Randomly throwing in ideas - maybe a theoretical `stack runghc` should look in some sort of special comment block to get its dependency information? Maybe `stack.yml` could be inlined into a haddock block attached to the module declaration? This might facilitate more easy one-file-script hand-arounds while avoiding the inevitable 'doesn't work on my machine' 'oh you need to use 7.10.5' conversations. EDIT: and then add `#!/usr/bin/stack` and Bob's your uncle ;)
There's definitely virtue in the type class approach if you want to derive generic readers for a data type. I was looking through the source code and it was closer to what I wanted than I thought. The part that was confusing me was the type of `fromEnv`: fromEnv :: Env -&gt; Parser a Doesn't `Parser` internally have a `ReaderT Env` to access the environment?
You could use StateT if you wanted, but what you have right now isn't too complicated.
Actually, I still write throwaway scripts in Haskell. The main reason is that these scripts often take a long time to run and I don't want to repeatedly wait 10 minutes to find out that there's an error in some downstream stage of my script.
Both this and OP's idea sound quite reasonable. Perhaps we can have a design discussion on the stack mailing list about what people want and then implement it? Seems like we can make something good really quickly.
Yea, that's a redundancy and should probably be removed. `instance FromEnv PGConfig where fromEnv env = do env2 &lt;- ask` There's really no reason to have a `ReaderT` if we're passing `Env` in already. The core type can probably be simplified to just `newtype Parser a = Parser { runParser :: Env -&gt; ExceptT String IO a }`, since we're just loading the environment once from `getEnvironment :: IO [(String, String)]`, unless we'd rather enforce that the user use the `ask` function. That seems like a bad idea though. 
[Irmin](https://github.com/mirage/irmin)?
How would I go about doing that? It seems like a good exercise to understand the StateT monad.
This would be awesome, thanks so much for your work.
It's not a bygone era if there is no modern replacement.
I just use this: https://github.com/sseefried/docker-build-ghc-android
It would be great, really!
While unfortunately I tend to agree with [Bryan's opinion](https://www.reddit.com/r/haskell/comments/3bazgv/the_path_package/csl4a0m), it's still important that we don't settle with the way things currently are, when the suboptimal "filepath" package is the default approach. IMO we need to keep digging in search for the best practical solution and this package and ideas behind it is a good investment into that search, but I can't consider it the solution. BTW, it looks like a part of this library could be abstracted over using [the "refined" package](http://hackage.haskell.org/package/refined), that is including the Template Haskell functionality.
You are correct, the Path library doesn't support this case very well (where you want a single value to represent either an absolute file or a relative file). The way I'd handle this is generally try and parse as rel file, if it works, then resolve the relative file against whatever directory it is relative to, so that ultimately I end up with an absolute file either way.
Agreed, it is annoying. Sorry, I don't know of a better way.
Hm, interesting. I wonder how much work it would be to create a similar one for Raspberry Pi. Guess I need to learn Docker...
I find purity to be the most important thing.
The summary is "algebraic datatype database". It would look roughly like the heap of a functional programming language. See also: http://www.reddit.com/r/haskell/comments/35x9e4/the_future_of_systemfilepath_and_systemfileio/cr92ijp
Alright, I've implemented support for the `--package` option on master. Consider you have the following in `foo.hs`: #!/usr/bin/env stack --resolver lts-2.9 --install-ghc runghc --package turtle {-# LANGUAGE OverloadedStrings #-} import Turtle main :: IO () main = echo "Hello World!" Running `stack --resolver lts-2.9 --install-ghc runghc --package turtle foo.hs` should have the desired effect (yay!). However, I really liked the idea of having the shebang just work so that this file could be made executable and run with `./foo.hs`. Unfortunately, on Unix systems, this ends up with: &gt; /usr/bin/env: stack --resolver lts-2.9 --install-ghc runghc --package turtle: No such file or directory For more gory details, see [Stack Exchange](http://unix.stackexchange.com/questions/63979/shebang-line-with-usr-bin-env-command-argument-fails-on-linux). One possibility to solve this is to provide an improved `env` utility that does proper argument parsing before passing off to `stack`. Another (fairly hacky) thing is to embed that functionality somehow in `stack` itself, such that it has special handling for the case where it's called with file name... somehow. There are many different things we could do here, but few that would be convenient, minimize system installed code, and be easily cross-platform. If it's not obvious: thoughts are very welcome on how best to move forward here. __EDIT__ One more idea which I just tried implementing: have a separate `runstack` executable that takes a single filename as an argument, parses out command line arguments from stack from a comment line after the shebang, and reruns `stack` with those arguments. If that was too abstract a rambling, check out [this Gist with the idea implemented](https://gist.github.com/snoyberg/b28f0b7dc115dd190b69) (though the options parsing would need to be better for real usage).
Any update on how this affects performance?
I've seen your last idea in many context, and it usually works pretty well (https://github.com/madjar/manuscript/, https://github.com/madjar/nixbang, http://nixos.org/nix/manual/#sec-nix-shell).
Yes, you're right. "runstack" will be good and enough solution in my opinion.
&gt; An implementation of getDirectoryContents that's "type correct" must return a heterogeneously typed list, or a pair of lists typed as (things that are directories, things that are files). To be fair, you almost always want to process them differently: recurse (or not) into directories and read/write/unlink files. &gt; And what happens with exotic things like symlinks or named pipes? Named pipes and file symlinks are files: you read/write/unlink them, not recurse into them. Directory symlinks are inconvenient to deal with; many unix commands have special flags to recurse into them or not. So they should be categorized according to such a flag.
The distinction between link and non-link is often am important security and termination concern. 
In my experience the solver is not very good at figuring out that reinstalling some packages that have outdated versions in the global package database in a sandbox is the better option. This is particularly true if we are talking about a dependency of something where we do have a usable version in the global package database and cabal's extreme aversion to reinstalls is invoked. 
I think this is very important to note. IMO the proposed `runstack` script should have some sort of 'hey I'm going to go grab X many MBs of GHC now kay? yes/no'.
Well, the shebang line is posix specific I think. 
Another component is the fact that even when you have Haskell setup, a script that worked yesterday might just fail today because of a different or missing package, which has been a demotivator in using my Haskell scripts in the past. A script I wrote months ago is a crapshoot whether it'll work today. Being able to specify "this exact LTS release" means at least it can be consistent from run to run.
You're overthinking this.
I disagree, I think that'll lead to terrible ergonomics.
Does `#!` happen in Windows? I guess under msys...? I confess I have hardly done any development in Windows since finally starting to dual-boot on both my machines :p.
Yes, but under Windows there is already MSys and CygWin. Jenkins support it by default, too. So such scripting will be useful in Jenkins, too.
A similar argument was brought up internally when I presented it to the team, which was initially skeptical. (Concerns were that having a "path" meant that a thing existed on the filesystem and could be deleted or moved at any time.) My response was and is that it's not supposed to be modelling the state of the current world, that's definitely an unrealistic goal (nor really worth pursuing IMO), and now that other members of the team have been using it in practice, they've had no trouble shaking this notion. We already have such a directory listing function that returns a tuple, the overhead in exchange for type information is a fair trade-off. How often do you want to look in a directory but don't care what type the things are? To count them or something? Produce a checksum of the filenames? In which case you don't even need the path strings to enter your program's data structures, there's nothing semantic about them (and therefore wouldn't want a Path type for them). I'm not saying it never happens, but the usual case is pretty worth it. The distinction isn't just "don't accidentally append `file &lt;/&gt; dir` or `file &lt;/&gt; file`" (which is of course handy) but the overall guiding of program writing. Symlinks and pipes are a concept in the filesystem, but not in path formatting; there's nothing structurally distinguishing about paths that point to them that differs to regular files, so there's nothing worth modelling differently there. Maybe it might mislead some newbies into thinking a Path is a resource rather than an address, but I think after using it that becomes obviously false.
In main, you can similarly turn: -- This buckets &lt;- bucketKeys mapM_ handleBucket buckets -- Into This import Control.Monad ((=&lt;&lt;)) putStrLn "Start..." mapM_ handleBucket =&lt;&lt; bucketKeys putStrLn "Done" Again, it's up to you which you find clearer. I just don't like variables that are only used on the next line unless they add commentedness or clarity. Or split a line up. In this case it's not shorter or clearer, in my opinion. In a longer method it would, though, cause me to look to see if buckets was being used elsewhere in the code.
I don't know if this is just hear-say but with an executable like this I think you can get some extra-aggressive optimizations from only exporting the main function from the module. Since that's the only one an executable needs, declaring that that's the only export lets the compiler know none of the rest of the functions in the file need to be kept intact so long as they serve the same purpose to main, allowing inlining etc.
getRequestBody: Again, we have some one-use pure variables, but they're a little more ok here. At least the ones after the getAuth depend on its output, so they can't trivially be turned into where. But, that's not to say I don't have opinions I again want to remind you you're free to ignore! Like before, the only IO in this function is getAuth. I like to make that clear. All of the rest of the function is just transforming the result of that call, without using any inputs. This makes it easy to instead use our friend fmap (in the form of (&lt;$&gt;)). getRequestBody bucket uuid = ourDecode &lt;$&gt; (getAuth defaults $ messageUrl bucket uuid) -- I don't remember if the () are needed. where ourDecode = fromMaybe emptyObject . decode . BL.fromStrict . encodeUtf8 . getBody For me this says "We compute the messageURL, then get the auth. From that we get the body, encode it, make it a BL, decode that, and then if that failed make it empty. Again, if that's not as friendly for you, then use your thing.
Yeah, I think you will eventually need to support embedding Stack information as a comment within the source file because otherwise you might hit limits on command line lengths. However, the command line is still a good approach for most use cases. Also, thanks a lot for doing this!
getUUIDs is very similar to this method. If you are trying my opinions, I'll leave this as an exercise to the reader.
Your construction of getAuth, to provide contrast, I do agree with. Turning that into some sort of mappy applicative thing seems like it'd make it far less clear what's going on.
Yes. Oleg Kiselyov's [write up](http://okmij.org/ftp/Haskell/index.html#memo-off) about avoiding sharing is such an example. Here's a telling excerpt (emphasis added): &gt; The fix that was supposed to help iterative deepening has made it worse. &gt; Such an unexpected result was quite a puzzle. It seems GHC is just too smart. Apparently it notices that a thunk `(\() -&gt; e)` can only be applied to the same argument. Therefore, the first time the thunk is forced by applying it to `()`, the result can justifiably be memoized: the next time around the thunk will be applied to the same `()`, and hence, will give the same result anyway. &gt; **The new fix is to deliberately confuse GHC. We obfuscate the tree-construction** operations `(&gt;&gt;=)` and mplus with auxiliary functions `app` and `app1`.
By default it will follow stack's normal rules of not downloading GHC without the user running `stack setup`. But for automated tasks, we don't want to force user interaction, so I'd imagine the `--install-ghc` flag will more often than not end up getting used in these scripts.
i guess it's more a matter of taste. It is probably important though that you see that you're not the only one with that idea. note that as you can see in the answers, lens changes the API to achieve this regularly, therefore there is some disruption, and it may also make it clear that only a small minority is interested in this.
IO results in rust are already sort of an `Either IOError T`. ~~Without a convenient bind operator, monadic error handling becomes somewhat more awkward to use.~~ EDIT: Actually, the stdlib already implements a bind equivalent (thanks, /u/desiringmachines).
&gt; Now the question is do [I] write cost as a getter My recommendation is to never write a `Getter` directly. It is universally just slightly less useful than a function. Just a normal function into a `Getter` with `to` when you need to drop it in the middle of a lens chain in some particular concrete situation. If you wrote cost i = _quantity i * _price i then foo^.to cost or cost foo would be both be perfectly fine to use. You can avoid `(^.quantity)` with `view quantity` as well: cost i = view quantity i * view price i This may appeal more to your sensibilities, and lets you avoid needing access to the _'d record accessors. Finally, you can worry a lot less about the 'hidden field accessors' being deep private details you shouldn't access directly here in Haskell -- mostly just because with the lens laws being what they are they don't have secret side-effects elsewhere in your structure that you aren't getting with the _foo versions. No lens police will come arrest you for "using lens incorrectly" or "not using it enough." There are lots of people in the Haskell community who see no real value to them at all and still manage to get code written. =) It is just a tool, sometimes one that is very useful, but sometimes the direct way is cleaner.
I've seen your recommendation about using `to` and that's what I did in my code. Then I needed `Item` to be an instance of `Semigroup`. Obviously it doesn't work, so I had to swap `cost` and `price` in the data definition. data Item = Item { quantity :: Double, cost :: Double} price i = i^.cost / i^.quantity Now, all the legacy code uses `price` as a `Getter` and `cost` as a function (as well as `_price`. I have to change `to cost` to `cost` but I can get away of modifying `price` and `_price` by doing _price i = _cost i * _quantity i price = to price If I had done that initially with `cost = to _cost ..` I would have pretty much nothing to change in my code to refactor `Item`, hence my question. About the "lens police", you've created a monster , and the same way Haskell creators didn't know lens would be even possible, a "lens police" might appears sooner that you think and you'll probably even be the first arrested ;-) 
I was actually wondering the same thing as /u/Tekmo (but in this case I am the newish the programmer), but not finding anything, I just downloaded it and started using it. I have to say that I am blown away: I typed in `stack build` after cloning a git repo of a haskell project and stack guided me the whole way. First it told me I needed to run `stack init`, and then, guessing again, I tried `build` after that. At that point, stack recommended I run `stack setup` because of a wrong GHC version, so I did that too. Finally, I ran `stack build` and it ran through to completion on a project that I had struggled mightily to build on my own in the past. I am always hesitant with new tools, but my first impression has been really good.
I like the "return" name very much. I think it's just great to see how often monadic code in Haskell looks exactly like it's equivalent in a nonfunctional language. 
&gt; that mathematicians call plus and nothing more I think you mean &gt; that maths teachers call plus and nothing more because from a mathematician's point of view, the structure is important and the symbol is not. There's an isomorphism between the positive reals under multiplication and the reals under addition. There's also an isomorphism between the upper triangular 2x2 matrices over a ring under multiplication and the addition in the ring. So you see its mathematically impossible to define some essential mathematical plusness that excludes multiplication, since there are plenty of examples of isomorphisms between additive and multiplicative structures. The things we call + are very diverse mathematically: the positive integers have no identity element, the reals are complete and ordered, whereas the Gaussian integers are unorderable, and integers modulo 12 are finite and have zero divisors. To try to distill a formal mathematical notion of addition that matches exactly the use of + is to try to formalise social convention and historical accident. 
What is the advantage to this over a transformation to a state monad? Can you use Cofree to interpret other effects?
Thanks! I just discovered the Free monad on this sub reddit a couple weeks ago and I've really enjoyed this series as well as the material by Gabriel Gonzalez and Aaron Levin. In an attempt to get handle for this style of programming I'm trying to write a little text adventure game. As useful as all the blog posts have been, I find I'm flipping between about 6 articles while I work on this project. Does anyone have a good example of a full project that effectively uses this approach?
&gt; the mutability is necessary for the performance Many algorithms were classically described in a way that uses destructive update in what seems to be an essential way. The main reason is that until recently, most authors never really considered the purely functional setting. As far as I know, it is still an open question whether mutability is ever truly necessary for performance. In the case of the disjoint-set algorithm, there is a hidden factor of `O(log n)` which is ignored: the algorithm relies on looking up an object in a random-access table of arbitrary size. But that operation has a theoretical optimal complexity of `O(log n)`. Modern computers have built in parallel hardware ("RAM") which make the operation appear to be `O(1)`, but only if the size of the data fits within a fixed maximum size, so using that for computing asymtotics seems dubious. In any case, if you allow that for the imperative version, then you must certainly also allow the use of `STArray` and the like in pure Haskell code. There are implementations on Hackage of the disjoint set algorithm that include the `O(log n)` lookups explicitly, and others that use imperative-style mutable arrays. More generally, it is often possible to find an analogous (but different) purely functional algorithm with the same asymtotics as an imperative one. For example, see the derivation in [Brodal and Osaki](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.48.973) of a purely functional equivalent of Brodal's imperative priority queue algorithm with optimal asymtotics. Those are the same asymptotics as for a fibonacci queue which uses destructive updates in what appears to be an essential way.
The central feature of `system-filepath` is a `FilePath` type which implements the application-level semantic conventions for paths on each of the major platforms. So, for example, `FilePath`s constructed from two unicode strings with the same normalization are equal on Mac OS X and unequal on Windows.
Are there different path semantics there that could not be supported with an API like the one I described? If so - what else would be needed?
&gt; ghc takes up a nontrivial amount of space Maybe a --temp-install to address this concern?
...or timestamps and time differences?
Rust's monadic types implement methods `and_then()` and `and()` which are analogous to the `&gt;&gt;=` and `&gt;&gt;` operators; what do you mean when you say it isn't convenient?
the derived Eq instances, and most of the manual instances I write, satisfy being an equivalence relation. the documentation is lawless, but we should just fix the docs. (unless it's left unspecified in the haskell report or something? but then it shouldn't be called Eq). that's (symmetry in particular) what makes Eq better than when I write equivalences in Java. you can't have some class with an Eq method that always returns true or false, when it's on the left.
Haskell has ADTs, where Clojure would need untyped dicts. I tend to have a sweet-tooth for syntax, and I don't see a need for Map literals. band Set syntax exists as OverloadedLists (except for the slight loss of inference).
I like syntactic consistency (the where clause introduces declarations), but here I agree with "module Foo".
I'm not quite sure what you mean by "transformation to a state monad" - could you expand on that? You _could_ write `add`, `clear` and `total` in terms of a state monad. In that case you don't get the DSL / interpreter separation, which a) has benefits in certain use cases and b) is the topic of this series. You can use Cofree for a lot more than what I'm covering in this series - for these posts I'm just focusing on using them as an interpreter for a DSL. I'll be covering the use of IO in the interpreters later on. I've only just woken up and am pre-coffee - hopefully that goes part of the way to answering your questions.
Not a problem. I think it'd be a GHC bug, just because of the non-determinism between builds.
Forget the [cloud-to-butt extension](https://chrome.google.com/webstore/detail/cloud-to-butt-plus/apmlngnhgbnjpajelfkmabhkfapgnoai?hl=en), I want a monad-to-burrito extension!
I've built something like this a few times but never really maintained it (https://github.com/tel/env-parser). It's a simple, neat idea and I'm giving a talk at Lambda Jam CHI using this as a motivating example. A neat trick with these is to completely eliminate the monadic component to force a "configuration parsing" phase to occur prior to launching the app. What's neat about this is that it automatically gives you (1) notification of *all* missing environment variables and not just the first one and (2) the ability to autogenerate an always-accurate "help" displaying all needed environment variables. Something like: data Collect e a = Errs [e] | Val a deriving Functor instance Applicative (Collect e) where pure = Val Val f &lt;*&gt; Val x = Val (f x) Errs es1 &lt;*&gt; Errs es2 = Errs (es1 ++ es2) Errs es1 &lt;*&gt; _ = Errs es1 _ &lt;*&gt; Errs es2 = Errs es2 newtype Env a = Env { unEnv :: IO (Collect String a) } deriving Functor env :: String -&gt; Env String env name = Env $ do mayVal &lt;- System.Environment.lookupEnv name return (maybe (Errs [name]) Val mayVal) instance Applicative Env where pure a = pure (pure a) Env l &lt;*&gt; Env r = Env (liftA2 (&lt;*&gt;) l r) gets you the "show all failures" property. Then newtype Env a = Env { unEnv :: [String] } deriving Functor env name = Env [name] instance Applicative Env where pure = Env [] Env l &lt;*&gt; Env r = Env (l ++ r) gets you the "all names needed to generate a help file" property. Then finally you glue them together data Env a = Env { runEnv :: IO (Collect String a), envHelp :: [String] } deriving Functor
Now I'm unable to reproduce "No instance for (Control.Monad.Trans.Error.Error Foo)" I made an empty cabal executable project with Main.hs like [this](https://gist.github.com/anonymous/72e779d0d31e7ab697b7). It's in an empty sandbox, and the only global packages I have installed are those that come with GHC 7.10.1 `cabal build` says &gt; No instance for (Alternative (Either Foo)) If you're able to reproduce it, feel free to add the bug to tracker. Or give me a hint and I'll try and reproduce and report.
The tendency toward "operator soup". People always seem compelled to come up with clever operators to support their use case but all you end up with is syntactic noise that is impossible for a newcomer to your library to understand. Try mixing **conduit**, **hunit**, and **lens**. Ugh. Is it so bad to just write `fmap function structure`? I have a hunch a lot of this would go away if `(+)` (or at least `(++)`) evolved to be `mappend` or similar.
Thanks for clarifying (and that's a very cool post, by the way). This series of posts is half about using `Free` for DSLs. The main idea running through this series of posts is that `Free` gives us some nice advantages, like the separation of the usage DSL from the interpretation of the DSL, which lets us do things like build libraries on top of the DSL without having to specify which interpreter they'll be used with or using one interpreter for testing and the other for running in production. I'm guessing from you comment and from the post that you linked to that none of that is news to you :) The other half of the series is about using Cofree to get those same benefits for the interpreter (and how far we can push that). This post is where some of the additional payoff comes into play, because now - modulo improving some of the typeclass machinery - we can develop several DSL / interpreter pairs and mix them together as we need to. The series is more about what I think is a neat set of implementation techniques rather than a deep theory dive. Hopefully in the next few posts I'll cover some of the surrounding techniques that I'm excited about / think have a bit of potential.
I'm grateful for every effort to make installation of Haskell easier. But I think at any given point we should strive to recommend what we (the community) use ourselves. So I would very much welcome a survey to find out what that is. And if it were me I would keep questions about problems or possible improvements of any one solution (Haskell Platform, stack, etc.) out of that survey.
Having a working solution for this would be awesome. Some thoughts: * The dependencies could be put into pragmas to clarify that they are not normal discardable comments. * At zalora we've toyed around with something similar (https://github.com/soenkehahn/runstaskell). The main difference in our approach was that scripts cannot have arbitrary dependencies, but can only depend on a defined subset of stackage, which would be installed by default. That has the obvious disadvantage of limiting the possible imports but has the advantage that script execution wouldn't prompt download and installation of packages. So you don't need a working network connection to execute scripts and you don't run into the situation that scripts can take much longer to execute than normal because they have to install a bunch of dependencies first. (We've abandoned `runstaskell` so I'm excited to see something like this in `stack`.)
Thanks. I look forward to the rest of the series. If you look at it at the right level, these methods are actually pretty related. Consider these simple functors: data F a = F X (Y -&gt; a) data CoF a = CoF { getCoF :: X -&gt; (Y,a) } Then a coalgebra on `CoF` is very similar to a natural transformation from `F` to `State`. S -&gt; CoF S == S -&gt; X -&gt; (Y,S) (forall i. F i -&gt; State S i) == X -&gt; (Y -&gt; i) -&gt; S -&gt; (i,S) That's why I'm curious whether `Cofree` can be used for other effects, like exceptions or nondeterminism.
[It sounds like although Tabula's technology was promising, and received strong initial investment, the business did not get enough traction](http://www.eetimes.com/document.asp?doc_id=1325499). You may also be interested in [Adam Megacz's Generalized Arrows](http://www.megacz.com/berkeley/garrows/), which builds up type classes similar to those used in the talk.
Comments in this [EETimes article](http://www.eetimes.com/document.asp?doc_id=1325499) suggest that the increased power required for designs like these do not outweigh the other benefits.
What happens is that to derive `Alternative Bar` we need in instance for `Alternative (Either Foo)`. There is an instance for this in `Control.Monad.Trans.Error`. So the compiler uses this. This instance then requires an instance for `Control.Monad.Trans.Error.Error Foo`. This would all be correct if the instance was in scope. However `Control.Monad.Trans.Error` is not transitively imported. This therefore seems like a GHC error. The correct error is `No instance for (Alternative (Either Foo)) ...`
Btw, minor thing, but it is really annoying that in result type of `random` the accumulator is the second element of the tuple, while in `mapAccumL`, it is the first element... These should use the same convention (I would choose the `mapAccumL` one).
Right now you have to copy/paste a lot of code to build your own GHCi front-end, we could build another layer to make this easier. In fact we've already done this for our internal GHCi variant, we just need to integrate those changes with the GHC build. The overall picture is that the GHC API is complex, because it's serving a lot of use cases - batch compiling, interactive evaluation (with the debugger), and tools like Haddock. Building narrower and simpler APIs on top of the GHC API that make specific use cases simpler is definitely a good idea. Should we do anything in GHC to make that easier? I'm open to suggestions.
For Mac it doesn't.
I quite like that idea. I'll think, but it might just be "better" than what we have suggested already, and fix the issues of /u/yitz. We did start going in those directions with the data WindowsPath type, but going fully down that route, and in directions already travelled quite successfully by filepath, sounds like a good plan.
I wrote a while ago a description of the implementation of the "7 wonders" game over XMPP. The game rules were described with an program from the `operational` package, which is isomorphic to the free monad where the functor is `Coyoneda Instr`. You basically have rules expressed in a given monad, and then several interpreters that can run them. Descriptive text is [here] (http://lpuppet.banquise.net/blog/2014/05/16/7-startups-part-3-an-interpreter-for-the-rules/) (jump to *"Implementing the GameMonad"*), interpreter building function is [here](https://github.com/bartavelle/7startups/blob/master/Startups/Interpreter.hs) and game rules are [here](https://github.com/bartavelle/7startups/blob/master/Startups/Game.hs).
&gt; As for your lemma, I have a hard time seeing how it would be a theorem (though I'm willing to be convinced). Supposing the premise isn't a contradiction, how do you propose to construct an inhabitant of A? Just because we've proven that some x:A must be related to every A, that doesn't let us put our hands on an actual concrete A. Ah, yes, exactly as I suspected. Going back to when you first mentioned the distinction: &gt; The HoTT-style approach manages to distinguish ∑ from "∃", which is something at least In HoTT, ∃ is simply the truncated version of ∑, but [truncation doesn't actually erase the information](http://homotopytypetheory.org/2013/10/28/the-truncation-map-_-%E2%84%95-%E2%80%96%E2%84%95%E2%80%96-is-nearly-invertible/), so the quantifier you're talking about is not the HoTT mere existence quantifier.
Fair enough, my comment was prompted by seeing the link to your library (which I admit I haven't looked at closely) but mostly directed at other references to burritos I've seen recently. What you're doing sounds great. 
Maybe like [ipfs](https://github.com/ipfs/ipfs).
[Rasterific](http://hackage.haskell.org/package/Rasterific) use a [free monad](https://github.com/Twinside/Rasterific/blob/master/src/Graphics/Rasterific/Command.hs) to describe the scene, and use two "interpreters", the [rasterizer](https://github.com/Twinside/Rasterific/blob/master/src/Graphics/Rasterific.hs#L494) and the [PDF output](https://github.com/Twinside/Rasterific/blob/master/src/Graphics/Rasterific/MicroPdf.hs)
Nice! Definitely put it on Github.
I first thought it was a joke: &gt; Haskell on a truly vast scale \*clicks\* Not Found The link gives 404.
I agree that existential types are an awkward way to model this in Haskell, but I don't think your example is complete until you discuss the [popular alternative](https://lukepalmer.wordpress.com/2010/01/24/haskell-antipattern-existential-typeclass/). This is the money quote from the linked article: &gt; See how this is going? Classes become data types, instances become functions. I’m just manually “expanding” what GHC does for you. Of course, the expansion is smaller than the original. While I suspect you may be able to construct an example that is hard to model well in Haskell, I'm not sure this is it.
What the hell? The site links to this: http://www.geopense.net/distrib/FP.pdf 0_o
I've used data-filepath for a few projects with a lot of path munging, and like you, I found that it's pretty hard to go back.
GHC will not warn about unused identifiers beginning with an underscore. For regular identifiers it will warn if they are neither used locally nor exported. This is an advantage over the suggestion of two regular identifiers with underscore suggested above if you plan to only use the lenses.
I never did anything complicated with it and just used GHC.
Clever, using -fdefer-type-errors like that. I could actually use this in some packages to confirm that some things are impossible to express in the type-system.
I can come up with some reasons why they make a poor analogy but by nature of analogy they may be wildly different than yours. Since my understanding of monads is probably middling, I'd love for you to explain why they are bad analogies from your POV.
 where doubleEveryOther' acc lst2 You have to provide a definition for this. Right now, Haskell sees that you're defining a term `doubleEveryOther' acc lst2'` and starts looking for an `=`.
I've never seen the hackage badge on a readme like that, cute! We should have those for LTS Haskell, too. [edit] issue created. https://github.com/fpco/stackage-server/issues/109
You can use `(x:y:xs)` as the pattern I think you're asking for. You use mostly the same syntax for patterns as you do for constructing values from their data constructors. There's also an error on line 3. The guard needs to be a Bool. You could do ` | null lst = []` to check that the list is empty, or the more idiomatic: doubleEveryOther [] = [] doubleEveryOther lst | len lst `mod` 2 ... I don't think the first issue with cause a parse error but a type error; I thought the second might, but it seems like it's also a type error. The only thing that I can see that might cause the parser error is the where clause on line 6 like /u/timmy_tofu said. There's also another typo on line 7 (doubleEver`Y`Other). There are a few other type errors that I can see if you want to know what they are, but I dunno if you'd prefer to work them out yourself after fixing the parse error. It could just be spaces vs. tabs. When I was using SublimeText I found it sometimes inconsistently replaced sequences of spaces with tabs when auto-indenting.
I've wanted this on a couple occasions. Nice!
Take a look at [shields.io](http://shields.io/)!
Yep, an (unfortunate) consequence is that any bottoms (except for those caused by deferred type errors) will be forced and propagated up through `shouldNotTypecheck`, failing your test. In previous versions, before I started forcing the test expression, I only evaluated to Weak Head Normal Form. Maybe another function (`shouldNotTypecheckShallow`?) that only evaluates to WHNF would be useful?
Typo: doubleEverYOther vs doubleEveryOther `doubleEverYOther' []` implies that the argument is a list. `doubleEveryOther' (x,y,xs) ` implies that the argument is a 3 element tuple. The argument type must agree. You probably intended to use the cons operator (`:`) instead of the tuple constructor (`,`) : `doubleEveryOther' (x:y:xs)`
Probably. Criterion provides both `nf` and `whnf`: [criterion library](http://hackage.haskell.org/package/criterion-1.1.0.0/docs/Criterion.html)
Then I see this error message : *Main&gt; doubleEveryOther [1,2,3] [1,4*** Exception: creditcard.hs:(18,12)-(19,73): Non-exhaustive patterns in function doubleEveryOther' 
The function names aren't the same. The Y is capitalized in one of them. `doubleEverYOther` is being treated as though it's a different function.
I have not used the lens library in a while so I don't know how they generate the lenses today but depending on the method used they wouldn't necessarily use the named accessors, e.g. with something based on DeriveGeneric you could iterate over them without actually using the generated functions.
I did a quick scan over it and there was some stuff about Haskell and computation, but not enough to make sense of the title IMO.
&gt; So far, none of the interpreters we’ve defined have done any IO. The next post will look at our options for dealing with effects in our DSLs and interpreters. I was wondering about this as well. With a "naive" pattern-matching style interpreter, you can just write the function in IO: ``` interpret :: Free Whatever a -&gt; IO a ``` But with a comonad interpreter, how can you perform side effects during interpretation, since `IO` is not itself a `Comonad`? 
I imagine you've fixed this by now, but for the record, doubleEveryOther' reverse lst Will apply `doubleEveryOther'` to two arguments, the first being `reverse` and the second being `lst`. I imagine what you want is actually doubleEveryOther' (reverse lst) i.e. applying one argument to the function, the reverse of `lst`.
You are aware that WinGHCi is a special program to give a GUI to ghci on windows? https://code.google.com/p/winghci/ And that the problem this person has is likely not with the haskell toolchain, but with teaching winghci to find it? And thus that installing stack as such may or may not have anything to do with their problem?
Stay tuned :) If you don't mind spoilers - what I'm going to be doing next is using `Monad m =&gt; CofreeT f w (m b)` and a corresponding adjustment to the helper function for pairing monad / comonad transformer stacks. There's a few other variants on that theme in the wild, but they mostly involve dealing with the final result rather than interleaving the interpretation with the effects. Off the topic of my head - and so possibly incorrectly - these including things like pairing `Free f (a -&gt; b)` with `Cofree g a` or pairing `FreeT f m a` with `CofreeT g w (a -&gt; m b)`. I'll probably make of these in the next post, although they won't be the focus. There's some related info [here](http://blog.sigfpe.com/2008/03/transforming-comonad-with-monad.html) and [here](http://comonad.com/reader/2011/monad-transformers-from-comonads/)
If you installed GHC as part of the platform, adding something like C:\Program Files\Haskell Platform\blaBla\bin should work (depending on your install directory, of course). In might be trite advice, but I'd recommend uninstalling the Haskell Platform/GHC/anything else you have, removing the `cabal` and `ghc` folders from your home directory and deleting anything that looks Haskell-related from your PATH. Then do a clean install of the Haskell Platform and nothing else. That tends to solve most problems. If it doesn't, then there's still /u/codygman's advice about stack.
``turtle`` is awesome! Thank you!
Thanks for everybody's help! I ended up doing something similar to this: abort = do putStrLn "Retries exceeded; aborting" exitFailure retry :: Int -&gt; String -&gt; IO () retry 0 _ = abort retry n s = do c &lt;- shell (T.pack s) empty case c of ExitFailure x -&gt; retry (n - 1) s _ -&gt; return () retry4 = retry 4 main = do let ccl = createCommandList r &lt;- sequence_ $ map retry4 ccl putStrLn "Done!" 
[It's come full circle :p](https://twitter.com/callumorphism/status/612767714646601728) 
[**@callumorphism**](https://twitter.com/callumorphism/) &gt; [2015-06-21 23:43 UTC](https://twitter.com/callumorphism/status/612767714646601728) &gt; @SamirTalwar Oh, I'm interested in that too. I've seen Haskell peeps hack doctest for this. Relevant reddit thread: https://www.np.reddit.com/r/haskell/comments/2t4q6m/unit_test_that_a_particular_expression_does_not/ ---- ^This ^message ^was ^created ^by ^a ^bot [^[Contact ^creator]](http://www.np.reddit.com/message/compose/?to=jasie3k&amp;amp;subject=TweetsInCommentsBot)[^[Source ^code]](https://github.com/janpetryk/reddit-bot) 
Thanks. I'll have look
Yeah agree with that but I had the impression that I'm not going to be much deep in the language. Thanks for your reply 
You can see the generated code using the `-ddump-splices` flag. In this case you'd get makeLenses ''Item ======&gt; price :: Lens' Item Double price f_a4vc (Item x_a4vd x_a4ve) = fmap (\ y_a4vf -&gt; Item x_a4vd y_a4vf) (f_a4vc x_a4ve) {-# INLINE price #-} quantity :: Lens' Item Double quantity f_a4vg (Item x_a4vh x_a4vi) = fmap (\ y_a4vj -&gt; Item y_a4vj x_a4vi) (f_a4vg x_a4vh) {-# INLINE quantity #-} 
You're welcome!
After I replied to you earlier I got curious about more complete approaches to OOP in Haskell and started googling around. I eventually landed [here](http://arxiv.org/abs/cs/0509027), although I haven't gone completely through it yet. It looks neat so far. &gt; Let's say you model animals with a record: In order to fully appreciate your point here I need to know more about what you want to be able do with these things. Because when you say I *can't* write some function in a satisfactory way, I get really tempted to try. The bigger goal affects which alternatives I consider satisfactory. After all, if the corresponding Java solution involves explicit runtime type inspection I wouldn't feel so bad about using the same tools in Haskell. &gt; Why is it an anti-pattern? It seems perfectly fine to me. I think of it as an antipattern because your code gets longer and your types get more complicated (from my perspective). Since your interaction with a list of existentials limited to dictionary elements anyway, I don't see what you lose by storing a list of dictionaries instead. Although, if you know of a compelling example for which this coversion doesn't simplify things that would be cool to see.
 &gt;Though I wonder if winghci is installable through stack without issue. Looks like not as it requires special setup. 
I'll be pretty excited when that actually happens, been on the cards for a while now, I appreciate these things take time though :)
I solved that already but thanks for saying.
It's already possible to re-export modules. However, it would be particularly nice if it were possible to re-export *qualified* modules. People have thought about this on and off, such as [here](https://wiki.haskell.org/GHC/QualifiedModuleExport) and [here](https://ghc.haskell.org/trac/ghc/ticket/8043) and a few other places linked to there. To be honest, it seems like the main reason this hasn't happened is that no one has decided to take it on; it seems like a fairly straightforward patch to GHC. The other issue of the `LANGUAGE` pragmas is different, because I see real problems with an `import` statement enabling or disabling `LANGUAGE` pragmas. I'm not sure how you would reduce this boilerplate, or if it is necessary to reduce. One option would be to define a "modern" Haskell, which goes beyond Haskell2010 and includes by default a number of extensions now considered completely commonplace. From the above list, I would say it is safe to enable by default ConstraintKinds, DataKinds, DeriveGeneric, FlexibleContext, FlexibleInstances, GADTs, MultiParamTypeClasses, TypeFamilies, and TypeOperators. I chose these because they seem like ones that add strictly more power and new syntax, and don't make Haskell2010 code break when it previously worked (I think?). GeneralizedNewtypeDeriving scares me a little, I'm not sure about PolyKinds, and TemplateHaskell/QuasiQuotes I've always felt were too hacky and cludgey to deserve to be part of a "default" set of extensions. With all of this in mind, we did previously have `-fglasfow-exts` which is now deprecated; this was before my time so I'm not actually sure what convinced people this sort of "kitchen-sink" approach is a bad idea. (Does anyone more experienced care to comment?) Perhaps its time for a new Haskell standard and a new Haskell compiler to compete with GHC... if only.
Interesting, I'd never heard of WinGHCi (I've only recently been using Windows for anything). I tested things out, and: * Running WinGHCi.exe without GHC on the PATH resulted in the program freezing * Running `stack exec winghci.exe` fixed it My guess is that anything which will modify the PATH to include ghc(i).exe will work, but `stack exec` may be a nice, easy way to do that modification. __EDIT__ As a bonus, `stack --resolver ghc-7.8 exec winghci` and `stack --resolver ghc-7.10 exec winghci` are easy ways to switch between two different GHC versions.
Huh. You're right, it doesn't actually do the normalization step: darwin = Rules { ... , fromText = posixFromText ... } which just splits the text into path pieces and unpacks each as a `String`, skipping the required normalization step. That's a bug. But in any case, `system-filepath` does provide a great example of *machinery* for platform-dependent file paths and platform-dependent `Text` and `String` coercions, which is the point here.
Except we haven't written a web app through servant and persistent and wired them together with DSLs. We've done nothing. EDIT: Of the 34 effective lines of code we've written so far, 0 of them include any application logic. In a perfectly expressive language, we wouldn't have any zero-logic lines of code. As it stands, we've taken almost 10 times as many lines of code as it would have taken to write a ["Hello World" Sinatra app](http://www.sinatrarb.com/) to express 0% of what our webapp does.
To be fair, if you look closely, there's 1 `import` for *servant*, 2 for *lens* and 4 for *persistent*. The code in the post uses *many* more libraries, but the focus really is on the 3 mentionned above.
Great, thank you!
I can't help with your current problem but just wanted to reassure you that these setup teething problems are worth it. I had the same trouble getting Haskell set up when I started but I'm *so* glad I persevered. It's a fantastic language and the most helpful software community I've encountered.
This is great. Have you thought about trying to move the permissions DSL to the type-level? 
&gt; convenience operators like lens should be a part of the Prelude. Ah, the PHP approach. Just include everything and the kitchen sink in the base library of the language and when people want other behaviour, just use the `mapReal` function handily provided by the `mapRealFixed`library.
That would be awesome, but I don't know how !
Something like this? https://gist.github.com/jkarni/948e59d2985d67c36453 There's still a lot of functionality missing, and I'm not sure if it's in the end worthwhile, but it's a thought.
Right, it gets tricky when "access captures" are intertwined with non-access-related path elements (in the specific case of `:&amp;&amp;` separating them out would work, but for `:||` it wouldn't). I'll have to think some more ;)
&gt; How did Yesod get involved in this one at all? When it's not lens, people seem to like blaming yesod for all that's wrong in the world. I like both myself, so I suppose I am part of the problem :/ &gt; And for the record: the Database.Persist and Database.Persist.Sql imports appear to be redundant. It does indeed build without `Database.Persist`, will fix the post tonight.
Strangely enough I answered almost exactly the same question on Stack Overflow earlier: http://stackoverflow.com/questions/31135606/using-commercialhaskell-stack-with-a-proxy?noredirect=1#comment50286972_31135606
I've seen the `{-# LANGUAGE KitchenSink #-}` joke pragma a few times now, and I'm starting to want it to be a real thing. My imagined behaviour is for GHC to (somehow^(TM) ) figure out the language extensions necessary to make my program legal and turn them all on.
the real solution here is to grab all these packages' prelude modules together and smash them into one big `UltimatePrelude` module naturally, this will handle name clashes with gratuitous typeclass usage
Heh, very close in time! The title of the post you answered doesn't help finding it.
Thanks. I'm on it.
Several of the extensions you listed (GADTs, TFs, DataKinds etc) in practice go through either constant tweaks or massive overhauls (e.g. we just got closed TFs, and we'll get injective ones soon). The whole point of it being an extension is that it is allowed to be something of a moving target, versus the standard. I don't know how amazingly the plan has worked out or what people think, but that's kind of the rationale. In practice, this occurs quite frequently for type level features because the type checker is prone to overhaul and change. It's actually probably quite realistic to implement the Haskell you described, but you'd basically just have to define whatever behavior it implemented and never change it ever again for those features. Then GHC will just have new extensions later, like `-XClosedTypeFamilies` or `-XInjectiveTypeFamilies` because your compiler doesn't implement them, just "type families". Competition would be great here, but I'm not sure if this will quite 'fix' the heart of the issue. &gt; With all of this in mind, we did previously have -fglasfow-exts which is now deprecated; this was before my time so I'm not actually sure what convinced people this sort of "kitchen-sink" approach is a bad idea. (Does anyone more experienced care to comment?) Principle of least surprise, I'd say. It just makes it impossible to determine if your program will ever really 'work'. The meaning of that flag changes almost entirely in a lot of subtle (and not subtle) ways from GHC version to GHC version, because by being an extension a feature is allowed to be a moving target... Some programs that were valid may then become invalid because some extensions change the syntax (like `StaticPointers`). It may never be clear *why* a program was suddenly accepted now but rejected in the past (because now it uses `BinaryLiterals` or something). Or any other number of bad interactions over time, really - those are just very simple obvious cases.
Seems to me like that is a problem with judgmental equality. Judgmental equality is a bad apple of dependent types anyway because it makes the whole thing non modular.
No, the problem is that truncation isn't designed to be erasure. If you want erasure, use a type theory with support for erasure. Truncation is meant to be truncation, with the eliminators that are appropriate for the truncation in homotopy theory. If they wanted truncation to be erasure, they wouldn't have chosen an eliminator that lets you write `(∃x:A. ∏y:A. x=y) -&gt; A`.
Well, even the `x &lt;&gt; x` example given by /u/hvr_ requires a reduction of the type synonym behind-the-scenes, in order to solve the `Monoid FilePath` constraint. And indeed, it's not obvious that class instances for `[Char]` will continue to be available for the new abstract type. I've been trying to construct more compelling examples, but GHC is impressively good at retaining synonyms! Perhaps there is a way to distinguish between type synonym reductions that are visible in the types of subexpressions, and those that are not, and use that as a heuristic for displaying a warning.
Well you answered your own question :) Don't change Dockerfile every time :) Also docker is NOT for active development, it is for automating deployment. In other words you develop outside of docker and when you app is tested and ready you package it with docker and deploy to production environment. And in this case you do not need ghc, alex, happy, cabal install, you do not need yesod-bin etc. 
Sure, truncation isn't erasure, but the claim that "truncation doesn't actually erase information" is very misleading. The information needs to be there at run-time to execute programs, but you can't actually do anything with it (except by abuse of judgmental equality). You can't write a function `|T| -&gt; T`. It's like a closure: yes it does need to store its lexical variables somewhere, but you can't actually get at them. When you have a dependent function `f : (x:A) -&gt; B x` and A has a nontrivial equality `x = y` then `f x = f y` clearly, but the `=` in the latter is subtle. We are equating two different types `B x = B y` which is okay because `B` itself respects equality, so we get an isomorphism between `B x` and `B y`. Even if `B x` and `B y` are identical as types (e.g. both are Nat), the `B` can still give use a nontrivial isomorphism. For example if `B : |Nat| -&gt; Type` we can have B k = Nat for all k, but still give us a nontrivial isomorphism. For example we can have an isomorphism `natIso n k = \m -&gt; if m==n then k else if m==k then n else m` which swaps `n` and `k` but leaves everything else the same. So that way we can have `f |3| = 3` and `f |4| = 4` and since `|3| = |4|` we have `3 = 4`. This is okay because the `=` in the last equation is over the isomorphism `natIso 3 4`. That's what's happening in the mystery function.
`myst` was only one example of truncation not being erasure, which is why I keep bringing up the fact that you can write a function of type `(∃x:A. ∏y:A. x=y) -&gt; A`. This is a counterexample to a very specific sort of erasure that /u/winterkoninkje mentioned earlier: &gt; As for your lemma, I have a hard time seeing how it would be a theorem (though I'm willing to be convinced). Supposing the premise isn't a contradiction, how do you propose to construct an inhabitant of A? Just because we've proven that some x:A must be related to every A, that doesn't let us put our hands on an actual concrete A.
There is nothing any Haskell specific tools can do about that because when you do that Docker starts from a state of the filesystem where any caches they might have created do not exist yet.
Really interesting. I agree that GHC error message are sometimes hard to read (and understand).
&gt; Unfortunately, on Unix systems Actually, the latest UNIX specification specifically allows *any* behavior on a shebang line. Without a shebang line (but with an appropriate environment) a UNIX shell (not necessarily /bin/sh) is used. Ref: http://pubs.opengroup.org/onlinepubs/9699919799/idx/shell.html (2.1.1)
&gt; you might already have noticed that the major version got incremented. That means there’re compatibility breaking changes. But this is 0.3; aren't the rules different before 1.0? Hmm! I can't find anything on the subject in the [PVP](https://wiki.haskell.org/Package_versioning_policy). I had not previously realized that this part of [semantic versioning](http://semver.org/) did not carry over to the PVP. I guess I should re-read both documents and make sure I understand the other differences if any.
For instance, in [this](https://hbtvl.wordpress.com/) article, the author writes DSL's for use with persistent and persistent (original discussion [here](http://www.reddit.com/r/haskell/comments/3bjguk/integrating_servant_and_persistent_with_a_couple/)), but to do so required 14 language extensions. 
I think most of us already admit that we're writing GHC. No sense being in denial about it.
I've heard talk about Haskell 2017 being the next "release", with dependent types built in. But that's far off.
I can't believe I typed Unix, I meant Linux systems.
Very interesting. I'd love to see some of this improvements in GHC. Wonder how it works when RankNTypes and the like come into picture. I've had nightmares with those .... *shivers*
Fair enough. Honestly I'd thought it went out to `ghc-devs`, but apparently it was just sent to the existing `haskell-prime` committee members to make sure there'd be no hard feelings over rebooting the process and for feedback a few hours ago, so I appear to be guilty of spilling the beans. The next step, assuming things proceed normally, would be organizing nominations for such a committee publicly. There's no insider thing going on here beyond someone far better at dealing with public affairs than I am asking "hey we'd like to reboot your committee, would you mind?" before making a broader announcement.
I saw a talk by Richard Eisenberg at Lambdaconf and that's what he said. IIRC he's working on implementing dependent types in Haskell.
Haven't heard anything about Hacphi yet this year. Is Hac Boston a replacement?
You should come uncork it (or preferably a more suitable libation) at [Hac Boston](http://www.meetup.com/Boston-Haskell/events/223607923/). ;) 
Yeah, squiggly lines and colours and showing the code you wrote is all redundant when your editor just shows you the code in question. Having the compiler actually output semantic structured error types in JSON or whatever is ideal. There is an unfortunate architectural state of affairs in GHC that messages are not all in a sum type and instead every error is a nonce error constructed stringly in place (and apparently SPJ does not want to fix this), making any kind of meaningful interpretation of the messages be reduced to fickle regex matching in IDE/teaching systems. 
Admittedly it's just a nice wrapper over `forall m. Monad m =&gt; ReaderT (a -&gt; m b) m r`, but with a special not-always-a-monad-but-don't-tell-anyone "transformer" instance over an extra Traversable :)
There are some widely used extensions that have been implemented for many years, that are still not in the standard. I doubt full-blown dependenent types will ever be in hte standard. 
Well that stinks.
You're talking about the Second System Effect. And it's OK; we've had several versions of Haskell by now.
"Nonce" is a relatively uncommon English word meaning "(of a word or expression) coined for or used on one occasion." Vaguely synonymous with "one-off" or "ad hoc."
It's even more confusing because the PVP and semantic versioning don't even refer to the parts of the version number by the same name! For the version number 1.2.3.4: 1. the PVP refers to 1.2 as the major version number, 1. the PVP refers to 3 as the minor version number, and 1. the PVP refers to 4 as the patch-level version number but 1. semantic versioning refers to 1 as the major version, 1. semantic versioning refers to 2 as the minor version, 1. semantic versioning refers to 3 as the patch version, and 1. semantic versioning doesn't like the 4. The other difference, the one I was talking about, is that semantic versioning allows arbitrary breaking change between any two pre-1.0 versions. So, to clarify: I wasn't saying that you used the name major instead of supermajor or something like that, I was saying that I was surprised that a bump from 0.2 to 0.3 would mean that there is more compatibility-breaking changes than between 0.2.1 and 0.2.2, because I expected any pre-1.0 change to be allowed to make compatibility-breaking changes. I was mistaken, because the PVP doesn't say anything special about pre-1.0 versions. Which is good, because the Haskell ecosystem has a *lot* of pre-1.0 libraries!
Is there a ticket associated with this so that interested people can share notes on what changes would need to be made?
"act in good faith" is probably stronger language than you meant. From a whole host of evidence I don't trust that the prime process will be kickstarted again, given that a few past reboots have failed, due to inertia more than any particular cast of characters. It's the usual thing with projects based on who is willing to step up to the plate -- plenty of people would _like_ for some modern features to be added to the standard. But standardizing things is a pain, and even proposing extensions that may not be standardized can be a handful as you've seen. That said, if anyone is going to get things in gear, I think hvr well may, since he seems to have a knack for actually following through on all the troublesome details :-)
I think I read /u/chrisdoner say they have plans for Emacs' haskell-mode to support pop-ups with custom syntax for strings/comments/quasiquotes. e.g. This would let you click on the string `"\"quoted\""`, see it as `"quoted"`, then edit it into `"quoted" "again"`, and get `"\"quoted\" \"again\""` inserted back into your source. of course, you are asking about Vim, and about something existing ;-)
I have seen these: * https://ghc.haskell.org/trac/ghc/ticket/8809 (I'm really annoyed by goldfire's push back) * https://ghc.haskell.org/trac/ghc/ticket/9173 But some interested party should bring this blog post up on the GHC mailing list. I want to see what the core devs have to say.
The dramedy that occurs whenever people try to change things. This is neither the time nor the place for an airing of grievances, and I'd like to give the folks doing this a chance to actually do something before wasting many words on it.
I'm guessing the dramedy referred to was around [this](https://ghc.haskell.org/trac/ghc/wiki/Prelude710). From where I was sitting it was a really unfortunate communications failure followed by a debate with time pressure and strong opinions on both sides. There's a lot of other details to how it all played out, but that's the very, very high level simplistic view.
These were all wonderful, thank you for taking the time to go through my code. It was very enlightening to see, in a concrete way, what things like =&lt;&lt; and all that do. They've been rather opaque when they come up in other people's code. I wish someone had just told me I can read it as "fed from". I finally got it working with sessions / TCP pooling, and I'm wondering whether I should take the step and try to make this parallel as well. Any tips in that regard?
Cool. How does it compare to [wizards](http://hackage.haskell.org/package/wizards) ?
The Foldable Traversable Proposal (FTP, aka BBP) that shipped with GHC 7.10.1 is a good example of how heated debates about change can get. You can tour the historical battlefield here on reddit or on the mailing lists. There was even a deleted post. It was pretty intense.
Alright, I yield. I'm not going to get into this here as it's not my place to do so. I should have kept my thoughts to direct message, but I want to be sure that anyone else who was or is frustrated by the community processes knows that they are not alone in that feeling. Keeping my remarks to a private channel when what I'm against is the amount of private decision making was too much irony for my constitution. I'm sorry if my portmanteau describing heated arguments culminating in a controversial satirical web page offended.
I think the libraries committee did exactly as you said: the issue started in the public, then the committee didn't communicate regularly (which they apologized for), then there was an outcry. My objection was that the outcry carried as much weight as it did given the small fraction of the community it represented. If every decision where the voting is 5-1 takes that much time, that many emails, that many blog posts, and that many reddit comments, we will struggle to move forward. This was a case with broad public support where the work was already done, and it was very nearly dismantled at the last minute.
I was waiting for this response. Comments like his are how things never get fixed. 
I guess he means JavaScript 😜
He forgot about purity.
Ah, sorry, I was not referring to the libraries committee. I will be forever grateful that they pushed things forward against so much resistance, and it is that resistance I'm complaining about. I guess I assumed rather a lot of context in my original remarks.
I don't think that is a good thing. Having only one compiler means that we have a single point of failure for the language and no competition to spur improvements. For example, in the discussion on error messages the consensus was that GHC's poor design choices were blocking the entire community from exploring better error messages. Had there been a competing well-maintained compiler implementation we might have had a better chance that at least one implementation could easily implement suggestions and drive the other to improve to do the same.
Yeah, emacs support for Haskell is *vastly* better than available vim tooling. It's worth learning spacemacs at least, it seems.
That is a shame. If you (or anyone else) are feeling extremely generous, you could look into just making some of these changes without waiting for approval. You will get pushback on a lack of pre-existing editor support from various people; I do not know why this is a thing in the community, but it is (from goldfire here, and I've seen it from hvr, too). The downside is that the patch might not be accepted. I don't know how this process can be unclogged, but I would love to see some actual efforts -- even experiments that could prove to be unsuccessful -- at improving this rather than hand wringing about whether emacs tooling can be adapted.
&gt; Having only one compiler means that we have a single point of failure for the language[.] While I agree with the rest of it, I'm not sure what sort of failure you're envisioning where "single point of failure" is really a relevant notion here... 
I mean failure in a very general sense, not in the sense of a program crashing. I already gave the example of a poor design flaw in GHC.
tl;dr: `cabal init` will now create a Main.hs for you (as long as it doesn't already exist and you are creating an executable). I got this change merged a couple of months ago, but it may not be as necessary given how fast `stack` is coming along!
I don't understand how this comment is preventing any code from being written. It looks like there is just a lack of volunteers to work on the proposal. 
&gt; You alluded to my recent attempt to improve syntax, during which Herbert introduced to the discussion a statement about how it would break things due to not being compatible with how cabal has always worked, then opened an issue on cabal to make it work that way. Just to clarify, I never claimed that Cabal already implemented this. In fact, I stated it wasn't implemented yet [in my post](http://permalink.gmane.org/gmane.comp.lang.haskell.cafe/116806): &gt;&gt; If I understand correctly, the initial proposal was to enable the new syntax by default and it mustn't break any code (full backward-compatible). &gt; That would be a departure from how language extensions were handled in GHC in the past afaik, and if there's no language pragma to toggle this new syntax, Cabal has no way to know that a new language syntax is required and that thereby needs exclude **(not implemented yet)** the affected package versions from the install-plan configuration space. Moreover, I repeated this in a subsequent [post](http://permalink.gmane.org/gmane.comp.lang.haskell.cafe/116843) by linking to the newly created [Cabal ticket](https://github.com/haskell/cabal/issues/2644) And for the record, I was neither for nor against your import-syntax proposal. My interest was solely in keeping up with the tradition to guard syntax extensions to the Haskell Report behind pragmas (there's *alot* of other syntax extensions that we could enable by default), as otherwise that would, IMHO, thwart the purpose of the `other-extensions` Cabal field and the language pragma facility introduced in H2010.
It's far from terrible, but how can you read the first and last sentences without coming away with a negative feeling on this effort moving forward? I had never encountered anyone who didn't immediately appreciate clang's error messages until seeing this ticket tonight, so I can sympathize with why the proposer was put off. As I already harped on, so much is said about what are irrelevancies at this point in the discussion. When all is said and done, most of the words on the entire ticket are now spent making weak arguments for not making these changes. It's like burying a lawyer with paper work to win a court case. Perhaps it wasn't an unreasonable comment, but if I'm cruising around looking to work on GHC, I'm probably not going to dive into something so weighed down by negativity. And if that negativity is focused on things like editor support, I'd figure this really isn't going to move at all. You can't tell potential volunteers who were put off by a comment that the real problem was a lack of volunteers.
&gt; so the quantifier you're talking about is not the HoTT mere existence quantifier. Which is why I put the ∃ in scarequotes when talking about HoTT ;)
As I said, you've earned the benefit of the doubt that you had the best of intentions, but I don't know how this isn't a strangely worded comment: &gt; That would be a departure from how language extensions were handled in GHC in the past afaik, and if there's no language pragma to toggle this new syntax, Cabal has no way to know that a new language syntax is required and that thereby needs exclude (not implemented yet) the affected package versions from the install-plan configuration space. You start off by calling this a departure, and saying how cabal would have no way to know, etc. with a parenthetical that was intended to make the entire paragraph hypothetical? Is that an argument you bring up for every language extension, given that the treatment of this one would be in no way different? Even after reading your message and my restatement of it, Sven did not believe me when I said cabal did not work like this, nor had it ever. So, best case, this was worded so as to be incredibly misleading.
Thank you for your advice. I'll develop the app on local machine and then deploy it using Dokcer Machine.
Fair enough. I didn't like the way the original poster gave up on their original point when pressed. When someone says, "Comments like this are how things never get fixed" before I said a word, I think you're giving my influence too much credit.
Also, gratuitous CPP
What do you mean the slopes? For [Bézier](https://hackage.haskell.org/package/smoothie-0.3/docs/Data-Spline-Key.html#g:1), you can specify the input and output **normals** – are those the slopes you are talking about? :)
On the bright side, we can mostly ignore that `return` is a method. I.e. we don't even have to override it when defining `Monad` instances (from post-AMP base-4.8/GHC 7.10 on). In fact, we could easily have a future Haskell Report and the associated `haskell201x` package consider `return` to be a top-level binding rather than a class-method, even if `base` continues to provide it as a method `Monad(return)` for the forseeable future. In other words, the next version of the Report that incorporates the AMP doesn't have to contain this `Monad(return)`-wart.
And also colloquially means pedophile
Have you thought about implementing an ide-protocol like Idris does? http://idris.readthedocs.org/en/latest/reference/ide-protocol.html That way all IDEs just have to implement speaking that protocol and we have a central place to improve. I think https://github.com/commercialhaskell/stack-ide is such a thing for Haskell but that doesn't ship with the compiler. I would like such things be shipped out of the box to be easy accessible. 
Is this an alternative to free monads? In my opinion `Prompt` is a restricted version of a free monad.
Previous instance of this discussion https://www.reddit.com/r/haskell/comments/2ao3ul/cabal_semantic_versioning_and_endless_experimental/
Oh cool, thank you very much for pointing that out! I’ll add a link in my post.
This probably is material for a whole dissertation of a real smart person.
Purity = (1) the output is completely determined by the input parameters and (2) functions have no effects besides computing their output value. `userInput :: String` would be a constant in Haskell, but with implicit IO, you could write string userInput() { return readLine(); } Variable mutation is a problem for the same reason: by reading/writing non-local state, functions influence each other via hidden channels. (2) is a bit more muddy because some people count non-termination and failure via `error` as effects too.
I’d wager small improvements to the user output strings would increase readability by a great margin (although GHC does really well compared to e.g. Clojure), albeit probably destroying most of the tooling out there. A general solution for abstract type-y things though …
&gt; There was even a deleted post. It was pretty intense. A deleted post is intense? Admittedly I don't have context, but in isolation that seems pretty silly.
&gt; uses actual functions instead of typeclass magic and hackery to have different backends Can you go into more detail about how wizards uses typeclass magic and hackery in your opinion? It will help me calibrate my architectural decision making.
The only thing that has changed is the creation of the Main.hs file: module Main where main :: IO () main = putStrLn "Hello, Haskell!" Are you saying that you're worried that the above code would be considered my copyrighted code?
Yes. This may seem overly pedantic, but we are talking about potential legal issues here.
I'm concerned about both. Most projects deal with this by explicitly releasing such files into the public domain. For instance, the glibc's `crt0.S` contains an extra clause that allows for unlimited distribution in compiled form.
I probably don't have the answer you're looking for, but I just want to dispel a myth: you can (and frequently do!) have mutation in pure languages. If the same input array always gives the same output histogram, then the histogram function is pure. It doesn't matter whether or not it's implemented with mutations. (You sort of hint at knowing this already, I just want to make it abundantly clear.) A couple of other observations: * Logarithmic update time might not be as bad as it seems. As the saying goes, "O(log n) is O(1) for any realistic n". * I've found that immutability throughout generally has worse performance than mutability, as you observe. But this is not surprising nor a bad thing, given that our von Neumann machines are kinda stateful in their core. If we were using languages with a focus on immutability all along, we would probably view mutability as a performance optimisation akin to unrolling loops or inlining functions. So in a way, your question reads like, "In a language with no loops, this code is really fast because there are no conditional jumps. How would you do that in a language where a loop is more idiomatic, while retaining the performance of an unrolled loop?" (Barring any cache issues, branch prediction and the like which might make the loop actually faster.) The answer is a pretty boring, "By unrolling the loop and writing unidiomatic code, if performance is critical." Similarly, I suspect the boring (but correct) answer to your question might be, "By using isolated mutation in the histogram function, if performance is critical."
If that's the case, I would suggest opening an issue on the [cabal issue tracker](https://github.com/haskell/cabal/issues). In [cabal's LICENSE file](https://github.com/haskell/cabal/blob/master/LICENSE), I don't think it makes any specific mention about `Setup.hs`, `.cabal`, etc. Also, you might want to raise the issue with the `stack` project as well, since they have a command similar to `cabal init`.
It's not exactly what you've asked for, but you should be able to build the desired list in linear time from this import Data.List (group, sort) import Control.Monad (liftM2) hist :: Ord a =&gt; [a] -&gt; [(a, Int)] hist = map (liftM2 (,) head length) . group . sort This function will take a list of `Ord a =&gt; a` and builds an association list from it, with the original values as the keys and the number of occurrences as the values. It should be trivial to insert the missing keys inbetween when we restrict it to integers. I'd probably just use `lookup` and `fromMaybe 0` though, depending on how much the data is gonna be used. If you want fast access to the values, a list won't do anyway, but once you've got that list you can easily populate a vector with it and get O(1) lookups. EDIT: Of course this is asymptotically slower than the imperative solution. If I'm not mistaken it should run in O(n log n) where n is the length of the input list. It's really just the first thing I came up with.
Let me repharse the post: Java + Haskell features - Java features = Haskell. Who could have guessed?
OP asked whether an O(n) solution is possible. This doesn't seem relevant to that. 
As I see it, `Prompt` is more or less unrelated to free monads in any meaningful way. The only real link is that `Free` over a specifically crafted `Functor` will give rise to something isomorphic `Prompt`, but this is literally the case for every single Monad ever...so saying that `Prompt` is like a "restricted free monad" is like saying `Maybe`, `State`, `Writer`, etc. (+ every single monad ever) are "restricted free monads". Which really doesn't mean anything :) One superficial link might be that typically to create things like `Prompt` people have traditionally used `Free` over a specially crafted `Functor` instead of doing things themselves/by hand. So *prompt* is an alternative to free in the same way that Maybe, Either e, State, Writer, etc., are "alternatives to Free" -- you could implement State, Writer, etc. using Free if you really really wanted to. State from *transformers*, *mtl* (and *prompt*) are "alternatives to free", in that sense. (But it's not really a meaningful sense, I think)
I didn't read the blog, I only ask your question, and you didn't put a O(n) restriction, did you? 
Yeah, these points are not new to me. I'm mostly interested the purely theoretical question.
Yes, with a counting sort, which uses a histogram, which is what I'm asking about. Or do you mean something else?
http://stackoverflow.com/questions/21132026/frequency-of-characters
Radix sort complexity depends on integer width, which is O(log n) in my problem.
By the same token, would you say that functional arrays have O(1) update because address space is finite? That feels a bit borderline, not 100% within the spirit of the question.
My post mentioned ST.
Well, I'm not sure what the spirit of the question is. Questions of complexity are hard unless you nail down exactly what you're asking, and what kind of primitives you assume. From a practical perspective there's no doubt the imperative version you gave is faster than any "pure" version. But if you're talking asymptotic complexity things are different. Also, (I've not tested this, so take it with a grain of salt) if you use the package I linked to and make a histogram of a [Int] you will get a straight line when plotting time as a function of input length. Is that in the spirit of the question?
By slopes I mean derivatives. A cubic spline generally has whatever derivative the user wants at each transition. This is typically achieved using the four cubic hermite polynomials. (PS: I doubt you meant to use the word **normals** there. A normal is perpendicular to a curve. If this is really what you meant, that's an unusual way to specify a Bezier curve.)
Oh, that's interesting. Makes sense. Still weird, but yeah. My bad.
For what it's worth, my intended machine model is something along [these lines](https://en.wikipedia.org/wiki/Transdichotomous_model). Operations on "machine words" (integers of a certain fixed size) take constant time, including array indexing, but the "word size" isn't known to you in advance, so you can't claim that its logarithm is a constant factor.
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Transdichotomous model**](https://en.wikipedia.org/wiki/Transdichotomous%20model): [](#sfw) --- &gt; &gt;In [computational complexity theory](https://en.wikipedia.org/wiki/Computational_complexity_theory), and more specifically in the [analysis of algorithms](https://en.wikipedia.org/wiki/Analysis_of_algorithms) with [integer](https://en.wikipedia.org/wiki/Integer) data, the __transdichotomous model__ is a variation of the [random access machine](https://en.wikipedia.org/wiki/Random_access_machine) in which the machine [word size](https://en.wikipedia.org/wiki/Word_size) is assumed to match the problem size. The model was proposed by [Michael Fredman](https://en.wikipedia.org/wiki/Michael_Fredman) and [Dan Willard](https://en.wikipedia.org/wiki/Dan_Willard), who chose its name "because the dichotomy between the machine model and the problem size is crossed in a reasonable matter." &gt;In a problem such as [integer sorting](https://en.wikipedia.org/wiki/Integer_sorting) in which there are n integers to be sorted, the transdichotomous model assumes that each integer may be stored in a single word of computer memory, that operations on single words take constant time per operation, and that the number of bits that can be stored in a single word is at least log2*n*. The goal of complexity analysis in this model is to find time bounds that depend only on n and not on the actual size of the input values or the machine words. In modeling integer computation, it is necessary to assume that machine words are limited in size, because models with unlimited precision are unreasonably powerful (able to solve [PSPACE-complete](https://en.wikipedia.org/wiki/PSPACE-complete) problems in polynomial time). The trans-dichotomous model makes a minimal assumption of this type: that there is some limit, and that the limit is large enough to allow random access indexing into the input data. &gt;As well as its application to integer sorting, the transdichotomous model has also been applied to the design of [priority queues](https://en.wikipedia.org/wiki/Priority_queue) and to problems in [computational geometry](https://en.wikipedia.org/wiki/Computational_geometry) and [graph algorithms](https://en.wikipedia.org/wiki/Graph_algorithm). &gt; --- ^Relevant: [^Michael ^Fredman](https://en.wikipedia.org/wiki/Michael_Fredman) ^| [^AF-heap](https://en.wikipedia.org/wiki/AF-heap) ^| [^Dan ^Willard](https://en.wikipedia.org/wiki/Dan_Willard) ^| [^Smoothsort](https://en.wikipedia.org/wiki/Smoothsort) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+csombh7) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+csombh7)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](/r/autowikibot/wiki/index) ^| [^Mods](/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Call ^Me](/r/autowikibot/comments/1ux484/ask_wikibot/)
It's sad to read that works by Asperti, Lawall and Mairson [1,2,3] are still misinterpreted as results about the efficiency of optimal reduction or sharing graphs, while the point was the quest for a complexity metric for λ-calculus. I shall try to clarify that a bit and summarise the state of the art about efficiency of optimal reduction. The **problematic result** shows that there are λ-terms such that: * they can be normalized with n steps of shared-β reduction (the "main" rule of optimal reduction), but * the complexity of any implementation of optimal reduction is lower-bounded by a Kalmar-elementary function of n (i.e. a tower of exponentials). The point at that time was that, contrarily to the expectations of many, shared-β reduction steps are not meaningful from the complexity point of view, since almost all work can be transferred from the shared-β steps to "bookkeeping" rules of optimal reduction (the eta-expansion technique exploited by authors does precisely this). If one is looking for conclusions about efficiency of optimal reduction or sharing graphs (the former essentially being a lazy strategy for the latter) it will be disappointed. Indeed, recall that complexity of normalisation of λ-terms on *any* implementation is in general non-elementary in the size of the term. Hence, the only admissible conclusion one can deduce from those work is obvious---optimal reduction is not a miraculous machinery. This has been further confirmed some years later, when Asperti, Coppola and Martini [4] showed that the same result could be achieved moving the cost of normalisation not on bookkeeping rules (which one may suspect to be superfluous work) but on rules devoted to duplication (an operation that looks unavoidable on λ-terms where occurrences of variables are more than unitary). So, what theoretical and general results do we have about **efficiency of sharing graphs and optimal reduction**? The only results in this sense is positive, although restricted to a case where optimal reduction needs no bookkeeping operation, i.e. the so-called abstract algorithm. Indeed, Baillot, Coppola and Dal Lago at LICS '07, considered a class of λ-terms of known bounded complexity (polynomial and elementary time) and analysed cost of normalisation using sharing graphs (including non-optimal strategy). The result [5] is that the cost of sharing implementation stays in the same complexity class. More recently, using the same restriction, but employing a direct syntactical comparison with a standard graph rewriting machine, Guerrini and I showed [6] that the cost of sharing implementation (again, including sub-optimal strategies) is actually bounded by the first, up to a constant factor. This means that in such a setting, sharing reduction can only improve performances. 1. J. L. Lawall and H. G. Mairson, “Optimality and Inefficiency : What Isn’t a Cost Model of the Lambda Calculus?,” in ACM International Conference on Functional Programming, 1996, pp. 92–101. 2. A. Asperti, “On the complexity of beta-reduction,” in In Conference Record of POPL ’96: The 23 rd ACM SIGPLAN SIGACT Symposium on Principles of Programming Languages, 1996, pp. 21–24. 3. A. Asperti and H. G. Mairson, “Parallel Beta Reduction Is Not Elementary Recursive.,” Inf. Comput., vol. 170, no. 1, pp. 49–80, 2001. 4. A. Asperti, P. Coppola, and S. Martini, “(Optimal) Duplication is not elementary recursive,” Information and Computation, vol. 193, 2004. [Publisher version, restricted access](http://www.sciencedirect.com/science/article/pii/S0890540104000884) 5. P. Baillot, P. Coppola, and U. Dal Lago, “Light logics and optimal reduction: Completeness and complexity,” Information and Computation, vol. 209, no. 2, pp. 118–142, 2011. [Publisher version, restricted access](http://www.sciencedirect.com/science/article/pii/S0890540110001604) 6. S. Guerrini, T. Leventis, and M. Solieri, “Deep into optimality – complexity and correctness of sharing implementation of bounded logics,” DICE 2012, Tallin, Estonia, 2012. [Authors' manuscript](http://ms.xt3.it/research/DeepIntoOptimality_2012-02-17.pdf)
I don't have time to look at it, but you should be able to use [this](http://www.haskellforall.com/2013/08/composable-streaming-folds.html) approach to do everything in one pass, sans the sort.
Sorry, that was sarcasm. I think these debates should be taken with an unhealthy portion of salt.
I'm not sure that counts. We're discussing that with /u/augustss in the neighbouring thread.
So the model explicitly favors random access array indexing, which your example uses but you've prohibited others from using. What's your goal here?
vim, tmux, stack. I've always wanted to get ghcmod running and integrated with vim, but there always seems to be some problem where ghcmod doesn't work. I perceive it to be very useful, and when I move to GHC 7.10 I will being giving in another go. Also, the [vim syntax plugin for shakespeare](https://github.com/pbrisbin/vim-syntax-shakespeare) is very useful for me.
But how can you claim constant time operations on words if you are not willing to give a size bound on the words? You can assume that, but it's not a realistic machine model. 
Yeah, sorry, O(m).
That's not totally accurate AFAIK– the original author was the one to deprecate them (quoted from Google+): &gt; I'm declaring bug bankruptcy on system-filepath and system-fileio, and deprecating them. &gt; &gt; These libraries were written to support Linux file paths containing non-Unicode byte sequences, which was varying degrees of broken in GHC 6.10 through 7.2. GHC 7.4 greatly improved support for these type of paths, to the extent that system-filepath and system-fileio were really just a grossly overbuilt compatibility shim for people that needed to support old GHC versions. &gt; &gt; Since the number of library developers who still need to support GHC &lt;=7.2 is approximately zero, it's time to get rid of the shim and migrate back to the standard library. &gt; &gt; This is something I'm very happy about, because I can stop researching obscure undocumented Windows-only UNC meta-prefixes and go back to pretending that Windows doesn't exist. &gt; &gt; If anyone out there is interested in maintaining these libraries, I'd be happy to transfer maintainership. 
I'm still learning, but I use [IHaskell](https://github.com/gibiansky/IHaskell) a lot to prototype, which provides a Haskell kernel for the IPython/Jupyter notebook. It also integrates with hlint to give you suggestions on code improvements, invaluable for a beginner. I need to set up a proper Vim environment, but I haven't had a chance to properly fold one into my current vimrc.
First of all, thank you for telling me these approaches in detail kindly :) I'll try Approach and keep my mind this idea; "get the performance and caching of a typical cabal sandbox". 
Just plain emacs with haskell-mode.
Does haskell-mode work with stack? 
That's not quite within the spirit of the question, but thanks for the pointer anyway!
Ah, it would probably have been less confusing if you'd made that more clear earlier on. :P
I looked at `TcErrors`earlier. What kind of info is present in the `Ct` data type? The only information I could find said that it was purely information about type classes, not inference errors or the like.
I use * vim with only a few plugins (nothing haskell specific) * cabal (haven't had time to try stack yet) * ghci * bash GHCi is great, and cabal is fairly decent. I handle many many languages in my every day job and vim is great for that (everything is text, vim handles text, fin). In any case, I've never been fond of fancy autocomplete (beyond C-n) and GHC is fantastic enough to facilitate prototyping (holes, defer-type-errors, etc). And bash well, it's just amazing.
I use emacs with haskell-mode, but with a few additional packages: * https://github.com/chrisdone/haskell-flycheck This provides a syntax checker for flycheck that uses the current GHCi process spawned by haskell-mode for syntax checking * Plain company mode, with no extra haskell plugins. By setting 'completion-at-point-functions to '(haskell-process-completions-at-point), the completion will uses GHCi to complete imports, language pragmas and names. By relying only on the GHCi for all haskell functionality, I only need to configure ghci to be able to find the packages. So it works easily for cabal (just use cabal repl for the process) or nix (i use a wrapper script to launch ghci inside a nix shell)
-haskell platform -eclipseFP Send help
What is stack ? Can I get a link for it ?
You can simplify things even further: forM_ numbers $ \i -&gt; do response &lt;- get (problem i) putStrLn (response ^. responseBody . to decodeUtf8 . title) threadDelay 2000000 Edit: `s/title . traverse/title/`
Does `traverse` in this case work as a `concat`, merging all available `h2` tags to a single string?
https://github.com/commercialhaskell/stack - it's rad. It's an alternative to cabal-install that reads cabal files and works much more predictably. Only downside is it doesn't integrate with ghc-mod and hdevtools. 
I feel all the commenters in the thread but me failed to pick up the sarcarm. (that or I am seeing things) 
kqr: `(^.)` will use a `Monoid` on the result type to smash together the 0 or more answers you can get if you are using it with a `Traversal`, but won't demand a `Monoid` from you if you have a `Lens` or `Getter`.
IntelliJ + Haskforce (which uses HLint and ghc-mod). I like the idea of moving to emacs at some point, but the intuitive out-of-the-box functionality I get with an IDE has been too much of an attraction so far. The functionality I require for any serious development is: - Jump to definition - Show usages - Good searching + find and replace - Show type - Autocomplete - On-the-fly syntax and type error highlighting - I also like having the list of project files and also list of symbols in the current file IntelliJ + Haskforce provides these (except show usages and list of symbols), although some can be flakey. Having said that I have not managed to get emacs into as reliable a state after many hours of configuration. Haskforce is under active development, and I always eagerly anticipate new releases.
I'm not sure what this means, sorry. Bear in mind my understanding of lens(es) is mainly operational.
Normally you use `(^..)` or `(^?)` to access the result of a traversal, the former gives you back a list of all results, the latter gives you back the first match in a `Maybe`: &gt; ("hello","world")^..both ["hello","world"] You can use `(^.)` to read from a `Lens`: &gt; ("hello",5)^._2 5 but when you feed it a `Traversal` &gt; ("hello","world")^.both "helloworld" it asks you for a `Monoid` to glue the answers together, here `String = [Char]` has a `Monoid` which appends the results.
Ohh, that's neat and makes a lot of sense. I did not know that!
`ghc-mod` is a must have. `hasktags` is really good. `hoogle` I use it sometimes, but not that much. The atom plugins are great even, especially the linter. I don’t use `haskell-ide` though. `ghci`, of course.
I like this a lot, but I have some gripes. * Generally, I don't want my compiler error messages to be novel-length. Stuff like "As I infer the types flowing through your program" is just fluff. I'm never going to read that. Tell me the details or go away. * Type mismatch could be greatly improved by tagging which type came from the function's type and which type came from the argument you provided. GHC does this, but it doesn't look like Elm does. * I know that all elements of a list should have the same type. I don't need you to justify your type system in the error messages: "so that we can iterate...". It's not a blog post or compiler documentation. I just want to know what's wrong with my code. * You should probably also print the code syntax highlighted. I find this to be a big improvement also.
absolutely fantastic, thanks!
Wow, this is probably the best short explanation of this work I've seen/heard you give!
I use. * stack * tmux (I run emacs in a terminal window) * emacs * haskell-mode * structured-haskell-mode * https://github.com/chrisdone/haskell-flycheck for flychecking * hindent * haskell-dash * hlint * dipping my toes in the water with stack-ide With stack my tool chain has become a lot less brittle. edit: fix haskell-flycheck link 
Good point. I think the idea is that you take something and you transform it using functions. The functions are named after what they do, not what comes out.
 &gt;&gt;&gt; Data.Foldable.toList $ accumArray (+) 0 (0,49) $ (,1) &lt;$&gt; [1,3,1,1,1,1,2,2,3,2] [0,5,3,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0] uses the ability to construct an array in linear time in the number of elements and updates using `accumArray`. Does this use mutation behind the scenes? Yes. Can you observe that fact? No. Alternately you can embrace an MSD or LSD radix sort using discrimination, etc. `n` is included in your problem size. You have to build an array of that size anyways, so a counting sort or radix sort doesn't change the asymptotics.
You've included `n` in your problem size already by including the construction of an array of that size in the problem, so the radix introduces no new hidden time bound. You had to touch all that memory to zero it anyways.
There's a Ruby library for authorization that I really like called [pundit](https://github.com/elabs/pundit). I've been thinking about how to port it over, and I think type classes might be able to cover it, though I haven't played with the ideas as much as I want to. 
I still have a bunch of work to do in this space: `sorting` is offline, since you can always get something earlier than one of your equivalence classes. I'm currently working on incorporating techniques from burstsort to improve the cache locality of `sorting`. `grouping` is now based on an online technique which effectively forces me to build a trie in memory by mutation as it runs and then use [promises](http://hackage.haskell.org/package/promises) to drive the computation on a demand driven basis. I'll also likely change the way the internals of grouping works for integers to use array-mapped trie techniques rather than the big mutable tree I do now, this would let me win a great deal (3-4x) on constant factors. I may wind up having to add back another form of offline grouping to get better constant factors. The memory footprint for online grouping is _way_ higher than the old offline footprint, but productivity means you can drop it in as a replacement for `nub` for all usecases and just go linear.
"What are you so excited about?" "We're getting linear nub!"
Sure thing! I'll try to get a basic implementation together and write up my thoughts.
Replied [over there](https://www.reddit.com/r/haskell/comments/3bqlis/how_do_you_compute_a_histogram_in_a_pure_language/csosz7t).
I'm sorry :(
How do you use hasktags? Integrated with some editor, if yes, which editor?
- vim with https://github.com/raichoo/haskell-vim (only does syntax higlighting) - I'm running `cabal repl`, `stylish-haskell` and `hlint` manually from a terminal.
For what it's worth, I was thinking about this earlier today and together with @edsko came up with what seems like a reasonably painless way of incorporating Idris-style metadata into the `SDoc` we have today. There is a mini-proposal on [#8809](https://ghc.haskell.org/trac/ghc/ticket/8809#comment:3). It's a small first step, but it at least sounds like the impact wouldn't be too terrible.
* vim. No haskell-specific plugins, just the default Haskell syntax highlighter. * the hoogle website. Sometimes I use FP compete's version of hoogle when the default one can't find what I'm looking for. * ghci * I've started using ghcid again, I like it.
Vector is pure. All functional languages compile down to some program that actually mutates memory. The difference is, they have been carefully written so that the interface is pure. 
The equivalent functional data structure is a binary tree. In Haskell, both reading and updating a value in a binary tree is O(log N) thanks to sharing.
My question wasn't dictated by a practical need. I'm trying to figure out some stuff in theory, under the artificial constraint that you can't use any "under the hood" mutability except the limited mutability provided by laziness and thunks.
What do you use hoogle for ? Can you use it to search for usages (of a given identifier) ?