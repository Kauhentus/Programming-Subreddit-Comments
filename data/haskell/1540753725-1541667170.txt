 type family IsFunction (a :: Type) :: Bool where IsFunction (_ -&gt; _) = True IsFunction _ = False
Well, I guess my _current_ workflow is quite similar, but it is my desired workflow which is different. Currently, I create slides using my [git-slides](https://github.com/gelisam/git-slides#readme) tool, I then record a screencast in which I go through the slides and narrate them, and then I edit the result in Final Cut Pro. The narration is the hard part for me, which is weird because I think I do a pretty good job when presenting to a live audience. I guess talking in front of my computer isn't stressful enough to trigger my last-resort improvisation skills :) One thing which has worked for me is to start a sentence, take a break to think about what I'm trying to say, utter a few more words, take another break, and so on until the sentence is finished. This takes quite a bit of edit to make it sound like a natural sentence, as it turns out that the amount of silence between words and sentences varies depending on the context, and humans unconsciously notice when the gap is incorrect. I like creating the slides, but I really hate editing the videos, which is why I post so few videos on haskellcat.com despite having a lot of things to say. I am hoping that creating a specialized video editing tool will allow me to pump out lots of videos once it's finally ready. So here's what my ideal workflow would look like. Since I already have slides, I think it would make more sense to tie the audio to the slides, rather than to record the screencast and the audio separately. When recording the audio, I still want to use the sentence-separated-by-breaks approach, but I would like to be able to listen to the sentence I have recorded so far before recording the next bit, and I would like to use binary-search to the find the right gap between fragments. Finally, since I have slides, I don't think I need to record a screencast at all, I should be able to generate the full video from a sequence of slides with associated audio. So I'm thinking of a terminal-based audio editor which would basically only support one action: play what has been recorded for the current slide so far, then record a new fragment to add at the end, then binary-search for the right gap between what has been recorded so far and the new fragment. So far I made a tool called [`interplay`](https://github.com/gelisam/rerec#interplay) which can play the fragments with the right gap between them (the hard part is that it needs to support negative gaps in order to compensate for the silence which is already included in the recording). Congratulations on finishing your project, I always get distracted by something newer :)
Your function has a very strange result for an input that looks like `["aba", "bcc"]`.
There's a few shortcomings with this approach. Consider the action "move the second cursor back one character": * You lose safety, since you can't talk about "the second cursor" without indexing * You lose performance, since you need to `init` the middle string. You can recover some performance (and type precision) with something like this: import qualified Data.List.NonEmpty as NonEmpty newtype RevList a = RevList {unRevList :: [a]} deriving (Eq, Show) newtype FwdList a = FwdList {unFwdList :: [a]} deriving (Eq, Show) revList = RevList . reverse fwdList = FwdList data Cursors a = Cursors (NonEmpty.NonEmpty (RevList a, FwdList a)) deriving (Eq, Show) example = Cursors $ NonEmpty.fromList $ [(revList "My tex", fwdList "tu") ,(revList "al ", fwdList "example")] main = print example But it's still unsafe to refer to a specific cursor. If you're okay with always having a single "main" cursor, you can do something like this with zippers: import qualified Data.List.NonEmpty as NonEmpty newtype RevList a = RevList {unRevList :: [a]} deriving (Eq, Show) newtype FwdList a = FwdList {unFwdList :: [a]} deriving (Eq, Show) type Span a = (RevList a, FwdList a) data Zipper a = Zipper (RevList a) a (FwdList a) deriving (Eq, Show) revList = RevList . reverse fwdList = FwdList type Cursors a = Zipper (Span a) example = Zipper (revList []) (revList "My tex", fwdList "tu") (fwdList [(revList "al ", fwdList "example")]) main = print example This makes operations such as "next cursor" or "insert character at all cursors" easy. If you add a flag to cursors for something like "active for insertion", I think you can get all of the behavior of `evil-mc`. I wonder what `xmonad` and `yi` do/did for these kinds of problems. /u/NorfairKing2 Have you looked?
I’m mostly pleased with it. I remember being quite frustrated with iMovie but don’t remember why. The main feature for me, but for a different kind of videos, is the multi-cam support.
replicate will allow you to do that. I'm not at a computer right now but it should just be map (replicate . uncurry)
This might fit better in /r/haskellquestions. Can you post the question text? I don't see an issue with the code you posted, and how it relates to takeWhile and iterate. Hmm.
Now I can't use these functions. It should be similar to the previous function
Ah sorry, I wasn't even aware of the sub. The full question text says "Reimplement each of the following functions in a more idiomatic Haskell style. Use wholemeal programming practices, breaking each function into a pipeline of incremental transformations to an entire data structure. Name your functions fun1’ and fun2’ respectively." This function was given in the question with the hint "Hint: For this problem you may wish to use the functions iterate and takeWhile . Look them up in the Prelude documentation to see what they do." I looked up the functions, I understand _how_ I could use them to rewrite it, but I don't see why the given implementation is considered not to be idiomatic
What is preventing you from using those functions?
No effects like that, I'm afraid. I've considering adding an "export to X project format" feature so that you could do such post-processing in video editor X afterwards, but that's not supported yet.
Thanks for the detailed reply! I see how you're workflow makes your requirements very different. Would be interested I seeing what you come up for your work. Thanks for the kind words, by the way!
It's a homewrok. Both functions I just can't do the second one. I have another function make :: Int -&gt; a -&gt; \[a\] make n x = if n &lt;= 0 then \[\] else x : make (n-1) x it does: generate 3 2 = \[2,2,2\] n length list with x elements I was thinking to use this make function. &amp;#x200B; funtion2 :: \[(Int,a)\] -&gt; \[\[a\]\] function2 \[\] = \[\] function2 (x:xs) = make length (head x) tail x : function2 xs but it doesn't function
Hello! I'm big into games featuring procedural generation and would like to kinda have a "rapid prototyping" sandbox to try out things. I love Haskell as a hobby language, and thus selected it and wrote my noises generators, filters etc in there. Very nice to do composition and stuff! But what I am currently lacking is just a simple library that would allow me to just display a window with an image, kinda like processing does, so I could actually see what I'm doing. Is there something like that available? For example, processing just allows you to change the pixels on the canvas and receive inputs, so very simplified. Thanks in advance!
The "unidiomaticness" of your code might mostly be about the `even n` case, which is not tail recursive and would build up a large stack of unevaluated values (disregarding -XBangPatterns, etc.). --- To use `iterate`, one option would be to write `fun2` to make only a single "step" instead of an entire recursive call tree (also nicely avoiding the entire tail recursion).
&gt; but it doesn't function What's the error?
Shouldn't we make the distinction between function application and functions?
Hi. What is the best way you would recommend for a noob to learn programming? I have those homework coding practises but i dont think they are sufficient... should i also try learning by reading codes written by others or writing more codes by myself?
I think what you're being asked to do is avoid the explicit recursion (many of the higher order functions in the prelude are a factoring out of different sorts of recursion schemes). This is normally a good idea and leads to more understandable code etc. But I would describe the code here as idiomatic and am skeptical the suggested refactoring will be better, but you can judge for yourself. One thing to ask is whether e g. The expression `(iterate ... n)` might be useful perhaps in another function or during debugging; we like to be able to write programs by composing reusable pieces (these are benefits of laziness and higher order functions). But sometimes a simple recursive definition is clearer
Dr. Yorgey is an very well-established and constructive voice in the community. So even as a long-time Haskeller myself, I would quickly defer to him on what's "idiomatic". Even so, the vocab "idiomatic" seems problematic in the learning context. His very next sentence (quoted in your other comment here), though, elaborates in a clear and meaningful way: "Use wholemeal programming practices, breaking each function into a pipeline of incremental transformations to an entire data structure". In the context you gave, I would concretize that guidance to "start with some infinite list, incrementally transform that to the infinite list you want, then reimplement `fun2` by accessing the relevant element from that list." (Or some such… I haven't carried out the exercise.) I would agree that "that feels more idiomatic" to me. In actual practice, that might matter or not, depending on context. The "least" idiomatic thing about `fun2` is its impenetrable name :) But snarky teasing aside, it's the use of direct recursion; that's ultimately what he's characterizing here as non-idiomatic. Especially this somewhat exotic recursion we have here that isn't "obviously" well-founded (ie the argument isn't always "decreasing" as in structural recursion, eg). HTH! Welcome!
I've rewritten the relevant sections to address these objections
I personally think GHCJS isn't the best for such real-time applications, even though it provides such beautiful client-server code sharing and abstractions. The performance isn't there yet. If you can't wait for the WebAssembly version, then I would sadly recommend using vanilla JavaScript / TS. Sounds like a fun project to use a Haskell backend for though!
I haven't tried ghcjs, but [Haste](https://haste-lang.org) worked [well for us](https://github.com/gelisam/ludum-dare-34)!
This looks really cool! I wonder what a boilerplate comparison to MTL with deriving-via looks like.
Very cool! How does Haste compare performance-wise to GHCJS, and is it still actively developed?
Was able to get 60fps binding to three.js, http://threejs.haskell-miso.org/. Albeit there isn't much logic here, I think you can get close to the performance you want if you keep most of the game state in JS land (in a JSVal) and newtype it in Haskell land. This way you can define functions that operate over this newtype that alter the state, in IO. Source: https://github.com/dmjio/miso/blob/master/examples/three/Main.hs
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [dmjio/miso/.../**Main.hs** (master → 07e215c)](https://github.com/dmjio/miso/blob/07e215c112dfb5f6d830444e2751878371ca2b07/examples/three/Main.hs) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply e8n3mzs.)
I have been able to run GHCJS apps very well on my phone from the same time period, as well as on my macbook. I have not had any issues until this realtime multiplayer game which understandably tends to have more perf issues than other web apps. What browser are you using? I hear Firefox is the worst of the bunch for GHCJS apps. Can you try out a GHCJS website [polimorphic](https://www.polimorphic.com/) that I am one of the developers of and see how it runs. I'm mostly focused on browsing performance after everything is loaded initially, as the initial page load is definitely slower than I'd like due to some known issues (insufficient server side rendering and third party geocoder performance issues).
Have you considered Elm? Its supposed to be pretty fast. I'm writing something
Thanks for the info! So right now basically all of the logic is reading in state diffs from the network, applying them to the local state, and then repeatedly rerendering with this changing state. As well as a small amount of client only state and logic that will evolve over time. I'm not really too sure how to structure the app to have it in newtypes without losing the parts I care most about, namely the type safe diffing and serialization, and shared functions between the backend and the forntend. The actual rendering is mostly done with a JS library at the moment, so the above is mainly what I am focused on.
Elm to me seems like a good react-style framework, and I would consider using it for that if I wasn't so happy with Miso (and then wasn't happy with reflex-dom as that is what I would go to next). But it seems like a poor general purpose programming language. The lack of ad-hoc polymorphism is my main focus, plus the fairly wimpy type system when compared with Haskell/GHC's. It does have some cool features that I like though.
Color me impressed. polimorphic runs no problem and in fact performs better than most other websites I visit. This has not been my experience with ghcjs apps, and I'm curious as to the discrepancy. Is it running reflex? Are you doing anything clever to get the perf? Was my old company just really bad at making ghcjs apps?
Thanks for the info! The output size itself I am not overly concerned with, the game is not going to be practical on mobile anyway and a couple second load time for a game is pretty acceptable. So my main focus is the runtime performance once everything is up and running, particularly as time goes on, as the user will ideally be on the same page not refreshing for quite some time.
You need `NoImplicitPrelude` to hide instances that some from Prelude. *Any* import statement for Prelude will bring in all the instances.
Usually hiding some symbols and qualifying some works better. `import Prelude qualified` or even `import Prelude as StandardPrelude` is probably a lot simpler than a LANGUAGE pragma. That said, instances don't have names, so there are some cases where you'd need `NoImplicitPrelude` (and also not import anything from Prelude).
Haste worked well for me. Maybe try that?
Can you elaborate? GHCJS works for me for a lot of tasks too, it's specifically the realtime multiplayer game aspects that make me most nervous. Do you know how active Haste development is also, and how the perf compares to GHCJS?
So the game logic is mostly keeping track of a subset of the real server game state relevant to the client and rendering it. Specifically we occasionally send a full state over the wire, but most of the time we send a calculated diff instead, at around 100 diffs per second via websockets. Which then gets applied to the state. The client also has some state of its own. Events are locally converted into client state changes and into messages to the server. This client state + server state combination is also sent to a renderer, which is largely written in JS via Pixi.JS, which then renders everything.
I think you won't be treated badly by ghcjs on that front. It generally seems to "just work" and Luite has put a shocking amount of clever engineering into industrializing it.
Why would you want multiple cursors?
What are "compact regions"? (I have no idea of GHCJS).
Shouldn't that be `x -&gt; e` if you're talking about the exponential? Also, would you happen to know where I could find some reading material on ADT derivatives? I've never encountered that before. 
http://strictlypositive.org/calculus/
Compact regions are actually a GHC thing, not specific to GHCJS. You can add values to a region, and all the value's children will be added as well. If any value in the region is live to the GC, the whole region is considered live. This means the GC will never have to traverse the whole region; it only has to traverse from the GC roots until it hits *any* element of the region, and the entire region will be instantly marked as live and skipped. That means the pause time caused by the region is only as great as the number of references into the region, not the size of the region. So if you can move 90% of your memory usage into a region, you might see a 90% reduction in pause times.
Tries to use the browser GC first then handles the rest iirc https://github.com/ghcjs/shims/blob/master/src/gc.js
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [ghcjs/shims/.../**gc.js** (master → de9560e)](https://github.com/ghcjs/shims/blob/de9560ee1fb8d1ca58c12da20c71a778eb08f3db/src/gc.js) ---- 
Can't cover every your question but will answer on some of them. &gt; Did cabal (backpack?) make stack obsolete as far as avoiding dependency hell? `cabal-install` has implementation of _Nix-style local builds_ so dependency hell (_cabal hell_ from the past) is not a case anymore. Just use commands with `new-*` prefix and you're good :+1: You can read the following blog post to compare basic workflows with `cabal` and `stack`: * https://kowainik.github.io/posts/2018-06-21-haskell-build-tools Not sure that `stack` is obsolete. Backpack is definitely great! But a lot of people are still using `stack`. Shameless plug: if you want to read about Backpack, you can read my blog post with short tutorial: * https://kowainik.github.io/posts/2018-08-19-picnic-put-containers-into-a-backpack &gt; How has tooling improved? I'm still using `ghcid` to get fast feedback from the compiler. I heard that VSCode, Atom and even Intellij IDEA have pretty good plugins. But I've also heard that they work very slow on very big projects. &gt; Any new fancy features? GHC-8.6.1 got `DerivingVia` and `QuantifiedConstraints` which are both great features! Even less boilerplate code and even more type power! &gt; Is String finally becoming obsolete? Are partial functions on their way out of the standard Prelude? I don't think `String` will ever disappear from `base` (as well as partial functions like `head`). But you can always use alternative preludes that doesn't have such problem. I'm working on the alternative prelude called `relude` and I wrote short comparison with `protolude` recently on Reddit here: * https://www.reddit.com/r/haskell/comments/9ptae0/what_is_the_easiest_way_to_always_use_text_and/e84ldo3/
Well, I was thinking about multiple cursors that you can have in Sublime Text or Vim. You can't really move specific cursor but once you move one you move all. And if you hit the end or the beginning of the list they'll just naturally get merged together.
i agree that OOP is bad; short reasoning : it fundamentally involves state mutation and thinks that _everything_ should be dispatched dynamically [my full article about this](https://libeako.github.io/c/ID_205033498.html) avoiding of OOP i advise not only for schools, but for everyone; OOP does not have any sense for anyone IMO; of course it is especially harmful for novice programmers, who are easily mislead and their learning process is made largely more difficult by OOP, not only unnecessarily, but for bad
A better way of think of the derivative is using a sort of "linear type" `Hole` with the requirement that there must be one and only one value `hole` of that type. Then the derivative of `f t` is `f (Either t Hole)`. In the case of a `Bag`, this means that the `hole` must be a member of the `Bag`, but that it can't be repeated. Due to the nature of `Bag`s as a collection, this provides no information that a `Bag` without a hole wouldn't also have. Contrast with, say, a list, where the hole would provide information by picking out a specific spot in the list.
elimination of a Variant is possible only via pattern matching? in that case : the lack of exhaustiveness check feels to me a showstopper; but : i suspect that writing a general eliminator function [1 input continuation for each case of the variant] should be possible; in my opinion : eliminator functions are good alternative of pattern matching
Don't fret about style in Haskell - besides avoiding state, effects, partial functions - there's no such thing as idiomatic Haskell. Don't be past me, don't waste a _lot_ of time on that.
Which version control system do you prefer and (if you have the time) why?
I read through just the first part and then skipped around and browsed. This looks very nice! Thanks for doing it. Two small naming issues that I noticed (in my quick and shallow blast-through) which could cause some confusion: 1. You used the name `project1.cabal` for your sample cabal file. The name `cabal.project` is magical in cabal - it's the project-wide cabal file. Backwards, I know, but that was enough to confuse me. Maybe some other name would be better. 2. You have a chapter called "The Sequencing Operator". I thought it would be about `seq` whose name is short for "The Sequencing Operator". But it's not, it's about `&gt;&gt;=`. It might be a good idea to slightly change that chapter title.
Regarding the notions of types in programming languages: this paper [https://arxiv.org/abs/1510.03726](https://arxiv.org/abs/1510.03726) presents an historical reflection and argues that the connection between the implementation and logic came gradually during the 60s and culminated with Curry-Howard in the 80s. &amp;#x200B; &amp;#x200B; &amp;#x200B;
It’s a common feature of editors. See e.g. SublimeText or Emacs; you select many things at once (lines or symbols) and perform editing operations all at once. It’s like a more user friendly keyboard macro. Demo video: https://m.youtube.com/watch?v=jNa3axo40qM
...Well, that does remind me: I personally have found significant benefit, especially as the night gets later and later, in *not* taking advantage of Haskell's generous permission to end names with primes. I'm a beginner too, so I don't know if that is considered "idiomatic." (I think I've only seen one pedagogical author agree with me; I can't recall. Perhaps I encountered Yorgey himself writing somewhere.) But it should be.
No idea.
Yes we can also use continuations as you suggest. It was an oversight as it was documented for EADTs but not for Variants. Fixed: https://docs.haskus.org/variant/safe_pattern_matching.html 
A bit of both. The browser is doing its own thing, but there are some things that ghcjs does that the native gc can't finish for it, like supporting GHC finalizers, throwing exceptions at computations blocked on unreachable MVars, etc. To handle them it has to occasionally walk the heap itself.
Let's take `(x, x)` as an example, which is the same as `Bool -&gt; x`. The `(x, x)` form is equivalent to `x*x` while `Bool -&gt; x` is equivalent to `x^2`, which is the same.
Let's say, hypothetically, that I don't know anything about effect systems, where would I go to understand their motivation? While looking at other effect system implementations, they are positioned as an alternative to MTL, is that accurate, or is there more?
It's just a single datapoint, but almost every function in our codebase starts with `= do`, so yeah. Probably pretty common. What's pretty confusing to me is that you don't seem to have a problem with monads in general, just the fact that `do` notation hides the explicit `&gt;&gt;=`s. The thing is, it does it in such a direct way that I have trouble seeing where the confusion arises. 
How might TLA+ fit into this process?
I believe in the field people mostly care about readability and clarity. So I just use the notation that results in the most readable code. If I just want to glue a couple of statements, I can use bind, but I find do-notation clear in most cases. copy f t = readFile f &gt;&gt;= writeFile t -- ok, readable parsePair px py = px &gt;&gt;= (\x -&gt; fmap ((,) x) py) -- meh parsePair px py = do -- ok, readable x &lt;- px y &lt;- py return (x, y) parsePair px py = (,) &lt;$&gt; px &lt;*&gt; py -- even better parsePair = liftA2 (,) -- perfect
The intuition is that somehow nullary functions just have type of their result. You have a func from a -&gt; b, but there's no a involved so now it's just a b. It's not right, but it's plausible, as I said before.
Re: `extend`. If you use that order of arguments it is clearer that extend, just like `(&gt;&gt;=)` is really just a special kind of function application: ($) :: (a -&gt; b) -&gt; a -&gt; b fmap :: (a -&gt; b) -&gt; f a -&gt; f b (&lt;*&gt;) :: f (a -&gt; b) -&gt; f a -&gt; f b (=&lt;&lt;) :: (a -&gt; f b) -&gt; f a -&gt; f b extend :: (f a -&gt; b) -&gt; f a -&gt; f b
\&gt; I find this style incredibly confusing. &amp;#x200B; I avoided it at first, too, but it turns out to be pretty essential. I'd recommend taking a glance at \[How to Desugar Haskell Code\]([http://www.haskellforall.com/2014/10/how-to-desugar-haskell-code.html](http://www.haskellforall.com/2014/10/how-to-desugar-haskell-code.html)) by Gabriel Gonzalez. Do notation is down at the bottom. I think you'll probably find out that it looks worse than it really is. &amp;#x200B; \&gt; I hope everyone will consider toning down the do-material greatly in their tutorials. &amp;#x200B; Or at least provide both versions. I recall a similar thought when I was struggling with the learning curve. I'll keep it in mind if I &amp;#x200B; \&gt; But will it be "idiomatic Haskell" in the field \[...\] to avoid it myself? &amp;#x200B; Probably not. There are instances where it's much clearer to use it than not. They \[pop up\]([http://hackage.haskell.org/package/transformers-0.5.5.0/docs/src/Control.Monad.Trans.Maybe.html#line-156](http://hackage.haskell.org/package/transformers-0.5.5.0/docs/src/Control.Monad.Trans.Maybe.html#line-156)) all over. I think you may be making a bigger deal about it in your head than it is. My prediction: with a little practice (and probably only a little) you'll find it's actually very convenient and wonder how you ever got on without it. &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B;
Yeah, there are plenty of situations where we use `&gt;&gt;=` directly. It often lets you avoid naming a new variable.
I would find that to be somewhat confusing, personally. `do` notation is a light weight syntax that works quite well for sequencing monadic computations. You can always mentally rearrange code like: do foobar a &lt;- runBaz runQuux a b &lt;- getThing a return (a + b) into the equivalent `&gt;&gt;= \x` function calls, like so: foobar &gt;&gt;= \_ -&gt; runBaz &gt;&gt;= \a -&gt; runQuux a &gt;&gt;= \_ -&gt; getThing a &gt;&gt;= \b -&gt; return (a + b) The first seems cleaner and simpler to me. What do you find challenging about `do` notation?
Yes I bought it from the Spanish shop and it arrives in the 31st!
Not sure what you mean by "fit into this process", but TLA+ specifications are state machine specifications. See e.g. the following [paper], by Lamport, (https://www.microsoft.com/en-us/research/publication/computation-state-machines/) for an explanation. 
Looking at the series definition of exp: exp(x) = 1 + x + x^2/2! + x^3/3! + .. As a data type, x^3/3! is a list of 3 values, but where their order doesn't matter. So it is a Set with 3 elements. That means that exp(x) is a set with any number of elements. In other words, `exp(x) = Data.Set.Set x`. I don't think you can get a constant e from this, since it is not true that `Set a -&gt; b = Set (a,b)`, and `Set 1 ≅ 2`.
Ending a term with a `'` is something I might do as a way of expressing "`x'` is an `x` that I have modified in some way" where that modification may not be worth describing. It's the same feeling that determines whether I name a parameter something like `x` or `a` instead of giving it a meanginful name -- is there a meaningful name? Consider the definition of `modify :: (s -&gt; s) -&gt; State s ()`: modify :: (s -&gt; s) -&gt; State s () modify f = do s &lt;- get let s' = f s put s' I could instead write `modifiedS`, but that seems overly verbose, and the `'` convention is common enough that this is pretty clear. The other common use for `'` suffixes is to indicate a stricter variant of a function -- eg `foldl` vs `foldl'`. I think I would prefer to see `foldlStrict`, but as a matter of convention, it's not worth arguing about imo.
No argument there! Ah but you see what you have done. You have said that `extend` has the particular virtue of having this type signature that is so elegantly comparable to function application itself. But then so does `(=&lt;&lt;)`, which you have conveniently reversed from the standard idiom as well! My original point was not that `=&gt;&gt;` ought to be facing in "diagrammatic" instead of "functional" order. (Indeed, since that order *was* the one selected by our forefathers for function application, I find it rather more comfortable to stick with it when reading theoretical literature, and hate it when authors "helpfully" write things the other way round.) My point was that the two orders are *mixed*, with monadic bind facing one way and its counterpart the other, and this is enormously confusing for a beginner who has enough to struggle with when trying to understand the dual concepts. The only possible justification would be if there is a good strong "field service" reason to make `extend` face the way it does, just as there was enough such convenience to give `&gt;&gt;=` diagrammatic order in the first place, even though that itself did make it slightly more difficult to grasp "monadic bind" at the start as you point out.
I seem to be a real magnet for downvotes, which is very much not the norm on this sub. I do not want to be a bad user, though at this point the areas in which I can contribute are a bit limited. If anyone can help me understand why this post may be inappropriate I would greatly appreciate it! It will help me become a better user.
Oh thank you!!! Excellent and very helpful comment; and that link is one of the very best and most useful pieces of literature I have encountered in the entire Haskellverse! What an incredible contribution by Mr. Gonzalez! Everyone should read this. The "bad" news of do-notation's pervasiveness has certainly been tempered by the news that someone else went through it (I am not taking crazy pills, or we both are) and found it less scary than it looks. I will learn to stop worrying and love the do-blocks!
In JS I get good performance by allocating object pools up front when developing an engine and using Immutable.js; it tends to prevent GC pauses by avoiding fragmentation on the heap. &amp;#x200B; I haven't tried writing an engine in Haskell+GHCJS myself but if you can't find a way to make it work on the front-end of your game you \_could\_ still share types from the back-end by generating your TS definitions from your Haskell ADTs using generics: [http://hackage.haskell.org/package/aeson-typescript](http://hackage.haskell.org/package/aeson-typescript)
I'm shooting blind here, but your post could be read a bit like a troll post in style: - very hyperbolic self-deprecation - trying to hide "judgy" words behind opinion ("think [...] awful") - prescribing others what to do (while also (hyperbolically) claiming to be a beginner) To be fair, it also reads like rather advanced english, good job on that! However, that can also be quite intimidating (much like very abstract math for me), perhaps evoking an image of superiority. That's just what I managed to devil's-advocate up, I don't actually feel this way.
&gt; I am not taking crazy pills, or we both are Both, I'd wager. And yeah, Gabriel's got a ton of great content on his blog. Good luck on your journey! 
http://okmij.org/ftp/Haskell/extensible/exteff.pdf http://okmij.org/ftp/Haskell/extensible/more.pdf
Thank you! I appreciate your taking the time to help me. &gt; very hyperbolic self-deprecation Of course I do not actually think I am super stupid, any more than super smart; I'd barely thought twice about opening with what I thought was a throwaway joke. I now see that it was inappropriate, and seemed like I was making fun of everyone or something. I hope the sub gets the message that I am merely awkward, and intended no such thing. &gt; trying to hide "judgy" words behind opinion ("think [...] awful") &gt; prescribing others what to do (while also (hyperbolically) claiming to be a beginner) This is a very positive and chill sub; and I can see the need to be protective of that. But I have seen that people tend to have strong opinions (though not directed at individuals, as indeed mine were not) about various issues like design decisions in Haskell, the direction development is going, and so forth. I'm obviously in no perspective to have opinions on such matters, but this was more in my wheelhouse. If anyone's sentiment is that it's out of line for a noob to have such strong opinions, then frankly I feel a lot less sympathetic to that. No one should give off the impression that he's throwing his weight around, of course; but a noob's perspective has its own value too. You all have a lot of experience in teaching others, but I am less removed from being on the other end. I think if I have strong opinions on what works for me as an individual learner, that may be good data for others to use (alongside much else, it goes without saying) as they seek to improve their pedagogy. Once again, I appreciate your responding to my request to help me improve my online etiquette and generally become a better communicator; I certainly do understand you're merely taking your best shot at doing so, rather than saying you yourself ever thought I was trolling. ...Of course I've probably only been further ruining the atmosphere of this sub with yet more awkwardness by dwelling on this! But seriously...who trolls a *Haskell* discussion group? I have been surprised to learn lately that the programming community in general can actually get a bit edgy and dramatic from time to time. But I've certainly seen nothing of the kind in this rather highbrow corner of it.
&gt; a throwaway joke Also see [Poe's law](https://en.wikipedia.org/wiki/Poe%27s_law). &gt; I hope the sub gets the message that I am merely awkward, and intended no such thing. Pretty sure it's the case now. It seems to me that another thing in reddit (overall, not only /r/haskell) is that there are some angry / negative people that downvote a lot of things, but that lasts for a short time. &gt; This is a very positive and chill sub; and I can see the need to be protective of that. I think it's more like human nature, if any of the things caused the reader to feel intimidated or otherwise form a negative initial judgement it's too easy to continue that road and not challenge it (for "some random post"). Personally, I hate it and would rather be able to say how things appear to me (e.g. even just write "awful" without "think"), but it seems more productive to hide that better (e.g. "I'd like to question"), however strongly you feel. And if you specialize [Murphy's law](https://en.wikipedia.org/wiki/Murphy%27s_law) to potential interpretations of text... &gt; Of course I've probably only been further ruining the atmosphere of this sub with yet more awkwardness by dwelling on this! Anyone disliking this post is very unlikely to also read this comment, no worries. [Some trolling attempt at #haskell](https://gist.github.com/quchen/5280339).
**Poe's law** Poe's law is an adage of Internet culture stating that, without a clear indicator of the author's intent, it is impossible to create a parody of extreme views so obviously exaggerated that it cannot be mistaken by some readers for a sincere expression of the parodied views. The original statement, by Nathan Poe, read: Without a winking smiley or other blatant display of humor, it is utterly impossible to parody a Creationist in such a way that someone won't mistake for the genuine article. *** **Murphy's law** Murphy's law is an adage or epigram that is typically stated as: "Anything that can go wrong will go wrong". *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
Interesting perspective. I agree with the "sequential" bit for sure; that certainly goes to why `&gt;&gt;=` is indispensable for the student to learn in addition--so much closer to actual use of the concept in practice than `join`. I don't know that I can see at all, though, that the "higher-order-ness" appealed to by `join` is either particularly challenging *or in any way avoidable* as a concept to be grasped. So it's a natural transformation; so what, so is `return`. It's really no more sophisticated. Containers of containers, "flattening," etc. As I mentioned, instructors regularly appeal to the `join` intuition anyway; they just do it "informally," without laying it out with any rigor. That to me is a serious, and common, mistake; you end up thinking you are avoiding getting into "scary" "theory" when all you are doing is forgoing the clarity it could have brought to the table. I can't think of a way to explain intuitively what `&gt;&gt;=` "does" without the "join" concept--and, again, that's exactly what most authors do do whether they are aware of it or not. Also, I agree that the sequential concept is valuable as well. But the truly straightforward manifestation of that concept is `&gt;=&gt;`, which expresses the analog of function composition--whereas `&gt;&gt;=` "focuses on where the action is taking place" in such composition but has a weird type that cannot possibly be the most straightforward way to understand monadic operations. Of course characterizing `&gt;&gt;=`'s relationship with `&gt;=&gt;` is a bit less easy to grasp than its relationship with `join`. But `&gt;=&gt;` has a bit of a story to tell as well. 
This is going to sound mean, but you're getting downvoted because you're acting like you want to not bother learning a part of the language everyone uses just because you currently find it confusing, or perhaps don't like it. People are then probably imagining working with you, where you use the long monadic form all over the place for no reason other than you didn't want to learn something, and there's your downvotes. You seem to already understand monads so, it's a relatively tiny step further to understand do notation. Your post just seems pointless, the answer is yes, learn do notation and use it when it's appropriate like everyone else, you already knew this was the answer, right?
Exactly my thoughts about the do notation. I eventually came to be ok with it and can mentally parse it without much difficulties (I'm still a beginner in haskell) but I would like to see it less in tutorials that explain other things or when just learning about monads. Monad transformers explanations gave me headaches because the do notation was used in the tutorials which I read to understand them.
Hehe! I laughed loud at the opening but I couldn't get much further so I have no idea how it ends. I can't even stand to watch gentle old *Candid Camera* like our grannies do, let alone *Punk'd* or *Crank Yankers* or whatever; seeing "practical jokers" interact with people acting in earnest, who are earnestly trying to help, makes me cringe like squeaky chalk. Sorry Jerky Boys.
You're getting downvoted because your question is pointless, and you should be able to infer the answer. Yes, you should learn and use do notation like everyone else does, when appropriate. What will happen when you work with others and you don't use it when appropriate, and can't read it when others use it? It's like one of the core features of the syntax, what kind of question is it to come ask if you can get away with ignoring it "in the field", and will it be ok if I don't bother to learn it?
Thank you, I'll read these papers.
Miles better than JS/TS, though. :)
I personally found the kleisli arrow definition the theoretically most intuitive. The laws are much clearer: a &gt;=&gt; (b &gt;=&gt; c) == (a &gt;=&gt; b) &gt;=&gt; c f &gt;=&gt; pure == f pure &gt;=&gt; f == f Join is a nice contrast that clarified the difference to applicative forme. Though in practice `&gt;&gt;=` is easily the most convenient.
&gt; for the more conceptually straightforward bind-notation. I still don't see where the huge discrepancy in straight-forwardness in your mind is. The `do` notation is nothing but _very_ light syntactic sugar over `&gt;&gt;=`. It really only let's your skip a couple of glyphs and parentheses.
&gt; Is there any use for &gt;&gt;= in the field, by the way? Why waste space use lot line when one line do trick?
This one has a twist, unlike those TV shows (I guess?), try to continue.
Perhaps it's bike shedding, but I feel like a bit of formatting makes a world of difference: foobar &gt;&gt;= \_ -&gt; runBaz &gt;&gt;= \a -&gt; runQuux a &gt;&gt;= \_ -&gt; getThing a &gt;&gt;= \b -&gt; return (a + b) But I would still prefer do notation here.
Closure Compiler is definitely supported. We run everything through ADVANCED optimizations.
Cabal's new-build has solved the biggest problem that Stack was originally created to solve, but I wouldn't say that makes Stack obsolete. There are many things that the tools do differently, so there are still reasons to prefer either of them. The biggest differences are that new-build is not the default yet, and that Stack does Stackage by default and Cabal does solving by default. Though it is harder to do a solving based workflow with Stack than it is to do a Stackage based workflow with Cabal. Other than that the differences between the two are fairly minor IMO. I think the only other potentially major difference is that Stack will install the right GHC for you for each project, but I personally consider this minor since installing arbitrarily many GHCs on your system and choosing one with Cabal is pretty easy (plus auto-installation across distros is a pretty hard problem that Stack only *mostly* solves; it's usually more reliable to install GHC with your system package manager if available, but this isn't preferred with Stack).
&gt;Tutorials should IMHO just introduce return Post-AMP it's better to focus on *pure* and forget about *return*, IMHO. It doesn't behave like *return* in imperative languages anyway (i.e. break control flow), so the argument for familiarity is flawed.
Do notation doesn't require newlines 😛 main = do putStrLn "Look ma!"; putStrLn "No newlines!"
&gt; is inhabited by one value, absurd, so I guess it fits, but it feels a bit weird. This is wrong. `absurd` is not a value. For example, it's wrong to say the unit type `()` (`1` in algebraic notation) is inhabited by `()` and `absurd` and thus has 'two' values. Similarly, it's wrong to count `absurd` as a value of `Bag`. In reality `Bag Void` has exactly one value `mempty`, which is the bag with no elements. If we look at the definition of one particular implementation of the [Bag data type](http://hackage.haskell.org/package/bag-0.1/docs/Data-Bag.html), this corresponds to the `Empty` constructor, which is a perfectly valid, non-bottom member of `Bag Void`. Of course, we can also look at `Bag a`s as a function from `a` to `Nat`. A function can be thought of as a univalent relation, and a relation is a set of binary pairs. In this case, `Bag Void = Void -&gt; Nat = {(Void, Nat)}` also has exactly one element `{}`, the empty set. So the implementation matches the expected properties of `e` even more than you gave it credit for.
Not to be negative or anything, but navigating around the site seems to involve a noticeable delay (\~200-400ms before opening and closing the signup modal, for instance). For reference, I'm on a 2014 MacBook Pro (2,2 GHz i7) with no other apps open but Chrome 69.
`pure` and `return` are now straight-up synonyms, right? I think I remember hearing that was one of the more recent changes. Since it's now just a matter of terminology, I'm kind of glad `return` has the legacy behind it. I just think it's a cooler sounding name, even if "pure" probably is the more intuitive and thus the better choice pedagogically. When I downloaded Idris, which I find to be an incredibly exciting language, I was disappointed that it used `pure`! Also disappointed that it has such an ugly logo compared to Haskell which I think has the coolest one of any language. Not at all disappointed that it dropped Haskell's weird colon conventions though, along with a few other such quirks. Nor with the more informative, colorful REPL. I hope to someday be experienced enough to have strong, loudly held opinions on its eagerness.
Thanks for the info. I can reproduce that delay if I open and close it on the home page. Clicking on links, and clicking on the signup modal on other pages, is very quick on my machine. Would you mind seeing if the situation is the same for you. My hunch is that the equality check for the large amount of state on the home page is delaying the re-render.
Ooh I am glad I did! That really made my day! I think I'll bookmark it to go over whenever I'm feeling a bit down.
The reason you run into infinity is your highly non-constructive definition of multiset: your multiset just says how many of each item it contains, but isn't actually a data structure. As a series, you have Multi(z) = (z → N) = (z → 1 + 1 + 1 + ...) because each element can occur 0, 1, 2... times.
Performance has not even remotely been a problem with Scala.JS for us. The code gets translated to what looks fairly similar to the original Scala which in turn looks pretty similar to JS. I think the biggest problem would probably be if you're using highly functional code which doesn't play well with the browser's GC tuning -- but that's the same whether you're using GHCJS or Scala.JS. The good thing about Scala.JS here is that you can go with a simple mostly-imperative code style or you can choose to go functional with monads, etc. There's no choice in TS. We actually switched away from TS to Scala.JS and have not regretted the decision. Note, though, this this was before TS 2.x and we heavily leverage sum types for our business logic. We also already had a backend written in Scala, so that meant we could avoid quite a bit of friction by having the frontend written in Scala. Anyway, I can only say that's it's worked great for us. (That said, I think I'll have to look into GHSJS myself. Apparently some things have changed since last time I tried it.)
I really like the having the margin notes in the side :) It's quite a nice way to expand upon a statement or subject. Great job!
When I first was trying to grok monads I would write code with explicit binds. And while for short expressions I still do, it becomes unmanageable for large blocks of code. I think over time you’ll develop a taste for when to use one over the other. Remember the most likely person to read your code is you. And you should aggressively listen to your gut instinct about what is readable and what isn’t. As far as tutorials go, while I agree that do notation can obscure what the semantics of the language are to beginners it is always a balance when to teach pedagogy which helps with fundamental understanding and real world examples which helps your intuition when writing code yourself. I’m glad you have spoken up about what you find confusing as that helps all of us teaching it know an additional perspective. At the end of the day, the hardest part about learning Haskell is believing you already know how to code. The truth is that the teaching style must vary wildly depending on whether the person is brand new to programming or whether they have already formed a habit of thinking in state machines and sequential instructions. I’m having the same trouble learning rust right now because all the tutorials are geared toward veteran C programmers. Knowing where your audience is coming from is as important as knowing where they are going, yet online it’s difficult to specify who the tutorial is for. I know that’s kind of a nonresponse but it’s a hard problem in general, and we only tend to run into it in Haskell because it’s such a small minority of programmers at large.
Your site’s definitely faster than what I normally associate with GHCJ, awesome job! 
&gt; it's wrong to count `absurd` as a value of `Bag`. well, under usual circumstances, sure! but using the definition u/wnoise referred to, `type Bag a = a -&gt; Nat`, it makes sense, no? &gt; In this case, Bag Void = Void -&gt; Nat = {(Void, Nat)} also has exactly one element {}, the empty set. Afaik `absurd` is supposed to be exactly this, the "empty function", mapping nothing to nothing: absurd :: Void -&gt; a absurd x = case x of {} &gt; `absurd` is not a value Why not? 
I think he's trying to say that `absurd` is bottom, which usually isn't included in this sort of algebraic manipulation.
I agree, even function composition is easier using arrows in my opinion: let add1ThenProduct = product . (+1) vs. add1ThenProduct = (+1) &gt;&gt;&gt; product Most people just say it's the style of haskell to compose in this manner, but it definitely has affected my reading habits: reading everything backwards in haskell when english is left to right is frustrating. 
Are derivatives useful? 
`absurd` is implemented using bottom because you can't do the "empty case" thing i wrote above in Haskell98 – you need GHC's `EmptyCase` extension. but they're two distinct concepts, and the "empty function" does make algebraic sense. from a CT point of view, `Void` is the initial object in `Hask`, which means it has a morphism to every other type. This is exactly `absurd :: Void -&gt; a`, the family of morphisms from `Void` to any type `a`.
Would it help if cabal had a [PubGrub](https://medium.com/@nex3/pubgrub-2fb6470504f) (re)solver?
Fully explained here: https://www.youtube.com/watch?v=AIv_9T0xKEo QuickCheck requires people writing arbitrary instances to also write shrink functions. Hedgehoh has integrated shinking so the programmer doesn't have to write them. 
`absurd` isn't bottom though[1]. it's actually a total function – `Void` has no constructors[1], so `absurd x = case x of {}` handles every possible `Void` value. [1] however, `Void` and `absurd` *used to* be implemented using bottom/nontermination because Haskell98 doesn't allow the "true" definition. See https://stackoverflow.com/a/38559050/5534735
What's an example of a domain-specific effect you might implement in a private codebase, but definitely doesn't belong in `fused-effects` itself, that is _not_ just some simple codified API like `Teletype` that you (presumably) want to mock and test outside of IO?
Bag as a quotient using the development version of agda. {-# OPTIONS --cubical #-} module Bag where open import Agda.Primitive.Cubical open import Agda.Primitive.Cubical.Path infixr 5 _::_ data Bag (A : Set) : Set where [] : Bag A _::_ : A → Bag A → Bag A permute : (x y : A) (rest : Bag A) → x :: y :: rest ≡ y :: x :: rest 
FWIW, even if it's small compressed, it's still going to take a long time to parse at 20M. Parse time is a surprisingly huge cost of big JS. I personally wouldn't consider this a problem for a game, but it's something to be aware of
You need a function which takes a `(Int, a)` and then calls `generate n a` (where `n` is the Int. That is: helper :: (Int, a) -&gt; [a] Can you write such a function?
I went to [play around](https://gitlab.com/boyd.stephen.smith.jr/hs-pubgrub/blob/wip/src/Lib.hs) with it during my ICFP "vacation" this year. I haven't revisited it since, but yes I think it could be quite cool. I don't actually have many cabal problems these days, but I thought it might also be a nice replacement for that part of aptitude/apt on my Debian systems (in particular, my FrakenDebian systems),
Yes. In fact, you can safely avoid learning any parts of Haskell. Because there are whole paradigms that aren't used for anything. Except if you want to write something in Haskell. In which case you might have to learn about it a little.
 data Alt a b = Cons a (Alt b a) | Nil data Mutual a b = No | A a (Alt b a) | B b (Alt a b) Mutual a b is probably what you want. Something similar arises in a paper by Ghani on taking coproducts of ideal monads. Where you want to smash flat layers of the same monad.
[A thread I made recently](https://www.reddit.com/r/haskell/comments/9s8rh7/is_ghcjs_appropriate_for_realtime_game_development/) might be of interest to you. To answer your question though there is `h$gc` which is GHCJS's garbage collector, and JS's garbage collector runs as usual.
That's a fair point, and yeah it does take a bit of time to parse, although so far it does not seem to be a significant issue.
yes
Oh my goodness! I can't believe I got a response from the real live Ed Kmett! Now that I think about it it doesn't seem that strange; and it is probably a bit silly to be starstruck. But you are the person who has inspired me to make a go at a career in programming. My goal is to do what you do someday. I wasn't aware that `=&gt;&gt;` really was supported. In that case none of my criticism should apply to the language design itself. Nearly all of it should fall on the pedagogues. They face a student who's familiar with monads--sometimes having been taught them earlier by that very author. So the student's task is "learning a dual concept," which inevitably is a bit different from learning the first concept of the pair. Now he's trying to understand its *duality*. He needs to see, he wants to see, the "syntactic" analogies to what he has already seen. And it just seems baffling, when the student is trying to get a handle on comonads in this manner, to make such a change in syntax in midstream. This bind/cobind operation is intuitively challenging enough in its type as is. Imagine a dual concept in category theory being taught in this way; unthinkable even from the worst pedagogue! And yet in Haskell for some reason here is one of the very *best* pedagogues (in any subject) out there, Bartosz Milewski, making the baffling choice to teach comonads, with hours of video and pages of text, without *once* even acknowledging the *existence* of `=&gt;&gt;` (this after teaching monads almost exclusively using diagrammatic syntax). He just uses `extend` exclusively, attempting to pump the duality intuition from that syntax while making the direct comparison to `&gt;&gt;=`. Nothing clicked for me; I was confused and intimidated until I finally made the decision to simply translate it all into `=&gt;&gt;` and teach myself that way; then instantly it all clicked into place and was the easiest thing. Again, I bring up Bartosz precisely because he is normally such a revelation, a miracle worker; yet even he made this choice that I found baffling. As I mentioned, I myself definitely find the "functional" order to be a more conceptually straightforward choice for the syntax of monadic/comonadic composition *overall*; I just found the choice of diagrammatic (and believe me I normally hate diagrammatic) to be a far lesser "sin" than teaching one dual concept one way and the other the other, especially without so much as an acknowledgment of it as Bartosz and so many others have. I didn't have much trouble at all understanding monads taught with the standard binding syntax; I wish I could say the same for comonads (inevitably learned in the context of their duality). Little wonder either that your own, more conceptual, work finds `bind` the more convenient direction than `&gt;&gt;=`. I had just figured that "reverse bind" must really be much more convenient in the field, and that was why it ends up appearing in so much more working code. And I was curious to know whether, for comonads, it's the "right" way round that ends up with the advantage in working practicality. If `&gt;&gt;=` was in the *first* place a bum design decision ossified into convention, I did not see that coming! So in general the case you make for functional order overall, including the greater usefulness for building library items and such, strikes me as something of a "theoretician's" reasons. It would not surprise me to learn that for the average programmer "in the field" other syntaxes turned out to be of more practical convenience and appear more often in working code. (After all, otherwise we'd be seeing `join` and a bit of `&lt;=&lt;` all over the place instead of `do` and a bit of `&gt;&gt;=`!) But I think the major persuasive theme of mine on this thread has been to get people to see that, for a rank beginner struggling to grasp a concept, the "theoretician's" needs, his "practicality," often actually has much more relevance to his own than do those of the field programmer, which are motivated by all sorts of more remote factors. It's not like you're opening up a copy of Lambek and Scott on the poor noob to acknowledge that. 
A tangential question - does Kotlin guarantee TCO in cases of mutual recursion? I recall reading that Scala has some issues around this (due to the JVM?), leading to stack overflow with certain coding styles that work okay in Haskell.
Technically there’s a newline in the output in the program /s
Hey everyone, I hope this is appropriate to post here! If you want to start from the (boring but necessary) start: [https://ptival.github.io/card-game-01](https://ptival.github.io/card-game-01) &amp;#x200B; This is an ongoing series, and I think this is the first post that is "cool", in that I use a bunch of techniques like lenses, extensible effects, zippers. My intent here is to document how this code came into shape in small increments. It's aimed for people who are a little more than beginners, but not experts either. Every post comes with a Git commit that compiles and adds one unit of functionality, in as small an increment as I can. &amp;#x200B; I hope to run into all sorts of trouble implementing the more complex rules of this game, and hopefully to display some cool solutions. This post already has a neat example of a GADTs to get some compiler support in avoiding dead code. It might be too technical for real beginners, and too boring for experts, though YMMV... :-) &amp;#x200B; Feedback/Questions welcome!
I just want to say that I stumbled upon Arrow a few months ago, and I was completely blown away by the documentation for it, as well as how nicely crafted a library it looked and how much coverage it had of so many typed functional concepts. I feel inspired by it! Also, while you're here.... ;) I'm curious if you think Arrow will ever work on Kotlin Native, and how long it will be before you'd comfortably start building production level cross platform mobile apps on Kotlin Native (as opposed to say React Native if you care about code share between platforms). Much as Typescript and React have done some good wonders for me, I'd prefer to use Kotlin with Arrow And I realizing this is a lot to be asking on a haskell post :)
The presentation of the monad laws is also far nicer for Kleisli composition `&lt;=&lt;`/`&gt;=&gt;` than monadic application/binding `=&lt;&lt;`/`&gt;&gt;=`. I didn’t quite understand them until seeing them phrased with composition and going “Oh! It’s just a monoid. …*Oh*, ‘…in the category of endofunctors’!”
I've been working on an EDSL that compiles to SPIR-V shader code, which uses indexed monads to type-check programs within Haskell before converting into an AST and emitting SPIR-V instructions. It's been coming along really well, and with a few more days of debugging should be producing correct SPIR-V assembly that can run on the GPU (at the moment there's still a bit of bookkeeping missing). There are still many bits of functionality that I have yet to implement; my next goal is to include nontrivial control flow, generating the corresponding [phi instructions](https://en.wikipedia.org/wiki/Static_single_assignment_form). It's been a really fun project so far, and I'm hoping to make it available for people to play around with before the end of the year. I think it'll be really cool that we'll be able to write all the graphics code in Haskell, using Vulkan, with this library generating the SPIR-V (with some nice type-checking features, which should be an improvement over writing GLSL code).
This sounds really cool and interesting. I've always been interested in computer graphics but the professors that taught that topic at my university, sadly left :( Is there any literature, courses or tools you'd recommend me to start getting into this complex world ?
The Zipper of an ADT happens to be it's derivative. 
I’ve had an embarrassing obsession recently with making abstractions to write things in pure CSS (eg, a [playable Sudoku](https://identicalsnowflake.github.io/sudoku.html) that only allows valid states). I may write about it at some point—it all started perfectly reasonable, but one thing led to another and now I’m way off the deep end.
I'd recommend diving straight in. For shaders, it's probably best to start off with OpenGL, because it is very tedious to set things up in Vulkan (but once you've set things up, it does end up being a bit simpler/principled... in any case the basic concepts cross over). For instance you can start looking into the [programmable pipeline](https://www.khronos.org/opengl/wiki/Rendering_Pipeline_Overview). It's very straightforward to write a simple shader that just does a few matrix multiplications and texture lookups etc, and you can build from there. Setting that up in OpenGL isn't that much work either, you basically store data onto the GPU with uniform buffer objects, which is then accessible to the shader when it runs. Of course there's a lot of other things you might be interested in graphics-wise, some much more theoretical than that. I've always enjoyed the theory behind path-tracing, and the various different algorithms that exist. In that case I like to search around and find some papers, read along, and try to implement some of the ideas. I'd recommend starting with something simple (embarrassingly simple). Once you manage that you can start building on it, and before you know it you'll have written something with a lot of features. It can be very daunting when you start, because there are a lot of different parts that need to work together and it is difficult to just get it all right at the same time. ocharles has [a few videos](https://www.youtube.com/channel/UCDbZjVGI4A0PVxGQiRoZt_A/videos) where he sets up rendering for Quake 3 using Vulkan in Haskell. I think these videos illustrate this point quite well: it was a lot of work to get a simple triangle to display on screen (and a lot of debugging and wondering why the screen is blank), but then before you knew it he was rendering a whole Quake 3 map! That's part of the nature of programming: it isn't essentially different to a computer to render 5 triangles or 100 000 triangles, so once you set the basics up (e.g. writing to a depth buffer so that closer triangles are correctly displayed on top of triangles that are further away), you can import 3D models and start rendering them without much more work. Let me know if anything specifically interests you and I'll try to give more concrete recommendations. I'm not an expert, mind you... I just like to play around with graphics; I find it quite satisfying to get pretty stuff on the screen after working through some code / mathematics. It's a nice counterpart to the abstract nature of programming in my opinion. 
That's one of my main issues with algebraic effects (apart from the sheer amount of libraries which scares me;)). LAst time I was tinkering with Haskell app, I was going towards MTL-like classes describing my domain, and I haven't really felt the need for granular effects. Maybe I'm just wrong and an answer to this question would shed some light... 
In the last quarter I: * Finished a bullet hell game - [nyx](https://gilmi.me/nyx) * Published an opinionated list of resources for learning Haskell - [haskell-study-plan](https://github.com/soupi/haskell-study-plan) * Continued working on a learning project compiler [nyanpasu](https://gitlab.com/gilmi/nyanpasu). Currently working on closures and first class functions. * Worked on a stack language Morph (private repo currently) - got up to modules, a repl and factorial working. * (PureScript) - Wrote a simple [english &lt;-&gt; morse converter](https://gitlab.com/gilmi/morse) using purescript-specular (reflex-dom inspired ui framework) And I plan on participating in [Github Game Off](https://blog.github.com/2018-10-15-game-off-returns-november-2018/) and build something with Haskell.
Another thing that can help - in addition to closure compiler - is setting a few GHCJS flags that can help. Basically [these ones](https://github.com/jappeace/awesome-project-name/blob/master/frontend/frontend.cabal#L57-L58). I usually put `-dedupe` everywhere and `-DGHCJS_BROWSER` in the executable and things work out pretty well. You might be doing that already, but if not it seems to really let the minification and compression go wild.
All productivity-related tools: * Smos: [https://github.com/NorfairKing/smos](https://github.com/NorfairKing/smos) * Cursors: [https://github.com/NorfairKing/cursor](https://github.com/NorfairKing/cursor) * Validity: [https://github.com/NorfairKing/validity](https://github.com/NorfairKing/validity) * Intray: [https://github.com/NorfairKing/intray](https://github.com/NorfairKing/intray) * Tickler: [https://github.com/NorfairKing/](https://github.com/NorfairKing/intray)tickler
Trying that out now, thanks!
The heterogeneous zippers you describe exist here too: [https://hackage.haskell.org/package/cursor-0.0.0.1/docs/Cursor-List-NonEmpty.html](https://hackage.haskell.org/package/cursor-0.0.0.1/docs/Cursor-List-NonEmpty.html) This may allow you to simplify your code a bit, because you wouldn't have to roll your own.
God damn those immediately reduced the output size by like 35%. What are the downsides, if any?
Beautiful, well-documented and useful. Thanks for sharing!
After a few years of dipping in and out of Haskell in very limited free time I'm now working on my first non-trivial Haskell program where I'm not relying on any frameworks for the heavy lifting. It will ultimately be a fairly simple chess training tool. https://gitlab.com/garry-cairns/emanuelhasker
Me and @Peaker are working on [Lamdu](http://www.lamdu.org) - a live programming, projectional editing environment for a purely functional language similar to Haskell. [While it has been posted here in the past](https://www.reddit.com/r/haskell/comments/46uxdh/lamdu_towards_a_new_programming_experience/), only now we've finally made serious efforts to tell the world about our project, releasing a video a few days ago, building installers, and also going to talk about it in Boston next week at [the LIVE Programming Workshop at SPLASH](https://2018.splashcon.org/track/live-2018-papers#About)!
wow, any tips for how you keep so proactive and finding things to code? 
I try to write down most of the ideas that pops into my head, no matter how big or small. I then try to flesh them out and reduce them to the essentials, and keep them all in a list at some repository or something like that. This process can take between minutes to an hour. If I'm still very interested I start working on it. If not, or if I'm currently working on something else/doing have time for that right now, I leave it on the list for later. Then I try to make a rough design and break it down into small incremental tasks so that i can see some results quickly (which will make me want to keep going - so this is crucial. [I wrote about this a bit here](https://www.reddit.com/r/haskell/comments/76r04v/advice_for_haskell_beginners/dog34t5/)) and estimate how much work it's going to be. Most of my ideas do not leave the notepad, but this practice helps generate ideas and at some point find stuff that you really want to do and feel like you mostly know how to make. At some point you'll get to a situation where you have more ideas in the backlog than time to do them.
No downsides that I'm aware of. I had a look at the various ways to slice things with a toy-ish problem [here](https://www.reddit.com/r/haskell/comments/9mnxl1/fullstack_haskell_reflex_and_servant/e7h64ha/). I'd be really interested in seeing what happens when you add minification and zopfli into the mix (like mentioned [here](https://github.com/ghcjs/ghcjs/wiki/Deployment)). 
Yes, there's a `tailrec` annotation that gives IDE and compiler warnings when the function isn't properly written. See for example the implementation of `tailrecM` for NEL: https://github.com/arrow-kt/arrow/blob/9a9f68b2aba658220f34207230dccd841c31017f/modules/core/arrow-data/src/main/kotlin/arrow/data/NonEmptyList.kt#L95-L118 
Cheers! It's been a collective effort over many months, and we're so happy of the success so far :D We now know many people are not afraid of mobile development anymore, and we got to level up the community meanwhile. &gt;I'm curious if you think Arrow will ever work on Kotlin Native `arrow-core` works with Kotlin Native. Now, anything with `arrow-typeclasses`, including `arrow-data` depends on reflection to access do-notation so that's JVM-tied for now. We've asked JB for an API to serialize and deserialize `Continuation` state to fix it, and it's a maybe. `arrow-effects` will remain on the JVM, so anything from that part of the stack up will depend on interest from KNative folks to reproduce. The most we can do is split typeclasses and effects into the JVM and non-JVM parts and go from there. That'll happen if there's enough noise around KNative.
Two small libraries I've written recently: * paripari - Parser combinator library with fast and slow parsing strategy: [http://hackage.haskell.org/package/paripari](http://hackage.haskell.org/package/paripari) * persist - Serialization library (similar to store, but a bit simpler and machine independent): [http://hackage.haskell.org/package/persist](http://hackage.haskell.org/package/persist)
TaskLite - A CLI Todo list manager that's supposed to improve on Taskwarrior by being simpler, more stable and multi user friendly. Didn't want to build this originally, but Taskwarrior had several irreproducible bugs which randomly deleted tasks. 😣
1. Contributing to [gloss-export](http://hackage.haskell.org/package/gloss-export), to make it useable in one of my video editing project 2. Exploring the category of F-algebras, in the hope of [finding laws for my new implementation of recursion-schemes](https://github.com/ekmett/recursion-schemes/issues/58). Currently distracted by trying to determine whether that category has exponentials. I think not, but how to disprove it?
Can you be more specific? I don't see what you are alluding to.
Thanks for the kind words!
After some POC website to learn Haskell, I'm now working on a bigger scale website. The more I work in Haskell, the more I learn concepts, and the more I'm motivated to learn more. Very positive circle! :) I also try to contribute to some libraries when I see some detail to improve. But overall the quality of the Haskell environment is excellent.
I think it's a pretty universal approach to separate test and source code, since they have widely different purposes. The reason for the separation, for me, is: - Not polluting dependencies with test-only dependencies (compile-time in larger projects is a concern) - Not bloating the code with functions that are not business logic, but serve only to test the code - Clear distinction between what are tests and what are not - Quick way to locate tests, instead of having to investigate every single source file in `src/`, I just look in `test/` - Your tests also need a single entry-point, usually, which can get awkward having to pull all source files in - TH staging restrictions can become annoying For your ghcid point, I normally just use `stack test --file-watch --fast` or smth like that :)
I really miss having tests in the same file from when I was writing Erlang - http://erlang.org/doc/apps/eunit/chapter.html
richard hickey
I think they should be separate. A test suite is not part of the program itself. I've never seen this approach to putting tests mixed in with the program source as the standard in any language, although I'm sure someone will point one out now.
Well, you can easily include small unit tests in your code documentation with [doctest](https://hackage.haskell.org/package/doctest). It should work within GHCi. For larger tests, such as integration tests, it is much preferrable to have them in a separate directory, for all the reasons highlighted by /u/Tehnix.
I'm a big fan of [doctests](http://hackage.haskell.org/package/doctest), in part because the tests are close to the code being tested and so are more likely to be changed together. The possibility of putting non-doctest tests just as close had completely escaped me, I like this! Maybe with some `#ifdef`s around it so we those who depend on my package don't need to build the dependencies of my tests?
You can get `ghcid` to run your tests whenever a source or test file changes: stack exec -- ghcid --command 'stack ghci your-package:test' --test 'Main.main' https://www.parsonsmatt.org/2018/05/19/ghcid_for_the_win.html
Separating source code from test code is standard practice across programming languages. It's not a Haskell specific thing. Java, C#, JS, &amp; python projects all usually separate source code from test code.
I'm working on a compiler for Bitcoin Cash smart contracts. http://spedn.rtfd.io
Came up with the name before I came up with the project tbh. Pun driven development.
In [D](https://dlang.org/), you have `unittest` code segments that are compiled only when a compiler flag is enabled, and they have access to unexported symbols in the same module. They are auto-executed by the test runners. That sounds like what you wish for here.
One advantage of the current approach is that it forces tests to only be able to use the public interface of a module. This means everything the module exports can be treated as a blackbox (which it is to any other module). I consider this a feature because the moment you have a test that needs to get into the internals you are forced to revisit your API design. You got frustrated as a user of the library - the test suite is an API consumer after all - so it's entirely possible the API is suboptimal.
You can do that in Rust. The test portion of your code doesn't even get parsed if you are not running any tests.
Fixed! You got it! :)
We have defined an effect for diffing blobs of text, which is parameterized by and separate from the actual diffing algorithm, comparator function, and equivalence relations. This used to be a `Freer`-based DSL, but pulling it out into a first-class effect gets us tons of speed and lets it compose with other effects.
Pijul looks pretty cool. Also I think you are right. Even if Git may be the bet VCS right now I actually think it is more important to be open to alternatives to encourage competition and progress, than the inconvenience of (worst case) having to send `.patch` files per mail because you are not familiar with the VCS used.
I’ll get this worked out. Thanks for drawing attention to this.
&gt;That being said, if your point was that it's bad to use do notation before introducing monads, I think a good chunk of the community would agree with you. Perhaps a good chunk, but I hope not most! I think one of the most important lessons we've come to as a community is that it's been a mistake to try to teach abstractions before concrete examples. This is almost **the** classic failure of the Haskell community. Once you're teaching the abstract concept of a monad, I agree that doing it without do-notation is a good idea. But I cannot imagine having a good time teaching someone how to do basic I/O in Haskell without relying on do-notation pretty heavily. When someone wants to know how to read a String from the console and echo it back to the user, you tell them to write: main = do str &lt;- readLine putStrLn str It seems clear to me (does anyone disagree?) that this is more obvious to a new Haskell programmer than `readLine &gt;&gt;= putStrLn`, and that the difference will only get larger as they attempt to build more complex operations. Eventually, yes, you want them to understand that it's sufficient to consider (&gt;&gt;=) and lambdas to accomplish all of their complex control flow, and then point out that I/O is a monad (probably after starting with some simpler monads). But the important thing that do-notation does is to remove understanding monad combinators from the list of things you need before you can be minimally productive in the language.
Im not primarily a *Haskell*er, but another option to /u/presheaf 's great info is to use an existing 2d/3d engine, and just build a game project you want on top of it. Thats if you dont want to do the low level stuff, or have more of a desire to do the game stuff. Nothing wrong with either approach, it only matters what sounds fun to you. 
At least when I joined the community there was a huge amount of pressure not to mix tests and code, because there were two versions of `QuickCheck` running around in common use, and no consensus about which one was the "right" one to use. When you locked down your test suite to use one or the other, if the instances and code were directly smashed into your library then nobody from one camp would be able to use code from the other camp. This isn't really as much of a going concern today. =) However, it really pushed the community into a space where it heavily decoupled tests from running code. Mind you, this is pretty much the status quo in other languages, but it drove the wedge in pretty hard here. In some sense, a large part of the power of doctesting for Haskell was that it brought some of that convenience back without any code bloat or user complaints.
By-all-means create a better *cross language* benchmark suite and publish those measurements!
&gt; In my opinion an ideal benchmark… Where are the measurements you have made for your benchmark? *“The best is the enemy of the good.”* 
Im a game programmer (MMO backends primarily) and this just sounds amazing. Im always super impressed and interested in what you can do with *Haskell*. This [monads-to-machinecode](http://www.stephendiehl.com/posts/monads_machine_code.html) thing is super awesome. Ive written that code in a couple languages, but to see how easy it is to embed a DSL into *Haskell* is impressive to me. 
&gt; I think it's a pretty universal approach to separate test and source code, since they have widely different purposes. In my experience it *used* to be universal, but actually more modern testing frameworks like jest (for js) and go-test are both in favour of co-locating test and source files. There's likely a reason for the change. &gt; ⁠Not polluting dependencies with test-only dependencies (compile-time in larger projects is a concern) This presumes that we couldn't find a way to co-locate tests while keeping the dependencies separate, it doesn't sound so tricky to figure out 🤷‍♂️ &gt; ⁠Not bloating the code with functions that are not business logic, but serve only to test the code Put those functions in your test files. &gt; ⁠Clear distinction between what are tests and what are not The other frameworks I mentioned use a standard filename suffix; jest uses ".spec.js" and golang uses "_test.go"; many haskell specs already end in "Spec.hs" so this isn't a problem IMHO. &gt; ⁠Quick way to locate tests, instead of having to investigate every single source file in src/, I just look in test/ I'd argue Co-located tests are actually much easier to find. Where're the tests for "Blah.hs"? Oh they're right next to it in "BlahSpec.hs" can't miss it! It means you're more likely to split your specs up into smaller files too! &gt; ⁠Your tests also need a single entry-point, usually, which can get awkward having to pull all source files in This isn't an issue either, hspec will auto- discover your tests and run them all. This is preferred IMHO because it means you don't have to manually manage your test-main and means you can't be "forgetting" to run any of your tests. &gt; ⁠TH staging restrictions can become annoying I don't really know what you mean by this Not saying we necessarily *should* co-locate tests, but I think we should avoid making too many straw-man arguments against it and offer it some consideration first 😄 
Working on IOHK's projects, mostly. The bad part about being a working Haskeller is that you want to do something else with your spare time :)
That's not a "plain" ADT. That's at least a HIT, because of `permute`. Are you sure you got `trunc` typed correctly? It seems to introduce a couple of bindings (`xs` and `ys`) that it doesn't use and uses a couple of bindlings (`x` and `y`) that it doesn't introduce. Also, if `A` is sufficiently large, isn't it a problem to implement `trunc`? I thought uniqueness of equality was only true for the lower H-levels (Prop and smaller).
This has been fixed now. You can view the benchmarks [here](https://s3.amazonaws.com/www.patrickthomson.net/results.html).
The notion of mixing test code and production code should be at least a little scary. It might be obvious to you what your top-level test methods are, but what about the utility code that you use in your tests? The failure modes in these are typically much different from production code. In particular, partial functions are extremely useful! Or even if you insist on avoiding partial functions in test code (probably a bad idea because it makes tests harder to write), you still have very different performance expectations. A quadratic-time unordered list comparison without an Ord type class requirement is a completely reasonable thing to write in test code. But if you leave a bunch of helpful utility functions around your production modules that can blow up or cause DoS vulnerabilities if you aren't careful, this is courting disaster. As someone else said, this isn't Haskell-specific. There's a strong tradition in most of software engineering of separating test code from production code, at least at the file level, and ensuring that test files can't be linked into production binaries. There's no particular reason to be less cautious 
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [deech/fltkhs/.../**widget-table-windows.png** (master → 663a258)](https://github.com/deech/fltkhs/blob/663a258dbdc35b7178e80fc4032b632d8e7ff98b/images/widget-table-windows.png) * [deech/fltkhs/.../**tree-complex-windows.png** (master → 663a258)](https://github.com/deech/fltkhs/blob/663a258dbdc35b7178e80fc4032b632d8e7ff98b/images/tree-complex-windows.png) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply e8qitae.)
That's a really cool project!
* whole program optimizer for functional languages: (https://github.com/grin-tech/grin) * GRIN backend for GHC: https://github.com/grin-tech/ghc-grin
Ah yeah, I looked into this library but avoided this particular module because I was not convinced by its name that it was what I wanted! I'll look into it and maybe post an update! :-)
_very carefully_ (more seriously, see http://strictlypositive.org/diff.pdf or the even-more-gentle https://en.wikibooks.org/wiki/Haskell/Zippers to start you off perhaps)
&gt; I'm not sure if we're actually disagreeing. I think you're right. I had taken you to mean `absurd === _|_`, but I think you're using another definition. Your definition of absurd is not a bottom value, as far as I can tell (although it produces bottom). 
Yeah support hasn't always been perfect and setting it up such that ADVANCED doesn't rewrite things incorrectly can be a bit tricky (which is the case regardless, not just with GHCJS).
http://sci-hub.tw/https://dl.acm.org/citation.cfm?id=3242745# Fixed link :D
What would one use this for?
What are some motivating examples of how they're used?
Jekyll and some plugin developed by 47degrees, I'm not too familiar with it tbh.
I created a simplistic 3d graphics app where you can compose a scene of various primitives and then fly around in it. I then used this to implement a 3d version of Conway's Game of Life as a study of comonads. https://github.com/aaronallen8455/Haskell3D Right now I'm rewriting the nodejs server from work in Haskell using servant and persistent. I've already cleared the major technical hurdles, but the hardest part will be convincing co-workers to make the switch.
I am finally getting some time to work on an approach for an incremental front-end for GHC, integrated via a language server. - https://github.com/alanz/incremental-play/ Based on the same underlying paper as for [tree-sitter](https://github.com/tree-sitter/tree-sitter) Just experimental toys at this stage.
If nothing else, it makes it super simple to avoid packaging your test code with your library code. 
You can get it free of charge at [http://www.sigplan.org/OpenTOC/haskell18.html](http://www.sigplan.org/OpenTOC/haskell18.html) (just look for the name of the paper).
I dont know how easy it is to link to C or C++ apis, but AMD has an easier *vulkan* interface that you can use called [V-EZ](https://gpuopen.com/v-ez-brings-easy-mode-vulkan/) that you or /u/orehcelE might want to look at. This brings *vulkan* a bit closer to the ease of use of OpenGL, without giving up what makes it fast and nice. 
Isn't `pure f &lt;*&gt; x` usually the same as `f &lt;$&gt; x`? You have this helper defined in post 2 or 3. 
Firts one sounds awesome, will check it out Second one sounds challenging and interesting for showing people that "easy to use tool" doesn't mean "the right tool". Good luck on that!
You mean this company ? iohk.io
Such awesome and interesting projects, have built a huge pile of thing to read and get into out of this post. Thank you very much!
UIP (uniqueness of identity proofs) is the characteristic property of h-sets (although it also holds for lower h-levels since h-levels are upward closed.)
\[Nyanpasu!\]([https://www.youtube.com/watch?v=anhaSZ4aJe8](https://www.youtube.com/watch?v=anhaSZ4aJe8))
`Term` has kind `* -&gt; *` rather than the `Incr -&gt; *` you're probably thinking of. `Incr` here is basically `Maybe` rather than `Nat`.
Also: [https://www.youtube.com/watch?v=gUPuWHAt6SA](https://www.youtube.com/watch?v=gUPuWHAt6SA) [http://reasonablypolymorphic.com/dont-eff-it-up/](http://reasonablypolymorphic.com/dont-eff-it-up/) helped me grok most of it.
You must have misread it based on habit. My helper does: `f &lt;*&gt; pure x` I mostly use it when I want to apply a constructor to a bunch of values that may or may not be effectful: `let r = Ctor &lt;$&gt; ev1 &lt;*&gt; v2 &lt;*^&gt; ev3 &lt;*&gt; v4` where `Ctor :: a -&gt; b -&gt; c -&gt; d -&gt; e` `v1 :: f a` `v2 :: b` `v3 :: f c` `v4 :: d` `r :: f e`
&gt; I think trunc also truncates any paths that would be induced in Bag A due to paths in A. I think what I mean is that you can't implement it except as an assertion/axiom if `A` is allowed to be `Circle`. I'm really not used to dealing with higher h-levels, but it seems like if you have UIP for Bags of non-UIP types, you'd end up with a contradiction, since `x = y` might not be `refl`, but paired with `xs = xs` (refl) you'd get that `x : xs = y : ys` (ture, not not `relf`) which trunc says is `refl`.
&gt; trunc is declaring uniqueness of identity by fiat Seems like this might be a way to introduce inconsistency if `A` didn't have UIP. I'm really not used to dealing with higher h-levels, but it seems like, since `x = y` might not be `refl`, but paired with `xs = xs` (refl) you'd get that `x : xs = y : ys` (true/inhabited, but not `relf`) which trunc says is `refl`.
That's one level up from Prop?
Amazing, thank you!
Your probably thinking of how some categories (when working in set theory) are *proper classes*, they are also not sets (as you said, because they are "too large"). But that is a different reason than for why some types in HoTT are not sets (like the circle). These types have "higher-dimensional" structure, which is what makes them non-sets. 
I don't quite get your example (where did `ys` come from?), but I think I get what is giving you trouble. `trunc` isn't saying the other path *is* `refl`. `trunc` is saying in `Bag A` there is a (higher-)path between that path and any other parallel path (including `refl`). The eliminator for `Bag A` forces us to send them to the same place (up to paths). It think the same problems your thinking of may apply to the set truncation. data set-trunc (A : Set) : Set where inj : A -&gt; set-trunc A trunc : (x y : Set-trunc) -&gt; (p q : x ≡ y) -&gt; p ≡ q which is mentioned in the HoTT book.
Yes
If it were possible I'd much rather be able to painlessly test the private innards of modules without having to move code around. Unexported helper functions are smaller and take fewer arguments, so I get high coverage with default arguments/iterations when writing property tests.
This is pretty much what Rust does, IIRC. I'm not sure if it's a familiarity thing or what, but I find that it mostly just increases the size of a file without significant readability increase.
&gt; where did ys come from? It was from when I was making the example needlessly complex. I fixed it; should be xs.
&gt; trunc : (x y : Set-trunc) -&gt; (p q : x ≡ y) -&gt; p ≡ q Yeah, sort of the same problem here. To me it's a really weird way to effectively limit A to not all types, but rather only low-h-level types. Each time you prove it for a particular A, you prove that A has UIP.
This works for any `A`! This is like the propositional truncation (∥A∥ in the HoTT Book). It doesn't matter what h-level A has ∥A∥ is always a proposition. data prop-trunc (A : Set) : Set where inj : A -&gt; prop-trunc A trunc : (x y : prop-trunc A) -&gt; x ≡ y Likewise, the set truncation takes in *any* type `A` and then `set-trunc A` is a set (∥A∥₀ is the notation used inn the HoTT Book). Not necessary to my point here but might help in conceptualisation, the set truncation turns each "connected component" in `A` into a point (well actually a group of points all connected by paths, sort of like the Interval) in `set-trunc A`
Even if all that was true, and I really hate the syntax for HITs, what does any truncation have to do with bags?
yes, set is the next level up from Prop. the levels are : - contractible - prop - set - (1-)groupoid - 2-groupoid - 3-groupoid ... - n-groupoid ... 
Because the (first) definition of bag I used truncates itself down being a set (the `trunc` constructor) using the same method as the Set truncation. If I don't do that the Bag would be at least a groupoid because it contains subparts that have the structure of a circle.
I don't think it is a bottom up vs top down thing. If you just use 1 module then: 1. Everything can see everything else, so you cannot enforce abstraction boundaries (e.g. you cannot have abstract types). 2. Changing one thing means that now you need to recompile the whole thing.
There is a commit from a day ago splitting a single main file into modules
One argument for modules is that it can for you to think in terms of polymorphic types. For example, in my project the module structure is something like: Api/ Post.hs User.hs PostReply.hs ApiUtil/ Response.hs Api.hs The setup is like this: - `Api` imports `Api.User`, `Api.Post`, and `Api.PostReply` - Each of the `Api.*` modules imports `ApiUtil.Response` So when I import `ApiUtil.Response` into `Api.Post` and later into `Api.User`, I am forced to write functions that are reusable across all of my `Api.*` modules (particularly because `ApiUtil.Response` **cannot** access the types in the `Api.*` modules, or else I would have circular inheritance). I find that structuring things like this is conducive to writing functions that take polymorphic types and thinking in terms of type classes.
I'm hoping to get back into CLaSH [after the one-month RetroChallenge sprint in September making a CHIP-8 computer](https://gergo.erdi.hu/blog/tags/retrochallenge/): * I want to rewrite my [6502 core and the whole Commodore PET](https://gergo.erdi.hu/blog/2015-03-02-initial_version_of_my_commodore_pet) from Kansas Lava to CLaSH * Play around with the [one-page CPUs](https://revaldinho.github.io/opc/) to see what they'd look like in CLaSH with the RWS-based CPU descriptions I've come up with for the CHIP-8
Why sort things into bins? Why not just have a giant pile of stuff that's all in one place? That sort of organization works up to a certain scale. Past that scale, you need to start moving things into bins to keep track. As with physical objects, also with functions. Modules accomplish the same thing as the bins. Sure, there are other benefits, like separation of concerns, clear testable boundaries, etc. But primarily that all flows from the same basic concept - you drew lines between things to separate them into distinct categories, which is a helpful thing to do sometimes. Note, also, that there does exist a scale at which it actually is just easier to keep everything in a 'giant pile.' It's just not very common to see that scale in software projects of any size past toy programs, so it's not generally considered to be best practice.
You’re right. That’s quite a habit... Cool series, btw. I played a ton of Yugioh circa the fourth grade and it’s nice to see my old hobbies getting along with current ones :)
Is it for some reason bad for the category of bags (of a certain element type) to be a groupoid?
The LoT trick is neat!
Sometimes it's namespacing, sometimes it's isolation, sometimes it's just separation of concerns, sometimes it's minimizing edit/compile/debug cycle time. But, I usually stick with one module until I have a real reason to split it up. I also think the top-level `Data.` and `Control.` prefixes are fairly useless.
This is a post on the PureScript Discourse, but it refers to design challenges shared by Haskell. I am curious to hear opinions from the Haskell community on the topic! I've been spending some time building a new application while trying to follow best practices for designing data with types. Doing so on the backend is a little different from the frontend, but there are many times when I reach for a type like `String` instead of a better fit for the domain simply because it doesn't seem worth the effort to provide a better type. How do you decide when it's worth writing a custom type for a set of values vs. reaching for existing types, especially ones which aren't particularly informative on their own (`Either`, `(,)`, `String`, `Int`, etc.)? Is there a "best practice" to guide this sort of decision?
You'd expect Bag's to be equivalent to the type of sorted lists e.g. `Bag Nat ≃Σ (List Nat) (\ xs -&gt; isSorted xs). This isn't true if you don't kill the higher structure.
The main project I have been working on is [Polimorphic](https://www.polimorphic.com/), it's a site for easily accessible and personalized objective political information. The frontend is written in Miso, with some Miso also running on the backend for server side rendering, as well as servant for routing and ajax endpoints, and then postgresql+persistent+esqueleto for the db.
In my mind the key thing that modules provide is conceptual unity. "Stuff pertaining to X lives in module Y". So you can "understand" a module as a whole, and then in _your mind_ abstract that knowledge and refer to it when understanding other components that rely on that.
Thanks, but I'm specifically interested in implementing typeclasses, not dependent types. (Correct me if there's something I'm missing). I'll remember these for future reference though.
##r/idris --------------------------------------------- ^(For mobile and non-RES users) ^| [^(More info)](https://np.reddit.com/r/botwatch/comments/6xrrvh/clickablelinkbot_info/) ^| ^(-1 to Remove) ^| [^(Ignore Sub)](https://np.reddit.com/r/ClickableLinkBot/comments/853qg2/ignore_list/)
Modules provide abstract interfaces, abstract types and abstraction boundaries. Or, if you prefer, modules are values with existential types. One thing I cannot nag enough about Haskell is the lack of "real" modules and lack of support for existential types, and I think this is one of the few bad design decisions in Haskell (the good ones are awesome, but the bad ones are really bad, I have to say). Well, you have them as an extension, but it's really wacky and that's not enough. Rather than what we have now, I want a more ML-like solution: if we have an abstract interface for something, say a Map, I want to have multiple modules implementing that signature, say AVLMap and RedBlackMap, which are themselves abstract in their key type. Currently, Haskell cannot quite express those delicate contraints: I might be able to achieve something similar with type classes, but I really want my existentials. (Map typeclass? Sounds idiosyncratic.)
Any complex system or information will be easier for people to work with when it is broken down into digestible chunks. For example, in a book the ideas are organized into sentences and paragraphs, and at the highest level, chapters. In a software project the code is organized into functions, and at the highest level, modules People also benefit from organizing and presenting things so that similar elements are obviously similar and located in a similar place and dissimilar elements are obviously dissimilar and located in different places. It helps one understand how elements relate to each other and the system and where to look to find things. So to continue the example, in a book the chapters spatially organize related information and basic elements such as chapter titles, page numbers, and the main text will usually be differentiated visually and spatially by use of different font and position on a page for each. In a software project the modules spatially and semantically organize related code, and basic elements of the program such as keywords, symbols, and literals are differentiated visually and spatially by the syntax of the programming language and colouring applied by the editor
You wrap an integer identifier in nestle not only so that you don't accidentally write other integer there. You do this, because ide tinier don't have the semantics of integers - you don't want to perform arithmetics on them, it's just unique identification. And because it's unique it makes also sense to have such newtype for other records, because they're just not the same (even if underlying representation is). When it comes to your boolean example, this field is just a boolean, and you in fact, might want its value to be coming from other booleans - suppose from checkbox on the Ui. So there's little value in wrapping it innewtype - you *want* its boolean semantics.
Kanren in Haskell is a long-standing interest, gotta check out your project!
\- Mainly "Komposition", a video editor for screencasts: [https://github.com/owickstrom/komposition](https://github.com/owickstrom/komposition). Lot's of stuff going on here, mostly refactoring right now but also new features for 0.2.0 release. Experience report over here: [https://wickstrom.tech/programming/2018/10/26/writing-a-screencast-video-editor-in-haskell.html](https://wickstrom.tech/programming/2018/10/26/writing-a-screencast-video-editor-in-haskell.html) \- From that project [https://github.com/owickstrom/gi-gtk-declarative/](https://github.com/owickstrom/gi-gtk-declarative/) was born. \- Also, I'm eager to get back to actually recording screencasts. I've prepared a tutorial on gi-gtk that I think will be the first one. \- Giving a Haskell talk at Oredev in Malmö, Sweden.
They can be used to model higher categories (e.g. \*opetopes\*), which is pretty cool.
Iirc the implementation follows [The Syntax and Semantics of Quantitative Type Theory](https://bentnib.org/quantitative-type-theory.html) since Idris 1.2
There's a classic paper by David Parnas called "On the Criteria To Be Used in Decomposing Systems into Modules" [that]](http://sunnyday.mit.edu/16.355/parnas-criteria.html) answers your *why* question, and goes on to explains *how* to do it.
Hello, do you know if you can do smt based bounded model checking with clash for designs in clash?
Now you are making excuses not to finish them :)
I use modules as abstraction barriers. A module exposes a bunch of black boxes that should compose together sanely, if the types align. They also talk about very domain specific things. For example, rather than so general `STM Map` type that I use to match job ids to job progress (if I'm building some kind of concurrent job processor), I will literally have a module `JobMap` that exposes just the interface for managing jobs. I find this approach helps me boil down the essence of the system and keep asking the question "what is this thing responsible *for*".
_quietly goes away and fixes his tests..._
I prefer to err on the side of having more granular types (e.g. I'd use one in your Bio example like `newtype Bio a = Bio (Either a a); pattern TooLong = Bio . Left; pattern Ok = Bio . Right` ...). If it gets too cumbersome, the problem can usually be solved via helper functions or you can get rid of the newtype as a last resort, but not using them means that one might forget to handle a case somewhere and the compiler can't help you with that.
One specific example not (yet) explicitly mentioned involves public vs private APIs. Many libraries offer public interfaces which should be stable, even when the underlying representations may change, as represented by the primary module for users of the library. Those developers often choose to implement an `.Internal` library, which exposes everything (at a cost of more frequent breaking changes). Some libraries include `.Prelude` modules. Some developers (especially u/Tekmo) also create `.Tutorial` modules. Some large modules I've written span many layers of abstraction. These modules often have enormous lists of imports. Splitting these into smaller modules often introduces important abstraction barriers, making the code easier to understand (both for others and future me). The number of imports per module give me a clue here.
Rust does both. The rust book suggests unit tests within the source files (in separate modules) and integration tests in the `test` directory. I've found it makes simple unit and property tests very convenient (I usually use `hspec` with Haskell). However, it means `cargo` can't differentiate between imports for the library vs the test suite. Rust also has `doctest` behaviour built in to `cargo`.
`guanxi` has a bit of a different focus than `kanren`. In particular, I'm interested in meaningfully integrating SMT-like and abstract interpretation tricks to do better tree search. Kanren is basically doing a DPLL-style search with a weird choice of search strategy. How do you properly add (abstract) conflict directed clause learning to prune more of the search space? How do you use better abstract domains so you don't have to guess concrete values? How does my work on propagators apply? etc. The name is a bit of a joking dig at kanren. Both `guanxi` (in Chinese) and `kanren` (in Japanese) are words for relations/relationships. The difference is that the Chinese word carries a bunch of other connotations. 關係 captures the idea of the complicated network of relationships you need to maintain in order to actually get anything done. ;)
By Hedberg's theorem, if a type has decidable equality then it is a h-set, or conversely, if a type is not a h-set then it cannot have decidable equality. I think it is reasonable to have bags with decidable equality, which means they must be h-sets.
Will this be recorded? A lot of the talks there seem interesting, but I don't have the £1000 or so to attend...
Modules are also great for compile times: if you have a million lines of code spread evenly over a hundred modules, if you make a change you only need to recompile ten thousand lines, which is much faster than recompiling the whole million lines. This for instance is one of the primary reasons C++ is getting modules.
&gt; I think it is reasonable to require bags to have decidable equality If they contain items for which equality is undecidable, I don't think that's a reasonable requirement.
&gt; You'd expect Bag's to be equivalent to the type of sorted lists 1. I'm not sure I would. I expect bags to be more general and not require an ordering on the elements. 1. It seems to be that lists of non-sets would not be sets either, since you should be able to glue together a non-refl (x = y) and (xs = xs) to get (x : xs = y : xs).
* A compiler for tensor algebra, WIP : https://github.com/ocramz/taco-hs * Getting the DataHaskell monorepo project off the ground : https://github.com/datahaskell/dh-core * Organized and hosted a Haskell/Purescript meetup here in Göteborg, Sweden
I've been continuing work on "Ring of Worlds", [my web card game in Haskell and Elm](https://github.com/RoganMurley/Ring-of-Worlds). The biggest Haskell thing for me this year was writing the card effect EDSL. Effects can "metaprogram" other effects, so I went with a Free Monad approach. Interpreting the EDSL generates a list of state diffs and animations, which are sent to the client to visualise. This ended up being extremely powerful and got me a replay feature for free!
It has todo with our human limitations: we can hold more or less 7 facts in our short term memory. Push a some facts in, a few others will fade out. They just vanish without any warning. Using modules is basically a way to reduce complexity. We look at the module through their interfaces (fewer variables to keep in mind). Have a look at Bartosz Milewski's [enlightening explanation](https://bartoszmilewski.com/2014/11/04/category-the-essence-of-composition/) ("Composition is the Essence of Programming" section).
Oh, I've just learnt about purescript-specular from this answer! Been wondering if someone will make such attempt, now, before I'll have a chance to properly evaluate it, could you share your experience with it? 
1. I expect Bags to be more general too, but I expect them to coincide when they are both defined (The type has a linear order) 2. I could be wrong, but I don't think Types that aren't sets can have a linear order, so sorted lists don't really say what Bags of non-sets should look like, but yes lists of non-sets would not be sets
I newtype everything that has different semantics from the base type. That's admittedly somewhat subjective but when in doubt I usually just add the wrapper. For Bio I would use a newtype wrapper around Text, possibly with a smart constructor that logs a warning but continues anyway for invalid inputs. 
But now the variable bound by a command is no longer located on the same line
I'm continuing to work on using a Haskell dialect for teaching mathematics to children. This school year, I have my first chance for a long time to teach three days per week during the school day, so I'm creating learning resources and implementing a lot of speculative conclusions from the past few years, to get feedback on how they work in practice. I've recently finished a complete first draft of the first unit of curriculum, so I'm on-pace to keep up with teaching so far. At the same time, I've been continuing to fine-tune the CodeWorld API and platform by observing details that cause problems in class and tweaking the UI to address them. A couple weeks ago at the Norcorss Haskathon, I re-enabled GHCJS incremental linking to reduce download sizes by a fair margin in the server, and have been working on modernizing the API to fit what I've learned from teaching. At the same time, I'm working on ways to make the platform more useful to the core Haskell community. In addition to using the CodeWorld graphics API, you can also build simple console-mode programs that write to stdout, and also use QuickCheck for property testing. I'm interested in adding support for diagrams, reflex, and gloss (CodeWorld was originally based on gloss, but it's diverged over time, and I'd like drop-in compatibility for true gloss programs). So those are next on my list, aside from general bug fixes and maintenance.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [soupi/msg/.../**client** (master → 2402059)](https://github.com/soupi/msg/tree/240205900868f46469258fa52de34ab78c19ac0d/app/client) ---- 
I used to want this so badly, but ever since I discovered R I'm not sure Haskell will ever catch up in terms of interactive data analysis and exploration. I really hate to be a Debbie Downer here, but the fact is R and Python have huge momentum in statistics, machine learning, visualization, and data science. I don't see Haskell catching up. Haskell is by far my favorite language, and it is my language of choice for any non-trivial application going to production, but the language isn't all that matters; the ecosystem around the language is arguably more important. R is an awful language, but actually _using_ it with some proper, modern libraries for data analysis is a joy. I treat it like an interactive statistics/visualization DSL, and I wouldn't dream of writing a huge nontrivial application in it, but for interactive use and small data science-y programs it is awesome.
Yes, that's my reading as well. I'm willing to accept some pain, because my long-term goal is to build bigger data science applications in Haskell. And for that, there's huge value in being able to do the analysis in Haskell as well instead of having to rewrite all of it for production. But it's quite possible that it's too much to overcome the enormous benefits of the ecosystem, which is fantastic for both R and Python.
What I'm gonna say might be an unpopular opinion here in /r/haskell: A solid programming language (with a decent type system) is essential to produce quality software, especially as the software grows and the maintenance cost start kicking in. Does it mean that from the very first line make sense to use one of these programming languages? I don't think so. The problem is, many times, you don't know how long a piece of code is going to live and when it's going to start growing, so it's tricky to anticipate which technology is more suitable. For small exploratory tasks, I'd go with the technology that has less friction and offers the results you are looking for. If it's Python / R / whatever, go for it. Could it be the case that in 50 lines of spaghetti Python there is a bug and Python doesn't help you? Maybe. Factor in how critical is this code, are you doing super critical quantitative analysis that cannot be wrong? Maybe Haskell makes sense. Maybe not. If the Haskell ecosystem doesn't offer a rich environment of tools you need, what's the point of pushing in this direction. Having said that, if you are building a bigger piece of software, please use a sane language, Haskell is great for that. As an example, if I have to maintain a messy bash script that grows beyond 30-50 lines, yes, I seriously consider to move it to something else, even Haskell. Messy code beyond a few tens of lines shouldn't be acceptable. My personal trade-off would be: - Are these scripts small and written as a one-off exploratory tasks? Keep using whatever works better for you. - Are these scripts getting big? Maybe you need to drop Python. - Are these scripts being used by more than one person? Depending how messy they are, switching to something more principled will help to keep people productive using and extending them. - Are these scripts ending up in some pipeline in production? Please please please, think about switching to something more maintainable. BTW, have you look at Julia (https://julialang.org/)? I know it's a dynamic language but it support gradual typing, so it might help writing better code.
Mostly Servant :)
While I've got nothing to currently show for it publicly (as all work done so far is local), I've been spending the last few months occasionally working on the development of a new [entity-component system](https://en.wikipedia.org/wiki/Entity%E2%80%93component%E2%80%93system) framework for Haskell. Fundamentally, it takes the performance and rigor of [apecs](https://hackage.haskell.org/package/apecs) and (mostly) seamlessly combines it with the simplicity and usability of [ecstasy](https://hackage.haskell.org/package/ecstasy). Right now I'm at the fifth or sixth revision of the framework. Each revision has had major improvements to the simplicity (theoretical and practical) and usability. Earlier benchmarks for earlier revisions generally lead performance to be on par with or better than [apecs](https://hackage.haskell.org/package/apecs), though the current revision hasn't had its performance confirmed yet. I'll probably announce more information about the framework within 1-2 months, once it's ready. If anyone is interested in hearing more about the framework and its theory, feel free to ask questions right now! However, for the next week or so, I'll have to work on miscellaneous university-related work, so I won't necessarily be able to answer immediately.
I have absolutely no idea.
You may want to take a look at https://tweag.github.io/HaskellR/ and jupyter-haskell for exploratory work and https://github.com/tensorflow/haskell when you have a few models to play with. I think the difference between Haskell and Python's suite of tools is that of maturity and network effects. That alone may be the deciding factor to use Python.
Glad you found the DH `current-environment` page useful; please don't hesitate to flag (https://github.com/DataHaskell/docs/issues) anything that's missing from the list.
operators, like (:&lt;), (:&lt;?), (:&lt;:) : their usage is not going to be chained; hence using infix symbol as their name is not beneficial [i know, this is opinionated]; hence i would have chosen regular names, like "Contains c cs" instead of "c :&lt; cs"
TL;DR: If your mentality is, "I want to use Haskell for data science," then it's worth giving it a shot because there are a bunch of helpful people here, a lot of cool libraries, and a great programming language. If you are coming at it from the outside and asking, "Is using Haskell for data science a good tech choice?" then I think the answer is probably no. Everyone here is saying good things that I agree with. I think of it along two axes of problems: - Not enough users to rub down friction points - Not enough cookbook-style examples If you have a one-off data analysis task, I'd also recommend taking a look at R. It will probably install more easily and faster than a competitive Haskell environment. If somebody has posted a 20 line R script that does exactly what you want to do, your job has been made easy. Take the win and be happy. I try to offer specific problem-solution examples of using `Frames` in [the README](https://github.com/acowley/Frames#use-cases) and [the tutorial](http://acowley.github.io/Frames/), but this is a minuscule amount of content compared to what is available for the more popular choices. A side benefit of cookbook-style examples is that their absence can indicate a lack of features. This isn't a black-and-white more-is-better situation, but there are a lot of things that *should be easy*. If they're not so easy that they lend themselves to a "23 Amazing Data Visualizations... Number 12 Will Inform You!" listicle, then we need more helper functions. I think R is great at this. All that said, if you want to run an analysis as part of some larger pipeline, or you want data loading and massaging to fit into a larger Haskell application, then I'd certainly encourage you to try out what we have, and open Issues if something doesn't work the way you want it to. A quick note on use cases: we can use [significantly less RAM](https://github.com/acowley/Frames#benchmarks) than pandas. The value of types is always hard to quantify, but not entirely different in this context than any other: existing code that works is worth a lot, regardless of language; development with types can be great.
Indeed.
Great series! For what it's worth, I prefer using explicit structural witnesses over the just `~` or typeclass witnesses; to me, as first class data, they're much easier to manipulate and inspect. There's the common `Index` type: data Index :: [a] -&gt; a -&gt; Type where IZ :: Index (a ': as) a IS :: Index as a -&gt; Index (b ': bs) a Where it's only possible to construct a value of type `Index as a` if `a` is somewhere inside the type-level list `as`. Then your `move` type could be: data Move (phase :: Phase) where DrawCard :: Index '[ 'Draw ] phase -&gt; Move phase EndDrawPhase :: Index '[ 'Draw ] phase -&gt; Move phase EndMainPhase :: Index '[ 'Draw ] phase -&gt; Move phase EndTurn :: Index '[ 'End, 'Main ] phase -&gt; Move phase Note that the `=&gt;`s turn into `-&gt;`'s, meaning nothing is magical anymore, and all explicit. Then you could write `EndTurn IZ :: Move 'End`, and `EndTurn (IS IZ) :: Move 'Draw`. You can return to implicitness by using a typeclass like: class Known p a where known :: p a instance Known (Index '[a]) a where known = IZ instance Known (c ': as) a =&gt; Known (Index (b ': (c ': as)) a where known = IS known So you could write `EndTurn known :: Move 'Draw`, and `known` will be instantiated to `IS IZ`. The main benefit is that you turn a typeclass constraint/type family definition into just a normal first-class data type, so you can manipulate membership witnesses, which comes in handy when you start doing complicated things like generating new witnesses from old witnesses at run-time. Also, using GADT witnesses like this makes it much harder to write bugs; the definition of `In`, it's possible to write a bug (like return `IsIn` on every branch by accident). All type families risk this, essentially, without external proof mechanisms. With a GADT system you close off a source of bugs in the definition of a type family.
Yeah, but if bags are not an h-set then you lose decidability even in the case the element type does have decidable equality.
I'm working on [hledger](http://hledger.org)'s next quarterly release, as always - - issue/PR management, code review, design discussions - keeping up with the latest deps and tools - the next iteration in our docs infrastructure - probably moving dev docs into the main repo and moving user docs to website (instead of github wiki) - more expressive account declarations, eg to declare account classes and to detect misspelled/disallowed account names - hledger-ui cleanups, enhancements - about a zillion other things I wish I could get to :)
Thanks for the feedback. I like your suggestion, though I think for this particular instance, I might not switch to it yet. &amp;#x200B; I tend to use dependently-typed languages where indeed we can manipulate all those witnesses readily, but here, all I care about is a little bit of help from the type-checker at a low overhead in terms of writing/reading my code. But yeah, using `known` to instantiate the witness is a great idea, and I could definitely wrap all this up nicely. &amp;#x200B; Ideally, I'd like to have even more guarantees, for instance, knowing that all the moves that are declared as a possibility for a phase \*are actually\* returned by that phase (for instance, in those posts, I forgot to return `EndTurn` for some phase) . But this might require LiquidHaskell (which, unfortunately, I still believe is a bit too unstable to rely upon), or switching to something like Agda/Coq/Idris (probably the latter, since I care about having IO and effects readily available). &amp;#x200B; I definitely agree with you that it's a little scary to use type families in Haskell to essentially build little proof gadgets. &amp;#x200B; Let's see, the things I care about for this are: 1. \[Major\] The compiler should not let me return a move that does not belong in the declared phase. 2. \[Major\] The compiler should consider a case exhaustive when I have handled all moves for the declared phase. 3. \[Minor\] When I return a wrong move for the declared phase, the error message is somewhat helpful. 4. \[Minor\] When I have an extraneous case for the declared phase, the error message is somewhat helpful. 5. \[Minor\] When I am missing a case for the declared phase, the error message is somewhat helpful. 6. \[Minor\] More crazy guarantees like I mentioned earlier My current solution 1. Good 2. Good 3. Decent: `• Couldn't match type ‘'In.IsNotIn 'Main '['Draw]’ with ‘'In.IsIn’` 4. Decent: `• Couldn't match type ‘'In.IsNotIn 'Main '['Draw]’ with ‘'In.IsIn’, Inaccessible code` 5. Decent: `Pattern match(es) are non-exhaustive, In a case alternative: Patterns not matched: Move.EndTurn` 6. Nope Your solution: 1. Good 2. Nope :-( 3. Decent: `• Could not deduce (Known (Move.Index '['Draw]) 'Main) arising from a use of ‘known’` 4. Nope :-( 5. Decent, except it also lists the ones I don't want: `Pattern match(es) are non-exhaustive, In a case alternative: Patterns not matched: Move.EndTurn, ...` 6. Nope &amp;#x200B;
In the hledger project, I'm not worried about contamination production with test code, as cdsmith talks about, yet. Nor about the cost of building and shipping tests in production - that's negligible. I'm speaking about "unit" tests here, which tend to be small, fast, portable, stateless, safe to run. In fact I like having the tests built in and expose them as a command developers and even users can run - not that useful you might think, but in fact I think it can be pretty useful to be able to ask any user reporting a weird bug to be able to immediately run a bunch of tests on their platform as a sanity check. It also simplifies developer workflows (fewer flags, directories, failure modes with tooling). I'm much more concerned with minimising friction for developers (myself and others), so that there's no resistance to writing/updating/improving tests. Minimising the "distance" between code and related tests is one way to reduce that friction, and can also improve code comprehension (code with a bunch of examples right next to it is easier to understand and use). Of course it can also hurt comprehension by adding too much clutter. I've experimented with unit tests (first hunit, then easytest) written right after the function they test. My current compromise is to collect unit tests at the bottom of each file. There are trade-offs.
I'm not sure that's true. It's not got a good runtime, but I could determine the exact places to call permute / refl (or show both are contradictions) to decide xs = ys between to bags give these constructors and access to a decision procedure for x = y in A.
How do you dependency injection in Haskell? How do you answer this question to people from Java? My Java coworker asked me this question. My initial thought was Haskell doesn’t have/need a dependency injection. I use function compositions and module instead. And I know it wasn’t a good answer and it confused him more. But I don’t know how to explain better. 
DI in Haskell, in as beginner friendly terms as I can think of to show it: -- File one Module One where printWithAdjustement :: Show a =&gt; (String -&gt; String) -&gt; a-&gt; IO () printWithAdjustment f input = putStrLn (f (show input)) -- File two Module Two where import One takeFive = take 5 printFirstFive :: Show a =&gt; a -&gt; IO () printFirstFive = (printWithAdjustment takeFive) -- Main import Two main = do printFirstFive "HIHIHIHIHIHIHIHIHIHIHIHIHIHIHIHIHHII" printFirstFive 1234567890 printFirstFive [1,2,3,4,5,6,7,8,9,0] 
I haven't used it, but there's [hs-di](https://hackage.haskell.org/package/hs-di).
You can lift an error exception into a base monad: lift (Left "no i really mean fail") :: ParsecT e s (Either String) a 
Have you tried [Data.Vector](https://hackage.haskell.org/package/vector)?
Vector is O(n) snoc/cons, though
I've looked at it, but unless they are lying or I looked at the wrong version, [cons is O(n)](https://hackage.haskell.org/package/vector-0.12.0.1/docs/Data-Vector.html#g:12) there.
The data structure you're looking for is difficult to provide in a persistent form, which makes it not very idiomatic in Haskell. It relies on the ability to mutate shared memory during append operations. But what would that means for something like this: let a = ArrayList.fromList (replicate 99 'a') b = ArrayList.append 'b' a c = ArrayList.append 'c' a Only one of `b` or `c` can use the same underlying storage block as `a` to store its new value. Which is it? You could pretty easily implement a mutable ArrayList in ST or IO, though. I'm not aware of existing implementations, but it's a straightforward bit of code.
This is getting into Haskell arcana, but is it possible to have `b` use the same storage as `a` and have `c` get a copy (incurring an O(n) overhead)? That is, have it provide a functional interface, but with optimizations for linear access. The `array` type in Lean (a dependent FP language) acts like this, although the data structure there is a bit complicated since it supports a variety of access patterns.
I generally have to process tab separated files and then find the largest value in a column, or average out some other data. I end up doing this in perl because I have no idea how to get started in Haskell A recent example I had was a 3 column data file, of type `A_xx B_yy value`. For a given A_xx and B_yy combination, I need the largest `value`. Can you please tell me how to write this?
Does anyone have resources which discuss how one might generalize recursion schemes to higher kinded data, especially data that's like `HList`? For example, what should I do in the second half of this code block: {-# LANGUAGE PolyKinds #-} {-# LANGUAGE NoImplicitPrelude #-} {-# LANGUAGE LambdaCase #-} {-# LANGUAGE TypeOperators #-} {-# LANGUAGE GADTs #-} {-# LANGUAGE RankNTypes #-} {-# LANGUAGE DataKinds #-} {-# OPTIONS_GHC -Wall #-} module HFix where import Prelude (($)) -- Normal recursion schemes data List a where Nil :: List a Cons :: a -&gt; List a -&gt; List a class Functor f where fmap :: (a -&gt; b) -&gt; f a -&gt; f b instance Functor List where fmap f = \case Nil -&gt; Nil Cons x xs -&gt; f x `Cons` fmap f xs data ListF a b where NilF :: ListF a b ConsF :: a -&gt; b -&gt; ListF a b instance Functor (ListF a) where fmap f = \case NilF -&gt; NilF ConsF a b -&gt; ConsF a $ f b data Fix f = Roll {unroll :: f (Fix f)} class Functor f =&gt; Recursive f where cata :: (f a -&gt; a) -&gt; Fix f -&gt; a instance Recursive (ListF a) where cata f x = f $ fmap (cata f) $ unroll x -- heterogeneous recursion schemes data HList f xs where HNil :: HList f '[] HCons :: f x -&gt; HList f xs -&gt; HList f (x ': xs) class HFunctor f where hmap :: (forall x. g x -&gt; h x) -&gt; f g xs -&gt; f h xs instance HFunctor HList where hmap f = \case HNil -&gt; HNil HCons x xs -&gt; f x `HCons` hmap f xs -- data HListF r f xs where -- HNilF :: HListF r f _ -- HConsF :: f x -&gt; r f xs -&gt; HListF r f _ -- instance HFunctor (HListF r) where -- hmap f = \case -- HNilF -&gt; HNilF -- HConsF x y -&gt; _ -- I think this is where I start to go wrong data HFix r f x = HRoll {hunroll :: r (HFix r f) x} -- class HFunctor g =&gt; HRecursive g where -- hcata :: (forall x. g f x -&gt; f x) -&gt; HFix g f -&gt; f y 
It seems like adding length doubling would strictly improve the asymptotics of `MVector`, which is why most languages do it out of course. Is there a reason it's not implemented this way in `Data.Vector`?
It is possible to have whichever of b or c is evaluated first claim the next block of the underlying buffer. It's not possible to make that depend on their declaration order in the let block. As you say, this makes linear access efficient, but you might find it quite tricky to reason about whether your use case really is linear. Depending on evaluation order is fraught with fragility.
It sounds like you're trying to write C in Haskell. That's fine, and you can do it, but if you want to do that, you should look at the C-style subparts of Haskell -- IO and ST. The data structure you are asking about ("ArrayList") is commonly known as `deque` in C++. The asymptotically growing version with O(1) snoc but no cons is called `vector`. I don't believe there are Haskell implementations of either of these in the standard library, but neither are particularly complicated to implement.
Thank you, I will try this.
But we do have backpack now since some time. Just not sure about its adoption...
Looks fun. A couple typos/errors I noticed on a quick browse of the opening sections: `catch` in `CojoinedMonad` has the wrong type catch :: c e a -&gt; (a -&gt; c e' a) -&gt; c e' a -- should be catch :: c e a -&gt; (e -&gt; c e' a) -&gt; c e' a -- or perhaps to more clearly mirror &gt;&gt;= catch :: c e a -&gt; (e -&gt; c f a) -&gt; c f a The implementation of `Applicative` for `Const` violates Applicative's laws. I think this instance was intended: instance Monoid m =&gt; Applicative (Const m) where Const x &lt;*&gt; Const y = Const (mappend x y) In particular, I believe Applicative requires `fmap (\x f -&gt; f x) a &lt;*&gt; pure id == a`, which isn't the case with the given definition.
This for the Move: [http://paste.awesom.eu/8KzJ](http://paste.awesom.eu/8KzJ) This for the MainPhase: [http://paste.awesom.eu/Omdz](http://paste.awesom.eu/Omdz) The warnings are on the line: `mainPhase = validMoves &gt;&gt;= GameEffects.chooseMove &gt;&gt;= \case` where `validMoves :: ( GameEffects e ) =&gt; Eff e [Move 'Main]` So, in my version, I get no warning on the `\case`, but using yours, I get: &gt;Pattern match(es) are non-exhaustive &gt; &gt;In a case alternative: &gt; &gt;Patterns not matched: &gt; &gt;(Move.Attack \_ \_ \_) &gt; &gt;(DrawCard \_) &gt; &gt;(Move.EndBattlePhase \_) &gt; &gt;(Move.EndDrawPhase \_) &amp;#x200B; (also, as an aside, should the instance not be: `instance Known (Index as) a =&gt; Known (Index (b ': as)) a where` ?)
Ah yeah. GHC can prove that those cases don't work, but it does need some help at the moment. You'd write: Move.Attack i _ _ -&gt; case i of {} And that will make GHC happy. I think it might be because of the semantics w.r.t non-strictness? It unfortunately still requires you to write down the pattern, but ghc will make sure that you don't do anything dangerous with it. RE: the instance, that instance should work, as well, but It's nice to explicitly write the cons constructor pattern in `as` so that we don't have any issues with overlapping instances. Remember that the first pattern matches `b ': []`, which should also unify with `b ': as`. I understand your point about the one-off, quick and convenient way. Using a type family to juggle around magic/invisible constraints is a simple thing to do in the short term. If you don't ever do anything too complex with membership it should be "fine", but things get hairy if you do anything dependent. I understand that there's a trade-off, so i'd say I agree that it's not 100% something to do every single time :)
This is indeed the best solution
[impure-containers](https://hackage.haskell.org/package/impure-containers-0.4.3/docs/Data-ArrayList-Generic.html) provides an implementation of `ArrayList`s. You only get O(1) snoc, not cons but afaik the same holds for Java’s ArrayList.
Do the following: newtype HFix f x = HRoll {hunroll :: f (HFix f) x} hcata :: forall f a j. HFunctor f =&gt; (forall i. f a i -&gt; a i) -&gt; HFix f j -&gt; a j hcata f (HRoll x) = f $ hmap (hcata f) x The `Recursive` classes are unnecessary because `cata` and `hcata` are already implementable for everything without dispatching on types. Also, they both can be written a bit faster, demonstrated for `hcata`: {-# language ScopedTypeVariables #-} hcata :: forall f a j. HFunctor f =&gt; (forall i. f a i -&gt; a i) -&gt; HFix f j -&gt; a j hcata f = go where go :: forall j. HFix f j -&gt; a j go (HRoll x) = f (hmap go x) 
Seems like you can get those properties-ish by mimicking Go slices, which have immutable `append` and are backed by arrays. 
&gt; In particular, I believe Applicative requires fmap (\x f -&gt; f x) a &lt;*&gt; pure id == a, which isn't the case with the given definition. That Const instance is well known and implemented in base. I'm pretty sure it's correct.
This is because you're using selenium version `&gt;2`. Please downgrade selenium-standalone to version 2 and try again. https://github.com/kallisti-dev/hs-webdriver/pull/144
Fair enough, there is clearly more work to do in order to present Haskell as an easy to use language. Additionally, those optimization techiniques he used seemed as standard practice, such as never using String when perf is important. This just point out at where time should be spent as a community of ease of use is one of our goals. That said, I’m pleased Haskell is competitive against go in performance :)
I'm not familiar with rrdtool but is it necessary to create a system process? It sounds Go used goroutines, but Haskell code spawned a system process. But anyway, this is somewhat I encounter daily at work. It is so easy to write a performant code in Go due to the goroutines... I'm more impressed with the OCaml result though.
Same I was actually impressed by the memory use of haskell being comparable to Go (even in the worst case)! I've always thought one of the biggest drawbacks of Haskell was the memory usage patterns/leaks/etc (as well as laziness surprises), but compare that to OCaml and Haskell is doing quite well.
The rust community made things 'easier' by making error messages less cryptic: https://blog.rust-lang.org/2016/08/10/Shape-of-errors-to-come.html I liked that approach as it didn't involve any ongoing efforts.
Error messages in GHC 8.4 are somewhat similar to those. Definitely better than in previous versions, that's for sure!
very impressive stats
I also tried to do something similar: I implemented the same webserver (fetch \~5 rows from PG, using some native DB data types (arrays), render HTML using embedded DSL) in many languages (Common Lisp, Go, Haskell, Ocaml, Java, Clojure, Python) and compared the quality of the resulting solutions. &amp;#x200B; Haskell was nice, but I spent quite a lot time digging for usage examples and documentation. The tooling is still quite poor. The performance was OK, but mostly because I have some experience in Haskell optimization and used Text everywhere right away. The bottleneck of the whole application is blaze-html, btw. &amp;#x200B; Ocaml is great: great tools, impressive performance and low latencies, but I had a lot of problems with the libraries. First time I tried, I couldn't find a working PG connector. Later I've found [Caqti](https://opam.ocaml.org/packages/caqti/), but I had to implement array datatype parser myself (I used [Angstrom](https://github.com/inhabitedtype/angstrom)). &amp;#x200B; Go version was really slow at first (comparable to async python), but the performance improved drastically (\~4x) after I switched from \`\`\`net/http\`\`\` to \`\`\`fasthttp\`\`\`. I couldn't find any DSL for HTML generation at all. The tooling is ok, but not perfect. I find Ocaml tooling being far superior. &amp;#x200B; Common Lisp was really good. Emacs + Slime is a dream, [postmodern](https://github.com/marijnh/Postmodern) mostly worked out of the box (even array parsing), the code was really fast, the latencies are pretty low, and performance is comparable (and sometimes better) than that of Haskell. I had some small problems with integrating the HTML DSL (cl-who) though and had to debug some exceptions, but the overall impression was very good. My biggest concern is that the Lisp image becomes polluted while I change my code, and things tend to stop working after I restart the application. &amp;#x200B; Java version was the most performant one after I found the right implementation of the connection pool, but it was really painful to write a small application in it (I was a Java developer for a long time and used IntelliJ IDEA instead of Emacs, and it was still painful).
I'm not sure what you mean by "call permute/refl". Besides, if by any means you can decide equality on bags, then bags are an h-set, but I'm considering the case where they are not.
See also [a follow-up post on the libraries used in the experiment](https://pl-rants.net/posts/libraries-vs/).
&gt; I'm not sure Haskell will ever catch up in terms of interactive data analysis and exploration This attitude certainly won't help build tools and libraries &lt;/polemic&gt;. More seriously, I think no language ecosystem is "made" for a given task until people make it into that. Python wasn't certainly born a modelling language, and R was a curiosity from the GNU project until not so long ago. It takes a long, coherent effort to create something like what they achieved.
Interesting. Which implementation of Common Lisp did you use? SBCL?
Miss my Emacs + Slime days. For some time we even used to patch bugs live in production without restarting the server. Sigh.
The Background is also a good introduction: https://docs.haskus.org/eadt/background.html
I'm interested in having a sort of tagging system based on type-level symbols, to reduce `data`/`newtype` declarations. That is, suppose a user might have read an article or not, and might have liked or not, independently. We might have articleState :: ... -&gt; (Bool, Bool) but that's not clear what each Bool means. So we might do data ArticleState = { hasRead :: Bool, hasLiked :: Bool } articleState :: ... -&gt; ArticleState or data ReadState = ReadYes | ReadNo data LikedState = LikedYes | LikedNo articleState :: ... -&gt; (ReadState, LikedState) or newtype HasRead = HasRead Bool newtype HasLiked = HasLiked Bool articleState :: ... -&gt; (HasRead, HasLiked) and those are all fine, but you can't really use any of them by just importing `articleState`, which is kind of annoying. I'd like something like articleState :: ... -&gt; (Tagged "HasRead" Bool, Tagged "HasLiked" Bool) and when you have a tagged Bool, you can do case tagged of Tagged "HasRead" True -&gt; ... Tagged "HasRead" False -&gt; ... or if (untag "HasRead" tagged) then ... else ... where the type system would complain if you used the wrong tag or no tag. I tried a few things but couldn't quite get it to work. Something like data Tagged (tag :: Symbol) a = Tagged a let tagged = Tagged @"HasRead" True compiles, but you can mostly just ignore the tags and type inference will supply them for you. Also it looks like you can't use type applications in a pattern? (So no `case Tagged @"foo" True of { Tagged @"foo" True -&gt; ... }`.) Is there some way to do this?
Link in post description requires login. Link in your comment doesn't. Which link is right one?
Oh dear. I linked to the wrong thing. I'll delete this post and make a new one with the correct link. Thanks for the heads up! Sorry! 
Link in the post is crap. First requires login; if you use a google account, it needs to request access to see and download your contacts (!?), and once you allow it it says "You do not have access to this base".
I am excited to announce the 2018 State of Haskell Survey! This is the second annual State of Haskell survey. I am happy to say that this year the survey is co-sponsored by Haskell Weekly and Haskell.org. The goal of the survey is to better understand what people think of the Haskell programming language, together with its ecosystem and community. Whether you have never used Haskell or you use it every day, we want to hear from you! The survey opens today, November 1st, and stays open for two weeks. It closes on November 15th. Please take a few minutes to fill out the survey! We want an accurate picture of the Haskell community, so please share this link to help us out: https://bit.ly/haskell2018. Thanks!
&gt;Just FYI, &gt; &gt;blaze &gt; &gt;, which essentially lets you build HTML using an expression tree in your code &amp;#x200B; I was talking about Go in this paragraph, not Haskell. For the haskell version I used blaze-html. As I mentioned, it was the bottleneck of the whole application.
Oh! I misread your comment. Yea, you're right.
Perhaps you could pin this post to the top of the page until the survey expires? 
You can define a type equality operator as a type family (`(Data.Type.Equality.==)`) that doesn't result in unification constraints, so that both sides need to be specified separately. In this case one is the inferred, expected tag, whereas the other is the manually provided one. {-# LANGUAGE AllowAmbiguousTypes, DataKinds, TypeFamilies #-} import Data.Type.Equality import GHC.TypeLits data Tagged (tag :: Symbol) a = Tagged a -- smart constructor with mandatory explicit tagging tagged :: forall tag tag' a. ((tag == tag') ~ 'True) =&gt; a -&gt; Tagged tag' a tagged = Tagged test :: Tagged "3" Int test = tagged @"3" 0 
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [darioteixeira/pgocaml/.../**PGOCaml.ml** (master → 4ecb0c5)](https://github.com/darioteixeira/pgocaml/blob/4ecb0c55c9e4d775abe1a9ad27c2026ae4386f78/src/PGOCaml.ml) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply e8ugj01.)
Just charge through it. Most of them are clickety-clack answers.
What am I supposed to do with the hearts? Are they supposed to to be some sort of "meta rating" of the various review sections itself? If yes, then they should be at the end of each section, not the top.
Note: this exact `Tagged` type exists in a library. No need to reimplement it!
This is really cool. I wonder how hard it would be to bootstrap GHC with one of these. You'd probably need to start with *really* old GHC sources, but it'd be really cool to go from just a C compiler to GHC 8.6.
&gt; did you not find pgocaml, or were you just not able to get it working? Actually, I was the first library I tried, but I had problems trying even to build it. `opam install pgocaml` failed with mysterious error messages. Even now, after several months, it still doesn't build out of the box on my machine: #=== ERROR while installing pgocaml.2.3 =======================================# # opam-version 1.2.2 # os linux # command make # compiler 4.06.0+spacetime # exit-code 2 # [...] E: Failure("Command ''&lt;hidden&gt;/.opam/4.06.0+spacetime/bin/ocamlbuild' src/pgocaml.cma src/pgocaml.cmxa src/pgocaml.a src/pgocaml.cmxs utils/pgocaml\_prof.byte -tag debug' terminated with error code 10") make: *** [build] Error 1 I then checked out the source code and tried to build it from source, that didn't work either. I spent several hours trying to debug the problem but gave up in the end. In contrast, Caqti installed perfectly and worked out of the box.
Thanks!
O, you're very ambitious, success!
I'm just saying the existence of the `permute` equality constructor doesn't prevent me from producing a {A : Set} -&gt; (decEqA : (x : A) -&gt; (y : A) -&gt; Dec (x = y)) -&gt; (xs : Bag A) -&gt; (ys : Bag B) -&gt; Dec (xs = ys) Now, when `A` is not a h-set, then I don't expect `Bag A` to be an h-set either.
I don't see why.
If I remember correctly also the Intel one was open sourced some time ago, but I guess they don't allow redistribution?
1) this is not stackoverflow 2) I doubt even SO would accept a question with no repro steps 3) format the code blocks at least (4 spaces) 4) add repro steps
- when an `if` expression has type `Bool`, it can often be rewritten using boolean operators - idiomatic haskell uses `camlCase`, not `snake_case` - Use pattern-matching instead of `head` and `tail`. Incidentally, the comparison with `length towers` in `hopper` can probably be folded into the pattern-matching logic, because if `maxJumpDistance &gt;= length towers` (and if we don't check for it in `hopper`) then we would reach the end of the list in `hopperForNTails`. hopperForNTails :: Int -&gt; [Int] -&gt; Bool hopperForNTails n [] = True -- maxJumpDistance &gt;= length towers hopperForNTails 0 _ = False hopperForNTails n (_ : towers) = hopper towers || hopperForNTails (n - 1) towers hopper :: [Int] -&gt; Bool hopper towers@(maxJumpDistance : _) = hopperForNTails maxJumpDistance towers hopper [] = True -- Or "error" if this is not supposed to happen main = do print $ hopper [4, 2, 0, 0, 2, 0] -- will return True print $ hopper [1, 0, 2, 3] -- will return False 
Always go with what works. :) If I had to guess, pgocaml might depend on a slightly older Ocaml version? I'll check later to see what version I was building it with.
A pretty direct reading of the specification could be this: hopper [] = True hopper (n:ns) = any hopper (take n (tails ns)) But it might save some work to keep hold of the result at every position and finally just take the one from head: hopper' = head . foldr f [True] where f n bs = or (take n bs) : bs
Thanks! After posting, I ran into your [old post](https://www.reddit.com/r/haskell/comments/3sm1j1/how_to_mix_the_base_functorrecursion_scheme_stuff/cwyr61h/) on the subject and Bartosz's [brief mention](https://bartoszmilewski.com/2018/08/20/recursion-schemes-for-higher-algebras/) of it. I still don't see how to write `HListF` so that `HFix HListF == HList` (`==` to mean isomorphic). I also noticed that I need `HList :: (k -&gt; *) -&gt; [k] -&gt; *` rather than `HList :: [*] -&gt; *`, and wonder if that makes a difference.
Experimented a bit: pgocaml breaks after 4.04.0 (4.04.0 is the last switch where it works). Maybe I'll give it another try a bit later.
Not yet, but I think once the [type variables in patterns](https://github.com/ghc-proposals/ghc-proposals/pull/126) work and the [visible dependent quantification](https://github.com/ghc-proposals/ghc-proposals/pull/81) (for terms) get implemented we could do this by making `Tagged` a GADT. data Tagged :: Symbol -&gt; Type -&gt; Type Tagged :: forall (tag :: Symbol) -&gt; forall (a :: Type). a -&gt; Tagged tag a -- untag as pattern matching example untag :: Tagged "HasRead" Bool -&gt; Bool untag (Tagged "HasRead" True) = True untag (Tagged "HasRead" False) = False 
It says &gt; Each time you see a question like this, feel free to take a moment to let us know what you think of the survey so far so I really think it is literally just that. Maybe it would make a bit more sense to have them at the ends of sections but it doesn't seem to matter much.
Agreed. There's not much to it and we're all volunteers here.
Nice! Yeah, the issue I first saw that brought up the question was this one https://github.com/arrow-kt/arrow/issues/1007 I haven't used Kotlin a lot, but usually I find in a production haskell app i'll need to delegate out to a microservice at least once, usually due to a JVM library. When I've done it I've used Kotlin and really enjoyed it. I also miss how incredible Intellij is and Jetbrains as a company seems to do almost everything well Thanks for the response, I'm going to keep my eye on it going forward. I certainly couldn't own or maintain something like that, but I'd definitely be in the ranks of people who'd use it if it got good support
Neat, thanks! So in `tagged @"foo" True :: Tagged "foo" Bool`, the `@"foo"` supplies type variable `tag`, and `tag'` is inferred from the type signature. And `untag` is written similarly: untag :: forall tag tag' a. ((tag == tag') ~ 'True) =&gt; Tagged tag' a -&gt; a untag (Tagged x) = x Ah, but they can still be untagged in patterns, unfortunately: let ttrue = tagged @"foo" True :: Tagged "foo" Bool in case ttrue of { Tagged x -&gt; x }
Anybody else reading [The Book of Types](https://leanpub.com/book-of-types)?
Ah, good to know! That's one thing I really like about `opam`. It's so easy to switch between different versions/toolchains. (Since we're on /r/haskell -- `opam` is a package manager, but it also manages installations of the compiler itself. So you can easily install multiple compilers on your system, or multiple instances of the same compiler with different package-sets installed in them.) 
`getCPUTime` was giving me strange results when working on [`sleepp`](https://github.com/gallais/sleepp) today so I googled a bit and found this helpful write-up.
Yes, of course. View patterns are a bit ugly, but I'm not too worried about nesting, and `case untag @"foo" ttrue of ...` is totally fine - it just didn't occur to me for some reason. Thanks again!
This is a very reasonable and detailed case study. Very appreciated, thank you!
Selecting the extensions is the most tedious part if you really want to answer it. There are millions of extensions but I only want a few hundred thousands of those.
As a counterpoint I thought the survey was a great length. It is certainly a little longer than I would expect for a cold-call marketing survey, but I consider filling this out a service to the Haskell community. I'm glad /u/taylorfausak compiles this for us every year.
Java's ArrayList supports O(1) snoc and cons by treating the underlying buffer cyclically. It only needs to realloc when the total size of the list exceeds the buffer capacity.
Right, this is also the situation in Lean - you don't know which one gets the copy, and it is tricky to prove that your access is in fact linear (and if it's not, you may get a large performance hit). I wonder if the linear types extension will help here?
It's ironic that you say I'm writing C in Haskell. I'm actually writing a C interpreter in Haskell, and this array is the virtualized memory (so the O(1) is actually important).
Data.Vector's api is really wants you touse immutable vectors. If you don't want to manually reserve because doubling has the same asymptotically then you are probably be slower with the mutable interface. If you use Data.Vector.Generic.fromList [it will use doubling](Data.Vector.Generic.Mutable.html#vmunstream) which is why fromListN is preferrable. 
Yep, that would work too. You could implement that as a newtype on top of the existing Haskell ArrayList.
Data.Vector uses fusion and rewrites to inplace mutation for linear usage. The traditional deque type in haskell is okasaki's functional queue. Data.Sequence has O(1) at the ends, O(log(n)) random access, splitting and concat-ing. Using mutable types in haskell is rarely worth the hassle outside of numerical code. And if you do write numerical code you probably want c or c++, if only for the simd support.
&gt; HConsF :: f x -&gt; r f xs -&gt; HListF r f Remember that for lists, it's `ConsF :: a -&gt; r -&gt; ListF a r`, not `ConsF :: a -&gt; r a -&gt; ListF r a`! That's because the type of the elements is the same throughout the list, it doesn't change as we recur down the list, so we can pick it before we start recurring, and so the base Functor is `ListF a` (which is still missing its `r`), not `ListF` (which is missing both its `a` and its `r`). Similarly, since your `HList` uses the same `f` throughout the list, your base Functor should be `HListF f`, not `HListF`, and it should be `HCons :: f x -&gt; r xs -&gt; HConsF f r (x ': xs)`, not `HConsF :: f x -&gt; r f xs -&gt; HListF r f (x ': xs)`. &gt; -- I think this is where I start to go wrong &gt; data HFix r f x = HRoll {hunroll :: r (HFix r f) x} On the contrary, that's the right idea! The recursive positions now have kind `[*] -&gt; *` instead of `*`. If we remove the `f` as explained above, we get: data HFix (h :: ([*] -&gt; *) -&gt; [*] -&gt; *) (xs :: [*]) = HRoll { hunroll :: h (HFix h) xs } &gt; class HFunctor g =&gt; HRecursive g where &gt; hcata :: ... -&gt; HFix ... Note that the purpose of the `Recursive` typeclass in recursion-schemes is to make `hcata` work with more recursive types than just `Fix`, so if you're happy to use `HFix (HListF f)` instead of `HList f`, you don't need a typeclass. &gt; hcata :: (forall x. g f x -&gt; f x) -&gt; HFix g f -&gt; f y Hmm, that `y` at the end definitely looks wrong, as there is nothing in the rest of the type signature to constrain it. Since a single step of the recursion will have access to `ConsF`'s `r xs`, the result computed for the rest of the list, I think that single step has no choice but to return an `r (x ': xs)`, so it can be used by the outer step after this one, and thus overall the type-level list stays unchanged by `gcata`. So a type which would make more sense would be: hcata :: (forall xs'. h r xs' -&gt; r xs') -&gt; HFix h xs -&gt; r xs 
Oh, good one! I added it. They certainly allow repo forks, it's a BSD-ish license.
Time spent improving the documentation would be well spent for development of the community.
Two other interesting posts (and associated r/haskell discussions) related to that: [*Alternatives convert products to sums*](https://www.reddit.com/r/haskell/comments/2tfgol/alternatives_convert_products_to_sums/); [*Seminearrings*](https://www.reddit.com/r/haskell/comments/1x5bvf/what_the_heck_is_a_rightseminearring_also/). As far as I understand it, what it is generally agreed upon is that an `Applicative` is a [monoidal functor](https://ncatlab.org/nlab/show/monoidal+functor) from **Hask**-and-`(,)` to itself, and an `Alternative` is a monoidal functor from **Hask**-and-`(,)` to **Hask**-and-`Either` (i.e. it "convert[s] products into sums"). Additional distributivity and commutativity requirements aren't necessary followed by the instances we actually have (the discussions I linked to should illustrate some of that).
There's https://hackage.haskell.org/package/base-4.12.0.0/docs/GHC-Clock.html available now as well
Idk I don't find this particularly insightful. This is stuff that's been well-known for years and documented in the literature (in the case of the laziness observation). The author also seems to not believe there are people out there who know Haskell - it's one thing if he spends three times longer on the Haskell version compared to the Go version but acting like this is deep is inane. "my intuition about performance was always off..." uh okay 
&gt; I keep having the worry that exploratory data science isn't a great fit for Haskell I think Haskell is fine for exploratory data science in principle - it has a REPL just like Python. The problem is libraries. If you have experience writing FFI bindings you might have some success there. &gt; I didn't yet come across great use cases where the type safety and functional aspects would strongly improve the analysis What exactly are you doing? The simple answer is that existing languages are suited to performing matrix algebra relatively nicely. Personally I'd prefer maps and zippers over e.g. loops but ultimately both do work. You might have a look at [accelerate](http://hackage.haskell.org/package/accelerate) for some of the benefits of functional programming in data science. Particularly the ability to write code on arrays that works on both GPU/CPU. Type safety is great but when you're dealing with math (e.g. matrix algebra) you have theorems and it's easier to test your 
&gt; For small exploratory tasks, I'd go with the technology that has less friction and offers the results you are looking for. The advantage of Python (to me) is library support, not some mythical "less friction."
&gt; I think the difference between Haskell and Python's suite of tools is that of maturity Haskell has things Python doesn't too, e.g. Hoogle.
Impressive. Very interesting work
semirings package: [https://hackage.haskell.org/package/semirings](https://hackage.haskell.org/package/semirings) &amp;#x200B; &amp;#x200B;
you should look at \*-semirings. some links: &amp;#x200B; [http://stedolan.net/research/semirings.pdf](http://stedolan.net/research/semirings.pdf) [http://r6.ca/blog/20110808T035622Z.html](http://r6.ca/blog/20110808T035622Z.html) [https://byorgey.wordpress.com/2016/04/05/the-network-reliability-problem-and-star-semirings/](https://byorgey.wordpress.com/2016/04/05/the-network-reliability-problem-and-star-semirings/) &amp;#x200B; link #2 is my favourite, and it's a literate haskell file.
Could you recommend me some very simple plotting library ( I don't like custom operators ), which allows for updating the plot in real time? ( As I get results from some function. )
Yeah, agreed. Maybe I was too conscientious, but I ended up looking up a bunch of old and obsolete language extensions. There are also some in there that are just not reasonable to want on by default (for instance, \`RebindableSyntax\`). Next year, it would be great to curate these into the set that's really worth asking, even for a very liberal interpretation of "worth asking".
Never really used it..also, haven't really had a chance to dive into some codebase that relies heavily on ML-like modules. And I'm surprised, that very few languages follow that path (at least my impression). One of my favourite languages the progress I track - Idris - chosen the typeclass-like solution as well...
Have they improved significantly compared to 8.2? Could you give one or few examples?
It's more common for people to define custom data types for stuff like this. data MyType a bs = MyType a [bs] instance Semigroup a =&gt; Num (MyType a bs) where MyType a as + MyType b bs = MyType (a &lt;&gt; b) (as ++ bs)
Why isn't Vim one of the editor choices?
Thanks!
Someone with experience with haskell probably wouldn't have the same problems. If you know more about common libraries you might go for attoparsec to parse logs quickly and land on something like parseInt :: B.ByteString -&gt; Int parseInt = toInt &lt;$&gt; Attoparsec.takeWhile1 inRange where inRange x = x &gt;= zero &amp;&amp; x &lt;= nine toInt = B.foldl' step 0 step acc c = fromIntegral c - zero + acc * 10 zero, nine :: Int zero = fromEnum '0' nine = fromEnum '9' (didn't know about ByteString.Char8.readInt) But that doesn't mean that this isn't a real advantage of go. There aren't many resources to let you go from knowing haskell the language to actually knowing which library to go use when. Or even what best practices are. RealWorld Haskell is the closest I know of and that's quite outdated nowadays.
&gt; But that doesn't mean that this isn't a real advantage of go. I don't find it particularly insightful that one specific person finds Go easier to pick up than Haskell. 
Its not your fault, there seems to be no better way to ask that question, if you have to. Maybe this should be the last question so that people do not get bored with it and leave the survey altogether. Even if you curate them you will still be left with a hell lot of them. Maybe give a curated list and ask which ones you do not want, but that may also be fraught with problems.
Maybe I'll try to answer later, you could look at [this](http://dreixel.net/research/pdf/gpif_draft.pdf), which handles different input and output indices such as in `HList :: (k -&gt; *) -&gt; [k] -&gt; *`.
[removed]
Vi is an option, so I think there was no need for a separate Vim option.
&gt; I wonder if the linear types extension will help here? It should absolutely handle this use case just fine, although the actual implementation will either have to be baked into the compiler or use `unsafePerformIO`.
&gt; Is the author more proficient in Go? Well, certainly more proficient in Go than Haskell, yes.
Vi is not Vim.
Haskell's fairly focused on Linux for deployment and running code, and according to those benchmarks it blows Go out of the water on Ubuntu.
Observations from reading the paper: * The hook of the paper is a great catch, er no pun intended. * I love the observation that under this scheme `many` should have a more general type. * The remark in 4.5.1 about how IO is not an monad because RealWorld doesn't have an equality is wrong, though. The equality in question is definitional, not the in-language one. * CPS isn't _quite the Komogorov translation. The quantifier placement is all wrong. Cont r isn't the monad you get from taking `Not -| Not`, it is the monad you get from `(_ -&gt; r) -| (_ -&gt; r)`. [Edit: This seems to be at least talked about when it gets to Peirce's law.] * We should push the more generalized `catch` signature up into the transformers version of `ExceptT`.
How about “Vi family”? I mean, it’s a bit pedantic when listing it with other editors like Atom.
Agreed. I don't think it's useful to distinguish between `vim` (which has at least 4 "flavors" in Debian), `neovim`, `nvi`, and any other mostly-compatible vi implementation, not that this level. I use `gvim` and `vim` on MS Windows, on Linux I add Neovim as `$VISUAL` (and I have `vi` aliased to `$VISUAL`) and an additional flavor of Vim. I'm fine just calling it "Vi", as least until/unless the emacs users start requiring us to distinguish between emacs, xemacs, spacemacs, etc.
Great initiative! I would like to do something similar for other package
I think there's a few improvements. Firstly, checkboxes in multiple columns instead of a drop-down would be nice. Second, only give checkboxes for an extension that got at least one vote last year or this year (or otherwise proved it's relevance). In conjunction with the second point, provide a validated text box (or drop down if you have to) that allows a user to make relevant any extension they are really hot about. I'm not sure it's worth it, since (as far as I know) it'll require some custom software.
What `Parser` type are you using? `parseArrow` and `parseTuple` overlap because they both start with `"("`. Most of the parsers used in practice don't backtrack after consuming input, so that once `parseArrow` eats the `(` and fails, `parseTuple` is not run and the whole parser just fails immediately. You will need to either use `try` to override that behavior and have more lookahead, or restructure the parser so the choice between parsing an arrow or a tuple is made as late as possible.
To be clear, it's not my survey. I meant I was too conscientious when I answered it!
I think you're right about the need to do `Tuple [Ty]` - however, I'm unsure I can use the method you used for `parseTuple`. [This](https://github.com/myshov/programming_in_haskell/blob/3118695cc09a5ea0954917c318bf007b6ae5fcef/lesson8/parsing.lhs) is the parser that has been given to me.
[This](https://github.com/myshov/programming_in_haskell/blob/3118695cc09a5ea0954917c318bf007b6ae5fcef/lesson8/parsing.lhs) is the parser that we are using. Thanks for the ordering consideration.
Take a look at [Haskell courses](https://reactdom.com/haskell), if you wish to learn more effectively and try something different.
You can reconstruct your own `sepBy` from the pieces you were given (e.g. `many` and `(&gt;&gt;)`). The important idea here is the separation of concerns: you build a library of common combinators and if you have a collection expressive enough, you can turn a high-level description such as "a comma-separated list of Types of length at least two in between parentheses" into code in a fairly straightforward manner.
What do you mean by "doesn't work"? Did you define it, and the addition doesn't behave like you think it does?
Thanks! I put together a template with instructions that should be able to get you up in running in minutes ([https://github.com/m-renaud/haskell-rtd-template](https://github.com/m-renaud/haskell-rtd-template)). Let me know if anything is out of date :)
You're on the right track but not quite there yet. I'd expect: * `Tuple (t : ts)` in the `return` clause, here you're throwing away the first `Ty` for some reason. * a check to make sure that you have more than just one `Ty` (your specification demands that `parse parseTy "(Number)"` should fail). I maintain that it would be easier to expose the structure of the parser by defining `sepBy :: Parser a -&gt; Parser b -&gt; Parser [a]` as a generic combinator first.
Folks below have provided the answer for what you really need for the assignment you're working on. I'm going to give the "real world" answer: Usually what folks do is they write one parser that produces a `[Ty]` for `(ty1,ty2,ty3)` then they turn the `[]` case into a unit type `()` and the `(a)` case into just parenthesis handling. fixup :: [Ty] -&gt; Ty fixup [] = UnitTy fixup [a] = a fixup as = Tuple as then just map that over the result. This won't work for you as your class wants those other cases to be failures, but we usually don't leave them as holes in the grammar. (Note the conspicuous absence of the ability to just use normal parentheses in your grammar.)
Unfortunately, [money from book sales only goes to one of the two authors](https://twitter.com/argumatronic/status/1042434413580083200). https://old.reddit.com/r/haskell/comments/9ld9iv/bitemyapp_wrapping_up_haskell_book/
What else is redundant about this survey?
So, when I add `guard (2 &lt;= length ts)`it causes `ex1` and `ex2` to fail. 
What did you use for Common Lisp, Hunchentoot? I tried Hunchentoot but I can never make it fast unless if I use RESTAS (which is surprisingly based on Hunchentoot). I don't know what those Russians tweaked. It's pretty damn impressive.
It was a joke made in reference to the nature of the questions on the survey.
It was a pleasure to work with you as well :) You can send your gmail to [xrom.xkov@gmail.com](mailto:xrom.xkov@gmail.com), and we will invite you to our Slack to keep in touch.
Hi, the "Which country do you live in?" question seems to be missing "Hong Kong" in the list of countries.
Done 
I was familiar with the 1.5^(n) rule (I mean "length doubling" in a loose sense), but I hadn't heard this explanation of it. If you use a^(n) growth, then total space / used space is always between a and 1 (and if you do some logarithmic averaging it comes to (a-1)/log(a)). The total number of allocations you have to do to get to a^n is sum(i&lt;=n) a^i ~= a^n\*a/(a-1), so if you never reuse space then you are looking at a/log(a) overhead, which is minimized at a = e = 2.718, which is surprisingly high. Of course I'm not taking into account memory reclaiming here like you did, but this kind of analysis also makes sense if you are looking at time cost, and the limiting factor is the clearing of newly allocated memory. (If you aren't clearing memory and are only worried about time cost, then you can probably afford to waste a lot of space so the exponent goes up dramatically, probably without bound.) I'm having difficulty getting the golden ratio analysis. Clearly we have a nice nesting structure for φ since allocation n+2 takes up exactly the same space as allocations n and n+1 combined, but actually we can't take advantage of that directly because you will have two allocations in a row during the buffer copying, so you can't reclaim it immediately. If alloc n+3 = alloc n + alloc n+1, then you can have the pattern n+1,n,n+2 =&gt; n+3,n+2 =&gt; n+3,n+2,n+4 which repeats nicely. This is the Padovan sequence (not Fibonacci), whose limit is the [Plastic number](https://en.wikipedia.org/wiki/Plastic_number) a ~= 1.3247, which is a bit lower than the golden ratio.
In mathematics, the plastic number ρ (also known as the plastic constant, the minimal Pisot number, the platin number, Siegel's number or, in French, le nombre radiant) is a mathematical constant which is the unique real solution of the cubic equation
&gt;Note that actual, physical RAM has a small O(lg n) component. What are you talking about? What is the n being measured? A program that uses 1KB of RAM is not going to have faster per byte access than a program that uses 1MB of RAM. Talking strictly about RAM, so let's assume no accesses hit the CPU cache, and assume the data is truly in RAM and not swapped out to disk, as you said. Also, talking about some specific machine; not talking about the different latency costs of a machine capable of handling 32 GB of RAM vs some other machine capable of handling 1 TB of RAM.
I'm quite sure. Refer to page 3 of [this](https://arxiv.org/pdf/1710.09756.pdf) paper.
If the article author sees this: Some links are broken. At the least, these 3 links are all Page not found: " There is an [OS X](https://github.com/bos/criterion/blob/master/cbits/time-osx.c), [POSIX](https://github.com/bos/criterion/blob/master/cbits/time-posix.c) and [Windows](https://github.com/bos/criterion/blob/master/cbits/time-windows.c) C binding for each platform "
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [bos/criterion/.../**time-posix.c** (master → 7aa67dc)](https://github.com/bos/criterion/blob/7aa67dc047a2b57d9748ff20b0ecb42d3ab5f1c1/cbits/time-posix.c) * [bos/criterion/.../**time-windows.c** (master → 7aa67dc)](https://github.com/bos/criterion/blob/7aa67dc047a2b57d9748ff20b0ecb42d3ab5f1c1/cbits/time-windows.c) * [bos/criterion/.../**time-osx.c** (master → 7aa67dc)](https://github.com/bos/criterion/blob/7aa67dc047a2b57d9748ff20b0ecb42d3ab5f1c1/cbits/time-osx.c) ---- 
I found the survey too long. 
If you want performance in Haskell, its usually best to avoid `String` and use either `ByteString` or `Text`.
Hmmm, IIRC, Edwin Brady once mentioned that Idris interfaces got their inspiration from Haskell type classes, but are actually more similar to the ML module system. It was at OPLSS '17 I think. That was consistent with my impression, mostly. 
I'd probably use Data.Vector.Unboxed for something like that. IMO, you shouldn't be afraid of mutation and impurity in Haskell. Rolling your own monad for your code lets you control effects extremely well, and you only pay for what you use.
Indeed, one does not simply `string` and not regret it. The choices of libraries in OCaml seemed especially strange to me, batteries really? The author sounds rather experienced in Go and really inexperienced in the other languages. 
The UI for selecting which extensions you use is mildly infuriating. The multi-select dropdown closes after each item I select, requiring me to re-open and scroll through the entire list to find the next item, rinse and repeat. It requires O(n^2) time to answer this question when it should be linear time with me making a single pass through the list and checking each item as I come to it.
What's wrong with batteries?
After posting this, I see that there is a stickied post about this survey. I also says its being run by /u/taylorfausak who is well known to have highly partisan views. 
When the author started the project, the author is already experienced in Haskell and ocaml, even build some toy project. The author didn't seem to have Go experience previously. https://pl-rants.net/posts/haskell-vs-go-vs-ocaml-vs/
&gt; already experienced in Haskell and ocaml, even build some toy project The first half of that contradicts the second half of that. Building toy projects only scratches the surface of what its like to work in that language. 
I'm sorry, I mean 'has some experiences', I just want to contradict that the author is really experienced in go and totally inexperienced in other languages
Could you please put up an issue on [mysql-haskell tracker](https://github.com/winterland1989/mysql-haskell/issues)? It seems new version of MySQL server have got something we did not consider, but i'd like to support it.
It seems less actively used in the community. If you’re stepping into OCaml I would have expected someone to pickup Base/Core_kernel/Core. 
If anyone is curious, here are the results of [the last survey](https://www.fpcomplete.com/blog/2018-haskell-survey-results) which got significant criticism [here](https://np.reddit.com/r/haskell/comments/9mm05d/2018_haskell_survey_results/).
Might be my bias but I would expect someone that knew Haskell would know to use bytestring or text over strings. It’s commonly known, isn’t it? That said I haven’t seen a really good write up targeting newcomers on this topic. 
I have some questions about this survey that I posted as a separate thread [here](https://www.reddit.com/r/haskell/comments/9thula/questions_about_the_haskell_2018_survey/), but that post seems to have been shadow banned. My questions were: * Who is running this survey and collating results? * What are the survey results intended to be used for? * How is this survey trying to ensure that it is impartial and accurately reflects the whole of the Haskell user community? * How widely is this being advertised? * What is being done to prevent a single person submitting more than one response? In a follow up response I noted that the survey is being run by /u/taylorfausak who is well known to have highly partisan views. I am also well aware that he could level the same charges against me, but I am not running the survey. For reasons why this survey is questionable one only need to look at the criticisms against the previous [FPComplete's survey](https://www.fpcomplete.com/blog/2018-haskell-survey-results) which got which are [here](https://np.reddit.com/r/haskell/comments/9mm05d/2018_haskell_survey_results/).
Think about `ts` vs. `t : ts`;
Ooops, you are right in both cases. I will push v2 with these fixes eventually. Thanks!
Yes - the difference is that there's no requirement to have only one instance. I'm just unsure if that's the *only* real difference between Haskell typeclasses and ML modules (besides the syntax, that is?)
&gt; The remark in 4.5.1 about how IO is not an Monad because RealWorld doesn't have an equality is wrong, though. The equality in question is definitional, not the in-language one. Yes, but in the context there the "in-language" part is important because the proofs I give are in-language. I do agree that it warrants a footnote or something. &gt; CPS isn't quite the Kolmogorov translation. Yes, it's "generalized" a tiny bit as described in "Side B" section. &gt; We should push the more generalized catch signature up into the transformers version of ExceptT. Please elaborate, maybe I misunderstood what you are trying to say here. From my point of view `ExceptT` is fine as it has `catchE` already. The problem is that such a function is not very useful since all instances of generic `catch`-like operators have the wrong types.
Blogposts like this one are a great checkpoint for understanding how the Haskell ecosystem is perceived "from the outside". Only thing I don't understand from it is why OP made such a fuss about regexes (which, IMHO, are horrible no matter what language you use them from) and didn't even consider our great array of parser combinator libraries.
Are you aware that by writing FUDlike comments like these you're not helping the survey have the best possible turnout? Why can't you be more supportive of people when they invest so much of their time to provide the community with such an invaluable service. I can only imagine how frustrating this must be to Taylor getting thrown shade at by the old guards for trying to contribute back to the community.
&gt; Are you aware that by writing FUDlike comments like these you're not helping the survey have the best possible turnout? The survey was compromised before I made any comment here.
When you have both `div` and `mod` operating on the same values, it might be better to use `divMod` instead. When only dealing with positive values, consider using `quot`, `rem` and `quotRem`. In a sufficiently strict context, the compiler might spot these optimisation opportunities and perform them on your behalf.
This survey was designed in the open! Rather than waiting until after the survey was published to find an excuse for not participating you could have helped improving it by voicing your concerns at https://github.com/haskellweekly/haskellweekly.github.io/issues/206 
The only difference with 8.2 that I can remember right now, but that I use very often is the "valid type holes" suggestions. This new feature is also exploited by the haskell-ide-engine to offer quick fixes when using type holes.
Great, I'll play with it and think of a library to document this way :)
Dammit that is exactly what I thought. When the server was "blocked" it could still receive requests (based on the DB calls it did in response) but it would never respond before the client timed out. CPU and memory usage was very low throughout so that can't be it. The API in question was the US Census Geocoder and the response times occasionally get quite long if that is useful information at all.
&gt; The survey was compromised before I made any comment here. That seems quite far-fetched to me. If that was really the case Haskell.org wouldn't be officially endorsing the survey [so enthusiastically](https://mail.haskell.org/pipermail/haskell-community/2018-October/000342.html), would they? 
I don't think the OP is the author of this article
I had the same impression, but based on my own experience, it is difficult not to reach for the solutions and techniques you are already used to from other languages. Somewhere along the way of learning Haskell one realises how horrible the regular expressions language is compared to parser combinators.
i can live with printf being slow, but read . takeWhile . isDigit being slower than what looks like an identical algorithm is a bit sad...
Downvoting /u/Lossy (redditor since 2010) because he tells the truth about /u/E_Hackett being an account that is less than a day old?
Truth? /u/E_Hackett, are you a real person or a sockpuppet account as /u/erikd and /u/Lossy are implying?
Truth? /u/E_Hackett, are you a real person with other priorities than waging reddit flamewars or a throwaway account with some (not-so-)hidden agenda as /u/erikd and /u/Lossy are implying? 
If bags are not an h-set, then you cannot produce such a function, whatever A is. Period. It's a theorem. That's my whole point. Now, you may argue that `permute` does not prevent bags from being h-sets, which is fair. I don't have an immediate answer for this.
Maybe the client functions contend for the `Manager` (from http-client), somehow? IIRC those maintain a maximum number of live connections to have against a given server, you might want to tweak that setting by increasing the number, and see if that makes any difference. Off the top of my head, the few reasons that can block the processing of other requests is if all those threads wait on some shared concurrency-friendly mutable state (MVar, ...), which they don't by default but the request handlers might have code that make them do that, or if there's some funny business with FFI calls blocking all of your RTS' capabilities. If nothing like this stands out in your application, I'd probably try to reduce this to a smaller program with less deps and less logic, incrementally, until you end up removing whatever the source of the problem is. If you eventually reduce it to a very simple program that still reproduces this behaviour, it might be worth sending it our way over in the servant issue tracker, to see if we can help you track down the problem.
Until recently we've used an external fix point that would fit to Data types a la carte (but used it with an AST defined in the straightforward way), but we've transitioned to a form that supports heterogeneous co-recursive AST types, so for example the Lambda term could have a parameter type which is from a separate AST (currently it doesn't have one yet but that and type-editing is a planned change)..
Barring multiple submissions from people who _really_ want to skew the outcome (provided these exist? This is starting to sound like some tinfoil hat conspiracy theory), I still don't see the problem in collecting opinions (even the polarized ones), and _how_ exactly this self-selection mechanism would play out. I have a hope that most of those who'll complete the questionnaire are just "normal" users and not reddit warmongers with an agenda.
Take a look at the type of `+`: (+) :: Num a =&gt; a -&gt; a -&gt; a Both it's arguments have to be of the same type `a`. You're trying to apply `+` to arguments of different types (one is a `b` and the other is an `Integer`). 
It might seem like the same algorithm, but `read :: a -&gt; Int` can do more than the `parseInt` implementation in the article. The implementation in the OP can only parse positive decimal Integers, while `read :: a -&gt; Int` can parse positive and negative, decimal, hexadecimal and octal representations of Integers. For example: ghci&gt; read "10" :: Int 10 ghci&gt; read "-10" :: Int -- Negative -10 ghci&gt; read "0x10" :: Int -- Hexadecimal 16 ghci&gt; read "0o10" :: Int -- Octal 8 The `readInt :: ByteString -&gt; Maybe (Int,String)` function used later can also only read decimal representations as far as i know.
Regexes can be pretty horrible, but regular expressions really are awesome. They compile to fast finite state machines that have all kinds of handy properties (closed under intersection, union, reverse, inverse (given an alphabet) etc.), and can in practice[1] model all of natural language morphology. And regular expressions can look very readable, if you put some thought into the syntax: https://francismurillo.github.io/2017-03-30-Exploring-Emacs-rx-Macro/ Haskell's overly polymorphic `=~`, however, seems to have been inspired by the readability of Perl, throwing away the writability in the process :-/ [1] though [not in theory](https://linguistics.stackexchange.com/a/3159/1312)
Yes, surely parser combinators are often a better solution. However, regexes have their place. Even if it's just to allow the end-user of your Haskell program to use them (think of implementing egrep, an editor or an interpreter for a language that has regexes).
Sorry! I drew the list of countries from here: https://www.state.gov/s/inr/rls/4250.htm
Yes (I'm one of the patrons on Patreon and know the author in real life). Anything in particular you''re curious about? I found the book pretty damn good; allowed me to grok more advanced type theory that I used to, and was aimed square at the level I was at. Great book, though as hinted, I'm biased.
What level of haskell knowledge would one need prior to The Book of Types?
Yes, you can just aggregate your functions into one. So if you have functions f: a -&gt; b and g:a -&gt; c you would want a function h: a -&gt; (b, c) then foldMap over h
As a slightly related example, in a recent posting here in reddit regarding language performance comparison: https://www.reddit.com/r/haskell/comments/9t7jmp/haskell_worse_than_go_ocaml_yes_this_is_a/ The author of the blog post mentions at the end https://pl-rants.net/posts/haskell-opt-journey/: "Can Haskell be as fast as Go? Definitely yes, however the amount of effort I had to put into that was thrice of what I spent on the initial version while with Go I got the excellent results straight away." So, the potential to do impressive with Haskell is there, it's a great proposition of value. Does it sometimes end up being more costly? I tend to think that yes. 
Yeah, they do follow the structure: let me ask a categorical question, a follow-up categorical question for missing categories, and eventually ask specific questions that fall under one of those default categories. The problem with this structure is that the second question is not useful for making decisions and the first question could have been handled by allowing the user to skip those specific questions if they don't apply to them. 
Regexes can often be a simpler way to extract just some small part of a large string. Grabbing a JSON object out of a big dump for instance. I'm more concerned that they somehow didn't find the much newer and somewhat easier to use `regex` library. Or `Cassava` for csvs.
Have you tried bringing this up to the people at obsidian on the issue tracker? [https://github.com/obsidiansystems/obelisk/issues](https://github.com/obsidiansystems/obelisk/issues)
&gt; Blogposts like this one are a great checkpoint for understanding how the Haskell ecosystem is perceived "from the outside". This happens every time someone posts something ill-informed. "Oh, actually, the fact that this is ill-informed is further proof that Haskell documentation is bad/Haskell is not beginner-friendly enough!!" No. It's not insightful. 
&gt; Somewhere along the way of learning Haskell one realises how horrible the regular expressions language is compared to parser combinators. For parsing, yes, but for recognizing blocks of input regular expressions are great. And there are still plenty of places that's useful. It's why Alex uses them, for instance.
&gt; That said I haven’t seen a really good write up targeting newcomers on this topic. It's well-known among industrial users. 
Seems weird to exclusively compare things that exist in all three languages. One of the huge advantages of Haskell is things like `lens` and `recursion-schemes` which do not (and cannot) exist in Go.
"Excuse"? What is this, homework? No. I am under no obligation here, the situation is such that anyone who want me to participate needs to convince me that it's worthwhile. I merely gave a quick explanation why I didn't complete the survey. Based on a that, I also explained how this effect can (and probably will) introduce a bias, and I would love to see the authors show some intellectual honesty about it to avoid wrong or inappropriate conclusions to be drawn from the results.
Odd that the author says there were no CSV libraries. CSV and Cassava are both on Stackage.
Nothing wrong with this, as long as the bias is openly discussed and considered in the published conclusions. I just fear that that won't be the case.
From the author: &gt; So whom is this book for? The target audience I’ve been trying to write for are intermediate-to-proficient with the language. They’re capable of solving real problems in Haskell, and doing it without too much hassle. They need not have strong opinions on ExceptT vs throwing exceptions in IO, nor do they need to know how to inspect generated Core to find performance bottlenecks.
Thank you 
Yea, I have a good impression of the book so far. Just finished reading the "Promotion of Built-In Types" chapters which was the first part of the book I did not understand much on first read. The main question I have is: where does this book stand with respect to "next level" Haskell books like http://intermediatehaskell.com/ , https://lorepub.com/product/cookbook and https://joyofhaskell.com/ 
&gt; It fails to compile producing a terrifying error message I actually should agree about the compilation message. I understand they try to be helpful, but when a novice user sees 10 lines long message is it terrifying. And even if I'm not for example, scared, it is not convenient to scroll many screens of errors. I with there would be a mode to show them concisely, like 1-2 lines each.
&gt; Grabbing a JSON object out of a big dump for instance. Not necessarily the best example, since that requires parsing a balanced pair of braces. But out of a line that's form-encoded. Regexes may be awful (to some) but I still haven't seen anyone adequately replace `grep` yet.
Go is imperative. What would it want with `lens`? Also allow me to express again how much I detest the noisy lens operators, in spite of how much I like lenses in general.
To say nothing of the fact that he thought that rolling one's own CSV encoder with built-in print statements was a good idea. If the idea was to compare package ecosystems, shouldn't he have at least tried one of the several csv packages available?
And, for functions that you can use with foldMap, you always know their [pairing](http://hackage.haskell.org/package/base-4.12.0.0/docs/Control-Arrow.html#v:-38--38--38-) can also be used with foldMap because of [this instances for tuples](http://hackage.haskell.org/package/base-4.12.0.0/docs/src/GHC.Base.html#line-354).
keep in mind that this has different performance characteristics because of how tuples are lazy. To really combine folds, you'd probably want a strict tuple type.
Also, take a look at one of the other members of `Num`: fromInteger :: Num a =&gt; Integer -&gt; a ... might help you use that `y` argument with the results of `x z`.
There are plenty of tasks where `recursion-schemes` is a completely natural way to express what you want, e.g. constant folding.
&gt; you shouldn't be afraid of mutation and impurity in Haskell. In fact, Haskell is the best language for mutation and impurity because of how well it lets you control them! You can easily know when it's necessary to consider (not worry or fear) them just from the type signature! :)
What? I don't understand. `forall a. a` means that you can instantiate `a` with _any_ type, there isn't a special list of allowed types.
JSON error messages such as what you get from `-ddump-json` should at least help an IDE -- only if the message data is useful enough though, and not just the location and message. It seems the unfortunate fate of any highly polymorphic function is to yield baffling error messages when used incorrectly. Scala has `@usecase` in scaladoc that can at least show a simplified type signature, but I don't think it ever got to integrating with the compiler (being scaladoc and not a real annotation). GHC I suppose can always use `TypeError` instances for friendlier errors, or is there something more sophisticated I'm missing?
Thanks for making this, it's really cool to see everything in one place. Is the archive mainly for compilers not under frequent, active development? I recently came across papers on GpH, the [Glasgow parallel Haskell compiler](http://www.macs.hw.ac.uk/~dsg/gph/index.shtml), which might be a good addition to the list.
&gt; Note that `IO` is not a proper `Monad` since it cannot satisfy the laws simply for the fact that `RealWorld` cannot have an equality. Do you then view that `Reader e` or `State s` are "not monads" because you'd have to compute equality of functions to use the in-language equality, and we don't provide you that power in Haskell? When you say `flip . flip = id` you're not leaning on any class or in language notion for equality. `IO` isn't appreciably worse off in this regard. The ability to do equational reasoning does not require that those equations be between types that happen to have `Eq` instances or anything like that, it simply requires that one arrow can be replaced by another without changing the meaning of the program. From a purely operational standpoint, GHC's guts have a couple of requirements limiting its ability to optimize away `State# s` passing, and some rules for tracking when those things introduce effects, but threading it through code perfectly well respects its meaning. `State#` operationally has no storage representation, it is a 0 byte argument, but from an equational reasoning standpoint it is a perfectly reasonable thing to manipulate that just happens to have a funny kind. `RealWorld` isn't even the thing to be concerned with there as it is purely a phantom argument to the thing with the funny kind. &gt; From my point of view ExceptT is fine as it has catchE already. I'd missed that `catchE` was already appropriately generalized in `transformers`. `liftListen`, `liftPass`, etc. are apparently less general than they could be, though.
Then why does: f :: forall a. Bool -&gt; a f False = 'c' Fail with: Couldn't match expected type ‘a’ with actual type ‘Char’ ‘a’ is a rigid type variable bound by ...
`recursion-schemes` is also Template Haskell, which Go's anemic macro system can't even begin to approach. I'll grant that the concept is useful, sure, and so are lenses in otherwise imperative languages (they allow you write pure code). Sticking to things that exist in all languages makes sense when the object is to compare the language ecosystems. Not that this guideline was followed for all the examples, but the paper seems less about a rigorous comparison than to compare various language environments for their "whipitupitude" factor -- an important metric I believe haskell still lags behind in (but is still gaining) but where I found Ocaml did even worse. The author's experience differs, apparently, and that's exactly why I want to see more of these, with more detail on the package/build tooling used as well as factors like documentation quality.
Any plans to include HUGS and/or Gofer?
If I didn't miss anything, I think the following will do (assuming no negative values): hopper :: [Int] -&gt; Bool hopper [] = True hopper (0:_) = False hopper (n:xs) = hopper $ drop (n - 1) xs
I also think that dunking on the community-related work someone else does for free, with baseless allegations, is divisive and put simply, a d++k move. Why do some people insist this survey is devised without intellectual honesty, or that it is biased? It's open source man, go out and do better, otherwise you're just wasting our cycles.
I wonder how hard it would be to add recursive module support to Hugs. That seems like a simpler approach than the one the author ended up trying.
Claiming that "documentation is fine because only the ignorant have trouble reading it" is a little off the mark, no?
Yea. However I'm still much more confident that building HBC is going to be the simplest approach. I messed around with it yesterday and it seems like the only requirements are an old C compiler and an old yacc (though 2017 byacc worked as far as I got)
Char is one specific a. But you have to implement the function for *every possible* a, not some subset of them. You’re right in that it is not possible to write the totally generic function that has type a -&gt; b (without resorting to hacky tricks). However, you can still write some generic functions like (a, b) -&gt; a. 
&gt; Haskell people apparently hate MySQL Why does the author like mysql?? &gt; the regex interface in Haskell is as far from being user-friendly as humanly possible That's why I made [pcre-heavy](https://github.com/myfreeweb/pcre-heavy)! Apparently people don't know about it?? How do I make it more discoverable?
&gt; not clear which library to use [My library](https://github.com/myfreeweb/pcre-heavy)!! ;)
Yes! Just checked – I ended up using your library :) I don't remember if it covered all my use-cases or not. I think I've used `regex` package in [one case](https://github.com/k-bx/nlp/blob/master/lviv-forum-scraper/src/Main.hs#L94) to do a case-insensitive, non-multi-line search and replace which maybe your lib doesn't have (not sure?).
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [k-bx/nlp/.../**Main.hs#L94** (master → aaef5ce)](https://github.com/k-bx/nlp/blob/aaef5ce18fd8519e810094745b040cf4dbb2dd92/lviv-forum-scraper/src/Main.hs#L94) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply e8x3qjd.)
&gt; case-insensitive, non-multi-line Yeah, maybe that's not super obvious, but I made it possible to use any PCRE options, so that should be possible
Agreed.
Great news! Then I think all that's left is to advertise your lib which I will do in future (editing the wiki to hint at it would probably be nice as well)
Interesting. - does it work on windows ? - does it support backreferences in replacement strings ? - why no updates in 2 years ? - would you say it is production ready, eg has seen some heavy use and known to be free of major performance issues ? 
&gt; I also think that dunking on the community-related work someone else does for free, with baseless allegations, is divisive and put simply, a d++k move. I didn't mean to make any allegations; my response was meant as "hey, heads up, this survey is so long that it felt like a chore to me, so I didn't finish it, and I suspect you'll lose other potential participants as well". Not, "your survey is bad and you should feel bad". &gt; Why do some people insist this survey is devised without intellectual honesty, or that it is biased? Because it *is* biased. Selection bias, to be specific. I don't think this is done on purpose, and being a problem of surveys in general, it's hard to avoid, but it is definitely there. Intellectual honesty, then, dictates that this biased is acknowledged as such, and explicitly considered when drawing conclusions and presenting results. &gt; It's open source, so you either go out and do better or you're just ruining it for everyone else. Just because someone isn't invested enough to provide complete alternatives doesn't render their criticism invalid.
This is false on [2,2,0] which I think should be true - the OP says "you can move max 2 places" which I interpret as meaning you can move only 1 place if you want. I did find a linear time algorithm though, after I commented before. hopper = (&gt;0) . foldl' f 1 where f 0 _ = 0 f m x = max (m-1) x (The accumulator in the fold is the distance into the list that we know we can reach.)
Well, `f a = True` has type `f :: a -&gt; Bool`. For `f True = a`... What does the `a` mean? It isn't a bound name.
I also found the hearts a bit confusing and ambiguous.
&gt; windows Apparently [if you get pcre installed in cygwin/msys](https://github.com/Daniel-Diaz/pcre-light/issues/7) it should work. But I never used Haskell on Windows. &gt; backreferences in replacement strings Yes. It literally just uses PCRE, it supports everything PCRE supports. &gt; why no updates in 2 years ? There was no need to update?? It has all the features I ever needed, and no one sent any pull requests in a couple years. &gt; would you say it is production ready I guess? Again, it's just a nice wrapper around an FFI wrapper to PCRE, and PCRE has definitely seen very heavy use.
Right, I missed that part.
In your second example `a` is an unbound variable, so `f True = a` is not valid. As you said, you can define the function ```haskell alwaysTrueForWhateverArgument :: forall a. a -&gt; Bool alwaysTrueForWhateverArgument a = True ``` and for example `alwaysTrueForWhateverArgument 3 -- True` or `alwaysTrueForWhateverArgument ("hahaha", (3, False)) = True`. but you cannot define ``` alwaysAForWhateverBoolean :: forall a. Bool -&gt; a alwaysAForWhateverBoolean _ = a ``` because the variable `a` is not bound. As /u/ReedOei said, you cannot write such a function (you can, but not in a "satisfying" way). The type `forall a. a` does not have "satisfying" values either. You can see that the following program typechecks: ``` k :: forall a. a k = error "I'm one inhabitant" k' :: forall a. a k = undefined k'' :: forall a. a k'' = let k = k'' in k ``` 
Perhaps it could have been a two-tiered survey. First tier takes 3 to 5 mins tops, second tier takes another 10 to 15 mins getting into the details regarding build tools and language extensions. Perhaps that way we could get a less biased first tier survey, without losing the valuable input from those willing to complete the second tier.
Yes, but for Various Reasons, you really want to use the [`foldl`](http://hackage.haskell.org/package/foldl) library for this sort of thing.
`undefined` is a value. Depending on your point of view, either every type has its own `undefined` or there is one value `undefined` that inhabits every type. I don't know what you mean by resolving. The fundamental concept embodied by these values is *immediate failure*, to contrast with *non-terminating failure*.
regex-tdfa is pretty nice, if underdocumented
That "undefined" replaces a value that's impossible to "resolve" by the type system otherwise. Compare that to e.g. "infinity" in limits. If one evaluates a limit that doesn't exist, then one "resolves" the computation by saying that "it's infinite". 
I don't think it ever gets comfortable - this is simply part of the price you pay for not having to think about evaluation order at all most of the time, and reasoning about Haskell performance is ridiculously difficult. A decent strategy, then, consists of a few rules of thumb ("avoid foldl", "make things like state monads and mutable variable updates strict", "strict data structures, lazy control structures"), a general awareness that laziness may occasionally cause issues, and some basic debugging skills to detect and pinpoint the leak. Fortunately, you will rarely have to break out the big debugging guns - in years of writing Haskell, I have run into maybe half a dozen cases where space leaks were difficult to figure out. If you want to deepen your understanding of lazy evaluation in Haskell, I recommend looking into Stephen Diehl's stuff, especially the thing where he shows how you'd go about building a Haskell compiler from scratch in Haskell. You may also want to read GHC's source and the accompanying papers, although that is a rather daunting task and quite overwhelming, so probably overkill (but interesting nonetheless).
Ah you’re quite right, I was counting based on *excluding* the current allocation, but the source I was remembering, [Optimal memory allocation and the golden ratio](https://archive.li/Z2R8w) was counting based on *including* it: &gt; How about if we were willing to wait for n allocations? Then we’d be trying to solve: &gt; 1 + x + … + x^(n-2) = x^(n) &gt; which is more tricky. However, if we notice that the left-hand side is a geometric series, then we can sum it up and multiply through to get &gt; x^(n-1) - 1 = x^(n-1) - x^(n) &gt; This still looks a but hopeless, but when *n* is large the three terms involving x will dominate the ‘-1′ and we can neglect it. This corresponds to an allocation strategy where we know that we will be able to reuse a block of memory at some point in the future. We get: &gt; x^(n-1) = x^(n+1) - x^n &gt; This is looking more hopeful. Now we can divide by x^{n-1} and rearrange, giving a familiar-looking equation: &gt; x^(2) - x - 1 = 0 
The fact that `error` and `undefined` will type check as anything is a reflection of the halting problem, but it's not a representation of the halting problem. Ultimately, `undefined` and `error` are interfaces to allow the programmer to get work done, not a theoretically backed construct. This is seen in that the programmer can supply undefined or error wherever they like, not only in cases in which they are forced to. Bottom is the theoretical construct that represents values which can't be resolved. It has no runtime representation because it can't have one, because it can't be resolved. We haven't 'evaluated to bottom' when the runtime errors, we just hit the escape hatch before being swallowed by infinity.
A good understanding of lazy evaluation certainly helps. I wrote [the incomplete guide to lazy evaluation (in Haskell)][1], hopefully it is of use. [1]: https://hackhands.com/guide-lazy-evaluation-haskell/
&gt; I doubt I could spot a space leak in my own recursive functions. Space leaks in Haskell are not exclusively caused by thunk leaks. There's a nice blog post [here](http://blog.ezyang.com/2011/05/space-leak-zoo/) that explains some of the differences. I actually blew the stack in Idris due to excessive strictness once! It's not always easy to navigate but "laziness = space leaks" is simply not true. Laziness is in fact *essential* to performance in an immutable language. &gt; I don't understand the underlying patterns to watch out for. Is there any specific pattern in the definition of foldl and friends that is a warning sign? Sort of. A left-fold over a linked list is a bad idea to begin with in the vast majority of cases. You might also consult [Purely Functional Data Structures](https://www.cs.cmu.edu/~rwh/theses/okasaki.pdf). In general, knowing whether you should be working with a strict data structure or a lazy data structure is a good idea. &gt; In general, how many Haskellers are comfortable with laziness (or "non-strictness"), and how did you become comfortable with it? Space leaks are not a big problem in practice. The profiler will help you catch most thunk leaks. 
The other answers are good but I'll also point out that my last engagement consisted of another developer and I working on a boring line-of-business app in Haskell for almost 2 years. We were both fairly experienced developers but neither of us had any prior professional experience with Haskell. During that entire time we had no space leaks and exactly one non-obvious performance problem. Even then problem turned out to be due to an algorithmic complexity issue rather than anything exotic or specific to Haskell like laziness. Obviously your experience will depend on exactly what type of software you're developing, but you might be surprised how far you get with out ever having to know anything about diagnosing space leaks or similar things. 
&gt;Do you then view that Reader e or State s are "not monads" because you'd have to compute equality of functions to use the in-language equality... Expanding on determining if a data structure is a monad, outside of "by definition because it is an instance of the Monad type class", is it possible to argue that `Reader` is not inherently (mathematically?) a monad because all of the `Reader` functionality can be achieved _without_ the `bind` operator nor a `kleisli :: a -&gt; m b`. In context of how a `Reader` is used, it can be expressed as a product :: r -&gt; a -&gt; (r, a) This cannot be said for `Writer` and thus `State`. However, even a `Writer` where `Monoid s =&gt; s -&gt; s -&gt; s` captures the desired functionality, we have an `Applicate`. Thus, only when we need the additional "intelligence" baked into the body of a `kleisli`, intelligence that cannot reach `m b` by way of `a -&gt; b -&gt; m b`, e.g., a function `\x -&gt; 3 / x` where the only way to express `3 / 0 :: m b` is with `Nothing`. Along the same lines, on the hand `IO` might not inherently be a `Monad` (there are no mathematical properties that make it inherently so). However, to the degree the designers had only one possible way to maintain referential transparency when interacting with functions with side-effects, is this an inherent property (mathematical?) that requires recognizing `IO` as a `monad`?
I just wanted to voice my appreciation for the work you put into this! I hope you keep up the good contributions to the community, and don’t get discouraged :)
Just write a bunch of code that may or may not leak space, and then fix it when you need to. During this process you will learn three super important lessons: 1) Fixing most space leaks is generally not too horrible, and is sometimes easier than spotting them before hand. 2) Fixing space leaks is the best way to gain intuition about what is or is not likely to cause them. 3) You don't actually have to fix all your leaks to solve runtime memory allocation problems. It's actually very frequently ok to just write code that could theoretically leak space, because in the real world, it just won't "hold the door open" long enough to actually cause allocation to balloon to problematic sizes. #3 is the most valuable lesson of all, and it's also the easiest to learn. You can relatively quickly build an intuition for what processes in your application may become points of risk, and which won't. It won't be foolproof, but it will be enough to get your paranoia under control. 
90% of the time it doesn't matter, at all. About 8% of the time, laziness makes code more efficient. A decent chunk of this is code that wouldn't ever terminate in a strict language that can be used productively. About 2% of the time, laziness makes code less efficient. This is almost always caused by `foldl`, tuples, or other lazy record fields. I follow a few simple rules in my Haskell code: - Never use `foldl`. Always use `foldl'` if I want a strict accumulation, or `foldr` if I want something that fuses well with a lazy producer. - Use tuples extremely rarely. If you do use a tuple, especially as part of an accumulator, use bang patterns on the inner fields to enforce that they're strict. - If you have an accumulator parameter in a recursive function, and you don't want to use it lazily, then use a bang pattern on it. - If a datatype contains data, it should usually be a strict field. If a datatype models control flow, then it should be a lazy field. And I have never run into a space leak caused by thunk buildup. All memory use problems in Haskell have been caused by unbounded memory use (eg loading 4 million rows from the database, an unbounded concurrent queue, reading a 3gb file strictly, etc)
- Every time a big structure is reduced to something "smaller", there might be a whole lot of thunks hiding behind that seemingly innocent small value, so be careful. - `seq` establishes a relationship between two values. "if this value is reduced to whnf, this one will be, too". Maintaining strictness can sometimes feel like pushing on a string: you must preserve these relationships at all the necessary levels, and if you forget one, you are in trouble. Keeping a tuple in whnf in an accumulator isn't of much use if thunks still accumulate in its components! - Personally, I find strictness annotations in datatype fields *much* easier to reason about than free-floating combinators like `seq` or `$!` . I'm not sure why. Bang patters in function arguments are also ok. I often define throwaway accumulator types with strict fields like `data MyAcc = Acc !Int !Int`. - The usual suspects: use `foldl'`, not `foldl`. Tuples have lazy fields, better define your own composite accumulator types. Both `sum` and `product` from the prelude are lazy and can cause trouble for long lists. The lazy Writer monad and the strict Writer monad are *both* lazy in the accumulator—they are lazy in a sense unrelated to the accumulator. Prefer [`evaluate x`](http://hackage.haskell.org/package/base-4.12.0.0/docs/Control-Exception-Base.html#v:evaluate). to `return $! x`.
You might check out this post, though it is old: http://blog.ezyang.com/2011/05/space-leak-zoo/ The canonical thunk leak is something like: sum :: [Int] -&gt; Int sum = go 0 where go acc [] = acc go acc (x:xs) = go (acc+x) xs The issue is that `acc` is never forced and so we might build up a chain of `+` operations. It would be common to recognize this and add bang patterns to `acc`. This is part of the historical issue with `foldl` as well. But my GHC also compiles this to an unboxed loop (it not only doesn't leak space, but it likely does no allocation at all). Similarly `foldl` is now defined in terms of `foldr`, and is also subject to rewrite rules. I just checked and found that `foo = (+1) . foldl (+) 0 . take 100` also looks like it results in an unboxed loop (what I'm doing is looking at the core output, looking for a recursive function on `Int#` which means an Int that's in register) To be honest, I wouldn't worry too much about space leaks. The compiler is quite sophisticated, and no amount of expertise will stop you from stumbling on a space leak now and then. I definitely recommend building an intuition for laziness though, and thinking in terms of _productivity_, e.g. I think I found this helpful http://blog.sigfpe.com/2007/07/data-and-codata.html . For instance one reason to prefer foldr (historically) is that it is productive (so composes better, works potentially on infinite lists (given the right `f`), potentially short-circuits as we'd like to in e.g. `all`).
I've been developing small to medium sized apps in Haskell for the past 3 years and I haven't yet encountered any space leaks that I've could notice. I can say now that I'm comfortable with laziness in the sense that I don't think much about it anymore other than when I realise how much it helps at composing different pieces of my program.
&gt; There was no need to update Maybe to work with newer GHC versions or dependencies? That's the only thing I can think of.
What libraries have you tried?
Every Haskell type of kind `Type` has "bottom", which includes non-termination as well as exceptions. Consider: x :: Integer x = x + 1 If you try to evaluate `x`, your program will never finish. This means that the "value" of `x` is bottom. In Haskell it's not the only kind of bottom: exceptions, `error`, and `undefined` are also bottom, however, when thinking about bottom it's usually best to have non-termination in mind. Interestingly, the `Int` version of `x` *does* terminate: Haskell detects the loop and gives an exception instead. We can actually do a certain amount of calculation with bottom. For example, `seq a b` is bottom iff either `a` or `b` is bottom. It's also possible to write a function `f` such that `f a b` is bottom iff both `a` and `b` are bottom. If you want to examine exceptions, Haskell has functions to do this in the `IO` monad, though of course non-termination is still non-termination.
Apart from tuples being lazy, this also suffers from `foldMap` being a `foldr` instead of a `foldl'` for lists. I’m going to agree with /u/ephiron here. It is easier to get this right using the `foldl` library.
`undefined` and `error "foo"` are completely polymorphic: They can have any type you please. The *value* they represent is bottom, which in Haskell is a possible value for every type. This is necessary because Turing completeness and everything: If you write an infinite loop, its denotation is bottom. It's possible to write infinite loops for all types, so bottom needs to be allowed for all types^1. `undefined` and `error "foo"` are just other, arguably more practically useful, ways to construct such a bottom value: They have the same denotation as an infinite loop, but better runtime behaviour. Side note: Haskell can detect *some* infinite loops, in particular when the evaluation of a thunk depends on itself: The runtime will notice that it's already in the process of evaluating that thunk and, instead of looping indefinitely, throw an `error`. As said: Same denotation, better runtime behaviour. ---- ^1 Modulo "magic hash" types like `#Word32`, those are unboxed, strict, and a GHC implementation detail.
I am using a different manager as far as I'm aware. Servant seems to be creating its own manager, and the requests to the third party service are done using a library that creates its own TLS manager. The main server doesn't need this TLS handling as it is dealt with by Amazon. The pieces are very separate so nothing stands out. A servant Handler calls an `IO a` method of the library that internally creates and uses a tlsManager, then uses that result in the Handler. I will try and debug it more / reduce it down. Thanks for the help.
Shameless plug... my library [beam](http://tathougies.github.io/beam/) does just this. There are others as well like selda and opaleye
All of my questions should have been answered on the front page of of the survey. I am in Sydney, Australia I and I know a large number of Haskellers here are simply not going to respond because of the way the FPComplete survey was handled and the fact that the same person running the that survey is running this one with about the same level of transparency. 
It's a work related project, the author is migrating the backend from Python+Perl, which also uses MySQL
That's true. I opted for the simplest solution here but that's better.
Wow! Thanks for (re)posting that, albeit under somewhat provocative title. There seems to be some minor misunderstandings, which I hope to dispel by quoting the conclusion section: &gt;the experiment was successful at illuminating strengths and weaknesses of the languages in the context of writing small, one-off tools. In other words if you have *a bunch of Python/Perl/Ruby scripts that you want/need to make run faster, what language should you choose?* &gt; &gt;Go had the best performance per hour spent out of the box. If you have *a team of engineers of varying level of expertise* by using Go you could expect that a) the result would be good-to-great and b) anyone on the team would be able to maintain it. &gt; &gt;*Haskell’s laziness may be tricky but the excellent built-in profiling tools remedy it, albeit at the cost of longer development time*. \[...\] On the other hand it was the most powerful language I ever used and *if you have time/resources to build a set of domain-specific, optimised libraries it seems to be capable of solving anything you can throw at it.* &gt; &gt;OCaml appears be somewhere in the middle. On the one hand insanely fast compiler (much faster than Go’s!) and a sophisticated type system (and more powerful module system than the other two) *make it extremely pleasant to work with*. On the other hand it is somewhat lacking in centralised documentation. \[...\] The absence of a parallel GC may incur performance hits in some situations, although low memory footprint somewhat rectifies it. The main problem I faced with Haskell when I tried to step aside from writing toy projects and get something production-ready was (lack of) documentation. Resources like [Programming Haskell](https://www.goodreads.com/book/show/912217.Programming_in_Haskell) or [What I wish I knew when learning Haskell](https://dev.stephendiehl.com) are great but not enough, unfortunately. Something like modern version of [Real World Haskell](http://book.realworldhaskell.org/) would've been immensely helpful. Just try figuring out (or remember how many grey hairs you got while trying to ;) how exactly `(=~)` works. And then have a long, thorough discussion about it with people who used to write Perl code for living :D
&gt; What? I didn’t run the FP Complete survey. Ok, I was wong about that and corrected it. I am sorry. &gt; Why are you doing this? I am doing this because the FPComplete survey was not wide advertised and was therefore subjected to selection bias. I also doing this because the questions I raise should have been answered on the front page of the survey. 
Sure, lets play that game. [Two suggestion for further questions to disentangle bias ignored] (https://mail.haskell.org/pipermail/haskell-community/2018-October/000326.html). [A comment suggesting it should be clear hour the results were going to be used](https://mail.haskell.org/pipermail/haskell-community/2018-October)/000330.html). [A suggestion to that they surveys be marked as "X% of respondents ..." also ignored](https://mail.haskell.org/pipermail/haskell-community/2018-October/000333.html) And that is just the first half the the comments in that thread. 
&gt; transparency It seems you missed the long exchange on the community mailing list about this: https://mail.haskell.org/pipermail/haskell-community/2018-October/000323.html &gt; * Who is running this survey and collating results? You know it &gt; * What are the survey results intended to be used for? Not hard to guess; to inform the community once it's done. &gt; * How is this survey trying to ensure that it is impartial and accurately reflects the whole of the Haskell user community? It won't, if you keep up with this childish attitude and deliberately _not_ answer it. &gt; * How widely is this being advertised? Widely, on the usual Haskell-related channels. &gt; * What is being done to prevent a single person submitting more than one response? It's just an anonymous online survey on a programming language, not a government election. Why do you assume there are malicious actors who want to sabotage the outcome with multiple submissions. Moreover, why do you think FPComplete is related to this survey (it's not). It takes a grand total of 0.5 minutes to find out the community involvement of the author of this survey. I honestly don't understand this attitude and this amount of prejudice and vitriol. 
So let me get this right, you refuse to answer a survey, deliberately giving up the opportunity to influence the outcome _and_ accuse it of being biased. Very astute, Mr Opinion Haver
&gt; Why do you assume there are malicious actors who want to sabotage the outcome with multiple submissions. There was another survey this year by [FPComplete](https://www.fpcomplete.com/blog/2018-haskell-survey-results) which was not widely advertised, was almost certainly subject to selection bias, and was at least to me pretty obviously more an FPComplete marketing exercise than a survey. In addition to that, the person running this survey is known to have a bias towards FPComplete. I have been involved in FOSS for a long time and this would certainly not be the first time that the involvement of commercial interests in a FOSS community has become toxic.
&gt; Always use foldl' if I want a strict accumulation, or foldr if I want something that fuses well with a lazy producer. This is a really good rule of thumb for choosing between `fold'` and `foldr`. i.e. Choose between them depending on whether your operator function is strict or lazy in the accumulator argument. - If the operator is strict in the accumulator (i.e. the accumulator must be evaluated to evaluate the return value of the operator), then you likely don't want `foldr` since evaluating the accumulator like that means putting the recursion on the stack, requiring O(n) stack space. This is why sum is better implemented with `foldl'`; it has O(1) stack usage, whereas it'd be O(n) with foldr. - On the other hand, if your operator is lazy in the accumulator, using `foldl'` will walk the entire list before returning `op _ l`, where `l` is the last element of the list and `_` is the as of yet unneeded accumulator. With foldl', this has the undesirable side effect of computing the entire accumulator way before you need it. This is why `map` is better implemented with `foldr`; it would require walking the whole input list and evaluating the whole output list's spine to evaluate just the first element with foldl'. There's a lot of ways this can get murky though. For instance, what if you're tail recursive in the accumulator? filter f = foldr op [] where op elem acc | f elem = elem : acc -- Lazy | otherwise = acc -- Tail recursive In this function, we're lazy in one case and tail recursive in the other. Tail recursive is an odd in-between, in that it is technically strict (evaluating `op x y` evaluates `y` in the tail recursive case) but it doesn't actually cost anything extra. `filter` is tail recursive during filtered out elements, and lazy during retained elements. It's technically strict in the tail recursive case, but the effect is that it just skips all filtered out elements at no cost and lazily stops at the next retained element. So when you're tail recursive in the accumulator, even though it's strict, it shouldn't affect your decision about `foldr` vs `foldl'`. If your condition ends up *always* choosing the tail recursive case, it won't even matter which you choose. They'll both just recurse down the entire list and eventually return the initial accumulator. Therefore it's just the other cases that matter. If you're lazy in the other cases, use `foldr`, and if you're strict then use `foldl'`. op elem acc | condition elem = elem + acc | otherwise = acc This operator is better suited for `foldl'` because it's strict in the non-tail recursive case. Using `foldr` would result in excess stack usage. The other scenario is when your operator is sometimes lazy and sometimes strict (non-tail-recursive). I can't actually think of any instances of this though :P But in this case it'd be down to how often its strict or lazy. If it's almost always lazy, then you might want to use `foldr` since you'll only use a little stack space on the strict cases. If it's almost always strict, then of course `foldl'` is better since calculating just a few accumulators too eagerly isn't so bad.
"Game"? Look, it's just a survey, who cares. Don't take it. I certainly don't if you take it or not. You think it's one big plot to dupe you into Giving Away Your Precious Opinions On Software, but nobody really gives a dry sh++ about them. I'm pretty sure they are within the first standard deviation of the other respondents' replies anyway. What does matter is the divisive effect your allegations have on the community. You break something you wouldn't be able to create and won't be able to fix. 
And I said answers to those kinds of questions should have been on the front page of the survey. Good survey design is not a trivial exercise (I certainly don't claim that I am) but this survey does not seem particularly well designed.
&gt; the person running this survey is known to have a bias towards FPComplete. This is crazy. Like, a crazy, unfounded, completely utter bullshit allegation. Using the .. preference? for a software build tool to construe bias is a deplorable, misguided and ultimately evil mistification, since the author's position has been made clear already multiple times. Your fears for corporate involvement in this are _completely_ unfounded, if there were any need to set that straight once more. And I'm saying this as someone who has a strong FOSS loyalty.
Do grow up and give me a break. You're still referring to the stack/cabal thing, after all these years. It's obvious, tiresome, not nice, useless, trivial, divisive. Move on already
There is a lot about this on the web, probably better explained than I can, but still. Haskell is a lambda calculus basically. Lambda calculus is a math concept where you basically have functions which can take one argument at most. To make functions with many arguments these are applied in order. All this tells us one important lesson, that everything is made out of functions. And functions are mostly a concept you can formalize proofs for. They have no side effects. They might be unprovable sometimes, but you can still prove that they are unprovable and you shouldn't waste time, theoretically. So the concept of pure functionality in Haskell makes it's code easier to prove correct than imperative languages.
The beam documentation is seriously impressive but I don''t really have an operational intuition of how it works. Like, I think [this](http://hackage.haskell.org/package/beam-core-0.7.2.2/docs/src/Database.Beam.Query.Internal.html#QF) is a strongly typed ast but it could be avant-garde poetry or a chant to revive the old gods. The userland code seems quite pleasant, though. I did try opaleye before and remember really disliking the produced sql. Hadn't heard of selda before. It seems to follow a similar monadic-ast-builder plan. The implementation seems [quite readable](http://hackage.haskell.org/package/selda-0.3.4.0/docs/src/Database.Selda.Query.Type.html#Query) but the api for joins doesn't seem done yet. Also, after some more searching [I did find a list-comprehension style sql interface](https://db.inf.uni-tuebingen.de/staticfiles/publications/the-flatter-the-better.pdf). It is implemented in the [DSH](http://hackage.haskell.org/package/DSH) package but it seems to have gone the way of the research project and not be maintained anymore.
Haskell is [made of equations](http://www.haskellforall.com/2013/12/equational-reasoning.html) so that's gotta help.
&gt; everything is made out of functions in Haskell No! `21 :: Int`, `"hello"`, and `False` are not functions; they are, respectively, an `Int`, a `String`, and a `Bool`. All data *can* be encoded as a function, but that doesn't mean that it *is* encoded as a function.
While I might walk on a thin line programmatically, mathematically they are all functions of say zero order. E.g.: f :: Int -&gt; Bool f x = | x = 2. True | otherwise False This will evaluate to true or false, if argument applied. Probably a more concise naming, would be "expression" than function. Is e.g. 5 a function? Well it depends how you argue if you say it's a function of order zero then yes: 5*x^0 
Selection bias isn't about an intentional act of malfeasance. It is about the _unavoidable_ fact that the respondents to a survey bias the results of the survey, and any time there's a barrier, then it creates _some_ sort of cliff of which people unequally fall, and thus introduces _some_ new form of bias. How "people who get tired of longer surveys" correlates to any of the other sorts of questions we want to answer I have no idea. But there will be _some_ correlation, and it will introduce _some_ bias. So I think the point is appreciated.
I don't think anyone is saying "no, don't collect answers!" I support having a survey, I gave a ton of feedback on it, and I _still_ think that there's going to be a lot of fuzz in the answers, some induced by selection bias, some induced by other flaws in survey design, etc. Good survey design is hard, and even then inexact. There's no conspiracy theory I see on display outside of "oh man, survey design is really hard, even with the best intentions!"
I never said that there aren't any functions in Haskell. `f` is most certainly a function! There's a blog post about this somewhere, I'll try to find it tomorrow.
Maybe, but I don't know, either. Your interpretation is .. plausible, but still purely speculative. I wish we had some actual statisticians in here, who could say something factual about survey "quality" and on its diffusion. On the other hand, besides all intellectual considerations, there's this very basic fact that what we have is this survey, that it was designed in the open, and that now some people are piling up armchair criticism on it because reddit is just like that, I guess.
I mean s/survey/haskell library/ and it doesn't feel so weird, right? Just because something is developed in the open doesn't mean that we shouldn't be upfront about the issues therein. In fact -- it means such a discussion has a better chance of perhaps improving things in the future, if anything. (But it _is_ important to disentangle criticism of _motives_ which is dubious and hard to prove with criticism of _methodology_ which hopefully can be done in a collaborative and collegial way).
I don't follow any of your reasoning here, sorry. Mathematically, a monad is simply any functor that provides `return` and `join` that passes the three laws. Period. At the risk of diving into heavy jargon: The adjunction `(,) e -| (-&gt;) e` gives us `State e` when composed. Using composition of adjunctions, you can show `StateT e m` is also a monad. `WriterT e m` gets its power from `(,) e` being a left adjoint, preserving colimits, hence can be pushed into everything giving `m (e, a)` as a normal form. `ReaderT e m` gets its power, from `(-&gt;) e` being a right adjoint, which means it preserves limits, and hence distributes over everything, giving `e -&gt; m a` as a normal form. Adjunctions don't have to go to/from Hask, they just have to end there. All monads give rise to adjunctions that through the Kleisli or Eilenberg-Moore categories, but these are boring free/cofree constructions, unlike the ones above. Given the observation that the intermediate category doesn't have to be Haskell this you can build `Cont r` from the adjunction that goes to Hask^op that provides (_ -&gt; r) -| (_ -&gt; r). Going out to linear logic and back lets you build Eugenio Moggi's computational lambda calculus's `M`, as a recognizable way to talk about `IO`. This captures the notion that you aren't allowed to weaken or contract a `State# s`, inside `IO` or `ST s` as it doesn't really belong to Hask. &gt; is it possible to argue that Reader is not inherently (mathematically?) a monad because all of the Reader functionality can be achieved without the bind operator No. `join :: (e -&gt; e -&gt; a) -&gt; (e -&gt; a) isn't implementable with just the applicative bits. The statement that reader could be used as a "product `r -&gt; (a -&gt; (r, b))` is an observation that there is a monad homomorphism embedding `(-&gt;) r` into `State r`, no more no less. It _also_ arises from that adjunction way at the top of my reply.
Well it didn't do much, it changed the error message a bit, but just adding other relevant bindings ... yoneda.hs:44:23-26: Couldn't match type ‘HomFunctor a x0 -&gt; f x0’ … with ‘forall x. HomFunctor a x -&gt; f x’ Expected type: f a -&gt; NatTransHom a f Actual type: f a -&gt; HomFunctor a x0 -&gt; f x0 Relevant bindings include to :: NatTransHom a f -&gt; f a (bound at /home/johan/Documents/haskell/yoneda.hs:46:9) from :: Functor f =&gt; f a -&gt; NatTransHom a f (bound at /home/johan/Documents/haskell/yoneda.hs:48:9) yoneda :: Bijection (Functor f) (NatTransHom a f) (f a) (bound at /home/johan/Documents/haskell/yoneda.hs:44:1) In the second argument of ‘Bijection’, namely ‘from’ In the expression: Bijection to from Compilation failed. &amp;#x200B;
&gt; it is important to disentangle criticism of motives which is dubious and hard to prove with criticism of methodology which hopefully can be done in a collaborative and collegial way This is a very important observation; thank you for clearing it up.
While nullary functions are a thing, even in some branches of maths, I don't personally think it is any more useful to think of a value as a nullary function in Haskell than in most other languages. When you want to draw a line between a thunk and a WHNF value, then maybe it helps to talk about the thunk as a nullary function. But, even then, a String literal isn't ever a thunk. I also think even in maths it's a bit sketchy to talk about nullary functions since "the lack of an object" isn't in any domain, and every function needs a domain and codomain, in my mind. Unary functions on a singleton domain can basically used anywhere a nullary function would, and have a well-defined domain.
&gt; the incomplete guide to lazy evaluation (in Haskell) Is it incomplete because you're lazy? I'll see myself out 
The language constructs make it easier to separate the parts of the program with side effects from the parts of the program without side effects and recognize the difference.
This is true, but saying "Haskell isn't as good for formal verification as Idris or Agda" really isn't saying much :P Very few languages are. But on the spectrum, Haskell is far closer to those than the vast majority of languages.
I think you just need `{-# LANGUAGE ScopedTypeVariables #-}` and to annotate all your type variables with `forall`. By default, you can't reference type variables declared in the top level type from a where binding's type signature. You have to enable this extension and explicitly declare the type variables with `forall` in the top level type declaration.
So the list comprehension style you seek is easily achievable via `RebindableSyntax` and `MonadComprehensions`. As to how beam works. Beam uses a finally tagless style which abstracts away the AST representation. The AST in the core module is used for testing. That’s it. The individual backends co opt the typeclasses in the core module to provide a custom representation. The Postgres, salute, MySQL, and firebird backends all generate full snippets of SQL using bytestring builders or a free monad like data structure optimized for the backend at hand. If you keep your queries polymorphic over backend you pay the cost of pointer redirection. If you monomorphize or specialize, then all this is inlined. For static queries, GHC will sometimes generate the whole sql expression at compile time. 
Maybe no one has asked for the end of it yet.
Why do you put the `Functor f` constraint on both `yoneda` and in `Bijection (Functor f)`? The constraint is available within the `where` clause, so the `to` and `from` implementations don't need to ask for a second copy.
&gt; Do you then view that Reader e or State s are "not monads" because you'd have to compute equality of functions to use the in-language equality, and we don't provide you that power in Haskell? When you say flip . flip = id you're not leaning on any class or in language notion for equality. IO isn't appreciably worse off in this regard. Firstly, in the paper I wrote &gt;&gt;&gt;&gt; Note that [this definition of] `IO` is not a proper `Monad` since it cannot satisfy the laws simply for the fact that `RealWorld` cannot have an equality. I still don't see how reinterpreting `IO` as a free monad as in your comonad.com article invalidates that statement. Secondly, I do see a difference between free monad reinterpretation of `IO a` and e.g. `State s a`: - I see extensional equality as a part of the language (irrespective of `Eq` type class or whatever) simply for the fact that I can eval `runState` on any given value `s` and get an `a`. Let us call this type of extensional equality "weak". - Meanwhile, with free monad `IO` after evaluating `IO a` into a `yield` of `FFI` with `FIO :: FFI o i -&gt; o -&gt; (i -&gt; FIO a) -&gt; FIO a` I would need both a "strong" extensional equality to prove the equality of `i -&gt; ...` parts (as I can't just evaluate them since I can't know a result of `FFI o i`) and, more importantly, an equality on `FFI` itself. On that latter point, if you define `FFI = (-&gt;)` then you would need a full-blown affect system with an equality on `(-&gt;)` that respects side effects, which kinda destroys the whole point of the free monad translation, IMHO, and does not solve the following issue anyway. So, you define `FFI` as an ADT with some non-free equalities (e.g. let's say the OS provides a transactional FS API and we want `atomic (createFile "x" &gt;&gt; rollback) == id`), now you reduced the problem of defining equality on the `RealWorld` to the problem of defining equality on `FFI`s. Not that much of a difference, IMHO. So, sure, free monad version of `IO` is a `Monad`, but that argument almost anything is a "`Monad`" as I can wrap pretty much anything into a free monad to get the laws for free and then hide all the dirty business behind an opaque `FFI` with an evil interpreter.
It is not crashing at evaluation time, it's just ill typed, and the error you get when you try to apply it is a type error. Ask for its type: &gt; :t eval_ex eval_ex :: (Fractional p, Integral p) =&gt; p -&gt; p -&gt; p This is saying that it has type `p -&gt; p -&gt; p` for every type `p` that is an instance of `Fractional` and `Integral`. There is no such type. The typeclass `Integral` represents numeric types whose inhabitants are integers only, whereas `Fractional` represents numeric types that support division (not integer division). Here `x` should indeed be a `Fractional` type, and `n` should be an `Integral` type (`eval_ex` and `factorial` are defined by induction on natural numbers, and `(^)` actually expects an `Integral` as its second argument). Unless you need absurdly high precision, `n` as an `Int` should be enough. So I would propose the following types: factorial :: Num a =&gt; Int -&gt; a factorial 0 = 1 factorial n = fromIntegral n * factorial (n - 1) -- explicit conversion from Int to different numeric type eval_ex :: Fractional a =&gt; a -&gt; Int -&gt; a -- same implementation Those types are good enough for defaulting to take over in ghci.
isn't python implemented with a stack based abstract machine? as far as I know, abstract machines are basically mathematical logic and sometimes category theory (for example the CAM)
Python has assignment and unfettered mutation which destroy any chance you have of applying equational reasoning beyond trivial cases.
I am just starting out with haskell. Any recommendations on what to read?
In relation to the books you mentioned, the **Book of Types** is the next level. &gt; I'm also impressed that author set about to write this book only a few months ago, and finished it in time. In his own words, the book was already written in his head, he just had to write it down.
+1, yeah, thanks for taking the time to put this together.
I don't think anyone reasonably suspects that the data will be tampered with. This just isn't a realistic concern.
&gt; I and I know a large number of Haskellers here are simply not going to respond because of the way the FPComplete survey was handled. That survey was criticized for almost certainly having selection bias This is weirdly circular justification. By not filling out the survey, you are creating the very narrative of "selection bias" that you are using to justify not filling out the survey. Can you explain how this survey could achieve a less biased result? Where else should it be publicized in order to ensure maximal reach?
The document will reveal itself more and more when you put it into cases
You are missing something fundamental about the type system. I used to have similar issues in understanding a lot of type system behavior for a lack of this understanding. Most of the Haskell tutorials out there does not really make it explicit. To put it shortly, the polymorphic types are not decided by the entity that a type signature is attached to (let us call this entity A), but the entity that is actually using A. When it comes to functions, it means that the types are decided by the caller of the function, and not by the function itself. So a function signature `String -&gt; a`, does not mean that this function can choose the return type as it like. But it means that *this function should be able to return any type that the caller requires*.. The problem with wrapping ones head around this concept is that they should first be aware of another obscure (in the sense that Haskell tutorials does not make it explicitly state it's importance), but fundamental thing, that is return type polymorphism. Haskell type classes enables one to write functions that can return different types as expected by the call site. Without including that in your understanding that, it will appear to be impossible to have a function of type "String -&gt; a" where the function should be able to return anything as decided by the call site.
Have you tried tutorials that are available online?
Recently I had an impressive space leak in a long-running recursive function wrapped in an exception handler. In a pseudocode ``` f = handle (actions) (do {computations; f}) ``` The leak has gone when I moved `handle` inside iterations, i.e. ``` f = do handle (actions) computations f ``` Looks like `handle` won't let computations to be garbage collected as it potentially expects exceptions and all iterations are held in memory until some exception occurs.
Your wording suggests a moral obligation on my part that doesn't exist, and a hostility that I never intended to express. I am not refusing anything, I'm just noting that I was originally willing to casually help out, but after the first few question, it felt too much like a chore. I am not accusing anyone of anything either, I just felt that sharing my observation that the survey is biased in a way that is (I assume) unintentional would be helpful and prudent. Again, my comments were meant as a heads-up, not accusal or aggression. If you prefer to not hear my feedback in the future, no problem. (I will, however, keep participating in community discussions that concern me and that use results from this survey in their arguments, pointing out methodological flaws that I observed - not out of hostility, but in the interest of having a meaningful, evidence-based discussion).
It's not correct to say that everything is a lambda expression. In Haskell, lambda expressions are things like `(\x -&gt; x + 2)`. In Haskell, there are clearly constructs that are not lambda expressions, like `True`, `False`, `"hello"`. There is no implementation of a Haskell compiler that compiles down to typed lambda calculus, and it would be pretty much impractical or impossible to do so. Haskell *not* compiled down to typed lambda calculus; it's compiled down to C and imperative machine code (and sometimes javascript or JVM bytecode).
foldl and foldl' are implemented in terms of foldr. No reason to use foldr to get fusion anymore. 
So then what is Simon Peton-Jones talking about here with System F as an intermediate language in GHC: https://youtu.be/re96UgMk6GQ?t=1584.
[This paper]( https://www.microsoft.com/en-us/research/publication/demand-analysis/) really helped my understanding of strictness in haskell. The algorithm is actually quite straightforward. 
Well, orginally, I wanted to write a complete guide, but then.
I was hoping [my suggestion to get haskell.org involved with the survey](https://www.reddit.com/r/haskell/comments/9mm05d/2018_haskell_survey_results/e7ka8xn/) would address the concerns some people expressed about past surveys. Unfortunately it did not as evidenced by the controversy in this thread. I hate to say it but from what I've seen so far I think that as long as /u/taylorfausak is actively involved with the survey it will continue to remain controversial and cause unnecessary drama.
More importantly even *if* taylorfausak felt it necessary to "cleanup" the data who cares? It's just a survey which tries to measure the current temperature in the room to satisfy general curiosity about ourselves. Nobody in their right mind will base any decisions on it. I hope.
I don't fully agree. Although not the last stage of compilation, Haskell does get compiled to an intermediate language that *is* a typed lambda calculus.
To upgrade GHC globally with stack you can simply do: `stack config set resolver lts`
Interesting. Only two things: 1) I hope that this does not unleash the fundamentalist mobs to alter core libraries in incompatible ways as happened recently. 2) non commutative rings are a better fit for applicative and alternative.
Strange: when I try to compile your code, I get a different error saying that GHC doesn't support impredicative polymorphism. You didn't try to turn on the broken `ImpredicativePolymorphism` language extension, did you? That limitation means datatypes can't be parameterized by a `forall`, e.g. `Maybe (forall a. a -&gt; a)` is not allowed. In particular, `Bijection ... (NatTransHom a f) ...` is not allowed since `NatTransHom` is a type synonym for `forall x. HomFunctor a x -&gt; f x`. To work around this limitation, I made `NatTrans` a `newtype` instead of a type synonym, adding wrapping and unwrapping calls where appropriate. I also had to eta-reduce `HomFunctor` (that is, to make it point-free), because partially-applied type synonyms are not allowed either. I also removed the redundant use of ConstraintKinds, as per my other comment. And now the code compiles! {-# LANGUAGE ConstraintKinds, KindSignatures, RankNTypes, RecordWildCards, ScopedTypeVariables #-} data Bijection a b = Bijection { to :: a -&gt; b , from :: b -&gt; a } type HomFunctor = (-&gt;) newtype NatTrans (f :: * -&gt; *) (g :: * -&gt; *) = NatTrans { runNatTrans :: forall x. f x -&gt; g x } type NatTransHom (a :: *) (f :: * -&gt; *) = NatTrans (HomFunctor a) f -- Yoneda : yoneda :: forall f a. (Functor f) =&gt; Bijection (NatTransHom a f) (f a) yoneda = Bijection {..} where to :: NatTransHom a f -&gt; f a to alpha = runNatTrans alpha id from :: f a -&gt; NatTransHom a f from fx = NatTrans (\f -&gt; fmap f fx) 
[removed]
You've can also try the Haskell path (directed problem sets) at exercism.com. Free, 89 Haskell problems with mentors who can give direction and code review your solutions. https://exercism.io/tracks/haskell
/u/disya2 I think you were the author?
Um, you could search the subreddit, there have been many posts along these lines :). Suggestions on what to read would depend heavily on your background. Given that Haskell is such a different language from mainstream ones, different people learn Haskell in very different ways.
You need `foldr` if you want the result to be produced lazily wrt the list. eg `foldr Map.insert Map.empty` vs `foldl (flip Map.insert) Map.empty`. `foldl` must walk the list before it can begin inserting any elements, while `foldr` can start producing a `Map` immediately, and if the map is only partially consumed, then the list won't be fully traversed.
There are so many tutorials of varying quality online that this isn't a helpful answer. What tutorials do you recommend? I would suggest [bitemyapp/learnhaskell](https://github.com/bitemyapp/learnhaskell). I learned Haskell primarily from this resource, and I now work with Haskell. [Haskell from First Principles](http://www.haskellbook.com) is a fantastic resource if you can afford it (and if you're a student or in a disadvantaged position, the authors give discounts if you email them).
GHC 8.8 will add foldMap’ to Foldable, which fixes half of this problem. But tuples are still lazy, so it still may space leak.
Yes, everything I’ve seen uses clock.
Neat, I also just learned about https://github.com/haskell/ghcup, I just tried it out and it's great!
[What I Wish I Knew When Learning Haskell](http://dev.stephendiehl.com/hask/) Some parts are a bit out of date, but it's still a very good starting resource.
Im giving this a try right now thanks for your input
Not useful for values themselves, but named constants like `zero = 0` do feel like nullary functions
Awesome post as usual! Also, `Enter Idris.` would make a great tattoo.
He mentions this explicitly in “Linear Logic: Its Syntax Andy semantics”. “Linear Logic first appeared as a kind of linear algebra built on coherent spaces; then linear sequent calculus was extracted out of the semantics”.
I disagree. I think they feel like *bindings*. Since I never pass it an argument, it certain doesn't feel like a function to me, more like a function argument or a member variable (from other languages). They don't act like it. At least in GHC, `zero` is never a thunk, and always has the same representation as the constant.
"… but it's a lot of work". The Idris following this segue looks a lot like what I think the Haskell would also look like (maybe using algebraic nats). That dissonance snapped me out of an otherwise really nice narrative. Thanks for writing!
The ast for core looks like data Expr b = Var Id | Lit Literal | App (Expr b) (Arg b) | Lam b (Expr b) | Let (Bind b) (Expr b) | Case (Expr b) b Type [Alt b] -- See #case_invariants# | Cast (Expr b) Coercion | Tick (Tickish Id) (Expr b) | Type Type | Coercion Coercion deriving Data which is impressively simple but `Lam` definitely isn't the only constructor. The top level of a haskell programm is a list of (potentially recursive) `Let` binding groups.
Comparing two programs that run on the same abstract machine for "equality" is generally done through bi-simulation, which is basically just running both of them. Equational reasoning is generally used when we *don't* want to run both programs. Bi-simulation is also only good for whole programs, for program fragments, you are likely to get stuck on an abstract (function) value. Equaltion reasoning works better on program fragments
There 5 bits of "Learning Material" linked from the sidebar, along with other useful things.
What we know from Julie's tweets is that the rights of the book have gone to Chris via some legal agreement. Whatever the agreement was, it must have been mutually acceptable for both parties to agree upon it. It seems bizarre to me to punish one of the authors because they resolved whatever conflict they have in a mutually agreed upon manner.
&gt;Equational reasoning is generally used when we *don't* want to run both programs. I see what you mean, thanks for telling me that.
&gt; I feel that you are not interacting with me in good faith. Were you acting in good faith when you [posted this to twitter](https://twitter.com/taylorfausak/status/1058559307858493441)? There is small mostly silent minority that don't like stack as a build tool. Some of this minority can escape it but others cannot because they have joined teams that have already chosen stack. These people usually can't just use their preferred build tool, because the design (I believe unintentional) of stack makes it trivially easy to build a non-stack project using stack, but it is often completely non-trivial to build a stack project with non-stack tools. And yet this minority gets told: &gt; You're still referring to the stack/cabal thing, after all these years. It's obvious, tiresome, not nice, useless, trivial, divisive. Move on already. I'll admit it, stack has won. It is the most widely used build tool in the Haskell community. For the moment, stack and Stackage depends on Hackage, but for how long? Breaking this dependence would be a tiny effort in comparison to the effort that has already gone into Stackage and at that point, the whole Haskell community depends on Stackage, run by a private for-profit company. As a Linux user during the 1990s and 2000s I have very clear memories of a large for-profit company doing whatever it could to extinguish Linux and FOSS. I for one do not like the idea of the Haskell community becoming fully dependent on a private for-profit company. The small minority of people who don't like stack as a build tool and/or are concerned about the stack/Stackage/FPComplete hegemony will continue to feel marginalized until one of the following happen: * The minority dies of old age/gives up/stops using Haskell and effectively disappears. Under this scenario, the Hackage/Stackage decoupling becomes more and more likely over time. * The majority acknowledges there is a problem and works with the minority to bridge the gaps. I see the chances of the second possibility as basically zero (for both technical and social reasons), which makes the first a foregone conclusion. 
I think you are fighting a war that only exists in your imagination
Haskell has the same syntax for referencing a constant and evaluating a thunk though: `zero` *might* have been a thunk, you can't tell from looking at the invocation/reference. In OCaml, you'd have to call `zero ()` if it was a (very explicit approximation of a) thunk
I stated in another thread that "stack has won". That is an admission that any "war" that existed is already over. Is the minority that feels marginalized by this result also in my imagination? 
Meh, just deal with it when an issue presents itself.
Nit: `#Word32` is actually `Word32#`. Entertaining enough, `error`/`undefined` are levity polymorphic, which is another way of saying you can call it to get boxed values as well as unboxed ones. ghci&gt; :set -fprint-explicit-runtime-reps ghci&gt; :info error error :: forall (r :: GHC.Types.RuntimeRep) (a :: TYPE r). GHC.Stack.Types.HasCallStack =&gt; [Char] -&gt; a -- Defined in ‘GHC.Err’
&gt; I think that as long as /u/taylorfausak is actively involved with the survey it will continue to remain controversial and cause unnecessary drama. I understand that this seems to be the situation. However, I have a hard time believing that Taylor's involvement is *really* the thing that people are getting upset about. It may be what they say they are upset about, but that's just ad hominem. What it seems like is that among those that don't like `stack`, some are particularly adamant that stack does not hold any place of importance within the Haskell community. Taylor's last survey indicated that many people use stack, and prefer it over cabal, which obviously contradicts the idea that stack can be safely ignored. (It also seems that Taylor's last survey is being conflated with the last FP Complete survey. These were actually two separate surveys; I remember it being confusing at the time that both were happening.) There is certainly selection bias that played into the conclusion that stack usage is higher than cabal usage. However the anti-stack camp, instead of suggesting actual solutions to get fair and accurate survey results this year, is just kicking up dust and trying to discredit the survey results, so that when it inevitably ends up again indicating that many people use and like stack, they can simply plug their ears and ignore this information. (Again, this does not characterize everyone that prefers cabal over stack, I'm just saying a select vocal minority within that group exists.) The "solution" to the drama, perhaps, is to simply remove any survey questions that allow respondents to express preference between cabal and stack. There is plenty of other good info on the survey.
Would love to hear other's experiences with issues like these. I am working on a Haskell API binding right now and just today began considering how I should model errors. I found the post super interesting but it left me wanting a resolution!
If you read [Chris's post](http://bitemyapp.com/posts/2018-10-03-wrapping-up-haskellbook.html) on the matter, then it suggests that there were a few possible outcomes: &gt; * We would continue as equal partners, but there’s a structured agreement in place for the book’s completion and management. &gt; * I would sell Julie my rights in the book or vice versa. Chris doesn't mention what the details of the final agreement are, other than that there is a confidentiality clause. Julie says that she no longer has the rights to the book. So we know that Julie rejected: - continuing as equal partners with some structured agreement towards finishing the book - buying Chris out of the rights For Julie to no longer have the rights, she must have either a) gave them away as a gift, or b) traded them for some amount of consideration. Courts can nullify contracts for [disproportionate consideration in the case of bad faith negotiation](https://www.nolo.com/legal-encyclopedia/consideration-every-contract-needs-33361.html), so if Julie truly believes that Chris acted in bad faith while negotiating the contract, she can sue to have the contract nullified and her rights restored. If Julie believes that Chris coerced her into whatever deal, then the contract could be nullified due to [duress](https://study.com/academy/lesson/duress-and-undue-influence-in-contract-enforcement-krysa-v-paine.html). Julie is a really smart person, and by all accounts, she's tough and has overcome a lot. She owned half the rights to the book. And she decided, after a year of legal negotiation that she initiated, to trade them for something. We don't know what that something is -- both parties appear to be legally bound to keep it confidential. What we do know is that she agreed to it, and we can believe with fairly good confidence that there wasn't bad faith or coercion in the negotiation process (otherwise, it'd be reasonable to challenge the contract in court and nullify it).
For a real-world example of Bifunctor IO type, you can look at Scala's [ZIO](https://github.com/scalaz/scalaz-zio), it uses subtyping and covariance to manage the error branch so no manual lifting of error branches is required at all.
This is another example of why we need real structural typing, polymorphic variants and extensible records and row types and so on. I made [a post](https://www.reddit.com/r/haskell/comments/8uhj1f/what_is_the_status_on_structural_typing_row_types/) about this earlier. Real extensible anonymous structural types would solve this problem very quickly. As long as you have some primitives for extending/combining these structural types, such as unioning and inserting into variants. I would say this is my biggest issue with Haskell right now. Generics and various type level machinery help but they really aren't enough. Extensible types are not first class citizens, you can kinda hack around that issue with HLists and similar, but it's not going to give the desired ergonomics or performance.
It's definitely quite impressive but it is also quite inefficient (big error list = hugely nested `Either` = wasteful indirection), and as you said in the gist there were some "nasty constraints" which would be nice to avoid. Now I'm not saying generic-lens is a bad solution, but I do think proper structural types would improve things.
The main places I notice it are Databases, Persistent has overall been very pleasant, but `Entity` is kind of a hack to workaround lack of nice structural typing, really there should be a type which is just the required fields for insertion and `Maybe` wrapped around optional ones, and then a full type for values already in the DB. Another place I notice it is in multiple layers of types. Such as having an internal DB type, some wrapped create/update/read types to preserve DB invariants and hide implementation details, as well as some further wrapped create/update/read api types for the client to deal with that deal with things the client shouldn't be aware of / authentication. Each layer is a fairly verbose manual type + fairly verbose conversion functions + long prefixes for everything to avoid name collisions. Structural types would make the names of things much shorter without collisions as the names would be resolved based on the object type, so `user.name` would work instead of `userViewName user`. It would also allow these conversion functions and types to be built mostly automatically with manual intervention in the places that matter. It would also greatly reduce the need for template Haskell, as long as the necessary primitives were put in place for defining these types, none needed for lenses or for things like Apecs.
Also, newer versions of GHC have been getting better at transforming code that used to leak into code that doesn't leak automatically
Nothing about [length-indexed lists in Haskell](http://www.parsonsmatt.org/2017/04/26/basic_type_level_programming_in_haskell.html#vectors) is remotely as straightforward as in Idris. Even if, for the sake of argument, you were right about this, where are my useful typed hole suggestions and editor autocompletion in Haskell? We're a long way off.
GHC had a quite specific syntax for evaluating a thunk. `case`, or anything that desugars to it (like multiple function clauses). Anything else is just passing the thunk or value (if it has already been evaluated) around. Yes, Haskell (the report) doesn't really define much operational semantics, and denotationally it doesn't separate "a thunk that will evaluate to X" and X. In fact it doesn't require thunks, just laziness, so an alternative implementation would be possible.
Why you `cannot imagine using Matt's generic-lens approach `? What is the other approach?
I've come to agree with you. In the past, I'd hoped for something with a lot more bells and whistles, but it's clear no one's ever gonna figure it all out. Just give me the basics please :P
I tend to favor the simpler parts of the Haskell language and ecosystem than most others. My [novelty budget](https://www.shimweasel.com/2018/08/25/novelty-budgets) is low. The other approach is mentioned in Matt's post as an error sum type the constructors of which may contain other error sum types. I agree with Matt that is approach is not perfect, but its simple (Haskell98) and doesn't require generics or lens. 
This seems like an attampt to reinvent stack trace.
Are you using nixos? Otherwise you need to work around nix bringing its own graphics driver, that doesn't match your X and kernel (at least on linux, don't know, how it works on darwin). &amp;#x200B; See [https://github.com/NixOS/nixpkgs/issues/9415](https://github.com/NixOS/nixpkgs/issues/9415) and [https://github.com/guibou/nixGL](https://github.com/guibou/nixGL). Those talk about OpenGL but it is the same issue.
I believe that dialogue in general is the future of programming systems, and is one of the reasons I am so excited by the Language Server Protocol, because (in [haskell-ide-engine](https://github.com/haskell/haskell-ide-engine) anyway) a plugin to be able to - do some kind of analysis on the existing code, and present it to the user - receive the analysis from all plugins when the cursor is at a particular context and generate possible actions to take - apply an action that the user chooses between the available ones. This is basically a dialogue, the mechanics are there, we just need to add meaningful conversations. And typed holes for haskell is already implemented.
oh, thanks for the suggestion!
Thanks for writing this! I've been wrestling with this recently and have been kind of shocked that there isn't a better solution. It's worth mentioning that this all gets even worse if you start trying to use `MonadError`, since you can't even `mapLeft` over it reliably - only in the concrete instantiation as `ExceptT` on the outside! I had high hopes that [freer-simple](http://hackage.haskell.org/package/freer-simple) would be what I want, and I think it actually is, but it has performance problems :( It's nice that it does handle the discharging an error case gracefully.
This reminds me of [Catch me if you can](https://dl.acm.org/citation.cfm?id=2044490) paper. All these toy examples with exact error sets tracking look nice at first, but I don't believe it's going to work in real life. Because usually you don't really care what exactly went wrong, you want to know what can you do about it. Assume you have a complex multi-step transaction and you experience an error in the middle. You have at least 3 options: * Drop the whole transaction and bail out, propagating the original error (+ some context) to the caller. * Retry the step that failed (maybe after external event, like a timer or a notification from the network connectivity layer). * Retry the whole transaction (again, maybe some extra event required). You don't need precise error tracking in order to be able to do it. Assume now that you actually adopted your precise error tracking library and work on a simple application that needs to read and update some records from remote database. My claim is that the exact error types will be so huge that they're practically useless. There so many things that could go wrong (and will go wrong): * Network is temporary down (wait for the connectivity, retry the whole transaction?). * The database is overloaded (wait, retry?). * You have too many files open, can't create another socket to connect (bail out?). * There's a dead lock in the DB and the transaction was aborted (retry the whole transaction?). * There's a mistake in the generated SQL query (bail out?). * The record requested doesn't exist (bail out?). * The transaction in DB has a trigger that executes some PL/SQL code that failed (bail out?). * ... Ok, you can layer the error types and wrap the precise error type into generic ones (e.g. all the DB-related errors into some `DbError`). But then you lose some really important information. Again, the exact error matters when you read the log or report it to the user, but when you write your code you don't really need it. Just pass the details around as text. One approach that I believe works and scales nicely is "generic errors". You have a predefined list of errors that represents the most important error conditions. You can define as many error types as you like, but for every type you have to define a projection to the generic error space. The haskell version would be something like: ```haskell -- No matter how many systems you have, you need to learn and understand -- the semantics of these canonical codes by heart. data CanonicalErrorCode -- | The operation was aborted by the system, probably due to -- concurrency reasons. For example, database engine aborted a -- query because of a deadlock. = Aborted -- | The resource the operation attempted to create already exists. | AlreadyExists -- | The operation was canceled, most likely by a user request. | Canceled -- many (~15) more ... | Unknown deriving (Enum, Eq, Ord, Show) -- New types of errors have to provide a projection to canonical codes -- and a category name. class (Show t, Eq t, Typeable t) =&gt; ErrorCode t where errorSpaceName :: t -&gt; Text toCanonicalCode :: t -&gt; CanonicalErrorCode -- Erases the exact type of errors so that we can pass them around explicitly. data GenericErrorCode = forall t . ErrorCode t =&gt; GCode t -- GenericErrorCode + some details. data Error = Error { _errorCode :: GenericErrorCode , _errorLocation :: (Maybe Loc) , _errorCause :: Text -- ... } type ErrorOr = Either Error -- We can gen a canonical code from any error. canonicalCode :: Error -&gt; CanonicalErrorCode canonicalCode = getCanonical . _errorCode -- We can also get the precise original error back if we need it. getErrorCode :: (ErrorCode t) =&gt; Error -&gt; Maybe t getErrorCode = cast . _errorCode ------------------------------------------------------------ -- Adding new types of errors ------------------------------------------------------------ newtype HttpCode = HttpCode Int deriving (Eq, Show, Typeable) instance ErrorCode HttpCode where errorSpaceName = const "http" toCanonicalCode (HttpCode x) = case x of 400 -&gt; InvalidArgument 401 -&gt; PermissionDenied 404 -&gt; NotFound 408 -&gt; DeadlineExceeded 500 -&gt; InternalError 503 -&gt; Unavailable _ -&gt; Unknown downloadFile :: URI -&gt; IO (ErrorOr ByteString) ``` So it's like exceptions with type erasure: you pass them explicitly, you can try to cast them programmatically if need the precise information (thanks to `Typeable`), and you have projection to generic error space so that you can decide what to do without having to know the unimportant details of the underlying systems. Even better, this approach composes perfectly because you always have just a single `Error` type! Janestreet [does something similar](https://blog.janestreet.com/how-to-fail-introducing-or-error-dot-t/) in OCaml. I work on a code base with many millions of lines of C++ code and we use the approach I described. It scales perfectly. I tried many ways of handling errors in my career, this one feels superior by a wide margin so far.
That was awesome. Do you have another code example on GitHub or something where you utilise this pattern?
As one of the major people who has tried to promote skepticism about these surveys, I first want to apologize (especially to /u/taylorfausak) for the partisanship and uproar this skepticism has caused. My intention was only to promote a healthy understanding that these surveys are not the word of God. It seems clear to me that your comment is response to comments I've made regarding the past surveys (among comments by others), so I want to try to explain my position and state that this shouldn't be a partisan issue. &gt; However the anti-stack camp, instead of suggesting actual solutions to get fair and accurate survey results this year, is just kicking up dust and trying to discredit the survey results If there's one thing I've learned during this whole issue, it's that survey design is pretty hard. I really want to know accurate numbers about the Stack vs Cabal usage out there. I do not consider myself anti-either-of-these (and the insinuation that anti-either's are even common is a major reason the partisanship exists in the first place). Stack is still the tool I recommend to newcomers, despite my personal preference for both Cabal and Nix. So when I question these surveys, I'm not *trying* to "kick up dust" and "discredit the survey results". I'm trying to approach information that is useful for both stack and cabal users and developers, because both are important to me. Suggesting actual solutions to this problem is a very hard problem considering survey design is very hard. But step 1 is acknowledging the issues, and that's the only part of this I feel confident I'm capable of doing. So I'm genuinely sorry I don't have better solutions for you, and I'm sorry this skepticism has been used for FUD rather than for approaching real solutions. &gt; so that when it inevitably ends up again indicating that many people use and like stack, they can simply plug their ears and ignore this information. I'm perfectly willing to admit that it's likely the majority of Haskell users use Stack. But it's dangerous to be making claims like it's 80-90% without some extremely reliable data to back that up. I think it'd be really bad if people concluded that the tools they write only need to work for Stack. Aside from the question about whether that's the right move for popularity's sake, it also just prevents innovation and development of alternatives (because people feel pigeon-holed to avoid those alternatives and prefer Stack). I'm happy if Stack solves a lot of people's problems, but I'm not so happy if our community begins to create indirect problems because of that. `intero`'s emacs plugin is a very minor example of this. I just don't want to see 20% of the community cut out and ignored from good tooling solutions (and yes, selfishly that 20% includes me). I know this isn't a goal of any of the Stack enthusiasts, but it's a consequence we could see nonetheless. So my point is that I consider these conclusions dangerous. Not *bad*, but definitely capable of *producing* bad consequences. I want to emphasize that I'm not "anti-stack," and that I really, truly, do appreciate the efforts Taylor has gone through to try and improve this survey. I hope it's clear that my goal here is to approach improvement, not to discount anyone's efforts.
Thank you! I’m happy that we were able to hash this out. 
(!!) :: [a] -&gt; Int -&gt; a, so the second argument of !! should be an integer. Your function passes it an Integral b.
Nice! Thanks!
Sadly reddit doesn't allow marking code blocks with triple-backticks… :/ -- Copyright 2018 Google LLC -- SPDX-License-Identifier: Apache-2.0 -- No matter how many systems you have, you need to learn and understand -- the semantics of these canonical codes by heart. data CanonicalErrorCode -- | The operation was aborted by the system, probably due to -- concurrency reasons. For example, database engine aborted a -- query because of a deadlock. = Aborted -- | The resource the operation attempted to create already exists. | AlreadyExists -- | The operation was canceled, most likely by a user request. | Canceled -- many (~15) more ... | Unknown deriving (Enum, Eq, Ord, Show) -- New types of errors have to provide a projection to canonical codes -- and a category name. class (Show t, Eq t, Typeable t) =&gt; ErrorCode t where errorSpaceName :: t -&gt; Text toCanonicalCode :: t -&gt; CanonicalErrorCode -- Erases the exact type of errors so that we can pass them around explicitly. data GenericErrorCode = forall t . ErrorCode t =&gt; GCode t -- GenericErrorCode + some details. data Error = Error { _errorCode :: GenericErrorCode , _errorLocation :: (Maybe Loc) , _errorCause :: Text -- ... } type ErrorOr = Either Error -- We can get a canonical code from any error. canonicalCode :: Error -&gt; CanonicalErrorCode canonicalCode = getCanonical . _errorCode -- We can also get the precise original error back if we need it. getErrorCode :: (ErrorCode t) =&gt; Error -&gt; Maybe t getErrorCode = cast . _errorCode -- Adding new types of errors newtype HttpCode = HttpCode Int deriving (Eq, Show, Typeable) instance ErrorCode HttpCode where errorSpaceName = const "http" toCanonicalCode (HttpCode x) = case x of 400 -&gt; InvalidArgument 401 -&gt; PermissionDenied 404 -&gt; NotFound 408 -&gt; DeadlineExceeded 500 -&gt; InternalError 503 -&gt; Unavailable _ -&gt; Unknown downloadFile :: URI -&gt; IO (ErrorOr ByteString) 
Have you tried the haskell-ide-engine? It offers both features
Thanks. I'll try it out when I get home
I'd be curious to see how languages that have some of these features handle it. Purescript has row polymorphism for example. Also, the type signature of `foo` with all the constraints on err reminded me a lot of effect systems, so I wonder, are errors a sort of effect, amenable to the various effect libraries out there? It's also kind of depressing that we're _still_ trying to find a unified way of dealing with exceptions in haskell. Well, at least we're not using `error` (hi, Prelude)
it's crazy that there's not more of this. Corporations and their product depend on open source, but they're not giving back (besides some pull requests once in a while). Running an open-source project takes a lot of time and commitment.
I haven't tried it. I'm aware of HIE, and I think it's a fantastic project (thank you Alan!). But please consider what you're proposing I do here, on top of having GHC available and having a basic understanding of Haskell: 1. Use advanced, non-obvious, and non-trivial type-level features of Haskell. I gave this presentation at Haskell eXchange this year and an audience question was "what about singletons"? 2. Install and configure a non-trivial suite of tools to communicate with GHC. 3. Fiddle with [the Vim plugin settings](https://github.com/haskell/haskell-ide-engine#using-hie-with-vim-or-neovim) in order to get everything set up correctly. 4. *Maybe* get useful suggestions, modulo the presence of `undefined` etc. This is a similar problem to the one I face when I try to recommend IHaskell: it's a pain in the butt to set up, and that (to my mind) puts a ceiling on how popular it can be. In contrast, here is what I had to do for Idris: 1. [Install a Vim plugin](https://github.com/vaibhavsagar/dotfiles/blob/c4f83478108a9b612bb8ab327490e6f7a06f0678/vimrc#L26). I think it's great that we're working towards a future where the Haskell development experience is similar to Idris, but we're not there today.
Though ghc is happy to replace (unproductive) infinite loops with crashs.
Actually it does now, so they display just fine on new reddit, just not on old.reddit.
Oh, thanks for the hint. I might just switch then.
There is no better way to learn than to write your own functions! However, what you want can be very neatly solved with the tools Haskell already provides you. What you want, is to map each element in the first list to its corresponding element in the second list. So: map (function to retrieve element from list2) list1 
One issue with your implementation is that it will produce an error if there is an index in `xs`, but no corresponding element in `ys`. You could eliminate the above error and avoid explicit recursion with the following implementation (`atMay` is from the `safe` package): import Data.Maybe (mapMaybe) import Safe (atMay) getById :: [Int] -&gt; [b] -&gt; [b] getById xs ys = mapMaybe (atMay ys) xs I might also be inclined not to name the function arguments `xs` and `ys`, but that's just stylistic.
I just want to add that the "flipped either monad" is on hackage and they call it [`EitherR`](http://hackage.haskell.org/package/errors-2.3.0/docs/Data-EitherR.html) or "success monad".
Never do the former. It breaks every bit of code that assumes `Eq`works correctly. Floating point types have a similar issue with `NaN` and let me tell you: days of my life lost to that nonsense. Your latter example is fine. Why do you think it's similar to the first?
you know how JavaScript's equality operator (`==`) doesn't have a lot of the properties you expect equality to have, and this is a big source of confusion and bugs? yeah, don't do this sort of thing, please You have some other concept than equality; name it, define it, and use it, but don't call it equality.
I've just installed this, not got much use out of it yet naturally but it looks like a very useful tool. I've already given it a whirl with a few test signatures and it works well. 
The problem is the people making the money decisions are too far away from the code in most corps. Your usual rung of Engineering Managers and the like have their head filled with KPIs and OKRs to track and they usually forget about stuff like this. On the other hand the devs are really inclined to contribute but they have no money authority and usually need permission to contribute to OSS if using office hours and equipment. So yeah engineers once you become managers, don't forget where you started from! 
Cool, just wanted to point out that an alternative exists, even if today is more than a single step to install. I'm working towards making the haskell-ide-engine experience as easy as painless as possible.
How exactly does your \`Eq\` instance for \`Q\` violate Reflexivity?
&gt; Do you have another code example on GitHub or something where you utilise this pattern? Unfortunately, no. I wrote a small library in Haskell just for myself to understand how to nicely map `error_category` / `error_code` ideas from C++ to Haskell and so that they actually become usable (we use custom error handling types at work, which look really close to `Either Error a`), but I never published it (and it requires a lot of work before it's actually ready for production use). The approach is very simple (well, most of it is in the comment), the hardest part is to decide on the exact list of generic codes and use them consistently in the whole ecosystem. You can easily have this in a huge commercial code base, but it's really hard to force things like that in an open-source community, so I never actually bothered. In addition, I don't have a significant experience of using it in Haskell, I'll test it first and maybe wrap up the experience later.
Please see the updated post on more information on what I mean by "neutral elements", it also explains my motivation for these types. Thank you for your warning! I will try another method.
Thank you! However, I'm having trouble coming up with a suitable name. Do you have any ideas?
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [alanz/happy/.../**5b877cfaa7d460d31ab68872bf50c70646e00adf** (repetitive-rebased → 5b877cf)](https://github.com/alanz/happy/tree/5b877cfaa7d460d31ab68872bf50c70646e00adf) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply e91zo95.)
This makes me warm and fuzzy inside. Great news!
Also called “primitive recursion” in mathematical literature
This is good news for haskell.org but it would have been better for the ecosystem if the money went to a project which uses a less restrictive license we all can benefit from.
In your own program. If it's useful go ahead, but remember that there's a lot of code out that that assumes instances are lawful. I would prefer you not upload these kinds of instances to Hackage / Stackage.
In violation of Betteridge's Law, the answer to the question in your headline/title is: Yes.
&gt; I had to disable the GC and call it manually at opportune times to avoid GC pause issues. You would do this in a desktop app as well, this is not unique to Haskell, it's a problem with GC's in general, Most games have been, and still are using C++ for this very reason. [for example](https://unity3d.com/learn/tutorials/topics/performance-optimization/optimizing-garbage-collection-unity-games). You could try purescript, and maybe something [like this](https://hackage.haskell.org/package/purescript-bridge)
Note that the thing you did for `Ord` does not violate any laws and is thus OK.
I find that when I have trouble naming things I get a lot of mileage by trying to be very precise in describing what it is/does/etc in long form language and then googling for those terms. Usually, especially in technical areas, you'll find an exact term for your thing. If not you'll at least have some inspiration.
As far as I am aware purescript does not outperform GHCJS, it only really wins in terms of binary size. I also would rather not invest too much into purescript as I think GHC-&gt;WASM will make it fairly unnecessary. 
-- https://old.reddit.com/r/haskell/comments/9t8q9y/2018_state_of_haskell_survey/e907386/ As one of the major people who has tried to promote skepticism about these surveys, I first want to apologize (especially to /u/taylorfausak) for the partisanship and uproar this skepticism has caused. My intention was only to promote a healthy understanding that these surveys are not the word of God. It seems clear to me that your comment is response to comments I've made regarding the past surveys (among comments by others), so I want to try to explain my position and state that this shouldn't be a partisan issue. &gt; However the anti-stack camp, instead of suggesting actual solutions to get fair and accurate survey results this year, is just kicking up dust and trying to discredit the survey results If there's one thing I've learned during this whole issue, it's that survey design is pretty hard. I really want to know accurate numbers about the Stack vs Cabal usage out there. I do not consider myself anti-either-of-these (and the insinuation that anti-either's are even common is a major reason the partisanship exists in the first place). Stack is still the tool I recommend to newcomers, despite my personal preference for both Cabal and Nix. So when I question these surveys, I'm not *trying* to "kick up dust" and "discredit the survey results". I'm trying to approach information that is useful for both stack and cabal users and developers, because both are important to me. Suggesting actual solutions to this problem is a very hard problem considering survey design is very hard. But step 1 is acknowledging the issues, and that's the only part of this I feel confident I'm capable of doing. So I'm genuinely sorry I don't have better solutions for you, and I'm sorry this skepticism has been used for FUD rather than for approaching real solutions. &gt; so that when it inevitably ends up again indicating that many people use and like stack, they can simply plug their ears and ignore this information. I'm perfectly willing to admit that it's likely the majority of Haskell users use Stack. But it's dangerous to be making claims like it's 80-90% without some extremely reliable data to back that up. I think it'd be really bad if people concluded that the tools they write only need to work for Stack. Aside from the question about whether that's the right move for popularity's sake, it also just prevents innovation and development of alternatives (because people feel pigeon-holed to avoid those alternatives and prefer Stack). I'm happy if Stack solves a lot of people's problems, but I'm not so happy if our community begins to create indirect problems because of that. `intero`'s emacs plugin is a very minor example of this. I just don't want to see 20% of the community cut out and ignored from good tooling solutions (and yes, selfishly that 20% includes me). I know this isn't a goal of any of the Stack enthusiasts, but it's a consequence we could see nonetheless. So my point is that I consider these conclusions dangerous. Not *bad*, but definitely capable of *producing* bad consequences. So they need to be handled with extreme care. I want to emphasize that I'm not "anti-stack," and that I truly do appreciate the efforts Taylor has gone through to improve this survey. I hope it's clear that my goal is *not* to discredit anyone, but to express skepticism and caution.
My personal machine is Linux, running GNOME Desktop; I occasionally use GIMP to edit images I use in my Hakyll (and therefore Pandoc) powered blog. I also know other people who use Hakyll for their blogs, or Pandoc to directly to make their presentation slides. So the ecosystem does benefit from Pandoc. Maybe not directly, but there are huge amount of material, which wouldn't be there (at least in as pretty form) without it.
Re: name, it depends on what you're using `Q` for. - If `Q` is only used in a specific context, I'd go with a domain-specific name, like `userEquiv` for comparing `User` values - If it's generic, maybe something like `equiv`(alent) - equivalence relations are often used as "kind-of equality". If you like symbols, `=*=` doesn't look too bad (the asterisk represents the "special element"). 
Thanks for this thoughtful reply. I suppose I myself am guilty of perpetuating the perception of partisanship. I was not thinking of your comments in particular, but rather, I think mentally I conglomerated a few disparate comments from various people and wove a story together that isn't really accurate in regards to the motivations of each individual commenter. As someone who might be perceived as "on the stack side", I *strongly* prefer that all packages maintain compatibility with both stack and cabal. Covering the cabal use case is never something I want to see considered inessential. On the contrary, I think healthy competition between stack and cabal leads to the betterment of both. Skepticism and caution are valuable things. I hope *my* comments have not led to any artificial suppression of critiques. We can and should always be striving to do better!
You can define whatever you like. The compiler does not rely on any type class laws. But be very careful how you use definitions that break the laws. It’s easy to make mistakes. The best advice is, don’t do it. 
I'm trying to reset my database, so I'm trying to delete everything. Based on the tutorial, I thought I could just do a deleteWhere where the condition would always be true, but I can't use it, because everywhere I put it, there's a type error because the deletion is a ReaderT. How do I do this?
I think it's worth noting that even Idris will happily accept a `map` implementation that just applies `f` to the first element of the list and duplicates the result as many times as the length of the list. You'd have to encode the law `map id = id` at the type level to stop that implementation from type checking.
This is great, thanks! It took me a year of fiddling before I was comfortable replacing my existing Pelican blog with Hakyll.
&gt; The compiler does not rely on any type class laws. There's *no* REWRITE rule that uses the laws for correctness?
indeed, most wealth is wasted. few companies give back "in kind" with open source, let alone with actual money. iirc, Torvalds was granted enough stock by several companies (which succeeded due to Linux) to have financial independence, from gratitude. but I'm not aware of such "voluntary redistribution" towards the other several thousand of critical, well, volunteers of time and labor. 
As an enthusiastic but inexpert Haskell newbie, this sounds to me like either dependent types or some other type-level magic. I look forward to seeing if anyone has any actual solutions to suggest.
For me, the important aspect of Monad is that it allows computations to make decisions based on previous values; this is the key to it allowing actions to be sequenced together. (&gt;&gt;=) :: m a -&gt; (a -&gt; m b) -&gt; m b You can't write the following using just Applicative, for example: check :: m Int -&gt; m Int check mi = mi &gt;&gt;= \i -&gt; if even i then pure i else fail "Be more evenhanded please" There's also an interesting analogy between what is possible with a context free grammar and a context sensitive grammar, which makes pretty closely to the distinction between Applicative and Monad (IIRC).
For rendering strings, the _formatting_ library is quite nice, and should avoid a lot of the use of Show all over the code. For parsing, using an actual parser combinator library or at least a CSV library like _Cassava_ or [_sv_](http://hackage.haskell.org/package/sv) (which can use rank-select to produce some absolutely amazing results performance wise) would be a much better option for both parsing and rendering, particularly since this will avoid the use of String anywhere - if you want fast, you don't want String. For handing time, the _Thyme_ package is great - a project of mine which was producing CSVs in the order of hundreds of thousands of lines, each containing timestamps, moving to Thyme doubled the speed of the service (I'm not sure if this mainly came from rendering time stamps or the fact Thyme's internals use a very similar representation of times which I could exploit to turn parsing into just maths). We have a lot of really amazing libraries in Haskell, the best thing for the Author to do would be to jump on IRC and ask about them - the original post about comparing Haskell, OCaml and Go showed this hadn't been done and the author had a really bad time because of it.
i feel like a richly-typed SQL EDSL like `opaleye` would be significantly more useable if `proc`-notation desugared to (true) records instead of tuples.
Cited, updated the related section. Thanks. v2 is available from https://oxij.org/paper/ExceptionallyMonadic/ right now, arXiv will publish the update in about 23 hours.
I noted this[1][] interesting project to promote evidence based PL design. They have some nice and surprising insights, for example, they measured and compared programmer productivity of having a `.` (dot) symbol or a `:` symbol as operator for accessing fields in objects - and found a significant difference. While Haskell's beauty is partly due to (probably accidentally) a "nice" syntax, it is the purity and conceptual elegance that I personally like. Generally I think that there is a dire need for evidence based PL design and that the current language designs and programming habits are based on folklore and guru charisma. (Less so in the FP community since we got rid of OO design patterns) Why not improve even further? I think there is no need to base every single operator and function name in `base` on evidence and ergonomics, but I think the academic FP PL community could be a little less full of itself. [1]: The Quorum Project (https://quorumlanguage.com/evidence.html)
When you have a generic/polymorphic function, you can think of the type variables as *additional arguments* to the function, which are passed implicitly by the compiler, or can be passed explicitly with `TypeApplications` using e.g. `id @Int` (of type `Int -&gt; Int`). The *caller* decides what concrete type to use as the value of each type variable. So when you write this: f :: forall a. Bool -&gt; a f False = 'c' You’re saying that `f @Int` returns an `Int`, and `f @Bool` returns a `Bool`, and so on. The error is that you always return a `Char`. If you want to return *some* type that the function decides, you can use an existential type: data Some = forall a. Some a -- Or, with GADTSyntax/GADTs: data Some where Some :: forall a. a -&gt; Some a f :: Bool -&gt; Some f False = Some 'c' f True = Some (0 :: Int) But unless you add more constraints to the type hidden in `Some`, the *caller* now has to treat it polymorphically, because it has no information about the type! So in practice what you usually do is pack additional information along with an existential, either a typeclass constraint or equivalently a collection of operations that all work together on the hidden type: data SomeIntegral where SomeIntegral :: forall a. Integral a =&gt; a -&gt; SomeIntegral useSomeIntegrals :: SomeIntegral -&gt; SomeIntegral -&gt; Integer useSomeIntegrals (SomeIntegral x) (SomeIntegral y) = fromIntegral x + fromIntegral y useSomeIntegrals (SomeIntegral (2 :: Int)) (SomeIntegral (3 :: Integer)) == (5 :: Integer) -- The type of abstract set modules data MySet a where MySet :: forall s a. { setEmpty :: s a , setInsert :: a -&gt; s a -&gt; s a , setMember :: Ord a =&gt; a -&gt; s a -&gt; Bool , setRemove :: Ord a =&gt; a -&gt; s a -&gt; s a } -&gt; MySet a -- An implementation of sets that uses lists internally myListSet :: MySet a myListSet = MySet { setEmpty = [] , setInsert = \ x xs -&gt; x : xs , setMember = \ x xs -&gt; x `elem` xs , setRemove = \ x xs -&gt; delete x xs } -- Uses a set module to do some computations (returning False) -- Only has access to these operations; -- Cannot tell what type the underlying implementation uses useSet :: MySet a -&gt; Bool useSet (MySet empty insert member remove) = member 'x' $ remove 'x' $ insert 'x' empty With higher-rank polymorphism, you can pass a polymorphic function `f` as an argument to another function `g`, in which case the caller of `f` is `g`, that is, `g` decides at which types to apply `f`. For example: manipulateLists :: (forall a. [a] -&gt; [a]) -&gt; ([Int], [Bool]) manipulateLists f = (f [1, 2, 3], f [True, False]) -- Or: manipulateLists f = (f @Int [1, 2, 3], f @Bool [True, False]) manipulateLists reverse == ([3, 2, 1], [False, True]) The function that you pass to `manipulateLists` needs to work *for all* possible types `a` that `manipulateLists` chooses, so this won’t work: manipulateLists sum Because `sum` only works on containers whose elements are constrained to be instances of `Num`. What “higher-rank” actually *means* is that the nesting depth of `forall` quantifiers to the left of a function arrow is greater than 1. Examples: * Rank 0: `A` * Rank 1: `forall a. A a` * Rank 2: `(forall a. A a) -&gt; B` [higher-rank] * Rank 3: `(forall a. (forall b. A b) -&gt; B a) -&gt; C` [higher-rank] This is similar to the concepts of *higher-order* and *higher-kinded* polymorphism; when the order of something is greater than 1, it’s “higher”: * Order 0: `A` * Order 1: `A -&gt; B` * Order 2: `(A -&gt; B) -&gt; C` [higher-order] * Order 3: `((A -&gt; B) -&gt; C) -&gt; D` [higher-order] A higher-kinded type like `Functor` or `Monad` is a type-level higher-order function. * Kind 0: `*` / `Type` (e.g. `Int :: Type`) * Kind 1: `* -&gt; *` / `Type -&gt; Type` (e.g. `Maybe :: Type -&gt; Type`) * Kind 2: `(* -&gt; *) -&gt; *` / `(Type -&gt; Type) -&gt; Type` (e.g. `Functor :: (Type -&gt; Type) -&gt; Constraint`) [higher-kinded] (Note: we don’t usually say “kind *n*”, this is just for illustration.) 
&gt; It shouldn't be *merely* an instance of Category with the arrows flipped. What I want to be able to be able to express is that the opposite category *refines* the base category by interchanging domain/codomain, but I know of no way in Haskell to do this. What's the difference?
That’s not a fruitful proposition. Haskell classes are not a closed world, so you can add new ones. But there is no way to inform the compiler what laws you expect to hold. Until we have a language (like Cayenne, Agda, Idris, etc) where the laws can be checked, the laws cannot be part the program semantics. 
Have you seen the `Down` newtype and it's `Ord` instance? It sounds analogous to what you want to do.
[removed]
I'm a bit confused. I suppose this satisfies your requirement a) data Op cat a b = Op (cat b a) instance Category cat =&gt; Category (Op cat) where id = Op id Op f . Op g = Op (g . f) But what exactly does b) mean? I don't get it. 
You're welcome
I'm not sure exactly what you mean, but I don't see how to write `return` with just `fmap` and `join`.
Only Phd students? :(
OP here: if you feel comfortable, please thumbs up or down the issue on GitHub so I can gather some data on how popular this proposal is (up or down voting this post is a helpful signal as well). If you have any comments or concerns feel free to leave them here, on the issue, or if you'd like to remain anonymous feel free to PM me. Thanks!
You seem to be using typeclasses as if they were "classes" in a more object-oriented sense, which they really aren't. You should ignore everything you know about the object-oriented notion of class when considering type-classes. Instead, typeclasses should more be seen as a combination of a constraint and a structure that can be applied to types. For example, `Eq a` could be thought of as the constraint requiring there to exist a function that decides equality on `a`, or it could be considered to be an equivalence relation structure on `a`. In math, a category can be viewed as a structure on a graph (rather than a set, which is the underlying collection for most algebras) that allows composition of edges and identity edges for each vertex. The structure does not include the object set or hom set itself, because the category is a structure *on* these sets. A first attempt at encoding this in Haskell might be like the following: class Category obj hom where id :: (a :: obj) -&gt; hom a a (.) :: (a :: obj) -&gt; (b :: obj) -&gt; (c :: obj) -&gt; hom b c -&gt; hom a b -&gt; hom a c The category typeclass does not have any member describing its objects type or homs type, because it is instead a structure *over* the objects type and homs type. This definition has a number of problems, most importantly that it requires dependent types, which Haskell doesn't have. However, what we can do is bump the objects collection up to [kind](https://en.wikipedia.org/wiki/Kind_(type_theory)) level, such that the hom set is allowed to depend on it (with the side-effect that the objects are only going to be available on type level). This would look like the following: class Category hom where id :: hom a a (.) :: hom b c -&gt; hom a b -&gt; hom a c ... which is just the standard haskell definition of categories. This then allows defining the opposite category: newtype Op c a b = MkOp (c b a) instance Category c =&gt; Category (Op c) where id = MkOp id (MkOp f) . (MkOp g) = MkOp (g . f)
Still breaks transitivity for the relations defined by some of the operators: `Q 1 &lt;= Q'` and `Q' &lt;= Q 0` but not `Q 1 &lt;= Q 0`. `Q 0 &gt;= Q'` and `Q' &gt;= Q 1` but not `Q 0 &gt;= Q 1`. Definitely a broken (i.e. non-law abiding) `Ord`.
It still wouldn't surprise me if they exist. In particular, I think we have some for the free theorems around `Functor` et. al. and I'm not sure those free theorems actually hold in the presence of \_|\_. That said, I've never felt compelled to write my own `REWRITE` rules, so maybe that bit of compiler magic has been used more responsibly than I expect.
Nice to see Mazzola and (Mazzola's) Valery cited by way of introduction to the Yoneda lemma.
Thanks for writing this. It's pretty useful feedback on experiences using these tools.
It isn't crashing per se. It is just stopping after type checking but before it picks concrete types for your code. &gt;:t eval_ex eval_ex :: (Fractional p, Integral p) =&gt; p -&gt; p -&gt; p Based on the functions you are using in eval_ex, it has decided whatever you passed it must both be fractional and integral. We know that isn't possible but there's no reason for the compiler to think that there isn't a type that fits both of those type classes at the same time. My recommendation is to set concrete types and then let the compiler tell you why that type won't work. I'm going to assume from looking at the code that you intended both x and n to be integral. eval_ex :: Int -&gt; Int -&gt; Int • No instance for (Fractional Int) arising from a use of ‘/’ So you can't use Int because the (/) function requires both its arguments to be fractional and integers are not. At that point you'll have to look at what you intended to happen and figure out what you need to do about it. There may be some use of `div`, `divMod`, `round` or `floor`. 
Agreed, if this change were to be made this behaviour could be disabled (via ~/.cabal/config as you mention), and the generated cabal.project file would contain comments for how to unpin the version (by commenting out the `with-compiler` directive). &gt; I can imagine it helping someone My motivation for this proposal is that folks who aren't as familiar (or familiar at all) with the tools should never run into a situation where their project build breaks due to some external factor that they were unaware they needed to care about.
[https://47deg.github.io/sbt-microsites/](https://47deg.github.io/sbt-microsites/)
&gt; Mazzola identifies a whole *Yoneda Philosophy* in the arts: understanding a thing through its relationships with other things. Mazzola cites Paul Valéry’s dictum that “c’est l’exécution du poème qui est le poème” (that is, the essence of a poem is determined by its readings, public or private, and not just its text), Theodor Adorno’s statement that “die Idee der Interpretation gehört zur Musik selber und ist ihr nicht akzidentiell” (that is, performance is an essential aspect of a composition's identity), and the common experience of walking around a sculpture to see it from all angles in order fully to appreciate it. Comment if you have examples of the *Yoneda Philosophy* from art or anywhere else in life, especially where you wouldn't expect
So the filter I was using was deleteWhere [TableTablecontents !=. "" ] (which is well typed and everything) Putting it in a handler that reads from the DB like appMain or a routing function and would get an error like Couldn't match type ‘ReaderT backend0 m0’ with ‘HandlerFor App’ (or in the case of appMain, sub IO () for HandlerFor App)
There is also pure.
I do not agree the Opposite should be a refinement of Category (whatever it may mean). Opposite is a way to construct a new category from an existing one. (Namely, you take the same objects, and for every map f:x-&gt;y you create a new map f’:y-&gt;x.) At best, Opposite should be an instance of Category.
Somewhat tangential: I used to love darcs and hate git. About three years ago I realised that git's terrible UI belies its simple and elegant internals and since then I've far preferred git to darcs. I wrote [a summary of how I use git](http://h2.jaguarpaw.co.uk/posts/git-survival-guide/).
Fernando Zalemea has a great set of lectures at Pratt from a few years ago on grothendieck along side Valery. Mazzola makes some appearances as well. And Mazzola has a presentation at eflux 7 or 8 years ago online where he talks about yoneda and composition. 
I'm sure there is a rewrite rule for `fmap f . fmap g` to `fmap (f.g)` which may break if the `Functor` instance is funky.
&gt; instance Ord x =&gt; Ord (Q x) `Q'` is a constructor of `Q` which has this, lawless `Ord` instance.
Oops, that's what I get for not reading OP's post properly.
Have you heard about Pijul? IIRC, it still needs lots of "front end" work, but it's got internals based on Category theory (inspired by Darc's approach to patching) and speed comparable with Git or Mercurial.
I don't understand any of this, but I think [this gist](https://gist.github.com/ekmett/b26363fc0f38777a637d) by /u/edwardkmett may be like what you're asking for? He talked about it in this video: https://www.youtube.com/watch?v=ZL9ehIJhk98
https://ghc.haskell.org/trac/ghc/ticket/15777 is a nice fix in this release 😀
In addition to this, there are also quite a few permissively licensed projects which benefit from this more-or-less directly: BSD3 licensed projects maintained by John MacFarlane, pandoc's primary developer: pandoc-citeproc, skylighting-core, cheapskate, commonmark-hs, and Haskell bindings to cmark. Most of these are or will be used in pandoc. There is also the MIT-licensed hslua, which would possibly be unmaintained if it wasn't for pandoc. 
I used Learn you a Haskell for Great Good so far ive learned alot. Thnks for the help.
This looks cool! I don't know much about these frameworks, so I'm wondering: what is the advantage is of using pytorch vs binding directly to torch? Part of why I'm asking is that I maintain hslua, which might be useful for this (the latest release dropped support for LuaJIT, but I'd consider to maintain previous versions if there's actually a need for it).
Take a look at http://hackage.haskell.org/package/data-category for a nice embedding that’s very powerful 
To narrow things down for you, you can start here: [Learn You a Haskell](http://learnyouahaskell.com/).
To be clear, that ticket was only closed since it is a duplicate of a much older ticket ([#12088](https://ghc.haskell.org/trac/ghc/ticket/12088)). That one has *not* been fixed yet.
I have one function that has type `String -&gt; Either String [Token]` and another function that has type `[Token] -&gt; Either String N` where N and Token are sum types defined elsewhere in the file. It feels like I should be able to chain these together some way more elegant than case matching, but I’m still sort of vague on monads and how they all work together. Is this doable? The ultimate project is to write a toy json parser. I've already got the basic version working, but that throws errors and I'd rather use Either.
Interesting, thanks!
This also seems to be in the neighborhood of my target; I'll study this in addition to kmett's take. Thanks
What I mean by "refinement", informally, is subsumption + alteration/addition. The Opposite category has exactly the same specification as Category, except for the distinguishing feature of contravariance. If the Opposite category is defined as an instance of Category, we're left without a way to apply the Opposite construction to other constructions, such as the product category you cite.
By "refinement" I mean that the Opposite type class, should one be defined, should specify exactly the same constraints, modulo the induced contravariance. If the Opposite construction is just encoded as an instance of Category, then that's a problem because there's no way then to apply the construction to other constructions such as the product category you cite. 
*Long hard stare in Facebook's general direction*
*cough* Facebook 
&gt; It's also kind of depressing that we're still trying to find a unified way of dealing with exceptions in haskell. I wouldn't really say it's a Haskell problem, its far more general. Most systems punt and say anything that can throw, can throw any exception (or something any thing) and let the programmer figure out. Sometimes you can limit which things can throw, sometimes not. Asynchronous exceptions can be problematic in other languages, too. Sometimes you get something where a function has to be more specific about what it can throw (Java, if you ignore Throwable and RuntimeException, e.g.), but HOFs can get really awkward, either indicating they throw too many or too few exceptions, require manual lifting/lowering, or *very rarely* you get something that looks like the prism based solution in the article, which may actually be one of the cleaner approaches I've seen -- but I don't know how well it performs and have well type inference works with it. And, of course, async exceptions could mess everything up, again. Well-typed non-local flow-control that is programmer-friendly is just not easy.
I'm not on [old.reddit](https://old.reddit.com/). I'm on www.reddit.com and they don't work for me. Though, I *am* on the old layout because the new one is whatever the opposite of an improvement is and I opted out of the redesign in my user preferences. Maybe once RES gets better support for the new layout I'll switch. RES would need to restore some of the features that the redesign ditched though.
&gt; I'd say that Java's checked exceptions was a terrible idea. I'd say it was a terrible implementation. I *consistently* want checked exceptions. However, Java's implementation (and syntax) doesn't, as you point out, have a way to abstract over what a function/method can throw, which make HOFs (and even callbacks) a pain in the neck -- code either has to declare too few exceptions, and wrap things in RuntimeException/Throwable or it have to declare too many exceptions and that infects every caller. In Haskell, we do have ways to abstract over those types so there's a least a hope that we can put together a good system. &gt; I believe that error handling should be simple as hell. I don't. There's a reason the industry refers to the no-errors case as the "happy path". Handling errors well is going to the hardest part of any engineering challenge, and I don't think software is any different here. I do always expect a checked system to be *more work* both to implement and use than an unchecked system, because it has to maintain more precise information, but I think the advantages are worth it. (And when I decide they aren't for some project, I'll just switch to unchecked exceptions without thinking twice; switching in that direction is almost trivial -- just throw away the extra information you have.) If you *just* want to fail safely in the software, you rarely need checked exceptions. Doing failure *prevention* in the case of errors ("automatic recovery" or "self-healing"), is generally easier the more information you have about the error -- that could come from checked exceptions or it could come from somewhere else, but it has to come from somewhere. I generally find the checked exceptions approach something I can rely on easier than the other approaches I've attempted, but I'm certainly open to whatever works.
That's old.reddit in practice, it's just not in the URL. I'm in the same situation re. RES. I'm holding out as long as I can.
Just skimmed quickly. Instead of git fetch &amp;&amp; git rebase origin/master you can use git pull --rebase, which afaik does exactly the same thing
Ah! URLs are interesting beasts. If your file is already prefixed with an ISO date, such as `2018-11-06-my-blog-post.md`, then it'll use that filename; if it's not, it will try to pull the date in from another means and then construct the filename. I'm doing a post in the next couple of weeks on generating your own slug from your blog's title in order to avoid confusion, and this same method would mean it can be extended to make it consistent however you like. On the resources front, there's really not much I could find. I had to dig through the source code to understand how these things worked, and that's why I'm wanting to write about it. I'll be sure to include any other tutorials/resources I can find on these subjects, as well.
You can: ‘Product (Opposite C) C’ would make perfect sense. I think it’s easier to combine type constructors than to combine type classes... However, I never did any of this stuff in Haskell. Also “being an opposite category” seems like a strange (but valid) property to satisfy. So encoding it as a class seems strange. But not every category is an opposite category. So there should be no subclass relation. 
GHCi session: Prelude&gt; data Token Prelude&gt; let f = undefined :: String -&gt; Either String [Token] Prelude&gt; data N Prelude&gt; let g = undefined :: [Token] -. Either String N Prelude&gt; import Control.Monad Prelude Control.Monad&gt; :t (&gt;=&gt;) (&gt;=&gt;) :: Monad m =&gt; (a -&gt; m b) -&gt; (b -&gt; m c) -&gt; a -&gt; m c Prelude Control.Monad&gt; :t f &gt;=&gt; g f &gt;=&gt; g :: String -&gt; Either String N You just need the Kleisli fish from `Control.Monad`, in base. Honestly, I think the Kleisli fish is a better way to present the monad laws than either `join` or `(&gt;&gt;=)` (bind). It make them look nearly identical to the monoid laws. Prelude Control.Monad&gt; let f = const $ Left "Fail!" Prelude Control.Monad&gt; either id (const "N") $ (f &gt;=&gt; g) "Test" "Fail!" 
?
I meant don't switch to using ``` this kind of preformatted block ``` Feel free to use new design or old design as you prefer.
Would constraining the effects in \`EventHandlers\` buy back some of the trade-offs made in your conclusion?
&gt; There's also an interesting analogy between what is possible with a context free grammar and a context sensitive grammar, which makes pretty closely to the distinction between Applicative and Monad (IIRC). This makes sense to me, although I do remember /u/edwardkmett mentioning that while this "sounds" true it's not "actually" true and that, in practice, CFGs can be parsed with Applicatives. (I also vaguely remember him mentioning infinite parse trees of finite depth but it was a while ago on the IRC and I was tired, so take a few grains of salt with this)
Nice article, but I have the opposite opinion of rebase/merge. When I'm working on a feature branch with many commits, rebasing it on top of master is so tedious, because I have to replay and fix up commits individually (possibly needlessly resolving issues that I know are going to disappear in a later commit). It's much easier for me to just merge master into my topic branch. Yeah, I'll get a merge bubble going both ways when my topic branch gets merged back into master, but I don't care much about having a nice, linear history.
David Roundy switched from darcs to git. That was it for me.
I think in theory yes, but in practice I'm not sure how it would work. It is hard to know in advance how users would like to instrument your library. They might want to use dtrace probes or whatever. An `IO ()` or similarly is going to be the most general way.
&gt; It's much easier for me to just merge master into my topic branch. The [maintainer of Git recommends against this](http://kentnguyen.com/development/visualized-git-practices-for-team/#comment-423841808) (the comment, not the article), in general. It sounds to me like you need to be squashing (perhaps [autosquashing](https://robots.thoughtbot.com/autosquashing-git-commits) with --fixup and the like) on your branch before you attempt to rebase anywhere else. Making sure rerere is on will also help. I do [care about history](https://about.gitlab.com/2018/06/07/keeping-git-commit-history-clean/), because I'll use things like bisect to find the origin of defects and generate security updates for older releases quickly and other things.
It's not even enumerability so much as varianc, aka functoriality (and it's not about freeness, either). You can't map over the contravariant Bag. Although it is a functor in the opposite category, and you can co-map over it :) Both (a → N) Bags and functions themselves are codata, yes. But e.g. (N → a) is a functor in a. So it's not function types or codata *in themselves* that are badly behaved, it's a matter of variance. Any type with a nice Taylor expansion must be functorial (covariant) (although that's not by itself a sufficient condition)
If you used your `Ord` instance in a map, then the map might drop elements you didn't want to drop, or otherwise act nondeterministically based on the order in which you added to it, which would be bad. For checking the equality of foldable elements, another (safe) approach is to lift things into a monoid, then just `fold` -- or in this case, to just branch on if there's at least one element, and if so, check if all the other elements are equal to it. Much simpler too!
Hey! I wrote that code. There is a problem with that code it is actually unusable. You can’t have levity polymorphic kinds in the negative position so box is useless. There are other trac issues to relax this restriction.
&gt; recursion-schemes is also Template Haskell Very little of recursion-schemes uses Template Haskell. I only contributed a little to it, but I know my part doesn't use Template Haskell at all. (I tend to avoid Template Haskell; it's not as typed as I would like it, it destroys compile time and makes cross compiling difficult, and while I'm sure there are cases out there, I never found something I couldn't cover with CPP or Generics.)
finite width is the concern. You can parse any context sensitive grammars over a finite alphabet with an Applicative. This was something Nils Anders Danielsson pointed out to me, and I vaguely recall it is was also the basis of his total parser combinators: http://www.cse.chalmers.se/~nad/publications/danielsson-parser-combinators.html To need the power of monad you need to be doing something pretty artificial like taking parsers that run over lists of functions or something you can't readily count and so your tree would have to have infinite width to try them all. If Applicative was limited to finite depth, e.g. because you wrote the type signature for it in a strict language without laziness or something, then you'd have the easy correspondence between CFGs/Alternatives and CSGs/MonadPlus.
Thank you for the information, I'm aware that merging negatively impacts `git bisect`, but that "Git is inconsistent" link is horrifying... I'll also point out that does squashing an entire feature branch down just so rebasing it onto master is less work _also_ makes `git bisect` less useful ("Your bug is... somewhere in this 3000 line diff!").
Slight correction: getByID :: Integral a =&gt; [a] -&gt; [b] -&gt; [b] getByID xs ys = map ((ys !!) . fromIntegral) xs
Yeah, but even if the commit it huge it gives me a point in time to know which releases need a patch (once we find a fix), which can be especially important for security issues. I've certainly gone over a 50k diff (multiple commits), to try to eliminate a code change as the source of our bug. (And, yes, it did turn out to be somewhere else.) So, a 3k "the bug is definitely in this commit" is really not so bad. Eliminating 97% of a 100k codebase is not always easy. Finally, I didn't say you *had* to squash it down to a single commit. You mentioned rebase asking you to fix up a conflict in an early commit, that you know would be fixed by a later commit. That's a good reason to combine two two commits, specifically; not necessarily the whole branch. The worst is the git-can't-be-made-consistent reply. Branch+cross merge+final merge = zombie code none asked for.
As a long time pytorch user and Haskell programmer I really want this to be great! But if I'm looking at the API currently it seems like Haskell's strong typing is really doing nothing but making a lot of things more inconvenient. For example \`unsafeMatrix\` - is there a reason this needs to be in \`IO\`?
I actually used it for this several years ago. Needless to say it was not a very satisfactory solution!
Thanks, that's perfect! Followup question - is this a standard way of constructing Haskell programs?
Thanks, that works perfectly!
This seems to generalize to _any_ kind of settings-like value, and I agree with the pattern. One recommendation from my own experience (which others will disagree with): don't expose the data constructors and fields, as it will make calling code unnecessarily brittle to changes to the data type. You can take the `Monoid`al approach you have right now and provide functions like: connecting :: IO () -&gt; EventHandlers But done naively, this could end up with a bunch of unnecessary `pure () *&gt;` calls. Instead, I'd probably go for helper functions like: addConnecting :: IO () -&gt; EventHandlers -&gt; EventHandlers addConnection callback eh = eh { _connecting = callback *&gt; _connecting eh } Then you could either still provide a `Monoid` instance for `EventHandlers`, or provide a `defaultEventHandlers` or `emptyEventHandlers` monomorphic value.
Non-technical question: Is the name derived from the Warhammer 40K "Waagh" ?
Partially, yes. :) 
I've heard of it, but I don't have a compelling reason to switch from git.
I frequently use the :reload command in GHCi. I don't know if it was something that I did, but now the reload command loads all the dependencies even if I made no changes whatsoever. It used to reload only the changed modules. Does anyone have any idea how to resolve this? I tried Googling this but couldn't find any help.
you might need to `lift` one "effect" (monadic expression) into another. `ReaderT ...` is a monad-transformer, while `MonadReader` is a typeclass (the corresponding one). assuming `HandlerFor ...` is an instance of `MonadReader`, you would `lift` the "concrete" `ReaderT ... a` into an "abstract" `(MonadReader m) =&gt; m a`, which then unifies with `HandlerFor ... a`. (this may or may not be the issue)
you might need to `lift` one "effect" (monadic expression) into another. `ReaderT ...` is a monad-transformer, while `MonadReader` is a typeclass (the corresponding one). assuming `HandlerFor ...` is an instance of `MonadReader`, you would `lift` the "concrete" `ReaderT ... a` into an "abstract" `(MonadReader m) =&gt; m a`, which then unifies with `HandlerFor ... a`. (this may or may not be the issue)
I'm a happy darcs and [hub.darcs.net](https://hub.darcs.net) user. I hope it stays online. If Simon decides he needs some financial support to keep it running, I'd happily pitch in.
I'm afraid there's a terminology issue here. When I say "lambda expression" I mean any well-formed term in a (possibly extended) lambda calculus, which may of course include primitive terms (like the constructors of an inductive type) that are not lambda abstractions. A lambda abstraction is just one way to construct a lambda expression.
Am I the only one who thinks that "3. Examples of Yoneda" is a strange section title? I mean, Yoneda is a person, not some kind of tool. Fun fact: Yoneda also worked on the programming language Algol N.
Thinking about it, I think I'm wrong. The rewrite rule is not on `fmap` but on the `build` / `foldr` which are under.
Which we can probably agree, happens far more often than we'd like... Even so, this sort of thing can be quite nice to have if you're building interactive tools, or when you're debugging, building... it's a side-effect (geddit!?) of the 'round-trip' property, which has been so very helpful in ensuring this library works as desired. :)
Needs a companion article: _why_ do we needa know about Yoneda
I've used Hasql (a PostgreSQL library) a lot and they have a similar approach with encoders and decoders in that they don't use type classes either. One thing I found curious was that you used Applicative for encoders instead of contravariant functors like Hasql did.
The `Encoder` structure is an instance of `Contravariant` so you can still have those nice things. However the Encoders don't explicitly require the use of the Contravariant constraint, only Applicative. :) if that makes sense. 
Wonderful!.
anecdote: Erik Meijer Haskell Mooc used Hugs instead of GHC, which was somehow fun. Happy to run gofer :)
The JavaScript Promise object, as a solution to callback hell, was always the example that made Monads finally 'click' for me. It's funny to see that Haskell's IO was initially also callback based :)
it would be very useful with some non-scientific results in the readme.
I followed this: https://www.rohanjain.in/hakyll-clean-urls/
&gt; When I say "lambda expression" I mean any well-formed term in a (possibly extended) lambda calculus That may have contributed to the confusion. Since actual lambda abtraction *are* all (single-argument) functions. The various primitive terms are NOT functions, even if they are present in a (extended) lambda calculus.
`parse . print = id` should be true most parser printer combinations. You also want `print . parse = id`.
What do you want to do for `"test\"foo"`? What do you want to do for `"test\" with space \"foo"`? I don't think you are going to find a single call, but you can write a really small Parsec/Attoparsec/Megaparsec parser for this simply enough.
&gt; The Json data structure within Waargonaut keeps track of all the whitespace, as well as the presence or lack of trailing commas in objects and arrays. ‘Why bother?’ you ask, why store all of that extra information? The reason is for the combination of a parser and printer, there is a specific property that when proven to hold, provides immense guarantees about the accuracy and robustness of both functions. &gt; &gt; That property is called the ‘round trip’ property. You can have round tripping `parse . print == id` without preserving whitespace =). Did you mean to use `print . parse == id`? If that's the case, I'm not sure why the second equation should be preferred over the first... people can always use whatever fixed JSON formatter they want, if the output is intended to be human readable. I can understand why you might want to have whitespaces etc when parsing a PL (e.g. editor support), but it seems overkill for JSON.
I was expecting someone to point this out! I initially discussed this in the post but removed most of it because it warrants a much longer conversation (which I'm planning on adding to the followup). Apologies as I inundate you with these details. This release makes the torch bindings usable in Haskell, but development on Hasktorch is far from complete. Functions prefaced with an underscore, like `_marginCriterion_updateOutput`, are unprocessed calls to ATen which only unify the CPU and GPU interface. Almost all of these functions exist in the neural network library understanding these functions are non-trivial and coming up with a pure interface to them, like [`conv2d`](https://hackage.haskell.org/package/hasktorch-indef-0.0.1.0/docs/Torch-Indef-Static-NN-Conv2d.html#v:conv2d) is a considerable effort in getting the dimensions to line up correctly. This process involves reading ATen source (which is painless, but time-consuming), and mapping shape-checks into singletons. Leaving this up to a user to figure out dynamically will most likely result in segfaults, which is why the dynamic THNN functions will probably not get any love until a much later revision. Because of the speed of development in ATen, I was of the opinion that it would be better to give users full access to these raw functions and get folks to help out with this process as they create examples. As you are probably aware, mutation is preferred when running operations with large tensors and, to that end, all function in torch (except for those prefaced with `new`) mutate the first argument in place. Aside from the NN functions, every function in TH-proper (essentially anything not in the NN namespace), comes with two APIs: one pure function, where we construct the correct empty tensor for you, and one in-place version, where we duplicate the tensor reference you want to mutate to the first argument. You can see this in the source of [dynamic scalar multiplication](https://hackage.haskell.org/package/hasktorch-indef-0.0.1.0/docs/Torch-Indef-Dynamic-Tensor-Math-Pairwise.html#v:mul). You'll also notice that we stick to PyTorch's naming convention of the postfix \_ for in-place operators and that functions we can guarantee are pure will reflect this in their type (so no IO), this includes tensor math. Except for NN-function and `unsafeMatrix` (I'll address this in a minute), you can expect this to be the current lay of the land. Regarding IO: The next iteration of hasktorch should replace this with `ST` and possibly some fancier types for static, in-place changes (one example is that squeeze1d\_ will mutate the tensor in-place, but there is still be a reference to the original type-level dimensions in the first argument). IO is unavoidable at this stage because we are working in the FFI (which can only happen in IO), and because we need to start targeting libATen-1.0, which will require us to change how we generate the FFI (so that's a bit unstable, but backpack *should* save us here). Overall, at this stage, Hasktorch can be used with a moderate amount of pain to write full end-to-end examples without needing to worry about your program segfaulting because you've misused ATen. The convenience functions of `vector`, `matrix`, `cuboid`, `hyper`, and Static's `fromList` are still in IO primarily because there used to be a bug in the CPU-based construction and I have not tested these on my GPU yet. These functions are wrappers over `newWithStorage`, so that can be used as a workaround if you want to keep everything pure without deferring to `unsafePerformIO`. The noise in our API comes from the fact that our two-dev team has had to play catch up with the oldest deep learning library to date. At this point, the situation should improve, but we need more eyes than we currently have. I'm planning to write a followup to this post to go over architecture (ie, "we use backpack, backprop, and some simple codegen"), caveats (everything here), and a call for contributors. Torch bindings offer the most embedded integration into a host language out of all of the deep learning libraries, mostly because there is a bunch of work to do, we need more hands who are familiar with Haskell and PyTorch (like yourself!) or who are willing to dive deep into PyTorch internals.
so I'm pretty new to Haskell, but got something just now that seems to be working, however, I am not sure if there is a better way to do this: \`\`\` parseLongString :: String -&gt; \[Text\] parseLongString cmd = let regx = "\\"\[\^\\"\]+\\"|\[\^\\"\\\\S\]+" :: String firstSplit = getAllTextMatches $ cmd =\~ regx :: \[String\] in map T.pack (parseLongString' firstSplit) &amp;#x200B; parseLongString' :: \[String\] -&gt; \[String\] parseLongString' \[\] = \[\] parseLongString' (x : xs) = if take 1 x == "\\"" then filter (/='"') x : parseLongString' xs else words x ++ parseLongString' xs \`\`\`
How about something like Prelude.concat . Prelude.zipWith ($) (cycle [Data.Text.words, pure]) . Data.Text.splitOn (Data.Text.singleton '"')
This may serve your purpose short-term, but for the long term you will definitely want to use one of the Parsec libraries. You'll end up with a more robust, more efficient function that will gracefully handle unexpected input.
Hi, /u/WightKnight1 , here's a curated directory of "data science"-related libraries, including a couple SVM implementations : http://www.datahaskell.org/docs/community/current-environment.html. I'm afraid there's quite a bit of investigation to be done around at least some of them, but the more we are looking at this stuff, the better. Also come say hi on our chatroom: https://gitter.im/dataHaskell/Lobby 
What do you mean with non-scientific? 
Thank you I'll check out Parsec
Not sure [regular expressions are the right tool](https://blog.codinghorror.com/regular-expressions-now-you-have-two-problems/) here, but if it works, fine. You didn't answer my questions. Also, what do you expect for `"\\\"\"\\`? Also, is the any possible way for the output to be `[ "cmd\"with\"quotes", "arg\"with\"quotes" ]`?
This is exactly the kind of stuff I was looking for. Thank you!
Essentially I need to take a command like `service-name --arg argvalue --arg2 "second arg"` and split it into a list of elements `["service-name", "--arg", "argvalue", "--arg2", "second arg"]` so regarding your first question I wouldn't have a case with one `"` however if there were I would expect an error for your second question I think I would expect an error `"\\\"\"\\` but not entirely sure, this is my first project venturing into haskell
That was a good idea and got me thinking, but turns out it was much simpler--all I needed to do was runDB, so the line ended up being _ &lt;- runDB $ deleteWhere [TableTablecontents !=. "" ]
Great answer.
Well, "an error" isn't a `[Text]`, so you might want to change the type of the function you are producing. Instead of `String -&gt; [Text]` you might want `String -&gt; Either String [Text]` where a `Left str` indicates failure with a particular error message and `Right args` indicates success. --- Seems like you are trying to do [field splitting](http://pubs.opengroup.org/onlinepubs/9699919799/utilities/V3_chap02.html#tag_18_06_05) and [quote removal](http://pubs.opengroup.org/onlinepubs/9699919799/utilities/V3_chap02.html#tag_18_06_07), note that both these processes assume the input has already been [tokenized](http://pubs.opengroup.org/onlinepubs/9699919799/utilities/V3_chap02.html#tag_18_03) into words. So, you'll probably want to do that (tokenizing) first; you can probably simplify that process some, since you don't need to recognize shell operators, keywords, or command-substitution, but I would generally proceed down that path. If you've never written a lexer before, just try it directly or with one of the parsec-family libraries. If you have written a lexer before, (they are pretty boring) you might want to see if you can write the rules in alex or flex.
It uses [hw-json](https://github.com/haskell-works/hw-json) under the hood, so it sounds like it could go pretty fast. I don't know if there are published benchmarks yet.
I thought do-notation was only for bind. Does it infer how to combine the functions automatically?
That's in there too, there are tests that check that. :) 
&gt;- Never use `foldl`. Always use `foldl'` if I want a strict accumulation, or `foldr` if I want something that fuses well with a lazy producer. Amendment: Never use `foldl` on lists or other right-biased structures. If you have a reasonably symmetric strict structure, such as `Set`, `Map`, or `Seq`, then `foldl` and `foldr` work pretty much the same performance-wise.
Why do you think that not using typeclasses is going to be slow? Benchmarks? Also there is a benchmark package in the repo on github if you would like to try it out. :) My experience thus far is that speed is not an issue, courtesy of hw-json. 
Kleisli composition is just a special use case of bind. &amp;#x200B; \`\`\` combined :: String -&gt; Either String N combined x = do a &lt;- f x g a -- Note, your parent commenter had an erroneous \`return\` \`\`\` &amp;#x200B; Desugars to: &amp;#x200B; \`\`\` combined x = f x &gt;&gt;= \\a -&gt; g a \`\`\` &amp;#x200B; Kleisli composition is defined as: &amp;#x200B; \`\`\` k &lt;=&lt; h = \\x -&gt; h x &gt;&gt;= k \`\`\` &amp;#x200B; So the kleisli definition of combined turns into: &amp;#x200B; \`\`\` combined = g &lt;=&lt; f \-- Inlining &lt;=&lt; you get: combined = \\x -&gt; f x &gt;&gt;= g \`\`\` So you can see that, short of the redundant lambda with the \`a\` argument in the do notation desugarring, this is equivalent. 
There are types with no sensible `return` function that have both `fmap` and `join`. `Map` is an example. The canonical typeclass for these types is [`Bind`](https://hackage.haskell.org/package/semigroupoids-5.3.1/docs/Data-Functor-Bind.html#t:Bind).
Um, there's some magic behind `ApplicativeDo`, but for the most part you can think of it as always using `(&gt;&gt;=)`. But, `(&gt;=&gt;)` and `(&gt;&gt;=)` are quite related: mergeA, mergeB :: (a -&gt; f b) -&gt; (b -&gt; f c) -&gt; a -&gt; f c mergeA f g x = (f &gt;=&gt; g) x mergeB f g x = f x &gt;&gt;= g In fact you can implement them in terms of one another: x &gt;&gt;= g = (g &gt;=&gt; const x) () f &gt;=&gt; g = (&gt;&gt;= g) . f If you wanted to use `do`-notation for your functions, you might so something like: f_then_g :: String -&gt; Either String N f_then_g str = do toks &lt;- f str g toks
You could have made the correspondence exact by defining the `Cardinality` typeclass like so: newtype Cardinal a = Cardinal BigInt deriving (Eq, Ord, Show) class Cardinality a where cardinality :: Cardinal a instance MultiplicativeSemigroupal Cardinal where product (Cardinal a) (Cardinal b) = Cardinal (a * b) instance MultiplicativeMonoidal Cardinal where one = Cardinal 1 instance AdditiveSemigroupal Cardinal where sum (Cardinal a) (Cardinal b) = Cardinal (a + b) instance AdditiveMonoidal Cardinal where zero = Cardinal 0 Then you get the tuple instances for free. Also, `Contrvariant` functors can be monoidal as well: class Contravariant f where contramap :: (a -&gt; b) -&gt; f b -&gt; f a Note the opposite direction as opposed to a normal `Functor`. It's inhabited with values like newtype Predicate a = Predicate (a -&gt; True) instance Contravariant Predicate where contramap f (Predicate p) = Predicate (p . f) And it's a semiring as well. instance MultiplicativeSemigroupal Predicate where product (Predicate pa) (Predicate pb) = Predicate (\(a, b) -&gt; pa a &amp;&amp; pb b) instance MultiplicativeMonoidal Predicate where one = Predicate (const True) instance AdditiveSemigroupal Predicate where sum (Predicate pa) (Predicate pb) = Predicate (either pa pb) instance AdditiveMonoidal Predicate where zero = Predicate absurd The [`Divisible` and `Decidable` typeclasses](https://hackage.haskell.org/package/contravariant-1.5/docs/Data-Functor-Contravariant-Divisible.html) demonstrate this semiring structure. If you really want a mind-blowing demonstration of this kind of structure, look at `(***)` in the `Arrow` typeclass and `(+++)` in the `ArrowChoice` typeclass.
Ahh derp, sorry I see what you're saying. I'll try to update the post today. :) Was maybe too excited to finally release it. &gt;&lt;"
I enjoyed the talk you gave at the Berlin Haskell meetup; very interesting to learn about potential and the challenges of using pandoc (and FOSS in general) on such a large scale. A structured write-up for the rest of the internet would be great, IMHO.
Yeah I snafu'd my property explanation slightly, lesson being don't publish whilst overly excited. &gt;&lt;" It may _seem_ like overkill, but Waargonaut isn't meant for a single use-case and I try to be as flexible as possible and permit all that I can. So for a day to day REST API throwing JSON all over the tubes, that preservation may not be interesting. But that's fine, you don't have to care! :) But someone might be interested in building a command line editing or schema migration tool. In which case the precise preservation of that information may well be very important. In which case they're able to let the library handle it all, instead of having to go through that pain themselves (some parts were super not fun). Or trying to hack it into Waargonaut, after the fact. With the added bonus that the properties make it suuuuuuuper difficult to write a busted printer/parser. Property-based testing is wild. :D 
Thanks for the update! That makes things much clearer.
MLTool looks very promising and up to date so I will add it to the DH directory, but sadly I can't say the same for `svm`; its author simply disappeared and stopped updating it 8 years ago (which, on the Haskell timescale, is a _long_ time). Like, there aren't even tests for svm. I would _really_ love to use it but it's simply too bare-bones.
The compile can't exploit the fact that the decoder/encoder will be statically known if you use ordinary functions. 
Thanks to both of you for the detailed feedback! Hopefully efforts like yours can be fertile grounds for future experimentation and I'll be definitely having a play around. It's something I've spent a lot of time on and achieved very little, so I've stuck with pytorch. Compared to torch it's ergonomics improved out of sight, it's definitely a model worth emulating if possible at all. I've little understanding how their AD system works, as far as I can tell it's basically similar to the 'Lens' based formulation which was posted a couple of weeks ago, with a pair of functions `(x -&gt; y, x -&gt; dy -&gt; dx),` what makes it really easy to use (and difficult to implement for a strongly typed pure language) is that basically all operations on tensors are overloaded to support AD as well. 
Having said that, the pytorch 1.0 API supports full tracing of expressions, (as well as simple python code, loops, conditionals etc.! ) So they must build a full syntax tree.
I should read about this period. Was there a time when GHC was only one player in the field ? is there any ~competition ?
Very cool! As someone who got their degree in electrical engineering but almost immediately switched over to pure software, I love reading about cool fusions of the two. To the author: were they already using Haskell or did you introduce it for this project? If you introduced it, which language was it replacing, or which are normally used for these tasks? I feel like Haskell is a perfect fit for something like this (high level enough to cleanly describe algorithms, great at parallelization, good performance, good variety of helping libraries) but imagine that it's fairly rarely actually used.
And once you've verified your (potentially commutative) monoid, you can automatically solve equations about it: https://github.com/jdevuyst/rekenaar
I had not heard of \`flat\`; it's very impressive that it excels at both size and speed. While the two are often positive correlated, I would have had some concern that the tight packing would induce overhead for being unnaturally aligned, but it seems that the authors have balanced these concerns very nicely.
You can make things a little more similar by rearranging some things: appendAssoc :: Sing '(a, b, c :: m) -&gt; mappend a (mappend b c) :~: mappend (mappend a b) c
The presence of type classes or not doesn't really affect compiler performance. The "statically known" property you're referring to can only eliminate indirections when functions are inlined, which happens regardless of whether or not type classes are used. In fact, passing ordinary functions is exactly the same as type classes after the type checker has run.
Links ? Video ?
The 2000's had nhc, uhc, jhc, and of course hugs, but ghc was always the big fish. Nowadays, ghc competes with other haskell-inspired languages close to its space (Elm, Purescript, Idris, Frege) rather than with other haskell implementations.
Well, at the time when ghc used hbc for bootstrapping it certainly wasn’t the biggest fish. :)
First, `main` was never `String-&gt;String`. Second, `main` was `[Response]-&gt;[Request]` until it became monadic. 
You've had a lot more experience maintaining software that with a huge amount of dependencies so I'll trust your take on this. 
Yeah good points.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [etorreborre/registry/.../**motivation.md#encoders** (master → 0c67773)](https://github.com/etorreborre/registry/blob/0c67773710fe2e260eee02fdc292acaca390b2aa/doc/motivation.md#encoders) ---- 
I guess he wants a plain bar chart comparison.
Sorry, the documentation is quite sparse at the moment. The best resource is probably [the wiki](https://github.com/WebGHC/wasm-cross/wiki). Like I said, it's *technically* usable today, but I wouldn't recommend it. We've got quite a ways to go before its production ready, and WebGL hasn't even entered into consideration yet.
Thanks again! This really helped clarify things for me. I've got a few functions like this, and with your help I could write them in all three ways, and I think I understand it a lot better now.
The domain area is way out of my comfort zone, but a really nice post. I especially love the good use case for `parallel-strategies` and `accelerate`.
In fact, Parallel strategies are a very good idea and we should be using more of it.
I disagree. It's a really difficult library to use effectively. For me I usually gain a negative gain from parallel strategies, radically increasing the runtime. It's definitely because of my inexperience, but this also leads me to believe the library is finicky to use.
As a person who cares about lightweight dependencies for libraries, I want to say thank you for not depending on 100500 packages like `aeson`, various serialisation libraries and others! I understand that situation with orphan instances might be difficult but I appreciate what you didn't dump heavy dependencies on users who just want only strict tuples 👍
&gt; The various primitive terms are NOT functions, even if they are present in a (extended) lambda calculus. I'm not sure who you are talking to, since I never claimed they are. I'm not even sure why I am being downvoted, since you can find my definition verbatim in any textbook on the topic.
&gt; I haven't been able to find a package that offers large strict tuple types TBH, good! I don't think tuples beyond 3 elements are a good idea. I almost always prefer the use of a record data type. I just wish it were easier to have anonymous records. Anyway, I should "almost" above which I suppose doesn't rule out the need for this library :)
Yes it does affect performance, the optimiser treats type class methods/dictionaries differently to normal functions and records. 
Sorry, we did not record any video, it was a rather informal event. If you are curious, you can read the [slide contents](https://github.com/danse/rotterdam/blob/master/espo/espo.md). That document is just an accessible side product of writing the slides, not meant to be read as is and probably impossible to fully understand. Anyway it can give you an idea about the topics covered
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [danse/rotterdam/.../**espo.md** (master → f3f6bb6)](https://github.com/danse/rotterdam/blob/f3f6bb649fc743d0a1b35a1fc3d95f02a3f63fca/espo/espo.md) ---- 
Even if you decide to use the typeclasses in Waargonaut, you have much more flexibility available to you. Check out the https://hackage.haskell.org/package/waargonaut-0.2.0.0/docs/Waargonaut-Generic.html module for more info. 
Thanks for the suggestion! Will try to use it, but in that module I also widely use laziness. Will think :-) 
I don't think it's as much due to the library and its API(which is pretty clean and simple IMO) as it is due to the fact that parallelization is hard. It's very easy to split work into chunks that are too small or too large thus either dwarfing efficiency with overhead or not getting enough efficiency. Parallel programming is hard.
This is true, however the functions are statically known even without the type classes involved. If you do the dictionary transformation yourself, and add the appropriate inline pragmas, there is no difference in the generated core.
No it doesn't, seeing as once you hit core the dictionaries \_are\_ just values. That said, the values are more likely to be inlined (among other things), but you can achieve the same result by marking your own values with pragmas.
Was there a problem with conduct before?
I wonder what `11. Chastise the body` means, in the context of databases.
The [communication guidelines draft](https://github.com/snoyberg/stack-coc/blob/b9324c83093715c415c047b2170bc86fe2b49d1d/COMMUNICATIONS.md) does point out much of what I find objectionable the flamewar-oriented single purpose accounts that operate here on r/haskell (the ones who take every opportunity, no matter how tangential, to rehash old arguments and make insinuations about people with opposing viewpoints). 
Enforcing religious rituals is usually the best way to make sure everyone feels welcome in 2018 indeed.
Have you flagellated yourself today?
Maybe in the context of SQLite it refers to the fact that SQLite is untyped?
I recommend [A Sermon on Ethics and Love](http://principiadiscordia.com/book/45.php) from the Principia Discordia as a code of conduct.
No idea why they're being so trollish and/or obtuse, but it *could* have something to do with them never[1] accepting outside contributions. In that case they don't really have to care about pissing anyone else off or the loss of potential contributors. [1] Or maybe just very, very rarely?
&gt; you can find my definition verbatim in any textbook on the topic I'm not sure that's true. I've read several, and still when I saw the term "lambda expression", I immediately jumped to "lambda abstraction (expression)". In any case, it's not the normal use of the term. Lambda expression refers exclusively to anonymous function expressions one the first page of a Google search or a DuckDuckGo search. You may also be getting downvoted, because the way you've used the terminology is confusing the reason this particular thread on conversation started: &gt; Haskell is a lambda calculus basically. [...] &gt; This tells us one important lesson, that everything is made out of functions in Haskell I am trying to correct this misunderstanding of "everything is a function in Haskell" that starts from "Haskell is a lambda calculus". You came back with: "Haskell is a lambda calculus so everything is a lambda expression", and are now claiming that when you said "lambda expression", you didn't mean "function", you just meant "an expression in any lambda calculus". Either you first comment it the thread was derailing the conversation (and you deserve downvotes on that comment) or you've changed what you meant by "lambda expression" to "save face" (and you deserve downvotes on that later comment).
Is there any legal precedent for this? Has it ever happened that a contributor was banned from a project for violating the CoC and then that person retaliated by withdrawing his/her contributions from the project?
That's some more flexibility indeed. If you introduce your own tag because you want to redefine one of the encoders, do you have to define instances for all the others, with that new tag, as well? If that's the case a `Registry` will be more practical because you just add your modifier encoder to the registry without changing anything else.
I problem with that is that some of the worst actors don't care that they insult, demean, offend, trigger, or even traumatize others if they reach goal X. X might be write the best kernel, or X might be troll a community or sub-community they perceive as powerful. So, the point in the story where "But nobody wants it! Everybody hates it." isn't true in general. That *why* put together these documents that try and establish more restrictions on behavior than the Principia Discordia. If all persons were angels, we wouldn't need governance; but unfortunately, there are some devils (or at least people that like to wear devil's masks for a time) out there.
The closest I can think of is Joerg Schilling trying to prevent Debian from distributing an older version of cdrtools -- or something like that.
&gt; the point in the story where "But nobody wants it! Everybody hates it." isn't true in general The point of the story is that it's not true in general either, but many people who (unknowingly) participate in perpetuating that kind of behaviour don't necessarily think about what impact their little contribution is having. Policing should be based on common sense, not a rigid set of rules. If I am discriminated against on a project using a very strict code of conduct, but on the basis of something that is not in the explicit list of non-discrimination-factors listed in the CoC - is that suddenly okay? No, of course not, and policing will be done anyways. Instead of listing things that you *shouldn't* do, tell people what they *should* do and help them along the way. It should be about community, not control. I quite liked Richard Stallman's [announcement e-mail](https://lists.gnu.org/archive/html/info-gnu/2018-10/msg00001.html) for the GNU Kind Communications Guidelines. He makes this point much better than I could in this comment here :)
Dear Mr. Snoyman, I am not sure where best to join in the discussion, but since you posted this here, I guess here's as good a place as any. I am also not sure exactly how to say what I want to say without coming off as unsubtle, and in all honesty, I guess that is perhaps the point I am trying to make, as circular as that sounds. I think my point is simple, but because I have seen these types of discussions devolve before, and because I have seen people get smeared and disparaged, I am afraid to speak up. I am afraid to speak up against a CoC, because I think I will get called a racist, sexist, transphobe and be subsequently ostracized. I think that is the point. Incoorporating a CoC, with the intent of fostering an inclusive atmosphere, will automatically exclude people as well. You intend to make people more comfortable, but I find it hard to express myself subtly even in face-to-face communication. There, I can use the tone of my voice and smiles to soften the impact of my clumsy nature, but those are tools that I lack in written form. So when the blog-post you put up contains the following paragraph of "What to avoid": Avoid offensive language. This is actually more complicated than it seems; one of the ways I’ve upset people is my highly sarcastic-by-nature communication style. I’ve worked hard on tamping that down. I point this out because what’s offensive to one person may be normal to others. This is especially true in a global community. Then I start getting seriously anxious. I do not know how not to offend people. Some people are very easily offended. Some people go out of their way to be offended because that allows them a modicum of power. Doing certain things offends some people, but not doing exactly the same thing offends others. In the end, offense is taken, not given. By putting the onus of offense on the 'offense giver', you have just created an environment where 'taking offense' has been incentivized. I really wish people would focus more on developing a thicker skin, and less on making sure nothing offensive occurs in the world out there. "The princess and the pea"-effect is a very real thing. If you try to get rid of offensive input, then you will end up getting offended by smaller and smaller issues. Now, that is not to say that certain people do not go too far, are actively abrasive, and go out of their way to be dickish, but I do not see why we can't deal with that on a case by case basis. If someone is being a deeply unpleasant person, then that can be addressed. Why the need for the CoC? Sorry if this is unclear or a bit stunted, I just really like Haskell, and I really don't like the culture of oversensitivity that I've seen taking root, and seeing one encroach on the other makes me a bit nervous. Like I said, I'm much better at explaining myself in person, so if you would like to discuss this, drop me a PM and we can set up a skype/discord/hangouts call (not expecting this to happen, just letting you know I'm open to it.).
Note that I suggested `StrictData`, not `Strict`. The one I suggested is equivalent to putting bangs on all data definitions in the module, but no more. You can still make a lazy field by explicitly marking it with a tilde (`~`) where you would have put a bang, and you can always factor the data into another module. You can also get a readability bonus if you drop the `UNPACK` pragmas, since they aren't necessary for `Int`s and `Double`s (though check the core and your benchmarks to be sure).
Perhaps a minor point, but many test suites would create circular dependencies if merged with the library they are testing. For example, some of the build dependencies of hspec have hspec based tests.
Yes. There are always things to balance in life. Offensive people should introspect about pain they cause others. Sensitive people should introspect about whether the world is really so hostile. **Neither process should be enforced by "the system".** tldr; Everyone should calm down and get back to writing Haskell, the one true panacea.
This made me SO happy today! Thank you for posting this :D
I oppose CoCs, too, because I believe people are reasonable and a project should be welcoming anyone who contributes and disregard private behavior and lifestyle. A project advances from contributions and not from its members being pals. I left a project where I contributed for over 15 years over a CoC recently. One-sided agendas are an assault on any tolerant person. You want tolerance? Show your tolerance first! And most importantly, be reasonable.
I agree with your objection, and agree that the fear-of-speaking-up is real. The anti- side is often only publicly represented by its loudest and brashest members for exactly the reasons you state; this skews public perception of how well-supported these measures actually are. I think there needs to be a kind of Postel's Law for offense, because non-offensiveness doesn't scale. The more people you address, the more likely it is that someone takes offense to a statement that was made without malicious intent.
There's limits to that. IANAL, but §42(3) mentions that the author has to reimburse licensors for damages - no one wants to bother with that. Just saying "nuh-uh, it's mine" won't cut it, you'll have to reimburse those who license it, i.e. the collective of authors in case of a Contributor's License or the end user in the case of any other license. What's more, without a contributor's license, http://www.gesetze-im-internet.de/urhg/__8.html would like to have a word, as you'd then (definitely) be a co-author: &gt; "Ein Miturheber darf jedoch seine Einwilligung zur Veröffentlichung, Verwertung oder Änderung nicht wider Treu und Glauben verweigern." Roughly: &gt; A Co-author may not in bad faith deny his consent to publishing, use or editing. So, that's another block in the road of trolling the project. Even more so if you then publish a fork that complies with your ideological vision. And I'm not sure either whether that particular paragraph (§8(2)) impacts the rights of a co-author to retract a license, i.e. go from GPL to all rights reserved; not sure how "die gesamte Hand" works here. Retracting your consent to the open source license of a contribution you made to a work under that license - that sounds like bad faith to me. You knew when writing the code that it would be used widely and without your control. Good find though, wasn't aware of §42.
&gt; I don't think either process needs to be enforced by "the system". Agreed, people need to fix their own issues.
[removed]
I certainly prefer your interpretation, but since stack is (mostly, in not entirely) an FPComplete project, it might benefit them to pass the idea through their legal council who (presumably) are lawyers. The only reason I'm even tangentially aware of this (having only visited Germany once, for a *whole week*) is due to the Joerg Schilling claims. I think in that case, he had a much better claim to being an author (rather than a co-author) than J. Random Troll that contributes to your project well after it is established and mature (like stack); he had written the *vast* majority of the cdrtools code himself and was certainly the original author and maintainer of the project. It's something to think about, anyway. I honestly wish we didn't need CoCs (and especially not CLAs) everywhere, but it seems like there are enough bad act(or)s that something is desired and I think CoCs have so far been more good than bad in practice.
Really nice work so far! 👏 Thanks for playing. I think the approach of breaking the problem down into a list to simplify the permutations algorithm and then converting it back to a list-shaped tree is the best way. 👍 There's a small hiccup with your algorithm. I've pasted [a gist of your code with `Show` constraints added](https://gist.github.com/chrisdone/5bb3099c2e63979858c1fe85c366ab18), so that I could share this result with you: &gt; mapM_ print $ permT $ Ap (+) (Ap (+) (OneT 1) (OneT 2)) (OneT 3) (1,(2,3)) (1,(1,3)) (1,(3,1)) (1,(3,2)) (1,(1,2)) (1,(2,1)) There are currently duplicates (1,1,3) (which is not ideal, but that's easily fixable), but the first element in the tree is always `1`. I'll award reddit gold for the fixed version! 
Studies show that attacking people's beliefs is the worst kind of approach you can make towards other cultures. It's not very surprising that many countries have capital punishment for these kinds of offenses. Religion, politics and CoCs are based on beliefs, one-sided beliefs. An individual person is the sum of their experiences and all these things that try to describe a common idea will exclude people who have (maybe an interesting) alternative idea. The only correct way to approach people with different views is to tolerate them. This is the least aggressive approach. Forcing them to conform is the most aggressive approach. The SQLite CoC is particularly funny. And while being funny, it is also a philosophical punch in the face. It shows you a common ideology that the SQLite devs seemingly agreed on, with a high probability that *you* don't want to agree with it. It's sarcastic. It has the intention to show you how it is like when you don't feel well within a software project anymore. The second thing is that the SQLite CoC is a joke. And at the same time, it exposes all the people who don't understand jokes as jokes anymore. This sarcasm should should also show you that they are tolerant. They don't look at who you are and what you believe. You won't be judged according to any made-up standards.
I agreed with this point of view until somewhat recently. The things that informed my change of perspective were [The Tyranny Of Structurelessness](https://en.m.wikipedia.org/wiki/The_Tyranny_of_Structurelessness) essay, and the extreme success and cohesiveness of the Rust programming language, which enforces a Code of Conduct upon contributors and people engaging in the community. The tl;dr is that I’ve come to believe that any sufficiently large and disparate community will inevitably develop informally specified power hierarchies if no formal specification or structure exists to impose them from the beginning. This is, by default, non-inclusive to members of the community who don’t actively engage in the social aspect of power aggregation within a community, and it means that the rules by which peoples’ conduct is governed is not explicitly spelled out, in the open, for everyone to understand and abide by. In the face of a choice between informally specified structure and rules of enforcement and formally specified one’s, I’ll almost always prefer the formal set (given that they’re specified reasonably and in an agreeable fashion to most of the affected community).
&gt; I do not know how not to offend people. For me, don't ever question one of my technical or stylistic decisions. I'm offended by any resistance to the code I develop via devilish revelation. ;) --- In all seriousness, "don't offend people" is probably an impossible goal. Some people might be offended (or at least claim to be) by objective facts!
&gt; Religion, politics and CoCs are based on beliefs, one-sided beliefs. It seems that the one taking CoCs too seriously might be you. If a stipulation of basic ground rules for interaction in/around a project gets lumped with religion and politics then I have a hard time believing you're arguing in good faith. &gt; The second thing is that the SQLite CoC is a joke. Of course it might well, be but when communicating on the internet one should always beware Poe's Law. I'll leave it at that.
Many people understand criticism as offense. This is not what I believe. Many projects without CoCs tell users that criticism should not be taken the wrong way. And there are people who are furious about this sole statement.
[removed]
Well, in that case, it's not so much a matter of interpretation, but more facts. While by the letter of the law, he'd still be a co-author, co-authorship is weighed by amount of contribution, so Schilling would have wide latitude and his claims would be at least credible. FWIW, I only head shouting about how bad CoCs are anytime anyone mentions one; but I've yet to hear substantial harm coming from one, while I do believe they have the capacity to bring substantial good to those protected by it. Naturally, the latter is something you wouldn't expect to be well documented. Number of *phobic attacks prevented is not something easily trackable; number of regular contributors alienated through application of a CoC is much more trackable. (Note I say application. People who run once you mention or implement one are intentionally disregarded because that's just kinda hysterical.) So, in case I ever am confronted with the option of implementing a CoC on my project, I'll expect the con side to bring some hefty arguments about the harms of CoCs.
Edited the solution above. I admit I hadn't run any tests before posting the previous version.
Nice! Not that bad at all. Of course, there's still all of the Sing noise but that's understandable since we don't truly have dependent types (yet).
My use case at the moment is handling database query rows with Hasql. Rather than make a one-off record to give a name to a `SELECT foo, bar, baz`, I'm just using `Vector (Foo, Bar, Baz)`. But yes. Big tuples bad.
The CoCov is adopted enormously widely at this point, as are a few other CoCs. I think all the legal arguments about potential problems are pure FUD.
I would recommend not going to Bloomberg unless you want to be viewed only as a resource and nothing more.
I have seen people being forcibly excluded and also leaving voluntarily because of agenda-laden CoCs. It makes me sad, especially when these people invest much time into the project. I am simply one of these guys who does not like risks. And being a part of a project where there is a small risk affecting private life and my profession is unacceptable. I try to avoid people with a victim mentality, because they are the most dangerous to smear other people. I tolerate them, but I avoid interacting with them. This is not only my believe, but also experience. And I find it's simple and reasonable. I am sorry if you think I am arguing in bad faith. You should know I am a peaceful, tolerant and nice person. At least I try as much as I can. 
Unfortunately this is precisely the usage that I don't really agree with. It quickly becomes pervasive. These are things that can be clearly named, and imo it's worth paying the tiny extra cost of writing out a data type and being explicit.
Looks good! &gt; sort (map (\x -&gt; "["++show x++"]") (permT $ Ap (+) (Ap (+) (OneT 1) (OneT 2)) (Ap (+) (OneT 3) (OneT 4)))) == sort (map show (Data.List.permutations [1..4])) True
While it has been adopted by quite a few projects, including some large ones, the document effectively didn't exist before 2014. I'm not sure there's been enough time to judge some of the legal threats. I think they are mostly overblown though. People that aren't willing to participate under the terms of a CoC are *probably* not the people you want to be working with, even if they are technically brilliant. Removing someone's contributions from an existing codebase can be expensive, but if the alternative is to let project policy be dictated by anyone that has more than a trivial amount of copyrightable material in the project is a path to madness.
The thing is that there's no special teeth to the CoCov outside of it being a set of guidelines by which contributors can be removed from projects, among other things. So the FUD is really about some magic "taking my contributions and going home" exception to free software licenses that would let you just "take them back." And of course there's no such exception -- that's been well tested before any sort of CoCs ever existed and it it was exactly the nonsense peddled back in the day when people were warning about free software licenses at \_all\_.
 It’s my perspective that people become incapable of “solving their own problems” when the avenues for presenting and addressing those problems in constructive settings aren’t clearly defined. I'd like to add a bit of nuance. Not having a CoC might (or might not) lead to unclearly defined ways of solving problems. Many communities have very clear, but implicit ways of addressing problems. Having an ambiguous CoC with vague language, however, is by definition an unclearly defined way of solving problem. Just having a CoC will not help clarify communication if the CoC is vague, ambiguous, or politically loaded. Additionally, you bring up Rust as proof that CoC's help grow communities, but I am not sure that you can make this causal connection. Rust might just fulfill a particular need that made many people interested in contributing to it. Linux didn't have a CoC for ages and attracted a great deal of interest all throughout that time. 
Honest counter-proposal: hop off of the CoC-hype train, grow some thick skin.
That's a bummer. It was cool to see some ocaml related work come out of their groups in recent years.
Do you have some references for your statement that these threats are well-tested?
I love how clearly Professor Hutton is able to explain things when speaking! Yet, in his book "Programming in Haskell" that I bought some time ago, I find it somewhat too condensed, and not understanding a bit of a phrase makes me unable to understand the whole paragraph. So often I have to read it over and over. Anyone else had this issue?
I see a lot of negative comments here, and I just wanted to offer a positive one, and sincerely say thank you. I've been writing Haskell for 10 years now, and although I don't remember exactly when I adopted stack, it's been a while now. For as much as I really love the language, I've never been all that involved with the broader Haskell community. Pockets of it seem very friendly, but there's also a lot of it that feels very unfriendly for someone who isn't already deeply embedded in the community. Seeing projects adopt a CoC is one of those little things that makes me think a project might be worth spending some time trying to engage with and get involved with as more than just a silent anonymous user.
Hear that? It's the sound of the point rushing over your head. Any community that discusses the adoption of a CoC will immediately get a reply like this one. You're parroting the exact same thing that has been said by the handful of people who are offended that they are not welcome in a community of people who want to set some rules about conduct. You're boring. Codes of Conduct are meant to exclude people. They set rules and expectations. Found a rule that makes you feel not welcome? Good. It's working. If you don't agree with the terms of a CoC then don't participate in that community. Find another community. Start the Haskell Community for People Who Like to Make Insensitive Jokes and Put People Down. Nobody is stopping you. Go ahead. I'm sure you'll have great conferences and meet a lot of really fun people to be around and finally feel welcome.
It's great to see somebody from the Haskell community stepping up and reflect on his past behavior and try to fix the problem. Reading through the comments in this thread makes me wonder if he's not the only one to benefit from CoC. Why would anybody even oppose a CoC if they're conducting themselves in a proper way? 
Ironically enough, the tone of this comes across to me as insulting and slightly ad hominem. I take /u/leaf_cutter's comments at face value, and am happy to discuss the technical merits. Instead, you seem to be dismissing their concerns in an insulting fashion.
I recommend an "uncode" of civility, nothing more. By "uncode" I mean a vague statement of desirable behavior. Adding some power to a document will cause more harm than good IMO.
What a decent and thoughtful proposal. I have struggled with other people's account of the toxic behavior that permeates various Haskell communities. I've been disappointed to hear the opinions of other tech people I respect who see Haskell going the way of Common Lisp as a side effect of this loud and small group of people who insist that they should not be excluded for their poor behavior. It makes my life more difficult as a Haskell user who wants to include more people and increase Haskell's adoption in the work place when the poor behavior of a few people becomes the reputation of our community. So thank you for putting this out there. It gives me hope that we may one day shed such reputations. And hopefully more people will find contributing to Haskell projects easier and more pleasant than before.
I hate when people tell others what to do, especially in areas where you're supposed to exercise good critical thinking skills. You can't enforce good through sheer force, that just makes you a virtue signaling asshole. Just because you're nice doesn't mean you aren't also an asshole. You can be annoying by constantly trying to control people because you think it is the right thing to do, but controlling people just leads to chaos or a group of brainwashed individuals that can't think for themselves because they wait on the virtue signaling assholes to tell them how to behave. 
&gt; Maybe others would be offended by that, but it wouldn't bother me. And you sound like a very reasonable person. :-). I think the point is that the CoC would put you in a position to have to avoid language that *might* be offensive to someone, and since people are different and some are very easily offended, you couldn't point out "the bald guy wearing a kippa" because that *might* offend. If instead the focus was on reconciliation and healthy conflict resolution (instead of prevention) then that might be different. &gt; It's about whether the person with the power to abuse it can be trusted not to. Exactly. Without a CoC conflict resolution is a decentralized process where people try to get along. Occasionally there is a particularly problematic case and someone has to be brought in to pull rank. If you create a CoC and a group of people responsible for enforcement and 'offense prevention', that will inevitably bring in people who are attracted to that power. The famous adage about absolute power applies here. I hope you are not feeling too attacked here. I think you are handling this discussion admirably, and I am not sure we disagree on many points, actually.
I have been working through Benson Joeris' course and I find it outstanding. I just love how succinct each video is, and yet mentions possible pitfalls and nuances. Even the sequencing and organization of the course seems so natural, and each topic connects to the next one in a nice flow. I have been using 'Haskell from First Principles' and 'Real World Haskell' as supplements but when I want a quick refresher, it's the course videos that I reach for.
I’d encourage you to read Graydon’s post that I linked to. I have anecdotally found the Rust community to not suffer from nearly as many of the social disagreements and problems that I’ve perceived in Haskell, and I imagine that if you were to ask contributors form stereotypically disenfranchised or minoring groups, they would express similar statements. On your first point, though, I strongly disagree that implicit means of addressing problems are worse than explicit ones. Implicit standard should, by definition, are less clear than explicit standards, and are less obvious to both the person engaging in potentially problematic behavior as well as the person who is the target of that behavior. You end up in situations where people don’t understand why their behavior may be construed as problematic, or how they may take action against the problematic behavior of others. Further, implicit standards are subject to constant evolution as the community evolves without being explicitly recorded anywhere. What may have been acceptable behavior at an earlier point in time may evolve to no longer be acceptable without someone realizing. When things are explicitly expressed, we can point to times at which expectations have changed and explain the reasoning behind them. When things are implicitly expressed, the individual being corrected can 
&gt; Then I start getting seriously anxious. I do not know how not to offend people. This is a spot on, in my opinion. But I believe this doesn't cancel out the CoC, but can be viewed as an edit proposal. What's important is the intent of the author. If you sincerely didn't want to offend somebody – you should not be viewed as an immoral person, while you would also be able to tune your intuition if another person informs you about their feelings politely.The sincere intent is what's essential, not the outcome. There was a great discussion with many examples where the requirement of an outcome (instead of an intent) had seriously hurt people and led to their careers and lives going sideways https://samharris.org/podcasts/137-safe-space/
I think an important point on these CoC discussions is trying to grasp the volume of contributors and contributions lost up until now due to their absence. In particular we should be investigating if there are people driven out by their absence, were those people disproportionately from a particular demographic. A lot of the time these debates seem to be framed in terms of helping or hindering the people that already have a strong track record with a project. They also seem to assume that every accusation will have the alleged abuser broadcast and shunned, and the people enforcing the code will disregard all reasonable standards. I have family members with autism and also some with disorders that make it harder for them to read body language, so I understand how people can be unintentionally offensive. But there is an enormous gap between using an unintentionally abrasive term or savaging a particularly poorly written code patch on one side and making sexual comments or tossing obscenities at a person on the other. And I'm sure snoyberg is aware of those differences. There isn't one or two or five or fifty stories of people that left a technical community or started to get involved and then stopped because of abusive treatment. There are many thousands of public accounts. What possible benefit would there be to manufacture false accounts on this scale? Who would be behind it?
&gt; I am afraid to speak up. I am afraid to speak up against a CoC, because I think I will get called a racist, sexist, transphobe and be subsequently ostracized. The limits of one's freedom of speech are given by other's right to not be verbally assaulted, it's really that simple. 
Harsh but fair.
thanks so much for giving this review, trying to decide whether or not to get the pluralsight sub!
Then they fail at their job. 
If people could just fix their issues on demand, there wouldn't be calls for a CoC in the first place.
&gt; I’d encourage you to read Graydon’s post that I linked to. I have anecdotally &gt; found the Rust community to not suffer from nearly as many of the social &gt; disagreements and problems that I’ve perceived in Haskell, and I imagine that if &gt; you were to ask contributors form stereotypically disenfranchised or minoring &gt; groups, they would express similar statements. I have taken your encouragement and read the post, the Rust CoC, and the Citizen CoC that they link to in their description of 'harrassment'. Let me start by saying that my experience with toxic online communities is very, very small. I have participated in a few open source projects, made some contributions here and there, and have found nearly everyone to be very welcoming and helpful. That is not to say there isn't a problem, and I am happy to take your (or Graydon's) word for it. There are certain parts of Rust's CoC that I can see working. The prohibition of violence or threats of violence seems fine, although it is unclear to me what 'violent language' is. There are various other concrete rules that make sense. But then the 1st point under the moderation header mentions that 'hurtful remarks' violate Rust's standard. Who decides what is hurtful? The person claiming hurt? But then you can just claim 'hurt' and automatically force moderation. You can hurt others by claiming hurt. &gt; On your first point, though, I strongly disagree that implicit means of &gt; addressing problems are worse than explicit ones. Implicit standard should, by &gt; definition, are less clear than explicit standards, and are less obvious to both &gt; the person engaging in potentially problematic behavior as well as the person &gt; who is the target of that behavior. You end up in situations where people don’t &gt; understand why their behavior may be construed as problematic, or how they may &gt; take action against the problematic behavior of others. I am not sure I follow this paragraph. The language is a bit confusing, so I might just not be getting your point. I will take a chance to clarify what I was trying, clumsily, to convey: Although a community might not have written down their rules for enforcement, and there is no body enacting the rules, the rules are clear. Submit buggy code to the Linux kernel and Linus yells at you. Implicitly defined but very explicit. The CoC (especially the contributor covenant) is the reverse. It is a very explicit document that clearly lists a number of rules on behavior, but those rules are written in such vague and subjective terms that they really could mean anything, depending on who's doing the interpreting. Explicit in their form (a written document) but so vague that you are constantly worried about whether or not you are breaking the rules. I am sure that if /u/agentultra had the power to enforce the Citizen CoC on me, then just trying to argue these points would have fallen under: "Advocating for, or encouraging, any of the above behavior." under the harrasment clause, and would have been grounds for exclusion from the Haskell community.
"Avoid Offensive Language" is ... fine by me, actually. Yeah, it's vague, yeah it could be abused, but how about we just take the statement at face value. Act like you're a guest in someone's living room, among other guests you don't know well. Be collegial. Be professional. Don't be an asshole. That's all it's saying.
Just wanted to say that: A) I really like the code of conduct. B) The responses and corresponding upvotes are making me feel drastically less excited about this subreddit. 
&gt; I really wish people would focus more on developing a thicker skin, In other words, let others suffer rather than change myself. What a fucking nitwit you are. I hope I did not offend you. In the end you probably did develop a thicker skin as you demand others do. And oh, please do not go away. What will humanity do without your valuable input and brilliant ideas? 
You argue that asking of you to adapt your communication style to the sensitivity of others is demanding too much, but then you suggest that people should "grow a thicker skin", that is, adapt themselves to your communication style. The only way out of this impasse I can see is meeting in the middle: there are things one might say that *a community generally agrees* on them being offensive; everyone should try and steer clear of them. What is really interesting is that the proposed guideline is in fact compatible with such a view: it says "avoid offensive language", and not "never offend anyone, ever".
&gt; I think an important point on these CoC discussions is trying to grasp the volume of contributors and contributions lost up until now due to their absence. In particular we should be investigating if there are people driven out by their absence, were those people disproportionately from a particular demographic. 2 points: 1. That coin has 2 sides. If you investigate how many people are driven out by lack of CoC, you should also look at how many people are driven out by CoC. 2. I am not sure what their demographic has to do with it. A person is a person, and if an environment is too unpleasant for them to function in, then that is a problem regardless of their demographic. However, I do completely agree with you that people going around making sexual comments or going out of their way to make someone feel bad through intentionally hurtful language is a bad thing. But the question remains: Does a CoC solve that, and what other effects does it bring?
I think in most cases the person who raised harassment as an issue is the victim... However, remember dongles guy? He made a joke about a dongle to a friend while in a public place, but not as a public comment and got fired for it because someone 'felt harassed'. I believe that it's ridiculously uncommon, but it's not something that we should support being used as a weapon. I personally support a (very carefully worded) CoC.
To add to your list, there are some video series on typeclasses.com
foldr will give you linear memory usage (and even optimizations don’t help here so it’s even worse than foldl). E.g. for `sum = foldr (+) 0` and `sum [1,2,3]` you end up building the thunk `1 + (2 + (3 + 0)))` and only then you start collapsing it whereas for `foldl'` you will end up with `0 + 1` evaluate that, `1 + 2`, evaluate that then `3 + 3` and finally 6 so your memory usage stays constant. The code generated for that will usually also be significantly faster so it’s not just about memory usage.
No, I do not remember the "dongles guy", which supports your second claim that it is ridiculously uncommon. Lets not become hostages of hypothetical and far fetched scenarios. In fact I think bringing up such ridiculous cases to prop up "fair and balanced discussion" is exactly what republicans do with their claims of welfare queens when smearing government programs for the poor. There really is not sane argument against civilized code of conduct. 
I started by highlighting that passage of yours because I find it particularly telling. Let's re-establish some context; we're talking about a code of conduct for an open source project, i.e. some sort of informal contract between people interested in freely collaborating towards some shared goal. You say you're "afraid of getting called" racist/sexist/transphobe, meaning that 1. you know full well you harbor those thoughts and 2. you know that much of society finds that language and those thoughts problematic to say the least because they make people suffer. I personally do see a pretty macroscopic problem here if you willingly use harassing language within a community of volunteers (or anywhere for that matter), and I'm completely serious here.
You're not wrong about me. I will direct you to Ashe Dryden's well-thought our CoC FAQ: https://www.ashedryden.com/blog/codes-of-conduct-101-faq#cocfaqcensorship The aforementioned FAQ is aimed at conference organizers and much of the same answers apply to the context of contributing to open source projects in my opinion. If you try harder and follow the codes of conduct there isn't a problem. Everyone can be excellent to one another. I don't have any tolerance for these sorts of discussions because I've witnessed the harm caused by them first hand.
&gt; You say you're "afraid of getting called" racist/sexist/transphobe, meaning that 1. you know full well you harbor those thoughts You just crossed the line there.
This was fascinating and the writing was really clear - thanks!
Why? He admitted as much himself. 
For reference: https://techcrunch.com/2013/03/21/a-dongle-joke-that-spiraled-way-out-of-control/ We'd need a study to get actual rates, I only wanted to show that it wasn't a hypothetical, just uncommon. You're right. A CoC should be fine as long as it's not weaponised.
&gt; you just accused someone of being a racist sexist transphobe for no reason, or for your own preconceived notions of why someone may object to a CoC. Not at all; let me copy the original passage in full: &gt; I think my point is simple, but because I have seen these types of discussions devolve before, and because I have seen people get smeared and disparaged, I am afraid to speak up. I am afraid to speak up against a CoC, because I think I will get called a racist, sexist, transphobe and be subsequently ostracized. To be exceedingly clear, I am absolutely in favor of (constructively) criticizing a CoC in an open source project (or anywhere for that matter), but I do see a qualitatively different problem here. OP is not simply afraid of being technically sub-par, or coming across as unfriendly, but has these oddly specific concerns. 
Dear all, I think that discussions like these tend to devolve quickly, since written text is a medium that doesn't carry the same nuance as face-to-face discussion, and this is an emotionally loaded topic, both for those advocating for, and against CoC's. Therefore, I am making this my closing statement. I will not be responding in this thread again, but if someone wishes me to clarify anything, I'd be happy to engage in PM's. Please view this post as a summary of my views, and if it seems to contradict what you thought I was saying before, please take this as the authoratative version. I think it is a good and noble undertaking to try to create a healthy, kind, and pleasant environment for any contributor to flourish in. I am not denying that there are dicks on the internet. I am not denying that certain communities are unkind to eachother. I am not denying that certain people have felt so unwelcome as to have left. However, does a CoC solve these problems? Does a CoC introduce new problems? These are important considerations. The language, both in the blog post (the section about offensive language), the contributor covenant, the Rust CoC, and the Citizen CoC contains elements of "avoid offensive behavior". I am not claiming people do not engage in behavior that is offensive to me. I am not claiming that when you feel offended that this isn't unpleasant. I am not claiming that certain kinds of behavior that I deem offensive shouldn't be policed. Threats of violence should not be tolerated. Overtly sexual behavior should not be tolerated. However, "avoid giving offense", "avoid hurtful behavior", or other similar phrases put you in the position where you have to not just consider a set of concrete norms and rules, or what a specific person has told you about how they want to be treated, it puts you in the position of having to consider what *anyone* might find offensive, and this is simply impossible. The vague, subjective language in those CoC's, that is so open to interpretation, is not a good guide for healthy behavior. It is open to abuse, and to justify terrible behavior in the name of inclusivity. For example, from somewhere a bit down in this thread: My words: &gt; I am sure that if /u/agentultra had the power to enforce the Citizen CoC on me, then just trying to argue these points would have fallen under: "Advocating for, or encouraging, any of the above behavior." under the harrasment clause, and would have been grounds for exclusion from the Haskell community. /u/agentultra 's response: &gt; You're not wrong about me. Now, I do not think that I crossed a line here. I tried to be civil and argue my point dispassionately and avoid using offensive language. However, this was still offensive enough to someone to want to have me banned from Haskell existence. Someone who, were they on some CoC regulatory board, would try to have me expelled. I think this proves my point. Additionally: /u/ocramz: &gt; You say you're "afraid of getting called" racist/sexist/transphobe, meaning that 1. you know full well you harbor those thoughts This seems to indicate that fear of giving offense is cause for offense. The anxiety I expressed about CoC's, ambiguity, enforcement and centralization of behavioral regulation have their proofs right here in this thread. I know a lot of it is emotionally driven and subjective, and I don't harbor resentment for it, but I would regret it if people who thought like that could get in to positions where they have the power to regulate who gets to contribute to some of my favorite projects, and who doesn't. Decentralized, reactive, context-driven problem resolution is not perfect, but it is less vulnerable to abuse. Fin
Depends if the other instances have a fixed tag or it was left as a variable. Currently you can use your own tag but pick up the instances that I've defined. 
I mean, you ended up not posting what would not have been a constructive contribution to the conversation. Seems like a win to me?
Some people might view any attempt at shutting down any speech as dangerous. Especially when coupled with increasingly common hyper-sensitivity to any opposition. And as mentioned, there is no objective way to classify offensive speech and behavior. CoC may be harmful when it starts giving unwarranted protection from opposition, when people start abusing it to shut others down.
Being afraid of being called something does not imply that they _are_ that something. You're bringing in a premise that all insults are true, apparently including even hypothetical ones. If that were the case, the CoC discussion would be quite different, as would the world. So, again, you've called someone a racist sexist transphobe on wildly insufficient grounds, and you've crossed the line here, under any reasonable interpretation of civility in the modern age, and owe leaf_cutter an apology. You've actually completely and utterly proved leaf_cutter's point, by doing exactly the thing leaf_cutter is concerned about. You've already labeled leaf_cutter in your mind and are feeling like you can fling what are in the modern era the worst secular insults you can at them.
The SQLite CoC, which I believe has been replaced with something more modern, was actually not a joke at all.
I agree that to “willingly use harassing language” is inexcusable, even out of ignorance. But it’s also perfectly possible to be afraid of serious accusations (racism, sexism, homophobia, transphobia, ableism, ageism) even if you *aren’t* guilty of those things, and would *never* use such language anyway. In my experience, usually that fear stems from a different sort of ignorance, due to cultural normalisation of those horrible things. For example, if a man says “Nowadays I don’t know how to talk to women without being accused of sexual harassment”, that suggests to me, at face value, that he *literally doesn’t know what sexual harassment is* because he’s never been forced to really consider the perspective of a woman. To his mind, *any* innocent action of his can now be misconstrued as harassment—patently false, but he doesn’t know that. It might be due to any number of things: a culture that has indoctrinated him with the bullshit of “men are from Mars (rational, comprehensible), women are from Venus (emotional, alien)” from childhood, preventing him from internalising the fact that women are just people; or perhaps he’s simply never *seen* actual harassment because the (small) fraction of men who (repeatedly) perpetrate it do so mainly when there aren’t other men around to call them out on it. Thing is, if someone is concerned about this, in my experience they’re probably already more considerate than virtually all of the *truly* problematic people, and thus have nothing to fear. 
Sure. If that's the accusation, no problem. I was more worried of being seen as "toxic" because I did not bother to patiently explain the exhausting saga of non-stop infighting and flame wars that have been raging for years in this community, to this new person, who is so new they apparently haven't seen any of it. And then I reflected a bit on *why* I felt that way, and it's just some combination of this Internet call-out culture, and the fact that tone never comes across well in text communications, so I just let it be (but not before instead summarizing my experience in a meta-comment, and now here I am explaining myself yet more so as to not be misinterpreted, and on it goes...)
Yo, that looks like a great book, dude! I was reading “Learn you a Haskell...”, and its pace was too slow for me, with very few real-life examples (although it’s pretty hilarious at times, that’s pretty cool), and this book has lots, which is great!
Moderating a public forum on the internet doesn't make someone a 'virtue signalling asshole.' See also the difference between most forums, like this one, and 4chan. It'd be really neat if we lived in a world in which projects were automatically filled with all of the nicest people and no moderation was ever necessary. We don't. 
This is actually a valid point, made obliquely: In mediums where there is some *expectation*, in a CoC or otherwise, this sort of thing happens a lot. People coming to a small misunderstanding and then resolving it through meta-message upon meta-message. It could be argued that this is a result of not wanting to come across as a toxic / "bad-faith" actor. The thing is though, I think that's actually not a bad thing. It might make conversations more cumbersome sometimes, but (a) people get better at avoiding these misunderstandings, ie they get better at communicating; and (b) the corrections happen because people are thinking more about how they come across and how people will respond to what they say, which is a good and natural part of dialogue. I worry sometimes that a lot of discourse, particularly online but also offline in many Computer Science circles, is just a series of monologues without regard for who's listening or what they're going to infer.
&gt; The limits of one's freedom of speech are given by other's right to not be verbally assaulted, it's really that simple. I seem to remember freedoms ending at the tip of another's nose; not their safe-space bubble they've built. No matter what I say/write, it's not going to infringe on your bodily autonomy. That said, personal attacks or inaccurately correlating personal attributes with the quality of your contributions do seem behavior we'd like to curtail. And, intentional abuse (verbal or otherwise) or a habit of unintentional abuse should be grounds for temporary removal from the community pending a suitable penance and a behavioral change.
It just depends on the enforcement. If you enforce a code of conduct, while it might sound nice, all that is happening is that a small group of people (or worse just one person) will be the one *deciding where the line is*. The disconnect is wanting an Internet that is both free and also safe for *everyone*. Unless you enforce it, it's just a suggestion and it doesn't matter, and if you do enforce it then there is a *huge* risk of centralizing power that becomes corrupt and destroys the community.
It is indeed a great book and goes waaay deeper and more into the reasons why some things came to be the way they are (Monads anyone?) compared to learn you a Haskell for great good. Yet, sometimes it's a bit too complicated, or maybe it's just me that basically never learned any other programming language in books, so I'm not used to it. I'd still recommend it, though!
Great introduction to haskell. The smartass in me wants to add that this [isn't the sieve of Eratosthenes] because, say, 10 is crossed off by both 2 and 5 instead of just 2. However adding the whole priority queue thing kind of would have destroyed the point of the video.
Nailed it.
I read the Mozilla CoC. And I don't like it. The point is that I am socially inept and when I say something, it always sounds weird. But I don't mean it and I wished such a CoC would consider that it does not apply when I don't mean to cause harm. In a friendly world, I allow myself to be naive and open. It doesn't work here when every word I say is judged. This is a hostile world. It's a trap for me. A CoC like this one causes me to go to *my* "safe space" and this is one where I don't need to have any social interaction. The irony behind it is that I don't expect any tolerance in the end. And I am not alone with this problem, I've seen. But point is that many people struggle to be included, but they won't come forward, because... you know... it's difficult. I'd rather protect them than any people who obviously misunderstand these people.
What do you mean by "as they are meant to"?
Shit sorry I forgot to put examples in I'll do that now Thanks
You shouldn't need `CReal` in order to use `cos` and `sin`, they should work on `Float` or `Double` which are more easily available (in `base`). It looks like `CReal` in the `Data.Number.CReal` module actually comes from the `numbers` package: here on [hackage](https://hackage.haskell.org/package/numbers-3000.2.0.2/docs/Data-Number-CReal.html). I don't think you need it for your project but if you need to use other packages than `base`, you'll need to learn about either `cabal` or `stack`. These tools will help you install and use other Haskell packages. Good luck! :-)
There is no disconnect here. Moderating a community explicitly excludes some behaviors, and by extension, the unrepentant perpetrators thereof. I'm fine with that. I hand the 'huge risk' of curating my community of to some authority in every community I've ever been a part of. In this one, you can see their usernames displayed in the sidebar. It's an extremely normal part of being online, or, really, being in a community of any kind. You're posting on a moderated subreddit, so, you also seem more or less ok with this phenomenon. We have rules here too. That is all this is, and all it ever was - A written rule on the wall of the room that someone can point to when they show you the door for behaving inappropriately. I can understand having issue with trusting specific people given past behaviors, or having issue with specific rules or types of rules. Having an issue with the concept of rules existing in the first place just seems kind of ridiculous.
&gt; Trolls are going to ignore the rules and non-trolls don't need to be told what to do. 
I though pi/2 would be treated as a radian and give the correct answer? anyway I've added a small function to take in floats etc... of degrees and return the radian equivalent will update post
When you evaluate the cosine of a value that's very close to pi/2, you get a number very close to 0. That's what I expected. Similar, the sine of a number very close to pi should also be a number very close to 0. If you were expecting to get *exactly* 0, then the problem is with your expectations. All floating point numbers are rational (with a power of two in the denominator, even), so you're never computing with values that are exactly the mathematical pi, or pi/2 -- those exact values are irrational, and can't be represented. Furthermore, floating point operations only promise to produce a result that is within 1 ulp of the mathematical answer, but NOT necessarily the closest choice. There are good reasons for this in numerical analysis, but ultimately that means these functions are always approximations.
Well put. I understand the confusion but /u/ocramz no doubt is in the wrong, because he misinterpreted what was said. It wasn't, "I have ...*things*... to say out loud... but that dastardly CoC is going to get me kicked out of the community!" Rather, it was just an expression of the fear that *merely opposing the concept of a CoC*, for what reason it does not matter, was enough grounds to be called a racist, sexist transphobe. Which is exactly what happened.
I’ve quite enjoyed my time at Bloomberg over the last year, the people are great and I’ve always felt valued and respected as a person. I also know the lead and a few other members of the team working on analysis/refactoring of Fortran code, and I can easily recommend working with them. 
It *is* giving you the right answer... with a (very small) rounding error. I don't think there's much you can do about that. If you're using the outputs to render an image, the error shouldn't be visible.
Big +1 to this. You’ve very neatly and articulately outlined what I like about the notion of explicitly defined CoCs. IMO, homogeneous communities won’t run into these issues as much, because the implicitly accepted guidelines won’t deviate much between participants. However, I believe more diverse communities won’t have the without the same implicit understanding among participants. From this point of view, a community’s historical “ingroup” may feel as if a cumbersome and unnecessarily restrictive set of rules are being applied to their means of communication. I still think this is a net good though, since (as you said) it encourages individuals to think a little more carefully about the real intent behind their thoughts before they express them.
&gt; By putting the onus of offense on the 'offense giver', you have just created an environment where 'taking offense' has been incentivized. I love this wording. Thanks for writing the posts here, I enjoyed them very much. I have only two words for people taking offense on the Internet: grow up.
Sweet, thanks! 
I am in full support of this CoC. I come from a really backwoods place. I spent a large portion of my life around people who are were essentially had only one set of cultural norms that many people would probably consider really backwards. I understand the anxiety that comes from being ignorant of what may or may not be perceived as acceptable behavior in a more metropolitan context. When what you see in the media or on the internet is so wildly and violently critical of behavior that you've spent you're entire life thinking is innocuous, that generates a real anxiety about what might happen if you step outside these lines you can't see. I have also had the fortunate experience of spending the latter half of my adult life in a more metropolitan context, and am becoming more comfortable with those lines, how to see things from someone else's perspective, and how to repair relationships with someone if I have accidentally misspoken or given someone the wrong impression. Not everyone has had that second set of experiences. Their fears and anxieties about these interactions might seem totally outlandish, but that's because they literally don't have the context about what to expect. Maybe it's because they had a life with a more narrow scope of cultural exposure, or maybe they are just generally very reserved or socially anxious people, or perhaps they're just built differently in some way. But the presence of those anxieties doesn't mean that they're fundamentally incapable of acting in good faith - It just means they're coming from a totally different perspective. If you don't have the kind of patience to deal with that perspective, I understand that sentiment. Especially for folks who feel marginalized, that kind of work can be enormously emotionally draining. But on behalf of people like me, who did want to act in good faith, and simply needed the context to understand how to do so - Please don't assume the worst. Please don't attack someone for hypothetically bad behavior. 
I don't have problems with dependencies of `aeson` :) If you need JSON parsing, it's completely okay to depend on `aeson`. But you don't always need it. There's library for type-safe file paths called `path` but it depends on `aeson` which means that if you want to bring extra little type-safety to your code, you will add a lot of dependencies if you depend on `path`. And here is relevant issue: * https://github.com/commercialhaskell/path/issues/136
Ultimately, though, each and every one of us is responsible for what we say and write. To be sure, the attitudes implied in what you call "a friendly world" are all great things to have in a community: assuming good faith, interpreting things charitably whenever reasonable, caring about the difference between clumsiness and malevolence, avoiding snap judgments based on isolated incidents, taking extenuating circumstances into account, and appreciating visible efforts at dealing better with others. Still, at the end of the day, if a sustained pattern of unpleasant behaviour by someone leads to them not being welcome by a community anymore, they can't shift responsibility to the community that had to deal with the unpleasant behaviour all along. (Also, motives can only be gauged to the extent they are made visible by actions.)
&gt; a project should be welcoming anyone who contributes and disregard private behavior and lifestyle. The scope of most CoCs does not extend to one's private behavior/lifestyle. It only has to do with your conduct when engaging with the corresponding community.
My take on your comments here is that you have seen the bullet point "Avoid offensive language", and all of your resistance is against that particular point being too vague. If you take a look at the Contributor Covenant Code of Conduct (the one currently proposed to be the "unofficial stack code of conduct", whatever that means), you'll find it is not vague in this way. It is quite specific, actually. https://www.contributor-covenant.org/version/1/4/code-of-conduct Is there anything in this specific document that you find makes you anxious? To me, this document is not a product of "the culture of oversensitivity," but rather, it merely expresses a lot of unspoken rules that our community by and large already adheres to and believes in. It's used by tons of other open source projects, too.
I'm sold
this looks like an incredible resource, I like that there's a whole section on tooling. thank you for sharing
To offer my opinion as a counterpoint, I think people who spend time creating codes of conduct and discussing about it are people who care about their community and are seeking to improve it.
The Contributor Covenant CoC claims to have been adopted by over 100,000 projects. At this point, it's less of a "hype train" and more just kind of standard procedure.
I'm really on the fence about CoCs, so I have a question that I'd love to hear your thoughts on. There are people in this community who use somewhat offensive and aggressive language as a tool in technical discussions. The people I'm talking about aren't over the top; their offensive language is more inflammatory and anxiety-inducing than it is exclusionary or attacking. The language would, IMO, be in violation of most CoCs, but not overtly so, and only on occasion. I.E. they are not problematic enough that I believe these people should be ejected. Though I would prefer they change their ways and communicate more amenably, it is better to endure them than to lose them, especially since some of them are particularly valuable to the community from a technical perspective. How does a CoC interact with these people? Certainly this is in the grey area of CoCs, but it's one that will come up concretely in relatively short order. In a similar vein, there are sometimes *pairs* of people who just have some bad blood between them, and just won't stop being ugly toward each other for anything. The language between one of these pairs can become offensive, but otherwise they might be perfectly upstanding members of the community. How does a CoC address people like this? Edge cases like this, where a CoC has unintended side effects such as removing generally acceptable (but occasionally unacceptable) people, give me pause. For the record, I'm not taking any stance here other than skepticism. But addressing questions like these (both predictable and unpredictable edge cases) would likely be a deciding factor for me.
Sure fair enough. On one hand we have decentralized moderating, where everyone has to deal with it, and on the other, moderating gets focused on a small subset of people. It boils down to who moderates the moderator's moderating. 
As another person who has come from a less metropolitan area and moved to a more metropolitan area, I second this. I'll never forget a r/haskell thread I participated in years ago. I said things which I thought were normal and innocuous, and I got some pretty strongly worded replies letting me know how wrong I was about that. It was literally the first time that anyone had ever told me that maybe saying some of the things I had said was not appropriate. It was the sort of thing I had heard role models in my community say numerous times; I honestly did not think it was a controversial or even inappropriate thing to say. Thankfully, at least some of the participants in that discussion were patient with me and continued attempting to help me understand their point of view, and I was able to ultimately learn a lot from the experience. People who've been in the metropolitan context for a long time tend to take that cultural exposure for granted.
I’d say the prevailing majority of projects doesn’t use any kind of CoC and they are just fine. Most people know that they should behave, those that don’t will hardly change after reading a document if they read it at all. It is however the goto “solution” of recent years and communities that have no real problem start to adopt CoC just because. I would still concider that a hype and a naive one at that.
And on irc as well.
&gt; I'm not sure of the "right" way to do this There has been some work on how to do parallel programming with in-place mutation while guaranteeing a purely functional result. You need the type system and library to work together to ensure that the parallel tasks operate on disjoint subsets of the heap. One example is the type system in [Deterministic Parallel Java](http://dpj.cs.illinois.edu/DPJ/Home.html). Another example is our approach to using "Par" monads in Haskell, as exemplified by this PLDI 2014 paper that includes parallel sorting as a benchmark: https://www.cs.indiana.edu/~rrnewton/papers/effectzoo-draft.pdf The idea there is to use a state monad (transformer) with a state that is structured to be splittable in some trusted way. `forkSTSplit` creates parallel sibling tasks that operate on the different halves of the state, running arbitrary ST code to mutate the part that they "own". Parallel sorting looks even better in Rust, where ownership enforces that only one child task operates on one half a split array. 
Unintended offense is inevitable. Intended offense is inevitable too. If someone want to offend you, he will find a way to offend you circunventing the code of conduct. even making the offense even more obnouxious. What a bigger offense than ignoring or downplaying you in subtle ways for example? that's easy and has no code of conduct that could avoid it. This last form of "offense" is pervasive in communities driven by groupthinking. Anything not related with re-designing the bike shed is downplayed/ignored. Perhaps decorating it with a dozen semigroup grafittis or adding the MVC package number 1000. That's the real offense for me, The insanity of normalization of normality. I have no problem if someone with talent say rude things to me. that's the only way to learn and advance Being passionated about the work inevitably produces creative comfrontation. The CoCs are a sign of mediocrity, lack of creative genious, stupidity. it is a sign of the times. The source of alambicated and void conversations that fill the time of people without real interests except socializing and having job security.
They used to market for different needs. `conduit` was about "resource usage" and native support for leftovers (think incremental parsing over a stream), and `pipes` about being "clean" and "based on laws". Turns out that except for the leftovers issue, they are pretty similar, and both have large-ish ecosystems.
Sorry if this is obvious but in case you missed it, the number `6.123123123e-17` is actually just scientific notation for `6.123123123 * 10 ^ (-17)`. In other words, it's pretty much zero.
Thanks! Definitely will use it.
I personally prefer `streaming`. - I like that it doesn't have a separate type for intermediate stages, kind of like Java's streams. - The way it has of representing grouping operations is quite elegant. - The core library doesn't commit to a particular exception/resource handling library (this might be a disadvantage for some.) Pipes and especially conduit seem to have bigger ecosystems though. pipes/conduits/streams are interconvertible, so it is possible to mix and match libraries, at the cost of some efficiency.
You won't find better documentation about that particular topic (`conduit`/`pipe`) than [this](http://www.haskellcast.com/episode/006-gabriel-gonzalez-and-michael-snoyman-on-pipes-and-conduit).