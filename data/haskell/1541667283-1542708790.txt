I will be messaging you on [**2018-11-15 08:54:41 UTC**](http://www.wolframalpha.com/input/?i=2018-11-15 08:54:41 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/haskell/comments/9v2rvp/carnap_is_a_free_and_opensource_haskell_framework/) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/haskell/comments/9v2rvp/carnap_is_a_free_and_opensource_haskell_framework/]%0A%0ARemindMe! 1 week) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! ____id____) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
Sounds strange, can you give an example? You should not create threads for too small tasks and also sometimes it would be faster to run one thread GC instead of parallel. That is why I process 4 frames per thread, but not one frame.
This text you wrote is perfectly within bounds of politeness. It's even hard to see why you think you're not able to talk in a way that doesn't offend people when you wrote a few paragraphs like this. Just keep writing like this. It's not that hard.
For beginners, I highly recommend taking https://github.com/data61/fp-course and following along with https://www.youtube.com/watch?v=NzIZzvbplSM&amp;list=PLly9WMAVMrayYo2c-1E_rIRwBXG_FbLBW after solving each part on your own
What is the problem of getting offended by smaller and smaller issues? We complain every day about smaller and smaller issues. We buy services and products to deal with smaller and smaller inconveniences. We learn every day how to make them go away. Why language and public discourse should be any different? For the overall picture, I don't really why it matters. Arguments about productivity and the collective time lost to discussion is much more attractive to me than the above, which would ironically not be an issue if there wasn't so much back and forth. 
&gt; Sure, the Rust community is "friendly", but that's only a facade - you can make any community "friendly" by simply deleting every comment that isn't "friendly". ah the [Hot Fuzz](https://www.imdb.com/title/tt0425112/) approach
&gt; Why would anybody even oppose a CoC if they're conducting themselves in a proper way? Why do you think?
&gt; I like that it doesn't have a separate type for intermediate stages, kind of like Java's streams. On top of this, the `Stream` type is meant to be basically effectful lists. That's why a lot of things just take a `Stream` as an argument - things like `take` also just take a list as an argument. I find once you get familiar with this idea, you actually feel like you're basically doing list transformations again.
How I miss typed holes when programming in other languages... :( 
I feel like type hole messages are way too verbose. Like, what's up with: • In the first argument of ‘fmap’, namely ‘_’ In the first argument of ‘Free’, namely ‘(fmap _ f)’ In the expression: Free (fmap _ f) Why not have Free f &gt;&gt;= g = Free (fmap _ f) ^ with some syntax highlighting? Also, I think I never read the 'where' block at the beginning (though I should) and having Constraints include Functor f (from Free.hs:3:10-36) Instead of a plain list like Constraints: - Functor f (from Free.hs:3:10-36) Otherwise, they're the bees knees.
Have you heard of [Hazel](http://hazel.org/)? Hazel is a PL based on the idea of Typed Holes. I've never use it but you should give it a try...
we have a word for describing this proposal which couldn't fit better: **Farpotshket**
Great. Thanks! This is my solution: `untilMatchingClosing &lt;- rule $` `(\x -&gt; [x]) &lt;$&gt; satisfy (\x -&gt; x == ')') -- CLOSING` `&lt;|&gt; (\x y z -&gt; [x] ++ y ++ z) &lt;$&gt; satisfy (\x -&gt; x == '(') &lt;*&gt; untilMatchingClosing &lt;*&gt; untilMatchingClosing -- OPENING` `&lt;|&gt; (\x y -&gt; x : y) &lt;$&gt; satisfy (\x -&gt; x /= ')') &lt;*&gt; untilMatchingClosing -- OTHER`
Yes, I have, and it's awesome! However, I wanted the suggestions to be integrated into GHC itself, so that beginners could use it without having to go through any additional installation process beyond installing the platform. Hazel is very nice, but requires a specialized language and IDE to support it. While I hope Hazel and the like will be used more in the future, I felt that the typed holes in GHC had a lot more potential than what it had had so far.
[removed]
&gt; I've taken a couple of these courses and they are a bit short, but overall well-put-together, the 'mastering haskell' course seems very in-depth and advanced Thanks! Btw Packt specifically asked for the videos to be short, so I couldn't go as in-depth as I wanted on some of the topics.
Don't forget about [`machines`](https://github.com/ekmett/machines)!
On a lighter note, it often turns out that people with social "ineptitude" and/or anxiety overestimate how badly others would react to what they say. So even if you feel the things you say come out "weird", odds are that they are nowhere near as awful as you think they are. Cf. the excellent comments by /u/evincarofautumn ([here](https://www.reddit.com/r/haskell/comments/9ux5te/comment/e990c9v) and [here](https://www.reddit.com/r/haskell/comments/9ux5te/comment/e993ke4)) on this thread, which make related points.
I agree, the message needs an overhaul. I haven't fudged anything in the typed-hole error message itself, since I suspect that some tools are parsing the raw message. However, I could probably get away with changing it behind a flag. Thanks!
``` replace :: a -&gt; a -&gt; [a] -&gt; [a] replace x y [] = [] replace x y (z:zs) | x == z = y:(replace x y zs) | otherwise = z:(replace x y zs) ``` Explanation added later if you want :D
One might think that growing "some thick skin" would involve not complaining about downvote, especially in a venue as hopelessly fickle as Reddit is. (For the record: while there are many comments in this thread that I didn't downvote regardless of strong disagreement, I did downvote your parent comment, because I see it as unenlightened repetition of a cliché.)
`map (\x -&gt; if x == 1 then 2 else x)`
Thanks for your response. I'd just like to make I'm certain that I'm understanding you correctly. Is it your understanding that the comment "Hear that? It's the sound of the point rushing over your head." would be tolerated (in certain circumstances) under the CoC that Michael is proposing? 
Site seems to have been taken down by reddit! Here's the post for those who are wondering: &amp;#x200B; &amp;#x200B; Typed-holes and Valid Hole Fits =============================== By Matthías Páll Gissurarson When developing non-trivial programs, there is often a lot of type complexity at play. We, as developers, are often quite good at keeping track of what it is that we're working with while writing programs in Haskell. But often, there is a lot of polymorphism going on, meaning that we're not quite sure what each "type variable" in the type represents and what the type of the current expression is. One way to figure it out is to just write `blah` (assuming that `blah` is not in scope) and then trying to decipher the error message that the compiler outputs. However, there is a feature that is becoming increasingly common in strongly typed programming languages called "typed-holes", which allow you to query the compiler more precisely and interactively, without going the round-about way of generating an error. Typed-holes are especially useful when dealing with *Domain Specific Languages (DSLs)*. By encoding into the types properties about your domain (like the fact that phone numbers have to have a specific length, or that you can only call a function in a secure context), you can use typed-holes to see what the options are in each instance, and to remind yourself what invariants must hold at that specific point. In this post, I will try to explain typed-holes in the context of GHC (everyone's favorite compiler), and some cool new contributions that were adopted by GHC that I proposed and implemented, namely, *valid hole fits* and *refinement hole fits* to make them even more useful for developers. But first, what these "typed-holes"? Typed-holes ------------ Typed-holes were introduced to GHC in version 7.8.1, which was first released in April 2014. In GHC, you write an underscore (`_`) anywhere you would write an expression, and when you then compile the code, GHC will output a typed-hole error message. Let's look at an example: If you write (in GHC 8.6.1 or later) ```haskell f :: String f = _ "hello, world" ``` GHC will respond and say: ``` F.hs:2:5: error: • Found hole: _ :: [Char] -&gt; String • In the expression: _ In the expression: _ "hello, world" In an equation for ‘f’: f = _ "hello, world" • Relevant bindings include f :: String (bound at F.hs:2:1) ... ``` Which tells us the inferred type of the hole (`[Char] -&gt; String`), the location of the hole, and the "relevant bindings" (local functions whose type is relevant to the type of the hole). Of course, this isn't a very useful case, i.e. it is pretty clear what the type of the hole is in this case. A more interesting example is perhaps the `Free` monad: ```haskell instance Functor f =&gt; Monad (Free f) where return a = Pure a Pure a &gt;&gt;= f = f a Free f &gt;&gt;= g = Free (fmap _ f) ``` Here, the type of the hole involves a few things like `Functors`, local functions and instance variables, but still the typed-hole can tell us: ``` Free.hs:6:29: error: • Found hole: _ :: Free f a -&gt; Free f b Where: ‘a’, ‘b’ are rigid type variables bound by the type signature for: (&gt;&gt;=) :: forall a b. Free f a -&gt; (a -&gt; Free f b) -&gt; Free f b at Free.hs:5:10-12 ‘f’ is a rigid type variable bound by the instance declaration at Free.hs:3:10-36 • In the first argument of ‘fmap’, namely ‘_’ In the first argument of ‘Free’, namely ‘(fmap _ f)’ In the expression: Free (fmap _ f) • Relevant bindings include g :: a -&gt; Free f b (bound at Free.hs:6:14) f :: f (Free f a) (bound at Free.hs:6:8) (&gt;&gt;=) :: Free f a -&gt; (a -&gt; Free f b) -&gt; Free f b (bound at Free.hs:5:10) Constraints include Functor f (from Free.hs:3:10-36) ``` Which tells us a lot about the hole, and a lot about the inferred types of the arguments. Valid Hole Fits --------------- See that `...` in the first error message? That's where my addition comes in. The idea is that, when you're working with the standard library like the `Prelude`, or even a more esoteric library like `lens`, you might not always realize what the possibilities are. You might ask GHC for the type of the hole, but if it's some weird type like `((Int -&gt; f0 Int) -&gt; T -&gt; f0 T) -&gt; Int -&gt; State T a0` (where `f0` and `a0` are ambiguous), the type might not help you out that much (even if you Hoogle it!). However, the compiler generates the error message, and the compiler knows perfectly well what would be a "valid fit" for the hole. In GHC 8.6.1, compiling: ```haskell f :: String f = _ "hello, world" ``` will now output the following: ``` F.hs:2:5: error: • Found hole: _ :: [Char] -&gt; String • In the expression: _ In the expression: _ "hello, world" In an equation for ‘f’: f = _ "hello, world" • Relevant bindings include f :: String (bound at F.hs:2:1) Valid hole fits include cycle :: forall a. [a] -&gt; [a] init :: forall a. [a] -&gt; [a] reverse :: forall a. [a] -&gt; [a] tail :: forall a. [a] -&gt; [a] fail :: forall (m :: * -&gt; *) a. Monad m =&gt; String -&gt; m a id :: forall a. a -&gt; a (Some hole fits suppressed; use -fmax-valid-hole-fits=N or -fno-max-valid-hole-fits) ``` Where the valid hole fits are those functions in scope which best fit the type of the hole! This means that by replacing the hole with e.g. `cycle` or `init`, you will have a valid program (in the sense that it is well-typed). Of course, this works better for more complex examples, like `lens`. Consider the following: ```haskell import Control.Lens import Control.Monad.State newtype T = T { _v :: Int } val :: Lens' T Int val f (T i) = T &lt;$&gt; f i updT :: T -&gt; T updT t = t &amp;~ do val `_` (1 :: Int) ``` Here, we're defining a very simple data type `T` which contains a single value `_v` of type `Int`. When we write ```val `_` (1 :: Int)```, we're asking "what operator can I use here?". With the new valid hole fits, GHC will tell you: ``` Lens.hs:13:7: error: • Found hole: _ :: ((Int -&gt; f0 Int) -&gt; T -&gt; f0 T) -&gt; Int -&gt; State T a0 Where: ‘f0’ is an ambiguous type variable ‘a0’ is an ambiguous type variable • In the expression: _ In a stmt of a 'do' block: val `_` (1 :: Int) In the second argument of ‘(&amp;~)’, namely ‘do val `_` (1 :: Int)’ • Relevant bindings include t :: T (bound at Lens.hs:12:6) updT :: T -&gt; T (bound at Lens.hs:12:1) Valid hole fits include (#=) :: forall s (m :: * -&gt; *) a b. MonadState s m =&gt; ALens s s a b -&gt; b -&gt; m () with s ~ T, m ~ (StateT T Identity), a ~ Int, b ~ Int (&lt;#=) :: forall s (m :: * -&gt; *) a b. MonadState s m =&gt; ALens s s a b -&gt; b -&gt; m b with s ~ T, m ~ (StateT T Identity), a ~ Int, b ~ Int (&lt;*=) :: forall s (m :: * -&gt; *) a. (MonadState s m, Num a) =&gt; LensLike' ((,) a) s a -&gt; a -&gt; m a with s ~ T, m ~ (StateT T Identity), a ~ Int (&lt;+=) :: forall s (m :: * -&gt; *) a. (MonadState s m, Num a) =&gt; LensLike' ((,) a) s a -&gt; a -&gt; m a with s ~ T, m ~ (StateT T Identity), a ~ Int (&lt;-=) :: forall s (m :: * -&gt; *) a. (MonadState s m, Num a) =&gt; LensLike' ((,) a) s a -&gt; a -&gt; m a with s ~ T, m ~ (StateT T Identity), a ~ Int (&lt;&lt;*=) :: forall s (m :: * -&gt; *) a. (MonadState s m, Num a) =&gt; LensLike' ((,) a) s a -&gt; a -&gt; m a with s ~ T, m ~ (StateT T Identity), a ~ Int (Some hole fits suppressed; use -fmax-valid-hole-fits=N or -fno-max-valid-hole-fits) ``` 
 Refinement Hole Fits -------------------- Of course, finding single functions is not always that useful. In Haskell (and with functional programming in general), we often find ourselves programming using higher-order functions and combinators. To facilitate this style, we introduce the `-frefinment-level-hole-fits`. Take for example a function of type `[Int] -&gt; Int`. Using GHCi, we can search the Prelude for any such function: ``` λ&gt; _ :: [Int] -&gt; Int &lt;interactive&gt;:25:1: error: • Found hole: _ :: [Int] -&gt; Int • In the expression: _ :: [Int] -&gt; Int In an equation for ‘it’: it = _ :: [Int] -&gt; Int • Relevant bindings include it :: [Int] -&gt; Int (bound at &lt;interactive&gt;:25:1) Valid hole fits include head :: forall a. [a] -&gt; a with a ~ Int last :: forall a. [a] -&gt; a with a ~ Int length :: forall (t :: * -&gt; *) a. Foldable t =&gt; t a -&gt; Int with t ~ [], a ~ Int maximum :: forall (t :: * -&gt; *) a. (Foldable t, Ord a) =&gt; t a -&gt; a with t ~ [], a ~ Int minimum :: forall (t :: * -&gt; *) a. (Foldable t, Ord a) =&gt; t a -&gt; a with t ~ [], a ~ Int product :: forall (t :: * -&gt; *) a. (Foldable t, Num a) =&gt; t a -&gt; a with t ~ [], a ~ Int (Some hole fits suppressed; use -fmax-valid-hole-fits=N or -fno-max-valid-hole-fits) ``` But maybe we want to do something different. By setting `-frefinement-level-hole-fits=1`, we get the following: ``` λ&gt; _ :: [Int] -&gt; Int &lt;interactive&gt;:33:1: error: • Found hole: _ :: [Int] -&gt; Int • In the expression: _ :: [Int] -&gt; Int In an equation for ‘it’: it = _ :: [Int] -&gt; Int • Relevant bindings include it :: [Int] -&gt; Int (bound at &lt;interactive&gt;:33:1) Valid hole fits include head :: forall a. [a] -&gt; a with a ~ Int last :: forall a. [a] -&gt; a with a ~ Int length :: forall (t :: * -&gt; *) a. Foldable t =&gt; t a -&gt; Int with t ~ [], a ~ Int maximum :: forall (t :: * -&gt; *) a. (Foldable t, Ord a) =&gt; t a -&gt; a with t ~ [], a ~ Int minimum :: forall (t :: * -&gt; *) a. (Foldable t, Ord a) =&gt; t a -&gt; a with t ~ [], a ~ Int product :: forall (t :: * -&gt; *) a. (Foldable t, Num a) =&gt; t a -&gt; a with t ~ [], a ~ Int (Some hole fits suppressed; use -fmax-valid-hole-fits=N or -fno-max-valid-hole-fits) Valid refinement hole fits include foldl1 (_ :: Int -&gt; Int -&gt; Int) where foldl1 :: forall (t :: * -&gt; *) a. Foldable t =&gt; (a -&gt; a -&gt; a) -&gt; t a -&gt; a with t ~ [], a ~ Int foldr1 (_ :: Int -&gt; Int -&gt; Int) where foldr1 :: forall (t :: * -&gt; *) a. Foldable t =&gt; (a -&gt; a -&gt; a) -&gt; t a -&gt; a with t ~ [], a ~ Int const (_ :: Int) where const :: forall a b. a -&gt; b -&gt; a with a ~ Int, b ~ [Int] ($) (_ :: [Int] -&gt; Int) where ($) :: forall a b. (a -&gt; b) -&gt; a -&gt; b with r ~ 'GHC.Types.LiftedRep, a ~ [Int], b ~ Int fail (_ :: String) where fail :: forall (m :: * -&gt; *) a. Monad m =&gt; String -&gt; m a with m ~ ((-&gt;) [Int]), a ~ Int return (_ :: Int) where return :: forall (m :: * -&gt; *) a. Monad m =&gt; a -&gt; m a with m ~ ((-&gt;) [Int]), a ~ Int (Some refinement hole fits suppressed; use -fmax-refinement-hole-fits=N or -fno-max-refinement-hole-fits) ``` Which correctly identifies that if we apply `foldl1` to a function of the correct type, the type of the entire expression will be `[Int] -&gt; Int`. Conclusion ========== And there you have it! As we've seen from the various examples, valid hole fits and refinement hole fits can help developers work with the Prelude to find both simple functions (like `sum` and `product`) and complex functions (like `(=&lt;&lt;)`), even more complex and hard to guess functions from `lens` (like `(&lt;&lt;*=)`). For more details (like using typed-holes in conjunction with `TypeInType` to search by annotated complexity), check out the paper and the talk I gave at Haskell Symposium 2018: [Suggesting Valid Hole Fits for Typed-Holes (Experience Report)](https://mpg.is/papers/gissurarson2018suggesting.pdf) &lt;a href="https://mpg.is/papers/gissurarson2018suggesting-talk.pdf"&gt;[Slides]&lt;/a&gt; &lt;a href="https://mpg.is/papers/gissurarson2018suggesting-talk.html"&gt;[Demo Output]&lt;/a&gt;
&gt; B) The responses and corresponding upvotes are making me feel drastically less excited about this subreddit. If it serves as solace, r/haskell is not alone in that. The recent discussions at (Meta) Stack Overflow about their welcomingness initiatives and CoC were full of the kind of reaction you refer to -- notably, the unreasonable *reductio* arguments ("We shouldn't make unwelcoming comments? But anything might be seen as unwelcoming by someone, therefore we can't say anything anymore!") and the displays of oversensitivity in the process of claiming someone else is being oversensitive.
I'd say all of GHC (error) messages are too verbose.
&gt;You can crack jokes and be **sarcastic** or even downright ornery in a way that doesn’t ever “punch down” or exclude anyone. *sarcasm is a subtle form of abuse – verbal violence – and to be sarcastic is to obtain amusement at another’s expense ... it is a particularly cutting form of teasing, with vindictive undertones, and thus qualifies for the lowest rating on the humour scale. It is less obvious with irony yet, just as sarcasm is designed to make the recipient feel ridiculed, irony is designed to make the recipient feel rueful. They are thus both pathetic wit, even by definition, as the word ‘pathetic’ is derived from the root ‘pathos’, which indicates sorrow. Which all goes to show that the giver of either sarcasm or irony wishes the recipient to feel the incipient sorrow that is endemic among humans.*
 map (*2) ;)
It's a harsh statement but that would be up to whoever enforces a CoC to decide if it was out of line for the community standards. If it was then I'm fine with apologizing and will check myself in the future.
Both are USA :(
This is wrong -- it requires an equality constraint on `a`.
I've never used streaming-prelude. But I [avoid conduit](https://www.iguanasuicide.net/node/22) when I can and prefer pipes.
Yeah you're right :D
&gt; If you just try to be professional and considerate, and are prepared to sincerely make amends in the unlikely event that you do inadvertently hurt someone, you really have nothing at all to fear. citation needed! Where exactly is this expressed in the code-of-conducts mentioned in this thread? And what does "sincerely make amends" mean? I really have nothing to fear, apart from the big, undefined, vague "if" right before. If the problem is that judgement hinges on unclear phrases, I do not understand how this addition helps, as it seems to hinge on yet another, vague phrasing.
I'm not sure I'm rendering this correctly. This is how I see the examples in the "about" page, in Safari: https://imgur.com/a/EQVtYxq In other browsers the demo section doesn't even show up.
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/EIvwe9V.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) 
It's not about offense or controlling conversations. There's nothing wrong with being constructive, ornery, or occasionally confrontational. There is something wrong with using racial slurs in offhand remarks. There's nothing wrong with pointing out that someone misunderstood something (even if it offends their pride). There is something wrong with calling someone names using ableist slurs. There are resources out there for you to learn from and sort out what codes of conduct are for and what acceptable behaviors are. If you're offended by that then find a community that is accepting of your preferred behavior.
I still get tripped up over these error messages. Your suggestion sounds like a huge improvement.
You're virtue signalling right now. It's about controlling behavior because some people don't find it offensive to use gendered/ethnic/ableist slurs. There are people who are harmed by the use of such slurs and we don't want to participate in communities that normalize such behaviors. Period. Personally I don't care if you invented the cure for cancer. You can still be smart and insufferable. A CoC will limit your options in places where you will be welcome to present your research. Moderate your behavior and you will be welcome in more places. Will that hurt humanity? I doubt it. There are other smart people who are not insufferable. They may have cures for cancer. And they will make the world a better place. You see? The auteur genius who is insufferable and must be tolerated is a myth. There are no snowflakes. They will simply not be tolerated and everything will continue on fine without them.
I have never used Disquis, but I believe that you don't need special support from Hakyll or any other static site generator. You probably just need to add the necessary JS script to the page where you want to have comments. To add JS scripts, or YouTube embed videos you could add them to your template as if it were an ordinary HTML file. You can read more about templates [here](https://jaspervdj.be/hakyll/tutorials/04-compilers.html#templates). In the case of YouTube videos it's probably easier to simply dump the html tag that YouTube gives you into your markdown files. Pandoc will keep any html tags that you add to your markdown files.
&gt; but then you suggest that people should "grow a thicker skin", that is, adapt themselves to your communication style I am not /u/leaf_cutter, so this is purely my interpretation. That said: No, those two are not at all the same thing. Let's consider the situation where something you said hurts my feelings. I could: 1. Do not let it affect me, trusting on the good in all people 1. Have it affect me, but I remain passive, say nothing, don't complain, if necessary silently stop participating 1. Privately ask for clarification on the relevant bit 1. Give brief negative feedback in the form of a plain downvote 1. Privately give feedback about how this affected me 1. Publicly give feedback of how it affected you, in a reflecting manner 1. Privately seek re-affirmation from other participants, but not the "offender" 1. Invoke a CoC or other means of top-down moderation. 1. Publicly react, in a less reflecting, more controversial manner, justifying with "you were (objectively) offensive" 1. Publicly react, by calling out the offensive behaviour to a wider audience Firstly, I do not intend the order of these to correspond to "better" or "worse" forms of reacting. Now what does "thickness of skin" and "adapt to your communication style" translate to here? I'd say adapting implies 1/2. But in my understanding, "thicker skin" merely means the ability to react more calmly, less controversially. 3/4/5/6 instead of 9/10. 6/7 instead of 8. And occasionally, 1. Crucially, a thick skin does not mean I have to remain silent. It means the ability to react to conflict in a manner that allows the (subjective) offender to reconcile and improve their ways, without making a mess out of the situation that is hard to overcome for all participants. Demanding a thick skin ("or else they create the mess of a situation") is victim-blaming. It cannot be demanded. If I am hurt you cannot tell me not to react in some way for the benefit of the community, but at my cost. All we can do is make it a vague wish, and perhaps a personal goal. 
Hum, "private" in the sense of "personal" or "non-official"? There are tons of media out there that will be considered personal (not connected to the persons place of employment etc.) yet are public. And without any references, I am sure that personal-but-public utterances have been used to invoke CoC action in the past, more than once.
Just as feedback: Your condescending tone may very well be offensive to some.
Yea having looked at sitepipe, Hakyll seems a bit much. To me at least.
&gt; Start the Haskell Community for People Who Like to Make Insensitive Jokes and Put People Down This implies that leafcutter endorsed such behaviour. I can only view this as a deceitful, purposeful misrepresentation of what has been said. I for one do not welcome your non-constructive, offensive and deceitful feedback to a valid hypothesis in this community.
Please see my response to this at https://old.reddit.com/r/haskell/comments/9ux5te/proposal_stack_code_of_conduct/e9ats3r/ "Thicker skin" does not equate to "having to remain silent" in my reading. And I don't think it is fair of you to assume your reading of the term either, without requesting clarification from the original author.
As an interesting point of information, the notion of "protecting the community" is not new in the Haskell world https://wiki.haskell.org/Protect_the_community
See the difference? On the one hand leafcutter, afraid of being inadvertently offensive with no intention of being so, but disliking the vague threat of consequences for participating in a well-meaning manner. On the other hand, agentultra being intentionally harsh and offensive. Do you really prefer the latter over the former? It seems so inconsistent, in conflict with various CoCs..
This is precisely what CoC's exist for. Did you see the point rushing past you too? In most guidelines I believe to be good there is no rule against hurting someone's pride by being critical or even abrasive. That is not the generally accepted understanding of what means to be, "offensive." If we can agree that critical feedback, even harsh feedback is acceptable as long as you don't use sexist/ethnic/racist/ableist slurs that cause harm to someone then we have established a code of conduct. Now when we interact further if one of us violates the code with our conduct there is a way for the other harmed to address the problem if they so choose. What people are implying in these threads when they suggest people need to have a, "thicker skin," will inevitably vary. In my experience the people who advocate for such policies are asking people to accept that they will sometimes use sexist/ethnic/racist/ableist remarks and don't want to deal with "politically correct police" calling them out on it. For others who support this policy that may not be the case... so why bother with having any ambiguity? Codes of Conduct are not able to banish bad behavior in the same way that laws cannot banish crime. Do some research and get yourself educated. They are a useful tool that do more good than harm. The same cannot be said for telling people to have thicker skins.
&gt; CoCs are applied uniformly. Such a blanket statement. No, they are applied exactly as uniformly as those that enforce the CoC manage to remain impartial and purely objective. You cannot just attribute some perfect properties to some social construct while ignoring that all members of this society are human, and rarely objective.
Does `streamly` allow for grouping operations like `streaming`s [`groupBy`](http://hackage.haskell.org/package/streaming-0.2.2.0/docs/Streaming-Prelude.html#v:groupBy)?
I should have said that CoCs are supposed to be applied uniformly, yes.
No matter if you call it offensive or not, you talk in a way that makes me not want to continue talking back. This vague criticism is the best I can do; I am not sufficiently wise to approach this situation better. Sorry.
I don't think that failure aborts parsing directly. If there are other alternative parsers they will be tried, and worse, the original error will not be shown.
I understand the coin has two sides, and my point is that my impression is that most of these discussions only focus on the second side you listed. Demographics matter because if the absence of a CoC affects all demographics evenly then I think you can argue that everything works well or as well as possible as-is. But if it turns out the absence of a CoC means a project tends to lose or never attract a disproportionally high percentage of a particular group, then the community in its current state is expressing some kind of bias. It may be unintentional, but it's there. And historically that would be women, or blacks, or whatever but it would be just as true for a project that tends to lose white men at a high rate.
It makes it so that `do` notation (among other things) doesn't desugar to the default `&gt;&gt;=` and `&gt;&gt;` operators from the `Monad` class but instead to some other custom operator with the same name that's in scope. Potential headaches are weird compiler errors at best and completely random semantics at worst. 
You know how people talk about the difference between `foldl`, `foldl'`, and `foldr` and how having a strict accumulator is really important, but so is being lazy in your input? The monadic versions of folds and traversals present in base are all strict on input during traversal of lists - They'll traverse the entire thing, and there is nothing you can do to stop them. It's like being stuck with `foldl`, except in IO, where you have way less control over your input, because it's coming from outside the program. Streaming libraries all present slightly different solutions to that basic problem - They let you evaluate on 'step' at a time, strictly and with effects, and they stop when you tell them they're done. The net effect of this is that you can safely traverse unbounded input in constant space, in a monad (IO is a popular choice but it's not the only choice). So that's the common problem they're all trying to solve. They each present different interfaces to enable this, and the differences are basically trade-offs between control over effects (how do I handle finalizing resources or exceptions in IO, for example), and presenting a simpler and less opinionated interface to the programmer at the cost of potentially doing something they might not want with the effects. There is also the ever present difference of who is using what features of Haskell's type system, and the perceived pro/cons of those choices and the resulting API, but as far as I've been able to tell, that whole discussion has no reliable objective measure of superiority one way or the other. I personally prefer `conduit` and `foldl` for my streaming and folding needs, but that's largely because I'm paranoid about IO, those two are what I learned first, and someone wrote an attoparsec compat shim for conduit so I don't have to.
One way to do this that's pretty fun import Control.Applicative import System.Random -- returns replacement if itemToReplace equals item, otherwise item replaceIf :: Eq a =&gt; a -&gt; a -&gt; a -&gt; a replaceIf itemToReplace replacement item | item == itemToReplace = replacement | otherwise = item -- Partially apply what we care about to make a new function that replaces 1 with 2 replaceIfOne = replaceIf 1 2 -- Returns 0 replaceIfOne 0 -- Returns 2 replaceIfOne 1 -- We can lift replaceIfOne and apply it things like lists pure replaceIfOne &lt;*&gt; [0, 1, 2, 3] -- Or Maybe's pure replaceIfOne &lt;*&gt; Just 1 -- Or any other Applicative! And this works on any equatable (`Eq`) type!
Indeed! I've added a new section to the post showing how we can use both ``Variant`` and the ``Either`` monad.
This is a great resource! What a fantastic idea.
Next steps would be binding to all functions / types here: https://github.com/JuliaLang/julia/blob/master/src/julia.h
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [JuliaLang/julia/.../**julia.h** (master → d76101d)](https://github.com/JuliaLang/julia/blob/d76101d0edad873a500206898809cf4de96dcd7b/src/julia.h) ---- 
The [`foldl`](http://hackage.haskell.org/package/foldl) library is great, and it can be used with any streaming framework. In Java terms, it is roughly analogous to [`Collectors`](https://docs.oracle.com/javase/10/docs/api/java/util/stream/Collectors.html).
Is there a transcript?
Yeah, probably should've clarified that. Thanks.
are you using intero? If you do you can use `M-x intero-restart` which is by default mapped to `(, i r)` (so if you are in normal-mode you just have to hit , then i then r to reload)
I'd be interested to know more about your thoughts on The Tyranny of Structurelessness. I found it very hard to read.
"why patterns failed and why you should care": [http://www.cs.uni.edu/\~wallingf/blog/archives/monthly/2018-10.html#e2018-10-05T15\_39\_10.htm](http://www.cs.uni.edu/~wallingf/blog/archives/monthly/2018-10.html#e2018-10-05T15_39_10.htm) [https://www.deconstructconf.com/2017/brian-marick-patterns-failed-why-should-we-care](https://www.deconstructconf.com/2017/brian-marick-patterns-failed-why-should-we-care) \[video\] \- patterns failed to live up to their promise or to the goals of the programmers who worked so hard to introduce Christopher Alexander's work \- the building blocks from Alexander's book are much richer. There are many, many more of them -- and they're interconnected in various ways. \- *the analogy I want to make is, basically, the design patterns in the book* ***Design Patterns*** *are at the same level as the "building blocks" in the book* ***A Pattern Language****.* in Design Patterns, there is little interconnection between these patterns. So what that pushes you toward is a very mechanistic implementation-focused application of the pattern. &amp;#x200B;
A plain functor would work just as well though.
I think the section about the Pipeline/Monad equivalence could use some work. The first example, as you note, is trivial, and really is just function composition, or at most another Applicative. You try to address this in the second section, by writing &gt; [...]we are free to implement the andThen operator in any way [...] as long we maintain [...] the monad laws. ...followed by an example implementation that breaks the monad laws! You can't have a counter on the number of binds, because that breaks the law that `(&gt;&gt;= return) === id`. Personally I think the reason you're having trouble finding a good example here is that Pipeline is in fact just boring old function composition, and not any very interesting Monad.
You really can't tell when haskellers are dead serious or just having some lighthearted fun.
Thanks for the response and upvotes, which have fuelled some much-needed maintenance, see updated news link on the front page. I have also set up a Liberapay button and a baby-steps funding goal. More progress soon I hope.
&gt; Codes of Conduct are meant to exclude people. No, they're meant to exclude behaviour.
&gt; I’m sure that API improvements could be made to make this style more accessible and remove some of the lens knowledge prerequisites. I [made a version](https://www.reddit.com/r/haskell/comments/9vbfpk/an_answer_to_the_trouble_with_typed_errors/e9bl0ap/) which doesn't use prisms and also produces nice residual constraints after handling an error, unlike the generic-lens solution which produces hairy generics-based constraints.
I am not currently using intero, but I have in the past. And from what you're saying just switching would fix my problem. Thanks! 
I think I understand and agree with both sides here. But it seems that existing CoCs (again?) leave this item open, at least as judged from the written words. I cannot spot clear statements addressing the scope. Maybe I overlook something. But assuming I don't, I am not sure this is the best approach - leaving it not addressed just gives the worst of both worlds. The people that hope of protection by the CoC, they might not get it because the scope ends up being too small. The people that fear unforeseeable judgement for well-intended behaviour in private communication channels, this merely adds to uncertainty. As it stands, it (again) all hinges on the judges. Those in power to enforce the CoC. And yes, you can assume those to be fair, objective and reasonable. But it seems to be a rather risky, naive, and strangely un-democratic assumption.
Heroic :D
On the other hand, I feel like it helps me wrap my head around OO design patterns. "Ooooh that's what dependency injection means! Duh, that's much simpler than what I thought it'd be..."
yeah, tbh, type level lists and rebindable syntax is exactly what I meant by clumsy :\
&gt; Insofar as you are implying that this criticism is not constructive. I'm holding a conversation and asking clarification on his take on the subject. Do you think this is implying his criticism as not constructive? I'm not dismissing his concerns, I'm directly taking them to try to change his view. &gt; This already contains two logical mistakes. Not at all. You are missing the point, I'll bold what I change so you can clearly understand why I'm not implying anything. &gt; Why could you not also develop a thicker skin so you ignore when you are called a racist and etc when you know you are taking the necessary steps for not **seen as** one? You don't have to imply that the label is true, just that a sincere poster has found reason to label him one. That doesn't make him right, but that doesn't mean you can adjust your behavior. That also doesn't mean that you have to. Your second 'mistake' is also missing the point. I have not said that it counteracts. I'm not trying to make a judgment of the consequences of the CoC. I'm nowhere near that subject. The objective of my comment, and that phrase, is to point out towards a compromise between having a CoC and people getting tougher skin. He has concerns with the CoC, that's fair, let's talk. However, if he thinks reasonable to ask everyone else to have tougher skin, it's reasonable to include him too. Worst case scenario, he should accept been offended by someone using a weaponized CoC. Note that I'm also nudging that his thinking, while not one, is close to be paradoxical/hypocritical. But it's only a nudge, I'm not willing to deconstruct his arguments, judge the validity of his concerns and fears - as this requires me to do the same about the CoC, which I haven't. This does not stop me from being curious on OP pov on the CoC (first question) and a more balanced approach (second question). 
&gt; There are people who are harmed by the use of such slurs and we don't want to participate in communities that normalize such behaviors. The main reason it's signaling is because when you try and protect others you are in part doing so to show others you are a good person that cares. I'm simply taking the neutral stance and saying less control over what people say is better because not only will there be no risk of abusing this power, but also in general, people will defend others without the need of them having to ban people or punish them. 
I think you may find this FAQ enlightening: https://www.ashedryden.com/blog/codes-of-conduct-101-faq In my opinion there is no neutral ground. You either support having a CoC and the benefits it entails or you do not.
I hate the whole thing, the current docs look better. Technical feedback: Typeclass instances are expanded if the user has JS disabled, this is the opposite of the current behavior.
Pretty sure that code is equally valid for ints, floats, or doubles, so it can't infer the type of the literal.
Honestly, I think this is being brigaded at this point. The poster you're replying to is apparently a frequenter of KiA (as is qci), and a few others. Just look through their posting history see exactly how 'objective' they're being. AFIACT this is straight up concern trolling.
Yea but the error message could use improvement.
(Sorry if my reply feels too much like arguing semantics. Hopefully there is something of substance to be found in it.) Though as a non-native speaker I might be missing some nuance, I always understood thick-skinned as being primarily an internal attribute; a matter of how easily one is made upset by criticism and unpleasantness. While external behvaiours of the sort you describe certainly are associated with it, I'd say in this context they feature mainly as consequences of subjective dispositions. To put it in another way, mechanisms of conflict resolution are can be looked at objectively. If leaf_cutter were concerned with them alone, they could raise the matter without reference to people "developing" dispositions (as they have done elsewhere in the thread, but not in this specific comment). &gt; Demanding a thick skin ("or else they create the mess of a situation") is victim-blaming. It cannot be demanded. If I am hurt you cannot tell me not to react in some way for the benefit of the community, but at my cost. All we can do is make it a vague wish, and perhaps a personal goal. Indeed. In hindsight, that is why I chose to describe leaf_cutter's stance as "you suggest that people should [...]", and not "you demand that people [...]", as the latter felt too harsh an interpretation. Still, the vague wish, however attenuated, remains being a normative claim, referring to how things should be.
I just tested this with GHC 8.4.3 in ghci and it gives me the type `cube :: Num a =&gt; [[a]]` for the first example. Perhaps you're using an older version of GHC? 🤔
&gt; The CoCs are a sign of mediocrity, lack of creative genious, stupidity. I disagree. &gt; It is the source of alambicated and Netflix-grade conversations like this that fill the time of people without real interests except socializing and having job security. I disagree. Locking your doors is not a guarantee that no one will break in to your house, but it sure does act as a good deterrent. Analogously, I do not presume that a CoC is capable of eliminating all offense, intentional and unintentional. I do presume that an enforced CoC will help deter offensive behavior. This blustering argument that "CoCs are written by those lacking talent" is uncompelling.
I'm interested in getting this, but it would be great if I can get sample chapters to see if I really like the content.
 -- we can't index the Variant by -- type because `a` is ambiguous, -- so we do it by index explicitly This, plus rebindable syntax, are both what I would consider "clumsy". Peppering your code with `liftFlowT` at the term level, or with `:&lt;&lt;` at the type level, is also what I would consider "clumsy". Of these options, at least `liftFlowT` is comparable to `lift` clumsiness that Haskell programmers are probably already used to when dealing with monad transformers. To be fair, peppering your types with `AsType Blah e` constraints is also clumsy. Tangential: resorting to RebindableSyntax is one of those things that is symptomatic of Haskell not having a "real" macro system that enables syntactic abstraction. This is one of the things I would like to steal from Racket, and one of the reasons I love the idea of Hackett so much.
I don't hate it but I would prefer the current one too
I already pointed at the case in some Linux CoC thread and I would like to leave the guy alone. I don't know if I am doing harm for him or not. Yes, you understood correctly with the risk I mean. Many people have jobs in their real life. And it's a risk to participate in public groups where many persons watch you closely and have a tool like a CoC to measure your behavior. I use the term SJW non-ironically. I have been affected 2 times personally by SJWs because of my stance against censorship. These type of people install things like CoCs and don't obey them themselves. This is the real irony.
&gt; this is a shit example 195 ... Imagine a concurrent application which which needs to acquire some locks, but &gt; 196 will deadlock if it attempts to acquire the same lock twice lol
&gt; KiA What's that? &gt; Also note the huge number of new-to-/r/haskell posters... Including yourself, apparently, and your account seems to be only 3 days old, unless I am misunderstanding Reddit's interface ...
Neither `Float` nor `Double` has a `Bits` instance.
I like the design, also it works better on smaller devices, but instances are overflowing the screen on eg horizontal phone. I would add a scroll rect there for horizontal scrolling
Absolutely agree. An editing-complete final draft sample chapter would be super useful.
KiA = "Kotaku In Action: The almost-official GamerGate subreddit!" &gt; Including yourself, apparently,... Nah, I'm not new around here, I just periodically delete all my comments. (Check the account karma.) ... and besides my statement remains valid whether or not my account is new around /r/haskell :) It's not, but obviously having deleted my comments, I can't prove it to you. (There are services which archive deleted comments, but whatever. I also create a new account once in a while, so...)
"Many people". You've pointed at one case... maybe two?
This is pretty easily solved by just using type annotations for your function definitions themselves, like so: import Data.Bits cube :: [[Integer]] cube = [[ a `adj` b | b &lt;- [0..7]] | a &lt;- [0..7]] where adj :: Integer -&gt; Integer -&gt; Integer a `adj` b = if elem (xor a b) [1, 2, 4] then 1 else 0 This avoids most all ambiguous type errors and is pretty standard practice in Haskell. It's more readable, to boot. If you so desired, this "problem" actually lets you make this code more polymorphic than just integers, with just a change of type signature: {-# LANGUAGE ScopedTypeVariables #-} import Data.Bits cube :: forall a. (Bits a, Num a, Enum a) =&gt; [[a]] cube = [[ a `adj` b | b &lt;- [0..7]] | a &lt;- [0..7]] where adj :: a -&gt; a -&gt; a a `adj` b = if elem (xor a b) [1, 2, 4] then 1 else 0 There's really nothing wrong with the design here.
Quickjump is already supported by the current released version of haddock. If you have GHC &gt;=8.4 and build haddock's with `--quickjump` (or by setting it in you `~/.cabal/config`) you can already build docs with it enabled. It's just the current docbuilder doesn't use it yet, sadly.
That does indeed sound clumsy. A couple of times I've wondered why we can't parameterize extensions. So, for example, `RebindableSyntax` would essentially be represented as ```data RebindableSyntax = RebindableSyntax (forall a b. m a -&gt; (a -&gt; m b) -&gt; m b) (forall a b. m a -&gt; m b -&gt; m b)` and you'd then use the extension like e.g.: ```{-# LANGUAGE RebindableSyntax myBind myBind_ #-}```
I commend your effort to use all the horizontal space; but there are horizontal overflows even for a 24" screen (e.g. `Applicative ZipList` instance in the docs for `Control.Applicative`) The colours of the "Synopsis" button clash with the general colour scheme; the placement of this button seems off. My general feeling is that the style seems to be better than the current one, but some tweaking is still in order.
Implementing some kind (pun not intended) of row polymorphism will be my bachelors thesis. To make the work help more people than just me, I want to discuss the design first. Please leave feedback there
Here's what disturbs me about codes of conduct. I've seen over and over in this thread that the people *advocating* a CoC are themselves behaving worse than anyone else. There are unfounded allegations of racism, sexism, homophobia and transphobia being tossed around at people for airing their concerns about the necessity of a code of conduct. You in particular are mocking someone whose concerns and phrasing suggest he or she may not neurotypical, and you top it off by calling that person a *fucking nitwit*. I am not racist, sexist, homophobic, transphobic, or rude, and I do not harass. I admit that I may not be attuned to /r/haskell's or Stack's racism, sexism, homophobia, or transphobia, rudeness, or harassment because I am a white male and when I am harassed I don't particularly dwell on it. If it exists, appropriate redress should be given. But I've spent a lot of time here, and I can't recall a single instance of it. I've always felt Haskell's was hands-down the most welcoming community, which is a major part of why I lurk here all the time. On the other hand, Node's is among the most toxic, and it's very often because people abuse their positions, not infrequently using the CoC as justification. I've seen people pilloried on the Node IRC for using the term *guys*. Is that *really* appropriate? My girlfriend refers to girls as guys. My mother does, too. It's a gender-neutral term, at least as far as my immediate peer group is concerned—but it can get you banned there. Elm's Slack enforces an identical rule (though I'm not sure of the consequences). I have never violated these rules. I don't intend to. I understand the spirit that led to their creation, and I want to call it admirable. I absolutely believe in inclusiveness. But is it really commensurate punishment to ban someone for something so innocuous? Is it fair to call them a *fucking nitwit* for disagreeing with you on a CoC? Or are you using morality as a false justification for your incivility and power tripping? I would prefer not to go down that road. It's my experience that enforced etiquette creates tension and unfamiliarity, and it's particularly condescending when the people bound to it weren't behaving poorly in the first place. It's downright toxic when people use it as a justification to toss around ad hominems and obscenities. If the Haskell community resembled a YouTube comment section, I would be demanding a CoC. I am absolutely in concord with the ideals of nondiscrimination and inclusiveness. But it's not; it's a small, welcoming community, and I don't appreciate being condescended to.
Here's what disturbs me about codes of conduct. I've seen over and over in this thread that the people *advocating* a CoC are themselves behaving worse than anyone else. There are unfounded allegations of racism, sexism, homophobia, and transphobia being tossed around at people for airing their concerns about the necessity of a code of conduct. You in particular are mocking someone whose concerns and phrasing suggest he or she may not neurotypical, and you top it off by calling that person a *fucking nitwit*. I am not racist, sexist, homophobic, transphobic, or rude, and I do not harass. I admit that I may not be attuned to /r/haskell's or Stack's racism, sexism, homophobia, or transphobia, rudeness, or harassment because I am a white male and when I am harassed I don't particularly dwell on it. If it exists, appropriate redress should be given. But I've spent a lot of time here, and I can't recall a single instance of it. I've always felt Haskell's was hands-down the most welcoming community, which is a major part of why I lurk here all the time. On the other hand, Node's is among the most awful, and it's very often because people abuse their positions, not infrequently using the CoC as justification. I've seen people pilloried on the Node IRC for using the term *guys*. Is that *really* appropriate? My girlfriend refers to girls as guys. My mother does, too. It's a gender-neutral term, at least as far as my immediate peer group is concerned—but it can get you banned there. Elm's Slack enforces an identical rule (though I'm not sure of the consequences). I have never violated these rules. I don't intend to. I understand the spirit that led to their creation, and I want to call it admirable. I absolutely believe in inclusiveness. But is it really commensurate punishment to ban someone for something so innocuous? Is it fair to call them a *fucking nitwit* for disagreeing with you on a CoC? Or are you using morality as a false justification for your incivility and power tripping? I would prefer not to go down that road. It's my experience that enforced etiquette creates tension and unfamiliarity, and it's particularly condescending when the people bound to it weren't behaving poorly in the first place. It's downright toxic when people use it as a justification to toss around ad hominems and obscenities. I am absolutely in concord with the ideals of nondiscrimination and inclusiveness. If a CoC can be crafted such that it ensures that people who would abuse it never get the chance to, all the better. If the Haskell community resembled a YouTube comment section, I would be demanding *any* CoC. But it's not; it's a small, welcoming community, and comments like yours make me very leery of giving up self-moderation because of what I've seen occur in other communities.
Lack of user-friendly treatment of row-oriented data types (a la SQL tables with non-unique field names supporting various data transformations) has been a sore point for me preventing me from using Haskell more, so improvements in this area would be a relief. &amp;#x200B; I didn't see mention of the following operations (my own terminology) in the proposal. Do you consider them? * projection -- extract a subset of the fields and without explicitly defining a new type for the result * concatenation - join two tuples together that have non-overlapping field names * expansion - given a field that is a list, \[a\], expand the tuple into a list of another tuple type where that field becomes simply a, e.g. (\[1,2\], 'x') -&gt; \[(1, 'x'), (2, 'x')\] without the field names. I think the idea can be generalized beyond lists, though.
cool - never noticed - thanks!
Uhm, yeah, numeric literals are overloaded in Haskell. It's actually a good thing, and also explained on page 1 of *every* book/tutorial on the language. What's the problem again?
1. If you take a look at my PoC, you can see the `RowCons` constraint, that given a row and a label will return the type of the label and the rest of the row. You could just create another class wrapping it to project a list of symbols 2. There is also a merge constraint 3. Not sure what the exact behavior is that you want, but I am pretty sure you can achieve it with a typeclass and `RowCons`
The suggested syntax for rows is very similar to tuple syntax, which may be confusing. Admittedly, I haven't found any scenario where there would be ambiguity issues with GHC. The closest I can get is type Foo baz bar = '(baz :: Bool, bar :: Int) type Foo bar baz = (baz :: Bool, bar :: Proxy baz) would be a row under the suggested syntax. Granted, this *is* a parse error in current GHC, but it's still worryingly similar to type Foo bar baz = ((baz :: Bool), (bar :: Proxy baz)) which is accepted by the compiler as having kind `:: 
Wow. This is *exactly* the proposal I want, right down to the api and runtime rep. Perfect!
Yeah, that's why I have it in the open questions section. At the moment it is just the PureScript syntax
Correct me if I'm wrong, but I'm on mobile. Does \`adj :: Int -&gt; Int -&gt; a\` work? The \`if\` expression seems to completely decouple the type of \`adj\` vs \`cube\` so you could just use an efficient internal type such as \`Int\`.
RebindableSyntax is actually way more flexible: the operators can have far more general types. That's a large part of its use.
&gt; But recently, I had a “straw that broke the camel’s back” moment. Someone I like and respect expressed opinions that I think are antithetical to healthy community growth. I won’t call anyone out or refer to those discussions, I'll go out on a limb and suspect that if we knew the specific discussion you're referring to it would turn out to be a completely harmless and friendly discussion. And that person you liked and respected was likely just openly discussing in a brutally honest way the relative merits and deficiencies of build tools or something related and you didn't like what they were concluding or postulating. Am I off-base with that guess?
Pardon my ignorance, but this is like, one of the most wanted features since ages, and now you come, and say you want to do this as a part of your bachelor thesis? Where's the catch :D (I'm not being negative, I just don't have enough technical knowledge to determine what the potential problems/shortcomings would be)
Oops, I should get rid of that. That's the very first draft I published. I'll look into getting a few sample chapters up today. Thanks for your interest!
I am not the author.
Looking forward to reading this! When I get type-level type errors I am never quite sure whether it's ghc or my code is weird. For instance when I tried to write some machinery to make defunctionalized type functions easier to curry: type family PAPType t (ls :: TList t) where PAPType (a -&gt; b) (TCons (v :: a) xs)= PAPType b xs PAPType a TNil = a data TList xs where TCons :: x -&gt; TList b -&gt; TList (x -&gt; b) TNil :: TList b type family Apply (ls :: TList tf) (f :: tf) :: PAPType tf ls where Apply 'TNil f = f Apply ('TCons x xs) (f) = Apply xs (f x) Which works fine: Apply ('TCons 'True ('TCons 'False 'TNil)) (==) :: PAPType (Bool -&gt; Bool -&gt; Bool) ('TCons 'True ('TCons 'False 'TNil)) = 'False But trying to add a defunctionalized version: type Exp a = a -&gt; Type type family Eval (k :: Exp a) :: a data CurryN (f :: a -&gt; r) (ls :: TList r) (v :: a) :: Exp (PAPType r ls) type instance Eval (CurryN f ls v) = Apply (TCons v ls) f Gives fun type errors that I don't understand: • Illegal type synonym family application in instance: PAPType b ls • In the type instance declaration for ‘Eval’ | 182 | type instance Eval (CurryN f ls v) = Apply (TCons v ls) f | ^^^^ 
&gt;I hate the whole thing, the current docs look better. This is simply not good enough feedback. Its first of all rude to the people who have invested time and energy into doing this work for us. Second of all it is not actionable. If you want to contribute valuable feedback you have to give constructive feedback.
Don't get me wrong, I won't be able to finish this for my bachelor thesis, maybe I will only be able to write about different implementation approaches and their pros/cons. But even if so, I will continue afterwards
Holy cow that looks fantastic! The only thing I could see tweaking would be the amount of margin space.
Only the first argument of `adj` needs to implement `Bits`
That looks pretty nice! I like this implementation more, partially because one is probably already using MTL-style constraints in their codebase, but maybe not classy-prisms and generic-lens. Besides the usual MTL complaints, is there any downside to this approach? If not, why isn't something like this used more? Or is it just that people are waiting out for a more standardized built-in version before doing anything at the library level, and that bleeds out to the application level too?
And [here is the BIO version](https://gist.github.com/gelisam/187f54494da03855890698ed96c65bef) :)
&gt; Besides the usual MTL complaints, is there any downside to this approach? We'll have to try and see! This is fresh off the oven, I haven't had the opportunity to try it on a real codebase yet. &gt; If not, why isn't something like this used more? ...because I just invented it?
No offense taken, and honestly I think you take it too far if you police yourself this much. No need to apologize. I am clumsy with my words all the time, most non-native speakers are, whenever I try to explain myself after unintentionally making an offensive remark I make things even worse. I know people expect me to try my best, and I do, but I also expect them to not go out of their way to find offense in everything I say, I know I don't. &gt;there doesn't seem to be much ground to claim ... I grant you this, I can only have my suspicions what motivated the downvotes (some of which might have been lifted after my remark). &gt;the issues CoCs attempt to address (...) are rather more serious than downvotes Precisely, the issue (where it exists) is serious and I would say much more complex. That is why I accept simple rules to govern usage of the downvote button, while I reject attempts to codify usage of language. &amp;#x200B; I am really not aware of anything that would justify the need for, in my opinion inappropriate, regulation of language in this community. I am not denying there are accounts of events that would change my mind, I have not been presented with one. Having said that, I might be persuaded that there is a problem in this community, and still I could continue holding my belief that CoC is not the right way to deal with it. &amp;#x200B; I am concerned about the effects of CoC and I would rather people stopped taking offense at every opportunity. Obviously extreme cases like threats are entirely different matter from taking an arrogant tone with someone.
Can we have type level sets instead of just rows as that would be more general? Rows can just sets consisting of pairs of labels and types.
My main point was and is that "thicker skin" is rather different from "adopting someone else's communication style". I don't see that being refuted. Whether "thick skin" is a disposition seems to be an unrelated question. Similarly, whether my "vague wish", or the original phrasing, is a normative claim (wishes are not necessarily normative claims) (and whether making normative claims in general or in this specific instance is a good thing) seems to be unrelated. (And to be very clear, "unrelated" is not meant to imply "unimportant". These are important questions, and I may certainly also concede that a normative claim, if it was one, even though valid, may have been inappropriate in the original context.. but I will have strong requirements for criticism of a statement that starts with a plain "i really wish..".) I will nonetheless reply to the "disposition" topic: &gt; While external behvaiours of the sort you describe certainly are associated with it, I'd say in this context they feature mainly as consequences of subjective dispositions. &gt; .. they could raise the matter without reference to people "developing" dispositions .. I am not sure what you mean by term "disposition" here. My own direct understanding would be an internal, unchanging property. Wikipedia disagrees: "A disposition is a quality of character, a habit, a preparation, a state of readiness, or a tendency to act in a specified way __that may be learned__." (emph. added). Yet you too seem to consider it hard to affect (?) Regardless: I think we can work on our "thick skin", we can train ourselves to be less affected, and that training can result in it taking less mental effort to not let our emotions affect our external behaviour. Claiming otherwise also risks belittling the efforts of people that _do_ spend that mental effort at what they may very well describe as "training their thick skin".
Some people just like to complain about Haskell tbh. Looks like this person missed the bandwagon for complaining about error messages though.
&gt; I think there must be something really wrong with the language design here. I don't know exactly what it is, but I think someone must have made the wrong tradeoff at some point. Geez that's uh... unfortunately smug. Nope. Turns out fiddling with Haskell for 5 minutes does not 
Reformatted for old reddit: class IsIntegralLiteral t where fromInteger :: Integer -&gt; t class IsIntegralLiteral t =&gt; IsFloatingPointLiteral t where fromRational :: Rational -&gt; t --- newtype SocketRetryCount = SocketRetryCount Int deriving (IsIntegralLiteral) newtype Temperature = Temperature Float deriving (IsFloatingPointLiteral) --- class IsIntegralLiteral t where fromInteger :: Integer -&gt; t fromIntegerTH :: Integer -&gt; Q (TExp t) fromIntegerTH i = [|| fromInteger i ||] -- Example instance IsIntegralLiteral SocketRetryCount where fromInteger i | i &lt; 0 = error "failure" | otherwise = SocketRetryCount i fromIntegerTH i | i &lt; 0 = error "failure" | otherwise = [|| SocketRetryCount i ||] --- I, too, would like a way for literals to signal a compile-time failure if they are given a monomorphic type and the value is unsuitable for that type. I'd like `128 :: Int8` to fail. I'd like `(-1) :: Word8` to fail (although this might require additional changes). I'd like `"∀" :: ByteString` to fail even when `OverloadedStrings` is on. I know this won't catch many of the overflow/underflow issues with types, but I think they would catch some "facepalm" errors and make the use of Num / Fractional / IsString for range-limited newtypes more appealing.
You're not dreaming; I've just recently started working on a toy project (an alternative Prelude) which does just that. Using `RebindableSyntax`, integer literals will be interpreted with the `fromInteger`, `negate` and `fromRational` in scope, making it "Just Work". Though the literals might not be super useful as you have them—in my version the `IntegerLiteral` class has a somewhat `Num`-like superclass constraint. Also, I've never looked at it myself, but you might be interested in the validated-literals package.
Most of what you want is already done in ghc (at least 8.4): ``` Prelude Data.Int Data.Word Data.ByteString&gt; :set -Wall Prelude Data.Int Data.Word Data.ByteString&gt; 128 :: Int8 &lt;interactive&gt;:18:1: warning: [-Woverflowed-literals] Literal 128 is out of the Int8 range -128..127 If you are trying to write a large negative literal, use NegativeLiterals -128 Prelude Data.Int Data.Word Data.ByteString&gt; (-1) :: Word8 &lt;interactive&gt;:19:3: warning: [-Woverflowed-literals] Literal -1 is out of the Word8 range 0..255 255 Prelude Data.Int Data.Word Data.ByteString&gt; "∀" :: ByteString "\NUL" ```
Thanks for your hard work! Type-level programming is one of those foreign frontiers to me, I'd love to understand it better.
&gt; account seems to be only 3 days old Closer to 3 months. Still, not exactly one of the old guard, but unlikely to be a single-issue interloper.
&gt; You're not dreaming; I've just recently started working on a toy project (an alternative Prelude) which does just that. Using RebindableSyntax, integer literals will be interpreted with the fromInteger, negate and fromRational in scope, making it "Just Work". How do you insert the compile time check? Could you give a link to your alternative prelude?
I don't, I just meant my prelude has the literals typeclasses. It's also in very early stages, so not online anywhere. Re the compile-time checks, I was guessing validated-literals has something for that.
Though now that I think about it more, I do have class [...] =&gt; NegativeLiteral a where negate :: a -&gt; a If you don't make your data-type an instance of `NegativeLiteral`, then I think trying to use a negative literal as a value of that type actually should be a compile time error already.
&gt;I've seen people pilloried on the Node IRC Again with the far fetched cases no one ever heard of. &gt;Is it fair to call them a fucking nitwit for disagreeing with you on a CoC? You are telling me, you did not get the point of giving someone example of what CoC is meant to prevent? 
Yup, the definition of "disposition" I had in mind is the one you found in Wikipedia; dispositions can be acquired indeed. My point was that the process of acquiring a disposition is internal; it amounts to changing oneself. More concretely: we can work on our thick skin; the question would be about when, and to which extent, this "can" becomes a "should", or a "must". On another note, you have a point about wishes not necessarily being normative. I'll have to think about it.
Very cool. Coming from a Java setting, I find myself thinking a lot in terms of "how would I be able to represent this Java code more concisely in Haskell" or conversely "how would this Haskell code look in Java, or would it even be possible", especially if I'm trying to explain some concept to coworkers. This is like a well-formatted compilation of those ideas! I think it's very useful to people coming from OO that want to de-mystify and "rationalize" functional tools. That being said, would you be amenable to outside input in this project?
The catch is it is not easy to integrate it with the rest of the haskell type system. 
Hell yeah :D :D :D
Speaking of syntax, maybe the sugarfree syntax is not so bad. I imagine data Field = Symbol ::: Type type Row = [Field] data Record (r :: Row) = ... Then you can write type MyRow a = '["foo" ::: Int, "bar" ::: a] If you want to ensure that order doesn't matter, instead type family AsRow (fields :: [Field]) = ... -- sort fields type MyRow a = AsRow '["foo" ::: Int, "bar" ::: a]
&gt; hit M-x (there is an actual spacemacs shortcut too - I think space-x? The evil-mode spacemacs equivalent of M-x is `SPC SPC`. (SPC = space) 
The spacemacs Haskell layer has an option for using intero with it. I'm not at all familiar with it, so I'll say no more than that.
I've encountered the same issue using the spacemacs Haskell layer, and I'm not sure what the solution is. Seems like there should be an issue tracker for the Haskell layer. The [general spacemacs issue tracker](https://github.com/syl20bnr/spacemacs/issues) seems quite cluttered. 2000+ open issues.
Could you provide a brief comparison of the duplicate label scheme vs disjointedness constraints in the style of Ur/Web? My understanding is that - 1. When you're representing data, you do not accidentally want duplicates in the record. 2. If you're doing things with effects like Koka, you want to allow duplicate labels. 3. Adding disjointedness constraints can add a lot more complexity to type signatures. However, that can be alleviated to some extent by using identities for type-level map and (++) in the type-checker.
It's in/was in alignment is other overloaded literals. I'm not sure when overflowed-literals was added, but when I was learning Haskell, stuff like `10264 :: Word8` didn't even emit a warning and truncated to the lower 8 bits, and that's basically what the `IsString` instance for `ByteString` does; it truncates each literal `Char` to the lower 8 bits. When `Num` and `Fractional` and overloaded literals first were introduced, I don't think even `TemplateHaskell` existed. And, that -- or something better -- is basically what we need, so that we can handle the case where the literal is out of range (or invalid for some other reason) as an "earlier pass" and bail the compiler with the error. Something so that you'd define `fromIntegerSafe :: Integer -&gt; Either LiteralInvalidForType a` in the typeclass and `1` would be desugared into `$(qFromInteger (1 :: Integer))` where `qFromInteger` was some `Integer -&gt; Q Exp` TH provided by GHC that would call your `fromIntegerSafe` and raise a compiler error if it returned a `Left _`. You could go further an have some sort of "raw" literal form (`Vector Char`?) which might even vary by which literal grammar was parsed, so that you could the conversion from characters recognized by the parser to an `Integer` if you wanted to replace it, but I suppose that's a separate issue.
There is https://github.com/etorreborre/registry by u/etorreborre. 
You're right that standard functions accomplish dependency injection pretty much by default. [Here's](https://github.com/thma/LtuPatternFactory/tree/1be9521591f8a7f06b35a7ae5929e7beaf0b8e43) a great reference for translating OOP design patterns to their counterparts that arise naturally from FP.
Thanks for making this. Purchased.
&gt; two lines of code to build an adjacency matrix ahahaha nothing is ever straightforward, you see, because i had to add a single type signature
I bet it'd be more characters of types than we have characters of implementation
Hah, that's a terrible error message. The problem is that there is an invisible kind argument which matches on a type family application. Try writing an instance for ``` type family Eval a (k :: Exp a) :: a ``` instead and you'll see the problem. GHC really ought to show this invisible argument, at least when `-fprint-explicit-kinds` is enabled, but apparently it doesn't...
C'mon, just add a type signature for your top level functions and this problem will disappear 99.9% of the time...
Very exciting! One thing I will say is I'm not a huge fan of the `printName { name }` syntax. The main places in which I have been feeling the most pain from lack of extensible records is in places where I'm dealing with a variety of DB/Api types, in which case there will almost certainly be multiple in scope at the same time, so the above syntax will lead to some combination of name collision and readability issues (is this the organization's `name` or the person's `name`?). I'd personally quite like standard dot notation, as it would fit with other languages, and fit conceptually with qualified module usage (the thing to the left of the dot defines the name resolution of the thing to the right of the dot, rather than traditional global resolution). `printName x = putStrLn x.name` `printName = putStrLn . .name` This syntax should also work for newtype wrappers on top of these data types, and it should be possible to define your own fields at will, so that abstract data types can enjoy the same ergonomics.
Bachelor thesis: compare pro and cons of various approaches. Master/PhD actually implement this in ghc. That would be a couple of nice and very good thesis
I heard that enabling `--profile` makes the compiler to compile without optimizations (`-O`). However, if you have an unoptimized part of your code in the optimized version, it is high likely that it will appear in the unoptimized one too.
Is there an actual argument in this snark?
&gt; Can we have type level sets Yes, please! &gt; Rows can just sets consisting of pairs of labels and types. Nope. This representation would allow you to use the same label twice with different types. Instead, I'd also ask for type-level maps! What I'd want for this in particular is a type-level `Map Label Type`. &gt; Also the tuple syntax represents order pairs, so it is quite confusing to use that for unordered fields in a record. Agreed; I'd prefer curly braces for this sort of thing.
&gt; As an experiment I implemented all of the semantics as a library, this would mean that the only changes to the compiler would be syntactic sugar (I'm going to start sounding like a broken record, but I've finally found the words to describe my sentiments so I'm going to keep saying it whenever it seems relevant.) This is a symptom of the Haskell language lacking a macro system capable of syntactic abstraction. If we had that, then this whole thing could be just a library.
Oh, that clears up quite a bit for me, thanks! Tried a bit to get an intuition for this but I think I found an actual ghc bug while doing so: TypeOf (k :: p (Any x)) = x type family TypeOf v where TypeOf (v :: x) = x data PlusT (v :: Nat) = Plus Nat Nat &gt; TypeOf (Plus 1 2) :: * &gt; = PlusT (Any Nat) type family TypeOf v where TypeOf (v :: Any x) = x &gt; ghc: panic! (the 'impossible' happened) &gt; (GHC version 8.4.4 for x86_64-unknown-linux): &gt; piResultTys1 &gt; k0_a1mLr[tau:1] &gt; [x_a1mHX[sk:1]] &gt; Call stack: &gt; CallStack (from HasCallStack): &gt; callStackDoc, called at compiler/utils/Outputable.hs:1150:37 in ghc:Outputable &gt; pprPanic, called at compiler/types/Type.hs:1002:5 in ghc:Type &gt; &gt; Please report this as a GHC bug: http://www.haskell.org/ghc/reportabug 
Yeah I thought about type level maps already, because in my PoC i am basically emulating a type level map with an assosiacion list. I originally wanted to implement a type level red-black tree, but the problem is that for the constraints I need a data structure that fulfills `orig ~ Insert s ty (Delete s (orig))`, which is not the case for any search tree. The precise type would have to be `Map Symbol [Type]` though to allow for duplicate labels Curly braces are for records (kind `Type`) the other syntax is for Rows (kind `Row k`). The record constructor has kind `Row Type -&gt; Type`
This is just one way to access records (called named puns, similar to the current GHC extension for that). You can still access the data with an accessor function (`@.` in my PoC - `rec @. #foo`), standard dot notation could be implemented too, but i am not a fan of your second example, it's easier to use `get #name` in this case. I am also not sure what I think about newtypes
I like to think that Haskell learnt to optimize me
A record only has each label once, a row can have it multiple times, there is an important distinction between the two. In purescript for example, there is a library for algebraic effects (purescript-run) that would not be possible if rows were limited to only distict labels
&gt; called named puns, similar to the current GHC extension for that Yeah I'm not a fan of that extension, all those named-field related extensions I kind of wish didn't exist. That second example admittedly was not the best one I could have chosen, but I think with decent syntax highlighting it would be quite readable. A better example might be `map .name people` vs `map (\p -&gt; p.name) people` or as you said `map (get #name) people`. I feel less strongly about this than the standard dot notation though. Although this stuff isn't all that important as long as the primitives are reasonable, as syntax sugar can be decided on later. The bigger thing I am curious about is what your thoughts are on my last couple paragraphs about handling different types of records and packing and strictness.
I worked on something similar. You can see it here: https://github.com/tkonolige/inline-julia
Mainly through experience, and by studying the underlying performance model and the operational details of how the runtime actually evaluates things—the STG machine in general and the GHC RTS specifically. A few of the main insights: * Lazy data structures (lists, &amp;c.) are bad data structures, but they’re great control structures; if you just want to store data, probably make it strict. * Evaluation is driven by pattern-matching; stack frames are created by thunk forcing, not function calls. `seq` doesn’t add strictness, it just arranges dependencies between thunks. * Lazy streaming is often just as good as, or better than, strict tail-recursive loops; depends on how you’re generating the input and consuming the output. * Writing performant code is mostly about arranging data dependencies in a way that’s easy for humans, and incidentally the compiler, to analyse. * You can always “write C in Haskell” for the high-performance stuff, and it’s no more unpleasant than a Regular Old Imperative Language—and often much more pleasant because you still have a nice type system that you can leverage without runtime cost. 
I'm trying to understand why `purescript-run` (or something similar) would require duplicate labels in a single "row type", but from my cursory reading of it, I still don't get it. Can you give (or point me to) an example that illustrates this?
Perhaps it's just my domain, for front-end / back-end web development it does not seem useful, as there are basically always more than one non-trivial type in scope at a time. So writing DB code or Miso views I need to be clear what object I am referencing.
Allowing shadowing of labels enables records to natively encode variable scoping. [More info here.](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/scopedlabels.pdf)
Section 2.4 in _[Koka: Programming with Row-polymorphic Effect Types](https://arxiv.org/pdf/1406.2061.pdf)_ might be more helpful than my own words. My understanding is that Purescript takes inspiration from Koka for its effect system.
&gt; This is a symptom of the Haskell language lacking a macro system capable of syntactic abstraction. If we had that, then this whole thing could be just a library and would require no changes to the compiler. Did you skip reading the immediately following text? :P &gt; but the big problem is compile times. For type equality it is required that the type level data structure that represents the row has a "normal form", so that forall orig label type. Has label type orig =&gt; Insert label type (Delete label (orig)) === orig. The data structure also has to act like a Map Symbol [k]. Originally I wanted to use a type-level red-black tree for this, but a binary search tree does not have such a normal form, so I had to use a sorted cons list. This means the type families used to implement lookup etc have to do O(n) expansions and not O(log(n)).
&gt; Again with the far fetched cases no one ever heard of. It was a point I made in good faith. I don't know why you can't assume it as such. I can't prove to you it happened, but it did. If you don't believe me about Elm, go type "guys" into their Slack channel and see what happens. &gt; You are telling me, you did not get the point of giving someone example of what CoC is meant to prevent? And you're telling me that you're using your own poor behavior as a justification for a CoC? Ban yourself from the forum then, because this is the most uncivil exchange I've seen on /r/haskell. &gt; Unlike you I do not need to go somewhere like Node IRC for examples and can actually show you haskellers in this reddit calling each other moron and idiot. I'll hold you to it: show me. I won't be surprised if you can come up with one or two instances, but it's far from a problem. The only time I recall is when Snoyman got into a row with some other Haskellers and SPJ wrote a note about treating one another with respect. In absolute terms, that didn't rise to the level of hostility you're displaying now, and I *still* don't think you should be banned unless you decide to make a habit of attacking people. I do think that you should reflect on what you've said, though. The exceedingly occasional display of rudeness doesn't demand a CoC. Humans are creatures of nuance, but such prescriptivist rules must be enforced without respect to context in order to be seen as fair. I would rather we dealt with rudeness by talking to one another like adults. As far as I've seen, that has been very effective here in the past; incivility has been met with censure, and people have adjusted their behavior. As far as I'm concerned, CoCs are useful for addressing the concerns of marginalized groups and not much else. To the extent that this applies to the Haskell community, I'd be much more open for discussion. &gt; I guess it is not as small and welcoming as you are imagining it. And I think you're exaggerating the problem to the extreme. I have never felt unwelcome here, on IRC, or on Github. I've been down voted occasionally for airing this opinion or that, but that's about it. Again, if you're a marginalized group feeling unwelcome because of discriminatory language, that needs to be addressed. But the sanitization of interpersonal communication and incessant victimhood is disturbing to me. It smacks of someone simply wanting to enforce rules on others for the sake of doing so. I'd admonish you to follow your own before prescribing other people's behavior, and maybe I could take the complaint more seriously
If you're looking for "efficient" use a data structure besides a list. `Vector`s are can vary in size and store packed data, while `Array`s are... arrays. 
&gt; I heard that enabling --profile makes the compiler to compile without optimizations Eh?
I think strictness in data structures that will 'persist' for any length of time is probably a smart default. The issue isn't as granular as "it's either a control structure in disguise or a bucket for data to retrieve later" but that line of thought is a good place to start from, and rarely results in outright bad calls. Ultimately it's basically about space or time complexity as your trade-off anyway, and that high level take can get you there 9 times out of 10.
https://wiki.haskell.org/Performance Can not seem to find but ran into a blog post from an engineer whose took a 13 hour python process and bench marked it in multiple languages. Ocaml was actually the winner.
I’m the author of purescript-run. One example is catching an error and throwing in the handler. Without duplicate labels you can only catch, but can’t throw again once you catch. 
You might also look at purescript-checked-exceptions as well. Often times you want to catch something, inspect it and dethrone if it doesn’t meet some condition. This is not possible without duplicate labels. 
Is \`:t\` a function? How do we do a \`:t\` on \`:t\` (i.e. how do we find the type signature of \`:t\`)?
No. It's a ghci command.
The cost center thing is totally real. It caused much confusion to [this SO user], who was trying to profile some code but kept getting incorrect results because cost centers broke everything. The cause is that GHC wants the results to "make sense". It refuses to move things out of or into cost centers. The linked example's a bit obscure, but I think that another example is easy to construct. GHC will probably refuse to apply fusion in `map f ({-# SCC oops #-} (map g xs))`, because it'd get `map (f . g) xs`, with no sane location for the cost center. You have to be very careful with `-fprof-auto`, because it does this kind of thing a lot.
The current docbuilder _does_ use it! Go to any recently built docs, e.g. http://hackage.haskell.org/package/btree-0.4.0/docs/BTree.html and hit the "s" key and it pops right up. You can also access it using the "QuickJump" link from the main page for a package. The new redesign makes it more discoverable.
Note the style switcher. If you like the current design, you can keep it. 
It’s not a Haskell function, but a GHCi command, short for `:type`. All GHCi commands begin with `:`, in order to differentiate them from normal Haskell code. If it *were* a Haskell function, it would have a type like `forall a. a -&gt; Type`. The closest in-Haskell equivalent is probably `typeOf :: (Typeable a) =&gt; a -&gt; TypeRep` from `Data.Typeable`, which provides runtime representations of type information. you can use this module to implement dynamic typing, such as `Data.Dynamic` or other dynamic type checks that may not be directly/conveniently expressible within Haskell’s static type system. 
Duplicate labels may not be that intuitive, but they do allow for some nice properties: - you can always extend a row `r` to `(thing :: t | r)`, no matter what `r` is - if you extended `r` to `(thing :: t | r)` and peel off the `thing`, you always get `r` back. This might come up (using variants rather than records) when implementing effects. The `purescript-run` catch-rethrow example boils down to: `(exceptions :: t | r)` is the allowed effects of the code whose exceptions are being caught. The effects present after you `catch` are just `r` (ie exceptions have been removed), and consequently the set of effects allowed in the handler is also `r`. If you want to rethrow (possibly different) exceptions in your handler, you need to be able to instantiate `r` with a row that already includes `exceptions :: s`. You could also imagine it being useful if you want to take some collection of (row-polymorphic) records, extend them with an extra element (e.g. an expensive hash which you want to cache) and then strip your extra metadata out when you return them. If you have duplicate labels you never need to worry about what happens when your temporary field name (which your type signature says nobody else should ever see) collides with a field name in the polymorphic record tail (which your type signature says *you* can't depend on). The benign uses for duplicate labels seem to all arise when you cause a collision by taking a type that extends a row variable with a label and instantiating it with a row that already has the label in it. Maybe you could produce a warning for values that have *manifestly* duplicate labels not hidden behind row polymorphism, which so far none of the useful examples involve? It'd be awkward to reduce some warning-free code by hand and suddenly get a warning, though.
To provide another example, profiling can break vector fusion https://ghc.haskell.org/trac/ghc/ticket/12893
[miso](https://haskell-miso.org) is very simple, performant and featureful. Combined with **servant**, it's a full-stack solution. The advantage of using Haskell front-end web frameworks (Reflex-DOM, miso, etc.) is that you get type-sharing on both client + server, that's something that's hard to come by in frameworks for other languages IME. This really shines when communicating between client / server, Generics-based serialization eradicates tedious boilerplate that plagues web dev. Another advantage is that you'll probably get to use the nix package manager.
[removed]
If you could just generate records from the type level list with the efficiency of normal records...
Aside from concrete performance tips, which [I wrote up here](https://github.com/haskell-perf/checklist), I think the question you're asking is how to generally _learn_ these things for yourself. I think that until you start applying a scientific experimental process, anything you think about performance will be between wrong and superstitious. So I would say in any language, Haskell, Python, Rust, whatever: 1. Look for the tools that help you **measure** your programs (profilers and benchmarkers and even tools like strace); if they aren't there, then make them, even something rudimentary. 2. You need to write your programs in stages: from the simplest possible baseline, and build up, measuring each stage. This lets you know at which level you introduced a change that affected performance, and from there you can investigate better. * For example, I wrote a simple program to walk across a 1GB (of PITCH high frequency trading data) file, but I started from the most basic function calls and built up from there: - Nothing : 40,960 bytes allocated in the heap - Open file and close file : 64,768 bytes allocated in the heap - Walk the whole file with a buffer : 151,456 bytes allocated in the heap - Counting and reporting chunks : 163,440 bytes allocated in the heap - Peek each one into a Word16 : 163,448 bytes allocated in the heap - Peek each unit and message count : 307,640 bytes allocated in the heap * Elsewhere, in the SDL2 package, I wrote a simple "bouncing box around the screen" and discovered a number of fundamental functions allocate unnecessarily, like this one: https://github.com/haskell-game/sdl2/pull/179 So, don't assume that anything is well optimized, even if it's claimed to be well-optimized. It might be your code, it might be the library you're using, or it might be the compiler, or something else in the OS. Measure everything so that you don't have to make any guesses.
Yes but in particular https://pl-rants.net/posts/haskell-opt-journey/ I also like the real world Haskell recommendation 
Another example of a Haskell zealot failing to recognize what is evident. I don´t want to discuss the details that would be long and boring and useless with some people, so i would jump to the conclusions: Haskell was made with people with very different points of view or by the same people with split personalities. They first created a nice syntax, as human readable as possible to manage the system F monster. Then they hired some lazy semi-human math phD autistic and psychopatic robots who hated humans to program the error system. They simple dump the error data and added some words. Some of his evident mistakes were fixed when ghci was programmed. Default types for example. Since then, some timid effort have been made to ammend the evident failed approach which is a complete disaster. There are good solutions for this mess but GHC has no competition, so there is a long way before a major fix.
Go to https://benchmarksgame-team.pages.debian.net/benchmarksgame/, pick a haskell solution that looks significantly slower than Java or OCaml, try everything mentioned above (+ Control.Parallel, of course) until it's much faster, upload the result.
To simplify newtype wrapping-unwrapping, perhaps he could make use of [`coerce`](http://hackage.haskell.org/package/base-4.12.0.0/docs/Data-Coerce.html): import Data.Coerce matrix :: [[a]] -&gt; ZipList (ZipList a) matrix = coerce unmatrix :: ZipList (ZipList a) -&gt; [[a]] unmatrix = coerce And using the extra functions from [coercible-utils](http://hackage.haskell.org/package/coercible-utils-0.0.0/docs/CoercibleUtils.html_, he could write: matrixplus :: Num a =&gt; [[a]] -&gt; [[a]] -&gt; [[a]] matrixplus = under2 matrix $ \m n -&gt; (liftA2 . liftA2) (+) m n Which is slightly more elegant because you only have to give the packer, not the packer and the unpacker both. 
Would this support row polymorphism in sum types too, like in OCaml's [polymorphic variants](https://v1.realworldocaml.org/v1/en/html/variants.html#polymorphic-variants)?
Yes, a row can also be used to implement a variant. For example there purescript-variant that does exactly this.
Take a look at Magicbane as well: https://github.com/myfreeweb/magicbane/blob/master/README.md
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [myfreeweb/magicbane/.../**README.md** (master → 932dec7)](https://github.com/myfreeweb/magicbane/blob/932dec780451dfdb1a473fd1a4e80886864ddcd1/README.md) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply e9eyi99.)
I actually agree with this one. The Monad instance for functions (and less often for tuples) has bitten me a million times. Every time I forget a parameter to a function before binding it's result in a do block, I get to play this fun game finding looking at each binding in turn manually and checking where the missing param is. And that's only because I got used to the pattern in the error messages that tells me this is the issue. I have never deliberately used the Functor/Monad instances of functions. On the other hand, I've wanted the Num instances a couple of times but those aren't there.
[except that it calls itself one](https://nixos.org/nix/)
And you can generate a bunch of code for the FE you choose using: http://hackage.haskell.org/package/servant-purescript Or https://hackage.haskell.org/package/servant-elm 
The title confused me a bit, I first thought they were referring to the [`environment comonad`](http://hackage.haskell.org/package/comonad-5.0.4/docs/Control-Comonad-Env.html) but viewed as a functor. 
I am certainly interested in obelisk - https://github.com/obsidiansystems/obelisk I had it working at one point, but my lack of expertise with nix means when something got messed I could not fix it and have not been able to get it installed again. If you are comfortable with nix, then take a look. 
I've not tried, but seems to me like it should be possible to use Servant to generate yesod routes (instead of `parseRoutes`) as well. 
I know this isn't supposed to be a performance book but chapter 9 (type-safe printf) implements a quadratic string append... Also typo on page 46 (\`hsforall\`).
I've read 1/3 of it and I'm really really impressed. It reads like a prose and defines one thing after another, so far I wasn't lost a single time.
Yes, I feel the same way. I get why the instances are there (the principle that if a sound instance is *possible*, then it should exist, because of coherence), but I feel a bit like they subvert the practical purpose of a type system. It should by default be *rigid*, not just generalise until my expression can be assigned some weird generic type (or more likely, produce an error message referencing this confusing type). It's a difficult trade-off between abstract principles and concrete usability concerns.
Agreed. Almost none of the instances on `(-&gt;)` have ever been useful to me, but they've often created puzzling and unhelpful error messages, particularly the MonadReader instance
No idea. Maybe `fromInteger` is hard to define sensibly?
Every time I hit a hurdle with Reflex I think about Miso some more - do you have a solution for Android/iOS yet? It's the main thing that stops me (I appreciate my priorities may be different from your project's!)
`const . fromInteger` seems perfectly reasonable to me. Makes things like `2 * sin` behave how I'd expect them to behave.
I rather like the Applicative instance. I have used that several times. The Monoid instance is useful as well. But I won’t touch the Monad instance. It just feels like a maintenance burden to introduce that into code I’m working on.
I suspect that OP may mean efficient as in concise, since they referred to "by a combination of haskell functions".
Completely bare metal [warp](https://hackage.haskell.org/package/warp) - why : [performance](http://www.aosabook.org/en/posa/warp.html) Literally, someone took care of the first 4 octets of your network conversation. Even [mentioned as such by yesod](https://wiki.haskell.org/Web/Frameworks#Yesod)
Does it have to include a full JS runtime in every compiled app? Or does it only include the necessary JS?
People also flip out when `length (1, 2) = 1` yet `Foldable (a, )`
I see. Makes sense
&gt; I am really not aware of anything that would justify the need for, in my opinion inappropriate, regulation of language in this community. I am not denying there are accounts of events that would change my mind, I have not been presented with one. I think there is a point in preemptively and explicitly stating that certain egregious behaviours (such as threats, harassment and overt discrimination) are unacceptable. &gt; That is why I accept simple rules to govern usage of the downvote button, while I reject attempts to codify usage of language. I do agree that it is a fool's errand to micromanage routine communication in a community. Still, it is reasonable for a community to have a certain standard of discourse it aspires to. While such a standard can develop without a document about it, as a community grows there may be some use to explicitly state it somewhere, so that it can be referred to as necessary. That is why I like Michael's approach of having a document, clearly separate from any CoC, of soft guidelines which aren't directly enforceable, meant as "a set of communications best practices". &gt; A community that I value for its technical and theoretical insight, not its class or courteousness. I, for one, do value courteousness in the communities I take part of. In fact, the Haskell community was once known for being kind and welcoming. I want that to continue -- or, perhaps, for it to be restored. P.S.: No offense whatsoever was taken from your remarks, even without accounting for your disclaimers. Also, don't worry; I'm not all that hard with myself, though I do take "avoid personal attacks" very seriously.
While it might be heresy to mention it here, if you like the idea of functional programming and pattern matching, but you're afraid of pushback trying to switch to Haskell, you might have an easier time convincing management to give Erlang a shot. It lacks the static type safety of Haskell (although it is still strictly-typed, in that it doesn't coerce strings to numbers or anything like that), but it also has a long pedigree in critical, high-availability network applications.
If you find yourself wanting modularity, then type classes are the wrong tool to reach for. Type classes are exactly for expressing things that you want to be whole-program, globally coherent facts. They are, morally, *functions* from a type to methods associated with that type (or, as the implementation cares about, a function from a type to the record-of-functions defined by it's instance). Functions always return the same output for a given input. In some sense, a type class isn't any less modular than a pure function: &gt; The behavior of `id :: a -&gt; a` is the same in every part of a program. This global whole-program knowledge is antimodular. `id`'s behavior should depend on which module it's used in, and other implicit factors in the modular environment. Your question seems to be: If I create a "class alias," should I use the class alias, or what the alias refers to? For this case, you presumably made the alias for a reason, and so you should use the alias. For the less trivial case, where `Foo` is it's own class with it's own methods that implementing types must implement, and `Eq` and `Typeable` are ordinary superclasses, then you should use the minimal set of constraints necessary. If you need methods on `Foo`, then use that. If you don't, then ignore it. This is basically what you call "downstream discipline." So, I suspect that you are misusing type classes, and reaching for more advanced features to "fix" the problems you're encountering with them. 
Page 130: some LaTeX leaked out into the block (`\annotate{2}`).
The Reader Monad works when your app has just one level of dependencies. How do you do when you have components depending on other components? 
I didn't say syntactic abstraction solves all problems. Yes, some work on the compiler side needs to be done to make compiler times acceptable for this project. *That*, I believe, would be resolved if we had a better story for lifting efficient data representations and functions from the term level to the type level. A "more powerful macro system" could, again, accomplish this, but that's an aspect of the macro system other than "syntactic abstraction." Arbitrary compile-time computation can already be accomplished with Template Haskell, but the trick is it might have to have hooks into interacting with the type checker to make something like this work. [GHC Plugins](https://downloads.haskell.org/~ghc/8.6.2/docs/html/users_guide/extending_ghc.html#compiler-plugins) are probably pretty close what I want in this regard, but I haven't had occasion to look closely at exactly what they can do, yet.
Thanks Moritz for the mention! 
Unfortunately servant-purescript and its dependencies are not being maintained. Also, I found it had significant limitations, such as it could not handle type-constructors (e.g. Maybe Person). Servant-swagger is a much more flexible approach because you can control the generated types with its ToSchema typeclass, for example I can turn persistent's (Entity Person) into Person {id :: Int ... }. Unfortunately, purescript does not yet have a general-purpose Swagger generator.
Please don't make me cry you are so offensive with your suggestion that I suffered because someone made me suffer. And I suffer that you suffer for me. Really. That makes me suffer too. So it is a infinite loop who make us all suffer a lot. I think that people that suggest suffering make people suffer and this should not be allowed by the CoC. But I think that this would stress the half of th Haskell community who suffer because the CoC is not perfectly defined in category theoretical terms. A nicel netflix garbage could be made with this shit.
Oh, all of those are very nice, especially the last. Still, in context I think the right thing to do is not generalize yet. Two doesn't a big enough pattern make yet.
&gt; Locking your doors is not a guarantee that no one will break in to your house, but it sure does act as a good deterrent. Your door is alone in the field. there are no walls neither ceiling. The door is here to fix your attention at it while you are being robbed/offended lied, ignored from elsewhere. And if ever there were walls and ceiling you would be in a totalitarian group.
[removed]
https://www.reddit.com/r/haskelltil/comments/72cpmy/ghc_821_and_newer_allow_you_to_error_on_specific/ main = print (-1 :: Word) Put the above in `bar.hs` and run: % stack runhaskell -- -Werror=overflowed-literals bar.hs bar.hs:1:16: error: [-Woverflowed-literals, -Werror=overflowed-literals] Literal -1 is out of the Word range 0..18446744073709551615 | 1 | main = print (-1 :: Word) |
Reading the [STG paper](https://www.microsoft.com/en-us/research/wp-content/uploads/1992/04/spineless-tagless-gmachine.pdf) (you might also want to read [Making a fast curry](https://www.microsoft.com/en-us/research/uploads/prod/2016/07/eval-apply-icfp.pdf)) gives you a mental model for how to reason about the execution of STG (imho the important part is mostly the first half that describes the high-level ideas). Then the various `-ddump`-options, in particular, `-ddump-simpl` let you see what GHC generated from your Haskell code and whether the optimizations that you expected got applied or if not where you might need to make changes.
[removed]
Nice. How do I do that for a new type that is range-restricted? Just implement Bounded?
Yesod worked great for me.
Their website is down?! Did all 100 visitors who clicked your link give them the Reddit Hug of Death?! It brings into question the entire rest of your post.
I had the same thoughts. Also, it would be nice if the docs attached to instances looked better. This has been a problem for ages.
&gt; write C in Haskell Sort of, but you can't flip the libraries you use to switch to C in Haskell, unless you actually bring in c libraries. If there was a way to magically ask containers and other libraries to flip behaviors (i.e. each library author provided alternative implementations), I might return to Haskell as my main language. Alternatively, I might become better at c and c++ and master the Haskell to c interop someday.
Side question - any future plans for bindings to open street map, open layers on the client side? 
There's no style switcher on the parts of hackage that use the new style, so I seriously doubt it's anything more than a temporary option for testing.
Runtime will be included, but you can always deliver that as a separate js file that remains cached. If you do decide to bundle all the js together (I usually do), and use pre-rendering, it's a one time cost that happens transparently. On page load the js will be downloaded asynchronously while the user can view the page immediately (pre-rendering). All subsequent page refreshes will read js from the browser's cache. With gzip / zopfli, google closure compiler, you can get a reasonably sized payload. 
It's back up now, [https://haskell-miso.org/](https://haskell-miso.org/), I switched NixOS servers recently.
On a related note, I remember someone expressing a desire for an feature that would allow the user to poison instances. The context of the discussion was the foldable instance for pairs, but I feel like it could be useful in this context as well. In libraries and applications I write, I never intentionally use the `Monad` instance for `((-&gt;) e)`, and it seems like others avoid the `Functor` and `Applicative` instances as well. It would be cool if I could just tell the compiler this (without having any effect on downstream dependencies of my libraries), and then it could give me better error messages.
This is incredible thanks for sharing!
Mobile is constantly a forethought. Originally, the idea was to copy the react-native approach (use the javascript runtime available on mobile phones to call into Cocoa / Android APIs), but have since reconsidered due to the performance implications and complexity of managing 3 language runtimes. Of late have been looking into Manuel Chakravarty's work (very prolific in the embedded DSL space), [on Objective-C](http://hackage.haskell.org/package/language-c-inline) and [Cocoa APIs](https://github.com/mchakravarty/HaskellSpriteKit). As it stands, miso could target OSX (x86) piggy-backing on [language-c-inline](http://hackage.haskell.org/package/language-c-inline), but would need cross-compilation (contingent on [Moritz Angermann's work](https://medium.com/@zw3rk/what-is-new-in-cross-compiling-haskell-42ba93555c69)) to target ARM (iOS). So that's where things stand, as a side-note, miso fully expects to be ported to use either [Asterius](https://github.com/tweag/asterius) or [WebGHC](https://github.com/WebGHC) as Web Assembly support matures.
Huh that's a cool idea. I wonder if it's possible today to make a competing instance that *always* takes precedent. Then you could use the [TypeError](http://hackage.haskell.org/package/base-4.12.0.0/docs/GHC-TypeLits.html#t:TypeError) type as a constraint on that instance to make it error
agree - instances on \`(-&gt;)\` appear useful for golfing code. &amp;#x200B; Replacing having to play "state machine" in my head with "threading machine" in my head is not a gain.
It should be possible to use GHCJS's FFI to support this. All third party JS libraries are supported thanks to the FFI. If you want to walk through how to make bindings join the miso slack. `newtype OpenLayersMap = OpenLayersMap JSVal` `foreign import javascript unsafe "$r = new OpenLayers.map("map", $1)"` `newMap :: Options -&gt; IO OpenLayersMap`
I'll use Haskell frontend solutions when they'll be able to achieve something like this : [https://twitter.com/wSokra/status/1060520070827425792](https://twitter.com/wSokra/status/1060520070827425792) The react team is also working on a tool : [https://github.com/facebook/prepack](https://github.com/facebook/prepack) For now Haskell frontend bundles are just too big.
I do like `(||) &lt;$&gt; isSpace &lt;*&gt; isPunctuation`. I like `liftA2 (||) isSpace isPunctation` rather less, because you can then write `liftA2 (||) isSpace isPunctation foobar`, and the extra argument makes me do a double take. Other uses of that instance tend to be confusing indeed.
I'd say writing `join f` for `\x -&gt; f x x` isn't as evil as the other potential uses of that `Monad` instance. Otherwise, I agree.
Yes, if miso is going to support native, would rather have it use native components (i.e. Buttons, Pickers, Sliders, etc.) than solely a WebView. Miso websites currently load and cache in mobile browsers well (if you have an iPhone, load https://haskell-miso.org, then click "Add to Home Screen", it will create an app icon on the home screen that when clicked will load in fullscreen app-like fashion. The WebView approach optimizes for dev time since all of the code is reused, but the user experience won't be the same as using a native iPhone app like Photos or iMessage, etc.
*rolls eyes*
Yea even that example seems like obfuscation to me. I just can't see a scenario where this: (||) &lt;$&gt; isSpace &lt;*&gt; isPunctuation Is preferable to this: \bar -&gt; isSpace bar || isPunctuation bar Reading the former requires cognition; first your brain has to run a solver to determine that the `(-&gt;)` Applicative instance is in use, and then since this isn't used very often you have to remind yourself how this instance works. Meanwhile, the latter is instantly obvious.
Aside: posting this comment prompted me to try again. Long story short, on macOS 10.14.1, I installed Nix using the multi-user method[1] and everything seems to be just fine. Please see my previous comment about "lack of expertise" :) [1] https://nixos.org/nix/manual/#sect-multi-user-installation
Hi viercc, &amp;#x200B; Here's the actual typeclass that inspired this question. &amp;#x200B; **class** (MonadPlus m, MonadVarAllocator m, MonadIO m) =&gt; MonadMatchable m **where** hasVar :: MetaVar -&gt; m Bool putVar :: (Typeable a, Eq a) =&gt; MetaVar -&gt; a -&gt; m () clearVar :: MetaVar -&gt; m () overrideVar :: (Typeable a, Eq a) =&gt; MetaVar -&gt; a -&gt; m () getVarMaybe :: Typeable a =&gt; MetaVar -&gt; (a -&gt; m b) -&gt; m b -&gt; m b getVarDefault :: Typeable a =&gt; MetaVar -&gt; m a -&gt; m a getVar :: Typeable a =&gt; MetaVar -&gt; m a withSubCtx :: m x -&gt; m x withFreshCtx :: m x -&gt; m x withVarAllocator :: VarAllocator -&gt; m x -&gt; m x debugVars :: m () overrideVar m x = clearVar m &gt;&gt; putVar m x getVarDefault v = getVarMaybe v return getVar var = getVarDefault var mzero &amp;#x200B; &amp;#x200B; Are you saying that I should just be writing (MonadPlus m, MonadVarAllocator m, MonadIO m) as a constraint on 11 independent functions (or use ConstraintKinds)? &amp;#x200B; Why do you prefer the upstream version? Why does it matter that it's easier for the compiler to find that as a solution that makes your code compile?
I don't know that I can say it's the best, but I really enjoyed writing a web app with [Scotty](https://hackage.haskell.org/package/scotty), which is build on Wai and Warp. There's a [Scotty Starter Kit](https://github.com/scotty-web/scotty-starter) out there that demonstrates just how elegant it can be. It's really a joy to write HTML with Blaze, and CSS with Clay. Once you get used to that, everything else seems really verbose and clunky.
Hi ephrion! Your ideas about the behavior and meaning of the `id` function are....unorthodox, to say the least. How does using a "minimal set" address the problems I pointed out? If I need other things downstream of `Eq` and `Typeable`, then that's still my minimal set.
I think /u/Tekmo is a vanilla vi user.
Technically I use whatever is installed as `vi` on my system
(Your dislike is understandable and defensible; what follows is arguably nitpicking.) On the other hand, with the pointful version you have to come up with a variable name, and readers have to spend a small amount of time tracking where it is being used in the definition. That is admittedly a very small cost, which might not be enough to justify going pointfree. Still, depending on the circumstances there may well be other contextual clues to quickly reveal the `(-&gt;)` instance is in use, such as argument names -- for instance, `isSpace` looks a lot like a predicate -- type signatures, and grouping.
That's pretty exciting! I'll keep an eye out, for now I'm starting to get to grips with Reflex so I'll continue there.
I'll also nitpick :P Even if I *know* what `isSpace` is, I still feel as though I have to decipher how it's type is interacting with those operators to determine what's being done. As for naming variables, I think there's no difference between avoiding variable names entirely with point free style and choosing nondescript names like `x`; both are equally nondescript about the data in question.
&gt; I, for one, do value courteousness in the communities I take part of. In fact, the Haskell community was once known for being kind and welcoming. I want that to continue -- or, perhaps, for it to be restored. I still find it kind and welcoming. If there are violations to this it either are subtleties that I do not register in which case I am scared of the impact should these be dealt with. Else it’s something serious that I have seen no account for... in that case I would first like to learn about it, maybe you can help me with this. Preemptive strike is in my opinion not justified by any means - most men are capable of rape and we don’t go castrating them - please excuse the analogy.
To solve the posters original problem, I'd like to point out the nifty little function `zipWith :: (a -&gt; b -&gt; c) -&gt; [a] -&gt; [b] -&gt; [c]`. Let's play with this for a moment. What's the type of `zipWith (+)`? GHCI says `Num c =&gt; [c] -&gt; [c] -&gt; [c]`. Which looks an awful lot like vector addition to me. Which we can leverage to create matrix addition: `zipWith (zipWith (+))` has the type `Num c =&gt; [[c]] -&gt; [[c]] -&gt; [[c]]` and indeed does what we wish it to do. `(zipWith (zipWith (+))) [ [1,2], [3,4] ] [ [10,20], [30,40] ]` returns `[[11,22],[33,44]]`. This is a lesson everyone has to learn the hard way. When you're trying to do something simple in Haskell, and the type checker is fighting you, and you find yourself enabling all sorts of crazy extensions, that generally means you missed a left turn in Albuquerque. Learning a new language is hard. Go back, look at other functions, other type classes. You've missed something.
The ability to use `view` from Lens to inspect structures passed by function argument or pulled out of the environment is enough reason to keep it imho.
Great talk and interesting project! Long-term, it's going to be really interesting whether this "react"-like approach can 'scale' to everything or whether just *knowing* what to update (via an RFP-like model which tracks actual changes) will win out. (Obviously, a know-what-to-update model may still have additional constraints, in particular wrt. the DOM in the browser. There are certain attributes that you absolutely do *not* want to update+read lest you destroy performance.)
Gib! Now!
I want to second this. Miso has been incredibly pleasant to work with, and I have found the performance to be very nice (the server side rendering in particular is huge, plus all the usual SPA performance boosts). I have been using it for [Polimorphic](https://www.polimorphic.com/) in case you want to see an example of a fairly large Miso project in production.
I wish I had seen this thread two days ago. I have actually implemented shorter error messages for typed holes in GHC today. I am planning to get this submission-ready during MuniHac. 
YAGNI
You've told the compiler that `eval` takes a `TreeExpr` and produces a `Float -&gt; Maybe Float`. It's complaining that your implementation is producing a `Maybe String` instead, which indeed it does, since the `Var` constructor holds `x :: String` and you return `Just x :: Maybe String`. It's not clear to me what the semantics of `Var` are supposed to be, or exactly what you want to do with the `Float` parameter. Do you want `Var "x"` to be your x variable and the float to be its value? Having a `Var` constructor holding a `String` makes sense if you want to support more than just one variable, but then `eval` needs to know how to supply values to all of them, not just one. If you only want to support one variable, just use a nullary `X` constructor.
You might be interesting in [this recent post](https://old.reddit.com/r/haskell/comments/9v98md/so_you_want_to_write_a_dsl_interpreter_hcalc/) or checking out [buildExpressionParser](https://hackage.haskell.org/package/attoparsec-expr-0.1.1.2/docs/Data-Attoparsec-Expr.html#v:buildExpressionParser) from attoparsec-expr which makes building parsers for mathematical expressions easy.
&gt; Are you saying that I should just be writing (MonadPlus m, MonadVarAllocator m, MonadIO m) as a constraint on 11 independent functions (or use ConstraintKinds)? If it is only for grouping constraints of some functions, yes. &gt; Why do you prefer the upstream version? Why does it matter that it's easier for the compiler to find that as a solution that makes your code compile? I don't know the exact reason why GHC warns it. I naturally followed it, and never caused a modularity problem for me.
right, so we need to support passing values to x so something like this: \`eval (Addition (x) (value 3)) 1\` would be 4 because we passed 1 to x and then added it to 3.
Yeah `zipWith` was my first thought as well. It's fast, easy, and gets the job done.
How is adding one language pragma and a keyword at the signature *a lot of machinery*?
As I said, the actual modifications are simple. But RankNTypes are a significant change to the semantics of types, going from Hindley-Milner to full blown System-F, and like many extensions losing full type inference. Compare to some of the other solutions expressed here, with no fancy extensions.
 -- It's better style to put new constructors on their own line. data TreeExpr = Var String -- This string is still useless. -- What is the difference between Var "x" and Var "y"? -- eval gives them the same value. | Val Float | Addition TreeExpr TreeExpr | Subtraction TreeExpr TreeExpr | Multiplication TreeExpr TreeExpr | Division TreeExpr TreeExpr eval :: TreeExpr -&gt; Float -&gt; (Maybe Float) eval (Var _) a = Just a -- String is ignored, so why does it exist? eval (Val f) a = Just f -- Meaningless brackets here (and above). -- The rest of the cases have basically the same logic. Tiresome logic that -- it is to write out again and again, you probably (in general) want to make -- the declaration of eval into one big case expression so that you can write -- that logic once in a local helper function (using the where keyword), then -- write each case using that function. But as it happens you don't need to -- do that here. -- You can improve the other cases in the same ways I show below for the -- Addition case. -- Basic: don't use guards and partial functions (fromJust) where you should -- use pattern matching. This is not just better form, it's cleaner too. eval (Addition l r) a = case (eval l a, eval r a) of (Just lv, Just rv) -&gt; Just (lv + rv) _ -&gt; Nothing -- The logic conforms to the Monad instance for Maybe, so you can do: eval (Addition l r) a = eval l a &gt;&gt;= \lv -&gt; eval r a &gt;&gt;= \rv -&gt; pure (lv + rv) -- Equivalent to the above, but with do notation. eval (Addition l r) a = do lv &lt;- eval l a rv &lt;- eval r a pure (lv + rv) -- But actually Monad is more power than we need. Applicative suffices: eval (Addition l r) a = (+) &lt;$&gt; eval l a &lt;*&gt; eval r a -- Equivalent to the above; this is a common pattern: it's called liftA2. -- It's not really an improvement though. eval (Addition l r) a = liftA2 (+) (eval l a) (eval r a) -- The Division case has a change in the logic. You fail to Nothing when the -- denominator is 0, which is fine. But you also don't check that the -- numerator isn't Nothing, meaning the program will crash at runtime -- when it is. That's why you should avoid partial functions. -- This case actually does need Monad, and I would probably write it like -- this, personally (importing guard from Control.Monad). eval (Division n d) a = do nv &lt;- eval n a dv &lt;- eval d a guard (dv /= 0) pure (nv / dv) 
This is great - but I'm curious why does there need to be a special row type? More specifically what can you do with row types which you can't with type level lists? &amp;#x200B; Given that apparently duplicate names are OK and actually useful, row types seem a lot like a type level association list. Could you base a new record/variant system off type level lists (with maybe a little syntactic sugar)?
Profiling is absolutely straightforward, and indispensable. Hot-spots are often not where you suspect them to be. I don't think there's any substitute for it -- but it is also really easy! &amp;#x200B; Once you have a nice profiling build and a benchmark case you can profile, just run the profiler over and over, changing stuff around as you go, and see what happens. That's really it. &amp;#x200B; The typical sorts of things you want to change are discussed in other posts in this thread, but in brief, you want to consider A) algorithmic asymptotics, B) data structure choice, C) laziness/strictness, D) avoiding repeated or unnecessary work, E) minimizing allocations, F) avoiding memory leaks, G) judicious use of compiler directives and pragmas such as inlining or tuning GC settings. &amp;#x200B; I can't state enough that the important thing is to make and observe small tests -- often things that in \_theory\_ should help, like switching data structures, may not help, or may even make things worse.
Is there a standalone library (i.e. not a custom prelude) that provides all the functions from System.Process but with Text instead of String?
This is really fun and interesting, thanks. But in some ways it's the exact opposite of what I need, because what I need is to get the program done, and what this does is make me want to put everything else on hold and spend the next week studying \`Data.Coerce\`. &amp;#x200B;
I would strongly recommend against using Elm, mainly on the grounds that the creator is user-hostile and makes no effort to maintain backwards compatibility, but also on the grounds that it has not typeclasses.
This function must be recursive and I also think the guards that follow this pattern are redundant. | otherwise = Just (fromJust (eval l a) + fromJust (eval r a))
Is there a way to reduce boilerplate when deriving instances on newtypes? -- Turning this newtype Foo = Foo String deriving (ToJSON, FromJSON, Hashable, Show, Eq, Ord) newtype Bar = Bar String deriving (ToJSON, FromJSON, Hashable, Show, Eq, Ord) newtype Baz = Baz String deriving (ToJSON, FromJSON, Hashable, Show, Eq, Ord, Other) -- Into This? newtype Foo = Foo String deriving somePreDefinedListA newtype Bar = Bar String deriving somePreDefinedListA newtype Baz = Baz String deriving (somePredifinedListA ++ somePredefinedListB)
That, I don't know.
&gt; using "oplus" for opening things is confusing from the mathematically trained eye &amp;#x200B; I vote for a more established expand/collapse symbol, too. Yes, I got it fast enough, but it still was confusing, because it differs too much of the already established mnemonic. It gives a nice haskell-mathematic touch, but only to sacrifice the accessibility. (IMHO)
Agree. Love the environment functor/applicative/monad *when it is newtyped*. Letting regular old functions be an instance of these things is just giving you a way to shoot yourself in the foot.
I can understand your points and agree on large parts. There are annoying people and they should leave. Yes ok, but being annoying is subjective and not even that alone, it depends on the mood and situation how intense the feeling of someone being annoying is. I personally don't like to exclude people unless everyone agreed on it. No CoC is needed for this. And people should not make something appear a problem of ethics when it isn't. Throwing someone out is not ethical. &gt;However, someone throwing insults like candy and derailing discussions all the time is most definitely a problem. I don't know. First, I always think if people are oversensitive and react wrong, maybe ignoring crap said by certain persons would solve the problem or simply saying what's wrong. Most people tend to leave on their own, if they are repeatedly not understood in a group. I just want to say one more thing. Throwing someone out is an aggressive action and having a CoC provides means for aggression. Symbolically I don't like it. A person who uses a CoC to exclude is worse than the offender, in the cases I've seen a CoC being used. And I am not alone with this perception.
See the alternatives section. It just takes too long to compile.
I'm not 100% sure that makes sense. I know Linux process things (argv, envp) are defined in terms of bytes and don't map to Unicode except by lose convention. (Strict) ByteString probably make more sense than Text or String. I have files on my filesystem with filenames that aren't Unicode, so some Qt apps have problems interacting with them, though traditional UNIX utilities don't have problems.
Very cool, I hope it becomes successful.
Thanks!
The other answer involving nested `zipWith` is simpler and better, my own answer was overkill. At the risk of sending you through another rabbit-hole, [this talk](https://www.youtube.com/watch?v=D1sT0xNrHIQ) about a method of representing matrices in Haskell is interesting.
My point is, yes it makes ghc harder to write, but that is something the average user doesn't have to worry about. Loosing type inference just means you have to explicitly annotate a rank n type in the type signarure, but that is a good thing anyway (would you like other programmers to guess where you used Rank-N types?). I am not saying there aren't better solutions for this problem, I just don't think rank N types are a heavyweight feature for what concerns the user.
&gt; batteries really? Yes. What's wrong with the choice of Batteries?
Nitpick, you should be slapping in `RankNTypes` and `ScopedTypeVariables` if these examples are meant to be executable
Will these new records have `Coercible` instances?
Restructured it, there wasn't much structure to begin with
Yes. `pure` in place of `const`. 
Yeah at some point "maintain backwards compatibility" is the real argument. 
This is the same old garbage as usual from his blog. Fairly standard stuff and he acts like it's abstruse/deliberately mysterious. &gt; fmap f x = \e -&gt; f (x e) &gt; or shorter and more mysteriously &gt; fmap = (.) &gt; which follows by η-reduction, something Haskell enthusiasts never seem to get enough of. ...okay. Apparently spending the time to actually learn Haskell before complaining about it would be a burden. 
There are plenty of times when functions of type `forall a. ...` make it clearer. It's well-known as [projective programming](http://www.cs.ox.ac.uk/ralf.hinze/publications/ICFP09.pdf). The trick is to realize what's actually projective programming and what's just generalization for the sake of generalization. After some time writing Haskell, I have to say I haven't really had many instances where this was unclear. This blog post seems like someone complaining about a relatively trivial, known issue. 
&gt; Generalizing `matrix` and `unmatrix` to a single one-line function isn't too soon Actually, you started the generalization game even earlier: you didn't need fancy `Functor` instances at all: a simple list-specific `zipWith` would do. Instead of using simple types and functions to implement what you need, you assumed that `ZipList` is the right abstraction to apply without even looking at other operations on Matrices beyond simple addition. If you want to multiply matrices, `ZipList` only stays in your way. I implemented a batch of simple matrix manipulation functions once (I was solving an assignment on Coursera involving PageRank computation for a small matrix and I needed `Rational` answers), it had something like module Matrix where import Data.List (transpose) type Vector a = [a] type Matrix a = [Vector a] addV :: (Num a) =&gt; Vector a -&gt; Vector a -&gt; Vector a addV = zipWith (+) addM :: (Num a) =&gt; Matrix a -&gt; Matrix a -&gt; Matrix a addM = zipWith addV dot :: (Num a) =&gt; Vector a -&gt; Vector a -&gt; a dot x y = sum $ zipWith (*) x y mulMV :: (Num a) =&gt; Matrix a -&gt; Vector a -&gt; Vector a mulMV m v = map (dot v) m mulM :: (Num a) =&gt; Matrix a -&gt; Matrix a -&gt; Matrix a mulM a b = let b' = transpose b in map (mulMV b') a It took me maybe 10 minutes to implement all of these + some extra normalization procedures. I'd say it's much more interesting to combine the simple functions you already have to build new stuff than to spend time trying to fit `ZipList` into your model. After you see all these functions together, you might find some nice patterns and try to generalize things (like introducing vector spaces) to reduce the number of function names you have to remember. But generalizing eagerly is almost never a good thing. 
I know that this is not for beginners, but can someone provide an ELI5? 
&gt; Every time I hit a hurdle with Reflex I think about Miso some more - do you have a solution for Android/iOS yet? Checkout [obelisk](https://github.com/obsidiansystems/obelisk) which has support from producing iOS/Android builds of your app from the same source code.
Thanks, I'm actually using /u/ElvishJerricco's [skeleton](https://github.com/ElvishJerricco/reflex-project-skeleton) so I do have this capability. The comment I made was more about my troubles with FRP, and my feeling that I have to stick with Reflex for native support of some sort. I think I'm getting to grips with it though so my desire to use Miso is lessening.
Is there a way to provide automatic `FromJSON` / `ToJSON` instances for a GADT like the one in the blog post? ``` data EType a where ETypeWord8 :: EType Word8 ETypeInt :: EType Int ETypeFloat :: EType Float ETypeDouble :: EType Double ETypeString :: EType String ```
That's certainly very interesting, thanks a lot for the link - looks like it's a way off at the moment.
Although I didn't update the sample docs before merging, the \oplus stuff got swapped out for triangles. Still unicode though...
TIL: [`:type +v`](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/ghci.html#ghci-cmd-:type%20+v)
In Haskell, by default, a function's caller gets to pick what the type variables end up meaning. Often, this is what you actually want. Sometimes, though, a function wants to say "No, _I'll_ pick, and you have to give me something that will work with anything that I could have picked." This post explores how to work with that second case.
Very recent development: See [*Generic Programming of all Kinds*](http://www.sigplan.org/OpenTOC/haskell18.html) (search for name under **Generic Programming**) and associated package [*kind-generics*](https://hackage.haskell.org/package/kind-generics-0.1.0.0)
You need to write template Haskell currently, AFAIK. You're also going to run into a problem with the type index, so you may need to hide it behind another existential (in the `fromJson` instance at least): data SomeType = forall a. SomeType (EType a)
[Slides](https://owickstrom.github.io/declarative-gtk-programming-in-haskell/#/title-slide).
Thank you for your thoughs. I have been thinking about this for the past few days and realized how wrong trying to be an overanalyzing internet smartass can go. I honestly apologize to /u/leaf_cutter and all who happened to read my words. 
&gt; Yes ok, but being annoying is subjective Some forms of annoyance are very much objective. It is generally possible to tell beyond reasonable doubt whether someone is derailing a discussion, or attacking someone else with racial slurs. Furthermore, something being subjective does not mean it doesn't "really" exist, or that it can be entirely described with an objective metric, or that it can be ignored. Why I think so will become clearer shortly. &gt; if people are oversensitive and react wrong That is a big "if". It is not oversensitive to be upset because racial slurs are being thrown at you. It is not oversensitive to quit a community because you see everyone around you acting like jerks. &gt; maybe ignoring crap said by certain persons In an ideal world, this might solve a few of the problems (but not all -- see what I have said just above). In practice, though, there is no guarantee that will happen consistently -- after all, even people who know the importance of not feeding trolls will occasionally slip up and engage. And when it doesn't happen, you have, from a community point of view, objective problems: derailed discussion, time and energy wasted, disgruntled people, etc. &gt;Most people tend to leave on their own, if they are repeatedly not understood in a group. That is true for genuinely misguided people. It is not true for malevolent people, nor for people acting in bad faith, nor for trolls. It is not unknown for determined trolls to keep causing trouble and sowing discord for months and years. &gt;Throwing someone out is an aggressive action Attacking someone with insults is an aggressive action. More generally, a community should have ways to protect itself from bad actors. (Whether CoCs are appropriate ways to do so is debatable, but a separate question.) Is it really worth it to let a community rip itself apart out of adherence to an imperative of not throwing anyone out ever? We might discuss what is the best, or the fairest, or the kindest, course of action in this or that concrete case, but, taken as a blanket statement, I say the answer is "no".
I don't think it's easy to judge about people. From my experience, you cannot judge people objectively and I personally have been wrong about some people, too. To understand someone's motivations and intentions, it's unavoidable to be close with this person. And when you are close to someone, you would be tolerant and not reject them. The problem is that people get offended early. And because of tiny things. The better solution is to stop being offended as much as possible. This can be done with some self-confidence. Also let other people who watch the drama decide, while you don't react offended. If they think, it's offense, it's probably offense. Using this logic, a CoC is useless. The offense is still judged subjectively, but it's a good indicator for action. Generally, it's better to let others decide, while you are in the heat of a fight. A CoC just distracts from the situation.
Just to build on that, you don't need dependency injection in Java either. You make your behavior that can be adjusted an interface, and then pass in the instance you want at construction time. That's more or less the exact equivalent of the way Haskell does it with higher order functions. The Java problem is verbosity: manually constructing your interface, your implementations, and the code that switches between them based on compile time or run time choices buries you in boilerplate. So the Java developer community came up with tools like Spring Dependency Injection and Google Guice. Any language with higher order functions and less boilerplate doesn't need it. I would argue that even in Java it causes more problems than it solves, but that's a separate discussion.
I thought that this is one of the best-written Haskell books I have come across. What is more, it covers material that is hard to find elsewhere in assimilable form. What would be great would be a vol II or a supplementary section at the end of each chapter, perhaps) illustrating how the facilities are used in real applications, what they buy you. A minor gripe, it is hard to copy &amp; paste the examples into a text window, to try them out, because the PDF doesn't preserve newlines / indentation. &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B;
IIRC this was also a major theme in [Daan Leijen's "Extensible records with scoped labels" (PDF)](www.cs.ioc.ee/tfp-icfp-gpce05/tfp-proc/21num.pdf) paper. Surprisingly, it turns out allowing duplicates is actually a good thing.
Out of curiosity, what are you actually trying to do? Not “what output are you trying to get”, just “what purpose does this code serve in your application”? It looks like you might be trying to do argument parsing, in which case I recommend looking at something like optparse-applicative.
How does this help? It seems to only let you specify how to derive things, not combine them in any way?
Apology accepted. Apologizing in public like this is very brave of you, well done :-)
Are you really going to want to require that every monad implement all these methods every time? It seems like you don't really need the polymorphism provided by typeclasses here, so consequently I would avoid the typeclass and just use constraints on individual functions. My opinion is that you only create new typeclasses when you can write two or more instances, otherwise just writing constrained functions are preferable.
As an aside, here's the [performance numbers](https://user-images.githubusercontent.com/875324/48317364-daab3380-e5be-11e8-9292-136caa46e399.png) for haskell-miso.org on a mobile phone, per Chrome's audit tool.
What course is this?
&gt; Curly braces are for records (kind `Type`) the other syntax is for Rows (kind `Row k`). Would there be anything wrong with using curly braces for both? As in type MyOpenRow r = {foo :: Int, bar :: String | r} It looks like set syntax, and sets are more similar to rows than tuples, I’d argue. 
Off topic, but does the tag [ANN] stand for? I see it on a lot of posts, but I can't find any information on what it means.
Announcement. 
ah that makes sense.
Maybe we should just use [Announcement]? I feel like this gets asked every couple ANNs :P
Not at a computer so I can't test this, but I wonder if you could use ConstraintKinds? {-# LANGUAGE ConstraintKinds #-} type Foo a = (Show a, Eq a, Ord a) newtype Bar = Bar String deriving Foo
I am on Nix 2.1.3, I should have included that. Maybe give that a shot. 
Yeah, that’s what I’m on too. I guess I’ll have to wait until it’s fixed.
Could you provide a short comparison with [dimensional](http://hackage.haskell.org/package/dimensional-1.1/docs/Numeric-Units-Dimensional.html)?
Reading some of these responses, it almost sounds like some of you are programming in a different language. I actually used to program in the style you're discussing, using ConstraintKinds to avoid having to spell out all the constraints. After a while struggling with this, I discovered it was a bad idea. I don't remember all the reasons (error messages and compile times were among them, but not the main problems), but I had a discussion on StackOverflow about it. BTW, all the problems I'm discussing still apply even if you don't have catchall instances. They apply any time you have an instance where the RHS includes a type variable. So, my original question remains mostly unaddressed.
What I'm saying is that unless you intend on having more than one implementation of `hasVars` et. al., typeclasses probably aren't the right tool for the job. Typeclasses are for polymorphism, not really for grouping functions together. I honestly don't find putting constraints in type signatures to be particularly bothersome (even three constraints on 11 functions), but obviously it's pretty subjective.
great video, thanks for sharing.
Has anybody actually gotten HIE working with a Nix-based project? If so, any tips? The most straightforward process I've tried so far is to install HIE with https://github.com/domenkozar/hie-nix (specifically choosing 8.4 version). Besides taking a long time to build and install, it fails with ``` Got error while processing diagnostics: &lt;command line&gt;: cannot satisfy -package-id aeson-1.3.1.1-IIws8ihD9EF7nr6Y0MCLpZ (use -v for more information) ``` when the project is loaded up into Visual Studio Code. I'm assuming (though I could be wrong) that this is due to HIE not being built with my project's set of libraries. It seems like this issue or a similar one was supposed to be fixed by https://github.com/NixOS/nixpkgs/pull/46453, but I've pinned a recent version of Nix 18.09 that I would think has this fix. I've tried a lot of other methods that have ended up not panning out (and all take a lot of time to run), including trying to rig a local checkout of hie-nix into my Nix project's buildInputs. But this hasn't really panned out. It's a bit of a mess but if it helps, https://github.com/cah6/nix-test has some of the things I've been trying. The `simple-release.nix` and `simple-shell.nix` are the basic expressions that I'd like to use by installing HIE globally then just running: ``` nix-shell simple-shell.nix code . ``` and `release.nix` and `shell.nix` have various states of things I've tried to wire HIE into my project directly. Would appreciate any help or tips on this from somebody that has gotten this to work!
What I don't like about the typical discussion around Kafka is that decoupling is tightly coupled to queues in such discussions. Which is false. Decoupling services just needs naming systems and RPC mechanisms that will send to whoever wants the data. No queue is needed for this. Queuing is good at entry and exit from your system, but given a good RPC library with backpressure, why would you want queues anywhere else?
There is a README with examples and haddock, what is missing in your opinion?
&gt; I didn't make that sentence conditional because the times when you can write a truly stateless application that makes money are so few and far between as to be considered impossible. lots of programs falls in this area. a code formatter is one example. 
I've added a sample pdf on [leanpub](https://leanpub.com/thinking-with-types)!
Never mind, managed to get cabal2nix installed with `nix-env -f channel:nixos-18.09 -iA cabal2nix`!
Thanks! Fixed!
This looks really neat. Is there a plan to make a print version?
&gt;Haskellers frown upon state, This is not true. Haskellers frown upon uncontrolled and unmanaged state changes. But not state itself. Haskell provides state of the art mechanisms of controlling when and where the state changes. 
Might be a tooling issue, since sometimes this can cause titles to run off a single line, making them unreadable as notifications or on some reader apps.
I'm not sure what sort of answer this is to my post as it seems like you ignored what I wrote. &gt; A message system decouples the servicing endpoint from the requesting endpoint in a way that RPC typically does not. But it doesn't. The arch-typical gRPC has a more flexible decoupling of service endpoints than Kafka has. You can use DNS, etcd, a config file etc. In Kafka you have exactly one way of doing this afaik. &gt; and a much tighter relationship to availability of the service endpoint. Yes, and that's why I said it's fine with queues at entry and exit from your system, but why would you not want to use backpressure internally to get better latency and lower resource usage? 
Hi phischu! Purely functional programming can also handle codata (see e.g. the Agda work on copatterns), which should be enough for contracts I assume? But your link doesn't work, so I am not sure if your view of contracts can be modeled using codata or is something else.
3 constraints is nothing. In the Cubix framework, I was regularly getting constraints that filled several lines.
It's from Haskell Programming from First Principles.
&gt; Your ideas about the behavior and meaning of the id function are....unorthodox, to say the least. If you're referring to the block-quoted bit, then I was trying (and perhaps failing) to express "what it sounds like to me" when folks talk about the global coherence and "antimodularity" of type classes. My apologies for not being more clear :)
Yea, I thought I was on my university subreddit, so I was wondering if this was a uni course, lol.
yeah, managing state is what haskell programs do best. complex haskell programs aren't "stateless" (though most of the logic should be in pure code), they just **distinguish** stateful functions from statelry function, via `IO`. 
There sure is! Hope to have it out before christmas!
Kafka is a very beautiful architecture.
IN the license field I noticed BSD-style (See license file with the distribution). Can that be made just BSD-style with a link to the license file ? 
That's true for full rank-n, but for a rank-2 type (which is what a natural transformation is), [type inference is still tractable](https://pdfs.semanticscholar.org/0b9c/ecad2194aa8bfa01de605295b32a91cba55d.pdf).
I did not know that. Of course, while GHC does have a language pragma for rank-2 types, it's just a deprecated synonym for rank-n.
I've done small projects one level up using WAI as the "framework" and it's quite nice.
Indeed when you compile libraries with profiling, it actually compiles them twice -- once instrumented, once not. Only when you build/link the executable do you get _either_ the profiled or not, and not just both.
Interesting thought
[An example of a warp-only server](https://github.com/smigniot/tockhaskell/blob/master/src/Main.hs#L50) - and please forgive the coding style, I was a younger noob in my haskell journey :D But bare metal warp is still quite pleasant, even though yesod is fine once a project grows.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [smigniot/tockhaskell/.../**Main.hs#L50** (master → a1415c7)](https://github.com/smigniot/tockhaskell/blob/a1415c7e5fe7cac15da1c1d6aebf493cafd053c5/src/Main.hs#L50) ---- 
Messages written to Kafka are immutable. Certain topics are intended to be interpreted as "state mutations", but each individual message is still immutable. "Compaction" of such topics is just a form of garbage collection, allowing you to throw away old history that is no longer needed. I think of it less like "global state" and more like "global logs". You do need to adopt some principles about which services are allowed to read or write to which topics in order for things to remain sane. Many topics are "logs" and only one service should write to the topic. Some topics are for "placing requests" and there may be many writers, but only one service "handles" requests. There *is* a real danger for microservice architecture to get spaghetti-y. Nonetheless, I think Kafka can be pretty good for helping you organize a sane solution to complex scaling issues.
Could we disambiguate it in another way than changing the type of brackets used? Perhaps by adding a keyword?
I was working on something similar for one of my pet projects the other day. It is really interesting to see someone else solve the same problem you have been working on, but they do it in a very different way (in this case, much better) &amp;#x200B; I'm looking forward to dig into this library and learn a lot, so thanks for teaching me indirectly! :)
Existentials are about data hiding. When you have an interface reference/pointer in an object-oriented language, you have an existential -- there is some concrete type being referred to, but you don't / can't know what it is. But you can still use it via the interface. I think the best introductory example of existentials in Haskell is streams: data StreamResult s a = Done | Skip s | Next s a data Stream a = forall s. MkStream s (s -&gt; StreamResult s a) -- for example streamFromTo :: Integer -&gt; Integer -&gt; Stream Integer streamFromTo start end = MkStream start next where next s | s &lt; end = Next (s+1) s | otherwise = Done streamToList :: Stream a -&gt; [a] streamToList (MkStream s0 next) = go s0 where go s = case next s of Done -&gt; [] Skip s_next -&gt; go s_next Next s_next a -&gt; a : go s_next So `streamToList (fromTo 2 5)` is `[2,3,4,5]`. To make a stream (for example, `streamFromTo`) you have to provide a state and a *compatible* state advancement function. Then, the type of state used by the stream is hidden from code that uses it (like `streamToList`). It turns out that using this for procedural lists makes them much more amenable to being optimized by the compiler; folds, zips, etc. can work without constructing intermediate lists and instead turn into fast code that iterates on the stream state.
GHC inserts `Any` (which can inhabit any kind) when it needs to make up a concrete type, because it needs a value for a unification variable that is completely unconstrained. In the case of `TypeOf (Plus 1 2)`, the unconstrained variable is from the kind of `Plus 1 2 :: PlusT v`. For example, we have `TypeOf (Plus 1 2 :: PlusT 42) :: Nat = 42`. Really I think `:kind!` ought to report a variable here rather than `Any`. It looks like the `piResultTys1` bug exists in 8.4.x but is fixed in 8.6.x?
/u/bgamari: Thank you for the release! I've seen that the deb9 download is missing. Is there a reason for that?
I wrote a toy json parser to apply what I learned from Learn You A Haskell, would anyone be willing to do a code review? Much appreciated. https://github.com/sdubinsky/myJson
I found better to remove any cabal or ghc from the system and install only stack, and let it fetch whatever it needs. &gt; I checked the .stack-work and found the executable has actually built under the dist dir, but the stack is looking for the install/.../bin which does not even exist in my case lol. probably you should run `stack build` first
That’s what he said.
Offer to help add some :)
PSA: You can also use the Debian 9 (Stretch) Apt repo as documented at http://downloads.haskell.org/debian/ which does have a GHC 8.6.2 package for Debian 9.
Also stack builds executables as “executable”-exe so yours would be untyped-exe 
Mods could edit the title and add "Announcement" flair.
Awesome explanation, I thiiiiink I get it :) The `start` value is placed in the stream state, because that is used as the `s` argument in `MkStream` that we don’t know about? And the `end` is the “known" argument that gets passed in `data Stream a`? 
There is no haddock documentation, there are only signatures.
`IO` isn't for state, precisely. It's for side effects. The actual "state" being tracked is a phantom value.
It's really cool that units are represented as a list of nats. But it makes me wonder how you would search for the simplest representation made out of common units. Seems like a version of the knapsack problem so probably np hard?
Ah yes indeed 
Recently there was a really interesting post on state machine testing with Hedgehog (which is essentially a better quickcheck) http://qfpl.io/posts/intro-to-state-machine-testing-1/ I believe this approach has a lot of potential.
I use a simple state machine which models CRUD, and keeps track of server state. &amp;#x200B; The idea is to run random commands on both the CRUD REST API and state machine and make sure they're equivalent. &amp;#x200B; E.g. create user &amp; delete user &amp; read user. &amp;#x200B; I am sure there's a niche for a general testing service / library for CRUD APIs.. 
GHC plugins have only recently gained ability to change syntax. I bet they're still missing some critical functionality required for more ambitious extensions, so the sooner you start looking at them the faster they can get whipped into shape.
You could always turn on `{-# CPP #-}` and `#define` an appropriate macro, I guess.
Might be because structurals are inherently all about subtyping, while GHC wants exact types. I think even in this proposal you can't upcast `{ a , b | r }` to `{ a , b }`
Haskell for Mac, from the app store, if you can afford it. 
Ha. Poe's law strikes again! &amp;#x200B; I was referring to a completely different modularity problem, one which is very much not akin to ordinary functions. If I write a function &amp;#x200B; doSomething :: (Eq a, Typeable a) =&gt; Foo (Tree a) -&gt; Bar (Tree a) &amp;#x200B; where the body of "doSomething" requires an instance of (SomeClass a), then doSomething is making a global assumption that there is some chain of inferences from (Eq a, Typeable a) to (SomeClass (Tree a)). This is a very unstable assumption. Suppose I add an extra (Ord a) constraint anywhere in that line; then my program breaks, perhaps in confusing ways. &amp;#x200B; So, in that light, your first paragraph seems to be advocating the downstream discipline.
Last time I checked, Hedgehog did not support generating arbitrary functions. Is this still the case?
Well, they aren't really a list of nats. It's more like a 7-tuple with a slot for every SI base dimension in which there can be a positive or a negative nat. Therefore, it's always the simplest representation. Do you suggest to use J instead of kg \* m\^2/s\^2 ? That would be sadly in general impossible, because there are a lot of derived units with the same SI dimensions but completely different meaning. For simple cases, this is already done on the typelevel to provide a nicer type interference and usage of :t, look at [https://hackage.haskell.org/package/physics-0.1.2.1/docs/Physics-Units-Arithmetic.html#t:Pretty](https://hackage.haskell.org/package/physics-0.1.2.1/docs/Physics-Units-Arithmetic.html#t:Pretty) &amp;#x200B;
Yes, it is a bit succinct. I will certainly add more haddock documentation. Most of it is very easy in it's usage. Or do you suggest that I add for every constant the physical explanation?
You're welcome! If you've got any questions, I'd love to be of assistance.
Yeah, you are absolutely right, trying to infer more complex combinations for type errors could be super confusing. And storing the original types directly would require an extendable notion of type equality, I think?
Looks like this is not the case anymore: * http://hackage.haskell.org/package/hedgehog-fn
\- \`physics\` is less intrusive, you're not required to use a custom prelude \- in \`physics\`, there are all common constants in best known precision according to SI 2019 \- \`physics\` is much more minimalistic and neat, it's only dependency is base \- \`physics\` supports Planck units &amp;#x200B;
The original type is always canonical in SI dimensions, it's only sugarcoated with type synonyms. Preserving these synonyms for type errors is (as far as I know) not possible.
Hey everyone OP here, just finished and *very very lightly edited* part 2 of the blog series. This entry is longer, but feels like a drop in quality than Part 1, would love to hear some feedback on bits that you didn't like. While the code snippets are surely throwing off the metrics, it's supposedly a 71 minute read (!), but I feel like I managed to actually travel even *less distance* than in Part 1 (which is saying a lot, because part 1 ended up being basically config wrangling). To cut the size of this post I've also had to push out optimizations/generalizations to Part 3 as well, and as such the code in Part 2 is a little further from production quality. Anyway, hope someone out there enjoys it -- again would love to hear some feedback, maybe I need to make this series longer, or add a ToC for the post or something so people can jump to what they might be interested in instead of following the stream of consciousness writing style I'm using. The article has only been edited lightly, it's time to sleep where I am so I'm going to edit and check back in ~10 hours
`start` is the state -- it's "mutable". `end` is constant and is stored in the function closure for `next`. Since the function is outputting values from `start`, it doesn't matter what `start` is when we are in the middle of the output, all that matters is the value we are going to output next. But it does matter what `end` is, because we need to know when to stop. We could have also stored `end` in the state as a tuple `(start,end)`, but it would be wasteful because it never changes.
I'm so happy to hear that. Do you mind if I quote you on it when I get around to making some publicity material?
Wow, as a beginner only recently having grokked using classy lenses, prisms (or generic-lens) with transformers, to ensure your type signatures signify what your functions can do, this feels overly complicated. Can you briefly talk about why your approach is needed? Awesome post though, I’ll be reading it through many times :)
Thanks for this post. I'm about to embark on some haskell-based RESTish services for the first time so I will definitely be going through these and looking forward to part 3! 
You could use pattern-matching more often instead of partial functions: Line 52: if (nestLen &gt; 1 || tokenLen &gt; 0) then Left NestError else Right $ head ns -- could be case nestLen of [n] -&gt; Right n _ -&gt; Left NestError Simlarly, line 58, you could use a `case` instead of guards with boolean comparisons. You can also get rid of spurious `$`. singleToken (x:xs) | x == '{' = Right $ (LBrace, xs) | x == '}' = Right $ (RBrace, xs) ... | isDigit x = Right $ (TInt (read (x:takeWhile isDigit xs)), (dropWhile isDigit xs)) | otherwise = Left TokenizeError -- could be singleToken (x:xs) = case x of -- also makes it clearer we're looking only at x, and not xs '{' -&gt; Right (LBrace, xs) '}' -&gt; Right (RBrace, xs) ... _ | isDigit x -&gt; Right (TInt (read (x:takeWhile isDigit xs)), (dropWhile isDigit xs)) _ -&gt; Left TokenizeError By the way, in the above function, `takeWhile`/`dropWhile` is `break`, and in the quote case `tail` can fail if the list is empty | x == '"' = Right $ (TString (takeWhile (/= '"') xs), (tail $ dropWhile (/= '"') xs)) -- the tail is to skip the closing quotation mark -- could be (with the above `case` refactoring) -- note also how you can add bindings/patterns in a guard '"' | (str, '"' : xs') &lt;- break (/= '"') xs -&gt; Right (TString str, xs) | otherwise -&gt; Left TokenizeError -- this last clause overlaps with the wildcard below it, but it seems safer to be more explicit here The rest looks okay. --- Some alternative implementations of `isWhitespace`: -- original isWhitespace :: Char -&gt; Bool isWhitespace x | x == ' ' = True | x == '\n' = True | x == '\r' = True | x == '\t' = True | otherwise = False isWhitespace x = x == ' ' || x == '\n' || x == '\r' || x == '\t' isWhitespace x = x `elem` [' ', '\n', '\r', '\t'] or: [`Data.Char.isSpace`](https://hackage.haskell.org/package/base-4.12.0.0/docs/Data-Char.html#v:isSpace) 
[removed]
This is what I came up with after some time. eval :: TreeExpr -&gt; Float -&gt; (Maybe Float) eval (Var _) a = Just a eval (Val f) a = Just f eval (Addition l r) a = Just (fromJust (eval l a) + fromJust (eval r a)) eval (Subtraction l r) a = Just (fromJust (eval l a) - fromJust (eval r a)) eval (Multiplication l r) a = Just ((fromJust (eval l a)) * (fromJust (eval r a))) eval (Division l r) a | (fromJust (eval r a) == 0) = Nothing | otherwise = Just (fromJust (eval l a) / fromJust (eval r a)) where you can pass in a param as the values for x. `eval (Addition (Var "x") (Val 3)) 3` = 3
&gt;There's no style switcher on the parts of Hackage that use the new style It's pretty clear to me that he meant to say: &gt;There's no style switcher on the (other) parts of Hackage that use the new style I could, of course, be wrong. &gt;You'll be able to switch between haddock styles on the haddocks that are hosted on Hackage. If it's only for haddocks hosted on Hackage, that suggests it's a Hackage choice, but Hackage doesn't allow you to switch styles anywhere else but the haddock parts, suggesting it's a temporary choice. We all know what happens when these kinds of changes go through. The old style, which OP was arguing is the superior choice, has become a second-class citizen. Eventually, you might cynically expect the “old” style to be phased out because it's too much work to “backport” changes made to the “newer” style. I don't have a dog in the fight, I haven't really looked at the two themes yet. I'm just trying to faithfully put forward /u/Hrothen's view. I hope I've achieved that; /u/Hrothen?
Frick!
&gt; this whole thing could be just a library and would require no changes to the compiler. Or maybe, "would require *only general purpose* changes to the compiler".
[servant-quickcheck](http://hackage.haskell.org/package/servant-quickcheck) will let you run a bunch of predicates over one or more endpoints to check that some general properties of your API are respected. This requires having the suitable `Arbitrary` instances for the types that you use in the API and a description of the endpoint(s) in question in "the servant way". Relatedly, we have [servant-mock](http://hackage.haskell.org/package/servant-mock) and [servant-generate](http://hackage.haskell.org/package/servant-generate) which let you respectively 1/ generate request handlers that just generate random responses of the right type (using the `Arbitrary` instance for the response type) and 2/ generate "handler stubs" for one or more endpoints in a quite generic/sophisticated way (where sophisticated is both good and bad, in this case).
vscode with [simple ghc integration](https://marketplace.visualstudio.com/items?itemName=dramforever.vscode-ghc-simple) is quite easy to get started.
FYI I find the code color scheme hard to read, both because it's white-on-black while the article itself is black-on-white and because the contrast is quite low. (Comments, in particular, are almost impossible for me to read, and there's quite a bit of useful information in the comments.) This is partly subjective but it's also an accessibility concern. You can use a [contrast checking tool](https://contrastchecker.com/) to make sure your color scheme is widely accessible, but it's probably easiest to just pick a different theme that uses higher contrast colors (and perhaps black-on-white to match the article, although that is a personal preference). Please don't take this as a criticism of the material, which I quite like.
accessing a file, database, environment variable, service, etc all must be done through `IO`.
Anything that can store state can be abused to become a database of sorts :) I agree that message queues / stream processors are conceptually databases, but database is a less specific concept, so I don't think it should be preferred over thinking of Kafka as a stream processor, since it's more likely to just confuse the issue. The fact that NYT stores their catalogue in Kafka is pretty appalling to me. I'll be googling this... if you have any links, that would be appreciated.
https://www.confluent.io/blog/publishing-apache-kafka-new-york-times/ As for queue-optimised database abstraction, I find it helps to understand Kafka since it can be configured to be either a message queue, a pub/sub system, or a database. It can get a little confusing if you compare it to dedicated systems like ZeroMQ
Sure. Whole thread: [https://twitter.com/domenkozar/status/1061244117777297409](https://twitter.com/domenkozar/status/1061244117777297409)
Thanks! This looks very interesting! :) I haven't come far in your article yet, but I already feel like the initial type declarations could already profit from explicit kind annotations.
Do you know of any good examples of this? I haven't read a lot of code that leverages the lens library but I've been interested in uses of the MonadReader instance in Getters like this
At Freckle we use Arbitrary to generate mock data with this library [https://github.com/frontrowed/graphula/](https://github.com/frontrowed/graphula/)
Why do you call `Validated` a product type and why don't you use a `newtype`? data Validated t = Validated t 
&gt; I picked one suggestion given by stack that is adding `allow-newer: true` into the `stack.yaml`. I would have tried this too, but really, you're lucky that it worked! Compiling someone else's code isn't as easy as it sounds: a given codebase only compiles with some versions of their dependencies, some versions of the compiler, and some versions of the operating system. Cabal's solution is to ask the code author to specify what those versions are, and then when you try to compile, it tries to find a "build plan", a set of versions of the dependencies match both your system and the requirements of the codebase. The fact that you got an error about `&lt;&gt;` rather than a "no build plan" error means that the author did not specify those versions correctly. I don't blame them, it's not easy. Stack's solution is very different: it asks the author to pick a single version of ghc and of all the dependencies, and then when you try to compile, stack downloads that version of ghc and of all the dependencies. Sounds like this should obviously work, except as you've pointed out, it does not work with your OS! As much as stack tries to make builds reproducible by reproducing as much of the author's build environment as possible, it cannot download a copy of the author's operating system. Well, it can, by adding docker (which stack supports!), but I digress. Apple made some backwards-incompatible changes to their OS which makes it hard to build older versions of ghc, stack only supports one build plan and so you're out of luck if that build plan doesn't work on your OS, and cabal may produce an incorrect build plan if provided incorrect version information. All of those projects are trying to do the best they can, but it's a complex problem and so sometimes it doesn't work. I don't know why `stack exec` cannot find your executable, but it did succeed at building that executable, and again, that's pretty lucky, because it sounds like all the odds were against you this time!
Thanks! I bought the book, so far really good. Do you have plan to convert it to epub format? Reading PDF on phone isn't as nice as reading epub.
Hey I appreciate you taking a look at the article! If you're trying to get shit done (tm), I actually think [the servant guide](https://haskell-servant.readthedocs.io/en/stable/tutorial/ApiType.html) is 100% what you should be reading for a decent start. I do a bunch of stuff (especially in part #2) that is a little bit over the top (like wanting to be able to tell the difference between a `CompletedTask` and a `NotStartedTask`)... Part 3 will definitely be better though, we can actually make the HTTP service I've been promising this whole time! 
Hey sounds like you're going to be bored with part 3! I basically do that, but only use typeclasses on monads, without `lens`. I haven't actually fully grokked lenses/prisms/generic-lens yet (which is why I'm avoiding them), but [George Wilson's talk on Classy Optics](https://www.youtube.com/watch?v=GZPup5Iuaqw) was huge to me (there's also [one more talk by Ben Kolera which is relevant as well](https://www.youtube.com/watch?v=pzouxmWiemg)) . At that point I'd already been doing something similar but without `lens` (just stringing together accessors) and didn't feel too ashamed. I'm not sure which part you meant about why my approach is needed, but it's definitely *not*! I can justify *why* I wrote the article like I did: - The basic `Task` to show how - The `Task f` to be able to easily express partial tasks (`Task Maybe` vs `Task Identity`) - The `Task f state` to be able to express/enforce the state of the task at the type level (`Task Identity Completed` ~ `CompletedTask`), then creating `TaskState` to union over the various distinct task states when I wasn't sure what state something would be in (`Task Identity TaskState`). - `Validatable a` to capture the class of things that can be checked for validity -- this comes up pretty often while modeling/sanitizing inputs etc - `Component`s as an state-carrying organizational method for grouping functions on domain models - `WithID a` as a way to distinguish domain models from other concerns (I choose to consider objects as distinct from their identification) - `Constructable c cfg` as a way to capture the class of `Component`s that are constructable given some configuration - Migration machinery because real apps needed migrations of course - `TaskStore` and `SQLiteTaskStore` to actually show what the component would look like
Hey thanks for noting this -- how would you improve it? Honestly I'm kind of over the entire theme I'm using on the site (and the version of [highlighter.js](https://highlightjs.org/) it comes with, I think iI'm also wasting space too much space. I use [hugo](https://gohugo.io) and have been looking for [a new theme](https://themes.gohugo.io/) that's a little bit denser, more minimal, and uses more of the screen real-estate. How would you suggest I improve it? Should I just stick to black-on-white everywhere? or maybe go white on black everywhere? 
Ok I'm confused, what point are you trying to argue? Yes linear types doesn't make: b = ArrayList.append 'b' a c = ArrayList.append 'c' a work. In fact they do the exact opposite, they make the above not type-check. But initially I responded to: &gt; it is tricky to prove that your access is in fact linear (and if it's not, you may get a large performance hit). I wonder if the linear types extension will help here? I correctly said that linear types will allow you to prove that your access is linear. Later you said: &gt; Linear types don't guarantee that 'you' hold the only reference to a value. Thus, you can't safely mutate it. &gt; Uniqueness types enable safe mutation. The paper makes it clear that it DOES allow you to have guarantees about only you having a reference to a value.
If we had a proper ordering or ring hierarchy none of this would be an issue. `Bool` would be an instance of both `Lattice` and `Semiring`, and `a -&gt; b` would be an instance of `Lattice` and `Semiring` IFF `b` is an instance. So then you can write both: foo x = isSpace x \/ isAlpha x foo x = isSpace x + isAlpha x and foo xs = filter (isSpace \/ isAlpha) xs foo xs = filter (isSpace + isAlpha) xs Shared syntax for everything but still with a strong set of laws underpinning it all.
I gave this talk at the Bay Area Haskell Meetup in August. Tell me if you have any questions! Also, I'm definitely not an expert on Cryptonite so I'd welcome any feedback on this talk.
As the author of toodles, I'll chime in here - I don't have much to add on pros and cons of the choice of modules in general. However, for small projects, splitting things up in to modules is often just a small pre-optimization. Having things in one file is often very convenient and the drawback of the single-file approach don't kick in until the file is a certain size. So my overall thinking was "let's break this into modules when we need to, since that day may never arrive." Fortunately, it did! It's a great problem to have :) 
I know the author wrote a disclaimer about being "less rigorous" but I found some parts to be just plain wrong: &amp;#x200B; &gt;If we think of L(A) as a function, then the above is its Taylor-series expansion Not true, as a Taylor series is an infinite sum, while L(A) is a sum of a finite number of terms (for all finite n). &amp;#x200B; &gt;from which it follows that ... L(A) = 1/(1-A) This is also a problematic statement. It's true that "The Maclaurin series for 1/(1 − *x)* is the [geometric series](https://en.wikipedia.org/wiki/Geometric_series) 1+x+x^(2) \+ ..." . However, [that series is only valid when the magnitude of x &lt; 1](https://www.wolframalpha.com/input/?i=Maclaurin+series+of+1%2F(1-x)). And since A corresponds to x and is supposed to be the cardinality of some type, what does it mean for the magnitude of the number of values of a type to be less than 1? Every type I'm familiar with has 0, 1, or some other natural number of values (i.e. not a fractional number of values between -1 and 1), so this series interpretation is only true for types with 0 values. How is that a useful interpretation? So for the specific example the author chose, the algebraic representation of lists does *not* involve division nor subtraction.
You're actually using WAI there, not that the distinction is hugely important at this point ;). As I understand it, WAI is a slightly higher-level interface for servers, and Warp is like a "backend" that actually implements the underlying socket handling and so on.
It's rather handwavey in general, who even knows if subtraction or division is well defined on types, and if it is in this case? (There's some paper on it, but I don't believe it addresses this question)
I have plans! It's not entirely clear how to do that though, so I'm focusing on the print version this week and will then think about epubs! Thanks for the kind words!
Are codata types in Agda desugared to function types? OP goes to great lengths to avoid adding data types to his language so I infer he is even less eager to add codata types. Other than that, I agree. The link should be fixed, it's Fritz Henglein's "Compositional specification of commercial contracts".
Maybe drop special syntax for `Record` entirely, and use braces for `Row` only? It isn't so bad to prefix open-records with the `Record` type, like foo :: Record { x :: Int, y :: Int } -&gt; asdf Alternatively, since a `Row` is metaphorically a "lifted" `Record`, prefix it with a `'`? type Foo = '{ x :: Int, y :: Int | z }
&gt; as a Taylor series is an infinite sum, while L(A) is a sum of a finite number of terms (for all finite n, i.e. all finite length lists). huh? no, that's a perfectly valid Taylor series. ("all finite n" is an infinite set, btw, it's called the natural numbers...) Second: convergence is not that important here, you can work in the [ring of formal power series](https://en.wikipedia.org/wiki/Ring_of_formal_power_series) . You can even implement the reciprocal function for power series in Haskell. Third comment (to the original article): you can solve the problem of N = 1+N by introducing a formal variable, say q: N = 1+qN, which "refines" your solution by also counting how many natural numbers are of "size" n; and not surprisingly, you get back 1/(1-q) = 1+q+q^2 + ... as there are exactly 1 natural numbers for each "size". The same way you can count for example the number of any kind of trees for different refinements, for example you want to count binary trees on n nodes, just solve T = 1+q*T^2, and you get the generating function for Catalan numbers, counting binary trees (you have two solutions, but one of them is "fake").
Thank you for sharing. If you have multiple `ghc` executables installed, i.e. you have `ghc-7.10.3` and `ghc-8.2.2` somewhere, you can tell cabal which one to use with `cabal run -w /path/to/ghc-8.2.2` or `cabal install -w /path/to/ghc-8.2.2`. In addition to the `stack build` command there is a `stack install` command that after building your project copies stuff from `.stack-work/dist/....` to `.stack-work/install/...` and `~/.local/bin/...`.
&gt; Alternatively, since a `Row` is metaphorically a "lifted" `Record` [..] I don't quite understand this. Is it because `Row` is an uninhabited type?
Come to think of it, isn't a record type basically a good old `data` type, but without a constructor? data MyRecord = {name :: String, age :: Word} and, if we consider it as such, isn't a "traditional" `data` type: data MyType = MyType {name :: String, age :: Word} just a `newtype` of a record: newtype MyType = MyType MyRecord
If you pause to think about it, algebraic data types sound like voodoo magic. Now you're telling me you can have *derivatives* for types? Wut is this madness
cool, thanks for the recommendation
Don't forget [servant-aeson-specs](https://hackage.haskell.org/package/servant-aeson-specs)
Thanks! Is pattern matching generally preferred over using partial functions? Or just in this case because I’m using pattern matching anyway? What do you mean that takeWhile is break? 
Thanks for pointing this out -- I haven't actually used `DataKinds` or type families in my own code outside of making custom combinators for `servant` once upon a time. From what I understand about how they work (in the general "type-level function" sense), it seems like it could completely replace all those type synonyms I made. How would I express a partially-specified (so `Task Maybe` under the old scheme) but `InProgress` task? 
 data Task p = Task { tName :: TaskPhase p Text , tDescription :: TaskPhase p Text , tState :: TaskPhase p TaskState } taskInProgress :: Task 'InProgress taskInProgress = Task (Just "foo") (Just "bar") Nothing
That looks super powerful, thanks!
Thanks for sharing, the first 10min were enough for me to branch my code and add switch to cryptonite. 
This looks really interesting! For which areas of business have you written REST services? And what's the size (LOC) of these projects?
There's *infinitely many* lists, but any such list is finite of course. Try to manually define the type of lists with elements in `A`, `L(A) = ?`: First, you have the empty list `[]`, which is unique, so `L(A) = 1 + ?`. Secondly, you have the list with one element `[x]`, which has `A` possible values, `L(A) = 1 + A + ?`. Thirdly, you have the list with two elements `[x,y]`, which has `(A,A)` possible values, that is `A^2` by the isomorphism `to (x,y) false = x; to (x,y) true = y; from p = (p false, p true)`, so `L(A) = 1 + A + A^2 + ?` Then you realize that you can go like this indefinitely, because for any list of any length you can always append one more element, therefore the type of lists of elements in `A` is the sum of `A^n` for all `n`. About type division and subtraction, they can be given meaning with continuation shenanigans I'm not too familiar with. They represent some kind of "hole" (as all continuations do, hence the connection they have with zippers) needing to be filled with some kind of product/sum. What `L(A) = 1/(1-A)` is basically saying is that to make a continuation expecting a list, you need a pair of continuations, (sums of values are pairs of continuations) one to be called in the empty list case, one to be called with a sum of continuations, (pairs of values are sums of continuations) either asking for the rest case, or the head case. I might be wrong on the details on this one, but that's the idea of it.
Very nice post, I recently made a post asking what strategies people use for testing their REST api. Is this something you will cover?
[here!](https://www.cs.indiana.edu/~sabry/papers/rational.pdf)
To explain my response: 1) Nothing that need a central node is really distributed. That implies a single point of failure either hardware failure, software failure, maintenance, management, business or political failure. there are a lot of interest on centralizing distributed computing. No evolution of technology is neutral. 2) Never anyting that centralizes configurations for many applications is really distributed. A single thing for forwarding messages for many applications is a really bad bad idea. That kind of false distribution in a false cloud that gives a false impression of freedom is what greedy engineers, administrators, businessment and politicians with totalitary mindset want, to enslave their portion of Internet for their personal agendas. Never ever has been information and the power over information more centralized and vulnerable to high profile delinquents. 4) It is a mess to configure and maintain, so this reinforces 2. 3) The problem of publish suscribe in the network has been solved without the need of central relays. For example IP multicast, used by [this](https://ipfs.io/ipfs/QmXoypizjW3WknFiJnKLwHCnL72vedxjQkDDP1mXWo6uco/wiki/Data_Distribution_Service.html) . Also IPFS has a publish-suscribe mechanism without central infrastructure that probably use multicast too. 
Hey thanks for the code! Do you mind if I try and add this to the post once I can figure this out and credit your comment? Also, I might be confused about how type families work but why are `'Partial` and `'InProgress` both substituting for the task phase? Partial completeness of the task itself and the state the task is in were meant to be orthogonal -- like you could receive a `Partial` of a `Completed` task, etc. Would you mind pointing me to any good literature on type families? It's not immediately clear where the kind `'InProgress` turns into both the completion type (`Complete`, `InProgress`, etc), and *also* allows things to be `Maybe`/Identity`. Also, why does a `taskInProgress have `tState` set to Nothing? does `(TaskState p)` just always resolve to the the appropriate state for the phase?
That type system turned out to be trivial.
Does no-one use happstack? It's worked well for me in the past.
I have been thinking about making a library like this for a while now
I've been part of writing lots of REST services for a bunch of companies over the years, Java, NodeJS, Ruby, Clojure, etc. **Unfortunately I've never been fortunate enough to work at a company that embraced Haskell though**, but I do my own projects in it from time to time (like [techjobs.tokyo](https://techjobs.tokyo))! All these concepts are not really specific to Haskell, but I don't see articles that talk about them (nevermind how they can be implemented in Haskell enough), so I wrote this.
I really like wai / wrap because it is simple and enough for my needs.
&gt; a Taylor series is an infinite sum, while L(A) is a sum of a finite number of terms This is just... not true.
1. The cluster is distributed. The cluster is not a Single Point of Failure (SPoF) thanks to replicas. And it's not mandatory to put all your data in a single cluster, so even if you're afraid of having a single cluster for your organization, don't be. Kafka handles perfectly well High Availability, so there's no SPoF on hardware, software, maintenance or anything. The biggest risk is your code sending crap inside a topic. But this risk is technology-agnostic. Your code would do the same thing with any queuing/streaming solution. 2. Nobody talked about cloud but you. A Kafka cluster may very well be on-premise. What's the problem with storing configuration of several applications inside the same **distributed** cluster? I'm not even sure why you start talking about totalitarianism, this is off topic (no pun intended) by a long shot. 3. Here I tend to think you are talking about something entirely different. Management? Business? Lawyer? Political? We are talking about [Apache Kafka](https://kafka.apache.org/), an open source queuing/streaming technology. What are you talking about? 4. It has never been mandatory with Kafka to have a single cluster for a whole organization. Sure, it's best (to benefit from features like Kafka Streams), but in no way mandatory. Technically nothing prevents you from having a Kafka for each application inside your organization (though the resources/maintenance wastes will be huge).
Moved to the top level [To explain my response](https://www.reddit.com/r/haskell/comments/9w7v7n/what_is_your_opinion_of_kafka/e9jm8h6/) 1) Nothing that need a central infrastructure (node or cluster) is really distributed. That implies a single point of failure either of hardware failure, software failure, maintenance failure, management failure, business failure or political failure. There are a lot of interest on centralizing distributed computing. No evolution of technology is neutral. 2) Never anyting that centralizes configurations for many applications is really distributed. A single thing for forwarding messages for many applications is a really bad bad idea. That kind of false distribution in a false cloud that gives a false impression of freedom is what greedy engineers, administrators, businessment and politicians with totalitary mindset want, to enslave their portion of Internet for their personal agendas. Never ever has been information and the power over information more centralized and vulnerable to high profile delinquents. 3) It is a mess to configure and maintain, so this reinforces 2. This kind of false solutions drain power from the user to the specialists: tecnical specialists, management specialists, business specialists, lawyer specialist and political specialist. AWS-style of centralized cloud computing is a danger for freedom at a massive scale. But also a technical administrator of a small server in a small business can mount his own small dictatorship. 4) Its very name means something : The word Kafkaesque is often applied to bizarre and impersonal administrative situations where the individual feels powerless to understand or control what is happening. 5) The problem of publish suscribe in the network has been solved without the need of central relays. For example IP multicast, used by this publish suscribe standard . Also IPFS has a publish-suscribe mechanism without central infrastructure that probably use multicast too.
Since you're wrapping every field in an abstract type constructor as in data Task f state = Task { tName :: f Text , tDescription :: f Text , tState :: f state } you should think about using [rank2classes](http://hackage.haskell.org/package/rank2classes) or [conkin](http://hackage.haskell.org/package/conkin). This would simplify your `Validatable` and other code. 
Thanks for the recommendation, I'll look into it! If I can grok it I'll add it to the post!
In the bottom right hand of vscode there should be a menu which allows you to choose the module. Either choose the current module you're working on, or choose "all" make sure to click validate before exiting the context menu
1. What do you mean by software failure? A bug in Kafka code? Yes, it's possible, just like a bug in PostgreSQL which would result in data loss. I don't see how it's relevant. And how does any other technical solution protect against software/maintenance/management/business/political failures? If you refuse to explain why Kafka has so many failure risks, at least explain how other solution don't have these, and we will try to infer your argument. As for 2, 3 and 4, you simply did not respond with any argument/explanation, so I guess we will stop our discussion here. Me failing to see the big picture and you refusing to show it to everyone, we have reached a dead end.
&gt; Is pattern matching generally preferred over using partial functions? Anything is preferred over partial functions.
+1 for this for an absolute beginner
Can you make the source examples available?
There's at least two libraries already mentioned in the comments here that help you do exactly that, perhaps you might want to have a look at those first or do you find them obviously lacking somehow?
If you don't want Template Haskell, the instances for your types are not that hard to define manually, assuming you swap the type parameters on `Task`: data Task state f = Task { tName :: f TaskName , tDescription :: f TaskDesc , tState :: f state } instance Rank2.Functor (Task state) where f &lt;$&gt; Task{..} = Task{tName= f tName, tDescription= f tDescription, tState= f tState} instance Rank2.Apply (Task state) where f &lt;*&gt; t = Task{tName= tName f `Rank2.apply` tName t, tDescription= tDescription f `Rank2.apply` tDescription t, tState= tState f `Rank2.apply` tState t} instance Rank2.Foldable (Task state) where foldMap f Task{..} = f tName &lt;&gt; f tDescription &lt;&gt; f tState instance Rank2.Traversable (Task state) where traverse f Task{..} = Task &lt;$&gt; f tName &lt;*&gt; f tDescription &lt;*&gt; f tState instance Rank2.DistributiveTraversable (Task state) instance Rank2.Distributive (Task state) where cotraverse w f = Task{tName= w (tName &lt;$&gt; f), tDescription= w (tDescription &lt;$&gt; f), tState= w (tState &lt;$&gt; f)} So what are the instances good for? For example, you can replace the ad-hoc class `Validatable` with a `Task` record that contains a validator function for every field: type TaskValidation state f = Task state (f Rank2.~&gt; Either ValidationError) completedTaskValidation :: TaskValidation Finished Identity completedTaskValidation = Task{tName= Rank2.Arrow (checkNotNull getTName), tDescription= Rank2.Arrow (checkNotNull getTDesc), tState= Rank2.Arrow (Right . runIdentity)} where checkNotNull f (Identity s) | DT.null (f s) = Left NullField | otherwise = Right s validateTask :: Task Finished Identity -&gt; Task Finished (Either ValidationError) validateTask = (completedTaskValidation Rank2.&lt;*&gt;) Instead of `InvalidField FieldName` the I chose to shorten the code and produce the nameless `NullField` errors, but there's no loss because the errors are inside the record fields. 
You might appreciate [this](https://github.com/kosmikus/ghc-compact-holes).
&gt; here Checking them out now, as I was especially not aware of servant-mock and servant-generate. The thing I'm interested in doing is standardizing certain Servant CRUD endpoints with types like the following: ``` type CRUDResourceAPI (resourceName :: Symbol) c a i baseEntity = resourceName :&gt; ( GetListAPI c a :&lt;|&gt; GetItemAPI a i :&lt;|&gt; MutateEndpoints a baseEntity i ) type MutateEndpoints masterEntity baseEntity i = CreateAPI baseEntity masterEntity :&lt;|&gt; UpdateAPI baseEntity i :&lt;|&gt; DeleteAPI i ``` Then given a "grouped endpoint" like this, I can auto generate stateful properties of the server like: "Given a CRUDResourceAPI endpoint, if I do 5 POSTS, then a GET, I should get a list of length 5 back" In a given CRUD app I almost always have Servant definitions like this: (see this...though it's a bit dated https://github.com/smaccoun/beam-servant/blob/master/src/Api/Resource.hs) My idea is to automate based on these grouped endpoints. You might be right though, these libraries might already do the job. I need to dig into them a bit deeper 
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [smaccoun/beam-servant/.../**Resource.hs** (master → 1560f81)](https://github.com/smaccoun/beam-servant/blob/1560f81c56ca7a324be1ee68208fb05c4dfea844/src/Api/Resource.hs) ---- 
Great!
I'm using the finite-typelits package to create a type with only a finite number of members. I want a type that can contain elements from 1 to n: newtype ZeroToN n = ZTN (Finite (n+1)) Unfortunately, using this definition adds a constraint (1 &lt;= n+1) to many of my functions, for instance maxVal :: (KnownNat n, 1 &lt;= n+1) =&gt; ZeroToN n maxVal = let zero = natToFinite (Proxy :: Proxy 0) :: Finite 1 in ZTN $ shiftN zero I'd like to be able to propagate ZeroToN through my code using only KnownNat, not the 1 &lt;= n+1. It seems particularly silly as it's always true. However, &lt;= is a closed type family so I can't figure a way to convince the type checker that (KnownNat n) implies (1 &lt;= n+1). Any suggestions as to how to simplify this constraint? Or maybe a better idea as to implementing this type?
Awesome. I really want more compactness for GHC error messages in general. All this "in the expression ..., in the expression ..., in the equation for..." nonsense is useless when you can just underline the actual source. I also question the usefulness of the "Where: ‘t’ is a rigid type variable bound by ..." and "Relevant bindings include ..." sections; I get how they *might* be useful, but I've personally only found them to be noise. I really want a mode for GHC that says "Just tell me the expected and actual types, and don't give me any unsolicited information."
I thought the survey was generally OK and though it was a bit long, I didn't mind taking the time to answer the questions (which was insignificant in the grand scheme of things). That being said, I think "the state of Haskell", informed or not by the survey results, is clear: The Haskell language is one of the most expressive and capable there is. It represents the combined intellectual product of hundreds (thousands?) of smart theoreticians and developers over a span of decades. I have been immersed in Haskell for about six months (coming from a multi-paradigm background of C# /F# /C++ /SQL) and I feel like I've barely scratched the surface. Unfortunately, however, there are two huge problems that I have not figured out how to resolve and would ultimately preclude me from ever suggesting Haskell as an implementation choice for a professional development activity (unless I'm the sole developer on the project!): Tooling, Ecosystem and Integration The tooling situation is awful and the ecosystem is fragmented to a disturbing degree (dueling package managers, few canonical implementations of anything aside from the core libraries, etc.) Defining a "base" library for a specific effort feels like a research project instead of a development activity. On the integration front, it boggles my mind that several core contributors to Haskell hold (or held) positions in Microsoft Research and yet there is no real integration with .Net. There could be first-class integration, especially with .Net Core, and yet nothing. It's a pity because streamlined integration with .Net would open up the entire MS ecosystem to Haskell. So for whatever it's worth, from the perspective of a relative newcomer to Haskell, the state of Haskell is decidedly mixed.
Are there plans to upstream this (or similar) into mainline GHC? It doesn't seem like a rare opinion that GHC type hole verbosity is more obfuscating than helpful.
Indeed, thanks Julian!
&gt; The zipper of a list is a pair of lists! You should have a heart hardened by years of system-level programming or hollowed by years of corporate middleware drivel not to be enraptured by how beautiful all this is. This made me laugh so hard.
That's certainly a step backwards from your previous version, since you're no longer handling `Nothing` in any of the recursive cases. That actually makes the whole "handle division by returning a `Maybe Float`" thing pretty pointless, since a `Nothing` is every bit as dangerous to `eval` as the division by zero itself; all you've done is delay the explosion by one step. E.g. &gt; eval (Division (Val 2) (Val 0)) 0 -- Handled once. Nothing &gt; eval (Addition (Val 3) (Division (Val 2) (Val 0))) 0 -- Unhandled; explodes. Just *** Exception: Maybe.fromJust: Nothing Really, you should forget that you ever learnt `fromJust` exists. The Applicative and Monad versions might be a bit high tech right now, but pattern matching is fundamental. You're already using it to get the different cases for `TreeExpr`, why not use it to properly handle `Maybe a` values like I showed you?
Thank you! I'm glad you like it. &gt;Why did you choose gtk over Qt for instance ? Mostly based on the merits of other Haskell applications using GTK (gi-gtk), like MovieMonad and Gifcurry, by David Lettier. Also, the haskell-gi packages seemed active and of high quality. I have no real preference regarding the underlying GUI frameworks, themselves. &gt;Is gi-gtk the easiest / most maintained / most feature complete gtk binding ? I can't say for sure. There is also the older gtk2hs ([https://github.com/gtk2hs/gtk2hs](https://github.com/gtk2hs/gtk2hs)), that says you should consider using the gi-gtk package. &gt;Did you review haskell GUIs before choosing ? Yeah, I had a look in there. &gt;Cabal is ok, but does if fit stack lts ? Absolutely, both work. I'm primarily using it within a Nix shell and with Cabal new-build, but in the video I'm using stack, so that's perfectly fine. &gt;Is it cross-platform without quirks I've had some quirks, like GTK+ being slower to patch on macOS, and various high DPI issues on my 4k monitor. On Linux it's pretty good, on Windows it's blurry, and on macOS it scales weirdly. Most other things have worked well. &amp;#x200B;
From what? I ask because the Haskell ecosystem is pretty small as far as crypto offerings go and without too much overlap.
Yes, the demos are available at [https://github.com/MaxGabriel/cryptonite-talk/tree/master/demos ](https://www.youtube.com/redirect?event=comments&amp;stzid=Ugy91RZI-pOpHlUgAw54AaABAg.8n_JaV5SNOo8naonSfjoVL&amp;q=https%3A%2F%2Fgithub.com%2FMaxGabriel%2Fcryptonite-talk%2Ftree%2Fmaster%2Fdemos&amp;redir_token=3k0VQ0VT6wD-t1hf83W0BweDPeJ8MTU0MjIxODU3M0AxNTQyMTMyMTcz). That repo also has the full source code of the presentation itself
Glad you enjoyed!
We do have formalisms for the Core language which Haskell desugars to, see [`core-spec`](https://github.com/ghc/ghc/tree/master/docs/core-spec) in the ghc source tree. You may also be interested in [`hs-to-coq`](https://github.com/antalsz/hs-to-coq).
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [ghc/ghc/.../**core-spec** (master → 5b98a38)](https://github.com/ghc/ghc/tree/5b98a38a32f2bc8491dc897631be8892919e2143/docs/core-spec) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply e9mwaz0.)
Whatever is closest to your previous language. I've used vim since '99 or so, so I just continued that when I picked up Haskell in '08 (?).
It's worth noting that this does not fully define the operational semantics of Core in the way that GHC implements.
Yes please! This would save me a bunch of time scanning through my type errors.
&gt; If you find yourself wanting modularity, then type classes are the wrong tool to reach for. Amen. Type classes, particularly the uniqueness (or at least global coherence), is about as anti-modular as it gets. --- You might consider de-class-ification, and just pass around records; you can always use the Reader Monad if explicitly passing them around gets noisy.
It seems like this is another example of where proper anonymous row/record types is the best solution. As that would meet all of your requirements whilst being very ergonomic. 
I understand why partial functions are bad, but why are they bad if I’m defining the rest of the function separately?
https://hackage.haskell.org/package/bcrypt-0.0.11/docs/Crypto-BCrypt.html#v:hashPasswordUsingPolicy
Thank you!
If you want a small wrapper around WAI/Warp that handles url routing and otherwise pretty much gets out of you way, we wrote this: http://hackage.haskell.org/package/fn
What if we created a kind, where expressions with a type belonging to this kind require a proof that the expression terminates? There's a totality checker in Idris which can say whether a term is total without false positives (but with false negatives). Could this be used as a basis for inhabited types which cannot contain "bottom"?
The title sounds like "GHC has been patched with ..."
Maintenance.
Because it separates the parts of the function definition and you might erase or change one without changing the other?
A "partial function" is not a function defined with multiple clauses / "in parts". A partial function is one that provides a bottom result for a non-bottom input. For example, `head` is a partial function since it has no possible non-bottom value to produce when provided the (non-bottom) input `[]`. A total function will always be safe to call. A partial (non-total) function may be safe to call under a specific context with specific inputs, but later updates might migrate the call outside of the safe context, or alter the context so that the existing call is no longer safe with the current inputs.
same
Can I use Haskell instead of javascript to perform the logic of a web page ? if so how ?
Very long time ago I wrote this: https://ghc.haskell.org/trac/ghc/wiki/Proposal/ErrorMessages It never come to anything but good to see I'm not alone.
[There's several ways to compile Haskell to JavaScript](https://wiki.haskell.org/The_JavaScript_Problem#Haskell_-.3E_JS). There's also [PureScript](http://www.purescript.org/) is similar to Haskell, and also compiles to JavaScript.
Oooh thanks for the paper link -- I've added it to my reading list. It is almost certainly the case that the state parameter was unneeded... I felt it while I was writing it but even more so. I thought the ability to differentiate a `CompletedTask` and a `NotStartedTask` was neat though. The thing with `Validate` is that it actually had much less to do with the types themselves and more to do with semantics -- like if I decided for some reason to limit the characters that could go into a task's name for example. It seems like the definition you've built up there is more about the `Maybe`/`Identity` disparity and less about semantic checks. From what I understand the only way to bring the semantic checks that I want is Dependent types? GHC.Generics is another area I really need to bone up, there's a lot I want to do with it (and a lot that is now possible of course) -- not going to lie, the nomenclature is really intimidating, even with the knowledge that it's basically just an involved case match on all the constructors.
Thanks for the long and considered example! In this case (at least for the article), I hadn't considered that a completed task would have different validation than a task that hadn't been started -- I *want* (at least for now, until I see a case for the opposite) to abstract over the state for `Validatable`. Changing the order is an interesting suggestion and I think it would make things much easier actually. I haven't dug into rank2 types much but I will try and see if I can apply anything. I'm not sure if there's much gain from adding it here though, since I don't understand Rank2 types well yet, there seems to be a lot more abstract instances that compound to a prematurely specialized validation check... it doesn't actually seem much different than what I have now, except I still have the option of asbtracting over `state` in my definition as long as `state` is also `Validatable`...
This is really cool!
Nice! 
Hey I know this is really late but thanks for the help with this. I just got to the point where I really understand it and it has helped me a lot. Thank you!
No problem, you're welcome!
Is it possible to extend this to kill all threads transitively spawned by the forked thread? Otherwise you can still run into issues with background threads.
&gt; Is it possible to extend this to kill all threads transitively spawned by the forked thread? I am not sure if it is possible and I think it might also be best to let the forked thread decide the fate of its children. I have added the following to the [FAQ](https://github.com/hamishmack/ghci-fork/blob/master/README.md#what-if-my-thread-has-children): What if my thread has children? The parent thread(s) will need to make sure the children are cleaned up. One option would be to use `killThread` (but you could also signal the children with an `MVar` instead): import Control.Exception (bracket) import Control.Concurrent (forkIO, threadDelay, killThread) import Control.Monad (forever) :{ :fork slotName bracket (forkIO $ forever $ putStrLn "Child!" &gt;&gt; threadDelay 5000000) killThread (\_ -&gt; forever $ putStrLn "Parent!" &gt;&gt; threadDelay 5000000) :} If you are using the `distributed-process` library you can use [Monitoring and linking](http://hackage.haskell.org/package/distributed-process-0.7.4/docs/Control-Distributed-Process.html#g:7) to ensure children are clean up when the parent terminates.
You can see two possible solutions for this problem: using CPP or TemplateHaskell * https://stackoverflow.com/questions/45113205/is-there-a-way-to-shorten-this-deriving-clause
Thanks for the response, yours was the most helpful in trying to understand the article.
While I'd love to see ghc emit more compact error messages, I wonder if in the meantime a bit of perl to post-process ghc's error output might not also do the trick. Basically the same idea as how c++filt used to be used to demangle gcc errors before gcc got around to doing it itself. Actually I'm really wondering if anyone's already done so, or at least taken a serious stab at it.
Oh I totally misunderstood them. Thanks for explaining!
Thanks for the response. &amp;#x200B; As the article did not mention formal power series or anything like that, you'll forgive me for assuming that when the author said that L(A) is a function, they were talking about a function on real or complex numbers (and in Haskell, the cardinality of every type A is a natural number, further reason to make that assumption). If the author really intends for such a nonstandard interpretation, they should state it. 
We could change the parsing rules so that the function being applied in a function application cannot be a number literal and then that would be a parse error. One could recover the confusing behavior by parenthesizing (2) 4 
This is really nice for someone like me who enjoys working in the terminal - thanks a lot for your work!
That is right. But they still might do such a thing silently just for the convergence. Anyways, my line of thought is not related to the article. Was just an idea. Other comments mention formal power series, generating functions, and such, those are relevant here. 
&gt; An arbitrary linear function is not allowed to mutate its argument, because there is no guarantee (from this function's perspective) that there are no other references to that argument. newMArray is a special case, not the general case. To mutate in the general case, you need uniqueness types. Ah right, yeah I am fully aware of that, I know you have to do the whole `Int -&gt; (MArray a -o Unrestricted b) -o b` thing, but that doesn't (IMO) detract from the idea that linear types are sufficient for safe mutation, you just have to have the above interface.
Hi, I a member of the PL and compiler construction group at the University Kiel and I can assure you that Curry is still in active development. May I ask what give you the impression that the project is dropped? Ping me in private if you like. Furthermore, I like to do a little self-promotion, but emphasise that this is still a work in progress: we started to reanimate an approach by Abel et al. that translates Haskell programs to Agda by using a monadic transformation that is indeed similar to the transformation that one of the Curry compilers (KiCS2) uses to generate Haskell code. Instead of Agda we implemented it in Coq. https://github.com/ichistmeinname/free-proving/ 
I had the same trouble (it was rebuilding pandoc), which was fixed by upgrading to (then) latest stack and managing to do one clean build after which it worked. But I suspect there are multiple causes for this problem.
&gt; and in Haskell, the cardinality of every type A is a natural number that's not true either: data Bool = True | False -- cardinality is 2 data Nat = Zero | Succ Nat -- cardinality is countably infinite type R = Nat -&gt; Bool -- cardinality is uncountably infinite You are nitpicking while failing to see the forest from the tree. Try to be a bit more open-minded for a change.
I upgraded stack and still have the problem. However, I had some relative path that I changed to absolute it and seems to be working now. Thanks
Since there is no valid definition for this function in the first place: `forall a. forall b. a -&gt; b` There's is not a valid for this one, right? `forall a. forall b. ((a -&gt; b) -&gt; a) -&gt; a` Also if there was, there's still no way to know the type of a to return it with only `((a -&gt; b) -&gt; a) -&gt; a` as context, right? or am I missing something?
Probably won't understand any of this but I love the title
Nice!
Nice! I like it. Should be much easier to pop this into the GHCi proper too.
I am not an expert here, but I would have thought that `Text`, as a sequence of Unicode characters, cannot contain such escape sequences, and only binary data (e.g. `ByteString`) can. Does this only work if `Text` is encoded to `utf8` before printing it? Also, {-# NOINLINE isatty #-} isatty :: Bool isatty = unsafePerformIO (c_isatty 1) == 1 foreign import ccall unsafe "isatty" c_isatty :: CInt -&gt; IO CInt looks fishy: What if I want to print the color codes to something else than `stdout`? And finally -- Collapse surround/surround to a single surround before phase 1 {-# RULES "surround/surround" [~1] forall a b c d s. surround a b (surround c d s) = surround (a &lt;&gt; semi &lt;&gt; c) (b &lt;&gt; semi &lt;&gt; d) s #-} is neat, but changes the observable behaviour of the program. I guess it is debatable if that is ok for a rewrite rule.
Yes, you're right - WAI-only and not using *specific* warp features. So you're perfectly right, I've found pure **WAI** to be sufficient even for medium-complexity http routing.
Trump? sad
[removed]
It's great to see Haskell catching on in Belgium.
I'll give you an example of each one of them: - software failure, installation failure: anything related with software, code failure, configuration failure, installation of a new version failure etc. - Management failure: someone in other department manage the kafka installation and the manager that control your department and theirs fails in the arbitration of the conflicts between both departments. - business failure: you contract the management of the kafka installation to a third company in charge of maintenance and this company switch priorities or disspear, so the quality of the service degrades. anbther example: your service provider steals your kafka information to sell it to competitors. - political failure: a big service provider like AWS is forced by the gobernment to forward all your communications to a gobernment agency.
????
Word
The cost is another arbitrary wart to learn and remember.
ANSI escape codes are just sequences like `ESC[32m`, you can store them in `Text` just fine. Regardless of encoding, escape still has a character code. Whether or not the terminal will accept them is a function of the terminal device you feed the `Text` to, though. Generally if you're going to spew this stuff to standard out it is useful to have an option to detect by default and a way to force color output on or off, though. A more "correct" way to do this is to use terminfo/termcap or something to find the right terminal codes for the device you're talking to, rather than inject them raw into the stream, but just dipping down to the lowest common denominator like this is fine for most usecases.
I always enjoy hearing about a new company that I've never heard of before that is using Haskell!
FWIW- `prettyprinter` has rather robust `Text` support, and provides `ansi-terminal` compatibility packages. It has the benefit that you can work with "semantic" markup throughout much of the process and then switch to ANSI or other kinds of markup as needed later in the process. The major downside to the style used here is you need to know the "purpose" of the output text in advance to know whether or not to apply your combinators or not. These don't seem to actually rise to the level of an actually "pretty-printer" library, in the sense that the usual things that someone expects out of pretty-printing (e.g. formatting) aren't present, just some coloring. If that is all you need, then that may well be fine. 
&gt; I would have thought that Text, as a sequence of Unicode characters, cannot contain such escape sequences The escape sequences are ASCII (it's literally just `\27 E S C [`, followed by numbers interspersed with `;`, followed by `m`), so any `Text` encoding should do. This library doesn't do any encoding, just appending. &gt; Also, ... looks fishy: What if I want to print the color codes to something else than stdout? It is fishy :) - and can go wrong in all kinds of ways, such as printing to `stderr` instead, or a file, or any other handle that is not file descriptor 1, or a dup'd stdout... It's really just a feature for the common case of printing colored output on stdout (and occasionally redirecting this output to a file for some other processing). For anything else, you'll have to strip the escape sequences yourself. Is that acceptable? Or is there another technique I could use? &gt; changes the observable behaviour of the program. I guess it is debatable if that is ok for a rewrite rule. Yeah, I know :) I thought about this for a bit. In the narrow domain of using `Text` as an opaque type that can only be put to a terminal, then this rewrite rule is semantic-preserving. But you can check a `Text`'s length, slice it up, and so forth, so it is true that you can get different behavior at `-O`. Is that an uncommon for a rewrite rule? I assume (but am not positive) that you can similarly observe differences in, say, the interleaving of `pipes` constructors at `-O`. I'm happy to hide this pico-optimization behind a flag, or just remove it.
* Software failure: Any software is error prone, I don't see how Kafka is particularly more or less prone to failures than another tech. But it's open source, so I'd trust it more than closed source solutions. * Management failure: Again, it's never been mandatory to have a single Kafka cluster per organization. Have your own cluster for your team. * Business failure: If you're afraid of putting all your eggs in one basket, don't host your Kafka cluster on third party infrastructure. It has nothing to do with Kafka. Where do you host your application code? If in a third party, you have the same risk. If not, use the same infra to host Kafka. * Political failure: Exactly the same response as for Business failure. Don't use third parties you don't trust. This is not a Kafka issue.
I was mostly replying to &gt; I feel it occupies a missing place between ansi-terminal and a more powerful prettyprinter It is probably safer to say it occupies a place between invoking `ansi-terminal` and `text-show`. With that minor shift in claim almost all objections fall away. ;)
Whoopsies, let me edit that right out. Thank you sir.
&gt;looks fishy: What if I want to print the color codes to something else than stdout? Yep, like `stderr`, while redirecting `stdout` into a file. Extremely naive. For example, I do a lot of stuff from an Emacs `*shell*` buffer, which is technically a terminal, but a `dumb` one. Even programs like `apt` don't really respect capabilities, which is a shame. I'm working on a haskell library that occasionally prints colorful text to the terminal. Determining whether you can use fancy unicode characters and escape sequences in a cross-platform way is a pretty daunting task. I believe getting this right with minimal dependencies will be one of the most boring and time-consuming parts of the whole library...
See also https://hackage.haskell.org/package/slave-thread
I understand your point, and I agree... I'm aware of the situations where just naively checking `isatty(1)` goes badly. Maybe we can move on to another topic, like your library that I asked about, unless you have any concrete suggestions, which I am eager to receive.
I think these characters might have something to do with the [Byte Order Mark](https://en.wikipedia.org/wiki/Byte_order_mark). Try recreating these files (with a text editor) with BOM turned off, and see if these characters disappear.
Also, don't write UTF-16 to begin with unless you have no other option.
Thank both of you! I had simply saved the last files in CSV UTF-8 instead of normal CSV. 
For us the transition to Haskell has been recent. Finding talent in Haskell is the first question everyone asks, but people can be found I hope.
Any type ^(\[1\]) that can be built from the empty type, unit type, a type variable \`t\`, sums, products, exponentials/functions, composition, and recursion can be used to get a power series \`F(t)\` (the generating function) in the variable \`t\` with natural number coefficients. If the types \`f t\` and \`t\` have a finite number of inhabitants, then the series is finite and you can evaluate \`|f t|\` by computing \`F(|t|)\`. &amp;#x200B; But what if the series isn't finite? In many cases, all is not lost: even though the power series might have a small radius of convergence, the function \`F(t)\` might still admit an analytic continuation so that \`F(|t|)\` exists. This happens for the list type constructor, for example; lists of bools should have cardinality \`L(2) = 1 / (1 - 2) = -1\`, in that case. Is it weird that we got a negative number here? Yes, sort of; on the other hand, we are trying to extend this "combinatorial" notion of cardinality past finite types, so maybe we need to loosen our ideas about what is possible when we pass to infinite types. &amp;#x200B; There are still places where things can go bad: for lists, you aren't going to get a finite value for \`L(1)\` no matter what, so we can't say much useful about the cardinality of the Haskell type \`\[()\]\`. &amp;#x200B; There is also the issue that analytic continuation need not be unique, so there may be more than one apparent cardinality! The classic example here is the tree type \`data Tree t = Leaf t | Fork (Tree t) (Tree t)\`. The generating function for this type satisfies \`F(t) = 1 + F(t)\^2\`. Solve the quadratic equation to get \`F(t) = (1 +/- sqrt(1 - 4 t)) / 2\`. So the cardinality here is one of two different complex numbers! (or is it both?!) &amp;#x200B; This is all fun, but it isn't that terrific unless we can do reasonable things with this generalized notion of cardinality, right? Like, in the tree case, if we just look at trees with no leaf labels (\`t = ()\`), then the cardinality is \`F(1) = (1 +/- i sqrt(3)) / 2\`. This is a 6th root of 1, so \`F(1)\^6 = 1\`. Are we really supposed to believe that there is just one inhabitant of 6-tuples of trees? That's obviously nonsense. &amp;#x200B; On the other hand, \`F(1)\^7 = F(1)\` is not \*obviously\* nonsense; it would mean there is a pairing between 7-tuples of trees and single trees. [That would be interesting, right?](http://blog.sigfpe.com/2007/09/arboreal-isomorphisms-from-nuclear.html) It is also interesting to note that this isn't the trivial "they are both countably infinite" bijection; there is an actual constructive bijection between them, built using the type constructors and pattern matching. &amp;#x200B; [Fiore and Leinster](https://arxiv.org/abs/math/0212377) showed that this weird phenomenon of "not obviously nonsense ---&gt; actually true" generalizes to a large class of cases. All together, this suggests that the generalized combinatorial cardinality here does actually mean something significant, even if it seems superficially nonsensical. &amp;#x200B; ^(\[1\]) There is a caveat; we do need the sums that define each coefficient to be finite, but we don't actually care at all about the radius of convergence of the resulting series.
Not public ones, no. In our codebase at work the MonadReader side is used with classy lenses, so you can pull things out of the environment inside a do-block like `c &lt;- view somePartOfConfig`. For just one level, it's little better than `asks`, but once you have nested structures, then you get the payoff.
This is interesting
This is not StackOverflow, nor is it Google.
Did you try implementing these yourself? If so, where did you get stuck?
Search for "haskell run length encoding" in your search engine of choice, you'll likely find good things.
&gt; any insights you have from building your library I don't think they're applicable in your case. I just think you're trying to build a generic library that does too many assumptions about the environment it's used in. &gt;is it online anywhere? Not yet, I'm waiting for approval from my employer in order to open-source it. The idea behind the library is to provide predicate combinators for unit testing that result in much more useful and insightful error messages comparing to what one can get with something like `hspec-expectations`. I don't want to write yet another test framework, so I use HUnit assertions which could be used anywhere (plain `hunit`, `hspec` or `tasty`). The problem is that I don't get any configuration from the test framework, I can't read nor add any command-line flags to compute the pretty-printing configuration. I still want to be able to use fancy unicode symbols like ✔ or ✗, colors and italics, but I want to be sure they're used only if the output media supports them. So for now I use some basic heuristics based on environment variables, but I'm planning to also write some ugly platform-dependent C code to access isatty() and term capabilities on Unix and WinAPI on Windows to get information about the environment and the encoding being used.
Why? Is Belgium currently a void of Haskell knowledge? Do we risk a future in which the level of know-how for Haskell in Belgium is excessively lower than the rest of the world? What are you hiding?
Interesting! I worked for a while in the field of TQC, but more with a focus on Majoranas. What are your goals with this project? It would be pretty cool to have some Haskell framework to investigate quantum algorithms using various qubit constructions :)
Cool! And your company seems to be based in Ghent. Is there a Ghent based FP / Haskell / Scala community you know of?
My goals are to succinctly explain anyons without too much of the categorical trappings (maybe until part 3 or so) or other side bull. I really, really wanted that when I was first learning. I’m also trying to build it in a way that “feels right”, but is also simple-ish. As architected at the moment, I’m not really polymorphic in the choice of anyon model, which kind of stinks. I’m largely working on describing and manipulating the state space of anyons in a clean and type safe way. For a non topological quantum computer, building a vector space for qubits is relatively more straightforward, so you don’t need the extended discussion of braiding and special type constrained trees and stuff. I don’t yet know how easy it will be to program actual quantum algorithm’s in my framework, and that isn’t really my focus. I’ll need to build a qubit abstraction overlay over all the anyon crap for that. For more info on quantum computing in Haskell, Quipper is the big name I’m aware of.
Interesting! :) Can you share the slides?
Honestly I can't in good conscience recommend any IDE or editor tooling suitable for a brand new haskeller. We have some pretty good tooling out there, but it's all pretty finicky and a bit fragile, and it may be better to avoid it until you've built a few toy projects and gotten comfortable with the ecosystem. It's not that stuff is mysterious or complex, only that you're going spend some time fixing inevitable environmental issues, and it's drastically easier to do that if you're somewhat familiar with the processes that your editor extension is invoking.
I have this function, f, that takes a list and returns a list of lists but it's not returning exactly what I'm after. For example, this is what it does: f \[3,2,1,2,2,2,4,5,6,7\] = \[\[3,2,2,1\],\[2,2,2,2\],\[4,5,5,6,6,7\]\]. In my current output, for each list I want to remove elements at index 1, 3, 5..etc. but not remove those at index (length - 1) (or above). My desired result is: f \[3,2,1,2,2,2,4,5,6,7\] = \[\[3,2,1\],\[2,2,2\],\[4,5,6,7\]\]. I'm new to Haskell and I'm not sure if there's an easy way to do this!
I'm assuming the pentagon and hexagon diagrams come from the relationship with category theory? If so, do you plan on being more explicit about it later on?
That's very odd -- can you post more details on what you're doing? What operating system are you on? Is this with every project, or just a specific one? If it's a specific one, then posting the `stack.yaml` will help. If it's all of them, then the config in `~/.stack` would be useful to see.
I've never spent time to explore Smalltalk before. This was really eye-opening, thanks!
A very nice post! I've been teaching a small group of programmers Haskell at my day job and we've been working on a REST-ish API together. This post helped give us some inspiration for our work and interesting avenues to explore. Thanks!
Very interesting and informative talk.
Nice! I'm mostly able to follow along (my QM and TQC classes in undergrad are finally useful!). I am, however, curious as to why you wrote `dot` like you did instead of something like: dot :: FibTree a (b, c) -&gt; FibTree a' (b, c) -&gt; Q (FibTree a' a) dot x y | x == y = pure $ case x y of (TTI _ _) (TTI _ _) -&gt; TLeaf (TIT _ _) (TIT _ _) -&gt; TLeaf (TTT _ _) (TTT _ _) -&gt; TLeaf (III _ _) (III _ _) -&gt; ILeaf (ITT _ _) (ITT _ _) -&gt; ILeaf dot _ _ = mempty 
There is the Ghent Functional Programming Meetup. Talks are not as frequents as I would like, but we are still active. Typical audience is around 20 people. If you want to give a talk about what you do with FP, let me know. Industry wise, FP is still used in Ghent at Western Digital (ex-Amplidata), mainly Ocaml. Little bit of Haskell in the past too. And there is Piesync and Openvstorage as well.
Right, you’d be looking for `f` such that `f k` returns a value of any type `a` that the caller specifies; the only way to get an `a` is by calling `k`, but all that `f` can do with `k` is pass it a function of type `a -&gt; b` for any `b` that the caller specifies, and there is no (non-bottom) value of that type that it could use.
A character list can also be written more compactly as a `String` literal: ``isWhitepace = (`elem` " \n\r\t")``. Ideally a parser should use the exact definition of whitespace from the JSON spec, tho.
you might be interested in https://mooc.pharo.org it was published on a large platform but then duplicated on that website. I deeply enjoyed the smalltalk culture. Very very worth the time if you have some to spare.
I wonder if this approach is compatible with the the industry-wide trend of using language servers so that you can work with the language in your editor of choice, instead of sticking to a language specific IDE. On one hand there are many merits to this approach in terms of understanding the codebase. On the other hand, effort needs to be put in to reinvent lots of stuff on an editor-by-editor basis -- this shows in the video where the speaker asks the audience to move closer to be able to read things (presumably because the GUI doesn't support zoom in for arbitrary panes).
Ahhh thank you very much for clarifying, I will add this to the post in the near future. I'm going to try and take as much as I can from the feedback in this thread and update the post this week.
Are you proposing SAAS companies build their infrastructure on IPFS and other fully-decentralized systems? And even more, that not doing so is reckless and inherently bad? This seems..a little nutty lol.
I love Smalltalk and worked used it for half an year during a big university project; it was a charm to fix bugs in!
Hey, thanks for your quick answer, I'm glad my thinking wasn't too far off. I was consulting with other people and they found a function that Haskell sees as satisfying, and wanted to share it. f = t [] r where h p f = seq (map f p) (head p) r x = head $ map (\f -&gt; f x) [] t p g = seq (head $ [g (h p)] ++ [h p r]) g r is a non-terminating function, but still, pretty cool. 
Your first "proof" is right but has a gap, and the second one is not correct. The second reasoning is bad because, for the same argument, you can prove `forall c. c -&gt; c` can not have a program. The standard way to prove some type has no inhabitant is * By reduction: Given the function of that type, you can write a function which is known to be impossible to write. * By directly count every possible program of that type: See below (Below is basically what u/evincarofautumn said, but more verbose) We want to show no valid (finite, use no recursion) program has type `forall a b. ((a -&gt; b) -&gt; a) -&gt; a`. Assume that an expression `E` has such a type. We can assume it is normal form^(citation needed). A normal form program is either: * a variable `x` * `\x -&gt; E`, where E is normal form * `x E1 E2 ... En`, where `x` is a variable and each `E1, E2, ..., En` is normal form The type of `E` is a function, so `E = \x -&gt; F`, where `x :: (a -&gt; b) -&gt; a` and F is some expression of type `a`. `F :: a` can't be a function `\y -&gt; G`. If it was a function, `F :: c -&gt; d` for some c, d but `a` is a rigid type variable. `F :: a` also can't be a variable, because no variable in the scope has type `a`. Thus `F` must be an application `z G1 G2 G3 ... Gn`, where `z` is any variable in the scope with matching type. The only variable in the scope is `x :: (a -&gt; b) -&gt; a`, so `F = x G` and `G :: a -&gt; b`. Because we can't write `G :: a -&gt; b` using only `x :: (a -&gt; b) -&gt; a`^(proof needed), This is not possible. We considered every possible program of type `forall a b. ((a -&gt; b) -&gt; a) -&gt; a`, but there were no such thing. QED.
Hey, thanks for answering. To be clear, the E you reference in the normal form program definition is not the same as the one above, right? &gt;F :: a also can't be a variable, because no variable in the scope has type a. What do you mean about the scope, is the "parameters" of the lambda function?
&gt; To be clear, the E you reference in the normal form program definition is not the same as the one above, right? Yes, sorry for confusing naming. (I noticed I used "program" and "expression" interchangeably, which is also confusing. Sorry, sorry...) &gt; What do you mean about the scope, is the "parameters" of the lambda function? Yes, by scope I mean "all parameters bound by lambda function, available at a specific position of a program".
Easy example: `fromRight :: forall a b. Either a b -&gt; b` is impossible without bottom because `fromRight . Left :: forall a b. a -&gt; b`
I'm starting at the beginning of your blog posts and reading through them.... have you thought about creating a collection or organizing into topics for someone new to the blog?
&gt; r is a non-terminating function If you're fine with that, I can give you a function `forall a b. a -&gt; b`: f x = f $ x
One less indirection
What does the shirt say?
Yes, definitely. This would be great.
It typechecked in a repl but I confess to having not rigorously tested it. It's just pushing the conditionals up and lifting the pure out of the case; both which should hold validly without restraint in Haskell (thanks, laziness :)
We are willing to consider any applications though. I’ll change that part of the requirements. 
&gt; - Business failure: If you're afraid of putting all your eggs in one basket, don't host your Kafka cluster on third party infrastructure. The you reduce the Business failure to Management failure. what was a problem of companies becomes a problem of departments (and so on) That is inherent of logically centralized solutions.
No, it is inherent to all solutions. Host the cluster in your team, just like you do with your code and your database. Then you are exposed to the same risks. There's nothing Kafka specific here.
I would not say it is industry wide trend. 
It would help if I wasn't using stack because of intero ;-)
[removed]
He mentions it at some point during the talk. "You can run from your problem, unless your problem is a Haskell" &amp;#x200B; The "unless" and second "your" are in red, on either side of the skull, hard to spot.
Hmm, it might be that I have a one-sided view of things. Many new languages are adopting LSP (e.g. Swift, Rust, Typescript). Scala 3 is also going to have a main LS. Not sure about Go or Kotlin. Of course, the older mainstream languages might have more mature tooling, so LSP is a harder sell because people are already used to their particular IDE.
A bit of context would be nice. What are your goals for decompression? Rebuild your [[Block]] structure or have a lookup `Int -&gt; Int -&gt; Block`? Is your matrix sparse or dense with respect to one of block states? One of the obvious optimisations would be to store the positions of all, say, empty and destructible blocks and assume that all the rest is indestructible. Anyway, in its current state your question is not haskell-specific.
The survey is now closed. The results will be posted soon. Thank you to the more than 5,000 people that responded! 
I like the idea very much. I have been using \[Reverse dependencies\]([https://packdeps.haskellers.com/reverse](https://packdeps.haskellers.com/reverse)) to find examples of useage of the packages I was interested in, but Ex-hack would be much finer-grained, and actually find snippets of useage. Apparently, Ex-hack still requires some love, but it's definitely a good way to improve documentation in my view.
Ok now we only need hashell which is a shell configured in Haskell.
You could save all the nonempty blocks in a Map with their positions as keys. Even more efficient would be to use the CSR format for sparse matrices ([Wikipedia article](https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_row_(CSR,_CRS_or_Yale_format))). Both methods will only allow you to save space if your matrix has significantly more empty blocks than nonempty blocks.
and onadX, an X implementation configured in Haskell. Maybe systemH, a service manager configured in Haskell.
 f = t [] r where h p f = seq (map f p) (head p) r x = head $ map (\f -&gt; f x) [] t p g = seq (head $ [g (h p)] ++ [h p r]) g Is denotationally equivalent (delete `seq` and it's first argument) to: f = t [] r where h p _ = head p r _ = error "head []" -- additionally applied 'map' and 'head' t _ g = g In which almost everything is unused: f = r -- applied t where -- h was unused r _ = error "head []" -- remove t since it was no longer used So, `f = const $ error "head []"`, so it's not a proper inhabitant of that type, it's a function that always produces bottom.
This is brilliant !
Are you aware of [haskell-indexer](https://github.com/google/haskell-indexer)? A demo is running [here](http://stuff.codereview.me/lts/9.2/) for lts-9.2. I use it exactly for that purpose but the UI isn't as nice as yours. Maybe you could re-use its indexing capabilities since you seem unhappy with the current indexer.
I think that it is dangerous in the short term and lethal in the long term. https://www.reddit.com/r/haskell/comments/9w7v7n/what_is_your_opinion_of_kafka/e9octvz/
So, you got it to the point where you felt like sharing on Reddit. Well done!
The link colors are directly inverse to the default colors of visited and univisited links. It's really confusing and annoying 😒. Here is the issue for Hackage: https://github.com/haskell/hackage-server/issues/793
Don't forget the Do Operating System (DOS), an Operating System configured in Haskell.
How well does the LSP serve the needs of something like Agda or Idris? I imagine dependent types could be an issue with either. I imagine the anonymous modules of Agda could be an issue specific to it. I imagine the type-directed name resolution and implicit variable scope of Idris could be an issue specific to it. I think LSP is probably the right trend, but it may need some more work before we can support the whole gamut from languages from Coq to C, from LF to Lisp, from Smalltalk to Scheme.
That isn't going to work as it won't backtrack to the mempty case when x /= y, but they have the same constructor dot :: FibTree a (b, c) -&gt; FibTree a' (b, c) -&gt; Q (FibTree a' a) dot x@TTI{} y@TTI{} | x == y = pure TLeaf dot x@TIT{} y@TIT{} | x == y = pure TLeaf dot x@TTT{} y@TTT{} | x == y = pure TLeaf dot x@III{} y@III{} | x == y = pure ILeaf dot x@ITT{} y@ITT{} | x == y = pure ILeaf dot _ _ = mempty works though
I was not, thanks for sharing.
Thanks: [https://github.com/NinjaTrappeur/ex-hack/issues/14](https://github.com/NinjaTrappeur/ex-hack/issues/14)
Usually, Doctests are useful examples of usage. Perhaps, doctest should be more widely adopted.
Found the cayley table: https://www.reddit.com/r/straya/comments/78bfbx/yeah_nah/
Naming-wise, I'd vote for `hash` but for the collisions issue (pun not intended but gratefully embraced). Maybe "hush"?
so I'd suggest having: /home/users/devel/ /project/ da/ db/ d/ stack.yaml and let the `stack.yaml` control all of `da`, `db`, and `d`. Then you have one `stack` project and each of the packages will be visible to each other, and `stack` can intelligently control caching across all three packages.
That is one version of it. =) My preferred version is to view it as an exponential weighted average. That way you can zero in on any confidence level you want just by enumerating a sequence of yeahs and nahs.
haha well that would be super nice as far as a highlights page, there is a lot of content here but does seem a bit disconnected. 
What does it mean for a terminal emulator to have an underlying terminal emulator? What does `Termonad` do, then?
I really enjoy image-based development and Pharo is really slick. It would be really nice to be able to visualize all of the effects a particular type derives, explore available methods on it, view graphs of compositions, etc. How could we even get started if we wanted an environment like this?
&gt; I haven't really seen that kind of behaviour outside Stack vs Cabal threads and the odd troll popping up now and again. Perhaps I just have a mental filter that ignores it Two arbitrary examples: * Right now I'm looking at [this comment](https://old.reddit.com/r/haskell/comments/9vtis5/the_universe_of_discourse_i_hate_the_environment/e9h8yzq/), which calls someone's criticism of an aspect of the language "garbage as usual". Perhaps more significant than such a comment existing is the fact that it currently has around +6 score. * For something more subtle, I remember a thread some time ago in which someone was looking for tips for teaching [TidalCycles](https://tidalcycles.org/) to non-programmer musicians, and there were some gatekeeping comments, including one to the effect of "You *can't* get by with superficial understanding in Haskell -- it is no Ruby or Python". &gt; Even if it were the case that that behaviour occurs occasionally, it's far from clear that a CoC would do anything to improve the situation (see, for example, agentultra's behaviour in this discussion) If someone feels like they can flout the common expectation of courtesy because they are on the "good side" of some issue or another, it is up to the relevant moderators and/or the community to disabuse them of such a notion. That is largely orthogonal to the existence of a CoC, or of "communication guidelines" of some sort. The documents are something to refer to, point to, and guide further action if need be. It is up to the community to make sure they are used competently and evenhandedly.
Thanks for the concrete examples! Those have been conspicuous by their absence in this discussion. I did happen to read the thread you linked and I think I must have mentally filtered out the "garbage" criticism. I've been on the internet for long enough now that lots of it I think I simply don't see. The comment in question seems to fall foul of the following three principles of the CCCoC so my assertion that a CoC wouldn't improve the situation was wrong in this case. Thanks for bringing it to my attention * Using welcoming and inclusive language * Being respectful of differing viewpoints and experiences * Gracefully accepting constructive criticism
To what effect (pun intended) do you use `freer-simple-profiling`? Does it help you find bottlenecks in the application code? 
https://github.com/chrisdone/hell
From my experience there isn't anything that works out of the box, sadly. &amp;#x200B; I have been having an OK experience using stack+intero. I would tend to avoid the ghc-mod based stuff like haskell-ide-engine because ghc-mod tends not to work (for me) with a lot of compiler versions.
I found you can sorta overload Prelude functions like this import qualified Prelude import Prelude hiding (mod) Then you can define `mod :: MyData -&gt; MyData -&gt; MyData` and use `Prelude.mod` when needed. Now I need to overload `(+)`, but the interpreter and compiler complain that `Prelude.(+)` doesn't exist. How do I call the `(+)` function with `Prelude.` prefix?
Oh in a haskell file you can use `(Prelude.+) x y` and it works as expected, but if you try `:t Prelude.+` on ghci it doesn't work.
You shouldn't need `QuantifiedConstraints` for this one, at least. type family Arg x where Arg (f a) = a class f ~ (-&gt;) (Arg f) =&gt; IsReader f instance f ~ (-&gt;) (Arg f) =&gt; IsReader f If you have a richer example where this doesn't work, I'll take a crack at it.
What is the cleanest way to use stack and cabal new-build with different GHC versions? In day-to-day use, I want to use stack with GHC 8.4.x, but I also want the option to use cabal new-build to use GHC 8.6.x, so that I can run CI with both. In an ideal world, I don't have to duplicate a lot of configuration between the two.
Everywhere where you had `+`, use `Prelude.+`. For example, `x + y` becomes `x Prelude.+ y`; `(+) x ` becomes `(Prelude.+) x`. `:t Prelude.+` doesn't work for the same reason as `:t +`; you need `:t (Prelude.+)` or `:t (+)`. The expression Prelude.(+) is interpreted as function composition.
I built and started using this today. It's actually faster than the default Ubuntu terminal. I always test out new terminals with \`yes | lolcat\` - and this performed very well (not a great benchmark, but fun). The startup time is also noticeably faster. I'm using Yi as well, so this a nice addition. Thanks!
This is a great example use case. You could even group related effects together (i.e. a few known queries under one \`Query\` effect) while aggregating at the constructor and effect level simultaneously. 
How soon is soon? 
`:t (Prelude.+)` will work in ghci too.
This is a good question. I did't really make it clear in the blog post. Termonad should technically be called a "GUI wrapper around a terminal emulator". VTE is a terminal emulator library from the Gnome team. It provides a fully-functional terminal emulator as a GTK widget. VTE is widely used. It is the base for many popular terminal emulators, including `gnome-terminal`, `sakura`, etc. It also allows easily embedding a terminal emulator in other programs (like an IDE). Termonad also makes use of VTE for the actual terminal emulation. Termonad is really just a GUI wrapper around VTE. Termonad exposes the configuration options from VTE, as well as giving you the ability to do things like have multiple terminals in tabs. That being said, if there were an actual terminal emulator implemented fully in Haskell (instead of C++ like VTE), I would love to use it in Termonad.
There is a difference between the two functions. If you think that the variables a,b are Booleans (0 or 1) and the arrow -&gt; represents implication, the formula `a -&gt; b` is not a tautology: it can be made false by assigning a = 1, b = 0. A value of a given type constitutes a proof that the corresponding formula is a tautology. Example: you can show that it's not possible to define a value of type `forall a b c. (a -&gt; b) -&gt; (b -&gt; c) -&gt; c -&gt; a`. The converse is not true. The logic formula `((a -&gt; b) -&gt; a) -&gt; a` is a tautology, but there is no Haskell program that proves it. It is possible to implement it in languages with first-class continuations. The tautology is called Pierce's law, and [call/cc](https://en.wikipedia.org/wiki/Call-with-current-continuation) is an operator with this type. Here's the idea. To implement `f :: ((a -&gt; b) -&gt; a) -&gt; a`, we define `f x = x magic_value`. This magic_value has type `a -&gt; b`. If it is never used, then `x magic_value` will return a value of type `a` and we are happy. If `magic_value` is ever used, since it's a function, it must be called on an some argument `p` of type `a`. At the instant when `magic_value` is called on `p`, everything that has been done so far is aborted and `f x` returns `p`. In Haskell, you don't have first-class continuations, but you can use the continuation monad to achieve something similar.
You bring up some pretty good reasons not to use programs like XMonad and Termonad. Both XMonad and Termonad make you spend time creating a configuration file to get everything how you like. However, sometimes it is just easier clicking around in a GUI to setup everything. I think people who like programs like XMonad will also like Termonad. For instance, I would prefer to have slightly more complicated programs as long as they are configurable and extensible in Haskell. This is a trade-off. I definitely don't think it is for everyone. &gt; Also XMonad is renowned for stability, but it broke for me every few months: memory leaks, crashes, unresponsiveness. My config wasn't complicated, but I had to keep spending time on it to keep it working: not my idea of a good tool. I've experienced this as well in the past. It can be frustrating fighting with things like XMonad when you really just want to get some work done. In this sense, Termonad may not be the best for someone like you right now. After using it for the past few months, Termonad has been extremely stable, but it is entirely possible the API will change in the next year or two. This can be mitigated somewhat if you use `nix`, since you can be sure you will always be able to build an older version, but then you don't get the latest and greatest features.
XMonad is a library, as is Termonad. If you want to build a binary with no runtime dependency on GHC then you can just do so. It's basically a matter of using `launch` or `start` rather than `xmonad` or `defaultMain` respectively, then making what was your config the Main module of a project depending on the underlying library.
I second what ShrykeWindgrace said, but you're interested in reconstruction and you happen to have big runs then run length encoding is pretty easy: encode :: Eq a =&gt; [a] -&gt; [(Int, a)] encode = map (length &amp;&amp;&amp; head) . group decode :: Foldable f =&gt; f (Int, a) -&gt; [a] decode = concatMap (uncurry replicate) 
This is awesome! I think it makes sense for `freer-simple-http` to be separate from `freer-simple` itself, but do you think it would make sense for the others to be integrated into `freer-simple` itself? `freer-simple-catching` and `freer-simple-profiling` both seem like obvious, general-purpose improvements, and `freer-simple-time` and `freer-simple-random` have pretty light dependencies, as the Haskell ecosystem goes. I think it would be great for `freer-simple` to have more “batteries included”!
My preference is for them to remain separate, but with a prominent "see also" in the description of `freer-simple` to help people discover related packages in the ecosystem.
Could you explain why you would prefer them to remain separate? In the case where it really adds zero new dependencies, I can’t really understand why you’d prefer them stay distinct, but maybe there’s some concern I’m not considering.
If you want to use multiple GHC versions on the CI with both `cabal` and `stack`, you can see the example in the `summoner`: * https://github.com/kowainik/summoner/blob/016f058deb54b1674dfeb9c2c10ce0f760aed77a/.travis.yml &gt; Btw, `summoner` scaffolds Haskell packages with the config of similar structure The idea is to use PPA for `cabal-install` on Ubuntu and have multiple `stack.yaml` files for `stack` (like `stack-8.4.4.yaml` or `stack-8.6.2.yaml`). `cabal` doesn't have a way to install GHC in some local location, but PPA on Ubuntu work quite nice for me! You also might be interested in `ghcup`: * https://github.com/haskell/ghcup
Yup, I'm using PPA and that has been very smooth. I'm not concerned about multiple stack resolvers ATM, but I did copy relude's `.travis.yml` file a few hours back to make sure I have CI using both. :)
Sure I can. Let's start with `catching`/`profiling` since it's the hardest to argue since they only depend on base. If I can convince you of that, I think `random`/`time`/`http` should follow. As separate packages, maintenance is spread across more humans. Semantic versioning is more accurate, which makes breaking any individual effect's API a much smaller pill to swallow. Plus, (*Dave Rubin voice*) competition will start kicking in, which could lead to better effect APIs in the end. I think _most_ of the benefit of having everything in one package is solved by mentioning all of the `freer-*` packages in `freer-simple`'s description. For users, it's only one extra line in the cabal file. And users who *don't* want to use any of the additional effect packages benefit as well - less code to download and compile. It is nice to have the "one true way" to do each kind of effect in `freer-simple` proper, but there is definitely more than one way to rig something like `freer-profiling` up. So I'd just prefer it if `freer-simple` was a tight, blessed core, like `mtl`, and (to aid discoverability), again, a simple pointer to other `freer-*` packages would suffice. But your call, it's just my opinion. I stole it from `hasql` anyway :)
Just noticed this. Was holding off after the early draft but your sample sold me. Bought!
Not bad.
Thanks, that was the most informative response!
Dors anybody know of an introductory course book on algorithms in haskell? I recently tried to use haskell for one of the standard intro books but found it almost impossible. Note that I am a noob in both algorithms and haskell.
`factorizer` performs linearly `t = O(n)`, where `n = 600851475143` which is very long. You could use the same approach as in `isPrime` function to do implement `factorizer` in `O(sqrt(n))` time. Something like this: ``` factorizer y = [x, y `div` x | x &lt;- [2..intsqrt y], x `mod` y == 0] ``` And the take the maximum prime out of this list.
How do you like Yi? I don’t know of many people who use it. I’ve never gotten it to compile, so I haven’t gotten a chance to try it yet. 
I suggest you to read this paper: https://www.cs.hmc.edu/~oneill/papers/Sieve-JFP.pdf The algorithm is implemented by this library: http://hackage.haskell.org/package/primes-0.2.1.0/docs/Data-Numbers-Primes.html
I like the look of [this](https://stackoverflow.com/a/242274) answer 
Ok. Go read about [formal power series](https://en.wikipedia.org/wiki/Formal_power_series) (I already linked this above), then read this article to see how it can work in Haskell: [M. Douglas McIlroy: Power Series, Power Serious](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.333.3156&amp;rep=rep1&amp;type=pdf) (PDF link) Then come back if you have actual questions which are not of the type "this violates what I've learned". 
&gt; How can I have some kind of check that makes isPrime x have a break condition that immediately returns False if x mod something == 0 ? I was thinking something with takewhile. `isPrime` already does that. `null` returns `False` as soon as it sees the list being non-empty, which in your case is as soon as you found a factor.
The problem is the dependencies are used by other projects. 
@edwardkmett Many thanks for doing this. It covers some of the same ground that an earlier one of your screencasts does (which is not a complaint, but encouragement to do more of them, and bring some of the projects to a conclusion, perhaps with bigger example use cases).
It's hard to say for sure, but I expect to be done by Sunday at the absolute latest. 
Cool thanks! What's taking so long by the way?
Something like this? data Interval = Interval Double Double deriving Show yeah = Interval 0.5 1.0 nah = Interval 0 0.5 instance Semigroup Interval where Interval b0 e0 &lt;&gt; Interval b1 e1 = Interval b (b+s*(e1-b1)) where s = e0-b0 b = b0+s*b1 instance Monoid Interval where mempty = Interval 0 1 mappend = (&lt;&gt;) 
Hi, I have used haskus-utils-variant to build a DSL interpreter ([H-Calc](https://github.com/pcarbonn/H-Calc)), but I run into compile-time issues when there are more than 10 constructors, as you have. So, I'll give openADT a try. Any chance you put it on hackage at some point ?
Not OP, but I enjoyed that paper, so thanks. Some good lessons to digest from it.
I can't run your code right now to validate what I'm saying, but anyway: given that the output is a list, wouldn't it be more appropriate to use `nf` rather than `whnf`? (See also: ["Benchmarking pure functions" in bos' Criterion Tutorial](http://www.serpentine.com/criterion/tutorial.html)).
You're doing two passes (filtering out just the factors of the number, and then filtering out those which are prime) when it would be better to find the entire prime factorization in one go (you can use the grade school algorithm where you try to find a nontrivial factoring and then recurse on the two factors you found) and then selecting the largest prime factor from the resulting list.
I did that and it narrowed the gap for some functions and widened it for others. Overall, the ones using recursion-schemes are still slower. I updated the post with the new graph.
`capataz` is basically an implementation of Erlang OTP supervisors, with Haskell threads as nodes. `distributed-process` is basically Erlang node functionality, atop which you can build out an OTP supervisor. This library is basically required for any distributed programming, but is fully usable without. - `immortal` is extremely simple, it just keeps a single thread alive (until you tell it it's ok to die) - `slave-thread` does not have any supervisor functionality out of the box, it just propagates exceptions from children to their parent. - `supervisors` I had not heard of, but it does look like the closest to what you want. Unfortunately `slave-thread` seems like a good idea to me, I'd like to get experience using it in a real codebase. I recently started building a supervisor-like thing atop it, which you can see the, uh, progress... of... [here](https://github.com/mitchellwrosen/otp/blob/master/Lib.hs). The benefit of building a supervisor abstraction atop `slave-thread`, which `supervisors` does not do, is that your supervised threads can fork children of their own, and the hierarchy comes crashing down due to a failure at any depth. I also agree about sophisticated restart strategies (and discriminating on exception type - no point in retrying an HTTP request if the network interface is down). `capataz`, being (mostly) fully faithful to Erlang, has inherited all of its weird, inflexible restart strategies. I think we can do better in Haskell.
What are peoples conventions on reading haskell code aloud? Such as with a type constraint like `instance Eq a =&gt; Eq (T a) where ...etc`, I've taken to pronouncing the arrow as "leads to". Or `x &lt;- foo` I've read as `bind x from foo` rather than "assign", given that it's more than just that. I'm not saying all symbols are going to be pronounceable, but I've always found it helps me to have some mnemonic word to go along with every action. 
I like your code a lot. I don't think problems like “factoring prime numbers quickly” are a good measure for how good you are at writing neat, functional Haskell code, because the fastest ways to do it are [somewhat ugly and mostly imperative](https://wiki.haskell.org/Prime_numbers#Using_Mutable_Arrays). You broke the problem down into small pure operations that do the right thing when put together! From a “writing Haskell code” perspective you did a stellar job. Mathematical cleverness and number-crunching techniques are orthogonal to that. (This is why I don't like Project Euler so much. I think [99 Haskell Problems](https://wiki.haskell.org/H-99:_Ninety-Nine_Haskell_Problems) is pretty good. Or just write a real project! A chat bot, a tiny game, a random sci-fi movie plot generator, etc.) --- A tiny note: null [y | y &lt;- [2..intsqrt x], x `mod` y == 0] can be written equivalently as: all (\y -&gt; x `mod` y /= 0) [2..intsqrt x] which you may or may not find clearer. I think either way is fine here! Just be aware that `all` exists. (And indeed, just like `null` short-circuits as soon as it sees _any_ value in the list, like /u/jan_path points out, `all` does the same the moment it sees a counterexample.)
Let us know if you're still interested, though. 
I think that you should either get comfortable with haskell first, or with algorithms. If you're not comfortable with any of them, you're going to find it difficult to learn both at the same time. &amp;#x200B; I recommend that you first implement an algorithm you know in a language that you're comfortable with, and after that you can try to implement it in haskell. &amp;#x200B; If you find it really difficult to implement even the basic stuff in haskell, maybe you should give the problems on [HackerRank](https://www.hackerrank.com/domains/fp) a try.
I really like HIE when I can get it working, but I've had some issues getting it to work with Nix lately. Do you have any tips/suggestions on what I've posted here: https://www.reddit.com/r/haskell/comments/9t0p5n/monthly_hask_anything_november_2018/e9il47v?
It *appears* to me that you could handle your requirements with Capataz. I would configure your REST handler as a Static Worker with a restart strategy, and the remaining jobs as Dynamic Workers. 
Binary caching would be an amazing addition for the UX of HIE. I feel like having to build it yourself is a major hurdle to most casual adopters, and also means you are far less prone to updating it.
You are probably right. Thanks for the advice. I think I'll try to alternate between algos in python and hacker rank for haskell 
Given it is produced by Domen Kozar, I would assume it is on cachix.
[`inline-c`](http://hackage.haskell.org/package/inline-c) might be of interest to you.
Can recommend the youtube vid with mr. Hutton. Great explanation of laziness, Haskell and the prime sieve.
Interesting... I'm mostly interested in something I can use for large codebases. I don't know if I would want to use this extensively. I might experiment with how maintainable it is some time.
well I got 1000 minutes from circleci, I guess we can give it a go.
If I'm not mistaken, your approach is: Given a number *n*, find all the factors *fs* of *n*, filter the prime factors *pfs* from *fs*, and then take *maximum pfs*, which, by the way you've constructed *fs*, is *head pfs*. (1) Your implementation of *factorizer*, which calculates the factors *fs* of a given number *n*, is as follows: &gt; factorizer :: Integer -&gt; [Integer] &gt; factorizer n = filter (\x -&gt; n `mod` x == 0) [n, n-1..1] In this case, we know that each integer returned by *factorizer* is going to be subsequently checked for primality, so the filtered range can instead be *[n, n-1..2]*, as we know *1* is not a prime number. This will remove the need for the first guard in *isPrime* (see (2)). Personally, I would also use a list comprehension, but that's just my preference: &gt; factorsGT1 :: Integer -&gt; [Integer] &gt; factorsGT1 n = [ x | x &lt;- [n, n-1..2], n `mod` x == 0 ] u/andrybak in the comments below suggested that factorising can be done more efficiently by enumerating up to *intsqrt n* only: &gt; factorizer' :: Integer -&gt; [Integer] &gt; factorizer' n = [ x, n `div` x | x &lt;- [2..intsqrt n], x `mod` n == 0 ] While this is certainly an improvement, I'm going to overlook it and propose an alternative solution altogether shortly. When improving your own solution, you should definitely implement *factorizer* this way, and think about how it would affect the remainder of your solution. (2) Your implementation of *isPrime*, which checks the primality of a given number *n*, is as follows: &gt; isPrime :: Integer -&gt; Bool &gt; isPrime n &gt; | n &lt;= 1 = False &gt; | n &lt;= 3 = True &gt; | n `mod` 2 == 0 = False &gt; | n `mod` 3 == 0 = False &gt; | otherwise = null [ y | y &lt;- [2..intsqrt n], n `mod` y == 0 ] The first 4 guards can be removed. For example, the first can be removed by assuming the function will only be called on integers *&gt; 1*, which is a sensible assumption given the definition of "prime". From (1), we also know that *factorsGT1* only returns integers *&gt; 1*. The next 3 guards are really just specialisations of the last guard: Firstly, the range *[2..intsqrt n]* will be empty when *n &lt;= 3*. Secondly, if the conditions on the 3rd and 4th guards are *false*, then, in general, they will be checked again by the test on the last guard. This work is therefore unnecessarily repeated. The last guard is all we need: &gt; isPrime' :: Integer -&gt; Bool &gt; isPrime' n = null [ x | x &lt;- [2..intsqrt n], n `mod` x == 0 ] Equally this can be written as, &gt; isPrime' :: Integer -&gt; Bool &gt; isPrime' n = all (\x -&gt; n `mod` x /= 0) [2..intsqrt n] which is somewhat clearer as pointed out by u/13467. It's worth noting that this is not the most efficient way of checking if a number is prime. I think the fastest "general" method is [AKS](https://en.wikipedia.org/wiki/AKS_primality_test), but I could be mistaken. Your slightly modified solution, which ignores sanity checking, is then: &gt; largestPrimeFactor :: Integer -&gt; Integer &gt; largestPrimeFactor n = head (filter isPrime' (factorsGT1 n)) It's difficult to estimate the time complexity of this function because it depends on: 1. The number of factors of *n*: **in the worst case** each has to be tested for primality; 2. The size of each factor *f*: **in the worst case** *isPrime' f* has to check all numbers up to *intsqrt f*. Maybe someone else can give a rigorous time complexity analysis of this function, but for *n &lt;= 100,000,000* it seems to work OK. --------- Instead of trying to improve your implementations of *factorizer* (e.g., by using the implementation suggested by u/andrybak) and *isPrime* (e.g., by using the implementation suggested by u/13467), I recommended taking a different approach. Your solution generates all factors *fs* of *n* and then tests them for primality. Instead, we can generate all primes *ps* up to *intsqrt n* and then check them to see which are factors of *n*. In other words, we are swapping the generation and testing stages of the solution. First we can use the Sieve of Eratosthenes to generate an infinite list of primes. (Note that this is not the "true" Sieve of Eratosthenes -- see the paper linked by u/janmas below -- but it doesn't matter for our purposes.) There are numerous explanations online regarding how this function works, so I won't go into any details here. &gt; primes :: [Integer] &gt; primes = sieve [2..] &gt; &gt; sieve :: [Integer] -&gt; [Integer] &gt; sieve (p : xs) = p : sieve [x | x &lt;- xs, x `mod` p /= 0] Given the list *primes*, we can then check each *p* in *primes* up to *intsqrt n* to see whether it is a prime factor *pf* of *n*, and then take the largest such *pf*. Our solution (which again ignores sanity checking) would be something like: &gt; largestPrimeFactor' :: Integer -&gt; Integer &gt; largestPrimeFactor' n = last (filter (\p -&gt; n `mod` p == 0) (takeWhile (&lt; intsqrt n) primes)) However, this is still inefficient as *filter* has to traverse all prime numbers up to *intsqrt n*, checking to see if each is a prime factor of *n*. Nevertheless, I think this is an important step in the right direction. Next notice that our new function *largestPrimeFactor'* is essentially two functions: &gt; largestPrimeFactor' :: Integer -&gt; Integer &gt; largestPrimeFactor' n = last (uniquePrimeFactors n) &gt; &gt; uniquePrimeFactors :: Integer -&gt; [Integer] &gt; uniquePrimeFactors n = filter (\p -&gt; n `mod` p == 0) (takeWhile (&lt; intsqrt n) primes) First we calculate all **unique** prime factors *upfs* of *n*, and then we take the maximum; by the way *upfs* is constructed, we know the maximum is the last element in the list. At this point, we know the main source of inefficiency is the *filter* of *uniquePrimeFactors*, which must **always** traverse all prime numbers up to *intsqrt n*. As such, to improve this function's efficiency (and the efficiency of *largestPrimeFactor'* overall), we must reduce this search space. Let us consider the unique prime factors of some example *n*s: &gt; uniquePrimeFactors 48 = [2,3] -- 48 = 2 * 24 &gt; uniquePrimeFactors 24 = [2,3] -- 24 = 2 * 12 &gt; uniquePrimeFactors 12 = [2,3] -- 12 = 2 * 6 &gt; uniquePrimeFactors 6 = [2,3] -- 6 = 2 * 3 &gt; uniquePrimeFactors 3 = [3] From this we can observe that *uniquePrimeFactors 48 == uniquePrimeFactors (2 * 24) == [2] 'union' uniquePrimeFactors 24*, *uniquePrimeFactors 24 == uniquePrimeFactors (2 * 12) = [2] 'union' uniquePrimeFactors 12*, *uniquePrimeFactors 12 == uniquePrimeFactors (2 * 6) = [2] 'union' uniquePrimeFactors 6*, and so on. Hopefully this illustrates that each time we calculate a unique prime factor *upf* of *n*, the remaining unique prime factors can be calculated from *upf 'union' uniquePrimeFactors (n 'div' upf)*. Clearly when the argument *n* of *uniquePrimeFactors* is reduced (e.g., from *48* to *24*) the number of primes filtered is also reduced: &gt; uniquePrimeFactors 48 = filter (\p -&gt; n `mod` p == 0) (takeWhile (&lt; intsqrt 48) primes) &gt; = filter (\p -&gt; n `mod` p == 0) [2,3,5] &gt; &gt; uniquePrimeFactors 24 = filter (\p -&gt; n `mod` p == 0) (takeWhile (&lt; intsqrt 24) primes) &gt; = filter (\p -&gt; n `mod` p == 0) [2,3] Overall, taking advantage of this will substantially reduce the search space being filtered when *n* is large. The following function, *primeFactors*, implements this kind of approach: &gt; primeFactors :: Integer -&gt; [Integer] &gt; primeFactors = factor primes &gt; where &gt; factor :: [Integer] -&gt; Integer -&gt; [Integer] &gt; factor [] n = [] &gt; factor xs@(p : ps) n &gt; | p^2 &gt; n = [n] -- if p^2 &gt; n, then n is prime and we're done &gt; | r == 0 = p : factor xs d -- if p divides n exactly, then p is a prime factor of n and &gt; -- the others are the prime factors of n `div` p &gt; | otherwise = factor ps n -- otherwise, p is not a prime factor of n so check the remaining ps &gt; where (d, r) = n `divMod` p A few notes on this implementation: 1. Instead of calculating the **unique** prime factors of *n*, this function calculates *n*'s prime factor decomposition *pfs*. In practice, this is because calculating unique prime factors is unnecessary as the cost of *last pfs* is negligible. (I only discussed unique prime factors above to further the explanation.); 2. *factor [] n = []* is a redundant pattern match because *primes* is an infinite list; Notice that *primeFactors* has the property that if *pf* is the **first** prime factor of *n*, then the prime factor decomposition of *n* can be calculated from *pf : primeFactors (n 'div' pf)*: &gt; primeFactors 48 = [2,2,2,2,3] -- 48 = 2 * 24 &gt; primeFactors 24 = [2,2,2,3] -- 24 = 2 * 12 &gt; primeFactors 12 = [2,2,3] -- 12 = 2 * 6 &gt; primeFactors 6 = [2,3] -- 6 = 2 * 3 &gt; primeFactors 3 = [3] This is somewhat similar to the property of *uniquePrimeFactors* discussed previously. Finally, adding some simple sanity checking gives us an efficient and neat solution: &gt; largestPrimeFactor :: Integer -&gt; Maybe Integer &gt; largestPrimeFactor n &gt; | n &lt; 2 = Nothing &gt; | otherwise = (Just . last . primeFactors) n I'll leave you, or anyone else interested, to comment on the time complexity of this function. I hope this helps! Martin
ADTs are THE types, languages without sum types are missing the point (they can’t precisely express business logic as sets of possible values). Here’s an easy read https://guide.elm-lang.org/appendix/types_as_sets.html
/u/edwardkmett might be able to shed some light. My impression was that recursion-schemes was only faster if the compiler could optimize it away, so that the fold/unfold became an assembly loop, and it was written in a way so that it was "easy" for GHC to recognize this, and explicit recursion was a little more hit-and-miss on how well it optimized.
I think it helps with the intuition behind using them everyday. It's something we just take for granted that people know -- that `Maybe a` has one more inhabitant than `a`, and `Either a b` has inhabitants of `a` plus inhabitants of `b`. It affects how we can understand and read type signatures of functions on a more intuitive level, I feel.
I don't know if you're targeting a lone system or a somewhat large systems. If you don't want to stop your system I assume you want continuity of service. If you need continuity of service, you'll need to design for failure of a node at some point (e.g., if only to do a rollout). Thus, it's often OK to crash your application if some background job cannot work and let the outter-loop (e.g., your scheduler, the OS-level daemon job) alert you and do the restart.
I think developing that intuition also helps inform the process of making hygienic decisions like eg. creating a sum type with 10 specific constructors which gives you 10 possible states that are all valid instead of just using an `Int`, possibly with bounds checking, where you have 10 states that are valid and 18,446,744,073,709,551,604 that are invalid.
It's could become useful when you're thinking about runtime representation and performance. It is kind of important to understand that the 'bigger' your ADT is, the more space in memory you're taking for any live instance - You do pay a cost for a sum type that has 15+ different constructors, especially if some of those are also big product types. It's not really a topic you need to worry much about though - It's pretty hard for something to sneak up and surprise you. 'Big hoary types are probably big and hoary in memory too' is pretty much the long and the short of it. In idiomatic Haskell code the worst-case scenario of large, complex trees hanging out in memory is generally mitigated by laziness allowing you to not actually allocate the whole complex tree in order to traverse it. Hence, this is the sort of thing that's nice to keep in mind, but probably isn't going to be a core focus when writing a program. Write what you need to write and let GHC worry about sorting it out for you later. Ultimately, those kinds of concerns probably aren't ones you need to have for awhile as a new Haskeller - So I'm going to have to agree with the crowd here and say it's inclusion in 'The Book' is an odd fit.
&gt; recursive calls on nested subterms For this you use a histomorphism and look back one step instead of at the current step. Roughly: evenNR = histo evenCoFAlg where evenCoFAlg OF = True evenCoFAlg (SF (_ :&lt; OF) = False evenCoFAlg (SF (_ :&lt; SF (x :&lt; _))) = x &gt; recursive calls not on subterms at all You'd be operating on `([a], [a])` (all *morphisms are on a single object), which isn't exactly a (uniformly) recursive datatype. So, you'd actually be doing this as unfold, and there's no required subterm relationship between the current "seed" and the next one. Roughly: ileaveNR = unfold ileaveCoAlg where ileaveCoAlg ((x : xs), ys) = ConsF x (ys, xs) ileaveCoAlg ([], ys) = project ys
No hurry. I'll clone your repository in the meantime.
Why did you escape all your backticks? Some fancy-pants editor?
I do not think the chapter should put such intense focus on it. I do a lot of Haskell and virtually never think in these terms. The number of inhabitants is not nearly so important as the semantics of the data structure.
Processes can be cheap, depending on your needs.
Why would the potential values take up more space for each live value? That doesn't make sense to me. If that was the case wouldn't each string take a gigantic amount of space because it's got potentially limitless values?
They're useful because they are (as far as I know) the most abstract abstractions in programming*, meaning that even if you use Java you are using ADTs under the hood, except Java is giving you an "interface" to it, as oppose to "direct access" to this pattern like Haskell. In other words, they are powerful abstractions that are even operating in imperative-OOP code, they're just not as explicit as in pure FP. So I guess the value of ADTs goes up in pure FP, because they allow many features while still playing nicely with the type laws. *referring to denotational semantics, i.e. how Sums are math objects which end up describing a lot of patterns in programming
This is not really true. IIRC types with a very small number of constructors (4?) can have the tag shoved into the low bits of a pointer, but over that threshold it doesn't matter how many constructors a type has.
Analyzing results, producing graphs/tables, and writing stuff takes time. If you're simply interested in the CSV results, they're currently connected to this pull request: https://github.com/tfausak/tfausak.github.io/pull/148
Oh! My bad, I'd just assumed that since you'd have to tag it with some kind of identifier that the size would continue to increase along with the amount of space necessary to store a method of uniquely referencing each constructor. I guess that's a linear size relationship anyway, which isn't really the same math, but I did assume there was some sort of proportional overhead. How does that work for larger sum types then? Is there just some per type lookup table that gets carted around, or is there something more clever going on here?
Recursion schemes fold up leaf-first, so it's not always equivalent to a traversal that begins with the root node and then goes down to the leaves.
The point of a hylomorphism is that you have *guaranteed* fusion. For instance, you can "generate" a list and "fold" it while never allocating the whole thing. 
IIRC the representation of an ADT is a tag (i.e. an integer) identifying the constructor, followed by that constructor's fields (or pointers to them).
&gt; the fastest ways to do it are somewhat ugly and mostly imperative. I disagree! There's nothing egregiously wrong with OP's code, but one can still seek better *asymptotic* complexity. 
hylo doesn't _really_ work well as a fusion form. This is why we use foldr/build and destroy/unfoldr fusion for the most part.
&gt; 1) semilattices don't have inv (for example, + on natural numbers doesn't) `inv m = m` passes all of the laws for an inverse semigroup, and it is the only such choice that passes the laws. m &lt;&gt; inv m &lt;&gt; m = m, because m &lt;&gt; m &lt;&gt; m = m inv m &lt;&gt; m &lt;&gt; inv m = inv m by the same logic. &gt; 2) the idempotence and commutativity laws hold only for some elements, namely, idempotents of the form x * inv x (for example, these laws do not hold for your balanced parens data type). Given a join semilattice, _all_ elements are of the form x = x &lt;&gt; inv x for some x, and all idempotents (indeed, all elements) commute (+) isn't a semilattice. `max` on the natural numbers, on the other hand, does pass the laws. &gt; Could you make the "in some sense" bit more precise? Every group is an inverse semigroup. It has exactly one idempotent element `e`, which commutes with every idempotent element (itself), ee = ee. All idempotents are of the form `x &lt;&gt; inv x` for some x, indeed `e = x &lt;&gt; inv x` for _all_ x. The `x &lt;&gt; inv x &lt;&gt; x = x` style laws follow from the unit and inverse laws for the group. And I just worked through how semilattices are directly usable as inverse semigroups above. I think the thing you missed was that for a join semilattice `inv x = x`. From that everything else I said should follow.
Cool, I did miss the `inv x = x` definition! (And yes, `+’ was a wrong example, which misled me further.) This is quite thought-provoking. I'm working on a problem that has a linear-time solution for a group and a commutative semigroup, and now I wonder if an inverse semigroup is what I should aim for instead :) 
Non-recursive case wasn't quite *right*: ileaveNR = apo ileaveCoAlg where ileaveCoAlg ((x : xs), ys) = Right &lt;$&gt; ConsF x (ys, xs) ileaveCoAlg ([], ys) = Left &lt;$&gt; project ys
&gt; I don't think that library has ever been faster than what you would write by hand. I think it depends. Everything (... pretty sure ...) in recursion-schemes has had the [Worker/Wrapper Transformation](https://wiki.haskell.org/Worker_wrapper) applied to it. If you write something that could have worker/wrapper applied to it for improving performance **and** it's complicated enough that GHC doesn't do the transformation for you, `recursion-schemes` could be faster. I think *some* of the power of recursion-schemes comes from the fact they are "safe". They all perform well-founded recursion / productive co-recursion. Manually written recursion / co-recursion can inadvertently introduce non-termination through misplacement of recursive calls. Sure a(n) (co)algebra can be non-total, and you could try do strict fold on a co-recursive structure. Recursion-schemes doesn't prevent that at all, but neither does manually written recursion. But, it does eliminate one class of bugs. If you write something out using a recursion scheme / as a(n) (co)algebra first, it can help with correctness even if you have to sacrifice it at the altar of performance later.
I'm currently poking at the use of inverse semigroups to better model something like the underlying theory for "nominal renaming sets" and if not if i can use the pattern from nominal sets for other inverse semigroups i happen to be interested in that i'd like to be able to act on sets with.
Information needs but to live in. If you want to represent four possibilities, you need 2 bits. 256 possibilities needs 8 bits. 4 billion needs 32 bits. The same principle generally applies everywhere. Indeed, the "average" size of a string is infinite, for exactly the reason you say. (For reasonable definitions of "average". You need a limit to think about it formally.). Fortunately, your programs are all biased heavily toward shorter strings. That's what saves you.
Without an explicit fusion model, I wouldn't expect it to be faster. foldr/build fusion and unfoldr/destroy and its later improvement, stream fusion, all do better than hylo fusion when relevant.
Unfortunately, maintainer of `capataz` is MIA. I had a problem with the library and I had to figure the solution by myself: * https://github.com/roman/Haskell-capataz/issues/26
But each live instance only uses enough bits to represent one possibility, no?
I have a couple of questions. Firstly, don't we rely on overlapping patterns for recursion with a base case then a general case? E.g.: \`\`\`fac 0 = 1 fac n = n \* fac (n - 1)\`\`\` In that case, how does the compiler know not to issue a warning? Next up, in the GADT example where the definition of U was: \`\`\`data U a where UInt :: Int -&gt; U Int UChar :: Char -&gt; U Char\`\`\` How is it then reasonable to have an \`(undefined :: U Bool)\`? Shouldn't that be a type error since there's no way to get a \`U Bool\`? Similarly, I would think that \`getBool :: MustBe Bool -&gt; Bool; getBool (MustBe2 \_) = False\` should give a type error, since there's no constructor that can make it work. Why is this not the case?
I think an LSP server can go a long way to providing some of this functionality, especially using the [codeLens](https://github.com/Microsoft/language-server-protocol/blob/gh-pages/specification.md#code-lens-request-leftwards_arrow_with_hook) method to provide the various views onto the live code.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [Microsoft/language-server-protocol/.../**specification.md#code-lens-request-leftwards_arrow_with_hook** (gh-pages → 12b2b96)](https://github.com/Microsoft/language-server-protocol/blob/12b2b962534534cce6d8d10d66362042c36f56e3/specification.md#code-lens-request-leftwards_arrow_with_hook) ---- 
For example, Data.Sequence from containers uses this: #if __GLASGOW_HASKELL__ &gt;= 801 {-# COMPLETE (:&lt;|), Empty #-} {-# COMPLETE (:|&gt;), Empty #-} #endif To indicate that you've covered all the cases in a pattern match of a Seq.
There are still some limitations to this. They only later added the ability to put types on to hint when you have something that is _all_ patterns, and those types IIRC are required to be monomorphic, which gets quite painful at times.
This is a nice feature indeed! I've used in `co-log` library to provide shorter aliases for `Severity` data type: * https://github.com/kowainik/co-log/blob/494238bb10dbf529140e9223416f9b200edc384a/co-log-core/src/Colog/Core/Severity.hs#L47-L52 Although, `COMPLETE` pragmas are not completely supported by everything in GHC: * https://ghc.haskell.org/trac/ghc/ticket/15681
...and we are back to nominal sets! :-) Your 10-hour Twitch session is still in my to-watch list, I need to catch up!
Per [the Haskell 2010 report](https://www.haskell.org/onlinereport/haskell2010/haskellch6.html#x13-1290006.3.2), min x y | x &lt;= y = x | otherwise = y 
If you need persistence and use PostgreSQL, there's also consumers: [https://hackage.haskell.org/package/consumers](https://hackage.haskell.org/package/consumers)
This does lead to an infinite recursion for me. I think you'll need to post your code. newtype X = X Int deriving Eq instance Ord X where x &lt;= y = x == min x y main = print (X 2 &lt;= X 3)
Haven't heard about this library. Will take a look!
One slight problem with this is that if you have a large data type which is changing frequently, you might forget to update the pragma. That can be ameliorated by having the pragma close to the core data type (if there is one) and writing a test that gets all the constructors (using Template Haskell or otherwise) and does something with them, so you get a failing test when your COMPLETE pragma is outdated.
Thanks! Please see my edit.
Thanks! Please see my edit.
Thanks a lot! Solution is in the edit, if you're interested.
A minimal definition of Ord requires either `(&lt;=)` or `compare`, per the Haskell 2010 Report. This implies that you can't define either of these in terms of the other methods unless you provide definitions for them as well.
Oh, that's a shame, it looks quite promising.
[removed]
Can you give an example? I’m not sure I get this.
one of the key point of his encoding is that what is being modeled is the Hom functor `type Cat i = i -&gt; i -&gt; Type` I think it was said (Benabou) that category theory arise as a way to formalize set theoretic constructions, and bi-categories arise as a way to formalize categorical notions. Here `i` is a category if we can prove that `p : i -&gt; i -&gt; Type `is a functor. This corresponds to the definition of a bicategory, which starts with a bunch of objects and possesses such a functor describing its hom-set. We can navigate the various definition and get back all the properties. Note that we are looping among the definition, but thanks to `UndecidableSuperClasses` this is fine
Why not someFunc :: Switch -&gt; Whatefver someFunc = \case On -&gt; ... Off -&gt; ... Then there's no `coerce` in sight?
&gt; If there were types that bottoming expressions weren't allowed to inhabit, then that would be tantamount to solving the halting problem, as the compiler could statically determine whether any computation terminated or not by using one of these bottom-less types. Oh, come on... You know better than that. It's not "tantamount to solving the halting problem" unless it's a complete solution. Nothing about noticing that there are no constructors of `U` that could possibly lead to a `U Bool` is ruled out. Put differently, `undefined :: U Bool` could be a type error (well, probably a warning), not because `U Bool` is total, but because `U Bool` is "not well formed" by some extra rule. I really wish Haskell had more "best effort" sanity checking. This would fall under that categorization. Another thing I want is warning about `let x = x in ...`: I've run into a it before, and it's a pain to debug and almost never what you meant to write.
Because it is more verbose in certain cases. foo = do blah blah when (coerce flag) do blah blah vs foo = do blah blah case flag of Off -&gt; pure () On -&gt; do blah blah
Oh my, I just found a proof that all idempotents of an inverse semigroup commute (yeah, I failed to prove it myself), and the proof is ridiculously non-trivial: https://math.stackexchange.com/a/1093476
If you rewrite the type of `topdown` into the shape of a profunctor optic topdown' :: ??? p =&gt; p a a -&gt; p s s -- where p a b = (a -&gt; f b) `p` is what I've been calling a [monadic profunctor](https://github.com/lysxia/profunctor-monad), since, well, it's a `Profunctor` and a `Monad`. I have written about it a bit [on my blog](http://blog.poisson.chat/), but my focus was on using those to write pairs of "inverse transformations" in some sense (lenses, parser/printer). I realized later that they look like traversals or profunctor optics in this other way (`p a a -&gt; p s s`), but haven't gotten around to dig deeper into that idea. As you've noticed, you give up a lot of guarantees, in exchange for the expressiveness of the `Monad` class. On a related note, the `child` traversal probably admits implementations using the libraries [product-profunctors](https://hackage.haskell.org/package/product-profunctors) (see the `Adaptor` module) or [one-liner](https://hackage.haskell.org/package/one-liner), which provide generic traversals for what could also be called "applicative profunctors", a superclass of monadic profunctors. (Disclaimer: I've also had a hand in both those projects.)
Tried dwm? I love it
Why not? Is this a homework assignment?
Yeah 
Where can we see the full results of the survey?
How to be successful in Haskell: * create a very useful library * publish a scientific paper about that * once the paper is publish, abandon the library * start again &amp;#x200B;
Too hard to be seriously productive quickly
Probably because it doesn't like generating suggestions that are not always or nearly always valid.
Woah... people surely hate Haskell's records. I counted like 30 responses which ranted about records. Is there anybody working on addressing that wart?
https://www.reddit.com/r/haskell/comments/9vj2au/ghc_proposal_row_polymorphism/
Didn't the recent extensions (OverloadedRecordFields and the like) solve the majority of the problems?
"Well-formed type" is (IIUC) basically a syntactic check, so since `U :: Type -&gt; Type` and `Bool :: Type`, `U Bool :: Type`. One of the difficulties you'd run into with ruling out `U Bool` is type families: type family ProbablyBool a type instance ProbablyBool Int = Bool _1 = undefined :: U (ProbablyBool Int) _2 = undefined :: U (ProbablyBool a) Do you rule out `_1`, since you know that `ProbablyBool Int` reduces to `Bool` and `U Bool` is bad? How far do you take this? At some point, wouldn't it just be better to work on a totality checker?
The new user experience is, to be political, not in a good place.
**Team Role Inventories** The Belbin Team Inventory is a behavioural test, also called the Belbin Self-Perception Inventory, Belbin Team Role Inventory, BSPI or BTRI. It was devised by Meredith Belbin to measure preference for nine Team Roles; he had identified eight of those whilst studying numerous teams at Henley Management College. The Inventory assesses how an individual behaves in a team environment. The assessment includes 360-degree feedback from observers as well as the individual's own evaluation of their behaviour, and contrasts how they see their behaviour with how their colleagues do. Belbin himself asserts that the Team Roles are not equivalent to personality types, and that unlike the Myers-Briggs Type Indicator, which is a psychometric instrument used to sort people into one of 16 personality types, the Belbin Inventory scores people on how strongly they express behavioural traits from nine different Team Roles. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
Sure that's one way to do it. I don't feel there is a big difference between that and what I'm doing :).
Unfortunately, no. Try compiling this with `DuplicateRecordFields`... data Foo = Foo { name :: String } data Bar = Bar { name :: String } data Foobar = X Foo | Y Bar foobarName :: Foobar -&gt; String foobarName (X foo) = name foo foobarName (Y bar) = name bar
[Some quick word clouds](https://imgur.com/a/qJkmm36)
&gt; What's your least favorite thing about Haskell? &gt; **HASKELL** I know that's just because people are bound to use the name a lot, but it still got a good chuckle out of me.
Not yet.
Interesting... in the csv data 4978 respondents gave an answer to the build-tool question and 4729 out of these stated they use Stack. In other words this means that **~95% of the respondents use Stack!** I think this settles it. 
Settles what?
I cracked up at "Monads. I *hate* monads." xD
I don't like `coerce` when there's an alternative. I also wouldn't use pattern synonyms for this problem.
So I'm a "Plant"? Not sure how I feel about that designation.
thanks for clearing it up!
I empathize with most of the comments there, but some are just, I don't know how to describe them, confusing? * no printf - There is a [printf](http://hackage.haskell.org/package/base-4.12.0.0/docs/Text-Printf.html#v:printf) function in base. * let statements in do blocks - you don't like let? Huh? What's wrong with them? In fact, where is just sugar for let... * Need to enable 48298 language extensions to do anything useful - You can use the default-extensions field in your package.yaml/packageName.cabal file and not have to repeat them everywhere. Maybe create a template for starting new projects which you can easily copy-paste and tweak? * Foldable in the Prelude - something's wrong with Foldable? This is a genuine question, I have no idea what the problem is here. * Do syntax - something's wrong with do syntax? * Significance of white spaces. - You can use braces and semicolons if you wish :D!
Someone mention 'reddit.com/r/haskell' as bad things about Haskell also someone mentioned 'legacy features', I thought Haskell is the language that care the least about backward compatibility
Their ability to infer the type of a record selector is so limited as to be almost useless.
They almost certainly meant cruft like the `liftM` family of functions.
I know dwm, but I consider bspwm or 2bwm. 2bwm is not a tiling window manager, but it looks awesome. 
Nothing about compensation?
I recently started working at a Rails shop and am astonished at the constant stream of security issues that [snyk](https://snyk.io/) reports for our Ruby and JS dependencies. With Haskell I hardly ever heard about any issues. Admittedly I didn't do much web programming which seems particularly prone to these problems. How do professional Haskellers stay up to date with security issues (and other bugs) in their dependencies? Do Haskell's features help avoid these security problems?
`String = [Char]` and `FilePath = String` surely count as legacy features?
I think a more interesting question would be _why_ a particular tool is preferred over others. Is it because it's easier to use as a beginner? Is it better documentation? Is it because it supports a more complex workflow that you care about? Is it performance or caching (build artefacts)? etc. I would be really interested to hear from people what their typical workflow is (down to the commands they run) to get an idea of where one tool provides a benefit over the others. I think if we get to the bottom of what people really care about and want from their build tools then we can make the whole ecosystem better. 
https://packdeps.haskellers.com/ will give you an RSS feed to track that will get a new article anytime one of your dependency strings doesn't include the latest hackage version of a package. For example, I use http://packdeps.haskellers.com/feed/bss@iguanasuicide.net and got two new articles back in September. I'd actually like to figure out how to get my email *off* an old package, since the service it was designed to connect to has been discontinued, it will never be of any future use (possibly some sort of code archeology, but I don't think it notable enough) or get an update (at least not from me).
In other words, The world of haskell is huge and complex. That's the main complaint if you read past "there's no printf".
Given that not anyone who fills out the survey is an expert I think some of these answers expected.
&gt; My experience is the polar opposite -- everyone is so great and helpful I think the problem is, as one of the participants said, there are bad apples who bully others. Depending on whether you've met them in your early days or not determines your view of Haskell community.
Plz ;-; &amp;#x200B;
Certainly everyone's not an expert (e.g. many people have pointed out the difficulty curve, which is totally understandable) but I don't think the ones I have highlighted have anything to do with expertise. A few of them I don't understand at all, and a few of them can be resolved by a quick search online...
Same here. The issue of toxicity in haskell community is largely overblown. I'm not gonna say it does not exist. Certain topics (stack vs cabal vs nix, stackage vs hackage, versioning approaches on hackage, haskell on windows support etc, seem to have some bitter people who start flame wars. But overall it is a much healthier community than many others I've been part of over the decades. 
As long as `undefined :: forall a. a`, we also have `undefined :: U Bool`. It's not clear to me how you would want to prevent this. A better way for the given situation is to use a closed kind `data UKind = I | C` and then to define ``` data U :: UKind -&gt; Type where UInt :: Int -&gt; U I UChar :: Char -&gt; U C ``` and then you really could never even write something like `U B(ool)` because it would we kind-incorrect. Providing a warning if someone is writing something like `let x = x in ...` directly is a different issue and could probably be done.
https://haskell-lang.org/get-started gives a pretty decent new user experience. It doesn't instantly teach them monads, but it does help them over a few hurdles to "installing haskell" and "installing haskell packages".
I'm only asking for a very simple, superficial check that `U Bool`, if it occurs as a monomorphic type in the program, has any non-bottom values and a warning if it doesn't. After all, you are more likely to have written a monomorphic type that has no non-bottom inhabitants by accident than on purpose; something similar in spirit to the `let x = x in ...` check.
&gt; let statements in do blocks - you don't like let? Huh? What's wrong with them? In fact, where is just sugar for let... Maybe they meant that they dont like having to write `let` in do blocks for simple variable assignment. Because honestly I can sympathize with that, I particularly hate having to double indent whenever I have a multiline variable assignment (e.g. due to a case statement) in a do block.
I've written a blog post regarding how to start with Haskell build tools. Small improvement towards making ecosystem more beginner-friendly :) * https://kowainik.github.io/posts/2018-06-21-haskell-build-tools
That survey isn't ready for publication yet. It seems likely that it was spammed with a number of bogus responses, and those will need to be weeded out to get actually usable and actionable results.
&gt; This idea of a toxic community always seems to float about (sustained by who exactly???) and we really need to shake it. There is some toxicity in the "upper echelons" (for lack of a better word) in the community. Typically, between maintainers and some experienced users of the two main competing build tools Stack and cabal-install (it's more complicated and personal than that, but that's the birds-eye view). It usually does not intrude on the Haskell beginner experience as far as I can see, but if you spend a lot of time on the infrastructural mailing lists or GitHub issues (or /r/haskell), then it will seem like it has a very large community presence.
Oh, piss off. (Just kidding!) Seriously, I've never had a bad experience with bullying in the Haskell community either, which is a unique experience for me. For example, on Reddit in the Linux forums I get idiots thumbing their nose at me all the time, I've never seen this in the Haskell communities. But every so often you have one know-it-all ruin a newcomer's day, and as much as we try to minimize the effect of these people, we can't catch all cases. It is better to ask the question to beginners, "Do you find the community of Haskell enthusiasts to be helpful and welcoming?" Check how many people answer this question in the affirmative, I'll bet the number is close to 90%. I think the important thing to do is to keep vigilant, watch out for bullies, and step in when you think someone is belittling someone with an honest question. 
It still suggests intero and emacs, not exactly beginner-friendly tools.
What is the reasoning of length being defined for a two-tuple, but not for any other n-tuple?
&gt;It seems likely that it was spammed with a number of fake responses What do you base this claim on?
https://mail.haskell.org/pipermail/haskell-community/2018-November/000358.html
What is a *"type astronaut?"*
That survey isn't ready for publication yet. It seems likely that it was spammed with a number of fake responses, and those will need to be weeded out to get actually usable and actionable results. See https://mail.haskell.org/pipermail/haskell-community/2018-November/000358.html and subsequent discussion. 
I think what they mean is that there aren't really any "install it and it works out of the box" packages. It always takes a little bit of mucking around in the terminal to get the IDE tooling installed, like Intero or GHCMod, even when using Stack to install stuff.
Wasn’t this from Taylor’s survey? In which case it has nothing to do with stack :)
&gt; The survey was hosted by the creators/maintainers of stack... so... yeah. Please let's not start this again. And I mean it with the best intentions and no confrontational attitude.
Perhaps I assumed too quickly since the post was made by FP complete and the top couple google results when searching for it are FP complete. 
&gt; I find it really interesting that there are a lot of responses in that list about toxicity of the community... As a fairly new Haskeller (4y) I can't say I've ever experienced any of that. This idea of a toxic community always seems to float about (sustained by who exactly???) and we really need to shake it. I think that's a normal part of a growing community; any community usually starts off being "nice" (they don't survive otherwise)--Haskell, Ruby, in the past, were built on "nice" and now Rust's community is described as "nice" (that too will pass)--and eventually, _magically_, they become "hostile" or "toxic" or whatever; in reality they just encounter more diversity of people. This is exacerbated as more people use Haskell in their jobs (livelihood) and have dedicated years of their life into this language: disagreements have higher stakes.
Does the increased number of responses imply Haskell is getting more popular like statistically significant? Or was it overestimated due to a large number of spams?
If by "this post" you mean this Reddit submission, snoyjerk is not Snoyman. As for the Google results, the 2018 State of Haskell Report by FP Complete is not the same as this survey, in spite of the name clash.
Congratulations to the anonymous script kiddie who pulled this off! Much hacker, wow. 
What are the reasons for not removing them?
Just a quick note: You should probably look into pattern matching for discerning different values of your sum types, instead of using guards with comparison.
If it is "very useful" why is it not getting picked up by maintainers? Academics (which is who I assume you are targeting with your paper comment) are not evaluated based on the software they maintain so it's literally something that could cost them their job... If a company finds a library useful, surely they can support it by allocating a bit of time to an in-house maintainer.
I suppose the term is derived from [architecture astronaut](https://www.joelonsoftware.com/2001/04/21/dont-let-architecture-astronauts-scare-you/), and it is not a positive description.
Reminder: it still says that 100% remote is not an option.
Ya, &gt;1000 people disagree here - https://github.com/tfausak/tfausak.github.io/blob/7e4937e284a3068add9e9af6b585c8d0215ff360/_posts/2018-11-16-2018-state-of-haskell-survey-results.markdown#question-082 That’s a lot of bugs in object code generation. :) I wonder how easy it is to remove the junk responses. 
I thought most surveys these days have some kind of CAPTCHA to prevent this. Was this not the case for this survey?
Unfortunately, as the email concludes, this is in some sense the lucky case: if the attacker had been mildly clever, we'd never have known anything were amiss and blissfully proceeded to reason off of compromised data. We can lament the presence of bad actors, but somehow I doubt our pleas will deter them; the smarter thing to do is accept that they exist and develop more secure methods of data collection.
This is my personal opinion delete it of bury it in negative votes if you like. I have no proof. It is only my smell sense, but this kind of dirty play is not the only example. For some reason Haskell is perceived as an strategic asset and some power is interested in his total control. I can not imagine a company trying such dirty tricks since this goes against his business which depends on the image of Haskell and his widespread adoption .
Apparently people would rather enable `IncoherentInstances` than `TemplateHaskell` by default. Probably the Cabal folks being salty at Yesod.^^^^(/s)
The `Applicative` instance is inferred from the context. `pure "HaHa" ++ ["Ha"]`: since the first argument of `(++)` must be a list, `pure :: x -&gt; [x]`, and `"HaHa"` is a string so `pure :: String -&gt; [String]`. `pure "HaHa" ++ ["Ha"] = ["HaHa"] ++ ["Ha"] = ["HaHa", "Ha"]`. `getZipList (pure "HaHa")`, `getZipList` must be applied to a `ZipList`, so `getZipList (pure "HaHa") = getZipList (ZipList ["HaHa", ...]) = ["HaHa", ...]`. If the choice cannot be inferred or generalized, you will get a type error.
At first, when the instance for pairs was introduced, was no pressing need for also adding the instances for larger tuples. Later on, when adding them was suggested in a post-FTP context, some of those who dislike the instance for pairs objected, and since there was still no pressing need, nothing happened.
In do blocks you leave off the 'in'. So the following works: ```main = do ln &lt;- getLine let x = length ln -- note: no in! putStrLn $ show x ```
Trolls gonna troll. Good catch on those anomalies!
You said it, such allegations need strong proof to be taken seriously, so the only possible answer to this is "maybe". As in seriously, it's not entirely implausible but let's leave it at that for now. On a more "meta" note, I wish this hellish reddit platform didn't have "up/downvotes" at all; often civil discourse relies on more subtle cues than these, as downvotes can signify anything from "I don't agree" to "this is unclear" to "I don't like you"; symmetrically for upvotes.
Your comment is needlessly polarising. In addition, the causal link it suggests seems very tenuous: from liking cabal-install, to disliking Stack, to disliking projects associated with people involved with Stack, to disliking an extension heavily used by one of those projects, to *not* voting for this extension to an extent enough to give it such a low score in that survey question. Furthermore, it's not like there aren't other reasons for not wanting `TemplateHaskell` to be enabled by deafult.
Early on a group of people was trying to [FUD the survey by claiming it was compromised](https://www.reddit.com/r/haskell/comments/9t8q9y/2018_state_of_haskell_survey/e8wjgzo/) and now this.
inb4 survey on a blockchain
On the note of “civil discourse”; the only way to keep a discourse civil, ie, beholden to the quodien politics of intersubjectivity, is to limit the detachment people can have to accountability under its terms. Next survey should be blockchain and non-anonymous?
Not wanting to break compatibility, I assume.
Breaking lots of existing code for minimal gain. Lack of consensus that they should actually be removed.
Thanks, you were both very helpful.
If it is "very useful" why is it not getting picked up by maintainers? Your objection makes a lot sense. I would change point 1 to "create a very publicized library with all the flashy intellectual topics at hand" Then everything is coherent.
You need parentheses around `a:as` in `miniElem (a:as)`.
done. now i get this error: [1 of 1] Compiling Main ( Desktop/Haskell/miniElem.hs, interpreted ) Desktop/Haskell/miniElem.hs:2:1: error: Equations for ‘miniElem’ have different numbers of arguments Desktop/Haskell/miniElem.hs:2:1-17 Desktop/Haskell/miniElem.hs:(3,1)-(4,57) Failed, modules loaded: none. Prelude&gt; i guess my code is false but i dont really understand the error
&gt; done. &gt; &gt; now i get this error: &gt; &gt; [1 of 1] Compiling Main ( Desktop/Haskell/miniElem.hs, interpreted ) &gt; &gt; Desktop/Haskell/miniElem.hs:2:1: error: Equations for ‘miniElem’ have different numbers of arguments Desktop/Haskell/miniElem.hs:2:1-17 Desktop/Haskell/miniElem.hs:(3,1)-(4,57) Failed, modules loaded: none. Prelude&gt; &gt; &gt; i guess my code is false but i dont really understand the error 
In "Haskell Programming from First Principles" there is an example for a binary tree and a `insert'` method. The code works fine, but if I load the code into ghci with -Wall I get the following warning: `binarytree.hs:8:1: warning: [-Wincomplete-patterns]` `Pattern match(es) are non-exhaustive` `In an equation for ‘insert'’: Patterns not matched: _ (Node _ _ _)` `|` `8 | insert' a Leaf = Node Leaf a Leaf` `| ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^...` But for me, this seems to be a false negative - or I missed something about pattern matching. Any explanation would be great. The example code was: `data BinaryTree a = Leaf` `| Node (BinaryTree a) a (BinaryTree a)` `deriving Show` &amp;#x200B; `insert' :: Ord a =&gt; a -&gt; BinaryTree a -&gt; BinaryTree a` `insert' a Leaf = Node Leaf a Leaf` `insert' a (Node left b right)` `| a == b = Node left a right` `| a &lt; b = Node (insert' a left) b right` `| a &gt; b = Node left b (insert' b right)`
Given that we know [there were lots of bogus responses to the survey](https://mail.haskell.org/pipermail/haskell-community/2018-November/000362.html), it isn't useful to discuss the preliminary results before the data is cleaned up. (Note this isn't a complaint directed at the author of the Medium post, as he presumably didn't know about the problem. However, [the top comment to your other thread about it from a few hours ago](https://www.reddit.com/r/haskell/comments/9y4hcn/comment/e9y96tp) did point the issue.)
Thank you, /u/snoyberg for making Stack and Stackage. They have made my experience developing Haskell, personally and professionally, much better. What follows is a short list of what makes me want to move away from Stack even so, and one apparently trivial thing that I will miss. The biggest problem I'm having with Stack is recompilation, both too much and too little. * I work on a fast-moving project with lots of packages. Almost invariably, when I try to update and compile in a multiple-week-old directory, the build breaks in a way that cleaning a package fixes. Sometimes it's a GHC panic due to "symbol not found"; sometimes it's a nonsensical type error; once, it was a package in a custom snapshot. It's not always obvious which packages need to be cleaned, but it's always annoying and draining. * I also work with packages with lots of executables. They are slow to build, and usually I don't need them, but they are all rebuilt every time (or almost every time), despite Stack's assurances to the contrary. * Don't get me started on the `--test` flag. I added a shell alias to test in a separate `.stack-work` directory. When I want to build the tests of a package that I previously built without `--test`, it gets unregistered, which unregisters all of its reverse dependencies, which can be tens of packages (and tens of minutes of build time). * Worse, sometimes (I haven't figured out precisely when), building with `--test` will result in linker errors if I build without `--test` afterward. A much smaller, but still relevant problem, is that newer `Cabal` features take a while to get supported, and even then don't get supported fully. * I'm using an internal library in one case, to extract common code from executables that doesn't belong in the main library, and to avoid depending on e.g. `bytestring` for the main library. Or, at least I was, but we pushed for `haddock` support, and `stack haddock` chokes on internal libraries. I expect that this will _eventually_ get fixed (I know there's an open ticket for it), but I can't use it meanwhile. * I expect this will be even worse with multiple public libraries and other Backpack features. Finally, the thing I would miss if I moved to `cabal-install`: I can download a Stack executable which is decoupled from a GHC version, and then have it install the entire tool chain, for multiple different versions of GHC, without me even having to think about it. This makes environment setup and upgrades _so_ much less painful than they would be otherwise, especially on e.g. CI. I look forward to the day Stack fades away because it isn't needed any more (but Stackage should live forever). Thanks again. 
Your last case, as well as the type declaration, takes only one argument. You need another argument to put the minimum in (as you've done for the first case). 
Hey everyone, Maybe dismissing toxicity in the community as overblown isn't the best way to persuade people that it isn't toxic. This sort of thing is always contingent on the narrowness of individual perspectives, y'all's included.
If thera aren't enough stable libraries, mainstream companies will never pick up a language like Haskell. This situation will change, I hope, with the Eta language.
Issue 1: indent four spaces to format code. miniElem :: [Int] -&gt; Int miniElem a [] = a miniElem a:as = if a:as &lt;= b:as then a:as else b:as Issue 2: The first case has two arguments. The goal and the type clearly show one argument: miniElem :: [Int] -&gt; Int miniElem [a] = a ... Issue 3: The second case's arguments need parenthesis to indicate the pattern match instead of parsing as three arguments: miniElem (a:as) = ... Issue 4: Undefined variable `b` is used - there is not good interpretation of what you mean here and why you'd compare an entire list so I'm just going to write a new definition. You could use a helper function instead if you'd like to pass the current minimum on your recursive calls instead of use the stack. miniElem (a:as) = let minNext = miniElem as in if a &lt;= minNext then a else minNext Issue 5: Write it as a total pattern match, even if the function won't be total: miniElem [] = error "No minimum"
That's a different argument.
Getting back to maintainers, maybe a lot of people knows enough Haskell to use a library written by someone else, but not enough Haskell to maintain an enterprise grade library.
This is really nice to read. Thanks for the overview, /u/snoyberg. I especially love the "Philosophy" section and agree with pretty much all of it. &gt; We discussed with our customer, and made the decision that it was time to invest in a new tool. I promise you that this decision was not taken lightly. For internal usage, we realized we didn’t have much choice. Releasing it as a competitor to cabal-install was another matter. But it came back to the “promote (commercial) adoption of Haskell.” We had lots of anecdotal evidence to suggest that a new build tool would help that case. Was there any consideration given to the idea of patching cabal-install instead? It seems to me like even if you had to maintain this fork internally for some time, it would have taken much less time than implementing a new tool, and could have potentially found its way upstream in some form. Plus it would have been nice for someone to have implemented some form of revision pinning by now :)
Thanks! We (mostly I tbh) made the judgement call that it would take less time to do a from-scratch implementation, and I still believe that's true in hindsight. I also didn't see much advantage to a patch approach, since it didn't seem likely that the patches were going to be included. This isn't an attack on the Cabal team, but reflects the inherently different design goals at the time.
Quick note, /u/etorreborre’s blog is named “barely functional.” The post title implies he is claiming that the state of haskell is barely functional. 
About toxicity… here's my experience: I've been studying Haskell on/off for few years (whenever I have time, kids…). I've purchased couple of Haskell books and contacted one of the authors via social media to ask couple of tips on one specific subject. This is pretty much the response what I got: &lt;some vaque "tips" as useful as "just Google it"&gt; and then "I really dislike how you approaced me. You seem to be the kind of person who wants other people to do your work".
I do get reports of recompilation problems, and I wish we could pin them down and fix them. We rarely get reproducing cases though. And often, the first step of the debugging process is to figure out whether it's a GHC bug, a Cabal-the-library bug, or a Stack bug. I think the most common cause of the invalidated build artifacts is code which isn't async-exception safe, but that's more of a gut feeling than any hard evidence. As I alluded to in the blog post, I'd really love a world where there were multiple methods for GHC installation, the current `Stack.Setup` being one of them, and `cabal-install` could have the optional ability to trigger automatic GHC installation. I think that would give you back the feature you're missing. Personally though, an even bigger feature I'd miss is reproducible scripts and GHCi invocations. It's been invaluable in my efforts at documentation and training. I realize that's less important for some people's use cases, but I spend a _lot_ of my time these days in that department, and having a fairly reasonable expectation that "run `stack Main.hs`" will Just Work is nice.
The important line is Desktop/Haskell/miniElem.hs:2:1: error: Equations for ‘miniElem’ have different numbers of arguments It states that you have defined a function with multiple equations (as is proper for this exercise), but that each equation expects a different different number of arguments. This is a big no in Haskell, as functions may not have a variable number of arguments. In this particular case, you've defined your function as miniElem :: [Int] -&gt; Int miniElem a [] = a miniElem (a:as) = if a:as &lt;= b:as then a:as else b:as that is, you've stated that your function takes one argument of type `[Int]`, and returns an `Int` however, your first equation `miniElem a [] = ...` is a is a function definition taking **two** arguments, the first of which is bound to the variable name `a`, and the second of which must be `[]`, the empty list. In your second equation of the definition `miniElem (a:as) = ...`, you have it taking **one** argument, which must be a list with at least one element, the first element being bound to the variable name `a` and the rest of the list being bound to the name `as`. You should look up "Pattern Matching" to find out how the function definitions should be written. (Or I can attempt to explain it) 
I'm fairly certain it was /u/Tekmo (Gabriel Gonzalez).
These are suggested in the "improve your development workflow" section, which is obviously aimed at people who already have a "development workflow". They aren't suggested off the bat.
Sounds to me like you want: \`\[Either a b\] -&gt; (\[a\], \[b\])\`, which is \`partitionEithers\`: [http://hackage.haskell.org/package/base-4.12.0.0/docs/Data-Either.html#v:partitionEithers](http://hackage.haskell.org/package/base-4.12.0.0/docs/Data-Either.html#v:partitionEithers)
&gt; I can download a Stack executable which is decoupled from a GHC version, and then have it install the entire tool chain, for multiple different versions of GHC, without me even having to think about it. This makes environment setup and upgrades so much less painful than they would be otherwise, especially on e.g. CI. Take a look at `ghcup`! https://github.com/haskell/ghcup
I think I'm looking for the monad that produces the `[Either a b]` result in the first place. I guess it would have a `throwError`-like interface, and preferable with the ability to easily switch between error handling modes (fail on first `throwError` vs. collect errors). 
Sounds like you don't want exception style behavior at all. You could just use a WriterT and log everything instead.
Yup. We should be wary of [survivorship bias](https://en.m.wikipedia.org/wiki/Survivorship_bias).
Unn, I don't follow. I'm guessing you have some function that does this: &amp;#x200B; \`\`\` doTheThing :: k -&gt; IO a catchAndRaiseTheError :: IO a -&gt; IO (Either e a) \`\`\` &amp;#x200B; Then you have a \`\[k\]\`, so you do \` traverse (catchAndRaiseTheError . doTheThing) ks\` &amp;#x200B; That gives u a \`IO \[Either e a\]\`, which you can then do \`map partitionEithers\` on. Is this not correct?
When you want to provide a multi-line definition to GHCi, you need to surround it with the `:{` and `:}` "commands". Otherwise, each line is considered in isolation. If you just look at the second line of the example code, alone, that would be a function definition with an incomplete pattern match. --- Alternatively, double-check the spelling on both lines. `insert'` and `insert` can appear similar to the eye as can `insert'` and `insert''`. Also, I know the ASCII apostrophe is allowed in identifiers, but it's also possible there are some very similar looking Unicode characters that are also allowed in identifiers, and that could further contribute visual confusion. --- For the example code you provided, I get different errors: GHCi, version 8.2.2: http://www.haskell.org/ghc/ :? for help Prelude&gt; :{ Prelude| insert' :: Ord a =&gt; a -&gt; BinaryTree a -&gt; BinaryTree a Prelude| insert' a Leaf = Node Leaf a Leaf Prelude| insert' a (Node left b right) Prelude| | a == b = Node left a right Prelude| | a &lt; b = Node (insert' a left) b right Prelude| | a &gt; b = Node left b (insert' b right) Prelude| :} &lt;interactive&gt;:2:26: error: Not in scope: type constructor or class ‘BinaryTree’ &lt;interactive&gt;:2:42: error: Not in scope: type constructor or class ‘BinaryTree’ &lt;interactive&gt;:3:11: error: Not in scope: data constructor ‘Leaf’ &lt;interactive&gt;:4:12: error: Not in scope: data constructor ‘Node’ 
&gt; methods for GHC installation Can GHC be made parallel-installable? I don't like "toolchain managers". I like the OS-level package system. I just want to be able to `pkg install ghc82 ghc84 ghc86` like I can do `pkg install llvm50 llvm60 llvm70`.
For instance, the types `(a, a)` and `Bool -&gt; a` have the same number of values. But this is a witness to the much more interesting fact that there's a correspondence between their values. A pair of `a` values can be viewed as giving the function table for a function, where the first value in the pair gives the result when the argument is `False`, and the second gives the result when the argument is `True`. Similarly, `Either a a` is the same as `(a, Bool)`. The `Bool` in the pair gives the same information as whether the value belongs in the `Left` or `Right` side of the `Either`. Arguments for these facts might seem to be about counting values, but they are really about understanding the correspondences. In fact, if `a` is an infinite type, then counts are no longer useful, but the importance of the specific correspondences remains. Asking you to count is likely just a trick for getting you to reason about the ways these things match up and correspond to each other.
It is also worth noting that, left unchecked, even a single bully can ruin it for a lot of people.
Yes, this sounds like the solution. I need to add `WriterT` to my monad stack, and split the errors into fatal and non-fatal ones. Then throw the fatal ones to `ExceptT`, and the non-fatal ones are simply logged to the `WriterT` part. And the runner can return a result of type `Either FatalError [Either FetchError a]`. Thanks! This sounds a lot simpler than I thought it was. I guess I don't need a separate library.
“If i can convince someone else that its good, then i can assume that i think its good” -Haskell politics in a nutshell
OP here! I made a `sitemap.xml` for my hakyll site and wrote something up about it (feedback welcomed if any). Hope it helps someone
&gt; I think a more interesting question would be why a particular tool is preferred over others. I use stack at work because that is what the team chose to use. Projects that build with stack can be incredibly difficult to build with other tools. I actually prefer another build tool.
Hi, thanks for taking the time to answer. &gt; When you want to provide a multi-line definition to GHCi, you need to surround it with the :{ and :} "commands". &gt; Otherwise, each line is considered in isolation. If you just look at the second line of the example code, alone, that would be a function definition with an incomplete pattern match. I get the same warning even if I enter the code enclosed by `{:` and `:}` or if I put the code in a file and ':load` it. I guess a typing error can be ruled out, as the calls to `insert'` yield sensible results. &gt; For the example code you provided, I get different errors: Sorry, I forgot to add type declaration for `BinaryTree`. The complete code looks like this. data BinaryTree a = Leaf | Node (BinaryTree a) a (BinaryTree a) deriving Show insert' :: Ord a =&gt; a -&gt; BinaryTree a -&gt; BinaryTree a insert' a Leaf = Node Leaf a Leaf insert' a (Node left b right) | a == b = Node left a right | a &lt; b = Node (insert' a left) b right | a &gt; b = Node left b (insert' b right) 
I know. It's great, but it doesn't quite scratch the itch, not being integrated into the build tool or the LTS snapshot, and not being cross-platform (we use Mac and Linux).
&gt; Botton inhabits all the types. This and the fact that there a no semiproflunctoids are the most worrying things for me.
&gt; I look forward to the day Stack fades away because it isn't needed any more This part frankly makes no sense to me. I do not program in stack or cabal. I program in haskell. Package management is not something I'm excited about or "looking forward" to. Rather it is something I consider boring and I'd like to never bother me. And in that sense stack delivered 100%. I used to feel pain with cabal in the old days. Now that pain is gone. That's all I could possibly ask from a boring and dependent tool. To not get in may way and not remind me of its existence too often. 
To test an hypothesis it is necessary to make it explicit first...
Not my experience.. I have never got intero working. 
The issue is that GHC doesn't know that that one of the guards must be true. This is actually a good thing because unlawful instances of `Ord` can return `False` for all three guards! `Ord Float` and `Ord Double` are examples of such instances because of how they treat `NaN`: λ&gt; insert' (0/0 :: Float) (Node Leaf (0/0) Leaf) *** Exception: &lt;interactive&gt;:(22,1)-(26,43): Non-exhaustive patterns in function insert' λ&gt; insert' (0/0 :: Double) (Node Leaf (0/0) Leaf) *** Exception: &lt;interactive&gt;:(22,1)-(26,43): Non-exhaustive patterns in function insert' You can fix this a few different ways. The simplest would be to simply replace the last guard `a &gt; b` with `otherwise`. This would make it so that the last case will always be selected if the other guards return `False`: insert' :: Ord a =&gt; a -&gt; BinaryTree a -&gt; BinaryTree a insert' a Leaf = Node Leaf a Leaf insert' a (Node left b right) | a == b = Node left a right | a &lt; b = Node (insert' a left) b right | otherwise = Node left b (insert' b right) Another way to do this would be to use `compare` and a case statement, which make it explicit that exactly one of the conditions will be true: insert' :: Ord a =&gt; a -&gt; BinaryTree a -&gt; BinaryTree a insert' a Leaf = Node Leaf a Leaf insert' a (Node left b right) = case compare a b of EQ -&gt; Node left a right LT -&gt; Node (insert' a left) b right GT -&gt; Node left b (insert' b right)
I should also add, I have had the pleasure of interacting with Eric from my Scala days, he’s not the type that would make a claim like “haskell is barely functional,” he’s a good guy. 
&gt; Personally though, an even bigger feature I'd miss is reproducible scripts and GHCi invocations. Can you elaborate? Specifically, does [this](https://typedr.at/posts/what-i-did-on-my-summer-vacation/#cabal-new-run-for-scripts-and-cabal-as-a-script-interpreter) count? &gt; I do get reports of recompilation problems, and I wish we could pin them down and fix them. We rarely get reproducing cases though. Yeah... I know I've never submitted one. I'm not sure I could get it down to a minimal example, given how state-dependent it is. Is there anything specific to do when one encounters such a problem that might help make it actionable? &gt; I think the most common cause of the invalidated build artifacts is code which isn't async-exception safe, but that's more of a gut feeling than any hard evidence. That doesn't feel right to me. In the build failure/linker error case, once the repo is in a bad state, the failure is deterministic.
Are you asking how to write a "bridge" to another language? I know Haskell has a FFI: [https://downloads.haskell.org/\~ghc/latest/docs/html/users\_guide/ffi-chap.html](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/ffi-chap.html) There are also things like inline-c, inline-r, or inline-java, but I don't have any experience with them.
Awesome idea. This will help with static linking a lot. Lgplv2 and gplv3* by the way. Not the other way around
https://gmplib.org states LGPLv3 or GPLv2. But yeah, static linking would be great! Just got to know https://github.com/nh/static-haskell-nix today
Ah, /u/WhistlePayer's [comment](https://www.reddit.com/r/haskell/comments/9t0p5n/monthly_hask_anything_november_2018/e9zqgrn/) is right, then.
What does the author mean by "modules"? I would normally assume ML modules, but that didn't quite make sense in the context of this post.
&gt; I have no idea how to use it myself or how to fit it into your types. Perhaps you can make any sense of it. The key is that `Errors` accumulates errors but only has an Applicative instance, not a Monad instance, while `Either` has a Monad instance (and thus also an Applicative instance), but short-circuits on the first error. This makes sense: with monadic composition (`(&gt;&gt;=) :: m a -&gt; (a -&gt; m b) -&gt; m b`), the `a` returned by the first action is used to determine which action (which `m b`) to run next, and so if that `m a` fails, we won't have an `a` and so we cannot obtain nor run the `m b`, so we are forced to terminate the execution early. With applicative composition (`(&lt;*&gt;) :: f (a -&gt; b) -&gt; f a -&gt; f b`), we already know which `f a` action we want to run after the `f (a -&gt; b)` action is over, so we can run that `f a` whether the `f (a -&gt; b)` succeeds or not, and thus we can accumulate both errors if they both fail. `runErrors :: Errors e a -&gt; Either e a` and `eitherToErrors :: Either e a -&gt; Errors e a` convert between the two styles. In the following, I use this to run the C and D computation in an Either block, so if both fail we only get the C failure, then I run B and that CD block in an Errors block, so the B and CD errors get combined, and then I run A and that BCD block in an Either again, so if A fails we only get the A failure: import Control.Applicative.Lift (Errors, failure, runErrors, eitherToErrors) -- | -- &gt;&gt;&gt; combine (Right "a") (Right "b") (Right "c") (Right "d") -- Right ["a","b","c","d"] -- &gt;&gt;&gt; combine (Left "a") (Left "b") (Left "c") (Left "d") -- Left ["a"] -- &gt;&gt;&gt; combine (Right "a") (Left "b") (Left "c") (Left "d") -- Left ["b","c"] combine :: Either String String -&gt; Either String String -&gt; Either String String -&gt; Either String String -&gt; Either [String] [String] combine ea eb ec ed = do a &lt;- asEither ea (b, (c, d)) &lt;- runErrors ((,) &lt;$&gt; asErrors eb &lt;*&gt; eitherToErrors (do c &lt;- asEither ec d &lt;- asEither ed pure (c, d))) pure [a,b,c,d] where asEither :: Either String String -&gt; Either [String] String asEither (Left e) = Left [e] asEither (Right x) = pure x asErrors :: Either String String -&gt; Errors [String] String asErrors (Left e) = failure [e] asErrors (Right x) = pure x Note that if one of the steps fail, Errors only collects the failures, not the successes.
Fair enough. I have bumped into the opaque delays but so far my own experience with it has been fairly rosy.
If calling the other language from Haskell is substantially easier than writing the other language in Haskell -- which upon reflection seems likely -- that would be plenty. On your recommendation I checked out the FFI section of the 2010 Haskell Report. It says Mercury would have to adhere to the C calling convention. Mercury's not extremely well documented so I'll have to ask the list; I don't want to bother them too much so I'll be batching my questions, which means I won't know for a while.
`haskell-src-exts` and `haskell-src-meta` not keeping pace with ghc releases, and also running OOM when compiling them on CI services or the like, has caused nontrivial amounts of pain. I wish that this functionality would be more tightly integrated into ghc itself and be released in lock-step with it.
Hopefully, Trees That Grow will make it easy to parse Haskell exactly like GHC does in the next few releases, though it's anyone's guess how happy people will be to pick up a dependency on `lib:ghc` with all the lack of promise of stability that comes with it.
One problem I had with Intero was that it would sometimes work and sometimes not. I never figured out exactly what was going on, but I believe when I installed it using `M-x package-install` (and I saw in the log it launched a Stack process to compile everything from scratch) it apparently only installed into the current Stack project I was working on, rather than into the `.emacs.d` directory. So after changing buffers over to a different project, suddenly the Intero executable file couldn't be found. That is my hypothesis as to why Intero doesn't work, but I never was able to really prove what the actual problem was.
&gt; Is the code right? How could it be right? It generates a parse error. 
&gt; Equations for ‘miniElem’ have different numbers of arguments It tells you pretty much exactly what is wrong with your code. 
Validation is not a monad, so it's rather difficult to include it in a transformer stack.
&gt; I can easily compare competing Haskell libraries to select the best one The results here definitely trended towards the "disagree" end of the spectrum. The best platform I've found so far for this is https://guide.aelve.com/haskell, which with some community contribution would be a great resource. I especially like the section at the bottom of each page which has a place for a description, pros, and cons for relevant packages. Check out the [Strings](https://guide.aelve.com/haskell/strings-o62hqc69) page for an example. h.t. /u/peargreen for driving the effort :)
`Validation` is not a monad, but OP can certainly use [`ValidationT`](https://github.com/tonymorris/validation/blob/128fdadbd73acc7abccf480a23dda2a8a605c2f7/src/Data/Validation.hs#L434), and lift this into `ReaderT Config (ValidationT Err m) a`. This would give them everything they need.
If you're curious, this mailing list grew out of the "Haskell in K-12" dinner at ICFP. Just trying to make it visible to anyone who might be interested.
Thank you so much for this series! It gives a nice guided walkthrough as a complement to the docs out there. I have been thinking of switching from Hugo to Hakyll. What are your thoughts on Hakyll vs Hugo or Jekyll?
A nice approach—separate the concerns, then compose them together, letting the compiler derive as much boilerplate as possible. The third example in the final code sample seems to have a copy-paste error: -ghci&gt; decodeValues [IntValue 42, BoolValue True] user +ghci&gt; decodeValues [BoolValue True] user Left (WrongValue (SingleValueError {valueErrorExpected = "Int", valueErrorActual = BoolValue True})) 
Male: 82.2% Female: 3.6% That’s `Void -&gt; a` ...
For the `parent1` naming problem, you could just have a single field `parents :: (Eq.Dict a, Show.Dict a)`, whose type is exactly the superclass constraint including the "tuple-ness" of the constraint. I do not like the `as Eq b` syntax. That leaks the names of the function's local variable bindings. I'd prefer to follow suit with `TypeApplications`, where the first `@{}` argument applies to the first constraint, and so on, and you can do `@{_}` to skip one if you like. Alternatively, just like my `parents` suggestion, you could preserve the "tuple-ness" and require `@{(a, b)}`, again allowing `_` to skip part of it. I *love* the instance definition syntax. `instance Foo A = x` is something I've very often wanted. The ability to use `coerce` on instances is particularly useful, as a more general replacement for GeneralizedNewtypeDeriving and DerivingVia. There needs to be a way to reference a normal instance though; i.e. how do you get the default `Eq.Dict Int`? I actually preferred the idea that this is simply syntactic sugar for auto-generated newtypes and instances, rather than doubling down on the already shaky roles system. To solve the local data environment problem, I'd *really* like to see support added for `data` / `instance` declarations at non-top-level scopes. It just seems a lot more elegant if all of this is merely sugar for higher order data and instances; roles are not a system I'd like to see expanded like this. I disagree that the multiple constraints incoherence they described only results from "bad style." I could write a function with type `(Traversable f, Applicative f) =&gt; ...` and get the same problem because both classes have `Functor f` super classes. This is a really exciting idea. I really hope it makes it into GHC.
I don't know if I should be more annoyed at you for making that pun, or at myself for getting it so quickly.
The demographic skew is profound: we have almost as many members of the community identifying as transgender as identifying as female. Assuming these are largely mtfs (likely, given survey results of similar communities), it’s quite possible we have more mtfs than biological females. Make of that what you will, it’s just interesting how dissimilar our demographics are to the general population, where biological females are *slightly* more common than mtfs. One data point the survey didn’t measure that I wish it had is birth order. Popular wisdom and published research say it makes almost no difference, but it was included in the last SSC survey, and there was a wild overrepresentation of firstborns. So given the substantial demographic overlap between our communities on the metrics above, I’m now curious if we have a similar skew in birth order. Guess I’ll have to wait til next year to have a shot at finding out!
There's no reason why OP could not keep the `Validation` functions pure, and convert back to `Either` when it suits their MTL stack. This really is the more elegant solution. In fact, if OP only needs `ReaderT`, then `ReaderT Config m (Validation Err a)` is a valid type to work with. And one less transformer!
Thank you for the feedback about the blog post! Also, it is an example of how you can use stateful computations in Haskell (people often ask about such cases). The copy-paste error was fixed :)
Could it be fake data?
That's not quite the same thing. The problem with it is that it uses dependency solving instead of dependency pinning. The build _may_ succeed. It may fail. It may use slightly different versions on different machines or at different times that have subtly different behavior. What I'm talking about in the linker error case is that I believe GHC is using non-cautious file writes: it's beginning a write to a file path, getting killed, and then never rebuilding that artifact. Instead, it should write to a temporary file, and when the write is complete, atomically move it. I don't have hard evidence to back this up, but I've seen lots of reports of failures around people either using Ctrl-C or killing CI jobs.
I can't speak to having used Hugo beyond "Hello, world!", but I was very fond of jekyll (even had my site built in it years ago), but here was my path: 1. `jekyll` (good experience; thought I could do better so...) 1. wrote my own solution in `nodejs` from scratch (this worked, and I learned a fair bit, but it wasn't that good even though I had it for a couple of years) 1. considered using [gatsbyjs](https://www.gatsbyjs.org/) (and I will for client work that needs to connect to wordpress, etc) 1. found `hakyll` and was very happy with what I saw hakyll pros: * get to work with Haskell * am able to customize nearly everything and generally works as I would expect * super easy to load content and transform it (sitemap, RSS &amp; Atom, archive, etc) into whatever you want * looking forward to writing CSS in Haskell with [Clay](http://fvisser.nl/clay/) * fast compilation (even while "watching" for file changes) * can jump into the source code and figure out what's going on because of decent documentation in there * generally responsive on github issues &amp; their IRC channel * the template language for looping, coditionals, etc, forces you to do complex things elsewhere and mostly keep significant logic out of views hakyll cons: * because it is so customizable, you're now swimming in the deep end * just like haskell, if you straight up import modules entirely instead of doing named imports, you've got to track down where XYZ function comes from and figure out how to use it * need to be willing to jump into the source code to read types/defaults/what functions are available for use * making mistakes in templates means that your code will compile, and it will load, but your content could be missing entirely * the template language for looping, conditionals, etc, has nowhere near the flexibility of liquid, but you probably shouldn't be doing complex things in views, so that's okay :) The hakyll name is a hat-tip to jekyll, but they were completely different experiences. Jekyll seems to have a much more documented existence (on their site, blog posts, etc), especially since GitHub Pages runs on it, but I feel like I can now do _anything_ I need to with hakyll, so that makes it a winner for me for smaller things. If I am going to be doing client work, then (as stated above), I'm going to probably go with gatsbyjs because of everything it does out of the box, even though you may give up complete autonomy over every bit. Hope that helps
Thanks for the explanation and for pointing out the edge case of unlawful Ord instances. I learned something new today.
Are the slides available?
I think it might be societal pressure. Most of the data is taken from 25-35 year olds so maybe when they were younger it was very abnormal for girls to get interested in such fields. There’s the boys playing video games stereotype for example. This is all speculation though, I know video games and science sets were marketed for boys at the time but im not sure how the gap is that wide. Also I don’t get why the more unpopular domains have even larger gaps (compiler construction class vs machine learning class or javascript users vs haskell users). The firstborn thing seems to be fishy though.
So nice to see im not the only one thinking build tools and libs should be more compatible with each other. 
One thing that stands out is that Cabal is starting to regain some of its lost market share (`new-build` has got something to do with that) and Nix is also gaining whereas Stack seems to be stagnating | 2017 | 2017% | 2018 | 2018% | Diff | Diff% ---|---|----|----|----|----|---- **Cabal** | 464 | 27.7% | 654 | 48.1% | **+190** | **+13.3%** **Nix** | 227 | 13.6% | 311 | 22.9% | +84 | +5.8% **Stack** | 982 | 58.7% | 995 | 73.1% | +13 | -0.5% | *1335* | *100%* | *1361* | *100%* | +26 when comparing [2017](http://taylor.fausak.me/static/images/2017/11/15/chart-preferred-build-tool.svg) to [2018](https://taylor.fausak.me/static/images/2018/11/18/question-041.svg). This is going to be interesting if the trend keeps up!
We only have the video right now, but I’m sure you could ask Matthew for them.
Ugh. I realise the ambiguity now... the title was inferred by reddit based on the `&lt;title&gt;` HTML header. 
Whoa, hold your horses! Given the [survey tampering](https://mail.haskell.org/pipermail/haskell-community/2018-November/000362.html) I don't think you can genuinely infer any claim of one tool gaining popularity over the other. For all we know the Cabal and Nix numbers are more likely to be inflated as a result of potentially removing too many pro-Stack false positives.
Your comment was difficult for me to unpack - is this an accurate re-phrasing? --- 'X is biologically Y' means that X's gender (which is Y) has been determined by their biology. 'Female' is describing a gender. So, 'transgender women are biologically female' means that transgender women's gender (which is 'female') has been determined by their biology.
Purchased and enjoying it so far, thanks! I did, however, find a few minor mistakes. I think. Or maybe I didn't understand at all, which is entirely possible. p17: (a, Either (b, c)) is not a type, shouldn't that Either be a Maybe? p27: (1 + 17) Type 3, Type should read *, right? It this the right place to submit such comments?
&gt;It says Mercury would have to adhere to the C calling convention. Well, assuming Mercury also has a C FFI (likely, as almost every language with an FFI has one to C), you can just use their FFI to export functions following the C ABI (the only reasonably stable ABI around, hence why everyone supports it). Side note: The GHC user guide also has a lot of relevant stuff about the FFI. 
Also there were 16 responses from Norway, that can't be right, there's [just 17 of us altogether](https://www.haskellers.com/).
You mean *true* positives. Yes, I do wonder how the results would change if the filter removed non-Stack responses as well. Right now it has some rules but limits those rules to pro-Stack responses. In theory the number of pro-Stack responses that are removed should be dramatically higher than the number of pro-Cabal responses that would be removed if they were included as well. But we have to remove pro-Other responses of the same form to help eliminate bias. /u/taylorfausak, is there any chance you could run the numbers again this way; eliminating pro-other responses of the same profile considering the attacker may have included some such responses to offset any suspicion?
Let's say you have a function that normally returns a `Bool` but might also fail and signal that failure in its return type. You could model this with a pair of an integer error code and the actual result: `(Int, Bool)`. Another option would be `Maybe Bool`. Now if you do the calculation you see that `(Int, Bool)` has a lot more inhabitants than `Maybe Bool` and start to think about which comes closer to the actual number of outcomes for your function.
This is in fact a primitive parser using StateT and EitherT. I wrote a pretty similar one using MaybeT and StateT to build a primitive CLI options parser simulating a bash like imperative command line parsing using the "shift" operation. [See this](https://github.com/composewell/streamly/blob/75272473819b49d91ce901ab399daaf0a411d8a2/benchmark/Chart.hs#L73) . Perhaps, as you were writing this blog post we were discussing this at the Bangalore Haskell meetup on 17th Nov:-)
You could try [`ExceptT`](http://hackage.haskell.org/package/transformers-0.5.5.0/docs/Control-Monad-Trans-Except.html#t:ExceptT) on top of [`ListT`](http://hackage.haskell.org/package/pipes-4.3.9/docs/Pipes.html#t:ListT).
Sitemaps become invalid once they contain over 50 k entries or 50 MiB (or 50 MB, to be safe) of uncompressed data. This should be taken into account when generating them.
&gt; You mean true positives. I literally meant false positives as pointed out in the email thread &gt; Luckily for us, they were not very smart, and made some obvious errors, so in this case we can weed out the bad responses **(although, sadly, losing at least a few real ones as well).** 
Ah, I see. There is no source link here https://hackage.haskell.org/package/base-4.12.0.0/docs/Data-Ord.html#v:min I wonder why Stackage's Haddocks have `GHC.Classes` but Hackage's don't.
[This message by Gershom](https://mail.haskell.org/pipermail/haskell-community/2018-November/000377.html) (which, unlike me, had a close look at the data and played with Taylor's script) seems to suggest that would be unnecessary.
(Not the OP, but) I believe the key point there is "cartesian dualist". Talking about a "biological gender" would suggest a clean separation between "body gender" and "mind gender", with the former being determined at birth, and the latter being changeable.
There are more people in Norway writing Haskell than on your list.
Oh yes, totally my assumption! I don't suspect any nefarious play on your part. :) I just wanted to clarify, as that was first thought.
Author and presenter of the paper here. Thank you for sharing you interesting ideas on this. &gt;For the `parent1` naming problem, you could just have a single field `parents :: (Eq.Dict a, Show.Dict a)`, whose type is exactly the superclass constraint including the "tuple-ness" of the constraint. In terms of naming, this indeed seems nicer. However, this makes it less user-friendly to override a specific super-class dictionary. It would also complicate the implementation, specifically, we would no longer be able to simply cast an exposed dictionary to an internal one. &gt;I do not like the `as Eq b` syntax. That leaks the names of the function's local variable bindings. I'd prefer to follow suit with `TypeApplications`, where the first `@{}` argument applies to the first constraint, and so on, and you can do `@{_}` to skip one if you like. Alternatively, just like my `parents` suggestion, you could preserve the "tuple-ness" and require `@{(a, b)}`, again allowing `_` to skip part of it. We are open to better ideas for this, and we have considered positional syntax too. Our reasoning for the 'nominal' syntax is that, unlike type variables, constraints are *unordered*. Also, this would overload the meaning of `_` at the expression level. What if you want the `_` in `@{_}` to be a hole? &gt;I *love* the instance definition syntax. `instance Foo A = x` is something I've very often wanted. The ability to use `coerce` on instances is particularly useful, as a more general replacement for GeneralizedNewtypeDeriving and DerivingVia. There needs to be a way to reference a normal instance though; i.e. how do you get the default `Eq.Dict Int`? In the paper we propose the following type class to obtain the dictionary of the implicit instance in scope: class HasDict (c :: Constraint) where type DictOf c :: Type getDict :: c =&gt; DictOf c To obtain the default dictionary, you would write: `getDict @(Eq Int)`. Only the compiler would be able to generate instances of this type class (just like `Coercible`). &gt;I actually preferred the idea that this is simply syntactic sugar for auto-generated newtypes and instances, rather than doubling down on the already shaky roles system. To solve the local data environment problem, I'd *really* like to see support added for `data` / `instance` declarations at non-top-level scopes. It just seems a lot more elegant if all of this is merely sugar for higher order data and instances; roles are not a system I'd like to see expanded like this, and it doesn't really seem that reliable to me. Don't forget that the newtype translation that you prefer also relies on the 'shaky' roles system. The role criterion is just a shortcut to avoid the whole newtype translation. Also, generating the newtype translated version results in more work for the simplifier to optimise it to the GHC Core code that we can now generate directly. I would also like to see support for local `data` and `instance` declarations, but I don't want to tie our idea to this, as people have asked for this in the past and I don't see it happening anytime soon. Additionally, local instances would require modifications to the constraint solver, something we purposely avoided. &gt;I disagree that the multiple constraints incoherence they described only results from "bad style." I could write a function with type `(Traversable f, Applicative f) =&gt; ...` and get the same problem because both classes have `Functor f` super classes. It is true that there are some useful cases of multiple type-class constraints that have a common ancestor. An example we mention in the paper is `(MonadState m, MonadWriter w m) =&gt; ...` where `Monad m` is the common ancestor (as well as its superclasses). This is the reason why we turned it into a warning instead of an error (until we find a better solution). &gt;This is a really exciting idea. I really hope it makes it into GHC. Thanks! &gt;Finally, is there a more appropriate place to put these thoughts? Is this tracked on the GHC trac at all? I intend to write a GHC proposal about all this in the near future.
Is it possible to get static lookup tables (i.e. like constexpr in C++), using Template Haskell or otherwise? Or perhaps simpler, say an vector of unboxed ints?
&gt; Pure mathematics is basically at parity That's news to me. Do you have a reference?
Looks really clean and concise, as a beginner the pros and cons are especially helpful.
Everything about your mind is about your brain, which is part of your body and is biological. There is no non-biological gender. There is no aspect of personality or behavior that is not rooted in the biological mechanism of your brain.
/u/bgmari: Thank you for your talk and all the work you put into GHC! In the current design there are multiple generations 1 to N which are all usually collected by copying collection. In your talk, you describe a GC design with a major heap which is collected using concurrent and parallel mark and sweep. Does this new heap replace all the generations 2 to N such that in the end one ends up with only the copying-collected nursery and the major heap? Or do you keep all minor generations 1 to N and add the new major generation N+1 on top? This would then allow to use the copying collector for aging while still keeping the pauses bounded to reasonable times for small N and small generation sizes.
Cool, thank you! I think it's a good design! Tuneable latency vs throughput (within some limits), fast bump allocation, good locality in the minor generations, generations to handle excessive garbage (in contrast to Go), ... Concerning the term "aging", I don't quite understand how it is implemented in GHC right now. It seems aging is something which exists additionally to generations? What I meant above with "aging" is that you are keeping the minor generations and age from generation 0 to 1 etc. I probably misused the term. My question is answered though. How are you handling the stack scanning? Does it happen in the STW phase? GHC also uses mark and compact for the last generation if the system is running out of memory. Are you planning a compaction phase in the new GC too? You probably try to avoid it by using a good allocation scheme.
Big difference I see: If you want to build a `WriterT [FetchErr] a`, you always have to have an `a`. If you want to build a `Validation [FetchErr] a`, you do not have a have an `a`. If `a` represents something that was not previously present locally, which is why we needed to fetch it, I would tend toward a `Validation` since it doesn't force me to provide a dummy/default/empty/null `a` value. On the flip side, if the `a` was some sort of command-object or other control-flow-as-data, then I'd tend toward a `Writer`/`WriterT`.
IIRC, WriterT and ListT both have relatively poor performance due to massive thunk allocation. I believe either (or both!) can be replaced with conduits or pipes or other streaming libraries, FYI.
[Sure](http://spartanideas.msu.edu/2014/06/14/percentage-of-bachelors-degrees-conferred-to-women-by-major-1970-2012/): &gt; Surprisingly to me, most of the STEM majors aren’t doing as bad gender disparity-wise as I expected. 40-45% of the degrees in Math, Statistics, and the Physical Sciences were conferred to women in 2012. Harmonizes with my experience, too. I studied math in uni and there were only slightly more guys than girls—nowhere near the disparity I see in programming.
This is not correct. I wasn't looking specifically for fake responses on the build-tools count. I was looking for statistical artifacts in general and discovered a single source of them, which _so happened_ to be from an actor that was trying to juice those particular numbers. I have every reason to believe that the remaining numbers reflect actual responses -- though again, one _cannot_ generalize from actual responses to "all haskellers" because of prior discussed issues of sampling bias, etc.
The filter and the structure of it wasn't based around removing "pro stack" data. It was basically based around removing data without demographic info and then selectively allowing some data back from that category that otherwise showed signs of variance with the scripted attack. However, the amount of data in that latter category was _very small_ compared simply to the overall dataset, since nearly all the "no demographics" data was tainted.
&gt; The build may succeed. It may fail. It may use slightly different versions on different machines or at different times that have subtly different behavior. Except that in such a script, the dependencies can always be set to exact numbers rather than ranges, which gets things closer...
It would be really nice to add mac support to ghcup, and given the structure of the script, it doesn't seem like its an impossible task!
&gt; I don't have hard evidence to back this up, but I've seen lots of reports of failures around people either using Ctrl-C or killing CI jobs. Makes sense. FYI, though, the failures I am seeing did not involve Ctrl-C or CI, though they were on a Mac and probably involved non-trivial custom setup scripts.
Yes, you _could_ do that. You would need to specify the exact versions of all transitive dependencies as well. And you'd need to hope that future metadata revisions don't break the build. Stack script's default behavior handles this automatically.
So out ivory tower is made of brick and mortar? 
Where’s the pun?
Those libraries are very cool! I ended up being annoyed that rewrites over Data.List.NonEmpty so I went up using a type indexed list and GHC.Generics doesn't handle existentials. [kind-generics](http://hackage.haskell.org/package/kind-generics-0.1.0.0) filled in the gaps with surprisingly little boiler plate, though.
they're probably referring to `absurd :: Void -&gt; a`. Not sure why that's "absurd" though – the missing 14.2% probably checked "prefer not to specify" or something...
Because I do not have a guarantee that it will be in the .rodata section of the binary (I believe C++ can do this), it might be a thunk and be computed once at runtime. If it is a thunk and then computed once, I still have at least 1 extra indirection for all future accesses.
"IHaskell on WSL" doesn't have quite the same ring to it :).
Ah. I suppose that is maybe a pun, after all. 
Could you please help me to get it?
Sure. Building in remote freeze file or pinning support into cabal is a fine idea.
Ah, I think there are several separate question then. Correct me if I'm wrong but... &gt; How do you make static lookup tables with a minimum number of indirections? and &gt; How do you make compile-time static-lookup tables without resorting to FFI? or even: &gt; Is it possible to make a Lift instance for Vector? Good thoughts.
&gt; It may fail. It's not like dependency pinning with Stack is a panacea and will guarantee the build to succeed always on every OS, no? Stack's issue tracker is full of problems such as https://github.com/commercialhaskell/stack/issues/4399 or https://github.com/commercialhaskell/stack/issues/4373 or https://github.com/commercialhaskell/stack/issues/3487 or ... 
I have no idea why you think the 2017 results are so much more robust / protected against this kind of thing. It's just as likely that there is skew in the 2017 results as it is that there is skew in the fixed 2018 results. Regardless I also don't know why people care so much. With some things like base library changes or which library to standardize on I can understand people being pretty passionate and pushy. Because standardizing on one thing is generally to the exclusion of the other (if everyone is using Aeson you are not going to find many instances for a competing JSON serialization library). But libraries using stack will also be on hackage, libraries using nix will also be on hackage, and libraries focused on cabal-install can still easily be uploaded to stackage and pinned by nix. I personally essentially use all 3, stack for GHC-only projects, and nix + cabal-install (for fast incremental building within a nix-shell) for GHC+GHCJS projects. It would affect me very negatively if any of the above were abandoned.
This seems like a 'yes'. Thanks
[removed]
Where could I learn more about this limit? I also reckon this same method could be used to generate multiple sitemaps (`sitemapindex` and whatnot as noted on https://support.google.com/webmasters/answer/75712)
As you go, be sure to search through [the hakyll GH issues](https://github.com/jaspervdj/hakyll/issues) if you run into problems. If you uncover good ways to do things, consider using hakyll to write about hakyll and educate the rest of us!
a very interesting idea! I wonder whether this could be useful to fuse stuff.
I don't have the time yet to watch the full video, but holy crap those error messages look amazingly clear! I love it! 
I am very sorry that my first post making it to /r/haskell ends up being: 1. wrong because the data was manipulated 2. having such a misleading title! I have since edited my post with my thoughts on the proper results (and now I understand better why I was surprised the first time by some of the answers). I also thank /u/pokemonplayer2001 for jumping in and vouching for me (unfortunately I can't recognize you from your handle). My blog title intended to be self-deprecating, I never thought this could imply that Haskell is barely working (and honestly that really made me laugh :-)). My son suggested that I should call my Medium site "Fully functional", but I already anticipate the fate of my next blog post: "JavaScript, a year in review" :-D.
Well worst case latencies of 1.3s (in the talk) don't look inspiring. Also, even if the numbers today are manageable (a) they're only going to get better and (b) having a specific GC might help people market Haskell better to other audiences which balk at stop-the-world GCs.
Worth comparing [`build`](https://www.reddit.com/r/haskell/comments/19p6w3/the_build_function_explained/) (as in build/foldr fusion) build :: forall a. (forall xx. (a -&gt; xx -&gt; xx) -&gt; xx -&gt; xx) -&gt; [a] build builder = builder @[a] (:) [] where we think of `(a -&gt; xx -&gt; xx)` and `xx`as `(:)` and `[]` (instantiating `xx` to `[a]`) abc :: String abc = build $ \(·) nil -&gt; 'a' · ('b' · ('c' · nil)) with a version using `List`: build :: forall a. (forall zz. List zz =&gt; zz a) -&gt; [a] build make = builder @[] abc :: String abc = Main.build ('a' · ('b' · ('c' · nil))) or using `Builder` type f ~&gt; g = (forall xx. f xx -&gt; g xx) build :: Builder List ~&gt; [] build (MkBuilder builder) = builder @[]
This reminds my of the tagless final encodings... If I understand it correctly, you define a tagless final encoding of a list, and Yoneda provides you with a free functor for the "semantics" of the encoding.
Is MyProgram.hs a game? Why exactly does it have such high latency times? In practice writing a game I saw average latencies of 0.0001s and worst case around 0.001s which are basically means this is a non-issue in 60 fps games.
What's the GC working set in the talk? I'd expect most games state to be pretty small and GC to be potentially fine due to that (assuming you aren't holding assets into the GC heap as well). But maybe this new low latency GC would still be nice and make it harder to accidentally pause for too long.
Well, I haven't written any games in Haskell so I will defer to your experience. Actually I might try writing my own game now, once I finish my 5 different hobby projects 😅.
Stage-polymorphic functions are a very VERY cool idea. I love static analysis, static guarantees, and shifting work to compile time. I doubly love a good story for being able to interpret code on the fly, and elegantly shifting compile-time checks to dynamic checks when necessary. However, having to use typeclasses in that way to write stage-polymorphic functions seems very tedious and inelegant.
True, that's the best I could do, unfortunately 😕 I seriously tried getting it working natively, but got stuck during the build of IHaskell itself: one of the dependencies of IHaskell was the "unix" package, and porting something like that was not a task I could take on. The best alternative was to use WSL. Thanks for stopping by! 
I get what you're saying but everyone understands what is meant by "biological female," it's referring to gender (not that I believe in it) determined by DNA rather than the "biological" shit going on in the brain. I don't see a reason to change the understood meaning.
This seems to feel somewhat reminiscent of the "transducers" of Clojure in years past?
You don't identify someone's gender based on their karyotype. That would be impractical. &gt; I don't see a reason to change the understood meaning. I think a trans woman could tell you. 
I understand exactly why they would want to change it, I just don't share their reasons. Any "objective" reason to do so?
Are there any languages that allow arbitrary numbers of stages? Instead of a compile time stage and a runtime stage, it could be useful to have arbitrary numbers of stages. Possible ramifications: \- A form of dependent types, where types may depend on values, but only values calculated during previous stages. \- Startup code (e.g. loading up configuration files) could run as its own stage, configuration could be treated as constants in the main program. \- \`error\` during early stages would be perfectly acceptable, since it would amount to a compile time error. A lot of what is currently type-level programming could possibly be value-level programming during a previous stage \- Many things could potentially become library code, and run as an early stage, e.g. package management. Not sure what the interface to such a system would be... To interpret it you could run through each of the stages. Or could you run the program and output the next stage as a continuation?
I thought GHC already had some bidirectionality in the typechecker ([paper](https://repository.upenn.edu/cgi/viewcontent.cgi?article=1291&amp;context=cis_papers)), is that not the case?
I think I asked a similar question a while ago https://www.reddit.com/r/haskell/comments/6y8nhh/is_it_possible_to_make_a_vector_or_array_during/
Is there a difference between using a partially applied function and a function that returns a anonymous function? E.g plus1 a = (+) 1 And plus1’ = \a -&gt; 1 + a 
It might, but it does not appear to have percolated to awesome error messages yet, unfortunately. Hopefully soon!
Awesome, thank you!
Well, why do you care one way or another? Just a default to how things already are?
I'm opposed to changing language at all, in general. Individually they don't do anything harmful but after 200 years of tiny shifts all of a sudden you're speaking a different language, which creates problems. 
[My blog post and slides](https://gilmi.me/blog/post/2018/07/24/pfgames) describes how I thought the same thing about games in Haskell and what I discovered after writing a game. Maybe it'll give someone the push they need.
Have a look [here](https://www.algorithm-archive.org/contents/huffman_encoding/huffman_encoding.html) for some inspiration :)
Perhaps this work fixes this issue: https://making.pusher.com/latency-working-set-ghc-gc-pick-two/ Would be interesting to run the same code with this new GC.
&gt;I'd expect most games state to be pretty small Depends, are you planning on using some FFI magic to handle all asset caching? If we are talking about building a full-blown game engine in Haskell, then you need to manage everything not just minimal game state. Keep in mind we have some modern 3D games which can easily go over 8GB of RAM usage, and most of them are using C++, so garbage collection means you will easily have 10-20% overhead at the minimum. &amp;#x200B;
I would define (1) a function that parses just a single character and returns the remaining bit string along with that character and then (2) a function that invokes the former function repeatedly to parse a list of characters.
What is the point of this post? You just pasted a bunch of code and debugging bits, and some ghci output. Is there a problem that you're facing? Can you elaborate?
How do you want to help the community?
When I wrote the post I tried to see if I could look up sone, but I couldn't. A not-too-terrible way of thinking of them is that the coexponential A-B is a pair of an A and a continuation that accepts a B.
The desktop test is very important to understand how functions work in HASKELL. 
mods, i believe that this post is spam.
??? sorry... I'm new in Reddit. I just wanted to show the importance of the desktop test
Regarding the study, I wish it were possible to write some free text about default extensions. For example, I'd only want `OverloadedStrings` enabled by default if it's changed so that ambiguous string literal types default to `String`. Otherwise too much code would break if this were enabled by default.
Hi, I'd suggest you apply for the position, even if remote. We'll definitely consider remote options given the application is compelling. Our first preference will still be non-100% remote.
Even a 'full-blown' engine needs to have most of it's assets in renderer buffers (e.g. vertex buffers, textures etc.), not even the most naive developer would start completely from scratch these days. Most of that 8GB will be assets, which and the bulk of anything else big you'd want to be using unboxed/storable vector, neither of which would be a load on GC. &amp;#x200B;
&gt; As mentioned this uses the minimum number of bits possible for encoding. https://en.wikipedia.org/wiki/Arithmetic_coding#Huffman_coding
&gt; I'm opposed to changing language at all, Welcome to a world of disappointment and misery then.
Just to provide the counterargument (which I do not agree with): If you specify the versions of your direct dependencies, you are justified in expecting identical behavior other than buggy behavior. The idea of versioning with cabal-install is that a given version of a package must always mean the same intended behavior, regardless of changes to transitive dependencies. This is the reason revisions exist; to fix transitive changes that break this invariant. Again, I don't particularly agree with the philosophy, but it is *somewhat* sound