I disagree. It will make you understand the trade-offs that Elm has made.
Not sure how to address the compile time issues. I'm surprised that even in interpreted mode Haskell has that long of a package install time / reload cycle. The unit testable GUI things doesn't seem like a Haskell issue so much as a library issue (which basically again comes back to my popularity argument, more people = more libraries + more testing for them). I'm actually very surprised you are focusing so much on `Dynamic`. I have basically never seen it be useful in practice, and `PolyDynamic` seems even more niche. Can you give a real world example where that is genuinely the best approach to take?
That's just an effect, that probably has a name, wherein the closer two groups seem to be in their interests, the more strong their rivalry. The small differences become blown up. You can see this satirized in Life of Brian with the People's Front of Judea: https://m.youtube.com/watch?v=WboggjN_G-4 Look at e.g. Ruby vs Python, from where I'm standing they are basically the same language, but they have some bitter rivalries. A dyed in the wool programmer of one of those would do a spit-take to hear a claim like that. To some extent OCaml has Haskell bitterness, and Clojure vs Haskell is common. To outsiders they're languages in the same group. I don't think people hate JavaScript, just recognize it as a necessary evil.
Where are you getting that? [The link in this comment indicates](http://reddit.com/r/haskell/comments/7i4ukq/stacks_nightly_breakage/dqwzzh1) it's just about upper bounds, not lower bounds
&gt; PolyDynamic real world example Here: https://github.com/Wizek/hs-di I've written this library to aid with making even effectful code unit-testable. `Map Text PolyDynamic` would be the most straightforward approach to model the collection of dependencies/functions that are injectable. Or some other kind of heterogeneous map. But those don't come easy with Haskell. So I was lucky that I found an alternative architecture that more or less worked with the help of TemplateHaskell. But if PolyDynamic ever becomes available I'll consider refactoring to make use of it.
&gt; automatically relaxing lower bounds [from `^&gt;=` constraints] will be also feasible, since the machinery required for that is essentially the same as for relaxing upper bounds https://github.com/commercialhaskell/stack/issues/3464#issuecomment-333685140
&gt; automatically relaxing lower bounds [from `^&gt;=` constraints] will be also feasible, since the machinery required for that is essentially the same as for relaxing upper bounds https://github.com/commercialhaskell/stack/issues/3464#issuecomment-333685140
Interesting. That probably should have been in the release notes.
`possible ((x, _, s), (x1, s1)) = True` When would this return False?
ya i was playing around with the code possible ((x, "", s), (x1, s1)) = False is supposed to be true, when taking an empty string it should return True
I wouldn't call using cabal's brand new features in `integer-gmp.cabal` and `ghc.cabal` "malicious", however, it was unnecessary and caused avoidable breakage for stack users. Rather, I'd call it "inconsiderate", since they quite literally didn't seem to consider how this choice would impact a stack-based workflow. On the topic of what makes for healthy social behavior in our community, I would appreciate if cabal/hackage people would be a touch more considerate of stack users and devs.
Why should stack users and devs have preferential treatment? Can someone write code, on which stack depends, without having to care about stack, or is that inconsiderate and unhealthy? Is it unhealthy in all the other cases as well? Open-source used to be good.
You could do this with function such as: getFsts :: [(a,b)] -&gt; [a] getFsts ((x,y):xs) = x : getFsts xs However this is just a rewriting of the function map or fmap. Map (which is fmap specific to lists) has the following type signature: map :: (a -&gt; b) -&gt; [a] -&gt; [b] If we specialize the types to this specific problem by assuming we are going to pass fst as the first argument the type signature would be: map :: ((a,b) -&gt; a) -&gt; [(a,b)] -&gt; [a] a -&gt; b [a] -&gt; [b] Thus to get the output you desire you would need to do: map fst [(65, 'A'), (66, 'B'), (67, 'C'), (68, 'D')] = [65, 66, 67, 68] Hopefully that makes sense if you have any questions feel free to ask.
&gt; Can someone write code, on which stack depends, without having to care about stack, or is that inconsiderate and unhealthy? "without having to care about X" is, in the most literal sense of the word, inconsiderate of X. I'm not saying that contributors upstream of stack need to solve all of stack's problems. But I am saying that stack is a pretty big part of the Haskell community at this point, and being neglectful of it is kind of a dick move. &gt; Open-source used to be good. Collaboration is what makes open-source so good. Collaborating with projects that are downstream of you is a considerate thing to do.
OK, so I am inconsiderate of many things right now. Is that "unhealthy"? &gt; Collaboration is what makes open-source so good. Collaborating with projects that are downstream of you is a considerate thing to do. Many things make open-source good. I remember, quite well, when it was good. Is non-collaboration with projects downstream an unhealthy thing to do? Is there some obligation to be considerate? Should all people always consider all things? If not, what are the conditions? Why is stack demanding preferential treatment? Besides the political reasons that is. What makes stack so special that preferential treatment solicits this moral imperative? Have you ever had an open-source project downstream, which you did not consider? If not, is such a thing possible? If so, what would be the conditions?
Yep
The breakage was not expected. As the linked post, which we are discussing, explains, the upload of integer-gmp triggered some unexpected and unforseen bugs. Without knowing ahead of time those bugs, which nobody did, then there was no reason to expect, even with very careful consideration, that there would be any negative impact on a stack workflow.
The intended meaning is different, as described. Caret-bounds are intended to help distinguish between known incompatibilities ("hard" bounds) and those bounds that are _potentially_ incompatible, because, according to the PVP, they _may_ introduce breaking changes for any downstream packages.
&gt; Why should stack users and devs have preferential treatment? First, Stack users and developers aren't asking for preferential treatment. They are merely asking for GHC/Cabal developers to avoid breaking Stack for no good reason. Second, even if you don't personally use Stack, you should be aware of it and try not to break it. A majority ([almost 75%](http://taylor.fausak.me/2017/11/15/2017-state-of-haskell-survey-results/#question-23)) of the community prefers it. 
it would help if you specified what you meant by the unix philosophy -- people mean different things by it. one version is "programs should do one thing, and do it well." I would hope that Haskell authors take the same attitude towards their functions :-)
Well, yes, they is a demand for preferential treatment. It is unreasonable to expect downstream projects to not break. GHC does it, for example, when llvm or zlib changes require GHC to resolve the issue. I do it on certain projects. In fact, *all of us* do it, except for stack. Hence, there is a demand for preferential treatment. I am aware of stack. I don't see why I "should try not to break it." This is simply a claim with no support. Why should I not try to break it? What special case means I should care? Does "75% prefer it" change all of this? Ignoring the fact that this survey is bogus, does 75% somehow change all of this need for "consideration" and altering what is and is not healthy? How did all this even come to exist? What is the reasoning? Open-source used to be good. * The survey is bogus because some number of people refused to respond to it because it was so spammy (multiple emails) and political, being unable to unsubscribe without "not wanting to help the haskell community." https://i.imgur.com/bvS1MW2.png FP Complete does not speak for the haskell community. Quite the contrary, it has succeeded somewhat in destroying it.
I'll try to cut to the heart of what you're asking, rather than attempt to answer every single question. According to a recent survey, "stack is used by over twice as many people as cabal". http://taylor.fausak.me/2017/11/15/2017-state-of-haskell-survey-results/#question-22 And "five times the number of people use it [stack] as their preferred build tool compared to Cabal". http://taylor.fausak.me/2017/11/15/2017-state-of-haskell-survey-results/#question-23 Obviously the survey is subject to certain biases, so I wouldn't necessarily take that factoid as 100% accurate. But the fact remains that Stack plays a huge part in the Haskell community. Stack is not "demanding" anything. Just asking. And the things asked for are often *trivial*.
You're getting my survey confused with FP Complete's. My survey was published through Haskell Weekly: https://haskellweekly.news/surveys/2017.html
Sorry, I didn't look at the link properly.
OK, so yes, stack is definitely demanding all sorts of considerations, and you even declared it "unhealthy" otherwise. I thought it was rather obvious that demands are taking place. So, the conditions under which it is reasonable to make demands, is when some internet survey passes some not-yet-stated threshold. If that is the case, perhaps we should inform the entire world of open-source. Or, perhaps, open-source used to be good.
I think it applies to a lot of levels. Functions, modules, packages, you could even argue expressions.
...partially? &gt; Write ~programs~ functions that do one thing and do it well. Not really. We're quite proud of the fact that laziness allows a single function to perform both sorting in O(n log n) and min in O(n) for example. &gt; Write ~programs~ functions to work together. Absolutely! One of my favourite part of Haskell is the [combinator libraries](https://www.youtube.com/watch?v=85NwzB156Rg) pattern, in which you write larger solutions by combining smaller functions using a few simple combinators. &gt; Write programs to handle text streams, because that is a universal interface. Absolutely not! We hate stringly-typed values, we much prefer to use very precise types which make illegal values unrepresentable.
&gt; I am aware of stack. I don't see why I "should try not to break it." This is simply a claim with no support. Why should I not try to break it? What special case means I should care? I realize you may have a hard time understanding this one, but perhaps the reason is to "not be an asshole". &gt; FP Complete does not speak for the haskell community. Quite the contrary, it has succeeded somewhat in destroying it. Uh-huh, so all of the companies saying they would never use Haskell if we still used the old build tools, that stack made Haskell a viable development platform? Yeah, get your head out of the sand Ask cabal developers - did stack cause them to speed up development and try new things? It seems like it did. On the cabal side of things we'd still be stuck with sandboxes. So yeah, I don't get why you are so salty. Please find something to neutralize your acidity
Yeah, that's one of the funnier things about putting this into a core package. They aren't even sure what it means yet, though from the cabal docs it does sound certain. From the side conversation, it is uncertain, what to believe?
I know the upload of integer-gmp itself was, as far as anyone knew, completely benign. I take a little issue, though, that cabal 2 format was used for ghc.cabal and integer-gmp.cabal; this seems needlessly backwards incompatible, though of course hindsight on the issue is 20/20.
Perhaps the cabal documentation [here](https://www.haskell.org/cabal/release/cabal-2.0.1.1/doc/users-guide/developing-packages.html#build-information) should be updated to clarify this point? I don't understand why this would be introduced instead of soft bound operators that can be used. Why mix two orthogonal concerns - following PVP conventions (determining the wildcard position) - along with soft bounds? I guess maybe it makes sense not to have a crazy proliferation of operators, but this all seems ill considered.
At some point new features are going to be adopted, and since no breakage was expected, then I can find no fault with this choice. If you're going to be using a new ghc anyway, that's going to come with a new cabal-the-library, which is equipped with the ability to recognize that syntax anyway, so of all the places to adopt the syntax, other ghc libs seem pretty reasonable places to start.
Yep, baking in "ignoring version bounds" into the semantics of the version bounds really helps a lot!
I think it is the other way around. Haskell's philosophy is rooted in math and category theory, while unix philosophy is some fuzzy "feel good" unsubstantiated bullshit that somewhat resembles the principles of pure functional programming. 
Maybe [Chart](https://github.com/timbod7/haskell-chart/wiki)
How do applicatives elegantly compose? I thought `Compose` was just building an applicative in another, while *not* allowing effects to cross?
But it is. There's no unix philosophy. Some well known unix tools and libraries are big and all encompassing, others are small and orthogonal. Whiny neckbeards are cursing systemd for breaking unix philosophy (whatever the fuck that means). Meanwhile systemd is the best thing from engineering point of view that happened to linux in the last decade. I can go on and on. "Unix philosophy" is a flame-baiting and trolling concept people invoke when they want to prop their point of view by something that sounds authoritative. It's like religion. Everyone has its own version. 
&gt;"Unix philosophy" is a flame-baiting and trolling concept people invoke when they want to prop their point of view by something that sounds authoritative. It's like religion. Everyone has its own version. I don't think that Ken Thompson was flame-baiting and trolling when he introduced his conception of Unix philosophy and having bad followers isn't something that by itself illegitimizes something. Everyone of us knows that internet discussions often are dysfunctional and yield nothing good, so what?
Not necessarily different bugs, although I am not sure what I would use in place of Coq’s `Ensemble` type to describe the “abstract meaning” of one of my range sets.
I agree with you. Every language makes trade offs, each has strong attributes and weaknesses. It’s finding a balance for the job you need to do. 
It's worth acknowledging that the breakage from integer-gmp.cabal was fully predictable, as [the Stack 1.5.1 release announcement](https://mail.haskell.org/pipermail/haskell-cafe/2017-August/127686.html) discussed the same situation with ghc.cabal, and [the topic was discussed on this subreddit](https://www.reddit.com/r/haskell/comments/6rzw20/psa_upgrade_to_stack_151_if_suffering_from_cabal/). It's entirely possible that this upload unintentionally broke Stack, but the breakage was predictable. And following normal release procedures (like uploading only files included in the Git repo's release tag) would have prevented it.
Thanks for the link and quote, I think that's the info that was missing so far. It would be nice if somebody could update the user guide, so that it also contains this detail description, as sooner than later people won't look in the changelog in order to find what the purpose of `^&gt;=` is.
Only to the extent that both care about modularity—but so does everyone and their dog. Beyond building larger systems out of smaller components, the Unix philosophy and the Haskell philosophy (however hard it is to pin down) are entirely opposed. I found Peter Salus's summary of the Unix philosophy particularly cogent: &gt; 1. Write programs that do one thing and do it well. &gt; 2. Write programs to work together. &gt; 3. Write programs to handle text streams, because that is a universal interface. The two core conceits are that modularity is achieved by **programs** communicating via **text streams**. This is a *very specific* way to achieve modularity. I'd posit it is not a good way, and it's certainly not a Haskelly way. The base units of modularity are complex and heavyweight: programs are complete black boxes, have their own memory space, carry extra OS-level metadata and have *multiple* input/output channels. Haskell, on the other hand, achieves modularity with significantly simpler in-language constructs (most notably functions). Programs also communicate over text streams. This is completely unstructured: it's not even dynamic typing, it's "everything-is-a-stream-of-bytes typing". In practice, this means that Unix programs never do one thing, they do three: they parse some *ad hoc* text-based format, do some logic and then serialize to some other *ad hoc* text-based format. Haskell programs, on the other hand, communicate with richly structured typed data—the difference is night and day. More broadly, the Unix philosophy also implicitly carries a certain devotion to "worse is better" which is, thankfully, anathema to the Haskell community. Haskellers, as a group, are motivated to find the right thing to do rather than going with the first thing that sticks. If you're curious, I wrote a pretty long post about this on Quora. The context is more about Emacs than Haskell, but it expands exactly the same view of the Unix philosophy as I'm writing here: https://www.quora.com/Does-Emacs-violate-the-UNIX-philosophy-of-doing-one-thing-very-well/answer/Tikhon-Jelvis
It sounds like you're looking for the fixed point of a function with a little bit of Monad around it. [Here's](http://hackage.haskell.org/package/base-4.10.1.0/docs/Data-Function.html#v:fix) the non-Monad version of fix, which sounds like what you're talking about. Is this what you were imagining? Also see [wikipedia](https://en.wikipedia.org/wiki/Fixed-point_combinator) for fix point combinator.
The linked stack release announcement says: * Stack eagerly tries to parse all cabal files related to a snapshot. Starting with Stackage Nightly 2017-07-31, snapshots are using GHC 8.2.1, and the `ghc.cabal` file implicitly referenced uses the (not yet supported) Cabal 2.0 file format. Future releases of Stack will both be less eager about cabal file parsing and support Cabal 2.0. This patch simply bypasses the error for invalid parsing. So that would lead me to assume that the bug was fixed for 1.5.1, and indeed my understanding is people thought it _was_ fixed. As your blog post explains there was an interaction with an exception for boot libs that was I guess not anticipated.
&gt; We hate stringly-typed values, but how else are two programs supposed to communicate across network boundaries ?
Are you thinking of things like [this](https://hackage.haskell.org/package/monad-loops-0.4.3/docs/Control-Monad-Loops.html)? There's a nice loops package for this, and there's a few of these in base I think?
Text is a lie, but bits are what the universe is written in ;) Pass typed bytestrings around and encode/decode appropriately and you'll save yourself a lot of headache. Otherwise you're just passing bytestrings around but pretending they're actually something else and projecting semantic meaning onto something that doesn't necessarily have it.
I have a tiny [lib](http://lpaste.net/360592) for this. Can publish it on Hackage if you're interested.
Are you aware that stack users often install ghc through stack? Stack does not plug the ghc it installs (nor the libs) into itself, so the build tool one uses with a given ghc is not guaranteed to be pegged to the cabal-the-lib that comes with it. The train of thought you have presented is completely reasonable; I'm just trying to point out that it lacks the perspective of a stack user's workflow.
One of the first things mentioned in your library's readme is compile time verification, and `PolyDynamic` is inherently not going to compile time verify anything at all. In general it doesn't seem like dependency injection should have anything to do with dynamic types, in fact I would say quite the opposite. I haven't gone through your library thoroughly enough to say for sure but you wanting dynamic types for it worries me. http://mikehadlow.blogspot.com/2011/05/dependency-injection-haskell-style.html This doesn't seem to lead toward dynamic types at all. Do you perhaps have a fairly minimal and self contained code example where a specific line of code doesn't work due to the lack of PolyDynamic and there isn't a good alternative solution.
I have a [tiny](http://lpaste.net/360592) lib for this. Just added `lastOk` and `lastOkM` there. Can publish it on Hackage it you're interested.
Been doing UNIX since the early 1980s, and Haskell for about a decade. Nope.
Try `diagrams` or `Gloss`.
&gt; being neglectful of it is kind of a dick move The same can be said of any bug ever written. "They were just neglectful of the bug and that was a dick move!" The bottom line is that people make mistakes and actions have unforeseen consequences. When that happens, you fix it, get over it, and move on. Can we just apply the principle of charity here and dispense with the inflammatory accusations?
Sorry, I'm not trying to be inflammatory. I'm less interested in accusing and casting blame about the past, and more interested in discussing what we can do in the future so that stack-based workflows are kept in consideration, and ideally well tested. (One of the things I like about Haskell, after all, is the idea that good tooling can prevent more bugs before they ever occur.) Swaggler's argument seems to be that stack-based workflows are not worthy of upstream consideration, now or in the future. It is this, and not any bug in particular, that I consider to be at odds with what I envision for a healthy Haskell community. I'm at a loss when I attempt to apply the principle of charity to this argument. Are you able to interpret swaggler's argument more charitably?
I get the trade-offs, and they've done good work, but no type classes sucks. 
I agree with you that the do notation doesn't help. I think the "bound argument" bit is fine. It relates to types constructors. `a` is the argument of `m`, `(m a)` is also an argument of `m`. `join` does exactly what the documentation says, it takes `a` and use it to replace `(m a)` discarding the inner `m`. Maybe there is something I don't understand, but the language manipulate data structure. Whether it represents actions, non-determinisme, computation, exception, potential error, it is still a data structure. Except for IO, I don't know how it is implemented, but it's supposed to be magic. So aside from IO, everything can be reimplemented, and it is done via data structure.
I am not sure if they work for your use case but there are many good charting and plotting libraries in Haskell, some of them are: Charting: * https://hackage.haskell.org/package/Chart * https://hackage.haskell.org/package/chart-unit Plotting: * http://hackage.haskell.org/package/dynamic-graph * http://hackage.haskell.org/package/dynamic-plot * https://hackage.haskell.org/package/plot * https://hackage.haskell.org/package/plots Diagrams based
(sorry for the late revive) The problem is that you cannot display multiple pieces of information at the same time! For example, if you want to have type information on hover, the only place you can put it is in the echo box really. Now, if you also have a linter (Ale, Neomake, etc) it will put the warning/error of the line at the same place. These will override each other. This goes for several other pieces of valuable information as well.
Coming back to this, since I'm running into specifics these days. - No soft wrap at column that is [shorter than window width](https://stackoverflow.com/questions/989093/soft-wrap-at-80-characters-in-vim-in-window-of-arbitrary-width) - No tooltip support (e.g. balloonexpr is only GUI and has been removed in Neovim). Being worked on TUI support [here](https://github.com/neovim/neovim/pull/6619) for Neovim. - No syntax highlight in echo box, short of making your own parser and stitching together with `echohl`. - No support for any kinda of overlay feature that isn't the completion menu, which you honestly can't in any way straightforwardly hack into the info you want. Or the major hack [here](https://github.com/neovim/neovim/issues/5794) using Conceals. - Obviously no support for images/web pages/pdfs in TUI, but can't really complain about that one (although emacs does have it. Lots of little limitations as a library/vimscripter that I run into, when you want to make a nice IDE-like experience :(
&gt; We're quite proud of the fact that laziness allows a single function to perform both sorting in O(n log n) and min in O(n) for example. But aren't `sort` and `head . sort` (or `\n -&gt; take n . sort` for that matter) technically two different functions?
&gt; A majority (almost 75%) of the community prefers it (Stack). Interesting. 850 Stack users vs 160 Cabal users paints a very clear picture. We should question why Hackage is still optimized to the needs of such a small minority at the expense of the evidently large majority of the Haskell community.
/u/vagif is right. Doug McIllroy's statement of the principles of UNIX from the Bell Systems Technical Journal were as follows: &gt; 1. Make each program do one thing well. To do a new job, build afresh rather than complicate old programs by adding new "features". 2. Expect the output of every program to become the input to another, as yet unknown, program. Don't clutter output with extraneous information. Avoid stringently columnar or binary input formats. Don't insist on interactive input. 3. Design and build software, even operating systems, to be tried early, ideally within weeks. Don't hesitate to throw away the clumsy parts and rebuild them. 4. Use tools in preference to unskilled help to lighten a programming task, even if you have to detour to build the tools and expect to throw some of them out after you've finished using them. Point 2 draws an incorrect conclusion from a correct premise; structured data is a good thing that makes it easier to interoperate. Because of this mistake, Unix has never, ever adhered to point 1 -- look at the zoo of totally ad-hoc options basically every Unix command supports. Point 3 was abandoned with the invention of POSIX -- and it was abandoned because it was an invitation to vendor lock-in. Point 4 is correct, but is basically never followed in practice. 
I am too looking at `diagrams` and `gloss`. The both look great but seem like they don't designed for absolute (x,y) position plotting. Maybe I miss something. I want to draw a map with (x,y) position and value (or picture) at that position. Currently I just print it like this .....X........ ....M........ |.....XXX..
The style of the questions is great. What's not so great is that the first three questions require implementing a partial function.
If you are willing to get your hands dirty, basic "old-style" OpenGL (with glBegin/glEnd) is actually not too hard, and you have complete freedom on what you render.
I was recommended to use `plotlyhs` recently (bindings to `Plot.ly`) * https://hackage.haskell.org/package/plotlyhs And I find it really good! Though, not sure about dynamic updates, you need to research this...
I wrote some notes about the limitations of the unix composability. And how haskell would make a better shell for the cloud. [A cloud shell](https://gitter.im/Transient-Transient-Universe-HPlay/Ideas?at=597a2a65f5b3458e308a370e). Some shortcomings of the unix philosophy: a | b | c - No feedback: no upstream communication. No out-of-flow communications in general - No reactivity. b can not generate events. a is the only initiator. - No universal interface. only pipes. What happens when `a` is a server and `b` is a REST client? - No types - No remote communications (solved in Plan 9) - No universal identification. Elements only make sense within a node. Docker solves it - No installation: If elements are universally identified, they should be installed locally before invoked. So for a cloud shell, installation should be part of the execution in the same way than initialisation is part of execution of a program. 
Sorry I'm losing you. Where is the partial function for say the first example (the one from this problem). There is one argument a list with the base case of (x:xs). I should say stuff above using maybe also wouldn't have worked because at that point they generally don't know about maybe yet. What you are looking for when teaching this is something like (at the ghci prompt) lastelem ([x]) = x; lastelem (x:xs) = lastelem xs; I generally don't want them to handle the empty list case at all. Just get the idea of recursion step, termination case. Later of course after they have worked through examples this becomes a wonderful example of why you might want maybe. 
`myLast` is a partial function! Implementing `myLast :: a -&gt; Maybe a` seems perfectly reasonable to me. I don't see any reason not to know about `Maybe` at the same time as knowing about list pattern matching.
I don't see how you can get a new version of GHC without getting a new version of all boot packages. If you don't, you're working yourself into a massive ABI issue. If stack does do this, then it's doing this on at it's own peril and is not something GHC supports.
This was a stack bug. You can try to spin in whatever way you want. but this bug was caused by stack not checking the cabal-version field before it tried to consume a package. The fact that this new operator was used is besides the point. The package clearly states that it requires a Cabal version of at least 2.0 to read it. If stack didn't support it yet, it shouldn't have read it. 
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/arangodb] [Is there a library for performing queries to ArangoDB in Haskell?](https://www.reddit.com/r/arangodb/comments/7ieptc/is_there_a_library_for_performing_queries_to/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
Remember this is literally done as their first Haskell program they write themselves. It is question (1). At this point they are learning what f:: a -&gt; b means at all. An array and an element are concepts they have thought about before so myLast :: [Integer] -&gt; Integer myLast [x] = x are familiar ground. The goal is to get them familiar with the notation. myLast (x:xs) = myLast xs is an unfamiliar concept. While they may have heard of recursion and possibly done it before they aren't good at it. They aren't used to doing it. Normally they would do something like: myLast xs = xs !! ((length xs) -1 ) At that point very deliberately they don't have !! for precisely that reason. Maybe is also an unfamiliar concept. Maybe is a functor over a datatype. While obvious [] is also a functor over a datatype you don't want to introduce that concept that early. The goal is they swallow the food not that they choke. You don't want to combine two unfamiliar concepts at once if you can avoid it. It can be very easy to forget how think Haskell is with unfamiliar concepts since so many of them seem natural to us at this point. 
But *this very topic* on which we are commenting is about a user who couldn't solve problem number 1 because he/she didn't know what to do with the `[]` branch!
FWIW I'd say they are rather different. Haskell clearly belongs to the MIT/Stanford school not New Jersey school of design. The most obvious difference is in consistency. Unix argues the design must not be overly inconsistent. Haskell abhors any kinds of inconsistency, Functors which apply to all Haskell data types are a basic building block. And certainly Haskell has never sacrificed for implementation simplicity. The most obvious example was spending years getting I/O to work right rather than compromise with impurity. Just for reference here is a good article. It is about LISP system vs. Unix and is quite dated but the ideas hold up well 40 years later. https://www.jwz.org/doc/worse-is-better.html 
I wrote my reply about worse is better before reading your's carefully. I expanded a bit more but definitely we are thinking along the same lines. 
What is a dick move is if the bug is pointed out, but the person in control absolutely refuses to do anything about it, even though it is a 5 minute fix, and refuses to merge PRs. Because they would prefer to have a cabal file that breaks other's builds, for no reason - https://github.com/hvr/cassava/pull/155#issuecomment-337761696 Same with this integer-gmp situation - https://ghc.haskell.org/trac/ghc/ticket/14558#comment:17 . There is an extremely simple fix - revise the package on hackage. This way, the `integer-gmp-1.0.1.0` that comes with the ghc-8.2.1 tarball would match the cabal file served by `integer-gmp-1.9.1.0` on hackage. I have hope that in this case we can actually have some sanity, and that step will be taken. However, yeah, it will be a major dick move if that doesn't happen. Even more than the cassava thing, because that only breaks builds for cassava users.
Whatever you wrote after "In other words, ...." makes sense. shouldn't that be included in the documentation?
I really like your idea of a Haskell like cloud shell. Given Stack's ability to easily utilize Docker if this became a little more specific to a particular cloud infrastructure this could work. But I think a discussion ends up hijacking a thread. BTW I do know someone who is thinking about writing a monitoring system for big data computation (including Mesos, Kubernetes) If you thank about handling error on long running processes that might be useful. Anyway FWIW I hope if you do this you release it batteries included ready to run out of the box as part of a deployment rather than as a complex to configure stand alone. 
It would be great! I don't have the rights to change that documentation but I'm sure someone here does :) Perhaps a GitHub pull request on the library... I'll look at it after work
&gt; Not really. We're quite proud of the fact that laziness allows a single function to perform both sorting in O(n log n) and min in O(n) for example. Do we have many examples of that kind of thing? That seems to be the standard example because it's pretty much the only one, as far as I can tell. 
Certainly a ton of boilerplate you need to write in Elm would be smashed by some form of ad-hoc polymorphism.
Thank your, I never heard of the `MIT/Stanford style of design`, it seems interesting, I have read the article. however, is there any in depth article about this style, I couldn't find any.
The best serious treatment is by the same guy: https://www.dreamsongs.com/Files/PatternsOfSoftware.pdf For something shorter most of the days of LISP machines are pre internet but there are some nice places to find references to their design. For example this old add from Symbolics: http://www.symbolics-dks.com/Genera-why-1.htm . Do remember when you are reading this it is the early 1980s and consider what most development environments were like. You can find a collection of essays on this topic of worse is better at https://www.dreamsongs.com/WorseIsBetter.html There is also a more humorous attack on the problems with New Jersey design which is another classic (though the comparison is to VMS): https://simson.net/ref/ugh.pdf 
Unix do compose certain kind of programs in some way. Haskell current practices in general does not allow seamless composition within a single program. No monadic system used: monad transformers or free monads composes well. This makes Haskell artificially complex and unpractical for large projects. In this sense Haskell betrays his own philosophy of functional composition.
Good bot
Well remember when I'm teaching this problem it is instructor led. I don't know what the problem was from his short question but it sounds like he didn't know the syntax on how to pattern match an empty list. Most likely he didn't really understand [1,2,3,4] == 1:2:3:4:[] i.e. what (x:xs) means. That's what I would be jumping on because he is clearly ready to understand that idea. He is also ahead of the class. He's considering a case they aren't supposed to be worrying about yet. The goal would be to get him to write something like m [] = 0; m [x] = x; m (x:xs) = m xs and then notice that 0 has a problem that it is Integer (Num) specific while the rest of the expression isn't. He sees the problem at which point you could introduce undefined or Maybe. 
&gt; You could do this with Maybe if you applied a function f :: a -&gt; Maybe a to some x :: a until the result x_n :: Maybe a is a Just, but f x_n is Nothing. No, you can't, because you never get a new `a` to apply `a -&gt; Maybe a` to when you get a `Nothing`. You could write a function that applies it until you get a `Nothing` and return the last `Just a`, though. I almost always reach for `fix` when I need some "while loop" behavior. You combination of `flip fix` is a function that takes some initial state (optionally) and provides a loop callback that you can use to iterate. Eg: flip fix $ \loop -&gt; do now &lt;- getCurrentTime if now &lt;= someTime then threadDelay 100 &gt;&gt; loop else return "hello world" Will loop until the current time is less than some time, sleeping 100us every time. flip fix (Just 0) $ \loop mpage -&gt; do for_ mpage $ \page -&gt; do result &lt;- runDB $ selectPaginated page print result if null result then loop Nothing else loop (Just (page + 1)) This will loop over a count of pages, selecting a page from the database, print it out, and it will stop whenever the database doesn't return any rows for that page number.
Huh, I was just writing something about this yesterday. Call (G ∘ F) = Mon. Mon X is basically the set of "formal monoid expressions on members of X", and an algebra for it, Mon X -&gt; X, is an evaluation function that assigns to each formal expression the value it should evaluate to. Put it this way. Suppose you have a formal expression like [xyz] in Mon S (where [brackets] denote that something is a formal expression). This is basically just an AST, but if you had an actual monoid on the set S, you could substitute its actual multiplication for the formal multiplication operation in the expression and get xyz, which is a member of S. So given a monoid on S, you have an evaluation function Mon S -&gt; S. The idea of an algebra is that you have a partial converse: if you have a function that assigns to every formal expression a value (in a consistent way, this is what the laws for an algebra say) then you also have a monoid. What the algebra laws say in this case are 1. Values evaluate to themselves: [x] -&gt; x 2. All the different ways of evaluating an expression give the same result. For example, if [xy] -&gt; z, then [[xy][xy]], [xyxy], and [zz] all evaluate to the same thing. The monad operations are 1. pure takes a value x to the trivial expression [x] 2. join simplifies an expression with nested sub-expressions: [[xy][z]] -&gt; [xyz]
 doUntill :: Monad m =&gt; (a -&gt; Bool) -&gt; (a -&gt; m a) -&gt; a -&gt; m a doUntill test f value = if test value then return value else f value &gt;&gt;= doUntill test f Like that? This is just much, much less common than defining the loop structure as recursion in the do block. I think this is mostly because : A) Depending on `m` in this scenario, you may or may not inadvertently expose yourself to some gnarly space leaks and defining your loop on a per monad basis helps prevent this. B) Case matching is usually better as an exit condition due to both the utility of sum types, and also, the fact that it is VASTLY easier to ensure totality and make sure the loop does actually terminate at some point when you haven't reduced the concept of loop termination down to a single data point. Honestly, I would feel really uneasy about using the above, `forever`, or anything like them in more than one or two places throughout a program, and writing the recursive loop in a do block gives me more flexibility with termination conditions and strictness anyway, so it hasn't come up often enough outside of toy contexts for me to consider bothering with it. 
First, dragging in an unrelated ticket from an unrelated library makes it seem like you have some sort of vendetta going on. Second: what that ticket shows is now for the _second_ time in a row you've made an unfounded accusation of malicious behavior (in response to a bug in stack), only to walk it back. Maybe, next time, you can stop jumping to these sorts of conclusions? 
What is the the most simple IDE/editor configuration for Haskell dev? I think I have tried - Emacs + intero - Idea + Haskforce - VS Code with Haskelly, Haskero and all the other plugins... - Sublime Haskell The usual experience is trying to follow 25 step installation guide that is 2 years old and I end up having something wrong and the environment is not working. And if I manage to install required stuff without errors, I don't know how to use the installed Haskell plugins. The only way I have any success actually writing Haskell code is Haskell for Mac. But my mac is going away so I can't use Haskell for Mac. I have Windows machine and Linux machine but prefer using Windows. I find that most of the Haskell material assume you to use Linux/Mac. This is the same problem as with Clojure: to learn Clojure people should learn Emacs. This is way to difficult for people trying to learn the language.
[removed]
I'm pretty sure HVR goes to great lengths to listen to the concerns of Haskell users. 
lol cabal new-build is like already mostly done does things stack doesn't. 
- What does "mostly done" mean? - Will it be done in 2017? If not, what is the ETA? - What are the things it "does stack doesn't"?
In a properly Haskell world, they would communicate in well-defined protocols derived directly from types. Either something like Cereal or, if you have full control of the endpoints, something like Cloud Haskell. Things get hairier if you need to talk with programs that aren't in Haskell, but then that's their fault and doesn't have much to do with the Haskell philosophy.
Editor choice is often more about the programmer than the language. I use vim for most things so the following more or less works for me: ``` nix-env -i nvim # or install nvim with whatever your package manager is pip3 install neovim git clone https://github.com/TomMD/configuration.git cd configuration ; sh ./setup.sh ``` That gets me neovim and plugins suitable for Haskell development. On a fresh machine I'd have to install ghc, cabal, git, and perhaps python too but you get the idea.
look, I'm not a cabal developer, and I mostly use stack anyways. But cabal new-build already provides caching across projects (iirc, with a nix-style hash of the package name, version, package flags, and compiler version, transitively), which saves a lot of disk space. Stack seems to copy everything between projects, even with the same stack.yaml
In general I think of it as constructing a structured search space, and then only forcing computation as you explore it. So this is really the same thing as newton's algorithm type solvers or encoding branch-and-bound search in a sense. The only difference is that we don't normally see sorting as the maximal case of "give me the top n," although, really, we should.
You guys should format your date postings!
/u/jtomchak, If you like the Elm architecture, but not the Elm language (would rather use Haskell), I'd recommend checking out miso (https://github.com/dmjio/miso), it does require GHCJS.
Proud to work with Mikhail :)
I like that philosophy!
If X is any set, you should not expect to get an algebra map T X → X. In fact, having such a map, together with the usual monad algebra laws, is precisely the same thing as having a monoid structure on X. This can be expressed by saying that the category of monoids is *monadic* (over Set). In your example, if A is a monoid, then the map θ : T A → A will map the "word" [a, b, c, d] to the unique way of multiplying those four elements according to the monoid structure on A, in the given order. You know it's unique because of associativity. For arbitrary sets X, what you do get is a monoid structure on T X, since as you wrote T X is the *free* monoid on X, hence in particular a monoid. That means that you get an algebra map T (T X) →T X. Can you guess what that is?
We are all as a plain matter of fact under the ancient curse of "worse is better" which is why you're almost certainly writing your reply from a Unix system rather than some hypothetical Haskell-based OS.
Can you explain how #3 is invitation to vendor lock in? This is not obvious to me at all
&gt; Job posting last modified 2017-12-05T08:46:10Z, Star Date 1312.4, fifteen degrees starboard
At least that has some commas and spaces!
My reading was not that #3 was the invitation but that POSIX was.
No, S is the underlying set for a monoid, ie. its the set such that the multiplication runs from S^2 -&gt; S. If the monoid in question is the free monoid on some set A, then the set S is the set of words on A and the algebra runs from Mon S -&gt; S. Since the underlying set for the free monoid on A is basically Mon A, the free monoid structure for Mon A would be given by an algebra Mon (Mon A) -&gt; Mon A. Does that help?
You could generate json for your plotting data and invoke a javascript library like https://www.highcharts.com/demo on the command line with json input. https://www.highcharts.com/docs/export-module/render-charts-serverside
And it's not very useful, since `minimum` is `foldl1 min`.
Who are you and why are you using a visual mimic of the account I use on Hacker News?
As someone who recently started learning Haskell, thanks for this. It is very well explained, and well illustrated. 
Yeah this is what I am looking for I think. That `flip fix` trick is one I will remember, thanks.
[removed]
The "direct style" `IOSpec` given here does not correspond to a lawful instance of `Monad` (breaks `m &gt;&gt;= (\x -&gt; k x &gt;&gt;= h) = (m &gt;&gt;= k) &gt;&gt;= h`). It might be a source of mistakes for AST transformations, but if the only thing one is going to do with that type is to write interpreters, that doesn't seem so bad. To make `IOSpec` lawful, one way is to tweak it into the [operational monad (`Program`)](https://hackage.haskell.org/package/operational-0.2.3.5/docs/src/Control-Monad-Operational.html#ProgramT). Another way would be to literally define the quotient using a higher inductive type, which Haskell doesn't have; any other options? Nevertheless, that representation obviously makes `return` and `(&gt;&gt;=)` constant-time operations. In an interpreter, one might still need some tricks to optimize stack usage and such things. Overall do we lose anything by simply representing primitive operations with constructors? Or is that potentially as good as, or better than, e.g., freer (including the "reflection without remorse" optimization) [1][2] where conscious efforts were made towards improving asymptotic complexity? [1] http://okmij.org/ftp/Computation/free-monad.html [2] http://okmij.org/ftp/Haskell/extensible/more.pdf
Would the `Option` class be equivalent to class Functor f =&gt; Option2 f where drop :: f a -&gt; Maybe a ? Reading this library, it seems that if this change could be made it would simplify things a little.
Your `until` function probably requires a way to extract the value out of your container: until :: (m a -&gt; Maybe a) -&gt; (a -&gt; m a) -&gt; a -&gt; a until p f = go where go x = maybe x go (p (f x)) But this doesn't feel right generally. In particular, you kinda want your list version, with `p = listToMaybe`, to keep it's other values... I'll think on this. 
“action” is a perfectly cromulent term for this in the mathematical sense :-)
It is confusing because it is a type argument to a type constructor, which is not normally how the term is used. If it is bound — where is the binder? The forall is sort of implicit, and not everyone is used to thinking about type level binders.
[removed]
Were you able to get anything working based on that idea? I've been toying with something along those lines but I haven't been able to write a sufficiently lazy `append`.
Yes, true, this bullshit has inspired a bit of a vendetta in me and others. Unfounded accusation? No, this is just yet another example. Very simple actions could be taken to make everyone happy, but instead, HVR continues to take no action to resolve the problems. We have every reason to believe that he delights in being frustrating.
Got a quick question on Servant and Persistent, I've defined my user model something like this : User json firstName String middleNames String lastName String email String UniqueEmailId email password String phoneNumber String and that'll generate me a User. But suppose on a certain endpoint, I want to return a User encoded in json, but not all the fields (e.g hide the hashed password and phone number). I still want some endpoints to return everything, so I don't want to overwrite the existing ToJSON instance. My other solution was to create a new data type called CleanUser, which had the same data except for the sensitive fields, and write a ToJSON instance for that CleanUser. Is there a neater way of doing this?
There was a bug that prevented stack from sharing packages between snapshots for a few of the previous versions. However, now there is full package sharing. Package sharing of "extra-deps" and git packages is high on my list of desired features. So, by the time new-build is ready, stack will probably be able to do everything that it can. Except, it still won't do implicit constraint solving on every install. Instead, you'll have to ask for that explicitly.
What good do you think will come from this comment? No one who you're angry at is going to read this and change their mind, since you're being hostile toward them. As an important contributor, every comment you make should make this community a better one. But this just pisses people off and makes *everyone* look bad.
I don't know about Cabal, but the Nix-like style definitely supports this. Nix would be infinitely less useful without the ability to cache from git.
&gt; Stack is not "demanding" anything. When all the Stack contributors are constantly throwing fits and personal attacks at Cabal contributors, yea I'd say they're demanding. This not to say anything about whether they're right or not, but they're certainly extremely hostile about it.
Fair point, I've toned it down and shifted more towards "how can we fix this"
So do you mean nix+cabal? Sure, that should support it. It is really confusing that new-build stuff makes claims of being "nix-like". Sure, you've got hashing of transitive dependencies, but it is not nix.
You are correct, but I don't know if anything in Cabal's new-build that would prohibit git sources. It could e.g. annotate git sourced packages with their revision and include that in the hash.
woah, sweet! 
Is there a standard function `[a -&gt; a] -&gt; a -&gt; a` that applies each function in the list to a successfully? Meaning, if we had a list of function `[f1, f2, ..., fn]`, we would get the result `fn(...(f2(f1(a)))`
If you feel like taking a remote intern (0 pay, just feedback on written code and experience)/cultivating talent, I'm a highschool student. I'm pretty decent at haskell, and also very familiar with java.
&gt; Cabal new-build does do roughly the same thing for at least the Haskell deps I haven't looked at how it's done in quite a while. But AFAIU, it uses a hash that only includes stuff like the options that were used to build, and the versions built. I think that nix will actually hash all the input files and aspects of the environment. Not 100% sure in either case, I don't currently use either, though have dabbled in using nix here and there.
You're right in both cases (AFAIK; I don't know anything about what exactly Cabal hashes currently). But my point is that nothing prevents Cabal from including additional things in the hash in future features.
That's certainly true!
I am a CS student and have taken a Java and C++ class. I want to start on a Haskell project over the next month before school gets back in session. I picked up a copy of Real World Haskell at a used bookstore and will use that as a reference. What are your suggestions for basic projects. Alternatively, is stepping through the Haskell book a better idea than a "deep-end" project because Haskell is so conceptually different than C++? 
Not at cabal contributors, they're fine. No, the hostility is reserved for those who abuse their power.
Unfortunately, in many countries unpaid internships are illegal unless they're part of an accredited educational program.
If I remember correctly, in a talk John Hughes described quickcheck and his pretty print library and probably some other things as all being variations on the theme of generate a complete state space, and then explore it lazily.
By relying on shared data type definitions. For example, have a look at [top](https://github.com/Quid2/top), a content and type oriented protocol based on [simple algebraic data definitions](https://github.com/Quid2/zm).
I tried to rewrite the FST using free monads and I think I got a good working solution that has all of the niceties of the one with monads transformes as far as I can see: - it's as type-safe as the original solution - it separates the protocol and (what I called) interactions - basically asking a user to do some action that is not in the protocol - it's as general as the original solution: one can write different interpreters instead of using a different underlying monad instance with monad-trans What do you folks think? Feedback is very very welcomed: https://gist.github.com/futtetennista/24f83768eb0f3e04a30e522285f6e6d6
You have constructed a map, which takes a set X to the underlying set of it's free Monoid. (I am calling it Mon X for consistency with other answers.) Now you want to find a way to go back. As you say here, &gt;Supposedly these morphisms (G ∘ F) X → X are going to represent (in "mundane" terms) the various monoids on X. every way of going back is supposed to represent a monoid on your set. So given a Monoid to represent you can simply interpret the elements of Mon X in said Monoid. After that you have an Element in X and you are done. Put with more category theory, you have the underlying set of the free Monoid of X. From there you can go to the free Monoid of X. Since it is free, there is a monoid morphism to any other monoid on X. Follow one of those , forget that you are in a monoid and you find yourself in X just like you wanted to. 
Fuck you.
[removed]
There is no "other side", you fool. It's in your head, and now we all must suffer the practical consequences. Well done on wrecking shit.
Uh-huh, because the guy that keeps getting banned from communities and forums is really the one to take advice from in getting along with people.
Do you really want to play?
I don't know about the standard library, but that can easily be expressed by a fold: runInSequence :: Foldable t =&gt; t (a -&gt; a) -&gt; a -&gt; a runInSequence = foldl (flip (.)) id You can test it by running runInSequence [(*2),(+1)]
&gt; Well done on wrecking shit. As far as I can tell, stack users are quite happy. Judging by the increased rate of commercial use and job offers, it's working. Haskell is now viable for industry adoption. This is likely because, for projects with many dependencies, you no longer need to spend substantial portions of your time mucking around with your build tools. Perhaps nix can give similar benefits, sure. But most of industry probably isn't ready for nix, and in my experience, it is currently quite rough around the edges. So, if "wrecking shit" means "improving the lives of most haskellers", then sure I guess I have a hand in wrecking shit.
I feel you. I recently abandoned IntelliJ's Haskell plugins for Atom using this guide: https://github.com/simonmichael/haskell-atom-setup But I don't use the editor for more than simple highlighting and the occasional autocomplete, anymore. Whenever I am stuck on a type I just ask the compiler by using type holes.
You are welcome to believe this. I will not intervene. I will have a giggle; nothing more. If you continue fucking with people, I will play the game. I am currently undefeated at this game. That's not to say I am I will have a certain victory, but that I will happily accept the challenge. Lemme know, fool.
&gt; If you continue fucking with people, I will play the game. I am currently undefeated at this game. That's not to say I am I will have a certain victory, but that I will happily accept the challenge. The game is in your head. This kind of logic makes me think you may literally be a maniac. I say this with no disparagement of people suffering from (or reveling in) mania, or you, really. So, yes, you will have "victory", because you seem to think that you are always right. Good luck
Alternatively, for Arrows: runInSequence :: (Arrow arr, Foldable t) =&gt; t (arr a a) -&gt;arr a a runInSequence = foldl (&gt;&gt;&gt;) id
There is no need for an `Arrow` constraint on such a function, you only need `Category`.
Yup. Just wanted to fix that :)
Great! One more thing I'll mention is that `&gt;&gt;&gt;` itself actually only requires `Category` so your first example works fine and actually infers the more general type if you get rid of / fix the type signature. 
Oh, that's right. Seems like I have worked too much with the original definition from Hughes98. 
If you want code reviews to PM me. You can also pop in to #qfpl, #haskell.au, or #haskell (all on Freenode) to talk shop. 
No, `Option` is more general as it allows you to define `bindOption` (see the code) and a few other combinators unlike `Option2`.
This is right, but as you say it's not generic enough and tied to the particular `NonEmpty`.
Tony, I'm really sorry for being a dick to you in the past. I hope that things will be better in the future.
Nice! :) Would be awesome to complete the whole series. I see that I can edit the code, but is there any way to run it also? Dunno if I'm missing something, or if that's just simply not possible.
[removed]
[removed]
[removed]
&gt; Absolutely not! We hate stringly-typed values, we much prefer to use very precise types which make illegal values unrepresentable. Text streams are just a low-level implementation detail; they are orthogonal to types in the semantic sense. What that bit of the philosophy is really saying is that you should have just *one*, universal low-level interface - and then make that byte streams, for the sake of flexibility. Do not bother to have multiple low-level implementations to match the semantics of multiple "types" (this may be familiar from mainframe operating environments, where "files" can in fact be implemented in a whole lot of separate ways, of which byte streams is far from the most important.) Now, of course UNIX does not always adhere to this (block devices, for example, are special and do not work the same as character devices) but to the extent that it does, it makes for a more elegant semantics, overall. 'Types' and proper semantics for all data can be dealt with in higher layers of the stack.
Use session types and then lower those to a text-based "protocol" by automated means.
&gt; The base units of modularity are complex and heavyweight UNIX programs are not supposed to be overly complex or heavyweight. A 'program' is a protection boundary; that's all there is to it. Within programs, you can use a library-based organization in your preferred programming language (including Haskell) to achieve further modularity.
I believe (this is what I’ve heard) that the user you’re responding to is the same Melissa Click university instructor that was recorded screaming for “muscle” to force lawfully recording students off a public area where a protest was happening. Turns out she’s also a troll on reddit, funnily enough.
11 year old is pretty good, but 7? That’s really impressive.
Thank you for your interest. :) The code execution feature has not been implemented yet. I'm researching how to realize it. I think a main obstacle is avoiding evil code. It seems [mueval](https://hackage.haskell.org/package/mueval) will help me. If you have any idea, please comment it to this tree or the [github issue](https://github.com/syocy/a-tour-of-go-in-haskell/issues/12). (Expected server cost is one of obstacles too. Currently I pay only $0 to Firebase Hosting. Well, it'll work out in the end.)
This is fantastic! Thanks for making this, u/syocy. I think it would be nice to compare the Haskell code to the original Go code. Is there a way to do that? Otherwise I'll just use two browser windows :) 
You could probably crib from http://tour.eta-lang.org/ - or even fully replicate their setup, with your own content.
Thanks! The idea of folding over function composition is one of the most Haskell things I've seen, haha.
I know that you can embed repl.it into your site, but I'm not sure how you'd solve the package issue 🤔 Are there any existing repls with more packages? Then you could include it as an iframe in the side or something like that.
You can generate an instance much like the template haskell that yesod is using with GHC generics and the generic implementation that Aeson ships with. Makes the separate data type not as painful. Here's a sketch of what it might look like. {-# LANGUAGE DeriveAnyClass #-} {-# LANGUAGE DeriveGeneric #-} import GHC.Generics import Data.Aeson data CleanUser = CleanUser { firstName :: String , middleNames :: String , lastName :: String , email :: String } deriving (Generic, ToJSON, FromJSON) 
How is the stack used when evaluating thunks in Haskell? Why is it that in some cases (folding under specific circumstances, for instance) the stack grows while in others it doesn't?
/u/pforteath do you usually reply to applicants? I have applied a while ago but didn't hear back from you.
Well, I have to say this is very informative and concise. Seriously, a list of key ideas and code examples is way better than long cryptic explanations and very little code I often find on various websites. What should definitely be improved, though, are the `Prev` and `Next` buttons in the upper-left corner. They are _so small_ and so close to each other it's really hard to click the right one, especially on a smartphone. On a computer screen they're located far-far away from the content and are as small as the cursor, so, again, they're hard to click. I'd get rid of them and introduce two big purple Prev/Next buttons right at the bottom, after the main content. 
Fantastic work! I think it's obvious what I voted :)
Three options: * If the "stripped" version of the record is going to be used a lot inside your system boundaries (and not just at the edges), create a "splice" of record either using the metamorphosis library or [record splicer library](https://github.com/vacationlabs/record-splicer) * If the stripped version of the record is going to be used at a very few endpoints and only for emitting the final stripped JSON, call `toJSON` on the original record, remove the unwanted keys from the `Value` based ADT that you will get. * Create `newtype CleanUser = CleanUser User` and hand-write a `ToJSON` instance for it where the protected fields are omitted.
Let's take a typical example of what you're mentioning, the `foldl` function. It's definition is: ```haskell foldl :: (a -&gt; b -&gt; b) -&gt; b -&gt; [a] -&gt; [b] foldl f z0 [] = z0 foldl f z0 (x:xs) = fold f (f x z0) xs ``` If you're not used how evaluation in languages with non-strict semantics works, you'd think that `foldl` takes constant stack space since it's definition is tail recursive but that's not the case. Let's say you want to calculate the sum of numbers from 0 to 10_000_000, you would write smth like: `foldl (+) 0 [0..10000000]`; if you manually reduce this expression, it'll become smth like: ```haskell foldl (+) (0+0) [1..10000000] foldl (+) ((0+0)+1) [2..10000000] foldl (+) (((0+0)+1)+2) [3..10000000] … ``` The problem is that thunks - `(0+0)`, `((0+0)+1)` etc. - accumulate because under lazy evaluation there's no need to evaluate them until something forces evaluation; these thunks will occupy memory, way more than what its result would: this is technically called a "space leak". The way to solve this problem is manually forcing the evaluation of those thunks, the function `foldl'` doesn exactly that, and evaluation ends up taking constant space. Now you might wonder when to use `foldr` and when `foldl`: there's no silver bullet but there are a few rules of thumb, for example: - if the operation is strict in its arguments and its computation takes constant time - i.e. (+) - perfer `foldl'` - if the operation is strict in one of its argument prefer `foldr` - if you're interested in only part of the output prefer `foldr` There is more to be said but I'll leave you a few resources where you can deepen your knowledge on the topic: - the book "Introduction to Functional Programming" by Bird and Wadler has a whole chapter on the topic and I can wholeheartedly suggest to read it (not just that chapter but the entire book) - https://www.fpcomplete.com/blog/2017/09/all-about-strictness - https://queue.acm.org/detail.cfm?id=2538488 - https://futtetennismo.me/posts/haskell/2017-10-14-strictness.html Hope that clarifies things a bit, it takes some time to understand how lazy evaluation works so be patient :-)
Great work! My favourite is winning :)
Any code review are subjective, and mine will be, sorry ;) &gt; Goal: to practice using State and IO monad together by leveraging liftIO and unsafePerformIO I was prepared to see some *unsafePerformIO* and there is not, I'm sad ;) # `State` usage - Your usage of `withStateT f get` is not needed. There is three occurrences of it. I did not understand it initially, and actually you can replace `get` by anything you want, such as `pure undefined`. Actually you just want a call to `modify f`. - Then, I'll use `modify` inside the files loop (the last one). Why bother creating a intermediate list of files if you'll deconstruct it immediately after? - What about the `Root` argument in your `State` ? It is never used, never changed, except in the `k` function - The `State` usage should not leak outside of the `k` function. I moved the `runStateT` inside the `k` function, and removed the `Root` usage. - Why use `runStateT` and discard the resulting value (an `()`) instead of `execStateT`? # General - turn `-Wall` on, there is a few warnings which may be corrected. - First time of my life where I see a `main` function with a type different than `IO ()`, I'm still wondering if there is any implication. - Why do you use a lazy pattern match in the first line of your `Main` function ? - I'm not fan of the `Bool` argument to `pretty`. Actually your list of result order depends on the underlying file system. - avoid like plague `(++)` on two lists. Especially if your second list only contains one item. `dirs ++ [fullPath]` will be nicely replaced by `(fullPath:dirs)` which have better performance in all points. If you wonder about the reversed order, remember that `listDirectory` order is implementation dependent. - You use `fst` and `snd` to unpack the result of a complex `foldM`. I'll use pattern matching to unpack them right at the result site and give them a meaningful name. - I'm convinced that the `list` function as a reason to be at the top level of the module instead of in the `where` clause of the `k` function. - why `withCurrentDirectory path $ listDirectory "."` instead of `listDirectory path`? - You are doing a partition between sub files and sub directories, that's a really common case and I'll be tempted to use the `partition :: (a -&gt; Bool) -&gt; [a] -&gt; ([a], [a])` function. However there is no `partitionM` function which works inside `IO` (well, you wrote it in your code) in `base` but there is in `extra`: https://hackage.haskell.org/package/extra-1.6.2 I have used it. - Same, your loop was special casing for ".DS_Store", I use `filter` for that purpose. # `File` and `Dir` - Stick to one naming scheme: `String` of `FilePath`, they are both aliases. - I'm really annoyed by the fact that `date` and `dirDate` are `String` and not `UTCTime`. There is a `Show` instance for `UTCTime`, so the `Show` instance of your `File` and `Dir` will work well, but you'll keep the correct type. - One may want to discuss the type `File` and `Dir`, put the same name in their field, build lenses for them, or group them inside a commont type `data FSEntry = FSEntry {name :: FilePath, date :: UTCTime, content :: EntryContent}` with `data EntryContent :: FileContent {size :: Integer, fileContent :: String} | DirectoryContent :: { directoryContent :: [FilePath] }`. You'll find a *corrected* file on my gist here: https://gist.github.com/guibou/900da1e3740b7b3804f4d6b6778c4cb8 Keep in mind that I did a complete code review with all point that I don't like. It is my point of view, which is biased ;) I hope it helped. 
Thanks you so much. This is what I am looking for.
Well. Functional Programming as a way to program in general makes you think in these abstract ways much much more than imperative programming or OOP. I mean this is possible in Java as well: public static &lt;A&gt; Function&lt;A,A&gt; runInSequence(List&lt;Function&lt;A,A&gt;&gt; fns) { return fns.stream().reduce( Function.identity(), (x, y) -&gt; y.compose( x ) ); } public static void main(String[] args) { Function&lt;Integer, Integer&gt; plusOne = x -&gt; x + 1; Function&lt;Integer, Integer&gt; timesTwo = x -&gt; x * 2; Function&lt;Integer, Integer&gt; combined = runInSequence( Arrays.asList( plusOne, timesTwo) ); System.out.println(combined.apply( 1 )); }
Thanks for your feedback and suggestions! I have made the buttons bigger on mobile. i hope this helps. I also recommend using landscape view for mobile. On desktop, you can also use the arrow keys to change the current slide. Unfortunately, I'm not planning on adding a menu feature at this point.
Awesome folks at Obsidian.
I played around with the extensible snapshots feature a bit. Packages fetched from git repositories are both downloaded into `.stack-work/downloaded` and cached somewhere in `~/.stack/snapshots`. So, multiple projects that share the same git dependencies all end up cloning the same repos.
They'll be doing you a favour by refusing unless you get a MSc. You can get an MSc by darkness in a lot of UK univeristies. There's always the Open University as well. I have an extremely difficult time seeing anyone passing doctoral level exams without reasonable prior higher education experience. There's just too much to learn. It's an experience, not an IQ thing.
Yes! You can read off exactly the multiplication table from the images of expressions of the form [xy] and you can read off the identity from the image of the expression [] (the empty word). Giving another example, there's also a monad Rng where Rng S is the set of "ring expressions over a set S" (ex. [x+0], [2x(y+z)]) whose algebras are rings with underlying set S. You can read off the zero of the ring from the image of [0], the one from the image of [1], the addition table from the images of [x+y], and the multiplication table from the images of [xy].
Thanks. I wish some of the Haskell documentation is as clear as this. The Haskell committee should consider adopting writing principles for documentations. 
Serious question, what good is backpack while virtually none of the packages on Hackage are using it yet. Or in other words, what good is extremely early adoption of backpack?
I'm trying to understand what's happening with the monomorphism restriction here. If I put the following lines in a file: add1 x = x + 1 add2 = add1 . add1 and run `ghci` I get: *Main&gt; :t add1 add1 :: Num a =&gt; a -&gt; a *Main&gt; :t add2 add2 :: Integer -&gt; Integer But if I start up `ghci` without an input file and enter the definitions for `add1` and `add2` from the prompt I get: Prelude&gt; let add1 x = x + 1 Prelude&gt; let add2 = add1 . add1 Prelude&gt; :t add1 add1 :: Num a =&gt; a -&gt; a Prelude&gt; :t add2 add2 :: Num c =&gt; c -&gt; c In the first case, why isn't the type of `add2` reported as `Num a =&gt; a -&gt; a`?
I think it's been fixed on GitHub at least, see this commit: https://github.com/haskell/cabal/commit/871feaebb6dc2a11d592ebd9b411a12eced88cb8#diff-1470073e9713a98f17cf8ba16ccb6798R1733
Thank you for your explanation. I (believe I) understand how lazy evaluation makes folding use a linear amount of thunks under the circumstances you mentioned and when it doesn't. What I don't understand is how the stack is used when evaluating thunks. Does any arbitrary sequence of thunks being evaluated grow the stack proportionally? If so, there is a risk of even a non recursive program overflowing the stack. When I think of a graph of operations/thunks built in memory before evaluation, I don't see a reason why recursive and non recursive functions should be different wrt to the stack when evaluation happens. Sorry if I didn't make myself clear and thanks in advance for the help!
Yeah, but I think that can be considered an implementation detail. NonEmpty makes a free Semigroup, so as long as you have: throwError :: e -&gt; Errors e a runValidation :: (Semigroup e) =&gt; Validation e a -&gt; Either e a then you should be able to use the same interface as the more strict version. Also, now that I think about it using `(e, DList e)` is probably better than `NonEmpty`, just to make sure things associate correctly.
Backpack is a tool to abstract parts of your code (application or a library), it is useful regardless of adoption by other packages on Hackage.
Looks like there's also another open PR that updates the docs with much more info: https://github.com/haskell/cabal/pull/4813/files
I don't actually know anything about the actual internals, so take this answer with a grain of salt. &gt; How is the stack used when evaluating thunks in Haskell? let x = 1 + y y = 2 + 3 in print x When `x` is evaluated (in the use of `print`) a new stack frame is pushed onto the stack for the call to `(+)`. Then to evaluate `y` another stack frame is pushed on the stack. To answer your second question: This is the reason why your stack might explode. This is the reason why you should use an accumulator (that is strictly evaluated, otherwise your heap might explode). Also any pointers to thunks, that were given as arguments, may reside on the stack or in registers. This assumes that thunks are represented as heap objects. Otherwise it makes sharing impossible in some scenarios and it will be impossible to write generic functions without creating a version for every type (see C++ templates). That all depends on the exact representation.
GO NIX! I really hope all consultants start to include nixpkgs and nixos as prominent options within their portfolio. Right now, I don't know of a Haskell consultancy that I would recommend to a colleague that deeply cares about soft skills and the basics of CRUD systems, the way that some Ruby/Python shops do. Obsidian or similar might grow to fill that niche soon. I can also understand if no consultancy emerges in that area, as there are many interesting non-CRUD applications to build.
Another example is: or = foldr False (||) or or = any id In a strict language, you have to use explicit recursion - otherwise it would traverse the whole list. 
You are right that evaluating a nested thunk like `(((0+0)+1)+2)` uses stack space proportional to the nesting. However, - Don't forget that `foldl` is recursive, hence we did build this thunk with recursion. However, it is indeed possible to run arbitrarily stack-intensive computations without recursion, using e. g. Church-encodings, but this is very rare in ordinary production code. - Stack/heap distinction is loose in GHC. Stacks of threads (including the main thread) are allocated on heap. Also, you only get stack overflow exception in current GHC if use use up more than 80% of physical memory for stacks.
Nice tour! If you’re looking for a one-to-one analogue, there’s the `gochan` package: https://hackage.haskell.org/package/gochan It’s based on the exact same algorithm that Go uses, and would allow you to right a line-by-line translation from Go to Haskell.
Getting the basics right is incredibly important to us, even though Haskell and FRP open up so many possibilities beyond that, such as server-pushed events.
Thanks! The feeling is mutual :)
Our internships are paid, but it's still a bit too early for us to consider candidates for Summer 2018 internships.
This sounds like a great option for anyone who can do it! The guys at QFPL are really fantastic.
Wow, didn't know about `haskell-ide-engine`. Looks like an awesome project. Is there any documentation on integrating it with IntelliJ?
Here is something to consider. Two months ago, give or take, there was an outcry in which many people argued that PRs against repos to give them version bounds and help them to compile against more configurations (due to aiding the solver) were "harassment". This was nonsense, but it was argued. And those same people are arguing today that if an author does not _act_ on a PR that they don't agree with, then that too is out of line. So I personally think PRs are fine, and discussing PRs is fine, and not acting on PRs is also fine, because that's all in how open source works. But you need to reconsider the basis on which you are making arguments if you are in a situation where you want to claim both that filing PRs is out-of-bounds and also that not acting on filed PRs, due to disagreement, is out of bounds. At this point it seems to me like there's very little regard for or understanding of the basic norms of open-source social interactions developed over the last 40 years. These disastrous threads wouldn't occur we all agreed you can't get someone do something they don't want to -- you can ask nicely, or you can fork, and that's it. (And furthermore, PRs aren't for brigading or moral grandstanding -- they're for polite discussion, and that's it.)
Not sure yet. In theory, if intelliJ uses LSP then it should work almost out of the box. In practice, I don't think intelliJ does... They like to do their own stuff for languages
Can I assume that thunks produced by the same function recursively will grow the stack and in other cases this won't happen, then? Is there some reading material on this?
If I understand you correctly, then any sequence of thunks necessary to obtain a value will grow the stack. Does this mean that if I write let x = 1 + 1 +1 + 1 + 1 + .... + 1 (a very long but finite sequence, and supposing (+) were non strict) Then evaluating x could overflow the stack?
You definitely can do that with diagrams, which is an extremely powerful choice if you ever have more sophisticated needs. You can see the Located type in ([Diagrams.Located](https://archives.haskell.org/projects.haskell.org/diagrams/haddock/Diagrams-Located.html)), or the [position](https://archives.haskell.org/projects.haskell.org/diagrams/haddock/diagrams-lib/Diagrams-Combinators.html#v:position) function, there's also [transformations](https://archives.haskell.org/projects.haskell.org/diagrams/doc/manual.html#d-transformations) if your pictures aren't the right size or orientation. There are several [backends](https://archives.haskell.org/projects.haskell.org/diagrams/doc/manual.html#rendering-backends) that could be used to create something interactive with minimal fuss (SVG, Canvas, HTML5, Gtk...) depending on you exact needs and skills.
They have good people. Check em out! (And hit me up if what I’ve been up to in nyc also intrigues :) )
Can you pull the latest version of miso and try to build the example project with nix again? I've fixed the `unfree` issue. ``` git clone https://github.com/dmjio/miso &amp;&amp; cd miso/sample-app &amp;&amp; nix-build ```
I don't care what your beef is with mgsloan, but please at least keep this profanity free...
Who are *you* and why are you using an account name that looks remotely like my Tinder screen name?
&gt; we all agreed you can't get someone do something they don't want to I agree! Surely you also agree that it's pointless to have Stackage users coerced into follow Hackage's rules which are of little relevance to Stackage and only benefit the small minority cabal users, no?
You really have to write a lot and I don't know whether there are any optimizations, but in general yes. &gt; foldr (+) 0 $ replicate 2000000000 1 -- Basically (1 + (1 + (1 + ...(1 + 0) ...))) *** Exception: stack overflow Most of the time it really doesn't matter so much. But you can still make it strict when you need it. I.e. instead of stacking up you use an accumulator and force it in every step. [This article](https://wiki.haskell.org/Foldr_Foldl_Foldl%27) could be of interest to you.
Not trying to troll here, serious question. Didn't `intero` obsolete this project?
This is interesting. If the "x" in my example could in theory overflow the stack, then the stack is used when evaluating any arbitrary chain of thunks. Honestly, it makes sense if this is the case. I would think that whichever way thunks are evaluated, it should be the same mechanism whether recursion is involved or not. Thanks for the help!
No. Intero is tied to Stack whereas Haskell IDE Engine actively supports Cabal. 
No, intero definitely came after. It was usable first though, because it was based on ghci. First commit of Haskell IDE engine - https://github.com/haskell/haskell-ide-engine/commit/9fc95d876e7e1face14ed237683b783e1c67fb20 First commit of intero - https://github.com/commercialhaskell/intero/commit/2be50d0bb5754c095fcf6d008a7251ddc1a8a275
Thanks, I stand corrected. But what's the answer to /u/MelissaClick's question then?
&gt; [..] then the stack is used when evaluating any arbitrary chain of thunks. Calling a function will create a new stack frame as long as the function is not **tail-recursive**. &gt; I would think that whichever way thunks are evaluated, it should be the same mechanism whether recursion is involved or not. Correct. Conceptually a thunk is a nullary function with sharing (i.e. keep track of whether the value is computed for all *users* of the thunk). However, the reason that there is a stack overflow is not related to thunks. You will get the same error in every other programming language. (It's just that you usually use iteration combined with mutation instead of recursion and as a consequence don't notice it.)
Nice! I'll have a look.
I'll [add my touch](https://gist.github.com/Gurkenglas/6421869a5278c73482ea5c2460daee39): If there's no `unsafePerformIO`, why have `State`? Also reducing the `monad` part by using applicative style. Also getting rid of `k` by generalizing `list`.
That's another reason against writing _programs_ which do a single thing and collaborate with each other, as opposed to _functions_ which do that: types are lost when data crosses program boundaries. I much prefer Cloud Haskell's approach, in which both sides of the network are considered to be part of the same program, and so the compiler can safely send typed values across the wire without forcing you to think about what happens when deserialization fails: it should never fail, just like the bits of memory in which an Either is allocated should never contain any bit pattern other than that of a Left or a Right.
Just looked at it. Not a go user but I think that's a terrific tutorial for people used to the classic. Your explications are excellent. Well done! 
Thank you, thank you, thank you! Your review is *exactly* what I need as a newbie, will dive in and revise accordingly :D
No. Intero chose a different path with a much more immediate payoff. Intero has a simple architecture involving a GHC API-based server, and a tightly coupled elisp client. It also demands a stack-controlled environment to reduce problems encountered by earlier tools revolving around having consistent versions of Cabal and ghc (the library) in the user’s project’s build environment. These decisions were made to ruthlessly minimize developer effort because there just isn’t that much available effort! HIE is a more ambitious project in design by aiming to, iiuc, significantly reduce the coupling between a GHC API server and editor-hosted tooling. Most exciting to me is the use of the language server protocol, which could enable Haskellers to benefit from tooling investment contributed by tooling developers of other programming languages. The downsides to this approach are that maybe LSP doesn’t express precisely what you want, and the looser coupling might make it harder to just get in there and scratch a particular itch because part of your thinking is now on how this fits into a broader compiler tooling ecosystem. We really want both projects to exist because if Intero works for you today, you win, and if HIE continues getting better, we’ll all benefit tomorrow (I’m being poetic; HIE works today, if with a different feature set).
Fun! Here are my solutions: [gist](https://gist.github.com/mitchellwrosen/d47a504b7e0404f9144d65744407084e)
My understanding is that intero is intended mainly to support emacs, although intellij, vim, etc. plugins are also available. HIE is intended to grow in the longterm into a universal editor-agnostic engine and currently has support from a wider range of editors including atom, sublime, vscode, etc. I don't know how they go toe-to-toe featurewise.
What do those two numbers returned from `mkStdGen` mean?
I would like to point out that emacs lsp support is not in a mature state. A lot of language plugins i use don't yet make use of lsp in emacs yet. I hear vscode has solid support as lsp client.
Generator state. You don't need to know what those means to use `StdGen` 
Does Haskell use a stack of activation records at runtime like most procedural languages (java, c++, etc) do?
main = do let prefix = ['+','a','a'] let letters = ['a'..'z'] let operators = ['+','-','*','/','^'] let opperandlist = [] let finallist = [] reverse prefix transition transition [] = finish transition letters = saveletter item transition operators = joinoperands item drop 1 $ reverse prefixe transition $ head $ reverse prefix saveletter templist = [] ')' : templist item : templist '(' : templist templist : opperandlist joinoperands ')' : finallist head opperandlist : finallist drop 1 opperandlist item : finallist head opperandlist : finallist drop 1 opperandlist '(' : finallist finallist : opperandlist finish opperandlist : finallist putStrLn $ finallist
Considering that vscode invented LSP (iirc), I'd imagine it has solid support :)
I think Damien's logo, while awesome is too busy and complicated fro a logo. I would suggest lose "i" in "IE" part, and connect E directly to the haskell symbol and make it the same height. 
[removed]
Quite the opposite IMO. `intero` is a GHCi-ish (with proprietary extensions) server, and HIE is 100% LSP, and supports a great deal more features, in large part due to its dependency on ghc-mod's engine, which is capable of an awful lot. It also comes with plugins for a variety of other useful tools, including `brittany`, which are all used as libraries so they don't require you to install them yourself, and work over the LSP. Once editor's have good LSP support (which they are incentivized to do), I see no reason for intero to continue existing. We should be trying our hardest to go all in on LSP.
[This](https://ghc.haskell.org/trac/ghc/wiki/Commentary/Compiler/GeneratedCode) is an explanation of how haskell code is evaluated at runtime. It's a bit of a long read, but quite interesting.
Thanks so much! So it looks like with the STG still stores stuff in activation records on a stack and the heap is still used right? Just want to make sure I'm comprehending it correctly 
Has anyone heard back after filling out the form yet? I'm wondering if maybe my submission didn't go through.
https://stackoverflow.com/questions/12719435/what-are-skolems
https://hackage.haskell.org/package/lambdabot ?
It actually stores a lot in closures and sometimes, when things are written in “CPS” style, it doesn’t need to record where it has been .. which is why most programs will never have a stack or heap overflow even though they are written with tail recursion
Thank you for your information. I'll see the eta tour. AFAIK embeddable repl services don't have enough concurrent/parallel packages. But investigating how they are implemented should help me.
Thanks for the kind words. I actually went ahead and added a usage example directly to the README in case anyone doesn't want to sort through the hackage documentation.
I have an [incomplete, unedited draft up](http://extradimensional.space/posts/2017-11-27-fizzbuzz-followup.html) revisiting [my original article](http://extradimensional.space/posts/2012-10-4-finding-fizzbuzz.html) and talking about using the monoid instance of Monoid b =&gt; (a -&gt; b) and the applicative of (-&gt; a) to do it. 
I'm sorry for the inconvenience. I thought that copying&amp;pasting whole the original Go code maybe conflicts with some copyright law or something. But I think I can add a link to original code in each page as a little workaround.
When do you consider candidates? I'd love to apply for that position.
I like the ie past, personally; it would look a bit cleaner if it was sharper and vectorised, but that's an easy touch-up to make if it's wanted. Attaching the e to the Haskell symbol would make the whole thing lose it's meaning and get far too busy. The existing Haskell symbol is already a mashup of &gt;&gt;= and lambda and the = of the bind operator is cleverly changed into being "ie" here; you would lose that if you made the E the same height and merged it in for no real gain, I think. (Of course, this is subjective as all design opinions are...)
Thank you for your information. I didn't know the package. It looks a good reproduction of Go's. If write-only and read-only channel of Go is implemented, I think it becomes even better.
I'm happy to provide a little help. :)
My idea is to reuse the = part of haskell logo. Just add the third line and stylize it as E with middle line shorter that's all. This makes the haskell logo change very unobtrusive.
Thank you!
It looks your solution try to become even closer to the Go syntax. Thank you for your information!
I should probably do you the curtesy and note that I posted it on HN[0] :x There were a couple remarks about usability that might be useful to know :) [0] https://news.ycombinator.com/item?id=15885310#15886687
Damn that is a nice setup! :) I'll add that to my list of things I'd like to replicate to Haskell, - [The Elm introduction](https://guide.elm-lang.org) - [The full Tour of Go](https://tour.golang.org/welcome/1) - [Tour of Eta](http://tour.eta-lang.org) (although without login)
Language like that won't be tolerated (not so much the content, as the direction and intent). We're adults here, act like one or I will remove you.
thanks. good to know.
Ah, okay, that makes sense. I didn't get that from your first description for some reason. I'm not sure the change would be distinctive enough to really be a new logo but I like the concept behind it!
Brick author here - I'm glad you found it useful!
I've added your project to the Brick Featured Projects list: https://github.com/jtdaugherty/brick#featured-projects
Mad Sed/Awk skills
Thanks for weighing in, the applicative approach is quite eye opening, I don't event know one could write code like that :) However, when I try to run it there were 10+ errors, the first two being: src/Main3.hs:41:13: error: • Couldn't match expected type ‘Bool’ with actual type ‘IO Bool’ src/Main3.hs:42:10: error: • Couldn't match type ‘[]’ with ‘IO’ Expected type: IO Dir Actual type: [Dir] 
Thank you so much.
Look at this link to see the logo construct process: https://github.com/haskell/haskell-ide-engine/issues/267#issuecomment-350435110 Your idea will make the HIE logo too similar to the Haskell logo. I think a better logo will show some concepts behind HIE: - the "engine" (maybe using gears or car engine components) - the Language Server : communication between a text editor and a process responsible for Programming Language analysis.
The formatting on your message makes it difficult to read. Could you edit it, and add four extra spaces at the beginning of each line?
Actually, the images shown in the poll are PNG generated from a [SVG file](https://gist.github.com/damienflament/6f8a1b43386157f36881b3f70a9bc232#file-hie_logo-svg). I started by downloading the Haskell logo as SVG from [this wiki page](https://wiki.haskell.org/ThompsonWheelerLogo).
The monomorphism restriction is turned off by default for symbols defined in GHCi, which is why you see the difference in behavior. If you're asking why you're seeing those types in the source file: the monomorphism restriction only applies to `add2` because it's defined without parameters.
Except that even if 1.0.0 was API-compatible with 2.3.4, that's not saying anything about performance. If the programmer wrote his code knowing that a particular function was O(n) but in versions prior to 2.3 it was O(n^3)... You just broke his program if it rely on this for any interaction. I think a solver should always try to use newer versions than the minimum specified (and in this direction, it's fine if it just checks API compatibility) but should only use older ones if it's the only way to get a working plan and even then emits a warning !
How's this look? 
Thanks, at least we can read the code block now. However, I'm not sure how to start. The code in your message is pretty far from valid Haskell, and has more than one problem. Can I suggest that you start with a simpler version of your problem first? Here are a couple observations to start from. 1. When you define a variable inside of a `do` block, that variable is only in scope within the block. So your references to `letters`, `operators`, `finallist`, and so on from the rest of the program will not work. If you meant to be able to access these variables from the entire program, you will need to define them at the top level, just like where you defined `finish` and `joinoperands`. 2. It looks like you're writing a number of expressions trying to *change* the values of your variables. That's not how Haskell works! When you write `item : finallist`, for example, that doesn't mean to *modify* `finallist` to add `item` at the beginning. Instead, it's just an expression describing a *new* list, which has `item` as its first element, and all of the elements of `finallist` after that. (By analogy, think about a math problem, where you have a variable x that's equal to five, and you write x + 3. Well, x + 3 is 8, clearly; but that doesn't mean x is now 8. The colon is an operator just like +, and you can use it to write an expression combining some values. It cannot be used to change the values of other variables that already mean something.) If you wanted some of your variables (like `finallist`) to change their values over time, they cannot be Haskell variables. That's just not what variables are in Haskell. They could be something called an `IORef` instead. But honestly, you're presumably using Haskell for a reason, right? You more likely want to review how to accomplish what you're looking for in a more idiomatic way, instead of trying to write Java or Python-like code in Haskell.
Okay, I thought I was making a new list, and since I was calling transition recursively it would keep acting on this new list? But to simplify a question, I need the final list to be fully parenthesised, how could I append my value and the parenthesis at the same time?
&gt; Therefore you should avoid implementing select using MonadPlus. Ummm... Are you sure?
I'd caution against getChanContents in general. Any kind of lazy consumption of IO is asking for trouble, and newbies who read tutorials like this have enough trouble in their learning process.
You might be in luck regarding copyright. The [github repo](https://github.com/golang/tour) of the golang-tour states: &gt; Unless otherwise noted, the go-tour source files are distributed under the BSD-style license found in the LICENSE file. From a quick look, it seems the code examples are not specially licensed, so the BSD license should apply to them as well.
I will start sending out confirmations on Monday, I'm currently away.
Sounds like a good version. (you have a typo: `throwError :: e -&gt; Validation e a`)
I like this logo. Perhaps it would look nice if the 'i' was flush with the 'e'.
https://hackage.haskell.org/package/monad-loops-0.4.3/docs/Control-Monad-Loops.html
Corrected.
acid-state is my favorite. 
Thanks, this is reassuring :)
If GHCi is stable enough (it doesn't seem to make as efficient use of memory, and doesn't seem to be able to use as much in total) and fast enough, I say keep using it. You should be able to launch it in the background, and then (pipe commands to it)[https://serverfault.com/questions/443297/write-to-stdin-of-a-running-process-using-pipe]. I have not done this though, so YMMV.
&gt; I would be careful with `undefined`. You don't really gain anything by using it here as opposed to `()` Apparently my remark &gt; Not to say you should (you should use `()`) was not working as expected :p jk, that was a valid expansion on that. Thanks.
&gt; (subDirs, subFiles) &lt;- mapAndUnzipM list $ filter (/=".DS_Store") $ map (path&lt;/&gt;) $ dirContent dir Thank you, I learned about `mapAndUnzipM` and that's great. Also, I think you need to flip the filter and the map ;)
&gt; the applicative approach is quite eye opening Actually, in this example, the applicative approach is just a more compact way of writing the call to the `Dir` and `File` constructor which are taking input from `IO` actions. However the point which is really important here is that you don't need `State` for this kind of computation. I'm trying to avoid `State` when possible, and when I'm seeing something such as `StateT s IO ()`, it really smells something which can be rewritten as `IO s`, especially in your case when you only write to the state, but never read from it.
Thank you for the educational material !
Because Go is funded by Google and therefore there is a lot of money and people behind it.
I merely copied that behavior from the previous fork. `hlint` told me about `mapAndUnzipM`, I'm not sure whether it deserves a name.
Have you seen Chris Done's [tryhaskell.org](https://www.tryhaskell.org/) site? It achieves safety by using the [pure-io](https://hackage.haskell.org/package/pure-io) package to simulate a limited number of IO actions in pure code. You could extend that to provide any additional IO operations needed for your tutorial.
The pleasure is mine!
Damned you are right. It is my mistake ;) sorry about that.
The only I know about is http://www.datomic.com/ and it seems amazing! A **lot** of big companies use it.
Do you have a source for this?
(I'm not an expert) I suspect the performance you've achieved is pretty close to what you're going to get. A few things I'd try: 1. Use `unsafeRead` and `unsafeWrite` to avoid bounds checks (I have no idea whether or not Rust does them). https://wiki.haskell.org/Performance/Arrays 2. Use plain `Int` instead of `STRef Int` for `ind` and `ttl` 3. While you're at it, unbox those `Int` types to `Int#`. https://downloads.haskell.org/~ghc/7.0.1/docs/html/users_guide/primitives.html
I thought that was a Clojure solution. No?
We often make the mistake of underestimating the abilities of kids. It is also a perception that Haskell or FP is hard to learn but I guess it is easier for those who do not have to unlearn the imperative paradigm. Also, when I teach them both together they get competitive and it makes them learn faster. The 7 yr old struggles with some of the stuff but sometimes she manages to even excel the 11 yr old.
Good job
Do not use `STRef` in that context. `STRef` are really slow, because they are internally an indirection to a boxed Int, which is an indirection to an `Int`. So reading an `STRef` introduces many indirections to something which will not be in a machine register. The mistake here, really common when using mutable array in Haskell, is that you only want your array to be mutable, nothing else. If you write a proper recursive function, with "unmutable" `Int`, chance are that the terminal recursive function will be optimised as a really thight loop and all your `Int` will be unboxed and stored in machine register. 
I'd like to help you, but at this point, the most helpful thing I can think of to say is to put this particular on the shelf for a while. This is a moderately complex problem, and if you expected this code to work, then you're missing far more basic concepts. In addition to what I've mentioned already, it's filled with syntax errors, and confusion about types. If you don't go back and figure those out by trying a sequence of simpler tasks, you're setting yourself up to fail. https://wiki.haskell.org/H-99:_Ninety-Nine_Haskell_Problems would be a great place for you to start.
Not popular. Very experimental. But stimulating: https://github.com/agentm/project-m36.
Thanks for pointing the unsafe methods, got rid of another 100ms I didn't manage to use Int#, it isn't a lifted type? And yeah, I'll try to make it into recursive solution
URef is available as an unboxed alternative https://hackage.haskell.org/package/mutable-containers-0.3.3/docs/Data-Mutable.html#t:URef I wrote about that in my "Fast Haskell" post re STRef vs URef http://chrisdone.com/posts/fast-haskell-c-parsing-xml#the-dom-speed 
From their webpage: &gt; the Datomic Peer library runs in the JVM Seems unlikely to be written in Haskell.
How is `kay` defined in Rust? Is it a Vec, or a stack array? &gt; Is it rust's own credit, for being exceptionally fast? I mean, Rust definitely strives to be the fastest it can. Haskell intends to be fast, but it has other goals that are higher priority.
I'm a big believer in the idea that we underestimate kids, as well as the idea that Haskell is _not_ actually all that difficult to learn. I could easily see an 11-year-old learning Haskell. I'm more impressed that you had the inclination to teach the kid than that they were able to learn. The 7-year-old really surprises me, though; I guess even I underestimate kids.
OK it helped a lot.but I needed to add `FlexibleContexts` pragma for it to compile, why? Something about non type variable argument... And like I said, it helped a lot, around 50% faster, down to around 290ms still far, far, from beating rust... Would the Unboxed Ints (Int#) help?
Also, inline your function (with `{-# INLINE #-}`), this way, at the call site, it will be inlined around the `rule` function and will get ride of all the overhead of calling a function.
If you want to get to the bottom of this properly you'll want to make a repo or gist with exactly the code sample of Rust and Haskell available with build instructions for each, preferably with Haskell having a benchmark using criterion for the function you're testing. Criterion docs here: http://www.serpentine.com/criterion/ I've made a set of example repos that benchmark different Haskell things here: https://github.com/haskell-perf Which show how to use Criterion. I agree with others on making those two auxiliary variables parameters in a recursive loop, rather than making mutable boxes to read/write to. I wouldn't bother changing the `Int` to `Int#` until you've done everything else. In my experience manually unboxing Ints usually leads to the same Core output as letting GHC do it for you. Where's the Rust code? How does the Haskell code get its inputs? How are you measuring both? And are you really passing in a linked list (memory indirections, allocations of boxed slots) from somewhere? I imagine the Rust version is not. Your Rust version seems to inline the rule, whereas the Haskell version has it passed in as a closure; it might be worth adding an `{-# INLINE countSteps #-}` pragma so that it inlines like you've manually done in the Rust version. Lastly, I don't see any `-O` or `-O2` flag passed to GHC in your cabal file https://github.com/samosaara/advent-code-2017/blob/master/advent-christimas.cabal Are you passing it as an argument when you build?
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [samosaara/advent-code-2017/.../**advent-christimas.cabal** (master → 442d941)](https://github.com/samosaara/advent-code-2017/blob/442d94113d8c0722f13602d083969c5796a304e0/advent-christimas.cabal) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
Check if your accumulator is not too lazy and inline your function.
I can imagine that `URef` beats `STRef`, but what about `URef` versus `Int` when you can write your loop counter mutation as a recursion? I'm inclined to think the GHC inliner will work better with an `Int` by simply removing all the `box . unbox` occurences, to finally generate a perfectly unboxed `Int` which fits in a register. On the other hand, the `URef` will always have an indirection and I don't think the optimization pass can remove it, because it will change the semantic of `URef` which can be modified by an other thread. A quick and totally irrelevant micro benchmark: uglyLoop :: Int -&gt; Int uglyLoop n = go n 0 where go !0 !c = c go !n !c = go (n-1) (c+1) uglyLoop' :: Int -&gt; Int uglyLoop' n' = runST $ do n &lt;- newRef n' :: ST s (URef s Int) c &lt;- newRef 0 :: ST s (URef s Int) whileM_((0/=) &lt;$&gt; readRef n) $ do modifyRef n (subtract 1) modifyRef c (+1) readRef c Gives: λ skolem ~ → ghci -fobject-code -O2 ~ GHCi, version 8.0.2: http://www.haskell.org/ghc/ :? for help Prelude&gt; :l BenchGHC.hs Ok, modules loaded: Main (BenchGHC.o). Prelude Main&gt; :set +s Prelude Main&gt; uglyLoop' 100000000 100000000 (0.24 secs, 103,632 bytes) Prelude Main&gt; uglyLoop 100000000 100000000 (0.06 secs, 98,800 bytes) I don't know how much it is relevant, because some other optimisation which favorise the `Int` may be involved (and I may have missed something else).
I just thought criterion to be way too complex to to the kind of thing I was using, so I'm just measuring wall time of total execution, it works because It's the only thing the algorithms are doing. [Here](https://gist.github.com/samosaara/9ddcd3e4ccb13b90cd257fa9c88dd9ec) is my rust solution, and the repo of the haskell solution you seem to already have found. Yeah I though the same about the Int#, and did convert into a recursive function, 2x faster thanks. Both read from a file. Do you even think in-lining it would make that much of a difference? the rules I'm using are `(\z -&gt; z + (if z &gt;= 3 then -1 else 1))` and `(+1)` doesn't GHC inline those? If not HTF Do I inline a lambda and partially applied function? I do were compiling both with optimizations!
GHC is far better at optimizing tight immutable loops, and the garbage collector really doesn't like mutable references. It's not surprising to me that a loop using immutable variables that are trivially unboxed by the optimizer is much faster than an unboxed ref solution. This is something that I wish haskell provided better control over. As you make these algorithms more generic and polymorphic, you begin to rely a lot on the inliner and specializer to keep them fast. The INLINABLE pragma goes a pretty long way, but it's not perfect.
And [vcache](https://hackage.haskell.org/package/vcache) has become the favorite of mine in the last several months. I find it to be conceptually similar, yet superior to acid-state: - I find it similar, since it provides an ACIDic interface for writes, and a transparent, pure, lazy interface for reads. I'm currently using it in a desktop application, and it's a breeze how quickly and seamlessly it works in the background. I'm basically looping most of the application state through it, and it saves every change the user makes efficiently, quite at the instant the user makes them, in the background without posing as any distraction. And it seamlessly reads whatever the user needs into memory, lazily, whenever the user needs them. And since it's ACIDic, I can basically kill the process or yank the power cord out of the computer and on the next startup, the database will cheer me in a consistent state, having lost at most the last few milliseconds of changes that it didn't have the chance to persist yet. Not bad. And I only needed to write some VCacheable instances and the simple state-loop through vcache to make this work: it's persistence for almost free in terms of development time as well. I'm basically persisting the Haskell algebraic data structures as they are, since the serialization and marshalling is taken care of in the VCacheable instances. - And I find it superior, since, as I wrote above, it only needs to read as much into memory from disk as necessary, on demand. acid-state always needs to have all the persisted data in memory all the time. That's a conceptual wart in my view, and a no-go in some of the applications that I write. [Here I've published](https://www.reddit.com/r/haskell/comments/6p24ci/ive_briefly_tried_vcache_and_here_is_how_it_was/) a literate Haskell file with some of my initial experiences with vcache if you are interested in further reading. 
https://gist.github.com/samosaara/9ddcd3e4ccb13b90cd257fa9c88dd9ec Yes, compiling both with optimizations 
The accumulator is being strictly checked at every iteration. Laziness can't be a problem here.
Question for someone more familiar with hardware than me: Will branch prediction make the use of `unsafeRead` and `unsafeWrite` unnecessary? Or do the bounds checks actually hurt much?
`ttl` is incremented but never read (I'm talking about an implementation where `ind` and `ttl` are `Int` and not `STRef`).
Oh I see. Yea that is likely accumulating thunks, if it's never read at all.
I were using the strict version of `modifySTRef'`, I was aware of that possible problem, doesn't matter now, I've made the loop into recursive function. https://github.com/samosaara/advent-code-2017/blob/master/src/Xmas.hs#L105
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [samosaara/advent-code-2017/.../**Xmas.hs#L105** (master → 72bae55)](https://github.com/samosaara/advent-code-2017/blob/72bae55af9f75efda9422367d0edf3f508486231/src/Xmas.hs#L105) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dr1pcuy.)^.
My comment was referring to your recursive implementation, which, if you wrote it without taking into account the strictness of the `ttl` parameter, will space leak.
Did anything come of this? I see the GitHub repo hasn't moved.
&gt; Use unsafeRead and unsafeWrite to avoid bounds checks (I have no idea whether or not Rust does them). Rust is memory-safe, so bounds checks are present unless the compiler is able to prove they are useless.
If you use unsafe methods in Haskell then you could use them in Rust too: https://gist.github.com/anonymous/690acf2f294650ed158ad458729ea894
[SQream DB](https://sqream.com) is an SQL database written in CUDA, but the query engine and parser are written in Haskell. The Haskell part is very well suited for the hundreds of optimizations that the GPU engine needs to run the SQL query well. Source: I worked on the SQream DB query engine
Datomic is written in Clojure
Unfortunately I haven't really had the time. Most of my free time has been consumed by the WebAssembly effort :P The idea was to use a monad that yields symbolic results without actually performing work. Then you just ask for it when you need it, and the monad commits all the queued work. I was debating between having the symbolic result be a lazy thunk that uses `unsafeInterleaveIO` to commit the work and get its value, or just having a data type (like a free applicative) that lets you compose these symbolic results and execute them on demand. The other thing I was considering was having a regular monad transformer with a lawbreaking applicative instance which does not commit any work. This was the thing I'd like to see the most, but had the hardest time figuring out.
Well, I did think about that; I'm not sure it's possible to have a regression in performance if the code hasn't changed at all since some version. There's a difference between "this function exists" and "this function exists and its code hasn't changed at all" and ideally, older versions would automatically be used only if the latter was true. It would be neat to have an optional "only preserve API compatibility" flag for testing, though. Using newer versions also breaks because a newer version of a function might accidentally introduce a quadratic performance regression, so that's not any less dangerous if the performance is critical to the code's behavior.
Didn't help rust. Made 0 effect, both versions runs just as fast.
Ah okay, good to know. No idea why (on my web browser) the "ie" seems to have 2-3x the aliasing radius that the other part of the logo has, then ¯\\\_(ツ)\_/¯ [(img)](https://i.imgur.com/BE6rtBo.png)
If they read from a file and you just measure how long the program runs for, how do you know what you're actually measuring? I might take the time to install rust and figure out how to build and run your example with proper optimizations and then write a Haskell equivalent and compare them scientifically. But it's probably easier if you check what you're measuring here!
Well I'm measuring how long it takes from them to parse the numbers and then make the calculations.I mean, sure, parsing ints in Haskell might be much slower then rust's, but still proves that rust is much faster. But yeah, I guess it could help in improving if I knew where Haskell was taking all that long, I'll look at it
That's interesting, I've never thought about a GPU-backed database before. What's a motivating use case? What advantages would it have over a regular in-memory database?
To be honest, I'm not quite sure what branch prediction has to do with unsafeX functions. I mean, they're essentially "if safe then do", but branch prediction doesn't erase having to do that check, it just means the computer will preemptively execute both code paths in parallel if it can't figure out "which universe will happen" and the CPU will get very good at guessing that you're not writing out of bounds, but that doesn't optimize away the branching instructions. What would be ideal is for safeX functions to be optimized away into their unsafe versions by static analysis on the compilers part. That's the only way to really save that speed. The safe writing can hurt as much as a single pointer indirection in a hot loop, but it depends on a lot of things and my low level knowledge gets a bit fuzzy at this point because of all the black magic CPUs do now to mitigate these types of checks :) Branch prediction is really useful for things like case statements. You have 15 different code paths and the CPU needs to load the next instructions into memory. Which path does it pick? That's where branch prediction shaves off orders of magnitude in time. Single branch paths don't benefit much, especially if the CPU can't optimize them away entirety.
It happens :-) On my system my version is about 10% faster, from the asm you can see the missing bound tests in the hot loop. Part of the performance improvement comes from the array of i64.
It's for big data, so you're talking about databases where "tiny" means 1-10TB of data. Pretty much any SQL query you write is going to be a pain in the ass to execute on all of the servers and hard drives the database might be distributed around. Much better to parallelize the crap out of that if at all possible, and that's where GPUs shine. (This is just an educated guess of mine. I didn't feel like putting info in for their white paper)
I made a program to count the number of vowels in a given string and came across a problem while working on them. Apparently while using WinGHCi the program works, but strings input into the program longer than 1024 characters are automatically cut off to 1024, leading to a lexical error because it takes off the quotation mark at the end and some other characters. I was wondering why this happens (I'm assuming it has something to do with GHCi itself since other Prelude functions also do not allow it if it's that long) and if there was a way around this other than to just manually cut the strings into sections of 1024 characters. 
Long live acid-state!
So the thunks themselves don't really touch the stack. It's when you force a nested thunk and there is no way to give back intermediate results as you go. So: 1 : (2 : (3 : (4 : []))) Doesn't grow the stack. But: 1 + (2 + (3 + 4) Does. 
I’m not sure I believe your rust timings: $ time ./day5 28040648, is the answer, it took 5.9820592 ms real 0m0.062s user 0m0.058s sys 0m0.004s 62 ms is not 6 ms. fwiw, My very boring Haskell solution based on Data.Vector.Unboxed.Mutable completes in 81 ms to your code’s 62ms. Not that much of a difference?
Here’s a fast(er) int parser in Haskell (this one reads from stdin): import qualified Data.ByteString.Char8 as C import Data.Char readints :: IO [Int] readints = parse &lt;$&gt; C.getContents where parse = unfoldr go go s = do (n,s1) &lt;- C.readInt s let s2 = C.dropWhile isSpace s1 return (n,s2) 
Have you experimented with SQLite vs vcache in performance? I have an out of memory array computation project looming and am looking for backends ...
&gt; SQLite vs vcache in performance? Although I've used SQLite separately in other projects, I haven't compared the two directly. If you do, please do share your results. And feel free to ask vcache questions if I can help you on that journey. 
&gt; Joachim Breitner’s excellent inspection-testing tool, which is now integrated into the automated test suite, is making sure that the optimisation happens by automatically doing this comparison. _Nice_ I did not know this library existed, and I am very glad you've revealed it to me.
I bet you didn't stop to consider my CPU... Which is an Intel i5-3320M 2 cores 4 threads 3.3ghz, and my cpu's timings are: The answer of the 2# part is: 25608480 It took 0.280501495s advent-christimas 5 0,28s user 0,00s system 99% cpu 0,282 total In haskell, against rust's 3x faster 25608480, is the answer, it took 67ms target/release/rs-advent-code 0,07s user 0,00s system 99% cpu 0,071 total
Try using the LLVM backend to ghc: ghc -O2 -fllvm with ghc 8.2 got me a 2x speed boost on this problem using a recursive function defined over a mutable unboxed vector.
Don’t do this blindly. If the call site is recursive, pitting INLINE everywhere actually breaks performance. Use INLINABLE instead !
On AWS: shipping metrics directly to Cloudwatch (which is by the way also is technically a REST API). In-house: ekg + Splunk polling
You are benchmarking a mutable collection in a pure immutable language against a systems programming language. People rarely write code like this in Haskell. If you transform your algorithm to a fold, it will probably match Rust in performance. Also; using ‘time’ measures the startup time of the runtime, which includes spawning a bunch of threads, initialising a memory space, etc.
I tried to benchmark the int parsing alone to see if that was the source of the landslide but what I got was that haskell took `0.000134279s` and rust's `0.000046922` sure once more much faster, but neither of them even make up for a single ms, definitely doesn't explain the almost 200ms discrepancy unless... Haskell laziness leaves to actually parse the string when trying to read it from the mutable array, but I'm pretty sure making the list into an STUArray is strict and reads the entire array right? And even if it didn't, the loop runs almost 2M times against the list length of 1057, it would not even make a huge impact... 
About `operational` I found a bit surprising that only a few packages are using it given its general nature: https://packdeps.haskellers.com/reverse/operational
Your rust code runs in 62 ms on my 5 year old laptop. My Haskell runs in 82ms. Your CPU is irrelevant to this comparison.
I'm trying to find performance metrics for different use cases for sqream but I'm not finding anything beyond the info graphic. Do you have any sources?
I would say that intero did not obsolete haskell-ide-engine, they are just two different projects. Along with editor support for information, haskell-ide-engine is more intent on providing an environment that tools like HaRe, etc can get direct access to information and types from GHC. I would like to see stuff like that in intero too, but the focus is on editor integration, particularly emacs. One of my concerns with haskell-ide-engine is that it may be difficult to retrofit ghci atop it. Using ghci as the basis makes a whole lot of sense. That may not be necessary to be quite useful though, only time will tell.
Maybe what I'm missing is LLVM then, mine is also 5 year old, I've got a thinkpad t430 
Well that feels seriously limiting, I mean sure, I know certainly programming languages are more proficient t certain tasks but, I though dealing with numbers was a natural thing to any language and I was also curious the reason rust being so much much faster. But you make me puzzled, what you are suggesting is very interesting, could you actually convert such problem into a fold? Aren't folds supposed to be linear (start to end)? And like you need to back track a lot how the heck are you even supposed to do that with an accumulator? And like, how you even you are supposed to split such a linear calculation? One result always needs the previous to keep going,
True, he does seem to care, that's why it's weird that he has been rather stubborn in some of these circumstances. However, things are all good now, because he has made a change that fixes the builds - https://github.com/haskell-infra/hackage-trustees/issues/120#issuecomment-350561879
That aside, your Haskell solution is imperative and doesn't make use of the functional programming paradigm. This sort of imperative code is a bolt-on .. for example, perhaps try using `repa`, zip and V.map instead of all these state references. 
This is fantastic, I was talking about doing this a month or so ago but never got around to it. It would be nice to see the [OverloadedLabels](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#overloaded-labels) use to clean up the syntax a bit: `field #name`
How does this compare with Kmett's lens library?
Doesn't this make the built-in `HasField` class obsolete? If you can get its features using generics, there's no need to bake it in the compiler.
Thanks, using overloaded labels has been brought up a couple times before, actually, but it's unfortunate that the only way to make it work is to provide an orphan instance... Maybe I should just put it in as a separate module anyway?
The derived lenses are compatible with the `lens` library's representation. You probably still want to use `lens` (or another library that uses the same formulation, like `microlens`) to use the utilities. `generic-lens` is only concerned with providing the lenses, i.e. it could replace the TH functionality of `lens`.
Oracle is making custom ASIC / chips for SQL query processing...
Please ignore the part about a fold, I hadn't realised that the problem skips indices. I had a quick snoop around and I think you could write your own tight pure loop using unboxed Data.Vector and unsafeDrop which would generate quick code.
There's this: https://github.com/duog/generic-lens-labels Which seems very nice, but the author has not released I suppose because of this problem.
OK, sorry I don't have a Haskell env on me to test, but I think something like this might work using the `vector` (Data.Vector.Unboxed.Mutable) package and pure functional values. ``` {-# LANGUAGE BangPatterns #-} import Data.Vector.Unboxed.Mutable as import Data.Vector.Unboxed (fromList) as VU doIt :: [Int] -&gt; Int doIt l = let !vec = V.force (VU.fromList l) !size = V.length vec in fix (\f !i !n !v | i &gt;= size = n | otherwise = let step = V.unsafeRead v i in f (i + step) (V.unsafeWrite v i (if step &gt;= 3 then step-1 else step+1))) 0 0 vec main = show $! doIt [0 3 0 1 -3] ```
As a side note, I had a revised version [up here!](https://gist.github.com/coodoo/3be2e687b37e1ae4a10b8ca0a28616a8) according to /u/guibou's review, thanks again for taking the time to review and a great learning experience :)
Converting all calls `f x` into `seq x (f x)` is equivalent to just not having laziness.
This is already a thing, but figuring out the 'sufficiently' strict f part is hard and, whilst it works most the time, you're encouraged to use the BangPatterns language extension to help it out.
I'd really recommend using criterion for this. Wall time can be effected by all kinds of things, and folks often take the average of n of measurements to try to work around that without wondering if n is statistically significant or if their timing data has outliers, etc... You can sidestep all of those potential problems with criterion, and you probably wouldn't have to do anything that strays from the tutorial. It's worth it to have numbers that you trust / have some idea how much you can trust. 
thus 'sufficiently strict'
Oh. This is the strictness analyzer. It already exists and does a lot of work. It's responsible for allowing a lot of data to become unboxed, which is a huge reason Haskell is fast.
I would not recommend using GHCi as a deployment strategy. Fast deployment times really shouldn't be so critical that you can't wait until a new binary is built and uploaded to kill and restart the process. GHCi will hurt your performance, and leaks memory on `:r`.
&gt; Lastly, I don't see any -O or -O2 flag passed to GHC Not too important for the topic but I thought I'd mention it because it often confuses people: `-O` is the default for building stuff via stack/cabal, but `-O0` is the default for building with plain ghc.
&gt; and I’m not sure why that is My guess: * People who are used to write languages in which most databases are written in (typically C), have a much easier time switching to Go than to Haskell, because Go and C are more similar than Haskell is to either. * Go is easy to learn. The language is more primitive than Haskell, but that does make the learning faster. * Thus there are a lot more Go programmers. * There are few commercial users of Haskell. I could probably name 80% of the companies deeply invested in Haskell within 20 seconds. When you have a small sample like that, it is likely that any given problem domain isn't covered at all. There may also be other, more technical reasons, such as: * Go gives you stupidly simple, predictable performance. This is much harder in Haskell. You care for that when making a database. * Go has an excellent run-time (no libc, direct use of syscalls, easy static linking, thus easy deployment). * Go has a well engineered build toolchain (20 MB-executables compiled from source within a second). But I suspect those reasons don't play much into it.
&gt;What's the status of this feature for other environments? I don't have anything in Vim besides `hoogle`, so I think you're best sticking with IntelliJ. 
It's only for short lived testing instances, with rapid commits and quick demos... People I work with already despise Haskell because of past baggage, so I am trying avoid adding "it is slow to rebuild" to the list of complaints. For example, if someone wants a small tweak in Html or the json from a REST endpoint to be formatted slightly differently, I don't want them to complain about waiting for five minutes for a rebuild from.cabal. I could probably ensure ghci is shutdown a few times a day as well. I am using cabal build for our normal long lived production instance.
What about: data LensLabel field field :: HasField field s t a b =&gt; LensLabel field -&gt; Lens s t a b instance s ~ s' =&gt; IsLabel s (LensLabel s') where fromLabel = LensLabel
Can we also 'unlift' some data? Sometimes we don't have access to the constructors
While of course Datomic is written in closure, "runs in the JVM" is doable in Haskell through the JNI.
Quick question -- recommended for large-scale "industrial" use, or is it better for wait till the project hits 1.0?
I agree. The plan is to only use a single HIE server, and expose a ghci session through it too. HIE basically has an interactive ghc session allready for the checking, which can be used for the REPL too. One of the problems is that there is not a standard LSP way to provide a REPL to the UI. So in the meantime we are focusing on getting the LSP stuff working as a start.
Cool! Yes, I think LSP support makes a whole lot of sense. A very good way to support many editors.
Branch prediction is not that. Branch prediction is that the CPU predicts which branch will be taken, and loads the code into its pipeline from that branch. If it was wrong, you get a huge penalty because it now has to refill its pipeline (and lose as many cycles as the pipelines length). For those who might not know of it, here is an [excellent article on the history of branch prediction](https://danluu.com/branch-prediction/) (also, the whole blog is great).
So `x @"foo"` is equal in power to `#foo`, and this was known when `HasField` was introduced. So it might be better to think of `#foo` almost as syntax sugar. One could then maybe argue that `#foo` should become genuine syntax sugar for `@"foo"` instead of using a special type class. This seems reasonable to me. One downside is that `#foo` is no longer an expression in isolation, so you can no longer do `#foo &lt;#&gt; #bar`, however due to how bad type inference is for isolated uses of `#foo`, the above only works with sufficiently monomorphic operators in the first place. So this isn't likely to be a big loss (e.g. `#foo . #bar` isn't going to be a thing regardless). 
I believe you are talking about `OverloadedLabels`, which can have other uses. I was thinking about `HasField` from `GHC.Records`, which is independent of `OverloadedLabels`.
Remember that in PVP, it's the first two components combined that are considered major. 0.1.0.0 is basically equivalent to semver's 1.0.0. Whether it's production ready or not depends on whether it does what you need it to or not. What I want from this library is pretty basic (just generic lenses), and it covers that perfectly and with good tests. For me that would constitute production ready.
Those are unfortunately mostly internal for now. I think we have some in our whitepaper, but I can give some more information: SQream DB is not a 'millisecond speed' database. Most queries take a few seconds. SQream DB is designed to perform well at tens to hundreds of terabytes of data... For example, we were asked to compete with IBM Netezza in a retail scenario, where we'd calculate the [ACV](https://en.wikipedia.org/wiki/All-commodity_volume) for a big 300 billion entry fact table. The data size was about 24TB just for that table. We beat IBM Netezza - they did 33.7 seconds for that query, and we did 31.7 seconds. These results are the average result for a few dozen runs... The big difference was that SQream DB ran on a single Dell box that costs about $30k with GPUs, while the Netezza cluster was an 8 rack monstrosity, clocking in at about $12 million.
**All-commodity volume** All-commodity volume or ACV represents the total annual sales volume of retailers that can be aggregated from individual store-level up to larger geographical sets. This measure is a ratio, and so is typically measured as a percentage (or on a scale from 0 to 100). The total dollar sales that go into ACV include the entire store inventory sales, rather than sales for a specific category of products – hence the term "all commodity volume." ACV is best related to the key marketing concept of placement (Distribution). Distribution metrics quantify the availability of products sold through retailers, usually as a percentage of all potential outlets. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
You are correct, wow there sure are a lot of different ways of approaching the overlapping names problem currently in GHC. I think you are correct in saying we don't need `HasField`.
That's pretty accurate. You need a data size big enough to warrant "warming up" the GPU. If the problem size is small (ie. it fits in main memory), you won't reap too much benefit from the GPU. Having said that, if you're running VERY intense computations, like cryptography - you might still benefit from having a 'coprocessor' like a GPU alongside, to offload these operations to.
That's how Netezza got started with TwinFin - they'd have FPGA boards powering through some of the heavier physical relational operators.
I assume you're referring to replacing uses of `Int` et al. with `Int#` et al. This is also something GHC already does.
&gt; It’s important to mention that as of this release, only the lenses are optimised away completely, the prisms still have some leftover overhead. This is planned to be fixed in a future release. Cheap generic prisms would be really nice to have. I tend to use those just as much, if not more than lenses personally. 
This may just be a weird quirk with my build of GHC, but does anyone else get better performance by removing the "module Main where" line?
It would help a lot if you could post code, yes. I would recommend using a hosting service like [gist](https://gist.github.com/) or [lpaste](http://lpaste.net/) and linking to that.
On my computer, using vector cuts the time for the 2nd part in half. To ensure a fair comparison, I transformed OP's code to the bare minimum: {-# LANGUAGE FlexibleContexts #-} module Main (main) where import Control.Monad.ST import Data.List import Data.Char (isSpace) import qualified Data.Array.Base as B import qualified Data.Array.ST as S main :: IO () main = do xs &lt;- map read . lines . dropWhileEnd isSpace &lt;$&gt; readFile "input/xmas5.txt" print $ countSteps (\z -&gt; z + (if z &gt;= 3 then -1 else 1)) $ xs countSteps :: (Int -&gt; Int) -&gt; [Int] -&gt; Int countSteps rule xs = runST $ do let siz = (0, length xs) arr &lt;- S.newListArray siz xs :: ST s (S.STUArray s Int Int) let manySteps ind ttl | S.inRange siz ind = do curr &lt;- B.unsafeRead arr ind _ &lt;- B.unsafeWrite arr ind $! rule curr manySteps (ind + curr) $! (ttl + 1) | otherwise = return ttl subtract 2 &lt;$&gt; manySteps 0 0 And this is my code using vector: {-# LANGUAGE FlexibleContexts #-} {-# LANGUAGE BangPatterns #-} module Main (main) where import Control.Monad.ST import Data.List import Data.Char (isSpace) import qualified Data.Vector.Unboxed as V import qualified Data.Vector.Unboxed.Mutable as M main :: IO () main = do xs &lt;- map read . lines . dropWhileEnd isSpace &lt;$&gt; readFile "input/xmas5.txt" print $ countSteps (\z -&gt; z + (if z &gt;= 3 then -1 else 1)) $ xs countSteps :: (Int -&gt; Int) -&gt; [Int] -&gt; Int countSteps rule xs = runST $ do arr &lt;- V.unsafeThaw $ V.fromList xs let step ind !ttl = do mcurr &lt;- (V.!? ind) &lt;$&gt; V.unsafeFreeze arr case mcurr of Nothing -&gt; pure ttl Just curr -&gt; do M.unsafeWrite arr ind $! rule curr step (ind + curr) $! (ttl + 1) subtract 2 &lt;$&gt; step 0 0
The haskell report is (in my opinion) overly strict in how it allows typeclass constraints to be used. One of those limitations is that you can only have type variables in constraints. When you start using multi-param typeclasses you often want to pin one class argument to a specific type. FlexibleContexts tells GHC to be less uptight about following the spec and loosen these restrictions from the report.
That's what GHC already does. I mean, for example, values of type `a` (i.e. type variable that we don't know about) can also be 'banged'
You can use [codex](http://hackage.haskell.org/package/codex) for this. I used it long time so can't recall all the details, but it downloads sources and generates a tags file for your project (including all the dependencies) and you can use any ctags-supporting editor to jump to the definitions.
That’s right, the library is considered to be stable, and I know of some people using it in a professional setting!
Still kind of a shameless plug of a pre-release, but strongly related, so [here you go](https://sgraf812.github.io/blog/2017-12-04-strictness-analysis-part-1.html)
You don't need any plugins for this, see my response.
Some good points here - I would like to add that in my experience with these labels, the composite “#foo . #bar” definitely worked, without any type signatures 
Cool, thanks. I'll look for the white paper. 
If you want another layer of strongly opinionated review, here it is: - /u/Gurkenglas shows you that you can merge `list` and `k` together to create a simplified function `list`. - I like your usage of `elem` for the filtering list, however if your filtering list grows, you may want to use `Data.(Hash)Set`. - About `pretty`, I'll pattern match on the `Order`: mapM_ print $ case rev of Desc -&gt; reverse src Asc -&gt; src In that case, you can even remove the `Eq` deriving from `Order`. - Finally, I want to discuss your usage of `State`. /u/Gurkenglas told that you may do everything without `State`, but I understood that you want to practice with monad transformers. In that case, I think that `State` is not the right monad: your `list` function only produces value, it never read the state. I think `Writer` may be more appropriate, because it produces a `Monoid` using `tell`. Using a `WriterT ([Dir], [File]) IO ()` you can append to the final result using `tell (newListOfDirs, newListOfFiles)` because `(a, b)` is an instance of `Monoid`: Prelude Data.Monoid&gt; ([1,2,3],"ab") &lt;&gt; ([4], "c") ([1,2,3,4],"abc") However, if your writer is producing list, it will use list concatenation and this is slow. But you can use a `DList` from `dlist` package or a `(Hash)Set` from `(unordered-)containers`. 
Let me save you some time https://info.sqream.com/hubfs/pdf/SQream%20DB%20Technical%20Whitepaper.pdf
Uhh. `#foo . #bar` doesn't even typecheck without orphans instances, so I have no idea how you even went about using that in a non-evil way. That was just a hypothetical example.
Ah, it was an orphan of course. I only commented on type inference
:+1
I really enjoyed reading that. I'm still having a question: you replaced two bangs by one and see improvements. However I understand the reasons why one bang pattern is not needed (strictness analysis find it), but not why the other is generating less efficient code. At the end of the document, you concluded: &gt; The takeaway is that using foldl is great as long as list fusion kicks in. It tooks me a while to understand "... compared to the hand written `go` recursion for which list fusion cannot kicks in".
&gt; However I understand the reasons why one bang pattern is not needed (strictness analysis find it), but not why the other is generating less efficient code. That's because GHC still can't unbox the `sum` part of `RunningTotal`. Consider a call like `go (RunningTotal undefined 0) []`: `go` immediately calls the `printAverage` function and prints the error, if we were unboxing `sum` this would crash due to `undefined`. &gt; It tooks me a while to understand "... compared to the hand written go recursion for which list fusion cannot kicks in". Will make this more explicit when I set things up with a DNS :)
 &gt; /u/Gurkenglas shows you that you can merge list and k together to create a simplified function list. but the simplified version of list didn't early bailout if the root isn't a directory? &gt; I like your usage of elem for the filtering list, however if your filtering list grows, you may want to use Data.(Hash)Set. That sounds great, I didn't even know that exist... &gt; About pretty, I'll pattern match on the Order: &gt; In that case, you can even remove the Eq deriving from Order. Makes sense, originally I plan to settle down on this one liner: mapM_ print $ (if rev == Desc then reverse else id) $ src but being able to get rid of Eq deriving sounds intriguing too, will give that a try. &gt; Finally, I want to discuss your usage of State. /u/Gurkenglas told that you may do everything without State, but I understood that you want to practice with monad transformers. Yes, I'm totally aware of State monad is not needed, and probably not the best choice for this example, the original plan was to both read and write to the State (by having a lookup table there to check ownership of each file...etc), along with utilizing unsafePerformIO, but those two were scraped halfway in order to make the sample concise. Aside from that, I had already wrote a couple of examples using Writer/Reader/RWS monad transformers, but I'll definitely check out Dlist and HashSet, thanks for mentioning it. As to the Applicative solution, although it's indeed a very novel approach to me, I'm a bit concerned about the simplicity over readability/comprehensibility issue, for that I spent 5+ hours trying to figure out how those lines work, can't even imagine how long it takes for /u/Gurkenglas to write it in the first place, hence I'm worried about it might be very challenging for the team to maintain and debug in the long run, in that case, being verbose might be a huge plus? That being said, I did hear a couple of times before that Applicative could replace Monad 90% of the time, and it's encouraged to do so, will look into it, starting [with this article!](https://stackoverflow.com/questions/23342184/difference-between-monad-and-applicative-in-haskell) :P
Datomic is weird and differs from relational databases in many respects. One of those differences does feel very functional: the database handle isn't a connection object which allows you to imperatively read and mutate the live data, it is (a proxy for) an immutable snapshot of the state which the database had at a particular time. I wish more databases worked this way.
Thankfully I upload everything I do to GitHub, here’s the link: https://github.com/PoquitoDondito/Haskell/blob/master/Haskell%20Code/Side%20Projects/Test%20Files/Test01.hs Sorry that it’s messy, I’m pretty new to Haskell.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [PoquitoDondito/Haskell/.../**Test01.hs** (master → 79697c9)](https://github.com/PoquitoDondito/Haskell/blob/79697c9b3dddce0db7fbfcd0477acf58a4d8e7f9/Haskell%20Code/Side%20Projects/Test%20Files/Test01.hs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dr2xoyz.)^.
I'll second the recommendation for criterion. It is actually really easy to use (and I am only an early intermediate Haskeller) and gives you some very nice output and confidence that what you are doing is correct. 
[Specialization?](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#specialize-pragma) GHC does this to some extent automatically afaik. (Turning [a] -&gt; a into [Int] -&gt; Int and the like)
Whoa I not even ever heard of Bang patterns before, will have to study them, and you can't really say that vectors are faster since you changed my logic, but wow does that dirty trick makes it faster? If I'm following your whole logic revolves around unsafe freezing the array just to index it faster with the `?!` operator that also makes the bounds checking for you. Is indexing an frozen (vector at least) that much faster to justify? 
As some new logo proposals have been made, a new poll will be launched when the current poll ends (next Friday). Candidates will be the winner of the current poll and other proposals made until Friday considered pertinent by the HIE team. You can submit new proposal here or in the [GitHub related issue ticket](https://github.com/haskell/haskell-ide-engine/issues/267). **Feel free to submit your logo !**
Somewhat simplified but branch prediction is mostly based on past jumps taken. * So for the first 1-2 jumps it might predict them wrong causing a stall leading to a performance loss. * Even for predicted jumps the check costs additional instructions. It can also cause other jumps to be predicted wrong by increasing pressure on the cache recording jumps taken. So while branch prediction lessens the impact a lot they are never free. 
If you want to make code unconditionally strict just use -XStrict.
You try to compile the source code from the interpreter (ghci). Go back to the regular prompt and try again!
Welcome to /r/haskell! The problem is that `$ ghc --make helloworld` is not something `ghci` understands. If you see a line beginning with a `$` in a tutorial, this usually means "run this line (without the $) in a terminal. So you probably wanted to run `ghc --make helloworld` not *inside* `ghci` but *instead* of `ghci`. `ghc`, the Haskell compiler, is a different program than `ghci`, the Haskell Interpreter. To run your code you can either use `ghci` to interpret it or use `ghc` to compile it to an executable (`.exe`) and then run that. The interpreter is really useful during development, as you can quickly test your code without having to wait for it to compile. The screenshot shows, you've started `ghci` from the terminal. Inside ghci, where you've tried to write `$ ghc --make helloworld`, you could instead write ghci&gt; :load path/to/Main.hs ghci&gt; main hello, world to run your code. You can also experiment in `ghci` by evaluating expressions, e.g. ghci&gt; 2 + 3 5 If your `Main.hs` file has changed, then `ghci` needs to be reloaded with the command `:r` to notice the changes. 
[Imgur](https://i.imgur.com/j5GXvyc.png)
Actually, if you want - there's also a video describing a couple of the use-cases, with performance numbers... https://www.youtube.com/watch?v=-d6wL5ukRJ4&amp;t=19m06s 
Took like a minute. `mapM_ print $ (case rev of Desc -&gt; reverse; Asc -&gt; id) $ src` does the job.
Can you elaborate a little bit more on the first example? I couldn't replicate it unless I introduce a dummy argument like in the second example ``` flip fix "" $ \loop s -&gt; do … ``` because the type of `flip fix :: b -&gt; ((b -&gt; c) -&gt; b -&gt; c) -&gt; c` calls for one. Thanks in any case for the neat trick.
Thanks. I think that that a discussion on this would hijack the thread. I want to test how farther I can go. I know that it is doable. But it would take some time.
I'm not actually sure if BangPatterns make it faster since you already used `$!`. It's just become instinct for me to use them whenever I want strict recursion. Indexing a frozen vector should be the same as indexing a mutable vector, I just froze it because the mutable API doesn't provide a `?!` operator. I'm sure that checking the bounds manually and using `unsafeRead` would be just as fast. You're right to say that my post doesn't prove vectors are faster. I was just experimenting with a bunch of stuff and got really excited when I found an improvement.
Can you show me the code? [My code](https://github.com/frostyrock/day5) shows no difference in performance between default and llvm backend. I wonder what's wrong with my code. I've used llvm@3.9 
Sure. I make no claims to Hakell brilliance though :) (I used ghc 8.2) https://gist.github.com/PhilArmstrong/5c5e8a19db13902f4c0e86905f4ebdee
How does one create those fancy new HTML based presentation slides?
&gt; but the simplified version of list didn't early bailout if the root isn't a directory? You are right. However you can write a simplified version (which is actually more generic and more compact) and call it with a toplevel version which bailout if the root isn't a directory. &gt; but being able to get rid of Eq deriving sounds intriguing too, will give that a try. You can pattern match on every constructor without an `Eq` instance. That's convenient and reduces the need for `Eq`. &gt; As to the Applicative solution, [...], can't even imagine how long it takes for /u/Gurkenglas to write it in the first place, hence I'm worried about it might be very challenging for the team to maintain and debug in the long run, in that case, being verbose might be a huge plus? It may be interesting for you to detail what you are finding "challenging" in the cited solution. Because the part which is "Applicative" is limited to the two usages of `&lt;*&gt;` and that's just another way of writing the `do` notation you wrote in your original piece of code. The rest is just usage of functions available in libraries (in this case, in the `Prelude`). &gt; That being said, I did hear a couple of times before that Applicative could replace Monad 90% of the time, and it's encouraged to do so, will look into it, starting with this article! :P `Applicative` does not replace `Monad` because `Monad`s *are* just less general `Applicative`s. The `do` notation works with `Monad`, so it also works with the more general `Applicative`. The `do` notation is only a convenient syntax in the language, but really that's the same as discussing the `map (+1) (filter odd [1..10])` versus `[i + 1 | i &lt;- [1..10], odd i]` syntaxes.
Great post, thanks for writing it! I've added a "further reading" section to the end of the original blog post I wrote linking to this one.
It seems to me the morally right way to fix this issue is to make the fields of `RunningTotal` strict, that is to say, make the program most closely model your domain. This is the solution that /u/snoyberg came to in his original blog post. Putting a bang pattern mysteriously in the middle of an auxiliary function seems far too magical to me.
yay, row polymorphism!!!
Oh, right. In the no-argument case it's just `fix`, not `flip fix`. Thanks, I'll correct it :)
Just a note. mzero = retry isn't mentioned when talking about MonadPlus. 
It turn out that `{-# INLINABLE #-}/{-# INLINE #-}` pragma doesn't work if function passed multiples times down the road. When I manually inline the update rule function, I got the expected performance.
I agree. But there are situations where this isn't possible without placing a bang in the right position. Like when implementing [`insertLookupWithKey`](https://hackage.haskell.org/package/containers-0.5.10.2/docs/Data-Map-Strict.html#v:insertLookupWithKey). Consider (sorry, untested): insertLookup :: Ord k =&gt; k -&gt; a -&gt; Map k a -&gt; (Maybe a, Map k a) insertLookup = Map.insertLookupWithKey (\_ a _ -&gt; a) f :: Int -&gt; (Maybe String, Map String String) f args = let k = someBigComputationToFindTheKey args in insertLookup k "hi" Map.empty main = do print $ map (fst . f) [0..1000000] The `insert` part of `insertLookup` is never needed, yet each suspended key `k` (and its closure) is kept alive, because a naive implementation of `insertLookupWithKey` would not `seq` the key argument in case the map was empty for just a lookup. Only when the map in which the entry is to be inserted is forced, `k` will be forced. Fortunately, that's not how `lookupInsertWithKey` is implemented. I guess the point I'm trying to make is to find the right position to place the bang and being able to debug why things don't unbox.
Thanks :)
The strictness analyzer also effects polymorphic values.
so, what exactly does LS(S) mean?
&gt; Sed Comes in handy surprisingly often!
Shoot us an email to jobs@obsidian.systems in early March.
It's the strictness placed on a tuple/record. For whatever reason (succinctness?) there are no commas separating the different fields. So the first `L` indicates that `sum` is used lazily, whereas the `S(S)` notation on the second field `count` means the wrapper around `Int#` is always unpacked, resulting in the `Int#` being used strictly.
I've done a little maintenance work on codex. I like codex but one caveat people should keep in mind is that it has all the caveats of a typical `ctags`/`etags` type program, it can't intelligently disambiguate name overlaps. So, sometimes it'll take you to the wrong place.
Unrelated, but you *really* should not be writing code in Notepad. Try https://atom.io/ or https://code.visualstudio.com/
thanks!
What do you mean by distributed systems? That's a fairly vague term and can be anywhere from HTTP libraries to CRDTs like you mentioned. Cloud Haskell is one of the canonical DS libraries for Haskell. It takes an Erlang like approach with all the beauty of types in tact. 
Haven't been updated in a while as I can see, but I remember playing around a bit with [cloud-haskell](https://hackage.haskell.org/package/cloud-haskell).
Thanks, somehow I didn't even try just `fix`. Anyway, the versatility of `flip` will never cease to amaze me.
Also [transient-universe](https://github.com/transient-haskell/transient-universe)
Hey thank you for your offer, I appreciate it :)
If you're concerned about Haskell's image, people might get the impression that the best way to deploy is to copy up source and run an ad-hoc repl, which is a bad image. Anyway, I'm not sure GHCi is really going to load code that much faster than an incremental Cabal / Stack build. They should be rebuilding the same modules, and the native code gen isn't *that* much slower than the GHCi interpreter (plus there are flags to make it fast).
The fact that the compiler is able to take your `foldl` and make the accumulator strict is nice… but why not help the compiler a bit and use `foldl'`? I am a bit worried of telling people to use `foldl` when it is basically never the right choice.
The only way I could see inference working for that would perhaps be having the type to the left of the function array infer the type in the right via equality constraints. 
I guess I was showing off your work :) Added a clarification regarding being explicit about the fact that we expect strictness.
Thanks :-)
Pattern matching works more or less the same even when you have two parameters. You do have a few options though. You could pattern match on the base, for example to handle 0: power b n = case b of 0 -&gt; 0 -- https://en.wikipedia.org/wiki/Zero_to_the_power_of_zero _ -&gt; undefined You could also pattern match on the exponent, which you'll probably have to do anyway for the recursive solution: power b n = case n of 0 -&gt; 1 1 -&gt; b _ -&gt; undefined Or you can pattern match on both at the same time using a tuple: power b n = case (b, n) of (0, _) -&gt; 0 (_, 0) -&gt; 1 (_, 1) -&gt; b _ -&gt; undefined Finally you could choose to nest the pattern matches: power b n = case b of 0 -&gt; 0 _ -&gt; case n of 0 -&gt; 1 1 -&gt; b _ -&gt; undefined
I am also a beginner but I would say: power :: Int -&gt; Int -&gt; Int power n 1 = n power n a = n * power n (a - 1) 
Think about what would happen if our `IsLabel` instance had some constraints, like (not the actual instance) ``` instance (HasField' field s a) =&gt; IsField field (s -&gt; a) where ... ``` this would mean that any time we use a label as a function, the above instance gets picked. That will then require the `HasField` instance, which in turn has a functional dependency `field s -&gt; a`, constraining what `a` can be. You're right in that there's an equality constraint taking care of inference here (through the fundep), but there need not be a concrete value for that constraint to be picked up!
Ah I should have read up more on `HasField`, yeah that instance plus the fundep is strong enough to recover inference.
https://hackage.haskell.org/package/lvish https://hackage.haskell.org/package/Quelea
&gt; Don’t do this blindly. If the call site is recursive, putting INLINE everywhere actually breaks performance. Use INLINABLE instead ! Why does this happen?
You should look at [Distributed computing](http://hackage.haskell.org/packages/#cat:Distributed Computing) tag instead. Some notable libraries/packages are: * http://hackage.haskell.org/package/transient-universe * https://hackage.haskell.org/package/cloud-haskell Cloud Haskell * http://hackage.haskell.org/package/distributed-process * http://hackage.haskell.org/package/distributed-closure * https://hackage.haskell.org/package/sparkle Apache Spark FFI * https://github.com/jcmincke/Blast inspired by apache spark * http://hackage.haskell.org/package/courier A message-passing library for simplifying network applications (Erlang like facilities) 
Is there something different between GHC 8.2 and 8.0? This file: {-# LANGUAGE BangPatterns #-} module Foo where foo :: Int -&gt; Int -&gt; Int foo start end = go 3000 start end go :: Int -&gt; Int -&gt; Int -&gt; Int go acc current end = go' acc current where go' acc current | current /= end = go (acc + current) (current + 1) end | otherwise = acc With ghc 8.0 I have: [...] go = \ (acc_axu [Dmd=&lt;S,1*U(U)&gt;] :: Int) (current_axv [Dmd=&lt;S(S),1*U(U)&gt;] :: Int) (end_axw [Dmd=&lt;S(S),1*U(U)&gt;] :: Int) -&gt; case current_axv of _ [Occ=Dead, Dmd=&lt;L,A&gt;] { GHC.Types.I# x_a1Fj [Dmd=&lt;S,U&gt;] -&gt; case end_axw of wild1_a1Fl [Dmd=&lt;L,1*U(U)&gt;] { GHC.Types.I# y_a1Fn [Dmd=&lt;S,U&gt;] -&gt; case GHC.Prim.tagToEnum# @ Bool (GHC.Prim./=# x_a1Fj y_a1Fn) of _ [Occ=Dead, Dmd=&lt;L,A&gt;] { False -&gt; acc_axu; True -&gt; go (case acc_axu of _ [Occ=Dead, Dmd=&lt;L,A&gt;] { GHC.Types.I# x_a1FD [Dmd=&lt;S,U&gt;] -&gt; GHC.Types.I# (GHC.Prim.+# x_a1FD x_a1Fj) }) (GHC.Types.I# (GHC.Prim.+# x_a1Fj 1#)) wild1_a1Fl [...] foo = \ (start_axs [Dmd=&lt;S(S),1*U(U)&gt;] :: Int) (end_axt [Dmd=&lt;S(S),1*U(U)&gt;] :: Int) -&gt; go lvl_s1FQ start_axs end_axt So no unboxing. But with GHC 8.2: $wgo_s28G = \ (ww_s28t [Dmd=&lt;S,U&gt;] :: GHC.Prim.Int#) (ww_s28x [Dmd=&lt;S,U&gt;] :: GHC.Prim.Int#) (ww_s28B [Dmd=&lt;S,U&gt;] :: GHC.Prim.Int#) -&gt; case GHC.Prim.tagToEnum# @ Bool (GHC.Prim./=# ww_s28x ww_s28B) of { False -&gt; ww_s28t; True -&gt; $wgo_s28G (GHC.Prim.+# ww_s28t ww_s28x) (GHC.Prim.+# ww_s28x 1#) ww_s28B [...] foo = \ (start_aSt [Dmd=&lt;S(S),1*U(U)&gt;] :: Int) (end_aSu [Dmd=&lt;S(S),1*U(U)&gt;] :: Int) -&gt; case start_aSt of { GHC.Types.I# ww_s28x [Dmd=&lt;S,U&gt;] -&gt; case end_aSu of { GHC.Types.I# ww_s28B [Dmd=&lt;S,U&gt;] -&gt; case $wgo_s28G 3000# ww_s28x ww_s28B of ww_s28F [Dmd=&lt;S,U&gt;] { __DEFAULT -&gt; GHC.Types.I# ww_s28F [...] So clearly Unboxed.
I know [some people are using my dejafu library to test distributed code](https://github.com/barrucadu/dejafu/issues/130) to some degree of success.
Did you compile with optimizations on for the first one? No worker/wrapper happening although those `Int`s are immediately unpacked is really odd.
Isn't LVish for parallel programming, not distributed systems?
I use the same command line for both (including `-O2 -fforce-recomp -ddump-stranal`), just the `nix-shell` changed. I may be doing something wrong somewhere.
&gt; The second case is our exit-case But in the code above it's the first case, it also wouldn't be correct if it were the second case :) &gt; It was a good visual feedback from tests that we needed to perform a left associative operation, which exactly what foldl does. Interesting! I wouldn't have gotten such insight from those tests if I were to implement `reverse` without prior knowledge.
This isn’t true. See [here](https://stackoverflow.com/q/47734825/465378) and [here](https://ghc.haskell.org/trac/ghc/ticket/14570). According to SPJ, this difference is intentional. Relatedly, as he mentions, fundeps have no witnesses in System FC, but families do, which also causes interesting problems under the hood like [this](https://stackoverflow.com/q/47732667/465378).
Ok, issue found. GHC 8.0 does not include the worker/wrapper in `-ddump-stranal`, we need to add `-ddump-worker-wrapper` to get an output similar to the one of GHC 8.2.
Oh, that's nasty. `ddump-simpl` is more reliable for that I guess. On second thought, strictness analysis runs multiple times and the first happens before WW. Doesn't explain the difference in log size, though.
You may by right. But in her PhD thesis she relates LVars to CvRDTs.
I’m not at al familiar with sqlite but you may want to check whether it supports array types of json. Postgrsql, for example, supports both allowing you to encode your list into a native array or jsonb array.
You might indeed get better performance without that line. The default implied module header is `module Main (main) where`. That makes everything except the `main` function private. GHC may be able to optimize private functions better, perhaps by inlining or specializing. 
Read about it here https://ghc.haskell.org/trac/ghc/ticket/14013 .
Using a relational database (which SQLite is), you would generally manage `tags` as its own table, along with a join table to model the many-to-many relationship. This is because you presumably want to be able to filter by tag (since that’s essentially the purpose of tagging in the first place), so you want to be able to query using an index. If you stuff all the tags into a single column, you wouldn’t be able to query articles by tag without a full table scan. As for how to model that in Haskell, I’m not sure. Modeling object *graphs* in Haskell is one of the things I don’t feel like I know how to do elegantly without some boilerplate. This would probably be a lot easier if Haskell had anonymous/row polymorphic records/sums.
From a database perspective, the right way to model that would be to have a separate table with two fields: a CreateArticle ID ("foreign key"), and a single article tag. You would use two queries to recover the CreateArticle from the database: one for the CreateArticle itself without the tags, and one for the tags. The query for the tags is a join query returning multiple rows, which you combine into a Haskell list. If order is significant, your tags table might have a third field - the position number of the tag in the list of tags. Then you use ORDER BY in the join query to ensure that the list is recovered in the correct order.
It adds a lot of strictness, but it doesn't exactly make all code unconditionally strict. See the [GHC User Guide](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#strict-by-default-pattern-bindings) for the details of what the `-XStrict` language extension exactly does.
Your first implementation of `reverseList` was better, except that you should use `foldl'` (from `Data.List`) instead of `foldl`. The basic rule is: always use `foldl'`, never use `foldl`. Your second implementation builds up unevaluated "thunks" for each element of the list, so it will cause a stack overflow if you run it on a large list. Unlike the fold, it doesn't even begin to construct the output list until it has already examined the entire input list.
You can create them with `slides`: * https://slides.com/ `slides` is just a convenient editor. You can use `reveal.js` framework directly: * https://github.com/hakimel/reveal.js/
This looks pretty rad. Thanks 
I've found this really interesting and useful: http://chimera.labs.oreilly.com/books/1230000000929/ch14.html
&gt; It may be interesting for you to detail what you are finding "challenging" in the cited solution. I spent a lot of time trying to wrap my head around following two lines. (subDirs, subFiles) &lt;- mapAndUnzipM list $ map (path&lt;/&gt;) $ filter (/=".DS_Store") $ dirContent dir return (dir : concat subDirs, concat subFiles) Things I found challenging: 1. Use of `mapAndUnzipM` and it's inner workings 2. The recursion caused by `mapAndUnzipM list` 3. the execution sequence with the line below that returned a pair In short, probably it's just that I'm not familiar with this kind of coding style yet, but I'm pretty sure I couldn't have come up with those two lines, hence worried it might be challenging to other people I work with too.
&gt; I am aware also that I could have a table for tags but I think this complicate things for no benefit.what do you think? It depends on how you use tags. If you want to search the tags using SQL functions (these are limited in SQLite of course, but postgres for example has great text search), then you may or may not want these in a separate table to help facilitate that. If these aren't actually used for anything until you are in Haskell land, then using a CSV string sounds fine to me. I would definitely keep the Haskell type as `[Tag]` though.
You could use [project-m36](https://github.com/agentm/project-m36) which features native support for [Haskell ADTs as database values](https://github.com/agentm/project-m36/blob/master/docs/tupleable.markdown) (including lists). 
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [agentm/project-m36/.../**tupleable.markdown** (master → f6d863a)](https://github.com/agentm/project-m36/blob/f6d863aa06950665db19c5c76ee4e9a7c058b639/docs/tupleable.markdown) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dr44c8d.)^.
Do you **need** to treat tags as an independent data model? Eg. Do you have a UI which lists all unique tags with total count of articles in each tag? Or something else like that? * If yes, then the correct way to do this would be to store articles and tags separately and build associations between them using a "join-through" table. If you don't do this, running queries on "tags" is going to be very difficult. * if no, then storing as a comma separated list in the articles table is absolutely fine. (take care of the edge-case where a tag, itself, has a comma). (do you need to search based on tags? Make sure your queries will use an index. In PG there are many ways to store tags in a single column and still make the queries fast. Not sure about sqlite). All that discussion on top is not directly related to Haskell. It's about DB design according to your use-cases. When it comes to mapping this data model to Haskell, having tags as `[Tag]` is the only sensible way to do it - else you aren't using the type system to your advantage. 
I would prototype with cloud Haskell, then use something lower level when you refine the system you want to build
... or use tag directly as foreign key of text type instead of joining with a tag_id of int type. Little if any performance impact and allows you to create article without knowing a bunch of tag_ids.
Haha I remember the countless times me and you complained this package didn’t exist! Glad one of us was motivated enough to make it! This looks great but I’m wondering what r/Haskell thinks of Proxy now that we have type applications. I am fine with the ambiguous type from removing them and I’m curious what y’all think. 
A couple decent workarounds were posted on [stackoverflow](https://stackoverflow.com/questions/30489597/input-length-restriction-in-ghci?rq=1) a while back
In my opinion, `TypeApplications` are syntactically nicer to work with than proxies, providing that you can get away with using them. This is sometimes tricky though, since they're necessarily "directional" in a way that proxies aren't. If there are a lot of type variables floating around, proxies may be less verbose. Regardless, preexisting libraries should probably stick to proxies for a while longer, in the interest of stability. 
IMHO production ready also means you aren't likely to find extremely embarrassing bugs, or things like "I'm too lazy to return `Maybe` here, so I'll just use `error`".
Shameless plug: https://github.com/NorfairKing/validity Does this, much more, and doesn't require proxies.
True. Though in Haskell I tend to find that bar is reached in most cases. I tend to have more trouble finding the library that does what I need than finding one that's safe.
How would you compare this to `checkers`? https://hackage.haskell.org/package/checkers
You'll need a bit extra in there to make sure your function is total (can handle all possible inputs); integers can be negative too :)
This is a trick to encode isomorphisms **tl;dr** data Functors :: (Type -&gt; Type) -&gt; (Type -&gt; Type) -&gt; Type type f &lt;~&gt; g = Functors a b -&gt; Type -- Witness data TagId :: a &lt;~&gt; a data TagComp :: (a &lt;~&gt; b) -&gt; (b &lt;~&gt; c) -&gt; (a &lt;~&gt; c) -- Type class class Iso1 (tag :: f &lt;~&gt; g) where from1 :: f ~&gt; g to1 :: g ~&gt; f newtype AsIso f (iso::f &lt;~&gt; f_other) a = AsIso (f a) instance (Iso1 (iso::f &lt;~&gt; f_other), Functor f_other) =&gt; Functor (AsIso f iso) instance (Iso1 (iso::f &lt;~&gt; f_other), Applicative f_other) =&gt; Applicative (AsIso f iso) instance (Iso1 (iso::f &lt;~&gt; f_other), Monad f_other) =&gt; Monad (AsIso f iso) where With this we can specify chains of isomorphisms, and derive through them with `DerivingVia` data Pair a = a :# a deriving (Functor, Applicative, Monad, MonadFix, ...) via (Pair `AsIso1` TagRepresentable) data TagRepresentable :: f &lt;~&gt; rep instance (Representable f, rep ~ (Rep f -&gt;)) =&gt; Iso1 (TagRepresentable::f &lt;~&gt; rep) where from1 :: f ~&gt; (Rep f -&gt;) from1 = index to1 :: (Rep f -&gt;) ~&gt; f to1 = tabulate
Request to please fix the following: * Add the package to the correct category on Hackage * Add some motivation of why the library is required and other libraries which solve a similar problem. How is this library different?
Neither - LVars are a concurrency tool. But concurrency, parallelism, and distributed systems are all related to each other. 
Project-m36 looks really cool! It looks like it doesn't support any of the standard DB backends though. That is often a hard requirement. Wouldn't it be possible to wrap those imperfect DB engines somehow, so that the interface is still conformant to the project-m36 goals? Some other popular libraries for modeling DB tables as Haskell types: * [persistent](https://hackage.haskell.org/package/persistent) * [acid-state](https://hackage.haskell.org/package/acid-state) * [groundhog](https://hackage.haskell.org/package/groundhog) Perhaps you could say that [haskelldb](https://hackage.haskell.org/package/haskelldb) also qualifies.
Thanks!
&gt; But in the code above it's the first case, it also wouldn't be correct if it were the second case :) Yes, its a bit confusing but what I've meant by "second case" was not the order in which it was written in code, but the order in which it was described in post. I didn't want to start with an exit case, so I've started with the second case to which I referenced as the first one that was described 
Thanks, good to know
From the perspective of [Port and Adapter architecture](https://herbertograca.com/2017/09/14/ports-adapters-architecture/), what you are trying to do now is putting "external details" to "domain logic". Which is a violation of the architecture. If I were you, I kinda welcome to have different types. One optimized for the domain, another optimized for the database details. 
I think for the most part, just use `-Wall` to get useful warnings, `-O2` to make stuff fast, and `-threaded` when building executables (as opposed to libraries) to enable parallel processing. Other than that the defaults are fine. Some people might argue against the default GC settings, but this isn't something most people have to worry about and is very dependent on your application.
I don't really understand why `-XStrict` doesn't interpret `(x,y) = foo` as `(!x,!y) = foo`. I would have expected it to bang every symbol I bind.
Unless you're writing single-file apps you should probably use Cabal and Cabal helps with these cases. I usually add `-threaded -rtsopts -Wall -O2` to `ghc-options` field, and `ScopedTypeVariables, FlexibleInstances, FlexibleContexts, OverloadedStrings, TupleSections ... a dozen other extensions` to `default-extensions` field. 
It's needless to say, all of this are default options in every "normal" project... I was just interested in that thing. Like, i was testing some parallel implementations(Par, Eval, MVar, etc) and in one case i get no paralleliz at all because function was performing in "perfect loop" and executed in one fell swoop, but I found -fno-omit-yields option that execute it in parallel. But yes, it was edge case, in usual projects I don't even think about all this stuff, default options performs perfectly well.
For my own projects, I usually add `-Wall` to all build targets, and `-threaded -with-rtsopts=-N` to executables/tests/benchmarks for reasonable parallelism. When I'm really obsessed with performance I'd use `-ddump-to-file -ddump-stg -ddump-cmm-raw` along with `-O2` and read the dumped STG/Cmm to figure out what's really going on under the hood.
Thank you so much! I had looked around on stack overflow but obviously not hard enough.
GHC by default compiles with **ZERO** optimizations (-O0). So for any program that you expect to run for more than 1-2 sec it's at least worth to pass -O (or -O2 if you don't mind the increase in compile time).
There's also the HdpH DSLs, which implemented the Par monad in a distributed setting. Details in the Haskell'14 paper: [The HdpH DSLs for Scalable Reliable Computation](https://dl.acm.org/authorize?N00315) , source: https://github.com/PatrickMaier/HdpH .
Yep, core is a nice way to see what's going on, but it can help only with pragmas optimizations(see if it inlines smt, did something, etc), but i was asking about ghc default options and what this beasts are.
Something like [this page](https://wiki.haskell.org/Numeric_Haskell:_A_Vector_Tutorial) maybe?
It's pretty similar. One difference is that `quickcheck-classes` is simpler to understand, both in terms of how to use it and how it works. `checkers` introduces some new typeclasses: `Model`, `Model1`, and `EqProp`. Users have to figure out what these do and write instances of them to test things. Checkers also relies heavily on `FlexibleContexts` and type synonyms, which makes type signatures difficult to read. Compare the checks for the applicative laws from `quickcheck-classes` and `checkers`: -- checkers applicative :: forall m a b c. (Applicative m, Arbitrary a, CoArbitrary a, Arbitrary b, Arbitrary (m a), Arbitrary (m (b -&gt; c)), Show (m (b -&gt; c)), Arbitrary (m (a -&gt; b)), Show (m (a -&gt; b)), Show a, Show (m a), EqProp (m a), EqProp (m b), EqProp (m c)) =&gt; m (a, b, c) -&gt; TestBatch -- quickcheck-classes applicativeLaws :: (Applicative f, Eq1 f, Show1 f, Arbitrary1 f) =&gt; Proxy f -&gt; Laws The both do the same (or nearly the same) thing, so choosing between them isn't something I'd lose sleep over.
This is first google ref, I saw it, but it's like general references and not detailed guide.
I'd say most people should use stack to tame Cabal rather than using it directly. Too many years of conflicting packages and nuking the cabal directory. Forwards, not backwards!
Cool package. I like that it provides a way to assert that something should fail a certain set of laws. I don't use hspec very often, so I don't typically see libraries that require using hspec (especially for property-based testing). It doesn't have the `Foldable` laws (yet), so it doesn't quite do everything that `quickcheck-classes` does out of the box, but that wouldn't be hard to add.
As much as I feel that `TypeApplications` are prettier to use, my gripe with them is more about principle. In nearly all current uses of type applications, it's because there's an invisible argument that GHC cannot solve for. While it's cool that we now have a way to explicitly pass invisible arguments, it's weird to me that we have to write functions with invisible arguments that cannot be solved for. I want it to be a visible argument since we always have to pass it explicitly anyway. If we had the [dependent haskell quantifier table](https://github.com/ghc-proposals/ghc-proposals/pull/81#issuecomment-336892922), I would want to write something like this: ordLaws :: (Ord a, Arbitrary a, Show a) =&gt; forall a -&gt; Laws Although that probably doesn't work because `a` isn't in scope. I think it would have to be: ordLaws :: forall a -&gt; (Ord a, Arbitrary a, Show a) =&gt; Laws Which looks pretty weird, and I'm not sure if dependent haskell will let you stick a constraint in the middle like that. Maybe there isn't anything I'd actually be happy with.
[Done](https://github.com/andrewthad/quickcheck-classes/commit/a102f2b5d40b5baf74fd5846d8c77c13f470bcc2). I'll release this to hackage once I add the `Traversable` laws.
Why? Is there a problem with Cloud Haskell we should be worried about?
Why not simply use Stack?
There are modules that use `ByteString`. There are no way to know how a given file name is encoded, so using `Text` just like that will make little sense.
TIL `DerivingVia`
Semi-objection! *After optimization* the result of `flip (:)` is *always evaluated* because it's just a cons cell. Don't use `foldl'`; even [ghc's stdlib here](https://hackage.haskell.org/package/base-4.10.1.0/docs/src/GHC.List.html#reverse) doesn't use a strict accumulator. I consider this is an interesting exception.
&gt; But they can't be unlifted until the specializer hits it, That's sad... And what my post was about!
&gt; LVars are a concurrency tool Is it that clear-cut? Lindsey herself wrote a blog post titled "[Yet another blog post about how parallelism is not concurrency](http://composition.al/blog/2014/11/24/yet-another-blog-post-about-how-parallelism-is-not-concurrency/)" in which she explains that concurrency is about structuring your program in terms of separate threads of control, while parallelism is about using multiple core or similar to make a program run faster, regardless of whether that program is structured as a single or multiple threads of control. In the last section, she clarifies that her LVar work is about both parallelism and concurrency, because programs that use LVars are written using multiple threads of control, but they use LVars so they can run in parallel and still be deterministic. I think this example demonstrates that it's not concurrency _versus_ parallelism, it can be both. They don't even talk about the same kinds of things: concurrency is about programs, while parallelism is about execution. Here is a classification I find more useful, because they all talk about algorithms. * A parallel _algorithm_ specifies which parts _should_ run in parallel. Whether they actually run in parallel depends on whether the hardware and the runtime system supports parallelism. For example, with GHC the default is `+RTS -N1`, which only uses a single core and thus no parallelism, but this doesn't change whether algorithms written using `Par` are considered to be parallel algorithms or not. * A single-threaded _algorithm_ is the degenerate case of a parallel algorithm which doesn't specify any parts which should run in parallel. In most languages, this is the default, but you could also imagine a system in which everything runs in parallel unless otherwise specified. Some clever compiler may choose to analyze a single-threaded algorithm and to run parts of it in parallel without changing the result, and in fact that's what instruction pipelining in modern CPUs does, but the fact that those instructions run in parallel doesn't change the fact that the algorithm itself is specified as a single-threaded algorithm. * A concurrent _algorithm_ is a (strictly) parallel algorithm in which not only there are multiple threads of control (even the expressions sparked by `par` could be considered separate threads of control, so that concept is meaningless), but also, more importantly, those threads of control interact by reading from and writing to shared mutable variables. Typically, some blocking and notification mechanism is available as well. * A distributed _algorithm_ is a (strictly) parallel algorithm in which the different threads of control communicate exclusively via message-passing. So once again, depending on the hardware and the runtime, those threads of control may or may not run on separate nodes, and that would not change whether the algorithm itself is distributed or not.
You can't use stack without having a .cabal file so I'm not sure what you mean.
You said &gt; default compile options is bad so I showed you a way to override defaults project-wide.
Indeed. Stack can obviously read .cabal but you seemed to imply that you'd need to use cabal. Or is it not possible to use Stack in this case?
Few hrs every day... I think you would have greater success finding free labour if you accepted that people have other responsibilities too. Allowing to test it at their own pace, requiring only a few hours a week would probably lead to more balanced responses (ie. not only only school students and millionaires). 
A reference is something like the [Data.Vector documentation page](http://hackage.haskell.org/package/vector-0.12.0.1/docs/Data-Vector.html), which gives a succinct explanation for each type and each function. Some libraries don't even have that, because types are often good enough for a reference (not for a tutorial), but the authors of the vector package have taken the time to diligently document every single function anyway. They even include the complexity of each operation, how many package even provides this level of detail? Since hackage is designed to display that style of reference, there isn't a standard place to put a guide, so few package authors bother to write one. In this case, they went the extra mile, they wrote a tutorial, and they linked to it from hackage. Now, you found that tutorial, you have read that tutorial, but clearly you didn't find what you were looking for. I can see that the last few sections of the tutorial are incomplete, so if what you're looking for would have been in those missing sections, or if there is another section which would have been useful but isn't listed in the table of contents, then it does make sense to ask a question here and even to encourage us to contribute that explanation to the tutorial. But that's not what you're asking. You are dismissing the hard work which the authors of the vector package have clearly put into this tutorial, and you are asking for an even more comprehensive tutorial to be given to you. Well, maybe you're coming from a big community in which every package has 100 tutorials for it, but in this small community, I'm sorry to tell you that this is already a lot more documentation than you're going to find for most packages. Users like you who are learning about a package for the first time are ideally suited for helping out with this documentation situation, because you know which parts of the documentation isn't clear enough, and once you do the research and figure out the answer, you have a good idea about what kind of explanation would have helped you-from-30-minutes-ago figure it out. So, since the tutorial is on a wiki, I encourage you to help complete the tutorial! Sure, doing the research and writing a tutorial isn't as easy as reading an already-existing tutorial, but we are a small community, so if we want our packages to have great documentation, we all have to pitch in.
&gt; Which looks pretty weird, and I'm not sure if dependent haskell will let you stick a constraint in the middle like that. You don't even need Dependent Haskell for that—that typechecks in today's GHC with `RankNTypes`! {-# LANGUAGE RankNTypes #-} f :: a -&gt; Show a =&gt; String f = show
Heh :) Is it really possible to learn Haskell in a few hours per week? Wasn't my experience. Open to other first hand experiences. 
Since 8.2 the strictness analyzer runs more often (there is a new final run to update one-shot information), so you get more output dumped.
Hi! I'm looking for help improving the performance of this routine for extracting the vertex set of a connected graph component. I'm going to inaccurately refer to this as the "subgraph" from now on because "connected graph component" is a mouthful. We'll also consider each vertex as being labeled with a unique integer value. types: data Edge = Edge Int Int deriving (Eq, Show) data Graph = Graph [Int] [Edge] deriving (Eq, Show) An edge is a pair of integers and a graph is a vertex list and an edge list. utility functions: -- Return a list of edges adjacent to a vertex edges :: Graph -&gt; Int -&gt; [Edge] edges (Graph v e) g = filter (\(Edge a b) -&gt; a == g || b == g) e -- Return a list of vertices adjacent to a vertex adjacent :: Graph -&gt; Int -&gt; [Int] adjacent graph g = map vert $ edges graph g where vert = \(Edge a b) -&gt; if a == g then b else a the BFS subgraph finder: -- subgraphGo graph queue seen -- "seen" are the vertices we've already visited, they won't be enqueued again subgraphGo :: Graph -&gt; [Int] -&gt; [Int] -&gt; [Int] subgraphGo _ [] _ = [] subgraphGo graph (x:xs) seen = x : subgraphGo graph nq (x : seen) where nq = (nub $ (++) xs $ adjacent graph x) \\ (x : seen) -- ^^ new queue is the existing queue plus the adjacent vertices minus the already seen vertices -- routine entry point subgraph graph v = subgraphGo graph [v] [] I'm guessing the list difference is a major slowdown, as is the adjacency checking. But how can I improve that while remaining idiomatic? Thanks!
Probably the best Haskell package I've seen in a year
Actually I'm observing that 8.0.2 with `-ddump-stranal -ddump-worker-wrapper` gives rouhly the same output as 8.2.2 with only `-ddump-stranal`
[Efficient Packed-Memory Data Representations - vector library ](https://haskell-lang.org/library/vector)
&gt; Haha I remember the countless times me and you complained this package didn’t exist! Interesting that from the [comments](https://www.reddit.com/r/haskell/comments/7j7fav/ann_quickcheckclasses/dr4ijow/) here it looks like there are two other similar packages. I guess this means Haskell libraries have a discoverability problems?
Depends on the depth and purpose. According to the last survey, many use it not at work, but for side/hobby projects. I'm pretty sure those people didn't learn it with "a couple of hours every single day until you are a guru" approach.
It appears that way. Although since one of them is called checkers no wonder it's hard to find. That's *obviously* going to be a board game implementation.
`megaparsec` is a big, industrial-strength library designed for programming language authors and the like. For something as simple as this I'd just use the standard `Text` functions, namely [`splitOn`](https://hackage.haskell.org/package/text-1.2.2.2/docs/Data-Text.html#splitOn) to handle the line and comma separators and [`breakOn`](https://hackage.haskell.org/package/text-1.2.2.2/docs/Data-Text.html#v:breakOn) to handle the `&lt;-&gt;` separators.
ahhh! That's so cool. I've never seen anyone do that. But I do need dependent haskell for my example since I actually want the type to be the argument (not a value of type `a`). So, with: ordLaws :: forall a -&gt; (Ord a, Arbitrary a, Show a) =&gt; Laws Then we have: &gt;&gt;&gt; :t ordLaws Integer Laws
random has this type: class Random a where random :: RandomGen g =&gt; g -&gt; (a, g) When you apply `snd` you're only taking `g` there - the generator. However, it's a class that lets you generate various types of random values so `a` could be `Int` or `Bool`, etc. Since you only discarded the `a` part, that could be anything and is therefore ambiguous. If you are in a do block, you can do something like: (_ :: Int, gen) &lt;- random $ mkStdGen 0 This will ensure the type isn't ambiguous. Just an example and I believe that will need the `ScopedTypeVariables` extension turned on.
Let's look at the types. &gt; :t random . mkStdGen $ 0 random . mkStdGen $ 0 :: Random a =&gt; (a, StdGen) Okay, so without further information, the type `a` is ambiguous, so neither should typecheck. So why does your first example work? &gt; fst . random . mkStdGen $ 0 &lt;interactive&gt;:1: warning: [-Wtype-defaults] • Defaulting the following constraints to type ‘Integer’ (Show a0) arising from a use of ‘print’ at &lt;interactive&gt;:1-27 (Random a0) arising from a use of ‘it’ at &lt;interactive&gt;:1-27 • In a stmt of an interactive GHCi command: print it 9106162675347844341 Ah, right, type defaulting! I think I know what's going on now. `random (mkStdGen 0)` has type `Random a =&gt; (a, StdGen)`, like I said. This is ambiguous, but this is fine, we might still learn some more information about `a` which makes it non-ambiguous, via a type signature for example. You then take the `fst` component, so the result has type `Random a =&gt; a`. This is still ambiguous. Now we have reached the top-level, and the type is still ambiguous, so the type-defaulting rules kick in. In this case the rules have picked Integer. The type is no longer ambiguous. This is what the `fst` example works. Next, let's try the `snd` example. We again begin with `Random a =&gt; (a, StdGen)`, but this time we take the `snd` component. The result has type `Random a =&gt; StdGen`, which is still ambiguous. But now the `a` no longer appears on the right of the `=&gt;`, so there is no way for any future type signature to make this `a` non-ambiguous! So the type-checker fails with an ambiguity error, before we reach the top-level, before the type-defaulting rules kick in.
Hmm.. So that might now work for me right now. I need some quick feedback till I reach 7-9 chapters. Then I'll open it to public and people can keep going through it at their own pace. 
You can use stack as well. I'm not aware of any stack projects that don't use the cabal file for stack; it increases compatibility and you'll find it everywhere else. That's why they didn't bother to mention stack.
I've never used Megaparsec, but my Parsec solution wasn't much different: import Text.Parsec -- ... type Graph = M.IntMap [Int] graph :: Parsec String () Graph graph = M.fromList &lt;$&gt; (adjacency `endBy` char '\n') where adjacency = (,) &lt;$&gt; integer &lt;* string " &lt;-&gt; " &lt;*&gt; (integer `sepBy` string ", ") integer = read &lt;$&gt; many1 digit I think a reason that there's no `integer` parser in `Text.Parsec` is that it would force some decisions that may not be universal: - all integers? or just non-negative? or just positive? - if we allow negative integers, is `+` required or optional for positive integers? - leading zeroes ok or not? I'm okay with the authors punting the problem down to the user; I'm comfortable being lazy (like `read &lt;$&gt; many1 digit`), but it's nice to be able to customize my definition of integer when I need to. I can see your point, though.
I don't think it's in GHC yet. Icelandjack has been working on upgrading deriving for a while (to great effect!)
Just be aware that your testers won't come from a balanced Haskeller demographics, but rather specific, potentially unrepresentative group if you demand high commitment and time investment without offering something comparable in return. But it's not my business, it's your enterprise.
I would guess that it wouldn't bang tuples because `fst (x, non-terminaging-computation)` would no longer terminate if both are strict?
&gt; Okay, so without further information, the type a is ambiguous `a` is ambiguous because it's could be any type that is instance of Random?
That sounds like exactly the sort of thing `-XStrict` is supposed to do...
This announcement motivated me to quickcheck the missing mtl laws ([open issue](https://github.com/haskell/mtl/issues/5)). There are a few variants I could think of, and I hope this will help ascertain which ones hold in practice. I started with `MonadState`, and since the simplest implementation is `s -&gt; (a, s)`, I can't use `Eq1` for equality. What do you think of parameterizing the properties by a more general equality (`a -&gt; a -&gt; Property`), to be passed explicitly to avoid `EqProp`?
That's correct. So you need to either use the value in a way that makes it unambiguous (like if you passed it to a function that only expect `Int` then the type inference will figure out that `Int` is the only thing `a` could stand for) or you can annotate the type to remove the ambiguity.
First of all, the content of `Text.Megaparsec.Char.Lexer` is not only lexers, but also some parsers of really common atoms, and that's why it is inside the lexer module... To parse this problem, you can use something as simple as: many $ do i &lt;- L.decimal string " &lt;-&gt; " is &lt;- L.decimal `sepBy` (string ", ") char '\n' pure (i, is) With `L` the `Text.Megaparsec.Char.Lexer` module. The lexer / whitespace thing is here when you need rock solid parsers, but mostly, for AoC, you don't really care because the amount of whitespaces is strict.
I mean, I'm not disagreeing at all with you on that :) I suppose I'd have to chat up #ghc to figure out why it doesn't work like that
&gt; y often, so I don't typically see librari Most of the validity packages have nothing to do with hspec. All of the hspec-related combinators have non-hspec variants.
What do you find deficient with above tutorial in terms of what you're trying to do or learn?
Latest release adds live-reloading and prettier error messages; Also a few new responders for loading responses from files. https://github.com/ChrisPenner/dumbwaiter/releases/latest
There is an even simpler approach, something like: case words curline of a : "&lt;-&gt;" : xs -&gt; (read a, map read xs) _ -&gt; error ("Can't parse " ++ curline)
You can get extremely close. I mean, technically, this is a nested fold, it just doesn't use `foldl` and instead uses some specialized folds (namely, `iterate` and `length`). You could write the same in terms of two `foldl` calls easily enough. module TwistyTramp where import Data.Vector (Vector, (!), (//), (!?)) import Data.Maybe (isJust) import qualified Data.Vector as V type Stack = Vector Int type Pos = Int type World = (Pos, Stack) sample :: Stack sample = V.fromList [0,3,0,1,(-3)] sampleStart :: World sampleStart = (0, sample) jump' :: World -&gt; Maybe World jump' (ix, st) = do inst &lt;- st !? ix let nix = (ix + inst) return (nix , st // [(ix, inst + 1)] ) result = length . takeWhile isJust . iterate (&gt;&gt;= jump') $ Just sampleStart
Right, Thanks !
Don't use cygwin with stack, ever. Use the instance of msys that comes bundled with stack. In fact, just uninstall cygwin now and save yourself the trouble. If you need a general shell env on windows, install `Git for Windows SDK `, which includes a version of MSYS2 and a package manager. It's sort of overkill, but it's the most thoroughly tested and fully featured MSYS2 distribution you're going to find. The rest of this is likely down to sorting out weirdness with `PATH`, which is, unfortunately, a puzzle only you can solve. I recommend taking the entire `PATH` variable, splitting it up into constituent pieces, and removing stuff until it works. Keep track of what you removed with each attempt, and use that to try to track down your problem. It sounds like a PITA, but it's actually not that bad.
Thanks for checking out the project! Project:M36 is a clean-room implementation of the relational algebra precisely because the other backends do not implement the algebra faithfully. As a basic SQL example, tables are not sets of tuples. Such pseudo-optimizations have serious repercussions in the algebra. Like Haskell itself, we believe that a strongly-typed, mathematically-coherent design offers greater opportunity for interesting optimizations, not fewer. Thus, Project:M36 is a standalone DBMS with an unapologetic emphasis on correctness. Due to this pursuit, Project:M36 already has some unique features (including optimizations) which would not be possible with a standard SQL backend such as: * trans-relational expressions - executing relational expressions across multiple past commits * relational-expression-based notifications - to know when relevant information has changed * transaction graphs - transactions can be branched and merged * ADTs as values - SQL databases don't offer sum or product types as values * server-side Haskell - we use GHC-as-a-service to compile database functions at runtime * relation-valued attributes - fundamental to the relational algebra, there is no reason that relations cannot be atomic database values within a tuple As you can see, Project:M36 is much more than a client-side API for Haskell, so I hope you will take a second look.
GHC ships with its own version of msys. You need to install dependencies via the pacman with the bundled msys. You can teach cabal to always use the msys environment by adding variables to your cabal config file as described in the haskell platform install instructions (for the appropriate choice of paths): extra-prog-path: C:\Program Files\Haskell Platform\8.2.1\msys\usr\bin extra-lib-dirs: C:\Program Files\Haskell Platform\8.2.1\mingw\lib extra-include-dirs: C:\Program Files\Haskell Platform\8.2.1\mingw\include I still have used cygwin, but not for a development environment as such -- but just for all the basic tooling with regards to shells, etc.
Why not just open it now? Mark it alpha and let folks poke at it. 
How about something like this: iterateAlt :: (Alternative f, Monad f) =&gt; (a -&gt; f a) -&gt; a -&gt; f a iterateAlt f = go where go x = (f x &gt;&gt;= go) &lt;|&gt; pure x iterateAlt (\n -&gt; if n &gt;= 10 then Nothing else Just (succ n)) 2 -- Just 10
Yes, its possible. 
my question was triggered by being pointed to [this library](https://github.com/Netflix/Hystrix). i've been impressed by the high level of abstraction the library operates and i was curious to find Haskell libraries that handled services at a similar level of abstraction. maybe the correct terminology for the domain is "remote call middleware"?. anyway i am happy that the question brought up interesting related content
If you're speaking from first-hand experience, how many total (calendar) weeks did you spend in learning Haskell? If you following any book, till which chapter did you reach?
Afraid of the amount of criticism it might draw, to be honest.
You could try the approach of developing in the open. Write public drafts, if they were helpful for you they could be helpful for other people, submit them to someone interested and get feedback
As a general rule of thumb, IIRC, if you find yourself in a situation where you want to use `::` to introduce a type, for something other than a top-level function, then `ScopedTypeVariables` must be turned on. 
Driving became easy when the first pioneers ceased to insist into making their own cars and learning about combustion engines.
If you're interested in user experiences regarding time spent learning haskell, there was a [thread](https://www.reddit.com/r/haskell/comments/3z9nsi/how_long_had_you_been_with_haskell_when_you/) a while ago, where I've also posted [my experience](https://www.reddit.com/r/haskell/comments/3z9nsi/how_long_had_you_been_with_haskell_when_you/cykfloy/). In my experience there are two major components to learning efficiency: 1. repetition is very important for learning, requiring a commitment over time. 2. the amount of information covered in a single repetition should be neither too small nor too big. - If it's too small, then the overhead of switching context (getting familiar with the subject again) is too high. - If it's too big, then the neurons for this area of information get over-stimulated, and it would be better to relax or switch subjects. 3. Interleaving periods of focused learning with periods of lying on the bed with closed eyes (saves energy from visual cortex), trying to reconstruct and classify the freshly new concepts, and meditating. I've had *really* good experiences with this. Interleaving 30-60min learning with 10-20min of such breaks sometimes significantly increases the time I can efficiently learn in a day while also making me feel less tired. ;) I think a few hours per week is definitely possible, allthough maybe not ideal regarding efficiency.
you can use a Set
Yes it definitely sounds great. But I am not asking to try to use SQL for relational algebra. I am asking about implementing a project-m36 relational datastore on top of some other storage backend besides memory or disk files. After all, memory and disk files are quite far from relational algebra. You are using them to store data, not to do relational algebra. Couldn't something similar be done for other persistence interfaces? At worst - just store everything as tuples in one giant table, similar to what you do in memory. But perhaps you could do even better than that with a SQL database, despite its flaws.
Doesn’t that need ViewPatterns?
Software engineers are more like auto mechanics than drivers.
Nope! 
What goes wrong here if you use `foldl'`? I understood that the only reason the GHC stdlib gets away with the traditional `foldl` as specified in the Report is that GHC is able to optimize away this simple special case and make it the equivalent of `foldl'`, but what you really want is `foldl'`.
even mechanics is easy if you don´t insist into making your own engines
If Lindsey says it's both, then it's both. And I did say that they are related. But at least superficially, it seems like LVars are talking specifically about concurrency. The race guarantees make just as much sense and are just as useful for a single processor switching contexts between threads as for multiple processors running in parallel.
I am against the idea that Haskell without the theory is any less pragmatic than that with theory. It's fine if you don't want to learn what you call "the theory" but the implication that Haskell is now more practical, as in it can express more real-life programs is wrong.
I'm not sure that main audience of `megaparsec` is programming language authors. There exist a lot of daily parsing task which you might want to perform And because of that you want well-tested library with good performance and nice error messages. Sure, `megaparsec` has excellent tutorial which can help you to start playing with your own language in a single evening. But personally I used and saw usages of `megaparsec` in more _production-like_ tasks much more rather than in implementing programming languages.
Doh, missed the case statement :) Think it reminded me of the Haskell on r/adventofcode which parsed the input with parse :: String -&gt; Map Int [Int] parse = Map.fromList . map parseLine . lines where parseLine (words -&gt; x : "&lt;-&gt;" : xs) = (read x, read . filter isDigit &lt;$&gt; xs) which I thing does need ViewPatterns?
Wouldn't that be... good?
That's right, here are the [examples so far](https://github.com/Icelandjack/deriving-via/tree/master/examples). I'm hoping to create a proposal once the paper is further along, Andres Löh said that this `Iso` stuff might split up into a separate paper so stay tuned
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [Icelandjack/deriving-via/.../**examples** (master → 4462083)](https://github.com/Icelandjack/deriving-via/tree/44620830e248733084b1e97cc9e2bf2d39b650bf/examples) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
Well, you can combine two approaches to have both (1) visible `Proxy` and (2) have more convenient interface :) Instead of lawsCheck (monoidLaws (Proxy :: Proxy Ordering)) you can write just lawsCheck (monoidLaws @Ordering Proxy))
You could probably find a way to do this with Stack, since it downloads &amp; unpack the source for everything. I don't know if it exposes any commands for locating the source files though, so you might need to upstream some fixes to get it to work. Alternatively, Hoogle generates a local copy of the docs, which link to a HTML formatted version of the source, so that's another option for viewing it locally.
Nice! Next step: implement library, which reads `LAWS` annotations across your project and generate such tests automatically for you for every instance :)
Haskell without the theory? What's left?
An even simpler approach is: readMany = unfoldr $ listToMaybe . concatMap reads . tails So for example: Prelude&gt; readMany "2 &lt;-&gt; 4, 6, 9" :: [Integer] [2,4,6,9]
For many things, WSL is a great way to go. It's actual Linux, not an emulator like Cygwin or MSYS2, running directly on the NT kernel, built by Microsoft into Windows 10. It works great. But you won't easily get GTK+ to work there though. For now, WSL is command line only. Microsoft says they're working on an X server that is just a wrapper for the standard Windows GUI.
[Joining Forces: Toward a Unified Account of LVars and Convergent Replicated Data Types](https://www.cs.indiana.edu/~lkuper/papers/joining-wodet14.pdf) I don't believe there's anything superficial of their relationship. 
Nice quickstart for GHCJS+Miso. Even though it says that it's opinionated, which is fine, I still would have liked a mention of GHCJS and Miso somewhere prominent right near the beginning, not only at the end. Small nitpick: Instead of rolling your own `maybeRead`, why not just: `import Text.Read (readMaybe)`
Lol. That’s more of a Joachim Breitner style approach.
If using Text makes little sense, how does it make *any* sense to use String (a linked list of chars) ;) I see your point, I've found System.Posix.Directory.ByteString and related packages that use ByteString. Looks ok for now, though not as elegant as System.Directory. Thanks
&gt; I still would have liked a mention of GHCJS and Miso somewhere prominent right near the beginning, not only at the end. Oh, I though it'd be clear we were using GHCJS from the fact that we were using Haskell. And I added a comment mentioning Miso :) I'm surprised hoogle didn't find `maybeRead` but I will definitely be using it here and in the future. 
I tried to install the Haskell Tool Stack on my Linux subsystem via `curl -sSL https://get.haskellstack.org/ | sh` but it keeps getting stuck at $ stack ghci Preparing to install GHC to an isolated location. This will not interfere with any system-level installation. PrepDownloaded ghc-8.0.2. Installing GHC ...
I actually use wine to build the Windows releases for a (non-gtk) program on Linux: https://github.com/nomeata/arbtt/blob/master/.travis-setup-windows.sh
 exec :: IO () exec = startApp App {..} where initialAction = NoOp model = 0 update = updateModel view = viewModel events = defaultEvents subs = [] This is just a style preference thing but this to me feels pretty magical and error prone. Why not: exec :: IO () exec = startApp App { initialAction = NoOp , model = 0 , update = updateModel , view = viewModel , events = defaultEvents , subs = [] } Or perhaps with a `$` before `App`, as I also dislike how `x {}` has higher precedence than `f x` and prefer not to take advantage of it. Maybe others disagree but personally I dislike half the record related extensions as they all seem fairly magical and they all seem to increase the likelihood of errors. I mean most of them just straight up encourage name shadowing or even do it for you.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [nomeata/arbtt/.../**.travis-setup-windows.sh** (master → 9cc767b)](https://github.com/nomeata/arbtt/blob/9cc767bdaf4e9e077fdfcfcb53a73a8a33a070fb/.travis-setup-windows.sh) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dr5vwnv.)^.
This is not true
Very little is more elegant with strings. They are just historically the default type for many core libraries.
new-build has basically solved this. Pretty much the only important difference between stack and cabal these days is that new-build is not yet the default, and stack has more convenient support for Stackage. If we just made new-build the default and added Stackage sugar for cabal.project files, I think the debate would pretty much be over and we could eliminate the controversy.
This call for help, is primarily to get private criticism. But, not going public is also about perception management. Too much public criticism, early-on, can kill something that might have turned out good after some fine tuning. 
I've been meaning to play around with Haskell on Windows for a while so I took a stab at it and got it to build and [run](https://i.imgur.com/DNT7yIg.png).. Here's my running commentary: I'm on 64-bit Win10, no Stack, GHC, Cabal, Haskell Platform, Git or MSYS2 installed from beforehand. I installed the 64-bit Windows version of Stack from [here](https://docs.haskellstack.org/en/stable/README/). Ran `stack init` from the project folder. It can't find aeson&gt;=1.2 in any of the Stack snapshots, so I changed it to just aeson (with no version constraints) and Stack chose lts-9.18 (with aeson-1.1.2.0). Ran `stack setup` to install GHC. It failed during unpacking of the archive, [this](https://github.com/commercialhaskell/stack/issues/1917) indicates it might be because of anti-virus. Opened a new cmd prompt as admin, ran stack setup again and it succeded (there are other workarounds in that link as well). I ran `stack build` and it failed a couple of times as expected (missing pkg-config, cario, pango, etc) and I would install the missing parts one by one. This should install what you need: stack exec -- pacman -Syy stack exec -- pacman -S mingw-w64-x86_64-pkg-config mingw-w64-x86_64-glib2 mingw-w64-x86_64-cairo mingw-w64-x86_64-pango mingw-w64-x86_64-gtk3 Also, at some point during stack build I got this fun error message: removeDirectoryRecursive:removeContentsRecursive:removePathRecursive:removeContentsRecursive:removePathRecursive:removeContentsRecursive:removePathRecursive:removeContentsRecursive:removePathRecursive:removeContentsRecursive:removePathRecursive:removeContentsRecursive:removePathRecursive:removeContentsRecursive:removePathRecursive:DeleteFile "&lt;path to GTypeConstants_hsc_make.exe file in a temp folder&gt;" (The process cannot access the file because it is being used by another process.) I just re-ran `stack build` and it proceeded. It took a while but it built successfully. However it fails to run with `stack exec ProjetSyme.exe` with the Windows popup error message: The procedure entry point inflateValidate could not be located in the dynamic link library C:\Users\username\App\msys2-20150512\mingw64\bin\libpng16_16.dll I found a reference to the same error in gtk-rs [here](https://github.com/gtk-rs/gtk/issues/539) without a solution. Not sure what's going on. So instead I just copied ProjetSyme.exe and the src directory (and created an empty onche.cod file) to the msys2 folder with the DLLs (C:\Users\username\AppData\Local\Programs\stack\x86_64-windows\msys2-20150512\mingw64\bin) and it runs.
It won't all be useful criticism. 
Nothing that I know of, just seems like you would want more flexibility over time
Probably not. What are the counterexamples? 
I've always gotten the feeling that I need a thorough knowledge of category theory to attain even "beginner level" in Haskell. I think "without a primary emphasis on theory" would be a more useful (if not accurate) description of a good way to approach Haskell. But definitely a much less catchy title.
The [wiki page on ScopedTypeVariables](https://wiki.haskell.org/Scoped_type_variables) gives examples of how to avoid many usages of `ScopedTypeVariables`, but generally speaking the only time I've had to use them is to put the type declaration on the left hand side of the `&lt;-` operator in do notation or if I need the type variables of a value declared somewhere inside a function to match up with the type variables of the top level function definition. Most other places you can just inline the type declaration, possibly wrapping in parens if necessary.
Are you familiar with this function? fmap :: (x -&gt; y) -&gt; [x] -&gt; [y] Try replacing x and y in this function and see if you can get it to look more like pairList. Keep in mind that these two types are equivalent in haskell: h -&gt; i -&gt; j (h, i) -&gt; j
 pairList :: (a -&gt; b) -&gt; (b -&gt; c) -&gt; [a] -&gt; [(b,c)] So pairList is a function with the following 3 arguments: first: `a -&gt; b` or a function from `a` to `b` second: `b -&gt; c` or a function from `b` to `c` third: `[a]` or a list of items of type `a` That returns something of type: `[(b, c)]` or a list of `(b, c)` pairs. Remember that `a`, `b` and `c` can be anything, say `a = Int`, `b = String`, `c = Bool`.
Maybe my GHC-8.2.2 is misconfigured, but this doesn't seem to work for me: ./src/Main.hs:368:14: error: • Found hole: _asd :: a0 Where: ‘a0’ is an ambiguous type variable Or perhaps ‘_asd’ is mis-spelled, or not in scope • In the first argument of ‘show’, namely ‘_asd’ In the first argument of ‘shouldBe’, namely ‘(show _asd)’ In a stmt of a 'do' block: (show _asd) `shouldBe` 2 • Relevant bindings include spec3 :: SpecWith () (bound at ./src/Main.hs:365:1) | 368 | ( show _asd) `shouldBe` 2 | ^^^^ Note how `a0`is lacking any mention of `Show`.
While a lot of folks around here will not like the idea of Haskell "without the theory", I do think that title/concept has a way of trying to pull down an incorrect negative stigmatism about Haskell. So, even though I have *learned* to truly appreciate the theory, I realize that it's an acquired taste for many and you need to start somewhere. Keep up the good work.
Oh I see, but how do I combine everything together into one function?
Hello All. I was hoping to contribute to the documentation of GHC, predominantly the `base` package. and I want to be able to run haddock to check if the pages render correctly. Currently I'm at a loss and am beginning to think it's probably easier to contribute to GHC proper rather than the docs. I have prepared my machine (OSX) and cloned, booted, configured and made GHC. Everything works fine with the compiler: $ git clone --recursive git://git.haskell.org/ghc.git $ cd ghc $ ./boot (snip) $./configure (snip) sphinx-build : /usr/local/bin/sphinx-build xelatex : /Library/TeX/texbin/xelatex (snip) Tools to build Sphinx HTML documentation available: YES Tools to build Sphinx PDF documentation available: YES (snip) Okay, so the configure script tells me my environment is ready to go to make the docs. Okay next step: configure the build $ cp mk/build.mk.sample mk/build.mk I add the lines: HADDOCK_DOCS = YES BUILD_SPHINX_HTML = YES BUILD_SPHINX_PDF = YES to build.mk, perfect. Make works, and everything is (supposed to be) good. My stage 1 and stage 2 compiler compile, no errors, everthing is ok. Everything is built, and as far as I can tell everthing works, except the documentation. I try the example command [here](https://ghc.haskell.org/trac/ghc/wiki/Building/Docs) but there isn't even a makefile in that directory! I traverse up to &lt;ghcroot&gt;/ghc/libraries and try again $ make html (snip verbose compilation output) make[2]: *** No rule to make target "html_libraries". Stop. okay, so where has the needed target gone? The troubleshooting guide says this is most likely because I have disabled building docs in one of the *.mk files. That's not true, and I have all of the prerequisites. I've also tried invoking haddock directly, but it gives me parsing errors, and I would have to start modifying the source code to get it working. Does anyone know how I can just build the documantation for ghc base?
Stress should be 'Haskell without *the* theory'. You can't do something without theory. But you definitely don't need hell complicated voodoo magic to learn with some nice Haskell.
So you are going to need something like this: pairList first second third = ... `first`, `second`, and `third` are your arguments that have the types I specified before (feel free to rename them to something better). `...` is the actual body of your function. Here's an example of such a three argument function: dontPairList first second third = (first, second, third) Now this has type: dontPairList :: a -&gt; b -&gt; c -&gt; (a, b, c) Which is not you want, but it should give you an example of a working three argument function, without spoiling the fun!
Ok so I know (x -&gt; y) -&gt; [x] -&gt; [y] is a map function from someone else's comment, so my function will use two maps + another list? Is this right? I'm sorry, I'm really new to this.
Keep in mind in that example x and y are abstract types, they are not variables that will contain values. In the function declaration you would have something like `map f a` where f would be the function from x to y, a the list of x. You would then apply f to each element of the list. The implementation of your function would be similar, but you need to also get the value for the other element of the pair.
That's `map`, not `fmap`. And it may be unclear to a beginner what you mean by "equivalent" in that context.
Hi all, I am planning to start my very first Haskell project and I came across ["Write Yourself a Scheme in 48 hours"](https://en.wikibooks.org/wiki/Write_Yourself_a_Scheme_in_48_Hours) and ["Write You a Scheme"](https://wespiser.com/writings/wyas/00_overview.html). The latter seems to be an updated version of the former. My question is: will it be fine to jump right into the updated version of this exercise or will I be missing some important parts from the original one? Thanks a lot!
That has the disadvantage of crashing on a error.
Since you are new to Haskell I would recommend you first try to write the function pairItem. pairItem :: (a -&gt; b) -&gt; (b -&gt; c) -&gt; a -&gt; (b,c)
Baby, bathwater, etc
I suggest installing a full msys env separate from stack. It works perfrctly fine.
This is an advent of code exercise, which is some sort of speed competition. For 'real' code, this would be horrible!
In this exercise you need to build a graph, so a `Map Int [Int]` is what you would need. That is the reason I chose to separate the first value from the others in the output ...
&gt; how does it make any sense to use String (a linked list of chars) The only reason it makes sense is because `String` came first. It is wrong though :/
If you're ever stuck getting started writing a haskell function, one way to start is by pattern matching whatever you have. pairList fAB fBC [] = ... ? pairList fAB fBC [a : as] = ... ? try to build one element of the list here, then make a recursive call to do the rest There are ways to write this function without directly pattern matching on the list, but I think it'll be easier to work backwards from a pattern-matched version once you have a solution at all.
Nice! I tried to add live-reloading on my own last week for funsies and couldn't figure out how to do it. Glad to see where I went wrong :) I'll have to show it to my co-worker tomorrow!
That explains it, thanks!
`weirdInterlaveIO` has none of the properties I normally want from `unsafeInterlaveIO`! Granted, I've only used it once, but I did not want to perform any effects until *after* I knew that I needed the pure value. The code was this: lookupOrCreate :: (MonadMetrics m, MonadIO m, Eq k, Hashable k) =&gt; (Metrics -&gt; IORef (HashMap k a)) -&gt; (k -&gt; EKG.Store -&gt; IO a) -&gt; k -&gt; m a lookupOrCreate getter creator name = do ref &lt;- liftM getter getMetrics -- unsafeInterleaveIO is used here to defer creating the metric into -- the 'atomicModifyIORef'' function. We lazily create the value here, -- and the resulting IO action only gets run to create the metric when -- the named metric is not present in the map. newMetric &lt;- liftIO . unsafeInterleaveIO . creator name =&lt;&lt; liftM _metricsStore getMetrics liftIO $ atomicModifyIORef' ref (\container -&gt; case HashMap.lookup name container of Nothing -&gt; (HashMap.insert name newMetric container, newMetric) Just metric -&gt; (container, metric)) I explicitly only wanted the effects to fire in the case that the `Nothing` branch is evaluated. This may have been a better use case for `unsafePerformIO`, though, as then I could scope the `newMetric` variable under the `Nothing` branch.
Fwiw, fixIO uses unsafeInterleaveIO internally, so I don't think that counts.
I'm sorry you have that feeling. I'm a professional Haskeller and I don't use category theory at all in my day-to-day work. You can be a very productive programmer without knowing any CT at all!
[Yup, good catch](https://www.stackage.org/haddock/lts-9.18/base-4.9.1.0/src/System-IO.html#fixIO) fixIO :: (a -&gt; IO a) -&gt; IO a fixIO k = do m &lt;- newEmptyMVar ans &lt;- unsafeInterleaveIO (takeMVar m) result &lt;- k ans putMVar m result return result 
Well, a lot of operations use unsafe operations internally. My question I guess is if you can do it without *directly* using unsafe operators (that is, it can be written with safe operators from `base`).
That's why I asked the question, and named the operator "weirdInterleaveIO".
Yes, this is a problem, which is why I think ghc should ship with the appropriate llvm version. 
&gt; Driving became easy when the first pioneers ceased to insist into making their own cars and learning about combustion engines. `Using software` became easy when the first pioneers ceased to insist into `writing their own programs` and learning about `algorithm`.
Let me just add `totallyNotUnsafeInterleaveIO = unsafeInterleaveIO` to base ;)
Furthermore, because it's a set of ints, there's the [IntSet](https://hackage.haskell.org/package/containers-0.5.10.2/docs/Data-IntSet.html) type which should be even better here.
Trying breaking it down a bit further and writing a function for `(a -&gt; b) -&gt; [a] -&gt; [b]`. Then write a function for `(a -&gt; b) -&gt; [a] -&gt; [a,b]`. At this point, the composition should reveal itself to you. 
Yeah, I probably dived into the deep end too soon. I'm not even sure how I would go about doing this simpler version. Do you know if there are any good resources that would help me out with this?
At Linux it should be not worse than String, and other platforms have Only One Encoding. btw Text internally uses UTF-16 to store the strings, so at windows it could even be directly passed to/from Win32 calls.
Does the String overhead matter when you anyway have to use IO?
Why didn’t you use an `MVar` together with `withMVar`? Performance?
Well, it depends. I need to cache directory trees in memory to perform various operations on them (diffs, checking for files, etc). So yeah, you could say that converting String to Text or ByteString at listing time should suffice.
So it might help to replace `a`, `b` and `c` with concrete types, and write that function. Most likely you will end up with the exact same function! So do you have any idea how to write: pairItem :: (String -&gt; Int) -&gt; (Int -&gt; Bool) -&gt; String -&gt; (Int, Bool) If we break down the above type signature we get: first argument: `String -&gt; Int`, a function that converts strings to ints second argument: `Int -&gt; Bool`, a function that converts ints to bools third argument: `String`, a string result: an `Int` and a `Bool`. Given the above try and fill in the `...` below: pairItem stringToInt intToBool string = ... Remember that you need to give back an `Int` and a `Bool`. I'll give you another hint by solving a somewhat related problem for you: applyTwice :: (Int -&gt; Bool) -&gt; Int -&gt; (Bool, Bool) So we have an `Int -&gt; Bool` argument and an `Int` argument, and need to give back two `Bool` values. Here is how we can do that: applyTwice intToBool int = (intToBool int, intToBool int)
I suggest you take whatever textbook you're using, and revisit the chapters you have worked through so far. I have a hard time believing that this is the first thing they present you wrt type signatures.
Nice, good idea. I've dropped you email. See ya ;)
I would suggest first reading my other reply that talked about how to implement `pairItem` before reading through this. But anyway: So you are correct that `map` has type `(x -&gt; y) -&gt; [x] -&gt; [y]`, and remember that `x` and `y` can be replaced with absolutely anything you want, so for example `map` can have type `((Int, Int) -&gt; Bool) -&gt; [(Int, Int)] -&gt; [Bool]` if that is what you want. Perhaps you could try and pick out the right `x` and `y` types to get you something that will help you.
Hi, sorry for the delay, been dealing with other things. I wasn't aware of an "outcry" against version bounds, but it certainly wouldn't surprise me. The difference is simple. In the case of cassava and integer-gmp, superficial changes were made which broke some compiles. The integer-gmp case was much worse, because it broke configurations that previously worked. I am glad this is resolved. With cassava, it was just a flag name choice. It could easily be a different flag name with 0 change to functionality or maintainability. With integer-gmp, it is just a substitution of syntax sugar. 0 change to functionality or maintainability. The difference is that adding version bounds is not a superficial change. It is a substantial difference in the amount of future maintenance. Off the top of my head: 1. Herding version bounds can take significant maintenance, on the part of the maintainer or hackage trustees 2. Storing version bounds in cabal metadata is rather non ideal, and there isn't adequate tooling to make it sustainable / tolerable. 3. There is no clear procedure to set broad and correct version bounds. Once again, lack of adequate tooling. So, you are drawing an analogy between two very different circumstances. I want to do a blog post which clearly explains why version metadata in a package format doesn't make much sense, and what approaches do make sense. However, atm, I have many bigger fish to fry.
The idea I was trying to get at is that it wasn't intended `fixIO` would leak `unsafeinterleaveio`. Also, `fixIO` has better defined semantics.
Hijacking your comment, if you don't mind. Because list is a functor, isn't map and fmap equivalent in this context? 
I decided to actually test this out on a computer, instead of blowing smoke out my ass. In the absence of actual type variables, you need `ScopedTypeVariables` to do the following: Do-notation `&lt;-`: do a :: Int &lt;- stuff blah Do-notation `let`, on the LHS: do let a :: Int = 5 blag Pure `let` and `where`, on the LHS: k = let j :: Int = 5 in j + p where p :: Int = 8 You **do not** need it for stand-alone type signatures (top-level or otherwise): q :: Int q = 1 + r where r :: Int r = 2 Nor do you need it for inline type signatures on the RHS: let z = 8 :: Int in do let a = 5 :: Int b &lt;- return (6 :: Int) burp where y = 4 :: Int ----- In summary, apart from when you want type variables to be "inherited" in function bodies, *you need `ScopedTypeVariables` only when you want to specify the type signature off a pattern*, patterns in Haskell being introduced by `let`/`where`, and `&lt;-`. With my limited understanding of type theory, it makes little sense to me what **scoped** type **variables** have to do with an independent, monomorphic type such as `let x :: Int = ...`, thus my slight confusion (that, and the fact the GHC tells you precisely when to turn `ScopedTypeVariables` on).
Good advice but, on the second line, those square brackets should be parentheses.
Equivalent really isn't the right word, no? Sure, one is curried and one is not, but they can't be substituted for one another in Haskell.
They are. But the comment above makes it look like `fmap` has the type signature of `map`.
Right. I'd say isomorphic instead.
Thanks, I should've clarified I didn't mean that with regard to the comment but just for my own better understanding.
Have you gone through list comprehension in your textbook? Try following that line of thinking and perhaps review it to give you a hint of possible implementation of this function.
[removed]
If you're using `stack`, you're using `cabal`-the-library anyway, including the functionality of specifying `ghc-options`. Whichever one you choose to use, creating a `.cabal` file is enough to to do what /u/semanticistZombie was suggesting. In general, **do not** use naked GHC to build software. Create a `.cabal` file and use `cabal`-the-program or `stack` to build it.
The common wisdom seems to be that `-O2` should be a very last resort, as it significantly increases build times, and almost never actually makes the binary faster/smaller. Use it for your final release binary, not during development.
Try to make this simpler function: pair :: (a -&gt; b) -&gt; (b -&gt; c) -&gt; a -&gt; (b, c) Then you can do pairList :: (a -&gt; b) -&gt; (b -&gt; c) -&gt; [a] -&gt; [(b, c)] pairList fun1 fun2 xs = map ... -- can you figure out what goes here?
I think you misunderstood me. I did not make an analogy. Nor did I want to discuss version bounds. I pointed out an incongruity between two very different attitudes towards pull requests, coming form the same camp. Both attitudes strike me as not in keeping with open source norms, but in opposite directions. That's all I was talking about.
Mmmm, it was the second point I was driving at. What I tend to do is `stack new` for my programs and then edit the stack.yaml and .cabal as apropos. Does `cabal install` still put things in a global location? I remember all sorts of conflicts. I'm not sure why the downvotes, do people not remember the headaches that were dependencies two to three years ago?
Ok, well I don't think anyone would disagree that different PRs should get treated differently. So, I see nothing incongruous here. Nowhere was it said that all PRs should be accepted. Nowhere was it said that maintainers have no right to exercise their prudence when accepting or rejecting PRs. However, it was said that it's pretty crappy to not accept a change that has only upsides and no downsides. Especially when you did not write the package, and just inherited maintainership of it.
If we have a Functor data StateF s x where Get :: () -&gt; (s -&gt; x) -&gt; StateF s x Put :: s -&gt; (() -&gt; x) -&gt; StateF s x then we can get a monad with state operations as `Free (StateF s) a`. How would I have to instatiate `g` in `Cofree g a` to get for example the `Store` comonad observations?
You just said (elsewhere) you considered the cassava thing water under the bridge and you weren't going to keep arguing about it. Now you seem to want to keep arguing about it. Sorry, I don't.
I did not mention cassava in that comment. I feel like this is a discussion, I'm sorry you feel like it is an argument. I suppose I have put you on the defensive because I think your analogy is quite flawed. I am confused why you think I misunderstand, I'm pretty sure I do understand your point. I just disagree with it, because from my perspective the two things are very very different. If there were any good reasons for the changes we are discussing, then the discussion around refusing to revert the changes would be very different.
Sure. Just pointing out another approach. Since `readMany` is such a dead simple parser, you could use that and then peel off the first element after the parse.
I wouldn't use -O2 during development because it increases compile times significantly. However it's outdated information that -O2 doesn't make a difference. I think it was true in older versions of GHC. The user guide for versions &lt; 8.* still stated it as well but that was removed in 8.0 after it has been shown that -O2 gives almost always better performance.
The `MVar` is still needed by the other thread!
That's a bit like his gaslighting, pretentious apology, followed by a return to shit talking. It's almost as if… oh never mind.
&gt; Even when you wouldn't do the same thing in a submitter or maintainers shoes, I think there is never any reason to get mad at them for acting in a totally normal way in keeping with open source norms. When it negatively impacts the users of your software I think it is reasonable to get mad. Sure, you could say "then fork", that would be with opensource norms. There isn't currently a mechanism for that with hackage packages. Currently, the namespace is entirely controlled by the package maintainer and hackage trustees.
Can you just use any old Windows X server?
Not sure I follow, what other thread? My point was that the "clean" way to do atomic var updates with IO would be to use `MVar/withMVar`, not `IORef/atomicModifyIORef'/unsafeInterleaveIO `.
That's right :)
Installing Haskell stack on Windows itself was something of a project for me. Not big, but just that it relies on it's root being in the C: drive, specifically in c:\sr. This was by default put there, and hence every other package installation went into C: drive. Since I got to this, and wanted my stack's root to be in a different drive, I worked on a solution for this. And put my learnings here: https://blog.ramdoot.in/installing-haskell-stack-in-windows-7c8fd2c79f I hope this helps you too, atleast to start with stack (which, by the way, is the best method to start, because it already comes with MSYS and the tools required for building). 
In the current implementation state: `foo = 1.2.*` is morally `foo &gt;= 1.2 &amp;&amp; &lt; 1.3` but `foo ^&gt;= 1.2.3` gives the tighter bound that presently acts like `foo &gt;= 1.2.3 &amp;&amp; &lt; 1.3`. On the other hand `foo = 1.2.3.*` gives the tighter still bounds `foo &gt;= 1.2.3 &amp;&amp; &lt; 1.2.4` which is a tight bound both above and below on patch level releases, not on major versions. The goal is a bit different though, one could envision a future where we have some sort of `allow-soft-newer` bounds that allow you to use newer-than-known-good bounds for all ^&gt;= bounds, but which don't try to build where hard upper bounds are known. In that setting ^&gt;= indicates a floor version we know we work with and a soft upper bound, while `&gt;= &amp;&amp; &lt;` or `= x.y.*` gives hard bounds for known incompatibility. In this setting, the functionality of ^&gt;= can't really be replicated with .* or &lt; bounds, as the meaning of the implied upper bound is different.
Saying that your opinions are extreme and can safely be ignored is not really shit talking. It's just reality
Yeah mate. You got me again!
/u/guibou , I think that was my question (poorly stated originally, so thanks for helping me clarify my thinking): *why* are those parsers of common atoms bundled in the Lexer module, and not in something like `Text.Megaparsec.Atoms`?
A reply to all in this subthread: yes, I know Megaparsec is vastly excessive for just this little job. But I'm using AoC to learn about some these tools on some small examples. That's useful! For instance, I hadn't come across View Patterns before, so thanks for that!
Lol. That’s more of a Joachim Breitner style approach. 
Do you ever really *stop* learning Haskell? The language continues to change, and there's always more. I've been writing Haskell casually for about 2 years. Before that I did briefly 'try' it for like 4 months or so and gave up.
I'm not familiar with this field but you might be able to find answers at the [data haskell site](http://www.datahaskell.org/). specifically [this page in the docs](http://www.datahaskell.org/docs/community/current-environment.html).
Yes you can. But those are slow and memory-intensive, because they are built on emulation layers. Microsoft says they are working on an X server that exposes kernel-level graphics primitives and allows Linux GUI applications to be equal partners with Win32 applications. Hope that pans out.
For the people who downvote, please remember that downvoting means "this post not constructive to the discussion" and not "I don't want the statement of that post to be true".
Is this a case of letting the tail wag the dog? The first sentence of the post says that &gt; My initial motivation for -XDerivingVia was deriving across isomorphisms. If that is the case then could we not directly achieve that with a slight modification to `DerivingVia` to derive "via" an isomorphism? ``` data Pair a = a :# a deriving (Functor, Applicative, Monad, MonadFix, ...) via iso (representable) ``` or for a chain of isomorphisms.. ``` data Pair a = a :# a deriving (Functor, Applicative, Monad, MonadFix, ...) via iso (representable . iso2 . iso3 . iso4) ``` or coercions ``` data Pair a = a :# a deriving (Functor, Applicative, Monad, MonadFix, ...) via iso (coerceIso @(,)) ``` What advantages does this clever technique have? 
But until mechanics did not agreed in common practices and interfaces even driving became a nightmare only possible for specialists.
Right, so despite you not mentioning cassava by name in the post above, we're still arguing about cassava. Got it. Open source norms don't have a mechanism for taking over a namespace in general. Indeed when something is forked, then people need to choose to adopt the fork over the original, and there is some non-insubstantial friction to the whole process, which is why people tend to avoid forks. Or -- get this -- stackage could just keep cassava pinned to an older version for a while (which it did!) -- and then move to a new version when a new stack came out that fixed the parsing bug. And nothing would really break for anyone. Nor was this a one-off exception. There are any number of packages on stackage, that for whatever reason, for the time being have upper bounds set: https://github.com/fpco/stackage/blob/master/build-constraints.yaml#L3072 I don't see why having, temporarily, one more among them, was such a cause for consternation to lead to all... this.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [fpco/stackage/.../**build-constraints.yaml#L3072** (master → 18dac1d)](https://github.com/fpco/stackage/blob/18dac1d3bcc9c644c61eb22056dd1823ae23b932/build-constraints.yaml#L3072) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
Here's the problem. As someone with opinions, maintainers will make changes that they think are good but you do not, or not make changes that you think are good but they do not _all the time_. In fact, most people in the world, on most occasions, will behave in ways that most other people in the world wouldn't necessarily agree with. This is because lots of people disagree on lots of things, people value different things, and people also think differently than one another. So you can't have a rule that says "well, the determining factor in all social interactions is if I'm right or not." This is because other people won't agree if you're right or not to begin with! So the rule isn't useful. Instead, we have to have some way of saying "well, I think I'm right, and someone else I disagree with thinks they're right, and nonetheless we won't flame one another on reddit until we keel over from lack of sleep and dehydration." One element of this is recognizing that going on a "vendetta" against maintainers not only isn't helpful to getting a PR accepted, but it just might predispose people to not want to interact with you or consider your arguments, in general, because they find dealing with you draining, exhausting and frustrating.
Good catch- edited.
[24 days of hackage](https://ocharles.org.uk/blog/) is a good place to start. Don’t know about machine learning libraries though. 
For getting an overview, I really like the article [State of the Haskell ecosystem](https://github.com/Gabriel439/post-rfc/blob/master/sotu.md)
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [Gabriel439/post-rfc/.../**sotu.md** (master → 9aec61c)](https://github.com/Gabriel439/post-rfc/blob/9aec61cb08bded38ea454b52b5b1a3bb7426db17/sotu.md) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
Deleting with the link did not work. I don't see how this comment should help. Would really like to delete it...
Monoid and Monad are pretty much the only pieces of theory that I found necessary in learning Haskell, which really are quite practical. I notice a lot of beginners getting side tracked into highly unnecessary theory, like "what is a category" or something. But really you just need to learn how to write programs in Haskell. I think Haskell Programming From First Principles and LYAH both do a pretty good job of staying on track in this regard.
I toyed with similar ideas. Staying within the system of constraints provides many benefits. Apart from keeping the extension simple it is not enough to witness the isomorphism, we must know how to apply it: newtype AsIso a (iso :: a &lt;-&gt; a') = AsIso a instance (Iso (iso :: a &lt;-&gt; a'), Eq a') =&gt; Eq (AsIso a iso) where AsIso a == AsIso b = from @_ @_ @iso a == from @_ @_ @iso b In your system how do we specify this? With `DependentTypes` ([*singletons*](https://hackage.haskell.org/package/singletons), [*reflection*](https://hackage.haskell.org/package/reflection-2.1.2) or even `ImplicitParams`) we can pass the value-level functions as `Constraint`s but then we miss out on creating a hierarchy of isomorphisms, and in the case of *reflection* and `ImplicitParams` we still require a special `deriving` form to provide a witness. In the particular case of isomorphisms it is not a big concern but isomorphisms are not the only thing we can encode like this. Other uses include `Monoid` / `Applicative` homomorphisms as well as `Monoid` actions are also amenable to this treatment where we want them to appear as constraints. [Example](https://hackage.haskell.org/package/monoid-extras-0.4.2/docs/Data-Monoid-SemiDirectProduct.html#t:Semi): newtype Semi s m = Semi (s, m) instance (Monoid m, Monoid s, Action m s) =&gt; Monoid (Semi s m) One way forward is creating a type class for witnesses of `Monoid` actions: type m -· a = (m, a) -&gt; Type class Monoid m =&gt; Action (wit :: m -· a) where (·) :: m -&gt; (a -&gt; a) newtype Semi (wit :: m -· s) = Semi (s, m) instance (Action (wit :: m -· s), Monoid s) =&gt; Monoid (Semi wit) I started playing with this a few days ago so this is completely uncharted territory. No doubt there are other ways of achieving the same thing. This is where my gut feeling lead me and it usually takes a few years for my brain to catch up.
One option is to use `Cofree (Co (StateF s))`, if you just want a pairing with your free monad. If you then expand `Co`, you get Co (StateF s) a ~ forall r. StateF s (a -&gt; r) -&gt; r ~ forall r. (Either (s -&gt; a -&gt; r) (s, a -&gt; r)) -&gt; r ~ ((s, a), (s -&gt; a)) So you get something like data CoStateF s x = CoStateF { coget :: (s, x) , coset :: s -&gt; x } which is what you'd expect. This is the "most specific" such functor in the sense that any other which pairs with `StateF` must factor through this one, in the sense of the post.
Any chance of cabal and stack joining forces and have one build/package tool? I find wrappers with their own config files annoying--and now with hpack, yet another yaml file. 
The current implementation only displays _given_ constraints (the ones required by the function you're writing), and doesn't display _wanted_ constraints (the ones that are required by the functions you're calling). For reference, here's the original ticket for the feature: https://ghc.haskell.org/trac/ghc/ticket/10614 There are endless ways you can tweak the definition of "relevant" here. Maybe it's a good idea to open a ticket for this?
I usually just go to the [hackage package list](http://hackage.haskell.org/packages/) and do a browser search through the page for keywords. It usually seems to work pretty well for me, but I'm not sure if people newer to Haskell would have more difficulty with that approach.
They're already joining forces, Stack provides the UI to Cabal the library, see https://www.fpcomplete.com/blog/2015/06/why-is-stack-not-cabal
nm.
You seem to imply that Stack users are unhappy and would rather use cabal the tool if it had a bit more some syntax sugar. I don't think so. Stack already works great so why would I want to switch tool in the first place?
I'm not the bot author, but the purpose of the bot is to capture the github link as it was when you posted the link. In this case, it's not terribly useful as using the most up to date version of this list is best, but if you were e.g. linking to a repo and your comment said something along the lines of "check out the code in the `foo` function", when master gets updated X months later and `foo` is moved to a different file or is refactored out entirely, your link would no longer contain `foo` while the github bot link would still show `foo` since it links to a particular commit instead of whatever the master branch is currently at.
Here's a well-known blog post about that idea: [On the Unsafety of Interleaved IO](http://comonad.com/reader/2015/on-the-unsafety-of-interleaved-io/)
This is an old article. I think a fair amount has changed in the two years since it was written.
Yup, I actually quite like this bot for "archaeological" reasons. E.g. it can be very unclear what a commenter was referring to 2 years prior if their link points to https://github.com/Foo/Bar/blob/master/file.ext. Whereas https://github.com/Foo/Bar/blob/123abc/file.ext is fully unambiguous. --- Aside, a bit of git(hub) trivia: - https://github.com/Foo/Bar/blob/master@{2-years-ago}/file.ext, and - https://github.com/Foo/Bar/blob/master@{2015-12-13}/file.ext do work, but they are not robust against force pushes, commit squashes, repo changes, and branch renames. Whereas the commit hash is fully robust against these too.
Sorry for the very high level description here, that's all I can do right now. I'm not familiar with spock, but katip needs access to a couple of simple environment values (`LogEnv`, I believe), typically to be kept inside of an application state type. The canonical use would be to instantiate a `LogEnv` during your application's init, then stick that value inside the state where your handlers can call into. If there's a monad stack on top of your application's state/environment, you'd then instantiate `Katip m` for your monad to make it easier to call the logging functions. `KatipContext` is not needed for an initial use case - it's for future cases where you want to bake in, automatically, some environment variables/context from your transformer stack to every single log message emitted.
That post made a lot of sense at the time, but is there a particular reason for the repost? Did I miss a recent flamewar?
In typical use, cabal puts things in a sandbox. The new cabal commands do something like nix.
In my complete-lack-of-understanding, `cabal install [library]` -- today -- will fuck your shit up by default. As has been the case for perhaps half a decade now, you can ameliorate (or totally eliminate) the resulting "Cabal hell" by running `cabal sandbox init` in your project directory before doing anything else. You should only ever run `cabal install` -- without a parameter -- to install executables produced by the current project globally, **not** to install library dependencies globally. Modern `cabal build` still does the wrong thing by default, but `cabal new-build` does what everyone expects, and in the near future `cabal build` will become an alias for `cabal new-build`. And apparently with the Nix-like functionality, it will actually share stuff far better than `stack` currently manages. Now, the thing is, there has never been a good reason, as far as I know, to run `cabal install [library]`, apart from trying to speed subsequent builds of other projects up. Why would you want to install dependencies globally? It always strikes me as awfully peculiar when modern Haskell GitHub pages still tell people to run `cabal install lens` as a first step. What you should do is you create a project with a `.cabal` file, and run `cabal build`, and if you want to keep your mental health you run `cabal sandbox init` between those two commands -- there's no reason to run `cabal install` to bring in a project's dependencies...
Yes, I used to do this too. However, I recently noticed that this exists: - http://hackage.haskell.org/packages/search?terms= Anyone knows if this is a new feature? I quite like it! It even allows sorting the search results (or even all packages if one does not enter a search term!) by downloads in the last 30 days, (the new rating system) score, and last upload date. Very handy compared to trying to find something on the brows page!
I looked at the thread and couldn't figure out the context in 2016. What was the debate that got heated? 
Yeah, I struggled a bit with this, as well, in the beginning. Very broadly, in Haskell there are two universes: types and values. These are separate currently, although there are various tricks which can make it seem like they are bleeding into each other, but let's ignore that for now. Roughly, types only exist at compile-time, while values only exist at run-time. When you see something like data Foo = Bar then `Foo` is a **type**, and `Bar` is a **value**. Now, since types and values exist in separate universes, Haskell allows you to share names between the two, since there is no theoretical reason not to: data Blag = Blag Don't get it twisted: the `Blag` on the left is a compile-time type, while the `Blag` on the right is a run-time value. The page you linked to does something even more bizarre and confusing: data Cube = Edges Corners type Edges = [Edge] The `Edges` on the first line is a "value" (it's actually a constructor, since it requires arguments before it can be a concrete value, but it sits firmly in the values' universe). The `Edges` on the second line is a type -- **it has absolutely nothing to do with the `Edges` on the first line**. They're naming constructors after types, yes, but this is merely syntactic noise. If you remember that types and values live in separate universes, then you'll see that the names do not interfere, although that code is admittedly unnecessary complicated.
But what about the constructor reuse? Is that legal?
What has changed? Most Haskell libraries still depend on an implementation of TLS that as far as I know, is still a personal project (no industry backing), still contains big chunks of C code (questionable benefit over OpenSSL), and has not received a formal audit. 
The integer-gmp issue got pretty heated. I counted more than one personal attack. Users were banned, comments removed by the mods. Complete shot show, lots of disrespect from both sides.
No chance, if only because of politics. I really think new-build just needs the final polish, and resolver support to be just as good as Stack, but no one on Cabal's side is ever going to implement the blasphemous Stackage field, and no one on Stack's side is ever going to yield to the Haskell committee.
No they're not. Stack has been diverging ever since it's first release. They're separating much faster than they're joining
I'm not implying people are unhappy with Stack. But people *should* be unhappy with the ludicrous political divide it has caused, the likes of which I've never seen in any other programming community (except python). The fact is, the committee cannot make someone else's software the official tool. If we ever want to consolidate, it must be on Cabal, and Cabal is a few short contributions away from being good enough for this.
&gt; stack exec -- pacman -S mingw-w64-x86_64-pkg-config mingw-w64-x86_64-glib2 mingw-w64-x86_64-cairo mingw-w64-x86_64-pango mingw-w64-x86_64-gtk3 Hmm... So you're using the `msys` which comes with `stack` to install Linux dependencies, but you're using `stack exec` to get the right "context"? That's a pretty neat trick!
Where? When they use the name `Edges` for a constructor, and then "re-use" it for a type (synonym)? Yes. Perfectly legal. They live in separate universes. They are used in totally different parts of the Haskell's syntax. **They have nothing to do with each other**, besides sharing a human-readable name.
`Was` and `Is`
That's a syntax error. lol That code will never compile. In general the Haskell Wiki tends to be... problematic.
What would be the advantage of storing the data in another engine if it was only accessible as a big set of tuples in one table? You wouldn't be able to use the normal engine to query the data in a meaningful way except through M36 at which point you might as well be using it's native storage engine.
Several people have said `[Tag]`, but why not `Set Tag`?
There's this comment from that page: &gt; Edit by somebody else: I'm not the author of this, however I think there are some erros in the definition of all the datas below. I think they all are missing the constructor, when you're reading the code keep that in mind. I think that may be saying that for the `data` declarations, they should read data Cube = Cube Edges Corners type Edges = [Edge] -- or Edges = Edge Edge Edge Edge Edge Edge Edge Edge Edge Edge Edge Edge type Corners = [Corner] data Edge = Edge Face Face data Corner = Corner Face Face Face data Face = Face Was Is data Was = R|L|U|D|F|B data Is = R|L|U|D|F|B That looks a little better to me, I haven't loaded it into a module and made sure it compiles, so someone will surely correct me if I'm wrong :)
Ah, that makes a little more sense. Still don't get the whole "was/is" thing though, could they be unified into a single "direction" type?
stack vs cabal. We all know you really love each others. When both sides eventually concede to the tension and make a baby, please call it stackabal. 
Thanks for the context. As someone who hasn't really been following the goings on in the Haskell community lately I was rather confused by the post.
In truth - nothing. I already saw and read it. It's just that it's something, I get a bit of information from... So, for user that will be interested in this post here will be smt that he could learn...
Yeah, I don't think having those two types with the same constructors would allow it to compile, and it would probably make more sense to combine them in a single type like you mention.
Yeah, I understand, but it's really annoying, you know...
Is there any development towards a NixOS like OS that instead uses Haskell as it's language? I know GuixSD uses Guile Scheme, but a Haskell OS would be very comfy (and GuixSD makes life difficult if you don't have libre wifi drivers, like myself).
From the article's author yesterday on [Twitter](https://twitter.com/solatis/status/940746022040453122): &gt; Cryptonite looks like it addresses both the entropy and the random number bias problems! I'll update the blog post
There's a [Tensorflow](https://github.com/tensorflow/haskell/blob/master/README.md) package made by people at Google.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [tensorflow/haskell/.../**README.md** (master → cbd607f)](https://github.com/tensorflow/haskell/blob/cbd607f4d6515cd95a7fd85de43d523c31467bae/README.md) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dr7icgp.)^.
IIRC, this was after the "Evil Cabal" post.
Very thorough! Thanks a lot (:
Debate about who administered the haskell.org website, and whether instructions for new users should discuss cabal-install, Stack, Haskell Platform, etc. 
None of those points are the ones raised in the article, which is largely about entropy generation.
This was the debate that ultimately led to Ed Kmett's [resignation](https://www.reddit.com/r/haskell/comments/50389g/resignation/).
It was quite confusing for me when I [posted a support question](https://www.reddit.com/r/haskell/comments/7hs20y/how_to_fix_stack_unable_to_parse_cabal_file/) and some people started attacking cabal developers out of the blue. What was this all about?
Ok, so if I'm understanding this correctly it should be something like this: pairItem stringToInt intToBool string = (stringToInt String, intToBool int)
&gt; In your system how do we specify this? For example.. ``` data Pair a = a :# a deriving (Functor, Applicative, Monad, MonadFix, ...) via iso (representable . iso2 . iso3 . iso4) ``` you would say `from (representable . iso2 . iso3 . iso4)` for one direction and `to` for the other. Seems quite straightforward! I don't understand what you mean exactly by "a hierarchy of isomorphisms", could you explain? What is the advantage of using a constraint rather than passing a value argument as you have to specify which tag you intend anyway using type applications? &gt; keeping the extension simple Is there a specification for the extension yet as if it requires clever encoding tricks to use properly then it doesn't seem as simple as it could be. 
I'm not trying to poke a hornet's nest here, I'm just clueless about the politics. I know integer-gmp was a bit controversial because of LGPL license (though I'm not sure why, Haskell is open source). Was that the issue? 
Is there another round of pro / anti FPComplete fighting going on? 
On the contrary, I think resisting the crypto-community's input on RdRand, and re-implementing your own TLS library are actually *exactly* the same problem.
Or, even better: ballstack 
That's quite close. One issue is that you accidentally capitalize `String`, you want `string`. The other issue is that you called `intToBool` on `int`, but you actually don't have access to a variable named `int`, you are going to need to create that `int` somehow.
Oh yeah you're right, how about: pairItem stringToInt intToBool string = (stringToInt string, intToBool (stringToInt string)) 
Perfect! Now onto the original problem!
`DerivingVia` is very minimal (inserting a single `coerce` per method) and generalizes `GeneralizedNewtypeDeriving`. All the heavy lifting is done by the `Coercible` solver. Don't let my experimental encoding of isomorphisms fool you — isomorphisms and their encodings have nothing to do with the extension or most of its use cases: just take a look at the [`examples` directory](https://github.com/Icelandjack/deriving-via/tree/master/examples). You have shown me how to invoke `from` and `to` but where do they get inserted? You haven't specified how `via iso` works. To be clear: Users of `Pair` don't need type application. Instances of `AsIso*` belong in a library, and yes the library authors would need to use it. The benefit of constraints is that I reuse the entire Haskell machinery for type classes. I make no other commitments. But no worries, with `DependentHaskell` you can pass Haskell functions / values directly: data Iso a b = MkIso { from :: a -&gt; b, to :: b -&gt; a } newtype WithIso (iso :: Iso a b) = WithIso a instance Eq b =&gt; Eq (AsIso (iso :: Iso a b)) where AsIso x == AsIso y = from iso x == from iso y data BOOL = FALSE | TRUE deriving Eq via (AsIso isoBool) isoBool :: Iso BOOL Bool isoBool = MkIso .. .. 
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [Icelandjack/deriving-via/.../**examples** (master → 2283ad1)](https://github.com/Icelandjack/deriving-via/tree/2283ad191e76ddd367e7e6b957bb3bd35a8e55ac/examples) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dr7p69w.)^.
The problem was "Unable to parse cabal file for integer-gmp-1.0.1.0". The reason this problem arose was due to using cabal 2 features in integer-gmp.cabal, which stack was not prepared to handle yet. The exchange of heated words was in regards to this bug and the who/how of addressing it. The "cabal people" feel the stack people are overly demanding, while the "stack people" feel neglected and spurned.
Not at all. See: https://github.com/commercialhaskell/stack/issues/3624
That's a shame. It seems so silly to me to have all those build/package files in the top dir: stack.yaml, package.yaml, and &lt;proj&gt;.cabal just build one's project--and all because of silly politics. We all have our moments of childishness and eventually come to our senses but this seems beyond the pale for adults to behave.
There's a group of people who want to control haskell and do everything differently. So they try to replace build tools, websites, packages, etc with theirs. They are very hostile towards everyone who didn't join their cult.
You may find this interesting to read: [Group Theory and the Rubik’s Cube](http://www.math.harvard.edu/~jjchen/docs/Group%20Theory%20and%20the%20Rubik%27s%20Cube.pdf)
You dont need any lexing. Take a look at my solution (uses megaparsec 5.2.0) https://github.com/Dean177/advent-of-code-2017/blob/master/Day_12.hs
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [Dean177/advent-of-code-2017/.../**Day_12.hs** (master → 43b982e)](https://github.com/Dean177/advent-of-code-2017/blob/43b982ef8d1ad2494a31d4d782ef229e44a03a91/Day_12.hs) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dr7qpk5.)^.
I don't know if this is a change, but it looks like although the entropy package has a UseRdRand constructor, it doesn't use it **exclusively** (except if you're on Xen). It wraps a file handle to the OS' randomness source and [xors them](https://github.com/TomMD/entropy/blob/master/System/EntropyNix.hs#L76). But then again, I guess this is the kind of thing DJB was writing about.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [TomMD/entropy/.../**EntropyNix.hs#L76** (master → 43457a7)](https://github.com/TomMD/entropy/blob/43457a7637ffef53d0604fc4b835cb76592db800/System/EntropyNix.hs#L76) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
With all due respect to SPJ, and without commenting on *this* kerfuffle specifically, people sometimes do have hidden (and bad) motives, or are dangerously incompetent or deliberately obtuse, and responding to that swiftly and publicly is one of the most effective and least violent ways of preventing massive harm.
I know it is not right to post answers, but this one is quite cryptic and would be a great exercise to understand how it (Functor/Applicative) works ```f g h = map $ (,) &lt;$&gt; g &lt;*&gt; (h . g)```
Or maybe we just like to have good tools and a website that is welcoming and clear to newcomers?
Perhaps the same problem in an abstract sense, but not the same subject. It's not the subject of the article, and therefore it is accurate to say the article is out of date.
 fpair :: (a -&gt; b) -&gt; a -&gt; (a,b) fpair f x = (x, f x) --useful sometimes worstPairList f g = map (fpair g . f) 
Thank you for the solution, but do you think you could help break it down a bit for me? It kind of flew over my head...
I think it fits the signature type. By using substitution, I can see that it seems to have the proper form of pairItem :: (a -&gt; b) -&gt; (b -&gt; c) -&gt; a -&gt; (b,c) For the pairList version, I noticed someone posted the answer f g h = map $ (,) &lt;$&gt; g &lt;*&gt; (h . g) which seems vastly more complicated and different from the simpler version lol. 
I've been seeing Cabal vs stack since the time I've started learning Haskell. My observation is, that the processes and devops around GHC/Hackage/Cabal (I don't completely know what is administered by whom) are not up-to-the-mark. For a long time Hackage docs were broken. GHC is released with buggy binaries, or missing checksums. One of the main points of the current debate was integer-gmp having a different Cabal file in the release vs the git repo. Let that last statement sink in. What was released was different from what was in the repo. I hope the recently formed devops group can fix this. Btw did anyone from the devops group participate in that "lively" debate to take control of the situation? If people are going to respond to this by saying "lack of volunteers/time/money" then I'd have to vehemtly disagree. One possible solution could have been to sign a water-tight contract between Haskell.org committee and FPCo, and give the latter rights and the responsibility to administer this infra without making anything proprietary whatsoever. FPCo is anyways investing time and money in this. Might as well make it official and force stack and Cabal to merge. 
`unsafeInterleaveIO` is *not* clean code...
If you write `(:) x xs`, it's *already* in whnf, so you don't need strictness here.
And 'optimization' is to get rid of `flip` not make it strict...
&gt; If people are going to respond to this by saying "lack of volunteers/time/money" then I'd have to vehemtly disagree. What makes you disagree? I think what GHC/Hackage/Cabal is missing are more full-time paid people. That's a place where companies like FPCo could get involved (and I think some do): paying someone to work on fixing Trac bugs, Cabal bugs, Hackage problems. &gt; Might as well make it official and force stack and Cabal to merge. I strongly disagree with this. Cabal is fundamentally more coupled with GHC (and in a lot of ways lower level). Stack uses it (as it should) as a library under the hood. What Stack brings to the table, is a reliable FPCo-backed way of getting reproducible/fast builds. There are some opinionated choices made there and that's OK - because Stack is a 3rd party tool. For instance, a lot of the work behind Stack involves curating sets of compatible packages.
Well, upgrading to `stack` 1.6.1 today just broke our app. Shit happens. But I certainly wouldn't want a for-profit company having for sole responsibility for core parts of the ecosystem. We use stackage snapshots and stack and it's great, but if that all went away tomorrow we could adjust in a few hours (at least I hope so).
&gt; The exchange of heated words was in regards to this bug and the who/how of addressing it. The "cabal people" feel the stack people are overly demanding, while the "stack people" feel neglected and spurned. With no knowledge or understanding of that issue, I'm an impartial person who can make this general observation (which may or may not be relevant - ie I'm taking this excuse to say something at best tangentially related that's bothering me)... Demands that people should be respectful can be abused. They can be a way of silencing dissent, justified or otherwise. This is particularly the case because whether people are being respectful is itself a subjective judgement, or if forced into an objective classification, a childishly naive one. Disrespect doesn't need to be intended to be perceived. This issue includes, but is not limited to, the loss of the nonverbal channel. An old blog post [Why I hate advocacy](https://www.perl.com/pub/2000/12/advocacy.html) is one of my personal obsessions - I've mentioned it here at least once before. Very near the start, one quote - "Advocacy has become so natural to us that we forget there is any other way to discuss programming languages. Even if we don't forget, other people can't understand us because they hear advocacy whether we want them to or not.". A neutral statement such as "a has feature x whereas b has feature y" tends to be interpreted not just as a "therefore a is good and b is bad" value judgement but as an "everyone who likes b is a moron" personal attack. Obviously this mostly isn't a problem when everyone involved is trying to be reasonable - but when that's not the case, how obvious is it where the blame lies? One of the issues is, as already suggested, that false accusations of disrespect can be used to try to shut down criticism and silent dissent, or else a false perception of disrespect can lead to dissenting opinions being ignored and "no-platformed". And (very often) anyone who dares consider the dissenting point of view will be dismissed as a naive idiot, or as a whatever-ist "sympathiser" of the dissenters "offensive" claims. For anyone who believes that playing devils advocate is a good way to better understand the issues (especially if their social skills are, like mine, less than perfect - but note that there are two meanings of "bad", incompetence is not the same as malevolence) this can be a recurring issue. When it's genuinely happening is itself (for non-psychics) a subjective judgement. Pretending it's not happening when it is just makes one a naive convenient victim for being shut down and shut out. And of course the perception that one is being shut down and shut out leads to frustration and anger, and therefore to genuine disrespect which then becomes a further accusation to be applied potentially in a one-sided way, especially by those who are expert in subtle, underhanded plausibly-deniable disrespect. An example of a place that has artificial standards for "respectfulness" is Parliament. You'd have to be incredibly naive to believe that disrespect wasn't routinely expressed within those rules. I'd even go so far as to call it indirect discrimination against people who have a disability in that they aren't underhanded enough to compete. In an ideal world, it'd be possible to address this issue as it happens by jdentifying the issue and respectfully discussing that too, but of course the accusation is then taken as a personal attack. At least one reason for that is because if a person is being deliberately shut down to suppress debate, obviously a rational evaluation of how that's happening will be unwelcome. None of this argues against trying to keep things respectful, of course - the (admittedly hypocritical) point is that we shouldn't be too quick to judge what is or is not disrespect, or what is or is not justified, or what should be calmed down yet taken seriously as opposed to shouted down or ignored. Except of course that who has the time? 
Please add an issue to the stack issue tracker if you haven't already. The 1.6.1 release was somewhat rushed out the door due to the integer-gmp stuff. It was also a release that changed a whole lot of code. So, it isn't surprising that there are some issues. The only regressions I'm aware of have to do with builds on arch linux - https://github.com/commercialhaskell/stack/issues/3630 https://github.com/commercialhaskell/stack/issues/3648 (not sure if the latter one is a regression or not)
I agree against tone policing people, but there were personal attacks, not just neutral statements being misinterpreted. 
Ha ha. Because ballsack.
There were actually [some plans](http://www.well-typed.com/blog/2015/01/how-we-might-abolish-cabal-hell-part-2/) to fix all the Cabal problems before Stack was first released, but fpco decided to do their own thing instead of contributing to that effort. The result is that the plan was never finished, and what *was* planned has been put into action extremely slowly. Fpco is an extremely valuable force of work, and I believe they are acting in mostly good faith, but it's clear they have very little interest in contributing to tools that don't belong to them. I don't think handing them the keys officially is a good idea, especially since I don't want all our tools beholden to a for-profit entity with their own agenda. If it were up to me, I'd try and execute on those unfinished Cabal plans: - Switch new-build to be the default - Add support for GHC version management in Cabal - Add support for one-line Stackage usage in cabal.project files (preferably in a way not explicitly tied to Stackage, but still just as easy) - Deprecate Stack in favor of Cabal, since they are now functionally on par for the most part. This has the effect of consolidating on software maintained by the committee, while encouraging everyone, including FPCo, to shift the volunteer time there. I don't believe Stack can logistically become the official tool, so we have to consolidate on Cabal if we ever want to mend the political divide. There's not really a way around this; we've incurred a bunch of debt by splitting the ecosystem in two, and now we have to pay it off. The only thing in the way is politics.
&gt; The result is that the plan was never finished LOL, stack fixing the problem first is why cabal never did. Cool story bro.
&gt; acting in mostly good faith Do you ever think maybe you're not throwing quite enough shade?
Well if you've got limited volunteer time, and the problem seems to have been solved elsewhere, yea I'd say solving it in Cabal becomes lower priority.
You're right. I'll reword
I've always liked `staball`
That sounds like an expanded lens library.
Sounds like maybe a tangent to it. This time the beef looks to be between Stack and Cabal and was triggered by gmp, or at least that's my take so far knowing almost nothing about the latest goings on and just reading this thread. Actually when I kind of stopped paying attention to most of the Haskell stuff besides major announcements was right around the height of the last FPComplete fight.
This. The *real* killer feature of `stack`age is that it is a "`sta`ble H`ack`age". Given that your code builds with a specific (hand-curated) LTS snapshot today, then it will almost certainly build perfectly with the same LTS snapshot half a decade in the future, even if all your dependency libraries change their APIs, or you didn't bother trying to juggle their version upper bounds. This is the true utility of `stack` LTS. At the time, better handling of global dependencies was almost an added bonus, but unfortunately now the fights tend to be around this ancillary feature. Admittedly, I'm not too obsessed about stability. I bump up my LTS versions routinely, and have always strongly believed in leaving out version upper-bounds until you can *prove* that a certain version of a dependency breaks your code non-trivially. With `stack`'s snapshots, though, I can choose when I want to introduce newer library versions that will potentially break my code, without the administrative headache of having to run around chasing newer non-breaking versions (half of Hackage seems to break every time a new version of `aeson` is released, for example) -- I can have my cake and eat it. --- Now, one misfeature of Hackage, one which I consider pernicious, in a similar vein to the feature which allowed the `integer-gmp` situation in the first place, is the fact that `.cabal` files can be changed on Hackage after a library has been uploaded. That means that relying on a version number as a stable tag is full of folly. I can't remember how `stack` works around this, but I think it has something to do with keeping its own hash database, and possibly even mirroring Hackage itself (I'm very unsure on this last point). 
What about `stack` scripts? Or `stack exec` and its "local" environments? The two have diverged a great deal more than your bullet points suggest. 
I don't really feel like getting into responding to most of this message, had enough debate this week! I would like to discuss one part, though: &gt; but fpco decided to do their own thing instead of contributing to that effort &gt; but it's clear they have very little interest in contributing to tools that don't belong to them. I believe it was clear at the time, that it would be very hard to get agreement for the kinds of features that stack has, and that it would be very hard to get PRs merged. So, it needed to be demonstrated that people want those features. Since then, things have shifted a bit, and it may be easier to get PRs merged than it used to. There are big differences in engineering philosophies. cabal-install tries to keep minimal dependencies, whereas stack freely depends on anything it finds useful. So, stack benefits from a bunch of great libraries, whereas in cabal-install, usage of large dependencies is discouraged. This leads to a lot of development effort being dumped into local forks and/or NIH-ing various things. &gt; I don't believe Stack can logistically become the official tool, so we have to consolidate on Cabal if we ever want to mend the political divide. Who determines what is official, and why do they matter so much? IMHO in the long run this is determined by what is popular. GHC is the official haskell compiler because it has been for so long. There is no need to consolidate on it, because there're good reasons to try out alternative approaches. I think the political divide comes down to a mix of particular personalities clashing, along with differences in technical opinion. In particular, opinions on how software should be written, how much a constraints solver is really needed, and whether it makes sense to specify version constraints within a package rather than externally.
I agree with your point partially. Nothing is wrong if a for-profit company is shepherding a community driven process with a clear contract/charter to not make anything proprietary. They can continue to use this as a badge-of-honour to help them further their business interests, and if they want to exit, anyone else can take over the established processes. (I'm sure some financial grant will have to be given by the Haskell.org committee for the pain that the company is taking) 
Doesn't Cabal have exec? New-build has something equivalently powerful to the local environments. There's probably a list of minor features that could be uncontroversially added to Cabal, but none of them are very hard to implement. I think this is a hurdle we could overcome
So, please read my comment as a stack/stackage vs cabal/hackage point. Not Cabal as a library. I'll edit my comment to make this more explicit. 
Yes, I believe Stack has done a good enough job of demonstrating its obvious value that a migration plan for Cabal would be much more easily accepted. However, I did link to a plan that more or less describes exactly the same plan that we'd do today, before Stack was even released. People seemed mostly ok with it then. As for what's official, this one seems pretty obvious to me. The authority is the Haskell committee, so they decide what is official. This matters because they control haskell.org which will recommend the official tool, and which beginners will often go to first; this should be a straightforward process, but it currently cannot be because a third party tool can't be the official way to do everything. The political divide comes from power. The committee and FPCo are the two centers of power, which naturally generates a divide as long as they're not consolidated.
This is the classic ["I could build that in a weekend"](https://danluu.com/sounds-easy/) comment applied to Stack. 
Sorry, I don't mean to suggest it would be easy in total. Just that the plan is simple, and the individual components are easy. It's just a function of volunteer time. If the amount of time put into Stack were put into this instead, we'd have an even better situation.
Thanks to Henk-Jan van Tuyl for posting this on haskell-cafe. Looks like we lost some features… inline c directly supported by GHC? Well, we have http://hackage.haskell.org/package/inline-c now.
The doc has some good ideas, but nothing to do with managing the compiler install or having snapshots. I know that Duncan is favorable towards something similar to snapshots for cabal-install, at least when we discussed this at ICFP a few years ago. However, I haven't seen any current plans to implement this. &gt; The authority is the Haskell committee, so they decide what is official. True, so it is important that this power is fairly divided such that no one party ends up with full control of shared community resources. However, oddly enough, things like this happen - https://wiki.haskell.org/index.php?title=Haskell.org_committee&amp;diff=60691&amp;oldid=60456
Thanks to Henk-Jan van Tuyl for posting a reminder on haskell-cafe: https://mail.haskell.org/pipermail/haskell-cafe/2017-December/128289.html
*25 years ago.
That's the unfinished part. Part 3 of that series was going to cover it, but never got written.
Stackage is about curating compatible package sets. Hackage is about listing all packages and their versions. Stack is about reproducible builds, Cabal about package management. They are _different_. Stack/Stackage is fundamentally opinionated, cabal/hackage isn't. (And again, that's not bad!) &gt; leadership and direction needs to be much better That is a very opinionated statement. :) &gt; one company is already investing their resources in solving this problem. Empower them. Doesn't require extra resources. Well-Typed, Galois, Tweag I/O and almost certainly others are also investing resources by funding people to work on GHC/Hackage/Cabal. Why doesn't FPCo do that too? 
We deal with revisions by keeping a hash of the cabal file contents in the snapshot itself, and checking those specific copies out. Originally, I thought this wouldn't be necessary, as revisions were not intended to break old build plans. However, we began receiving bug reports of revisions adding unnecessary upper bounds and breaking old snapshots, and had to add this feature. With Stack 1.6.1, you can include revision information in your extra-deps as well, making them more reproducible. Tangentially: the fact that we use specific revisions is part of the bug that landed trouble for Stack 1.5.1 with integer-gmp.cabal. It turns out that there was a similar issue with cabal-install-1.24 (see [issue #4624](https://github.com/haskell/cabal/issues/4624), and this was worked around by removing all cabal 2.0 formatted files from the 00-index.tar.gz file. That's the old version of the index of cabal files, which does not include revisions. cabal-install 1.24 and earlier were able to bypass the problems with `^&gt;=` because they were still using this index. But Stack has been moved over to `01-index.tar.gz` since at least 1.5 (I think 1.3, but I'm not sure). Therefore, it wasn't able to get the protection of that Hackage workaround. The mirror that you're referring to is likely the `all-cabal-hashes` Git repo. Before hackage-security was released and 01-index.tar.gz available, it was the only way to get historical revisions of packages. Since hackage-security was released, we moved Stack over to it. Sorry for the long tangent :)
The committee has no stance between the two tools. The platform includes both the `stack` and `cabal-install` tools.
Thanks to all the people who worked on this underlying stuff so that I can do the programming I love
Well that seems even more confusing to newcomers.
You are correct that it does have the proper form! That is a correct implementation of `pairItem`. For `pairList` I would actually use your `pairItem` function to implement it, in combination with either `map` or manual recursion. See where you can get with that approach and let me know if you are having trouble. There a variety of concise and clever ways to implement it, like: pairList f g = f &amp;&amp;&amp; g . h But its better for you to try and come up with a simpler working implementation.
More like when you step on a broken `lens`
The committee doesn't maintain software. It maintains infrastructure and provides and centralizes resources. At times it coordinates funding of projects, e.g. through summer of code programs. GHC is maintained by ghchq, libraries by the libraries committee, hackage by the hackage maintainers, cabal by the cabal maintainers, etc.
It's worth noting that you can create *lens* `Iso'`s from that, if some other library happens to use this encoding import qualified Control.Lens.Iso as Iso type (a &lt;--&gt; b) iso = Iso (iso :: a &lt;-&gt; b) mkIso :: forall a b iso. (a &lt;--&gt; b) iso =&gt; Iso.Iso a b mkIso = Iso.iso (from @a @b @iso) (to @a @b @iso)
The haskell.org committee doesn't have enough money to fund protracted development of anything, nor has it done so. It has coordinated summer-of-code funding in the past, but that's the absolute maximum capacity. It coordinates resources and donations.
Cabal files are not changed on hackage. New revisions can be _added_. Cf: https://github.com/haskell-infra/hackage-trustees/blob/master/revisions-information.md
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [haskell-infra/hackage-trustees/.../**revisions-information.md** (master → d2867a1)](https://github.com/haskell-infra/hackage-trustees/blob/d2867a11612dcecff2d79f941c3744d570597f1c/revisions-information.md) ---- ^(Shoot me a PM if you think I'm doing something wrong.)^( To delete this, click) [^here](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply dr85xzl.)^.
&gt; I believe it was clear at the time, that it would be very hard to get agreement for the kinds of features that stack has, and that it would be very hard to get PRs merged. So, it needed to be demonstrated that people want those features. I totally understand this. Funnily enough though, I'm _happy_ that Stack is a separate tool from cabal-install. They are both useful and have come to cover very different needs for me (I reach for cabal when hacking on GHC, Haddock, projects depending on custom built variants of GHC but stack is incredibly useful for quickly testing out my libraries on a bunch of resolvers). Finally, I just wanted to say a big thank you for all the work you have personally done on Stack - it is a great tool that has really helped the Haskell community along.
GHC is maintained by ghchq. Hackage is maintained by the hackage maintainers, and the box is admined by the haskell infrastructure team. Cabal is maintained by the cabal maintainers. These are three different teams and they shouldn't be conflated. I don't know why you're mixing up the efforts to improve the ghc release process with build tools that are for the most part downstream of this.
Indeed a key element of the devops group is trying to marshal further resources for ghc to bring more attention: https://ghc.haskell.org/trac/ghc/wiki/DevOpsGroupCharter
&gt; I reach for cabal when hacking on GHC, Haddock, projects depending on custom built variants of GHC I hope to make stack handle this usecase well! I really need to get back to work on a GHC diff I've had in flight for far too long..
We also used to ship _papers_ with GHC! I think we need to bring that back.
&gt; fpco decided to do their own thing instead of contributing to that effort. Your timeline is way off. The [initial announcement for what would become Stackage](https://www.yesodweb.com/blog/2012/11/stable-vetted-hackage) was posted in **2012**, a full three years before the post you linked was authored (and [a year before Cabal got sandboxes!](https://mail.haskell.org/pipermail/haskell-cafe/2013-September/110140.html)). The [proposal blog post](https://www.yesodweb.com/blog/2012/11/solving-cabal-hell) doesn't even mention diverging from `cabal` at all, and the official way to use stackage was [through cabal sandboxes](https://www.yesodweb.com/blog/2014/11/cabal-sandbox-stackage). The first public beta of `stack` was announced [in June 2015](https://www.fpcomplete.com/blog/2015/06/announcing-first-public-beta-stack), and according to the blog post, FPCo had been working on `stack` internally for about a year before that. If these timelines are to be believed, then `stack` was six months into internal production use while the cabal team was considering Nix-style builds (a feature that has landed, but is not yet the default workflow). So, as I understand it, Cabal Hell [was a pretty massive problem](https://www.fpcomplete.com/blog/2015/05/thousand-user-haskell-survey), and had been since at least 2011 ([first mention I can find of it on Google](https://cdsmith.wordpress.com/2011/01/16/haskells-own-dll-hell/), and the post indicates that it's been a problem for some time). The "stable vetted Hackage" post references a project called *GPS Haskell*, which was supposed to be the Cabal/GHC/etc. blessed system. Where's GPS Haskell? It never materialized. Why not? Possibly because Stackage filled the void so well. From my conversations with FPCo folks, they did *try* to get these things upstreamed into Cabal, but these features weren't accepted. I do not remember why or any details, to be honest. I can understand why `cabal-install` wouldn't want scope expansion to manage GHC versions, snapshots, etc., and the general GHC/Cabal/Haskell mindset of "let's wait for the Right Decision before making a potentially wrong one now" is a Good Mindset -- it's how we got here, after all! `stack` itself sits nicely on top of `Cabal` as a convenience/opinionated project layer, and, at the very least, a test bed for experimental new features that might be upstreamed into cabal when it's clear that everyone wants them. So, to recap my understanding of the timeline: - the beginning of time - 2011: `cabal hell` is a thing - 2012: `stackage` is born - 2013: `cabal` gets sandboxes, which technically fixes cabal hell, but these are not the default, leading to continued terrible UX for beginners (raise your hand if you were bitten as a newbie with this) - 2014: `stack` is born in private - January 2015: The `cabal new-build` plan is formulated - June 2015: `stack` is released, essentially solving cabal hell and dependency problems forever - 2015-present: `cabal` is slowly absorbing features from downstream projects, but the really important ones (`new-build` etc) *still* aren't the default, so the bad default workflow UX is *still a problem*.
Got it. Thanks for the clarification. Is there any merit in actually merging the devops for GHC, Cabal, and Hackage? Aren't they very tightly coupled?
I think the devops group has its hands full at the moment just getting GHC into shape. GHC also has a lot of special needs. There is certainly some coupling between these various tools, in that cabal-the-library is a boot lib for ghc, and cabal-install needs to talk to hackage. Buy I don't know what sorts of unification would necessarily help things here. Perhaps extending cross-cutting CI. Some of this seems likely to emerge if the plan for testing a wider set of packages against ghc nightlies (as discussed at the HIW this year) moves forward. That said, having a shared build infra for nightlies and releases across a wider range of haskell tools and packages (not just cabal, but also core libraries, etc.) is something I've long dreamed about -- providing uniform integration builds across windows, linux, mac, maybe ppas, etc. There's also the question of improved support for more exotic architectures. GHCJS could also use support in this regard.
&gt; From my conversations with FPCo folks, they did try to get these things upstreamed into Cabal, but these features weren't accepted. This is contested history, fwiw. I'll just leave it at that.
Hmm, I still don't fully understand this enough to be able to determine a version of the pairList function myself. I've been playing around with some ideas using ghci, but nothing seems to be working. I would be very grateful if you could show me how to do it, as I tend to learn better by example, but I completely understand if you don't want to do that. In any case, I appreciate all the help and time you've given me. Thanks to you, I've learnt quite a bit about how the type system works.
I've seen that ticket before. I doubt I'll have extra bandwidth for this soon though - improving GHC and Haddock are more important to me right now. That said, it isn't _just_ specifying a custom GHC path that has made me reach for cabal-install - I've found `cabal new-build` was able to sometimes construct build-plans when `stack` couldn't for these types of projects.
I love that for as long as Cabal has had sandboxes, and for as long as Stack has existed, there's always been this persistent sub-group of perfectly happy Nix users, living in some sort of deterministic (as of some recent GHC version, I think?) cross-compilation package utopia. I hope we can end up with a best-of-all-worlds package manager. ^^^We ^^^could ^^^even ^^^call ^^^it ^^^`crappack` ^^^as ^^^a ^^^homage ^^^to ^^^`stack`, ^^^`cabal`, ^^^and ^^^Backpack.
I think my comment is mostly accurate considering I was talking about Stack, not Stackage, and the stuff about Stack before that blog post was all private.
If you think you'll learn better by knowing the solution then sure, I'm more than happy to walk you through it: pairItem :: (a -&gt; b) -&gt; (b -&gt; c) -&gt; a -&gt; (b, c) So we have this so far, and we also have map: map :: (a -&gt; b) -&gt; [a] -&gt; [b] And what we want is: pairList :: (a -&gt; b) -&gt; (b -&gt; c) -&gt; [a] -&gt; [(b,c)] So now one thing we should notice is that `map` has some overlap with the type we actually want, so lets go ahead and replace `a` and `b` with types of our choice to try to get as close to `pairList` as possible: -- a = a -- b = (b, c) This gives us the following type for `map`: map :: (a -&gt; (b, c)) -&gt; [a] -&gt; [(b, c)] So now notice how the result of this is the same as the result of `pairList`, so now instead of finding `[(b, c)]` directly, we need to find `(a -&gt; (b, c))` and `[a]` and then tell `map` to do the rest! pairList f g xs = map ... ... So we now have `f :: a -&gt; b`, `g :: b -&gt; c` and `xs :: [a]`. And we need to get `a -&gt; (b, c)` and `[a]`. Well the `[a]` part is easy, that's `xs`: pairList f g xs = map ... xs So now we need to replace `...` with something of type `a -&gt; (b, c)`. Now we are going to remember `pairItem` from before, since if you give `pairItem` a `a -&gt; b` and a `b -&gt; c` you will get back an `a -&gt; (b, c)`. Which is exactly what we need! One big thing to remember with Haskell is that you don't have to give all the arguments at once, you can apply some of them and leave the rest for later. So lets go ahead and add `pairItem` into the mix: pairList f g xs = map (pairItem ... ...) xs where the first `...` now needs to have type `a -&gt; b`, and the second needs to have type `b -&gt; c`. Now this part is pretty easy again, as `f :: a -&gt; b` and `g :: b -&gt; c` from above: pairList :: (a -&gt; b) -&gt; (b -&gt; c) -&gt; [a] -&gt; [(b,c)] pairList f g xs = map (pairItem f g) xs And now we are done! Well assuming you still have `pairItem` hanging around: pairItem :: (a -&gt; b) -&gt; (b -&gt; c) -&gt; a -&gt; (b, c) pairItem f g x = (f a, g (f a))
&gt; The platform includes both the `stack` and `cabal-install` tools. Only after much wailing and gnashing of teeth, did `stack` finally end up being bundled with the Haskell Platform. 
This clears a few things up. Thanks. `cabal-install` was recommended for the same reason that GHC (as opposed to Hugs) was recommended: it's one of the best tools available, not because it is *the* "official" tool. 
&gt; GHC and Haddock are more important to me right now. Awesome. Your efforts are likewise appreciated! Glad you appreciate the efforts on stack. Making haddock work better does indeed seem quite important. It's getting better, I was pleased to see recently that orphan instances are now displayed. The biggest thing I'd like to see there is an alternate markup that is more intuitive to those familiar with github flavored markdown.
&gt; An extensible I/O system is provided, based on a "monad" Something about the structure of this sentence and the quotes around the word “monad” make me grin.
ghc 8.x along with anything compiled with it has been notoriously slow on WSL (see this [issue](https://github.com/Microsoft/WSL/issues/1671)). Recent insider versions have improved a lot but the speed difference from native Linux is still noticable.
A) that's irrelevant to what I said. B) no that's not true. the plan to add it was announced to basically unanimous acclaim: https://mail.haskell.org/pipermail/libraries/2015-July/025938.html 
Except it appears to be all but dead these days. The website is often down and I don't remember there being any updates for a long time.
No, we don't, sometimes you have to read things.
Yup, I'm very funny
When trying to write a polymorphic function like this, *typed holes* can be very helpful. You can write a “hole” `_` or `_name` in place of any expression, and the compiler will give you an error showing the type of that “hole” along with suggestions of values &amp; functions in scope that might help you fill it in. So you can start with a skeleton of a function: pairList :: (a -&gt; b) -&gt; (b -&gt; c) -&gt; [a] -&gt; [(b, c)] pairList f g xs = _1 GHC tells us that the `_1` hole needs to be filled with a value of type `[(b, c)]`, `f` has type `a -&gt; b`, `g` has type `b -&gt; c`, and `xs` has type `[a]`. We have an input list and an output list, so let’s write a list comprehension that gives us each element of the input: pairList f g xs = [_2 | x &lt;- xs] The `_2` hole has type `(b, c)`, and now we have an additional value `x` in scope, of type `a`. We know we need to return a pair, so let’s make a pair of holes: pairList f g xs = [(_3, _4) | x &lt;- xs] Here, we need to fill `_3` with a `b` and `_4` with a `c`. We have an `a`, namely `x`, and the functions `f` and `g`. You can figure it out from there. Once it typechecks, try it on some sample inputs like `pairList show reverse [123, 456, 789]` to get a feel for what it does. Some exercises to try: * Express this without a list comprehension. * Do you notice any repeated subexpressions? How can you factor them out? * Come up with an implementation of `pairList` with the same type signature but different behaviour. How many possible versions of this function are there? * Build `pairList` by composing multiple smaller, simpler functions with clear names. 
Recent conversation with a colleague: Him: "Haskell, hey? So, why you wanna learn all these newfangled, hipster languages?" Me: "Um. Haskell is actually older than Java." Him: "Why you wanna learn all these quaint, ancient languages??" 
Sadly none of the ftp sites mentioned in the release email is online right now and even the Internet Archive does not have a copy. Has anyone got a copy of the ghc-0.10 source and if so, post it somewhere?
"An elegant programming language for a more civilized age."
I actually think that would be a great idea - instant access to the papers that were implemented or were inspiration for it!
I had that with "perl"
That "monad" concept sounds a bit strange. It might need a year to fully catch on, perhaps even two. I'll write a tutorial once I understand it better.
Wow, that's really helpful! Everything makes a lot more sense now. Your explanations were very detailed and easy to follow. Once again I really appreciate all the effort and time you put into helping me with this. Thanks a ton!
There are already too many... to the point where the net effect turns out *negative* and newcomers start un-understanding monads somehow. Whether you want to append to this mess is up to you.
&gt; Ability to write arbitrary in-line C-language code, using the I/O monad to retain referential transparency. heyyyyy...
Reply this: Learn from the history so that we repeat the success and avoid the failure. (By not learning the history you already repeated a failure.)
TIL First version of GHC was released on my birthday.
The third Mail after the one you linked to already argues against including stack. I wouldn't call that unanimously.
&gt; Why doesn't FPCo do that too? Because FPCo seems to care more about taking control of Haskell under the pretense of representing the Haskell community's wishes than to humbly contribute like Galois, Well-Typed and Tweag I/O have been successfully practicing for years without requiring any aggressive marketing and shitstorms. FPCo shouldn't ask what haskell.org can do for them but rather what they could do for haskell.org. They might not like the answer though.
Just as the Linux world seems to be moving to Wayland :) But that would be very cool and WSL already makes my life a lot more pleasant. ^(Disclaimer: I work for Microsoft)
Except that swift, public, and — implicitly in your argument — verbally brutal responses sour the atmosphere, deter newcomers, exhaust old-timers like Ed Kmett, and engender resentment that leads subsequent sour debates about technical trivialities like ^&gt;= 
&gt; All the heavy lifting is done by the Coercible solver. This answers the question about where the `from` and `to` need to be inserted, the coercible solver must already decide this when deciding how to coerce functions as it must eta expand and coerce the arguments correctly. &gt; The benefit of constraints is that I reuse the entire Haskell machinery for type classes. I make no other commitments. But why when good old values are perfectly sufficient! `DerivingVia` is a generalisation of `GeneralisedNewTypeDeriving` as it allows you more control about which isomorphic data type you want to use rather than just having to use the concrete representation. `DerivingViaIso` is yet another generalisation as it allows you to specify which precise isomorphism you want to use rather than just a simple isomorphism (the coercion) supplied by the compiler. 
[Previous discussion](https://www.reddit.com/r/haskell/comments/7iumzb/haskell_mutable_collections_low_performance_vs/). Paging /u/samosaara.
You mean search for things in nether, yes? Because i created this post precisly to READ guide about vectors...
To be fair, Michael tried to reason with the committee but Ed Kmett instead sided with Gershom: https://twitter.com/snoyberg/status/770089276264546304
This is my favorite feature of hpack. Nice addition!
&gt; January 2015: The cabal new-build plan is formulated Sep 29, 2010 https://ghc.haskell.org/trac/ghc/wiki/Commentary/Packages/MultiInstances
Looks like the git history (which was clearly imported from some previous version control system) only goes back to [ghc-0.26](https://github.com/ghc/ghc/commit/e7d21ee4f8ac907665a7e170c71d59e13a01da09) from 1996. The commit message hints that the commits were copied from some other version control system. The [darcs-to-git switchover](https://github.com/ghc/ghc/releases/tag/ghc-darcs-git-switchover) happened in 2011 and the darcs commit messages are preserved, so this conversion must have switched from some earlier version control system to darcs. The [last commit which uses this style](8112cc17dda916302994b318ea2614a5cdbb48b8) / [first commit which doesn't use this style](https://github.com/ghc/ghc/commit/cebe3ed020cfe403c809545d94bd6b416785f11d) is in 2006, does anybody remember which version control system ghc was using before it switched to darcs?
Kmett continued to have his own opinion after someone reasoned with him?
Can we just make Cabal a programming language please? Dhall would be good. 
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/haskell_jp] [GHC 0.10, released 25 years ago](https://www.reddit.com/r/haskell_jp/comments/7jrfgj/ghc_010_released_25_years_ago/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
Unfortunately, doing that on a false positive has an equal chance of massive harm.
But we would lose analizability.
Thanks for doing these performance analyses! They are really helpful!
To be far, that's a feature of YAML, and so `hpack` gets it for free.
That's what I'm saying?
FWIW I've become aware of some specific context to SPJ's letter that I wasn't fully aware of when I originally commented. People who are first in line to perceive malice often do so only because they're malicious themselves. In that environment every conflict inevitably becomes a bizarre hall of mirrors.
Not if the language is strongly normalizing (like dhall) right?
A colleague noticed it and there was already an open issue (though I'm not sure which it was right now on my phone).
- imports (can be forbidden) - terminating *eventually*, we'd like stronger guarantees.
I'm not against (but not for either). Please make a prototype, e.g. `dhpack` :) (FWIW, `showGenericPackageDescription`in Cabal-2.2 will actually show everything, so you don't need to do text-concatenating, if you don't want to)
I'm not sure what you mean by this. Can you elaborate? 
Darn, stupid typo.
There is https://github.com/ghc/ghc/blob/master/docs/core-spec/core-spec.pdf – what other papers would anyone need/
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [ghc/ghc/.../**core-spec.pdf** (master → fa1afcd)](https://github.com/ghc/ghc/blob/fa1afcde4a3f9caaa0ac37e94f1d8fa3e624405f/docs/core-spec/core-spec.pdf) ---- ^(Shoot me a PM if you think I'm doing something wrong.)
But will it have any George Castanzas? 
But why "bless" some particular method of computation for generating the configuration file? Why can't we simply write libraries to do that, without baking it in the specification?
Whoosh 
Ahh! Finally! Thank goodness!
[The Configuration Complexity Clock](http://mikehadlow.blogspot.com/2012/05/configuration-complexity-clock.html)
Can someone explain to me what this means?
Do you honestly not recognize the absurdity of this? "OMG GM and Ford are just wasting resources splitting it and wasting effort. They need to just merge."
I actually found the Rust code to be remarkably readable compared to the Haskell one.
The original tarball is still available and the checksums on it that may exist still hold. That's very important. Furthermore, since people can always access revision 0, the data hasn't gone anywhere. In Haskell, we use immutable structures all the time, but often make reference only to the latest version of them. However, the fact that the earlier versions of them are not "mutated" away but still remain is important. Same here. Mutation -- bad, appending additional data -- good.
I think why I am wrong is that the `Coercible` solver uses a bidirectional quality so that the proof that `String -&gt; String ~R Html -&gt; Html` does not require knowing that we must use one direct for the contravariant argument and another direction for the covariant result.
Out of curiosity, and because it's hard to gauge from your description: Which parts do you consider heavy special GHC tricks?
`hpack` actually doesn't use YAML includes, it has a top level set of options that are merged into other stanzas like exe/test stanzas, for the most part. You could do that, though. Actually, in a sense, I suppose YAML includes are closer to what Cabal now provides, since you have to explicitly `import` the stanza in Cabal, but I don't believe you have to with `hpack`, so the merge is implicit. But with YAML you'd actually have to use whatever syntax it is...
In rust you can implement most things you'd use an infinite list for an as iterator.
With `DerivingVia` the compiler (`Coercible` solver) is not involved in figuring out where to witness your isomorphism (I am talking about `from` / `to`, not `coerce`). You, the library writer must explicitly specify where they go and which isomorphism they refer to. instance ((a &lt;--&gt; a') iso, Eq a') =&gt; Eq (AsIso a iso) where AsIso a == AsIso b = from @_ @_ @iso a == from @_ @_ @iso b The heavy lifting isn't heavy at all in this case, all `Coercible` does (when you, the user writes `deriving Eq via (AsIso a SomeIso)`) is to assemble coercions to get from AsIso a SomeIso -&gt; AsIso a SomeIso -&gt; Bool to a -&gt; a -&gt; Bool 
Wow, already updated. Bravo.
Interesting that Haskell's `Data.ByteString.readFile` is so much faster that Rust's. I'd be curious to know why that is.
In what sense are these two competitions different? And suggest otherwise what?
Or - though its not real Haskell any more - this could happen if you used [Frege](https://github.com/Frege/frege).
I'm not sure why either - it might require a deepseq but I didn't anticipate that as it's a strict function. Feel free to test that.
He was joking.
One may get improved read performance if reads are buffered, e.g. - let mut inpt = File::open("xmas5.txt").expect("file not found"); + let mut inpt = ::std::io::BufReader::new(File::open("xmas5.txt").expect("file not found")); On SSDs the effect may not be dramatic. 
Fwiw the INLINE pragmas made no noticeable difference on my system.
I agree that for working on binary data, ByteString is great. I just get queasy when it gets used for text, especially given that there's no compile-time type system to distinguish between ByteStrings full of ASCII and ByteStrings full of UTF-8.
The `String` has to be validated as correct UTF-8.
IMO storing utf-8 in bytestring should always be considered very risky and only done in special circumstances. On the other hand bytestring for ASCII is quite nice, as there is plenty of data I have dealt with where I know I only need to care about ASCII (e.g. Analyzing big CSV dumps of data I care about). So bytestring should be assumed to be strings of genuine bytes or chars and anything other than that should be very heavily documented or kept behind module boundaries. 
Is there a reason haskell code is a direct translation of rust code? Can we solve the task without looking at the rust implementation and achieve similar performance? 
You seem to be having a complicated relationship of the Jets &amp; the Sharks kind... but without the singing and dancing
Oh wow, thanks you read my mind. &gt; It's not absurd, but the Haskell version is slightly unidiomatic due to the effort to optimise it, whereas the Rust version is incredibly plain and ordinary. This pretty much **exactly** describes what I meant. But once again, I reiterate, I think it's fine for the code not to feel very idiomatic, we can have all the purely functional, and high level (ish) Haskell goodies with the freedom/potential to make real fast code albeit not very '*idiomatic*'
Cabal 2.0 can have multiple libraries in a single .cabal file now, but only one can be exported, the unnamed `library` stanza. Every other `library` stanza must have a name, and external packages can't depend on it. So the semantics for library exports have not changed, but you can organize code differently now (internally, anyway). That's good for organization, but its primary use right now is for using things like Backpack, which requires fairly fine-grained packaging of your code, so multiple private libraries are exceedingly useful to define separate packages for interfaces and implementations. (My example uses Backpack in fact, so it also uses some other new GHC 8.2 features, like `reexported-modules`) `common` stanzas will make usage of highly package-ified libraries much more palatable to use, though. (I could have reduced the code even further if I was willing to collapse the directory structure, and make `hs-source-dirs` common among everything) The example I gave is a bit frivolous of course, but `ghc-options` alone is pretty great to have. It would pay off much better as I expanded the options and basic dependencies.
OK, shitty example indeed, replace 'infinite lists' with 'some lazy evaluated, purely functional, immutable, language advantage'
Honestly, someone should maybe perhaps increase the speed of our standard library int parsing? Why is it so slow?
Thanks for the article and feedback. Could you elaborate a bit more on why avoiding pairing cygwin and stack?
Oh, neat trick. I might use that someday, if you are okay with that.
Isn't that what `build-type: custom` plus `Setup.hs` get you already?
We just had a thread related to this: https://www.reddit.com/r/haskell/comments/7iaw2k/is_some_haskell_philosophy_close_to_the_unix/ 
Hey, so the `With -&gt; OpenClose` one can also get closed when the GC notices that the closer is no longer referenced and kill the manager thread!
I'd put money on svn; I'm not sure if any others were even around 25 years ago.
Article no longer true. Even the linked github source clearly shows XOR’ing the rdrand entropy and the system source. 
I don’t get your idea. Didn’t you just demonstrate that with `fixIO` you can reimplement `unsafeInterleaveIO`?
I guess I need to educate myself about these goals, in that case. Are the project goals and/or roadmap clearly documented somewhere? 
Oh ok, that makes a bit more sense, thanks for the help.
&gt; If you need a general shell env on windows Why would you not just install MSYS2 here?
Mostly, keeping env clean. Cygwin has incompatible libs, and they have a tendency to magically show up in context when you least expect it. Msys2 gives you everything you might want from cygwin with none of the potential for failure. But it's not an impossible to do both or anything, just generally seems like a bunch of work for not much.
That would do it, but surely Haskell does something similar... right? :D
Why not just use records?
You can [say the same](http://www.haskellforall.com/2012/05/scrap-your-type-classes.html?m=1) for other type classes, for now I'm exploring this space because there are places in the type class hierarchy where I want to have that vocabulary -- monoids-extra instance (Monoid m, Monoid s, Action m s) =&gt; Monoid (Semi s m) -- currently missing instance ??? =&gt; Applicative (Sum f g)
For all client code I write, I do exactly that: I check the status code after every request. In the `requests` library in Python there's a property `ok` where you check if it's a non-successful status code, so I've written tons of code like this: resp = requests.post(some_url, json=payload) if resp.ok: do something...
Going to bed now. Share more examples of other type classes with a lot of overlap
I think I stated this pretty clearly above, but I'd like to reiterate that my criticism wasn't of the technical implementation here. My beef is with the practical impact upon users/maintainers; revisions change the outward-facing behavior of a package in a way that, to the casual observer, is the same as a mutation. cf. https://github.com/myfreeweb/magicbane/pull/2 for evidence that this behavior is potentially hostile towards maintainers.
Asking here because I was utterly undecided if this belongs on SO, CS.SE, CST.SE or Morte's issues...
&gt; You can say the same for other type classes Those type classes are different because you get implicit dictionary passing. You do not get implicit dictionary passing when you use tags because you have to specify the tag at the use site. &gt; for now I'm exploring this space because there are places in the type class hierarchy where I need this vocabulary Do you have an example where this approach is necessary?
Glad that it was helpful!
In SQL, the modifier clauses of a SELECT query can be thought of as actions; * WHERE (and HAVING) acts by boolean conditions with conjunction AND and unit TRUE * LIMIT acts by natural numbers with minimum and unit ALL * OFFSET acts by natural numbers with sum and unit 0 * SORT BY acts by lists of columns to sort by from left to right with list append and unit [] A well principled `Action` class packaged up might be quite useful!
Isn't this more a problem with Arch Linux than with Haskell? Most other Linux distros and macOS doesn't really suffer from this. Something about dynamic packages/linking? Obviously I'm not saying we shouldn't strive to fix the situation, just didn't care for the aggressiveness towards Haskell, just because your distro does th8ngs differently.
The problem here is a clash of philosophies. Arch thinks are build time dependencies should be managed by `pacman` instead of by `cabal` and hence all the libraries are dynamically linked. The truth is, no sanse Haskell developer actually does this as far as I know. And consumers of Haskell (pandoc, idris, xmonad) are _never_ interested in dynamic libraries anyway; they're interested in binaries, which are a lot more convenient in statically linked form. Arch Linux maintainer is shooting both developers and users in the foot at the same time with the decision to religiously follow the 'Arch' way of packaging. I think it would be a good discussion to have, but I'm not sure where the right place is. 
Yeah. Seems weird that they need all of the Haskell toolchain for it
I favor adding inductive types via eliminators. This can vary wildly in complexity, but the best argument in favor of this is the sheer complexity of the alternative version, which is primitive pattern matching and fixpoints/recursive definitions. If you want primitive Coq-style match and fix, you need: - A termination checker with a notion of structural order on constructors - An evaluator which takes into account which arguments of a fixpoint are structurally decreasing, and reduces accordingly. - An implementation of unification for left-hand-sides of patterns, which gives us dependent matching. In Agda-style, you would not have fixpoints, but instead have a primitive notion of pattern-defined function, and these functions behave as opaque symbols until applied to a collection of arguments which match a pattern. With eliminators, you need to have: - A syntax for valid inductive definitions. The simplest would be strictly positive non-indexed non-mutual types (or alternatively, strictly positive functors). - Constructor and eliminator symbols for each inductive type, with the right reduction behavior. The latter part is not easy, but there are quite compact and elegant schemes for implementing it. It's the topic of my current research (which concerns eliminators for higher/quotient inductive types), and I think I can cook up a nice implementation for simple inductive types if you're interested.
No, I responded to the part I quoted about needing a general purpose shell, clearly that's not talking about Haskell and it's giving bad advice. Git for Windows broke our path conversion code to support git.exe which does not qualify it s general purpose anymore. MSYS2 is the upstream of Git for Windows SDK. Neither of them are sandboxes, or at least to the extent one is so is the other. We don't add to PATH because of DLL hell. No software should do that IMHO. It means running an executable to setup the env and launch the shell but that's no more difficult than running any program but that's a far better option than risking breaking the other software on the user's computer. My job involves creating another cross platform software distribution and I wouldn't even add globally / permanently to PATH on Linux or macOS and those OSes have far better dynamic library searching and isolation capabilities. PATH is still very blunt tool to be wielded with care about consideration.
With "perl"
It is also telling that Haskell themselves use MSYS2. https://ghc.haskell.org/trac/ghc/wiki/Building/Preparation/Windows
There is an enhancement request for adding multiple public libraries as well: https://github.com/haskell/cabal/issues/4206
Small typo, Caley should be Cayley.
I think you'll be perfectly fine with the updated one! Have fun!
&gt; there are quite compact and elegant schemes for implementing it. Unless you need to encode polarity in order to be able to define data types like `Rose`. And you do need it. Yes, there is the Indexed Functors approach with its annoying limitations like an index can't depend on a parameter or mutually defined data types can't have distinct parameters (or maybe they can, who knows, I asked the authors and silence was the answer). My favourite test is to check whether your representation of data types is able to encode all the data types in Agda's [reflection framework](http://www.cse.chalmers.se/~nad/listings/equality/Agda.Builtin.Reflection.html) and derive eliminators/equality/whatever for them. Good luck with your research. Do you have any descriptions-style encoding of quotient inductive types? That perhaps requires very/insane dependent types...
Good luck. He's a stubborn tyrant as near as I can tell.
Not really :\ LYAH mentioned something about identities that's meant to help with something but I can't recall if that something was folds or perhaps something different
Map can work fine if concatMap is used, but it's probably less efficient than the fold solution.
Without straight up giving you the code, I'd say that I would either write this function with foldr but without map, or I would write this function with map and concat and no fold. Writing things as maps and folds is good, and getting used to reading them and understanding them is useful. But that doesn't mean the two of them are the only two functions you'll ever use. Map is just "transform each thing in this way" and fold is "combine each thing in this way", which makes them broadly applicable to many situations, but not all. 
I was just talking with some friends, and they pointed out: lists and folds are equivalent. So it is not hard to determine whether or not a list operation is a fold (they all are), but sometimes it is hard to find the translation from a recursive function to a fold
Sorry if I wasn't meant to post the answer. I'm at a christmas party and might not be thinking straight :)
Interesting example! Thanks
&gt; If you must have rose trees and such, you can inline external functors into mutual types. You have `Arg Term`, `List Clause` and `List (Arg Term)` in Agda's Reflection framework, so inlining would require too many silly data types like `ListClause`, `ListArgTerm`, `ArgTerm` and there is also a similar `Abs` there. That's just ugly and you either have to force the user to write this junk or to generate it silently behind the scenes and then confuse the user with a strange error that mentions types he didn't define. &gt; although we (Ambrus Kaposi &amp; myself) have only did formalization yet for infinitary indexed quotient inductive-inductive types Any postulates? Could you show how your universe looks like? Just very curious. &gt; we can also freely use arbitrary higher paths, and `Refl` and `J` in descriptions. That's impressive. &gt; The basic idea is that descriptions of inductive-inductive types are contexts in a "strictly positive" type theory. &gt; If we want induction-induction, the type theory must be indeed dependent But contexts are linear and don't allow earlier terms to depend on later. And IIT are not "linear". I'm perhaps misunderstanding something.
 x `par` y `pseq` (x, y) using combinators from [parallel](https://hackage.haskell.org/package/parallel-3.2.1.1/docs/Control-Parallel.html)
Why bother with uncurrying type a &lt;-&gt; b = (a, b) -&gt; Type rather than using type a &lt;-&gt; b = a -&gt; b -&gt; Type ? That said, if you're going to be doing multiple variations then replacing `(a, b) -&gt;` with `Whatever a b -&gt;` for some data type would give you the ability to distinguish `&lt;-&gt;` from `·-` by using different lifted product types.
There is a Zimbra Collaboration Repository, there are 3rd party modules for Apache, Perl has CPAN, Python Packages... (hundreds more) Arch has to have 1 and only 1 otherwise a modern Unix is just too confusing. The solution cannot be Haskell specific. A Linux binary maintainer doesn't want their users having to deal with any Haskell specific build tools. Which means the Haskell ones need to use a foreign build tool as a wrapper. I think a patch to Pacman to allow it to know "X is a Haskell library so send Y to Stack" and then wraps the to be installed files into a Arch binary package (binary + information on the final file locations) would sell a lot better. This technique BTW would be a good idea across the board for macports, fink, debian, RedHat, and finally Windows. Support for foreign building and packaging tools is extremely important. It is something that Haskell doesn't do right now. Worse for getting someone to do it, it is of absolutely no value to people who regularly use Haskell. This sort of things would be a good Summer of Code project though. 
I've been a release manager of a Linux distro, and also a packager. It's easy to look at someone you disagree with and assume they're wrong and stupid; it's more difficult to try to understand why they might do such a puzzling thing. But there are good reasons. Distros insist on shared libraries usually because of the burden of updates. If, say, OpenSSL has a security problem, you can build just the new OpenSSL and distribute it, instead of building and distributing all of the apps that use OpenSSL. All the distro packages get the update automatically; at most, a restart of the program may be necessary. With static binaries, all those packages need to be rebuilt, either by the vendor (at the expense of build server time, which is often scarce and/or expensive, not to mention the delay in getting the update out) or by the user (at the expense of time, system load, etc.; the comments in the linked thread should illustrate some of those problems). You've also got custom apps not built by the distro vendors, such as those installed using things like stack or cabal. With dynamic libraries, they get the update too. With static libraries, they don't. The burden is on the end user to make sure those get rebuilt, which is annoying at best. And when the update is security-related, and user apps don't get rebuilt, you can have security problems live on for years after they've been fixed. How does the Haskell community propose to solve these problems, if not with dynamic libraries? It also strikes me that, in the linked thread, there's talk that rebuilding code with shared libraries sometimes breaks things, such as how xmonad usually works. Is that true? I would hope the Haskell community considered that a bug, but some of the folks in that thread seem to be defending the behavior as desirable for some reason.
Yes the Twisted Pointers paper defines `Distributive` but I decided to follow the library for simplicity 
Yea, it sucks. I actually removed all haskell packages on my arch installation and installed them using a statically-linked cabal binary I downloaded somewhere. Works way better than the Arch way of packaging.
Does anyone here use the Haskell platform? My outdated experiences with it were problematic. I reverted to the bare-bones GHC installers and then moved to stack and since then all's been fine.
Thanks! This works (as in it compiles), but I'm not seeing any speed differences. Does this mean GHC doesn't think the computation would benefit from running in parallel? I'm compiling with `-threaded -O2` and running with `+RTS -N`
The problem is that dev should be able to break compatibility. _as long as the version number reflects this_ and the details of that haven't been worked out on Arch.
&gt; Does Haskell not do this? If not, why not? I'm not sure. It may be more relevant to ask "Did the maintainers of haskell packages on Arch not do this?" and the answer is clearly "no". I'm not sure why not.
So did I. But using nix is maybe better as it supports caching builds.
Lots of sections in the user’s guide actually mention the corresponding paper. If you think this is useful, maybe it can be expanded. And maybe adding a proper “References” section to the user manual where all cited papers are collected (see http://build-me-the-docs-please.readthedocs.io/en/latest/Using_Sphinx/UsingBibTeXCitationsInSphinx.html#rst-biblio-ref for ideas). As always, volunteers are welcome!
This is amazing. Great work! I'd do things to have the same for macOS...
Could you give a (link to a) brief explanation of why `Rose` is difficult to encode in an eliminator-based system?
Sadly, I can't understand enough of your comment to implement what it means. [This](https://gist.github.com/MaiaVictor/4d53e60bad6cb2c9c52a0d01d89c094d) is what I have so far, which is CoC (with Type:Type and Fix, but that's not important). My question stands: how do I take those 200 locs and extend it in a way that my small language will allow those things? I'd be lovely if eventually when you have some time you prepare an ELI5 answer to that question; like that Simpler Easier post, but now with the complete stuff.
Committee members should put their personal opinions aside. Wouldn't you agree that the Haskell committee should rather act in the interest of what's best for the community? And ever since that decision based on personal opinion was made the community has been divided.