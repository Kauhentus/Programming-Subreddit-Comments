The 8-year-old ticket about this: https://ghc.haskell.org/trac/ghc/ticket/989
I do, but it has a lot of problems (mutability, mostly).
It actually works (Mark, gave a BayHac presentation of it a couple of years ago), but I can't find any video recording of it.
Take it again! Lots of fully fledged software developers "just don't get" FP. Sometimes ideas don't fit well into our heads, but you'll have a much better idea of what to expect this time around. Ask some questions on IRC and here on /r/haskell.
a quick google reveals a workaround for the "too many variables" issue? http://stackoverflow.com/questions/25344259/too-many-variables-to-run-ghcjs-program-with-node
A quicker look shows that I was the OP of that question. I'll take a look again but I'd had little luck with their suggestions. Will try again, just getting ghcjs to run and do anything was a pain though. 
&gt; Cygwin/MinGW in a production environment is not something I'd recommend either; but we've had a build chain that depends on it working as our only method of preparing software for production since before I joined the company. 
A Clang-based MSVC is a really long way off a Windows friendly path. I think MSVC is really just a different backend target, far less work than something like ARM, but more work than Clang. At the moment there are places where I would link C++ with Haskell, but have to go through a separate dll because GHC is not MSVC compatible. It's annoying, but not fatal.
I think the underlying principle of "never break the build" is the same. My second principle is "assume you don't have enough resources to test everything", and I don't think that is shared there (in fact, I think they've gone off in the other direction). I agree with the first principle, but I couldn't find anything else to lift - but I didn't go as far as trying it in practice.
Then you woud compare the two applications as a whole. Of course you are right that using even the same piece of code in a different settings may have a significant impact on the performance. This is why using micro benchmarks mostly does not teach you a lot about their usage in any other settings. However, when algorithm A has 2X the performance of algorithm B, and you can statistically infer that this difference is significant and you are not simply looking at the best times, then you can draw some conclusions and you'd probably use A instead of B, unless you have good reason to believe that B will beat A in your application.
I think the one "twist" is that `bors` appears to have a pre-integration "review" phase to the workflow. That might be orthogonal to the goals of `bake`, but it does seem like something that some teams might want to integrate in some fashion?
oh, whoops! I shoulda checked that :-)
I don’t really understand the significance of this example. The outcome obviously depends on the logic you implement explicitly, regardless of the library.
Nice! I've got a similar [gadget](https://github.com/bburdette/hsnixinit) on github - a script for setting up the project, and another for adding a cabal project to a nixpkgs clone. [Here's](http://pr06lefs.wordpress.com/) a blog post on using it to set up a yesod project. Yay for automation! I'd like to see a way to automatically add all the dependencies for a given project into nixpkgs. With my script its pretty much compile, add missing package, repeat. 
I did not know that there are soooo many frp libraries out there. Did you make any observations on performance? As in event throughput? I am aware that is irrelevant in GUI applications, but anybody doing *"complex" event processing* would need low overhead from the frp framework itself.
Yes and no! *If* you implement the behaviour using a few basic FRP combinators, then indeed, the outcome depends on the logic we implement via those basic FRP combinators, not on the choice of library. However, what is interesting about this example is that some libraries can also implement some of the scenarios in a different way: by rewriting their signal graph at runtime. If they do, then the outcome will depend on the semantics of their signal-rewriting primitives, which vary from one library to another. Since there can be two ways to implement the example, the specification is split into two parts. First, the [static part](https://github.com/gelisam/frp-zoo/blob/master/README.md#static-implementation): As with TodoMVC, the value in FRP Zoo is mostly in having the same program implemented using different libraries. For this reason, the first part of the specification requires all the implementations to implement all three possible outcomes, using the same logic. In this part, the outcome indeed depends on the logic which is implemented and not on the library, which is why all three scenarios can be implemented using all the libraries. So far, any sufficiently-simple example would have been a good choice. What makes this particular example interesting is its role in the [dynamic part](https://github.com/gelisam/frp-zoo/blob/master/README.md#dynamic-implementation) of the specification. [Evan's presentation](http://www.reddit.com/r/haskell/comments/2h43hs/elm_controlling_time_and_space_understanding_the/) clearly describes the distinction between static and dynamic implementations and the three categories of dynamic implementations which lead to FRP Zoo's three scenarios, but in a nutshell, a static implementation defines all the possible interactions between signals before the program begins, while a dynamic implementation defines some of those interactions at runtime, in response to events. The second part of the specification allows implementations to demonstrate the dynamic features of their FRP library by reimplementing one or more of the scenarios in a dynamic way. So, *if* you implement toggling by rewriting the signal graph so that clicks don't reach the counting part, and then rewriting the signal graph so that clicks can reach it again, then the outcome will depend on the choice of library, because the different ways to implement graph-rewriting systems give rise to different semantics.
https://github.com/yesodweb/Shelly.hs
The sandbox has its own `bin` folder which you have to add to PATH. Or just execute .cabal-sandbox/bin/yesod
&gt; Because Cygwin/MinGW in a production environment is just some kind of cruel joke that never works. That's quite an exaggeration. It takes some experience to set it up correctly, but once that's done it does an amazingly good job of emulating the *nix environment, considering all the various hurdles it has to overcome. I do think that there needs to be some good documentation/tutorials (that aren't like 10 years out of date) to teach people how to use it correctly and effectively. There is a lot of scattered documentation out there but it takes a lot of experience to learn all the gotchas ("abstraction leaks").
Cabal 2! Nix 4! Dev! ___HIKE!___
The [haskell wiki](http://www.haskell.org/haskellwiki/Category_theory) has some material on categories and there is a [short wiki book introducing category theory for haskell programmers](http://en.wikibooks.org/wiki/Haskell/Category_theory)
There are a fair number of optimizations available. In regular FRP you have the push/pull type optimizations which let the system do minimal work by responding to incoming events and requested samples with independent systems. Unfortunately, the premiere example of it, Reactive, has a time-leak. In arrowized FRP you have the GADT-based rewriting in Yampa, you have the causal commutative arrows transform, you even have Sculthorpe's signal vector formalization which lets you get push/pull semantics in AFRP. Whether this is "fast enough" is pretty unknown to me right now, but optimization is the name of the game in FRP.
[GHC.Tuple](http://www.haskell.org/ghc/docs/7.0.3/html/libraries/ghc-prim-0.2.0.0/src/GHC-Tuple.html).
 data Pipe l i o u m r data Wire s e m a b Are probably the longest interesting ones I use in practice. Oh! Also I created this one: data Plugin url theme n hook config st from the `web-plugins` library http://hackage.haskell.org/package/web-plugins-0.2.6.1/docs/Web-Plugins-Core.html
In my own code? probably data family MArray world rep lay (view::Locality) (rank :: Nat ) st el I can hide all but the last 2-3 from most end users, but the rest really really make my internal library engineering much much easier. 
theres no work going on to use emscriptem for doing a ghc -&gt; js. (unless its a secrete). The way that GHC uses LLVM is very much at odds with the class of computation I believe emscriptem can support http://kripken.github.io/emscripten-site/docs/porting/guidelines/portability_guidelines.html talks about some of this a bit more. point being, until ASM.js supports goto / js supports tail calls, i dont think the way GHC uses LLVM can naively map to emscriptem. 
It'd probably be lens's types, not very surprisingly. Things like `type Optical p q f s t a b = p a (f b) -&gt; q s (f t)`.
He showed it off at icfp as well, it's incredibly cool. I even started using it for a few things on my local machine.
Is there a language construct analogous to a data declaration but for type variables? Eg if lots of functions are parametric in 3 types, can I package those into some type-tuple and simplify the declarations?
&gt; But none of this really changes the fact a lot of Haskell packages are simply hostile to platforms like Windows [...] One could also turn it around and say the Windows platform is hostile to POSIX-oriented software by *not properly supporting `autoconf`*, as `autoconf` is IMHO just the right tool to detect all those impedance mismatches in a generic way, you have to cope with when porting a POSIX application over to Windows environments. Otoh, the underlying problem probably is that you can't expect every Haskell package maintainer (of which the vast majority develop on POSIX-ish systems) to continuously look after keeping his/her package compatible with a platform he can't easily test his package on. So the best we can do IMHO is to keep living with a POSIX layer (together with a `gcc`/`clang` toolchain) on Windows emulating enough to keep basic tools such as `autoconf` happy to have Haskell packages developed for POSIX systems work out-of-the-box on Windows with a rather high chance. Here's btw one of those impedance mismatches I fought with just recently, only to find out that on Windows it seems you have to prefix POSIX function calls with `_`s, otherwise you get confusing linker errors in GHCi: - https://github.com/haskell/time/issues/2 - http://www.haskell.org/pipermail/ghc-devs/2014-October/006697.html 
Do type parameters of function signatures count? If so then look at: [Control.Concurrent.Session.run](http://hackage.haskell.org/package/sessions-2008.7.18/docs/Control-Concurrent-Session.html#v%3Arun), it has 17 of them!
I'm actually writing a tutorial on it: http://blog.spacekitteh.moe/posts/understanding-hask.html
the fuck o.o
Thanks for the answer! That's disappointing, but understandable. I'll fiddle some more with ghcjs I guess. Thanks! 
Will there be a video of the 'Structural Typing for Structured Products' talk? Thanks!
A Haskell x-mas tree!
loved the comment: &gt; Manuel says: Including one more declaration gives a segmentation fault.
Nope. You can make product kinds (or use promoted tuples) but they're not very useful. The typechecker (correctly) refuses to believe that `a ~ Pair (Fst a, Snd a)` simply given `a :: KPair k1 k2` because GHC.Any inhabits all kinds, and this prevents the technique from working well. 
Exactly the right amount of type parameters and you didn't use `b a c k s t a b`? For shame.
I'd like an alternative video source where I can view this without proprietary software.
I tried but GHC didn't like the repeated parameters
Hey Dan, Just uploaded it :-) https://skillsmatter.com/skillscasts/5297-structural-typing-forstructured-products
Sweet. Awesome domain name too. I can't find an RSS feed though. :/
I don't know about any specific Haskell library, but it's possible to compile reactive programs to very efficient code. The causal stream functions `Stream a -&gt; Stream b` (the core type of arrowized FRP) are in one-to-one correspondence with (possibly infinite-state) Mealy machines, which means that you can compile reactive programs down to very efficient "single-loop" state machine code. This idea has been advanced farthest by the (mostly French) work on synchronous dataflow programming, as embodied in languages like Esterel, Lustre, Lucid Synchrone, and ReactiveML. This work has seen an awful lot of real-world impact: the commercial version of Lustre, found in the SCADE environment, has been used to develop the control software for safety-critical applications like aircraft and nuclear power plants. So constructively we know this style can be used to meet hard performance deadlines in domains where the penalty for failure is *literally* radioactive flaming death. :) 
From looking about a bit it looks like inspired by or similar motivations to [TermKit](http://acko.net/blog/on-termkit/), right? I've been vaguely considering using Haskell's WebKit binding to maybe bring TermKit's ideas to fruition.
This talk describes the "Lucid" language that was invented at Barclays. It's a row-typing based type system, but lacks higher-order functions which allows them to move a lot of machinery directly into the types.
Manuel Chakravarty, perhaps? 
Ah, no HOFs sounds like an interesting tradeoff in this space! 
Would “whatever junk happens to be lying around” happen to include `.git/` and `.hg/`? I'll look into this, but I'm not sure I can filter files out as aggressively as you do while I'm still doing it via shell script. Thanks.
Can anyone point me to a tutorial on building a bare bones FRP implementation? I believe I understand how they work; but it would be pretty useful to read a minimalist version.
Yes, it's very, very similar to that!
I haven't given it a lot of thought, but in its current incarnation, linking resources together in 'rest' would be manual. I'm also not quite sure how to give this more structure in Haskell: you'd have to somehow explicitly mark references to other objects in your API. This could probably be done: you'd define a data type of references, and those could be turned into `href`s in the appropriate places. I'll thing about this a bit more. Do you know of any clients for consuming APIs that are written in this way? I.e. what are the benefits?
http://www.haskell.org/pipermail/haskell-cafe/2013-October/110891.html "edwardkmett: At one point we had a type in lens where one of its arguments started taking parameters of the form i m a s t a b u. Upon reflection, we let it win the argument and decided not to implement the function."
The implementation is almost shorter than the signature o.O.
It's funny, when you use more kinds than just *, tons of type parameters all of a sudden feels pretty manageable. I think it's analogous to the utter horror I would feel if I saw a function with seven Bool arguments (of course this is way worse with *, since there are an infinite number of types, whereas there are just three bools).
I like how the documentation starts with: &gt; Run! Which I did.
I wrote a [blog post about reimplementing the basic reactive-banana primitives](http://gelisam.blogspot.ca/2014/07/homemade-frp-study-in-following-types.html) a few months ago. I'm not sure it counts as a tutorial, though, since it's more about listing the steps I took than listing the steps I think others should take.
I don't have time to read it over at the moment but it looks like what I'm looking for, thanks!
Yeah. The challenge is of course balancing when a closed universe is enough vs truly needing an open world for a given parameter. Eg that locality datakind is a closed universe approximation of the precise memory layout of the buffers underlying my multidimensional arrays! And there's only 3 possible values in the approximation! In fact, I'm not sure if there's any meaningful finite refinement of that approximation that would be useful for my use case. That is, if a small finite taxonomy of cases can effectiveky classify all your scenarios, that can induce a nice closed universe. Or something like that. 
&gt; So constructively we know this style can be used to meet hard performance deadlines in domains where the penalty for failure is literally radioactive flaming death. :) I want to propose this for next weeks newsletter quotes ;)
&gt; three bools Spoken like a true programmer, and not a mathematician. NB: You should always use a capital 'B' when talking about the Boolean numbers, since they are named after their discoverer: [Mr. Boole](http://en.wikipedia.org/wiki/George_Boole). 
&gt; one of its arguments started taking parameters of the form i m a s t a b u. Upon reflection, we let it win the argument In the future, lenses are reinvented, but a [different choice is made](http://futurama.wikia.com/wiki/Roberto).
The benchmarks for this library look promising, but I'd like to see how it stacks up against [pipes-concurrency](https://hackage.haskell.org/package/pipes-concurrency), which is the tool I usually reach for when doing queue-based concurrent programming. If this library is performs better than pipes-concurrency, I'm going to seriously have to consider writing a wrapper for this that provides a similar interface.
Very cool, but shouldn't the route which creates the shortened URL be a `PUT` or `POST` instead of a `GET`, since it creates a new URL?
Thanks, fixed. Yeah, I'd really like a tool to automatically generate nix expressions including all deps &amp; add them to nixpkgs (manually adding packages to all-packages.nix is just so annoying :))
Interesting, if the call is idempotent GET would technically be acceptable, but I can't tell if the same shortened URL is returned on subsequent calls. (I can barely read Haskell)
&gt; A Clang-based MSVC is a really long way off a Windows friendly path. Why do you think that? As far as I can see, Clang/MSVC is already *extremely* good, and offers a real modern compiler for the toolchain while maintaining link/ABI compatibility - as of a few months ago, Mozilla was even able to build a fully working and tested Firefox using Clang-for-MSVC only, with no native MSVC involved. The only exception was a single file (of tens of thousands) that used SEH features. I think this is a pretty good stress test, all things considered. You can even download prebuilt binaries of this toolchain! But considering Microsoft's patents on Structured Exception Handling are dead now (or at least pining for fjords for a short while longer), I imagine it's not much longer until the final-patented-piece is in place. &gt; I think MSVC is really just a different backend target, far less work than something like ARM, but more work than Clang. I think I disagree with this claim. :) Clang is already essentially working as a C compiler/assembler for the compiler, thanks to its strong GCC compatibility, and was only a day or two to get working - except as a Haskell preprocessor (and using a C preprocessor for Haskell is *wrong no matter what*) - and ARM had the benefit of being developed in a very familiar Linux environment, using LLVM as a backend - it didn't really need substantial backend support, mostly runtime support. An MSVC port really doesn't have anything like that to leverage. &gt; At the moment there are places where I would link C++ with Haskell, but have to go through a separate dll because GHC is not MSVC compatible. It's annoying, but not fatal. Yes, I figured it was something like this; I imagine we don't really jive with MSVC's linker format among other things.
The main annoyance I have with something like Shake is it requires a recompilation of all the build rules for any change to any of them, which is a bit unfortunate, and probably error prone. Perhaps we should just have a wrapper around the shake build that automatically runs `ghc ghc-shake.hs` or something first, to make sure we never mess this up... Also unfortunately, the build system absolutely cannot rely on any 'fancy tricks' like using `runghc` to run the build rules without compilation, or e.g. having a tool load/evaluate them on-invocation using the GHC API. We can't rely on features that might not be there on some platforms; GHCi doesn't work everywhere. All things considered, a bit unfortunate, but I'll certainly pay that price for the benefits of not using Make. :) **EDIT**: and honestly, considering it's our standard policy, I'd be willing to bet we would almost certainly include a copy of Shake in GHC's repository for us to standardize on and use. But luckily it's better off than `Cabal`; it's nothing more than a build-time dependency, and we'll never ship it. We'll just compile it 'first things first'.
I'm interested to know is having access to the book Programming in Haskell by Graham Hutton an absolute necessary.
Yep, 'tis my bad. :)
Where do they say this? 
It's mentioned in an email I received today. &gt;Dear FP101x student, &gt;This Wednesday the FP101x course will start. We are very excited that so many people signed up! Functional Programming is an essential tool in your coding toolbox now that every contemporary programming language supports lambda expressions. &gt;We want to remind you again that this is a course on Functional Programming and not on Haskell. We use Haskell because it is pure, and unlike non-pure functional languages, the Haskell compiler slaps you on the wrists whenever you are trying to cheat. Because of the principles you learned here, you will end up much better programmer in whatever language you use once you reach the end of the MOOC. Even if you already know Haskell or Functional Programming, you should learn something new. &gt;We designed the course such that if you watch the videos and read Programming in Haskell [\(buy it here with discount\)](http://www.cambridge.org/gb/knowledge/academic_discountpromotion/?site_locale=en_GB&amp;code=EdX2014), the exercises and labs should be doable for every student, no matter what background knowledge you have. So don’t give up and keep hacking; we hope to break the record for the percentage of people that finish any MOOC on Edx. &gt;Think like a fundamentalist, code like a hacker. &gt;The FP101x Team
oh wow, I didn't received this email, are you on the verified certificate track?
No, regular track.
Come on guys... You've made a new blog page and completely failed to make it mobile device friendly. It'd take just a handful of lines of CSS...
I didn't create the stylesheet - it came with Phabricator. :) I imagine the default Phame stylesheet hasn't been mobilized yet (a lot of other parts of Phabricator already are mobile-ready). In any case, I wanted to tweak the stylesheet anyway, so I'll probably put it on GitHub for PRs. But I honestly have zero clue how to CSS. I mean, absolutely none.
I'll be the first to say it: I'm not that keen on purple. I know it's the imperial colour and all, but … I guess I just think grey is prettier. I like the rest of the look, though. And I know at least one person who will like the fact that there are talks visible on the main page.
The [rAUR](https://blog.haskell.org/diffusion/AUR/) link 404s
Hasn't `Cabal` been demoted to a pure build-time dependency with GHC 7.10 as well? I.e., the `ghc` package doesn't depend on `Cabal` anymore afaics.
Actually, you might be right, now that we got Duncan's changes out of the way to separate them.
The course is based on the book and it will be useful for additional reading, but it's not a requirement. Hope this helps!
Wow that;s quite a few languages. I work as a PHP developer mainly, some JS every now and then. I am learning python in coursera's programming for everybody and IIPP, I've been lookin into elixir as well.
That's a pretty inefficient way to implement fibonacci.
Put it up, it can't hurt. :)
Ah, this is a bit of a can of worms. Let me see if I can do a decent summary. What most people call a 'REST API' is actually just ad-hoc JSON/XML over HTTP using HTTP methods and URL formats to choose which server-side procedure to invoke. Full on REST is quite a bit different - in fact as I've said in another comment in this thread I think the browser is actually the most RESTful client I know of! HATEOAS is more than just giving up links to related resources. It means that you make a single request to a well-known URL and then all further requests are made based on the contents of that response. No URL formatting, no query string fudging, no nothing. Just making requests based on that response. In essence, you can think of the server as exposing a state machine to clients. The initial response contains the initial state plus a set of available transitions for the client to choose. The client makes a transition by making a request to one of the resources given to them in their 'current state' and the server returns the 'new state' plus all the available transitions to make. The beauty of the approach is that the 'determine which transition to make' phase is accomplished through standardized processing of the representations returned. How does a browser know which URL to go to next? Well, it renders the HTML it received from the server and the HTML bits tell the browser where to go when a user clicks on a link. Likewise, if you had a standardized workflow representation your 'browser' could just execute the workflow and make requests according to the steps contained within. The magic of REST is that the client doesn't need to be terribly sophisticated. Ideally, we could just write one client to rule them all and get any functionality we want by either pointing it at new resources or by teaching it how to grok new representation formats. This is sort of what we're doing now on the web, though we've chosen to just expand on a subset of representation formats (HTML/CSS/JS) instead of making new ones. If I had more time and experience with it, I'd use this space to make a comparison between monads and REST, specifically how do-notation allows disparate things to be composed in a unified manner. There's definitely some symmetry here which is why I think (a) REST is so attractive and (b) most people get REST wrong. I hope I've at least answered a tiny fraction of your questions. I know this response was long and rambling, but if you made it this far you have my thanks.
It may be because I'm not a terribly experienced Haskell hacker, but I find data Plugin url theme n hook config st Far far easier to understand than data Plugin u t n h c s Is there a syntactic reason for this? Or is it a similar situation to the inextricable sorcery of mathematical notation, where it's just part of the culture to use inscrutable single letters "because it's quicker to write/type"?
And if you're learning elixir, then you might as well learn ruby ;) I've found that there is a gravitational attraction toward learning these languages. My rabbit hole started with python, but it could have just as easily started with ruby - both being "gateway drug" languages of friendly readable syntax for the novice and widespread adoption in development (i.e. libraries, frameworks, and jobs). But soon you discover that there is no way around javascript if you wish to learn any programming for the web. Fortunately you discover CoffeeScript which is "just like" python and ruby (whichever path you came from). You soon learn that ruby and python are similar enough to be compatible while being different enough to still be interesting. At some point you figure out that you "have to" know javascript at some level - that neither coffeescript nor ruby frameworks can completely shield you from it - same as SQL. You also discover that python in particular borrows a lot of inspiration from Haskell in its syntax and its semantics. The best part of python is list comprehensions which are straight out of Haskell. You also learn that even javascript (like python) is more of a functional language than an object-oriented language. So you bite the bullet and add Haskell to your growing list, because it plays well with what you are already learning. Then the ruby community goes and builds elixir on top of erlang and the BEAM virtual machine seducing you with the promise of parallel thread-based, fault tolerant, concurrent programming all with friendly ruby syntax you are already familiar with and without learning esoteric erlang/prolog syntax. So: ruby/python -&gt; coffeescript/elixir -&gt; javascript/haskell seems to be where the rabbit hole takes you. Or me, anyway.
Did you just say "I am also probably going to re-write GNU as I need it except in a haskell way." ... like in "[every GNU program that exists](https://www.gnu.org/manual/blurbs)"?
I don't even think fibonacci is a good piece of code to put there.
I guess there is a valid reason why we can't just lift normal functions to the type level. But which one would that be?
But it is super easy! And because of purity and and Quickcheck we can just replace it with a better version and no testing. 
I noticed that if you try to do "let x = 1 + 2 :: Int" it behaves as expected, I'm pretty sure it has something to do with when types are coerced, but I don't know beyond that
I guess that the goal there is to show a solution of a famous problem that can be understood by someone who doesn't know Haskell. A more efficient implementation might be meaningless to them, and this is a promotional and educational website. 
http://new-www.haskell.org/downloads should definitely have a `cabal sandbox init` in there.
The homepage is mobile friendly, it's the blog itself that is not. http://i.imgur.com/66BBcc0.png
This is quite helpful. Thanks.
In many cases single characters are used because there really is nothing more meaningful. Such as: show :: (Show a) =&gt; a -&gt; String There is not really anything you can say about 'a' except that it is showable. Other times a little more could be said like: class Category cat where We know the type variable must be a category so 'cat' is arguably a bit better than 'a'. The compiler doesn't care -- but humans reading it will. In theory we would also have: class (Category arr) =&gt; Arrow arr where but the Arrow library is older and using single letter type variables was more standard at the time. Either way -- it is ultimately a matter of taste and there is no universal agreement. 
This seems to give a rough set of the syllabus which will be covered: https://github.com/fptudelft/MOOC-Course-Description/blob/master/TOC.md You can also see some of the [advanced topics](https://github.com/fptudelft/MOOC-Course-Description/blob/master/Advanced.md) that may be covered there for extra credit.
Ugh, you guys are using Cloudflare? They like to throw roadblocks at Tor users like me.
Thank you for the thoughtful and enlightening reply! It's always a breath of fresh air to see a response to an /r/haskell comment in my inbox. For single type variables it totally makes sense. I'm definitely a proponent of the same sort of approach to generics in C# - List&lt;T&gt; is preferable to List&lt;TItem&gt;. But when you start to branch out into lots and lots of type parameters, especially ones with specific semantics like those of lens, I still think it's more readable to spell out what each parameter is for.
I heard 1-2 months, but no telling. One thing they want to do is get the tryhaskell backend running locally first without calling out to the other server.
Obligatory: https://github.com/haskell-infra/hl
for 43 lines of code, it's very unreadable due to its imperative nature.
Yes, there is explicit acknowledgement of Tor in the post under the Cloudflare section. None of us want to artificially inhibit Tor users, of course. We have not put Cloudflare in front of everything (nor can we, for various other technical reasons). At the moment our security settings are set quite low, and we only enable it for a few domain records as the rollout is incremental - we don't want to break clients that may have pinned certificates (or, at the moment, may not support SNI properly; but we'll be moving off SNI soon enough hopefully). If you can see https://blog.haskell.org, which is actually delivered via CF, that's the configuration we're using on all our endpoints, which has minimal security configuration at the moment ("Essentially off"). I have been keeping an eye on this issue (mostly by following and pestering Cloudflare), because I of course don't want to inhibit Tor users; CloudFlare just gets a lot of bad actors using exit nodes (who ruin it for everyone else, because they're assholes). They're working on changes to Tor exit node traffic now apparently, so hopefully this will be less problematic soon. In the mean time, if you get flagged, it hopefully will only because we have consciously raised the security settings for whatever reason. If it happens then - or is happening now - please report it to us (IRC, email, and you can file bugs anonymously to Phabricator through email at `bugs@phabricator.haskell.org`), and we'll try to fix it.
The reason is that it is a bunch of work to to this in a general fashion. The goal is that we _will_ allow full dependent programming in a future release of GHC. The next NY Haskell Users Group (October 24) will have a talk on precisely that topic! http://www.meetup.com/NY-Haskell/events/209245642/
There are a lot of obvious inefficiencies on this code (such as calling `map fst` repeatedly without memoization, the recursive high order stack implementation, pure functional datatypes, and so on). But solutions would make the code more complicated. I don't want to introduce type classes or monads, I don't want to make the Graph datatype more complex, and I obviously don't want to fill the source with "!"s and `{#- INLINE -#}`s. The code is good as it is. That is why I was looking at super compilation as a possible solution. Considering the program itself is quite small, maybe it could work on this case?
So using a real graph library / data type is out of the question? If you're hell-bent on not using [a real graph library for Haskell](http://hackage.haskell.org/package/fgl), perhaps http://hackage.haskell.org/package/containers-0.5.5.1/docs/Data-Graph.html would help? Or using vectors instead of lists? Super-compilation is not simpler than fixing your data types and it won't recover as much performance as you'd like here. Fusion of list operations is still a list operation.
Yes, the whole point of the subject is coding it and explaining your implementation / why it works. Using existing data structures is allowed. Edit: I'm not sure the use of lists is actually a problem since I iterate through all elements anyway, so the complexity is the same of an array. There is cache miss, obviously, but do you think that would make such a big difference? I was guessing most of the slowdown came from the high order recursive implementation of stack, the unnecessary calls to `map`, etc, which I guess would be fixed by supercompiling. [Here is the profile info.](http://lpaste.net/112539)
Given that this is a class and team assignment, shouldn't your question be worked and solved wth your team mates? 
Then at least use vector in place of list. Check strictness. You're almost certainly not using linked-lists in any of the other implementations, no need to make the Haskell solution needlessly silly.
&gt;And why Data.Vector? http://stackoverflow.com/questions/9611904/haskell-lists-arrays-vectors-sequences &gt;"The Data.Vector package provides all of the array goodness, in a higher level and cleaner API. Unless you really know what you are doing, you should use these if you need array like performance. Of-course, some caveats still apply--mutable array like data structures just dont play nice in pure lazy languages. Still, sometimes you want that O(1) performance, and Data.Vector gives it to you in a useable package." Don't worry about strictness until you've fixed your data type.
This is really impressive. Thanks to all working on this.
Does anyone have some list of things to keep in mind on how to make sure you write efficient (high performance) code? 
Seconded. The example's purpose is to acquaint you with what this thing looks like (Oh! it's an ML. Oh! no mandatory type declarations, etc), it's allowed to be superficial. A more snappy example is very welcome.
Thanks. I'll send you a pull request to remove the word "Natural" from the header, unless you do it yourself sooner. It's at best [edit: used to say "worst", but that depends on which way up your head is] meaningless, but I think it goes as far as being problematic. Few of us find this stuff natural until we have gone out of our way to amend our nature. Telling people it's "natural" risks suggesting to them that they're stupid if they don't get it already. Society is already too full of people telling each other what is or is not natural.
No changes in performance from using Data.Vector.
...?
Viewport configured correctly, lack of horizontal width compensated for, tap points spaced out, and text set to a reasonable size for said devices mostly. Also, if you zoom in it completely falls apart.
Also I have no way of knowing how efficient the vectors version is. Is it immutable or mutable? I don't know what you did. Graphs are awkward in FP langs. To get immutable graphs, you end up turning them into trees. Try the containers Data.Graph as well.
&gt; and no testing Well, the act of writing and running quick check properties is something I would categorize under the "testing" effort. It's just a different kind of testing. I guess you were implying "no unit tests".
Seems like `Queue a` is redundant (and also not a queue). An equivalent implementation: type Queue a = [a] insert :: Queue a -&gt; a -&gt; Queue a insert q x = x:q extract :: Queue a -&gt; (a, Queue a) extract (x:q) = (x,q) empty :: Queue a -&gt; Bool empty = List.null stack :: Queue a stack = [] You're wrapping `[a]` in functions for no real reason.
ah, yes. The zooming thing is especially bad
Wow. That is all. Just wow.
I've tried that. In practice though, I've found it makes the code *harder* to read—it's too easy to confuse type variables and normal variables. After trying it a few times, I've stuck with single-letter type variables unless there are too many that overlap. Maybe if Emacs was smart enough to highlight type variables in a different color from normal variables... but I think that's a bit too difficult right now. It would probably involve calling out to an external utility that parsed the code and figured out what was what. (You really don't want to write a full Haskell parser in elisp!)
 Pipe leftovers input output upstream monad result It gets really long and distracting to write. Spelling it out helps people who are new to the library or use it sparsely. People who use it frequently will be able to glance over documentation easier with the short names.
I don't understand why the refusal is "correct." Can you elaborate?
Really exciting stuff:)
Awesome! I'm really happy to see these changes. Here's to a more stable and contribute-able haskell :)
So what does profiling say?
I think the reason they do that is so they can change the behavior to visit nodes in a different order with the same `graphAlgorithm` function. Currently they have a depth-first search (unless I'm missing something) using a stack to implement `Queue`, but they could replace it with a proper FIFO queue to get breadth-first search or a priority queue for various best-first searches.
I guess "clear" is equally problematic. "Clean"? "Straightforward"? or just "Declarative, Statically Typed Code" is fine I guess!
No, just the one I use on a daily basis.
If you wish to follow this rabbit hole even further then you learn that Lua is a procedurally oriented scripting language very similar in feel to python, but much faster execution (comparable to javascript) and with prototype-based object inheritance paradigm that you learn in javascript anyway. Javascript can do everything Lua can do, but no denying that Lua code is much prettier and "pythonic." Likewise, once you tackle Haskell you might be tempted to learn Elm which is based on Haskell and specializes in rendering HTML, CSS, and JS for you from a paradigm of "functional reactive programming" which seems to be the hot new thing. I guess my point is that the rabbit hole truly has no end, short of your personal cutoff. The great thing is that there are *so* many options in choosing a great language while completely avoiding the world of ugly C syntax languages (except for javascript unfortunately) which seems to dominate the mainstream professional world, particularly the corporate programmer's world. I have an aversion to PHP and Perl as well for syntax reasons, but that's just my taste. Great to have so many options in the rabbit hole.
Compiling with optimizations?
oh haskell tree, oh haskell tree, your tuples so fair and lovely 
It'd be really difficult to write out the specific semantics of the type parameters in lens. Optical firstProfunctor secondProfunctor representation whole whole' part part' is about as good as I can guess right now.
'Cus I don't know how to make one : D
Quicksort looks nice.
Looks like relrod jumped on this and beat you to the pull. Other pull requests also welcome of course! We have a bunch of text, so I'm sure there may be other polish required :-)
ahaha quicksort is even more controversial than `fibs` since it not only isn't very performant, but its not even considered the right algorithm! (Since as any _real_ pedant knows, quicksort must be in place :-P) If we could do a sufficiently simple mergesort?
That it enforces sharing in cases like this is, if I understand, one of the reasons why the monomorphism restriction was originally imposed. I take it you know you can bring it back with `:set -XMonomorphismRestriction` .
The FP pedants know that inplace is a trivial coincidence and parallelizability is the main focus of quicksort.
+1 for "Declarative, statically typed code"
There's an atom feed here: https://phabricator.haskell.org/phame/blog/feed/1/ I need to add this into the sidebar of the blog.
I thought the idea was to *not* imply it's a bizarre foreign beast that most minds can never hope to engage. This part is worse than "natural" for that, because "declarative" is meaningless to the uninitiated, and for that matter "statically typed" lies somewhere between vaguely understood and controversial. I'd like to see a plain English headline description, with no technical terminology whatsoever. E.g. "Define what things are, not what they do."
Lua is a little too minimal though, I very much miss the battery of Python.
Fantastic work, thanks! One small request: add underline text-decoration for links (to be more precise, remove "text-decoration: none" from link style). It's really hard to visually distinguish links. I tried to do pull-request, but couldn't find that oblivious.css file. Thanks!
or better: in the header so it gets automatically picked up by smart user agents. **edit:** i meant html header, not HTTP header, if that was not clear.
It's true that there's a lot of work in this direction. IMHO the biggest outstanding issue (leaving aside concrete syntax, of course!) is the question of operational semantics: at the moment, term-level functions and type families mean rather different things computationally. While we can automatically lift a function to a type family (e.g. via the singletons package), it's not clear that they mean the same thing. There's also the fact that anonymous functions and partial application are forbidden at the type level, but Richard has some promising ideas to tackle that difficulty, and singletons goes quite a way already.
Sorry, I'll put the style up on GitHub (it's only on our server, right now) and post an update, since a few people wanted it. I'm happy other people will fix it, since as I said, I don't know how to CSS. :)
Oh right, that's a good idea. I think it will require a change to Phabricator however, so I'll have to file another ticket (I do this to keep track of improvements people want, or if I ever fix them upstream myself). Thanks for the suggestion.
We'll hopefully do it in a few weeks at the max. It's pretty close.
Well, `GHC.Any` definitely isn't the same type as `Pair (Fst GHC.Any) (Snd GHC.Any)` no matter how you define the `Fst` and `Snd` type functions: `Pair` is injective (it's a type constructor) and so it should only be equal to other applications of `Pair`. So GHC's behavior is forced by the decision to have `Any` inhabit every kind. I personally would find type-level products with real eta laws much more useful than all-kinded `Any`, but for now there are workarounds like [edwardk's](https://github.com/ekmett/indexed/blob/master/src/Indexed/Types.hs#L92). 
`https://blog.haskell.org` does not work from (latest) Opera. It works from Firefox on the same machine. It seems that there is some issue with the `s` part in `https`...
Glad to see it. Production values on the site are looking very good. I think it's right to spend front page screen on welcoming beginners, with links to useful reference information discreetly in the corner. Copywise, I'm inevitably out-of-step with the line on "type inference": I think there's a bit too much emphasis on the way types give you errors and not enough on the way types give you code (type classes, generics, etc). It's also dodgy to suggest that types are optional when sometimes they're necessary. I was glad to see "bidirectional" in there, as good practice making types informative (for humans and for instance inference, etc) is to be explicit about top-level types, then propagate them. I often feel that "type inference" is a bit of a nebulous phrase which covers a bunch of techniques better teased out from each other: specifically, "follow-the-plan" type inference (epitomised by Milner's "var rule") is a wonderful thing, but "guess-the-plan" type inference (epitomised by Milner's "let rule") is increasingly (ie already too) difficult and in any case nowhere near as much fun as "guess-the-program" type checking. Spare moments are hard to come by, but if I can find one to propose modified text, I'll offer it.
Is the blog also on Planet Haskell? It probably should be...
are you sure about you pedantry? what about abelian groups (or closed categories)? you know you have been important when you are _not_ capitalized anymore. i am no native speaker though.
You may want to have a look at the source code of the [`Reactive.Banana.Model`][1] module. [1]: http://hackage.haskell.org/package/reactive-banana-0.8.0.2/docs/src/Reactive-Banana-Model.html
That message is generated by the tryhaskell binary, so I think network communication is fine. It seems that on some unclear occasion, [mueval returns something unexpected](https://github.com/chrisdone/tryhaskell/blob/633d95bff7af3bf2dc5aae9fe940776e226f4b95/src/TryHaskell.hs#L298..L300). I've added a log for when this condition happens to see what mueval is actually returning.
You're somehow triggering [this condition](https://github.com/chrisdone/tryhaskell/blob/633d95bff7af3bf2dc5aae9fe940776e226f4b95/src/TryHaskell.hs#L298..L300), but I don't know why from your screenshot. I've added a log for when this condition happens to see what mueval is actually returning.
&gt; I think there's a bit too much emphasis on the way types give you errors and not enough on the way types give you code I agree, more examples on what types *give* over what they take away would be most welcome. I tried to do that a little bit with the expansion code samples under the Inference section for Aeson. Is the click-to-expand feature working? It should be, but sclv said it wasn't for him. &gt; It's also dodgy to suggest that types are optional when sometimes they're necessary. True, that could be worded better. I was in a dispell-the-Java-type-sig-everywhere-myth mindset. &gt; I was glad to see "bidirectional" in there, as good practice making types informative (for humans and for instance inference, etc) is to be explicit about top-level types, then propagate them. Glad you think so! Coming from e.g. C# which has 'implicit' types via the `var` keyword, you get a picture of inference being sort of one-way and limited in scope (literally and metaphorically). &gt; Spare moments are hard to come by, but if I can find one to propose modified text, I'll offer it. That would be much appreciated. :-) 
another ticket? i'd have thought a simple template change would be all necessary. e.g. in `&lt;html&gt;&lt;head&gt;`: &lt;link rel="alternate" type="application/rss+xml" title="RSS" href="..." /&gt;
I know... but I had to avoid UTF8 encoding for some reason I don't remember anymore (maybe some locale issue I had with older GHCs), so I stuck w/ `\955` which is plain ASCII.
nice derivation, esp. foldl+unfoldr fusion. A hylomorphism, according to Wikipedia.
/u/chak on reddit.
Not true at all. There are several examples of mathematical concepts that are named after someone, but are not capitalized in common usage: abelian groups (Niels Abel), cartesian coordinates (René Descartes), gaussian distributions (Carl Friedrich Gauss). In physics you have watts, joules, teslas and volts (all units), fermions and bosons (particles) and the elements rutherfordium, curium and einsteinium, among others.
I can't speak to the original reason (I speculate "homework"), but one actual use case could be if you're using Haskell to prototype solutions for another language that doesn't have as good support for lists. Like now that I've got a listless tail-recursive implementation of `reversal` it's easy to port to C: bool isPalindrome(int n) { return n == reversal(n); } int reversal(int n) { int c = 0; while (n != 0){ c = 10*c + n%10; n /= 10; } return c; } 
...the last eight words in my previous comment? :)
You also don't need all the `Array` trickery, `Vector` already has the required machinery : [see this](https://github.com/bartavelle/haskellGraphLib/blob/b7a15986c437b05d31747a4d801ffa173b4fd75b/test.hs)
Also, it's still needlessly recursive, you could just pass the equivalent of a typeclass dictionary: data QueueDict q a = QueueDict { qEmpty :: q , qInsert :: q -&gt; a -&gt; q , qExtract :: q -&gt; (a,q) } stack :: QueueDict [a] a stack = QueueDict [] (flip (:)) list_extract where list_extract (x:xs) = (x,xs) graphAlgorithm :: QueueDict q Node -&gt; Graph -&gt; Node -&gt; [Node] graphAlgorithm (QueueDict empty insert extract) graph node = -- rest of code the same GHC can optimize this nonrecursive implementation much more simply, I'd bet. And at this point, it's also trivial to add an existential type for `q` to improve encapsulation.
Could we get a link to [Hoogle](http://www.haskell.org/hoogle/) on the new page? Or at least http://hackage.haskell.org?
you can further fuse `==` and `reversal` to get just one loop, `{n/1000 == n%10 &amp;&amp; ...}` and bail out even earlier on non-palindromes. 
The last eight words of your comment are an imperative specification. Can you provide a declarative one?
You describe `Const Kleene` as satisfying "all the other laws". What are the other laws and where are they documented? (I mean, I can guess what they should be, but I don't think we have a full set of Alternative laws, even sans `some` and `many`, documented anywhere, and if I recall we have the same "two candidate sets" problem as with `MonadPlus`?)
I tried to do something like that in [my original answer to the stackoverflow question](http://stackoverflow.com/revisions/26316718/1), but I screwed it up (It reported 80080 as a palindrome). I'd be interested to see your fusion!
Oh, in the C! (==) is a single op in the C, but I'd still be interested in seeing a fusion.
I really hope the answer is no! but I'm brining my popcorn anyway 
I had a similar conversation recently: https://github.com/feuerbach/regex-applicative/issues/19 Also, I don't care too much about satisfying all the laws to the letter. I treat them as guidelines, and if the functions are doing something sensible and useful, that's fine with me.
I never used Traversable. Lists are often good enough, so that i don't need the Traversable type class at all.
&gt; What are the other laws and where are they documented? I meant the `Applicative` laws, plus associativity and unitality of `&lt;|&gt;`.
Hopefully someone more knowledgeable will explain in more detail, but those bodies aren't actually used. Those are primative operations so they are not implemented in Haskell. (I guess they're are in C.)
The first few lines of the file you linked explain it pretty well: &gt; This is a generated file (generated by genprimopcode). It is not code to actually be used. Its only purpose is to be consumed by haddock. These functions are built into GHC. `let x = x in x` is just so that everything typechecks.
This code is not available as a C library. These functions correspond to some LLVM code, I imagine (I'm not familiar with GHC internals, just making some educated(?) guesses).
Using `undefined` would create an import cycle as `undefined` is defined in some other module which ultimately imports GHC.Prim. It's simpler to just use `let x = x in x` as it doesn't depend on anything at all.
There was a question on [stackoverflow](http://stackoverflow.com/questions/15893524/what-is-the-meaning-of-let-x-x-in-x-and-data-float-in-ghc-prim-in-haskell) a while ago about this. The long and short is that it's just there so we can have a nice pure haskell definition of compiler primops for the type checker. Since it's never run, `let x = x in x` will always type check and works nicely. *Disclaimer, I wrote the accepted answer*
Yep, absolutely. You can take more of a black-listing approach and just filter out things like `.git/` if you like (in fact that's what I've seen recommended elsewhere) - we were just more keen to prevent any unnecessary rebuilds.
&gt; We probably should add laws for the associativity and unitality of &lt;|&gt; to the documentation then? True, the documentation is not exactly clear there. It says "a monoid on applicative functors", which I interpreted as requiring the monoid laws for `&lt;|&gt;`. &gt; Do you have any proposal for what laws we could have that would still work for e.g. your Const Kleene example? I'd rather not have laws for `some` and `many` at all, but maybe I'd be ok with a (small) sensible subset of the [Kleene star laws](https://en.wikipedia.org/wiki/Kleene_algebra#Properties) for Kleene algebras. Note that since "multiplication" is not distributive in an alternative functor, the order relation is not so well-behaved, hence many of those laws are unreasonable. In another language, I might suggest having a proper lattice of subclasses for all possible combinations of laws. In Haskell, I don't think it's worth it. I think it's good to just have one class for the minimal workable set of laws (i.e. two independent sets of monoid laws).
&gt; Also, I don't care too much about satisfying all the laws to the letter. I treat them as guidelines, and if the functions are doing something sensible and useful, that's fine with me. The problem with this point of view (as evidenced by the issue with `Compose`) is that other code might make certain assumptions based on the laws, which then end up being invalid and break things. In fact, I had the same issue with `Compose`, and that's actually what prompted me to write this post in the first place. The `Alternative` instance of `Compose` is technically correct, since the "laws" of `some` and `many` completely determine the implementation. Unless, of course, those laws are not satisfied by the underlying `Alternative` instance (as they were not in my case, and I'm guessing in yours). In that case, `many` diverges when it should be perfectly defined. I've had to define my own `Compose` newtype with an implementation of `some` using `sequenceA` like you suggested, and I think it's quite an infelicity.
The short answer is "it depends". Primitive operations (AKA 'primops') like this are typically handled internally inside the compiler in a few ways. They don't necessarily correspond 1:1 to anything else. It's possible a primop *may* become a call to some foreign C code. Or not. The compiler may instead recognize e.g. your CPU has support for some features, and lower them to actual instructions. Or it might not call out to C, or use a specific instruction; it may just emit the needed assembly to implement that operation in a general way. The `byteSwap#` primitives are a good example of this. On x86 architectures, these get lowered directly to `bswap` instructions. On SPARC or PPC, they're lowered to external C calls to library functions.
The compiler handles these primitive operations very specifically inside the compilation pipeline. It *might* call out to C code if necessary, but not always. And for this, it doesn't need to use `foreign` imports to conjure up the necessary code to call into C, it can do as it pleases (depending on the platform or other things, perhaps).
See my other comment. The short answer is they don't necessarily correspond to anything. Some operations may get lowered into CPU instructions if you support it (as I mentioned earlier, `byteSwap` is a good example, which becomes `bswap` on x86) or may call out to external C code, or the compiler could just emit raw assembly to handle the operation in a general way.
Absolutely. I was hoping someone had a writeup. I'll definitely share my experience if/when my project finishes though.
Ah, right, for some reason I thought GHC used LLVM as the default backend. I shouldn't have commented without reading up.
I'm told that the Any problem is due to go away in 7.10, though i coudl be wrong
I suppose the other obvious thing to filter out is `dist/`.
There is also a large class of primops that are implemented directly in Cmm (possibly doing an FFI call to a C function to do the real work) which the compiler implements as normal calls to Haskell functions. Things like `newByteArray#`, `putMVar#`, `fork#` and so on.
Just as my implementation is morally (but not formally) right, I believe the `Alternative Compose` instance is morally (but not formally) wrong.
This is quite right. Also, on Android, there is the matter of turning the Android Java APIs into something pleasant to use from Haskell. While you *can* manually construct the necessary JNI calls, it's far more verbose and also less type-safe than just writing Java. The Cabal custom build type thing is really just a bug or missing feature, depending on your point of view. The Template Haskell issue is more serious. But aside from the TH and related restrictions on cross-compilers, the GHC side of things should pretty much just work.
Is anyone using it for haskell development? I wonder how it compares to emacs + haskell-mode. 
This question applies equally to all other standard typeclasses with associated laws. (In that context, I found myself writing illegal instances for `Monoid`, when I should have rather reached for `Semigroup`, or incomplete instances for `Num` which weren't proper `Num`s just to get integer literal support for my type)
How modular is Yi? Like if I wanted to make a clone of the "acme" editor, would I be able to reuse a bunch of Yi?
Brined popcorn? Sounds delicious!
They use different orderings on different domains. I can claim that 0 is the least fixpoint of a real function *f(x)=x^(2)-x*, but `fix (\x -&gt; x^2 - x)` will disagree.
Well its arm but not android: [these guys](http://tree.is/) are doing games in haskell on ios. One of them gave a talk at Bayhac about it. 
I actually wrote about one of those cross-compilation problems today: http://asivitz.com/posts/a_story_of_cross_compilation
From the blog post: Looks like they have some interesting content here: http://yi-editor.github.io/ 
BTW Hugs had syntax: primitive ptrEq "unsafePtrEq" :: a -&gt; a -&gt; Bool and Agda has: {-# BUILTIN INTEGER Int #-} I'm usually against having such narrow constructs in a language, but in this case they somehow make sense.
It can, but the LLVM backend is fairly new.
As a more theoretical aside, undefined is, heh, defined in the devotional semantics sense (that this, observable results of what expressions should evaluate to with no regard to how they are evaluated) as being equivalent to and endless loop which is in turn equivalent too or *denoted as* _|_ or bottom. Of course, in practice throws an exception. let x = x in x loops forever, but if the runtime or compiler was smart enough to notice this was an endless loop, it wouldn't be wrong for it to throw an exception back to IO, because either way, you ain't getting a useful value out. Edit: Actually, if you type it into ghci 7.8.3, I get: let x = x in x *** Exception: &lt;&lt;loop&gt;&gt; So it *is* smart enough.
Try splashing some par and seqs in there somewhere.
Yay ghc has solved the halting problem. We can all retire now :P 
Modular enough, if it supports Emacs and Vim programmable modes with different handling of major and minor modes in each editor. Please msg me, if you want further advice - I have contributed some patches to Yi, and am heavy user of the editor.
Done: https://github.com/haskell-infra/oblivious-haskell-theme
Done: https://github.com/haskell-infra/oblivious-haskell-theme
I shouldn't reply to comments at 3am... OK, theme is now available: https://github.com/haskell-infra/oblivious-haskell-theme
As someone new to haskell, could you explain? I think the reason the OP is wrong is because a) traversable is a type class that not only list like types can implement, b) that lists are more than traversable, which was created to generalize certain functions beyond the functor/applicative/monad boundaries/traditional paradigm. Is there anything else I'm missing? 
You'd have to write it yourself. import Data.Type.Equality (type (==)) class NotMember t r instance NotMember t () instance ((t == u) ~ False, NotMember t r) =&gt; NotMember t (u :&gt; r) Alternatively: type family NotMember t r where NotMember t () = True NotMember t (t :&gt; r) = False NotMember t (u :&gt; r) = NotMember t r someFunc :: NotMember t r ~ True =&gt; ... If you're not on GHC 7.8, then you'll have to dish out `IncoherentInstances` to get type inequality, i.e. (adapted from http://okmij.org/ftp/Haskell/typeEQ.html) class TypeEq x y b | x y -&gt; b instance TypeEq x x True instance b ~ False =&gt; TypeEq x y b class NotMember t r instance NotMember t () instance (TypeEq t u False, NotMember t r) =&gt; NotMember t (u :&gt; r) 
A solid quarter of `lens` is based on generalizations of `traverse`. This isn't always `Traversable`, but it is an indication that lists don't give you everything you may want.
Thanks for the response. I think I tried essentially your first example, and, while it type checked, it didn't actually work for me. I'll retry it, but I'm interested in trying your second example using Type Families when I get home.
I'm currently working with some code that has data structure which contains a Map, where the values of the map are a data structure that contain a list, and the values of the list are a data structure that contain a zipper. (I'm actually doing this in Scala, but I prototype it all in Haskell and then port it across to Scalaz) The map, the list, and the zipper all have instances of Functor, Foldable and Traversable, and so are the data structures that contain them. Lets call the top level type `T` and the type I have in my zipper `E`. I can now take a function f :: E -&gt; Maybe E and create a T -&gt; Maybe T in one line, where a failure to produce a Just value in any of the `E`s will result in a failure to produce a `T`. Since Traversable is general in the Applicative used, I can actually go from E -&gt; F E to T -&gt; F T , so `F` can be `Either MyErrorType` if I need some more information, or it can be (in the case of Scala) `Future`. All of these are really handy. Foldable is also really useful - if I have a function g :: E -&gt; [A] I can produce a function T -&gt; [A] that gathers all of the `A`s in a single, simple function. Also, Foldable generalizes to any monoid, which is pretty handy. I'm using it quite a bit with lists, but I also have `Score` and `CacheLoader` monoids that are getting a workout from this machinery as well. It took me a while to really get a handle on them, and now I'm finding all kinds of uses for them that feel really elegant. Once you have a handle on Monoid and Applicative, I'd highly recommend putting aside some time to get comfortable with them.
https://github.com/neurocyte/ghc-android - builds a working ghc on android https://github.com/neurocyte/foreign-jni - JNI interface, I've not tried it, but https://github.com/neurocyte/android-haskell-activity is a demo program. TH is a nightmare. I have 400 lines of patches that get network to build, and another thousand or so lines of patches to other libraries to build for android. I have not tried to upstream most of it because it's hacks, removing features that don't build, etc. It would be good to have some kind of community repo with this stuff like there is for IOS; for now it's buried in the source to git-annex (standalone/android/haskell-patches) There's been very little change since 2 years ago when I was able to build and run a yesod app on android. I still need the same tower of hacks today as I did then.
I just installed the newest version. Can you still not define a function such as: helloWorld :: YiM () helloWorld = withCurrentBuffer $ insertN "Hello, world!" M-x reload M-x helloWorld If I recalll correctly, you *have* to bind the function to a key before using it? Please correct me if I'm wrong, I'm ready to start hacking around on Yi extensions :D
&gt; you know you have been important when you are not capitalized anymore. http://en.wikipedia.org/wiki/Boolean_algebra_(structure) Mr. Boole is clearly not important enough, then. While it does appear to be common practice to lowercase the "Abel" in abelian, it is not common usage to lowercase the "Boole" in Boolean.
Does anyone have experience with ilred's comment on the gist, that Node JS and Haskell web frameworks have similar performance? I remember seeing graphs showing Haskell web frameworks come out far ahead, though the TechEmpower benchmarks show Node with a pretty strong lead. 
I use yi to write yi. Except when I break it in the process, then I have to use vim. There're some haskell utils here: https://github.com/Fuuzetsu/yi-haskell-utils. Personally, I just use steeloverseer in a tmux pane to see build errors.
i have only ever seen haskell excel at "pong" benchmarks where the server provides a trivial response to a local benchmarking tool (siege, wrk, ab etc) the techempower benchmarks are a little better, they at least involve tasks one could expect to see in real projects...querying an RDBMS, json serialization, etc...these benchmarks are still not comparable to a real application, but they're decent. i actually think haskell does pretty well...i mean...did you really expect that it would beat Go, Java, Openresty? what seems to be missing is some notion of adequacy and common sense. look at how far down the list tools like rails are...yet lots of high-traffic sites get by with rails. i think you can safely pick any technology that finishes in the top half of the techempower lists and not have to worry about performance in most cases. you may still have other things to worry about (libraries etc)...but not raw performance techempower also has lots of weird outliers to watch out for....i mean, for example, is *anyone* actually going to use cpoll over netty because cpoll has a performance lead over netty which is already incredibly fast? at some point its worth considering the actual req/sec instead of just the rank in the list
If I remember correctly, you can't simply call M-x helloWorld. You need to bind helloWorld to a custom keybinding. Check out https://github.com/Fuuzetsu/yi-config/blob/master/yi.hs and add your function to myKeymap.
Are you saying that the code is written in an imperative fashion or that the problem is a imperative problem naturally and can't be written in a functional way? If you're saying it's written imperatively when it could be written functionally, I would like to see what it would look like in functional form. I'm still trying to grasp the way functional code should be written compared to imperative code. 
The github pages have some easy to understand pages, like http://yi-editor.github.io//pages/vim-config/ which will hopefully make it easier to get started with Yi. It started pretty recently. There should be more pages in there soon.
I don't know how else you would do it. And it doesn't seem narrow to me. Agda's BUILTIN allows you to bring in any definitions in the run time into the language proper as an axiom.
Thanks, sent a pr!
This is depressing reading. If there's any area where an intermediate-level Haskeller such as myself can help rectify the situation, I'd be glad to try and help…
P. Snivels reminds me of Newt Gingrich as in "[Newt Gingrich] is a stupid man’s idea of what a smart person sounds like" (P. Krugman) 
Really? I found it needlessly aggressive and in many places the argument seemed to be all over the place (the "type safety" part and the aside wherein the history of the lambda-calculus was recounted being a good example).
I'm an avid proponent of type systems, but guaranteed abstraction is certainly possible even in dynamically typed languages: function Counter() { var value = 0; return { get: function() { return value; }, increment: function() { value += 1; } }; } There's no way to modify `value` outside of `Counter`, except for the exposed methods.
Cool! Some quick feedback (I'm on my phone): you're not using attoparsec idiomatically in some places. string "[" Is one example (idiom would be `char '['`, it's also faster) but I'm also seeing use of many's where they could be factored out for the significantly faster take* functions with a bit of effort. Attoparsec is not known for helpful error messages. Use Parsec if that's what you're after and don't need the speed of attoparsec. Lenses are great but I'm on my phone. Also look at Free monads to structure your AST, they're fun, I'm going to try using them for my orgmode file parser :)
Do you know why you can't just do "M-x helloWorld" ? Is it just that it needs to be implemented, or is the way Yi is designed making this difficult? 
**Edit**: I misread the code snippet somewhat badly. It actually does provide safe abstraction! I'll leave the below for discussion purposes and because it highlights another mechanism for achieving it. My motivation was driven initially by misunderstanding, but also by noting that the below is, in my experience, a far more common way of providing unsafe "data abstraction" in dynamic languages. --- That's information hiding, though, not abstraction. Abstraction occurs when a value's interface is restricted from that of its most concrete instantiation. This can happen in dynamically-typed languages as well but the discipline is entirely upon the programmer. A simple, if pathological, example is to note that some function uses some set of operations that only require a mathematical group. Then we can just consider that function to operate generically across any group (using some form of overloading probably). Now, within the body of that function our input values are considered abstract types exposing a group interface. Of course, the programmer could violate this by introducing more specific operations at any point. The counterargument is then that the function has suddenly become generic at a more specific type and the internal values are less abstract. Honestly, I think that's all silly though. True data abstraction requires stating your abstraction boundary and sticking to it. At the very least you'd want to make a comment of that and a bunch of unit tests at various implementers of the abstract interface. Or... just use types.
As much as I like and respect Ollie, I'm not willing to spend 25 dollars to watch him code :) No offence, just my personal preference. The website and the screenshots seems very attractive though!
Yes, a paid video isn't going to suit or appeal to everyone, but there's a definite market for them, and I'd personally like to see more people charging for expert content. Drew Neil's done a lot of work organising and producing this series, and it's the fee that allowed it to happen. Ollie has already recorded a follow-up video in which he hosted John Wiegley, by the way: "coming soon!" :-) I'd say that this video is best suited to those who are relatively early in their Haskell journey. It's not tutorial content for absolute beginners, and it's also not aimed at experts. Rather, it shows how an experienced Haskell guy approaches a concrete problem, so I think it provides a helpful insight into the thought and development process.
I see three solutions : - Go through a C interface. This is unsafe and tedious. - Serialize and use pipes. This is unsafe and slow. - Implement your thing in OCaml. This is dangerous, you might even like it (and it will good for you open-mindedness :p)
To complement tel's answer (which is interesting): your trick works because the internal state (which happens to be an integer) doesn't need to be mentioned in the returned object's interface. This is a restricted case of a more general pattern where you want to abstract over some type `state`, yet your functions may need to pass `state` values around. In your case the exposed interface is something like: type counter = { get : unit -&gt; int; increment : unit -&gt; unit; } But you may want to add an `equal` function/method to test equality between counter objects, and for this you need to expose a `state` function/method that returns a counter's internal state (which will not necessarily remain an integer in future versions of the program): type state (* abstract type *) type counter = { get : unit -&gt; int; increment : unit -&gt; unit; state : unit -&gt; state; equals : state -&gt; bool; } Now I can easily revise my program to add a name to counters (with a `name` method to access it). The abstract type `state` will probably become something like a pair of an integer and a string -- it's an implementation detail. type state type counter = { get : unit -&gt; int; increment : unit -&gt; unit; state : unit -&gt; state; equals : state -&gt; bool; name : unit -&gt; string; } In a dynamic language, starting from the point where `state` needs to be exposed in the interface, you can hardly control what people will do with it, except by social discipline or rarely-made-in-practice choices (such as sealing by cryptographic encryption with a module-private key, which is a bit overkill when the point is only to keep the ease maintenability).
`many` thanks for the feedback, much appreciated and some of it already implemented: [Optimization: used char instead of string where possible.](https://github.com/cies/toml/commit/1b4a787ced31f9430e0641cfbb32f77b5fae274c) Implemented that right away. Too bad I dont have a benchmark suite yet to show the difference. Though very glad that I have a test suite.
My guess is that the function is never loaded into the list of functions available when you do "M-x". At first I thought it just wasn't reloaded, but I couldn't execute it after restarting either.
I'll warn you... coding while talking and not really stopping to think is not how I normally code. Basically, standard disclaimer for any coder who has others watching over their shoulder - "I swear my coding is usually better than this!" That said, it was a blast to do, and Drew has done an incredible job editing the video. Would love to hear your thoughts if you do decide to pick up a copy!
Regarding the use of `many`, I don't see how I can convert them to `take*` functions. They seem to do quite a different thing.. I decided that Attoparsec is a better choice then Parsec for performance reasons. Toml is a configuration file format; and they are usually read when a program starts, slowness of the parser will lead to undesired start-up slowness of the program. Is there any example somewhere of using free-monads for ASTs?
Wow, that is really uncalled for. And it's upsetting to see that this currently has five upvotes.
I would love to watch this video, but I find that demanding payment for sharing knowledge goes against everything I stand for. I'm sure this is great quality, and it's probably taken time(= money) to edit and all that, but I still don't like it.
Congratulations. I truly think Haskell is great language to build an hacker's-text-editor in: fast, safe, easy to refactor -- 3 things I'd not say for either Vim nor Emacs core and plugin code.
I didn't get that sense at all. And I felt that the historical lessons added to his points.
Data abstraction is a dynamic notion, and can be dynamically enforced by the provider of the abstract type in a suitable language. One possible way to enforce abstraction is through *sealing* and it is a pretty old idea [Morris73]. Consider a language with a primitive `new_seal ()` that returns a pair of functions `seal_k` and `unseal_k` with a different `k` each time. The function `seal_k` takes a value and puts it in a box with a label `k`. The only possible operation on this box is to unseal it with the `unseal_k` operation for the same key `k`, all other operations crash. The operation `unseal_k` takes a box labelled with `k` and returns the value inside the box. It crashes if it is not given a box, or a box with the wrong label. Then, consider that I want to provide a specific interface (say, monoid) over some data structure (say, natural numbers). I can implement a monoid as follows, sealing and unsealing the concrete representation at the abstraction boundary: let int_as_counter = let (seal, unseal) = new_seal () in { unit = seal 0 add = fun x y -&gt; seal (unseal x + unseal y); is_unit = fun x -&gt; unseal x = 0 } Now, if I am given `int_as_monoid`, I won't be able to access the correct `seal`/`unseal` pair, so the type is opaque to me(*). I can't create invalid values and can only access a value using the provided operations. Parametric polymorphism can be interpreted as a safety result: well-typed polymorphic functions do not crash in the presence of sealing because the typing rules prevent them from breaking the seal. [Morris73] http://www.erights.org/history/morris73.pdf (*) All this breaks if one is allowed to, for example, examine the environment of a function.
&gt; I'll warn you... coding while talking and not really stopping to think is not how I normally code. Basically, standard disclaimer for any coder who has others watching over their shoulder - "I swear my coding is usually better than this!" As someone who is reduced to the intelligence of a shrew under interview pressure, I know the shoulder watching effect all too well! &gt; That said, it was a blast to do, and Drew has done an incredible job editing the video. Would love to hear your thoughts if you do decide to pick up a copy! Awesome. Yeah, I'm gonna watch it this evening. :-) 
Stuff like many (notChar '\'') is much slower than takeWhile (/= '\'')
No, I don't believe it is. Gingrich and Snively have a prescriptive outlook, both refer to history to show a vapid point, drop names and theories, etc. There's nothing wrong in any to all of above. But what makes G. and S. similar is that they have the same agenda: self promotion by trying to appear to be an expert on a topic. It's not that hard to detect that both know not what they talk about and shallow ignorance is mistaken for deep understanding.
I'm missing some nuance here. How does "Abstraction occurs when a value's interface is restricted from that of its most concrete instantiation." not apply to the example you say does not represent abstraction? I do think your last paragraph sums up the issue pretty well.
My "free" example does violate the equations altogether, and I don't think you can handwave the problem away. Concretely, in terms of command line option parsers, if `p` is a parser for a single option or flag, `many p` and `(:) &lt;$&gt; p &lt;*&gt; some p &lt;|&gt; pure []` will have different "usage" texts.
Well, if Paul made some factual errors in his blog post I'm sure he would be happy if you point them out. Making subjective remarks about his agenda and knowledge level is really non-constructive and doesn't enrich the discussion in any way.
No, it very much *is* uncalled for, and this seems to be a recurring problem with your comments here. Stop being an asshole.
What was not cool was your resorting to calling someone that you disagree with stupid.
This is a great demonstration. The key feature of a type that represents some state being visible in an interface is very empowering. It's a shame that we don't have more languages that make the expression of this kind of abstraction as perspicuous as ML. Programming with signatures is a wonderfully direct way of talking about such things.
Mainly, ghc-android needs a community, which does not seem to have come together around it to the same extent it has to ghc-ios. Copying the ghc-ios cabal repo setup for android would be a good start. If there was such a repo, I could feed lots of patched packages into it. Others could use it. Perhaps a community would grow. I don't have time to try to set it up myself. I forgot to mention that the ghcjs developers have some approaches to working with TH that might eventually work for Android as well. Including, I think, work on making TH code run in a separate process from GHC, which allows it to run in node.js (for ghcjs), or perhaps on an Android device or something.
I am the original author of the library, which you can find on [GitHub here](https://github.com/seliopou/toml). I wrote it about a year and a half ago and it hasn't gotten much attention until recently. At the [prompting of @clementd](https://twitter.com/clementd/status/506882545007337472) I put the library on hackage last month, which you can find [here](http://hackage.haskell.org/package/toml). Most of the feedback that the code is getting in this thread is code that I wrote, or related to decisions I made, a year and a half ago. The feedback is appreciated and pull requests are welcomed. Having said that, the OP [issued a pull request](https://github.com/seliopou/toml/issues/3) against the repo last week which I didn't accept. You can read the discussion if you'd like, but the long and short of it is this: The pull request started off with a few added tests and some random changes by the time I went back to look at the request, it had grown to include many other commits. It seemed to me like the OP was in full-rewrite mode, with some changes being worthwhile and others having dubious value. I asked the OP to help me out with evaluating and integrating changes, which he refused on the grounds that he wanted to devote time to the rewrite rather than getting changes accepted. I figured the OP's intention was to fork the project. The repo is made available under the BSD3 license, so that is his right. It's unfortunate however, that there are now going to be multiple (not even independent) versions of the library. Something that a bit of discussion and openness would have prevented. Anyways, suspicion confirmed.
You forked a codebase that was already using Attoparsec, make it not really your decision. Parsec is the better choice.
I got pretty lucky with my workflow video: https://www.youtube.com/watch?v=Li6oaO8x2VY It was my first and I was just recording my usual process. No artifice, no contrived problems. Just me doing real work, but with vocalization of my thought process. Code isn't as interesting as the example you used for your video though.
Let me know how you find it, I might change my mind :)
I think the 3rd option is best. Unfortunately, getting languages to play nicely is notoriously difficult. OCaml is a joy to work with though. It doesn't feel as high level as haskell (lacking good overloading via typeclasses, and automatic laziness) but it makes up for it with better "getting-things-done" tools like good modules and predictable performance. You could port the OCaml library to haskell. Depending on how it's written, this should be pretty straightforward and will likely be easier than learning ocaml from scratch (But why miss the opportunity?). Out of curiosity, what is the library?
Thanks! Managed to [get rid of quite a few of them](https://github.com/cies/toml/commit/f22bc759b54144dbfbda9586d30ad56adb2eaf99)...
Hi Seliopou! I indeed forked a codebase that was written by you, and used `Attoparsec.Char8`, that was indeed your choice. Before I moved it to `Attoparsec.Text` (inline with the Toml spec: UTF-8 everywhere), I considered going to Parsec for better error messages, but **chose** not to because I wanted it to be really well performing. Anyway, thanks for weighing in on Parsec. I'll have a look if I can switch it over.
The price is indeed a bit steep, but I think it's okay to charge for teaching, text books, tutorial videos or whatnot. Commercial offerings have to compete with the free alternatives, so there's hope that the competition will raise the bar for quality. If we can support a world where these things are free, all the better. 
It seems like you passed up a chance to get an upgrade to the 2.0 spec and a test suite for a library that you are sending out signals that you don't have much time to improve. These kinds of big contributions (no test suite to test suite is pretty huge alone for a parser) are quite rare for someone that isn't an author. Personally I would ask the sender to try to improve their practices in the future, but I would also look at the value of getting such a great contribution like that and weigh it against the effort of having to review an all-in-one pull request rather than just rejecting it on a procedural basis.
Hi Seliopou! As I mentioned I got started on fixing some thing in your lib that ended up in a total rewrite. There is pretty much nothing in the lib left from the original. I dont intend to fork at all; I think one --well maintained-- lib for Toml is enough and therefore I totally hope that our efforts may converge. You are free to read my code and incorporate any of my changes back into your tree, or even collaborate with me, all at your choice. In the [conversation](https://github.com/seliopou/toml/pull/3) I had with you, you seemingly to suspected me of "bad intentions". I hope to take that suspicion away. Have a nice day, and thank for releasing the starting point for my work under a permissive license. And try to be a little less suspicious, I'm just an enthusiastic Haskell programmer with a passion for open source! 
Rather than coming up with a fancy lens based interface to a Toml structure, consider just a Toml to JSON converter. That is what the yaml package does now (it even provides a yaml2json executable). The user writes configuration in YAML, but the programmer gets to see it as JSON. This way everyone can just re-use their JSON knowledge and if they need a lens, lens-aeson already exists.
Well, to be clear, the chance is not gone at all. After I have incorporated all feedback from this reddit post I will ask /u/seliopou again if he wants to converge efforts with me (but that would include to make me a maintainer). As I have told him in the [conversation](https://github.com/seliopou/toml/pull/3) we had, only if he rejects my work I will publish a second toml lib on hackage.
Are you also opposed to people charging for books they spend years writing? People should be compensated for preparing and curating information for public consumption, it's only fair.
Good point. And Toml is even made to map easily to JSON. Thanks!
Interesting. Can you give an example of something you check for in a custom setup that would be different on the target? I'd think that a library that requires such detection is likely to be non-trivial to cross-compile in any case. When I got the threepenny-gui example programs running on Android (about a year ago) I encountered at least one package that used a custom build type that I was able to build by building it for the host system and then for the target, since Cabal cached the Setup executable. Rather confusing until I figured out what was going on. In any case building the Setup executable with the cross-compiler will never work when building for Android, so better to exclude only those packages which genuinely need to determine something about the target system than every package which happens to use a custom build type.
&gt; You could port the OCaml library to haskell. Depending on how it's written, this should be pretty straightforward and will likely be easier than learning ocaml from scratch (But why miss the opportunity?). Don't underestimate the difference of idioms between OCaml and Haskell. I did it once, and the result was quite sub-optimal. :) The languages may seems very similar but functors, unbounded mutability and polymorphic variants really change how you express things compared to type classes and monads. 
Why is template haskell always so much of a problem? I would have thought that it would just be a pass that generates a some haskell code in memory then compiles it - nothing implementation specific?
This is a great and informative article, and as someone working with C bindings right now, that part of the article was particularly useful. However, it also shows how easy it can be to shoot yourself in the foot, performance wise. The fact that a few well placed, minor changes led to such as drastic speed improvement illustrates something that nags at the back of my mind when I'm writing Haskell code; am I structuring this correctly? Do I need a bang pattern here? Am I going to build up thunks? Sometimes it can feel like Indiana Jones trying to cross the booby trapped room without stepping in the wrong spot. The profiler is indeed helpful in this regard, but while it's a good tool to tell you *what* is happening, the real hard part is figuring out the *why*. For instance: &gt; The problem was that some object fields weren’t always being used, so old objects were being kept around for that reason. The solution was simple: adding bangs! Yeah, the solution is simple, but how long did it take to figure out *why* the objects weren't being garbage collected? Not using some fields of an object strikes me (as a Haskell novice-intermediate user) as an entirely non-obvious reason to keep objects from being collected. It would probably take me a nontrivial amount of time to figure that out, if I ever did, and didn't instead throw my hands up in the air and start shot-gunning bang patterns all over the affected code trying to get the memory footprint to change in a diagnosable way.
The OCaml Ctypes library has a 'reverse mode' that lets you expose a C API from an OCaml library. For an example of an OCaml XML interface exposed via C: https://github.com/yallop/ocaml-ctypes-inverted-stubs-example This could then be bound via the Haskell FFI as normal. It's hard to get more specific without knowing more about the particular usecase you have in mind. 
Yes, such libraries would need to be customized for the target. For two examples where I have been the bad bad maintainer: * entropy: Based on the host architecture (from Cabal's arch(..)) and using a Setup.hs to detect tool chain support for the RDRAND instruction, include runtime detection for CPU support of RDRAND and optionally use ASM routines to leverage said instruction. * cipher-aes128 similarly performs compile-time detection of tool chain support for AES-NI then uses run-time detection for CPU support prior to leveraging NI. Some of this might actually work out if Cabal does something smart such as `arch(x86) ~ false` when cross compiling to an ARM then the asm instruction issue goes away. However, I'm not sure where to get Entropy on most these platforms so that package is probably going to need to be customized for every target.
&gt; How long did it take to figure out why the objects weren't being garbage collected? 5 minutes to recompile all deps wtih profiling, 1 to run the game with profiling enabled, and 10min to come up with the solution, modify the game and profile it again. Objects are only created in one place (and should only be garbage collected after one -- other -- step). One of the good things of writing very declarative code (what the language enables) is that you can then find responsibilities easily. Premature optimization is the root of all evil. Part of the message of the article is that Haskell makes it ok to wait to introduce those optimizations. The code will be really good, many optimizations can be added easily much later with just a few changes, and the performance boost will be huge anyway. EDIT: We never, ever, ever worry about code optimization prematurely. If something runs too slow, so slow that testing becomes problematic, we take a subproblem apart, design an efficient data type for it, and put it back. Those cases are very rare. To illustrate this, one possible optimization would be to use a type other than lists (in some places), since they have O(i) complexity for access. This has *not* been done yet. Soon will be, but it would probably get in the way if it had been done from the early stages of development (simply because the code might be a bit harder to understand, nothing more). EDIT: The code will be really good because of haskell's abstraction facilities will help you a lot. The article "Why Functional Programming matters" is (still) very illustrative.
No. The concrete instantiation here is returning a get and an increment function, or an object containing those two functions, or whatever that syntax in javascript actually returns. 
&gt; It would probably take me a nontrivial amount of time to figure that out, if I ever did, and didn't instead throw my hands up in the air and start shot-gunning bang patterns all over the affected code trying to get the memory footprint to change in a diagnosable way. Really, that's what they did. They never actually figured out what was causing the space leak, just that making the object fields strict fixed the problem.
If the author is reading, could you please explain what you mean by finding referentially transparent functions and "tell[ing] the Haskell compiler that it was OK to cache those results"? This sounds suspicious to me: "because text barely changes from one frame to another, all text-rendering calls could be cached".
That's not what they said they did. They said "The problem was that some object fields weren’t always being used, so old objects were being kept around for that reason.". For example, you might have data Object = Object { foo :: Foo, bar :: Bar } and repeatedly create new objects objects like ... let newObject = Object { foo = f (foo oldObject), bar = g (bar oldObject) } doSomethingWith (foo newObject) ... -- never refer to `oldObject` again The old objects can only be garbage collected when the `bar` field of your latest object has been evaluated. It should be pretty easy to see that this can cause space leaks, whereas data Object = Object { foo :: !Foo, bar :: !Bar } would avoid them.
Hmm, you may have a point. It may just be because we're so used to people producing videos similar to these(maybe not the same level of quality) and being able to consume them without charge. Maybe I'm just a spoiled brat in that sense. ;-) If I had been in Ollie's shoes(and I had the skills required), I would naturally have accepted. That is, if I had the skills and the opportunity to provide such a service to the public, I would gladly accept. But upon being asked by this company, I would maybe have said something like "I would gladly do the video, but I think it should be free." This is *not* a comment on Ollie's choice. He's free to do as he wants and he's done a lot for the community(I'm a big fan of 24 days of hackage). And as he wrote below, he intends to do a writeup. 
Ugh, now I know I should be more careful with my reddit commenting. This actually is already an example of the "sealing" concept. I just misread the whole damn thing. My bad—initial comment noted as well.
Yes, of course, that's great. Please don't take my comment as a comment on your choice to participate in this -- it's great at any rate, and even though I don't like paying for it, I have to admit that it's obvious that people won't being spending money on nothing -- the quality is great and the annotations and all that are really helpful.
Jeremy Gibbons has released the slides accompanying the talk: http://www.cs.ox.ac.uk/publications/publication9022-abstract.html 
&gt;Don’t believe for a moment that C++ is not just as full of similarly sneaky performance traps I'm afraid I don't. I'd love to see some examples but it seems to be that laziness is far more difficult to reason about that strictness. I've seen far more articles trying to figure out bad performance in Haskell than I have in C++
Thanks. I'll have a look when I get time.
I suppose I just have a different criteria for what constitutes knowing the cause. In my mind, just knowing that some object fields are holding on to old objects through unevaluated thunks isn't enough. Which field is actually sitting unevaluated frame to frame? And why doesn't memory use ever go down? If there are fields that are only occasionally used, shouldn't we see memory use periodically drop as those thunk build-ups get evaluated? The graph they posted only ever increases. Not to imply that they should bother trying to figure that out if their problem is solved. I'm just saying they saw what looked like thunk build-up and added bang patterns to everything to fix it.
Thinking limits my ability to speak meaningfully about what I am doing, so I totally understand. 
I really would like some examples. It's a bit annoying that criticism against Haskell is so often written off as the person not being familiar with the language or being more familiar with another one. I think laziness is more difficult to understand because it is quite difficult to tell whether or not something is building a massive thunk or not. Even as you get used to it you will still need to stop and think for a bit which is a mental overhead strict languages do not have. Not only that but I usually find that code with a space leak in will work for the values I test it with but not with the values I actually use which makes the problem even worse. Finally how am I meant to get myself to think in the right way? There's no tutorial, there's no guide, unlike C and C++ where I find the guides/tutorials/books are very good at explaining how to write fast code.
Basically any language higher level than C has features which cause things that you haven’t explicitly written in the code happen automatically (and honestly, C also hides a lot of details on modern machines). For instance, if you look at a C++ snippet, it’s not clear at all what kind of allocations it will perform when run. Figuring out how you’re killing your cache performance can also be a fun exercise. There’s quite a bit of literature existing solely dedicated to the gotchas of optimising C++ programs. Laziness is just one feature out of many that detaches you from the bare metal. As with any other feature, you have to learn how to use it efficiently. Whether it’s easier or harder than others mostly depends on where you’re coming from.
&gt; Basically any language higher level than C has features which cause things that you haven’t explicitly written in the code happen automatically So Haskell included. I mainly take issue with the "just as full as". It seems to me Haskell has the usual high-ish programming language problems with laziness on top.
OK, so in general I really agree with this sentiment, but I think it's muddled a bit because of our expectations about how things like compilers behave. So I'm going to talk a little. :) You're right C++ can have plenty of performance issues, but often these don't manifest quite so abruptly sometimes. But they're crappy in other ways... I can do things like elide copies, remove unnecessary allocation, use tight packing and optimize cache behavior in something like C++. But all things told, in my experience, these kinds general (C++) optimizations don't win you *enormous boosts*. You may get a solid 20% or even 50% improvement with enough work. If things are particularly sloppy, you can go further with some low-hanging fruit. That's good - but it's only doing, say, 50% more work in the same amount of cycles. In contrast, with GHC, a few simple tweaks to your program can, sometimes, lead to *order of magnitude improvements* very easily once you see what's happening. That's sometimes **10x or 15x** the amount of work done in the same amount of cycles. Think about this carefully: A program runs in X amount of time, and does W amount of work. C++ may be able to get **W+(0.5*W)** work done in the same amount of X time. GHC may be able to get **(10*W)** work done in the same amount of time. Similarly, a C++ compiler's optimizations aren't so critical. `gcc` at `-O2` vs `-O3` (or `-O1` vs `-O2`) for example will normally win you 20% or 30% speed boosts. GHC on the other hand at `-O0` vs `-O2` can turn a practically-non-terminating program into one that runs in milliseconds or even less. It may not even be `-O2` boosting your program. Maybe you forgot an `INLINE`, or perhaps you should have *actually* not had an INLINE, to reduce code blowup. It's fairly easy to see why this may upset or mess with people's intuitions about how things should behave. There are some reasonable reasons why this is the case: - GHC has a very different execution model, with a lot more moving parts than a regular C++ runtime. This is just the world we live in. - GHC is a very robust, long-lived project, but modern compilers for C/C++ have millions (if not billions) of dollars, research and engineer hours stuffed into them in all kinds of ways. They are extremely robust and fairly predictable in general. - All abstractions have a cost, and you always have to pay for that cost in some way. AFAICS, this normally is done by either: - via compiler optimizations (to remove cost so programmers can use the abstraction very easily), or - via programmer time (by putting more burden on the programmer, and less on the compiler, to introduce abstractions that do not penalize you later on). C++ tends to go for #2, and we want to go for #1. Going for #1 makes the $$$ and robustness involved all the more important since you have to pay for *everything* in your compiler! If you go for #2, you spread the cost out more to programmers. Going back to the original thing: yes, I think it is relatively easy to optimize Haskell once you understand it. And not everything is bad in GHC land, of course. In fact, programmers almost *always* at one point will desire control over the execution model, and we're pretty good about that: - As a high level language, Haskell gives you surprising amounts of control and detail over the execution mode. - Most other languages do not allow things like `UNPACK`, i.e. unpacking composite types directly into registers and off the heap. - Haskell has "high ceilings, low floors". You can manually unpack heap objects into unboxed objects like I mentioned before, which lets you write surprisingly efficient and tight code in Haskell where necessary. - We have very, very efficient interop with C in the general case. JNI-like interfaces are typically much more expensive. - We do have a pretty good set of profiling tools I think, although we have very few 'online' capabilities, which is important. - Relatedly, GHC has a very efficient representation for heap objects for a compiled language, far more efficient than say, the JVM (1 words per object vs ~5 or so, IIRC). - GHC is, for the most part, a pretty good compiler (in my professional opinion! :) - Like C++, I'd say like, 80% of all Haskell optimization tricks are roughly of the same calibur and are pretty easy and very predictable, just different. But at the end of the day, it's not hard to see why these things may initially be very upsetting to people. They are very much an affront to people's general expectations. We have a lot less work put in, a lot fewer people, and a much more difficult job in general. Best practices are still being evolved, and we haven't hit such a critical mass to have large amounts of useful information about all this for newcomers. I think we manage quite well, however. And hopefully we'll be making things better. I could go on, on this note, but I'll drop it since this is long anyway. And finally, despite ALL of the above: having been a professional C++ programmer (on a low level application no less) in a past life, vs Haskell programmer now, I would **really** rather be stuck dealing with Haskell performance issues day to day than C++ ones. Yes, I'm a bit of an odd case - but C++ optimizations are typically pretty mundane, yet they are often costly, bug prone, and difficult to accomplish in the large. On the other hand, Haskell is (at this point, empirically I'd say) quite amenable to large scale refactorings and enhancements over time - even if I can't get the same absolute speed, I can have high confidence my program won't break horrifically, only on a Friday, when I wear a green shirt and the moon is like half crescent-but-not-quite, only after Halloween. It's worth it to not deal with that.
I completely agree
&gt; I really want a strict languange with explicit thunk datatype. Explicit thunk makes me think `fun () -&gt; ...` But I'm guessing you must mean something other than this or even a lazy keyword like OCaml/SML? Aren't the implications on the compiler significant, for optimizing default-lazy versus default-strict? Can this be done "fairly"? Overall, I imagine the impact on the compiler itself might play into this assessment of the tradeoffs.
An explicit thunk is basically a value of type `() -&gt; ...` where the first time it is evaluated it is overwritten with its result. Thus the computation in the thunk is never run twice. The impact on the compiler itself is huge. For example OCaml is a strict language with explicit `lazy_t` datatype, but it doesn't satisfy my (unstated) requirements that the performance of laziness should be on par with GHC. Nonetheless, GHC's runtime is very close to being able to support this kind of thing itself. What I'm after is very close to a surface level syntax change.
&gt; I can do things like elide copies, remove unnecessary allocation, use tight packing and optimize cache behavior in something like C++. But all things told, in my experience, these kinds general (C++) optimizations don't win you enormous boosts. You may get a solid 20% or even 50% improvement with enough work. If things are particularly sloppy, you can go further with some low-hanging fruit. That's good - but it's only doing, say, 50% more work in the same amount of cycles. Optimized cache behaviour can actually give you huge improvements once your code is memory-bound, I measured a 13x speedup by going through a matrix row-wise versus column-wise. &gt; In contrast, with GHC, a few simple tweaks to your program can, sometimes, lead to order of magnitude improvements very easily once you see what's happening. That's sometimes 10x or 15x the amount of work done in the same amount of cycles. &gt; &gt; Think about this carefully: A program runs in X amount of time, and does W amount of work. C++ may be able to get W+(0.5*W) work done in the same amount of X time. GHC may be able to get (10*W) work done in the same amount of time. That does not follow at all, this could just as well mean that unoptimized Haskell code is really slow with lots of easy optimization possible for the compiler and that unoptimized C++ is already pretty close to the most optimal code as far as the compiler is concerned. 
Let's be clear, under what license are you making your fork available? Might be time for a social experiment.
http://keera.co.uk/blog/2014/08/13/most-inspiring-green-screen-you-will-ever-see/ They're using ghc-android to do a game, with SDL for display. Since that blog post it's apparently gone from a green screen to a fully working game. "GHC’s backend is not officially supported, but it works very well, and the instructions on the README file allow anyone to get a working installation very, very easily. We tried them in several computers, and they’ve always worked." -- Cool, I think I wrote the instructions they're referring to!
In Haskell all expressions are typed. Computations that can have side effects have type IO Int, IO Char, IO String, etc. what is offen described as IO a, for some a. IO is (conceptually) short for a function: type IO a ~= World -&gt; (a, World) (I use ~= because this is not meant to be actual haskell code.) If you pass it the state of the world, it will give you a result of some type a (whatever a is in your case), and a new world. The paper Imperative Functional Programming, by Peyton Jones and Philip Wadler, explains this in more detail. In Haskell you can't cache a computations with IO result. You may pass it the same arguments, but the last argument, the World, will never be the same. It changes with every call. However, you can tell the compiler that the computation is referentially transparent and side-effect free, that is, that it will always give you the same result for the same arguments, regardless of when you call it. The link to unsafePerformIO explains the specific requirements it must fulfil. Calls such as: text &lt;- renderText someString somePosition someFont ... (with type renderText :: String -&gt; Position -&gt; Font -&gt; ...whatever... -&gt; IO Texture) are used to create the strings showing the level, the number of points, etc. Those don't usually change every single frame. If the arguments change (the string, the font, etc.) then the result should be different, but if neither changes, the result will be exactly the same from one frame to the next. By adding unsafePerformIO, you transform the IO a into a. The result will be cached, and you lose control over when the computation will be executed, or how many times (if it is inlined). Does that answer the question?
Library is called pgsolver for solving parity games.
But doesn't `renderText` has the side effect of outputting something to the screen? If that was the case, using unsafePerformIO would be wrong, wouldn't it?
&gt; Optimized cache behaviour can actually give you huge improvements once your code is memory-bound, I measured a 13x speedup by going through a matrix row-wise versus column-wise. I'll grant this is a totally fair counterpoint, considering we're talking about games in the OP (although it wasn't the source of slowness). But the case you note here seems like an *extreme* case of where it will help, giving over an order of magnitude improvement for a very general optimization technique in C++, in a specific bounded case. Most code is simply not so memory bound in my experience, and will not win so much from such simple techniques. As a counterpoint, I was working on fairly complex data backup software in C++, so it had to heavily interact with the kernel and a large variety of storage devices to achieve adequate performance (individual disks ranging up to terabytes). You're not going to get a 13x performance increase alone from cache behavior here pretty much ever, or through any general run-of-the-mill optimizations. We reduced memory usage a lot by being smart, but that wasn't the biggest factor here. We did actually get a 5000% raw performance increase - but that was from completely redesigning the backup pipeline from the ground up. Before and after this, we got nowhere close to 13x improvements (or even 5x) from *any* other general techniques. **EDIT**: Hilariously, for the curious readers - this redesigned pipeline was basically a version of `pipes` or `conduit` written in C++, on top of `epoll`/IOCP, back when `Enumerators` were still the biggest thing. :) It was quite silly to me - a coworker and I had bet we could have rewritten it all in Haskell at very little cost. &gt; That does not follow at all, this could just as well mean that unoptimized Haskell code is really slow with lots of easy optimization possible for the compiler and that unoptimized C++ is already pretty close to the most optimal code as far as the compiler is concerned. Well, this is all in my experience having written C(++) and Haskell for a long time. I have encountered plenty more cases where adding or removing an `INLINE`, adding a bang pattern, or hoisting/lowering a recursive definition has resulted in order of magnitude improvements. Unlike your matrix multiply example, where you get a 13x improvement for a general optimization in a *very* obvious case (it's easy to intuit why the behavior is better here), which is likely not applicable to a wide array of things - `INLINE` and bang patterns *are* very general, universal techniques for Haskell, work for tons of code, and can *still* often result in much bigger performance deltas than anything you may expect from say, C++. I don't mean to make this sound too bad. It's not like I spend 80% of my Haskell time fighting these insidious 10x performance losses. For the most part, it's really not a giant problem I think, and Haskell pays for itself many times over. I just think it's a matter of expectations, not just the raw #s of how often it occurs. I also do still think a lot of it has to do with compiler technology. C/C++ compilers have been ruthlessly honed for decades. We don't quite have the level of predictability and robustness they have, which is what sets these expectations of speed, and what the compiler is capable of.
As MelancholyMechanic says, if `renderText` is not a pure function then using `unsafePerformIO` is going to lead to incorrect behaviour, isn't it? Why not do the caching manually?
The output type of the function is `IO Texture`. This leads me to believe that the function doesn't render the text to the screen, but instead of a `Texture` object type. This begs the question though, why isn't the output type `Texture` instead of `IO Texture`? Why does it need to be in the IO monad at all?
Thanks for the advice. It turns out the ghc and ghci code can't be loaded into the ghci, due to the way they've arranged modules : apparently their folder structure doesn't reflect the modules structure. As for refactoring, I'm not really trying to refactor the WHOLE ghc codebase. However, this advice is valid when creating my own project from scratch. Thanks ! 
I'm still hoping its not a fork. And when you analyze the code you'll find it's pretty much a 100% rewrite. My reasons for wanting to converge should be clear to you by now. Though I will follow what's most common in the Haskell community to increase chances of it being adopted. I think that's BSD3, and that was also your choice. So I'll not change that. &gt; Might be time for a social experiment. Could you please clarify what you mean by this? -- No bad intentions brother, non of them..! Peace.
Maybe the point is that it doesn't need to be in `IO`, but in that case it should have been declared pure in the FFI import. This is a bit mysterious: "In particular, because text barely changes from one frame to another, all text-rendering calls could be cached."
You're welcome!
&gt; The problem was that some object fields weren’t always being used, so old objects were being kept around for that reason. Since [their haskanoid code requires `base ==4.6.*`](https://github.com/ivanperez-keera/haskanoid/blob/master/haskanoid.cabal#L84), they must still be using GHC 7.6. Interestingly, a new optimization in GHC 7.8 allows those unused fields to be garbage collected under certain circumstances. This optimization was the cause of a [bug in the sodium library](https://github.com/kentuckyfriedtakahe/sodium/pull/17) which I have fixed recently. In the case of sodium, the field was never used at all, so I don't know how simple the conditions guarding future uses must be in order for GHC to detect that a field won't be used, but I think it's nice to know that this kind of space leak should occur less often from now on.
What about studying the earliest version of GHC / System F and gradually adding commits until you get to v6?
Well that is extremely mysterious.
I feel that a lot of this really can and should be automated. If we have the cost center, and the types from profiles, it shouldn't be hard to automatically match the types with `!`, the cost centers with `INLINE` and just iteratively run a benchmark to help the programmer.
Totally. There's a very large design space for tools here. In a kinda-similar fashion, Gershom wrote an old [strictify](http://hackage.haskell.org/package/strictify-0.1/src/strictify.hs) tool that attempts to find a good combination of strictness pragmas, if you provide a hint. Building a more sophisticated tool would be a lot more work, but I think the idea you describe is sound.
In my experience, all constructor fields should be strict unless you have a semantic reason for them not to be.
Uh oh. Is this not worrying, or is it par for the course for compiler optimisations to cause regressions in this way?
Great article. I'm curious to know about the deployment of your software. I am currently learning Haskell with the hopes of using it for future projects. I have installed a couple of Haskell packages using the Haskell Platform and it has been quite troublesome, which makes me worried about deploying my own software written in Haskell. Could you please share your thoughts on deploying Haskell software? 
I would love for you to elaborate on this, such as example cases of when you should or shouldn't use strict fields as a rule. If you're willing, of course ;)
&gt; In Haskell all expressions are typed. Computations that can have side effects have type IO Int, IO Char, IO String, etc. what is offen described as IO a, for some a. IO is (conceptually) short for a function: &gt; &gt; type IO a ~= World -&gt; (a, World) Since I spend a lot of time on /r/haskell, I know that /u/tomejaguar is an advanced user who doesn't need this kind of explanation. Of course, users who are less addicted to /r/haskell than I am wouldn't know this. And since Haskell is such a deep language, it's sometimes hard to guess the level of explanation which would be appropriate for a given question. Luckily, Reddit has a solution for this kind of problem: flair! What do you all think, would it be worthwhile to standardize a flair, say, "[haskeller]", to distinguish users for whom it's okay not to simplify? Or perhaps even "[category theorist]", to indicate that you really don't need to hold back on "free co-endo-morphic adjoint contravariant pro-functors" and such :)
The article is quite interesting, but is sadly completely abstract. It could have been great to add a simple diff output after each of the said modifications to see what it actually looks like. Could you add that? Just copy pasting smartly the output of `git log -p` would be perfect!
What is mysterious about that bit?
&gt;this could just as well mean that unoptimized Haskell code is really slow with lots of easy optimization possible Yeah, see what /r/programming [thinks about the post](https://www.reddit.com/r/programming/comments/2jbl75/from_60_frames_per_second_to_500_in_haskell/). (They now think trying to program a game in Haskell is a non-starter because of the poor performance.)
Yes and no. It's easy to optimize C++ locally, but it is much harder to optimize it in the large (whole data flow). Take a game with a few hundred thousand lines of code - trying to do large scale optimization is going to take a very long time. Such as switching from entity to data flow architecture. So you really have to get the high level design correct early or you are going to be in a world of hurt.
There's always someone having the same ideas as me :( I planned to start working on that on this weekend since I need it for some stuff :)
There is a [DAWG](http://hackage.haskell.org/package/dawg) package which might be used or a [DFA](http://hackage.haskell.org/package/hDFA) one since lexically(structurally) DFAs and DBAs are the same and only semantics changes. I haven't looked if these two can be adapted though to suit the change in semantics or to do model checking or anything else.
Certainly I agree with this -- C++ is horrible to refactor. On the other hand, experienced gamedevs rarely end up wanting to do such system-wide performance refactors because they've been performance-minded upfront (even while chanting the mantra "premature optimization is the root of all evil"). If there's a desire to change the entire architecture, that'll be left for "the next project". I'm well aware of the advantages (20 years in games, and I use OCaml now). There's a freedom to implementing systems as you wish because changing them later is not hard, and most importantly: not likely to introduce bugs! There is trepidation making any large changes in a C++ codebase for all the side-effects and pointers. Still, this argument of "orders of magnitude of improvements" during optimization, versus C++, is disingenuous, when you're starting orders of magnitude behind where the initial C++ implementation was (and this is *why* you get such a degree of improvement).
I've subscribed for about a year and never posted anything. That's mostly because I haven't felt very opinionated about any of the topics.
"no matter how long I play, my score is always 0" is the bug I would expect from my initial parsing of that statement 
Only when you rely on a behaviour which the documentation explicitly says you shouldn't rely on :) The sodium bug was that a stream mysteriously stopped receiving updates after a short amount of time, which turned out to be the time of the first GC pass. The reason this happened is that sodium automatically unregisters observers once the observed stream of events becomes unreachable, and in this case the stream was incorrectly considered to be unreachable. In turn, the reason why the stream was considered unreachable was that the finalizer wasn't attached to the stream itself, but to an otherwise-unused field inside the stream record. In previous versions of GHC, this field was only garbage-collected when the stream record was, but this isn't guaranteed. In particular, the [documentation for finalizers](http://hackage.haskell.org/package/base-4.7.0.1/docs/System-Mem-Weak.html#t:Weak) says: &gt; WARNING: weak pointers to ordinary non-primitive Haskell types are particularly fragile, because the compiler is free to optimise away or duplicate the underlying data structure. Therefore attempting to place a finalizer on an ordinary Haskell type may well result in the finalizer running earlier than you expected. And indeed, the bug was that in the optimized code, the finalizer ran earlier than expected.
As does (some of) [Hacker News](https://news.ycombinator.com/item?id=8458701).
Thank you! Shame on me, i didnt even think of trying :: [Int]... 
Even if you get points once a second, at 500 frames a second you can save 499 calls to the text rendering function. I would say that "barely" changes. And that's just the score. There could be other text that is static as well.
(not the author) from what I've read and my limited experience, deploying Haskell software can be as simple as building it on your dev machine and copying the binary to the (compatible architecture) target machine. The binary will probably be largish (100 MB), but this arrangement keeps things simple. &gt; I have installed a couple of Haskell packages using the Haskell Platform and it has been quite troublesome What OS are you developing on, and what OS is your target?
I like this idea in general. I think there is potential. 
Curious to see the second part, but it at least occurs to me that just doing something like (mapM_ yield) from Pipes (or the equivalent in conduit) is sufficient, especially when effects enter the picture. Seq just looks like a Producer. Not that I don't feel like the collections interfaces couldn't be better in Haskell, though backpack looks like it will address some of the problems, and from what I've read Lens handles some other cases (monotraversable) in a potentially better manner than typeclasses. In general, having mainly typeclass-based ad-hoc polymorphism can be too restrictive, so it's nice to see people trying other things. EDIT: I think the biggest shortcoming with Pipes would be the lack of the Functor instance you would expect, though that can always be remedied with a newtype. Maybe the ListT implementation is all you need?
(Edit: bss03 was right. Thanks.) I tend to use `find . -name '*hs' -print0 | xargs -0 grep PATTERN` often on large projects. Cf. find-grep in Emacs and also git-grep. I'd be very happy to hear of tags program that is easy to use and robust. Related is cabal's --haddock-hyperlink-source flag. There's some tools for visualizing module dependencies, like graphmod or SourceGraph on Hackage. Again, I'm not sure if they are robust enough for the GHC source. We have such a great community, use it! You've obviously found this subreddit. And I'm sure many developers would be excited to see a question about approaching their code base in their inbox. Skype and such too. Lastly, make sure to look high and low for the docs -- especially any design documents. The GHC Developers Wiki has several goldmines, especially The Commentary. If you just have code, start by identifying the pervasive data types and classes and focusing on their intended semantics. For GHC, find the right ghci flags so that you can load it there. EG Log the output of make and find the flags passed to ghc stage 0 (there will be lots). Best of luck!
Try running ``evalStateT runTimer 0`` from ghci or: main :: IO () main = evalStateT runTimer 0 
You don't really need to sign up for anything; I just follow along on http://haskell.1045720.n5.nabble.com/Haskell-f3074698.subapps.html. Also never posted; just like reading along.
That's a solid first effort with monad transformers. A few tips * if you're using transformers, probably use `StateT` the whole way through rather than a mix of `State` and `StateT` (if you match up the `s` and the `m` in `StateT s m` between methods, then you're working in the same monad - see `newTime` and `runTimer` below) * you can do a lot with `put` / `get` / `modify` from the `State` monad * you rarely want to use `runState` inside your state monad when you're starting out - if you end up heading down that path, ask /r/haskell for some help :) Here's my attempt at a clean up newTime :: Monad m =&gt; ClockTime -&gt; StateT ClockTime m Bool newTime newtime = do oldtime &lt;- get let tick = oldtime &lt; newtime when tick $ put newtime return tick runTimer :: StateT ClockTime IO () runTimer = forever $ do newtime &lt;- lift curTime tick &lt;- newTime newtime when tick $ do lift . putStrLn . show $ newtime return () letsDoIt :: IO () letsDoIt = do t &lt;- curTime evalStateT runTimer t Edit: some further changes while I'm on my lunch break... just because I can't help myself. By changing the bounds to MonadIO it means you'd still be able to use the code even if you end up with a few more layers between your state transformer and IO. I mostly generalize like that to try to limit the number of compilable solutions available to me, rather than aimed for reuse as a primary thing. I could be Doing It Wrong though :) tickTock :: MonadIO m =&gt; StateT ClockTime m Bool tickTock = do newtime &lt;- liftIO curTime oldtime &lt;- get let tick = oldtime &lt; newtime -- Note: could have had -- tick &lt;- (&lt;) &lt;$&gt; get &lt;*&gt; liftIO curTime -- here, but that would be more for my own, private, hack-something-out code rather than -- something that's meant for general readability... when tick $ put newtime return tick runTimer :: MonadIO m =&gt; StateT ClockTime m () runTimer = forever $ do tick &lt;- tickTock when tick $ liftIO . putStrLn . show =&lt;&lt; get return ()
Cleaned the code up a bit well, you probably only want to use one level of StateT. posixToClockTime :: POSIXTime -&gt; ClockTime posixToClockTime x = floor $ (read $ init $ show x) * 1000 curTime :: IO ClockTime curTime = liftM posixToClockTime getPOSIXTime runTimer :: StateT ClockTime IO () runTimer = forever $ do newtime &lt;- lift curTime oldtime &lt;- get if (oldtime &lt; newtime) then do put newtime lift $ putStrLn $ show newtime else return () main :: IO () main = evalStateT runTimer 0 
you are wrong. with the exception of a few contrived examples, clang or gcc will create faster programs from c++ sources than ghc can with haskell. this is a valid generalization for anyone rational if the bar for haskell success is performance exceeding c++ for average effort, you can never succeed
instead of links to papers, give me idiomatic haskell source code that solves a large, difficult problem more efficiently than idiomatic c++ the haskell community should not have held on to this performance theme as a defining story. haskell is still cool even if it is only python-fast. haskell loses credibility when outrageous performance claims can't survive scrutiny tl;dr - put up or shut up
Good to know, and thanks for the detailed explanation!
Maybe 'memoised' is a better word to use in this case than 'cached'. They're caching values of the function with _specific inputs_, so if your score changes, that'd change one of the function's input parameters, and the cached text would not be reused.
I was thinking exactly that as I read about these 10-line changes! Even links to commit hashes at each stage.
`return runTimer` will wrap your `StateT ClockTime IO ()` into yet another IO layer, resulting in a value of type `IO (StateT ClockTime IO ())`. When you type `return runTimer` into ghci, this IO computation is executed, resulting in a value of type `StateT ClockTime IO ()`. Since values of this type are not printable, nothing is printed. In order to execute a computation made out of several layers of monad transformers, you need to peel off each layer one at a time, until you are left with the core. The type `StateT ClockTime IO ()` consists of a single state layer, `StateT ClockTime`, surrounding an IO core. So, how should we peel off this layer? Well, you already know about `runState`, which executes a computation in the ordinary State monad. To run a StateT monad, you need the analogous function [`runStateT`](http://hackage.haskell.org/package/transformers-0.4.1.0/docs/Control-Monad-Trans-State-Lazy.html#v:runStateT): &gt; :t runState (newTime 0) runState :: State s a -&gt; s -&gt; (a, s) &gt; :t runStateT runStateT :: StateT s m a -&gt; s -&gt; m (a, s) or, to use more concrete types: &gt; :t runState runState :: State ClockTime Bool -&gt; ClockTime -&gt; (Bool, ClockTime) &gt; :t runStateT runStateT :: StateT ClockTime IO () -&gt; ClockTime -&gt; IO ((), ClockTime) So we need two arguments: a `StateT ClockTime IO ()`, which will be `runTimer`, and an initial state of type `ClockTimer`, for which I'll use 0, for simplicity. &gt; runStateT runTimer 0 1413427634480 1413427634480 1413427634480 1413427634480 1413427634480 1413427634481 1413427634481 1413427634481 1413427634481 1413427634481 1413427634481 1413427634481 1413427634481 1413427634481 1413427634481 1413427634481 1413427634482 1413427634482 1413427634485 1413427634485 1413427634485 1413427634485 1413427634485 1413427634485 ^C Woah, that's a lot of output! It looks like the time is being printed on every iteration, regardless of whether the timestamp has changed or not. Why is that? Let's add a debug statement and figure it out. runTimer :: StateT ClockTime IO () runTimer = forever $ do newtime &lt;- lift curTime oldtime &lt;- get lift $ print (newtime, oldtime) -- new debug statement let (b,_) = runState (newTime newtime) oldtime if (b == True) then lift $ putStrLn $ show newtime else do return () &gt; runStateT runTimer 0 (1413428145694,0) 1413428145694 (1413428145694,0) 1413428145694 (1413428145694,0) 1413428145694 (1413428145694,0) 1413428145694 (1413428145695,0) 1413428145695 (1413428145695,0) 1413428145695 ^C Okay, so it looks like `oldtime` is always 0, the initial state we gave to `runStateT`. That's because the state is never changed! You need a `put` statement to change the state, and your program doesn't contain any. Let's fix that: runTimer :: StateT ClockTime IO () runTimer = forever $ do newtime &lt;- lift curTime oldtime &lt;- get put newtime -- new put statement let (b,_) = runState (newTime newtime) oldtime if (b == True) then lift $ putStrLn $ show newtime else do return () &gt; runStateT runTimer 0 1413428422316 1413428422317 1413428422318 1413428422319 1413428422320 1413428422321 1413428422322 1413428422323 ^C There is still a lot of output, because the timestamp changes all the time, but at least we're no longer printing the same value several times in a row.
A function I have occasionally made use of is: liftState :: Monad m =&gt; State s a -&gt; StateT s m a liftState act = do s &lt;- get let (a, s') = runState act s put s' return a Or, more generally: -- here f is a natural transformation between two monads mapStateT :: (forall a. m a -&gt; n a) -&gt; StateT s m a -&gt; StateT s n a mapStateT f act = StateT $ \s -&gt; f (runStateT act s) -- State s a ~ StateT s Identity a -- return . runIdentity :: forall m a. Monad m =&gt; Identity a -&gt; m a liftState :: Monad m =&gt; State s a -&gt; StateT s m a liftState = mapStateT (return . runIdentity) 
It might be easier to start by thinking about how to solve a simpler problem. How would you count the number of 0s that appear? If you can come up with something such that `f 0` gives you the number of 0s, then you can do `map f [0..7]` to deal with the iteration. 
Just finished watching it! Let me first off say that it's very generous of /u/ocharles to put himself on display like this, not many people work on a fresh problem in front of an audience. ### Regarding the general video * Ollie gives good explanations of what he's doing, explaining the reasons for choosing this or that, or why laziness is handy for debugging, etc. * I think Ollie demonstrates very good use of type holes. There were a number of times he used to help him write code. I'm only just getting onboard with this myself. Implementing instanced of MonadMask or MonadBaseControl are more approachable with type holes to help you out. * The TDD analog observation was interesting. Haskellers do work like that, I think. They will intentionally create compile errors as a way of moving forward. * Also would like to praise the problem statement, it seems to be the right combination of simple and tricky. * I think that Steve was a pleasant host but the approach taken was actually detrimental to the process and I'll explain why: this was essentially an interview setup. Steve was [backseat driving](https://www.hackerschool.com/manual#no-backseat-driving). It was clear that he's already studied the problem and been through the process of implementing it, he already sees where Ollie's going wrong because he has the advantage. This is clearly disrupting Ollie's ability to work with a clear head. At some point you can see Ollie's being deferential to Steve on his design decisions instead of working as an individual developer. So while I think Steve was being helpful and entirely sincere (probably trying to keep it at a good pace), it made for a slightly less interesting video and probably a less satisfying session for Ollie. Although I appreciate the work /u/purcell put into this. I'd be very interested to see a video where the host and the guest are actually collaborating on solving a fresh problem neither of them are familiar with. That would be very fascinating. As another suggestion it would be cool if you guys left the desk. I realise that requires more cameras and setup, but maybe if you guys described the problem and then got a coffee while talking about it and drew on a whiteboard or whatever it would have more, I dunno, je ne sais quoi. As Ollie himself said, “I'll warn you... coding while talking and not really stopping to think is not how I normally code.” The video was lacking in silence and thinking, I think you could've perhaps allowed that and then edited it out. Developers have to be in their comfort zone to perform normally. ### Emacs use nagging Forgive me, I must do it: * From an Emacs perspective, you move character-by-character too much. Use C-s way more and C-SPC and DEL to delete regions, also move by words with M-f/M-b. * If you used haskell-interactive-mode you wouldn't have to switch to the REPL in a separate terminal and you could jump to errors with a keybinding. * Of course I saw manual work done for things that are automated in SHM. * And also lots of manual formatting which hindent covers -- but you're already aware of and contributing to that. * Use C-e/C-a etc. ### Comparing with my own way of writing Haskell code (Stop me if I'm reading too much into the particular approach taken on the video.) Personally speaking, I tend to use the REPL a bit more in a bottom-up way. This is exemplified by Ollie's repeated use of the history to re-read the database from the REPL, and the fact the first thing he did was write the top-level function to read in a database. Personally, the first thing I'd implement would be some pure function like edit distance, because I can pretty much write that in the REPL. I noticed it was, I guess, a quarter way through the video before such a function appeared. Then I'd write some function like expand :: Text -&gt; Text -&gt; [Text] -&gt; [Text] that would expand two words into a chain. Or something. I can't say exactly what my approach would be, but it'd be bottom up for sure. Speaking about the REPL, for example the function neighbors was written purely in the editor -- i.e. a whole function was written, whereas for pure stuff I personally would've written that in the REPL from one expression and built it up by adding more functions around the expression incrementally so I knew what was going on at each stage. Although it's easy to say that from the peanut gallery. Needless to say I found the video very interesting.
Thanks for the hint. This seems to work. count :: Int -&gt; [Int] -&gt; Int count x [] = 0 count x (y:ys) | x == y = 1 + (count x ys) | otherwise = count x ys then I can call rs = [count i (concat rs) | i &lt;- [0..7]] 
Take a look at the [documentation for Set's map](http://haddocks.fpcomplete.com/fp/7.8/20140916-162/containers/Data-Set.html#g:9) &gt; It's worth noting that the size of the result may be smaller if, for some (x,y), x /= y &amp;&amp; f x == f y This is why Set does not have a Traversable instance and it does not share the same map operation with List, etc.
Thanks for the advices. I've already grabbed the source via git and built the whole project. (I'm on Linux, using Emacs). I was just curious on whether there was another way. I'll just start working on some bugs and start groking little pieces at a time. Debug.Trace then !!!
Out of interest, have you seen [this](http://www.seas.upenn.edu/~cis194/spring13/index.html])? It's pretty handy for getting started with Haskell. An alternative form of count is count x = length . filter (== x) which might use some tricks that are new to you. We could have written it as count x xs = length (filter (\y -&gt; y == x) xs) where (\y -&gt; y == x) is an anonymous function. We can replace (\y =&gt; y == x) with (== x) , which is known as a section. We can do this for any infix operator, so (/ 4) 12 == 3 and (6 /) 3 == 2 for instance. We now have count x xs = length (filter (== x) xs) but we can shift some parentheses count x xs = (length . filter (== x)) xs using function composition. We use `(f . g) x` as an equivalent of `f (g x)`. Once we've done that, we can use `$`, which is functional application but with very low precedence, to get to count x xs = length . filter (== x) $ xs and since they both end with `xs` we can simplify that to count x = length . filter (== x) Anyhow, I thought I'd step through that to show you there's light at the end of the tunnel :) I'm about 2.5 years into playing with Haskell, but I still remember being mystified about how to approach problems when I started.
can't `rs = [count i (concat rs) | i &lt;- [0..7]]` be replaced by `map ((flip count) (concat rs)) [0..7] ` ?
I don't see how the `Seq` abstraction is any different from just lists. In any case, the idea here seems to be closely related to `Foldable` and `Traversable` (and the mono versions of it), but not exactly the same. Here's how I think it maps out: 1. Use `[]` as the `Seq` type. 2. `Foldable` gives you the generic `toList :: Foldable t =&gt; t a -&gt; [a]`, which would play the role of the entry's `toSeq`. 3. But `Traversable` is too strong to implement the entry's `fromSeq`, because of [its peculiar shape preservation properties](http://stackoverflow.com/questions/19177125/sets-functors-and-eq-confusion/19192745#19192745). The closest you can get to `fromSeq` is `reassemble :: Traversable t =&gt; t () -&gt; [a] -&gt; Maybe (t a)`. (See Gibbons and Oliveira's ["The Essence of the Iterator Pattern"](http://www.cs.ox.ac.uk/jeremy.gibbons/publications/iterator.pdf) for an explanation of this function, section 4.1 in particular.) So which class ought to have the `fromList` operation? My first idea would be something like this: {-# LANGUAGE TypeFamilies, ConstraintKinds #-} import Data.Monoid import qualified Data.List as List import Data.Set (Set) import qualified Data.Set as Set import Data.Map (Map) import qualified Data.Map as Map import Data.Text (Text, pack) import GHC.Exts (Constraint) -- | If @Foldable@ is the class of things that can be turned into -- lists, then 'Unfoldable' is the class of things that can be -- constructed from lists. Law: 'fromList' is a monoid morphism: -- -- &gt; fromList [] == mempty -- &gt; fromList (xs ++ ys) == fromList xs &lt;&gt; fromList ys -- -- Minimal implementation: one of 'fromList' or 'unfold'. class Monoid m =&gt; Unfoldable m where type Element m :: * type Constraint m :: GHC.Exts.Constraint type Constraint m = () fromList :: [Element m] -&gt; m fromList = unfold step where step [] = Nothing step (a:as) = Just (a, as) unfold :: (s -&gt; Maybe (Element m, s)) -&gt; s -&gt; m unfold gen s = fromList $ List.unfoldr gen s instance Unfoldable [a] where type Element [a] = a fromList as = as instance Ord a =&gt; Unfoldable (Set a) where type Element (Set a) = a type Constraint (Set a) = Ord a fromList = Set.fromList instance Ord k =&gt; Unfoldable (Map k v) where type Element (Map k v) = (k, v) type Constraint (Map k v) = Ord k fromList = Map.fromList instance Unfoldable Text where type Element Text = Char fromList = pack {- Examples: &gt; fromList "Hello world!" :: Text "Hello world!" &gt; fromList "Hello world!" :: Set Char fromList " !Hdelorw" &gt; fromList "Hello world!" :: String "Hello world!" &gt; fromList (zip [0..] "Hello") :: Map Int Char fromList [(0,'H'),(1,'e'),(2,'l'),(3,'l'),(4,'o')] -} Maybe the `Monoid` constraint is superfluous, but the monoid morphism bit is the one type class law that first comes to my mind here.
This is a good point, but less a reply to the article than one would expect. Let's examine the claim in two parts. First: &gt; The problem with this in a dynamic typing context is you get no protection from either attempting to apply functions from the interface to terms that don’t support them... So we can see that here. We call counter and get back a pair of functions, supposedly -- one named get, and another increment. But without types, we can't be _sure_ we get that pair back -- someone else might implement OtherCounter, and give us back `peek` and `increment` with the same signatures, or `get` and `increment` with a different signature (one that takes the amount to increment) and we would have no way of statically checking for this. So the claim of the article here, makes sense -- types help us pin down abstraction. &gt; or from violating the abstraction by manipulating the underlying representation directly—indeed, this is often supported explicitly and in some cultures even encouraged, under the rubric of “reflection” or “monkey-patching.” And here we have the other issue -- many dynamic languages are not just language that _happen_ to be dynamically typed, but include other "dynamic" constructs such as the ability to mutate prototypes, or directly inspect and manipulate method dictionaries, etc. So while you may _think_ you have effectively hidden `value`, it turns out that in many languages there's still some sneaky way to get at it. In fact, if you take "the ability to allow late-binding" seriously, then it is a matter of _principle_ to allow some sneaky way to get at it. I grant this isn't about types per-se, but it certainly is about static vs. dynamic ideas of languages in the large.
&gt; I'd be very interested to see a video where the host and the guest are actually collaborating on solving a fresh problem neither of them are familiar with. That would be very fascinating. &gt; We almost made that video. Steve and I came up with a couple of possible challenges and pitched them to Ollie, letting him choose the one that most interested him.
i use `hasktags` (and `codex` for cross package browsing) to update my tags file that i use with vim (as with any other language). works like a charm.
Rather than using Haskell Platform, have you tried downloading the latest GHC and using Stackage? (and occasionally cabal sandbox)
Thank you. I've been using cabal sandbox with a varying degree of success, but I've never heard of Stackage, so I will look into it. Is it possible to use Stackage to install Haskell software like pandoc and hakyll?
Yes, use an inclusive stackage version. The current latest: http://www.stackage.org/stackage/805701d44dd044b9d44d3021826344cfe67ab411 That said, if all you want is a binary, cabal sandboxes with hackage do that just fine. But stackage is generally better because you don't have to make a sandbox for each binary... in fact you probably don't even need the sandbox at all. Stackage really shines when you're installing libraries for your development project(s). When you install the libraries piecemeal with hackage, there's no guarantee that later libraries are compatible. No such problem with Stackage. And then when your coworker wants to build your code, they just need to use the same stackage hash, and everything should just work.
Interesting read, though I'd prefer a more modest style.
&gt; Stackage really shines when you're installing libraries for your development project(s). When you install the libraries piecemeal with hackage, there's no guarantee that later libraries are compatible. No such problem with Stackage. And then when your coworker wants to build your code, they just need to use the same stackage hash, and everything should just work. This is exactly what I was looking for. Thank you for pointing me in the right direction. 
Nice ... Is this just to get your feet wet with SDL? Or are you venturing into digital music with Haskell territory?
First, some cases when you shouldn't: * If you're writing a data type that represents a stream of some sort, you want it to be lazy in the next element in the stream, just like lists are. * If you have a derived field that is rarely used. For example, if you have a `Text` field and also a `ByteString` field that contains the UTF-8 encoded version of that `Text` field. Here you could ask yourself: if written in an imperative language, would I have made this field be lazily initialized? * If you're implementing a data structure that relies on laziness for correct time complexity bounds. (You're probably not doing that.) The cases you should: * Basically all the rest. Definitely for all scalar field (`Int` and so on), but generally using strict fields everywhere lets you worry less about performance problems due to laziness. 
:) I should now state the fine print that stackage doesn't include *everything*. So it's still possible, just much less likely, to get into cabal hell. Best practice is still to have a cabal sandbox for each project you're working on. Or a shared sandbox for related projects (but shared sandbox is too much work for me). 
&gt; I'd be very interested to see a video where the host and the guest are actually collaborating on solving a fresh problem neither of them are familiar with. That would be very fascinating. Watch out for the next episode where I interview John Wiegley. I won't give anything away yet, but I had the idea for something I wanted to do, but had never done before (we built a clone of a GNU program). John was also familiar with the program, but it was also a new problem for us. That one was good fun :)
If you're doing some sort of recursion where you feed part of a result back in as an argument then you need lazy fields. I can't think of any other cases off the top of my head. Perhaps someone else can. f x = (x, 1) g = let (x, y) = f y in (x, y) -- *Main&gt; g -- (1,1) 
I *love* `ace-jump` mode. Well worth adding to your muscle memory!
 tick &lt;- (&lt;) &lt;$&gt; get &lt;*&gt; liftIO curTime Oh gosh, I actually understood that! I'm feeling my Haskell muscles grow.
Lazy lists are indeed a perfectly viable sequence abstraction, and as mentioned at the bottom of the article there are others. Lists have some advantages (they're a pure value, which is an incredible advantage, and they can represent infinite sequences), but also some disadvantages: transformations requires allocation which puts pressure on the GC, easy to accidentally introduce space leaks (space leak: average xs = sum xs / length xs), they don't provide any way to clean up resources such as file handles when the iteration is done, they can't handle parallelism, and they can't represent push collections such as an event stream. The plan is for the next part to be about the trade-offs of different sequence abstractions (maybe iteration abstraction is a better word). Unfoldable is a good place to put it yes. That also suggests some other options: representing seqs as unfolds, or going the dual route and instead of building up seqs you build up seq consumers, i.e. folds. Or we could go from the middle out, which brings us to transducers and pipes.
Yes, that is a reason why map on sets is a bad idea (and why it's better to work with a sequence abstraction). There is however a more powerful force stopping you: you *can't* make Set a Functor even if you wanted to, because Functor doesn't have an Ord constraint.
My plan was indeed to look at pipes (though maybe not yet in part 2), and at lens (which is more an abstraction for updating a collection than an abstraction for iterating over it, which brings us to an entirely new area). See also my [reply to sacundim](http://www.reddit.com/r/haskell/comments/2jdksc/the_best_collections_library_design_part_1/clay4s6).
It's effectively the 3rd edition of Bird's Intro to FP using Haskell book. The 2nd edition is the best book on Haskell that I know of, I am psyched that this has come out. I believe it's available right now from bookdepository.com.
I can't seem to make my configuration file figure out where to look for the Yi modules. When running `cabal exec ./dist/build/yi/yi` from within my yi directory, yi does start, but then complains: /home/maya/.config/yi/yi.hs:1:8: Could not find module `Yi' Use -v to see a list of the files searched for. The configuration file is the simple one taken from the top of http://yi-editor.github.io//pages/vim-config/: import Yi main = yi $ defaultVimConfig I am running GHC 7.6.3 and Cabal 1.20.0.3 on Ubuntu. Help?
[The book's website at Cambridge University Press](http://www.cambridge.org/us/academic/subjects/computer-science/programming-languages-and-applied-logic/thinking-functionally-haskell). Available from November 2014.
Unrelated: Is that arch and how did you get those system monitor widgets on the upper left?
That's a fun evening project! The Ableton people are probably getting scared right now :p
Looks like a small extension with big benefits. 
I really liked the 2nd edition, though I found myself growing impatient, having already read LYAH by the time I found that book. Pre-ordered, regardless. ~~Happy birthday to me!~~ Kindle edition was immediately available, so I didn't have to wait for my birthday. ~~PS - available for pre-order on [amazon.com](http://www.amazon.com/Thinking-Functionally-Haskell-Richard-Bird/dp/1107452643/ref=sr_1_1?ie=UTF8&amp;qid=1413461227&amp;sr=8-1&amp;keywords=Thinking+Functionally+with+Haskell)~~ Didn't realize the reddit entry linked to amazon already. Whoops... &lt;&lt; walks out quietly &gt;&gt; 
&gt; haskell-interactive-mode is something I haven't invested time in setting up to work with nix-shell. I don't have a global GHC, and this often complicates things - especially when emacs is assuming things to be on the global PATH. It sounds like it would be worth this investment though. It looks like we might have [nix-shell support](https://github.com/haskell/haskell-mode/pull/350) merged in pretty soon. That could be worth checking out once it's there. &gt; I used to use SHM, but found it went wrong 1% of the time, which was ultimately enough to stop using it. I may go back with a clear head and try and give you more bug reports again. I really do believe in the idea behind it. That's fair. When stuff is preventing you from working normally it's not useful. I was thinking the other day of extracting SHM into two projects: * One that comes with Haskell-mode which just provides "get the information of the current decl and setup the markers and such" as a minor mode that would let you do read-only navigation commands (M-a, ), C-w, C-M-s, etc.) and would never prevent you from doing anything, and then * SHM itself would be the more intrusive things like auto-reindenting as you type, hooking into backspace, auto skeletons and, copy/paste, C-k, etc. which are far more wrench-in-the-worksy. That would open up the navigation and info aspects of SHM to a wider set of people, I think. &gt; Are you saying C-a, C-e are bad? I'm actually in evil-mode in this video, but that's just details. Ah, I meant I saw your cursor moving char-by-char to the end of the line or to the start rather than hitting C-a/C-e. Same feedback as above. If you're using evil-mode I guess you come from Vim so figures you're still building up efficiency. Gotcha. But I have noticed I will sometimes move like that when I'm thinking rather than for actual navigation reasons. &gt; Maybe I'm just a top-down problem solver - in my real job I tend to just code and compile and follow the type errors until I think I'm done, and then poke things in the REPL until it's really done. Right that's probably the case too, having pair programmed with a few Haskellers, many work in this top-down way. :-)
I've wanted to make a toy like this for a while. I was inspired by this [the sound 15 sorting algorithms](https://www.youtube.com/watch?v=kPRA0W1kECg&amp;feature=youtu.be&amp;1) video and thought of tone matrices. If you had a nice sample and a grid it could be a fun toy and also you could put data on it, like, I dunno, render the Game of Life onto the grid and see what it sounds like. Or what if you made the blocks move like Tetris somehow? Fun ideas. I'll probably expand on this more with different colors for different samples or something. I've never used SDL for anything so this is also a litmus test for SDL.
It's Ubuntu and the panel at the top is what I wrote in Haskell with webkit, it's part of [my xmonad configuration.](https://github.com/chrisdone/chrisdone-xmonad). The widgets come from using i3status with [this configuration](https://github.com/chrisdone/chrisdone-xmonad/blob/master/i3status.conf), the icons are [FontAwesome](http://fontawesome.io/). 
The functional programmers are coming to get you, Ableton!
I like this a lot!
 &gt; find . -name \*.hs -o -name \*.lhs | xargs grep &lt;someliteral&gt; Try this some time: grep -rnE 'regex literal' --include=\*.{hs,lhs} . 
Did you do all the exercises? You have to do all of the exercises :) The fact that the 3rd edition has answers to the exercises is a big plus I think - I was lucky with the 2nd edition, since I started it right after working through "How To Prove It" I think I was better able to handle the lack of answers in Bird.
I didn't. I did the excercises when going through the wiki book, so it felt a bit repetitive. In hindsight, I suppose that repitition would have helped me commit things to memory better. Knowing that I have a way to check my work should motivate me to do them this time around, though.
Let me plug some more links here, including an article I wrote: - [Oh my laziness](http://alpmestan.com/posts/2013-10-02-oh-my-laziness.html) - [A talk by Trevor Caira](http://vimeo.com/69280214) : I Accidentally the Entire Heap (or How I Learned to Love the Profiler) - Edward Yang's series of post about the haskell heap, [which starts here](http://blog.ezyang.com/2011/04/the-haskell-heap/) - [The section about lazy evaluation](http://chimera.labs.oreilly.com/books/1230000000929/ch02.html) in Simon Marlow's *Parallel and Concurrent Programming in Haskell* If you have any question after you've read these ressources, we would be glad to help you so that you can one day help teach this to others :) Also, if you feel there's something that these ressources don't cover, please tell us so that we can fix it!
How is the Kindle edition? I'm sometimes hesitant to buy Kindle versions of tech books (due to code not being typeset well), but I don't need any more print books, really.
Side note: you can embed a `State` computation within a `StateT` computation with [the mmorph library](http://hackage.haskell.org/package/mmorph), specifically using [hoist](http://hackage.haskell.org/package/mmorph-1.0.4/docs/Control-Monad-Morph.html#v:hoist) and [generalize](http://hackage.haskell.org/package/mmorph-1.0.4/docs/Control-Monad-Morph.html#v:generalize). In fact, this is the first example in the [tutorial](http://hackage.haskell.org/package/mmorph-1.0.4/docs/Control-Monad-Morph.html#g:4). If you combine `hoist` and `generalize` you get this type signature: hoist generalize :: (MFunctor t, Monad n) =&gt; t Identity a -&gt; t n a This function can be used to generalize any non-monad-transformer (i.e. `State`) to the analogous monad transformers (i.e. `StateT`). It's easier to see this if you specialize the function to the case where `t = StateT s`: hoist generalize :: Monad m =&gt; StateT s Identity a -&gt; StateT s m a ... and `StateT s Identity` is the same as `State`, because that's how it is defined in `transformers`: type State s = StateT s Identity ... so the type can be simplified further to: hoist generalize :: Monad m =&gt; State s a -&gt; StateT s m a So then you can embed a `State` computation within a `StateT` computation like this: example1 :: State S A example2 :: StateT S IO A example2 = do blah &lt;- get lift (print blah) a &lt;- hoist generalize example2 ...
I'm really surprised that we don't use reddit for discussing these proposals instead of mailing lists. I find reddit's threaded comment system to be easier to navigate and it has upvotes/downvotes built-in
Could this extension also allow such datatypes to derive Storable in case the struct is on the heap?
I'm not sure how useful reddit's vote-system is for this purpose, as it's fuzzed: - http://www.reddit.com/wiki/faq#wiki_how_is_a_submission.27s_score_determined.3F - http://www.reddit.com/wiki/faq#wiki_how_is_a_comment.27s_score_determined.3F Moreover, I find it a bit difficult to follow a reddit-discussion beyond a certain size, which otoh becomes easier in a threaded mail-user-agent (where you can e.g. mark sub-threads read at once, unflatten the thread to read it in chronological order, and so on) Otoh, the reddit UI encourages a different style of commenting, which has some benefits as well.
yes, it is small extension in terms of user API. But it requires a lot of work under the hood. So I'm not sure it will be accepted by ghc devs.
No, but you already can do that, see http://hackage.haskell.org/package/storable-tuple
"the wiki book" - link please
You could send a sample to your kindle and see for yourself. (I'll probably do that too).
You are allowed to have more typeclass constraints than the typeclasses you implement, but this case is about typeclass constraints on *type constructors*. Take Functor: class Functor f where fmap : (a -&gt; b) -&gt; f a -&gt; f b This says: something is a functor if we can take *any* function `a -&gt; b` and a container `f a` and make a container `f b`. The problem is if you take `f = Set` then the type would be: fmap : (a -&gt; b) -&gt; Set a -&gt; Set b This would be invalid, because in order to construct a `Set b`, `b` needs to be `Ord`ered. We can implement a `mapSet` on sets: mapSet : Ord a,b =&gt; (a -&gt; b) -&gt; Set a -&gt; Set b But this is not a valid implementation for Functor's `fmap`, because Functor's `fmap` is required to work for *any* `a` and `b`. Does that make sense?
All I said was that I don't think lazy evaluation is more difficult to reason about than strict evaluation, given enough practice. Nowhere did I make any claims about performance.
http://yi-editor.github.io//pages/about-M-x/
The dependency that relies on specifically says: &gt; I cannot promise that the generated memory layout is compatible with that of a corresponding C struct. Why couldn't this extension provide a limited Storable derivation? It seems like all of the necessary elements (alignment, size, etc) are needed for the proposal anyway, would it really add that much?
I think so. So am I correct then in thinking that no containers who have constraints on their contained types can be functors? 
I'm curious if there are any issues with strictness. Since all foreign functions are strict in their arguments, does that mean that the fields of this structure are strict by default? Or only strict when used with foreign calls? If the former, that's different than what we're used to with Haskell product types. If the latter, do any issues arise? Overall it looks like something I'd want!
Well, I also not sure that my implementation (in the prototype for this proposal) is correct. But it is definitely possible to implement it correctly. I'm all for having Storable automatically derived, but I'm not going to implement it myself. Simply because the proposal is already too heavy for one person.
Yes.
The latter. At use site arguments will be deconstructed into basic foreign types, and the resulting structure will be constructed from basic types. I don't see any issue there, but I can be wrong. That's good question btw, I'll probably add a section into wiki page.
&gt; `flip` Newbies don't need to even know this function exists. Once someone *asks* if it exists, then it's fine to tell them the name, but it point-free style (where `flip` is almost always used) is initially confusing for many people.
&gt; grep -rnE 'regex literal' Try one of these some time: git grep -n 'basic-regex' git grep -nE 'extended-regex'
&gt; find . -name '*hs' | xargs -0 grep PATTERN I bet you probably wanted a `-print0` somewhere. For projects in git there is the `git-grep` subcommand and will save you from typing the find. It can also grep a single historical commit without checking it out. 
How did you compare your answers for "How to Prove It" ? Right now I'm solving it and checking the answer from some of the blog posts.
The `Num` issue is a bit of a hassle, better to have had some typeclass along the lines of `IsString` and had `Num` inherit from it.
A: Because it messes up the order in which people normally read text. Q: Why is top-posting such a bad thing? A: Top-posting. Q: What is the most annoying thing in e-mail?
Now that I think about it, you can make Set a Functor using Coyoneda. I believe it looks something like data Coyoneda f b where Coyoneda :: (a -&gt; b) -&gt; f a -&gt; Coyoneda f b instance Functor (Coyoneda f) where fmap f (Coyoneda g a) = Coyoneda (f.g) a It works because Coyoneda stores the functions you're going to map separately from the actual Set, using the GADT to express the typing rules that Functor needs. So mapping just becomes function composition on the separately stored function, and the optimization fmap f . fmap g = fmap (f.g) gets expressed explicitly. You still need an Ord constraint on the unwrapping function of course. You can even make it a monad by wrapping the Coyoneda in a Free, and then a Codensity. Though I think that's equivalent to just using ContT (which also gives you Functor). I think you could implement fusing filters by making it a monad, though I believe both techniques give poor asymptotic complexity when used as a monad. Still it's a useful technique to be aware of.
I've had people chew me out for bottom posting, which is why I've been top posting. I never knew it was an issue.
One interesting thing is that when things are pure and lazy certain types of parallelism can be achieved almost for free. The Concurrent and Multicore Programming chapter of Real World Haskell goes into it in more detail, but you can usually do something just like someList `using` parList rseq To make the evaluation of someList parallel. Even if someList is defined in terms of some big expression (like fmap f . filter p . fmap g $ someOtherList) the work will still be done in parallel. If you're set on Sets I don't think it should complicate things too much, though I can't be sure. More details below: http://book.realworldhaskell.org/read/concurrent-and-multicore-programming.html http://stackoverflow.com/questions/5606165/parallel-map-in-haskell http://hackage.haskell.org/package/parallel-3.1.0.1/docs/Control-Parallel-Strategies.html
No, I don't, because that's time consuming and difficult to get right (at least with Thunderbird). This is why I prefer reddit for comment threads: quoting the parent should be opt-in, not opt-out.
I generally prefer bottom posting, but I don't let either style really get to me. My general rules for when I send email are: * Someone non-technical: top post, they'll be confused otherwise. * Someone technical, I'm initiating the first reply: bottom post. * Someone else has already replied: follow whatever style they used. * Multiple different people with different posting styles: flip a coin.
Desiring opt-in quoting indeed seems like a fair position.
This is getting dangerously close to where I'm planning to go ;-)
&gt; I'm curious if there are any issues with strictness. I wouldn't imagine any more issues than normal. I'm pretty sure CInt (etc). are still lazy, and that would apply to the new `CStructures` I'm pretty sure all FFI calls are strict in all their arguments.
http://en.wikibooks.org/wiki/Haskell, I believe.
&gt; emphasises fundamental techniques for reasoning mathematically about functional programs I would LOVE to read this. I just had a course about Coq. That was pretty fun. But I would love to transfer that knowledge to a more popular language.
If this is a topic of interest for you, you should try out Coq. It's pretty fabulous! 
I would like it if GHC would warn me that I forgot to bang my fields and I had to explicitly type ~ to imply a lazy field, I use it so rarely. I think this would cure a lot of performance woes for newbies and intermediates alike. Or maybe, e.g. if I enable -O2, GHC will consider "okay they want speed, *now* I'll warn about unbanged fields". Something, *anything*, to make people more aware of this would be swell. Performance linting in general.
I just bought the book for Kindle so you don't necessarily have to wait until release of the paper copies!
Since the rest of this comment thread has gone off talking about comment threads themselves (perhaps ironic, given that it was a claim that Reddit is good for discussions), I'd like to make at least one comment on topic: I like this proposal, and at work we're already using deepseq-generics. I look forward to the possibility of writing `deriving DeepSeq` as well :).
&gt; An explicit thunk is basically a value of type () -&gt; ... where the first time it is evaluated it is overwritten with its result. I *think* the Lazy and Inf (codata) types in Idris are this way. They share an implementation; they are different because the first gets totality-checked and the second get progress-checked.
Wow, you're a hero of email!
&gt; idiomatic haskell [...] idiomatic c++ I actually don't think that's a valid comparison when you are talking about performance. In areas where performance is critical is it common in both languages to write things very differently (and preferrably add massive comments) than "normal" code. Things like [Q_rsqrt](http://en.wikipedia.org/wiki/Fast_inverse_square_root) are not idiomatic C++, but such techniques are very valuable and common in high-performance C++ code. Things like [unsafeUseAsCString](http://hackage.haskell.org/package/bytestring-0.9.1.4/docs/Data-ByteString-Unsafe.html#v%3AunsafeUseAsCString) are not what I'd recommend to a Haskeller, until they can show me (via profiling numbers) that it's really what they need. Performance-tuned Haskell is about takes somewhere from a little less time to 3 times as long as performance-tuned C++ code. [[source](http://benchmarksgame.alioth.debian.org/u64q/benchmark.php?test=all&amp;lang=ghc&amp;lang2=gpp&amp;data=u64q)]
go back and read the thread. you responded to someone who made the common-sense claim that haskell will not result in faster programs than c++. the crux of your argument was a claim that they merely have a shallow understanding of haskell's challenges. i suggested you show me something that demonstrated how a deeper understanding of haskell could support your argument. either no such code exists or no one has the understanding you feel is a prerequisite
...don't you rather mean `... deriving (..., NFData)`? =)
Lazy lists can handle parallelism, of the Control.Parallel.Strategies variety. I *think* that can be adapted to infinite sequences as well. I was thinking ListT was the optimal solution, but now that I've thought about it longer it seems like it and trivial parallelism are rather diametrically opposed. Though when effects aren't necessary I think you can get both efficient &gt;&gt;= and ++ with folds; suddenly I'm curious if you can write a cheap parallel maps as well.
Lazy lists can handle parallelism if the individual operations are expensive. Say you have a huge array in memory, and you want to increment each element. You can do that in parallel by splitting it up into a couple of chunks and let each thread deal with one chunk. If you did a conversion to- and from a lazy list you'd lose almost all parallelism because dispatching each individual element to a worker will be more expensive than the operations themselves.
It's not possible since all IO function have the "wrong" type. If they had had type `MonadIO m =&gt; S -&gt; m T` instead you would have been all set. That would have been nice in general, because then there could have been different IO monads for different kinds of operations. 
&gt; you should try out Coq. It's pretty fabulous! I just... really, did anyone think that through?
Hooray for typeclasses as a kind of weird subtyping.
Oh, that's very nice. I'd been using my version for over 15 years and never realised this version was possible. 
Yes, me too --- looking forward to bedtime reading!
Odd. Amazon.co.uk has it as published on 9-Oct-2014, and is already selling used copies.
It's surely not a non-starter, but it lacks high quality libraries for image/texture/audio loading, has non-optional garbage collection and optimizing some of the low level code would probably be harder to write in Haskell than in imperative C++. It is much easier for an expert C++ programmer to simply program in a data-oriented, almost functional style in C++, avoid most of the pitfalls of it and not be forced in a programming model that does not reflect the underlying hardware.
I feel as if I'm about to make a very shallow observation of a very deep topic (and perhaps wedge my foot firmly within my mouth), but .NET's IEnumerable interface seems like a fairly good fit for Seq here at least as far as a common interface for a collections library goes.
At least for functions from base there are already lifted versions in [lifted-base](http://hackage.haskell.org/package/lifted-base).
Whoah, 4K quality!
It seems as though C2HS already supports [this](https://github.com/haskell/c2hs/tree/master/tests/system/structs), including support for getting at the [inner members](https://github.com/haskell/c2hs/blob/master/tests/system/structs/Structs.chs#L29) of a struct.
What functions are you using? Depending on what they are, there might be an equivalent library that sits in MonadIO m =&gt; m.
Lets see what that code is doing: 1. It maps `f` over the set, which returns `Maybe a`. That means that it will construct an ordered set represented by some tree structure that is ordered by the Ord instance of `Maybe`. All the duplicate Nothings will be filtered out, leaving only one. 2. The `filter (/= Nothing)` goes over all elements, and throws the one Nothing out, and builds an entirely new ordered set represented as a tree. 3. It removes all the `Just` wrappers, and constructs the final ordered set. This is an excellent example why `map` and `filter` should not work on the collections, but on a sequence abstraction. Not only would that make your `map'` generic over all collections, it would also be much more efficient.
It is hard to follow the example because of a lot of typedefs, but it seems to pass/return *pointer* to structure, not the structure itself. Did I miss something?
That's the reason why recently started to write code like this: foo :: (Applicative m, Functor m, MonadState Foo m, MonadReader Bar m, MonadIO m, etc.) =&gt; m () With `ConstraintKinds` I can even use a convenient type alias for this: type App m = (Applicative m, Functor m, MonadState Foo m, MonadReader Bar m, MonadIO m, etc.) foo :: App m =&gt; m () However, I'm wondering if this is idiomatic. Or if there are any downsides.. 
I've no idea whether this is idiomatic, but I've been doing the same. Hooray for `ConstraintKinds`.
&gt; [..] it's almost universally used in business contexts and spills over everything. Some time ago, when I had to endure such TOFU replies, and consequently responded in a proper sensibly-quoted-trimmed + bottom-posting fashion, hoping my email partner may notice at some point and get the hint, only to be told to use TOFU style explicitly... I was very close to rage-quitting that conversation...
I was writing something similar for years. Now I just write "foo :: App -&gt; IO ()" and I'm happy. Usualy it makes no sense to stack monads if you anyway have `MonadIO` instance.
I'd think you could hack that up using Generics already?
&gt;&gt; did anyone think that through? &gt; They did indeed. They were thinking in French.
&gt; different IO monads That makes me worry... The `Prelude.IO` should be at the bottom of the transformers stack if you require `MonadIO m`. How to replace it with different one? 
I responded to the claim that "laziness is far more difficult to reason about that strictness". Furthermore, I explicitly quoted the bits that I was referring to and none of them mention performance. I absolutely never made the claim that "idiomatic haskell ... more efficiently than idiomatic c++". Stop constructing straw men.
You can do it without constraint kinds using older extensions class (Applicative m, Functor m, MonadState Foo m, MonadReader Bar m, MonadIO m, etc.) =&gt; App m instance (Applicative m, Functor m, MonadState Foo m, MonadReader Bar m, MonadIO m, etc.) =&gt; App m Then turn on UndecidableInstances, etc. until it stops screaming. This has the benefit that you can talk about `App :: (* -&gt; *) -&gt; Constraint` without applying it to `m` and there are situations where that can matter.
I didn't really have a solid plan for checking answers last time. I'm currently working through Software Foundations with some others from my local FP meetup group - once that is done I hope to run a study group for How To Prove It, using Coq to check the answers. I highly recommend Software Foundations by the way ☺
What /u/satsujinka and /u/bss03 said
One benefit I've found to using `liftIO` everywhere is that when you need to refactor, it is completely obvious where your code is using `IO`. I used to look for ways to hide `liftIO`, but this has helped me so many times that I now try to make it as visible as possible. It helps me to distinguish the boundary between `IO` and pure computations.
&gt; told to use TOFU style explicitly... I was very close to rage-quitting that conversation... I know [that feel](http://cdn.meme.am/instances/500x/53838429.jpg).
If you wanted to write a higher order constraint, like: class All (c :: k -&gt; Constraint) (as :: [k]) (this one says the constraint constructor 'c' holds for every element of the type-level list 'as') You couldn't use the Constraint type synonym with this class, because type synonyms can't be used unless they're fully applied. However, if you wrote it like /u/edwardkmett suggested, you could write a context like: (All Foo as) =&gt; ... In this case, 'as' would be of kind '[* -&gt; *]'. That help? 
Ah, right. Then maybe Pipes.ListT Control.Parallel.Strategies.Eval, to give you more fine-grained control over chunking, as well as fusing maps/filters. Though I'm not even sure if that would work perfectly... it seems a lot more complex than on the surface.
Is this a Mark V Shaney?
That quote is _extremely_ true of the 2nd edition - it sounds like this book was more or less made for you :)
Depending on how much IO you're doing and what you're doing it for, you might consider the use of one or more free monads / free monad transformers to cover that functionality.
How do 1 and 3 work?
&gt; Lazy lists are indeed a perfectly viable sequence abstraction, and as mentioned at the bottom of the article there are others. Perhaps a more interesting question is what do these viable abstractions have in common. "Free monoid" is the first candidate that comes to mind, but it might not be it. &gt; Lists have some advantages [...] but also some disadvantages: transformations requires allocation which puts pressure on the GC, [...] In the presence of list fusion, this is not unequivocally true. We need to separate the *abstraction* from the *implementation details*—the allocation/GC concerns are the latter, not the former. &gt; [Lists] don't provide any way to clean up resources such as file handles when the iteration is done, [...] Good one... &gt; they can't handle parallelism, [...] But perhaps the better solution there is *not* to use sequences. Some sort of balanced tree might be best. For example: -- | Split a tree into two subtrees. The two trees should ideally be -- roughly half as large as the original. Laws: -- -- &gt; uncurry paste (split t) == t -- &gt; split (paste t1 t2) == (t1, t2) -- split :: Tree a -&gt; (Tree a, Tree a) -- | The inverse of 'split' paste :: Tree a -&gt; Tree a -&gt; Tree a class Splittable t where type Element t :: * toTree :: t -&gt; Tree (Element t) fromTree :: Tree (Element t) -&gt; t
I got sick of having to write import Data.Text (Text, pack) import qualified Data.Text as T and thought others might find it useful to have a snippet to automate the process a bit more. If you have improvements or bug fixes let mew know and./or fork the hist and submit a pull request. I've submitted it as a patch for SublimeHaskell too.
You need a or b to be enumerable.
These are highly non-computational things in general, just FYI. Well, the second one is computation (it's just lookup on `Set` and doesn't need `(a,b)` specifically), but the others aren't.
I'm unconvinced about the last one. b -&gt; Set a contains const ∅.
It should be `(b -&gt; Set a) ~&gt; (a -&gt; Set b)`
`Set (a,b) ~ ((a,b) -&gt; Bool)` seems to be related to the subobject classifier to me.
Chris Taylor covers #2 in [this video](https://www.youtube.com/watch?v=YScIPA8RbVE&amp;feature=player_detailpage#t=3106) (51:46 - 58:10) and ultimately gets to "Set x ~ x -&gt; Bool".
There is a design and I started working on `StrictData`, which is a subset of `Strict` where all fields are strict by default. I have lacked time to work on it lately though.
&gt; In the presence of list fusion, this is not unequivocally true. We need to separate the abstraction from the implementation details—the allocation/GC concerns are the latter, not the former. This is true, on the other hand, *predictable performance* is important. It's very valuable to be able to write your algorithm a certain way, and be guaranteed that it's going to perform well, rather than having to pray to the compiler gods, who may change their mind on a whim if you decide to write your algorithm in a slightly different way. This is related to the idea of *zero cost abstractions*. &gt; But perhaps the better solution there is not to use sequences. Sequences is not the right name for it, but the idea still applies. The operations map/filter/flatmap support parallelism. So if you write a pipeline `map f . filter p . flatmap z . filter q map g` then you ought to be able to use that as a parallel algorithm against a "parallel sequence" abstraction. If those operations are defined in terms of lists, then that's not possible.
I did the rendering of Game of Life on a similar grid on a music hackathon this summer. Sound was horrible but very fun!
Haha, nice!
Care to elaborate? I might be missing something, here's a demo of what I hacked up now: data T = T { word8 :: Word8, int64 :: Int64, word8' :: Word8, int32 :: Int32, word8'' :: Word8 } deriving (Show, Eq, Read, Generic, Typeable) -- Note this is not Foreign.Storable.Storable instance Storable T main :: IO () main = do putStrLn $ "Have some T: " ++ show t putStrLn $ "Its size would be: " ++ show (size t) putStrLn $ "And alignment: " ++ show (alignment t) where t = T 1 2 3 4 5 When running: Have some T: T {word8 = 1, int64 = 2, word8' = 3, int32 = 4, word8'' = 5} Its size would be: 25 And alignment: 1 https://gist.github.com/NicolasT/6ccfbfe5bcc79111f9c6
That's nice to know. Thanks!
Will definitely go in.
For an equivalence, it should be what you say. But the one in OP makes sense as “inverse image” if you flip the direction: `(a -&gt; b) -&gt; (b -&gt; Set a)` (assuming again that `a` is enumerable and `b` is an `Eq` instance).
That's certainly a good point. I will keep this in mind!
Cool, thanks. Guess this book is being published just in time for Christmas :)
Mighty kind of you. A little too far away from Denmark though, and I'm stuck with exams now. 
I wrote my comment late last night. Perhaps I wasn't thinking totally clearly. Writing and reading isn't as relevant as using a library and implementing it. Or something like that. 
tabulation is taking the homset, right? classification isn't about functions at all, but just a property of sets in general. `Set a =~ a -&gt; Bool`. in that context, I think as /u/tomejaguar says, its really just the subobject classifier. As others have noted, the other doesn't yield back a function, since it may not be defined on all `a` -- you need a bunch of other conditions for this to work out.
Maybe but I find in C++ you write a lot more idiomatic code and still get decent performance but to get the same performance in Haskell you would need to write very differently. The C++ example you highlighted is well known because it is so out of the ordinary.
You're welcome!
&gt; Free monoid I actually think we want a free magma + co-magma family. If you have fast split, join (`(++)` not `Control.Monad.join`), and singleton, you get fast (single-element) pre-/ap-pend and functional updates "for free". viewL/viewR are special cases of split with more refined types. * split :: i -&gt; m -&gt; (m, m) [co-magma family] * join :: m -&gt; m -&gt; m [magama] * singleton :: Elem m -&gt; m [free magma] Generic containers should strengthen these requirements to include monad / comonad. * explode :: m e -&gt; m (m e) -- the "natural" split, or singleton [comonad] * concat :: m (m e) -&gt; m e [monad] But, I'm just spitballin' here.
&gt; I really would like some examples. It's a bit annoying that criticism against C++ is so often written off as the person not being familiar with the language or being more familiar with another one. There really is no need to be quite so arsey by quoting back what I've said at me slightly changed. The changes you made means the statement is not true anyway - people don't write off criticism of C++ as people not being familiar with it like they do in Haskell. Usually they will agree or argue there point if they have one rather than just repeat some cliché. Generally you're comment isn't very nicely worded at all. Being civil isn't that difficult Most naive C++ code will outperform the equivalent Haskell code and if you stick to idiomatic C++ you will get very good performance. If you stick to idiomatic Haskell you will not get the same performance as Haskell is a slower language than C++. &gt;Idiomatic Haskell doesn't mean a lot of lists / laziness (or poor performance) any more than idiomatic C++ means lots of object copies (or poor performance). Fine but even when you don't use lists and laziness you're still going to have worse performance than idiomatic C++. If you want the same kind of performance you're going to have to write some pretty nasty Haskell to get anywhere near.
&gt; If you want the same kind of performance you're going to have to write some pretty nasty Haskell to get anywhere near. That has not been my experience. 
And in concatenative languages—in which not application, but composition is the default—the solution is: x0 x1 … xN foo = x0 x1 … xN qux bar foo = qux bar If your language requires you to do type system gymnastics to make simple pointfree code work, it’s probably better to just avoid pointfree style beyond the basics. 
Tabulation is taking the graph of a function, i.e. the set { (*x*,*y*) | *f(x)* = *y* }.
I haven't had time to give this a real read yet, but for building and applying n-ary functions, there's a trick at https://ocharles.org.uk/blog/posts/2014-04-26-constructing-generically.html. That may simplify your final functions, I'm not sure.
Another approach, if you consider this more of a syntactical convenience problem, is: {-# LANGUAGE TemplateHaskell #-} module Data.Function.Wrap where import Language.Haskell.TH wrap :: Int -&gt; Name -&gt; Q Exp wrap i f = lamE [varP wrap] (lamE (map varP args) (appE (varE wrap) (foldl appE (varE f) (map varE args)))) where wrap = mkName "w" args = map (mkName . ("x" ++) . show) [0 .. i] Then if you have: bar :: Int -&gt; Int qux :: Int -&gt; Char -&gt; [()] -&gt; Int You can write: λ&gt; :t $(wrap 2 'qux) (Int -&gt; t) -&gt; Int -&gt; Char -&gt; [()] -&gt; t The splice looks like: λ&gt; :set -ddump-splices λ&gt; :t $(wrap 2 'qux) &lt;interactive&gt;:1:3-13: Splicing expression wrap 2 'qux ======&gt; \ w -&gt; \ x0 x1 x2 -&gt; w (qux x0 x1 x2) $(wrap 2 'qux) :: (Int -&gt; t) -&gt; Int -&gt; Char -&gt; [()] -&gt; t And finally: λ&gt; :t $(wrap 2 'qux) bar Int -&gt; Char -&gt; [()] -&gt; Int I'm not saying this is a better solution, but in terms of simplicity it's worth considering.
Except it's on haskell 98... Not 2010
&gt; It's more of a type hackery puzzle than a practical problem though. Oh, carry on then. :-)
Preaching to the minister here, but others who want to learn more should read /u/evincarofautumn's ["Why Concatenative Programming Matters"](http://evincarofautumn.blogspot.com/2012/02/why-concatenative-programming-matters.html). To see people trying to play with row polymorphism in Haskell, take a look at the example code in /u/shangas' ["Concatenative, Row-Polymorphic Programming in Haskell"](https://github.com/leonidas/codeblog/blob/master/2012/2012-02-17-concatenative-haskell.md) and my ["Playing with Factor's Row Polymorphism in Haskell"](https://gist.github.com/rampion/1847747) (I cannot vouch for the quality of my code, it's over 2.5 years old now).
[`hoist`](http://hackage.haskell.org/package/mmorph-1.0.0/docs/Control-Monad-Morph.html)
I'm just picking up the basic skills (like understanding Quantifiers etc.) and the journey has been pretty slow. For some days, I'm stuck on some problems for two or three days. Do you think it would be a good idea to try out Coq on the initial stages ?
It seems like the problem could be solved if we could just view the argument list as a "first class", uncurried argument list (hlist) rather than as nested function calls. Then it would just be normal composition. Generally, it would be nice to have polymorphism over the shape of function arguments. Though I suppose this is just an instance of the (nontrivial) row-polymorphism problem huh?
nope, you're correct Yuras, C2HS can't read a struct off of registers! :) 
It's simpler if you represent the argument list more explicitly with DataKinds: {-# LANGUAGE TypeOperators #-} {-# LANGUAGE DataKinds #-} {-# LANGUAGE TypeFamilies #-} {-# LANGUAGE OverlappingInstances #-} {-# LANGUAGE FlexibleInstances #-} module Test where type family Args f :: [*] where Args (x -&gt; r) = x ': Args r Args x = '[] type family Result f :: * where Result (x -&gt; r) = Result r Result x = x type family Fun args r :: * where Fun (x ': xs) r = x -&gt; Fun xs r Fun '[] r = r class Lifting f where lifted :: (Result f -&gt; b) -&gt; f -&gt; Fun (Args f) b instance Lifting f =&gt; Lifting (x -&gt; f) where lifted m f x = lifted m (f x) instance (Result f ~ f, Args f ~ '[]) =&gt; Lifting f where lifted m x = m x
Yeah, that article has a lot of issues that have been rehashed to death, but it’s a decent introduction. I might do a proper sequel sometime. I did amend it to refer to polymorphism on stacks as “stack polymorphism” rather than “row polymorphism” to avoid confusion with Leijen’s row unification. And [Kitten](https://github.com/evincarofautumn/kitten) now has a compiler targeting C, in addition to the interpreter mentioned in the April 2013 update. 
Coq is mostly a proof assistant. So if you are stuck with your proof, Coq is a good idea. If you are stuck with functional programming keep doing haskell
I think Paul isn't helping his argument by using the OCaml set example to demonstrate the Curry Howard isomorphism. (BTW, it is often called propositions-as-types or formulae-as-types these days because I'm pretty sure Howard has mentioned that he doesn't like the original name) &gt; . Look again at the OCaml Set module example above. You already get a universally-quantified property just from using the type system to guarantee an invariant expressed in an ADT. The abstraction guarantee here is not at all a consequence the formulae-as-types correspondence. Nowhere is the type system used to prove any of those invariants. It's more in the Reynolds view of a type system as a syntactic means of enforcing levels of abstraction (in other words abstraction invariants), than the Curry-Howard view of type systems as logical systems and programs as proofs. Formulae-as-types can't be used to express properties of your actual program in a non-dependent type system because your propositions (types) can't mention your program (proofs)!
You can very easily implement concatenative programming in Haskell: just use nested binary tuples to implement the stack. This has the effect of making functions of "multiple arguments" (i.e. curried functions) instead functions of a single nested tuple, and then you don't have issues composing functions in point-free style because everything is just a function from a tuple to a new tuple. That ends up being exactly analogous to what you just wrote.
Of course. But no one is going to actually program that way in Haskell, not least because none of the libraries are written like that. And you still have different types of composition for pure vs. impure functions because Haskell’s effect system is in the same band as its value system. Concatenative-land is closer to STG than to Haskell. Moreover, GHC’s compiler and runtime are both so overpowered for such a simple language that I doubt they could conspire to achieve time, space, or power performance as good as a straightforward native implementation. But I’m getting sidetracked. The point is that I like composition, and I think application isn’t a very good default if you want to do dataflow more complex than the simple pipelines you can build with `(.)`. 
Concatenative style is used very often in all kinds of languages, even in OOP. It's called method chaining. Functional folks call it point-free. Same thing, really, even uses (by accident, I think) the same symbol: `.`. Just in different directions. What's used way less often is *stack-based* concatenative languages. Which might be, you know, because a little application, a little bit of variable binding, goes a long way making code readable: The stack doesn't have the function to enable composition, but to avoid variables and application at all cost. If you're out for a language to write in, not ideological purity, you should toss ideological purity out of the window.
A while ago I looked at dispatching on function types, and made the following observations: - Extracting polymorhpic return types from function types is only possible with `IncoherentInstances`. Unfortunately closed type families can't ascertain that a polymorphich type variable is *not* a function type, since such a variable can be possibly instantiated as a function type later. `IncoherentInstances` is an ugly tool, but at least it works, and we can quarantine it in a separate module anyway. - When writing instances, only the absolutely minimum amount of typing information should be present in the instance head, everything else should be in the constraints, possibly as a type equality constraint. That said, here's a full implementation of generalized function composition that works with polymorphic types too: {-# LANGUAGE UndecidableInstances #-} {-# LANGUAGE MultiParamTypeClasses #-} {-# LANGUAGE FlexibleInstances #-} {-# LANGUAGE TypeFamilies #-} {-# LANGUAGE IncoherentInstances #-} class CompN a b f g where ($.$) :: (a -&gt; b) -&gt; f -&gt; g instance (f ~ a, g ~ b) =&gt; CompN a b f g where ($.$) = ($) instance (CompN a b d g', g ~ (c -&gt; g')) =&gt; CompN a b (c -&gt; d) g where ($.$) f g = ($.$) f . g 
I guess you are aware that there are real sequencer controllers that works like this?
arbitrary type alias given.
Sure.
[Sorry about the crappy hosting website](https://www.sendspace.com/file/37b5o4). It's the third I'm trying and all of the ones I have found are awful. Edit: for the record, [the scribd link](https://www.scribd.com/doc/243376518/Visual-Haskell-A-First-Attempt-H-John-Reekie-1994) where the document is being prepared since what feels like forever.
*A little* application, and *a little* variable binding are great, and totally necessary to write readable programs. My concatenative language, Kitten, supports variables and uses them a lot more than Factor or Forth, and I think that’s a good thing. Take ideological purity and bin it! But the only concatenative operation in common use is 1-input, 1-output chaining, and I think that’s because that’s the only dataflow abstraction most languages have. Method chaining in OOP uses the “this” parameter. `(.)` in Haskell operates on unary functions with one result. Unix pipes use `stdin` and `stdout`. While many useful data flow graphs can be expressed as straight lines, many cannot. The most popular solution for that is named local variables. But there are other solutions! We can abstract away from names in data flow in the same way that we have abstracted away from `goto` and recursion in control flow. `goto` has way more power than I need to just write a `while` loop. Recursion has way more power than I need to just `map` a function over a list. And named variables have way more power than I need to just `dup`licate a value. It’s akin to using a `Functor` or an `Applicative` instead of a `Monad`. You ditch the powerful interface to gain more powerful reasoning, because in a weaker system there are stronger theorems and fewer places where things can go wrong. 
&gt; But the only concatenative operation in common use is 1-input, 1-output chaining, and I think that’s because that’s the only dataflow abstraction most languages have. In the presence of ADTs it's the only necessary, as the rest is isomorphic. What you don't get is all that fancy stack mangling and rearranging, and I'd argue that that's a *good* thing as composing that kind of stuff gets confusing and opaque very fast and it is exactly that thing what gets made explicit by having variables and product types. As such, I'm all for being able to write 2 3 * [x -&gt; x x] * and even abstracting it to 2 3 * dup * . Composing two `dup`s would still be easy. But composing `swap`s and fancier stuff? Madness. Hence, in my opinion, leave out the invisible stack, there's better solutions. Acquire lenses and stuff, instead.
Ah, whoops, crossed my wires there.
I'd guess it wouldn't work, because many people won't want to self-identify themselves as advanced, no matter how advanced they are.
&gt; In the presence of ADTs it's the only necessary, as the rest is isomorphic. I’m not sure I know what you mean by this. Would you elaborate? &gt; …composing that kind of stuff gets confusing and opaque very fast and it is exactly that thing what gets made explicit by having variables and product types. It does get confusing when you have lots of stack shuffling. The typical Forth response is “don’t do that”, but I find that needlessly hostile. Instead, I’d say you should be able to use names when it makes your code clearer, *and* you should be able use data flow combinators when it makes your code clearer. Of course, it also depends on what exactly you mean by “explicit”. I say “explicit is better than implicit” and write `map` or `fold` instead of a `for` loop—being explicit about my goal and implicit about the means. Whereas, many Python programmers would repeat the same mantra and come to exactly the opposite conclusion. Anyway, pure concatenative programming is not something I find practically viable, nor even particularly appealing. However, I do have a long list of reasons for believing that stack-based functional languages are a nice *starting point* for a practical language design, and I’m just exploring that space as long as it seems useful. 
Hey thank you dude... People keep on publishing these little tricks and they are starting to add up.
That's another way to do it, but my ghci hacking is often on a per file basis and each file has different things that I might find useful. With this, I can load a file and start messing about. If I find something I want to have easy access to, I can add it and a single reload and I've got it. I'd actually like to combine this /u/chrisdoner 's ghci persistent storage idea. 
I usually do it the other way round: My .cabal files often include a ghc-options: -DCABAL field, so that I can load my library both interactively and build it with cabal. This is extremely useful for getting the right path to an extra source file.
Why encode that in the tree rather than the class? instance Foldable LeftFold Tree ... EDIT: Oh sorry, it's the `Foldable` from `Data.Foldable` so has to conform to that class definition. In which case I'd say don't bother using typeclasses for this. Just write `leftFold` and `rightFold` as functions and be done with it.
I have done something similar using implicit parameters, type functions, and string lits (as phantoms). It was annoying to get working!
I don't understand the premise of the series. If the idea is to have a universal sequence abstraction, why have other datatypes at all?
Don't forget the Haskell Music Suite, its a library for representing and working with music. It makes pretty heavy use of type classes so as to be as general as possible. For example, to support non western tunings and scales. Edit: Link: http://music-suite.github.io/docs/ref/ Edit: Just realised you're probably more interested in synthesis and signal level stuff, so the music suite may not be relevant as it operates at the 'note' level only.
[This](http://stackoverflow.com/questions/9656797/variadic-compose-function) question from SO is relevant. 
I don't know if this helps the problem you are trying to solve, but you can use `cabal repl` to open up ghci with all the right libraries loaded, allowing you to interactively load your cabal-dependant haskell files.
It is also related to _fibrations_ when `a -&gt; b` is (remotely) projection-like. `Set a` being the fiber over `b`.
My ghci hacking is almost always per project with very occasional per file. Interesting how habits differ. 
A sequence only represents elements "in flight". Sometimes you need a real data structure. For example if you are filtering duplicates, then you need to build an intermediate Set. In philosophical terms: a data structure represents items laid out in space, a sequence represents items laid out in time.
&gt; &gt; In the presence of ADTs it's the only necessary, as the rest is isomorphic. &gt; &gt; I’m not sure I know what you mean by this. Would you elaborate? Well, for one, you can represent the stack as passing *one* nested tuple in and *one* nested tuple out. There's nothing you can do with a stack that you can't do with ADTs.
This is a good read. I'm looking forward to part 3. I welcome any attempt to improve Haskell. The original Haskell was too list-centric, even defining String as a list. I hope Haskell will continue to get radically better.
I feel like the non-fiction books I have bought for Kindle in the past have been weird to read. The formatting has taken a weird turn when being converted. How did this book survive the conversion?
Yes, that solution is amazing! I knew there would be an elegant and simple approach.
Very well, although I read it on a combination of computer and iPad. Sometimes the code can be line-broken in the iPad in a bit of a weird way.
I wouldn't bother with a phantom type. I'd use your left fold behaviour for the `Foldable` instance, and use the [`Dual`](http://www.haskell.org/ghc/docs/7.8.2/html/libraries/base-4.7.0.0/Data-Monoid.html#t:Dual) monoid when I wanted the right fold behaviour. Edit: Perhaps there should be a similar way of wrapping `Foldable` to get the reverse behaviour.
Hmm, this actually looks pretty good. I'm not sure if it will produce C compatability or not, but looks promising. The only part I'm not sure of is why the galignment of a product is the alignment of the LHS of the product?
Nice this could come in handy in an upcoming project!
The `Fold` type from this post already exists in [my foldl library](http://hackage.haskell.org/package/foldl). What's more, you can implement an `Applicative` interface for the type so that you can build derived folds by piecing together smaller folds: average :: Fold Double Double average = liftA2 (/) sum genericLength That will compute the average in constant space without space leaks.
There is a link to your foldl library in the post. There are a few differences with Clojure transducers: 1. You provide folds with arbitrary monadic effects, Clojure transducers only support Clojure's built-in effects. 2. Your library doesn't support flatmap (could be easily added I think) 3. Clojure separates associative folds from left folds. This allows transducers to support parallelism if your operations are associative. 4. Clojure separates out fold transformers from folds (consumers). This allows you to use those transformers on push collections (event streams) as well as pull collections. 5. Clojure supports early stopping, which allows it to support functions such as takeWhile.
Oops! I missed the link. Sorry about that. Yeah, I've been thinking of providing a `FoldMap` type for folds that use an internal monoid and then providing a conversion function to explicitly downgrade it to a `Fold`. You can actually use `Fold`s on streams, too. See `Pipes.Extras.scan` for an example of this. Even though I call these "Fold"s, I actually frequently use them as scans.
Link?
Oh, sure, that’s how I model it in the type system, but it does pay to have a nice syntax for it.
It doesn't work with polymorphic types though, just like OP's original version.
I am talking at this event about my relational query EDSL "Opaleye" (due for release December 1st). Simon Marlow, Neill Mitchell, and José Pedro Magalhães will also be talking about Haskelly matters. Arnaud Bailly will be talking about Haskell QuickCheck as well as Clojure.
This seems to get reinvented at least once a week (with subtle differences). Can we get this functionality merged into cabal already? :-/
I've forked a similar script and tuned a bit -https://gist.github.com/svalaskevicius/684325189b96b1496a0d
Paul always seems be a bit too glib when talking about the propositions as types principle, not being precise enough about what logic corresponds to a language and what properties are supposedly being provided in a Curry-Howard kind of way. I think he's mistaken as you suggest, and also don't see how Curry-Howard alone can tell you anything about your values without at least having indexed types/GADTs.
I always thought `ghci` used `haskeline`, which provides functionality equivalent to `readline`
I just assumed it was readline because all my readline stuff works. But maybe not.
Yes, it uses `haskeline`. I was just looking how to enable vi mode and managed it by editing the `~/.haskeline` file, which supports the options listed [here](http://trac.haskell.org/haskeline/wiki/UserPrefs).
&gt; you can even force readline to accept vim keybingings! I like how you worded that. Like it would be unwilling.
Try Bird's [Introduction to Functional Programming using Haskell](http://www.cs.ox.ac.uk/publications/books/functional/), which this seems to be an update of! One of the best books on FP ever written IMO. And his [Pearls of Functional Algorithm Design](http://www.amazon.com/Pearls-Functional-Algorithm-Design-Richard/dp/0521513383). And (it's a bit pricey though!) his [Algebra of Programming](http://www.amazon.com/Algebra-Programming-Prentice-Hall-International-Computer/dp/013507245X).
My version of this script is here: http://neilmitchell.blogspot.co.uk/2014/10/fixing-haddock-docs-on-hackage.html
Too bad I left the UK the weekend after the Haskell Exchange :) have fun!
This was actually the first thing I tried before I found the subject script, but it was throwing errors at me.
If you still have it handy, what was the error? It certainly works for me, but I've only tried it on one environment, and things like curl/tar can be quite different in different places.
As has been pointed out, ghci uses haskeline. I just found out the other day that it was possible to configure it to use vim keybindings, which made me very happy because it was nearly the only program I use that doesn't support that to some extent, but I was subsequently very disappointed when I tried it out. The delay between hitting ESC and entering normal mode is something like a whole second. This would not be a problem if it worked like zsh -- hitting ESC using vim bindings for zsh also has a similarly long delay, but you do not have to wait for it to pass to start using normal mode commands. In Haskeline, hitting ESC followed by a normal mode command immediately afterwards(before the delay is over) results in getting put back into insert mode. A second may not sound like much, but it really is, and it makes the vim bindings nearly unusable. Before everyone starts yelling "submit a pull request" -- that was my intention. I looked through the code for probably an hour and I simply could not determine where it was possible to tweak the delay, if the fix is even that trivial. EDIT: I'm full of shit and I didn't look into the problem properly. Haskeline had *nothing* to do with this delay. The problem was tmux. See below for the solution. I will leave this here as a reminder not to jump to conclusions.
It works fine for polymorphic types. The problem is with ad-hoc polymorphic types. It might be solved if we get closed typeclasses someday, though.
I have just the function for you. `mapMaybe readMaybe :: [String] -&gt; [Int]` It has the added benefit over `read` that failure is handled purely. import Text.Read (readMaybe) import Data.maybe (mapMaybe) main :: IO () main = print $ (mapMaybe readMaybe :: [String] -&gt; [Int]) ["1","a","2" :: String] -- result: [1,2] :: [Int]
We used to use readline back in the day, then switched to haskeline to enable you to build GHC without any properly GPL licensed code linked in. We still have GMP, but we can link against `integer-simple` to avoid it.
Is there any tutorial for linking against `integer-simple`?
I've created an issue on your tracker: https://github.com/ndmitchell/neil/issues/8
A FoldMap type would be great. One thing you'll run into is that you want to define premap etc for both Fold and FoldMap, and ideally you want one definition that works for both. That's the kind of thing I'm trying to solve here: one definition of map that works on all things that can be mapped (functors, text, sets, event streams, infinite binary trees, parallel arrays, etc), and at the same time support automatic fusion. `scan` is nice but it's more limited than transducers (Refold) because it can't can't filter or flatmap event streams.
Sorry, its something I'm hacking on at work and hence is closed source. 
I actually started work on an external ghci just recently and then got distracted by some cabal hacking.
Yeah I should have mentioned the main issue with the GMP was the static linking concern, not that it was somehow full GPL, just that it acts like the full GPL when statically linked. 
I've got one too! https://gist.github.com/hvr/4b7b3074826ccc327a7d
Simply adding a premap specific to event streams doesn't give you fusion though, and it also doesn't let you use the same `map` function in all the different contexts that `map` can be used in. That's what I'm trying to solve: operations that work in any context where they apply, and automatic fusion. Another way of saying that is that the law `map f . map g = map (f . g)` is a general law for `map` regardless of which context it's used it (to map over a list, an infinite binary tree, a Text, a Set, etc.). Therefore we can use it to fuse `map` in any context. That law becomes the *definition* of map. Of course for map it's trivial; just do this: data Mapper a b = Mapper (a -&gt; b) Mapper f . Mapper g = Mapper (f . g) map :: (a -&gt; b) -&gt; Mapper a b map f = Mapper f It gets more interesting when you get to other operations like filter, flatmap, take, zip, unzip, scanl, foldl, scan, fold (with associative operation), mapM, filterM, etc.
The solution I found is similar, but uses `FlexibleContexts` instead of `UndecidableInstances` and `IncoherentInstances`: {-# LANGUAGE FlexibleContexts, FlexibleInstances, MultiParamTypeClasses #-} -- ($.$) :: (a -&gt; b) -- -&gt; (x0 -&gt; x1 -&gt; ... -&gt; xn -&gt; a) -- -&gt; (x0 -&gt; x1 -&gt; ... -&gt; xn -&gt; b) class FunctionMap a where ($.$) :: a instance FunctionMap ((a -&gt; b) -&gt; a -&gt; b) where ($.$) = id instance FunctionMap ((a -&gt; b) -&gt; xa -&gt; xb ) =&gt; FunctionMap ((a -&gt; b) -&gt; (x -&gt; xa) -&gt; (x -&gt; xb)) where ($.$) = (.) . ($.$) It's not very type-inference-friendly, though, as it requires both arguments to have a non-polymorphic type.
Why use CPP for JSON?
For finite domain and codomain you could write all these isomorphisms. Beyond that, you might be interested in Escardo's searchable sets. [Here's](http://hackage.haskell.org/package/infinite-search-0.12/docs/Data-Searchable.html) one place to start.
Function application ((-&gt;) r) is a functor. So composition for a function with many arguments can be seen as mapping over a nested functor. deepFmap :: (a -&gt; b) -&gt; ((-&gt;) x1 ((-&gt;) x2 ... ((-&gt;) xn a)...) -&gt; ((-&gt;) x1 ((-&gt;) x2 ... ((-&gt;) xn b)...) Oleg wrote generalized functor application http://okmij.org/ftp/Haskell/deepest-functor.lhs in 2006. Though for some reason it does not work for function functor. GHC has changed a lot since then. edit: It actually works well but requires more type annotations than regular fmap. (f_map show max) (2 :: Int) 3 == "3" (f_map (+1) (*)) (2 :: Int) 3 == 7 (f_map (+1) (*2)) (3 :: Int) == 7
Hi, thanks for the report of your issue. I am the creator of haskeline, and getting the vim bindings working well is pretty important to me since I use them most of the time myself. I haven't noticed any delay between pressing ESC and entering normal mode during my own use of ghci/haskeline; but this could very well be some sort of system-dependent problem. Can you please file a bug with details of your OS, terminal program, version of ghc, etc. at http://trac.haskell.org/haskeline? Also please check whether the behavior changes when you're running on a local machine versus, e.g., running over an SSH connection.
I'd be glad if you could post a link or point me in some direction. Thanks !
Very fair. =)
Does `MenuCompletion` work for anyone? On my system it just cycles through the available completions but there's no menu shown at all. Or maybe this is the intended behaviour? Am I missing something?
The GHC has a poorly documented feature to register a callback when an exception in not caught. This has the advantage of working with existing libraries. See my original gist: https://gist.github.com/NathanHowell/97540a0ae42860ef953b And Steve's writeup: http://www.steveseverance.com/2012/09/01/ahgth-the-unhandled-exception-handler/
I wrote a [solution](http://www.reddit.com/r/haskell/comments/1xr35l/variablearity_composition/) a while ago, I think with the changes I made in the edit it works reasonably well without requiring type annotations but it's been a while. edit: stuff :: Ord b =&gt; (a -&gt; b1) -&gt; [a] -&gt; (b1 -&gt; Bool) -&gt; (b1 -&gt; b) -&gt; b -&gt; b stuff = flip (foldr max) @. flip map @. flip filter @. map is a contrived example that seems to work
Its worth mentioning that the doc builders are back up and running and some really really amazing folks have graciously volunteered to make sure they continue to work! :)
factorize it: myFunction :: MyState () myFunction = liftIO $ do putStrLn "I will start changing the state!" -- (make changes to state, etc.) putStrLn "Ok, the state is changed now!" return ()
True, False, and `_|_` ?
Theoretically. So, for a program written in C, that basically means bundling the object files together with your distribution, so that someone could re-link with an API-compatible replacement for the functions provided by readline. For a program compiled statically by GHC, though, I'm not so sure how easy or practical it would be to re-link with a readline replacement. So then it comes down to, just how "possible" do you need to make it to re-link? You need to think about that both from the moral point of view, and from the point of view of how much legal protection you need.
It seems like a beefed-up "Concurrently" from the async package, with a more hierarchical flavor.
Thanks, and fixed. The issue was you put your name key (name:) and value (stm-containers) on different lines. I made my "parser" more general, and it now works.
Awesome, I was looking something like that the other day for Bake. I suggest you clarify the docs - a casual look makes it look a lot like the existing forkIO stuff (which is great), but somewhat hides the "clever" that this adds over the forkIO. In particular, you should clarify the invariants: if you only ever call your functions exceptions will never get lost? If I call forkFinally will exceptions still propagate to their calling parent? How is forkFinally different from a composition of fork and finally?
Perhaps [thrists](http://hackage.haskell.org/package/thrist) or some variation on them would be helpful here.
Firstly I will note that you don't need these bang patterns on the function arguments, because you already have strictness annotations on the definition of `ByteGrid`: `(ByteGrid !w !h !vec)`
Secondly I will note that `write` is just `modify` with a constant function.
Your heap being eaten up is probably a space leak, which is due to the code that *uses* your data structure. My suggestion: Use a simpler data structure first, like `Data.Array` or even a nested list `[[a]]`. Fix the space leak that is likely to persist and switch to your fancy data structure only when the simpler one is too slow.
I have real-time constraints to meet. I can't give up that much performance. [Some test code](https://gist.github.com/RTS2013/525f97954f8ce2dbee8a)
Not particularly, just an observation.
I get the same behaviour on my system. Also it seems to be a `haskeline` specific feature. I was trying to find something equivalent for `readline` and failed.
That won't work, because you cannot "make changes to state" within `IO`. you can of course write a function `modifyIO` :: (s -&gt; m s) -&gt; StateT s m a` (using `get`, `put` and `liftIO`) and write myFunction = modifyIO $ \s -&gt; putStrLN "changing the state...` &gt;&gt; return s
This is not about Haskell being "too list-centric". Lists are the fundamental way to express iteration denotationally, so saying that Haskell is "too list-centric" is like saying that C is "too while-centric". Instead, these posts are about designing a generic interface to containers. Besides representing iteration, lists can also be used as one kind of container, so they do get a mention here. But that doesn't change the natural central role played by lists in any language that strives for clarity of denotational semantics.
apfelmus is proposing to * switch to simpler data structure, * find the space leak, * and switch back to fancy data structure (**edit**:) in the hope your space leak is not inherent in your data structure. 
This is intentional. There are major flaws with categorized namespaces, which lead to ambiguity and complication. The convention of merely accomodating the root namespace to the package name is more consistent. See the following: http://www.reddit.com/r/haskell/comments/1qrilm/packages_and_namespaces_naming_convention/ http://fvisser.nl/post/2013/may/28/towards-a-better-haskell-package.html
The problem is that `seq`ing a `Vector` does not force its contents. We say that a `Vector` is "value lazy". You can see a demonstration of what is going on at the bottom. It indicates that when you write `!vec = V.modify ...`, the thunks built up by that modify operation are not actually being forced. This is one of the many reasons I discourage writing `!` patterns everywhere in a desperate attempt to fix space leaks. Instead you should can write `!vec = DeepSeq.force (V.modify ...`. This will do a lot more work than necessary because it forces everything when only one cell was modified. If you want to do as little work as possible I suggest !vec = V.modify (\vx -&gt; M.unsafeWrite vx xd8 $! V.modify (\vy -&gt; M.unsafeWrite vy yd8 $! runST $ do ... B.writeByteArray mg (ym8 * 8 + xm8) $! (f $ readUnsafe bg (x,y)) ... I susect this will work. (You need to do the same for `write`, but as I said, `write` should probably be written in terms of `modify`.) Please report back. Demonstration of strictness: import qualified Data.Vector as V import qualified Data.Vector.Generic.Mutable as M import qualified Control.DeepSeq as D x :: V.Vector Int x = V.singleton 1 y :: V.Vector Int y = V.modify (\v -&gt; M.unsafeWrite v 0 undefined) x z :: V.Vector Int z = D.force (V.modify (\v -&gt; M.unsafeWrite v 0 undefined) x) y' :: () y' = y `seq` () z' :: () z' = z `seq` () -- *Main&gt; y' -- () -- *Main&gt; z' -- *** Exception: Prelude.undefined 
That test code produces a space leak. I've ran heap/retainer profiling. I cannot figure out why garbage isn't being cleaned up. I'm looking for help finding the space leak.
Thank you so much! That small change worked perfectly. You were right, it was building a chain of thunks.
You're welcome! Did you try both fixes? The one with `$!` should lead to better performance than the one with `DeepSeq.force`.
Sorry, I used $! and that worked fine.
Excellent!
I'm sorry I know it's off-topic, but I just couldn't resist refactoring your code ;) I would extract a `readUnsafe` and `isDefinedOn` function as followed: readUnsafe :: ByteGrid -&gt; (Int,Int) -&gt; Word8 readUnsafe (ByteGrid !w !h !vec) (!x,!y) = vec `V.unsafeIndex` xd8 `V.unsafeIndex` yd8 `B.indexByteArray` (ym8 * 8 + xm8) where (xd8, xm8) = x `divMod` 8 (yd8, ym8) = y `divMod` 8 isDefinedOn :: ByteGrid -&gt; (Int,Int) -&gt; Bool (ByteGrid !w !h !vec) `isDefinedOn` (!x,!y) = x &lt; w &amp;&amp; y &lt; h &amp;&amp; x &gt;= 0 &amp;&amp; y &gt;= 0 You could then rewrite the `readDefault` and `readMaybe` as followed: readOrDefault :: ByteGrid -&gt; Word8 -&gt; (Int,Int) -&gt; Word8 readOrDefault bg def coords = if bg `isDefinedOn` xy then readUnsafe bg xy else def Also your `modify` function is silent, just skipping the modify if the coordinates are out of range. This might lead to awkward debugging situations, if you accidentally pass the wrong coordinates. Ok, sorry again for being offtopic, have a nice day! :)
&gt; a casual look makes it look a lot like the existing forkIO stuff (which is great), but somewhat hides the "clever" that this adds over the forkIO Well, the documentation is made with presumption that the user is aware of the information presented on the index page. However I agree, the documentation could probably use some tuning. &gt; if you only ever call your functions exceptions will never get lost? Correct. &gt; If I call forkFinally will exceptions still propagate to their calling parent? Correct. &gt; How is forkFinally different from a composition of fork and finally? 1. It's guaranteed to be executed. In case of `fork $ finally ...` nothing protects your thread from dying before reaching the `finally` part. 2. You're abstracted from dealing with the thread management. Particularly, the `ThreadKilled` exception. 3. The order of execution of the "finally" action is different. Regardless of exceptions it is guaranteed to go like this: fork1 ------------------------------------- finally1 | + fork2 ------------------------- finally2 | | | + fork3 ------------- finally3 | + fork4 ------------------------- finally4 IOW, "finally3" is guaranteed to be executed before "finally2", since it's from a deeper level. "finally2" and "finally4" don't have any enforced order of execution amongst each other, since they are on the same level, but they are both guaranteed to be executed before "finally1", since they are slaves of its thread. 
Thanks for the reply. I might just have to steal your refactoring. Also I agree that the silent behavior could lead to future hardship. I'm making it complain now!
You're welcome!
Mapping over the deepest functor is nice but a bit unpractical if the return type is e.g. `String`, then you're suddenly required to map over the `Char`s.
I would first switch to an unboxed vector. If you need mutable then use the unboxed mutable vector. Then unpack your strict fields in your `ByteGrid` type. Then go see how that performs. Normal Haskell starts off boxed and you have to explicitly say to unbox. Boxed values have to be pointers which means heap and very unlikely that all will be put in the cache at all. I think the solution of deepseqing just brings to the fore that your data structures are boxed and have laziness built-in. 
Sure, but all of our other high profile packages like {prelude, mtl, transformers, lens, containers, all web frameworks, all data crunching frameworks ie repa accelerate ...} currently follow categorized namespaces so being inconsistent is actually just unhelpful. Kind of like the guys who contribute GNU tab schemes to FOSS java projects. Sure, it might make sense under a certain logic. But nobody else will do it, so doing it is just destructive and annoying.
He's using the very good convention of top-level module = package name
Real simple as long as you want to build GHC from source :) Not possible otherwise
&gt; So, for a program written in C, that basically means bundling the object files together with your distribution, so that someone could re-link with an API-compatible replacement for the functions provided by readline. That's the easiest option. But since the motivation of the LGPL is to be able to give you the freedom to update/fix/modify the LGPL part independent of the vendor providing the combined work, you can also provide a way to dynamically link against a LGPL library, but fallback to the embedded statically linked version if the DSO isn't found. But this requires control over the dynamic linker and possibly use of weak symbols, and thus is less portable. However, as this involves the formally-not-so-sound system aka "copyright law" where there's nothing even remotely close to anything like a mathematical truth, there be dragons...
&gt; If your thread dies, isn't that always from an exception of some kind? And thus doesn't finally always fire? (I imagine the answer is no, but it would be lovely to know exactly why the answer is no.) It all concerns the asynchronous exceptions and the `mask` function. The `action` block in the `fork action` expression gets executed in an unmasked mode. This means that it is not protected from asynchronous exceptions. The `finally` function that you're suggesting does bring its finalizer into a masked mode (meaning protected from asynchronous exceptions), but even despite it being the first action to be executed in the forked thread, there still remains a tiny moment before the thread enters the masked state, when it accepts asynchronous exceptions. Consider it to be somewhere here: fork $ finally action finalizer ^ the moment So if for whatever reason an asynchronous exception gets thrown to this forked thread at that moment, it will die without even reaching the `finally action finalizer` part. While it is a very unlikely scenario, it is still a possibility. The `forkFinally` function on the other hand provides you with an absolute guarantee that it'll execute the finalizer, using a masking strategy like the following: forkFinally finalizer action = mask $ \unmask -&gt; forkIO $ do r &lt;- try $ unmask action finalizer ... 
There are lots of places that are starting to use GHC as a basis for other projects. e.g. They pick up from the core on down, etc. But the license _is_ end-user affecting. You get a runtime linked into your executable that contains GMP, etc if you do a static build, which until very recently was the default everywhere.
Considering this is the first time I've heard of this feature and I've never used it, I don't think I'm the man for the job.
&gt; all of our other high profile packages like {prelude, mtl, transformers, lens, containers, all web frameworks, all data crunching frameworks ie repa accelerate ...} currently follow categorized namespaces so being inconsistent is actually just unhelpful. This is simply not true. Here are a few examples: * http://hackage.haskell.org/package/pipes * http://hackage.haskell.org/package/snap * http://hackage.haskell.org/package/yesod * http://hackage.haskell.org/package/fay * http://hackage.haskell.org/package/haste-compiler * http://hackage.haskell.org/package/ghcjs-dom As you can see, it's quite a tendency already and I've already linked to some strong arguments against the older convention. 
I recommend using the official cabal-install binaries http://www.haskell.org/cabal/download.html
Yes exactly. This is rather an argument that Haskell should be more list centric than less. He's right that strings should't be lists, but to operate on strings it's fine to go via lists.
&gt; delay between pressing ESC and entering normal mode This symptom is certainly not haskeline specific. zsh (which has it's own readline variant, IIRC) under Cygwin GNU screen in a Cygwin minTTY on MS Windows 7 suffers the same fate. Esc + pause + / does a search; Esc + / prompts for autocomplete. In truth, I think it a nigh-unavoidable condition if you have a program that needs to treat Alt+&lt;key&gt; different from &lt;Esc&gt;,&lt;key&gt; under certain terminal configurations. Some terminals can pass Alt+&lt;key&gt; as a single keycode, but others pass Alt+&lt;key&gt; as an escape code followed by the keycode for &lt;key&gt;. Most usually the escape code is the same as the keycode for &lt;Esc&gt;. The ambiguity is resolved by requiring some minimal delay between &lt;Esc&gt; and the next key for them to be recognized as separate keystrokes. A related symptom is arrow keys (in particular) *sometimes* showing up as (e.g.) "\^[[A" based on varying network conditions, repeat rates, and your current level of frustration.
 slaves :: Multimap.Multimap ThreadId ThreadId slaves = unsafePerformIO $ Multimap.newIO Shouldn't this function have a `NOINLINE` pragma, or are the rules different from using something like a top level `unsafePerformIO newTVarIO` and such which I have read that you need to mark `NOINLINE`? (Also, not that it matters but that `$` seems redundant.)
Thanks. I'll fix that.
Has anybody tried it on 10.10 yet?
No problem. Your package looks quite useful, thanks for making it available!
sorry, your argument is flawed. all of the packages above use a top level identifier that indicates that they belong to a specific group of packages. they are not named e.g. `GHCJSDom` but `GHCJS.Dom`. for each and every one there are other packages that use the same top-level namespace. btw: there are some serious disadvantages as well. read e.g. edward kmett's posts in the thread you linked.
&gt; they belong to a specific group of packages. they are not named e.g. GHCJSDom but GHCJS.Dom. They still are examples of packages that do not follow the categorisation convention and most of them have root namespaces accommodating the package title. Whatever the reasons the authors had - groups or what, this still renders the argument about all high profile packages following the old convention invalid. &gt; there are some serious disadvantages as well. read e.g. edward kmett's posts in the thread you linked. This is extremely debatable. Edward is probably one of the very few people who have to be concerned with the arguments he brought up simply because he has to maintain packages which are depended on by most of the Haskell world. And even then I don't find those arguments strong, which is already explained why in the original thread. (It was actually me arguing with him from my older account).
Yeah works fine for me on 10.10.
This is both a great intro to quasi-quoting in Haskell as well as a really cool application. Thanks!
apfelmus was suggesting a way to determine whether the problem is a problem in the data structure itself or your code that uses it, which I see you've already determined. You might enjoy his [Space Invariants](http://apfelmus.nfshost.com/blog/2013/08/21-space-invariants.html) post as it provides a nice way to reason about these problems for future reference.
This thread is the perfect example of why the Haskell community is awesome.
&gt; If your language requires you to do type system gymnastics to make simple pointfree code work, it’s probably better to just avoid pointfree style beyond the basics. It does not. It requires you to use them to make things that are *not* eta reductions *look like* eta reductions. This is not "basic pointfree style". I think it's appropriate that such an unusual behavior (for Haskell) require an unusual implementation (in Haskell).
I also use this for all my OSX dev and it's been working great (I have multiple versions installed)
The big difference is when the package is an extension to an existing package, then the original 'namespace' it's extending should be used, i.e. Yesod.WebSockets instead of YesodWebSockets and Control.Exception.PartialHandler instead of PartialHandler (or PartialExceptionHandler). Nobody complains about top level single-word library namespaces that make sense, like Yesod or Pipes.
This is a good chunk of OTP style supervision trees. I'm really glad to see it arrive in Haskell.
Isn't this exactly what `withAsync` does?
&gt; The big difference is when the package is an extension to an existing package, then the original 'namespace' it's extending should be used Now you're just making rules up. And, more importantly, it has nothing to do with the convention that you're trying to make me use. &gt; Control.Exception.PartialHandler instead of PartialHandler And what exactly made you so sure that it's the proper category? What if in the next release I'll update the library to produce general handlers, and not just exception-oriented ones? What will I have to do then? Obviously keeping it in that namespace will become an absurd. And it's just a single example of why this convention is ambiguous so much. &gt; Yesod.WebSockets instead of YesodWebSockets I'm actually okay with titles like "Yesod.WebSockets", as long as the package title is according, i.e. "yesod-web-sockets". And it actually appears to be exactly the convention that is followed by Yesod and Pipes at least. &gt; Nobody complains about top level single-word library namespaces that make sense, like Yesod or Pipes. You do realise that this is a highly subjective statement, right? From an argument about conventions we've now arrived at whatever makes sense to you personally. Well, what makes sense to me are simple and strict rules, a gut feeling and ambiguity are not. Anyway, I've had too many arguments on this subject already. If you want to find out my point of view, just follow the links I've previously provided. But I assure you, you cannot change my mind about this. 
I'm not overly familiar with C struct alignment rules, so what I coded up was an 'educated guess'. It seemed logical to me a struct-of-values (~ product) should be aligned on the alignment boundary of the 'first' value it contains... Anyway, I'm fairly positive it should be possible to generate correct Storable instances, taking alignment etc into account, generically.
This comment inspired me to add the NOINLINE check to HLint: http://neilmitchell.blogspot.co.uk/2014/10/hlint-now-spots-bad-unsafeperformio.html
Doesn't Amazon have a [i forgot the word] link system? You know, something where if I follow the link to this book and buy it (or anything else as Amazon), the haskell.org or another relevant not-for-profit organization gets a small cut of the sale? If so, anyone got a good link link that? I believe I will be buying this book; it's got great reviews and currently my roommate owns for Haskell books than I do. That will not stand.
&gt; Refactoring is really cheap in Haskell. Do you men any tools or that typechecker will point down all places to fix? I found the latter tedious…
[Made a start](https://github.com/chrisdone/ghci-ng) this evening.
There's an issue for this on the homebrew repo https://github.com/Homebrew/homebrew/issues/33314
The question is whether writing `write` in terms of `modify` would necessarily have a performance penalty.
The latter
&gt; This is not "basic pointfree style". Composing two functions isn’t basic? I kid, but this is one of the hazards of using currying to implement partial application. And if this isn’t eta reduction, it’s certainly some analogue of it. But I agree, it is appropriate; and Haskell is certainly more compositional in nature than your average imperative language, so I suppose I shouldn’t grouse too much. :) 
And specifically, the latest package version's documentation?
I believe GHC 7.8 [unboxes](https://www.haskell.org/ghc/docs/7.8.1/html/users_guide/release-7-8-1.html) strict machine size types by default. 
I think GHC's gc would collect zero-referenced threads, right?
I might be on my own here, but I tend to find it easier to work with map / filter explicitly rather than using list comprehensions. Even if you are required to use list comprehensions as part of the assignment, perhaps it might be worth using map and filter to get it working and then to convert it back later on, just to get a different perspective on the problem.
That did it. Thanks
How hard would it be to make this work without the global? I expect you'd need some state associated with each thread.
Cloudflare? So I need to go through a captcha every time I want to go on Hackage now?
Thanks for clearing that up, Austin. But I *will* pester you on IRC if I see a captcha in the future. ;-)
Agitates' test code is a strict fold and thus not the problem, so my guess didn't pan out. As /u/tomejaguar showed, the problem was that `ByteGrid` doesn't have the right space invariant -- the inner `Vector` may be an unevaluated expression.
Thanks for the nudge! Would have forgotten about it otherwise. It was a good talk; it gave me a good idea of the options and applications. Have a nice day!
Well, actually the first version was implemented [exactly that way](https://github.com/nikita-volkov/slave-thread/blob/1317b82c0ab217e51cdf2627929c4c4aa1c372d2/library/ThreadedIO.hs). However as you can see it requires either a new monad (or a monad transformer) to keep the state in. Otherwise we'll have to make the state explicit and burden the user with carrying it, thus adding boilerplate and extending the possibility of bugs. Then I thought: this library deals with a problem that pertains to the scope of the whole program, not just some part of it. So it would be nice to just somehow extend the environment that the `IO` monad carried with the state of this library. Consider a global variable a way of encoding this. Why exactly do you want to get rid of it?
No. Why would it? A thread is not some data lying in the memory doing nothing. It can pretty much be intentional to have it unreferenced. Here's an example, which according to your statement, should be garbage collected: main = do forkIO $ forever $ putStrLn "I'm alive!" threadDelay $ 3 * 10^6
It's T+10 hrs. and Hackage seems to be working great. Thanks for the great work!
Unfortunately we had some glitches that prevented the move yesterday, although now it's mostly a matter of a switch being flipped (the remaining work was done except the DNS change); I'm going to post an update shortly, and Duncan and I will return to fix this soon. In the mean time, the server should remain very stable until the actual move, and we have a new isolated doc builder chewing through things, too...
This seems very useful. However, I cannot install it on GHC 7.4.2. For example `base-prelude` fails, which is both a direct (though completely unused?) and indirect dependency, and there are other issues too? `focus` fails, too. Would it be too much for ask for some backward compatibility? Not everybody lives on the bleeding edge...
I have ported the parser to Parsec. You can find it in the master of my "fork" (which I do not intend, nor hope to result in a fork) on Github; the Attoparsec version is in its own branch now. Currently I'm stuck a bit as some parsers that don't combine well together, running the tests with cabal build; ./dist/build/test-toml/test-toml --hide-successes should give a good idea of the things that are currently not working. With cabal repl you can run little test, like `parse tomlDoc "" "[a]"`, since I've setup up `.ghci` to load all the necessities. Kindly, Cies.
GHC 7.6 was released in September of 2012. I don't think that a term "bleeding edge" is applicable in that context. All the packages you mentioned are tested to support it. I'm sorry, but I just don't have time to invest into a support of something that is more than two years old.
+1 because your comment kept me reading.
&gt; What if in the next release I'll update the library to produce general handlers, and not just exception-oriented ones? Then you can provide an additional module `Control.PartialHandler` providing the general interface. `Control.Exception.PartialHandler` would reexport it and provide additional functions like `totalizeRethrowing`, which do not make sense for the general interface. My personal opinion is that new top-level identifiers should be used sparingly. Libraries like `Pipes` or `Yesod` might be “big enough” to get their own top-level module, but small libraries with a few utility functions should use a more descriptive name using the existing module structure.
Many environments (schools, corporations, and certain hardware-tied operating system versions...) are rather conservative with upgrading, and 7.6 is a single major version behind. But it is good to hear that you support at least one version back. (also, these days unfortunately the platform release date is much more important to some people than GHC's release date, and the platform was updated to 7.8 only in this year I believe?)
&gt; My personal opinion This is the bottom line. I don't agree with it.
&gt; Many environments (schools, corporations, and certain hardware-tied operating system versions...) are rather conservative with upgrading I understand, but as a mortal creature I have a limited amount of time, so it's about compromises. I believe I'll do a better service to the community by using my time to release another useful package for a modern environment, than to keep hacking in my older ones with CPP-stuff.
&gt; work with map / filter explicitly rather than using list comprehensions. Do you also prefer explicit bind / `when` over `do` notation? 'Cause the desugaring is almost identical. I think Haskell 1.4 even had monad comprehensions. 
Yeah, list comprehensions are usually less clear than map/filter/reduce.
This is slightly different, because Edward also released [folds](http://hackage.haskell.org/package/folds), which is a closer parallel to the `foldl` library. The main difference between `reducers` and `foldl`/`folds` is that `reducers` relies on type class instance to implicitly combine folds whereas `foldl` / `folds` explicitly use `Applicative` operations. Moreover, because a `Fold` is a first class value instead of a type class instance you can also manipulate it using `Fold` combinators like `fmap` or `pretraverse`, which you can't do with type class instances. However, neither of us were the first to come up with the idea. The original credit goes to [Conal Elliot](http://conal.net/blog/posts/more-beautiful-fold-zipping).
Cheers for the awesome tool once again
I think we should not encourage any use of `unsafePerformIO`, especially not for global variables. The suggested solution should be to remove the `unsafePerformIO` and initialize the value explicitly within `main`.
You can also write [c | c &lt;- pairs, let x = fst c, x /= a] which corresponds directly to your original formulation (but is syntactically correct) 
This is debatable. There are many situations where list comprehensions are clearer, and also many people who find reading list comprehensions easier (and I think the same goes for explicit recursion).
I agree that's generally a laudable goal. I also think it's out of scope for a tool like hlint. If we're willing to tell people that linting means making major changes to code structure, we should similarly have a C-lint, Java-lint, etc, that just look at your source code and say "You should have written that in Haskell." :)
While your arguments are reasonable, they all become either negligible or invalid as soon as you start considering versions of GHC prior to 7.6 as archaic, which I do. So with that context, having standard extensions, which are supported since GHC 7.6, enabled by default has absolutely no affect on dependant packages. The supported versions of GHC are transitively declared in the Cabal files of my packages via the supported versions of "base", so there's no misinformation there either. The only thing that's left are the questionably redundant dependencies to `loch-th` and `placeholders`. I just prefer to develop having these libraries at hand, and since I maintain my packages I don't want to have another obligation of including/excluding these dependencies with every release cycle. They are both very small and they don't bring a whole universe of dependencies like "lens", so the effect they have on the dependency graph is close to none and all the complaints about having these libraries can only be subjective. I however may remove these dependencies from the "focus" package soon, since I don't even expect to start using them there. But nonetheless I expect it to make absolutely no practical difference to the users of my packages.
Factoring out impure code is really rudimentary Haskell. That's like saying that `-Wall` shouldn't warn about incomplete pattern matches because it would be too radical of a change.
Without looking at it in detail: if HLint can do that, why does GHC not mark the function as NOINLINE automatically in such cases?
&gt; I'm sorry, but I just don't have time to invest into a support of something that is more than two years old. This community has a very interesting idea of how old "old" is. But that said, this library is itself very new, so only supporting new things is not a suprise at all.
That kind of thing normally only happens when you try running a binary linked to a newer osx platform on an old system. So yes its broken. :) I'm guessing that was compiled with a beta sdk or possibly xcode and didn't have the -mmacosx_version_min=10.N flag set to say 10.9. http://stackoverflow.com/questions/14268887/what-is-the-illegal-instruction-4-error-and-why-does-mmacosx-version-min-10 
It's sometimes out of your hands, for example if you're binding to a C library that has global state you might sometimes need global state of your own to manage the binding.
Could someone provide example code were GHC *really* inlines the `unsafePerformIO`?. Or is it safe to assume that it *certainly* will happen at a given optimization level (e.g. `-O2`)? Not that I use `unsafePerformIO` at all..
Sure. Actually, is there a public list of Tor exit nodes available, even a semi-complete one? I believe whitelisting these should be fine and remove CAPTCHAs for users, but it would be nice if I could start with a list of exit nodes and remove abusive ones ourselves until CloudFlare upgrades their Tor handling.
I'm saying that you could be more compatible with a very small effort. You choose not to do so, which is a valid viewpoint, but I don't think it's particularly helpful. I believe that if everybody would spend just a little effort on compatibility, the Haskell ecosystem would be rather more robust. Of course if you are decide that you are *actively un-supporting* anything older than 7.6, as opposed to simply supporting 7.6 and then passively hoping that it may or may not work on older versions, *then* it doesn't make sense to remove unnecessary stuff, since of course *then* you will have exactly zero users on those systems (because they have zero chance to actually build). But excluding potential users is your decision. 
This is the only reason I have ever felt like I had to use this trick, in fact.
It's not that it inlines the `unsafePerformIO` but that it inlines the whole top-level value (meaning each usage is on a different `IORef`). I actually haven't come across any cases where GHC *wouldn't* inline this, so the bug would be visible as soon as you try to use it more than once. However, by the time you get this far, it may not be obvious what's going on anymore, so it still seems nice for HLint to warn you about it.
This is also possible: [c | c@(x,_) &lt;- pairs, x /= a] or [c | c &lt;- pairs, fst c /= a] Personally I don't prefer comprehensions though. filter ((/= a) . fst) pairs It's even shorter.
Try to do that when you need once-only IO in a template.
It's not always under your control, though. `unsafePerformIO` is essentially an absolutely necessary evil for someone like me, who writes a lot of FFI-based code. The simple reality is that a global IORef is not only useful, it is sometimes *the only way to make things sane for API users*. If my C library has an init call, but is otherwise pure (many libraries do this), then this API is fundamentally broken: initLibrary :: IO () -- Must be called after `initLibrary`. doFrobThing :: ByteString -&gt; ByteString The `HOpenSSL` package is a good example if this, where everything in the library simply ceases to work unless you call the initialization function before any other pure functions. Unless you use `unsafePerformIO` to manage initialization in the background, that's about the best you can do. Unless you move the entire API for everything into `IO`, or do something else weird like thread some `token` parameter around that you can only get from `initLibrary`, but that just makes your API annoying to use. Keegan McAllister gave a good [presentation about this sort of stuff](http://ugcs.net/~keegan/talks/high-level-ffi/talk.pdf), including turning a very low-level C library into a beautiful high-level Haskell interface. Spoiler alert: `unsafePerformIO` was needed, as were plenty of other things. That's just the world we live in. Relatedly, Keegan also wrote [safe-globals](http://hackage.haskell.org/package/safe-globals), which is an even nicer API to ensure you can't mess these things up, such as forgetting a `NOINLINE` in the OP. We should encourage *safer* means of accomplishing these things, not just outright removing them. `safe-globals` is a good example of that. 
I think the more compelling argument was the one somebody else mentioned that you need to prevent multiple users of a library from all initializing the same variable. The example you gave, though, can be fixed by just using: data Ops = Ops { doFrobThing :: ByteString -&gt; ByteString , ... -- any other operations } initLibrary :: IO Ops
HLint uses a moderately complicated set of heuristics to guess at some functions which obviously look like it got missed. It has both false positives and false negatives. Of course, I think my heuristics are pretty good, and probably catch most real cases, but not good enough to make automatic decisions based upon. That said, if GHC just saw unsafePerformIO anywhere in a function and went "hey, I'll just default to NOINLINE, unless there is an INLINE pragma", that might be worthwhile.
In some cases HLint does throw its hands up and say "dude, bad idea" without offering a replacement. Here I can offer up a reasonable fix, so it seemed better to, but you can certainly declare your own rule that unsafePerformIO is verboten - I believe the Darcs team do that with lazy-IO operations. You can write: error = unsafePerformIO ==&gt; bad
Yes, my experience is that GHC almost hunts out unsafePerformIO, it's amazingly robust and breaking things without NOINLINE.
&gt; But excluding potential users is your decision. You got me right. I'm alright with losing a portion of users, but I believe a case like yours to be extremely rare. Unfortunately we have no statistics though. I as well don't understand people caring so much about support for different compilers, since I'm sure almost everybody uses GHC these days. This exclusion of potential users is also temporary, since eventually they'll have to upgrade. I'm just giving them another reason to.
&gt; Reasons would include -- the global ioref hack happens to work under GHC today (and for compatability probably will for quite awhile) but is an accident of how GHC happens to work (hence the NOINLINE requirement, etc) and not actually a guarenteed Haskell semantic under various implementations. Okay I understand. But I'd rather stay practical. From the user perspective it is very convenient to have this in the `IO` monad with no state to manage manually.
I consider this in the same bucket as passing a dummy token around, and I just don't think it's nice ergonomically. Consider HOpenSSL again: What if my API exports over 50 functions or more, or like a hundred? Having to access *every one of them* out of a record quickly becomes tedious since you always have to use the accessor function. You can of course hide the data type, but then why do I have to use accessors to get out what is literally just a set of functions anyway? The fact that `initLibrary` may close over some variables to provide state just isn't something users probably care about, I think, and it's a detail they shouldn't have to care about anyway. And some of them don't need an accessor, either, since they may actually be unaffected by the `init` call (yes, this happens). So the only way to have a consistent API as far as I can tell, is all-or-nothing. It just seems like a way of making your API annoying to use, while it provides no actual benefit to the users over the alternate approach (they'll never do anything but access functions via accessors anyway - the only difference is that I now must type more things on my keyboard). I don't consider the lack of `unsafePerformIO` to be a benefit in this case, because users are isolated from it, and in other cases you'll have to go back to it for other reasons like thread safety concerns anyway, so you're not saving yourself much more, not to mention the fact you'll often have to use `unsafePerformIO` for a pure API anyway. &gt; I think the more compelling argument was the one somebody else mentioned that you need to prevent multiple users of a library from all initializing the same variable. Yes, another I left out is simple thread safety, which plenty of C libraries fail at, yet we do all the time in Haskell. It's not an unusual need. A global lock in the background is pretty much necessary here in the same manner to make the API relatively consumable by the average Haskell user.
That's effectively the same as passing a token around, isn't it? 
+1 for Lennart's remark
Do notation is simpler than explicit bind 90% of the time. However with list comprehension, I get confused about what is a list and what isn't, and the syntax of it makes it difficult to break it up to figure out where you went wrong.
So the reason I proposed the record version of this is so that users can use the `RecordWildcards` extension to write: let Ops{..} = o ... where `o` is the record of operations, and then they can use the operations "raw". Using my original code, that would bring into a scope a function named `doFrobThing` of type `ByteString -&gt; ByteString`.
I had just about the same experience. Also it seems that you need to have all your packages lying around for "compiling" the plugin, so it's not really easy to distribute something ...
Yes, but the difference is that this supports unpacking using the `RecordWildcards` extension: let Ops{..} = o :: Ops That will bring every field into scope unqualified so that you don't need to apply them to the token to access the operations.
Totally naive question: have you taken a loot at [Yi](http://www.haskell.org/haskellwiki/Yi)? You can read the original paper [here](http://publications.lib.chalmers.se/records/fulltext/local_72549.pdf).
&gt; with list comprehension, I get confused about what is a list and what isn't Hrm. I'm a bit surprised you don't get the same confusion on `do`-notation then. `[ expr | var &lt;- gen, let v2 = e2, cond, ... ]` = `do { var &lt;- gen; let v2 = e2; guard cond; ...; return expr }`
Switched from darcs to git several years ago. Not looking back. The tooling ecosystem is so much richer for git. 
I have written a generalized [web-plugins library]( http://hackage.haskell.org/package/web-plugins) which is the basis of a [wordpress like CMS](http://www.clckwrks.com). I am also the current maintainer of the old plugins library -- though all I am willing to do on that library is apply patches other people send me. I did not write the old plugins library and I really have no idea how it works. It seems to me that we need a new plugins library built around the GHCI API. I have started one: https://github.com/Happstack/plugins-ng But it is mostly proof of concept -- definitely not usable yet. It should really be a suite of plugin libraries: - plugins-core - provides the low-level wrapper around GHC API for compiling and loading files - plugins-watcher - uses file watching to see if local files have changed and if so, it will recompile and reload them. This could be used for development modes but also when you use Haskell for your configuration file, etc - plugins-cabal - a mode that uses cabal sandbox and can download and install plugins from hackage with out restarting the parent process. This would be great for things like a CMS where you want to try out different themes and plugins with out having to restart the whole server. I'd be happy to work with someone on finishing up plugins-ng. It's important enough to me that I will get around to it 'someday' if no one else picks it up -- but it is pretty far down in the work queue at the moment. 
Doesn't seem naive to me. I have not. I will give it a look. Thank you. Edit: looks like internally it uses the [dyre](http://hackage.haskell.org/package/dyre) library.
Sorry I should have clarified - I was referring to Ed's Reducer typeclass (which could also be formulated as a data type), in response to the mention of building a `FoldMap` type. One argument in favor of formulating this concept as a typeclass is that there are no nettlesome questions when results are being combined that were created via different monoids (which would break the associativity assumption). The assurance that typeclasses provide of a global, unique instance is useful here. Also, I should add Conal to that list of people. :)
12 years old and still well- and actively- maintained and developed! Darcs is still the gold standard by which I judge all other version control systems and find them wanting, even when I can't use it everywhere I want for various social reasons, etc.
It is because I don't use the monadic property of lists very often. I guess it would be fairer to say I get confused about what is and isn't a list using lists monadically, which is the main way list comprehensions are used.
What's wrong with the third option? If you do break up surface into a type class you don't even need to wrap the Circle/Rectangle datatypes, since providing polymorphism is the only thing you're using it for. If you want to avoid type classes for this kind of polymorphism you can also do the value level option, defining a dictionary type with the methods that should be offered: data ShapeDict a = ShapeDict { surface :: a -&gt; Float , ... } Though note it's more common to use Doubles, see http://www.haskell.org/haskellwiki/Performance/Floating_point
EIther of your two last examples are probably the best ideas. As a general rule, less than total functions are a bad idea. If the function takes a value of a given type, it should work on *all* values of that type. If it takes an int, it should work on all ints- including negatives and zeros. If it takes a float, it should work on all floats- including NaNs and infinities and denormals. If it takes a list, it should work on all lists, including the empty list. If it takes a shape, it should work on both circles and rectangles. If it doesn't, then sooner or later you will call the function wih an "unsupported value", and things will blow up. There are exceptions to this rule, but very few of them (and even those are not uncontroversial). You never go wrong with a total function. A better solution is to use the type system to express the real types the function takes (which leads to the Circle and Rectangle types solution), or at least expresses that sometimes there is no result (the Maybe result solution). I think of the two, I prefer the second to last (seperate Circle and Rectangle types), but sometimes the last is more useful.
The main limitation of the type class is the inability to `fmap`. For example, if you want to do an `average` fold, you can get partway there in `reducers` using the `(Sum Double, Count)` instance, but the user must still explicitly divide it on the way out. However, I agree about the global uniqueness property, which is definitely useful.
I'm kind of disappointed we don't have the standard c/c++ benchmark on the list. It'd be nice to be able to get an idea how much actual overhead is involved. 
Why wouldn't this be the ideal solution?
Yes, this is a good solution. One thing about the empty data declarations `Circle` and `Rectangle` that irks me, though, is that they aren't really "grouped" together (I guess they are grouped by the module). On the plus side, they don't introduce values like the example data ShapeType = CircleType | RectangleType does. But since they aren't "grouped", you can't introduce a constraint on `t` in data Shape t where ... How useful that constraint is, though, IDK...
I see your distaste. If it's any consolation, there is at least the grouping in the sense that you cannot construct a value of type `Shape t` where `t` is not `Circle` or `Rectangle`.
&gt; If you do break up surface into a type class you don't even need to wrap the Circle/Rectangle datatypes, since providing polymorphism is the only thing you're using it for. This is a good point.
There's also strings, of course: data Shape t where Circle :: Float -&gt; Float -&gt; Float -&gt; Shape "Circle" Rectangle :: Float -&gt; Float -&gt; Float -&gt; Float -&gt; Shape "Rectangle" Such an extension is so new I don't know what to feel about it. This has the disadvantage over the empty data decls in that with data decls you can remove cases and code doesn't compile until you deal with code that used them. With strings this is less true. I think this would've been a fun solution: data Shape t where Circle :: Float -&gt; Float -&gt; Float -&gt; Shape 'Circle Rectangle :: Float -&gt; Float -&gt; Float -&gt; Float -&gt; Shape 'Rectangle But alas this is too recursive: Data constructor ‘Circle’ cannot be used here (it is defined and used in the same recursive group)
&gt; PHP implements its arbitrary size arithmetic in C. However, it doesn’t have a BigInt data type, so its arbitrary size numerical functions take and return strings!
You could write `ShapeType` like that and just not export the constructors. It looks like GHC doesn't like it when you try to just export a type defined with data kinds without exporting the data constructor, so you might need type synonyms module Shape (CircleType ,RectangleType ,Shape (..) ) where data ShapeType = CircleType' | RectangleType' type CircleType = CircleType' type RectangleType = RectangleType' data Shape (t :: ShapeType) where Circle :: Float -&gt; Float -&gt; Float -&gt; Shape CircleType Rectangle :: Float -&gt; Float -&gt; Float -&gt; Float -&gt; Shape RectangleType Of course, while that does allow you to constrain `t`, it doesn't avoid the duplication so the original solution is probably better. If they do add kinds without data to GHC, this would work out nicer.
Well, you *could* do (with TypeFamilies) class ShapeType st where type ShapeData st :: * -- probably a bunch of Floats :-) data Circle data Rectangle instance ShapeType Circle where type ShapeData Circle = (Float,Float, Float) -- a newtype is maybe better! instance ShapeType Rectangle where type ShapeData Rectangle = (Float,Float, Float,Float) data Shape st where MakeShape :: ShapeType st =&gt; ShapeData st -&gt; Shape st circumference :: Shape Circle -&gt; Float circumference (MakeShape (_,_,_,r)) = ... but then `area` becomes harder to write, because that'll need either a typeclass of it's own, or an additional member in `ShapeType`, with signature `area :: ShapeData st -&gt; Float`. By using a new typeclass you can add operations 'externally', and doing it this way let's you add instances as well. By then you're firmly out of LYAH territory, but hey no more expression problem :-)
&gt; By using a new typeclass you can add operations 'externally', and doing it this way let's you add instances as well. Interesting! I hadn't thought of this solution. Not having to update the `Shape` GADT with new instances could be useful in library code. 
I wish this was posted a week ago; it would've saved me a lot of time! I stumbled my way through [testing a Scotty app](https://github.com/tfausak/hairy/blob/22145de/test-suite/Main.hs). The end result is the same, but my approach is messier. [hspec-wai](http://hackage.haskell.org/package/hspec-wai) in particular looks very useful. 
It also wouldn't kind-check. But what you sketch is basically the idea behind my proposal for the Omega2 language: allowing this type of circularity, and self-parametrise data types from the "left": data Nat = Z | S Nat and then have S Z :: (S Z ° Nat) where the "left" parameter (a singleton-like index) is exposed by the ° (degree) operator. The idea goes on to also allow guards: someOddNatural :: (n | odd n ° Nat) Of course this enters well into the "dependently typed" territory. 
Much agreed. I vividly remember when I was but a wee lad (like, actually a teenager, but whatever) and I first discovered source control. It seemed like a good tool to use. So I got this book on Subversion. I began reading it, and it immediately talked about setting up a server - and I was a poor teenager so F that and I never opened that book again. Not long after I found Darcs, and it did *just work* for me, for small projects, and it was deadly simple with a great UI. It was also probably one of the first programs I ever used written in Haskell, so it was quite interesting to use and explore from that POV too. I didn't use subversion ever again, until years later when I got my first job, and at that point it was so alien and crippling it was bonkers. Darcs definitely had and still does have a strong place in my heart - despite using `git` for years, and mercurial professionally too, nothing is quite as simple, and I miss that (and yes, I think the Mercurial UI sucks a great deal in comparison too, before people jump on that one).
IIRC someone was working on a pure-Haskell alternative to gmp. Does anyone know what the status is of that project, or if there has been any development since it last appeared here on Reddit?
I encountered this problem as well. I would expect there to be a way to do it in which I don't need to anticipate all the libraries the plugins might use ahead of time.
I've also found Darcs to be much easier to use from the command line than other DVCSS. I've been using for the last 6 years and even with some co-workers to quickly setup some form of repository.
Integer-simple has been available for awhile as a drop-in replacement for integer-gmp (the standard default), though I'm not sure what its current status is (the darcs link on Trac returns 404, for example). It's BSD licensed, but it's SLOW. I believe it's stable, so I wouldn't expect to have seen any new development, other than making sure it compiles. edit: Integer-simple is available from [git.haskell.org](http://git.haskell.org/packages/integer-simple.git). It was last updated in January. edit2: It's also included in the GHC source. It can be enabled in mk/build.mk at configure time.
I facedesked so hard at reading this, my GF got scared because she thought I died.
Isn't there a function like `error` which does the same so it can return the line number? Could have been a toy extension though. 
We actually already have a Haskell `Integer` type, written entirely in `Haskell`, and it ships with GHC, called `integer-simple`. We just use the GMP-version of `Integer` by default instead. The work you're referring to is work by Erik de Castro Lopo, one of the GHC committers. The primary motivation for that work was to remove the LGPL'd dependency named GMP, while also providing a faster alternative to the very naive `integer-simple` library. This is still AFAIK the primary motivation for removing GMP; licensing concerns, not the fact it's too slow or anything. I don't know what the conclusion from Erik's work were. It was definitely faster than `integer-simple`, while being more complex, but the discussion sort of stalled there. But IMO, no matter what, none of these situations are ideal. Certain users are going to want the extra speed, yet be completely OK with an LGPL dependency. Commercial users may not care about `Integer` being fast, only licensed in a way that permits them use. But today, GHC hardwares the chosen 'Integer' representation into the source, and that's less than ideal - you have to rebuild GHC to provide a new `Integer` type. As a spoiler alert, Herbert Valerio Riedel and I have recently laid out a basic attack plan to fix this in another way, and we think it's pretty feasible. Instead, we are merely going to 'un-hardwire' the `Integer` type from GHC. Next, Herbert wrote a GMP-based `Integer` backend with almost no GHC magic - just pure FFI C calls, and supported as an external `.cabal` library. This change also does not compromise performance. So very soon, `GMP` is going to become significantly less magical inside GHC. After that, we're then going to write *another* integer library, but likely using something like [TomsFastMath](https://github.com/libtom/tomsfastmath) underneath, which is fast, stable, and public domain. We'll then finally rework things to have a single Haskell-level `Integer` API, which can be backed by separate *implementations which are controlled at link time*. That means the choice between GMP and TomsFastMath can just be controlled by a GHC user, every time they compile a program. No need to rebuild GHC or fiddle with all that other nonsense. We could even allow end-users to add their *own* `Integer` type and have it work without issue, providing they follow the specific "GHC Integer API" we lay down. We should really end up writing all this down, but we're pretty confident it can be done without too much of a performance hit, while adding a lot of flexibility hopefully. Stay tuned.
There didn't seem to be anything that approached the ease of loading plugins from C (where you just dlopen a compiled shared library and suck it in). Unfortunately, all the Haskell plugin systems seemed to require having an entire GHC build environment wherever your application is deployed, which basically makes it impractical for many (most?) use cases. It's one of the few things I just couldn't get working for me with Haskell. I pretty much gave up on it. Fame and riches\* await whomever fills this vital niche with a practical solution. ^(* Probably more of the former than the latter, to be honest.)
Interesting, thanks for the effort. The Haskell version is compiled with `ghc --make -O2 -XBangPatterns -rtsopts pidigits.hs -o bin` and there's a commit message for the most recent change of the build file stating: *"Irritatingly, GHC can't use LLVM 3.5"*. I've built and run the code on my machine both with and without `-fllvm`, but have not observed a difference in performance. Then I noticed that I also have the ghc version 7.8.3, which may be affected by this bug. But since llvm seems to be statically linked into my ghc, I wasn't able to confirm if the affected llvm version is used there. Does any one know, how this bug was (not) resolved? Is the `-fllvm` flag simply ignored in this ghc version? **edit**: Is my assumption correct, that [this](https://ghc.haskell.org/trac/ghc/wiki/Status/GHC-7.8.4) is the before mentioned bug?
My usual rule of thumb here would be to make separate Circle and Rectangle data types: data Circle = Circle Float Float Float data Rectangle = Rectangle Float Float Float Float class HasSurface t where surface :: t -&gt; Float instance HasSurface Circle where surface (Circle _ _ r) = pi * r ^ 2 instance HasSurface Rectangle where surface (Rectangle x1 y1 x2 y2)) = abs (x2 - x1) * abs (y2 - y1) Since Circle it its own type you can define circumference :: Circle -&gt; Float circumference (Circle _ _ r) = 2 * pi * r just like you want. Now we can define lots of things that have surface areas: instance (HasSurface a, HasSurface b) =&gt; HasSurface (Either a b) where surface = either surface surface instance HasSurface a =&gt; HasSurface (Maybe a) where surface = maybe 0 surface Then I'd ask myself what it is I'm using Shape for. If I really need to distinguish shapes, then Shape can be something as simple as `Either Circle Rectangle`. If the set of possible shapes is fixed I can use something like your `Shape`. On the other hand if it is just something that has a known surface area, why not just make something that represents the _properties_ you want to observe about all shapes. newtype OpaqueSurface = OpaqueSurface Float instance HasSurface OpaqueSurface where surface (OpaqueSurface a) = a Now in reality you probably have more properties you are using about your shapes, so you may have a record that defines a lot of properties about the data type. You can go a little nuts with this and start exploring the space of ["abstract" data types](http://www.cs.ox.ac.uk/jeremy.gibbons/publications/adt.pdf) if you need to mutate the things you put in the box e.g. translate them, scale them, rotate them, or whatever. You can keep the space of shapes open by moving those properties to typeclasses and working with something like Either, but now the set of shapes becomes visible in the type, or gets baked into something like your second Shape data type. Or you can fix the properties you want to observe and make something like OpaqueFoo holding the fields that are what it means to do whatever you can do to the thing or extract whatever properties it has. This is an instance of what Philip Wadler called the ["expression problem"](http://en.wikipedia.org/wiki/Expression_problem).
That's interesting. I would like to package GitIt wiki and a plugin as a binary distro but currently it's not possible because GitIt uses GHCI to load the plugins. Seems that with this package one could achieve that.
This looks promising. I will definitely check this out later.
anyone know why? why should it beat julia, for example, which you'd assume to be pretty much equivalent to calling c from c? the only thing i can think of is that ghc is managing type tagging / wrapping better, somehow? is that dumb? any other ideas? maybe i should ask the julia people why they're not as fast as ghc...
in terms of php wtf's, this doesn't even rate try division by zero, and it gets uglier from there
Well, I would take a big step back from this concrete question and ask: 1. What motivates the desire to write such a function? 2. Does that motivation call for a different factoring of the program's types? If, for example, your program has a lot of logic that works only with circles, then you probably want to have a dedicated `Circle` type for that code, [as edwardkmett already recommended](http://www.reddit.com/r/haskell/comments/2jsz4t/function_on_a_single_member_of_a_sum_type/clf1sj1). In fact, I just realized I was about to say mostly the same stuff he did, except that I'd throw in the record-of-methods pattern ("objects") before type classes, but that's (a) just me, and (b) very dependent on the problem anyway...
There's a wiki on this at https://ghc.haskell.org/trac/ghc/wiki/ReplacingGMPNotes. It possibly needs to be updated though, with the possibility of replacing libgmp with [bsdnt](https://github.com/bsdnt/bsdnt).
I like that you've posted three different replies to essentially the same complaint, and that all three of them included distinct example code.
I find it unusual to be the one defending PHP, but this article is misleading. The PHP code uses the interface to `bc`, which does use strings -- but this would only be needed if you wanted to do *arbitrary precision* arithmetic, not just bigints. PHP also has bindings for GMP ([the `gmp_` family of functions](http://php.net/manual/en/ref.gmp.php)), which use a specialized type. The benchmark should be using this instead.
&gt;As a spoiler alert, Herbert Valerio Riedel and I have recently laid out a basic attack plan to fix this in another way, and we think it's pretty feasible. Instead, we are merely going to 'un-hardwire' the Integer type from GHC. Next, Herbert wrote a GMP-based Integer backend with almost no GHC magic - just pure FFI C calls, and supported as an external .cabal library. This change also does not compromise performance. So very soon, GMP is going to become significantly less magical inside GHC. &gt;After that, we're then going to write another integer library, but likely using something like TomsFastMath underneath, which is fast, stable, and public domain. We'll then finally rework things to have a single Haskell-level Integer API, which can be backed by separate implementations which are controlled at link time. That means the choice between GMP and TomsFastMath can just be controlled by a GHC user, every time they compile a program. No need to rebuild GHC or fiddle with all that other nonsense. Yay for backpack? :D
This is super useful, I'd love to see this linked to by the Wai/Warp/Yesod documentation. 
There's no such thing as too much documentation! :)
A practical example is that you can write a type safe printf function. 
Thanks for the extensive answer! Nice to hear that this is still being actively worked on. :) It would have been interesting to see how integer-simple and Erik's work would have fared in the linked benchmark, although perhaps not of much practical importance.
This implementation is very fast: http://benchmarksgame.alioth.debian.org/u64q/program.php?test=pidigits&amp;lang=php&amp;id=5
I have added this to my bag of tricks. Now to use it.
Here's an interesting one: using partial evaluation to write an [interpreter competitive with Java in speed](http://eb.host.cs.st-andrews.ac.uk/writings/icfp10.pdf).
It's a wiki. You can do it. That's the point of wikis.
You might find [this thread](https://www.reddit.com/r/haskell/comments/2gomte/fairly_small_projects_1_to_7_days_to_do_and_learn/) of some use.
I'd like to say that this library is wonderful. I've been working with it for only a couple of hours, and I have achieved more in that time than I did all weekend. I do have a hiccup, though. Does it work in ghci? I've been testing in ghci, which so far has allowed me to identify and overcome a few hurdles, like not loading .so file for a C library, not loading the .o files for my dependencies, etc. However, now when I try to finally load my function, a simple String, I get "Could not load symbol: Mabel_hdf5Root_closure." I don't know where to go from here because I believe I have everything in order similar to the example on github. Could you take a look? http://lpaste.net/112965 
Ok, show me printf then. Not some other way of formatting, but exactly printf. E.g., `printf "hello %s, %d" "foo" 42`. There are many ways to write other formatting functions (without GADTs). 
[This is it](http://caml.inria.fr/mantis/view.php?id=6017), although I could be misinterpreting something. *Edit: oops, I was wrong, looks like it requires some help from the compiler.
This is really cool, the `with` setup is far nicer than how things need to get more manually threaded with hspec-1. It looks like quite a bit of the code from yesod-test for HTML parsing/CSS pattern matching would be a great candidate for hspec-wai-html (as a sibling to hspec-wai-json). That would open up things to other web frameworks besides Yesod. What do you think? Also, how production-ready is this? hspec2 is still marked as alpha, which makes me a bit hesitant to- for example- have the Yesod scaffolding use hspec-wai.
You could check if /r/dailyprogrammer has something interesting. Or maybe try going through [Python Challenge](http://www.pythonchallenge.com/), though some of the problems aren't quite as fun without python standard library.
I've just in the last couple of weeks finally got around to reimplementing the machinery to do this in the latest version of Idris, and extending it a bit to specialise definitions with type classes and higher order functions. Haven't tried reproducing everything in this paper yet though...
But that is using support in the compiler. The compiler turns the formatting string into a GADT tree. So that doesn't really count. 
I think we are not even scratching the surface in how these might help us in mathematics. I find the static guarantees in hmatrix really useful for preventing bugs: https://hackage.haskell.org/package/hmatrix-0.16.0.6/docs/Numeric-LinearAlgebra-Static.html. I am hoping to be able to provide even more guarantees about my mathematical derivations e.g. one can write the Haar functions with the constraints checked by the compiler data Haar (a :: Nat) (b :: Nat) = Haar { unHaar :: Double -&gt; Double } haar :: forall n k . (KnownNat n, KnownNat k, (2 * k - 1 &lt;=? 2^n - 1) ~ 'True) =&gt; Haar n (2 * k - 1) haar = Haar g where g t | (k' - 1) * 2 ** (-n') &lt; t &amp;&amp; t &lt;= k' * 2 ** (-n') = 2 ** ((n' - 1) / 2) | k' * 2 ** (-n') &lt; t &amp;&amp; t &lt;= (k' + 1) * 2 ** (-n') = -2 ** ((n' - 1) / 2) | otherwise = 0 where n' = fromIntegral (natVal (Proxy :: Proxy n)) k' = 2 * (fromIntegral (natVal (Proxy :: Proxy k))) - 1 But these are early days. One of my colleagues suspects that there are many published papers in mathematics that would fail if checked by machine.
assert False does it, although my hope is people spot the word "bad", and don't actually go as far as putting it in their code
Valid join sequence: NICK yournamehere USER yournamehere 0 * :helloitsme JOIN #haskell-blah
Could it be used more generally? For instance if I had a function that sums two ints, a and b, could I specialise it such that a gets specialised to a value read in at runtime (for instance 2), then have it perform as well as a function defined at compile time that just adds two to its input?
I think the most important practical use of DT is for software assurance. You can include in your source code properties that your functions are required to satisfy. Unlike quickcheck or smallcheck properties, which are checked empirically by running tests, these properties are mathematically verified when your program is compiled.
Yes, and that's a shame.
Can someone explain why is Shape un-promotable?
... because it has `Float` fields. Floats, chars, ints, integers, anything `Word`y, etc. is not promotable.
Could be nice to `pipe`-ify [simple-irc](https://hackage.haskell.org/package/simpleirc).
You implemented `nc`. ;-)
Hi! I'm also a Haskell newbie :). You should probably take a look at [Exercism](http://exercism.io), they have some really cool practical small projects that help you exercise your coding abilities in different languages, including Haskell.
Is there a version of [haskeline](http://hackage.haskell.org/package/haskeline) for pipes?
Besides that, TomFastMath is very focused on a particular use case, name ly certain specific crypto algorithms, and makes some pretty invasive design choices to squeeze out tiny bits more performance in that case. It doesn't look suitable for a general bigint library.
It needs a lot more updating than just that. This wiki page hasn't been modified for 6 years (as of this writing). It doesn't even mention the existence of `integer-simple`; it only says that it might be possible to create such a thing. There does seem to be a lot of information there that is still valuable. It would be great if someone would bring it up to date. EDIT: Oh, it *does* mention `integer-simple` in the "Current status" paragraph tacked on at the beginning, 6 years ago. The rest of the page was written even further in the past, and it is written as if `integer-simple` doesn't exist yet.
Unfortunately not, but line user input is maybe something that is less suited for being piped around. I'm not sure pipes would make it any easier unless it was reading big chunks in from stdin.
First, thanks for the gold :-) But the real credit goes to Hampus (and to some extent to Simon Marlow, who was involved originally). I only added some (nice, IMHO) error handling mechanism and pushed it through Hackage. Currently (that is "this week") I am pretty tight with my schedule, but the weekend looks good. If you want, fork on github and prepare pull requests. I am very glad that I could help you!
This paper has some excellent examples: [The Power of Pi](http://www.staff.science.uu.nl/~swier004/Publications/ThePowerOfPi.pdf) And if you want to know how to implement a DT language yourself, this is an interesting read: [A tutorial implementation of a dependently typed lambda calculus](http://www.staff.science.uu.nl/~swier004/Publications/Tutorial.pdf)
I think, a link-time `integer-gmp2` adapter could probably easily be implemented based on bsdnt's `nn.h` API. `integer-gmp2` basically just relies on the availability of "big naturals" being provided as an explicitly memory managed[1] `(HsWord* limbs, unsigned limb_count)`-pair. [1]: since we want to use GHC's `ByteArray#` to represent such pairs
Just use rlwrap
I expect it could be done more generally (multi stage programming like in MetaOCaml can do this sort of thing) but it only works at compile time at the moment. Even this is already useful for things like specialising type class dictionaries or higher order functions.
I've got an error &gt;connect: timeout (Connection timed out) But ping goes well. What it could be?
Excellent! &lt;/sarcasm&gt; I couldn't even get this compiled. For some magical reason, GHC couldn't find `Pipes.ByteString` after I installed `pipes-bytestring` via cabal AND apt. So I tried it on another system, but there is a slightly broken package `network-simple-0.4.0.1` on Hackage. I can only guess what your program does. But when i see this right, its just a TCP reader &amp; writer? Because that could have been done without Pipes and just using the basic libs available with Haskell-Platform. import Network import System.IO import Control.Concurrent main = connectTo "irc.freenode.net" (PortNumber 6667) &gt;&gt;= (\h -&gt; forkIO (link h stdout) &gt;&gt; link stdin h) link h h2 = hGetContents h &gt;&gt;= hPutStr h2 
https://en.wikipedia.org/wiki/Netcat
it is actually a binary protocol: encodings are not handled in a standard way, and messages can contain binary escape sequences (e.g., to do /me style actions)
If `xs` is a list, then you have the following isomorphism: xs &lt;=&gt; \f a -&gt; foldr f a xs
I don't think that package existed when I was banging my head against the problem. It looks very useful! I don't have a need for it right now, but I am definitely going to keep it in mind. Thanks!
Ideas off the top of my head: - Simple JSON &amp; HTML web site with Aeson &amp; Scotty &amp; Blaze - Command line calculator or unit converter - Image manipulation / creation (make fractals maybe?) - Eliza terminal app - A text game (zork style) - A graphical game of some sort. Implement pong? - Redo something from work. (My current approach), compare &amp; contrast results (speed / code cleanliness / extensibility) 
To expand on this a little, there are some standard encodings of characters (spaces, explicitly ASCII spaces, are used to separate parameters), but the majority of the protocol is encoding-agnostic.
I've thought arrows (circuit-diagramey-things) might be less painful with dependent types. Instead of encoding the input and output as a big nested tuple you could use a Vect n [Type], and have *** (running two arrows in parallel) append the two vectors, giving you associativity instead of a further nested tuple. I think you could also encode a sum of products type, to have uses of *** and +++ expand into a single representation, instead of one of a dozen different isomorphic versions of "(a,Either (b,c) d)" or "Either (a,(b,c)) d" or "Either ((a,b),c) d" etc etc. I don't think there's any reason for that to require extra type annotations, since the algorithm for distribution is pretty straightforward. Though I don't think automatic commutativity can be expressed so easily (or at all). I suspect there are a lot of little interface issues that people don't even think of as dependent types that could be simpler with the extra power. Anonymous sums should look roughly something like this: data Eithers : Vect n [Type] -&gt; Type where MkEithers : (m : Nat n) -&gt; lookup m (Vect n ts) -&gt; Eithers (Vect n ts) Read, an anonymous sum consists of a natural number which indexes the vector, and a value of the corresponding type. I've wanted something like that in Haskell when defining a new datatype for some single use seems too much, and with a function like (Eithers t1 -&gt; Eithers t2 -&gt; Eithers (t1 `union` t2) you should be able to encode exceptions with a monad-like interface that allows you to statically combine different exception types safely. Though my attempts with idris have been pretty unsuccessful, so I'm not 100% how easy those things would be.
Don't blame OP for your broken GHC/cabal setup and stop being a jerk.
You might be behind a firewall that blocks traffic to the IRC port.
May well be. I uploaded it on Hackage after 7.6.1 came out.
I am the author, and I would love feedback, from either beginners or those more knowledgeable than me. I would love to be able to show this to someone that doesn't know any haskell, to show them how delightful coding in Haskell is, but I'm not sure it's worth it to explain how to read type signatures and everything else. I guess I can just link to some beginner target articles where appropriate.
Sorry about that. I'm on rage the whole day. Wasn't a good day at all. Also i don't blame the OP for broken things. I just pointed out, that it isn't working easily. It might be jerking, when i tell someone, that there is a better solution. But i think its better i tell them now than never, for the sake of a better world.
No problem. I'll keep hacking on it to see if I can figure it out. I will let you know if I resolve the issue. If not and you are able to find time to look at it eventually, I would certainly appreciate it!
&gt; It might be jerking, when i tell someone, that there is a better solution. I don't think that was the "jerky" part. It was the unnecessary sarcasm and condescension. 
This example doesn't really uses pi-types, right?
A few thoughts, first off your `mapSnd` function is precisely `fmap` across `(,) a`. I don't know if you avoided this since it introduces functors, typeclasses, and type currying all at once. That could be a bit overwhelming if you're talking to someone new to Haskell :) Secondly, the statement &gt; Now let's generalize those variable names. Specific names get in the way of TRUTH. Scares me a bit. Yes, there are a lot of Haskell customs with using obscenely short variable names but you should justify this a bit. As a new Haskeller I was flummoxed why on earth that was a good idea. Even something so simple as &gt; When we're looking for general functions, we're trying to forget as many details of the specific problem as we possibly can. Once we start generalizing our functions though, the specific variable names no longer apply! Would probably remove some raised eyebrows. Finally, please include type signatures for your toplevel functions. It's hard to keep track of what has what type. Even though it's a bit redundant to restate types for toplevels, it does improve readability.
&gt; A tutorial implementation of a dependently typed lambda calculus I would recommend reading [How to implement dependent type theory I](http://math.andrej.com/2012/11/08/how-to-implement-dependent-type-theory-i/) before tackling [2]. 
I was having this same problem, thanks for the quick fix.
Thanks so much! These points are very helpful. I'll be sure to address them. I didn't actually know about the tuple functor instance (as I am somewhat of a beginner myself), but I agree writing the mapSnd function is probably a bit more approachable.
So I'm not sure if this is obvious or unhelpful or anything, but what jumped out at me is that the type of `accumModelEffects` looks like `foldM` specialized to `Writer [b]`. I believe that that is what you implemented, too. In fact, if you change `concat` at the end to `mconcat`, you should get `foldM` for arbitrary `Writer w`. Maybe there are ordering issues, I'm not sure.
Speaking of pipes and networking, I believe the bidirectional flavor of pipes is well suited for modeling RPC clients. To test that, I wrote a small JSON-RPC 2.0 client library called ["Colchis"](http://hackage.haskell.org/package/colchis). [Here](https://github.com/danidiaz/colchis/blob/master/examples/Main.hs) is a small example of usage. exampleClient :: JSONClient Text IO Int exampleClient = do i1 &lt;- plusone (5::Int) i2 &lt;- plusone 7 return $ i1+i2 where plusone = call "plusone" main :: IO () main = do r &lt;- runTcp "localhost" "26060" $ runJSONClient tcp jsonRPC20 exampleClient case r of Right (Right (Right i)) -&gt; putStrLn $ "result: " ++ show i e -&gt; putStrLn $ show e 
Concurrently is very useful, in many cases it can be used instead of forkIO and async.
Edit: [Here's the code](https://github.com/egonSchiele/chips). I factored out the base code into a framework, [here is a link to that](https://github.com/egonSchiele/actionkid). Finally, I made a walkthrough on how to use the framework. [Here's the video](http://vimeo.com/109663514). [Here's the source code for the walkthrough](https://github.com/egonSchiele/actionkid/blob/master/src/Main.hs). Once you understand that, it should be pretty easy to understand the code for the game. I have wanted to make a game in Haskell for a long time, and I finally did! This game is built on top of Gloss (which uses OpenGL) and lenses. I'm hoping to open source this code soon. I am using SDL for sound, but I have had a lot of issues with it. If anyone has a better solution for sound, please let me know! (There is no sound in the video)
Oh interesting! I'll look into that. I never thought of using Writer like that before.
I've been anxiously awaiting for more installments :D
Can you think of any large difficult problems as an example? Is it necessary to have a large difficult problem? Wouldn't it be necessary to be complex across many different areas? It seems like it is even more difficult than programming 2 solutions to a large difficult problem to come up with useful commentary on performance.
On windows, I was getting a socket error. Some googling suggested to apply withSocketsDo. This fixed the socket error and reads the irc connection stream. It seems there is still some kind of problem reading from stdin due to this error: `&lt;stdin&gt;: hGetBufSome: resource exhausted (Not enough space)`
`mapSnd` is the same as `second` from `Control.Arrow`. Is this intentional? Prelude Control.Arrow&gt; :t first first :: Arrow a =&gt; a b c -&gt; a (b, d) (c, d) Prelude Control.Arrow&gt; :t second second :: Arrow a =&gt; a b c -&gt; a (d, b) (d, c) Prelude Control.Arrow&gt; second (+1) (1,2) (1,3) 
Okay. He enraged me with this post and I couldn't resist. It was just too stupid what he wrote, without any sign of an acknowledgement that he knows that his post is probably not so wise. It enraged me even more after no one told him that and everyone seems happy to learn Pipes now. So I did, with a load of pepper. PS those guys who just click on downvote because everyone else does, annoy me too. Two hours after I posted my now-negative post I had an positive vote record. These aren't sane social mechanics ...
In this case I would recommend that the circumference of a Rectangle be undefined. At the level of abstraction we're using here, it seems the most sensible way to express that a Circle has a circumference but a Rectangle doesn't. If you're not happy with partial functions, then maybe the abstraction itself is not the right one.
Incredible! This was one of my favorite games growing up. I would love to see the source behind this. (Also, recognized your avatar from your work on contracts.ruby - just want to thank you for your great work!)
I love sin. Well, no. But I think lazy IO is an advantage. In many cases, laziness is a minefield in a deep fog, but for trivial issues, its fine. 
I'd only seen the newest semester very early on and didn't know about the Monoids, I/O chapter. Thank you for pointing this out, it's *very* good! Props to byorgey :)
That's really neat. What did you think of the experience of using Haskell to write a video game?
Check http://checkpad.de/jobs/ in Germany. Just let google translate the webpage and you should be fine. Also http://www.tsurucapital.com/en/jobs.html
Looks very cool. Is there any reason why this isn't there? concurrently' :: [IO a] -&gt; IO [a] concurrently' l = mapM async l &gt;&gt;= mapM wait 
Source coming soon! And thank you, glad you like contracts.ruby :)
It has been really nice. Whenever I googled for making games in Haskell, I kept coming across heavy FRP theory. But this game is written very simply. Example code: case gameState ^. player.direction of DirLeft -&gt; player.x -= tileSize DirRight -&gt; player.x += tileSize DirUp -&gt; player.y += tileSize DirDown -&gt; player.y -= tileSize I could do a walkthrough of the code too if anyone is interested. Having Haskell's type system made some things really elegant.
This is awesome. I'm looking forward to the source as well. :)
&gt; In this case I would recommend that the circumference of a Rectangle be undefined. At the level of abstraction we're using here, it seems the most sensible way to express that a Circle has a circumference but a Rectangle doesn't. I'd push back and suggest that `circumference :: Shape -&gt; Maybe Float` is better. I also wouldn't encourage a beginner to write partial functions or explicitly return `undefined`. &gt; If you're not happy with partial functions, then maybe the abstraction itself is not the right one. Yes, and I think this is a tension that may not always be communicated in learning materials.
A MOOC using Haskell just started a week ago, taught my Erik Meijer. The course is not Haskell specific, covers fp in many languages, but the main "medium for understanding" is Haskell. https://www.edx.org/course/delftx/delftx-fp101x-introduction-functional-2126 . The second week's assignments and materials are released tomorrow, 22 Oct.
because `mapConcurrently id :: [IO a] -&gt; IO [a]`
Thanks for sharing the ADT paper. It seems to me that [yatima2975's `Shape` datatype](http://www.reddit.com/r/haskell/comments/2jsz4t/function_on_a_single_member_of_a_sum_type/clex8l8) is nearly an example of codata (where additional functions could be defined on `ShapeType`), except the `ShapeType` parameter `st` appears on the left.
&gt; Chris done sparked off a discussion about making GHCi awesomer [...] That is an amusing typo.
PDF version here: http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.56.1582&amp;rank=1, plus related research.
as a bonus, it's also generalized to `Traversable`
Belgian here. I know that the University of Ghent has a language research unit in Haskell. Maybe they could be interested. It's in the Dutch speaking part of Belgium, but English should be OK. 
I'm really surprised that `base` doesn't have something like: -- The returned `String` is read using lazy `IO` withContents :: FilePath -&gt; (String -&gt; IO r) -&gt; IO r This solves some (but not all) of the issues associated with lazy `IO`.
If your post had 1,000 points, I still would have downvoted. For me, it was because of the content not the points. I suspect the majority of the people who downvoted did for that reason as well. This was not an effective way to communicate your idea. Really, regardless of the relative advantages and disadvantages of the two approaches, this is a very inappropriate way to respond to such a problem. It is ultimately a pretty minor issue and you do not come off very well replying in this tone. If you're having a bad day and you want to have a discussion with someone who said something you strongly disagree with, especially if it made you angry, wait at least a few hours before you respond. A day or more is even better, especially for a medium like reddit where it is easy to respond a while after the original post. Also, I know it's cliché, but try to put yourself in the other person's shoes as well. The original post was probably not done with some great evil intent. If you posted some code you wrote that you thought was fun and you got this response (imagine the same general wording of this last post), how would you feel about that? Would it color your view of what that person says in the future? What would you think of the prospect of working with them in the future? I know this can be easier said than done, I've had this problem before too, but if the best way to describe your feeling about a computer programming issue is "enraged," it's probably time to leave it be for a while and come back to it later. Consider also that sometimes people advertise programming jobs here. I know this advice is unsolicited, but I really wanted to say this. My suggestion: A month from now, reread the original post and then reread your replies, especially this last one. 
 withContents :: FilePath -&gt; (String -&gt; IO r) -&gt; IO r &gt; &gt; This solves some (but not all) of the issues associated with lazy IO. I don't see anything solved at all, care to elaborate?
Damn, I played the heck out of that when I was a kid. I would love to see the source, to get the jist of how you did it.
the docs for 0.2.0.0 have been up for a bit (and i uploaded that on the 17th) so its definitely chugging along i would've waited for the new ones to build but the 0.2.0.0 docs are almost identical save for some broken import stuff, so looking at those should give you a fair estimate of the library there's also a readme at https://github.com/intolerable/api-builder with a short example, and https://github.com/intolerable/reddit which is essentially the reference project for `api-builder`
It's more obvious where the file/handle is closed (at the end of the `withContents` block), compared with `readFile` where it's not obvious at all when the handle gets closed. This makes it easier to reason about resource management.
I'd love a walkthrough of the code! :)
&gt; Prefer library routines after custom ones typo: after -&gt; over
Thinking about it further, I'd agree that `Maybe Float` is better. It clearly shows how to safely represent (and deal with) an absent value. To a beginner I think this would be a nice example of the power of types. The best solution presented here though is probably /u/chrisdoner's GADT technique. In general though I would highly recoomend at every level to use `undefined` while thinking about and hashing out a solution.
The lens does.
It would be really nice if the link to the repository was on Hackage. That was the first thing I looked for: a link to the repository in the hopes of finding a Readme. Additionally, you should provide a bug tracker link (i.e. github issues) and homepage link (presumably to the github readme).
I found yesod-test while I was searching for a tool for parsing html with CSS selectors. It looks nice, have you considered pulling the css selector parsing and matching into its own library? 
can i add those without having to upload a new version to hackage?
A walkthrough would be really cool. Can't wait to see the source :)
Slightly off topic (of Haskell), but wow, seeing that brings back memories. Never did quite finish all the levels. Is it playable anywhere?
I don't know - hopefully someone else does
Nope, that's why I made it. Coming soon to a github near you.
I commend you for making it through How to Prove It. I stopped reading around page 80 several months ago, but now getting the itch to continue. 
Definitely prefer `second` from Data.Bifunctor (or Control.Arrow) to `fmap`, makes it clear what you mean.
I had to take 2 runs at it, and could probably do with another run through if I'm honest. The first time I slowed down about 2/3 of the way through, and when I tried to get back into it I found that I'd lost a bit too much to continue. I felt a lot more comfortable on the second pass. I've done that with a couple of books now, and I think I'm starting to make peace with it. I figure if it's worth spending the time to work through a book and do all of the exercises, it's worth spending a bit more time to make sure that you really understood it all.
the `State` combinator lenses kick ass -- that's awesome!
Is the source somewhere?
This is great. Just one question: is it possible to set remote-repo to something like "the latest GHC 7.8 snapshot"?
There are many answers to "what" questions. Obviously I was not looking for the answer involving the above program, but rather the word "netcat".
Nothing so complex, really. You just gave the wrong Functor instance instance Functor Something where fmap f (ResultA i g) = ResultA i (f . g) fmap f (ResultB g) = ResultB (f . g) fmap f (ResultC i d) = ResultC i d fmap f (ResultD i d) = ResultD i d For values of ResultC and ResultD there exist no values inside of the construct of type `blah` so there's nothing for the function `f` to operate on.
I think the main advantage of doing this in Haskell is that the Haskell community encourages this sort of thing and considers higher-order combinators more idiomatic. That and it's more syntactically lightweight.
Certainly, but I meant more to say that the above program is a remarkably curt spec for netcat. Sorry if I appeared to imply anything more.
Thank you, after adjusting accordingly, ghci seems to accept it. I tried to do something like `fmap _ c@(ResultC _ _) = c`, however it then complained about `Couldn't match type ‘a’ with ‘b’` when it came to `Something a` and `Something b`. Though I've followed more of your pattern instead, but with `f` as `_` on ones that don't have `f` on both sides. It does not seem immediately intuitive why I'd be using the constructors again each time. Does this harm efficiency at all?
The Haskell community is a really nice place and there is often someone who can help in the most friendly way. Which is great. But People should get a thicker skin. Really. It was a rather extreme case of commentation, but I don't think I have done something wrong. Likely because I have a different background. I was a active gamer several years ago and there was always a kind of sarcastic argumentation all the time. In discussions of other programming languages, namely C++ and C#, there is far bigger potential of enragement and harsh comments and everyone is supposed to live with that. I even had a nice flamewar with some Lisp coders the last days. The style of communication was aggressive, but i learned a lot. Its also was fun. Its also a point, that we are all coders in a broken world without time to write good code or not even can learn how to write good code. Coding isn't fun in RealWorld™ . We are driven mad and crazy. I get mad when I think about all this bullshit, while hardly trying to write good code. Which brings me to this example. The OP's post was a nice idea, I guess, but for the expectations of at least me, it was low in quality and value. Or rather negative. So negative, that it has bad effects on the opinions of community about coding. Be it in the expectations of Pipes or in the meaning of Occam's razor. Which is ultimately not a minor issue. Its probably an example of the greatest flaw of education and communication: no one told them, that it can be wrong and many people cannot think for themselves. Everything would be fine, when he had admit, that this can be not a optimal way or that this is a joke, just to save the beginners and light-minded from misleading information. Thats what really grind my gears in an extreme extend: broad misleading information. English is my second language. "enraged" was a sufficient word I found. Maybe not a good one to describe my anger. I still can delete posts for societies sake of hiding all the terrible things, when I would care that much about job offers. I also wouldn't like to work for someone who thinks that way. Feel free to say everything you want to me. Really. You can only make me better. And if you feel anger, use bad words. Because it helps you more than it hurts me. I just pull over my thick skin. So warm here. 
I shall keep that in mind. I've never used that before.
Yes, but I don't recommend it. See: http://www.haskell.org/pipermail/beginners/2014-September/014095.html
In a similar vein, Chalmers University in Sweden is involved in Haskell stuff. I don't know to what extent, but it doesn't hurt to take a look. English should work just fine there too. :)
Yes, yesterday, when I posted the above comment ;) Generally speaking, I've stopped spinning off code into separate libraries prematurely, as (1) people complain about having to install extra dependencies, and (2) it's extra work to maintain. I'm not opposed to the spin-offs when they make sense, but I don't seek out opportunities for them.
Interesting. I haven't seen much code like that in traditional imperative languages. Some ruby I guess.
I wasn't familiar with those (and I'm glad I am now!), but I kind of like the way it reads as I meant it to be somewhat of a tutorial on using Hoogle. But I will add a note about it!
So I just got up and have a cup of coffee in the morning. Read this. Made my day. Cheers!
&gt;less bug free I take it you mean 'more' here? :)
I'm investigating stuff like this in PHP. Quite interesting what is already possible. No type system, though :(
OK. Thank you. :)
By "this summer" do you mean summer 2015? We (Signal Vine) may have something (including data mining), but it's hard to say so far in advance. Drop me a line at roman@signalvine.com.
&gt; It has been really nice. Whenever I googled for making games in Haskell, &gt; I kept coming across heavy FRP theory. Abstractions in Haskell are sometimes like design patterns in object oriented languages. Some are nice and useful and some tend to go in the direction of over-engineering and add more complexity then they remove. 
or use typed holes that behave like undefined but will even guide your implementation.
I think what you're really trying to do here is this: data Something' blah where ResultA :: Int -&gt; Something' Int ResultB :: (Num a) =&gt; Something' a ResultC :: (Num a) =&gt; Int -&gt; a -&gt; Something' blah ResultD :: (Show s) =&gt; Int -&gt; s -&gt; Something' blah This is not a functor, but you can turn it into one simply by using `Coyoneda` from the `kan-extensions` package: type Something = Coyoneda Something' This way you get the `Functor` instance for free. The following might or might not help, but here is what is really going on here categorically: `Something'` is a functor from the [core](http://ncatlab.org/nlab/show/core) of `Set` to `Set`, and `Coyoneda`, which is essentially defined as `Lan Identity`, is in reality the left Kan extension along the canonical functor `core(Set) → Set`. The resulting functor is `Set → Set`, hence a proper functor. EDIT: I should add that I learned this from the excellent paper [Constructing Applicative Functors](http://link.springer.com/chapter/10.1007%2F978-3-642-31113-0_15) by Ross Paterson. There, this construction is used to explain why Conal Elliott's [`WithCont` type](http://conal.net/papers/type-class-morphisms/) (which is the same as the `Fold` type from /u/Tekmo's [foldl](http://hackage.haskell.org/package/foldl) library) is indeed an `Applicative`. 
&gt; it immediately talked about setting up a server Bad book. You can use svn with a local repository without the need to set up a server.
*I bought myself a textbook on category theory.* Which one? Would the blog author recommend it?
&gt; However, despite the claims from aficionados that Haskell is a silver bullet that will magically solve all your problems and fix all your bugs, that's also not true. I'm trying to think of a name for this mythical straw man Haskeller who makes such silly claims. I see comments like this all the time. “Haskellers claim X ridiculous thing, but it's not true!” 
This may be a silly question but I'm a bit confused. People say that in dependent type theory you can't meaningfully separate type checking, compilation and execution phases (pretty much a direct quote from here: http://math.andrej.com/2012/11/08/how-to-implement-dependent-type-theory-i/), does this mean that you can't type check you program before running it? If yes, why not just go back to dynamic typing? Isn't that the whole point of strong static typing, making sure that there are no type errors *before* running the program?
There are two that come to mind: 1. *Categories for the Working Mathematician (Graduate Texts in Mathematics)* by S. Mac Lane 2. *Category Theory (Oxford Logic Guide)* by S. Awodey The latter is probably more approachable than the former. Both are good text books. 
You got it right. It's "straw man." I'm sure there are some Haskell talking-heads who say such ridiculous things. Both seem quite common: straw-man fallacies and hyperbolic nonsense with talking heads or blogging heads.
I've been watching some lectures on category theory by Awodey. He's pretty good at explaining stuff. It's the ones under the "Category theory foundations" heading on [this](https://www.cs.uoregon.edu/research/summerschool/summer12/curriculum.html) page. 
..which is a problem most languages share, except the commercial languages that include the particular functionality you want in their huge standard library. But I *really* would like some votes, community "see also", "related packages" etc on hackage. Hackage is really not giving anyone much help in figuring out which package to prefer. Couple that with stackoverflow closing any questions that try to compare packages, there are only blogs left.
&gt; I'm sure there are some Haskell talking-heads who say such ridiculous things But are there? I'm just not sure there are. If there really are then we need to find the posts where they make these claims and bring them back to earth.
I think something like that on Hackage would be great. I would love to see in one place what all of the alternative libraries are, how well they are maintained, and what makes them different. Sounds like a lot of coordination and work, though.
I'm a big fan of *Basic Category Theory for Computer Scientists* by Benjamin C Pierce. It's quite a slim volume (~100 pages), and doesn't go into that much depth, but it's a great introduction IMO -- I find Ben Pierce's technical writing a joy to read in general, he has a very digestible style.
The reason that C++ has all that ````std::```` stuff (and has had since the early 1990s) is exactly to afford and encourage this style of programming. 
btw, debian-stable ships with a 2012 haskell-platform... which means ghc 7.4.1
You might be interested in [this](https://github.com/trehansiddharth/6046-algorithms-haskell) github repo. I'm trying to write down functional versions of all the algorithms covered in one of MIT's algorithms classes.
http://i.imgur.com/GJLQTXb.png
As far as I know the only thing you are allowed to change is version ranges in the .cabal file. You are definitely not able to upload new files. 
The lightweight is very true. But then, cout &lt;&lt; is not light either :-) The larger C++ community does support this. But there are also plenty of coders who have still not embraced the STL, Boost, etc. I have worked with a fair number of them.
Yeah I had "less buggy" and changed the wording. Sigh. When I code a little too fast I have tests and testing to cover me. When I write a little too fast I only have the Internet to check me :-) Imagine letting someone else compile your code and it is kind of like that.....
Wow I loved that game. I have a question, how are you finding Haskell game development? Maybe I'm thinking about this incorrectly, but it seems to be that games are mostly IO (and have a lot of state), which makes me think game development isn't an ideal application for game development. What are your thoughts, especially in regard to programming a game in Haskell vs. programming a game in another language. 
If you can represent package ratings as a natural transformation or a category or an algebra, someone will do it.
I was thinking a fun project would be a dwarf fortress style game (a world, being rendered, maybe interacting with itself, some characters with some "ai"). It's one of those easy-to-make-something-work, but also infinite-depth problems. I could do a map renderer in a few hours, or I could spend 10+ years making a game (the way dwarf fortress has gone).
I hear that Haskellers claim to hear critics claim to hear Haskellers make ridiculous claims, but that in itself may be a ridiculous claim. *(and so on)*
&gt; withContents :: FilePath -&gt; (String -&gt; IO r) -&gt; IO r Would you mind showing the code, please?
It turns out that Haskell is still a totally great language to use if you just throw IORefs into your program wherever you want.
Correct me if I'm wrong. You say that the timescale for constructing the tree is O(n log n), but I think it is in fact better: O(log(n!)). This is due to the n in O(log n) depending on the size of the tree constructed at the point of insertion. The worst case is still O(n^2)
In particularl I meant a base package that could be shared by both hspec-wai-html and yesod-test, rather than yesod test building on a wai specific library which was how I read your comment. I'd love to see just the css parser independent of any matching in its own library, during my search I saw like 5 separate half-implemented css/3 parsers bundled up with matchers. I personally don't mind having lots of dependencies, is there a reason I should besides potential inconvienence of listing them?
The difference is I have evidence. I'm quoting someone. Here's [another](https://www.facebook.com/notes/kent-beck/functional-tdd-a-clash-of-cultures/472392329460303) that was on reddit last week: &gt; Haskellians have assured me that type checking is sufficient double checking. I've already written programs that type check just fine and produce wrong answers, so pencil me in as skeptical. 
There are projects in ruby like http://www.ruby-toolbox.com which provide categorized projects, which provide at least some of that functionality. It helps since gem is pretty useless by itself for searching, along with Ruby's silly-name culture. I don't know how well it'd fit into haskell-land, but a similar tool may have a place.
I'd love to see where the applicants to this job are currently working.
I find list comprehensions cute when exploiting the failure semantics of the List monad, e.g., localMaxima :: Ord a =&gt; [a] -&gt; [a] localMaxima xs = [ y | (x : y : z : _) &lt;- tails xs, x &lt; y &amp;&amp; y &gt; z ] but I generally prefer the map/filter style.
I looked through the main game logic. It looks so clean and simple. Thanks for open sourcing it!
Brilliant: you have evidence to show that some people within a technical subculture are going to make a silly statement. http://1.bp.blogspot.com/-SWs3WT_vU5w/US0DGu9UhOI/AAAAAAAAGXs/-kqY44cDVHE/s1600/cat+duh.jpg
Looks pretty easy to use. Had a question. You're using a custom serialization typeclass here: https://github.com/bitemyapp/snmp/blob/master/src/Network/Protocol/Snmp.hs#L313-L316 Any reason you couldn't have used binary or cereal? I actually see this fairly often and while I know of some complaints about each library, the one-offs aren't usually much better.
Did darcs ever deal with those exponential blowup issues when a project got too big? I ask because it has been several years and I haven't payed any attention.
I highly doubt they would want that information to be made publicly available.
I'm just saying it depends on context (sometimes one is the "better", sometimes the other), and also on the person reading/writing the code. For example list comprehension is basically the same as the set notation in mathematics, so people used to that may find it very natural.
I've played around with a blog post idea for a while, that goes something like this. Every Haskell program has the same type: `IO ()`. Any program you can run type checks\*. And since all of those programs have the same type, they can at a type level obviously be swapped with each other. But hello world clearly has different runtime semantics than fire the missiles. QED type checking doesn't do anything, right? Obviously that argument is vacuous. But explaining *why* gives an insight into things for people unfamiliar with using a strongly typed language. The type system will only type check as much as you allow it. High level statements like "this does an action and returns nothing interesting" are clearly uninformative. But when you dig down a few layers, and get to meaningful types, *that's* when the type system starts paying its due. \* Barring use of `-fdefer-type-errors` of course.
It's obviously not safe against the string escaping from the block, but that's a more fundamental issue with the `with` idiom, which also plagues `withFile` (you can leak the `Handle` from `withFile`). The sole purpose of `withContents` is to make it easier to reason about when the handle gets closed, not to prevent the string from leaking.
&gt; The sole purpose of withContents is to make it easier to reason about when the handle gets closed, not to prevent the string from leaking. Could you give an example where `withContents` demonstrates easier reasoning versus, say, `readFile`?
Why would you think that would be difficult? There are a large number of very experienced Haskell devs out there. I've seen plenty of people with professional, academic, and/or hobbyist experience with Haskell.
I am mostly in the startup world in nyc. I've seen plenty of people with hobbyist and SOME professional experience, but I haven't meat too many gurus. That being said, there are meetups that I haven't gone to yet but always get messages for so I'm sure I could go there. I guess I was wondering what places are actively hiring haskell devs, because honestly I am interested in making a transition.
Given it's in Sunnyvale, which isn't that far from Stanford, and more specifically from Stanford Secure Computer Systems (group). I don't think they'll have any problems filling this spot.
There's an incomplete list at: http://www.haskell.org/haskellwiki/Haskell_in_industry That might be a good place to start.
Your best bet in NYC are quant. and hedge-fund shops.
Here's an example: http://stackoverflow.com/q/5053135/1026598 The OP doesn't realize that the handle is still open after `readFile` is done.
There is a site which has almost everyone who is anyone in the Haskell community, but I forget where it is. You can enter whether you are looking for work. Oddly, I ran across myself there, having forgotten I had created an account. Anyone remember where this is?
This a cool blog of this paper: http://www.cs.bham.ac.uk/~mhe/papers/exhaustive.pdf I really like this use of blogs where an author writes a blog of a fairly dense paper. There should be more of this. 
I don't need Put or Get monad here, new typeclass simple for my usage: https://github.com/bitemyapp/snmp/blob/master/src/Network/Protocol/Snmp.hs#L808-L826
http://www.haskellers.com/
haskellers.com?
So the SO link you gave recognizes that read-then-write to the same file is trickier than the different file case, regardless of language and OS. &gt; The OP doesn't realize that the handle is still open after readFile is done. This statement suggests that `readFile` always leaks handles and we know that isn't true. But returning to `withContents`, how would you use it to solve OP's problem?
If hackage would just allow inclusion of some javascript that could call out to an external API, then this could be solved (i.e. I don't understand the hackage philosophy of monolithism). I mean, for example adding disqus would be 3 lines of javascript. Linking to a dedicated reddit discussion per package would be another 3 lines of javascript.
It works in GHC because it applies type erasure to runtime terms. Probably other implementations also do this. OTOH, the compiler should really do this for you when safe. Did you have a look at the generated Core? See the `-ddump-*` flags in GHC.
If that's what they say then it's oversimplified. The thing you can't avoid with full dependent types is doing some evaluation/execution during typechecking - that's one reason dependently typed languages are generally designed so all functions terminate (or at least some easily identified set of functions). You still typecheck before compiling or running the program (so you actually need to evaluate terms with free variables during typechecking). edwinb's thesis is on efficiently compiling dependently typed languages, including how to erase some of the extra type arguments that it might naively look like you would have to pass at runtime (e.g, vector lengths).
Data with existential types are like black holes. Those types do not occur in the result type, so as soon as the "event horizon" closes (i.e. the value is constructed) there is no way to re-obtaining the type. Residual information may "leak out" however, given constraints. At the maximal end of the spectrum there is the (~) constraint, which leaks *all* information. A class constraint (like `Num`) allows you to use the corresponding method vocabulary on the existentially typed value. Consider: class OneMethod a where meth :: a -&gt; String data Foo where Bar :: OneMethod a =&gt; a -&gt; Foo All you can do with the argument of `Bar` after pattern matching on it is to pass it to `meth`. So a clever compiler, behind the scenes, should only store a `String` (thunk) in the `Bar` constructor instead of a `OneMethod` dictionary and an opaque value. Similarly for a fully existentionally typed constructor argument no bit should be wasted, and in a user-facing pattern match the value can always be rematerialized out of thin air as an `undefined`. I never came around to check whether GHC implements these tricks. Maybe someone could write a feature request.
Demonstrating the use of knowing multiple languages
I think the name is "fanboy." My take is Haskell's cursed with fewer of them than some languages out there (*ahem* Clojure), but the amount is not zero... Oh drat, I forgot my flame-resistant suit today...
Yeah I have to make the effort to go. I feel stupid for not going
&gt; Although it seems weird that all of my constructors end in Something blah, it seems to be the pattern of what free expects. I see. You eventually want to construct values of type `Free Something a`, which are built out of values of type `Something (Free Something a)`, so if you have `Something` constructors which produce a `Something Int` or a `Something (Maybe a)`, then you won't be able to use them to construct the `Something (Free Something a)` values you need. &gt; From my understanding, that basically captures the following monadic chain. That's right! `Free f a` represents a computation consisting of a sequence of `f`-steps and eventually returning an `a` as an `f` whose result is an `f` whose result is an `f` ... whose result is an `a`. But you can let `Free`'s implementation worry about that; let's focus on implementing `f`. Your `Something` is supposed to be an implementation of such an `f`, right? Your first constructor seems quite appropriate for an implementation of `f`. Because of their abstract names, I still don't know which monadic effects the constructors are supposed to represent, but clearly `ResultA` is an effect which receives an `Int` as a parameter, and produces an `Int` as an output. If it wasn't for the interaction with `Free`, such an effect would best be represented via the following type: ResultA :: Int -&gt; Something Int But in order to make `Something` work with `Free`, you are quite right, the output type must be `Something blah`. Hence the continuation-passing-style encoding, where `ResultA` is given an extra argument representing what the rest of the computation will do with the `Int` output, eventually returning some arbitrary `blah` value: ResultA :: Int -&gt; (Int -&gt; blah) -&gt; Something blah Your second constructor is a bit weird, but also makes sense. If we reverse the continuation-passing-style encoding, we get: ResultB :: (Num a) =&gt; Something a So `ResultB` is an effect which takes no arguments, and which is able to produce a number of type `a`, for any numeric type `a` chosen by the caller. Since you say that the type class you are truly interested in working with is `Serialize`, I guess this is supposed to be an effect which reads the next packet from the wire, and deserializes it into whatever format the caller expects, kind of like [`readLn`](http://hackage.haskell.org/package/base-4.7.0.1/docs/Prelude.html#v:readLn)? Anyway, on to the third constructor. Now this one looks very suspicious, because it is not using continuation-passing-style. `ResultC` is an effect which accepts an `Int` and another numeric argument, and then... never returns? How will you produce the value of type `blah`, if you don't have a continuation from which to compute it?
Yes (depending on your definition of "dealing with"): http://darcs.net/DarcsTwo
Scratch an itch of yourself. For example, I wrote a program that notifies me about new videos on Youtube playlists. Give it a list of playlists, get a list of unseen videos from those playlists. Definitely beats checking those playlists manually or subscribing to their creators and then wading through an enormous list of new videos, most of which don't matter to me. [Source for anyone interested.](https://github.com/KarolS/PlaylistSubscriber/tree/master/src)
There is a GHC ticket about implementing this optimization, http://ghc.haskell.org/trac/ghc/ticket/9291.
Great post, oldie but goodie. Is it possible to do the same thing in a total language like Agda? Surely function types in a partial language cannot have decidable equality, even if the type is as simple as () -&gt; (), so we need a total language to tidy things up. But then the "find" function itself must be equipped with an in-language proof of totality, which must proceed by induction on the modulus of uniform continuity of the function passed to "find". Is that even possible in a total language?
There's also Soostone and Skedge.me. And CapitalIQ has a group with a number of noteworthy Haskell people (although they mostly do Scala and Ermine afaik).
It is a challenging course, but you will learn heaps. i have started it and knowledge is rising rapidly
Our next meetup is this Friday 7pm at Pivotal Labs.
If you show up to the meetups you'll meet people from other companies too! And then there are also other people I occasionally meet who don't show up to the meetups but also are doing this or that in NY. It is true though that the big growth in Haskell popularity (relatively) is fairly recent, so finding intermediate devs is feasible but the _really_ experienced ones are now in somewhat shorter supply.
&gt; Basic Category Theory for Computer Scientists by Benjamin C Pierce. I think it is good for giving concrete idea for how Category theory can be applied. Often when I am reading a science/mathe/programming book I want to apply the ideas I am reading about to situations that I already understand and some that I don't but I have struggled with to see how it changes my understanding/perspective. That was difficult to do reading some of the category theory books since the examples they give were often about mathematical concepts that I did not have a firm grasp on. Benjamin C Pierce's book can given you a few examples to start off with plus a little more.
I understand the point is to connect this to matrix multiplication, but it would simplify this greatly if you represented `mapT2` as taking a list of functions of type T -&gt; T_i_, which would be like a lens-style Getter. You would need "knowledge" of how to deconstruct the type constructor in those functions, but you could avoid this if you want to by using a Church encoding for the product. For me at least, using subscripts for "field" instead of just saying "type field" and "algebraic field" made it quite a bit harder to read. Also, a semiring is probably closer to what you want than an algebraic field since it doesn't need to have inverses. In fact, since you are discussing product types and not product types together with sum types, a monoid might be even more appropriate. A monoid isn't necessarily commutative which is good as well, since product types and sum types are not exactly commutative, but only commutative up to a unique isomorphism.
Prima facie, I don't see why withContents file (writeFile file) is more obviously incorrect than readFile file &gt;&gt;= writeFile file Both require the same understanding of the intricacies of lazy I/O. And both are equally easily committed to memory as no-no's. Do you have an example where `withContents` is clearly superior because it works (unlike `readFile`) and it works because the handle gets closed properly? 
Conceptual Mathematics is good if Awodey seems a bit much. If it's been a while since you've done much in the way of rigorous maths, ,or if you don't have much abstract algebra (partly for practice, partly since it can make some category theory examples easier), or if you want to take your time, I often recommend: * How To Prove It by Velleman * A Book of Abstract Algebra by Pinter Awodey is pretty great. I haven't gotten to Mac Lane yet - one day...
A wild NPlusKPattern appears!
i particularly enjoyed that McKenna talk! Thank you for sharing. :-)
The first example is obviously incorrect *even if you use strict IO* because you are simultaneously reading from and writing from the same file. That's why I prefer the first example.
and richard is going to give an AMAZING talk
This one weird statically typed pure language doctors don't want you to know about!
I don't think there are really. Or I've never seen any such claims. Haskell has a reputation for being correct and like any defining feature of a language it will be taken to absurdity as a post-hoc rationalization for dismissing the language, it's not a unique Haskell phenomenon. There are similar myths in other communities (Erlang, Scala, R, C++) that need to be dismissed all the time. Pretty much nothing we can do about it other than get more people to take the plunge and try the language. As far as having mythical reputations go though, having a reputation for being for concise and correct isn't all that bad even if it does occasionally disappoint a few novices!
What's so hard about decidable equality of function types in a partial language? Take Haskell for example. Ignoring unsafeIO/unsafeCoerce, you just have to remember that all types are inhabited by \_|\_, and treat a partial function as though it were a total function where the "unhandled" catch-all case produces \_|\_. (You also have to pretend like you can't distinguish one \_|\_ from another: no peeking at error messages or whatever.)
Martin Escardo's subsequent blog post [Seemingly Impossible Constructive Proofs](http://math.andrej.com/2014/05/08/seemingly-impossible-proofs/) replays these constructions in Agda, together with their correctness proofs.
&gt; Perhaps the most surprising lesson is: while the popular opinion says that the main reason to pick Haskell is "to have fun hacking," in fact it's not so fun to hack in Haskell because there's not such much hacking to do at the first place. Having dealt with "I can't believe it's not C++11" the whole morning, that just made my day :)
I was tempted to download and run the code, *just* to remove that :)
In my experience, garbage collection can make structures contiguous. The first example, linked list, is not that bad if next element is adjacent to current. Which can be the case for copying garbage collected runtime systems. Even van Emde Boas layout is possible in first approximation.
Thanks, /u/dstcruz.
Thanks Roman for posting this link, that's a nice talk. While the situation is slightly more nuanced in our case, a lot of what Chandler says applies to Haskell and our code, and underlies many advices that you can see or hear in articles/talks about efficient Haskell code.
\_|\_ can't even be identified as \_|\_. So, how could I determine if \_|\_ = \_|\_ ? Determining if a function is extensionally equivalent to `const (fix id)` is equivalent to HALT.
Oh right, my bad. I forgot \_|\_ can't be distinguished from anything else.
That's an excellent talk, thanks for posting!
I believe that the GHC GC will have good data locality upon allocation, since it does pointer bump allocation, but the locality isn't necessarily preserved on collection.
Did you mean "demonstrating the absurdity of not using english on the Internet"? Don't get me wrong, I'm french too. But when it comes to the internet and science, everything should be in english, I think.
Thank you!
Nice talk! I'll have to try out nix[ops] and see how it compares with Vagrant+Puppet.
&gt; The only point you're missing is that with lazy IO readFile gets reading and writing the same file wrong. If you look earlier in the thread http://www.reddit.com/r/haskell/comments/2jvc78/simple_haskell_irc_client_in_two_lines_of_code/clh8jjm "lazy IO readFile gets reading and writing the same file wrong" was pre-mentioned. The thread started with the idea that `withContents` has the benefit that it solves some but not all of the issues with lazy I/O: http://www.reddit.com/r/haskell/comments/2jvc78/simple_haskell_irc_client_in_two_lines_of_code/clg068m So far all we've seen is that `withContents` is better via negativa, that it's more obviously wrong in certain places where it better make itself scarce. Surely you have an example that has `withContents` in it, as opposed to without it, that puts the function in the best light? The type signature, which you may recall, is withContents :: FilePath -&gt; (String -&gt; IO r) -&gt; IO r 
&gt; but the locality isn't necessarily preserved on collection. Is there anything in particular about GHC's GC that you're thinking of that can tend to frustrate maintaining locality? My rough/probably-wrong mental model of a generational copying collector seems like it would tend to preserve or increase locality.
Right, that's how I was interpreting it. Doesn't that imply that if your program makes impossibly-perfect use of cache it gets at most 2-4x faster? EDIT: not that that's not great, but I'd say that deadlines are the root of all performance evil :)
&gt; So far all we've seen is that withContents is better via negativa, that it's more obviously wrong in certain places where it better make itself scarce. Yes, this is exactly what I'm trying to say. I'm not making any stronger argument than that.