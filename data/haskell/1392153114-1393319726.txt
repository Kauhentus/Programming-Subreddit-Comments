It seems like you should be using Idris or Agda, as those are more focused on mathematics. In Haskell, you don't do mathematics*, you make programs. * except when you do, but... yeah, you should use a dependently typed language
Um, I never claimed usefulness, to quote myself "I'm not saying this would be useful; just that I wondered whether or not it was doable or to what extent." Also, I'm not exactly well versed in theory (while not an excuse, I'm still working on my bachelors degree at a school that isn't terribly theory oriented.) Haskell happens to be a relatively easy and straightforward language to me (or at least up until I get to creating monads, which I haven't actually managed to integrate into my design process yet.) Since Haskell is also one of the more expressive languages I know and this subreddit happens to be frequented by smart people, I decided the worst that would happen is I'd get a bunch of pointers on theory and such.
Why do you assume he's getting paid to work in Haskell? He only said that one of the client's projects was in Haskell, and his consulting page suggests that even if he's working on that project he might be doing statistical analysis for them rather than programming. In the context of the post, maybe he just went "Someone using Haskell for real? Looks like it's finally time to read up on it".
Fixed thanks :)
Ignoring the monad transformers bend for a moment. If we take regular function composition and apply it to type constructors we get STM ∘ IO = \ a -&gt; STM (IO a) IO ∘ STM = \ a -&gt; IO (STM a) Also by this interpretation, State ∘ Either doesn't make any sense as they both have 2 parameters and there is no definition for a composition of 2 functions with 2 parameters (so like wise no definition for a type constructor of 2 parameters.) State s ∘ Either a would make sense though. That said, I don't really know what any of this would imply as far as characteristics of the result (or if this interpretation would even make sense in any situation, haskell or otherwise.)
&gt; Everyone would say that it’s important for their software to be correct. But in practice, correctness isn’t always the highest priority, nor should it be necessarily. As the probability of error approaches zero, the cost of development approaches infinity. You have to decide what probability of error is acceptable given the consequences of the errors. Then FP is probably not where you should go honestly. Where I see it, there are two main reasons to use FP. - FP tends to contain fewer bugs (referential transparency) - Code re-use and code maintainability, it is easier to compose software written in a functional style from re-usable individual parts. (higher order functions) If neither of those are your interest you probably shouldn't use Haskell. Another thing is that Haskell obviously keeps the cost of correctness low. It's also false, it speeks of that the probability of error can only approach zero, it can actually reach it, there is software out there which has been formally proven correct and it didn't take infinity dollars to make. 
going to make a separate package for that :)
I was hoping someone would explain what Set is in relation to sorts. Does Agda have a hierarchy or sorts Term | Set | Set1 ...? 
The author made a post here when the library was created, but there hasn't been anything since. http://redd.it/1aazok Edited: Oops, I see that it's not abandoned on GitHub.
There was an announcement post by the author, but there hasn't been anything since. http://redd.it/1aazok
In Agda, we don't usually use "sort" in the sense of "syntactic category". Stuff left and right of the (single!) colon is in the same syntactic category. A "sort" is a set of sets or a "universe". Boring ordinary sets live in Set, but sets whose elements involve things in Set live in Set1, and in particular, Set : Set1. Set1 : Set2, and so on. The sort of a function type (x : S) -&gt; T is the max of the sorts of S and T. Some systems have "cumulativity", being a form of subtyping which embeds Set silently into Set1, Set1 into Set2, and so on. Agda doesn't. However, Agda does allow "universe polymorphism", so you can say "every level is equipped with such-and-such". I'm not completely enamoured of the way Agda manages this hierarchy, but what's good is the way that you don't need a new language to program at a new level.
Comparing your first example: {-# LANGUAGE KindSignatures, GADTs, EmptyDataDecls #-} {- No DataKinds -} data Black -- This is what EmptyDataDecls allows, data Red -- types with no constructors data Tree :: * -&gt; * -&gt; * where Leaf :: Tree a Black NodeR :: a -&gt; Tree a Black -&gt; Tree a Black -&gt; Tree a Red NodeB :: a -&gt; Tree a c -&gt; Tree a c' -&gt; Tree a Black with your second example: {-# LANGUAGE KindSignatures, DataKinds, GADTs #-} data Color = Red | Black data Tree :: * -&gt; Color -&gt; * where Leaf :: Tree a Black NodeR :: a -&gt; Tree a Black -&gt; Tree a Black -&gt; Tree a Red NodeB :: a -&gt; Tree a c -&gt; Tree a c' -&gt; Tree a Black I was puzzled because the first uses type names in the GADT signatures (Tree a **Black**, Tree a **Red**) but the second uses value constructors instead, which (in all of my limited experience with GADTs) is illegal. When using DataKinds like this, you can use value constructors from the relevant type within the GADT signatures, because DataKinds endows the value constructors with kinds. That is perhaps the entire point of the article, but it wasn't obvious to me, and (unless I missed it) was not explicitly pointed out in the text.
Why do so many people follow him? How do you even get that kind of exposure :/ 
In fact, SSRG at Nicta estimates that they could do functional correctness verification for half the cost of existing high-assurance software development, which the Pentagon puts at $1k per line of code. Our current research direction is to make verification of certain types of systems software _so cheap_ that it's competitive with _low-assurance_ development - It won't make sense _not_ to verify it.
It looks like the binary distribution does a nice job making a ghc-7.8.20140130 file to keep everything separate from my existing install. How do I go about using cabal and the like with the new ghc? I've done some googling and it doesn't seem obvious, or maybe it's just too obvious.
STM ∘ IO and IO ∘ STM are applicative functors, but not monads, as far as I know.
Bought! It's so rare that we have a new book on Haskell. 
Oh thanks! Yeah, looks like I actually read that very thread :)
Thanks! I think this example is considerably better at illustrating your point. The way I would word this problem is that in the parser: let parser = (,,,) &lt;$&gt; zoom (abiso . Pp.splitAt 1) Pp.peek &lt;*&gt; zoom (Pp.splitAt 3) Pp.drawAll &lt;*&gt; zoom (abiso . Pp.splitAt 2) Pp.drawAll -- * here &lt;*&gt; Pp.drawAll In `zoom (abiso . Pp.splitAt 2) Pp.drawAll` abiso is not delimited by `Pp.splitAt 2`. Which is not very very satisfying. You can delimit abiso with a lens on the left however like: splitAfterN i predicate k p0 = fmap join (k (to (i - 1) p0)) where -- to :: Monad m =&gt; Producer a m x -&gt; Producer a m (Producer a m x) to i p = do x &lt;- lift (next p) case x of Left r -&gt; return (return r) Right (a, p') -&gt; if not (predicate a) then do yield a to i p' else if (i /= 0) then do yield a to (i - 1) p' else yield a &gt;&gt; return p' let parser = (,,,) &lt;$&gt; zoom (abiso . Pp.splitAt 1) Pp.peek &lt;*&gt; zoom (Pp.splitAt 3) Pp.drawAll &lt;*&gt; zoom (splitAfterN 2 (==AEnd) . abiso) Pp.drawAll -- * here &lt;*&gt; Pp.drawAll and achieve the same behavior as the conduit code. It is not satisfying because both `splitAfterN 2 (==AEnd)` and abiso need to have code that handles the `A` stream termination `AEnd`. Edit: As noted below I made a mistake in my code above the parser should be: let parser = (,,,) &lt;$&gt; zoom (splitAfterN 1 (==AEnd) . abiso) Pp.peek &lt;*&gt; zoom (Pp.splitAt 3) Pp.drawAll &lt;*&gt; zoom (splitAfterN 2 (==AEnd) . abiso) Pp.drawAll &lt;*&gt; Pp.drawAll 
It was updated three months ago on GitHub. And the version was also moved to 1.0.0. While the package might not have been updated the software certainly seems to be in development.
It's definitely not abandoned. I've been coordinating with him on `mmorph` over the last couple of months to add some instances he needed for his library.
Thanks! I never knew about that flag.
*lens* is the new "dawg." Dibs! ;)
Oh, I think I see. Feel free to correct me if I'm misunderstanding you (though you could also ignore me I guess, you've been awfully patient already.) The problem isn't so much composing monads but that there isn't a function that can utilize the different distributive properties that arise in each monad. So for all combinations of monads m1 and m2 you'd need a different definition each(/most of the) time.
Well, it is more that composing monads as functors doesn't yield a monad that has the properties of each, and that there isn't another notion of monad composition that always works. A lot of people (myself included, once upon a time) have come along and tried to whack it with lots of algebra to make a notion of composition that works, but invariably in a lazy setting the resulting construction is often "too big" in that it contains stuff you don't want and "too small" in that it omits many things we can talk about as monads/monad transformers.. often both at the same time regarding different examples.
There was a bit of discussion in [this thread](http://www.reddit.com/r/haskell/comments/1n7mid/arguing_against_monad_transformers_for_the_right/ccg5i0r)
Yeah, thanks for helping me figure out something more concrete. It's actually possible to simplify `main` even further to demonstrate the issue (I've already updated the Gist): main :: IO () main = do let src = mapM_ yield [A, A, A, A, AEnd, A, A, AEnd, A] let parser = (,) &lt;$&gt; zoom abiso Pp.peek &lt;*&gt; Pp.drawAll res &lt;- evalStateT parser src print res Making the change you describe above doesn't affect the behavior of the code for me, it still throws an exception. Maybe I copied it over incorrectly? https://gist.github.com/snoyberg/8950411. I'd be surprised if altering anything but that initial `abiso` call has any effect. My understanding is that, once `abiso` returns a `B` value as a leftover, the *only* way pipes-parse can recover the `A` stream is to encode the entire `B` stream to `A`s, which requires encoding the entire `A` stream to `B`s. And once the entire stream is forced, the invalid parse would be triggered.
Hi, I'm the developer. It's definitely not abandoned! The next version is very nearly ready to release, it has loads of improvements. I'm in the middle of rewriting the documentation, but it's very tedious and I just got a bit burned out from it. I had to take a break from it for my mental health, but I'll continue when the time is right.
Tried it out, I like the initial search results, but the web view clicking through to hackage was off-putting.
&gt; Making the change you describe above doesn't affect the behavior of the code for me, it still throws an exception. Maybe I copied it over incorrectly? No my mistake. I did not include the `splitAfterN 1 (==AEnd)` lens needed for Pp.peek. I was working with two pieces of code and pasted the wrong one. let parser = (,,,) &lt;$&gt; zoom (splitAfterN 1 (==AEnd) . abiso) Pp.peek &lt;*&gt; zoom (Pp.splitAt 3) Pp.drawAll &lt;*&gt; zoom (splitAfterN 2 (==AEnd) . abiso) Pp.drawAll &lt;*&gt; Pp.drawAll again the same change in the updated example: main = do -- this is the code walked through below let src = mapM_ yield [A, A, A, A, AEnd, A, A, AEnd, A] let parser = (,) &lt;$&gt; zoom (splitAfterN 1 (==AEnd) . abiso) Pp.peek &lt;*&gt; Pp.drawAll res &lt;- evalStateT parser src print res `Pp.peek` needs to be delimited just like `drawall` does if you do not want it to act on the full stream. &gt; the only way pipes-parse can recover the A stream is to encode the entire B stream to As, which requires encoding the entire A stream to Bs. And once the entire stream is forced, the invalid parse would be triggered. This is not the case. With the code above: The first 5 values of src are read and converted to `B 4` `B 4` is undrawn and push back on to the stream as a stream of `A`s terminated by `AEnd` The entire stream is put into a list by `Pp.drawAll` `(Just (B 4),[A,A,A,A,AEnd,A,A,AEnd,A])` is returned and printed The trace is just: ("btoas",4) (Just (B 4),[A,A,A,A,AEnd,A,A,AEnd,A]) Only the peeked at value needs to be converted back to it's `A` form after being peeked at in it's `B` form.
Thanks for explaining. I, too, was puzzled.
I also pointed out this solution to him in private correspondence as well two days ago. I wish he would have at least mentioned it in his post. I agree that it is not the ideal solution, though, and I hope I can reuse his idea of two separate stacks (i.e. undrawn values vs. remaining stream) to fix this. Another solution I'm exploring is using rewrite rules to remove wasteful round-trips, but that is more tricky, mainly because the rewrite rule system is a bit fragile.
EmptyDataDecls is in haskell2010
This sentence says that the sort _which classifies types_ is unique, but doesn't say there are no sorts which don't classify types.
Sorry, I should have been more clear. I meant *with the original code I wrote*, that was the only way for pipes-parse to address the issue. You're correct that there's a way to work around that limitation. My recommendation in the blog post (keeping a separate stack of leftovers) would avoid the need to use such workarounds.
While not having much Haskell experience he seems to be a rather knowledgeable person otherwise - sometimes languages are just tools ;)
I upvoted this link not because of what the blog author writes - you're right, there's isn't much there - but because of the interesting discussion in the comments.
&gt; The pipes package contains a module Control.MFunctor which exports the following class: I think the `hoist` function is defined in the highly useful `mmorph` library, not in pipes
Yes, I'm very excited about this work for that reason.
MFunctor was still part of the Pipes package at the time that was written. It was separated out days later, which may not have been a coincidence.
This is a bit OT, but can anyone tell me where this use of *..., Oh My!* first appeared?
Thank you very much for trying it out. Could you elaborate on what in particular was off-putting about that so I can try something different and see how it goes ?
I guess this would be the obvious one, modulo what the parameters to `ops` should be: newtype VLMonadT ops m a = VLMonadT { runVLMonadT :: forall t. (MonadTrans t, Monad (t m)) =&gt; ops (t m) -&gt; t m a } instance Monad (VLMonadT ops m) where return a = VLMonadT (\_ -&gt; return a) VLMonadT m &gt;&gt;= f = VLMonadT (\ops -&gt; m ops &gt;&gt;= f' ops) where f' ops a = runVLMonadT (f a) ops instance MonadTrans (VLMonadT ops) where lift ma = VLMonadT (\_ -&gt; lift ma) Edit: changed my best guess what the `ops` parameter should be.
This originates from the movie "The Wizard of Oz", where at one part they say "Lions and tigers and bears, oh my!" a few times.
The [quote comes from The Wizard of Oz](http://www.youtube.com/watch?v=Etx-nDCZzLo) though has been used in the literature before in Brent Yorgey's [_Species and Functors and Types, Oh My!_](http://www.cis.upenn.edu/~byorgey/pub/species-pearl.pdf), which is probably the direct inspiration for its use here. Apart from its general appearance in popular culture (eg, "He's on safari. The lions, the tigers, the bears... oh, my!" from Predator 2) I don't know if it gets used anywhere else in technical literature. Maybe /u/byorgey can say why he chose the title in 2010.
You should get some sort of prize for your Haddock documentation. It's magnificent!
Just arrived. I don't have an opinion about the content yet, but have to say the layout.. it is terrible. Margins are maxed out, and the typography makes the whole text seem just flowing and flowing without segmentation. I hope the content would justify.
There is no need for you to pass around 4 arguments to the `paths` function, try just passing 2 (the number of squares the robot needs to go right and the number of squares the robot needs to go up). There are three cases that you will need to handle: * The robot can not move up any more (i.e. `m` is 0) * The robot can not move right any more (i.e. `n` is 0) * The general case, where the robot needs to move `n` spaces up and `m` spaces to the left. The third case is obviously the most difficult, but you can just use exactly the same method that you did in the C program. Although it works fine here, I wouldn't normally recommend converting programs from C to haskell like this. The easiest way to write a program in C is almost never the easiest way to write a program in haskell.
Could you provide the input file you were using for testing that caused the `conduit` program to throw an exception? I'm [working on improved decoding functions](https://groups.google.com/d/msg/streaming-haskell/Td5WTGfVYok/Msrmv-X0nkwJ) and I'd like to test this case, both for correctness and performance.
Sorry I deleted my last reply, I misunderstood what you wanted to do. Anyway, it's important to realize for this problem that 'being at point x,y on an w by h board for this problem is the same thing as being at the corner of an w-x by h-y board'. What you basically want is to reduce the problem to first finding a base case and then a recursion case. The appraoch I would take is simply constantly slicing off a row of the board if you will. That means the base case has zero rows, in which case there are no possibilities, so we get: paths (0, _) = 0; The recursion case is more difficult we have: paths(h, w) = ... And we want to define this in terms of paths(h-1, w) in some way, as in, if we have an (h,w) board, how much more possibilities does that give than an (h-1,w) board. The width w. Say we have a 6,4 board and the robot is at point 0,0 of course, you want basically for each position (n,4) then add the paths (n-1,4). So in the case of 6,4 the answer is paths(5,4) + paths(4,4) .. + paths (0,4) + 6. Now capture that principle into a general recursion case. Tip: It helps to use something like `sum $ map &lt;function&gt; [1..h]`, yours is to find that function.
http://lpaste.net/99813 
Wow. This looks very interesting. Especially since more and more apps just need "fast+reliable JSON&lt;-&gt;DB middleware". And if a particular case does not fit the DSL you have Haskell/Yesod to drop down into. Thanks! 
Thanks for the reply. I was aware that Brent used this (and I think if seen it a number of times since then), but I guessed that it came from somewhere else originally. Just the answer I was looking for, thank you very much ;) 
The second one takes longer to mentally parse for me personally. In the first one I can quickly see whats happening and all operations are close to each other.
 S.writeFile "encoded.pharse" $ toByteString (mconcat $ replicate 1000 (encodeFiles input)) &lt;&gt; S.pack (replicate 100 250) Note that the conduit parser does not manage to parse the whole sequence of files. In my case it breaks with a text exception about 100 files before the end, sort of like so: \NULU\NULT\NULF\NUL-\NUL1\NUL6\NUL"} conduit: TextException Cannot decode byte '\xfa': Data.Text.Encoding.decodeUtf8: Invalid UTF-8 stream As you can see it has just parsed the "utf16.txt" file in the middle of the repeated sequence of three: where it breaks evidently depends on where the nearest break in between ByteString chunks happened. The pipes one renders exactly the valid files and no more.
Wow! Fascinating. I'd love to see this reviewed by somebody who groks monad transformers. (Not me, although I have managed to cobble together one or two programs that use them. I might have a go at porting them to layers, in which case I'll let you know how it went.)
Almost character for character what I came up with. The does fall over rather quickly for large N since it ends up doing lots and lots of repetitive work due to the nature of the dynamic programming solution. My usual weapon against this in haskell is to throw a memoization combinator at it. Is this the usual idiom for most people or is there another way? 
The program I link makes a particular decision that pertains at the same time to correctness and optimization. The number 50 can be changed to 500 and the program is still faster than conduit, and it is still more correct. My first pipes version took 5 minutes to write and was visibly more correct than the conduit version in that it correctly handled bad bytes -- just what the conduit version claimed as its advantage -- but was distinctly slower. So I had to reflect on how to apply the parser lens for utf8 Text, not having used pipes-parse before except to read the tutorial. Not to have a restriction in this position (or, better, a more complicated one, of course), would be a dangerous programming error, since the parser (yours for example) would accept a handle sending an infinite sequence of 0's as the file name length without returning a file length much less a file. It is this consideration that led me to chose 50 and to put it in this position; again, obviously this was a convenient place to position the use of decodeUtf8; if your operating system permits files that have 1000 character names, I can of course adjust for that. You can see the defect in the number parser for conduit program, if you change 'Int' to 'Integer' throughout and feed the program a file that is just an immense succession of '1's. It promptly exhausts available memory calculating the length of the first filename. I emphasize again that the first naive pipes version I wrote was, unlike the conduit one, perfectly correct -- that is, it actually had the behavior the conduit program aspired to, but failed to achieve. It was just slower. You seem to shift back and forth between considerations about correctness and error handling, and considerations about performance. 
Heh, from the link: "[T]o do so you must use -XUndecidableInstances to signal that you don't mind if the type checker fails to terminate."
This is the amount of paths to reach the top corner (how I mistook the quaestion at first as well), what OP wants to know, or at least how OP technically phrases it is that it's also possible to stop in the middle, a (2,2) Grid for instance should have 5 as answer, not 2. Namely: - The robot not move at all - The robot can go left - The robot can go left, and then up - The robot can go up - The robot can go up, and then left
This seems to be a classic dynamic programming problem. Efficient solutions are easy to implement in imperative languages (not that you did the dynamic programming version). I still haven't exactly got the hang of how to do it in haskell. 
You are correct. The fix would be to count the sum of paths that reach all in between points: stop_grid n m = sum [grid1 x y | x &lt;- [0..n], y &lt;- [0..m]] 
I agree. This is why I'd prefer to see if the double stack solution works.
Actually, it was separated out because Michael wanted to reuse it for `conduit`.
That's a pretty complex solution though, what's wrong with (__OP DON'T LOOK__)?: paths :: (Int, Int) -&gt; Int; paths (w, h) | w &lt; 1 || h &lt; 1 = undefined | w == 1 = h | otherwise = sum $ map (\y -&gt; paths (w-1, y) + 1) [1..h] ; I basically cut off a column of the board until I arrive at a (1,h) board in which case there are h options.
Your C function is actually a pure function, but you have just mis-translated it. Try rewriting the C function first so that you do not explicitly initialize c1 and c2 at the top. Now, you will have two local variable bindings; these translate to Haskell as either a let-expression or where-clause. Also, note that your C function in the non-base case returns the result of an expression combining the recursive forms, but that your Haskell version does not. Recursion only helps if you actually accumulate a result while traversing. :)
I think I agree with Platz. Specifically for me, if you were to search for something in prelude (which has a tremendous amount of stuff), the app quickly becomes a web browser, looking at a huge page, instead of the function originally searched for (but its nice that it jumps to your function). Something I think that would neat is to parse the specific area for the function out, and display only it, then in an options menu, have an "open in browser" button that will go to the original page.
You could sprinkle only at critical points, where the subdata is possibly large enough to matter. Possibly you would know what units of data are computed in separate threads (and thus candidate to rnf-ing) and wrap those.
&gt; If neither of those are your interest you probably shouldn't use Haskell. I have to disagree here. There's at least one more point: Development time. I've only been working with Haskell for a little less than a year now, but I've found it to be extremely well suited to cut down on development time. Higher order functions for instance help immensely to encapsulate common patterns, which results in code that is more concise, often obviously correct and written much quicker than the imperative equivalent. There are also many other features in the language that support rapid development times.
Interesting! I think your version is computationally much more efficient than my version. I based my approach on something that can easily be extended to higher dimensions: cube x y z = fac (x + y + z) `div` (fac x * fac y * fac z) paths x y z = sum [cube a b c | a &lt;- [0..x], b &lt;- [0..y], c &lt;- [0..z]] 
&gt; Of course the conduit program as written isn't perfect, it's not meant to be a perfect program. It's not that it's not perfect. It's wrong. It breaks from a text encoding error at an arbitrary location determined by bytestring chunking. If put a chunk-length-randomizer on the producer, I can get it to break just about anywhere, but the pipes version will keep giving the same result. Try it. &gt; If you're curious as to why the pipes version is faster, it's most likely because it's taken advantage of a new UTF8 decoding API just released with text 1.1 No, my version of `pipes-text` is built against `text-0.11.3.1` from a year ago. If you don't care about performance for the present case, you can scrap the bit that says `splitAt 50` and cease to complain about 'different work'. The ratio between the performance of the result and the performance of the conduit program, will be as the performance of the conduit program is to the present one. The point about 're-encoding streams of data multiple times' is up to the programmer. It took me a few tries to see how this worked not having worked with this pipes-parse stuff til yesterday. In your alleged 'real world example' there really does turn out to be nothing in it; this is my point. It is quite obvious, on reflection, that a million `zoom decodeUtf8 drawChar` s in a row is a terrible idea with this particular interface. Where it looks like that is going to be necessary, you should switch to another approach. Or you can organize them rationally, thinking about what is happening. Of course that might not be possible. It is clear that these lens parsers don't do everything one could want them to do. 'Real' attoparsec parsers are also available, to mention just one possibility. But in the present case the lens-parser interface does fine, as the conduit machinery apparently doesn't -- as you seem to have to admit of the present program where you strike the `splitAt` that is now offending you. They claim that "this isn't just a performance regression, but changes behavior" would be credible if you had a correct conduit program for this goofy file format that I couldn't match. This is perfectly possible, but you keep not bothering to produce one. 
Well, this is part 2, re-usability. The reason FP saves on development time is because you are more effectively re-using a standard library. There are many common patterns in imperative code as well but without higher order functions it becomes difficult to capture those common patterns into a re-usable form.
Oh, and here i got all excited because I thought it would be something to spare me the pain of writing Javascript by generating it.
Oh, I see. Thanks :)
Hmm, I tried to create a version in Idris which uses dependent types to create a higher order function that takes the function as dimension `n` as inputs and outputs the version of dimension `S n` but I can really get it to work.
I have to disagree with almost every part of this response. It's not necessary to add a parameter to express this function in a natural, idiomatic way in Haskell; and I think trying to turn this into a fold would result in an unreadable mess. The answer he posted is actually quite close to the way I would translate the given C algorithm in Haskell -- he just needs [a few hints](http://www.reddit.com/r/haskell/comments/1xp7j3/how_can_i_write_this_program_in_haskell/cfdd0b3) on how Haskell behaves. It's true that there are better algorithms around, though.
Thank you for the feedback, I'll give your idea a try and see how it feels.
Sorry for not being clear on what the problem is about! I am actually counting the number of complete paths from (1,1) to (n,m)
The conduit program itself isn't the problem, it's the `Data.Conduit.Text` decoder. I acknowledged that it's buggy, and that's exactly why I'm referring to the text issue about provider a better decoder. pipes-text doesn't depend on text 1.1's *export* of the decoding function, it includes the implementation itself. You can see that from the source code itself: https://github.com/michaelt/text-pipes/blob/master/Pipes/Text/Internal/Decoding.hs#L5. If you build conduit from Git, this decoding issue is resolved, but I haven't released it yet since (1) it still needs to be optimized, and (2) I want to test it more before releasing. If you're convinced that there's no point at which this double-encoding issue would ever show up in practice, that's fine. The [example Tekmo gave above](http://www.reddit.com/r/haskell/comments/1xmmtn/some_ideas_for_pipesparse/cfdbvsu) of idiomatic usage does actually run into the double-encode issue, but maybe you'd say the right approach is just to restructure the program to avoid leftover preserving. Personally, I'd rather just make a tweak to the library to make that behavior less likely to occur.
Your `nextRow` is also just `scanl1 (+)`, and `pathCounts` can be written with `iterate`. Edit: Though, actually, the way you have written it is nicer in a sense because it generalizes more easily to other recurrence relations.
Oh, in that case my original answer to you which I deleted was correct and my answer is incorrect. In that case it is markedly more simple: You can still reduce it to a base and recursive case: base case: paths (1, m) = 1; The recursion of paths (n, m) is simply m added to the amount of possibilities in paths (n-1,m) of course. The solutions posted here are actually hypercomplicated by the way. I will reply to my own post with the full solution as a demonstration to how much more concize functional programming can become. Do not read the reply to my own post if you don't want the full solution.
 paths (1, m) = 1; paths (n, m) = (m-1) + paths (n-1, m); Yap, the above does the exact same thing as your gigantic C code.
Oh, yes, the new conduit encoding module does look really good, if I follow. Note though that it avoids the text exception system entirely ... again, if I understand. It is thus implicitly already following the standard pipes-binary protocol of returning to the ByteString in the event of error. But I don't see that your 'double encode' issue can be anything but a performance issue unless you are trying to interleave something like text exceptions into the stream. Your complicated demo with A and B is, if I understand, trying to do something like that -- in addition to making the obvious point that lots of zooms in and out can be costly -- but I don't understand everything that you and others are saying about it.
I feel like the example that it walks the reader through is well chosen. I have run into problems like this without realizing a solution, so now I know what to do! :)
Actually, Data.Conduit.Text *does* throw exceptions. What I've done in that other module (Data.Text.DecodeStream) is just expose a lower level set of functions that don't use exceptions at all. The bug you're seeing in the released Data.Conduit.Text is that we're not anticipating all of the cases where `text` may generate an exception. Regarding pipes, let's see if I can summarize: * Double encoding is a performance concern. * If you don't have a law-abiding isomorphism, double encoding means that you might change your stream. * As an extension of the previous point, if you throw an exception on invalid data, then the extra encoding can cause the pipeline to break. * The standard approach in the pipes libraries seems to avoid that previous point, since exceptions aren't used. (I only learned that through the discussions in this thread, which is why my blog post was much more vague about how serious an issue this was.)
The "a-ha" moment for me was when I started to think through what the System F representations of Haskell functions must be. In System F you must manually apply types as arguments whenever you have polymorphism. We can think of these as another set of lambda binders while I'll denote with double-slash, `\\`. length :: forall a . [a] -&gt; Int length = \\a -&gt; \x -&gt; case x of [] -&gt; 0 (_:xs) -&gt; succ (length a xs) When we do this, the valid `applyToTuple` has a completely different value, not just type applyToTuple :: forall b c . (forall a . [a] -&gt; Int) -&gt; ([b], [c]) -&gt; (Int, Int) applyToTuple = \\b c -&gt; \f (bs, cs) -&gt; (f b bs, f c cs) Notice that we explicitly pass the `b` type and `c` type as arguments to `f :: forall a . [a] -&gt; Int`. If we use the Rank-1 type applyToTuple :: forall b c . ([a] -&gt; Int) -&gt; ([b], [c]) -&gt; (Int, Int) applyToTuple = \\b c -&gt; \f (bs, cs) -&gt; (f bs, f cs) Then we see that `f` no longer accepts a type argument and thus cannot be specialized to match the `bs :: [b]` and `cs :: [c]` arguments.
I'd say it _is_ one of the best solutions. I always make these things type synonyms and it can make things very readable. This goes to the point of having things like Width and Height going on which are basically just Int's.
(btw, type lambdas are usually written in ASCII as `/\` since it looks like Λ.)
Right, `Data.Conduit.Text` does include them, I anticipated that; I was talking about the decode functions inside the DecodeStream module. (I was starting from the text issue hadn't realized that `Data.Conduit.Text` imported it already) I compiled the conduit code with it, and it doesn't break, it just prints an extra newline at the end where the bad bytes were. I can't figure out what I'm supposed to do with the extra bytestring producer, but this is a pipes-v-conduit difference. Your summary of points somewhat multiplies them and makes them seem less obvious than they are. That zooming in and out and in and out is a 'performance issue' is pretty obvious once you use this system a little, as I finally did yesterday; that hidden exceptions are likely to invalidate all isomorphisms is also pretty obvious. I agree of course that the conduit approach might be a natural one to turn to when the lensified parsers are not reasonable. But in the present case, there's also e.g. attoparsec. 
A pragmatic approach to this can be found in this paper: http://dx.doi.org/10.1145/2430532.2364514, or talk: http://www.youtube.com/watch?v=-IZjuQNC_uk
mind == blown. That last line there (the definition of nextRow) is deeply magical to me at the moment. Can someone take a crack at breaking down what's happening there?
Regarding your edit: * Your `paths 1 1 = 1` case is redundant: it's handled well by the next two clauses. * `sum [a, b]` is probably better spelled `a + b` * It's a bit slower, but probably because you're computing with `Integer`s. So the Haskell program is doing more work (and perhaps giving a more correct answer) than the C program. It wouldn't surprise me if turning that `Integer` to `Int` (and compiling with optimizations) put the two programs at parity.
Oh, that's much nicer!
Sure thing. Let me expand the definition of `fix`, since that can be distracting: nextRow xs = let ys = zipWith (+) xs (0:ys) in ys So what we've got for `nextRow` is something that creates a list `ys` that is defined in terms of itself. Crazy, no? The trick is that it's not turtles all the way down. Each element of `ys` is defined in terms of `xs` and prior elements of `ys`. For convenience, let's use the notation xs = x0 : xs0 = x0 : x1 : xs1 = x0 : x1 : x2 : xs2 ... = x0 : x1 : x2 : ... : xN : xsN So nextRow (x0:xs0) = let ys = zipWith (+) (x0:xs0) (0:ys) in ys Expanding `zipWith` we get: nextRow (x0:xs0) = let ys = (x0 + 0) : zipWith (+) xs0 ys in ys or nextRow (x0:xs0) = let y0 = x0 + 0 ys0 = zipWith (+) xs0 (y0:ys0) in y0:ys0 We can expand `xs0` to get: nextRow (x0:x1:xs1) = let y0 = x0 + 0 ys0 = zipWith (+) (x1:xs1) (y0:ys0) in y0:ys0 And expand `zipWith` to get nextRow (x0:x1:xs1) = let y0 = x0 + 0 ys0 = (x1 + y0) : zipWith (+) xs1 ys0 in y0:ys0 or nextRow (x0:x1:xs1) = let y0 = x0 + 0 y1 = x1 + y0 ys1 = zipWith (+) xs1 (y1:ys1) in y0:y1:ys1 Iterating these alternating expansions we get: nextRow (x0:x1:...:xN:xsN) = let y0 = x0 + 0 y1 = x1 + y0 ... yN = xN + yN_minus_1 ysN = zipWith (+) xsN (yN:ysN) in y0:y1:...:yN:ysN So you can see that to evaluate `ys !! n` we only need to know `x0, ..., xN` and `y0, ..., yN_minus_1`.
I think this pattern match is redundant: paths 1 1 = 1
Yeah this is a nice way of looking at it. Somehow HM knows how to insert the type arguments automatically.
Cool. Thanks for writing all that out. I think a key for me is understanding `fix` as encoding recursion and not some (truly magical) way of getting the fix point for an arbitrary function. e.g. typing `fix $ \x -&gt; x^2 - 2` and wondering why that generates an exception rather that 2. I really do like how your solution generates all paths for arbitrary N and M and getting your answer is just indexing it out of the resulting lazy lists. I suppose my next (and less serious) question would be "How the hell did that solution come off the top of your head?!" edit: add missing words to make complete sentences
&gt; just £440 &gt; just okay then...
[does this help?](http://www.reddit.com/r/haskell/comments/1xp7j3/how_can_i_write_this_program_in_haskell/cfdgbld)
Wow, yes, you are completely correct, paths simply adds m-1 for every level. I love how everyone made super complicated solutions while the above simply sufficed.
Well, it helps that this particular solution is wrong ;) According to the above formula there's: * (3-1)*(3-1)+1=5 ways to get to cell (3,3) * (3-1)*(2-1)+1=3 ways to get to cell (3,2) * (2-1)*(3-1)+1=3 ways to get to cell (2,3) But 3+3=6, not 5. That is, I can get to cell (3,3) via cell (3,2) or cell (2,3), so there should be 6 ways to get to cell (3,3), not 5. The correct number of paths is calculated by the binary coefficient as /u/rule posted.
I guess the key (other than practice, practice, practice) to thinking this way is to invert the problem. Rather than "okay, to calculate `a+b`, I need to calculate `a`, then calculate `b`", think "how can I arrange this so that when I need to calcluate `a+b`, I'm given `a` and `b`?"
HM type inference doesn't really have to worry about this because the only place it allows type arguments is on top-level bindings, preceding all non-type arguments. Any variable not bound at top-level cannot have type arguments and any expression will have all type arguments applied implicitly based on how it's used. This is why, in vanilla Haskell, the expression `(id True, id 'a')` type-checks, but the arguably equivalent expression `(\f -&gt; (f True, f 'a')) id` does not. Avoiding the question of when and where to apply type arguments is one of the reasons HM type inference works at all.
What about let-generalization?
Uuum, we do have Unicode for 23 years now. It’s time to start using it. &gt; length :: ∀ a . [a] → Int &gt; length = Λ a → λ x → case x of &gt; [] → 0 &gt; (_:xs) → succ (length a xs) &gt; &gt; When we do this, the valid `applyToTuple` has a completely different value, not just type &gt; &gt; applyToTuple :: ∀ b c . (∀ a . [a] → Int) → ([b], [c]) → (Int, Int) &gt; applyToTuple = Λ b c → λ f (bs, cs) → (f b bs, f c cs) &gt; &gt; Notice that we explicitly pass the `b` type and `c` type as arguments to `f :: ∀ a . [a] → Int`. If we use the Rank-1 type &gt; &gt; applyToTuple :: ∀ b c . ([a] → Int) → ([b], [c]) → (Int, Int) &gt; applyToTuple = Λ b c → λ f (bs, cs) → (f bs, f cs) &gt; &gt; Then we see that `f` no longer accepts a type argument and thus cannot be specialized to match the `bs :: [b]` and `cs :: [c]` arguments. (I’m using the [Neo layout](http://www.neo-layout.org/). [The arrow is on the numeric keypad.] But leksah has support for displaying code like that too.)
Thanks! Your explanation is really easy to follow and engaging. Even for such a sunday haskeller as myself.
Regarding the kinding issue, we should distinguish between `|- f :: * -&gt; *` and `a::* |- f a :: *` (etc for higher arities). Given the former we can derive the latter, which is what's going on with `Either` and other proper type constructors. However, given the latter we cannot necessarily derive the former, which is what's going on with type aliases. If we had type-level lambdas, then we could abstract over the arguments to type aliases, thus deriving `|- (/\a. f a) :: * -&gt; *` from `a::* |- f a :: *`. And if we had eta-conversion at the type level, then we'd have that `f == (/\a. f a)` from which we could conclude that `|- f :: * -&gt; *` and `a::* |- f a :: *` are "the same". However, in Haskell we don't have type-level lambdas nor type-level eta-conversion; whence the complication of needing our kinding judgement to carry around an environment.
You can define a new monad if you want which is only output by defining output-only primitives in it. module O (oPutChar, O) where { data O a = secret (IO a); instance Monad O where { return x = secret (return x); secret x &gt;&gt;= secret y = x &gt;&gt;= y; }; -- define some primitive operations for it oPutChar :: Char -&gt; O (); oPutChar c = putChar c &gt;&gt;= return; --etc oToIO :: O a -&gt; IO a; oToIO (secret x) = x; } The trick is to not export the secret data constructor secret which is used only to define the primitive operations, you can now define oPutStrLn in terms of oPutChar for instance. To use it with main though you will have to use oToIO: main = oToIO $ oPutStrLn "hello"; There is one thing that is weird though, instances of the Monad typeclass have to have kind `* -&gt; *`, as in they have to be unary type constructors, but an output only monad conceptually can be a nullary type constructor. It's basically _always_ of the form `O ()`. We could theoretically define a new typeclass especially for such occassions which does not define `&gt;&gt;=`, only `&gt;&gt;`.
It's absolutely possible to create restricted versions of the IO monad that only permit a subset of IO actions. Real World Haskell has a section on it: http://book.realworldhaskell.org/read/programming-with-monads.html
Whoa. You wrinkled my mind. That being said, is that function fix efficient enough that this is like doing it the dynamic programming way? It seems a bit mysterious. It's defined this way: fix :: (a -&gt; a) -&gt; a fix f = let x = f x in x Which also kind of wrinkles my mind. I can't help but think that this approach is too easy to be efficient. It is really friggin' awesome though.
At the very least it needs to know what *order* to apply type arguments, but maybe since everything parametric anyway this is equivalent to a no-op. 
... and I keep agreeing with you. This looks like a nice tool, btw. Looking forward to finding an excuse to use it.
I haven't looked into `conduits` for a while, is there some way to recover the producer? With my bad bytes at the end, this for example doesnt yield any more bytestring main :: IO () main = runResourceT $ do (src,fini) &lt;- CB.sourceFile fp $$+ conduitPharse =$ CL.mapM_ (lift . print) (prod, fini2) &lt;- unwrapResumable src prod $$ CL.mapM_ (lift.print) fini2 finishes the same way runResourceT $ CB.sourceFile fp $$ conduitPharse =$ CL.mapM_ (lift . print) does. 
A) This is really nice work. B) We need to get source information into DWARF and a basic stack traced based on that going, so we can use low-overhead, off-the-shelf profilers for Haskell code.
This problem has a mathematical solution as `_jk_` said. If you want to get a very fast solution, you can just output that number, and make sure you solve this number very fast. 
yeah, basically the same concept except i'm using a proof object and typeclasses to construct it. didn't think of using functional dependencies.
Well, I have been experimenting with putting (heap) profiling results into the eventlog, albeit using code pointers. Going to, say, cost-centres from there should not be too hard. I think the most laborious part would be to build/adapt the front-end tools. I think I might be able to put some more work into this after we've wrapped up stack tracing.
[read on for an explanation](http://www.reddit.com/r/haskell/comments/1xp7j3/how_can_i_write_this_program_in_haskell/cfdohgt). It's just another way of doing dynamic programming. It works out to be O(n*m), as the dynamic programming solution should.
s/typeclasses/closed type families I is what I think you meant. I don't feel like thinking too hard about it, but I would be surprised if a proof object is needed. This type of stuff doesn't seem to catch on, but who knows maybe closed type families will change that. Also why the type annotation here: (length :: [a] -&gt; Int) ? 
Great post! I'm looking forward to your next one; I still don't understand that relevant section in *Lazy Functional State Threads*.
I already pointed out [here](http://www.reddit.com/r/haskell/comments/1xmmtn/some_ideas_for_pipesparse/cfdbvsu) that throwing exceptions in a lens is not idiomatic. If you want to include more detailed error information you include it in the return value. In the case of decoding isomorphisms, the most they will do when re-encoding is change chunk boundaries. The `pipes-text` and `pipes-bytestring` API explicity reserve the right to quotient chunk boundaries in their implementations for efficiency reasons.
Your use of semicolons and braces makes me nervous.
Thanks, I appreciate that! Your comment, and just the surprise of seeing this at the top of the Haskell reddit without me putting it there, has given me a good bit of motivation. It's great to see that people appreciate my work. I'm focusing at the moment on doing some web design work for a local social centre where I hang out, but when I'm finished that I think I'll try to finish the next release of `layers`. It's great knowing that other people are actually interested in that!
I was the person who wrote that comment. This is still very inflexible, and the required signatures kind of suck, but it is neat that it’s possible. For example, I don’t think you can write a function `swap` in Haskell such that `(f . swap) x y == flip f x y`. foo = (*) . pred . swap . succ -- foo 3 3 == 8 Whereas in Kitten you have multiple return values, so it’s straightforward: def foo (Int Int -&gt; Int): ++ swap -- * Other examples: def countWhere ([a] (a -&gt; Bool) -&gt; Int): filter length def val (Int Int Int -&gt; Int): + + ++ // FIXME: Way too cute. Not that it’s hugely important; it’s just a nicety I value. 
I'm not sure there's any likely situation where you would ever need to perform only input or only output. At some point along the way you need both, so why complicate things and separate them? What is gained? Edit: Not to mention that input and output streams are coupled pretty heavily at the system level. I could be totally wrong and making a wild jump to a conclusion, but my gut feeling is that something else in your design needs to be rearranged or refactored instead of considering that input and output should be separated. I don't very much experience in Haskell yet, but after all, couldn't you just create an interface to the I/O monad that only exposes the output? Then due to the static typing system you can be sure you are not creating unwanted side effects while not having to go nuts reinventing the wheel. Double edit: I have "over complication" radar because I do it all the damn time :)
You can model this using a `Producer` from the `pipes` library. Here's an example: import Pipes example :: Monad m =&gt; Producer String m () example = do let x = 1 yield $ "Log output #1: " ++ show x let y = someComputation 1 yield $ "Log output #2: " ++ show y someComputation :: Int -&gt; Int someComputation x = 3 * x + 1 The base monad of the `Producer` is polymorphic, so you can either convert it to a pure list: &gt;&gt;&gt; Pipes.Prelude.toList example ["Log output #1: 1","Log output #2: 4"] ... or you can loop over it using `for` to write the output to the console: &gt;&gt;&gt; runEffect $ for example $ \str -&gt; lift (putStrLn str) Log output #1: 1 Log output #2: 4 This lets you decouple your computation/logging logic from your choice of side effects.
i'm very much a beginner in Haskell, so i'm probably missing something important, but what about a writer monad like the [Control.Monad.Writer](http://hackage.haskell.org/package/mtl-1.1.0.2/docs/Control-Monad-Writer.html) class?
actually, by taking something from the polyvar-comp method, I adapted my approach to use functional dependencies class ComposableFunction a b c d n | b -&gt; c where proof :: Composable (c -&gt; d) (a -&gt; b) n instance ComposableFunction a b b c Z where proof = Match instance ComposableFunction b b' c d i =&gt; ComposableFunction a (b -&gt; b') c d (S i) where proof = Deeper (proof) (@.) :: forall a b c d. (ComposableFunction a b c d (Depth (c -&gt; d) (a -&gt; b))) =&gt; (c -&gt; d) -&gt; (a -&gt; b) -&gt; Comp (Composable (c -&gt; d) (a -&gt; b) (Depth (c -&gt; d) (a -&gt; b))) f @. g = comp (proof :: Composable (c -&gt; d) (a -&gt; b) (Depth (c -&gt; d) (a -&gt; b))) f g this seems to get rid of a lot of the required type signatures, and doesn't require 'base cases' 
Alas, it's from [a different community](http://blog.iphone-dev.org/). I was involved in that stuff years ago. Hard to say how many CSey folks make up my followers, but it's not nearly as many as the number you see on there.
I [added a test case](https://github.com/snoyberg/conduit/commit/93cf7403abd85994bccb3f824a2932c1656fb77d#diff-5de98ca3ec453cb0272e5afb305f53f5R462) demonstrating how to do this, which actually pointed out that I forgot to capture the leftover in the code I wrote yesterday.
Now just make it work like Google Now and its hot-word detection, so I can put my phone on the desk and just say: "Ok *hoogle*" "Give me a function that takes a function from a to b and a list of a that returns a list of b." "The function you are looking is map. It is defined in Data.List from the base package."
That makes sense technically, but for imo "FYI the GHC 7.8 branch has been created" sounds better. This is probably entirely because it's just how people in my surroundings spoke.
I've done a bit of programmativ manipulation of groups. My type looks like: data Group a = Group { identity :: a , invert :: a -&gt; a , mul :: a -&gt; a -&gt; a } The possible advantage over the typeclass approach is that you can do things like programmatically creating new groups out of old ones, e.g., prod :: Group a -&gt; Group b -&gt; Group (a, b) prod g h = Group { identity = (identity g, identity h) , invert = \(x, y) -&gt; (invert g x, invert h y) , mul = \(x, y) (a, b) -&gt; (mul g x a, mul h y b) } Which admittedly you can do using typeclasses as well. Come to think of it, you should always be able to create new groups out of existing ones using typeclasses by wrapping in a newtype if need be.
Is there a clean way to have both 7.6.3 and 7.8.0rc1 in the same system?
I don't know if there is a way to have co-existing Arch Linux packages, but ghc is relatively easy to build on Linux. See this SO question for a recipe on how to install it into your home directory: http://stackoverflow.com/questions/21717471/how-do-i-try-out-ghc-7-8-rc1-with-projects-requiring-libraries/21718512#21718512 Once you have ghc-7.8, your existing cabal program can install packages for 7.8 by just setting your PATH to pick up the ghc-7.8 executables `ghc`, `ghci`, `ghc-pkg`, etc. 
I strongly disagree. Let's stick with using characters that are easy to type, and not munging them into something that isn't for display. It's really nice when the characters shown match exactly the keys you press to recreate them.
Quite elegant! As the author of the original question it's interesting to see how the problem can be solved in LiquidHaskell :)
It's a module from *ghc* which is described as being &gt; [This is useful, general stuff for the Native Code Generator.](http://www.haskell.org/platform/doc/2013.2.0.0/ghc-api/src/OrdList.html) &gt; [Provide trees (of instructions), so that lists of instructions can be appended in linear time.](http://www.haskell.org/platform/doc/2013.2.0.0/ghc-api/src/OrdList.html) The three invariants are | Many [a] -- Invariant: non-empty ... | Two (OrdList a) -- Invariant: non-empty (OrdList a) -- Invariant: non-empty 
Simon Thompson is the author of "Haskell: The Craft of Functional Programming" and a researcher on the [Haskell Refactorer (HaRe) project](http://www.cs.kent.ac.uk/projects/refactor-fp/hare.html)
People's lack of it make me nervous, explicit &gt; implicit. OCaml gets this with its `+.` for float addition.
Thanks for another good solution, I toyed with several versions and found the existential to be by far the best. With the following data OrdList' ∷ IsEmpty → ★ → ★ where ... data OrdList a where OL ∷ OrdList' e a → OrdList a I defined all functions in terms of the exitential *OrdList* and tried using them on real code where *OrdList* is used in GHC, none of the type signatures changed and functions like *appOL*, *concatOL* and *toOL* that I had trouble with before (that represent 278, 16 and 109 uses out of 935 *OrdList* function uses in total) now work exactly as they did before as far as I can tell. I also tried your *LL* solution where *LL_Cons* accepted an existential *OrdList* with *OverloadedLists* which worked but the first solution worked well enough. Using overloaded lists for *OrdList a* also worked out well. instance IsList (OrdList a) where type Item (OrdList a) = a fromList = toOL toList = fromOL ghci&gt; [1,2,3,4] :: OrdList Int (Many [1,2,3,4]) I consider this problem solved :)
I should also note that with riotnerd's [second comment](http://www.reddit.com/r/haskell/comments/1xiurm/how_to_define_append_for_ordlist_defined_as_gadt/cfbx6kz) *appOL* ended up being the only function that needed any special machinery and the modules API effectively remained the same.
But did you write the Class GMonoid ... etc in your library, or did you use http://hackage.haskell.org/package/generic-deriving-1.6.2/docs/Generics-Deriving-Monoid.html ? I've tried the latter, and I can recommend it
And yes, I know I'm late to post about the Hack Night in specific, but at least you'll get to know about the meetup group and might come around for an upcoming event!
I don't know. My impression in this area is that there's been talk of work happening for quite some time, but I haven't seen anything land in the GHC repo.
This looks really good. And using diagrams rather than gtk is much appreciated.
And, presumably, of Type Theory &amp; Functional Programming ([pdf](http://www.cs.kent.ac.uk/people/staff/sjt/TTFP/ttfp.pdf)).
installing [nix](http://nixos.org)
Not surprised to see a bunch of KTH students! As you say, it's a little late for this specific event, but I might come along some other day if I find time.
We send our regards from haskell.stavanger.no :) Let's meet up in Gothenburg for the ICFP/CUFP conference! 
We are working on getting DWARF support for the 7.9 branch, see the [stack tracing ticket](https://ghc.haskell.org/trac/ghc/ticket/3693). The first patches might land soon, depending on how the discussion with Simon goes. For reference, the current "HEAD-to-DWARF" reference patch set is the [profiling-import branch](https://github.com/scpmw/ghc/commits/profiling-import) on GitHub.
Yeah, and a good way of looking at exactly how they differ.
Yeah, exactly. Instead of implicitly passing state between iterations, it is explicitly passed. Much neater, in my opinion!
Then you should realize that the only reason you think they are “easy to type” is because your keyboard layout is just as outdated and unfit for what you’re doing. You can easily use a modifier key to switch to a activity-specific layout. &gt; the characters shown You’re new to computers. You will stop caring about that when you learn more advanced typing methods. (Then again, if you *chose* to *stay* new to computers, then you’re wrong in thinking that you can judge any of this anyway.)
Good to see that you jump directly to personal insults instead of actually addressing the point that was made. One more time: unicode is bad for code examples, because it makes it hard to type what you see. Haskell already has a small community. If `forall` was replaced with `∀` in all examples that needed it, it would significantly reduce the size of the community, because people would say "That language obviously isn't made for programmers. It's just line noise, and not even as good as perl. I can at least type in perl line noise." And they'd be right. Hostility towards the tools available is a sure sign of a losing technology.
The entire Prelude, most of Hackage and GHC will make you nervous then.
This is a start: https://gist.github.com/sjoerdvisscher/8980824 I'm having trouble with `(&gt;&gt;~)` and `(+&gt;&gt;)`, but I don't think they're impossible to implement. Note that it isn't Van Laarhoven-style anymore, it's like we've fixed the transformer choice to the identity transformer.
A very nice review of the paper, although he appears to have a slight confusion as to whether "fold" should be "foldr" or "foldl".
I think of `fold` as simply lifting an operator on terms into an operator on lists—the same thing that `map` does, just more general. This is a thing: (//) = foldr map = (// []) . ((:) .) The explanation of “replace conses with a function and nil with a value” is visually intuitive, though, and probably the best way to explain it to a beginner. 
That's why there's this nice type isomorphism: foo :: (a → State s ()) → [a] -&gt; s → ((), s) foo f l = runState $ mapM_ f l (a → State s ()) → [a] → s → ((), s) ≃ (a → s → ((), s)) → [a] → s → ((), s) ≃ (a → s → s) → [a] → s → ((), s) ≃ (a → s → s) → s → [a] → s =α (a → b → b) → b → [a] → b which is the type of `foldr`.
You can't watch the file because that would place a lock on the file, which will block whoever wants to overwrite it (they need an exclusive lock). Locking the directory is no problem, so putting the watcher there and doing a mask on the file name accomplishes the feat equivalently (some APIs even provide the mask embedded).
Sounds you should alter your text editor to get the behavior you want instead of reformatting your code in a nonstandard way.
Omg omg omg omg the author commented! Thanks for the mindbending paper, hope I did it justice :) What've you been up to since?
It talks about folds in details but also about unfolds, hylomorphisms, folds on natural numbers and folds on trees. It also got some nice exercises.
Just found our Sthlm had Haskell meetups very recently. I wonder why Gothenburg has none (considering Chalmers is famous for Haskell stuff, plus Haskellers.com shows there are some Haksell users here at least). &gt;:) Anyway, cool and good luck!
The book that it comes from is [The Fun of Programming](http://www.cs.ox.ac.uk/publications/books/fop/) (available from Amazon UK, at least that's where I get my copy). It's an _awesome_ book that was put together for Richard Bird's 60th birthday. I believe that the book is intended to loosely follow on from Introduction to Functional Programming using Haskell (by Bird, previous edition by Bird and Walder). I can't recommend that book enough to people who want a bit more theory / rigour in their Haskelling. And it's coverage of fold is pretty nice as well :) 
i don't understand how you got from &gt; ≃ (a → s → ((), s)) → [a] → s → ((), s) to &gt; ≃ (a → s → s) → [a] → s → ((), s) to &gt; ≃ (a → s → s) → s → [a] → s could you elaborate on it, i'm really curious.
If I were you, I would add a list of elements to the Group datastructure. Then you could programmatically do stuff like check for isomorphism.
There isn't an output only monad because it's possible to define the IO monad in terms of an output only monad. Let's call that monad the Action monad because it does an action to create an output. All we need then is to have primitive actions defined like `getChar :: (Char -&gt; Action) -&gt; Action` and then we can define a new IO monad as `newtype IO a = IO ((a -&gt; Action) -&gt; Action)`.
Well ((),x) is isomorphic to x (in a total language) as it contains no additional information. It has the same number of inhabitants
Thanks. A great explanation.
Very simple concepts at base, though the implications are far-reaching. As is so often the case with these things, the best place to turn is SICP. [This section](http://mitpress.mit.edu/sicp/full-text/sicp/book/node10.html) early on in the book explains the difference between applicative order and normal order evaluation strategies. (Roughly: applicative order -&gt; strict; normal order -&gt; lazy) Much later in the book, there's an example where you build a lazy version of Scheme in Scheme (which is a strict language). The code to implement this comes out to about a page. Ah, Scheme. I may be seeing Haskell these days, but I still love you.
Awesome, I learned something myself! Thanks for elaborating on my example!
upvote as I'm reading http://cs.brown.edu/courses/cs173/2012/book/
&gt; I can't recommend that book enough Which one?
Personally, I would describe "Haskell like behavior" in Python with generators. def lazyPlus(a, b): yield a.next() + b.next() Or more interesting: def append(xs, ys): try: var x = xs.next() yield x yield append(xs, ys) except StopIteration: foreach y in ys: yield y
That would give you incorrect semantics, because you can only read the value once from the iterator before it raises `StopIteration`. ~~Your `append` is not right either. It forces the first list regardless of whether the result of the `append` is forced.~~ Arguably this is indeed "Haskell like" behaviour, but it is not a correct explanation of the semantics of lazy evaluation.
Append shouldn't force the first list until the append itself is forced. Append is returning a generator.
If that can help, you can try reading [Oh my Laziness!](http://alpmestan.com/2013/10/02/oh-my-laziness/).
Introduction to Functional Programming Using Haskell. If you're going to pick it up, do all of the exercises. Even if you don't normally roll like that.
I never understand laziness too much until I watched the slides in [this](http://www.reddit.com/r/haskell/comments/1te8hi/slides_from_haskell_singapore_meetup_20131220/) post.
&gt; In fact, the performance you can get from optimized lazily-evaluated code is surprisingly fast. I've heard in many cases it is the same or better than Just-in-Time Compiled bytecode for languages like Python, Perl, Java, or JavaScript. Gross understatement. GHC Haskell is currently one of the fastest languages on the planet. Haskell and it's non lazy brother OCaml pretty much rank only behind C, Fortran, and C++ at this point. They are faster than many imperative languages including Java, C#, Ada. http://www.haskell.org/haskellwiki/Benchmarks_Game Functional languages are not slow. Functional languages are slow when compared to C but C isn't the only imperative language, pretty much _every_ language is slow compared to C and that's not because C is imperative, that's because C is unsafe and low level. You could probably design low level functional language with barely a semblance of a type system as well that rivals C's speed. (C-- arguably come fairly close to this idea).
&gt; Ah, Scheme. I may be seeing Haskell these days, but I still love you. Scheme is arguably better designed than Haskell for what it wants to do. Haskell is simply the _only_ language out there currently that is designed for purely functional non strict semantics but as far as that niche goes it isn't even that well designed. There are a loooot of controversial design choices made in Haskell which people are now stuck with. 
The motivating insight is that evaluation isn't always necessary. A lazy value is like a partially explored graph. The underlying structure is determined by its source expression, and the free variables in its context, which link to other lazy values. (This is the "graph reduction" interpretation of lazy evaluation, which maps straightforwardly to a pointer-based implementation.) Consider this expression, which I'll call *x*: [f a, f b, f c] Or equivalently: f a : (f b : (f c : [])) Syntactically, you can view this expression as a tree structure: : / \ f a : / \ f b : / \ f c [] But if you give the expression to a lazy interpreter, its knowledge of the expression's *value* will start out as just a little node within a "fog of war," pictured thus: * For all we know, evaluation of the expression might not terminate; that is, it may be infinitely recursive or contain errors. That is to say, not all expressions *have* values. The correspondence between syntax trees and values is—since the language is Turing complete—very complicated. So leaving expressions "unexplored" is a way to stay on the safe side. However, at some point we might have to start exploring it. When? Well, when we need to know more about its structure. That is, when we evaluate a pattern match on *x*'s structure. So in order to evaluate something like case x of (_:_) -&gt; True [] -&gt; False we must scout into the `*` to see what's there. But we don't need to go far into its scary depths; only enough to see that it looks like this: : / \ * * This is called [weak head normal form](http://en.wikibooks.org/wiki/Haskell/Graph_reduction#Weak_Head_Normal_Form), since you can see unambiguously what the "head" of the value is, namely the constructor `(:)`, but the rest is left unexplored. If instead we evaluate `length x`, we will end up having explored quite a lot of the `x` territory: : / \ * : / \ * : / \ * [] In graph reduction jargon, we say we've evaluated the "spine" of the list: that is, we've explored the list structure without exploring any of its elements. This is very reasonable, since the expressions `f a`, `f b` and `f c` may be expensive or even non-terminating or erroneous. Consider also boolean expressions, e.g. `f a || g b`, which in most languages have "short-circuiting" behavior. Lazy evaluation simply goes all the way with this often useful semantics.
Ha, I like the "fog of war" analogy. Reminds me of playing Civ II.
&gt; boolean expressions, e.g. `f a || g b`, which in most languages have "short-circuiting" behavior The short-circuiting behaviour is actually more important in "most languages" than it is in Haskell, because evaluating `f a` or `g b` may have side effects, that is, it would have different semantics if it evaluated both even if the left argument was `true`. In Haskell the (denotational) semantics do not differ (if we ignore `_|_`), one just happens to be quicker than the other.
Cool article! As a minor nitpick, I would add that you have the word 'nad' in your 2nd paragraph. While this is perfectly acceptable in FP circles when prefixed with at least 'mo', I believe this particular occurrence is in error.
Like what? Certainly there are warts in the standard library like Applicative=&gt;Monad (wasn't that supposed to be fixed ages ago?) and `String = [Char]`. I'm not sure if I can think of any design choices in the actual language that I think are a problem. Maybe the fact that `\x -&gt; f x` isn't the same as `f`?
&gt;Haskell's module system has clearly been given not that much thought That's true, although I can't say I have found it that annoying myself. Honestly I've found package dependencies way more annoying than the module system. &gt;Haskell's syntax makes syntactic extensions difficult to easily achieve. Not sure about this one, I've never tried making a syntactic extension. &gt;Haskell could benefit from a lot of static rewrite rules without needing true dependent typing. For instance realizing a N,M matrix which is guaranteed to have that length in the type system is entirely possible currently in Haskell, it's just super inconvenient to do. TypeLits help here (GHC 7.8 is a few months away, and has been for years) &gt; Top level mutable state cannot be realized without the unsafePerformIO hack. Which is weird because as far as I know the implementation of randomIO requires it but the language offers no way of doing it except using a function which is heavily discouraged from using. Not sure how this could be solved, I also don't think that unsafePerformIO is considered all that bad (tons of libraries use it). Idris seems interesting, but doesn't seem to work on windows, so I haven't been able to experiment with it. What would the type of (::) be? &gt; Haskell is a "throw feature atop feature" language It's a research language at heart. Most of these features are hidden behind extensions. 
&gt; That's true, although I can't say I have found it that annoying myself. Honestly I've found package dependencies way more annoying than the module system. True, but Cabal isn't Haskell. The module system does greatly annoying me. That there is no "export all except ..." can be kind of frustrating and makes the export list at the top of modules quite frequently a mess if you want to export anything _but_ this one implementation detail. I also dislike that different names cannot coexist in one module imported via different ones which are just resolved via a hierarchy and types like Idris does this, I'd go so far to say that this coupled with paramatrized modules can provide a better way of ad hoc overloading than type classes which I think are a very ugly solution to the problem. &gt; Not sure about this one, I've never tried making a syntactic extension. Well, with respect to Scheme, many people would agree that Scheme has the most elegant system of syntactic extension in any language. Scheme's hygienic macros are very solid and it's as easy to define a syntactic extension as it is to write a function which means you can often encapsulate very complicated behaviour into something simple. &gt; Not sure how this could be solved There are a variety of proposed solutions over at the Haskell wiki, my own proposal comes down having a pure function `makeIORef :: (Enum a) (Eq b) =&gt; a -&gt; b -&gt; IORef b` which guarantees that it returns a unique IORef for any combination of its first and second argument. If you call it twice with the same arguments it will return a different IORef, it will also initialize the IORef it returns to the second argument. You can then at the top level simply write something like: data Secret = Secret; randomSeed :: IORef Int; randomSeed = makeIORef Secret 0; And not export the `Secret` type and constructor, thereby guaranteeing that in no other module the same IORef can be generated. But it again is a hack that relies on responsible use of the programmer, it doesn't actually extend the language in any way and only requires this specific function to exist. If you somewhere again use `makeIORef Secret 0` it returns the same IORef as that but it is not initialized to `0`. This ensures that it is fully independent of evaluation order. If you actually did `makeIORef Secret 1`, then it would return a different IORef. The major problem with this is that you can only make IORef's of Eq types or use physical identity to compare which is ill-defined. In either case it is not a true solution and puts responsibility on the programmer. However in all cases of use its behaviour is well defined and independent of evaluation order. You just should never export the secret constructor used to initialize it or use something that is freely available like an integer because in that case someone else in anothe rmodule might have the same idea and both modules will then deal with the same IORef. &gt; What would the type of (::) be? Depends on what `(::)` it is and defined in what module. It can be `List.(::)`, `Vect.(::)` or anything you want it to be, it just picks the one that type checks and if two type check it picks it based on the import order of the modules. Last one imported gets picked first. &gt; It's a research language at heart. Most of these features are hidden behind extensions. Indeed, but the difference with Scheme is that these extra features in Haskell almost always need actual implementation support while in Scheme extra features can be simply located in a library due to the low level of restrictions of the language. 
No I can't with Python, but I hope it can be better illustrated with ghci. So fire that up, and follow the "protocol": Prelude&gt; let x = [1..10] Prelude&gt; :sprint x x = _ Prelude&gt; head x 1 Prelude&gt; :sprint x x = 1 : _ Prelude&gt; tail x [2,3,4,5,6,7,8,9,10] Prelude&gt; :sprint x x = [1,2,3,4,5,6,7,8,9,10] Where underscore denotes unevaluated parts. Play around with various tuples, lists and functions over them and inspecting them in between with `:sprint`. I think that's the best way to form intuition.
[Chapter 2](http://chimera.labs.oreilly.com/books/1230000000929/ch02.html) in *Parallel and Concurrent Programming in Haskell* from Simon Marlow has an excellent introduction to lazy evaluation. I usually read it when I forget how it works and where to put a `!` to force evaluation.
In theory, but let's face it, people in the end downvote because they disagree and/or personally dislike whoever posts it. Especially on some bigger subreddits, you should check /r/starcraft out, whenever a progamer did something controversial like not being respectful to his or her opponent in a match then everything that person posts even completely unrelated to that matter is downvoted for a week and then people calm down again.
&gt; a "nice" functor (i.e., representable, traversable, etc) "etc"? What more could you want?
I really like the idea of a TLS implementation in Haskell, except for one thing: I have no idea how you can write Haskell code that is free from timing side-channel attacks. I'm not claiming it is impossible, but I'm curious how well this is handled.
I'm not a graduate student, and there's no way I could attend, but if videos of the lectures were made available, I'd definitely watch them. Failing that, slides. "Theory of Randomised Search Heuristics", in particular sounds interesting.
Why? Garbage collection? Other than garbage collection I don't see much difference between writing in C and writing in the ST monad. (But then again I'm not a cryptanalyst!)
I was thinking using the ST monad would eliminate laziness-based information usage, but considering it further there's still so much implicit laziness in ST that this would be very hard.
Nice article. This type of post is of considerable interest to me, as my work entails a combination of programming, optimization and differential geometry. On the topic of types that seem missing in analysis, I would mostly like to point out that differential geometry provides types for and formalizes a lot of the notions here. The [differential](https://en.wikipedia.org/wiki/Pushforward_(differential\)) is a common way to extend the notion of derivative in the "locally linear" sense that this post alludes to. In particular, it maps tangent vectors in one space to tangent vectors in another space. That in the case of some affine spaces, the result maps bijectively back to the original space is a result of modeling manifolds locally as vector spaces. In terms of what may be useful to codify in haskell, consider the notion of a [derivation](https://en.wikipedia.org/wiki/Derivation_(abstract_algebra\)) over an algebra. This simply says the operator is linear and satisfies the product rule (Leibniz rule). General derivations are kind of cool to study in their own right as they can for instance provide mappings between grades in graded algebras like the algebra of differentiable symmetric tensor fields. My haskell experience has been brief so far, but I would imagine that's the kind of thing that could be nicely transferred into haskell using typeclasses and laws. With that in mind another important notion of derivative is the derivative or "gradient" of scalar (and other-valued) functions over a manifold. Most relevant here is the [exterior derivative](https://en.wikipedia.org/wiki/Exterior_derivative) and its dual, the codifferential. For instance, the exterior derivative would take a smooth function M-&gt;R and provide a differential form whose type is M-&gt;TR^*. That is, the gradient of a function is not a tangent vector, but is a dual object called a cotangent vector. You can take exterior derivatives of arbitrary differential forms, and you wind up with a chain of differential forms (a graded algebra as forms can be "wedged" together). Notions like the gradient, curl and divergence are really nicely captured in types by considering them as exterior derivatives of differential forms on R^3. In particular, most of the common vector calculus identities that seem so arcane when you encounter them in a calculus class are very nice and simple when phrased in terms of exterior derivative, codifferential and Hodge star. Sorry for the long-winded response. This may be common knowledge to a lot of you haskellers and I'm just completely missing the point and blathering away :-). Anyway, as someone with some substantial differential geometry and other math experience, I was excited to find haskell, which goes a long way to scratching the itch that a lot of math-oriented programmers have afaik: that we'd like the mathematics to map more cleanly to the implementation. PS for those interested: As a starter project meant to help me learn haskell, I have started [a small differential geometry library](https://bitbucket.org/jhinkle/haskell-diffgeom) tailored mostly to my own needs in shape statistics (and image analysis in the future). It's in no way complete and probably obscenely hideous haskell code, but the associated type families for tangent and cotangent types are meant to be my basis for attacking something like this post was concerned with: encoding differentiable functions in types in haskell. *tl;dr - [differential geometry](https://en.wikipedia.org/wiki/Differentiable_manifold)
If you wish to attend, please register by filling out [this form](https://docs.google.com/forms/d/1mMTlVNnWQvac1oWSXWQLBO9GVvqv8a_fYTFrR8UNuDI/viewform).
If you are an example based person like me, it might help to ponder an expression like this: &gt; take 5 [1..] [1,2,3,4,5] 
There have been a bunch of excellent answers, but there's a little-mentioned tool that I found to be really nice to examine the structure of expressions in GHC: http://hackage.haskell.org/package/ghc-vis The manual has a bunch of examples, including a video showing ghc-vis integrating with the ghci debugger to visually step through evaluation: http://felsin9.de/nnis/ghc-vis/#basic-usage With this, you can generate your own examples and explore them interactively. Have fun!
I think it's odd to consider whitespace based blocks not "explicit".
A similar topic came up in http://www.haskell.org/pipermail/haskell-cafe/2014-January/112314.html https://github.com/aavogt/DimMat/tree/units came out of that discusion.
That initialization solution has some issues: * It lets you break parametricity by writing a function `a -&gt; b -&gt; Bool` that tells you whether two types are the same. This could be fixed by requiring `Typeable` on the first argument of `makeIORef`. * It relies on the `Eq` instance for the second argument being reflexive, which is not necessarily true (`Double`s, programmer error). If `a /= a`, then you can tell the difference between `let r = makeIORef x a in (r, r)` and `(makeIORef x a, makeIORef x a)`, which breaks equational reasoning just like `unsafePerformIO`. &gt; Depends on what (::) it is and defined in what module. It can be List.(::), Vect.(::) or anything you want it to be, it just picks the one that type checks and if two type check it picks it based on the import order of the modules. Last one imported gets picked first. This is giving up on principality, which is vital for usable type inference. Idris doesn't have this problem because, as a dependently-typed language, it already requires type signatures on (almost) all binders.
&gt; It lets you break parametricity by writing a function a -&gt; b -&gt; Bool that tells you whether two types are the same. This could be fixed by requiring Typeable on the first argument of makeIORef. Yeah, this was actually pointed out when I submitted it for comments, forgot about it. &gt; It relies on the Eq instance for the second argument being reflexive, which is not necessarily true (Doubles, programmer error). If a /= a, then you can tell the difference between let r = makeIORef x a in (r, r) and (makeIORef x a, makeIORef x a), which breaks equational reasoning just like unsafePerformIO. True, you would probably have to write something else than Eq to define when two are the same. Another thing which I debated which did it slightly differently was require a defaultable typeclass which has a method `default :: Defaultable a =&gt; a`. which is basically the "default" value for that type, 0 for numbers, False for Bools empty data structures for any structure which will in that case always initialize the reference to the default value. Another thing is just not to initialize it and make it an error to read an unintialized IORef, you have to write it once before you read it. &gt; This is giving up on principality, which is vital for usable type inference. Idris doesn't have this problem because, as a dependently-typed language, it already requires type signatures on (almost) all binders. Well, Haskell already gives up on this with type classes which create a similar if not identical problem. In practice it's a less severe problem because Haskell doesn't use overloaded names via typeclasses nearly as often as you encounter overloaded names via modules in Idris.
Usually lazy evaluation kills you when you don't know that you don't need something. I wouldn't be surprised if avoiding short circuiting would get you most of the way, just like in procedural languages.
Will the talks be recorded?
I've got a toy linear algebra library in my computer where instead of representing linear maps as matrices, we represent them as arrows in the linear subcategory of Hask. For example, a 3x3 matrix of doubles has type: Vector 3 Double +&gt; Vector 3 Double The `+&gt;` type is a subcategory of hask. Composition in the category corresponds to matrix multiplication, and the `mappend` function to matrix addition. Of course, matrix addition is actually linear, so we could alternatively represent it as a function with the type: add :: (Vector n Double +&gt; Vector m Double) +&gt; (Vector n Double +&gt; Vector m Double) +&gt; (Vector n Double +&gt; Vector m Double) This is actually a 3rd order tensor. We could represent this explicitly as a tensor in memory, however in this case the tensor would be extremely sparse---it will be much more efficient to represent it as a function that adds the corresponding elements in each matrix. The main advantage of this notation is that we can represent linear transformations between infinite dimensional vector spaces. The differentiation operator is an example of this. Let's say I have a differentiable hilbert space `Hilb`, then differentiation might have the type: diffHilb :: Hilb +&gt; Hilb I've found this notation of explicit linearity extremely convenient for my purposes. Notice that the domain and range of the differentiaion operator are the same. The OP, however, is suggesting that they be different---I can't imagine a situation where that would actually make things easier.
On the one hand, I fully agree that Haskell makes it tricky to implement timing-independent operations. On the other hand, if you _do_ implement timing-independent operations, Haskell can make it very easy to compose them together and retain that property using the type system, whereas C leaves you at the mercy of the programmer's discipline. A Haskell level proof of the timing-independence of a set of composed operations, which then somehow compiles down to something lower level (LLVM, [atom](http://hackage.haskell.org/package/atom)) could be an interesting approach. But it would take some ground work. (Not an impossible amount of it, though.)
And now traversable sounds dirty...
You speak the truth. I was trying to be humble in my statement because I didn't have any benchmark data to back my claim. Thanks for the link.
how do you want to track that with the type system _in haskell_? (i can imagine how it would work when generating code.
&gt; Well, Haskell already gives up on this with type classes which create a similar if not identical problem. In practice it's a less severe problem because Haskell doesn't use overloaded names via typeclasses nearly as often as you encounter overloaded names via modules in Idris. No, typeclasses were very carefully designed to preserve principal types. For instance, the function `neq a b = not (a == b)` has a perfectly good most general type: `Eq a =&gt; a -&gt; a -&gt; Bool`. Just like parametric polymorphism lets you write `id x = x` and defer the decision of which type you meant `x` to have from the definition site to the uses of `id`, typeclasses let you defer the decision of which *instance* you meant to the use sites of `neq`. Furthermore, in neither case does the success of type inference depend on which types or instances are in scope. Contrast this with a definition like `cons2 x xs = x :: x :: xs` in Idris. The typechecker has to decide which `::` you meant at the *use site*, and whether or not it can do this depends on how many modules with `::` in them you've imported (and what type it has in those modules). If both vectors and lists are in scope, there's no single type that's both valid for `cons2` and more general than all the other valid types it could have. *This* is what lacking principal types mean. 
Not sure if I agree, this assumes that only one instance fits depending on the context. The famous example of the can of worms in typeclasses is something like `x = show . read`, `x` itself is obviously typed `String -&gt; String` but whatever type exists in between show and read is more difficult. There is no real way to know it except by letting it default to some type if multiple options are available which is something you could have done just as well with an Idris-like module module system. Type inference without assuming some default types is not generally possible in Haskell due to type classes. In fact, type classes allow for situations where the type of an expression is not known at compile time, just that it type checks. This particular monster taken in mind: worms :: Show a =&gt; [a] -&gt; String; worms [] = ""; worms (x : xs) = show x ++ worms [x]; This function won't compile without it being given a type signature, it will compile with a type signature but apparently Haskell cannot construct this type. There is no way any more to statically choose instances in this function should you want to. The only reason type inference succeeds in practice in Haskell is because expressions relying on type classes are rare enough for them to be constrained as such that they can be decided. If they featured everywhere then the instances could not be chosen without a lot of type annotations, even something as basic as `main = print (0 + 1)` relies on that `(0+1)` has a default instance to `Integer` and many people don't object because as it stands show on Integers and Doubles tend to look similar enough. Say we added Bool to the num class for which there can certainly be made argument. Suddenly arbitrarily defaulting to Integer has its objections. Why doesn't the program print "True", if we add bools to the Num class `1 :: Bool` is actually valid code for "True" provided we give it a sensible fromInteger method. It's arbitrary, in the case of the Num typeclass the agreement is that the default instance is Integer, in the case of many other typeclasses it just seems to not be able to pick a default and not compile on you.
Did you literally type `sum_of_offsets nums = nums //Placeholder`? Because haskell's comments starts with `--` Are you typing each of these line by line into GHCi, or are you putting them in a .hs file and attempt to load it using :l filename.hs? 
I'm using GHCi, do I still need :l for that?
I think it would help if you wrote the type signatures you expect these functions to have. For instance, `length` always returns an `Int`, but there's no `Fractional` instance for `Int`, so you can't use `(/)` with it. The second type error is because you haven't defined `sum_of_offsets` in a meaningful way, yet. To get around this, use `undefined` as a placeholder instead.
Okay, so let's start with the types. squared :: Num a =&gt; a -&gt; a or, if we only want `Int`s squared :: Int -&gt; Int Now for mean mean :: Fractional a =&gt; [a] -&gt; a We know this is the case because we want to be able to get fractional values out of it (`mean [1..10] == 5.5` should hold). We can actually specialize this to `[Double] -&gt; Double` if you'd rather use concrete types. The trick here is that length will only get you `Int`s. If you want to use `(/)`, you need something with a `Fractional` instance. Try replacing `length` with `fromIntegral . length`. There's also `genericLength` from Data.List, but we don't need to mess around with that, yet. Additionally, in Haskell we call `%` `mod`, and you can use it infix as `mod` `(%)` comes from Data.Ratio and we use it to define rational numbers. Then we want offsets :: Fractional a =&gt; [a] -&gt; [a] since we start with a list of numbers that we want to interact with via `mean`. You could write `offsets` as offsets :: Fractional a =&gt; [a] -&gt; [a] offsets nums = let m = mean nums in [ x - m | x &lt;- nums ] We can then write sumOfOffsets :: Fractional a =&gt; [a] -&gt; a sumOfOffsets nums = sum $ map square (offsets nums) Finally we can write stdDev :: Fractional a =&gt; [a] -&gt; a stdDev nums = sqrt $ offsetSum / ((len nums) - 1) where offsetSum = sumOfOffsets nums len = fromIntegral . length Remember that we can specialize to `Double` (or any other `Fractional` instance) at any time.
I'm only understanding every other word here, so I'm going to go and read some more haskell tutorial stuff. The real problem here is that I'm trying to write in a language without knowing anything about it yet. Thanks for your help.
Get through the first couple of chapters of Learn You A Haskell and you should be golden. Have fun!
Put this in a file that ends with .hs stddev :: [Double] -&gt; Double stddev nums = let len = fromIntegral $ length nums :: Double mean = (sum nums) / len diff = [(x - mean) | x &lt;- nums] square = [(x^2) | x &lt;- diff] sumsquare = sum square in sqrt (sumsquare / (len - 1)) go to GHCi, traverse to the folder where you put the .hs file (using :cd), and do ":l stddev.hs". After that you can use the function like `stddev [1..15]` The function i provided is a very verbose step by step calculation. It can be simplified quite a bit. Hopefully this gives you some insight as to how to write a haskell function. however, if you want to do it directly in GHCi, the syntax is slightly different. (type the following line by line) let nums = [1..50] :: [Double] let len = fromIntegral $ length nums :: Double let mean = (sum nums) / len let diff = [(x - mean) | x &lt;- nums] let square = [(x^2) | x &lt;- diff] let sumsquare = sum square sqrt (sumsquare / (len - 1)) This is because GHCi is running an IO environment (like an IO() block). Don't worry if you do not understand what I'm talking about here, you'll know what i mean as you read more tutorials. 
&gt; The derivative of any fixed vector will just be 0 ah, I see. Sorry, I thought you meant differentiation of elements (functions) in your Hilbert space, instead of derivatives of curves in the hs. My bad. Yeah, you can always take derivatives of curves in a vector space, and they lie in its tangent space, which as you say is isomorphic to the vector space itself. No dual stuff comes into play in that case.
Got addicted to folds and unfolds and wrote a bunch of further papers about them :-)
Just a nitpick... Matrix addition is linear but not in the way you wrote: `add m` is not a linear function when `m` is nonzero. `u +&gt; v +&gt; w` is the type of a bilinear function, such as matrix multiplication. Matrix addition has a type like `(u,v) +&gt; w` (here `u = v = w = Vector n Double +&gt; Vector m Double`).
Are you sure you can define compositions of two operations that are both timing-independent, and have a guarantee it is still timing independent? What if the compiler inlines something and then eliminates some sharing? What about fusion or rewrite rules? Basically, I think you can't write timing-independent code unless you understand every optimization your compiler might carry out and know either that those are safe or how to counter them.
That looks interesting. I have tried to use units in solving real physics problems but had to give up because iirc you have to replace the prelude. I should really write up the problems I ran into.
in the annals of history
Most of the groups I worked with had types that were Bounded and Enum, so you can get that list with `[minBound..maxBound]`
Sure, but in Haskell as well as other languages you very often rely on `_|_` in short-circuiting expressions. if(p != NULL &amp;&amp; strlen(p-&gt;val) &gt; 0) { ... } `strlen` is as pure as C functions get, but `strlen(NULL)` is `_|_`, so it's very important to short circuit its evaluation. Similarly in Haskell, this is valid (if not entirely idiomatic) f :: Maybe String -&gt; something f x | isJust x &amp;&amp; length (fromJust x) &gt; 0 = ... | otherwise = ... 
Works for me with no type signature... &gt; :t worms worms :: Show a =&gt; [a] -&gt; [Char] I agree with you that the defaulting system is kind of crazy, but I'm not sure that has anything to do with the principal types argument; the problem is that applications that remove types from the principal type list can be ambiguous and thus have no known type--what's the type of `[]` in the expression `length []`? There's no typeclasses in sight here, and still we don't know, and the conversion to System F requires us to pick one. Fortunately by parametricity the compiler can pick an anonymous type and there's no loss, but when typeclasses come into the picture the result isn't parametric on the type any more, and thus `print (0+1)` needs a type declared for `0+1` or a defaulting system to pick one. Personally, I prefer 'it's ambiguous' to 'pick one', but for basic objects like numbers that gets pretty verbose, so we have to decide between the niceness that is polymorphic literals and the badness that is defaulting. Regardless, `print (0+1 :: Bool)` is totally non-ambiguous.
Well, in some desugared typed functional language, `length` has type `(a : *) -&gt; [a] -&gt; Int`, and `[]` has type `(a : *) -&gt; [a]`. We need to instantiate an `a` in order to call either of these functions! `length Int ([] Int)` or `length Unit ([] Unit)`. Fortunately, due to parametricity, any such `a` will do--AFAIK, GHC has a magic type `Any` it uses in this case. Typeclasses are a principled way of breaking parametricity, that still maintains principal types, but adds the possibility of ambiguities that will need to be resolved. Ad-hoc polymorphism loses principal types. That said, I'm not sure there's a reasonable way to maintain principle types in semi-parametric cases in a dependently typed language. Even the simple `flip` combinator is impossible to define generally in a dependently typed language. (A working definition would give the type `Int` to `flip length ([] Int) Int`)
&gt; This is perfect ! :D &gt; This also coincidentally helps explain "purely functional" : D Applies here as well : D
This is beautiful and one of the things I couldn't stop thinking about today. The power and speed and thinking style of creating in that manner is really intriguing. I'm a Noob-Sauce programmer so it just never occured to me until today. Thank you again for the explanation :D 
Exceptional find and explanation just in like the first paragraph (Chimera.labs). Thank you! 
Finally arrived. Haskell is finally starting to make sense and I can start doing something useful! Now, for the follow on book ...
I went to https://github.com/haskell/cabal/tree/1.18 and downloaded it (there is a Download ZIP link / button). Then I went to the cabal-install dir and executed the bootstrap.sh script. Just as is described at http://www.haskell.org/haskellwiki/Cabal-Install#Unix, but with the newer version as the current (1.18.0.2) doesn't work with the RC1 (as stated in https://ghc.haskell.org/trac/ghc/wiki/Status/GHC-7.8/RC1).
I I understood correctly, monad transformers are second-order functionals, right?
This is partly discussed here: http://www.reddit.com/r/haskell/comments/1xk8rr/after_lenses_come_free_monads_the_van_laarhoven/
You are right. Your example reinforces my point if you consider `_|_` an effect! "Most languages" just happen to have *more* effects than Haskell, so short circuiting behaviour is more important in them.
The talks aren't videoed but the lecturers slides are made available on the webpage as soon as they are ready. 
I had no idea about the `:sprint` command. Thanks!
typo: isomoprhism
and "Furtermore" "theorem tell us"
Oh, I see.
It's valid syntax. And SPJ uses semicolons and braces in his papers, why doesn't that make you nervous? Do C pre-processor macros in Haskell code make you nervous?
Excellent post. And I don't think anyone has actually ever unravelled this in full. There is definitely a nice paper waiting to be written here; I had half a mind to do it myself, but my differential geometry is not quite solid enough to make the process smooth… so I write other papers first!
It has been known for decades, but NOT by programmers. What has not been known (and is barely known by just a few), is how all this `well known math' can actually be directly useful to programming. This is because most linear algebra is taught to programmers as a "memory shuffling" game between arrays -- the underlying structure which justifies the rules of the game are never elucidated. So there is still a lot of work to be done to encode the knowledge of differential geometry in to a type system (with erasure!) so that one gets code as efficient as today's memory-shuffling nightmares.
You can't use / on an int. In Haskel, types are very strongly defined. You can't do things like "string" + 2, like in other languages, and you can't do 2.0 / 2. In short, you can't mix and match types like that. You have to be explicit about type conversions. To do: "String" + 2 = "String2" You say: "String" ++ (show 2) show converts a type to a string (similar to "toString " in other languages) To do: 2.0/2 You say 2.0 / (fromIntegral 2) fromIntegral turns any number integral into a factorial (2.0), which / requires. And yes, you should read "learn you a Haskell for great good", there are a lot of pitfalls in Haskell. Well, not really, there are a lot of pitfalls in other languages to the point that these pitfalls become "normal" (ie, string + number = convert number to string and concat with string. That is a pitfall, but it is so common it became "normal"). 
If you want help, please provide the `chart` code you have tried and the errors you are getting so that people can help you without each verifying the whole `chart` wiki ... ;) As far as `gnuplot`, if you use :t at the GHCi prompt or navigate to [the documentation](http://hackage.haskell.org/package/gnuplot), you can discover that the type of `plotList` is `plotList :: C a =&gt; [Attribute] -&gt; [a] -&gt; IO ()`, so, yes, you'd use this in the IO monad.
Thanks! I will add the requested items!
&gt; No window is opened or anything. Maybe you need to call gnuplot's `terminal` function in order to tell the library that you want the plot to appear in a window instead of being written to a file?
Also, the example you showed for Chart is definitely using a file-based backend, so look for some files from running that as well.
You're mistaken, this has nothing to do with typeclasses. To wit: worms :: [a] -&gt; String worms [] = "" worms (x : xs) = worms [xs] also has this behavior, and there are no typeclasses here. This problem is called polymorphic recursion. Probably the simplest example is the definition `x = x x` which gives an occurs check, unless you add a type signature `x :: a`.
Yeah fair point, never considered this example truth be told. Edit: it doesn't quite give the same problem though. Being able to do that with an unconstrained type doesn't really create any problems since it can only be used by functions which are agnostic to the type of the contents of the data structure any way. The problem with typeclasses is that a different instance has to be chosen for each different type. It actually inspects the contents of the data structure and depends on its type with typeclasses.
For simplicity, here's the Haskell School of Music library on Hackage: http://hackage.haskell.org/package/haskore This is exactly what I've been looking for for a while, and I'm super excited to try it out!
There a middle ground you may be interested in. If you wanted to, you could write it in Haskell and use IORefs and the IO monad wherever you want mutable state or random numbers or whatever. One of the great things about Haskell is that you can pick and choose what code should be pure.
How does `layers` compare to `extensible-effects`?
Wowie! http://www.johndcook.com/blog/2014/02/10/real-world-haskell/
Haskell can do this just fine. Random seed for random numbers doesn't need to be explicitly passed around. There are lots of monads for that sort of thing, in particular the MC monad for monte-carlo simulations has a nice GSL-backed random gen for this purpose: http://hackage.haskell.org/package/monte-carlo-0.4.2/docs/Control-Monad-MC-GSL.html As for the tree manipulation, one simple solution is to throw an IORef into each part of the tree. A more elegant solution may involve zippers, or exploiting lazy evaluation to avoid mutation altogether, but I'm not familiar enough with the domain to determine what's possible.
For anyone who's read this, do you have a review? 
I believe the new versions of this book use the Euterpea library: https://github.com/Euterpea/Euterpea, though it should be noted that Windows is the only directly supported platform.
My approach to monte carlo in Haskell is to just generate the entire game tree lazily. It is actually much, much nicer than the imperative, mutable alternative. If you need to annotate nodes with mutable scores or something, you can either use `IORef` as suggested or just rebuild that path, which actually isn't as bad as it sounds (you just did the work of traversing it, plus you are probably about to update every `IORef` you encountered along the way, anyway).
&gt; Flexability .. hotswaping .. unprivilaged .. Should be: &gt; Flexibility .. hotswapping .. unprivileged .. 
Thanks. Spellcheck is broken on my editor for some reason =)
The article is about a production environment for a Snap server not performance metrics. I may write something about that in the future since I do have some interesting results to report on that front as well.
Yeah definitely! I was thinking about wrapping all this up in Docker eventually as well.
It's real good. worth every penny.
The title makes no promises of performance metrics. How is it misleading?
How come you have to use Angel and Upstart? We're using `systemd` here, and I let that deal with daemonization and restarting - I would have thought `upstart` would be capable of doing that too, removing the need for another manager.
I'll take a look at that MC monad. I'd like to try a more elegant solution without IO. Perhaps I'll write a generic monte carlo tree search module first. Thanks for the help everyone.
the dimensional library does define + / - that conflict with the ones from the prelude. It's not a big deal to "import qualified Prelude" though.
Do we really want dynamic code loading to mean "just like erlang"? A lot of erlang folks i know seem to think that theres a bit of subtlety to it even working well in just the erlang context. That said, theres a lot of ways of doing dynamic code loading in GHC, they're just different from the ones in erlang (and with good reasons :) )
Is gnuplot on your path? You can find out by opening a terminal window and typing `gnuplot`. I don't remember if gnuplot's installer gives you an option to modify your path.
You definitely don't need mutable state to do Monte Carlo Tree Search. I wrote a GGP one in Clojure and the only impure function I used was ```rand-nth```. The algorithm was just a handful of short functions for the parts of the algorithm. They ended up being tail recursions or list comprehensions. I wrote it so the top level MCTS function was a single iteration. It took statistics as input (originally an empty map) and returned statistics updated from the iteration. Then you just keep feeding the returned statistics back in to iteratively grow the tree as desired.
"just like erlang" is just a start and to keep a well-defined target to achieve. Ofcourse, the implementation and semantics in haskell would be completely different since the underlying run time should not be changed to support this. Could you tell me a little more about various ways of dynamic code loading in GHC?
I wrote a piece of music this summer with Euterpea. I have it running on Mac OS X without much trouble. Over all I like the library, but some of the upper layers of the library are really just packaged examples from the book, and I often rewrote then to suit my needs.
Yes, upstart can respawn a daemon if it exits with a non-zero error code. Although if a daemon dies three times within 60 seconds, upstart won't respawn it again; presumably you can configure this though.
On linux, you don't need to start a server with elevated privileges to use privileged ports; just use capabilities: setcap 'cap_net_bind_service=+ep' /path/to/nginx
&gt; http://www.haskell.org/haskellwiki/Benchmarks_Game I don't know about "faster than Java". I look at this [comparison](http://benchmarksgame.alioth.debian.org/u64q/benchmark.php?test=all&amp;lang=ghc&amp;lang2=java&amp;data=u64q) between GHC and Java7 and it is very comparable. I would not say GHC is faster than Java just based on the "benchmarks game". 
You could take a look at my ggp package (https://github.com/ian-ross/ggp) for General Game Playing. I've not yet written an MCTS player (it's on my list for this week), but there's a simpler Monte Carlo player and a framework for writing players that deals with managing state and random number generation (using a big ol' stack of monad transformers...). It's not pretty in any sense of the word, having been written quickly to follow along with Michael Genesereth's GGP MOOC, but it might give you some ideas.
Thanks, it is handy to learn this stuff! By the way - it probably does not hurt anything - but it looks like there are a bunch of LF characters that got converted to &amp;#10; sequences in the Racemetric HTML.
It is
Couldn't find an email link to send this: page labeled 22 "No, proof by calcultation did not fail, since, as just pointed out,"
I've just finished reading Sussman and Wisdom's [Functional Differential Geometry](http://mitpress.mit.edu/books/functional-differential-geometry) and was thinking all the way through that it would be nicer with types! Their code is all in Scheme, and while it's good and pretty clear what's going on, it would be better if the types of things were made more manifest. They do distinguish geometrical objects of different types (using structures with tags), but there's obviously no inference or checking. Still, their [scmutils package](http://groups.csail.mit.edu/mac/users/gjs/6946/linux-install.htm) might be a good place to look for ideas. 
I've tried multiple times getting through an older version of the book: the thing that bothers me the most is I have never figured out how to actually get music to play! I tried looking through the docs for one, I couldn't really find an example that worked... overall quite frustrating
pretty much the same as any other compile language which has tooling for dynamically loading static or dynamic object code! note, however: that via the ghc api, and other tools, it is possible to easily send a String or AST over the wire, and compile it or run it directly via an interpreter on the other end. I can elaborate more with some concrete examples of these later (if requested), its a bit late for me right now so pardon if my answer is a tad vague. 
You can still do your example with polymorphic recursion and without typeclasses import Data.List (intercalate) type Show' a = a -&gt; String worms :: Show' a -&gt; [a] -&gt; String worms show' [] = "" worms show' (x : xs) = show' x ++ worms (showList' show') [xs] showList' :: Show' a -&gt; Show' [a] showList' show' xs = "[" ++ intercalate "," (map show' xs) ++ "]" My `worms` still triggers the Occurs Check if the type signature is omitted. 
The problem with this approach is that there is really no way to detect whether some part of the game tree is already evaluated. I expect that this information is pretty useful for statistics and such.
&gt;GLFW fails to build for me, so I can't build Euterpea. :( This issue has been a thorn in my side as well. Do you also run Mavericks?
Are you talking about loadObj from http://www.haskell.org/ghc/docs/7.4.1/html/libraries/ghc-7.4.1/src/ObjLink.html ? I am using the same approach via a "facade-api" library called hs-plugins by Don Stewart. But the problem is not in being able to compile and load new code. The problem is in maintaining the state and preserving the mailbox after the upgrade and making sure that processes behave "well" in a distributed system while upgrades are in place. Also, how can we use ghc api to send AST/code over the wire?
Yes, it type checks, but what function `f :: a -&gt; String` would you give to worms? It's the same function every time without picking a specific instance and a function `a -&gt; String` in practice is hard to make to do something meaningful. But it's the same function every time no matter what you give it. My main problem with type classes is that it doesn't produce a finite list of possible versions of show which can be decided at compile time. It isn't possible to if you want to rewrite it syntactically to a program no longer using type classes by using something like static dispatch. If you tackle the same problem with say parametrized modules you create something which generates a finite known list of functions which could be rewritten statically to fit in a "normal" parametric type system. God, I know I'm being terribly unclear, let's just phrase it succinctly in that typeclasses make static dispatch impossible creating potentially infinite different versions of show. I am personally uncomfortable with the idea that if I see `show 3` I can't generally just say 'this is actually just `showInt 3`. You can in that case obviously but in the worms function you can't any more.
Yes, indeed. I googled around for an hour but didn't turn up a result. If any kind soul can tell me what this means: glfw/lib/enable.s:499:0: Rest of line ignored. 1st junk character valued 37 (%). glfw/lib/enable.s:502:0: Unknown pseudo-op: .cfi_def_cfa_register glfw/lib/enable.s:502:0: Rest of line ignored. 1st junk character valued 37 (%). glfw/lib/enable.s:507:0: Unknown pseudo-op: .cfi_endproc I'd be most grateful. My `ghc` is configured with `gcc-4.2`, so it's not that `clang` issue, at least. Some mumbling over similar issues seems to suggest that Apple's `as` doesn't play nice with `gas`, but I dunno.
That's why I also included the bit about scores and such.
yeah, I'm also wondering this. I'm already experienced in Haskell but if this is "Learn Music Through Haskell" it may worth a shot for me ...
It seemed more like "learn haskell through music" to me, coming in with very little background in either as of my reading of the previous draft.
The ST Monad is not translated as directly to machine instructions which have known parameters. As well, the ST Monad doesn't insulate you from using pure functions in it, nor does it rescue you from laziness abounding elsewhere in the code. The problem with Haskell is fundamentally that knowing what instructions will execute based on a given line of code is *hard*. People who care about performance in Haskell have convinced themselves that it doesn't matter - and to some extent that is true. Haskell can do things with streaming composition that are not easy in other languages. But that is a different problem. What you almost need is an inverse `Hask` monad of sorts, one that prohibits all pure, lazy actions and only allows you to use mutable state and functions of constant-time. `if` and `case` statements would have to return actions which are dependently typed based on that time, and ensure the two functions return in the same time. (Branches, in general, are an anathema to cryptography. Perhaps they should just be illegal in this `Cohask` monad.) Writing a type checker for that would be an interesting graduate thesis. Edit: And these are only the most trivial side-channel attacks. Far more interesting attacks involve spurious processes intentionally poisoning the processor's cache and many other tricks.
The fix is easy... `cabal unpack GLFW`, then edit the file `Setup.hs` at the top level: diff -u -r -w orig/GLFW-0.5.1.0/Setup.hs GLFW-0.5.1.0/Setup.hs --- orig/GLFW-0.5.1.0/Setup.hs 2014-02-16 08:36:59.000000000 -0800 +++ GLFW-0.5.1.0/Setup.hs 2013-07-13 15:57:42.000000000 -0700 @@ -159,8 +159,8 @@ performTest :: Verbosity -&gt; String -&gt; Flags -&gt; IO Bool performTest verbosity contents flags = do tmpDir &lt;- getTemporaryDirectory - withTempFile tmpDir "glfw-test.c" $ \path inHandle -&gt; - withTempFile tmpDir "glfw-test.o" $ \objPath outHandle -&gt; do + withTempFile True tmpDir "glfw-test.c" $ \path inHandle -&gt; + withTempFile True tmpDir "glfw-test.o" $ \objPath outHandle -&gt; do Now build and install that, then Euterpea will build and install. For me, I used a newer cabal and the sandbox feature. What I did was `cabal sandbox init` in the Euterpea directory, then in the GLFW directory did a `cabal sandbox init --sandbox=../Euterpea/.cabal.sandbox`. Then, after patching `Setup.hs`, build and install in GLFW will install it into Euterpea's sandbox. 
This looks like your tool chain is inconsistent. Like you are using gcc for somethings, and either clang or some other gcc for others.
Also: Full code to my piece is open source here: https://github.com/mzero/PlainChanges2 In particular, you might consider grabbing [src/Sound/MidiPlayer.hs](https://github.com/mzero/PlainChanges2/blob/master/src/Sound/MidiPlayer.hs) which is a somewhat better midi player than the one that comes with Euterpea.
Haskell is a tool here, music is an application area. You need a lot more than Haskell to learn "music", but Haskell helps with some applications.
To be sure https://www.haskell.org/platform/mac.html
Yes, and it's very frustrating that the maintainers of this library have not uploaded Euterpea to hackage for several years now, despite numerous requests. So it's more difficult to share valuable work on things like that. Perhaps it's time for an unofficial third-party upload to hackage that tracks the original HSoM github? Does the license allow that? EDIT: Changed the wording a little. To be clear: Euterpea is wonderful, and I sincerely thank the authors for sharing it. But why not upload it? It would make it so much easier for Euterpea to generate so much more creativity. Uploading to hackage takes literally seconds to do, and, contrary to a common misconception, does not imply anything about a "release" or the quality of this particular revision. It just makes it easier for others to base their work on your library.
Nice stuff. Small haddock typo in `Data.NonNull`: `Data.List.NonEmpty` is quoted with single quotes instead of double quotes. That caused Haddock to link to `Data.List` in base instead of to where it really lives, `Data.List.NonEmpty` in semigroups.
Thanks for the report, I've updated the repo and will release a fix in a few days.
Oh, not bad. I'll give that a shot. Thanks!
Actually, if you literally type "2.0 / 2", ghc will infer the type of "2" to be Fractional a =&gt; a
You might have a look at [this](http://stackoverflow.com/questions/9022978/haskell-plotting-library-similiar-to-matlab) stackoverflow, and maybe the [plot](https://github.com/amcphail/plot) library.
Why not use [keter](http://hackage.haskell.org/package/keter)? Although it comes from the yesod world, it has absolutely no dependency on anything yesod whatsoever, and it's very easy to interface to and use. It solves just about all of the deployment issues mentioned here, and more, such as hot swapping of site upgrades without current users losing their sessions. We use it as the foundation for our deployment systems for many kinds of apps; it's hard to imagine what life would be like here if we had to deal with the kinds of hackery described in this post.
So is mono-traversable a typeclass-based alternative to Traversal/Foldable from lens? I.e., with lens you explicitly pass around your traversals but with this you can just use omap/ofoldMap etc? Are there any other major feature differences with lens?
I guess I could have been more specific. If you hold 2 in some value of type Int, then it would fail. If you just type "2", yeah, the compiler will figure it out. 
Some previous discussions of interest on NixOS if you haven't been keeping up recently: http://www.reddit.com/r/haskell/comments/1x0cjs/how_i_develop_with_nix/ http://www.reddit.com/r/haskell/comments/1vghgw/nixos_a_gnulinux_distribution_based_on_purely/
implicit traversals should be usable as lens' traversals if they are of the right shape. For example, `traverse` is a valid traversal usable as an "explicit" lens' traversal.
I had the same problem with gnuplot on my debian. The plotlist function was doing nothing, without error. The problem was that I had gnuplot-nox installed, instead of gnuplot-x11. Maybe that will help.
This is basically what [the `each` traversal](http://hackage.haskell.org/package/lens-4.0.3/docs/Control-Lens-Each.html#v:each) does.
Would this round-trip cause problems if the input is being streamed from disk? (i.e.: for data larger than memory, will this incur a second full read?)
Would be nice if those packages were linked from the blog page.
For example, a `MonoTraversable` is a sort of "default" choice of a `Traversal' mono (Element mono)` per type. So for `ByteString` you get a `Traversal' Word8 ByteString`. (`Word8` is chosen by fiat). One benefit the typeclass approach gives you is that many methods are included in the class and can be given specialised optimized implementations. (This mostly applies to `MonoFoldable`) 
No. It only slows things down by a small constant factor. Everything still streams. The pathological case is if you repeatedly round trip, like this: forever $ do x &lt;- zoom lens parse doSomethingWith x That will stream, but it will trigger quadratic time complexity because each iteration will incur an additional round trip. Fortunately, I may have a solution that eliminates the round trips and does not change the user-facing API, but it will take a little bit of time to complete.
Good point, done.
I can't speak for Snap or Yesod, but Happstack is really easy to setup.
I did my first Haskell project in `happstack-lite` and found it pretty simple. I've heard `scotty` is simple too, but I've never used it.
scotty (http://hackage.haskell.org/package/scotty) is straight forward. But if it is just for a blog consider a static site generator like hakyll (http://jaspervdj.be/hakyll/).
happstack-lite and scotty are great for beginners, while snap and yesod are, I think, a tiny bit more intimidating. One small kudo to happstack(-lite) because of [its crashcourse](http://www.happstack.com/docs/crashcourse/index.html). Any Haskell beginner however quickly reaches the point where you can pretty much use any web framework at the same level of comfort. Many of the components you use when writing a website can be used with other frameworks, so once you know your way around with one, you pretty much can use any of the others the day after without feeling lost.
`hakyll`'s great. It would be a fun way to get into Haskell.
I would love love **LOVE** to see an updated _Haskell School of Expression_ perhaps using Gloss instead of the SoE library? While this book is good it's hard for someone with no formal education in music to really get into IMO. While SoE uses visuals which are almost universal.
I use it in production in as far as ocharles.org.uk is deployed with NixOps. We're investigating a similar set up at work at the moment and comparing against Docker on Ubuntu.
When I [first tried](http://ocharles.org.uk/blog/posts/2013-12-05-24-days-of-hackage-scotty.html) `scotty` I was very impressed. It's really simple!
I never promised compositions of "pure functions" can be done. I promised compositions of timing-independent functions.... and I never specified the composition, either. It's a bit circular in a reddit comment since the idea is that _by definition_, one would produce a set of operations and compositions that would preserve them, but it should be fine. I rather hinted at the idea that it would not be something that would work in "pure" Haskell when I, uh, said it wouldn't be something that ran in pure Haskell, I thought. Again, follow that link to atom. It isn't the thing I'm handwaving here, but it shows the sort of techniques that would be involved.
Does it *have* to be a *framework*? Frameworks differ from libraries in one thing: Lock-in. You can’t just use a part of a framework as a module in your program frame. They force you to pull in the whole thing and plug your stuff into it, instead of the right way around. And they make it very hard to use more than one framework at the same time in the same project. I find the very concept of frameworks revolting, as it goes against the very principles of programming. So… libraries!
To be fair, the Arch wiki is probably the most effective and comprehensive manuals in the linux world, so it is a high standard to match. I use it all the time, even though I don't use arch itself.
... you have trouble deciding against Ubuntu as a server system?
Surprised it's just now moving off the DistroWatch waiting list. When I saw NixOS, it was the first truly interesting Linux distribution I had seen in years...there isn't much of anything interesting about another 'buntu, sorry. I don't know if Ladislav moves things off the list based on "interestingness" though.
Yes, at www.zalora.com we're using it for hosting internal web-based tools. We're in the process of switching our main websites over to running on it as well.
Now, I may be a little biased being one of the developers for this framework, but I think [MFLow](http://mflowdemo.herokuapp.com/) is by far one of the best web frameworks out there. Just as an example, check out the [3 page web app in a tweet](http://haskell-web.blogspot.com.es/2013/05/a-web-application-in-tweet.html).
Nifty! How does this compare to Liquid Haskell? 
[Here is the link](http://hackage.haskell.org/package/containers-0.5.4.0/docs/Data-Map-Strict.html#t:Map) to the location of the `Map` type. If you click the `Source` link to the right of it you will see how it is internally implemented. In this case the `Map` type comes from the `containers` package. Generally, if you want to find something a good place to start is to Google for "haskell &lt;search term&gt;" or "hackage &lt;search term&gt;". For example, Googling for ["hackage map"](https://www.google.com/search?q=hackage+map&amp;ie=utf-8&amp;oe=utf-8&amp;aq=t&amp;rls=org.mozilla:en-US:unofficial&amp;client=iceweasel-a&amp;channel=fflb) returns several excellent results.
Certainly, you can use [hoogle](http://www.haskell.org/hoogle/?hoogle=Map.Map) to look up Haskell things. By the way, you're probably confusing lowercase [map](http://www.haskell.org/hoogle/?hoogle=map) (which is indeed a function) with uppercase [Map](http://www.haskell.org/hoogle/?hoogle=Map) (which is a type constructor).
damnnnnn you brainnnnnnnnnnnnnnnnn!(Thanks btw)
SBV is, at it's core, a library for talking to SMT solvers. You build up symbolic computations using a nice eDSL and then ask Z3 et al to prove things about them. If I understand correctly, there's no direct connection to Haskell other than the implementation language, ie you can't ask SBV to prove that Haskell's map preserves the length of its input. LiquidHaskell, on the other hand, is a tool for proving and inferring properties about Haskell programs. We take a Haskell function and a specification, and essentially build up a large logical formula, which we then ship off to an SMT solver to prove validity (in reality it's more complicated since we also perform inference, but this is the general idea). So we're also talking to SMT solvers internally, and thus have some overlap with SBV, but the high-level goals are different. We could certainly use SBV internally, but we don't at the moment, in part because the SMT interface is actually handled by an older OCaml program :) Hope this helps!
What I meant is that, in lens, you still have to pass in a `Traversal` value, even if you use the "general" `traverse` value. In mono-traversable, the functions don't require any kind of parameter to indicate how to fold/traverse the data structure, it's always implicitly chosen based on the type.
Yup, that's it. Was on my phone earlier. Some notable departures though --- use cabal sandbox for your sandboxing needs instead of the one adit uses there.
It sounds like you figured it out, but if something begins with a lowercase letter, it's either a function or a reserved word (like import, case ... of, if then else, etc). If it begins with an uppercase letter, then it's either a type or a type constructor.
For beginners it should be noted that it is not only an convention that types are uppercase and functions are lowercase, but it is actually defined in the language. So you always know that an uppercase identifier can't be a function
Alternative. I think gnuplot installation is the main problem (was mine on mac osx). Now I found an interesting alternative cabal install d3js then in ghci &gt; import D3JS.Example &gt; test1 a file (html) is saved in the current folder and the code sample is here http://hackage.haskell.org/package/d3js-0.1.0.0/docs/src/D3JS-Example.html#test1 hope it helps 
Warp is very simple, essentially just an implementation of WAI - you simply give it a function Request -&gt; IO Response. 
now that you are here: does nixpkg support pgp-signed binaries? if so, how can you customize it (add/remove accepted keys)?
That wasn't the problem. I think when I tried to use other libraries I ran into problems. One day I will write up the problems I encountered.
It's the bit on the SBV website that says "Express properties about Haskell programs and automatically prove them" that I don't understand.
If you find yourself passing and returning a state value to every function, your code can be wrapped up in a "State" monad, available in the 'Control.Monad.State.Lazy' module Basically, any function of the form: f :: a1 -&gt; a2 -&gt; a3 -&gt; ... -&gt; aN -&gt; st -&gt; (b, st) can be converted to a function of the form: fs :: a1 -&gt; a2 -&gt; a3 -&gt; ... -&gt; aN -&gt; State st b fs a1 a2 a3 ... aN = Control.Monad.State.state (f a1 a2 a3 ... aN) If you need your state to also have access to function in the IO monad, like a random number generator provided by the operating system, you can use the state monad tranformer "StateT" in the same module: fio :: a1 -&gt; a2 -&gt; a3 -&gt; ... -&gt; aN -&gt; st -&gt; IO (b, st) can be converted to a function of the form: fios :: a1 -&gt; a2 -&gt; a3 -&gt; ... -&gt; aN -&gt; StateT st IO b fios a1 a2 a3 ... aN = Control.Monad.State.state (fio a1 a2 a3 ... aN) -- in mtl-2.1 and later, the 'Control.Monad.State.state' function is a class method, -- so it works the same on both IO and non IO functions. The advantage of doing this is you can use ordinary monadic syntax to write your functions, but if you want to update the state value, you can just use the "modify" function to do it. If you want to only read the state, you can use the "get" function. import Control.Monad.State (StateT, state, gets, modify) import Control.Monad.Trans (liftIO) import System.Random (StdGen, getStdGen, next) import MyOtherModule (MyTree, MyZipper, MyNullZipper) data MyState = MyState { theStdGen :: StdGen, myZipper :: MyZipper } showRandGen :: StateT MyState IO () showRandGen = gets theStdGen &gt;&gt;= liftIO . print nextRand :: StateT MyStateT IO Int nextRand = do (i, gen' ) &lt;- fmap next (gets theStdGen) modify (\myState -&gt; myState{ theStdGen = gen' }) liftIO $ putStrLn $ unwords ["Got random number:", show i] return i myActualMain :: StateT MyState IO () myActualMain = do ... main = do stdgen &lt;- System.Random.getStdGen runStateT myActualMain $ MyState{ theStdGen = stdgen, myZipper = MyNullZipper } Here is a link to some code I am working on right now that uses a monte-carlo like random generator for generating random grammatically correct computer code: https://github.com/RaminHAL9001/dao/blob/master/src/Dao/Random.hs I have created a monad RandT for generating random objects, which is a wrapper around a state monad. The stateful information contains information about the number of nodes in the grammar tree, the depth of the tree, and of course it contains the 'System.Random.StdGen' data. To generate a random number, I provide a "randInt" function which generates the next random number from the 'StdGen'. I also create a class "HasRandGen" and instantiate all of my objects into that class (in other modules, not in the Random module). That way I can do something like this: data Noun = Person | Place | Thing instance HasRandGen Noun where randChoice = randChoiceList [Person, Place, Thing] data Verb = Is | Do | See | Hear | Think instance HasRandGen Verb where randChoice = randChoiceList [Is, Do, See, Hear, Think] data Adjective = Big | Small | Loud | Quiet | Fast | Slow instance HasRandGen Adjective where randChoice = randChoiceList [Big, Small, Loud, Quiet, Fast, Slow] data Sentence = Sentence Noun Verb Adjective randO = return Sentence &lt;*&gt; randO &lt;*&gt; randO &lt;*&gt; randO -- here I am using the 'Control.Applicative.&lt;*&gt;' operator 
I can recommend Scotty: http://github.com/scotty-web/scotty You can also take a look at a simple scotty starter kit: https://github.com/scotty-web/scotty-starter I think you can also use Snap, as it uses mostly simple constructions (as opposed to Yesod, which relies on TH heavily). However, the framework itself is quite complex.
`hmatrix` is probably the best choice for this since it comes with a large number of built-in numeric algorithms. `repa` is more advanced, but comes with a smaller standard library of functions.
Technically this isn't completly true. Just :: a -&gt; Maybe a is a function. The rule should be "uppercase = constructor while lowercase = variable" but type synonyms break that.
Just is not a function; it is a constructor. However, the type of Just is functional and so it may be applied (like a function); the difference is that constructor may also be "unapplied" (by pattern matching). 
It's a constructor function.
There's also [hlearn](https://github.com/mikeizbicki/HLearn) if you want something more high level
I set up my own blog using Hakyll, as I have no need for dynamic content. It's awesome!
The timing side-channel attacks are against symmetric crypto. The only truly safe way to ensure against timing attacks is to implement in hardware where operations are time-padded with a hardware clock to time-equate all operations with the slowest operation. Intel's recent chips are instrumented with hardware AES. My understanding is that tls will employ hardware crypto when available.
 h&gt; prove $ \(x::SInteger) -&gt; x+x+x .== x + 2*x Q.E.D. http://hackage.haskell.org/package/sbv-3.0/docs/src/Data-SBV-Examples-Crypto-RC4.html#line-124 
Note that `hmatrix` is GPL as well so that may be an important factor.
It's both, I suppose one can argue that a data constructor that is not Nullary is a function as well. You can also store a constructor in a variable in which case you can't pattern match it any more. I suppose one can argue that uppercase is constant while lowercase is variable? Even in type signatures like `Monad m =&gt; m a -&gt; (a -&gt; m b) -&gt; m b` the lowercase `m` is a type constructor, just a variable one, the constant `Monad` is a type class.
I think there has been some work here, but I'm not entirely sure on what the status of it is. Things such as exporting from the store can be signed with keys I think, but I'm not sure about verification.
*sigh* I'll never get used to his favorite font. edit: Also, I hadn't realized that you could emulate dependent Types in Haskell. That's awesome.
Obligatory "Is there a video of this talk somewhere?" post.
SBV and Liquid Haskell are quite different things. SBV is all about programming SMT solvers in a painless way to do your bidding. It's an eDSL in its core, taking advantage of whatever decision procedures modern SMT-Solvers bring into the world of Haskell. (It's not the only binding mind you, hackage has several. But I do believe it's the most mature.) SBV fills in a nice niche of providing seamless support for all of Haskell's numeric types; not only sized words/ints, but also Integers, rationals, algebraic reals (which goes beyond rationals, but not quite the entire R), and now IEEE-754 floating point numbers. These are the domains that SMT solvers are good at, so to speak. You use usual Haskell constructs to write programs manipulating these symbolic values, and express and prove properties about them. SBV itself comes with a lot of examples: See the module hierarchy in https://hackage.haskell.org/package/sbv, most of the modules are just examples actually. There's code there about solving puzzles, handling CRC's, working with uninterpreted types, etc. Liquid Haskell, on the other hand, enhances Haskell's type-system with refinement types. It's a way of reasoning about Haskell programs, introducing some features of dependent-types, without going the full length. It provides a very nice system to further eliminate bugs in Haskell programs by carefully controlling the types and adding type-level invariants. It adds to the Hindley-Milner scheme, making the type-system more expressive. At the extreme, this would be like programming in Agda or Coq, but Liquid Haskell is much nicer to use since it automates the verification greatly, thus making it quite practical. Of course, this comes at the cost of restricting what you can put in the types, as for instance Coq would allow arbitrary terms. But then, Coq is not very easy to use. I think Liquid Haskell strikes a great balance in what you can express and what can be automatically discharged. It just happens that Liquid-Haskell uses SMT solvers in discharging some of its proof obligations; but otherwise there is no direct relation to SBV. Automated theorem proving and reasoning about programs is a hard topic. It is important to pick your battles. There're many fronts you can try to make a dent in, and SBV and Liquid-Haskell both try to make dents in quite different ways. A direct comparison is really not possible, as they aim at quite different things. However, I am happy to see that Liquid Haskell is coming along so nicely, and so does SBV, and we're all better off to have these tools at our disposal to improve our programming experience with Haskell..
The instructions here worked for me (on Windows): http://haskell.cs.yale.edu/euterpea/download/
Here is the page of the course at Yale with links to the book and Euterpea: http://haskell.cs.yale.edu/cpsc-432-532/resources/
Here is the page of the course at Yale with links to the book and Euterpea: http://haskell.cs.yale.edu/cpsc-432-532/resources/
Yeah, those make me nervous. When cpphs is more mature I'll feel better.
Wobbly types are no longer considered current or even a good idea. The new outsideIn(X) system used by GHC is considered a uniform improvement. So, probably ignore the last part of this talk and go read the [tome-of-a-paper](http://research.microsoft.com/en-us/um/people/simonpj/papers/constraints/jfp-outsidein.pdf) on OI(X) instead
not sure if sarcasm or joing in the rejoice of the great LYAH
Document properties of the PDF say it was created 24/08/2006.
One of the problems with the current version of hlearn is the lack of a really good linear algebra in Haskell. It's possible to do anything you might want; it's just not possible to it with a nice clean interface. For the OP: doing this project in Haskell will expose you to all the bad parts of Haskell, and very few of the good parts. That said, I also started HLearn as a class project and doing it in Haskell made me rethink (in a good way) how machine learning works.
Author here. The SCP proxy is written in Haskell, and the web server uses Snap. The scp library is on Hackage although not really ready for public consumption (maybe a few arguments are still hard-coded), and on [GitHub](https://github.com/noteed/scp-streams). [HN discussion (if any).](https://news.ycombinator.com/item?id=7253586)
Author here. The SCP proxy is written in Haskell, and the web server uses Snap. The scp library is on Hackage although not really ready for public consumption (maybe a few arguments are still hard-coded), and on [GitHub](https://github.com/noteed/scp-streams). [HN discussion (if any).](https://news.ycombinator.com/item?id=7253586)
What's the connection with Haskell? I like nix, but am a little confused by the upvotes this is getting on /r/haskell without one mention of Haskell in the comments.
No I truly find LYAH awesome, I myself started reading it :D How can a book with an elephant on the cover not be awesome? It can't
What does this news have to do with Haskell? Seems like this was posted in the wrong reddit. The only relation to Haskell that I can find on the NixOS website is a mention that the package manager is purely functional and they have a cabal2nix tool.
Yeah, I think a more integrated solution would be nice.
Great answer! Thanks Levent! (Btw, do you have any slides on SBV that I might use in a graduate class?)
He gave a similar presentation at OPLSS about implementing GADTs and System-FC in general. He goes through the whole song and dance (literally in the case of Simon) of describing the modern type system features. Videos are uncompressed (&gt;1GB), but available here: https://www.cs.uoregon.edu/research/summerschool/summer13/curriculum.html
A function in haskell is anything with a function type. Being a constructor doesn't make it not a function. This is a language design decision. You don't need to give your constructors function types--in the standard presentation of System T, for example, S takes an argument but does not have a function type. In haskell `Just` is a function. You can do things like map Just that would not be possible otherwise.
Curious if you think it could be taken in a direction that will work? Also on that subject have you seen this: http://shenlanguage.org/
The simplest way to see the data declaration is in ghci: Prelude&gt; import qualified Data.Map as Map Prelude Map&gt; :i Map.Map The ":i" is short for ":info". It will also output all of Map.Map's type classes. GHCi is pretty awesome. Here are the [GHCi commands](http://www.haskell.org/ghc/docs/latest/html/users_guide/ghci-commands.html)
maybe because Eelco would be glad to rewrite everything with some purely functional language (and waiting proposals). However, that requires Haskell to be as lightweight and dynamic as bash\perl combo.
Looking in detail, it seems to be a time-memory tradeoff: http://benchmarksgame.alioth.debian.org/u64q/benchmark.php?test=all&amp;lang=java&amp;lang2=ghc&amp;data=u64q Note the comments in the Haskell implementation of binary trees (the one exception to this rule), the tree has to get explicitly forced to meet the expectations of the benchmark, so if that case showed up in the real world where that unnecessary work wasn't required by the benchmark lazy evaluation might give nice benefits. The memory use issue isn't academic, real programs can consume a lot of memory and it takes time to manage memory with things like garbage collection or page faults. I would say iSlaminati is perhaps too strong in his statements (in part because the microbenchmarks do not test against the .NET implementation most people actually use but only against Mono, because Ada really does do better than GHC and as far as I know always has, and because Java's implementation has much improved recently). SBCL is also worth mentioning, it tends to do better on these benchmarks than Haskell or OCaml, and most people would call it a functional language (although maybe not ML-ers like ourselves).
This is because most languages do not have a property called "eta-equivalence". Haskell also doesn't, although for more subtle, surprising, and confusing reasons.
Thanks Ranjit. I don't have any slides. When I talk about SBV, I just shamelessly steal Leonardo's SMT slides (pick anything from here: http://leodemoura.github.io/slides.html, they're all excellent), and then I just run ghci and do some demos..
Thanks, that was what I needed.
don't forget the pop-cultural references i form of cartoons from lego to what-not
I'm not really sure what in this post you are referring to, but I'm curious to learn more about why Haskell doesn't have eta-equivalence, do you have any good sources for that?
I'll be releasing an alpha of hopenblas soon! (tentatively end of this week / early next week) https://github.com/wellposed/hOpenBLAS (mind you, the only interesting module is [this one](https://github.com/wellposed/hOpenBLAS/blob/master/src/Numerical/OpenBLAS/MatrixTypes.hs) , the rest are just ffi bindings and work in progress wrappers that arent quite done yet). If you can wait a a week or so, those should be pretty nice :) hopenblas is meant to be a very very nice self contained blas / lapack ffi binding thats usable on its own, and should support (with a teeny bit of work) 2dim versions of the vector api, with some restrictions that may depend on the matrix type. I'll be using hopenblas as one of (several) optional backends for my (not year released) numerical haskell library suite. (yes, I'm writing a ffi binding nice enough to use on its own, but will only be using it myself as an ffi binding, but I wanted it to be human friendly as needed, and easily extensible). Its important to note that a major goal of hopenblas is that it should Only provide operations that give you result matrices that can be passed directly to blas/lapack. So its not **as general** an API as the one i'll be doing for numerical haskell, but **surprisingly** it still manages to be more expressive/general an array api that you'll find in hmatrix or other semi performant libs on hackage Regarding Hmatrix: if your class at any point involves writing block oriented direct solvers, hmatrix **can not** express noncopying matrix slices, so you'd have a very very hard time writing any performant code that works inplace in a block / submatrice oriented way. Hmatrix will work just fine if you're only writing iterative solvers (but any good machine learning will likely touch on both). Likewise its a bit opaque how to easily extend the ffi binding for hmatrix. That said, if you need to use something this week, hmatrix isn't bad :) Repa and Accelerate are **not** general purpose array libs, though you can try to use them as such, they're very very much nice show cases for fusion as an (that you can use for the right problems). They are meant to illustrate "how far can we push fusion based optimization for numerical computing". Fusion is just **one** tool. And not always the right one! The moment your algorithm is built out of matrix mults, fusion stops being the correct choice. (The repa / accelerate authors will agree with me on this, i asked them at ICFP 2013 :) ). And often times the best way to write performant code that looks like math is to try to express your alg with just matrix mult and related algorithms! :) i'm sure these remarks of mine raise as many questions as they answer, and some of those subsequent questions I wont be able to answer on reddit (i've a LARGE series of blog posts / papers i'm trying to start writing to share a lot of my thoughts on how to write / use numerical tools) 
I thought Warp was pretty simple to set up to serve mostly static web content.
Why did you choose Snap for something that isn't an http server? Snap is my go to http server in the Haskell ecosystem. How did you feel about it as just a tcp/ip environment...
There is some misunderstanding here. I have implemented a tool that acts like scp, which is called through SSH (just like scp does when using remote machines). This is a pure command-line tool, nothing web-related (and it doens't use Snap). Then there is the web server itself, which uses Snap, to display bucket content, show and process forms for user registration, adding keys, payments. There are also additional non-web related tools (background jobs to detect corrupted files and repair them).
Ah, I hadn't seen those before! Thanks! Perhaps you have some files you use for the demos? Or do you work through the `Data.SBV.Examples.*` ?
There's some awesome work in that area. [Type families](http://research.microsoft.com/en-us/um/people/simonpj/papers/assoc-types/fun-with-type-funs/typefun.pdf) and [data kinds] (http://research.microsoft.com/en-us/people/dimitris/fc-kind-poly.pdf) are awesome on their own and make a lot of the emulating-dependent-type work nicer to do. [Singletons](http://www.cis.upenn.edu/~eir/papers/2012/singletons/paper.pdf) allow you to go further. The library has some very nice things in it (possibly depending on your view on Template Haskell for reducing boilerplate). There are some nice surveys of the landscape and examples of various bits of the above [here](http://homepages.inf.ed.ac.uk/slindley/papers/hasochism.pdf), [here](http://www.jonmsterling.com/posts/2012-11-10-faking-it-with-style.html) and [here](https://github.com/leonidas/codeblog/blob/master/2013/2013-02-19-typesafe-tictactoe.md). If you get through all of that and it just seems like an entree, there's [Adam Gundry's thesis](https://personal.cis.strath.ac.uk/adam.gundry/thesis/). 
Thanks! I'll certainly look into it.
I've looked at Shen and didn't see any indication that it was a design informed by current PLT and type theory research - or even papers from the last 25 years. It doesn't even seem to be exploring any particular evolutionary avenue of development beyond being a weird Lisp. I would be a lot less pessimistic if language designers showed a sign that they'd been reading white papers. The only languages I'm optimistic about right now are Haskell, Idris, and Rust. The latter is partly because members of the Haskell community *intervened* to help make certain particular mistakes weren't made.
Warp is super-simple, because it's not really a web-framework, not even a micro-framework. Its a webserver. (Ruby: Mongrel/Thin/Webrick/Puma) It exposes an API named "WAI" (Ruby: Rack; Python: WSGI), on top of which app-layer stuff is build. On top of that API some web frameworks are build... Scotty, very simple. (Ruby: Sinatra; Python: Flask) Yesod, full-featured. (Ruby: Rails) 
There should be something like OverloadedString for Fractionals/Integrals. Most people that start out and want to do simple division get bitten by this :D
Awesome. I bet stuff like this can help people out a lot when learning.
I have this vague recollection of needing to eta-expand all my constructors in ML for, what seemed to me like, no good reason.
The example specifically points to implementing laziness by forcing evaluation only when a useless function invocation is done, equivalent to turning a value of type 't' into a value of type '() -&gt; t'. This relies on the fact that introducing a lambda actually changes the semantics of the program. The #haskell IRC channel told me that one important difference is that \x -&gt; f x is partially defined even if f itself is fully undefined. This is probably the most obvious one; it illustrates that eta-expansion actually makes a function more defined than it is when it isn't expanded, IE, (\x -&gt; f x) . g may work when f . g will crash. Also, although I don't remember the specifics, the GHC optimizer also treats eta-expanded values differently from ones that aren't, so at times the seemingly useless act of removing explicit parameters or writing them down explicitly can make a difference in performance.
Nix stores all its managed derivations in a store indexed though a SHA-256 hash of their contents and dependencies, similar to how Git stores commits. Running `nix-store --verify --check-contents` will hash the store contents and ensure they match their original versions, (up to hash collision). If you want to use PGP to make a signed statement about the contents of a server installation, you would do it analogously to a how you'd use PGP to sign the Git-managed sources for a particular deployment: signing the hash of the derivation/commit.
We also could use a better system for typeclasses and instances, and in particular getting rid of the problems of overlapping instances somehow (another problem is that higher-kinded classes can't have contexts, IE the whole "set isn't a monad" spiel). Then there is the fact that instances are always implicitly imported from libraries even if you don't want them. On the first issue, I've been thinking of requiring instances to have names, partially ordering instances based on how monomorphic they are, allowing classes to resolve ambiguity with something similar to fundep syntax to specify which type variable should be made more specific in the event of a tie, and allowing instances to explicitly override other instances by name. 
Oh I didn't realize systemd or upstart could do this. I'd love to remove Angel to simplify my stack. Thanks for the suggestion! Ubuntu's moving to systemd so I plan to move from upstart at some point anyway. Any good systemd guide you can recommend for getting started?
Ah I've never heard of capabilities. That's pretty simple. Thanks! I'll still use Nginx which the system runs as root anyway though it seems that's not necessary.
I just type simple things at the ghci prompt, and use some of the examples. Sudoku is always a favorite example. People with theorem proving background typically appreciate the MergeSort and the Legato multiplier examples. For the more advanced folks, I talk about uninterpreted functions and axioms. Crypto-folks like the AES example and C-code generation. More fancy people like the regex-puzzle example: https://gist.github.com/LeventErkok/4942496
Yeah I'm not sure what is going on with the LF characters. I'm using Tumblr and I'm not sure how it's treating the text (thought it's clearly messing it up). Any suggestions on how to fix it would he helpful! Edit: Oh I thought you were talking about the article. Yes the HTML source of Racemetric has some extra LF characters which is a feature of Heist to make it idempotent. Doug, the author, could give you more detail on that.
I looked at Keter a bit but it just seemed fairly blackbox and complex for what it's intended to do. In fact /r/ocharles pointed out I can even remove Angel from my setup so rolling out to a new box is literally just upstart/systemd and nginx. I've realized I tend to shy away from the Yesod/Persistent etc world. I think it's because the tools just seem too overly complex and opaque to me. Maybe it'd help if you could give a simple writeup of how you're using Keter and what it's doing for you. Thanks for the feedback!
My girlfriend who isn't all too familiar with Haskell or a lot of programmings reactions: “No instance for (Integral Float)” Oh god, why are you showing me that? What does it mean? How am I supposed to know, I don't know Haskell “Float is not an instance of Integral” Huh... I'm not sure but that reminds me of css. Maybe kind of like when you get an attribute wrong? My theory: The second is friendlier, less scary, and makes it easier for people to form an intuition of what is meant.
The fact that the result is a structure of linear maps is pretty well-known, though still routinely obfuscated. As is the fact that there are two arguments to the differential operator. This latter obfuscation is due largely to Leibniz notation, which is a crime against nature. As for the differential manifolds part of things, that wasn't well-known to me :) All the same, noone's done the work necessary to convert that knowledge into a usable API for optimization libraries. Or if someone's done it, they never got it into any of the popular libraries. So even if it's "well known" among differential geometeurs, it's never made it everyone else doing linear algebra and real/complex analysis.
In my linear algebra library for Haskell, I don't believe in matrices since "matrices" can mean either linear transformations, bivectors, or bilinear forms. Instead I explicitly treat everything as (m,n)-tensors; and doing so cleans up so much of the code. I also have a nice hack for dualizing things so that we really do get that `(X^op)^op = X`, instead of just that they're isomorphic. Of course, the hack only really works for finite-dimensional tensors since otherwise we don't have that `(X*Y)^op = (X^op)*(Y^op)`.
https://nixos.org/wiki/Installing_NixOS_in_a_VirtualBox_guest
good error message must include link to StackOverflow answer with specialisation to kinds of error (e.g, “No instance for (Integral Float)” leads to other page than “No instance for (Show (Int -&gt; String))”
Awesome, thanks! That was an off-the-cuff idea. We could probably do even better (on all errors, not just this one) if we gave it some more thought.
The scipy / scikit-learn have optimization/learning on manifolds. We may want to check out their papers and see what we can do about these in Haskell I guess :-)
Verily thou troll’st.
I hope you are aware that SHA1 is not considered safe anymore?
"Not considered safe anymore" for what ? I use it to detect file corruption on the backend storage, not for deduplication. I want to use it for file deduplication too but in that case identical SHA1 sums would trigger a complete file comparison. If I provide Git/Bup as an optional bucket format, the "unsafety" would be per bucket and up to the user.
Haven't looked too closely at what scipy offers, but numpy suffers from the same problems as every other API I've looked at. E.g., if you take a look at [their L-BFGS-B function](http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_l_bfgs_b.html), their notion of "a suitable functor `F`" is "exactly `narray`" which is far too stringent and is extremely obnoxious to work with. Not to mention, the bounds should be of type `F (Min,Max)`[1] to ensure that the variable indexing matches up— but instead they just use a `list (Min,Max)` which makes it too easy for things to get mismatched. Also, why a `list` instead of using `narray` like everywhere else? These sorts of type errors are flagrant examples of why I hate all the available libraries for doing this sort of thing. Compared to the above, their conflation of `∆ℝ ⊸ ∆ℝ` with `ℝ` is nothing. At least this type error is mathematically standard and doesn't severely impact the usability of the API. [1] Presumably we really want `F (Open X)` where `Open X` is the type of open sets on `X`. But if we're only dealing with reals, then an open interval is fine. The main point is that these bounds should be stored in an `F` structure.
I have a friend who just now struggles with the GHC error messages. (To be fair, he complains about the error messages in every compiler that isn't gcc, but that's for another day.) Please continue this. It's looking really good so far, and I sense potential in it as a series!
Here is a script for the lazy: curl https://www.cs.uoregon.edu/research/summerschool/summer13/curriculum.html 2&gt; /dev/null | \ grep -o 'lectures\/.*\.mp4' | \ sed -e 's|^|https://www.cs.uoregon.edu/research/summerschool/summer13/|'| \ xargs wget -Nc
Actually, this problem is actively tackled by some other compilers for Haskell, for example: [Helium](http://www.haskell.org/haskellwiki/Implementations#Helium) , which was previously used for teaching Haskell at Utrecht University (now replaced with GHC though) &gt; Helium is a functional programming language and a compiler designed especially for teaching Haskell. Quality of the error messages has been the main concern both in the choice of the language features and in the implementation of the compiler. The language is a subset of the Haskell language. The most notable difference with Haskell is the absence of overloading. The compiler keeps track of a lot of information to produce informative messages. 
I havn't re-invented scp nor rsync. I have re-implemented the SCP protocol because I want to be compatible with it. One of the thing I want to do is to re-implement the rsync protocol. My scp implementation is just adding a few read/write in database for account permissions, and write the whole file SHA1 upon upload, and read it upon download to make sure users don't download corrupted files. (The SHA1 is also used by the background jobs to detect corruptions early.) Specifically about rsync, I don't have any rolling checksums in place. I couldn't do anything useful with them since the SCP protocol couldn't use them. Offering an "SCP proxy" instead of a "rsync proxy" doesn't mean at all that I (have to) reinvent rsync. The whole SHA1 thing is about whole files, and is used to detect corrupted files (live on disk or at download time).
&gt;lack of a really good linear algebra in Haskell. What about [linear](http://hackage.haskell.org/package/linear)?
IMPORTANT EDIT: A keter bundle is a tar.gz tarball, NOT a zip archive as I originally wrote. Sorry for the error! keter is really simple, but the documentation is a little lacking. Here's how to use it: 1. Have your web app check the PORT environment variable when it starts up, and have it serve on that port instead of port 80. 2. Create a fresh directory (on any computer, not necessarily the server). Copy your web app executable into it, together with any other files and directories it needs, and create a file called `config/keter.yaml` containing these two lines: exec: relative/path/to/your-executable host: the.domain.name.of.your.web.app.example.com 3. Tar-gzip the folder (not zip). By convention, give the tarball file name the extension `.keter` instead of `.tar.gz`. This zip file is called a "keter bundle". 4. Copy the keter bundle to the directory `/opt/keter/incoming` on the server. keter now deploys your app and starts various kinds of useful logging. Look around in `/opt/keter`; everything is pretty obvious. If you remove the keter bundle from incoming, the web service stops. If you replace it with a different bundle with the same name, keter hot-swaps the new version of your app. Repeat for each web app running on the server. For multiple isolated copies of the same web app running on different domains, use multiple copies of the same keter bundle, with different values for the `host:` field in `keter.yaml` and different names for the keter bundle. That's the basics. There's a lot more available, of course. If you're using a Postgresql DB, you can optionally have keter deploy and manage your DBs. You can tweak all kinds of things about reverse-proxying. Etc. Under the hood, keter works by reverse proxying. keter serves on the main port (80 by default, but configurable), and reverse proxies to the various web app instances running on the server. Yesod/Persistent in general - other than keter - is oriented towards web developers who are experts in all the many things that are needed to make a web site work well and not necessarily very much Haskell. To a web developer, Yesod/Persistent is similar to other industrial strength web frameworks, except simpler and more powerful. That orientation sometimes gives that "opaque" feeling at first to people who are more Haskell hacker and less web developer. The tutorials intentionally do not always make it so easy to see all the details of how the type machinery is working behind the scenes, because that would be distracting and confusing for the main intended audience. Once you get the hang of navigating the haddocks of the various packages, though, it's not hard.
Tekmo's comment is on point—I spoke without understanding the context well-enough. The two important points to GPL are that (1) it is an infectious license in that if you build anything which contains GPL code then that thing must itself be released as GPL and (2) it provides the rights to anyone who has access to a binary product of your code to also have access to the source. The upswing of that is that including GPL code in an application which gets widely distributed will force you to open source your code and further infect anyone's application which builds atop it. It's a powerful force for aligning people to open source their code, but it's not always feasible or reasonable to open source your code and so it's always worth being on the lookout for GPL compared with MIT/BSD which simply release the code to free use so long as it doesn't harm the author.
Is there any particular reason you're using SHA1 instead of SHA3?
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Secure Hash Algorithm**](http://en.wikipedia.org/wiki/Secure%20Hash%20Algorithm): [](#sfw) --- &gt; &gt;The **Secure Hash Algorithm** is a family of [cryptographic hash functions](http://en.wikipedia.org/wiki/Cryptographic_hash_function) published by the [National Institute of Standards and Technology](http://en.wikipedia.org/wiki/National_Institute_of_Standards_and_Technology) (NIST) as a [U.S.](http://en.wikipedia.org/wiki/United_States_of_America) [Federal Information Processing Standard](http://en.wikipedia.org/wiki/Federal_Information_Processing_Standard) (FIPS), and may refer to: &gt; &gt;* **[SHA-0](http://en.wikipedia.org/wiki/SHA-0)**: A retronym applied to the original version of the 160-bit hash function published in 1993 under the name "SHA". It was withdrawn shortly after publication due to an undisclosed "significant flaw" and replaced by the slightly revised version SHA-1. &gt; &gt;* **[SHA-1](http://en.wikipedia.org/wiki/SHA-1)**: A 160-bit hash function which resembles the earlier MD5 algorithm. This was designed by the National Security Agency (NSA) to be part of the Digital Signature Algorithm. Cryptographic weaknesses were discovered in SHA-1, and the standard was no longer approved for most cryptographic uses after 2010. &gt; &gt;* **[SHA-2](http://en.wikipedia.org/wiki/SHA-2)**: A family of two similar hash functions, with different block sizes, known as SHA-256 and SHA-512. They differ in the word size; SHA-256 uses 32-bit words where SHA-512 uses 64-bit words. There are also truncated versions of each standard, known as SHA-224 and SHA-384. These were also designed by the NSA. &gt; &gt;* **[SHA-3](http://en.wikipedia.org/wiki/SHA-3)**: A hash function formerly called Keccak, chosen in 2012 after a public competition among non-NSA designers. It supports the same hash lengths as SHA-2, and its internal structure differs significantly from the rest of the SHA family. &gt;The corresponding standards are [FIPS](http://en.wikipedia.org/wiki/Federal_Information_Processing_Standard) PUB 180 (original SHA), FIPS PUB 180-1 (SHA-1), FIPS PUB 180-2 (SHA-1, SHA-256, SHA-384, and SHA-512). NIST has [said](http://nvlpubs.nist.gov/nistpubs/ir/2012/NIST.IR.7896.pdf) that FIPS 180-5 will include SHA-3. &gt; --- ^Interesting: [^SHA-1](http://en.wikipedia.org/wiki/SHA-1) ^| [^Cryptographic ^hash ^function](http://en.wikipedia.org/wiki/Cryptographic_hash_function) ^| [^SHA-2](http://en.wikipedia.org/wiki/SHA-2) ^| [^Secure ^Hash ^Standard](http://en.wikipedia.org/wiki/Secure_Hash_Standard) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cfiasui) ^or[](#or) [^delete](http://www.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cfiasui)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/) ^| [^flag ^a ^glitch](http://www.reddit.com/message/compose?to=/r/autowikibot&amp;subject=Glitched comment report&amp;message=What seems wrong: (optional description goes here\)%0A%0A---%0A%0AReply no. 61408:%0Ahttp://www.reddit.com/r/haskell/comments/1y61bw/i_am_building_reesd_a_redundant_storage_service/cfiaspq)
Why are you suggesting SHA3 instead of SHA1 ? Are you worried that an attacker would gain access to the backend hard-drives to change a file, but withouth both changing its SHA1 hashes and its size (that's what the background jobs uses to detect file corruption) ?
Please see my answer to rpglover64. I don't use SHA1 for cryptographic purpose (nor as an ID), just as a checksum to detect early that a file on disk is not corrupted.
&gt; To be fair, he complains about the error messages in every compiler that isn't gcc, but that's for another day What, even if he swaps gcc for clang?
Ok but now suppose that you have `class Foo a b c` and the typechecker wants to throw `no instance for (Foo Bool Bool String)`. Now you can't do that same rewrite you suggested... So you could rewrite in some cases but not others, and now you've traded off easy beginner errors for nonuniform error messages for the "same" problem as code gets slightly more complicated...
Thanks a lot!
Thanks for taking the time to write this up! They should put this on the front page to kinda show what keter is all about. What you mentioned about yesod makes a lot of sense. I've used Django and Rails in the past and also felt they were to opaque. I would certainly consider myself more of a Haskell hacker than web dev. Again thanks for the write up!
&gt; We see that class Num — the most abstract number representation — has four possible instances Classes are not representations, and have potentially unbounded instances. Even in the prelude there are more than four instances of Num.
Then you probably don't need a cryptographic hash function and could get better performance from 'normal' hash functions like CRC32 or Adler32. Regarding the SHA3 over SHA1 thing: The german wikipedia page says that the NIST (which is responsible for the SHA algorithms) regards SHA1 as deprecated. But there is no link to a official announcement stating that.
Haven't made that experiment yet. (He's very good at not wanting to try new tools when the old ones work just fine.)
It would be very interesting to have a Haskell implementation of rsync. It's probably not very easy though. Anyway, this is a very nice service you've created.
Play around with them and you start to get a feel. You might want to google "well-typed interpreter" or "phantom type". 
You can do it all with existentials and type equality. See, Foundations for Structured Programming with GADTs, Patricia Johann and Neil Ghani. POPL 2008.
[[pdf]](https://personal.cis.strath.ac.uk/neil.ghani/papers/ghani-popl08.pdf)
&gt; but some of them can still be written with ADTs -- it just takes more machinery Really? Do you mean with less type safety? The type safe evaluator example can't be written without GADTs as far as I can tell.
Actually, it can. -- Looks just like the GADT class Term term where lit :: Int -&gt; term Int succ_ :: term Int -&gt; term Int isZero :: term Int -&gt; term Bool if_ :: term Bool -&gt; term a -&gt; term a -&gt; term a -- Our evaluator is only superficially more complex (requires a newtype) newtype Eval a = Eval { eval :: a } instance Term Eval where lit = Eval succ_ (Eval x) = Eval (1 + x) isZero (Eval x) = Eval (x == 0) if_ (Eval p) a b = if p then a else b -- With RankNTypes we can even define a type that can be interpreted multiple ways type T a = forall term. Term term =&gt; term a -- now, for example, eval can be instantiated with type T a -&gt; a It's worth noting that as long as you construct a `T a` instead of an `Eval a`, you can only have constructed it with the methods of the `Term` type class, so you can't actually construct any "exotic" terms (terms constructed using special knowledge about its representation instead of just the intended syntax).
It can be done with existentials and passing type equality proofs around. It's just not pretty.
This is a great idea for using Haskell to create a TLS library. But wouldn't you then lose most of the advantage of a pure Haskell library? If you bypass the compiler's usual code generation, the only obvious way for other Haskell code to use it would be via FFI. And in that case, why not just use the existing C libraries?
An alternative would be to provide a supplemental error message. "This could be caused by Float not being an instance of Integral" or something similar. That way you have a uniform error message while also having an easier to read message.
But note that you need a specific sort of type equality, which is stronger than the Leibniz equality you can get with rank-n types. For instance, you can't write a total function :: `(forall p. p Char -&gt; p Bool) -&gt; Void` without GADTs (or type families).
The [finally tagless](https://www.cs.rutgers.edu/~ccshan/tagless/jfp.pdf) approach is how languages that lack GADTs usually accomplish this. 
As a result of this thread, the author of keter has already added some of what we both mentioned to the README on the [keter github site](https://github.com/snoyberg/keter).
Alp, you forget, I'm writing that meta API too. Might be getting it out soon too
&gt; A framework is typically associated with a system-wide cross-domain non-composable inversion of control ... Yes, frameworks are highly opinionated. Sometimes, those opinions are aligned with my goals, and sometimes they aren't. I'm not saying you are for or against frameworks. I am simply stating that sometimes I agree with the opinions of a framework, and sometimes that choice can be too limiting in the sense that we can spend more effort treating edge cases than creating our overall solution to the problem(s) at hand. 
It seems that most (all?) things enabled by GADTs can also be accomplished with ADTs when you also have RankNTypes, but the solution with GADTs is usually much nicer. EDIT: I see that /u/augustss and /u/roche have now referenced a paper that shows that you can always do it. Here's an example of the kind of problem that GADTs help solve: [A practical Haskell puzzle](http://www.haskell.org/pipermail/haskell-cafe/2011-February/089719.html) The cafe archive lost track of the thread a few times, and some of the nicest solutions to the puzzle came later on. Here are the additional thread tree heads: * http://www.haskell.org/pipermail/haskell-cafe/2011-March/089771.html * http://www.haskell.org/pipermail/haskell-cafe/2011-March/089783.html * http://www.haskell.org/pipermail/haskell-cafe/2011-March/089802.html 
Judging by the current size of my VM, not really. I think it's smart enough to know when you already have a dependency and just symlinks it into the bundle.
Are you the author of the code in the video?
&gt; One of the great things about Haskell is that you can pick and choose what code should be pure. I completely agree. Having complete control over my application's purity is fiercely liberating. 
This is asking a lot, and please excuse my ignorance if you have already made this decision, but do have plans to write up an analysis of your workflow and source code, and are you going to send this up-stream to NetBSD?
Umm... I think the Ajhc Haskell compiler is not good to write UNIX-like kernel. It will be explained by natural language. I'll try to learn ATS language and rewrite NetBSD kernel with it.
Maybe write document explain my technique to rewrite C language with Haskell. Would like to write a paper about it.
I guess it resonates with Haskell fans because it is based around ideas of purity. Builds are pure functions on dependencies and source code, thus allow for reproducible builds, rollbacks, binary substitution and lots of other goodies. The expressions are also written using lazy evaluation.
It can require a lot of disk space, but there are tools to mitigate those problems. `nix-collect-garbage` will remove things from your Nix store that are no longer referenced by anything, which can clean things up drastically between deployments. This can be configured to run automatically. Also, there is `nix-store --optimise`, which will find binary identical files in the store and hard link them together. Recently, my laptop went from 41GB to 19GB.
Not quite a symlink, see [this answer](http://www.reddit.com/r/haskell/comments/1y2vx8/nixos_added_to_distrowatch_after_7_years_on/cfim9we)
Yes, that would be great. I am willing to donate my time and assistance by editing/technically reviewing the paper; meaning, I will ask you a lot of questions to help you clarify and elucidate your points.
Arafura is seriously cool!
I like to think of GADTs as four independent extensions: 1. Equality constraints - this extension adds a new constraint `a ~ b` which can be used on the LHS of =&gt;. For example: `f :: a ~ b =&gt; a -&gt; b; f = id`. By itself, it gives little extra power. 2. Qualified constructors - this extension allows to define datatypes such as `data A a = Num a =&gt; B`, here to construct `B` you need `Num a` in context and conversely pattern matching on `B` supplies `Num a`. 3. Existential quantification - this extension allows to use `forall` to introduce new variables. For example `data Stream a = forall s. Stream s (s -&gt; s) (s -&gt; a)`. This quantification can be reduced to a type `data Exists f = forall a. Exists (f a)`. 4. GADT syntax - no extra semantics, just syntactic sugar `data ... where` which gets desugared to normal ADTs + possibly extensions 1,2,3. Examples: * `data Equal a b = (a ~ b) =&gt; Eq` uses 1 and 2. * `data Showable = forall a. Show a =&gt; Showable a` uses 2 and 3. * `data Id a where Id :: a -&gt; Id a` uses 4 and is trivially desugared. * `data Equal' a b where Eq :: Equal' a a` uses 1,2,4 and is desugared to `Equal` * `data Exists2 f where E :: f a` uses 3,4 and is desugared to `Exists` * `data Weird a b c where W :: Weird [x] (x,x) y` uses 1,2,3,4 and is desugared to `data Weird a b c = forall x y. ([x] ~ a, (x,x) ~ b, y ~ c) =&gt; Weird` In GHC, it's a mess: 1 is enabled by TypeFamilies or GADTs, 2 is enabled by ExistentialQuantification or GADTs, the `forall` syntax from 3 is enabled by ExistentialQuantification, 4 is GADTSyntax. GADTs give 1+2+3+4 except the `forall` syntax.
The offerings in matlab, R, and even C++ are *much* better.
Oh, that's neat. Thanks for the clarification. I'll have to keep those tools in mind.
&gt; It seems that most (all?) things enabled by GADTs can also be accomplished with ADTs when you also have RankNTypes This once again reinforces to me that `RankNTypes` is my favourite extension.
Like i said: you have a point. I just, as explained before, don't see it so black-and-white.
GADT's _are_ existentials and type equality proofs. That's still GADTs, just minus the syntactic sugar (in fact GHC requires GADTs if you try and encode things that way).
What is the advantage of using an add-on like this vs just having buildbot call some shell scripts to call cabal, etc?
Suggesting SHA3 because it's considered newer and better, and I, personally, would default to SHA3, so I was wondering if you have any particular reason.
Under the hood all cabal methods simply call buildbot's ShellCommand. They are just convenient wrappers.
[SHA-3 has yet to be finalised](http://en.wikipedia.org/wiki/SHA-3#NIST_announcement_controversy), so it's probably wise to not depend on it just yet.
Sweet! It's a pain to have to install s3 tools/scripts to do this, and as you say, most machines already has scp. I'm interested in how you integrate, if at all, Docker into Haskell development. During development, do you run your Haskell application in a Docker container, or you only use Docker to run containers on your production environment? Regarding a private Docker registry SaaS, note that you'll have some competition: [Quay.io](https://quay.io/) and [Docker Inc](http://blog.docker.io/2014/02/introducing-the-docker-beta-group/). Random thought: I don't use a private Docker registry SaaS because I can't trust third parties with my code/images. It'd be neat if there was a Docker registry proxy which encrypted images in such a way that didn't stop the network bandwidth benefits that layered images offers us.
I've done some on again off again stuff with SBV. Always impressed by how elegant and well put together it is. Thanks for all your hard work, Levent!
Yeah, I've never seen GADTs defined any other way.
If GHC didn't have coercions natively, you could still roll your own equality proofs. You'd have to manually package them as constructor arguments, manually use them to cast values from one type to another, and manually build up complicated equality proofs out of simple ones. Of course, if your compiler didn't understand type equalities you'd probably have to implement `cast` with `unsafeCoerce`, but the *interface* of the equality proof library would be safe.
&gt; you could still roll your own equality proofs. How, without GADTs?
[ PDF](https://github.com/meiersi/HaskellerZ/blob/master/meetups/20130829-FPAfternoon_The_Haxl_Project_at_Facebook/The%20Haxl%20Project%20at%20Facebook.pdf?raw=true)
still a rose tree. if you don't want to store anything at certain nodes you can just use the maybe type.
As someone who doesn't have much experience with any linear algebra package (I still do matrix multiplications with my left hand moving horizontally and my right hand moving vertically), what makes a good linear algebra offering?
This seems to work fine for me: instance (Num a) =&gt; Num (Maybe a) where (+) (Just a) (Just b) = Just (a + b) (+) Nothing _ = Nothing (+) _ Nothing = Nothing (-) (Just a) (Just b) = Just (a - b) (*) (Just a) (Just b) = Just (a * b) abs (Just a) = Just $ abs a negate (Just a) = Just (negate a) fromInteger = error "error" signum = error "error" Not to talk down to you, but just in case, typically it is much easier to use functors, monads, or applicative to do math from a Maybe type. For example, importing Control.Applicative, you can do: liftA2 (+) (Just 1) (Just 2) = Just 3 liftA2 (+) Nothing (Just 2) = Nothing same goes for Control.Monad with liftM2: liftM2 (+) (Just 1) ...
Thank you very much. I can see now where my mistake was. I understand it is easier to use Functors or Monads, but I wanted to have a function like: f x = x^3 + x^2 + x and be able to compute f (Just 3) 
I believe that is the original PDF that was presented. My question was whether there have been any additional presentations.
You can! Check out this example: module Main where import Control.Applicative f x = x^3 + x^2 + x main = putStrLn $ show $ liftA f (Just 3) Or like you said use Functors. fmap f (Just 3) --or f &lt;$&gt; Just 3
I see. Thank you very much for the example. 
For a package to be acceptable, it would need every technique in the *Numerical Recipes* book. hmatrix pretty much has this. Good libraries have *many* more techniques, and clean syntax that directly maps to the math. Haskell falls on its face here. I suspect that a language like Haskell has the potential to outshine these other offerings, but the work just hasn't been done.
could you encode a type equality proof? data Eq :: a -&gt; a -&gt; * where Refl :: Eq a a
This combination of hard links and garbage collection is really neat!
[Idris](http://www.idris-lang.org/) is a Haskell dialect intended for systems programming. I don't know of any projects like this using it, but it may be worth looking into.
Idris has runtime? ATS has no runtime.
A sketch: module Equal (Equal(), (:~), cast, refl, sym, trans, app, first, second) where data Equal a b = Refl -- Constructor not exported type (:~) = Equal cast :: (a :~ b) -&gt; a -&gt; b cast Refl = unsafeCoerce refl :: a :~ a refl = Refl sym :: (a :~ b) -&gt; (b :~ a) sym Refl = Refl trans :: (a :~ b) -&gt; (b :~ c) -&gt; (a :~ c) trans Refl Refl = Refl -- This version requires kind polymorphism; there might be ways to avoid it. app :: (f :~ g) -&gt; (a :~ b) -&gt; (f a :~ g b) app Refl Refl = Refl -- These two are unsound in the presence of non-injective stuff; a real system would have -- something more refined. first :: (f a :~ g b) -&gt; (f :~ g) first Refl = Refl second :: (f a :~ g b) -&gt; (a :~ b) second Refl = Refl Clients of this module can have all the power (and none of the convenience) of GADTs, by basically writing down the System FC desugaring themselves. 
Cool! But consider getting a $2 camera tripod for these videos.
maybe i misunderstood you, so please correct me if i am wrong. what about the binaries to download? can i verify they correspond to the source? (i'm pretty sure that's not solved. see verifiable builds discussions.) if that's not possible, how can i delegate trust, i.e. verify binaries? when building the binaries myself and _hand-rolling_ installation it's easy to verify binaries (sign and verify). but is this build into the tools yet?
Please add dogecoin/bitcoin too
What word did I redefine?
I never played with S3 actually. Someone on HN said that you could just install s3cmd to achive something similar. I always use Docker. So whenever I run one of my processes, it is via Docker. Launching on my laptop or in production (and later on multiple machines) should be done with the same orchestration script. I plan to blog about my use of Docker. One of the thing I do is have a base image with all my Haskell dependencies already installed (I don't have GHC on my host machine, compiling is done with Docker). Yep, I know about Quay and obviously about Docker, inc. I have started to collect such services [here](https://gist.github.com/noteed/6882636). Every hosting solution I found propose a per-image pricing. I want to propose a used storage pricing, and not penalize for creating a bunch of smaller images. Anyway I think there are other possible differentiating factors. Yes, encryption seems a nice thing to have. I would like it to be a purely client-side concern. Thanks for your comment!
Apropos 1: Not with `Ord`, but an additional `Bounded` constraint should do it yes? newtype MaxMon a = MaxMon {unMaxMon :: a} instance (Ord a, Bounded a) =&gt; Monoid (MaxMon a) where mempty = MaxMon minBound mappend (MaxMon a) (MaxMon b) = MaxMon $ max a b Apropos 2: No. You can't define two instances of any typeclass for a given type. The approach Prelude and most people take is to wrap types in a newtype layer and make instances for those.
I don't think I can with the same power as the type equalities provided by GADTs, although I may be able to get close.
This is really interesting stuff! Could you give some examples of how these things can be used? It's far beyond my knowledge. Is, for example, `Equal a b` actually equivalent to `(a ~ b)` in some way? But `(a ~ b)` can only be used in a context wheres `Equal a b` is an actual type?
Here is another "must read" intro to monoids in Haskell by Dan Piponi: http://blog.sigfpe.com/2009/01/haskell-monoids-and-their-uses.html 
I have never seen a transaction involving more than 10 cents worth of dogecoin.
&gt; Apropos 1: Not with Ord, but an additional Bounded constraint should do it yes? Yes, and there are actually two valid monoids, one defined by `(minBound, max)` as you implemented and one defined by `(maxBound, min)`.
Could you elaborate on "broken"?
Hopefully Simon has something new to share at [ZuriHac 2014](http://www.haskell.org/haskellwiki/ZuriHac2014) coming June.
As /u/xpika said this is a rose tree. The [standard tree from `containers` package](http://hackage.haskell.org/package/containers-0.5.4.0/docs/Data-Tree.html) is a rose tree and equivalent to yours if you use `Nothing` as the empty label or `Just String` othewise. leaf s = Tree (Just s) [] tree ts = Tree Nothing ts empty = tree []
Idris isn't intended for that level of systems programming, as far as I know.
I'm very poor man... #orz
You can in fact make a generic instance for lifting Num a to a monad, e.g., Maybe. instance (Monad m, Num a) =&gt; Num (m a) where (+) = liftM2 (+) (-) = liftM2 (-) (*) = liftM2 (*) negate = liftM negate abs = liftM abs signum = liftM signum fromInteger = return . fromInteger Edit: Even better instance (Applicative m, Num a) =&gt; Num (m a) where (+) = liftA2 (+) (-) = liftA2 (-) (*) = liftA2 (*) negate = liftA negate abs = liftA abs signum = liftA signum fromInteger = pure . fromInteger 
Applicative is enough.
Very true.
Yesod has a lot of moving parts, and so I wouldn't call it "simplest" in the abstract. but it's pretty simple to *use* in my experience.
The following body can be made to work for any Applicative: import Control.Applicative instance Num a =&gt; Num (Maybe a) where (+) = liftA2 (+) (-) = liftA2 (-) (*) = liftA2 (*) abs = fmap abs signum = fmap signum negate = fmap negate fromInteger = pure . fromInteger 
One thing no one has addressed yet - this is certainly cute, but how good or bad an idea is it in production code (and why)?
You probably don't want such a general instance in scope, as there are other Num instances that take an argumnt of kin *, but defining it as a one off for each Applicative you want it to hold for works fine. =)
AFAIK, Idris is more comparable to Agda than to Haskell.
Portability is about how hard it is to port software; something is said to be portable when you can simply compile from source regardless of your platform. The most common classes of "non-portable" software are 1) source distributions containing assembly code or binary blobs, and 2) source code written with assumptions that don't hold for every platform (e.g. `sizeof(int)==4`). It's not optimal at all that the code size is large and that the Idris-built binary has more dependencies, but this says nothing about portability.
Typically you would want to use the Functor or Applicative nature of Monad to accomplish this, since it creates a stronger boundary between "I might have a Num" and "I have a Num".
i like oleg's lecture notes when learning about finally tagless. http://okmij.org/ftp/tagless-final/course/lecture.pdf
Interestingly enough, `aggregate` is already provided in Data.Monoid as a part of the Monoid typeclass as `mconcat` :) It's default implementation is `mconcat = foldr mappend mempty`, but it's a part of the typeclass so that it can be overridden and optimized on a per-monoid basis. 
I also have an interest in OS development with functional languages. The main issue you tend to run into is that most of the really nice language features are also dependent on a runtime system which makes them unsuitable for bare metal development. For the purpose of (limited) driver development you can usually get away with it, but if you're trying to write something like a kernel or a bootloader that tends not to work so well. There's a really interesting project to make a EFI bootable binary using Rust, but that works by essentially stubbing out the symbols for all the std c lib calls, and avoiding any Rust functions or features that depend on them. I've often wondered how hard doing something similar in Haskell would be, but I suspect you'd end up with some very very restrictive and very non-idiomatic Haskell code as a result (I.E. lots and lots of unboxed values, Ptr values, and C FFI types). It might be an interesting exercise to see what the minimal support structure you'd need in place to bootstrap a proper GHC runtime would be and then to implement just that bare minimum in something like Rust. Then you'd have a 4 phase boot process, EFI -&gt; Bootloader -&gt; Rust Bootstrap -&gt; Haskell Runtime. You might optionally be able to merge the Bootloader and Rust Bootstrap into a single step though.
Why don't you implement `fromInteger` and `signum`? `fromInteger i = Just (fromInteger i)` `signum Nothing = Nothing` `signum (Just n) = Just (signum n)`
That's not equivalent. There is no `empty` in OP's tree. And `Data.Tree.Tree (Maybe String)` has many other values that do not correspond to any value in OP's tree. So using that type instead would lead to ugly and fragile code that would need to handle "cases" that can never arise. It's possible to construct an isomorphism between OP's type and `Data.Tree.Tree (Maybe String)` (in many different ways), but it would be complicated and unintuitive. You would lose the advantages of using the library. I would recommend that OP stick with the type as defined; it's subtly different than the one in `Data.Tree`, even though both are commonly called "rose tree".
Why is it getting bitten? It's finding out that Haskell is strongly typed. I wouldn't want Haskell to have automatic type coercion; I've learned from other languages how painful that can be.
Admirably avoiding the shameless plug, he neglects to mention that he has also written one of the most popular introductory textbooks for the Haskell programming language. If you liked his paper, perhaps you should buy a copy of his book. :)
Some years ago, I started writing a code generator for augmented multiway tree data structures. It was one of a string of attempts to replace an earlier C++ template library that wasn't so flexible (or efficient or well written...). This particular project died because I was distracted with writing code generators for AST handling and code/text template handling to help write it and various other yak shaving / procrastination issues. Anyway, I did get constant built-into-the-program tables working before it died. The way I specified the augments is possibly relevant here. The multiway tree structure I used was a tweak of B+ trees. All data items are stored in leaf nodes, which are all at the same depth and form a linked list. Sequential access only needs the leaf nodes. The "augments" consist of extra data, primarily stored in the branch nodes (every node that isn't a leaf). The extra data in a particular branch node summarizes the items in the subtree descended from that branch node. For example, for key-based searches, the items in the tree should be in key order. The summaries can be (equivalently) either the first key or the smallest key in the subtree. The scheme for calculating a summary used the following set of computations... 1. A "filter" function, taking a single item and returning a boolean result. The item is included in the summary only if the result is true. 2. A "map" function, computing a single-item summary from a single item. 3. An "identity" value - a value to use as a summary of zero items. 4. A "bind" function which takes two summary values and returns a combined summary value, using an operation that's required (but not checked) to be associative. 5. An "unbind" function which "subtracts" one summary from another. The identity value is only required if there's a filter function (because a leaf node needs a summary to pass up even if all items are filtered out). The unbind is optional - used to optimize recalculating summaries when items are deleted. Some of this was semi-inspired by a then superficial knowledge of Haskell monads, but it's better to look at it as specifying an algebra of summaries. The bind and (associative) identity are sufficient to specify a monoid. The filter and map are just used to define the sequence of values combined. The unbind only makes sense for commutative monoids, which always have an inverse, so an unbind conceptually does a commutative bind with an inverse summary as it's second argument (just as subtraction is equivalent to addition of a negated value). For key-based search, I'd not define a filter, map would just extract the key from the item, and bind would return the first/least of the two keys. Another simple case - filter again not specified, map always gives 1, bind is addition, unbind is subtraction. The summary is the number of items in the subtree, which can be used for e.g. O(log n) search for an index. Add filtering to that and you can search for the nth item in a particular subset. This was a long time ago, but it's still embarrassing that it took a while before I realized this was what I was working towards, looked at my old discrete maths book that I once wrote off as useless, and finally got that straightened out. And of course in Haskell, the idea of needing to write a DSL just to do that kind of thing in the restricted context of that particular data structure is laughable. It's also a lot easier than my OTT approach in C++, of course, but Haskell is IMO better. 
Great! Thanks!
And this makes a (bounded) lattice.
You mean spelling out the liftA2 and such explicitly?
Many of the "normal" num instances don't satisfy the laws you would normally expect. For example, quickcheck (n+m+1 :: Float) == (1+m+n :: Float) 
Just so you know, supporting those two is tricky at the moment. So we are unlikely to add it as an option unless there is a high demand. **Explanation:** We are incorporated through Software in the Public Interest (SPI), and as far as I know they haven't made an official statement on the matter. Instead, I can tell you that a similar organization, the Software Freedom Conservancy (SFC), has provided an explanation saying that depending on how bitcoin/dogecoin are viewed they can't hold them for member organizations. They see it as risky and requiring a lot of administrative overhead. None of those objections are specific to SFC so we have no reason to think that SPI would see it any differently. Here is an example of SFC talking about bitcoin: https://identi.ca/bkuhn/note/89CRDJjPQeuz5EuMVIhj3A Now, there is at least one alternative to accepting the coins themselves, we could convert the bitcoins to USD at the point of donation using a service like this: https://bitpay.com/bitcoin-for-charities I would need to look into that more before I can say anything definite.
I am very excited about the potential for numerical, statistical and data-processing libraries coming out of this. Scala is becoming *very* popular in finance, and it would be great to see Haskell making similar gains. It's already in use, but more would be better :)
Let's look at some code here: {-# LANGUAGE ExistentialQuantification #-} newtype Automaton a b = Automaton {runAutomaton :: (a -&gt; (b, Automaton a b))} data Stateful s a b = Stateful {runStateful :: (a -&gt; s -&gt; (b,s))} data Stateful' a b = forall s. Stateful' (Stateful s a b, s) toAutomaton :: Stateful' a b -&gt; Automaton a b toAutomaton (Stateful' (stf,s)) = Automaton $ \a -&gt; let (b,s') = runStateful stf a s in (b, toAutomaton (Stateful' (stf,s'))) fromAutomaton :: Automaton a b -&gt; Stateful' a b fromAutomaton aut = Stateful' (Stateful stf,aut) where stf a aut = runAutomaton aut a This is a simplified version of the arrows(the Stateful' is your StateResult). First of all, note that something important about Stateful' is that it has a state, but it hides it behind an existential quantification(yours does that too, only that it uses a GADT instead): that means that the only thing that you can do with it is apply it to the stateful function that comes with it. That is important, because this is not different at all from an Automaton: the only difference being that that the automaton hides the state a bit more, namely inside the function. So to convert a Stateful' to an Automaton, we make an automaton, that, on each step, runs the Stateful on its state and the input and returns the result and the Stateful' with the next state. And to convert an Automaton to a Stateful', we just make a Stateful' that has an automaton as its state and whose stateful computation runs the automaton and puts the next one on the state. This sounds complicated, but in fact makes a lot of sense: the point is that a self modifying function and a function with a state are equivalent.
I hope this is OK to post to /r/haskell - the compiler is written in Haskell, and Idris very much like a strict, dependently typed Haskell. It seemed of interest to the Haskell community in any case.
Ajhc development will shift to "passive maintenance mode". Does not support to fix difficult BUGs. https://github.com/ajhc/ajhc/issues/45 Does not support to add major functions. https://github.com/ajhc/ajhc/issues/25 But would like to keep it able to compile and the regression test green, for future brave hacker. Chic also has a BIG chance on something outside the field of kernel domain. Android NDK, or small application. 
Ah. Sorry. It's my mistake to use the word. I mean "Idris is difficult to be port to out of POSIX API."
The helper functions `&lt;$&gt;` and `&lt;*&gt;` would work better IMO. a + b Becomes (+) &lt;$&gt; a &lt;*&gt; b If you were doing a lot of logic you may want to create a Num instance, but you would probably want to wrap it in a `newtype` and provide methods to convert to and from.
in that every total order is a lattice...
System APIs are also part of portability, but I don't see how that Gist is even remotely related to that... all I see there is a comparison of the file sizes, the number of dependencies, and the number of undefined symbols. How is that related to POSIX?
I've been working on a book to help get beginners started on Snap development. This is part of the chapter on digestive-functors (and a link to pre-order). If anyone wants to see more of the book I'd be happy to show it to them.
Permission granted. :-) Very interesting project.
I was playing around with Heist and digestive functors a bit for an encrypted-at-rest sign up form for privacy advocacy actions I've been build. The biggest points of friction for me figuring out how to combine the digestive functors heist splice with my own. Needed the "." combinator. https://github.com/zmanian/SafeSignUp
Speaking of opportunities to use monoids, MapReduce is a prime candidate. The HyperLogLog monoid linked at the end of the article is a neat way to compute approximate set cardinality on a large scale. Twitter put out some nice Scala libraries like Algebird for this kind of thing. 
The other answers here are good, but I want to check something -- are you doing this as an interesting exercise, or on advice from ghc? If it's the latter, chances are there's a type error in your code and your real problem is that, not the lack of a `Num` instance for `Maybe a`.
What are some of the coolest code artifacts written Idris thus far?
Rather than `Par` and `parMap`, you could use `ParIO` and `parMapM` to print whenever a chunk completes. I think the following would work (printing a "*" at the start of each chunk): parMapChunk' :: NFData b =&gt; Int -&gt; (a -&gt; b) -&gt; [a] -&gt; ParIO [b] parMapChunk' n f xs = fmap concat $ parMapM g (chunk n xs) where g xs = do liftIO $ putStr "*" return $ map f xs 
Laziness is precisely one of the things that makes Haskell beautiful.
Thank you. I will give that a try.
You might change your mind after reading this: http://augustss.blogspot.hu/2011/05/more-points-for-lazy-evaluation-in.html?m=1
I believe that SPJ said in a presentation that lazy evaluation by default was not the main win with haskell, but it was usefull to keep the language designers honest concerning purity. With a lazy language, you have to find a solution to the IO problem. And to the problem of managing effects. You cannot give up saying that it is too complex. You have to find something like monads to deal with it. http://research.microsoft.com/en-us/um/people/simonpj/papers/haskell-retrospective/HaskellRetrospective.pdf I think some people would now prefer a strict by default language with a convenient syntax for lazy evaluation. Lazy evaluation is still useful to create composable structures. It is also useful to have data structure with nice amortized performances. 
But relying on an optimization is not so beautiful. 
Edit: the example has been convincingly debunked below. I need to think more about it. "Broken" was too strong a word, but Leibniz equality `eq_cons a b := (forall c. c a -&gt; c b)` quantifying on type constructors `c` (that do not compute) is not as strong as the usual notion of Leibinz equality, as definable in a proof assistant, `eq a b := (forall f . f a -&gt; f b)` for arbitrary type operators. For example, if I write `(a, b)` the type-level pair (at the product kind `k1 * k2`), and not the type of term-level pairs, I can derive `eq (a, b) (c, d) -&gt; eq a c`, but not `eq_cons (a, b) (c, d) -&gt; eq_cons a c`. We instantiate the type operator on the first projection, which is not a type constructor. 
I am happy enough to rely on that kind of optimisation, its like expecting tco to be applied
&gt; Forcing strictness feels likes helping the compiler Actually doing programming at all is helping the compiler, at one level or another. If we didn't have to help the compiler make a useful program, the profession of software development would disappear overnight and anyone would be able to write a spec and run the compiler on it. As a software developer, you get paid for helping the compiler! You're one of the few people in the world who know how to help the compiler properly, and you can actually get quite a lot of money for that.
I know laziness is good for a lot of things, but the example, isn't it a flaw?
Is there a practical use for `foldl`?
But we "help the compiler" for performance, not for correctness.
First you say you don't want to help the compiler, and now you say you don't want to rely on the compiler! :) Look, laziness and strictness are both useful concepts to use in the implementation of software, but combining them can be fiddly, and no language yet gets it perfectly correct. Haskell is certainly one of the best languages for combining them, if not the best, largely because of the amazing work GHC's authors have done on optimizing both evaluation methods. However, suppose I try to implent an *even smaller* part of your example in a strict language: [x..y] = if x &gt; y then [] else x : [x+1 .. y] Oops, my naivety has caused a stack overflow. OK, instead I can use the accumulating parameter trick: [x..y] = reverse (backwards x y []) where backwards x y z = if x &gt; y then z else backwards (x+1) y (x:z) But isn't that "helping the compiler"? 
I think it's a mistake to consider tail calls to be an optimization. I know that in the imperative and the object oriented world they like to talk about tail call optimization as a compiler optimization. But in functional languages tail calls are not an optimization, it's part of the semantics of the language. And thankfully so, since, at least in Haskell, it's the only way to do any form of iteration. Any valid implementation, optimizing or not, will give you proper tail calls.
Call-by-need has a noncompositional performance model for space usage: you can't tell how much space a variable reference will use without knowing about all of its other occurrences in the rest of the program. That's pretty much game over as far as beauty goes, I'm afraid.
Can you give an example of a variable reference that has unbounded space usage and where the strict equivalent would not?
That's also part of what we get paid for. If any twelve-year old could write programs that ran at optimal speed there'd be much less money going around. 
This is a great question. Maybe if you want to process a lazy structure formed from a list, where the structure is built from the list in reverse order? But then it's clearer just to use `reverse` and `foldr`.
I completely agree that stack overflows are a serious problem. The Haskell type system provides a lot of benefits when it comes to writing reliable software, but in practice I've found that just because some particular Haskell program can handle a small file, that doesn't mean that it won't crash when given a larger file. That's frustrating. I'd like to have some confidence that my program isn't going to mysteriously break because of arcane memory management reasons, even when there's plenty of free memory available. It does look like the situation is going to improve, though, according to the release notes for the next GHC release: &gt; The default maximum stack size has increased. Previously, it defaulted to 8m (equivalent to passing +RTS -K8m. Now, GHC will use up-to 80% of the physical memory available at runtime. http://www.haskell.org/ghc/docs/7.8.1-rc1/html/users_guide/release-7-8-1.html I am curious if the way ghc handles stacks has changed at all, or if they just changed the default value to be huge? In the latter case, is there any downside?
No, because it's not unbounded -- it's just noncompositional. Whether or not a variable reference allocates memory depends on whether the thunk it is bound to has been forced previously or not, and that depends on the behaviour of the *rest* of the program. As a result, you can't reason about the space usage of programs under by-need evaluation in terms of the space usage of their subterms. I have on my to-do list to write a paper about using separation logic to reason about the space behaviour of call-by-need. It will be a useful thing to have, but mainly I think it's funny to use separation logic on a pure programming language. 
The case you posted is also about performance -- if you run it with a 10MB stack, it will succeed.
In practical terms, one of the things that people does not mention about laziness is that it eliminates the trade-off between coarse-grained and fine grained API's. That is, I no longer think about how big is the data structure returned by my API call, since if the caller need just one or two fields of it, my serving call will not spend resources generating the other unused parts of the data. So it does not matter. Look: my API call can pass to the caller an entire database tree just giving the top node. Then the caller can forget my API, and can navigate the three as if the entire tree were already there, but what really happens is that his code is calling my code when needed without his concern, using as much memory and resources as needed but not more. if beautifulness, according with evolutionary psychology, is a relation power/simplicity... Can you imagine something more beautiful? The second thing although related with the first, is that laziness eliminated the trade-off between streaming interfaces and function call interfaces. I can pipe an infinite stream trough a chain of procedures. In an strict language, that has to be done trough explicit read and write of stream blocks.
Having `foldl` in the standard library is a flaw, but it's too late to fix it.
But if you can bound it, surely the bounds are compositional, even if the exact space usage is not.
I believe it's because of the ticket filed at the end of this thread. http://www.haskell.org/pipermail/libraries/2013-August/020652.html
When I evaluate that expression in "GHCi" it evaluates immediately. The optimizer knows how to simplify that expression to a strictly evaluated expression. Lazyness allows the graph reduction of the program to be very *very* flexible, which makes it very easy to optimize. Almost always a lazily evaluated expression can be optimized to the fastest possible code. In fact, Haskell is one of the fastest high-level languages. Please refer to this linke: http://www.haskell.org/haskellwiki/Performance/GHC I never worry about performance when writing Haskell. I worry about correctness and elegance and beauty. When it comes time to hand-optimize my program, then I worry about placing strictness annotations. But most of the time, the compiler emits the optimal code for most expressions.
Why is it a flaw?
Still a goal worth achieving.
Do you know a use for `foldl` where `foldl'` is not equally if not better suited to the task? I would be interested to see one.
True, but on the other hand, foldl is just using stack space, just as foldr is. No one is questioning foldr, though.
But `foldr` doesn't necessarily consume stack space, since it can be evaluated lazily.
Space invaders gets my vote.
&gt; and now you say you don't want to rely on the compiler I meant we should not rely on code that once we turn off the optimization it produces runtime error.
[Regarding TCO and Haskell.](http://www.haskell.org/haskellwiki/Tail_recursion)
The strict version would fail when there's a bottom in the list, last = foldl1 (\_ x -&gt; x)
But you can write last = foldr1 (\_ x -&gt; x) instead. It should be "where either `foldl'` or `foldr` is not equally if not better suited to the task".
Because it's a trap for the beginners. If it was at least in `Data.List` instead of `Prelude`, it would already be better.
Would just having a lazy stream type be enough? That seems to cover 99% of the uses of laziness people have in this regard.
Writing code in any language always involves decisions about whether to delay evaluation and whether to save the results to be reused later. Haskell just makes the resulting code more notationally convenient. You are not doing anything fundamentally different. The only difference in this regard is the convention that laziness, not strictness, is the default when you believe you have no reason to prefer one over the other. The real flaw is that foldl is in the standard library. 
I think there's still tons of criticism of the defense industry, it's just less talked about right now than finance because nothing's changed there recently so there's a little less to talk about that everyone hasn't heard before.
The sudden failure of Facebook seems like it *would* put some serious hurt on Zynga. I don't know near enough about Zynga's finances to say whether it would fail, but that's not meaningfully different from Lehmen &amp;c. I agree that a bailout would be less likely.
It's about the number. 100,000 is not large enough for your environment. For me, it is about 100,000,000 to get stack overflow.
But that's what call-by-value does -- just use the upper bounds always (aka force everything and let the gc sort it out).
i don't even know what this means. what about f(a) for some unknown a, and certain obvious to write functions f? In general, in a strict language, we _never_ know how much space usage f takes without knowing he exact value of `a`. How is that any better? in a lazy language at least i can say "you will use no more than you force" while in a strict language i have no clue what you will use.
SPJ was asked once, "Why Comic Sans?" and his reply was basically "Why not?", saying that people love to hate on it, but never give actual reasons why its bad, they just don't like it. Apparently there are actually practical benefits to it for dyslexia, or something, too.
My experience with languages like python in this regard is that it works.. right up until your problem doesn't fit into the niche it covers, and then you have to break all of the code you've written into something completely different and alien to the culture to get that last 5%. Scala also exists. It has a lazy stream type, etc. It is just hell programming with it.
I know of no use for `foldl` over `foldl'` that isn't artificially constructed to have bottoms in strange places.
Both call-by-value and call-by-name have compositional cost models, in that the cost of evaluating a variable in evaluation position is the same for all variable occurrences. For call-by-value, this value is always 0, since you only ever substitute values for variables. For call-by-name, the cost is always the cost of evaluating whatever expression you are substituting for the variable. In both of these cases, computing how much it costs to evaluate a particular variable occurrence does not depend on any *other* variable occurrences. This is not true for call-by-need, because by using thunks to implement laziness, you need to reason about sharing and interference to reason about performance. 
Absolutely. The ideal compiler would be one you talk to and give examples and it makes a program for you. That's what we should be aiming for. I'm just saying that while we're not there yet, having to specify some things "manually" is not wrong -- it's the natural way of things. With that said, if you have a solution to the problem, feel more than welcome to submit a patch to GHC or write a paper. (Or both.) Until then, everybody knows that laziness is trouble sometimes, strictness is trouble sometimes, and compilers cannot yet figure out by themselves when to use what. 
Right, using `foldr` is in fact the correct solution, since one shouldn't be using O(n) space to find the last element of a list. I think the criterion should be "where a left fold is appropriate but `foldl'` is not equally or better suited not the task".
This is what I don't understand. Surely call-by-need can only ever *improve* time performance and never worsen space usage, assuming you correctly delay everything you don't want to be shared. (Your language would have to happily support preventing sharing. GHC optimizations make this a bit tricky!). Perhaps an concrete example of something hard to analyse would help.
I don't hate Comic Sans, I kind of like it actually. Comic Sans don't fit only some occasions. So, I was just joking, because I know the jokes. (￣y▽￣)~ giggling However, to be honest, I think this slides has used too much sans-serif font. (Which happened to be Comic Sans.)
"If you got some rational reasons why I should not, then I'll listen to them. But just being unfashionable - I don't care." - SPJ
&gt; Forcing strictness feels likes helping the compiler Forcing it in a few select places feels bad, but having it by default everywhere except where you need to force laziness (ie using a strict language) is OK? Personally, I'm nervous about laziness too, but the more I get used to the idea the less nervous I get. Don't forget - the compiler already does strictness analysis. My mental model (which is probably still naive) is that you should aim for "course grained" states. That is, at some point, you have an imperative view of the world with a succession of states - because I/O or because data structures or whatever. With a strict language, you end up specifying all the detailed in-between states in sequence. In practice, the compiler will re-order some of that, but you still have to understand that fine-grained sequence to understand the code, even though that's really unimportant detail. Laziness doesn't really avoid evaluation order issues - it's a different evaluation order. But you can still think of it as specifying what you want while leaving the details for later. When you force evaluation for the next coarse-grained state (or relevant aspects of it) the compiler figures out all the intermediate states. Compiler optimizations still mean the use of thunks etc will be minimized, but you avoid having to specify every little detail of evaluation order. Basically, if you want the compiler to decide the evaluation order of everything, you're going to have a slow program or you'll be waiting centuries for the compiler to finish for non-trivial programs (because that degree of optimization has a nasty computational complexity). But by specifying a few key things, you control the scale of the problem that the optimizer has to deal with in the cases where space complexity and the constant time factors really matter. Mathematicians get a lot of respect for proving things, even though the solution and all possible proofs are inherent in the specification of the problem. Just because it's there, doesn't mean finding it is easy or even (as Godel might point out) possible. 
These two errors are different things. Strict and lazy **foldr** both get stack overflowed, but they get the same result. foldr (+) 0 [1..100000000] or in C: int f(int x) { return f(x-1) + x; } Code like this is a mistake regardless of what language, and whether lazy. It's just because these code involves potential deep recursion. But **foldl**, the lazy version fails, and strict one does not. From how foldl is implemented, foldl f a [] = a foldl f a (x:xs) = foldl f (f a x) xs There is only a tail-recursion, no deep recursion. The stack overflow is because that **(f a x)** is not evaluated immediately, which expand into foldl (+) 0 [1..100000000] = foldl (+) (1+2+3+4+5+6+..n) [(n+1)..100000000] And finally **(1+2+3+4+5+6+..n)** overflowed. The **foldr** error is owing to the unawareness of potential deep recursion. The **foldl** error is owing to how laziness is implemented. If it's necessary for users to know about the implementation, it could be more beautiful. *I know perfection cannot be achieved, but we are all trying to get things better. :)*
I first used generators in Icon. I didn't much like Icon and never used it for anything serious - it was just one of the languages I played with for a while many years ago, and one where I liked some of the key ideas. The combination of generators and "alternation" in Icon seems really good for goal-directed searching. In some ways, I thought of it like imperative Prolog - though that could be a seriously misleading comparison. As a solution for a particular kind of problem in Icon, generators are very nice. And IMO, they're even better in Python. But OTOH, some of the things done with generators in Python worry me - they seem to fight the abstraction. Maybe I'm just not getting it. 
This is a dubious example. There is not necessarily any product kind `k1 * k2` with primitive operations `Fst :: k1 * k2 -&gt; k1` and `Snd :: k1 * k2 -&gt; k2`. And if there were, by hypothesis, one could write, for instance: newtype T p q = T (forall c. c (Fst p) -&gt; c (Fst q)) reflT :: T p p reflT = T id -- Fst (a, b) = a -- Fst (c, d) = c -- forall p. p (Fst (a, b)) -&gt; p (Fst (c, d)) = forall p. p a -&gt; p c = Eq a c out : T (a, b) (c, d) -&gt; Eq a c out (T f) = f mk : Eq (a, b) (c, d) -&gt; T (a, b) (c, d) mk f = f reflT -- f@T (a, b) :: T (a, b) (a, b) -&gt; T (a, b) (c, d) result :: Eq (a, b) (c, d) -&gt; Eq a c result = out . mk Generally, you can turn non-constructors into constructors by using `newtype`/`data`. It is merely (very) inconvenient.
Your Japanese style emoticons are too much. Especially with the words after them.
You're right that call-by-need never has worse asymptotic time complexity than call-by-value. However, its space usage can indeed be worse. For example, the very example in this post --- `foldl` --- can use linear memory on calls where the call-by-value version takes constant memory. (In this case, by-need evaluation behaves like by-name.) The Haskell function which does run more like cbv (i.e., `foldl'`) uses `seq`, which is flatly an abomination (`seq` breaks the eta-law for functions). It is wisest to pretend that `seq` does not exist, IMO. 
&gt; Can you imagine something more beautiful? For now, I can't. But if, I wish, someone can, he would be able to make a perfect Haskell.
Also lazy io isn't a problem as often as people think it is.
Ah interesting. It looks like you're saying that the claim is that the space usage under call-by-need can be asymptotically worse *with polymorphic functions*. With anything monomorphic you can fix space leaks by using monomorphic `seq`s. (By the way, if I was supreme dictator of all Haskell, I'd revert to the typeclass approach to `seq`.)
Really? They're pretty closely related, surely. Lazy IO is IO that is done when forcing parts of a lazy data structure.
Addition isn't commutative (neither is multiplication, but that might not qualify as an expected law) and neither distributive law holds.
&gt; No one is questioning foldr, though. **foldr** gets stack overflow because it involves potential deep recursion. foldr k z = go where go [] = z go (y:ys) = y `k` go ys But **foldl** does not. (It's a tail recursion.) The stack overflow is not about the recursion, but the lazy-evaluation. foldl f z0 xs0 = lgo z0 xs0 where lgo z [] = z lgo z (x:xs) = lgo (f z x) xs 
This is fantastic, thank you. I just pre-ordered your book and I'm very excited, I've been playing around with Snap but if there is anything I felt that was lacking it's a better overview of the framework as a whole. Good job :D
Yeah it means you don't have to think about optimisation issues like "I need the first N of this huge list". Just "generate" the huge list and I'll take what I need thank you. A good example is a compiler. A compiler effectively takes a bytestream, converts it into a lexeme stream and then converts the lexeme stream into a parse tree. Then you do semantic analysis on the parse tree and generate code. You could create a lexical analyser that dumps the entire bytestream into a list and then pass that list into the parser. You could create one that is like a lexeme dispenser. It has a pump that generates tokens from the bytestream as you ask for them so you can do the compile in one pass. In Haskell these two solutions are the same.
It's not offensive, it's just incredibly weird and overly expressive.
It works very well for certain well-defined scenarios, which are often exactly the ones you want in simple programs that just want to read some data and don't care how it happens. It's an immediate dead end once you start to care about how or when IO happens, and has a few highly unintuitive pitfalls like the closed-before-reading thing, both of which make it a bad idea to present to beginners as the *(snide scarequote)* simplest *(end snide scarequote)* way to read files. It's a "problem" in some quasi-moral sense involving purity and referential transparency and suchlike only for purposefully very contrived scenarios contrived for that very purpose.
All languages from Fortran and on have had some "built in" functions, called things like intrinsics, or builtins, etc. Some of these were probably written in the language itself (I could imagine, e.g., SIN being written in FORTRAN early on). So I think the idea of a "prelude" has been around a long time. For functional languages, Lisp came with a lot of functions most of which were written in Lisp itself, but I'm not sure they had a name for these. The first time I heard the name "prelude" for these definitions was probably in Ponder (Fairbairn&amp;Wray) in the early 80s.
I think what we need is a way of marking things deprecated, and eventually making them require opt-in to a backwards compatible standard library so we can remove them from future versions of the regular standard library. Get rid of head and tail, while we're at it.
Well, it's one character shorter and imported by default. So `foldl` wins over `foldl'` when it comes to golfing for source code size. A ringing endorsement, to be sure.
Or *stingy evaluation*, which is simpler. But as far as I know it has never been implemented in a serious way.
The issue is, we want almost everything to work this way. In haskell you don't have go lazy if you don't want. There are ways to force the language to be strict. Do a search and see what percentage of all haskell code uses it. I bet it's under 1%.
&gt; The ideal compiler would be one you talk to and give examples and it makes a program for you. Hasn't gained much traction out of research but exists: https://en.wikipedia.org/wiki/Inductive_logic_programming
You can have laziness without lazy IO. To propose that the drawbacks of lazy IO are drawbacks of laziness is incorrect.
I used to work in education. Studies show that, of the commonly available fonts, Comic Sans is the easiest to read, especially among those that have difficulties. In short, it's the most *functional* font ;)
Ah, true. And while someone working with these might well not themselves expect the applicative effects to commute they don't (and shouldn't have to) know what assumptions are made by any polymorphic functions they call. Is "applicative functors which preserve commutativity and such" interesting?
In other words, `seq` is bad but `deepseq` is ok (ignoring the whnf vs nf issue)?
Wow this is great :)
Though there is no good story for how to do 'interesting' pattern matching in the evaluator using finally tagless. In theory, the pattern matching doesn't add any actual power, but in practice it's extremely convenient for the same reason we have ADTs and pattern matching rather than just using Church/Scott encoding for all our data types...
Ok, now write this one in C: replace p ys = foldr (\x xs -&gt; if p x then ys ++ xs else x:xs) [] It should work for any input, no matter how long the list is. And no loops--this is a recursive algorithm, and we shouldn't be helping the compiler by performing tail-call optimization for it.
Writing the meta API is harder than the backends 
You're right about that. I was just saying I think they're not *orthogonal*.
That's a great idea.
Thank you. I realized that was my mistake after /u/ratzes post. 
That looks the best way to do it. Thank you. 
Laziness is a property of haskell language. Lazy IO is a design decision for specific library. You can use a different IO library that is not lazy without changing your lazy data structures. 
&gt; call-by-value version takes constant memory Only with TCO. Without optimisations call-by-value foldl use linear memory (on stack). With strictness analysis and subsequent optimisations call-by-need foldl also takes constant memory. So where is the difference?
Ok, good. Every so often you see a newbie trying to giving a Num instance to e.g. functions when what they really needed to do was fix the code.
I think digestive functors and similar libs in yesod are a dead end. They are very complex and have quite steep learning curve. Yet work only for simplest cases. Any non trivial forms, especially the ones that do not fit into a simple grid-like layout and complexity of making those with formlets rises so quickly, that it is just easier to hand code the html. Besides dynamic internet is moving away from static forms. Frameworks like angularjs, emberjs etc promote a new way of interacting between client and server: passing json messages.
I think `deepseq` being a class method is just a coincidence.
With the laws I mentioned plus `x*0 = 0` (part of distributivity, really) the only applicative functors I can think of that would obey the laws are `(-&gt;) r` for any type `r`.
Thanks. I think you could do it by wrapping a type family application in a newtype?
&gt; "foldl'". Man, that apostrophe is hard to see. :) 
Sure you can. Lazy sequences are pretty common these days, e.g. generators in Python or `IEnumerable` in C#, and those languages certainly won't stop you from putting all sorts of side-effect-filled stuff into a construct like that, which will then be executed at unpredictable times if at all, or sometimes more than once. It's generally a bad idea there, too.
Just some small corrections: * the looping example should probabily use _foldl_ rather than _foldl1_ (otherwise it wouldn't work for the empty list) * your conditional execution is doing doing IO as well as conditionals; also the _do_ keyword is missing in the Haskell version.
 Prelude&gt; foldl (+) 0 [1..100000] 5000050000 Prelude&gt; 
Regarding natural numbers: whats wrong with data Nat = Z | S !Nat ?
Have you tried this in ghci? It won't overflow the stack. You can raise the upper bound by a factor of 100 and it won't overflow. I even wrote a naive foldl inline, and *that* didn't overflow with an upper bound of 10^7. (Don't add that eighth zero, though...) You're three orders of magnitude away from trouble. `foldl`is a nice, straightforward example of what can go wrong with laziness. But I thing that, with the best of pedagogical intentions, we've allowed people to think that it's worse than it is.
It's just that I thought you wouldn't expose interfaces to accelerate/repa. Nevermind, then :-) Also, you say "soon" but I'd really need it now (and have been feeling a need for it for years), that's why I feel the urge to write it, although I can wait a bit if it actually is out soon.
Hmm ... I'm not sure I'd call that lazy IO, but I'm not the arbiter of such matters so I concede the point. I'll restate my claim as "I don't think you can really have lazy IO without involving lazy evaluation of something".
That type contains `_|_`. However, I'm still not quite sure I see the problem, because in a strict language you can have a non-terminating expression of whatever type, so it seems like there is not much practical distinction.
Are you really going to make the claim that a 12-year-old can write software that is correct up to runtime costs? ಠ_ಠ
It's the easiest to read *in certain circumstances*. You wouldn't want to read War and Peace set in Comic Sans MS, but it works jolly well for presentation slides and other short snippets of text, when it's set in a big size.
I get my bottoms in all the strange places, if you know what I mean.
Could you put a link to the homepage on hackage? I was under the impression that there was no documentation.
I'm mostly on Debian systems. I don't use the HP because I'm a relatively experienced Haskell programmer and I tend to chase the bleeding edge of a bunch packages. The HP does work too well for this because it constrains things a bit too much. My installation procedure is as follows: * Install ghc, alex, happy, hlint and cabal-install from the Debian repos. * Use the Debian GHC to install multiple versions of GHC in my home directory, allowing me to switch GHC versions by modifying my $PATH (I currently have 7.4.2, 7.6.3 and 7.9). * Remove the Debian version of ghc leaving alex, happy etc. * Install whatever I need using cabal install as I need it. One thing to note is that I tend to use Debian testing which has a very recent version of cabal-install. The most recent versions allow and correctly manages separate package installations for each version of the compiler. 
I've wondered this but never bothered to question it. I was wondering if we could get any insight from some senior Haskellers. At least one person thinks it's a bug in the grammar.
Why are people down voting this?
If we could get rid of it, that would be nice.
But the excuse in a strict language is that it's function arrow that is partial instead. 
The Haskell subreddit is a pretty kind place, unless you're blatantly trolling most people here will assume you're discussing in good faith. Don't need to do anything else.
With some exceptions of short-circuiting operators like `(||)` and `(&amp;&amp;)`, `k` is usually going to be some kind of a constructor and people usually call co-recursion for some reason (it constructs co-data, but the data/co-data distinction doesn't make much sense in Haskell as Haskell doesn't really meaningfully distinguish the two). "Co-recursion" in Haskell is at least as important if not more so than the more familiar "tail call elimination" it inherits from other functional languages. Perhaps a bit of a surprise; `foldl` will blow the stack by constructing a huge expression call but `foldr` may not because it doesn't have to build the huge expression all at once! Less surprising when you realize that lists are really just huge, perhaps infinitely long expressions.
Yes, pretty much. 
&gt; One thing to note is that I tend to use Debian testing which has a very recent version of cabal-install. The most recent versions allow and correctly manages separate package installations for each version of the compiler. As long as you add ~/.cabal/bin to your path and then do `cabal update; cabal install cabal-install`, that should be okay regardless of which version e.g. Debian stable ships. You can verify that the cabal-installed cabal-install is being used with `which cabal`.
Cool. =) A couple of comments: 1. Snap 1.0 should be out in the next few months, which most notably will use of the `io-streams` library instead of `enumerator`. Do you plan on revising the book soon after? (And I do think that covering streaming is worthwhile...) 2. I really cannot recommend the use of snaplet-postgresql-simple at the present time, because it does not handle transactions correctly at all. 3. Threepenny-gui may well be worth a chapter as well; its not suitable for general-purpose web development, and certainly not anything public-facing, but I'm finding it useful for prototyping some interactive webapps that, for now, will only be used in production on a high-quality low-latency not-quite-LAN. (Eventually it might prove useful to implement a proper web UI, that would be usable on networks with greater latency.) The second point isn't terribly relevant for my work, as not many of my web projects are public-facing, and accordingly I need a different type of connection pooling. I do have a few side-projects that would be public-facing if they come to fruition, but I've found the lack of transactions to be debilitating. So instead I use a hacked-up, stripped down variant of `snaplet-postgresql-simple`. On the first count, I do have some some code that's almost-but-not-quite ready to release, which implements `io-streams`-based interfaces to postgresql's copy, cursors, and large objects, available [here](https://github.com/lpsmith/postgresql-simple-streams/), which I am currently using to stream data with Snap.
For anyone concerned that allowing it would make the grammar harder for humans to understand, let me remind you that the grammar as it stands already allows this: \f -&gt; \x -&gt; f x :: Int :: (Int -&gt; Int) :: (Int -&gt; Int) -&gt; Int -&gt; Int 
That’s me. I experience a whole iota of suffering every time I have to write: forM xs $ \x -&gt; do … Instead of: forM xs \x -&gt; do … Also, this would remove the need for the `$` typing hack because we could write `runST do …`
What a lovely example :) Though you have a nice example of hard Haskell syntax, it is contrived to the extend that i cannot remember seeing more then one `::` on a line in all the Haskell I've read so far. However.. someFunction $ do ..and.. forM xs $ \x -&gt; do ..I frequently encountered. 
Is that [ML](http://en.wikipedia.org/wiki/ML_%28programming_language%29)?
I don't understand what the multiple ::'s mean.
Hm, fair point.
Nonsense. I make lazy/infinite trees all the time. 
Tone.
&gt; \f -&gt; \x -&gt; f x :: Int :: (Int -&gt; Int) :: (Int -&gt; Int) -&gt; Int -&gt; Int The first defines the type for (f x). The second defines the type for \x -&gt; f x And the third defines the type of the whole lambda expression.
Making forms dynamic is a primarily front-end concern (and to that end, passing JSON messages is not a new, previously unsolved problem). Pre-Angular, given html, it was still trivial to make even the most static forms dynamic with javascript. Simply listen for the form .submit and hijack it. Then, in the return call, deal with the information you get back. Worst-case scenario, you can just replace the DOM with the html you get back from the server. Given client-side validation (which should be happening anyway with something like [Parsley.js](http://parsleyjs.org/)) it should be fairly rare to accept data that can't be parsed unless it's a deliberate attempt at passing information that doesn't conform. To which "grid-like layout" are you referring? I'm curious as to why you think the html structure of a form can not be changed. In any case, you may be interested in [digestive-functors-aeson](http://hackage.haskell.org/package/digestive-functors-aeson-1.1.5/docs/Text-Digestive-Aeson.html) which uses digestive-functors to provide errors in JSON format. As a side-note, if you want something indexed by google, serving a blank page and using `ng-cloak` while the page is being setup won't cut it.
I was reading the type signature backwards. I hope that never becomes idiomatic haskell.
Well, all of that in installed from source on Gentoo, and the ebuilds + eclasses are basically instructions for how to do that. So, [this repository](https://github.com/gentoo-haskell/gentoo-haskell) will probably answer whatever question you might have. Here are [the ones for GHC.](https://github.com/gentoo-haskell/gentoo-haskell/tree/master/dev-lang/ghc) And these are [the Haskell Platform meta-ebuilds](https://github.com/gentoo-haskell/gentoo-haskell/tree/master/dev-haskell/haskell-platform). If you’re unsure what a certain thing means, here’s [the ebuild manual](http://devmanual.gentoo.org/ebuild-writing/). And finally, if any ebuild references any [eclass, those can be found here](https://github.com/gentoo-haskell/gentoo-haskell/tree/master/eclass). But you probably won’t need any of that. Although it’s of course easier if you OS/distribution already allows from-source installation.
Thanks for pre-ordering!
alp, you do realize Accelerate has a Repa backend you can use, right?
Ah the fairbairn reference was what I was looking for! I know the notion of the stdlib itself had a long history but the name "prelude" is so perfect I wanted to hunt down where it came from. (I was doubly inspired to ask this because the one HoTT lib underway has an "overture", which is positively adorable)
Sent you a message about seeing more of the book.
Messaged you about about the book.
Is there an equivalent to -9999 (live) ebuilds on Debian? On Gentoo you can just unmask the -9999 versions, and install things like normal. So maybe…
It won't even break existing code. $ $ == $.
Holy shit... that makes so little sense.
These are not the only backends I am interested in.
Actually it does make sense, if you parse it correctly. Which is the problem, because the parsing rules here amount to something like "parse it in the unique way that makes sense". Basically, the body of a lambda extends as far to the right as possible until it hits something that can't parse as part of the lambda body. Of course, whatever comes next has to be valid in the outer context, and generally there's not much you can put *after* a lambda on the same line that will parse but not as part of the lambda. But in this case, the successive type signatures make it stop parsing each lambda body by annotating the expression inside the lambda, allowing the following type signature to associate with the surrounding lambda. Allowing stuff like 'runSomeMonad do ...` would potentially allow similar abuses since the justification is the same--only one parse makes sense anyway, so use it. In practice, nobody actually does stuff like this except for entertainment purposes, like my example. :] EDIT: Not sure why the parent comment is being downvoted. That's pretty much the reaction that bit of code was designed to provoke.
Don't worry about that. If memory serves me, this example was specifically contrived because I was reading the language grammar and realized how tolerant of pathological formatting the "parse until you can't" rule for implicitly-ended constructs are, and wanted a good demonstration.
That last bit is enough to persuade me, TBH. It's an incredibly silly hack.
i know that. we just spoke about it on gchat before you posted this comment :) (i think it was before) point being, having a good user experience thats expressive is tricky if you want both deep and shallow embeddings and an expressive substrate. 
And let's not forget [`Data.Foldable`](http://hackage.haskell.org/package/base-4.6.0.1/docs/Data-Foldable.html), the "[c]lass of data structures that can be folded to a summary value," which is based on monoids: fold :: (Foldable t, Monoid m) =&gt; t m -&gt; m foldMap :: (Foldable t, Monoid m) =&gt; (a -&gt; m) -&gt; t a -&gt; m 
I actually prefer to install the cabal-install from Debian. I completely blow away my cabal installed packages every 2-4 weeks and its kind of nice not to lose cabal-install in this process. 
What do you mean “tone”?
Thank you for your feedback. I didn’t aim to draw up a comprehensive list, just few examples. But you’re right about _foldl_ and _foldl1_.
The opening section isn't about anything to do with the headline but is just an excuse to divide people into the camps of interested/fluent in FP or uninterested/ignorant and therefore wrong. This is not a moral issue and shouldn't be made into one. Nobody should be looked down upon for the programming languages they use or the education they've had. 
Thanks, this explanation made it clear to me exactly what the issue is. I've been learning some functional programming concepts and lazy concepts and it wasn't obvious to me at first why you would even need `foldl'`
Sorry, I am not sure, what silly hack are we talking about here?
&gt; It’s a simple picture of a house that a child might draw. Pff, yeah... a child.
Here is another excellent paper on monoids by Brent Yorgey http://www.cis.upenn.edu/~byorgey/pub/monoid-pearl.pdf 
Hmmm, but why? I would have thought it wouldn't make any difference.
Keep in mind that the above code is just syntactic sugar for this: (+) = \x -&gt; case x of Just a -&gt; \y -&gt; case y of Just b -&gt; Just (a + b) Nothing -&gt; Nothing Nothing -&gt; Nothing When you look at it like this, it kind of makes sense that the cases for one value should all be together. There is no technical reason why this restriction is in place (as long as all cases are in the same module).
See [this Stack Overflow question](http://stackoverflow.com/q/9468963/157360) for an explanation.
Thanks!
Very nice! I also think pipe composition is a wonderful and promising way to structure programs. (Not surprisingly, considering the success of UNIX pipelines, and the allure of graphical languages to which pipes are the ASCII art counterpart...)
That is because (++) and (:) are lazy, but (+) :: Int -&gt; Int -&gt; Int, is strict. When evaluating (a + b), the final value of a and b are required. replace :: (a -&gt; Bool) -&gt; [a] -&gt; [a] -&gt; [a] if you just do some print/show in ghci, it's OK. show (replace [1..10000000] 1 [2]) But if you eval something like: last (replace [1..10000000] 1 [2]) It will overflow.
Sorry, I was unhelpfully terse. I didn't downvote this, neither did I vote it up. I'll try to give some constructive criticism. The opening section which divides people into groups comes across as arrogant, and also doesn't seem accurate in my experience. It doesn't mean you should leave it out, but perhaps you can propose your opinion in a friendlier way. For example if you had started with a personal anecdote which supported your groupings and then written something like, "and so this caused me to wonder if programmers could be grouped in these following categories..." Secondly, in the following section on looping, you describe a thought process, but it's very confusing to follow. The part where you seem to be calling the reader a "filthy machine" is kind of odd. It reveals personality, but it was also alienating. Keep writing and practicing! I hope this criticism was constructive enough.
Why not xmonad?
I switched from xmonad to i3 a while ago. I find it Just Works better than xmonad does. I've done very little configuration from it out the box, and I'm much happier with it. The idea of containers that can be split without affecting the rest of the layout is exactly what I want (and is how stumpwm used to work, when I started with tiling window managers).
Any pictures? :)
Thanks.
http://i.imgur.com/x9vEhVX.png Right, I totally forgot to include a screen shot!
I'd be curious to see how close. Would it involve using Leibnizian equality somewhere along the way?
For me it's mostly laziness. I used XMonad for a while a few years ago. But then I had to upgrade my desktop machine and all, and I dropped it. And this time I really wanted to switch from gnome, but didn't want to invest a lot of time setting it up so it fits me perfectly. And with i3 it was much easier. And yeah, I agree with ocharles, I also like the i3's model better.
Thanks a lot! I had no idea that existed!
From what I can see, many Nix packages run ugly and error-prone snippets of shell code to do stuff at installation time. This seems to be an area where the Guix fork of Nix does better, since it uses scheme (in funtional style, it even has its own monad) at both package build time and install time.
It's a problem specifically with snaplet-postgresql-simple. The problem is that snaplet-postgresql-simple adds a connection pool of the type you'd typically want for a public-facing webapp, and (although the pool allows for it) it doesn't expose any way to reserve a connection for the duration of the transaction, and have all the intervening queries issued on that connection. Thus, when you use a purported transaction, your `BEGIN` might go out on one connection, your intervening queries might go out on another, and the `COMMIT` or `ROLLBACK` might go out on a third. And other queries from concurrent requests might be issued on the connection that's actually in a transaction. So basically, in my stripped down version of `snaplet-postgresql-simple`, all I have is a single `withPG :: (HasPostgres m) =&gt; (Connection -&gt; IO a) -&gt; m a` function that grabs a connection and does something with it. It's not quite as convenient, but it is correct. Unfortunately restricting the handler to `IO` also means you can't really use this method to stream data from the database to web users, but I didn't need that and that avoids potential exception-handling complications. I have a sketch of a proper solution (that also attempts to retain the full convenience factor) as [`postgresql-simple-implicit`](https://github.com/lpsmith/postgresql-simple-implicit), it needs to be rewritten as a snaplet and fleshed out a bit, but it should work.
Of course not. It'd be Haskell on Rails or something. Don't be silly. Seriously though, I think Haskell is way more suited to serious web programming than any of the dynamic bunch. Dynamic programming languages are a mistake, plain and simple - they're nice for ad-hoc throwaway code, but for anything serious, they just don't scale well (in terms of developer performance, that is).
Is xmonad actually abandoned? I unsubscribed from the mailing list because I no longer used it but there still seemed to be stuff going on. What are the bugs you found? It was always pretty solid for me. 
Yesod is pretty much heading on that direction.
Eh, so Smalltalk, Lisp, and so on — all just mistakes, plain and simple? Sorry, but that's just parochial!
Well, more precisely, the assertion would be worded as "using these languages to build production software systems is a mistake". That is, in the long term, the maintenance costs they add outweighs all the good features.
I'll play devil's advocate and argue that a lot of the people using Rails are probably not betting maintaining their system in the long run. In certain sectors time-to-market and ease of prototyping are much more essential even if they come with the high cost of having large quantities of buggy, error-prone, dangerous code running in production. If the question about whether your company will even exist in six months is an open one, then why bet on a technology designed for long-term stability?
My concern with Yesod is that it encourages a big fat classic system, whereas I think that micro-services are the better way to go. 
Allow me to disagree with you regarding "serious" web programming. Much of current web development revolves around pretty simple CRUD apps, one-off pages, etc. - things that don't require a great deal of correctness. Dynamic programming fits this niche well because you don't need strong static guarantees for something that e.g. a quick WordPress install could handle. Now if you are creating large or massive infrastructure on the scale of Google, Amazon, etc., I'm inclined to agree with you. But the fact of the matter is that [about 19% of the web is WordPress](http://venturebeat.com/2013/07/27/19-percent-of-the-web-runs-on-wordpress/), and that currently works just fine.
I don't know the CRUD app business well enough to make a well informed response, but I do know Python pretty well and I'm pretty sure the difficulty the average developer will have transitioning from Python to Haskell is not to do with static types *per se* but the completely different mindset that is required to program in a functional way. When I first wanted to write a program in ML I was completely stumped by the lack of a for loop. "How can I write a program to process this list if there isn't a for loop?!" On the flip side, I really do think static types can be a great help even for CRUD apps. I am writing an SQL generation library that allows developers to write composable relational queries that are guaranteed not to fail at compile time.
I write far *fewer* unit tests, because with a bit of thought I can express many invariants in the type system. I still write some unit tests and I wouldn't say I find it *easier* in Haskell, probably about the same. QuickCheck is much more powerful than any testing framework I ever used in the Python world, though.
That will depend on whether or not the Haskell community can get its head out of its ass and place documentation and teaching high up on its agenda.
I've built up web services from nothing in both ruby on rails and in Haskell. The time to construct something new is just about the same. Haskell's type system is a *fantastic* aid for rapid prototyping. In other words, having done it, I don't buy that reasoning.
I have also done both (well Python not Ruby) but with differing levels of understanding of the frameworks involved, so it's not a fair test. Is your code by any chance open source so I could have a look?
Consider posting to /r/LaTeX instead of here.
Basically what you want is a way to write a datatype data Equal a b = ? with a fold on that data type that looks like this: foldEq :: forall a b r. ((a ~ b) =&gt; r) -&gt; Equal a b -&gt; r but without using the type equality extension `~` in the definition of foldEq or Equal. 
I was trying to let you have the credit/karma, because you found the link and I did not. but okay, [I will.](https://pay.reddit.com/r/LaTeX/comments/1yk5mj/sharelatex_the_online_latex_editor_and_real_time/)
Unless there's a formal specification, being "correct" is a not-necessarily a well-defined concept. Within the context of web development you're often allowing the expression of some illegal state in the system given some invalid input from the user. Whether or not the system is useless ( at least in the context of a business ) is going to depend on how often that illegal state occurs and ramifications it has for the rest of the system. If you occasionally saturate some resource on the server and spit out a 404, the system is technically incorrect but it's a benign failure so long as it happens infrequently. Allowing the user to execute arbitrary SQL code on the database is also incorrect, but not so benign.
The Haskell community needs to work on its marketing efforts if this is going to happen. Look at many of the popular javascript frameworks these days, you have to have a sexy web presence.
Oh, I completely agree that static types can help CRUD apps. Please understand that I'm not trying to advocate for dynamic typing in any way. I just don't think it's fair to say that dynamic programming languages are only good for code that you throw away.
This is great, thanks! This will save a lot of time.
Also: &gt; &gt; things that don't require a great deal of correctness &gt; They still require just as much correctness as any other software. A poor and misleading choice of words on my part. Forgive me. I simply ment that it is much easier to get simple things correct, even without static guarantees. 
Sure, at least in a lot of cases. But my point was about the lack of a binary "correct" and "incorrect" in real world software.
&gt; In some cases **avoiding GPL**, for people who work at companies that cannot publish their proprietary analysis source code but would still like to publish compiled applications (ಠ_ಠ)
Based on previous posts it seems ok to post jobs here, let me know if I'm doing it wrong.
You may not bet on it but it happens and it can be really bad when it does. I work on a ~7 year old, &gt;100KLOC Ruby on Rails codebase and let me tell you it is a living nightmare. I also know that I'm not alone in that. Rails may get you to market faster but if you don't have a good intuition (and the ear of management) as to when is a safe time to throw away your prototype, you will be maintaining a system that is impossible to reason about and one that nobody who values their sanity will work on. You get to a point where you miss your chance and starting over is an impossible choice. Maybe the best of both worlds is to start out building micro services from the start and then swap them out when they prove their market value? 
&gt; And I was addressing your statement that untyped languages are good for simple web development. Ah. What I was trying to say is that they are usually good *enough* for simple web development.
It's not open source, it's all running in production at my former employer. Except for the few bits I kicked back into Snap itself, at least.
You've got the magic words in your description. "Our stack is Haskell, Ruby, Git, Ubuntu and we are primarily looking for a Haskeller with a deep understanding of computer language development to help build the next generation of the Signal Vine platform." Good luck. Lots of people who hang out here would be great for this.
Why is this (effectively an ad.) interesting?
Judging by the article, they still have nothing. And nothing is all that's really going to come from FP Complete: a headless chicken looking for its head. 
I understand where you are coming from, but then I find myself doing random scripting tasks in haskell that others in my company would do with php or python most likely. The only time haskell takes longer really is when I get wrapped up in trying to use some cool library like lens.
I think the current query methods will also fail for things like an insert followed by another query for getting the previous insert's row id.. You might get another connection's insert row id. I worked around this in snaplet-sqlite-simple by restricting concurrency by allowing only a single connection and protecting that with an MVar. Of course this kills concurrent performance in a big way, but it works without surprises. The same serialization is of course not an option for production systems running on a "real" database.
Haskell on Hydroplanes. This reminds me, I wonder if [haskell-on-a-horse](http://haskell.on-a-horse.org/) can be revived?
Seconding what zmanian said. Looks cool. 
That's right; `Equal a b` and `a ~ b` are equivalent. The difference is that the compiler infers automatically arguments of kind `Constraint`, while you need to supply explicitly arguments of kind `*`. You can convert any type of kind `Constraint` to `*` with data Dict c where Dict :: c =&gt; Dict c i.e. `data Dict c = c =&gt; Dict`. Then you can pass dictionaries explicitly. E.g. Dict (Num a) is the record type containing functions `(+) :: a -&gt; a -&gt; a`, `(*) :: a -&gt; a -&gt; a` etc. A value such as `f :: Num a =&gt; a; f = 3 + 4` is converted to `f :: NumDict a -&gt; a -&gt; a; f (NumDict (+) (*) (-) negate abs signum fromInteger) = 3 + 4`, i.e. the =&gt; arrow is converted to -&gt; arrow, and the + in 3 + 4 comes from the NumDict argument. Consider this datatype: `data Number a = Num a =&gt; a -&gt; MakeNum a`. So `MakeNum :: Num a =&gt; a -&gt; Number a`, and we can convert it to `MakeNum :: NumDict a -&gt; a -&gt; Number a`. Note that if we write it as a standard ADT, the =&gt; arrow effectively becomes a pair: `data Number a = MakeNum (NumDict a) a`.
I just migrated an almost 7 year almost 300k LOC RoR project from Rails 2 to 4. They might not think it will last that long but it does and my god. The pain... It paid very well but I will never do something like that again; it is kind of text book why you really want them to write this in something like Haskell in the first place. It's a mistake to think your code won't live long; how do you know that? So don't assume that PLEASE. And like you say; this is not an isolated case. Rails and the libs (OH CHRIST THE GEMS) change so fast and so ad hoc that nothing is compatible after a few years. And the gems which 'die' (are not updated) will not work, even with heavy changes, on never versions of Rails/Ruby/other gems. Until you run / test every single execution path, there is no way to know if anything will break, so if you don't have full coverage unit tests you are in for some serious fun. And who maintains unit tests when you are in a hurry / don't get paid for it / whatever? Well not the guys who wrote these 300k lines. They have coverage until end of 2008; all updates after that do not. There are 1000s (10-100 000's?) of projects like that , probably most of them smaller, but still, all over the world. They were thought to be shortlived, maybe even prototypes and now they run as vital infra for companies and will be extended forever. 
Where are your books, casts, tutorials, blogs, etc for this magic you speak off? I could write them, you could write them, maybe they exist, but unlike the rails ones, the Haskell stuff is hard to find, usually outdated and generally unreadable even if you are not a beginner at Haskell anymore. The stuff you talk about is exactly what people who are interested in switching would like to hear about, but they can find no examples of this crushing power. At least none that explains it in speak they understand. 
This is all very interesting. I shall have to read more about System FC. 
Here's something posted yesterday. http://www.reddit.com/r/haskell/comments/1ydwui/snap_for_beginners_sample_chapter_digestive/
tnx.
The big fat classic system (bfcs). I'd say big and fat are a problem because they stand for: (1) bad performing, (2) hard to maintain, (3) large API-touchpoints. Or am I overlooking things. The first 2 I think they are not showing in Yesod; in Rails to some degree I see them. The 3rd point: sure framework have larger API-touchpoints then request-response interfaces. When it comes to defining "classic" in "classic systems": what wrong with that? I think so-called frameworks brought much good to web development --it was a great improvement-- as the pre-framework era web development was a lot messier. 
I'm not sure what you mean here, I think there must be a difference between sqlite and postgresql I'm not aware of. And doesn't SQLite support some level of concurrency? (Though perhaps it's not worthwhile inside a single process...) In postgresql this isn't ever a problem, you can just `INSERT ... RETURNING col_a, col_b` if you have a table with columns that are generated on the backend. There are plenty of instances of stateful interaction with libpq as well, but `postgresql-simple` already protects this with a single `MVar` around each individual connection. It should be safe to use a single `postgresql-simple` connection concurrently, as long as you aren't doing things such as transactions and the like.
I tried a slightly different take on the same idea in [postgresql-simple-implicit](https://github.com/lpsmith/postgresql-simple-implicit/). The same approach should be possible with snaplets, the problem being that to be very useful, I really do need a generic `bracket`, and `MonadCatchIO`'s bracket doesn't play very nice with snap.
I have noticed that they also do a better job of explaining things simply. Every javascript library is designed from the bottom up to be mindlessly easy to implement, with examples that make learning the interface trivial. With Haskell, although I love the language, I often feel that learning a library first requires that I appreciate the person's genius before I can begin to implement it, but I usually just don't care. There isn't nearly as much effort to make libraries easy to implement (examples and such).
One thing I've been side-eyeing cautiously about Yesod has been persistent. When Rails tried to create an underlying-database-agnostic ORM, things went wrong. The db layer by and large had to handle the lowest common denominator, so people forgot about foreign key constraints, unique indexes, etc, in favor of ruby-land validations. You could technically access these features via plugins and raw SQL, but none of the core resources encouraged/covered it so nobody did. I don't have much experience with persistence, but I think database agnosticism is by and large a trap.
The hole in Haskell pedagogy between beginner and expert is well known at this point. FP Complete is trying to solve it with School of Haskell. Lots of people have discussed doing a new modern edition of Real World Haskell. There are books that are trying to better document the bigger frameworks/libraries. It is a big problem. We really need more resources between Learn You a Haskell and I'll just look at the source code. I'm just starting to escape that hole myself. 
The funny thing is that almost all the circumstances that lead to these projects are painted as calculated risks to save costs. Projects like this absolutely chew through developers and billable hours. How much money will you save if your original team burns out and quits and you have to hire an extremely expensive contractor to fill in the gap?
I've never seen a Ruby app go over 100k lines and still be database agnostic. I'd say Rails is db agnostic "at first". And see no reason why this wouldn't also be true for Yesod/Snap/etc.. 
I haven't looked at the article, but the comments in this thread have been very interesting, and a good look into the state of Haskell web development.
By then we'll be rich!
I'l love to see more of the book if available, it would be a great help as I'm working on my first Snap application right now! Edit: Nevermind gonna pre-order
"Our business is investing in and making a bet on Haskell" isn't interesting to you? News like this is good for the whole ecosystem. Commercial users often report bugs, write bugfixes and contribute new code --- their livelihoods depend on it.
Except in the sense that you may very well want to throw it away when you find it again in three years (though I suppose that's true of most languages). :D
Hear that people? You'd be great! Apply! I am the only engineer and I need help...
BFCSs have been battle tested, we've become accustomed to them, and they are easy to package and deploy on a single server. But that's the thing. It does not work well when you need to scale parts of your service.
Bullshit. What you wrote isn't news. It's an ad. Also, your business is to make money. And there's absolutely nothing wrong with that but to say otherwise is bullshit.
&gt; Unlike MonoFunctor and MonoTraversable, you don't end up losing any flexibility in data types, since we do not need to retain the original data structure. Could someone explain how MonoFunctor and MonoTraversable loose flexibility?
s/beginners/prototypes/ Not every web app is doing weather simulations, it doesn't need maximum possible performance.
Then we have Java systems --- slow time to market AND bug ridden unmaintainable mess!
These problems are also things that many web business have, yet the business still functions! If they only launched sites without these problems, they wouldn't get to market before competitors :-(
Trying to find current articles/tutorials on Snap and Digestive Functors was huge headache when I was trying them out. Everything was woefully outdated for one or the other as each has had quite the API change over their lifetimes especially Snap in which 0.10.x greatly changed the api. I wasted a few days then finally gave up.
More please! Snap doesn't have enough tutorials on how to use it, and it's documentation is written from the perspective of someone who has deep internal knowledge of how all the parts move. These combined make trying to do something trivial a huge headache for the newcomer.
&gt; in theory. In theory.....
Teaching? Is that like where I show you how a webserver is actually just a particular comonad on the category of RESTful semiapps?
ZipList does seem to hold. Also seems to hold for (Monoid a =&gt; (,) a), when &lt;&gt; is commutative for a. I'm no longer convinced there's nothing interesting. 
Err, as far as Google's concerned, the big three canonical languages are C++, Java, and Python - Perl borders on verboten. Go is rapidly up and coming. And Google spends a truckload of time on *reducing* programmer effort in debugging, by exposing errors very early on, through exhaustive, constant regression testing. http://google-engtools.blogspot.com/2011/06/testing-at-speed-and-scale-of-google.html
&gt; Of course not. It'd be Haskell on Rails or something. Hoists for Haskell.
&gt; Are interested in dev-ops and build automation Let me introduce you to [Nix](http://nixos.org/nix/) and [Hydra](http://nixos.org/hydra/)...
Not my business and I didn't write this article but thanks :)
It almost certainly would, and in such a trivial way that it would seem boring.
Something like that. Some way to tell the type system "hey these things are the same!". I don't think it's possible without GADTs.
"There are too many Haskell job posts in this {subreddit,mailing list,country}", said no haskeller ever.
So, is Haskell (and in particular Yesod) the best tool I know to rapidly make complex an as bug free as possible app ? Yes. I recently switched for a new company and I have full choice on the technology to use to make their website. Did I choose Haskell? Unfortunately no. While I am a fervent Yesod promoter here is why (mostly): - I will have to work with some java code (NLP) - My colleague doesn't know Haskell, he is used to PHP and now node. Being productive in Haskell might take him some months. - not so easy to initialize your working environment (cabal hell made me lose hours in many occasions). So lacking a correct type system in clojure is a Hell, I suffer many times a day from this lack. But, in the end nothing stops me and while I feel a thousand bugs wait to be discovered, the job is done. So what is lacking for me ? - ability to work as easy as possible with the JVM. While I know it is hard to achieve, that would be a great power to sell Haskell explaining you could reuse all JVM compatible libraries. - For the deployment, a system to install CoreOS + Dockerfles to have a sane development environment (pgsql + snap, mogo + Yesod, etc...) all using stackage instead of hackage. Also I believe some very simple details could promote Haskell a lot. 1. Make a modern CSS looking for haddock on hackage (bonus if responsive). 2. Writing tutorial for people not even knowing what is a curried function (I made my part on that) 3. Fill the philosophy gap. For example, making CRUD JSON REST API tutorial. 4. Consider, development environment and deployment part of the problem. If you follow my Yesod minimal tutorial, imagine being in the place of the person doing it. Just install Haskell platform, wow it is big. Now I just have to "cabal install", now, wait about 10 to 20 minutes! Argh! Most of them are already away. Then you finish an incredible work in 10 minutes. And you ask yourself, how could I show this to the Internet? And chances are big that you'll be stuck here. With Ruby, everything is easier here, and you could publish on heroku for free in minutes! While FPComplete try to fill the gap, I don't know many people liking to work in an online IDE, putting their code we don't really know where, that doesn't support scripting, literate Haskell, and if you just want to try a paid internet service have to pay about $75/month even if it is a complete failure and nobody visits if. That was my two cents, I know there is always to work around most (if not all my problems), but they are still work around. I would like to have the same "everything is easy" feeling I have when I work in Clojure with my Haskell. 
You may find these two rakefiles of interest: https://github.com/topos/hray/blob/master/lib/task/ghc.rake https://github.com/topos/hray/blob/master/lib/task/hp.rake
Yeah, I sit in that hole too. Seriously, lenses seems to be the go-to abstraction nowadays, but you're not expected to try them "until you use the other things", because they are supposedly too advanced or confusing or something. This isn't a problem with Ruby or any other language outside of academia. If there's a library or language feature which lots of people are using, then I can pick it up in few hours.
Most code is throwaway code, until you have to support them. (Well, most of *my* code is throwaway)
I've spent an hour on the documentation and I still can't figure out what the Lens library actually does.
&gt; but they can find no examples of this crushing power. At least none that explains it in speak they understand. Having just read Learn You a Haskell and now exploring web frameworks to put the language into use, I'd say this is the essence of the problem in terms of Haskell gaining widespread adoption. Given that IIRC there is no IDE for Haskell, language/framework discoverability (i.e. mouse hover view type, or click type go to source file) becomes far more tedious/difficult. Blindly groping in the dark is the worst way to hit the ground running, more slogging through mud really, an experience that one gets fully in Scala's SBT, for example. Saying that, Snap looks quite interesting, saw a tutorial recently and liked it, quite similar to Ruby's Sinatra, or Scalatra in Scala. Beyond the getting started tutorials it would be nice to see a large scale web system built in Haskell, one where you can see how X Haskell web framework was used to modularize the application (i.e. beyond the simple hello world that covers basic framework functionality). 
&gt; including tasks such as **mainting** our Ruby on Rails application as necessary
Thank you /u/Jurily, I was out for the Friday night, so didn't have time to reply. About abandonedness: the last release was Jan 2013, then before that 2011. Both of these releases are very small changes for the most serious issues and you had to wait quite a while to get them, actually I used patches from their git when I was on xmonad. No, darcs, but still. I wanted to check out their darcs now to see if there are any new commits, but actually the repo is not responding. It's definitely not actively developed as a well-functioning, lively community on github or something. About Jurily's first point: that is not only on workspace switch, but sometimes with opening and closing windows too in full-screen or tabbed layout. I'd also like to add a 3. point: focus handling of floating windows. It's terrible, terrible, terrible: https://code.google.com/p/xmonad/issues/detail?id=4 And I don't know the exact details, but I have the feeling that they don't handle those ICCCM and related messages totally up to standards, because all kind of strangeness always happen with Java apps and so. Maybe Java is silly, but i3 just works. On the other hand, I don't want to sound very negative. I loved xmonad (actually I had my own fork of it, because I wanted to simplify/understand, just didn't find the time) while I used it. It's very good. But i3 is great, great, great. I already sent them some moneys as donation, it's that good.
Mistakes, maybe not. Back in 1970, dynamic typing was a valuable and important experiment, and many of the concepts they pioneered have rightfully made it into mainstream programming. I just think that using a typeless/unityped language in favor of one that can do compile time checks is a mistake for anything with a chance of transcending the throwaway/prototype phase.
Most code ends up living much longer than you anticipated. That single use marketing thing I wrote in an hour, 8 years ago? Still in production.
Well, the basic idea of lenses is composable getters and setters: data Lens a b = Lens { get :: a -&gt; b, set :: a -&gt; b -&gt; a } Given your own datatypes: data Person = Person String Address deriving Eq data Address = Address String String deriving Eq You can define lenses for them: name = Lens { get = \(Person n _) -&gt; n, set = \(Person _ a) n' -&gt; Person n' a } address = Lens { get = \(Person _ a) -&gt; a, set = \(Person n _) a' -&gt; Person n a' } street = Lens { get = \(Address s _) -&gt; s, set = \(Address _ c) s' -&gt; Address s' c } city = Lens { get = \(Address _ c) -&gt; c, set = \(Address s _) c' -&gt; Address s c' } As you see, these all follow a very simple template, and should really be generated with "deriving" or Template Haskell. Anyway, it should be obvious now that given a = Address "Elm Street" "New York" p = Person "Jacob" a We can use our lenses as follows: get name p == "Jacob" set name "Peter" p == Person "Peter" a But what if I want to change the street of a person? I'd like to compose lenses so that I can refer to the street of a persons address with one single lens. So let's define this operator: (|&gt;) :: Lens a b -&gt; Lens b c -&gt; Lens a c ab |&gt; bc = Lens { get = \a -&gt; get bc (get ab a), set = \a c -&gt; set ab (set bc c (get ab a)) a } Now we can get and set the street of a person: get (address |&gt; city) p == "New York" set (address |&gt; city) "Copenhagen" p == Person "Jacob" (Address "Elm Street" "Copenhagen") These are the basics of lenses. The actual lens library uses another, more powerful representation of lenses, and also works on collections etc. But these are some of the basic ideas. Edit: And the lens library confuses the hell out of me too. I learned this from looking at fclabels.
I should've been more explicit. It may be that the problem I talked about doesn't come up with Postgresql. Regarding sqlite concurrency, yeah, it supports concurrency, but you can really paint yourself into a corner when inserting into a table that's also being read on another connection. I have a [bunch of notes in github](https://github.com/nurpax/snaplet-sqlite-simple/issues/5) on this if you're interested. I didn't want to expose users of snaplet-sqlite-simple to these types of issues, so figured an MVar around a single connection will do just fine. :) For a larger site, you'd probably need to switch to another database anyway. SQLite doesn't support "RETURNING", so you often need a second query to get the previous insert's row id. E.g. with db $ execute "insert into posts (text) values ('post contents') rowId &lt;- with db $ query_ "select last_insert_row_id()" -- do something with rowId The `last_insert_row_id()` returns the row id of the previous insert that was invoked on the same connection. E.g. with explicit connections: execute_ conn1 "insert into posts (text) values ('contents 1')" execute_ conn2 "insert into posts (text) values ('contents 2')" postId &lt;- query_ conn1 "select last_insert_row_id()" The `postId` would refer to the first insert, not the latter. But with earlier snaplet-sqlite-simple version with connection pooling, the following code might obtain the wrong `rowId`: -- execute runs on connection 1 with db $ execute "insert into posts (text) values ('post contents') -- query_ may get connection 2 from the connection pool, thus potentially returning an unrelated connection's rowId rowId &lt;- with db $ query_ "select last_insert_row_id()" Fortunately, it's easy to protect against this by being explicit about connections: -- | Run an IO action with an SQLite connection withDb :: (S.Connection -&gt; IO a) -&gt; H a withDb action = withTop db . withSqlite $ \conn -&gt; action conn lift $ withDb $ \conn -&gt; do execute conn "insert into posts (text) values ('post contents') rowId &lt;- query_ conn "select last_insert_row_id()" 
Yea that was the "until you have to support them" part. There is some wisdom in planning far ahead, but I still like the peace of mind of writing throwaway code, knowing that if I want something solid I should just delete and design it right this time (since now I know what's supposed to do)
Very much no. :)
The problem here is as well that by far most people will never look 'in the source code'. From my work with Rails in large projects I have seen that, because of the badly documented libraries, the 'leaders' open up the source and check what is going on. But the 'assemblers' (ref: http://www.reddit.com/r/programming/comments/1yiy6b/simplicity_in_software_what_you_is_not_what_you/cfl27va) of this world use StackOverflow and will never, not even once, open up the source. So there must be a way for those, mostly copy/paste, devs to use this technology, otherwise it won't happen I'm afraid. 
I learned Haskell to not do that!
&gt; ability to work as easy as possible with the JVM. While I know it is hard to achieve, that would be a great power to sell Haskell explaining you could reuse all JVM compatible libraries. Agree. It's getting there with a lot of languages; anything compiling to anything which would be the base for this; if ghc compiles to Java, the FFI interface is Java, .NET it's .NET etc. Then you can pick the runtime you want which, outside the pure Haskell libs, the portable libs and the prelude, will allow you to easily use any library from the runtime easily. Ajhc should do it, but I find it annoying to use something besides GHC as it is the forefront and so GHC must provide this. If not, I don't really see an 'easy' way in which this can happen. 
That's interesting. I pretty much always thought of Haskell data as actually being functions (with memoization). That's the only way I can understand what's going on.
Finally, someone who *isn't* making the assumption that if you want to log, then presumably you want to log strings. I have much richer types that I want to log! I wrote [`libsystemd-journal`](http://hackage.haskell.org/package/libsystemd-journal) to interact with the `systemd` journal, which has support for rich metadata - log entries are just key-value maps from log properties to binary values. Unfortunately, it doesn't seem like any of the off the shelf loggers can actually be used with this. It looks like this should be trivial to wire in with this library though. However, this library doesn't really seem to solve the problem of being on-the-fly - you still have to delimit flushing the log, which is annoying. It feels more useful to have something more like `monad-logger`. I think what I really want is `MonadLogger` that isn't so string orientated. Btw, your blog post talks about `JournalT` - but all the source code is `LoggerT`.
My bad, I’m gonna fix that right now. Thank you for noticing, and your feedback.
I disagree, because I think there are systems where the very notions of static types and separate compilation affect the user in a way that's not obviously beneficial. My example is Emacs, which is immensely dynamic. Maybe this is just a quirk of my personality, but I understand the viewpoint of Lispers and Smalltalkers who see static typing as "bondage and discipline." Of course, one could argue that Emacs itself is a throwaway, silly thing, a poor example of software engineering, to which I can only reply "but I like it."
You know, it's possible to import Text like this: import qualified Data.Text as T import Data.Text (Text) That way you won't have to write "T.Text" all over the place.
I haven't really used a lot of monad transformers, but I've heard of problems with WriterT leaking space. Does JournalT avoid this problem? 
I think you're referring to [this](http://www.reddit.com/r/haskell/comments/1qrp8l/improving_performance_of_complex_monad/cdfrzna) reddit comment and the mailing list posts linked in its children. Looking at the implementation of `JournalT`, it uses `StateT` internally, instead of `WriterT`, which addresses part of the problem, but `journal` is lazy in its argument, so there is still a potential [thunk leak](http://blog.ezyang.com/2011/05/space-leak-zoo/). I could very well have misunderstood something, though.
Yes, basically like that. You could get `refl` and `trans` from `Category` in the form of `id` and `(.)` respectively, then you only need to add `sym`. I don't think the type classes actually gain you any power here, though, just genericity.
`journal` is indeed lazy, but I may change that since you’ve come up noticing :)
Did you watch the SPJ talk about lenses?
I used to have cabal hell 2 years ago or so. I've not had cabal difficulties in quite a while...
I think cabal has improved in this regard the last couple of years. I've seen some blog posts on work being done to avoid diamond dependency problems.
Exactly, the point is that you don't want that. I want to think about a pair of ints as a pair of ints, not as a thunk of a pair of thunks to ints. For those cases where I want a lazy value, a `Lazy t` type constructor works fine and lets you see exactly where something funky is going on with laziness, just like `IO t` lets you see where something funky is going on with IO.
Right, sounds like a nice idea. I guess the question is where in practice there will be so many `Lazy` annotations to make it a complete mess. I'm not sure.
&gt; Are intellectually honest I thought I'd be perfect for this job until I saw that. Oh well, maybe next time!
Lenses you use deeply nested data structures(like a zipper of lists of tuples) without writing boilerplate accessor/update functions. The only sense in which lenses are advanced is that type errors are crazy if you make a mistake. Step 1 in solving a problem in Haskell is modeling your problem domain in a datatype. For any moderately complex domain, Haskell used to punish developers with some giant file of accessor and update functions. Edward solved that for us at de-minimis performance penalty on the access side and a small penalty on the update side. 
Whenever I start a yesod app, the first thing to go is persistent. While I would like a haskell dsl for queries, because the alternative is usually concatenating strings together, persistent is not that alternative.
Did you have problems using diagrams-gtk (http://hackage.haskell.org/package/diagrams-gtk-1.0/docs/Diagrams-Backend-Gtk.html) ? That module provides an interface so you can render to cairo within gtk without diagram rescaling for just this purpose.
I've found it useful to have a simple abstraction that lets me compose SQL more easily, in my case Python's SQLAlchemy.
kudos to you for making this, but seeing the code to do even the simplest of UI actions (not just your code) and the lack of actual UI libraries has made me give up hope for haskell being used in an actual production UI. this is at least currently. object-oriented programming and user-event driven UIs excel at creating UIs, and i have yet to see any evidence that functional programming, much less haskell, can be used to build a large, complex user interface. i plan on only ever using haskell for backend code.
Is remote work an option? I'm sure I'm not the only one who'd like to know.
From my experience of programming in strict functional languages, needing lazy is extremely rare. &gt;90% of cases are covered by lazy sequences alone, and most other uses can be hidden in a couple of library functions. Lets ask the question another way: what are some compelling example uses of laziness? How many annotations do you need?
You're trolling, and from their screencast ( http://www.youtube.com/watch?v=gxrp3YTe7Ws ) I think they have some compelling offerings. Creating analytics can be a cumbersome or repetitive process even in modern stacks, and I'm impressed with how much they're simplifying the process down to a couple of clicks. Being behind a paywall is not the same thing as vaporware. Moreover, I think it's great that they're trying. Or do you believe that greater industry adoption of haskell will hurt the language somehow? I'm not being dismissive by that question, I actually am curious - why not pursue something like idris or agda if FP Complete is not taking haskell where you want it to go? 
Hey; How do you get out of nix-shell? 
&gt; However, this library doesn't really seem to solve the problem of being on-the-fly - you still have to delimit flushing the log, which is annoying. That's what I thought as well. Could something like type LoggerT w m = ReaderT (w -&gt; m ()) m be a feasible alternative?
This ought to be a link not a text post.. Maybe I missed something, but why do you need the GADTs extension?
Good question. I'm collecting some thoughts on the issue here: http://h2.jaguarpaw.co.uk/posts/strict-haskell/ Please let me know if you have any responses or additions to make. I'd be interested to learn from your perspective. The point that is most concerning to me is the `Applicative` instance of `Either`. How do we arrange a nice syntax for it so that the equivalent of foo = bar &lt;$&gt; arg1 &lt;*&gt; arg2 doesn't evaluate `arg2` if `arg1` evaluates to a `Left`? 
This actually did help a bit, thanks.
It would be nice if there was the ability to do both though - buffered logging for non-IO based monads and streaming logging for IO based monads.
Use `exit`. You can also hit Ctrl+D in most terminals.
When I read this kind of explanations about alternatives to lazy IO, I always wonder what exactly is understood by *lazy IO*. Is it when using... - any IO function from `Prelude` or `GHC.IO.Handle`? - lazy `ByteString`, lazy `Text` or `String`s in IO operations? - any code where `unsafeInterleaveIO` is being under the hood?
Yes indeed. I came up with a compromise with the `sink` function that lets you defer logs in pure / whatever monadic code, and then empty the *buffered history logs*.
anything involving "semiclosed" handles, I think. So the last one?
Lazy I/O is where you read a lazy ByteString / lazy Text / lazy String and it only reads from disk as necessary. This is somewhat dangerous, as 1. It is not clear at which point the handle will be closed. 2. Exceptions while reading may crop up elsewhere. EDIT: And also the additional problems that Tekmo mentions in his linked post.:)
Is this argument really sufficient? I don't know the intricacies of random number generation, but the argument seems to be "the range [0..53667] is large, therefore the distribution is bad" -- but is it really large when you consider the possible range of the seed argument?
That doesn't look right to me. When the lazy `f &lt;*&gt; a` is forced it will force both `f` and `a` before passing them to the strict `&lt;*&gt;` which is exactly what we want to avoid. I don't think you can do this without explicitly customising the evaluation order for each individual `Applicative` instance. Is my reasoning right, or have I missed something?
Local candidates will be given preference, but I'm not adverse to building a distributed team. So, yes. 
Why would f and a be forced? Both f and a are Lazy types. It is the responsibility of the client to construct them as such. Certainly the &lt;*&gt; will not force evaluation of f and a. Maybe you can give a concrete example which is problematic?
My understanding is that Lazy IO is when IO actions occur implicitly when forcing thunks, rather than explicity when combining actions of type `IO a` into the definition of your `main`.
OK, imagine a strict Haskell, with a strict `Maybe`. It has an `Applicative` instance, but it is bad to use it because x :: Maybe Foo x = Nothing &lt;*&gt; expensive always runs `expensive`. You suggested that `Lazy Maybe` would solve the problem, where Applicative a =&gt; Applicative (Lazy a) f &lt;*&gt; a = delay (force f &lt;*&gt; force a) (He we have to have `Lazy :: (* -&gt; *) -&gt; *` when typically that name would be assumed to be something of kind `* -&gt; *`, but I don't think that's anything more than a nomenclature issue. Also, I'm assuming your `delay` is a special form here, otherwise even that definition itself is problematic). However, I don't see that this solves the problem. Suppose I write x :: Lazy Maybe Foo x = delay Nothing &lt;*&gt; delayedExpensive Then `x` evaluates to `delay (force (delay Nothing) &lt;*&gt; force delayedExpensive)` with the `&lt;*&gt;` here the one from the `Applicative Maybe` instance. When I force `x` both `force (delay Nothing)` and `force delayedExpensive` are evaluated *before* being passed to `&lt;*&gt; :: Maybe (a -&gt; b) -&gt; Maybe a -&gt; Maybe b`, but forcing `delayedExpensive` was exactly what we wanted to avoid! Thus I don't think your proposed solution works (please correct me if I am wrong). Still, *something* along these lines is sure to work. It's just a question of getting it right, and getting an acceptable syntax. 
I still don't understand why you wouldn't have two different types `Nomex a` and `NomexEffect a` with a `liftNomex :: NomexEffect a -&gt; Nomex a`. This is much simpler in my opinion. The typeclass solution may make sense if you have a huge hierarchy of effects, but if there are only two it hardly seems worth it.
I can't see any use of GADTs there either.
I'm with Oleg on this one — the main problem with lazy IO for me is semiclosed handles (i.e. resources). And sometimes I can even [tolerate that](https://github.com/feuerbach/tasty-golden/blob/master/Test/Tasty/Golden/Internal.hs#L26) to enjoy the convenience of lazy IO. As for the "side effects" hidden in lazy IO, I suppose Gabriel is referring to reading from a Handle? I find it much less dangerous than memory allocation and release during evaluation. If Gabriel found a way to reason about those things, it would be much more interesting.
My objection (or at least part of it), and I believe this is Gabriel's opinion too, is that the IO in lazy IO is implicit, and doesn't really fit into the Haskell paradigm of giving effect types to everything that you can. If I run `readFile "filename" :: IO String`, I can call whatever functions I like on the `String` I get back, and it will implicity read the contents of `filename`. It's this implicitness that I find very unHaskell. I like Haskell because the types (generally) tell me as much as possible about what's really going on.
The argument is more that the first random number you generate (from any of those seeds) is predictable, which is bad (something similar to this caused a weakness in WEP ([link](https://en.wikipedia.org/wiki/RC4))).
I was referring to the use of `unsafeInterleaveIO` and more generally anything that ties side effect order to evaluation order.
Yes, I think that's right. You can do it on a case-by-case basis (which is not such a bad thing).
I think it's a good thing. The reason why you can't do it generically is that in a strict language you can't peek at the internals of a function to determine in what order things get evaluated. In contrast in a lazy language you can observe that these two are not equivalent: foo, bar :: Bool -&gt; a -&gt; a foo b x = case b of True -&gt; x | False -&gt; x bar b x = x So the meaning of what constitutes a function is a lot more intricate in a lazy language. In a strict language a function on data is pretty much like a mathematical function, but in a lazy language what constitutes a function also includes which parts of its arguments it evaluates when and under which conditions.
Yes, I think we're in agreement. Thanks for the discussion; it's been interesting. Now I'm just waiting for someone to implement this magical strict Haskell. If no one does, I'll have to do it myself!
Thanks you too :) BTW, isn't DDC (Disciplined Disciple Compiler) like strict Haskell? I've never used it but I heard that that was part of its goal.
Note that this has been written up as [the `foldl` library](http://hackage.haskell.org/package/foldl) in case anybody wants to play with it. It doesn't use monoids internally for efficiency, but you can recreate the original behavior described in that post by using [the `foldMap` helper fold](http://hackage.haskell.org/package/foldl-1.0.2/docs/Control-Foldl.html#v:foldMap).
Why, what are you intellectually deceitful about?
thanks; those work. More questions if you have time, i've got nixos on my dev machine which is fine, but ubuntu due to limited choice, on a server, that i'd like to set up hydra, with maybe shake. Installed i've got nix running on ubuntu, but not hydra, it installs fin with nix-env -i hydra, but i'm not sure how to run it. Command not found etc. WIth nix and and apt on a ubuntu distro, is there something tricky I have to do, to locate packages installed with nix?
That's.. interesting.. I wonder what would happen if the range were adjusted.
In the parent comment, I suggested I was intellectually deceitful when I'm actually not. That in itself was intellectually deceitful :(
You might find [this post I wrote on streaming logging](http://www.haskellforall.com/2014/02/streaming-logging.html) useful. One of the solutions I discuss is the one you just presented.
&gt; The point of having a lazy language is to be able to write things that you couldn't write in a strict language, like using infinite lists. I don't think I agree with this. In my experience, (spine-) lazy data structures are almost always a bad idea. Although infinite lists look cool, they are confusing and just space leaks waiting to happen. When I write a function, I want to only reason about two cases: it is evaluated, or it isn't. The example of getting a priority queue out of a sort function (`take n (sort xs)`) bothers me because it puts an extra burden on the author of `sort`. If this is considered idiomatic, then anyone writing a sorting algorithm has to analyze in detail whether it supports this operation and how efficiently, both of which can be difficult. I would be perfectly happy with data structures that were strict by default. However, I view non-strict evaluation as extremely important. It allows equational reasoning, meaning that if a say `x = expr`, then I can replace `x` with `expr` or vice versa anywhere in the program and get the same result. Additionally, ... e ... is the same thing as let x = e in ... x ... not just denotationally, but operationally. Most importantly, this means that I can write functions like `findWithDefault` or other control structures neatly. This is possible with explicit thunking (every lazy application is), but it is much uglier: m ! k = findWithDefault (delay (\() -&gt; error "key not found")) k m
Just wave your hands fast enough and you can't see the bottoms in Haskell either. ;)
&gt; I want to think about a pair of ints as a pair of ints, not as a thunk of a pair of thunks to ints. This makes perfect sense, and I think that Haskell's tuples probably would be much easier to grok if defined strictly. However, there is nothing preventing a better implementation: data Pair a b = Pair !a !b Arguably this should be the default, but that isn't a deficiency of non-strict languages - it is a deficiency of the Haskell Prelude, which almost everyone agrees needs some work.
What about uhm… very remote? Visas?
I agree, and I tend to think the same. For me, the two solutions have pros and cons. I made those blog posts because I like to have a clear vision, and to sum up the comments I got from the community. The objective now is to choose one of the two solutions :) (this is a design choice for my game Nomyx)... Anyway, to write your function liftNomex, you definetly need an 'unsafeCoerce' isn't it?
&gt; In contrast in a lazy language you can observe that these two are not equivalent: How? The only way to do so would be to pass a non-terminating or erroring value in for `b`, but then your observation function would error or not terminate. If you are talking about a human being able to differentiate between `foo` and `bar`, then by equational reasoning this makes sense. If `b` was some non-terminating expression, then you would expect case b of { True -&gt; x ; False -&gt; x } to be different from `x`. Given that, why should you expect `foo b x` and `bar b x` to be the same? &gt; In a strict language a function on data is pretty much like a mathematical function, but in a lazy language what constitutes a function also includes which parts of its arguments it evaluates when and under which conditions. The problem is that both strict and non-strict languages have differences from mathematics due to non-termination. The question of which type of language to use stems from a tradeoff of whether to have mathematical data or mathematical functions. In a strict language, your data types are the same as they always were - there is not special "bottom." However, functions no longer work the way they used to - they can either fail to terminate or return, and you lose beta reduction. In a lazy langauge, you have to extend most types with bottom, but you keep logical treatment of functions (beta reduction). You are arguing that because the data has an extra element, non-strict languages are confusing, while I am arguing that because you lose beta reduction, strict languages are confusing. Unless you use a total language, this choice is unavoidable, and so you have to choose the lesser of two evils.
If you are right for the position we will do all we can to make it work.
I agree with you that the tricks with lazy sorting are gimmicks, but I didn't want to go that far since I know this is a sensitive topic ;-) I agree that being able to extract a value into a let binding in any subexpression is a nice feature of lazy languages, but I do not think that this weighs against the disadvantages of lazy evaluation at all. Furthermore, a VERY closely related thing works in strict languages: ... e ... is the same thing as let x () = e in ... x () ... More importantly, this actually lets you substitute equals for equals and the result will be the same operationally. In a lazy language there is a difference between ... e ... e ... and let x = e in ... x ... x ... Whereas in a strict language there is no difference between ... e ... e ... and let x () = e ... x () ... x () ... In my opinion findWithDefault is far better written as returning a Maybe.
That's fair enough. It's good to have articles documenting a variety of approaches. Nice work on your Nomyx game by the way. &gt; Anyway, to write your function liftNomex, you definetly need an 'unsafeCoerce' isn't it? Oh no, not at all. I would never recommend such a thing! You just have to choose the correct implementation.
How does that work exactly?
I suggest trying a simpler lens library like data-lens
Roughly, for a function set theoretic function `A -&gt; B`, you can say the domain theoretic function is a total function `A -&gt; Lift(B)`, where `Lift` ass bottom. Or you can say it's a function `A ~&gt; B` where ~&gt; is partial function arrow.
Looks very useful!
&gt; Given that, why should you expect foo b x and bar b x to be the same? This is not really a question of what I expect, this a question of how you want functions in your language to behave. I like my functions to behave in simple ways. &gt; and so you have to choose the lesser of two evils Exactly, and in my view, it's better to quarantine this non termination to function types, than to have it in every single type.
https://github.com/alanz/HaRe/blob/master/README.markdown ghc-hare 
But then when you apply the function you get an element of Lift(B), so you're still using Lift(B) for a term of type B. I don't get how that excuses it.
You're getting a B or nothing, so you can blame it on the function rather than saying the function returns bottom.
Aww. Not my area. Would be interested in meetups around Hamburg/Germany. Any tips? 
There was a nice paper at ICFP this year called [Splittable Psuedorandom Number Generators using Cryptographic Hashing](http://publications.lib.chalmers.se/records/fulltext/183348/local_183348.pdf) which describes a new random package, `tf-random`. The rundown is basically written on the tin: use cryptographic research to leverage results on psuedorandomness and PRFs for high quality splittable randomness. The problem in particular is that splitting is defined in a rather ad hoc way in the `random` package, and results in very weird correlations between random numbers sometimes. You might not think this is a big deal, but the example on Page 1 is a rather humorous example of how this ends up very bad for QuickCheck. The problem here is likely the same thing. Sure enough, the `randomR` definition for integrals is built directly off of [`next` and `split`](http://hackage.haskell.org/package/random-1.0.1.1/docs/src/System-Random.html#randomIvalIntegral). If you change the ranges, you'll probably get (dramatically) different results. I enjoyed this paper a lot because the basic idea seems pretty simple in retrospect: let's tap into well known cryptographic constructions to get good, high quality randomness. Seems like the Haskell way! If we modify the sample a little in the post to use `tf-random`, we get a much, much better distribution: module Main (main) where import Control.Monad (forM) import Data.Array.IArray (Array, accumArray, assocs) import System.Random (randomR) import System.Random.TF (TFGen, seedTFGen) rollDice :: TFGen -&gt; (Int, TFGen) rollDice = randomR (1,6) get_first :: TFGen -&gt; Int get_first g = fst $ rollDice g histogram :: [Int] -&gt; Array Int Int histogram l = accumArray (+) 0 (1,6) $ do { x &lt;- l; return (x,1) } main :: IO () main = do let results = map (\i -&gt; get_first $ seedTFGen (0,0,0,i)) [0..53667] mapM_ print $ assocs $ histogram results $ runghc ./badrandom.hs (1,9009) (2,8814) (3,9009) (4,8808) (5,9006) (6,9022) I hope at some point it can subsume the default `random` package, because I think this is a big flaw, and the speed impact of this approach is likely not a huge practical difficultly for most people (~8% impact on QuickCheck in the paper).
Whether a RNG is good or bad depends on what you want to use it for. I use System.Random as a "just give me a few different numbers" library. It certainly is not a library for when you need (guaranteed) unpredictable or well-distributed numbers; I would blame the programmer, and not the library, if this leads to security holes.
Read my other comment. The problem is that the ad-hoc nature of the `random` package means that anything built on `split` is susceptible to very weird resulting distributions, which can have a real impact on things you might expect to work. See the QuickCheck example in the paper I linked for a real case. It's true you should use the right generator when needed. If you want really good entropy, you should probably use something well proven of course (`/dev/urandom`, a stream cipher seeded from it or something, etc). But I don't think that's an excuse for the very basic and fundamental `random` package to be pretty broken internally - especially when its core API that differentiates it (splitting) exposes such broken-ness in unpredictable ways. It doesn't have to be cryptographically secure, even - but it should actually generate numbers with a good distribution, which is a basic thing you'd expect.
Somehow I overlooked diagrams-gtk! Thanks for the pointer; this would have simplified some of the work. I have tried it out and did run into some small problem: it would be nice if `defaultRender` and querying could work together. I could only get querying to work right with `renderToGtk` + `toGtkCoords`. 
So take this as constructive criticism, please. &gt; Happstack rubs me the wrong way. Sorry Happstack devs, you did a great job but it just doesn’t feel right to me. I think all four candidates are very capable and have many interchangeable parts, so it comes down to style and personal preference. And I don’t like Happstack. Gosh the above is useless. Even when one has a personal preference regarding style, typically it is possible to describe the _elements_ of that preference. For example if a shirt just rubs me the wrong way, it may be because I do not like the color, or the font, or the texture of the cloth. This would all constitute "personal preference" but it would also help others to decide if these elements of the shirt (color, font, etc.) suited their own personal preferences or not. Obviously when we have various very fine frameworks, personal preference/style concerns will constitute an element of the choices that are made between them. But it would be nice for those sorts of issues to be catalogued in a more helpful fashion. Edit: I see that you modified the blog post in response. Its certainly clearer to me now where you're coming from. Thanks.
Nice write up. Did you write it as you went? It feels like your immediate reactions to things as they occurred. Also if you haven't decided as to how you're going to host your app (if it needs hosting) then let me recommend this [heroku buildpack](https://github.com/begriffs/heroku-buildpack-ghc). I tried it just recently and although I haven't gotten around to using it fully, getting started was rewardingly simple. You just have to make sure you're on ghc-7.6.3.
I see that you are doing a traversal of hist for each pixel in the image. What you could do is update the elements of the array "in place", possibly with a constant time operation. If that's not possible with repa arrays (I'm not familiar with the library), I would suggest you take a look at IOArrays, which allows for destructive updates.
I don't understand how dependency bumping works here. Say I want to set the upper bound of process to '&lt; 1.3', is that possible or does it just depend on what's installed in the sandbox? The syntax I had in mind for this would be `--upper process '&lt; 1.3'` or similar. I think cabal reformatting would be a blocker for me unfortunately. I use bumper and that just uses some hacky regexes to preserve formatting which is nice ;) This is a bit different from bumper though. Bumper focuses more on bumping transitively and doesn't allow you to bump external packages, so it's really only useful if you are developing several related packages. 
Well, unless you need to modify the histogram after it has been created, if there is no need for modification of the historgram elements after the fact, I would first try using ordinary immutable unboxed arrays. So you could just use Data.Array.Unboxed, then use the 'Data.Array.IArray.array' or 'Data.Array.IArray.listArray' functions to build the histogram from a list. Forcing strictness and unsafe operations can often have a negative impact on performance. Under the hood, lazy traversal of the image and strict summing into each bin is the kind of thing that can be optimized a great deal by the compiler. Then, after the histogram has been constructed, you can use unsafe operations to get a pointer to the unboxed elements if you really need it.
&gt; ... does it just depend on what's installed in the sandbox? Yes, that's the case. &gt; I think cabal reformatting would be a blocker for me unfortunately. I use bumper and that just uses some hacky regexes to preserve formatting which is nice ;) Well, the reformating is annoying, but otherwise I just don't care that much about the format of the cabal file. Removing the bounds might be feasible with a regex, but I wouldn't want to write a regex based updating nor I would like to maintain it. But just trying to support with a regex what cabal-bounds already does, only operating on a certain section in the cabal file, would be already some kind of a nightmare.
This was a perfect article for me. I'm a seasoned Haskell developer and looking to start writing a web application of my own in the near future. The terse writing style in this article is very easy for me to read, it's like reading my own notes. Thanks for this.
Thanks!
The Internet says that it's still around!
Edwin just wrote up a [well-typed hangman](https://github.com/edwinb/idris-demos/blob/master/Hangman/hangman.idr) - that's pretty neat!
Your travAll function visits the entire 3x256 elements just to update 3 of them in computing a new hist, which is very inefficient. Better use mutable array so that you can selectively update 3 values. 
noted. will expand this in a few hours when i have some time to write.
Yes this was written during usage. "While it installs" really means I was waiting for cabal to install some packages. I was planning on using heroku yes. Thank you.
In fact, I identified 3 solutions: 1. with type parameter, with a polymorphic type 'r' to denote effectless-ness 2. with type parameter, with a concrete type 'NoEffect" to denote effectless-ness 3. with type classes The solution 2 have more elegant type signatures, but it uses an unsafeCoerce. I see no way how to get rid of it. I uploaded all three solutions here: https://github.com/cdupont/Nomyx-design
Wow, thanks you. I am creating a yesod app and I was trying to figure out how to deploy it to heroku.
In response to the comment of /u/sclv the author tried to tone down that expression of opinion about Happstack a little. I agree with /u/sclv and I still wish that line wasn't there at all. But other than that, this is just an amazing review of the major Haskell web frameworks.
I was thinking of a different approach import Control.Monad.Free (iterM, liftF, Free) import Control.Monad (join) data NoEffectF a = ReadAccount (Int -&gt; a) | SetVictory (NoEffect Bool) a data EffectF a = NoEffect (NoEffectF a) | WriteAccount Int a instance Functor NoEffectF where fmap f (ReadAccount g) = ReadAccount (f . g) fmap f (SetVictory cond x) = SetVictory cond (f x) instance Functor EffectF where fmap f (NoEffect x) = NoEffect (fmap f x) fmap f (WriteAccount amount x) = WriteAccount amount (f x) type NoEffect a = Free NoEffectF a type Effect a = Free EffectF a liftNoEffect :: NoEffect a -&gt; Effect a liftNoEffect = iterM (join . liftF . NoEffect) readAccount :: NoEffect Int readAccount = liftF (ReadAccount id) writeAccount :: Int -&gt; Effect () writeAccount a = liftF (WriteAccount a ()) setVictory :: NoEffect Bool -&gt; NoEffect () setVictory cond = liftF (SetVictory cond ()) moreMoney :: Effect () moreMoney = do a &lt;- liftNoEffect readAccount writeAccount (a + 200) winCondition :: NoEffect Bool winCondition = do a &lt;- readAccount return (a == 200) play :: Effect () play = do liftNoEffect (setVictory winCondition) moreMoney 
Looks like it could be, yes. And I certainly prefer this more robust approach to regexes. But it would be nice if there were a little more explanation in the documentation about what it does exactly. What does it mean to "update the upper bounds by the cabal build information" - does that mean it sets all upper bounds to be exactly equal to the versions that were actually built? And ditto for the lower bounds? Or does it mean bumping to the nearest 1-component or 2-component or 3-component version? The docs should say which. And all of those would be very useful options. Another thing I am worried about is that the dependency on the Cabal library could cause this code to bitrot very quickly unless it is very actively maintained. Especially since the "shoemaker is going barefoot" here - this tool itself doesn't specify an upper bound on the Cabal dependency in its cabal file. Thank for this nice tool.
Note that [io-streams](http://hackage.haskell.org/package/io-streams) has a [Process module](http://hackage.haskell.org/package/io-streams-1.1.4.2/docs/System-IO-Streams-Process.html).
&gt; What does it mean to "update the upper bounds by the cabal build information" - does that mean it sets all upper bounds to be exactly equal to the versions that were actually built? And ditto for the lower bounds? Or does it mean bumping to the nearest 1-component or 2-component or 3-component version? The docs should say which. And all of those would be very useful options. I have extended the README. &gt; Another thing I am worried about is that the dependency on the Cabal library could cause this code to bitrot very quickly unless it is very actively maintained. Especially since the "shoemaker is going barefoot" here - this tool itself doesn't specify an upper bound on the Cabal dependency in its cabal file. Well, I haven't added an upper bound, because you most likely have to explicitely constrain the version of the cabal library to the one used by the cabal binary, because I don't know if the cabal developer increase the version numbers accordingly if they only change something on their internal data structures and cabal-bounds reads one of these. Nevertheless I have now added an upper bound for the cabal library. 
I agree that the `random` package needs more serious improvement. But for now - can we please just fix this obvious bug?
That's a very nice paper, but I would be opposed to a cryptographic randomness library subsuming the default `random` package. If you need that, what's so hard about importing a library? The default `random` package should provide the kind of randomness one expects from the default randomness library of a programming library: pretty fast, but not necessarily the very fastest, and pretty good, but not cryptographic quality. Some languages use mersenne twister. For Haskell, I would be willing to trade in a little bit of speed and/or quality for an algorithm whose splitting properties have been studied. But an algorithm of cryptographic quality would either be too slow, which I believe is confirmed by the benchmarks in the paper, or depend on crypto hardware. And you anyway wouldn't be able to use it in practice for crypto - in real life crypto you specialize your choice of algorithms and techniques for the requirements of each individual application.
&gt; But for now - can we please just fix this obvious bug? That's what this project does. The reality is up until now *we did not know how to do splitting correctly*. Or at least, nobody ever wrote it down. To expand on that: we had no solution that was well-studied with understood properties about the randomness generated by it when the RNG was split. This project taps into the crypto world - who have tons of work on provably secure PRNGs - to make strong assumptions about a new random package, under well-understood cryptographic principles. That it uses cryptographic constructions is merely a by-product of the fact those people have done substantial work in the realms of PRFs and work very similar to splitting. Leveraging that is precisely the right thing to do. `tf-random` is certainly slower than `random`, and the ThreeFish phase is comparatively expensive (see the paper for details). But between a library that's pretty fundamentally broken at what it does vs one that's not, the choice seems very clear to me. I think your position is entirely in-line with the reasoning and purpose of the paper and this project.
Oh, wonderful! That was the missing link. I updated my example: https://github.com/cdupont/Nomyx-design/blob/master/TypeParamEffect-Concrete.hs So now, have 3 real alternatives. Hard time to choose one!
&gt; but I would be opposed to a cryptographic randomness library subsuming the default random package. If you need that, what's so hard about importing a library? The purpose of the package is not to provide cryptographic randomness: it's to fix `split`, which is fundamentally broken in the current implementation/design. &gt; The default random package should provide the kind of randomness one expects from the default randomness library of a programming library: pretty fast, but not necessarily the very fastest, and pretty good, but not cryptographic quality. Yes, and I'd argue one of the basic things one expects is that it provides reasonable resulting distributions under many different cases. &gt; For Haskell, I would be willing to trade in a little bit of speed and/or quality for an algorithm whose splitting properties have been studied. That is *exactly* what this project does! PRFs and PRNGs have been meticulously and religiously studied by cryptographers under very well understood, well-known assumptions for *decades*. Tapping into that research is 100% the correct thing to do. And there is no quality tradeoff: it is obviously superior in terms of quality, as it does not randomly (*cough*) break down, and we can make strong assumptions about it with sound principles. &gt; But an algorithm of cryptographic quality would either be too slow, which I believe is confirmed by the benchmarks in the paper, or depend on crypto hardware. ThreeFish is surprisingly fast although it is dominant in the benchmarks. I'm not convinced it's "confirmed" the approach is too slow - the paper confirms `TFGen` as having about an 8% impact on QuickCheck, which while certainly not negligible, is not catastrophic, while being a superior number generator with stronger guarantees. QuickCheck is exactly the kind of place you want a solid RNG, and it *really* needs a good `split`, cryptographic or not (as seen in the paper). And even now, `random` is not a package people tend to rely on for high-speed high-security randomness anyway, as you have hinted. You use it in much more basic situations - like when you need `split`. Which something the alternatives don't provide. And you expect `split` to work. And anyway, between an approach that's fundamentally broken - which we do not understand or have guarantees of - vs an approach that's not broken, with well understood properties... the choice is extremely obvious. &gt; And you anyway wouldn't be able to use it in practice for crypto - in real life crypto you specialize your choice of algorithms and techniques for the requirements of each individual application. Yes, I know this. That's why nobody suggested using it as a cryptographically secure number generator (you could replace ThreeFish with &lt;BLOCK CIPHER&gt; and it would be the same). The fact it uses cryptographic constructions is merely a by-product of the fact those people did all the hard work already, a long time ago.
Playing the Razor variations is good. Doing it using libraries like this is bad. All the library overhead is distracting and confusing, taking away from the core concepts. Also, doing it all in one giant stack is also bad. If you want to show people what monads can do, it's better to have a bunch of small little nuggets of idea, isolated from one another, so the reader can get a sense of whats going on in each case, and how each case is, in fact, a monad, and why this monadicity is what makes it easier and more beautiful to wrote these things.
I was simply suggesting that in the meantime we should drop the first production of the generator. This bug is not surprising. A `StdGen` consists of two `Int`s, and the function `newStdGen :: Int -&gt; StdGen` always sets one of them to zero. EDIT: But wait, that would do what I am arguing against in my other post: inflicting the pain of an algorithm change for an algorithm that doesn't really solve the problems yet. So I modify my proposal: we should change the documentation to recommend using `snd . next . mkStdGen` and `fmap (snd . next) newStdGen` due to reduced randomness in the first production.
We developed some tools to make including diagrams in haddock [easier] . Since we are making tools for making diagrams, this comes in handy for our own documentation (for [example]), but we would love to have more users :D. [easier]: http://hackage.haskell.org/package/diagrams-haddock [example]: http://hackage.haskell.org/package/diagrams-lib-1.0.1/docs/Diagrams-TwoD-Offset.html
I'm not related to Zalora or anything just posted here in case someone was interested. The description sounds pretty awesome besides being a Haskell job.
I think this was a sincere post, a great effort, worth looking at, and I would like to see more people doing this. The existence of possible helpful suggestions for improvement is not a reason to downvote. Definite upvote for me, especially in light of the undeserved downvotes.
Part of choosing libraries is knowing what's idiomatic, but another part is just developing your own style. For people who are already experienced developers in an imperative language, both of those take a little longer for Haskell than for just another imperative language. The package management situation has improved by a huge amount recently thanks to fantastic work by the Cabal team. I can't argue with your comment about crack :).
Great comment - you must have learned that just now!
&gt; I think cabal has improved in this regard the last couple of years. That is true - but in my opinion it has improved as much in the last six months as it did in the couple of years before that.
I think I've seen zalora pop up a couple times on this subreddit. They needed a lot of developers for some reason.
You're right. It is also true for the persistent library with regard to SQL. And that agnosticism at the simple level is still helpful. It gives you quick prototyping without committing to a DB, and it helps you to structure your app to make it a little easier to port to a different DB if you ever need to. But porting will always be hard. On the other hand, the automatic generation of Haskell types and marshaling code from a model is extremely valuable, and most of that is quite independent of your backend.
Standard Chartered is the main one (and seem to populate half the meetups), and I think our (Zalora) ads and events have been broadcast quite well across the community but yes, interest in FPLs in general and Haskell in particular is slowly taking off.
The query for [functional](http://www.meetup.com/find/?keywords=Functional&amp;radius=6&amp;userFreeform=Hamburg%2C+Deutschland&amp;mcId=c1007699&amp;sort=recommended&amp;eventFilter=mysugg) shows some results, but it seems there is no dedicated Haskell group yet near.
The success of Emacs, and the long life of its codebase, is indeed a testament to LISP, to RMS, and to the many who have worked on it. But the scaling complexity issues that had to be constantly overcome over the years are well known. Anyway, any day now, we in the Haskell community will finally get around to finishing up Yi, and then it will destroy Emacs. You just wait and see.
We might, we might. (I mean, we should)
Why is using libraries and doing it all in one giant stack "bad" ? Are you talking about some fundamental performance reason beyond pedagogy ?
Having the version number 1.4.1, would it be sufficient for you to have something like: --lower=major1 =&gt; 1 --lower=major2 =&gt; 1.4 --lower=minor =&gt; 1.4.1 The same would be possible for the '--upper' option. 
Most of the Num types have some "wrong" property - people like to be mean about floats, but you can't add 2147483647 + 1 in Int in the "right" way either, for example. (self publicity: I'm doing one of the talks at London HUG in two days time about just this www.meetup.com/London-Haskell/events/167064162/ )
out of interest, why do you see commutativity addition and multiplication as having different levels of expectedness?
This is a neat opportunity. I have a question about it. How does remote work for a job like this go if you're on the opposite side of the world? Is it common? I could see how it might be less than ideal for both parties, but still doable. Would somebody who was stateside have to work at night all the time? Remote work with Haskell is totally my dream job right now.
No, it's a pure pedagogical issue. Whch is the only thing that matters here since this is a post trying to help people understand.
Float is not a ring. Int is (even with overflow). Of course, with Int in particular, it's not defined *which* ring.
I'm working remotely from France, and it's quite neat to be able to work when you want, as long as you're productive. Sometimes, I just like to work on the evenings/nights, it's calm and helps me focus. I'm basically switching from "normal work hours" to a completely shifted rythm depending on my mood, schedule, etc. It requires some self-discipline but it's really nice.
Addition being commutative is pretty fundamental to what it means to be able to add, subtract and multiply. I guess the easiest way to show this is to prove it from the other laws: (1+a)(1+b) = (1+a)1 + (1+a)b = 1+a+b+ab but also (1+a)(1+b) = 1(1+b) + a(1+b) = 1+b+a+ab so subtracting 1 from the left and ab from the right we get a+b = b+a. But there are lots of useful systems where you can add, subtract and multiply in which multiplication is not commutative. One familiar one is n x n matrices (for a fixed n &gt;= 2). Or you can say that the (+), (-), (*) operations of Num are the structure of a ring, and it's not universally agreed in math whether the term "ring" means that multiplication is required to be commutative.
yups.. i believe so too.. well said.
I think WAI (the web application interface, that sit between the server(s) and the app(s), like Rack in Rubyland and WSGI in Python) is quite important. I'd shy away from an app framework that does not use it (or comes with a reasonable alternative). When it comes to full-stack frameworks, there are some necessities: generic authentication, i18n, pluggable sub-sites, pluggable persistence back-ends, pluggable template types, logging, ... Potentially you want a full-stack framework to have something in mind for getting Haskell to the browser. Fay or Haste come to mind. This Haskell-to-JS thing is a potential game changer. I'm currently using Fay as part of a Yesod-based pet project: awesome. Just my .02
SHRDLU is amazing. If you end up doing this, post about it. I'd love to see that!
How does that affect collaboration. I can only speak for myself as an aspiring professional Haskell developer but one of the main things I'm looking for is to learn the patterns and style of other professional Haskell devs. The fastest way I know to do that is collaboration/pair programming. How do you guys accomplish that exchange of information with such a widely distributed team?
&gt; for each &lt;noun&gt; &lt;var&gt; if a &lt;noun&gt; that &lt;trans-verb&gt; some &lt;noun&gt; and &lt;proper-name&gt;'s &lt;noun&gt; &lt;trans-verb&gt; 2 &lt;noun&gt; then some &lt;noun&gt; &lt;intrans-verb&gt; and some &lt;noun&gt; &lt;distrans-verb&gt; a &lt;intrans-adj&gt; &lt;noun&gt; &lt;proper-name&gt;'s &lt;noun&gt; &lt;adverb&gt;. Yeah, but does he &lt;trans-verb&gt; &lt;beast-of-burden-anaphora&gt;?
This is a heck of a lot easier than the approach I've been using in `lens`. Great!
sure. but 2147483647 + 1 doesn't work out "right", in the way that the man on the clapham omnibus would say is right. Num isn't a ring typeclass. Its a Num-ber type class, with very little constraint on it.
I feel sorry for @Tekmo. He created pipes-process (https://github.com/Gabriel439/pipes-process) only 2 days ago.
Certainly a fair position (and, like I said, I was mostly kidding). I do think Float breaks in more surprising places than Int, though.
I think Learn You a Haskell does a great job at this, especially when it convinces you that `-&gt;` is a `Monad`. However like most examples it then focuses all its efforts on `IO` after that. Not that I disagree with the approach in this case, since learning `IO` is very important at first.
Tangentally related, from the [project's site](http://attempto.ifi.uzh.ch/site/): &gt; Actually, Attempto ("I dare" in Latin) was the motto of Graf Eberhard im Barte ("Duke Eberhard the Bearded") who in 1477 founded the University of Tubingen, Germany. More than five-hundred years later, Attempto was the motto of Norbert E. Fuchs — a graduate of the University of Tubingen — when he started the Attempto project at the University of Zurich in 1995, defying "has been tried, can't be done" statements of some big shots in the field of computerlinguistics that Norbert had asked for advice. Now that's an attitude I like :)
It's strange that you like happstack the least, for me it's easily the best option, if only for the fact that they easily have the best documentation. I'm pretty sure they pioneered the typesafe approach to URL routing, and introduced acid-state. Also I hear rumors that happstack will switch over to pipes which would be a killer feature IMO. The biggest minus I find with happstack is that development is hard to follow and contribution is next to impossible because it's not on github (or even git at all).
It's a bit early to say since the remote guys started in the last few weeks, but it seems to be going OK. Our nix network and build farm were built entirely remotely, as well as a customer service QA tool. The team communicates via a group Skype text chat (for historical reasons: that's what the rest of the company, our users, have always used), one on one Skype (or IRC if you prefer) and occasionally email. We do code review after launching as we have had the luxury of being able to pick people who can write production quality code straight away. It's still a small team (in the single digits) so it's relatively easy for people to just ask someone else. I do think it's slightly easier in person, but I'm as guilty as anyone to Skype (text) someone across the desk from me (usually to avoid disturbing them when concentrating). So if anything, I see the future of the team as being more decentralized than office-based. Also, going remote has allowed us to greatly expand our available talent pool - there's some truly great people out there who for personal reasons will never move, and I really recommend it to any company who is wondering whether to try. 
For math, you can also use the `--lib` option to specify a custom `haddock-util.js` that dynamically loads up [MathJax](http://www.mathjax.org/). You do need to double-backslash your inline TeX, though.
&gt; I emailed L'Ecuyer, the author of the now outdated 1988 algorithm we are currently using. I should point out that, while L'Ecuyer's paper is the source of `StdGen`'s `next` function, `split` appears to be the spur-of-the-moment work of an anonymous Haskell hacker. There is no apparent principle behind it, and nobody even knows who wrote it. Would of course be nice to know L'Ecuyer's thoughts. &gt; We need something like Mersenne Twister, but with a working split. &gt; It just seems so unlikely that there is no reasonable way to do that. Mersenne twister has a huge state, so I don't see how anything like it can be splittable. My naive, possibly ill-informed impression is that `split` is hard because starting from a seed, splitting it n times and choosing each time either the left or right seed, there are 2^n possible paths. If any pair of these paths is correlated over the set of all starting seeds then you have lost - and n may be arbitrarily large. In particular 2^n can be much bigger than the number of possible seeds, so on any run many paths will give equal outcomes, but the distribution of equal paths has to even out over all starting seeds - nothing like sequential RNGs, where the seed never repeats in practice. So I do think you may be underestimating the difficulty of a good `split`. There is also the observation that any good-quality splittable RNG gives rise to a good-quality hash function, which (to my naive eyes again) seems to suggest that splittable RNGs are hash functions in disguise, and we can't really hope to turn a bog-standard sequential RNG into a splittable RNG any more than we can turn it into a hash function.
You'll be pleased to know that the switch to pipes also includes switching components over to github as well. darcs rocks.. but the angry mob has spoken -- so github it is.
Since my own experiment became more or less useable (well, or so I hope) I decided to upload it to Hackage. I intentionally avoided names resembling "pipes-process" so as not to create confusion when an "official" package becomes available. At first I intended to give the user direct access to the handles in case he wanted to use another streaming abstraction, but at the end the project became completely pipified (pipeified?). If nothing else, it has served me to become more familiar with the **pipes-parse**, **pipes-bytestring**, **pipes-text** and **pipes-group** parts of the ecosystem (plus **pipes-concurrency** and **pipes-safe**). In fact, perhaps people studying these libraries can find useful tidbits in the [tutorial](http://hackage.haskell.org/package/process-streaming-0.0.1.1/docs/System-Process-Streaming-Tutorial.html).
I had a hard time understanding what's going on here, partially because the font you used is unreadable to me: I kept going "WTF are Lacinace words?" I suggest making your "Enter text here..." message actually be the default filled in text, because that will provide plenty of examples for words of Latin origin. I kept typing in things without Latinate words, so for all I could tell, your app just made the first upper-case letter nice and big and just rendered the rest as is. For what it's worth, I only finally saw the suggested synonyms when I typed this into the box: &gt; Mind if I have some of your tasty beverage to wash down this tasty burger?
&gt; There are persistently recurring complaints that everything is too hard to understand because of all the math and the lack of tutorials. Maybe that’s because people generally aren’t exposed to mathematics, despite how valuable it can be in programming, so when they finally encounter it in the land of Haskell, they get discomfited and grumbly. &gt; You just need to be able to read type signatures. Hah. And understand their implications! *That* is the hard part. &gt; I still don't know a thing about what `F` or `Coyoneda` mean. But it isn’t really slowing me down. Well…maybe the hard part isn’t strictly necessary. &gt; All it takes is for people to focus on what’s present and how useful it really is. That is one of the most profound statements I have ever seen written, abut programming languages besides. 
Yet another great project by Chris. Thanks!
This is what I like to call "type tetris." It's fun, and you end up with correct programs without even having to understand what you just assembled. I assembled the "tardis" package largely by just playing type tetris, and then addressing the runtime bugs. This "gamification" is a familiar concept to the world of dependent types and theorem provers. You can prove stuff just by following the types without really understanding the proof. Usually with proofs, though, it's good to get an actual human intuition of why the thing you just proved is correct, so proof people will usually caution against just "playing the game" when you approach such tasks. It's still fun, though.
This is a great article to show people who are trying to learn Haskell. Even after they have learned to read type signatures, they often feel disorientated or even resentful when there is a "lack of documentation" or a "lack of tutorials", because they became accustomed to those things as the exclusive way to learn things in previous languages they've used. The realization of how powerful type signatures are as a documentation tool will set them free.