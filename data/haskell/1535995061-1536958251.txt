Best book and explanations about CT concepts. It will help the reader to understand well Bartosz Milewski comments on Haskell, retraction, section, product, coproduct. The fixed point explanation is just the best I read, remember Catamorphism of F-Algebra ?
Yes, that is precisely what that is.
Thanks! I hadn’t yet stumbled on the term “observable sharing”.
Thanks! Now everything works as desired. I also noticed that the timing from :set +s in GHCi displays the time taken by both threads together, so with test2 I got it to display 7.5 seconds while I measured it with a timer to be around 4.
Hello the GSoC student who worked on this here! As mentioned by /u/cartazio it will take a while to be merged. A lot of work related to operating with integers in SIMD involves better support for certain unlifted types(like `Int8#`, `Int16#`, `Int32#` etc) and I have made patches for them as well. This project is also my Master's dissertation so I am spending a lot of time finishing up my thesis right now detailing what I *have* done and what *needs* to be done. I will make it public, if anyone is interested. Currently I am also working on a library https://github.com/Abhiroop/lift-vector which is not using any FFI but natively calling the vector primops of GHC which I added. But to work with it you have would have to build my branch of GHC. I will add detailed documentation and many more examples in the next couple of weeks.
In the two lines with "env Map.! name", don't you mean to set t2' instead in the second case and pass it into "otherwise" right above?
Yes, it should have been `t2'` as you noted. Thanks. &amp;#x200B; As for threading the `seen` list between recursive calls, would that be more efficient but not necessarily more complete? Either way... it's just an example I tried to quickly make up to get the gist of what I'm trying to do across w.r.t. cyclic data ;-) Please don't anyone use this code without seriously testing/verifying it does whatever you think it should do xD
This is how you use the ideas /u/njiv points out, from [*Type-Safe Observable Sharing in Haskell*](https://pdfs.semanticscholar.org/presentation/4838/bd0a91b3058b467fa31ad9e0810121b46388.pdf) We use a funky looking type class [Data.Reify.MuRef`](https://hackage.haskell.org/package/data-reify-0.6.1/docs/Data-Reify.html) with an [*associated* type family](https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#associated-type-families) for the datatype you want to reify (`Tree`) without the recursion "factored" out -- MuRef :: Type -&gt; Constraint -- DeRef :: Type -&gt; Type -&gt; Type class MuRef a where type DeRef a :: Type -&gt; Type mapDeRef :: Applicative f =&gt; (forall xx. MuRef xx =&gt; SameDeRef xx a =&gt; xx -&gt; f b) -&gt; (a -&gt; f (DeRef a b)) type SameDeRef a b = DeRef a ~ DeRef b So in your case we can define `TreeF`, a "base functor" of `Tree` wherein all recursive occurrences are replaced with a parameter `tree` and use that as `DeRef Tree = TreeF` -- Tree :: Type -- TreeF :: Type -&gt; Type data Tree = Leaf | Node Int Tree Tree data TreeF tree = LeafF | NodeF Int tree tree deriving stock (Show, Functor, Foldable, Traversable) instance MuRef Tree where type DeRef Tree = TreeF mapDeRef :: Applicative f =&gt; (forall xx. MuRef xx =&gt; SameDeRef xx Tree =&gt; xx -&gt; f b) -&gt; (Tree -&gt; f (TreeF b)) mapDeRef f = \case Leaf -&gt; pure LeafF Node n left right -&gt; pure (NodeF n) &lt;*&gt; f left &lt;*&gt; f right and now we can easily reify your graphs `ones` and `threes` &gt;&gt; :t reifyGraph .. :: MuRef a =&gt; a -&gt; IO (Graph (DeRef a)) &gt;&gt; :t reifyGraph @Tree .. :: Tree -&gt; IO (Graph TreeF) &gt;&gt; &gt;&gt; reifyGraph @Tree ones let [(1,NodeF 1 1 1) in 1 &gt;&gt; reifyGraph @Tree threes let [(1,NodeF 3 1 1)] in 1 
I also like the [Abstract Syntax Graphs](https://www.andres-loeh.de/ASGDSL/ASGDSL.pdf) paper, if you want to make the recursion explicit.
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [githwxi/ATS-Postiats/.../**wclines.dats** (master → 94da1c2)](https://github.com/githwxi/ATS-Postiats/blob/94da1c250e665270df317bd4ca6162376c0bb308/doc/EXAMPLE/MISC/wclines.dats) ---- 
Seems like my story (guessing from the title), but the link 404s for me, unfortunately.
I lol'd at the brevity of part one (of Infinity, no less). I, too, have taken the way-of-the-yak(-shave) in under 300 words that begin with "I ought to be able to generalize this."
Heh, saw that earlier and wanted to comment that polynomials are better represented backwards, which fixes that (and many other problems), but the blog allows no comments and I think it's mostly irrelevant 
Depending on the distribution of `\n`s, that [may be significantly slower](https://www.reddit.com/r/rust/comments/99e4tq/reading_files_quickly_in_rust/e4nw7gz/?context=3).
Just BTW: For the example in the post, [`Data.Align`](http://hackage.haskell.org/package/these-0.7.4/docs/Data-Align.html) is pretty handy. In particular, [`salign`](http://hackage.haskell.org/package/these-0.7.4/docs/Data-Align.html#v:salign) is pretty similar to `mzip` from the post.
Here's [an archive.is snapshot](https://archive.is/WYJZl).
The fix is something like “stop doing that”. &amp;#x200B; Or maybe “make a note to follow up the generalization later” and then go with the easy solution. &amp;#x200B; &amp;#x200B;
Our cousins in a different subreddit have a different take on this: \[Haskaller too smart to get anything done\]([https://www.reddit.com/r/programmingcirclejerk/comments/9cpfy8/haskaller\_too\_smart\_to\_get\_anything\_done/](https://www.reddit.com/r/programmingcirclejerk/comments/9cpfy8/haskaller_too_smart_to_get_anything_done/))
I have the same problem. Gotta give yourself dead lines mostly just to justify writing the simple thing. Untested (and written on mobile) but I think something like the following would be my goto: zipWith(+) (a++replicate (length b-length a) 0) (b++replicate (length a-length b) 0) 
Yeah, what I was thinking!
There's no good way to handle this problem, but the problem with zipping here seems to be related to the idea that x^(3)-3x+1 = 0x^(4)+x^(3)-3x+1, but `Poly [1, -3, 0, 1]` != `Poly [1, -3, 0, 1, 0]`. Representing a stream that is zero almost everywhere with a list is pretty much the only sane way to do it, but leads to headaches like this. Until someone finds a computational representation for higher inductive types, of course, which will lead to a whole *new* kind of headache.
Switch to Idris and keep array length at a type-level.
I'll echo my Hacker News comment here: There's a weird interpretation here that this post is the author expressing frustration with this process. I often have a similar experience and I wouldn't want it any other way! This process of repeatedly asking "what is this?" just doesn't seem to come up in the same way in other languages. This gives me the ability to do some practice I wouldn't otherwise be able to do, one that often has tremendous transfer over to "real work", because I can start to see patterns and get a feel for what is really going on once I get rid of all the dull IO tedium. If you want an analogy, consider this like studying jazz or something. Sure, you could just notice a II V I progression and call it done, but if you pick away at each individual note, you can find a whole lot more going on behinds the scenes. Basically, I don't really consider what's happening in the blog post a bad thing. It just has a time and a place, and you need to be aware when it's the wrong time.
We'll it is! You just have to write the right `EQ`. 
And now you have six list iterations instead of two. zipWith likely borks up fusion on the second list anyway and the lists probably aren't terribly long but it still is kinda awkward.
&gt; How to test, and when? I've been using backpack more and more often for this purpose and building a module signature for the operations I want to instantiate. Then I instantiate the module signature for the real operations I want when I'm running the code, and for a set of operations that mock the API, possibly for a completely different monad, for debugging as needed.
Answering "when to test", because I recently gained some insight on the topic. Any time you run something to check if your code it doing the right thing, you should add that thing to your test suite. Running a function in the REPL and checking its output, or making a sequence of requests to a server and checking their responses - anything that you would be doing often to see if your application behaves correctly. This turns your development workflow into compile -&gt; run relevant tests -&gt; repeat, and you naturally build a robust test suite.
/u/sjakobi provides the solution: it's not `zipWith` but [`alignWith`](https://hackage.haskell.org/package/these-0.7.4/docs/Data-Align.html#v:alignWith) that OP wants. It is however not clear at all what OP could have googled to find out about it. :/
I think OP's straightforward solution was actually the right move: it's a lot clearer than the `zipWith`-based and does exactly the same thing. (Poly a) + (Poly b) = Poly $ addup a b where addup [] b = b addup a [] = a addup (a:as) (b:bs) = (a+b):(addup as bs)
I had this problem several times until I decided to create a [library](http://hackage.haskell.org/package/list-zip-def-0.1.0.1/docs/Data-List-Zip.html) for it. zipDefWith 0 0 (+)
May I use this opportunity to shamelessly promote [this kata of mine about polynoms](https://www.codewars.com/kata/54f1b7b3f58ba8ee720005a8)?
I haven't seen out of memory errors. I guess my blog is very small and it is very close to template Hakyll setup.
I realise that this might not be the perfect subreddit for this, but since the author mentions Haskell in the text, I figured it might spark an insteresting discussion - mainly how to work with complex structured unreliable data, such as JSON. For the record, I love Haskell and static typing, however, similarly to the author of the article, I found working with JSONs quite painful in strongly typed languages - mainly because we don't live in a perfect world and responses we get from a lot of APIs are far from perfect. 
Mostly because dynamic languages let people get away with garbage.
The thing is you don't _need_ to build perfectly strongly typed things that narrowly specify exactly what you'll see in a JSON document to work with it in Haskell. Eric argues against a bit of a straw man in this article. Write `aeson` instances for the nice, _interesting_ and consistent parts of your document, if any, where it is worth investing some time in serialization/deserialization code, and making sure you have types to avoid typos on one of 50 references to the "username" field in an JSON object. Then use `lens-aeson` to drill down to those interesting bits through the `aeson` `Value` data type. A lot of application development today is taking some horrible pile of JSON from some web service, mangling it and tossing it off to another. This sort of workflow, replete with types which rise to whatever level of specificity that is useful. Mind you, you still have _some_ typing even in `Value`, the catchall case.
I'm surprised OData or OData-esque solutions haven't become more popular for this problem. It seems like services and clients should have a type conversation before or alongside REST transactions.
You say it almost like they disagree! I'm a person that really enjoys the process of trying to make perfect snowflakes, because I really enjoy solving all these little side problems that often times are highly abstract and quite distant from whatever my stated goal is. That's my hobby. Work is different, but in my personal time, I'm not interested in concrete problems other than as a sort of workbench upon which I will continue perfecting said snowflake :) Also the tone of the post seems rather self deprecating so I think they probably wish they could stay focussed on the goal.
&gt; In other words, we can rewrite zoom as simply &gt; &gt; zoom :: Lens' st st' -&gt; State st' a -&gt; State st a &gt; zoom = id nice
&gt; haskallers are time wasters &gt; *goes back to adjusting .vimrc* we're having fun &gt; that's the longest haskal program i've ever seen eXCUSE ME
You don't need idris for that, it's possible in haskell http://hackage.haskell.org/package/sized-vector-1.4.3.1/docs/Data-Vector-Sized.html
&gt; Any time you run something to check if your code it doing the right thing, you should add that thing to your test suite. ... or your types / proofs.
just use `Value` or `Map`.
This seems like a bit of a straw man. At the time the author claims lenses were rather new and they weren't using them in their code. Times have changed and lenses are pervasive. You can compose arbitrary lenses and prisms over aeson Value structures. If you need to transform arbitrary JSON documents you have recursion schemes. The types are always there whether they're well structured or not. They're just more visible in Haskell and that's a good, useful feature to have.
Your use of `seen` is called memoization, remembering the results of previous calculations instead of redoing them. But since a node sends the same `seen'` first to the left node and then to the right node, what was inserted in the left node inside `lEquiv` will not be passed on to the right node when calling `rEquiv` which you probably want.
I learned Haskell with the rather old book "Real World Haskell", and I loved it. I tried many other books before, like "Learn you a Haskell", but in the end I just liked the style of the mentioned book the most. Now, my question is: Are there any similar books that are more up-to-date? I am specificially looking for ones that include chapters about lenses, data kinds, generics and template haskell.
... While not *entirely* untrue. I don't think the tone of the comment is good. I'd say 70% or more of dynamically typed code can actually be given a really simple, inferred type. In that case, they aren't "getting away" with anything. I'd say probably 60% of the remaining code can be given a (parametrically) polymorphic, inferred type. Again, no one is "getting away" with anything. Most of the things people "get away" with in a dynamically typed languages are things that don't have a good type in the type systems of C++ or Java, and even with as awesome as inference is in Haskell can require some serious work to make explicit to the compiler what the programmer already knows. Some times this even gets into proper dependent types, where the implicit knowledge (or assumptions) would appear as type indexes -- and most DTPLs would *still* require some cast/project/embed functions to be inserted explicitly. Dependent Haskell should help there, but we are there *yet*. *Of course* you can write trash in any language. You can write stringly-typed Coq if you put your mind to it. You can operate in Haskell, but entirely on Dyn values. No amount of static typing is going to prevent all "garbage". 
a majority of code being "typeable" doesn't really matter, or at least correctness isn't "linear" in well-typedness. since a single ill-typed bug can crash the program, even it doesn't "directly" crash the rest of the well-typed code.
Even Haskell is not needed for this. https://en.cppreference.com/w/cpp/container/array And even Pascal had array length in the type.
writing a hundred expressions of type `IO Dyn` is *much* harder in Haskell than Python, because of laziness, and no syntax for dynamic attribute. relatedly, even if your type-system is dynamic, a dynamic module-system (/ "name-system" (?)) enables/encourages even more obsecenely illegible programs. lookup (i.e. `\_\_getattr\_\_` and python's `import`).
Just this or last week I was thinking about adding binary numbers to `fin`.
That's why 90% of Haskellers do little more than massaging abstract lists and some even get paid in universities for it.
Codenvy.io You have a 4 GB instance for free and you can run any docker image on it. 10€ month an additional GB and 4 hours before swapping, Then you can start in a machine and continue in another. I use it at work and at home exactly the same environment and file state. I can make teams to work in the same image. I'm not affiliated to codenvy.io. I just want to increase the haskell user base to convince the codenvy developpers to better support Haskell.
It might be hard to top McBride &amp; Paterson's [Applicative programming with effects](http://www.staff.city.ac.uk/~ross/papers/Applicative.pdf) for long-lasting impact.
Hmm. I had assumed that the lists were fairly average polynomials (in memory, fairly short, num types)
Neat. Thanks. 
Same, it walks you toward what to expect for addup without any exterior code.
Some of my favourites: 1. [J. Christiansen, N. Danilenko, and S. Dylus, ‘All Sorts of Permutations (Functional Pearl)’, in Proceedings of the 21st ACM SIGPLAN International Conference on Functional Programming, New York, NY, USA, 2016, pp. 168–179.](http://informatik.uni-kiel.de/~sad/icfp2016-preprint.pdf) 2. [S. Dolan, ‘Fun with semirings: a functional pearl on the abuse of linear algebra’, 2013, vol. 48, p. 101.](https://www.cl.cam.ac.uk/~sd601/papers/semirings.pdf) 3. [M. D. McIlroy, ‘Power Series, Power Serious’, J. Funct. Program., vol. 9, no. 3, pp. 325–337, May 1999.](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.333.3156&amp;rep=rep1&amp;type=pdf) 4. [A. I. Baars, A. Löh, and S. D. Swierstra, ‘Parsing permutation phrases’, Journal of Functional Programming, vol. 14, no. 6, pp. 635–646, 2004.](http://www.cs.ox.ac.uk/people/jeremy.gibbons/wg21/meeting56/loeh-paper.pdf) 5. [R. Bird and R. Hinze, ‘Functional Pearl Trouble Shared is Trouble Halved’, in Proceedings of the 2003 ACM SIGPLAN Workshop on Haskell, New York, NY, USA, 2003, pp. 1–6.](https://www.cs.ox.ac.uk/people/ralf.hinze/publications/HW03.pdf) 6. [R. Hinze, ‘Functional Pearls: Explaining Binomial Heaps’, Journal of Functional Programming, vol. 9, no. 1, pp. 93–104, Jan. 1999.](http://www.cs.ox.ac.uk/ralf.hinze/publications/#J1) 7. [M. Erwig and S. Kollmansberger, ‘Functional pearls: Probabilistic functional programming in Haskell’, Journal of Functional Programming, vol. 16, no. 1, pp. 21–34, 2006.](http://web.engr.oregonstate.edu/~erwig/papers/abstracts.html#JFP06a)
Thanks, but that's not the problem I was talking about. I was referring to the difference between the computational representation of the data and the conceptual purpose of it, -- namely that "x + 0" is distinct from "x" -- which *does* have at least one workaround, but requires a dependently typed language with a feature that hasn't been implemented computationally (stating that a list, and that list with an extra 0 at the end, should be considered equivalent) and also sounds like a pain to work with. You could require that a list ends in a nonzero value, so that "x + 0" cannot be considered, but that is less intuitive and also sounds like a pain. But at least it's currently possible!
Shameless plug for my preferred setup (which, crucially for me, supports navigate to library source code inside the IDE) https://gist.github.com/androidfred/a2bef54310c847f263343c529d32acd8 
I thought mine was small too, but this kept happening. I don't know if this is an Arch Linux issue, but compiling pandoc on my machine can use up to 3.5 GB of RAM. CircleCI only has 4 GB. The docker image that I use in CircleCI is also based on Arch. I had to [disable optimizations and limit stack to 1 concurrent job](https://github.com/GAumala/blog/blob/master/.circleci/config.yml#L29) for it to work. 
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [GAumala/blog/.../**config.yml#L29** (master → 2751fb7)](https://github.com/GAumala/blog/blob/2751fb7aea8a812cd6e8803604e9201ad9882ecf/.circleci/config.yml#L29) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply e5es5qf.)
&gt; This avoids having to bury some kind of orphan instance in my test suite I can't say I have encountered the necessity to introduce orphans yet, except maybe for 3rd-party dependencies. Maybe I have been doing everything by the book (too much?). Could you expand on how that situation would naturally arise, or give an example?
&gt; Running a function in the REPL and checking its output, or making a sequence of requests to a server and checking their responses - anything that you would be doing often to see if your application behaves correctly. Since you talk about *doing it often*, I'd like some clarification. Does that imply to write (unit) tests only for things that have already been implemented, have a stable api and the semantics of which are already clear-cut?
Implementations are available here: https://github.com/ChrisPenner/update-monad Probably only useful for education, I wouldn't recommend using any of this stuff in production 😅
I'm currently teaching myself with _Haskell from First Principles_. It's an exhaustive book that is meant to start from no programming knowledge. You can see the table of contents [on the authors' website here](http://haskellbook.com/progress.html). I'm still a beginner so I might not recognize a specific term - I checked the index and did not find anything about lenses, but kinds are covered in the chapter on Algebraic Datatypes, generics and template Haskell are mentioned briefly. You might also want to check out *Get Programming with Haskell* by Will Kurt. Table of contents [available here](https://www.manning.com/books/get-programming-with-haskell).
Thanks! Never heard about those two, I will check them out!
https://github.com/ekmett/update/blob/master/src/Control/Monad/Update.hs has been around for a while, too. ;)
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [ekmett/update/.../**Update.hs** (master → 8d3dbe9)](https://github.com/ekmett/update/blob/8d3dbe9fff6d70e20e4d97adafeebca65549a5c1/src/Control/Monad/Update.hs) ---- 
[Fixed, I guess :)](https://stackoverflow.com/a/52176670/2751851)
That's like saying it doesn't matter how little Haskell code calls `error` or `undefined` since a single call can crash the program. Or even worse it doesn't matter how useful correct calls to `unsafePerformIO` are, since one incorrect call can crash the program. Few things in life are absolutes; "types are better" isn't one of them.
That's also an option; as demonstrated in Kmett's Lib you link above; this is just my take on it and a post describing it to those who haven't seen it before. It does require several extensions; but I figure most people reading will be familiar with them and there's not much point in enumerating them.
I'm fond of Gibbons, Lester and Bird's _Enumerating the Rationals_ (https://www.cs.ox.ac.uk/jeremy.gibbons/publications/rationals.pdf) and Hinze on Hanoi (https://www.cs.ox.ac.uk/ralf.hinze/publications/ICFP09.pdf) Of course there's also Huet's introduction of the Zipper (https://www.st.cs.uni-saarland.de/edu/seminare/2005/advanced-fp/docs/huet-zipper.pdf) and the subsequent "Weaving a Web" by Hinze and Jeuring (http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.19.445)
I'm not saying you *should*, but you *can* patch some stuff together with a variety of not-entirely-well-loved language extensions, like this: {-# LANGUAGE ApplicativeDo #-} {-# LANGUAGE MonadComprehensions #-} {-# LANGUAGE RecordWildCards #-} data R = MkR{a :: Int,b :: Bool} -- Oh my monadR :: Monad m =&gt; m Int -&gt; m Bool -&gt; m R monadR get bM = [MkR {..} | x &lt;- bM , let b = x , a &lt;- if x then get else return 2 ] {- Can't do this; no ApplicativeComprehensions! appR :: Applicative i =&gt; i () -&gt; i Int -&gt; i R appR incr get = [ MkR{..} | b &lt;- True &lt;$ incr , a &lt;- get ] -} -- Why not? monadR' :: Monad m =&gt; m Int -&gt; m Bool -&gt; m R monadR' get bM = do x &lt;- bM let b = x a &lt;- if x then get else return 2 pure MkR {..} -- works for Applicative, too! appR' :: Applicative i =&gt; i () -&gt; i Int -&gt; i R appR' incr get = do b &lt;- True &lt;$ incr a &lt;- get pure MkR {..} 
[Richard Bird. 2006. FUNCTIONAL PEARL: A program to solve Sudoku. J. Funct. Program. 16, 6 (November 2006), 671-679. ](https://www.cs.tufts.edu/~nr/cs257/archive/richard-bird/sudoku.pdf) Derives an efficient solver for Sudoku puzzles uses equation rewriting. 
Thanks, yep worth having in mind. Would be nice if vector was more well known / accessible such that length was O(1) and guaranteed to terminate.
I’m learning Haskell, but I also tried learning C, Perl, Python, and Ruby. I never seem to be able to get past code that.feels like it’s grinding me down with obscure bugs and problems I have difficulty fixing. Is Haskell really that much easier to debug than other languages?
no, because of laziness (as I said), it's much harder to write working programs with `undefined` or `unsafePerformIO`.
Hello! I created this repo to teach myself some of the various ways to approach computational effects in Haskell. I wrote the same effectful program 4 times: 1) with a handcrafted monad, 2) with a monad transformer stack, 3) with a free monad, and 4) with the \`Eff\` monad from the \`extensible-effects\` package. Of course this is not remotely close to a complete survey, just a small demo for educational purposes. Pull requests welcome!
The incoming data non-definedness can/should be pushed to the verification and parsing layer.
Probably the issue is that in the dynamic language you don't really know immediately hand is the code you are looking at gets into the 70%, or into the 60%, or something. It all looks alike. Basically, yes, it may be very well said that you can "get away with garbage", because a missing dictionary value in JSON does not cause parsing failure, but turns into missing object field, and you may never access it in business logic, so you should not bother describing it as "Maybe", or specifying different cases, so that it is needed in some, and not needed in others
Mocking up something for a particular concrete free monad things tend to get messy when you go to get some sort of syntactic term. You can use data types a la carte, but then inference is a mess. Often I'll just dump out a local instance that collides with _everything_ like instance MonadWhatever (State MyTestState) or instance MonadWhatever (Free SomeBaseFunctor) and just write the test using that as a one shot monstrosity.
This is the second time I've seen an article / video from this guy where he essentially just repeats exactly Rich Hickey's silly arguments. 
The only good answer about mtl-style is don't use mtl-style, at least with default mlt – it's inherently [non-scalable (O&lt;sup&gt;2&lt;/sup&gt; instances)](http://hackage.haskell.org/package/layers-0.1/docs/Documentation-Layers-Overview.html#g:4). Instead of writing a transformer+instance for each class, write a single instance of each for your main monad only: newtype ProductionM a = ProductionM (ReaderT Conf IO a) instance MonadDB ProductionM where ... instance MonadInput ProductionM where ... Then, in each test you define a new Monad with completely new instances, this lets you effortlessly use mocks, etc: newtype TestM a = TestM (ReaderT Conf IO a) instance MonadDB TestM where ... This may seem like a lot of boilerplate, but with DerivingVia we can put all class implementations into separate transformers, reducing boilerplate for each newtype to just declaring instances – https://gist.github.com/lunaris/86440b552c7cc282a5cc37fb89845f70 import Data.Deriving.Via newtype MonadDBProdT m a = MonadDBProdT (m a) instance (MonadReader Conf m, MonadIO m) =&gt; MonadDB (MonadDBProdT m) where ... newtype ProductionM a = ProductionM (ReaderT Conf IO a) deriving newtype (MonadReader, MonadIO, Monad, Functor, Applicative) deriveVia [t| MonadDB ProductionM `Via` MonadDBProdT ProductionM |] We can ofcourse prepare our mocks too, reducing our test newtypes' definitions: instance (MonadState (Map Text Text) m) =&gt; MonadDB (MonadDBMockT m) where ... This scheme + derivingVia basically gives you full static dependency injection with less boilerplate and better performance and flexibility out of the box than mtl+transformers. 
I agree RecordWildCards most suits OP's situation among currently available extensions. I'm not a fan of MonadComprehensions, though. To add to that, this method gives you warnings for missing/duplicate/extraneous fields, if not perfect. * Duplicate/extraneous "field" is warned as unused binding, so it's not as pretty as native record syntax. * RecordWildCards uses every variable with the same name of the field in the scope. So you might not like the following case exists. do let x = False record1 &lt;- do { a &lt;- barM x; pure MkR{..} } -- This gets no error/warning record2 &lt;- do { x &lt;- fooM; a &lt;- barM x; pure MkR{..} } pure (record1, record2)
To derive evaluation order from the `Traversable` instance. Without that you'd have to define it for yourself. This can be helpful, but also a potential foot-gun.
When should I use ana- or apomorphism instead of direct recursion? Isn't this co-stuff unnecessary convoluted for the sake of having a duality? Let's stow away `hylo` and friends for now.
I'm not sure if this approach is flexible enough as it reuqires using the same transformer stack to mock the transformer stack itself. E.g. in your example newtype ProductionM a = ProductionM (ReaderT Conf IO a) newtype TestM a = TestM (ReaderT Conf IO a) if I have a function foo :: ProductionM Int I should be able to mock it foo :: TestM Int foo = return 42 and then test this function without having to provide a `Conf`.
I asked /u/edsko on Twitter if he thought about whether they thought about this: ``` {-# LANGUAGE ScopedTypeVariables, RankNTypes, DeriveFunctor #-} import Control.Lens import Control.Monad.Trans.State import Control.Monad.Trans.Writer import Data.Coerce (coerce) import Data.Char (toUpper) zoom :: forall s s' a. Lens' s s' -&gt; State s' a -&gt; State s a zoom l x = coerce (l (coerce x) :: s -&gt; (a, s)) zoomAll :: forall s s' a. Monoid a =&gt; Traversal' s s' -&gt; State s' a -&gt; State s a zoomAll l x = coerce (l (coerce x) :: s -&gt; (a, s)) -- &gt;&gt;&gt; runState example [] -- ("ffoooobbaarr","FOOBAR") example = do put "foobar" zoomAll traverse $ do c &lt;- get put (toUpper c) return [c, c] -- Can 'Zoomable' be made an empty class -- So it has just right constraits, to be able to 'coerce'? newtype UpdResult e a s = UpdResult { getUpdResult :: Either e (a, s) } deriving Functor zoomE :: forall s s' e a. Lens' s s' -&gt; StateT s' (Either e) a -&gt; StateT s (Either e) a zoomE l x = coerce (l (coerce x) :: s -&gt; UpdResult e a s) ```
I am just starting out in parallel/concurrent Haskell, so I wrote a code to calculate the number of primes &lt;= n, [here](https://gist.github.com/mbrc12/a5fabb3c6f14308a06307e6e46b82424). However, weirdly, it just stops in between. But when I add a print statement in the `forever` loop, it works fine. What exactly is going on here?
Thanks for sharing your experience &amp;#x200B; I'm curious about the first incident: &amp;#x200B; \&gt; 120 min &amp;#x200B; Is there a reason not to start bouncing the service the moment your healthchecks fail? There's plenty of options to do that, from Monit to Docker.
That would be one option or autoscale :) I think at this stage it's a bit too much automation before I understand enough how it behaves, so I'm focusing on adding instrumentaiton and monitoring+alerting before I automate in the wrong direction. &amp;#x200B;
Hi, I experimented with parser combinators recently and wanted to try the idea of using two different parsers to achieve a faster parsing in the "good case". I've seen this idea came up multiple times before (Attoparsec+Trifecta), but I didn't want to use multiple parser combinator libraries at the same time (complexity, dependencies, incompatibility issues, ...). &amp;#x200B; So far the experiment seems to work well, however I have issues with the GHC specialiser for more complex parsers. Basically I have to enforce specialisation manually via a preprocessor.
Who administers the website at haskell.org? The MacOS platform download link - at least the one used by Homewbrew's Cask - eventually leads to an illegal redirect: $ curl -sSI "https://www.haskell.org/platform/download/8.0.2/Haskell%20Platform%208.0.2%20Full%2064bit-signed.pkg" | egrep '^HTTP|Location' HTTP/1.1 302 Moved Temporarily Location: https://downloads.haskell.org/~platform/8.0.2/Haskell Platform 8.0.2 Full 64bit-signed.pkg Note the unencoded spaces in the value of the `Location:` HTTP response header; that's not valid according to the spec, and breaks automated tools trying to download the package.
&gt; Unlike Parsec and like Attoparsec, the parser combinators backtrack by default. To avoid bad error messages due to the backtracking the `commit :: MonadParser p =&gt; p a -&gt; p a` parser combinator is provided, which raises the priority of the errors within the given branch. One use for the `try` combinator is to do the opposite of `commit` and *lower* the priority of the errors in its argument. Have you considered this? Also, there's a good argument to be made that the `&lt;?&gt;` operator should also `commit` the branch it operates on - in which case you may not even need the latter. 
The deriving via stuff, as well as backpack, will have put to the field test, since they allow a lot of design decisions for the user, not all of which will make sense. But if it allows to do stuff you would normally `GHC.Generics` or even `TemplateHaskell` for, I'm all for it. Regarding the `(n^2)` problem, it's really `(n*k)`. While this may seem overly pedantic at first, this is where the difference whether the typeclass/transformer is written in an application or library context becomes important. Consider class Monad m =&gt; MonadDB m where query :: String -&gt; m String -- This method sucks default query :: (MonadTrans t, MonadDB m', m ~ t m') =&gt; String -&gt; m String query = lift . query Now I can choose to either provide a transformer, or some function I use as default implementation somewhere else. But of course there's the gazillion pass-through instances. instance MonadDB m =&gt; MonadDB (Identity m) instance MonadDB m =&gt; MonadDB (MaybeT m) instance MonadDB m =&gt; MonadDB (ExceptT m) instance MonadDB m =&gt; MonadDB (ReaderT r m) instance (Monoid w, MonadDB m) =&gt; MonadDB (WriterT w m) instance MonadDB m =&gt; MonadDB (StateT s m) instance MonadDB m =&gt; MonadDB (HttpT m) And so on and so forth. First of all, if your typeclass is "well-behaved" in that it can provide default pass-through instances this way, this boilerplate could likely be reduced using a template haskell splice or something. I don't know of any package that provides this, but I haven't looked very hard either. On the other hand - and here I am arguing from the application development perspective again - I would argue that not all of these instances are terribly useful. How often do you use `IdentityT`? I don't use `StateT` when I am forced to be "mtl-ly" anyways, but would just introduce `PrimMonad`, or use `IORefs` or whatever directly, if for some reason the `MonadIO` constraint was already there. Likewise, I am unlikely to have a top-level transformer involving `MaybeT` or `ExceptT`. I might slap these on top locally, to simplify code that involves a lot of `case` expressions, but even then if the pass-through instance is not there, I would just need one `lift`. My point being, in `(n*k)` you can choose `k` to be small, if you want. Similar arguments apply to the `n`. You don't *have* to provide a transformer if you want to defer the instance declaration to the top-level monad. The `Control.Monad.Db.Class` module or whatever may just provide the implementations via some functions, or a record of functions or whatever. There's quite some room to be creative I think. Still, you are absolutely right in that it is not exactly ideal.
Maybe I could provide `try` additionally to `commit`, which as you suggest lowers the priority. I thought about using `&lt;?&gt;` for committing but wasn't really sure about it. Right now I am using `&lt;?&gt;` to collect labels. If the `hidden` combinator is used, the latest label is used as `EExpected` error, otherwise the labels are shown as context to the error message. But I still have to work on the error messages and test different cases with realistic parsers. For now I was more focused on the fast path `Acceptor`.
On small addition to the specialiser issue - I also tested the [parsers-bench](https://github.com/minad/parsers-bench) benchmarks with my ghc-specialise-parser hack, but it doesn't make a difference. But for larger parsers I observed a factor of 3 speedup when ghc-specialise-parser is used. Instead of the preprocessor I also tried `-fspecialise-aggressively` without effect. Is there something preventing GHC from specialising the parsers (size limit?) if the situation is as follows: All `Parser` are defined in one file, they are not exported and they are only used as `Acceptor` and `Reporter`. `Acceptor` and `Reporter` are both newtypes around Reader/State monads written in CPS.
You should provide good reproduction instructions and make a ticket.
https://www.schoolofhaskell.com/user/edwardk/heap-of-successes is an older article I'd written up on update monads. Technically, what you want for an update monad isn't _quite_ a monoid and monoid action. You need it to be a semigroup, yes. But the way of building the mempty equivalent `mempty` for `m` could be allowed to depend on `s`. This can let you get away with a smaller update vocabulary as you don't have to just have a neutral `mempty` lying around to make `pure`. This doesn't show up in this write-up on update monads by Ahman and Uustalu, but it shows up in another short write-up on them, of which I've apparently since lost track.
Free monads just give you a tool for throwing around raw syntax with no real semantics assigned yet. Update monads are all about semantics, as you can make things that are better behaved than state monads. The example I like to use is this. Commonly, in an application you might have something like `StateT (Map Handle Handler) IO` or something when dealing with lots of live file handles. But now you have to worry about someone grabbing the state of currently active file handles, then calling some of your application guts that closes one, and then restoring the set of file handles getting your application into a bad state. With an update monad that uses a monoid containing something like `Map Handle (Maybe Handler)`, where Nothing 'updates' the original map by deleting the key, and a Just inserts or replaces the handler, this is somewhat less of a problem. Going further where you have monadic updates that run in IO, would let you establish a base vocabulary for creating and deleting files, and not allow users to grab your whole state and pretend it still applies to the world later on.
You don't tend to build up lots of layers with the MTL that grow uncontrollably. Build one 'reader' level and throw all your environment stuff into it, one 'state' level and throw the stuff you plan to change, etc. Then uses lenses or whatever to help manage the fact that you are touching parts of a larger whole. https://www.youtube.com/watch?v=GZPup5Iuaqw
I think that a lot of Haskell companies or Haskell projects within existing organizations are somewhat new (which is to be expected of a quickly growing ecosystem). Newer companies and projects tend to be fighting uphill battles, especially Haskell projects in an organization with Haskell skeptics. My small sample size of n=3 says those skeptics are always present, and are probably someone's boss :) . Given these factors, Haskell projects are incentivized to find more senior developers who can quickly show good progress and hence lend legitimacy to the product/organization. "Accommodating noobs" is a bigger investment in human capital. In the case of Haskell that investment is likely to pay off, but some stability is required to make it. &amp;#x200B; An additional factor is that those starting off with the language usually aren't afforded the opportunity to use it at work. There's a difference between one year's worth of working in language X for 8+ hours a day with business goals on the line, and one year's worth of working in language X for a couple hours each day on personal projects when you can spare the time. In addition to this, the neat Haskell features often portrayed in blog posts and community talks often omit things you need to create a real product. I spend a lot more time writing FFI code and profiling than I spend writing zygomorphisms. This area is improving quickly though; I think increased adoption of Nix in the community is a good example. &amp;#x200B; You might consider seeking out organizations with bigger teams, as they're more likely to have the resources to invest in newer developers. I know that Formation ([https://formation.ai/](https://formation.ai/)) has a pretty large Haskell team. It sounds like you're just looking for remote work, but if there's a chance you're willing to relocate to beautiful Southern California my team at Anduril ([https://www.anduril.com/careers](https://www.anduril.com/careers)) is hiring Haskellers of all levels.
Nice! That is so mich better :) Just some questions: How sophisticated is your diffing algorithm? Libraries like reactDom or elm's virtual dom use "key" properties to make the diffing faster. Do you have something like that? Also, why do you need do notation/a monad for your ListBox's children? Why not simply a list?
Oh, yeah, I somehow forgot that you need an interpreter for bind. Maybe because I ran additional interpretations besides which probably is a messy abuse of the abstraction. I think your example is called PatchMap in reflex? Iirc there were also variants for gadt key's and ones that support swapping values.
&gt;I don't really follow where you see free monads as wrong though. I thought that in order to cope with large monad transformer stacks one could instead define the "most general" monad. From what I've read, free monad seems to me sort of a fixed point of a monad stack. This is still mostly based on intuition, so I might b totally wrong... Thanks for the link!
You're right, but I have to ask if you have a solution to the case where one consume a library and its functions are already in a monad stack. Then, you want to wrap it in logging monad and maybe one or two other contexts. Is there an elegant solution for that? I'm not a seasoned Haskeller and may very well be unaware of the idiomatic approach here. Any insight welcome.
 appR incr get = flip MkR &lt;$&gt; (True &lt;$ incr) &lt;*&gt; get If you actually want to use record syntax (Ugh!), you'll need to use TH or something similar to make a functor-parameterized version of MkR. monadR get bM = bM &gt;&gt;= (fmap . flip MkR) &lt;*&gt; (\x -&gt; if x then get else pure 2) Again, if you really need the horrific record syntax, I suggest TH to generate the functor-parameterized version.
Monads are about 'substitution' (`fmap`) followed by a renormalization step (`join`). (In Haskell we fuse these together into one operation `(&gt;&gt;=)`, which does both.) Free monads are monads such that the `join` step is trivial. You've had no semantics assigned to your monad that lets it collapse steps together. http://www.haskellforall.com/2012/06/you-could-have-invented-free-monads.html might be a decent intro to the topic.
The idiomatic approach is to write the library against the mtl classes rather than against a concrete monad transformer stack. Rather than writing a function that lives in `State`, write it to work in an abstract `MonadState`. Some times you'll need to write some manual "swizzling" code if you work against a library that internally didn't obey this discipline.
&gt; How sophisticated is your diffing algorithm? Not very. It's not using anything like keys, so any reordering of widgets would cause a lot of replacements. That would be a thing to work on. &gt; Also, why do you need do notation/a monad for your ListBox's children? Why not simply a list? It isn't really needed, but it's nice it you want to combine various `mapM_` operations, like: ``` myList = container ListBox [] $ do mapM_ foo [1, 2, 3] mapM_ bar [4, 5, 6] ``` The library should be able to support both through it's typeclass instances, but I haven't worked on that yet.
So free monads are just more specific types of monads and cannot be used in any way as a replacement for monad transformers, right?
Thanks that's really informative and a really nice insight. I was extremely lucky to have been able to make the move from JS to PureScript when I was employed by SlamData for a year then 6 months doing Haskell full time. Like you say full time exposure 8hr a day working on code is the best exposure. I definitely didn't write any zygomorphisms during that time!! haha Guess at the moment the foundation is still being laid as the ecosystem is growing and maybe Haskell is still at the point where it needs to prove it self so less experience developers aren't going to help with giving business results unless the company is already sold/settled on using Haskell. ps: would love to relocate but I'm just not in that position, now stuck between taking the risk to financial sustain myself with my savings whilst I learn more Haskell full time or go back to doing JS. I've just caught the FP bug and finding it hard to leave it behind!
Is `(MonadReader e m, HasMyStuff e)` a feature of TypeFamilies?
Yes, I'm familiar with the technique. In this case, would you still call it memoization? Here we are not remembering the results previous calculations, we're keeping track of previously seen nodes in a graph we are \_currently traversing\_ (roughly speaking) to avoid cycles, but we haven't yet decided the answer for those previously seen nodes, right?
Yes, I will do that if I figure out that there really is an issue which needs fixing. I mean it works as desired if I use SPECIALISE pragmas. It also works if I redefine `MonadParser p = p ~ Acceptor`. Backpack would also be a solution, by parametrizing the parser with the parser instance. I don't want to spam the GHC issue tracker...
I don't yet understand why its a monad... Wouldn't this API be possible: myList = container ListBox [] $ map foo [1,2,3] ++ map bar [4,5,6] I'm assuming there is something in there that needs it to be a monad, however I can't think of why.
Personally I believe a big part of being a senior developer is having junior developers to mentor and lead. Once you have spent time teaching and shaping inexperienced developers into solid contributors you're on your way to being a senior developer. And I just don't see the economy of Haskell jobs out there to support those kinds of relationships. Most small to medium companies that adopt Haskell do so with tepid reservation. They've usually been convinced by an influential developer on their team whom they want to keep happy. And so there's a lot riding on the success of those projects and Haskell is not a fully-adopted technology yet. Most of the other developers in that organization would rather eat sawdust than be told to adopt a new language like Haskell. Management doesn't want to waste precious money on an untested ecosystem that would require expensive retraining and adoption (and they'll jump ship at the first sign of trouble). So it ends up being the responsibility of that sole developer and the few that support them to make the project successful. If they even have the budget to hire a Haskell developer they'll usually want to hire someone with real-world, production Haskell deployments... because if that project fails, like so many will want it to, then that will likely cut off any future adoption of Haskell. It's kind of a vicious circle. I think the real way to get in as a "Junior" Haskell developer is to play the long game. Work in some other primary language. Write useful utilities, scripts, and tooling in Haskell. Have lunch with like-minded colleagues and teach them Haskell. Support projects that decouple services and allow you to integrate Haskell in a low-risk, low touch way... and eventually build a toe hold. Maybe in 2-3 years you'll be writing Haskell at your job at least part of the time.
I'm not using any type families there. MonadReader is a "multi parameter type class" but it was already there. `HasMyStuff` is just that little class I sketched below. There `m` determines some type `e` that I want to have a lens into to get out `MyStuff`. I'll typically use this when writing larger applications. Let's see an example! https://www.youtube.com/watch?v=YjjTPV2pXY0 defines how I handle input events from SDL2 (a graphics / user interaction management library). Then I just build up more and more constraints: https://github.com/ekmett/quine/blob/master/Toy.hs#L154 To run things, I bolt that `Input` type and whatever else I need into a larger `System` type that contains all the extra information I have about the display, camera, current simulation, whatever. https://github.com/ekmett/quine/blob/master/src/Quine/System.hs#L25 Then discharge all of those constraints in one go at the top level: https://github.com/ekmett/quine/blob/master/Toy.hs#L145
I don't know what's technically correct, but to me it looks like memoization. Consider replacing `Set (Tree, Tree)` with `Map (Tree, Tree) Bool` and doing insert True instead. Then the Map holds the result of our previous calculation, and if we find it we return early. But if our calculation becomes False, instead of storing it in the Map we "optimize" and terminate the computation as that's the final result.
assuming `foo` and `bar` are effectful (as many lower level Graphics functions are), the list version requires `traverse`s, which can be awkward. however, if we just building up a pure state, like VirtualDOM, I'd never use `do`-notation with partial instances (no `&gt;&gt;=`), it's misleading and less legible. /u/owickstrom why not provide `IsList` and `Monoid` instances (unless you already have), and document a pattern like: mconcat [ foo &lt;$&gt; [1,2,3] ... ] by default. Then, export an unlawful and orphan instance from a `.Sugar` module (for those who prefer it)? 
Looks like "the elm architecture" really nice ʕ•ᴥ•ʔ
I'm still getting easily confused with non-trival type signatures that have multiple type constraints. Sorry. In `core :: (MonadIO m, MonadState s m, HasSystem s (), MonadReader e m, HasEnv e, HasOptions e) =&gt; m a`, I take that all the instantiated but unused(?) constraints are there so that `core` fits into an interface. Is that correct?
I think you can even define lenses in terms of `zoom`: type Lens' s a = forall x. State a x -&gt; State s x For polymorphic lenses, you need indexed `State`: type Lens s t a b = forall x. IxState a b x -&gt; IxState s t x
&gt; should be noted my net was reduced with looking for remote positions only Also note that companies offering remote positions have the luxury of being pickier in whom they accept, since they are drawing from a much larger talent pool than companies offering local positions only. I would imagine that more junior Haskell positions are available as non-remote. &gt; Could it be detrimental to Haskell's industry growth, because right now it "feels" like you need a lot of determination &amp; spare time to get to a good enough level (under your own steam). To then being able of having a real chance at landing senior rolls. I think this is a very good point. I was lucky that I started to "train up" in Haskell while I was still in school. It's not as easy to do so while also holding down a full time non-Haskell job.
Of course there's always the off-chance of joining a larger team as others have suggested: Facebook and the like. There's more than one way to skin a cat but the point is you'll have to get creative if your goal is Haskell. (Though you should really consider the problems you'll be working on too... a programming language is a means to an end after all)
I would like to see this with skew binomial heaps.
you're right about remote, the market is fierce. I personally came to doing it because that was the only way to do what I loved and don't have the option of relocating and no longer being a whippersnapper with a free soul hehe. I'm just thinking that the barrier into learning Haskell is already fairly high, to then go through that shows dedication in my eyes. But right now you're met with well you need more than just that. Which is totally understandable as mentioned before due to the size of the ecosystem and the industry adoption number. But this becomes a bit of a catch 22 then. Everyone needs to be of a high level which is bound to be even more of a deterrent into the language. 
I'm not sure, because I have no experience with `unsafeInterleaveST`, but I think you are using it wrong. The documentation says: &gt; unsafeInterleaveST allows an ST computation to be deferred lazily. When passed a value of type ST a, the ST computation will only be performed when the value of the a is demanded. You are using it two times and you ignore the resulting `()` in your `do` notation. So it may not do anything actually. I'm pretty sure the garbage you are observing is just the pivot shuffling. To do that in `ST`, I'll use `forkIO` and `unsafeIOToST`, something such as (not tested, I don't have any idea of what I'm talking about), using `async` because that's great: ``` threadIdA &lt;- unsafeIOToST (async quickSortOnFirstSubArray) quickSortOnSecondSubArray unsafeIOToST (wait threadIdA) ``` Main thread will only do the right computation and the forked thread will do the left computation. Tell me if that solves you issue.
&gt; In the Haskell lens library, optics are written in a form known as the profunctor encoding, which ... Mistakes in the introduction spoil the fun :(
and harder to write "garbage" programs (which is what we are talking about, if you read the parent thread), where values being printed out or looked up is interleaved deep within pure logic. that's a good thing, but I don't think you know what I'm talking about.
I really like "Power series, power serious" by Doug McIlroy.
They clarify that this is not quite true later in the paper (Section 5), but it is close enough for a math paper’s intro. (Wouldn’t’ve minded a footnote, though.)
The catch-22 doesn't go away, even in a popular language. I remember going to a Ruby conference a few years back where one of the speakers got up and said "we need to start hiring and training juniors. People are flinching away from using Ruby in projects, because demand has made the limited supply of developers too expensive."
Exactly! Update monads could almost be called the "unfree" monad 🤣 because they strongly limit how you can interact with the internals. Unlike State you have the ability to limit the types of actions you can take and you have control over the function which actually performs edits to your state.
We're not actively hiring right now, but I have taken on a few junior devs with some (non-FP) programming experience and trained them up on Haskell with great results. It's worth pointing out that our HQ is in Alaska and our CEO has a strong stance against remote, which means hiring talented people is much harder than elsewhere and we sometimes have to make do with what we can get rather than pick the cream of the crop. Hiring someone with concrete Haskell experience is more like gravy on the top for a company like ours. I think it's more important to have someone that understands how to do the rest of the job, like interacting with people (devs, customers), using basic tools like git, writing solid documentation, and working on a team, than it is to have someone that only shows aptitude at using a specific programming language tool. The whole package matters. As an industry, I suppose we just need more companies to grow out and adopt Haskell before you start seeing more junior level positions. Most companies using the tech seem to know exactly what they want, and those desires lead to focused hiring of senior developers right now.
I narrowed down your problem to the following code: main :: IO () main = do x &lt;- newMVar False putStrLn "Forking" forkIO $ do putStrLn "OK Forked" threadDelay 10000 putStrLn "Forked thread done" _ &lt;- takeMVar x putMVar x True forever $ do done &lt;- takeMVar x if done then die "Finish" else putMVar x False This program hangs when I compiled it with `-O -threaded`, but do not hang with `-threaded`. There's a problem in GHC if I wasn't misinterpreting `Control.Concurrent.MVar` documentation. Aside from that, I want to notice you two problems in your program. (1) Your program has a race condition. `doneVar` can be `True` before each worker thread completes their work. (To check it, put `threadDelay` just before `forM_ ...`). (2) To wait for something completes, you can simply wait on empty `MVar`. do lock &lt;- newEmptyMVar putStrLn "Let's go!" forkIO $ do putStrLn "Wait me!" threadDelay 1000000 putStrLn "OK I'm ready." putMVar lock () _ &lt;- takeMVar lock putStrLn "Yeah." You can use `Control.Concurrent.Async` from "async" package to solve all problems! Spawn and wait the workers like this: -- You no longer needs cidxVar and doneVar worker :: Int -&gt; V.Vector (MVar Bool) -&gt; IO () forConcurrently_ [2..n-1] $ \idx -&gt; worker idx cvecVar
Well, it's a lot harder to write something that crashes. But, it's a little bit more difficult to debug if it runs to completion but gives the wrong answer.
Hah, that's a great idea.
Oh, that technique to block is nice. I realized that my code has that condition, but I was not sure why that would make things hang in between. I mean, it is possible to call `finish` before the task ends, but that probably cannot cause a hang, because either the worker blocks the Mvar in the vector, which it releases after a while or finish blocks it, which it releases immediately. But if course, I am not exactly sure of all this. Thanks for the help!
can you elaborate?
Later in the paper: &gt; In the Haskell lens library, optics are written in a form known as the profunctor encoding, which at first glance is completely different to that given in Section 2. 
I'm seeing a lot of misconceptions going around about the granular typeclass approach; I [wrote a little about it already](https://chrispenner.ca/posts/monadio-considered-harmful); but I don't talk much in the article about defining custom transformers. The reason is that I DON'T define custom transformers. I implement my main type classes against my AppM and again against my TestM and that's it. Both AppM and TestM will probably use some monad transformers "under the hood" to implement my custom typeclasses, but there's really only exactly 2 instances defined on concrete monads with no possibility for overlap or need for orphan instances. If you want more than one possible implementation in your tests you can easily parameterize your TestM with phantom data kinds (it's easier than it sounds; I'll write about it later) and provide an implementation for each of several options. This way you can even mix and match different Data Kind params to pick combinations of different test implementations of each typeclass without sacrificing anything at all. I haven't run into problems with this approach yet, but haven't used it in large projects yet either, let me know if you see any issues; I'll likely formalize it into a post eventually 🤷‍♂️
 &gt; if you read the parent thread I did and laziness doesn't come in until you made the weird comment that I replied to &gt; but I don't think you know what I'm talking about You're right, I don't know what you're talking about. Can you elaborate please? 
&gt; The same is not true for other optic variants, and indeed in the Haskell lens library, Prisms and Reviews use the profunctor encoding. It's not correct either. `lens` doesn't use that. I.e. optics in `lens` aren't: ``` type OpticPro s t a b = forall p. SomeProfunctor p =&gt; p a b -&gt; p c d ``` --- van Laarhoven encoding is tricky! https://www.twanvl.nl/blog/haskell/cps-functional-references We often think it as simply ``` type OpticVL s t a b = forall f. SomeFunctor f =&gt; (a -&gt; f b) -&gt; s -&gt; f t ``` But that's not good enough for e.g. Prisms (as mentioned in the paper). There fore it needs to be extended. We can do it in *two* ways! ``` type OpticVL1 s t a b = forall p f. (SomeProfunctor p, SomeFunctor f) =&gt; p a (f b) -&gt; p s (f t) type OpticVL2 s t a b = forall p f. (SomeFunctor g, SomeFunctor f) =&gt; (g a -&gt; f b) -&gt; g s -&gt; f t ``` `lens` uses `OpticVL1`, where `p` is often `(-&gt;)`. It unifies nicely. But e.g. for *indexed optics*, (which are some what ad-hoc construction - there are now laws for indices), also `IndexedLens`, have `p = Indexed`. `OpticVL2` "works: too, but that won't fit `traverse` etc. See e.g. https://r6research.livejournal.com/28432.html for `OpticVL2` Iso. --- There's also fifth? encoding (not counting a "concrete" one). ``` type OpticF f s a = f a -&gt; f s ``` https://twitter.com/kmett/status/944727625121124352 &gt; Fun fact: We figured out prisms, etc. for this form of optic before we figured out the profunctor versions. --- Btw, affine traversals are known as partial lens way before my blog post: http://hackage.haskell.org/package/data-lens-2.10.4/docs/Data-Lens-Partial-Common.html From "A Representation Theorem for Second-Order Functionals" (which is referred from the draft) &gt; An affine traversal from A to B is so called because it specifies an isomorphism between A and F B for some affine container F, i.e. for some functor F where F X ∼= C1 × X + C2 &gt; We also thank Shachaf Ben-Kiki for explaining why affine traversals are called so I guess I have to edit my blog post, to have these references... --- TL;DR if someone wants to p
Also, there's too much Haskell (code) to call that a *math paper* ;)
Maybe ghc could support a syntax for generating monoid-like things using something similar to "do", something along the lines of this: listOfThings = listOf something map foo someList [1,2,3] (Where "listOf" is a keyword) So that there is still the benefit of only needing only a newline to separate list items. I've seen "do" being used as simpler syntax for concatinating lists multiple times.
Cryptic errors aside, I would argue that lens fits well with the Applicative-Traversable-Foldable idioms prevalent in modern Haskell, for example in trying to keep the nomenclature consistent (`forOf`, `traverseOf`...)
It's certainly very informative to learn how the library works (there are several talks/posts about that), if you're ever interested in that. You can certainly survive without it. Many industrial codebases don't use lenses, many do. I would not give too much weight to recommendations and just go with whatever I feel more comfortable with at a given time, after looking at your options.
&gt; it uses language features that are outside of native Haskell. Aside from `RankNTypes`, which language features does it use? While `lens` as a library certainly uses a lot more, I think that the piece that 99% of the people use doesn't. Oh, and I guess `TemplateHaskell` to generate the actual lenses. You can do it by hand if you want.
It's worth learning. The good news is that a little goes a long way. The types you need to care about are these: * `Lens' s a`: given a type `s` that always has an `a` in it, a `Lens' s a` is a way of getting and setting that `a` inside of `s` * `Prism' s a`: given a type `s` that might have an `a` in it, a `Prism' s a` is a way of extracting the `a` if it exists, and being able to *create* an `s` given an `a` * `Traversal' s a`: target many `a`s which may or may not exist inside of an `s` * `Iso' s a` : says that `s` and `a` are different representations of the same type All `Lens`es are `Traversals` and all `Prism`s are also `Traversal`s. Not all `Traversal`s are `Lens`es or `Prism`s though. The canonical operations for these things are as follows: * `Lens`: * `view` (infix `^.`) to get a value view _1 ("hello", "world") = ("hello", "world") ^. _1 = "hello" * `set` `(.~)` to set the value in a bigger structure. When done in infix form, use `(&amp;)` to say what you're updating. set _1 5 (1, 2) = (1, 2) &amp; _1 .~ 5 = (5, 2) * the "usual" infix operators: `(+~)`, `(-~)`, `(*~)`, `(&lt;&gt;~)` etc -- add/subtract/multiply/mappend a value to the existing one (True, 256) &amp; _2 *~ 256 = (True, 65536) * `over` `(%~)` lets you run a function to change the value instead. the mnemonic for this is that `%` is the mod operator, and `%~% *mod*ifies things. over _1 show (17, 2) = (17, 2) &amp; _1 %~ show = ("17", 2) * `Prism`: * `preview` `(^?)`: maybe get a value out of a structure. mnemonic: previewing is getting a sneak peek Right 5 ^? _Left = Nothing preview _Left $ Left 5 = Just 5 * `review` `(#)`: construct an `s` out of an `a`. review _Left "hello" = Left "hello" _Just # True = Just True * `Traversal`: * `traverse`: exactly the function you know and love. more or less generalizes `over` [1,2,3] &amp; traverse %~ show = ["1","2","3"] * `toListOf` `(^..)`: get a list of everything the traversal targets: (1, 2, 3) ^.. each = [1,2,3] -- tuple to list!!! * `Iso`: use `view`, like a lens to go from `s` to `a`, and `view . from` to go from `a` to `s` Lenses, prisms, traversals and isos compose left-to-right with function composition `(.)`. (1, 2, (3, 4, 5)) ^. _3 . _3 = 5 Right (Left 17) ^? _Right . _Left = Just 17 [(1, 2), (10, 20), (100, 200)] &amp; traverse . _2 %~ show = [(1,"2"),(10,"20"),(100,"200")] --- Lenses turn out to be way more interesting than just getters and setters, but knowing this much will get you through 95% of the lens code you see in the wild. For more, go watch /u/jwiegley's fantastic talk: [Putting Lenses to Work](https://www.youtube.com/watch?v=QZy4Yml3LTY)
I usually just use a single state monad for all reader&amp;writer&amp;state needs. Possibly with exceptions if they are needed. 
I think if someone were a noob it would make more sense for them to be on-site to learn as fast as possible, so maybe it's hard to find a remote position? &amp;#x200B;
Totally for someone starting out it makes sense, think to go remote you have to be prepared to try harder at keeping communications working and not need hand holding too much. Which doesn't bare well for new comers. But saying that when I worked at SD I have about 6 months spare time experience with PureScript (so really not a lot) but think what I might have had over other devs who applied is showing that I'm prepared to work my ass off to learn (I think!). That can't be said for all candidates so can see why again it's a risk. :)
Currently I guess something like JavaScript is popular due to the frontend! so if all the Haskellers took on PureScript to popularise it in the FE world. Then the only logical solution would be to have Haskell back ends!! (this is a terrible plan). But you're right it's very much going to be easier to train people up if they are in house and like you said the companies actively using the tech right now understand exactly what they need and want to fulfil that vs risk taking with less able developers.
For learning purposes you can use `silica` library: it provides excellent error messages for wrong lens usages! * https://github.com/mrkgnao/silica
I sometimes write lenses directly for my records, at the cost of bolierplate. The "generic-lens" package is also useful.
As far as I'm aware, vanilla record syntax still has the highest power-to-weight ratio of all the ways to construct values by specifying labeled fields. I'm considering precise typing, error message quality, perspiciousness of code, in-memory representation efficiency, and so on. (Combinator soup is hard to maintain/share.) I'm hoping you can change my conclusion! Thanks.
Yeah, these about race condition etc. were side notes, were not an answer to your question. I'm sorry about that. Plus, after I wrote above reply, I realized it needs to be carefully checked before yelling "YAY GHC BUG". So I re-ran above code. Running it prints out this and hangs. $ ./mvar +RTS -N2 Forking The forked thread is not even starting. This means something strange happening in thread scheduling, which is way above my knowledge limit. So I changed `forkIO` to `forkOS` to ensure forked thread does run. That displays below and hangs. $ ./mvar +RTS -N2 Forking OK Forked The problem is not only the forked thread was not starting, also, the forked thread is not recovering from `threadDelay` after started. I can't answer why but seems bug to me. I also checked if this happens when using `TVar` and `IORef`. These have no problem.
I recently read [https://leanpub.com/lenses](https://leanpub.com/lenses) and did all the exercises. It's written in PureScript and I hope it's not very different from the Haskell implementation. I enjoyed working through that.
Wow, all this is pretty high above my level. :( Is there some library/technique to do these things in a way that works as I expect? Then I could probably try learning that, because I don't have much knowledge about how threading etc. happens on the OS level (although maybe I should work to know more).
&gt; My point being, in (n*k) you can choose k to be small, if you want. The problem is that, as long as you keep defining transformers, your `k` grows out of control quickly and adding new classes becomes impossible. If you never define transformers, as above, this doesn't happen. &gt; The Control.Monad.Db.Class module or whatever may just provide the implementations via some functions, or a record of functions or similar. There's quite some room to be creative I think. Sure, but that's a different approach entirely, the problem lies only with the transformer-per-each-implementation-of-class style – I don't know why this advice still persists, I seriously think it's a devious noobtrap that relies on the fact that no one writes big programs in Haskell to not be discovered. Every company I've interviewed had tried this style at some point and abandoned it for records of functions, [records of interpreters](http://www.parsonsmatt.org/2018/03/22/three_layer_haskell_cake.html), eff, deriveVia, monomorphic ReaderT-IO-only, anything but naive mtl.
There is also GTK3 for FrpNow: https://github.com/george-steel/frpnow-gtk3
I think a major reason is that monads are the way in which Haskell wraps up side-effecting operations. So in your example do a; b; c, it is much more natural to think that a happens before b which happens before c in sequence. If it were infixr, side effects from b and c would be observed before a which would be confusing. Can you elaborate on your runtime concerns? Infixl and infixr don’t imply any runtime cost, though the underlying monad instance may prefer one over the other. And if you don’t have ordering requirements you should probably be using applicative. Recent versions of GHC will actually try to identify places in which you could have used applicative inside of the do syntax with the ApplicativeDo extension.
To match the infix of (&gt;&gt;=), and (&gt;&gt;=) is infixl so you can write m &gt;&gt;= f &gt;&gt;= g which, through monad laws, is equivalent to m &gt;&gt;= \a -&gt; f a &gt;&gt;= \b -&gt; g b 
Doesn't the monad laws require `a &gt;&gt; (b &gt;&gt; c) == (a &gt;&gt; b) &gt;&gt; c`?
Yes, that's a consequence of it.
Deleted my comment to avoid spreading misinformation!
As I said in my OP, I am not talking about semantics, I am talking about runtime performance characteristics. Sometimes when implementing a monad, it is not possible to optimize it for both `a &gt;&gt; (b &gt;&gt; c)` and `(a &gt;&gt; b) &gt;&gt; c`. So you have to make the choice which of the two to optimize for.
Is is wothless snowflaking. Don't use that shit
Thank you for your reply. You and others responses are convincing me to jump in and learn how to use Lenses.
Do you use flycheck? Intero? hie? I had to disable flycheck as it was very slow for me (it calls stack path --project-root which runs very slowly for me, ~5sec)
When using unsafe operations you lose functional purity. You are sharing state, the antithesis of the functional paradigm. Programming like this is not much different than multi-threading in C; and just like in C you have to manually deal with concurrency issues. (Fun! 😅) In this case you need to force each sub-thread to complete before returning. I'm not sure of the "right" way to do this. Most functional purists would say there is no right way, but I think something like u/guibou's answer is your best bet.
I think this is right, but there might also be a issue with laziness. The threads may "finish" without actually doing all the steps. `unsafeInterleaveST` only cares about the return value and that value is `()`. It could skip to the end since the value of `()` does not depend on the state of the array. I had a problem like this with `IO`. Haskell's `IO` is unsafe interleaved by default and this reorders the execution of some actions. You should also use the strict version of `ST` just to be sure.
Take my upvotes! I've wanted this for a long time! Do you figure it's production ready?
&gt; the problem lies only with the transformer-per-each-implementation-of-class style – I don't know why this advice still persists It's not something I read in a beginner monad tutorial - I decided to try it after searching related reddit posts as well. For instance, in [this](https://www.reddit.com/r/haskell/comments/89qpdz/monad_transformer_stacks_vs_readert_pattern/dwukp2j/) thread, u/ElvishJerricco seems to recommend it. Or rather, at least to "mtl-style" this way, if you choose to do mtl-style anyway. Regarding *records of interpreters*, I don't see how exactly this follows from the three-layer cake post. Did you intend to refer to the RankNClassy post instead?
I use emacs with dante or intero. My setup: http://haroldcarr.com/posts/2017-10-24-emacs-haskell-dev-env.html
Yes but the std list sort is incredibly slow, like 10-20 times slower than my basic mutable quicksort. And you don't have to deal with concurrency issues when you operate on disjoint data. All you have to do in C is: if n&gt;treshold fork(f); join(); else f()
I've been using lens at work extensively and I can confidently say it is *absolutely* worth it. Normally I am skeptical about thing with complex implementations, non-idiomatic patterns and tricky error messages—these are all real costs. But lenses make working with larger Haskell projects *so much nicer* that it pays for itself many times over. It took me a little while to figure out why lens works so well despite seeming so weird *a priori*. I have a basic theory: * The lens library is coherently design and completely internally consistent—learn the core ideas and it all makes sense. * Lens neatly fills a *massive* hole in Haskell's ecosystem. Working with complex, nested, structured data is surprisingly awkward in "normal" Haskell; doing the same with lens leads to a better experience than I've had in any other language. * You only need to learn a small set of operators and abstractions. That's a much smaller cost to pay than it looks on the surface, and if you use lens consistently through a codebase, people will pick up the core naturally as they join the project. Despite being a heavy library, lens has a higher power-to-weight ratio than most other libraries I use. It's pretty incredible. One lesson I picked up is that lens gets more powerful and useful if you use it consistently. If you're using lens in a project, make sure that *every* record has a lens. Records get a lot nicer if you never have to reach for normal field accessors and can just use lenses without thinking. A package like [generic-lens](http://hackage.haskell.org/package/generic-lens) or [overloaded-records](http://hackage.haskell.org/package/overloaded-records) goes a long way in this regard.
Lenses are legitimately difficult and they increase the "required learning" for a project significantly. This can be a pretty big cost, especially if you're using `lens` beyond the simple `view` `over` and `set` functions. Lenses solve the problems that Haskell's "records" present very well. For this, they are worth learning. However, I'd suggest that if you're over-using nested Haskell records, then you're making an architectural/structural mistake somewhere else. I've been a fan lately of applying normalization (eg from relational algebra/SQL databases) to Haskell datatypes. Factoring out common structures into their own records and replacing nesting with references helps tremendously to *avoid* the problem lenses solve by not deeply nesting things in the first place. And you get all the benefits of a normalized SQL database, too. It is also somewhat straightforward to write `lens` code and hide it behind simple code -- `foo :: Lens s t a b` becomes easier to understand when you have `setFoo = set foo`, `getFoo = view foo`, `modifyFoo = over foo`. The scary type errors go away, at the cost of easy composition and boilerplate. But this gives you a way to write the lens code you need without exposing too much `lens` to the non-lens friendly folks in your team. 
ST is not the same type as STRef. You need a readSTRef that take an `STRef s a` and gives you an action `ST s a` which you can then run.
I feel that this isn't a full answer. A `STRef` (or `IORef` or `MVar` or whatever) isn't a container for a value. It's much more alike to a memory address. We can imagine the `ST` monad to be analog to a `Map Int Something`. Then an `STRef` is an `Int`. It should be pretty obvious that it doesn't make to ask for a function `Int -&gt; Something` because where would that function get the `Map` from? It only has a value inside of some context. Actually, even with a context, a `STRef` is much better described as a way to get a specific value from `ST`, rather than `ST` being used to get a value of a `STRef`.
In-memory representation efficiency is unchanged between record syntax and plain constructors. I disagree that record syntax is perspicacious. Try explaining the spaghetti that is an update of single field in a record in a record in a record. I'm not sure what advantages you see for records in "precise typing", or "error message quality". Perhaps you could be more specific? --- I'm all for specifying my data types as records -- I'll either use the field names as accessors or generate lenses. But, for most forms of update, and even creating values, record syntax is abysmal. It's basically the first thing that "works", not a refined solution. --- I think that the functor-parameterized version gets another layer of indirection and that it might be good to avoid that where it's not needed. If you need to do that, don't have `Rec` be an alias for `RecF Identity` and have TH generate `RecF` the `Iso (RecF Identity) Rec` from `Rec`. Most uses of `Iso (RecF Identity) Rec` will be inlined and optimized away, anyway.
I also avoid TH when I can. It makes cross-compiling particularly difficult. $(makeLenses) is easy enough to write by hand, if you want to avoid the TH. The other TH macros aren't that much harder to write either. I would actually say the best reason to use $(makeLenses) is ease of maintenance, if your record changes in the future. But, it can also save time up-front if TH doesn't bother you. --- TH *is* the reasonable macro system for Haskell. It *should* be used for the same things you use Python metaclasses, C++ (non-parametric) templates, C macros, Lisp macros, or Java source processors. The pain points around TH are not going to get fixed by avoiding TH, that just avoids them for your project, for now.
I recommend only using as much of the `lens` library as you find useful. Understanding the internals is cool, but unnecessary for most people. Any code that uses record updates can basically be improved with lenses -- and that might not even require using the `lens` library. There's some alternatives out there with less complicated internals and nicer error messages, but you don't have to import *anything* to do some useful things with "functional references" / van Laarhoven lenses. I'm fairly convinced that the `ref = inner a -&gt; inner b -&gt; f (outer a) -&gt; f (outer b)` form is, if not fundamental, so useful that *more* pure languages need to expose (non-dependent?) fields using this type.
That looks amazing! Do you know if there are plans for covering the rest of the lens api?
The server should be pretty good already: there's no horrible edge case I've had to hide in the carpet. If you look at the code, it's actually pretty small (no new background threads etc.). The library is built on top of: - Warp, which is maintained by a stellar team and the server is already pretty solid and performant (even though I've found and patched a few minor HTTP2 subtleties) - proto-lens: which has really good tests and is the fruit of rock solid engineers Both projects are actively maintained. All in all I've had to write very little. My main contribution was learning enough gRPC while writing the client to be able to write the server by mundanely gluing two libs together. I've even _deleted_ some code from my initial proof-of-concept as proto-lens code-generator evolved, as a testimony of the great foundations I've used. That said, I've not run it in production myself yet and you may find fewer tuning knobs than you may get with some other languages (e.g., tuning flow-control). Somehow, if you're already running warp in production, trying out this library will not cost you much.
You're absolutely right, I was being sloppy, and will fix that in an updated version!
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [quchen/articles/.../**lens-infix-operators.md** (master → 6e95427)](https://github.com/quchen/articles/blob/6e954273c00b405229de1444c062df69dc2cd2ff/lens-infix-operators.md) ---- 
The current type of `readV` doesn't admit a safe implementation. The following works: {-# LANGUAGE RankNTypes #-} readV :: forall a. (forall s. V s a) -&gt; a readV f = runST (case f of V r -&gt; readSTRef r) The type of `runST` is `forall a. (forall s. ST s a) -&gt; a`. This implies that `a` cannot possibly refer to `s`, thereby preventing the mutable state to leak out though `a`. In `readV`, you likewise have scope type variables in a way such that `a` cannot` depend on `s`.
That's great! Thank you this is what I was looking for.
What does this even mean?
unused(?) all of those parts are used. core needs you to be running in a monad that has access to run IO actions, that has some state, for which you have a lens that can get a "System" out,, which has an environment 'e' that has a lens to get an Env out, and has a lens to get the Options out that were passed on the command line. Each constraint was accumulated from something we called.
You can use markdown to invalidate your post. ~~Like this.~~
I believe "snowflaking" is being different/special with no (other) purpose. I don't believe `lens` in particular or van Laarhoven lenses in general are "snowflaking". References are useful things; purity is a nice thing -- van Laarhoven lenses are nearly the simplest general implementation of pure (functional) references. The simplest implementation has it's own disadvantages, in particular being difficult to unify with other optics.
He wants to be able to add polynomials of different degree.
Yeah, I know, Pretty sure you can store polynomial's info at a type-level to get more optimal merging, but I won't make author's mistake and won't spend my time trying it out ;)
That's more or less equivalent to `id :: a -&gt; a`
I don't think you can ever have a `forall s. V s a`.
Thank you for your reply! I will take a look at th articles :)
https://github.com/rfjakob/earlyoom
Somewhere I had gotten the impression that the reason `Prism`s are written type Prism s t a b = forall p f. (Choice p, Applicative f) =&gt; p a (f b) -&gt; p s (f t) in the `lens` library is just so that they gracefully decay into van Laarhoven `Traversal`s. Or is there more to it?
Oops, right. Then, the more realistic version is: readV :: (forall s. ST s (V s a)) -&gt; a readV f = runST $ do V r &lt;- f readSTRef r
Do you have a reason for lurking here?
That's a good point as well. Before I even get there though, when I paste in that definition I get an error about impredicative polymorphism (that GHC does not support it, that is). This was one of the core type theory things I never got around to understanding. Perhaps now is the time.
Learn abstractions. Lens is one of the most important and useful ones.
I really appreciate you having taken the time to respond to my Stack Exchange levels Haskell noob questions. You don't have to, obviously, so double thanks for surprising me. Excuse my Haskell noob ignorance/knowledge-gap. I said unused because I don't see where those parts are used. core is a function of arity 0, so no arguments passed, and my beginner Haskell eye couldn't figure out what we have all those constraints for. My monad transformer knowledge comes from http://book.realworldhaskell.org/read/monad-transformers.html. I mean, it took me a second to realize that render :: (MonadIO m, MonadReader e m, HasEnv e, MonadState s m, HasDisplay s) =&gt; m () -&gt; m () is a function that takes a function which itself is probably `kernel :: IO ()` because it's constrained to be a MonadIO monad from transformers. I'm missing the insight how the rest of the pieces in the parentheses are used/needed.
Couldn't you just use a rewrite rule (or always optimize for the correct one) 
Ping /u/hvr_ &amp; /u/gbaz1
I have actually done the same thing (parallel, mutable, in-place sorting) but [with mergesort instead](http://hackage.haskell.org/package/primitive-sort-0.1.0.0/docs/Data-Primitive-Sort.html). Basically, using `unsafeInterleaveST` isn't going to work. In an earlier attempt at implementing my library, I tried using `unsafeDupableInterleaveST` and GHC sparks, but I could never get it to work. You don't get great guarantees about work being repeated or terminated arbitrarily when you go that route. That approach makes the evaluation of a effectful computation dependent on the evaluation of a thunk. It's not a great fit for the kind of concurrent mutation you're trying to do since you're looking for an actions that are executed exactly once and never get cancelled and repeated. What I ended up doing was creating a `forkST` function and using `MVar`s inside of `ST` to wait for subcomputations to finish.
Ok after taking all of this into account, I did some major refactoring, basically throwing out all the transformers. The repetition of writing instance MonadFoo App where foo = fooImpl bar = barImpl lead me to try out the derivingVia approach. It seems to work very nicely when it does. Unfortunately, in a few cases of mine it didn't. For one it can't handle implicit parameters - used by the CallStack-based logging functions of `monad-logger`. The was the case where typeclass methods look like fun :: FilePath -&gt; ConduitT i ByteString m () In this case, `Data.Coerce.coerce` isn't able to swizzle between `ConduitT i o m ()` and `ConduitT i o (Wrapped m) ()`, since the first three type variables of `ConduitT` are `nominal`. Still, this is a large step forward. Thanks!
Add parentheses?
Here are a list of common bugs that just don't exist in Haskell: "Oh, that's an undefined behavior for this operator in this circumstance, behavior varies on context" "Wait, this method returns null on success?" "if type(x) is strr " "F*king semicolons" "Wait, default args are mutable?" "Oh, F****, prod has a totally different version of X in $CLASSPATH for some reason" "Damnit, I thought that class was a singleton!" "Undefined is not a function" "Method X does not exist on object : null" "Oh, right, this function expects _4_ args for that case..."
[Title renders a little weirdly for me](https://imgur.com/a/YtySWgw)
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/C2QUhAB.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) 
Hrmm, I'll try to think this through and write another post on this. You're correct about what I meant about phantom data kinds 👌 Of your last 4 examples my intuition leads me to the first one; it is granular but still has semantics; it is quite reusable, and doesn't have any constraints in the class so you can implement a mock version easily. You don't really need any constraints, because when you implement against the real monad stack you know the concrete type, and you don't want unnecessary constraints "constraining" which mock monads you could implement it against.
Yes but I don't understand, where? Since the error is cryptic.
Well done. :-)
Let's say you forego using generics in Haskell. Are you now in a worse position w.r.t. Elm? Does Elm have a simpler solution for problems that are usually solved with generics in Haskell?
Great! This seems to solve it, thanks!
You mean: `(inner a -&gt; f (inner b)) -&gt; outer a -&gt; f (outer b)` form?
Thank you very _very_ much :)
I've never heard of Brittany before. I've been using hindent (actually a [fork](https://github.com/AshleyYakeley/hindent) with tweaks and bug fixes). Also how does HSpec compare with tasty?
free monad allows much cleaner code because they don't use lifters, but they use runners. This impedes the use of two effects in the same sentence. Much of the code that haskell monadic code precludes is legitimate code that should be fine to program. What is wrong with this: amount &lt;- getPriceFromState * getAmountFromDatabase This is not possible neither using monad transformers nor free monads.
Not sure what to think about hpack. On one hand I don't like that cabal has its own (and terrible) file format, on the other hand not everyone uses stack and hpack causes trouble when I use a fork of a dependency in a project (e.g. directly from git repo, using cabal.project), and the dependency is a stack project and only has a package.yaml.
I'm honestly not sure what those boxy characters are supposed to be. Perhaps they're modifiers meant to put the numbers in boxes? 🤷🏽‍♂️
I'm not aware of any plans regarding support for the rest of `lens` library. But you always can open issue.
Concretely the stuff in the parens is telling you about properties 'm' must have in order to pass one in or get one back from the function. `StateT Display (ReaderT Env) IO` would be one such choice of `m`.
&gt; I would imagine that more junior Haskell positions are available as non-remote. Indeed; [at SimSpace](https://www.reddit.com/r/haskell/comments/8u35gj/job_simspace_is_hiring_remote_and_local_haskellers/) for example, we are hiring remote Haskellers, but if the candidate does not have a lot of experience with Haskell, we'd rather have them work from a city in which we already have a presence, so our experienced Haskellers can coach our less experienced Haskellers.
Instead of downvoting this comment, formulate an actual reply. I thought this subreddit was better than abusing downvoting for indicating "I disagree".
You may try to check if native -XDerivingVia in ghc-8.6 works out better. &gt; For one it can't handle implicit parameters - used by the CallStack-based logging functions of monad-logger This is definitely a bug that you may want to report to [deriving-compat maintainers](https://github.com/haskell-compat/deriving-compat/issues)
Good bot
Thank you, sam-barr, for voting on imguralbumbot. This bot wants to find the best and worst bots on Reddit. [You can view results here](https://botrank.pastimes.eu/). *** ^(Even if I don't reply to your comment, I'm still listening for votes. Check the webpage to see if your vote registered!)
^thanks ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) 
&gt; Your executable should look like this: module Main (module YourPackage) where import YourPackage (main) Neat trick, but why wouldn't you use `-main-is YourPackage`?
Sorry, this list is just bad. I've never seen so bad opinionated list. I'm worried that beginners could start developing packages basing on this guide. Some pieces of advice are really good and nice (like maintain Changelog), but others... &gt; Use Git for source control I'm personally using `git`. But I'm using it not because `git` is a good version control system, but because of GitHub. Instead, this advice could be more general: _Use some version control system to track changes in your source code_. &gt; Build with Stack It's completely okay to build with `cabal`. Beginners might have even fewer issues with `cabal` especially when there're `new` commands. And here is the blog post with the simple introduction to both `cabal` and `stack`: * https://kowainik.github.io/posts/2018-06-21-haskell-build-tools.html I'm personally building my packages with both `cabal` and `stack` on CI to make users of both build tools happy. &gt; Stack ensures it will continue to build tomorrow. This is not strictly true. `stack` can ensure this only for Haskell packages because it uses a fixed snapshot. But if you're using library like `rocksdb-haskell` you need to install a system level dependency. If you aim for 100% reproducible builds, use `nix`, `stack` is not enough. &gt; Define with hpack I think that defining a package configuration with `hpack` is a not a good idea. &gt; Name with kebab case Currently it's not clear from this section that this advice is for package names. This could be clarified. But I agree with kebab-case naming for packages. &gt; Nobody wants to type out `hypertext-transfer-protocol` I'm okay with packages of this length. Typing package names is not really a problem in developing packages. It would be much worse if this package would have some short name where the purpose of the package is not clear from its name. &gt; Use Semantic Versioning Please, no. Semantic versioning is really bad. It's very easy to version with PvP. Just use the following scheme: a.b.c.d * a = ALL-NEW AWESOME COOL, i.e. number useful for marketing basically * b = actually meaningful “this version possibly breaks API” number * c = “this version adds features but doesn't break API” number * d = “I fixed haddocks”, “I fixed compilation with GHC 8.0.2”, etc &gt; Test with Hspec HSpec is not the ultimate testing tool. There're other testing frameworks. And it's good to test with at least something. Personally, I'm using `tasty` with `tasty-hspec` and `tasty-hedgehog` and I'm quite happy with this combination! But this doesn't mean that everyone should use these libraries. There's still a possibility for better testing frameworks. Try to use some testing library, and if you're not satisfied, you can switch to some other. &gt; If your package provides an executable, define it in your library and re-export it. Well, if your executable contains dependencies like `markdown-unlit` or `optparse-applicative` you will dump extra dependencies on users who're only interested in using your package as a library. I think that this advice could be: _don't write a lot of code in executable because you can't import executable modules in tests and test them._ You can solve both problems by putting executable code into an internal library. &gt; Benchmark with Criterion Well, if you want to wait for about 30 minutes before your package is built, then yes. But I would recommend `gauge` library instead. It's much more lightweight. Unless you really need fancy HTML output, `gauge` should be enough. I owe an apology to the Earth for wasting too much electricity on building and running benchmarks with `criterion` under different GHC versions and different laptops... So this is my opinionated comment for this opinionated guide. Otherwise, it looks good :+1:
I think that `hpack` has some serious drawbacks and provides very small amount of benefits. 1. A lot of people like common dependencies feature in `hpack`. But `cabal-2.2` has even more powerful common stanza, where you can put not only dependency but pretty much everything. 2. Automatic module search might be convenient. But usually, you're not introducing new modules too often. It's not that inconvenient to add one line with module name to `.cabal` file. Moreover, there're packages like `proto-lens-protoc` that generate modules for you. So you still need to write modules manually in `package.yaml` file and `hpack` doesn't solve the problem completely. And nothing stops `cabal-install` from having this feature as well. It's not a fundamental problem of the file format or the build tool. 3. YAML. I personally don't like this file format. There're might be some problems with parsing ambiguous fields (like versions: it could be number or string). I've used `hpack` in one multipackage project and YAML aliases helped to reduce some copy-paste between packages. But it didn't manage to remove config duplication completely because YAML doesn't support list concatenation. Which means that you can't move common parts of `test-suite` dependencies into some separate place. If you want to reduce code duplication then it's better to use more powerful configuration format like `dhall`. 4. `.cabal` file generation with new hash each time. If there're multiple people working on the project, they will always have conflicts if you store `.cabal` file under your version control system. But if you don't store `.cabal` file under your GitHub repository then it's more difficult to build your package for users who don't have `hpack`, they need to do more effort to generate `.cabal` file. So, personally, advantages of `hpack` are not really great but this tools fractures ecosystem, it becomes even harder to maintain tooling (and we already don't have excellent tooling story in Haskell world).
&gt; Please, no. Semantic versioning is really bad. It's very easy to version with PvP. Just use the following scheme: 100% agree. I feel this is nicer than SemVer, because you can separate out "hey, we've accumulated a ton of new features over time/hey, we broke a bunch of important stuff, sorry" vs "hey, there might be some minor things that changed, please take a quick glance at the ChangeLog to see if we broke your code". &gt; I owe an apology to the Earth for wasting too much electricity on building and running benchmarks with criterion under different GHC versions and different laptops... 😂😂😂 You should try making a blog with Hakyll; you'll probably have your hair turn white by the time `pandoc` finishes compiling.
I'm sorry I should have phrased it differently. It appears to be template-haskell itself that can't represent implicit parameters. I am not sure how `deriving-compat` should be able to do it.
Yeah. I flubbed that.
&gt; Lens only exists because the native type system makes that (normal) kind of work too hard. Untrue. Doing a record update, even a deeply nested one in Haskell isn't *hard*. It's ugly and non-compositional (i.e. hard to chain like `obj.field.subfield[index]`) van Laarhoven functional references (lenses) get rid of both the ugly (at the term level) while making things compose easily.
I'd add to this that hpack is just yet another tool, yet another file format, yet another point of failure that users have to be aware of. The barrier to entry for introducing new tooling should be quite high, and there's absolutely no way hpack reaches that.
I suppose that ``` buildGrid :: Num a =&gt; [a] -&gt; [a] -&gt; [[(a, a)]] buildGrid [] _ = [] buildGrid (r:rs) cs = [(r,c) | c &lt;- cs] : buildGrid rs cs ``` Judging by the number of times the head is pulled from the rows list and the number of times with add to the `List` but I'm just assuming this using my experience with imperative programing languages.
N.B. "Got changes to contribute to the site? Fork or comment on Github" there's a link at the bottom.
\&gt; I'd add to this that \`hpack\` is just yet another tool... &amp;#x200B; I'd argue that this is a feature, not a bug. The existence of \`hpack\` puts competitive pressure on the \`cabal\` team to continually improve both the library and the CLI tool, and results in a net good for the community. &amp;#x200B; Even if you don't believe that competition by itself breeds better tooling, \`hpack\` gets to explore a different design space than \`cabal\` does, and in doing so exposes patterns of usage that \`cabal\` may not have done so on its own. &amp;#x200B; \&gt; The barrier to entry for introducing new tooling should be quite high, and there's absolutely no way \`hpack\` reaches that. &amp;#x200B; I would also disagree with this, and I would point to the proliferation of \`package.yaml\` files in Haskell projects as anecdotal evidence that it's generally incorrect. \`hpack\` has reached and exceeded that barrier by virtue of enough people in the Haskell community wanting to use it in their projects.
Your second proposed version looks best to me. I wouldn't worry about performance - list comprehensions are just shorthand that's expanded by the compiler into the long form, so neither is faster than the other. I don't much care for the monad version, especially with multiple `do` blocks instead of just one. However, this task of a nested loop over the contents of two lists, calling a function on each pair, is so common that a general version has already been written, to which you need only supply the function to call: buildGrid = liftA2 (,)
&gt; I'd argue that this is a feature, not a bug. The existence of hpack puts competitive pressure on the cabal team Well, that's not a feature, that's a social effect. It may well have positive effects on the community but that doesn't mean I would tell someone it's a good idea to use it. &gt; and I would point to the proliferation of package.yaml files in Haskell projects as anecdotal evidence that it's generally incorrect A) That seems largely irrelevant; popularity has little to do with the associated learning curve and how much it adds to the failure surface area. B) I can't say you've convinced me it's accurate; my experience is that most projects do *not* use hpack. We'd need non-anecdotal data.
&gt; core is a function of arity 0, so no arguments passed, and my beginner Haskell eye couldn't figure out what we have all those constraints for One way to think about constraints is that they're additional arguments passed by the language after figuring out the types of things, each of which is a record containing the members of the typeclass. (IIUC, that's actually how they're implemented in some cases, although things often get inlined).
I see. So it might or might not be sufficient to skip those constraints, and it's good practice to add them. Though, without a way to newtype that, it might be tedious to write that out in all places that operate within said monad stack. The other thing about transformers that's been puzzling is how liftIO can get at the bottom IO, while I have to know how many `lift` calls I need to get at one of the reade/writer/state contexts. Don't get me wrong, I love the abstraction power this provides, it just seems super important to understand how it all works and fits together before one can use it with confidence.
Yeah, I think most of the time you can omit them and it'll be sufficient.
&gt; Well, that's not a feature, that's a social effect. I feel like this is a bit pedantic. The feature of `hpack` is an alternative way of specifying Haskell packages, and the social effect is that exploring this design space (and peoples' reactions to it) helps inform the `cabal` team about potential areas for improvement. There are direct and indirect features of doing so, but I feel like `hpack` attracts a lot of ire just for existing. Not saying that you do so in your comment, but I feel as if a lot of people push back on `hpack` because they believe everyone should be using `cabal`. This feels kind of antithetical to the core of OSS in general: if someone perceives a problem, they can fork software and/or make improvements. If enough people support the changes or the new direction, then it can be incorporated upstream _or_ people can continue to use it. As it stands, `hpack` produces valid `.cabal` files, so I don't really see a huge chance that it splits the community meaningfully. &gt; We'd need non-anecdotal data. Definitely true, searching GitHub for `package.yaml` with the `language:Haskell` restriction turns up a lot of stuff under "Commits", but someone would need to go through that to see what the _real_ impact is.
I came back to it after a while to try again, and figured out the issue: by default, `--test` is not run in case of warnings, so I just had to add `--warnings`. I'm leaving it here if someone else has the same issue! Now the server launches, but it doesn't respond when I try to open a page, trying to solve this problem too :)
It's okay. Say, go to definition doesn't always work, true, but you always could fall back to global search (Ctrl-Shift-F), the way scripted editors' users do it.
You can't grow them, each tuple is a new type. You need a function (a,b) -&gt; c -&gt; (a, b, c)
Tuples have a "constructor function" that looks like `(,)`. Add another comma for each extra element: `(,,) :: a -&gt; b -&gt; c -&gt; (a, b, c)`. There is no syntax for adding an element to a tuple. You would need to write a function for each length: add2Tuple :: c -&gt; (a, b) -&gt; (a, b, c) add2Tuple c (a, b) = (a, b, c) or a type class which can work generically.
Then how to append new elements to tuples?
The other comments answer well, but if you want to grow the structure maybe you should use a more suitable data structure
You create a new tuple with more elements.
&gt; You can't grow them, each tuple is a new type. You need a function (a,b) -&gt; c -&gt; (a, b, c) append2 :: (a, b) -&gt; c -&gt; (a, b, c) append2 (a, b) c = (a, b, c)
Also, triple-\` blocks don't really render very well. For multi-line code snippets, just indent each line by four spaces.
You can't append anything to anything in Haskell. In Haskell everything is immutable, which means you can't modify objects once they're constructed. You need construct new objects from existing objects.
But I'm thinking of a function that might be able to produce the appended element. So that one could call like (..., ..., createTuple(xs)) 
What about hypothetical `SomeJSON` and `KnownJSON` types, a-la `SomeSymbol` and `KnownSymbol`, and then using more ad-hoc-ish types built from combinators that can accurately represent any JSON value?
I think you might be getting confused between Haskell's "Tuples", and "Tuples" in other languages, such as python. In Python, a Tuple is sort-of like a list, or array, but immutable. In Haskell (however) a tuple is something completely different - think of it more like an unnamed structure or class (in the object oriented sense). It would be nonsense to try an "append" something to a class, so the same is true of tuples. However, you can *create* a new tuple (remember - like a class) with more elements, unfortunately it's up to you to do the legwork to copy the elements over! Because each new size of tuple is a different type, Haskell does not (and really, cannot) provide a function that turns an n-tuple into an (n+1)-tuple. If you really need something that you can append to, you will need to use a list, or array.
Thanks! I've edited and reformatted my older comments to indent by 4 spaces.
Do I need to `import Control.Applicative`? as in import Control.Applicative (liftA2) buildGrid = liftA2 (,) Also about type annotations, this is what hoogle says this generates Applicative f =&gt; f a -&gt; f b -&gt; f (a, b) Now for coordinates to work they need to be `Num x` in my case, can I simply just use the type annotation I was using previously or do Y need to use the `Applicative` type/monad?
&gt;Sorry, this list is just bad. I've never seen so bad opinionated list. A word of advice: do not use words "I've never seen" and "just bad" when someone tries to contribute to community. It is always in good faith and most probably shared with the purpose to discuss decisions behind it. Whether you use this words or not, they won't change an opinon for the most, but you will discredit the work of the author, and what I consider most important, you will hurt their feelings. &amp;#x200B; You can still express the same doubt by saying: Thank you for the contribution, I very much disagree with some points, let me clarify below. &amp;#x200B; &amp;#x200B;
So I did some leg work myself in ghci: import Control.Applicative (liftA2) buildGrid :: (Num a, Num b) =&gt; [a] -&gt; [b] -&gt; [(a, b)] buildGrid = liftA2 (,) This works, but this has left me with more questions, more specifically the result type, my older versions had the return type be a `[[(a, b)]]` this version is a `[(a, b)]` and while a flat `List` makes more sense to me that the nested one I wan't to understand why two functions that return the same result given the same arguments have different type annotations. I apologize in advanced if I'm being a lazy slob I'm just a bit confused and don't want to just cargo cult things into my code examples and I really don't know how to search for really specific things like this.
This looks great. More ghci functionality is always welcome!
Here are two Trac tickets for ghci commands * [`:instances`](https://ghc.haskell.org/trac/ghc/ticket/15610), this proposal * [`:elaborate`](https://ghc.haskell.org/trac/ghc/ticket/15613), step through instance resolution
Couldn't this almost be done at a library level? Should be a shoe-in.
There even is a cabal field for this (`main-is`).
There are some recent unreleased fixes for this in the wai repo
I see, I misread the value you were trying to produce because with the triple-\`s it was all squeezed on one line and a bit hard to read. Now that it's fixed, I see liftA2 is not a good solution. 
&gt; library level how so?
This sounds like a kinda good idea, but it's a compiler thing, and no language thing and this is important! It is something which just makes typing easier and belongs to preprocessing. Actually integrating it in haskell standards would be not a good idea in my opinion because it's just (actually unnecessary but nice) syntactic sugar
Thank you! :) 
Heh, it was a nice detour though, like taking the scenic road :)
``` Prelude&gt; import qualified Data.Vector.HFixed as V Prelude V&gt; 1 `V.cons` (2,3) :: (Int,Int,Int) (1,2,3) ```
Which limitations do you think Source-Plugin could help with?
&gt;Limitations &gt; &gt;The implementation doesn't play very nicely with do-blocks bound via let. If it could be solved by for example some parantheses or some other type of syntax the source plugin could provide them. I am just currently playing around with them and so, like I said, every problem looks like it could be solved by them.
So the proposal is to allow anything of kind `Constraint` in an deriving clause? Have you thought about how it would play with things like `deriving (MonadState s)` -- where `s` isn't actually quantified anywhere? This works in a deriving clause today, but it's not immediately clear how to extend that into your proposal.
That would be really cool to have this feature in GHC! I wanted it for so many time. Currently you can use the following existing solutions to remove some code duplication: * https://stackoverflow.com/questions/45113205/is-there-a-way-to-shorten-this-deriving-clause
I don't think source plugins help here, because you need to know the inferred type of the monad in order to determine the correct instance of `(&gt;&gt;=)` to use. You might be able to get away with a type-checker plugin, but those things are nasty as hell and tied pretty directly to the GHC api.
\`hspec\` has a much simpler string matching system for selectively running subsets of tests.
I think it is natural to allow things of kind `Type -&gt; Constraint`, `Type -&gt; Type -&gt; Constraint` etc. or tuples of these, instead of restricting to `Type -&gt; Constraint`. The "tuples" essentially behave like unboxed tuples, flattening everything. So the following should work type MonadS r s = (Monad, MonadState s) data Burrito = ... deriving (MonadS Filling, ...)
Thanks for the ping! u/chshersh and I hashed things out. No harm, no foul :) 
It does? Hmm, I didn't look at the documentation carefully enough then. Thanks for the correction.
Thank you - I was thinking the same thing. If someone has a good counterpoint, I'd appreciate hearing it.
I could certainly buy that argument. There are a lot of syntactic sugar aspects already built into the language, so you could argue either way as to whether to incorporate more.
You can use `TemplateHaskell` to find out both the classes a type is a member of, and the types that are members of a class. So it's *almost* a library function, because I don't believe you can bind GHCi commands to TH functions, only to regular functions `:: String -&gt; IO String`.
Ghc makros would be cool, like in lisp, but with text replacement
 buildGrid = traverse . traverse (,) Should still give you O(n * m) time, as do both your solutions. (This uses that `[]` is both `Traversable` and `Applicative` and (ab)uses that `(-&gt;) e` is `Applicative`)
Maybe this could build on derivingVia, so that could could join strategies for separate type classes into a single strategy that derived both?
I love everything about this proposal, particularly the drunkenness.
I you don't mind could you explain a little how composition of functions that expect multiple arguments work? there are things that are dead obvious like say: inspect :: [String] -&gt; String inspect xs = (show . unlines) xs Both `unlines` and `show` only expect one argument so the way I read this is `xs` is passed down to `unlines` and the returned value get's passed `show` which also expect a single argument. But what's been throwing me off a little is when both functions expect multiple arguments such as this: deepMap :: (a -&gt; b) -&gt; [[a]] -&gt; [[b]] deepMap = map map So in this case doing `deepMap reverse [["foo"], ["bar"]]` will return `[["oof"], ["rab"]]` and it kind of makes sense since it's a deep `List`, but my brain resist it because of `map` expect 2 arguments but returns a single value, I read the `(.)` operator info and this is it's annotation: (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c I read this as I need 3 arguments, two functions one initial value and I will produce a function `c` that needs you can call with any arguments, but what I can't wrap my head around is how does this work internally, I feel like my brain is betraying me since I'm used to implementing composition in Javascript like this: const compose = (...funcs) =&gt; arg =&gt; (funcs.reduce((composed, func) =&gt; func(composed), arg); Where you use like so: const double = x =&gt; x * 2; const square = x =&gt; x * x; const squaredDoubled = compose(square, double); And doing `squaredDouble(4)` would return `32`, I know this is a poor implementation of composition since it only support one argument and I know that Javascript does not return a function when you partially call another function, but I fear that having the notion of this^ implementation is what's tripping me up when trying to understand composition between functions that expect multiple arguments, would you mind ELI5 what happens internally when one composes things like `(map . map)` in Haskell? Thanks in advance. 
Doesn’t `:info` only work on type constructors? So you can’t do `:i Maybe Int` only `:i Maybe`, which will then print out all instances, including those instances constraints. So if you have `instance Ord a =&gt; Ord (Maybe a)`, an `:instances Maybe Int` would show `Ord`, but an `:instances Maybe (IO Int)` wouldn’t. 
You can make a type synonym for a large bag of constraints you're going to use, but in my experience I find the actual minimal set of constraints you want for _every_ single function is rather different. type Foo m = (MonadIO m, ...) Unfortunately because of using MPTCs rather than TFs, this doesn't handle abstract constraints on state, etc. well. I prefer to be accurate because it lets me test in a smaller context without extraneous details and to know that nothing is happening _to_ those extraneous details
"All" functions in Haskell are curried, which means they take exactly one argument. If they look like they take multiple arguments it's because they return a function. Take the pair constructor `(,)`. It's type is `a -&gt; b -&gt; (a, b)` or *emphazing the curry* `a -&gt; (b -&gt; (a, b))`. You can look at it as a function taking two arguments, but it's just as correct to think of it as a function taking one `a` that returns (a function taking one `b` that returns an `(a, b)`). So, if you write `f . (,)` then the first argument to `f` must be *a function* that looks like `b -&gt; (a, b)`. If we have `f :: (b -&gt; (a, b)) -&gt; c` then `f . (,) :: a -&gt; c`. For something like `map . map` (the correct implementation of `deepMap`), it helps to avoid reusing type variables. So, the inner `map :: (ia -&gt; ib) -&gt; [ia] -&gt; [ib]` and then outer `map :: (oa -&gt; ob) -&gt; [oa] -&gt; [ob]`. Now, since we composed the outer map and the inner map we first have to think of the both as single-argument functions (since `(.) :: (grfa -&gt; fr) -&gt; (ga -&gt; grfa) -&gt; (ga -&gt; fr)` takes two single-argument functions. That gives inner `map :: (ia -&gt; ib) -&gt; ([ia] -&gt; [ib])` and outer `map :: (oa -&gt; ob) -&gt; [oa] -&gt; [ob]`, and also lets us know (using `~` to denote type equality) that `ga ~ ia -&gt; ib`, `fr ~ [oa] -&gt; [ob]`, and (via `grfa`) `[ia] -&gt; [ib] ~ oa -&gt; ob`. This later breaks down into `oa ~ [ia]` and `ob ~ [ib]`, and we substitute that into the equality for `fr` to give `fr ~ [[ia]] -&gt; [[ob]]`. Our composed function has type `ga -&gt; fr`, and expanding this based on the type equalities gives `(ia -&gt; ib) -&gt; ([[ia]] -&gt; [[ib]])` or *demphsizing the curry* and renaming variables `(a -&gt; b) -&gt; [[a]] -&gt; [[b]]`.
&gt; Which is not sorted correctly You throw a couple of `flip`s in there to fix that, if you want: buildGrid = flip (fmap . traverse (flip (,)))
&gt; Which uses infinite ranges to create any list this in it of itself isn't useful since cooling coordsGrid will infinitely run, Haskell is *lazy*. Just because the normal form of a term is infinitely large doesn't mean using it will take infinitely long. grid = map (\r -&gt; map (\c -&gt; (r, c)) [0..]) [0..] is a fine top-level binding. (Feel free to play with points-free / pointless transformations to remove the lambdas, if you want.)
What exactly is hard about `recObj { field = newValue }`?
Some people prefer 2 spaces :)
why should it be?
`deriving` boilerplate and uniqur instances, like `-XDerivingFunctor`, doesn't "just save typing". it: * saves typing * increases readability * provides a standard implantation that works, by construction, everywhere (i.e. if you have the compiler installed, you don't necessarily have the preprocessor) * determines a specific ordering (this is why processors are infamous for JavaScript development, for example) * saves **writing**, which is the non-fisingenuous way of saying "saves typing", because you're less likely to write something that's buggy or inefficient or whatever, and less likely to remember to update it when the origin changes * and so on this is all compilers, they standardized and Implement together a hundred features and "conversions".
Neat, thank you! TIL. Seems kind of a shame that we can't just share the same concept - 'type' for data can handle multiple kinds, seems like type for classes should also be able to as well. Not sure if it's quite so straightforward from GHC's perspective, but as a user, it seems logically consistent.
By that logic, the whole concept of deriving is 'unnecessary but nice' syntactic sugar. We're talking about a feature that is focused entirely on generating code so that the user doesn't have to / can't accidentally screw it up. This seems like exactly the same feature, just, one level removed, and drastically less powerful.
 Yeah it works great for types of kind `*`. But for higher-kinded types (with inductive instances), `:instances` from the proposal is superior. For example.. I can use `:info` to query the instances for `Maybe` and see that its `Eq` instance is inductive. &gt;&gt; :info Maybe data Maybe a = Nothing | Just a -- Defined in ‘GHC.Base’ instance Eq a =&gt; Eq (Maybe a) -- Defined in ‘GHC.Base’ and I can query both `Maybe` and `Int` for their instances Prelude&gt; :info Maybe Int data Maybe a = Nothing | Just a -- Defined in ‘GHC.Base’ instance Eq a =&gt; Eq (Maybe a) -- Defined in ‘GHC.Base’ instance Eq Int -- Defined in ‘GHC.Classes’ But I have to perform type class resolution in my head to see that `Maybe Int` has an `Eq` instance. With this proposal, I would be able to do this: &gt;&gt; :instances Maybe Int instance Eq (Maybe Int) whereas if I did it with `IO Int`: &gt;&gt; :instances Maybe (IO Int) -- The Eq instance doesn't show up because it isn't valid
How does it not need GC? Does it evaluate all interactions in the net to delete it? Or just delete trash after computation is done?
Is anyone able to provide some examples for how to use the Control.Category functions? The typeclasses I've worked with so far work well with common types (Int, Bool, \[\], String, etc) so it's been easy to create examples of how to use the functions provided by them. I could use a bit of help getting this ball rolling. &amp;#x200B; I'd love a few simple examples in the spirit of the below **Semigroup** Right "Valid 1" &lt;&gt; Right "Valid 2" &lt;&gt; Left "Invalid 1" Sum 1 &lt;&gt; (Sum 2 &lt;&gt; Sum 3) &lt;&gt; Sum 4 &amp;#x200B; **Monoid** Just \[1,2,3\] \`mappend\` Just \[4,5,6 \['a','b','c'\] ++ mempty &amp;#x200B; **Applicative** \[(+10),(+1000)\] &lt;\*&gt; \[1,2,3\] ("Hello ", (\*2)) &lt;\*&gt; ("World!", 2) &amp;#x200B; Thanks,
 &gt; we can't just share the same concept - 'type' for data can handle multiple kinds, seems like type for classes should also be able to as well. I don't quite follow. Could you give an example? 
Thanks you so much for your time, and sorry for the amount of typos in my previous comment, I think you've helped me understand, so I read your response a couple of times and correct me if my understanding is wrong: (.) :: (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c I think what I've been missing or ignoring about that annotation is that `c` can be itself a function and I've lead myself to believe it's just a value because it's represented by a single letter, that said: (map reverse) :: [[a]] -&gt; [[a]] -- And... (map (map reverse)) :: [[[a]]] -&gt; [[[a]]] -- And... (map (map reverse)) [[1,2,3], [4,5,6]] :: Num [a] =&gt; [[[a]]] What I conclude is that I can write something like m' :: [[[a]]] -&gt; [[[b]]] m' = (map . map) reverse m' [[[1..4], [5..8]]] -- Should return [[[4,3,2,1], [8,7,6,5]]] I can the apply the constraints I showed in the example above m' :: (Num a, Enum a) =&gt; [[[a]]] -&gt; [[[a]]] m' = (map . map) reverse I realize this may sound like a super obtuse way to reach the same conclusion, but it really does help to remember that partial application is a thing and that everything is curried. These are things I knew (or thought I knew), but since I haven't built any haskell chops yet I keep forgetting. 
It's not clear to me what you are looking for. Is there some interface you have in mind?
It's worth noting that the maximum tuple size GHC supports is limited (i believe to size 62), so this couldn't be done for lists of arbitrary length, even if it did make sense to do.
Are you perhaps looking for [`conduit`](http://hackage.haskell.org/package/conduit)? {-# LANGUAGE LambdaCase, ScopedTypeVariables #-} import Data.Conduit import Data.Conduit.List type Queue a = ConduitT () a IO () toConduit :: Queue a -&gt; ConduitT i a IO () toConduit queue = sourceNull .| queue -- | -- &gt;&gt;&gt; :{ -- runConduit $ sourceList [10,20,30] -- `bind` (\x -&gt; sourceList [x,x+1]) -- .| consume -- :} -- [10,11,20,21,30,31] bind :: forall a b. Queue a -&gt; (a -&gt; Queue b) -&gt; Queue b bind producer consumer = producer .| awaitForever (toConduit . consumer) 
Are you saying that you always want `Result` to look like `data Result = Win (Player g) | Draw`, regardless of what `g` is? You can define it as a top-level datatype, you don't have to define it as an associated type even though it depends on `Player g`.
Nice idea. I would expect the constrain synonym to take a type parameter `a`, though. Then its just a matter of “resolve type synonyms in `deriving` and look through tuples”. Quite reasonable. A type synonym might also help to derive classes where the interesting argument is not the last one. 
Nice idea. I would expect the constrain synonym to take a type parameter `a`, though. Then its just a matter of “resolve type synonyms in `deriving` and look through tuples”. Quite reasonable. A type synonym might also help to derive classes where the interesting argument is not the last one. 
Nice idea. I would expect the constrain synonym to take a type parameter `a`, though. Then its just a matter of “resolve type synonyms in `deriving` and look through tuples”. Quite reasonable. A type synonym might also help to derive classes where the interesting argument is not the last one. 
Nice idea. I would expect the constrain synonym to take a type parameter `a`, though. Then its just a matter of “resolve type synonyms in `deriving` and look through tuples”. Quite reasonable. A type synonym might also help to derive classes where the interesting argument is not the last one. 
Didn't the `ConstraintKinds` extension come with thus feature, but it was removed because apparently deriving a constraint synonym was a nonsensical thing to do? It's about time it is re-implemented!
Didn't the `ConstraintKinds` extension come with thus feature, but it was removed because apparently deriving a constraint synonym was a nonsensical thing to do? It's about time it is re-implemented!
Didn't the `ConstraintKinds` extension come with thus feature, but it was removed because apparently deriving a constraint synonym was a nonsensical thing to do? It's about time it is re-implemented!
Didn't the `ConstraintKinds` extension come with thus feature, but it was removed because apparently deriving a constraint synonym was a nonsensical thing to do? It's about time it is re-implemented!
Didn't the `ConstraintKinds` extension come with this feature, but it was removed because apparently deriving a constraint synonym was a nonsensical thing to do? It's about time it is re-implemented! The following used to work, and was certainly useful: {-# LANGUAGE ConstraintKinds, DeriveGeneric, DeriveAnyClass, CPP #-} #ifdef DDEV type Serialisable = (ToJSON, FromJSON) #else type Serialisable = Store #endif data Blah = Foo { a :: Bar } deriving (Generic, Serialisable) 
Didn't the `ConstraintKinds` extension come with this feature, but it was removed because apparently deriving a constraint synonym was a nonsensical thing to do? It's about time it is re-implemented! The following used to work, and was certainly useful: {-# LANGUAGE ConstraintKinds, DeriveGeneric, DeriveAnyClass, CPP #-} #ifdef DDEV type Serialisable = (ToJSON, FromJSON) #else type Serialisable = Store #endif data Blah = Foo { a :: Bar } deriving (Generic, Serialisable) 
Didn't the `ConstraintKinds` extension come with this feature, but it was removed because apparently deriving a constraint synonym was a nonsensical thing to do? It's about time it is re-implemented! The following used to work, and was certainly useful: {-# LANGUAGE ConstraintKinds, DeriveGeneric, DeriveAnyClass, CPP #-} #ifdef DDEV type Serialisable = (ToJSON, FromJSON) #else type Serialisable = Store #endif data Blah = Foo { a :: Bar } deriving (Generic, Serialisable) 
Didn't the `ConstraintKinds` extension come with this feature, but it was removed because apparently deriving a constraint synonym was a nonsensical thing to do? It's about time it is re-implemented! The following used to work, and was certainly useful: {-# LANGUAGE ConstraintKinds, DeriveGeneric, DeriveAnyClass, CPP #-} #ifdef DDEV type Serialisable = (ToJSON, FromJSON) #else type Serialisable = Store #endif data Blah = Foo { a :: Bar } deriving (Generic, Serialisable) 
Nice idea. I would expect the constrain synonym to take a type parameter `a`, though. Then its just a matter of “resolve type synonyms in `deriving` and look through tuples”. Quite reasonable. A type synonym might also help to derive classes where the interesting argument is not the last one. 
Nice idea. I would expect the constrain synonym to take a type parameter `a`, though. Then its just a matter of “resolve type synonyms in `deriving` and look through tuples”. Quite reasonable. A type synonym might also help to derive classes where the interesting argument is not the last one.
Didn't the `ConstraintKinds` extension come with this feature, but it was removed because apparently deriving a constraint synonym was a nonsensical thing to do? It's about time it is re-implemented! The following used to work, and was certainly useful: {-# LANGUAGE ConstraintKinds, DeriveGeneric, DeriveAnyClass, CPP #-} #ifdef DDEV type Serialisable = (ToJSON, FromJSON) #else type Serialisable = Store #endif data Blah = Foo { a :: Bar } deriving (Generic, Serialisable) 
Didn't the `ConstraintKinds` extension come with this feature, but it was removed because apparently deriving a constraint synonym was a nonsensical thing to do? It's about time it is re-implemented! The following used to work, and was certainly useful: {-# LANGUAGE ConstraintKinds, DeriveGeneric, DeriveAnyClass, CPP #-} #ifdef DDEV type Serialisable = (ToJSON, FromJSON) #else type Serialisable = Store #endif data Blah = Foo { a :: Bar } deriving (Generic, Serialisable) 
Didn't the `ConstraintKinds` extension come with this feature, but it was removed because apparently deriving a constraint synonym was a nonsensical thing to do? It's about time it is re-implemented! The following used to work, and was certainly useful: {-# LANGUAGE ConstraintKinds, DeriveGeneric, DeriveAnyClass, CPP #-} #ifdef DDEV type Serialisable = (ToJSON, FromJSON) #else type Serialisable = Store #endif data Blah = Foo { a :: Bar } deriving (Generic, Serialisable)
Didn't the `ConstraintKinds` extension come with this feature, but it was removed because apparently deriving a constraint synonym was a nonsensical thing to do? It's about time it is re-implemented! The following used to work, and was certainly useful: {-# LANGUAGE ConstraintKinds, DeriveGeneric, DeriveAnyClass, CPP #-} #ifdef DDEV type Serialisable = (ToJSON, FromJSON) #else type Serialisable = Store #endif data Blah = Foo { a :: Bar } deriving (Generic, Serialisable)
Didn't the `ConstraintKinds` extension come with this feature, but it was removed because apparently deriving a constraint synonym was a nonsensical thing to do? It's about time it is re-implemented! The following used to work, and was certainly useful: {-# LANGUAGE ConstraintKinds, DeriveGeneric, DeriveAnyClass, CPP #-} #ifdef DDEV type Serialisable = (ToJSON, FromJSON) #else type Serialisable = Store #endif data Blah = Foo { a :: Bar } deriving (Generic, Serialisable)
Didn't the `ConstraintKinds` extension come with this feature, but it was removed because apparently deriving a constraint synonym was a nonsensical thing to do? It's about time it is re-implemented! The following used to work, and was certainly useful: {-# LANGUAGE ConstraintKinds, DeriveGeneric, DeriveAnyClass, CPP #-} #ifdef DDEV type Serialisable = (ToJSON, FromJSON) #else type Serialisable = Store #endif data Blah = Foo { a :: Bar } deriving (Generic, Serialisable)
I sense this is likely an instance of the [X Y problem](https://en.m.wikipedia.org/wiki/XY_problem); we can probably help you better if you let us know the problem you're trying to solve using this. This is something you may consider doing in dynamic languages, but it's very difficult (and not very practical) to write well- typed programs with this sort of idea; I can pretty much guarantee that there's a different (well-typed!) way to accomplish whatever it is you're after though!
**XY problem** The XY problem is a communication problem encountered in help desk and similar situations in which the real issue ("X") of the person asking for help is obscured, because instead of asking directly about issue X, they ask how to solve a secondary issue ("Y") which they believe will allow them to resolve issue X. However, resolving issue Y often does not resolve issue X, or is a poor way to resolve it, and the obscuring of the real issue and the introduction of the potentially strange secondary issue can lead to the person trying to help having unnecessary difficulties in communication and offering poor solutions. The XY problem is commonly encountered in technical support or customer service environments where the end user has attempted to solve the problem on their own, and misunderstands the real nature of the problem, believing that their real problem X has already been solved, except for some small detail Y in their solution. The inability of the support personnel to resolve their real problem or to understand the nature of their enquiry may cause the end user to become frustrated. The situation can make itself clear if the end user asks about some seemingly inane detail which is disconnected from any useful end goal. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
Didn't the \`ConstraintKinds\` extension come with this feature, but it was removed because apparently deriving a constraint synonym was a nonsensical thing to do? &amp;#x200B; It's about time it is re-implemented! &amp;#x200B; The following used to work, and was certainly useful: &amp;#x200B; {-# LANGUAGE ConstraintKinds, DeriveGeneric, DeriveAnyClass, CPP #-} &amp;#x200B; \#ifdef DDEV type Serialisable = (ToJSON, FromJSON) \#else type Serialisable = Store \#endif &amp;#x200B; data Blah = Foo { a :: Bar } deriving (Generic, Serialisable)
Didn't the `ConstraintKinds` extension come with this feature, but it was removed because apparently deriving a constraint synonym was a nonsensical thing to do? It's about time it is re-implemented! The following used to work, and was certainly useful: {-# LANGUAGE ConstraintKinds, DeriveGeneric, DeriveAnyClass, CPP #-} #ifdef DDEV type Serialisable = (ToJSON, FromJSON) #else type Serialisable = Store #endif data Blah = Foo { a :: Bar } deriving (Generic, Serialisable) 
You can define your result outside of the typeclass and then parameterize it; data Result p = Win p | Draw Then gameResult :: g -&gt; Maybe (Result (Player g)) Make sense? Then you can happily export your result, but people can still define which type of player it operates over ☺️✌️
I miss the `BindSyntax` instance for `IxMonad :: ((i -&gt; Type) -&gt;(i -&gt; Type)) -&gt; Constraint`. Perhaps `BindSyntax` could be generalized to allow that.
&gt; and the types that are members of a class I was looking for something like this some time ago and was told it wasn't possible. Do you have a link showing how this can be done? 
Nice idea. I would expect the constrain synonym to take a type parameter `a`, though. Then its just a matter of “resolve type synonyms in `deriving` and look through tuples”. Quite reasonable. A type synonym might also help to derive classes where the interesting argument is not the last one. 
I think I'd write it like this buildGrid xs ys = fmap inner xs where inner x = fmap ((,)x) ys Though you should check whether you don't actually want the crossproduct (whicb is the already mentioned liftA2).
`Category` is just a more restrictive `Monoid`. Functions are a pretty common type. **Category** (+ 10) . (* 100) length . filter odd . drop 10 foldr (.) id [(+ 5), (* 10), negate]
It depends. Laziness is probably the main culprit. But it is much easier to argue about intermediate results that are pure.
Whoever use that it is very likely that he has a serious problem understanding and designing data structures. https://www.reddit.com/r/haskell/comments/9ded97/is_learning_how_to_use_the_lens_library_worth_it/e5i006a/ It is weird to think that a structure is optimal in disk and another different in memory once multiaccess by threading is commonplace. A record is a record, not a tree. These nested data structures are a byproduct of old object orientation ideas wrongly implemented that mix different layers of abstraction. They applied the concept of encapsulation and inheritance to the implementation as physical enclosing of data inside data. It is hard to make people aware of that error. 
Whoever use that he very likely has a serious problem understanding and designing data structures. https://www.reddit.com/r/haskell/comments/9ded97/is_learning_how_to_use_the_lens_library_worth_it/e5i006a/ It is weird to think that a structure is optimal in disk and another different in memory once multiaccess by threading is commonplace. A record is a record, not a tree. These nested data structures are a byproduct of old object orientation ideas wrongly implemented that mix different layers of abstraction. They applied the concept of encapsulation and inheritance to the physical implementation as enclosing of data inside data. It is hard to make people aware of that error. That nesting of data structures is not even done in OOP languages nowadays. Modern Java encourages the use of the same data structure in memory as well as in the database so the mapping is straighforward and it can use a in memory SQL-like query language. And, for the rest of people who really need to process legitimate tree structures the solution is simple: don't use records for that. use a tree.
Come to Warsaw next time :)
re Package Versioning Policy versus Semantic Versioning: the PVP was made for Haskell, and specifically addresses typeclass instances, which are unique among programming languages. fyi, the definitions: * PVP: https://pvp.haskell.org * SemVer: https://semver.org 
Yes I think this more properly belongs in the compiler
What is necessary is not lenses, but a good in-memory relational query language and a good database record cache.
This is exactly the point and it gets a lot worse with complex types We can verify that individual instances exist with [`Dict`](https://hackage.haskell.org/package/constraints-0.8/docs/Data-Constraint.html#t:Dict) but there is no way to enumerate all instances &gt;&gt; :set -XTypeApplications &gt;&gt; import Data.Constraint &gt;&gt; Dict @(Eq (Maybe Int)) Dict
This is *xldenis*'s first GHC proposal and contribution best of luck!
&gt; to know that nothing is happening to those extraneous details how do you mean? if you use a synonym, the signature is always the same and not subject to typos, so I think you meant something else that can happen here.
[removed]
Is there at least a link to the slides?
Unfortunately not, we will try to get them though
Yeah, but it's not easier to check/test your reasoning by sprinkling print statements around. Haskell also doesn't have a good debugger; the best I've seen is the one in ghci, and I can never get it to do what I want. So, while reasoning about pure things may be easier, if you do it wrong, it can be mighty hard to find out *where*.
&gt; `Category` is just a more restrictive `Monoid` I would say instead that it's a less restrictive `Endo`. 
While I didn't downvote your comment, I wouldn't be surprised if some folks have reacted negatively to the "**I'll see your heresy and double it**" opening gambit. It feels like some kind of confrontational posturing, which is quite out of place here.
There seems to have been [an attempt at this already](https://wiki.haskell.org/GHC/Error_messages), but it is far from complete. I'm just posting this here for reference.
That's a great suggestion. If I wanted to try and submit a PR myself, I would love to figure out what that would look like.
Let's suppose these are largish functions: foo :: (MonadReader e m, HasFoo e) =&gt; m Int bar :: (MonadState s m, HasBar s) =&gt; m Int Then I can write baz :: (MonadReader e m, HasFoo e, MonadState s m, HasBar s) =&gt; m Int baz = (+) &lt;$&gt; foo &lt;*&gt; bar but if something goes wrong in my code, I can trust when debugging and reasoning about `foo` that it doesn't touch my state at all and that `bar`doesn't touch by environment. When I'm sitting there at ghci trying to test `foo` I don't have to make up a Bar to test it with. Mutatis mutandi for `bar` and `Foo` respectively.
I'd like to hear from people who've chosen to use Squeal in production. Specifically I'd like to hear about why you chose Squeal over the alternatives, and your experiences with it. I'd also like to hear from people who've chosen to use another library over Squeal: what made you choose something else?
"This explodes" -- it does? I tried that example with GHC 8.4.3 and it worked alright for me with ConstraintKinds on.
&gt; When I'm sitting there at ghci trying to test `foo` I don't have to make up a `Bar` to test it with. Mutatis mutandi for `bar` and `Foo` respectively. `foo` deals with `Foo` and `bar` deals with `Bar`, so I'm confused why you say you don't have to come up with a `Bar` for `foo`. That leads me to the question how you test it, which will help me understand what you mean you don't need to make up a `Bar`. My understanding right now allows me confirm the first sentence, saying `foo` doesn't touch your state and `bar` won't access your env. Although I have to wonder why it is that `bar` couldn't in some way access your env by using internal knowledge and calling the right functions. Would that be prohibited by the type constraints given the scoped way to access said MonadReader environment?
There are [multiple](https://stackoverflow.com/questions/28690448/what-is-indexed-monad) indexed monad definitions for Haskell. It would be nice if you supported all indexed monads with do notation. I was referencing McBride's indexed monad from [here](https://personal.cis.strath.ac.uk/conor.mcbride/Kleisli.pdf).
IMO, it shouldn't be in an external wiki. We could add give error messages numbers and have a `--explain` flag (for longer explanations) like Rust.
Thanks for your kind words. From the next Meetup we'll experiment with recording the screen and then having a picture in picture with the speaker
`main-is` requires an unnamed module (i.e. named `Main`), afaik
We also have template Haskell, but it's not the easiest stuff to figure out.
Aha! Perhaps that explains it.
I’m still trying to wrap my head around what the traverse does here, but anyways – my attempt of trying to make it point-free gave me flip (fmap.(.(,)).flip fmap) which is not much longer, but since it really is \rs cs -&gt; fmap (\r -&gt; fmap ((,) r) cs) rs it works with any 2 Functor types, not just with a functor and a traversable. Well, actually “point-free” would probably have to be flip(fmap`fmap`(`fmap`(,))`fmap`flip fmap) ;-) — Anyways.. the types are, in comparison: (Traversable t, Functor f) =&gt; f a -&gt; t b -&gt; f (t (a, b)) vs (Functor f1, Functor f2) =&gt; f1 a -&gt; f2 b -&gt; f1 (f2 (a, b)) and here are some results to show correctness Prelude&gt; let f = flip (fmap.traverse (flip(,))) Prelude&gt; f [2,3,4] [5,6,7] [[(2,5),(2,6),(2,7)],[(3,5),(3,6),(3,7)],[(4,5),(4,6),(4,7)]] Prelude&gt; let f = flip(fmap`fmap`(`fmap`(,))`fmap`flip fmap) Prelude&gt; f [2,3,4] [5,6,7] [[(2,5),(2,6),(2,7)],[(3,5),(3,6),(3,7)],[(4,5),(4,6),(4,7)]] &amp;#x200B;
That would be cool. You probably have to adapt a lot of it though, since I made it hex specific. Also, you might want to consider minimax instead.
Do you have the code to look at? My gut says maybe the copy isn’t actually doing the work due to laziness, but it’s impossible to tell without code. 
I'll provide it tomorrow but I'm pretty sure both are doing the right thing since I tested it by printing some stuff in the functions.
This was in contrast to the style you mentioned with a type synonym. If you instead wrote type MyMonad = ReaderT Foo (StateT Bar) IO and then said foo :: MyMonad Int bar :: MyMonad Int to save typing like you suggested 2-3 comments ago, and to which I was responding and called baz :: MyMonad Int baz = (+) &lt;$&gt; foo &lt;*&gt; bar then you'd need to supply both a Foo and a Bar to locally test either function, even though each ignored one or the other.
(*about the flair* `123!!`*):* &amp;#x200B; 123!! = 10377906169198633132968954062505261124557626693878350555241116588640224946471700130339113335858154296875 
`SquealException` has partial record accessors. imo, `PQException` should be its own (record) type, and wrapped by `PQException`: data SquealException = PQException PQException | ... https://github.com/morphismtech/squeal/blob/master/RELEASE%20NOTES.md 
Haha, yeah, I also find partial accessors ugly. But I also dislike having to use double constructors \`throw PQException (PQException status code message)\` in my code. Haskell records, amirite? Maybe I'll change this in the next release; wouldn't want anyone accidentally using a partial accessor! Thanks for the critique!
Point free doesn't refer to the composition operator, which maths generally writes with a middle dot rather than a decimal point / full stop. It refers to defining the composition point-wise by assigning some name to the value being processed. Pointfree.io can give you a (the?) answer. Though playing around with things feels more like learning than looking up the answer.
traverse works for any applicative and for any parametric container that can have an ordering assigned to all its indexes in all its shapes. I think you might even be able to derive traversable.
Just to be clear, these two `foo` and `bar` are not equivalent to the previous versions above, are they? If they are, I'm not squinting hard enough to see it ;-). I can see how it might be, just not confident what I think I see is what Haskell sees. Concretely, here we have a stack of a `Reader` for `Foo` wrapping `State` of `Bar`, to be used in a function that's meant to compute a result of `Int`. Squinting a little longer, I start to think that perhaps the `m` in the previous version and `MyMonad` here are the context we're meant to run `baz` in, and we don't pass in the monadic values (reader env e and state -unsure-what- s), but they're provided by way of running `baz` inside a `MyMonad` context. Not sure how off memory, might have to check RWH chapter 18 again. &gt; then you'd need to supply both a Foo and a Bar to locally test either function, even though each ignored one or the other. As an extension of what I above, what does "supply" mean exactly in Haskell coding terms? &gt; This gets worse once you throw IO into the mix and your MyMonad now has to be tested in IO even for the pure functions that have no side-effects, etc. Isn't IO at the bottom the idiomatic stack and how `liftIO` is used? I still get confused about how `lift` and `liftIO` work, meaning what they do to get the right the desired context, but that's my inexperience, I guess.
good point, maybe a few utilities, like throwPQException :: String -&gt; IO a which fill in some defaults and collapse the layers. (and yeah, we do need records!)
I'm afraid your `MonadS` doesn't have kind `Type -&gt; Type -&gt; Constraint`. Type synonyms cannot be partially applied, so `MonadS Filling` is illegal.
What explodes is `type Ordered = (Order, Eq)`. What kind should `Ordered` have?
I also don't know understand the API you're after, but several concurrent queues offer `dupChan`. e.g. the standard Control.Concurrent.Chan. I'd recommend looking at how that library is implemented, also TQueue which is even simpler, if only because it's interesting.
I made a typo there, meant to delete the `r` type parameter. Fixed now.
Here's a direct link to the form to show interest: [https://docs.google.com/forms/d/e/1FAIpQLSdJPWIMroEyBuqP15ngmZrp0XwwP-C5uuEC9NYuObaRmzbd3g/viewform](https://docs.google.com/forms/d/e/1FAIpQLSdJPWIMroEyBuqP15ngmZrp0XwwP-C5uuEC9NYuObaRmzbd3g/viewform) 
This definition says “`Game` is the set of types representing game states, each of which has some type of move, player identifier, and game result associated with it, as well as the following functions: […].” That is, you’d use something like this to abstract over several different games in the same program; if you only have one game type, you don’t need a typeclass (since a set with one element is pretty boring). To use this class, you’d create a type like `data TicTacToe`, and say: data TicTacToe = TicTacToe { tttPlayer :: TttPlayer , tttBoard :: Triple (Triple (Maybe TttPlayer)) } data TttPlayer = X | O data Triple a = Triple a a a data TttMove = TttMove TttRow TttColumn data TttRow = RA | RB | RC data TttColumn = C1 | C2 | C3 data TttResult = Win TttPlayer | Draw instance Game TicTacToe where type Move TicTacToe = TttMove type Player TicTacToe = TttPlayer type Result TicTacToe = TttResult initialGame = TicTacToe { … } legalMoves state = … turn state = … play move state = … gameResult state = … So as written, each game defines what its possible results are. You might have a more complex game with multiple winners (`type MultiResult g = Set (Player g)`), or a cooperative game where everyone wins or loses together (`data CoopResult = CoopWin | CoopLose`). But if you don’t need a game to be able to specify the type of outcome, i.e., it’s always the same structure as `TttResult` in my example, then you can remove `type Result g` from `class Game` and just have a single *top-level* (not associated) data type `data Result g = Win (Player g) | Draw` that you use for all games. If you do keep the abstraction over `Result`, bear in mind that each game doesn’t need to have a *unique* `Result` type—it’s of course possible for multiple different games to have the same structure of outcome, players (e.g. two-player games), or even move types (e.g. Tic-Tac-Toe vs. Ultimate Tic-Tac-Toe). A typeclass should have some accompanying laws, and ideally the types should enforce those laws where feasible. For example, `play` takes a move and a game state and promises that it can always return an updated game state, but the existence of `legalMoves` implies that you should only call `play move state` if ``move `elem` legalMoves state``. Does the move simply not change the state if it’s not legal? You could make `play` return a `Maybe g` and say ``move `notElem` legalMoves state`` implies `isNothing (play move state)`. It’s possible to do some more complex type-level trickery to make sure that only legal moves can be played, but that might be beyond what you want to do right now.
Yeah, right, sorry. Posted by phone so wasn't able to confirm, didn't expect the behavior with a single constraint. Type -&gt; Constraint, I would guess. That does feel a little strange without the type parameters present explicitly, now that I think more about it - honestly the more I think about it, the more it seems the behavior of the deriving clause is the real culprit here. The implicit instances are just a little strange, because you're not really deriving Ord, you're deriving Ord MyData. So really it should accept any tuple of Contraints, where the kindedness matches that of the Type for which instances are being derived. But of course that'd break all of hackage, so, maybe not a practical idea.
What I meant by that statement is that it's not really a `Monoid` since that would be a mapping **MxM -&gt; M** (the laws are very similar). With `Category` you get these cases: * For distinct `a`, `b`, and `c` the sets `cat b c`, `cat a b`, and `cat a c` have pair-wise empty intersections. * For distinct `a` and `b` the sets `cat b a`, `cat a b, and `cat a a` have pair-wise empty intersections. * Only in the case when every type parameter is the same `cat a a` is truly a `Monoid`. (Which is what you mentioned with `Endo`'s `Monoid` instance.) 
Note that both V.thaw and V.freeze copy the vector. Also, without seeing more context this is basically impossible to answer. Vector has a bunch of optimizations that fuse away intermediate steps. If you mutate with modify it might go through `New` and mutate in place, for instance.
[https://pastebin.com/qAjVfNxN](https://pastebin.com/qAjVfNxN)
I am not sure if that's the problem but unsafeThaw does not make a copy. vec and vec2 in main are the same vector, so you are 
Yeah it doesn't make a difference, usually I didn't run the test together
Not at a computer at the moment but does the situation change if you SPECIALIZE the recursive functions and add INLINE pragmas to everything else with vector in the type signature?
They should already be unboxed. INLINE only makes it worse
Try switching the order in which you test.
Doesn't make a difference
Squinting further, is it correct to say that in `baz :: (MonadReader e m, HasFoo e, MonadState s m, HasBar s) =&gt; m Int`, we describe ways in which `m` must be able to be used in order for it to fit `baz`. And there's no need to use the reader or state monad, just stating `m` has to fit in like that.
I am really surprised that my first try fixed the issue because I have no idea why it worked. main :: IO () main = do g &lt;- getStdGen let list = Prelude.take 20000 $ randomRs (1,100000000) g :: [Int] let vec = V.fromList list let b1 = do vec2 &lt;- V.thaw vec vecQuickSort compare vec2 0 ((length vec2)-1) --print $ (sort list) == (V.toList $ vecQuickSort' vec) defaultMainWith myConfig [ bench "1" $ nfIO $ b1 , bench "2" $ nfIO $ vecQuickSort' vec ] The only difference in the core is in the [main function](https://gist.github.com/Tarmean/60a8371be9ff6884c1b3e4a42b5548a4) but they don't diff well and I don't have time right now to check it out.
Yes but we still don't hold a reference to the mutable vector without copying it from a different vector. Even replacing V.thaw by V.unsafeThaw breaks it ;(
Put the function body on a separate line, like so: function Some Long Pattern Match = do blah blah
Actually I missed the trees because of the forst, your pivot selection makes the sort O(n^2) on presorted arrays.
It shouldn't. It selects the median of the first, the middle and the last element of the vector and swaps it with the last element, then the last element is selected as the pivot. And the input arrays for the example are random anyways.
Oh, missed that but that's still the problem - it should be```let !mid = (high + low) `div` 2 in```!
If I may suggest an improvement, I would say to copy what screencasts and elearning (lydia.com) material does. Instead of the teacher/presenter, we see the screen grab of the laptop, including interactive demo sessions, and it's only the voice of the presenter. If you're doing a community event and want to self-host videos, it will be beneficial due to smaller videos because screen grabs will usually have content that's easier to compress (smaller file). Some presentations have no or just auxiliary slides, where it's more useful to see the presenter, but it's still unusual for the visuals of the presenter to be vital content. So, a screen grab of the laptop plus good microphone voice of the speaker, and you've got yourself a high-quality recording of a talk. Screencasts and elearning videos do this for a reason.
`m` in the former could be instantiated to be `MyMonad`. However, in the form I gave that actually had class constraints rather than being written against a concrete monad transformer stack, you had the _option_ of calling it with less stuff. I only offered the newtype version, which I do not recommend, because I was asked for a way to make a synonym for this stuff earlier in the thread. IO at the bottom of the stack _when you need it_ is idiomatic. However, paying for what you aren't use means you can use your code in fewer situations. By way of analogy I can write a function like foo :: (Show a, Read a, Eq a, Num a, Bound a, Enum a) =&gt; a foo = 2 but every constraint for that function except for the `Num` is pointless and just serve to make it so I can call this function in fewer situations and pay to construct and pass typeclass dictionaries I'm not going to use.
&gt; rather than being written against a concrete monad transformer stack, you had the option of calling it with less stuff. that's interesting and important to know. Hope all of this sinks in as I'm getting more Haskell experience. &gt; foo :: (Show a, Read a, Eq a, Num a, Bound a, Enum a) =&gt; a &gt; foo = 2 If I had a type `Tire` and that derived all those classes, and then wrote `foo :: Tire`, would that have the same effect?
If I had a type Tire and that derived all those classes, and then wrote foo :: Tire, would that have the same effect? The takeaway I was aiming for was that foo :: Num a =&gt; a foo = 2 was a better requirement because it was more permissive for what `foo` could be used for than foo :: (Show a, Read a, Eq a, Num a, Bound a, Enum a) =&gt; a foo = 2 as you can use foo at types that totally cannot and do not have all the rest of those instances. Moving to foo :: Tire foo = 2 makes your code _more_ restrictive in that foo can only be used as a `Tire`. This is the exact opposite effect from what I was trying to express. So no, it'd have a different effect.
Isn’t that the most popular use case for applicatives?
Due to lazy evaluation, Haskell will only calculate a value if you actually need to use it. So you can essentially write your function however you like, as long as the return value is correct, it won't do any unnecessary calculation.
Thanks, got most of it. What I meant to ask is if the constraint/straight-jacket is the same in `foo2` and `foo3` if `Tire` is a type that derives the same things. Or it's more likely deriving doesn't constrain but loosens the "type space". Whether you want to constraints more or loosen depends on the use case, and in this case I was aiming for an interface that's harder to misuse. You know, if `FilePath` was defined restrictively, we couldn't pass in `[Char]`. `FilePath` is just `String` and interestingly it's redefined in many libraries.
 foo2 would let you use it as an Int. foo3 would require you to use it as a Tire. So, yes, for the goal you are asking, yes, it;'s give you a more restrictive type. Sometimes that is good to avoid misuse. Not always. The reason I brought up the example here at all was to show how to loosen the constraint so that you can use more monad transformer stacks when you go to call it testing with fewer constraints. That was the source of my objection to the goal of locking your code into some particular `MyMonad` as bad style.
Oh so easy, don't know how I didn't come up with that myself haha. Thank you.
For the record. To partially apply constraints, use the ["constraint synonym encoding"](https://gist.github.com/Icelandjack/5afdaa32f41adf3204ef9025d9da2a70#constraint-synonym-encoding-or-class-synonym), to talk about type (cls &amp; cls') a = (cls a, cls' a) partially applied, `Eq&amp;Show :: Type -&gt; Constraint`, I define `(&amp;)` as a type class with an identical instance -- (&amp;) :: (k -&gt; Constraint) -&gt; (k -&gt; Constraint) -&gt; (k -&gt; Constraint) class (cls a, cls' a) =&gt; (cls &amp; cls') a instance (cls a, cls' a) =&gt; (cls &amp; cls') a the order is flipped, the RHS of the type synonym is on the left of `=&gt;`
It would be great to add an example of using `mtl` typeclasses instead of a concrete stack. That would illustrate some of the similar user land types that are leveraged. ``` setAccumulator :: MonadState Integer m =&gt; Integer -&gt; m () ``` vs ``` setAccumulator :: Member (State Integer) r =&gt; Integer -&gt; Eff r () ``` 
Warning: this is decidedly \*\*not idiomatic\*\* (at least not yet) and also not for beginners. &amp;#x200B; Using dependent-ish types, we can ensure that a function is called only with arguments which satisfy some property. Furthermore, unlike smart constructors, the validation is decoupled from the function itself and it's only necessary when the values are not known in advance. &amp;#x200B; Take this function: &amp;#x200B; {-# language DataKinds #-} {-# language ScopedTypeVariables #-} {-# language TypeFamilies #-} {-# language TypeOperators #-} &amp;#x200B; module Main where &amp;#x200B; import Unsafe.Coerce import Data.Proxy import GHC.TypeLits import Data.Type.Equality &amp;#x200B; difference :: forall n m. (KnownNat n,KnownNat m,CmpNat n m \~ LT) =&gt; Proxy n \-&gt; Proxy m \-&gt; Integer difference pn pm = natVal pm - natVal pn &amp;#x200B; The parameters to the \`difference\` function are passed as types, the firs must be less than the second. &amp;#x200B; Here's an auxiliary function that hides some necessary unsafety in its innards: &amp;#x200B; isLT :: (KnownNat a, KnownNat b) -- does this function already exist somewhere? =&gt; Proxy a \-&gt; Proxy b \-&gt; Maybe (CmpNat a b :\~: LT) isLT x y \-- dont' screw up this unsafeCoerce or chaos reigns | natVal x &lt; natVal y = Just (unsafeCoerce Refl) | otherwise = Nothing &amp;#x200B; And here's an example of calling \`difference\` with numbers read at runtime: &amp;#x200B; main :: IO () main = do (SomeNat proxyn,SomeNat proxym) &lt;- liftA2 (,) readLn readLn case isLT proxyn proxym of Just Refl -&gt; print $ difference proxyn proxym Nothing -&gt; error "first number not less" &amp;#x200B;
No, I mean that if you had a transformer where you had StateT s (ReaderT e m) a = s -&gt; e -&gt; m (a, s) ReaderT e (StateT s m) a = e -&gt; s -&gt; m (a, s) these have equivalent expressive power. On the other hand, the Maybe example was showing how not all monad transformers commute.
Equivalently: `checkedComputation :: Int -&gt; Int -&gt; Int -&gt; Maybe Int` `checkedComputation a b c = do` `guard (a - b) &gt; 0` `guard (a - b) &lt; c` `pure (doStuff a b c)`
The problem is with the call to `thaw`. The following variation of your `main` exhibits the same properties, but better illustrates what's wrong: main :: IO () main = do g &lt;- getStdGen let list = Prelude.take 20000 $ randomRs (1,100000000) g :: [Int] let vec = V.fromList list vec2 &lt;- V.thaw vec --print $ (sort list) == (V.toList $ vecQuickSort' vec) defaultMainWith myConfig [ --This should be normal but it's slow bench "1" $ nfIO $ vecQuickSort compare vec2 0 (length vec2 - 1), --This should be slow but it's fast bench "2" $ nfIO $ do vec3 &lt;- V.thaw vec vecQuickSort compare vec3 0 (length vec3-1) ] I haven't really confirmed, but I suspect that floated out call to `V.thaw` prevents the `V.fromList` call to fuse. Related GHC ticket: https://ghc.haskell.org/trac/ghc/ticket/917
Great idea—[done](https://github.com/stepchowfun/effects/pull/27).
Here are my two cents on these approaches: - Handwritten monad — don't do this - Free monad — [don't do this](https://markkarpov.com/post/free-monad-considered-harmful.html) - MTL — this is what I usually reach for in small projects because it requires the least boilerplate - Extensible effects — I would use this for a big project with lots of different effects in the program that need to be combined
Yeah, I probably oversimplified: I do want to manipulate the list in the meanwhile, something like: `uncurry f &lt;$&gt; sortOn (+) $ zip xs ys`
Nice! &amp;#x200B; Since \`hlint\` is written in Haskell, which has Dhall bindings, you could patch it to read Dhall directly. Not that that's the simplest thing to do. But did you consider it?
The right part of `~` is unimportant, I think, and with my current understanding the stacks are different, switching up what `lift` unwraps what in layer 1 and 2. 
Nice! I don't quite follow the comparison of lines though. The hints are mostly 1 per line in both Dhall and Yaml, so how did the Dhall end up shorter? Or are you comparing by laying out the Yaml over 5 lines? Note that HLint itself doesn't do that, but basically makes each line a JSON object, and thus fit on one line. I do realise the Dhall is definitely more compact, and has less duplication, so I can see why it's easier to fit on one line.
Right, what a bummer.
Remember to add a `$` or parentheses to the lines with `guard` to fix the associativity.
Both can be passed to a function that expects to take or return something with constraints `(MonadState s m, MonadReader e m)` and you can perform exactly the set of operations on them in that context. That is what I mean by equivalent expressive power. Under those constraints you can't write `lift`, but even if you wrote against the concrete monad transformer stacks there you could translate code for one to the other just by fiddling with lift, no expressive power in terms of what code can be written is gained or lost. On the other hand the example involving Maybe/MaybeT, there is a fundamental difference in that one backtracks on failure and the other does not. This is the distinction I was attempting to call attention to.
Over the past week or two, I have received a number of emails about the fact that the range library broke with the latest version of `free`; this has since been fixed. However, these emails have shown me that there is clearly a large amount of interest/usage of the `range` library within the community so, on top of fixing the issue with `free` I have done the following: * Improved all of the documentation: please check it out and [raise a PR](https://bitbucket.org/robertmassaioli/range/src/master/) with any improvements. Almost every function has example usage now. * Created [Bitbucket Pipelines builds for the latest and many prior versions of Haskell](https://bitbucket.org/robertmassaioli/range/addon/pipelines/home#!/results/20/). * Lifted the base requirement from 4.5 =&gt; 4.7 (Haskell 7.8, so still pretty old) * Made a number of small fixes that bothered me about the library. After looking at this library again, I have had a few ideas for future work. Please feel free to raise any suggestions in the comments here and thanks for the look/review.
This is an interesting idea! Need to think more carefully about this approach and benefits we get, because this will require some patches in HLint itself. This decision is going to be more on HLint maintainers rather than me, but thanks for an excellent proposal! As the good starting point I can create the issue to the HLint repository and try to discuss it there first.
I agree that it could be not very clear from the notes. The thing is that not all kind of rules can be written as one line without losing readability in `yaml`. For example, this can be one line in yaml: - warn: { lhs: Control.Exception.evaluate, rhs: evaluateWHNF } But the rules with more fields are not that readable when written in one line: - warn: { name: Use 'Generic' from Relude, , lhs: GHC.Generics.Generic, rhs: Generic , note: ! '''Generic'' is already exported from Relude' } And also, I was comparing the generated `.hlint.yaml` file with `hlint.dhall`, where the generated file has fancy alignment by default made by Dhall itself. Anyway, I guess that not the number of lines is so significant here, but that it can be maintained easier and it's very simple to make a change to &gt; 100 fields of HLint rules by changing only one line in Dhall configuration.
Why not call the module `Data.Range.Range` just `Data.Range` by moving `Data/Range/Range.hs` to `Data/Range.hs`?
We can get more wild here: {-# LANGUAGE MonadComprehensions #-} import Data.Ix checkedComputation :: Int -&gt; Int -&gt; Int -&gt; Maybe Int checkedComputation a b c = [ doStuff a b c | inRange (0, pred c) $ a - b ]
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [rikvdkleij/intellij-haskell/.../**README.md** (master → f31d097)](https://github.com/rikvdkleij/intellij-haskell/blob/f31d097cb18a230390160d38634fa92b2d5e5fa4/README.md) ---- 
If you are looking for a high level solution, have you looked at streamly, https://github.com/composewell/streamly ? &gt; Essentially, I want a producer to add items to the queue and a consumer to simultaneously take &gt; items from the queue Streamly does this idiomatically, take a look at the README, tutorial and haddocks of streamly. &gt; and transform them to create new queues (ideally in a non-destructive fashion, without actually &gt; removing anything from the original queue). This part is not yet implemented but being worked upon, may get released in next couple of months.
Is there a collection of interesting talks to watch? 
Yeah, this probably the more useful answer in the long run. Separate the checking from the actual computation, and force the caller to supply evidence that they've run the check. If we're going to leverage the type system in this way though, we could go a little further. Specifically, we could return evidence that the arguments aren't valid instead of nothing. This way the caller has at least gained *some* information. data NotABC = AminusBNotGtZero Int Int -- a b | AminusBNotLtC Int Int Int -- a b c makeABC :: Int -&gt; Int -&gt; Int -&gt; Either NotABC ABC -- def omitted because editing code on a phone is hard :( Probably overkill in such a simple case, but it can be quite helpful in more complex situations. 
yes To op: When you have two (Maybe a) e1,e2 and a function f that takes two a's you can do: f &lt;$&gt; e1 &lt;*&gt; e2
Iirc liquid haskell would also work but is almost certainly overkill. Returning maybe is an ok default for partial functions. There is a reason that division doesn't return Nothing when dividing by zero. If a bunch of functions rely on the same invariant you can make a struct with a maybe-returning smart constructor (aka a function) and then pass the verified struct around.
Syntax highlighting should be part of your editor, depends on what you use. I use neovim with syntastic and that's it. Second, you dont really need autocomplete, most of the time having a second terminal with ghci / ghcid is more than enough. Stack is basicly a superset of cabal and additionally handles installing GHC, providing sets of package version that work together, installing packages from git repos, etc
Based on what you wrote I think you should try: - [stack](https://docs.haskellstack.org/en/stable/README/) - VS.code - [Haskero](https://marketplace.visualstudio.com/items?itemName=Vans.haskero) or [Haskelly](https://marketplace.visualstudio.com/items?itemName=UCL.haskelly) I'm sure many people will disagree and if you went a long a bit you might want to switch to HIE but in workshops I use this setup and it's really easy to set up for beginners
Are you on Linux/Windows? I've heard that Linux typically has fewer issues. Did you try installing Stack? You could install GHC using it. After installing stack, run stack new foo cd foo stack build # this should succeed hopefully stack ghci # double check that this works I'm sorry you're having an awful experience. Feel free to PM me if you need further help.
Stack Vs code plus haskero plus hlint extension and Phoityne debugger extension. Works great for me on windows I usually keep a terminal with ghci running as well for experiments. Doctest + quickcheck also highly recommended for unit tests
&gt; Second, you dont really need autocomplete, most of the time having a second terminal with ghci / ghcid is more than enough. It's really sad that so many people in the Haskell community think this is true.
There was a GHC proposal to add construction for this case: * https://github.com/ghc-proposals/ghc-proposals/pull/38 If you don't like tuples, you can use this trick with `-XPatternSynonyms`: * https://www.reddit.com/r/haskelltil/comments/8wtvk4/pattern_synonym_as_a_workaround_for_matching_on/
It makes a lot of sense, I think I was on this track before I gave up! 
I think I got stumped when I implemented two games before I started writing the typeclass, so I assumed \`Result\` types had to be defined in game code, but it doesn't have to be. Makes sense now!
I peeked into the source code, and it seems to be a Writer \[w\] monad under the hood. This means, especially a use case with mapM\_ immediately gives a quadratic complexity. Any reason why it's not a difference list?
I had a terrible time trying out different setups on windows for ghci and purescript. Definitely recommend using a Linus vm if you're on Windows
Give [minimal-haskell-emacs](https://github.com/soupi/minimal-haskell-emacs) a try. It should mostly just work if you have ghci/stack on your PATH.
That's because \`sortBy (comparing length) (group (sort s))\` is no function, but has parameters and is actually already a value. 
I wonder if one could fake the result of sortBy as a function though? because intuitively it makes sense to think of concat $ sortBy as a function composition.
Try [this](https://github.com/dramforever/vscode-ghc-simple), just install in VS Code and it should work out of box.
What do you mean? In mathematics f(g(x)) also isn't a composition but a value. It's true that the notation is sometimes much more loose and people tend to say "the function f(x)" but I think almost everyone would prefer to keep ambiguity out of programming languages. For example, what should `map $ not` mean? You would translate that to `map . not` while it only makes sense as `map not`
Okay so you're arguing from some kind of "type strictness" viewpoint?
If you're not an Emacs user already, and seeing how you consider VSCode, I suggest VSCode with hie (haskell-ide-engine) plugins. If you were to use Emacs, I'd suggest to use-package dante.el, since hie is still not quite there yet. It will with time, and then I'd suggest hie plugged into Emacs via eglot or lsp-mode. haskell-ide-engine's git master doesn't build right now, but it used to 5 days ago, so it's not a general issue. May I suggest checking out a specific stable tag and build that with `stack install`? If you still don't manage to build haskell-ide-engine, then one of the VSCode ghc(i/mod) integrations should to the trick. dante is just a single elisp file and doesn't require a separate executable, it reused ghc(i) and detects stack/cabal project. If none of this works, then basic syntax highlighting coupled with Ghci is your last resort for quick build feedback in your editor. Hope this helps. I'm using Emacs with dante because I use org-mode and hie isn't as reliable yet.
I built haskell-ide-engine current master (b18351856bbc364d987cdfa7ef1b31e1bd5cd3a2) earlier today without problems. On Debian testing.
Shouldn't it be `concat . sortBy (comparing length) . group . sort`?
Streamly is actually what we're using right now, with some custom wrappers (like unfolding a stream from a mutable vector) to get the extra functionality we need! Cool to hear that it's in active development.
I agree, I really don't get why people say "you don't need X" for workflow related things. Instead, a better-phrased point would be "my workflow involves Y, Z. What functionality of X do you need that Y and Z don't provide? Perhaps you can use X' as a work around." When I told some C++ programmers about Hoogle, they were quite surprised and agreed that a similar tool for C++ would be useful, even though it doesn't exist currently. Cross-pollination of ideas is very important to making progress.
Goddammit, didn't know this, so I just gave it a try and it works as advertized! You and the author just made my week.
I've used VSCode with Haskero, Haskelly, HIE (aka Haskell Language Server Client), and will probably try haskell-ghcid soon. We have a containerized setup which complicates using these extension, which is why I've used so many (none have worked exactly the way I'd like). Love me some VSCode.
I agree. in /u/SuperManitu's defense, I understand the sentiment. I have also had to learn this workflow, and once I got used to it, I really started getting quite easy and fun writing Haskell. I would word it like this: "for now, here are the best tools we can use, and here is a tutorial on how to be very productive with them". This does not exclude the validity of observing that, with better tooling, the learning curve would be way nicer. I think we can easily forget how painful every roadblock is for newcomers, no matter how small. Having to set up 5 things, while learning how to even read library docs, while hunting down why plugin X failed to compile, while being told to "just use Y", can all be quite overwhelming.
You can't always replace `($)` with `(.)` because they have different types. In addition, as operators they have different precedence, so even if they had the same type, the implicit parentheses could be inserted differently. You question sounds very much like, "Why doesn't replacing + with / work?"
I do nested case most of the time, and case on tuples in a few limited scenarios.
You claim an intuition, you don't explain it. I could just as easily claim that `+` and `/` are interchangable. In this example: (-2.0) = (-4.0) + 2 (-2.0) = (-4.0) / 2 So, it's clear that it shouldn't matter whether you use / or +.
Haskell-ide-engine with an ide that supports it (emacs, spacemacs, VSCode) is the way of the future. I do a fresh pull from the master git branch every couple weeks, make build-all and spacemacs figures out what to do when I open a stack project. Sometimes it gets it wrong and I have to build the project and then open an ide, but it’s good and getting better. Also has some support for nix workflows. 
If you think of `$` as infix `id` instead of "function application" then the mystery should melt away
Thanks!
Even if you're not learning, why wouldn't you want good autocomplete for imports or when calling a function. On VS Code, every time I write the implementation of a function below its type declaration I have to type out the name of a function again. I find that absurd.
&gt; If you want to reduce code duplication then it's better to use more powerful configuration format like dhall. Speaking of Dhall... I'd actually *love* for Cabal to gain first-class support for defining the project using Dhall instead of the ridiculously ad-hoc current cabal.project and foo.cabal files. Unless I'm missing something critical, it would seem to be an almost perfect fit, IMO: It's safe to evaluate, there are no infinite loops, a whole multi-project build would be just as simple as a single-project build, etc. etc.
The state of the haskell tooling is pretty awful. It turns me off every time I try haskell. However, most recently I have had moderate success with the following: * Only use stack, not cabal. * Never use stack install. Only use stack build. (As long as you only use stack build, the result is only little bit crappier than the modern package management systems used by rust and dotnet core.) * Use vscode. Install the haskell highlighting plugin and Simple GHC (Haskell) Integration. (The older ide plugins are more mature but when using "stack build" you have to install their requirements locally for each project which sucks. You might be tempted to use them not use "stack build" for this reason and instead do a global install but this will break and you'll hate your life.)
Was getting this yesterday: gcc: error: .stack-work/dist/x86_64-linux/Cabal-2.2.0.1/build/Language/Haskell/LSP/Types/ClientCapabilities.dyn_o: No such file or directory gcc: error: .stack-work/dist/x86_64-linux/Cabal-2.2.0.1/build/Language/Haskell/LSP/Types/Diagnostic.dyn_o: No such file or directory Trying today's master now
&gt; It's written that: &gt; &gt; sumEuler = sum . (map euler) . mkList &gt; &gt; is equivalent to &gt; &gt; sumEuler x = sum (map euler (mkList x)) That is correct. &gt; it shouldn't make difference as to whether one uses ., () or $. That is incorrect. Consider the following variants: sumEuler1 = sum . map euler . mkList sumEuler2 = sum (map euler (mkList)) sumEuler3 = sum $ map euler $ mkList sumEuler4 x = (sum . map euler . mkList) x sumEuler5 x = sum . map euler . mkList $ x sumEuler6 x = sum $ map euler $ mkList $ x sumEuler7 x = sum (map euler (mkList x)) As the stackoverflow post claims, `sumEuler1` is indeed equivalent to `sumEuler7`. But note that this transformation adds an `x`, it does not simply replace the `.` with parentheses. In particular, `sumEuler1` is equivalent to `sumEuler4`, `sumEuler5`, `sumEuler6`, and `sumEuler7`, and `sumEuler2 is equivalent to `sumEuler3`, but those two groups of functions are not equivalent to each other (and the functions in the second group don't type-check).
There are two types of functions in Haskell: total functions (ones that have all the arguments in place, for example `f a b = a - b`) and partial functions (ones that lack some of the arguments, for example `f a = (a -)` or `f b = (- b)` or even `f = (-)`). In all four of those formulas the function has a type of `f :: Num a =&gt; a -&gt; a -&gt; a` and in fact all of those four formulas are identical. So a partial answer to your question is that `$` applies total functions, `.` applies partial ones. Therefore, `f a b c = (a -) $ b - c` is identical to `f a b c = a - (b - c)` `f a b = (a -) . (b -)` is identical to both previous ones. Now, the reason you got confused between the two is probably because `f a b c d = (a -) $ (b -) $ c - d` is valid, as `(b -) $ c - d` is a total function `f a b c d = (a -) . (b -) $ c - d` is valid too, because `(b -)` is a partial function You can think of `$` as if you're putting a `(` instead and a `)` at the end of your "sentence", So, depending on your stylistic preferences you formula can be rewritten: as `freqSort s = concat $ sortBy (comparing length) $ group $ sort s` or as `freqSort s = concat . sortBy (comparing length) . group $ sort s` or even as `freqSort = concat . sortBy (comparing length) . group . sort`, if you wish to appease the pointfree gods. Oh, there's also a big old page about this on [HaskellWiki] (https://wiki.haskell.org/Pointfree).
I like this response, but it's worth pointing out that "total" and "partial" functions actually mean something [entirely different](https://en.wikipedia.org/wiki/Partial_function) in general parlance!
**Partial function** In mathematics, a partial function from X to Y (sometimes written as f: X ↛ Y or f: X ⇸ Y) is a function f: X ′ → Y, for some proper subset X ′ of X. It generalizes the concept of a function f: X → Y by not forcing f to map every element of X to an element of Y (only some proper subset X ′ of X). If X ′ = X, then f is called a total function and is equivalent to a function. Partial functions are often used when the exact domain, X, is not known (e.g. many functions in computability theory). *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/haskell/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
`" "` isn't a Char. You might want `' '`.
Nah, it’s even worse, since in the (usual) cases where “replacing `($)` and `(.)`” works, it only does so because they have different fixities. So it’s more like 10 - 1 * 0.5 10 - 1 + 0.5 being the same.
 go (Just 5) "hello") where go Nothing (a:as) = _ go (Just x) _ = _ go Nothing [] = _ 
I would strongly recommend you start by reading up a bit on basic Haskell syntax. Especially about infix operators. Note that `.` and `$` are *not* part of the Haskell language but instead just defined in the standard library. Actually, avoid `.` and `$` as long as you don’t fully get them yet, they are so-called *higher order functions* and only the second step of getting into the language. Also avoid copying long expressions containing `sortBy`, `comparing`, both of which you - for sure - don’t really understand either yet, those are also *higher oder functions*. Instead use the syntax you already know and build your own code from scratch. Learn how to define your own infix operators and what fixity is. Then get into higher-order functions and make sure you are able to use a function such as `map`. And the last essential skill I’ll list here will be to understand the very basics of Haskell’s type system, ...oh, and look up *lambda expressions*, (how could I forget xD). Also try learning to read basic error messages from the compiler. If you accomplish all of the above, you will have learned some *actual* parts of the Haskell language and finally you can understand, how `($)` and `(.)` are more often than not just simple trick to avoid writing to many parantheses and explicit variables.
It's not function composition, but application. It doesn't combine functions to take a value, but it takes a value and applies a function to it.
Installing ghc platform (full version)works well on Windows , but ran into serious issues trying to get any sort of editor plugin to work well, due to version conflicts. I've tried Fedora and Manjaro (Arch based) with minimal success. Either linker problems, or a very strange version conflict on Fedora. After making this thread, a friend suggested trying Ubuntu, and it seems to be the best option, or the least effort, though IDE plugins are so far still a problem. But I think I'll just give up on that, unless other tips from this thread don't work either. Stack seemed to work well, until I tried to build the project, where it gave an error, which I've since forgot. Since the Ubuntu install seems to mostly work out of the box with platform, I'll probably focus on that, and see if I can add some IDE comforts on top of that. Thanks for the tips!
Also, a partial applicated function isn't anything special for the language. Practically, there is a difference, but in theory, they are all just functions.
/u/nomeata might get a kick out of this.
Just tried it, and it works great, as long as GHCi is on PATH, I suspect. As a side-note, installing just ghc-simple didn't also bring in the syntax highlighting plugin, and some features weren't working. Installing said plugin fixed all issues. Thanks a lot!
&gt;instead do a global install but this will break and you'll hate your life. Yup, that sums up about everything I've went through so far. I did what you suggested, and it works pretty great so far. Thanks a lot!
I tried those earlier, but had problems with intero, on which both rely. I'm on Manjaro (Arch based), and had trouble getting the ncurses dependency working properly, I suspect. While Arch is amazingly nice for Java development (can easily have multiple JDKs installed at once, without issues, unlike Fedora/Debian), it seems it's not so easy to get Haskell up and running.
I'm a full-time dev, albeit not Haskell. (Mostly Java, Python, and C for our company.) I use vim for almost everything. Haskell (and even Agda) work fine for me. I don't remember anything to install anything for Haskell syntax highlighting; agda-vim automatically reloads the agda syntax vim file generated each time I reload. I don't really use code completion that much for anything, but I have deoplete installed and occasionally use it.
Sorry, I'll summarize in pseudo-C: void setup_haskell_editor() { if installed("hie") { if (use emacs) { emacs-use-package("haskell-mode"); // and config emacs-use-package("lsp-mode"); // and config emacs-use-package("lsp-ui"); // and config } else if (use vscode) { vscode-install("common-haskell-plugin"); vscode-install("hie-plugin"); } } else if using("yi") { configure-and-use("yi-haskell-mode"); } else if using("leksah") { configure-and-use("leksah"); // leksah is a Haskell IDE } else { if (use emacs) { emacs-use-package("haskell-mode"); //and config emacs-use-package("dante"); //and config } else { if installed("ghcid") { vscode-or-neovim-install("ghcid-plugin"); } else { editor-of-choice("with-ghci-or-ghcmod"); } } } } If you have a preference for an editor, let us know and we can provide directions. Otherwise, if you just want to hack on Haskell and don't care about a specific editor, it's probably easiest to get started with VSCode. But if you care about resource overhead of an editor, then I'd suggest a vim or emacs configuration. I have most experience with Emacs and rely on org-mode, so...
That's an interesting use case. What about: ``` haskell inRanges (invert [SpanRange 1 10]) 15 ``` Would that work?
Just like `map not`? Since intuitively parentheses are optional. 
This is all way more sophisticated that anything I had in mind back then - I just wanted to solve the `map NewTypeConstructor`-problem. But glad it paved the way!
`map not` is not the same kind of construct as map $ not or map . not. Assuming that map wasn't of the form (a-&gt;b) -&gt; a -&gt; a. But rather some kind of (a-&gt;b)-&gt;a.
Hi, /u/mavavilj. Your question is too simple to be posted as a full topic. Please rather ask it under the Hask Anything post that's pinned to the top of this subreddit? Thanks. 😊
You can even go further and have things like: let banned = List/map { function: Text, module : Text } HLintRule ban [ { function = "head", module = "Prelude" } ] in ... Really once you have functions and bindings, you start looking at configuration very differently.
sorry to hear - I'm sure someone will help you with that issue (I've never used Arch but it's probably no big deal to install the right ncurses or something compatible)
Absolutely. If you find out his payment info and the one of the haskell-ide-engine guys, then I'll donate 4 beers to each. I raised an issue to make the vscode-ghc-simple depend on the syntax highlighting plugin, should be an easy improvement. Maybe we can see if we can improve the docs a little, to mention the need for ghci in PATH for example, and open a pull request. I also want to point you to this: http://www.parsonsmatt.org/2018/05/19/ghcid_for_the_win.html People aren't entirely wrong when they praise the effectiveness of surprisingly minimalist tools. Even if proper IDE integration is important too.
[Here](https://gist.github.com/georgewsinger/254a18d03f125bd7e87bbcc538f1741e) is a link to the full cabal file.
A quick tour into Dhall as a functional programming language :) I'll be working on my next post where I discuss Yoneda in Dhall soon!
&gt; f (g(x))= f . g x = f $ g $ x, right? No. (f . g) x = f (g x) -- By definition (see Prelude) f $ x = f x -- By defintion (see Prelude) f $ g $ x = f $ (g $ x) -- By fixity declaration (see Prelude) f $ (g $ x) = f (g $ x) = f (g x) -- Applying $ (outer first) f $ (g $ x) = f $ (g x) = f (g x) -- Applying $ (inner first) f . g x = \y -&gt; f (g x y) -- Eta expansion, then applying . Note that `(.)` and `($)` are *not* keywords or built-ins, and don't get special treatment by the language[1]. They are simply operators defined in the Prelude. [1] GHC defines `($)` with a more general type, IIRC, so that you can `runST $ do {- ST-Monad stuff -}`.
Yes, but it is dependent on ghc-mod/cabal-helper support, and this is being added, I understand. I do not have an expected timeline.
Dhall is so HOT right now. 
Sure, although it's also very easy to make it a non-breaking change by re-exporting one from the other.
the file looks fine. does `cabal new-build all` work? idk how `stack` works with newer cabal features, like `foreign-library`. also, i'd try running `cabal check` in the directory with the cabal file.
also, try including "src" in an unconditional field, while the other two are still conditional (i.e. three `hs-source-dirs` total). maybe stack isn't parsing the file correctly. relatedly, try building individual components. like `a stack build lib:..." (or whatever the syntax is). I'm not sure what works and what doesn't (the issue remains a bit under specified).
Since values are linear (i.e., they can't exist in two places at the same time), as soon as you apply a function to something, and that function does not use that thing, then that thing can be safely collected. It is, thus, bound to a special node that incrementally erases it from memory, while the rest of the program executes in parallel. There is no need for a world-pausing "mark-n-sweep-collecting" pass. Data is freed as soon as it becomes unreachable, in the same way Rust can automatically free data that goes out of scope. 
What is/was the `map NewTypeConstructor` problem?
It would help if you can try to explain where this intuition comes from. Perhaps it comes from the fact that `f . g $ x` means the same thing as `f $ g $ x`. If so, I would understand why this is confusing. But don't let yourself be fooled! The reason these expressions mean the same thing is a bit deeper. Resolving the precedence makes it clear that there's something rather *different* going on in each case: `f . g $ x` parses as `(f . g) $ x`, but `f $ g $ x` parses as `f $ (g $ x)`. So even though these expressions differ in only one character, their whole structure is different. * `f . g $ x` parses as `(f . g) $ x` because `.` has (much) higher precedence than `$`. That, in turn, is the same as `(f . g) x`, since `$` is just function application, which you can also get by juxtaposing the expressions with no operator. That, in turn, is the same as `f (g x)` because that's what function composition *means*. * `f $ g $ x` parses as `f $ (g $ x)` because `$` is right-associative. Now both `$` operators are just function application - the same as juxtaposing two terms again - so that's the same as `f (g x)`. You get to the same place, but by very different paths. So don't let this coincidence fool you into thinking that `$` and `.` are the same.
People like to say that `newtype`s are "zero-cost" because given `newtype N = N T`, a value of type `N` has the exact same run-time representation as a value of type `T`. But this zero-cost claim isn't quite true, because functions like `map N :: [T] -&gt; [N]` will actually end up allocating a copy of their argument. This is explained nicely in the introduction to nomeata's paper here: https://www.seas.upenn.edu/~sweirich/papers/coercible-extended.pdf I wrote a bit about this problem in the documentation for the `roles` package, too: http://hackage.haskell.org/package/roles-0.2.0.0
The idea is more "It is better to develop a workflow that only uses reliable tooling than to use features that are flaky and unreliable," not that "Autocomplete isn't useful."
I don't mind Dhall. Having all configurations in Dhall would be really nice because different Dhall configurations play really nice with each other. For example, if you're using this approach with Dhall for generating HLint rules, you can fetch `default-extensions` from your `.cabal` file if you're also using `dhall-to-cabal`: * https://kowainik.github.io/posts/2018-09-09-dhall-to-hlint.html Though, specifying `cabal` configuration with Dhall might be a little bit verbose. * https://twitter.com/jyothsnasrin/status/1037035845125988352 I think, unless you have a really big multi-package projects with a lot of configuration duplication, then it's not worht it enough to specify `.cabal` via Dhall.
I am not sure whether it is a coincidence but daal to an Indian is Lentils.
I don't find the latter example particularly verbose? Seems about the same as a basic cabal file. (I'm obviously assuming that all the data type definitions, etc. would be supplied by Cabal itself.)
Because they are different functions: infixr 9 . (.) :: (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c g . f = \x -&gt; g (f x) infixr 0 $ ($) :: (a -&gt; b) -&gt; a -&gt; b f $ x = f x As you can see, their arguments are not the same at all. (.) takes two functions, and creates a new function that chains them together. Meanwhile, ($) looks like it shouldn't do anything at all, until you notice the infixr declaration, and remember that normal function application is left-associative: `a b c` parses as `(a b) c`, while `a $ b $ c` parses as `a $ (b $ c)`. Additionally, the low precedence of ($) and high precedence of function application, means that `a $ b c` parses as `a $ (b c)`. So ($) is mostly a precedence/associativity trick, while (.) actually creates a new function out of the two it's given as parameters. 
Looks like recent blog post about Dhall to HLint came out just in time ;) * https://kowainik.github.io/posts/2018-09-09-dhall-to-hlint.html I think it's much easier to follow what's written there after reading this helpful introduction to Dhall. Thanks a lot to /u/fintanh!
That's what I was going for in the title :D
Can the pattern synomym trick be extended to match only one of the argument so you can write something along &amp;#x200B; case T2 (Just 5) "hello" of T2 Nothing (a:as) -&gt; \_ T2'1 (Just x) -&gt; \_ &amp;#x200B; Using nested tuples, we could even pattern on \`car\`, \`cdar\` etc ...
Would you really write this in real life over a case on tuple ? If so why ?
Where's the code? :)
&gt; I would rather just have the RawJSON and be able to write a bunch of if statements [..] &gt; The thing is, in JSON everything can be missing. [..] Do you put a maybe everywhere? It’s not as easy a solution as saying, “That’s what maybe is for.” The author's alternative to putting a `Maybe` everywhere seems to be to put an `if` everywhere. When dealing with external data you always have a choice - normalise it into some form that you use elsewhere, or just deal with it as it already exists. The first option is generally preferred even within dynamically typed languages, as it limits the exposure of your application to external api changes to just one area. So I don't accept that "a bunch of if statements" is a good solution even in a language like Javascript. &gt; Otherwise, I would just be typing all the time, writing types, modifying types. Putting aside the fact that working with raw JSON as `Value` is entirely possible, I think there is another issue here that is more subjective. Personally I would much prefer to spend my time "typing all the time", as I consider types to be the core of my application. I would much prefer to write an awkward series of transformations that end up with a coherent ADT which I can then operate on, rather than a bunch of if statements that will never approach anything like coherence spread out all over my program. In fact, the argument about apis changing to me seems like an argument for typing. There isn't some magic in a set of `if` statements that will make it resilient to api changes, and for me, it seems less maintainable. Because with a typed approach, at least once the dirty business of transformation is done, I have compiler guarantees about everything else (which would probably be the part of the program I am actually interested in). Whereas with raw JSON you never end up with any guarantees about anything - every single piece of code that touches the JSON has to handle existence/type failures in everything it does. To be fair, I would definitely have agreed with this article when I first started learning Haskell, when I viewed types as 'getting in the way' of programming, rather than being central to it. So if nothing else the author's perspective does expose the difficulty people have transitioning into Haskell from less type rich languages. 
&gt; There are things that you know that cannot be expressed in the type system. There are things that are not easily expressible, or maybe they are expressible but the type would be 14 pages long. These are real concerns. Generally speaking, true, but I just don't buy this at all with reference to JSON interpretation. I'd like to see a real world example to back it up. My guess is that any JSON schema that is deterministic, and can be handled by `if` statements, can be modelled by a series of product and sum types in a fairly straightforward way, and that it is possible to prove this to be the case.
Never using emacs before I tried to install dante but spent the whole weekend (20 hours or so) trying to learn enough emacs to get the config init file thing to work. In the end nothing worked and ended up incredibly frustrated due to the million different ways people code up their lisp config. In the end HIE and vscode is the simplest to get started 
I would be willing to learn emacs, it just seems anything I try do in it doesn't work and no tutorials online explain it in a way that makes it accessible to someone new. Sure I could try copy and paste someones config, but I absolutely hate not knowing what it does and its just putting a bandaid over a larger problem. To install dante you need to know alot about emacs already. The instructions on the github don't tell you half of what you need to know as it doesn't work out the box for me. For example if they actually had how to setup whatever other haskell plugins are needed it would help alot! I had to look at other peoples config which was completely different to the official githubs which worked but then had all these other config problems. Anyway, I do agree, for getting started with haskell VSCode suits me fine for the moment since I can just focus on haskell, not crazy emacs config.
Emacs is like Haskell in that it requires an investment, but it will definitely pay off. From experience I can say if you set out to learn Emacs around the next winter holiday season, it should be enough time to "waste" a week to get you productive. But you don't need Emacs for Haskell. Emacs is powerful because of its design, scriptability and because stuff you configured 15 years ago will work today, with the exception of a few deprecations, but those are super rare. The biggest change I did to my Emacs config over the years is moving to use-package, everything else has stayed the same for at least 20 years. Same is true for things like XMonad(not 20 years obviously) and FVWM. My FVWM config started in 1994 and it still works.
I recently wrote this up as a product-profunctors example :) https://github.com/tomjaguarpaw/product-profunctors/blob/master/Data/Profunctor/Product/Examples.hs#L62 Perhaps it's a little different from what you wanted because it returns `Nothing` when the length of the list and the length of the tuple do not match. It sounds like you want a type error (which may be impossible).
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [tomjaguarpaw/product-profunctors/.../**Examples.hs#L62** (master → 664c7aa)](https://github.com/tomjaguarpaw/product-profunctors/blob/664c7aa56a4d21b446330ffc0bead3cbade42580/Data/Profunctor/Product/Examples.hs#L62) ---- 
 map . not = \x -&gt; map (not x) map $ not = (map) (not) = map not 
If you were already well versed in mathematical logic, then there's are shortcuts you could take to learn some aspects of Haskell from there. I honestly wouldn't recommend studying logic just for that however. If anything, you're more likely to end up doing the opposite, using Haskell as a starting point to learn bits of logic.
This. I learned to "feel" Haskell first, then I bothered to learn some of the underlying theory.
Does this actually solve the map newtype problem? It seems like it still doesn't work in the polymorpic case or at least we still need to add a special method to Functor". I'd be really happy to learn it's already solved
By "logic" do you mean formal philosophical/mathematical logic courses (did one of them in undergrad by don't remember much)? If that's what you mean, then neither Lambda Calculus nor Logic is required. However, strong logical reasoning and analytical ability is required to become a good programmer.
Does *knowing* the (typed) lambda-calculus (or calculi) help with learning Haskell? Yes. Is knowing the lambda-calculus (or calculi) a requirement/prerequisite to learn Haskell? No. Is "learning the lambda-calculus and logic" (whatever that means) an efficient way to learn Haskell? Probably not.
Currently ant Haskell lecture 2 and there was only two simple concepts from logic and lambda calculus and they are unnecessary to code in Haskell. A little bit easier to understand but not worth time investment. Better just watch haskell tutorilas 
IMHO any configuration language that expects of the user to optimize configurations written in it using Yoneda lemma (or optimize, period) is plain wrong.
Lambda calculus is really helpful if you want to read papers about haskell's type system extension. But that's pretty irrelevant to learning haskell.
&gt; Completion is less significant in Haskell because you don't have the class.accessor pattern where the class gives you a very clear scope of potential symbols. I would argue that having smart embeddid completion is MORE important due to this, not less since there's no way to get the list of functions applicable to a type without tools.
Running `cabal check` lead to an "undefined flag" error, which led to me fixing the problem (see my post for an edited solution). Thanks for your help.
I also have an arts background and I came to Haskell from PHP and Javascript. The only theoretical material I've encountered so far that seemed productive to learn is category theory, just because a lot of documentation and libraries reference it. But even with that I wouldn't recommend studying it in advance - focus on code and you'll naturally run into areas that you can explore the theory of when they become relevant to you.
Well the use case for how we use Dhall is a bit more complicated than regular configuration. We essentially have a DSL configuration and we needed to optimise away some nested \`fmap\`s to get a smaller output. It's just as much a functional language as it is a configuration language.
how does this compare to \`interval\`?
I initially read this as “Dogs learning Lambda calculus and Logic help with learning Haskell.”, which has a rather different meaning. :)
While I liked the video, I don't think this is the correct forum. /r/Linux or a perl forum or even /r/LateStageCapitalism would be better.
Hi, author here. Thanks for using this extension. 1. As for depending on Haskell Syntax Highlighting, I just realized that my extension actually depend on it *for identifying Haskell files*, and [issue #8](https://github.com/dramforever/vscode-ghc-simple/issues/8) tracks this problem. 2. As for requiring GHCi on `PATH`, my extension currently assumes that `stack exec ghci` works out of the box. If this is not how things work for you, or `stack exec ghci` works for you but not for my extension, then just file an issue describing your setup and I'll see what I can do. Also, if your project has `stack.yaml` it will try 3. As for the beer, I think you can just direct that to haskell-ide-engine and donate, in total, 8 beers to them. It has more people working on it after all :). Also I've just started university and I'm not quite in the spirit of diving into code-I-wrote-a-month-ago yet, so you might need to wait a bit :). Hope you partially enjoy the suboptimal experience in the meantime.
so what plugin does emacs use. by default it do not support haskell too. Maybe we can port a vim plugin.
We need something like [dante](https://github.com/jyp/dante) It works without any dependencies by using ghci. 
That make sense. Thanks.
`QuantifiedConstraints` gives `Representational` a much better story too, which is already widely recognized: class (forall a b. Coercible a b =&gt; Coercible (f a) (f b)) =&gt; Representational f instance (forall a b. Coercible a b =&gt; Coercible (f a) (f b)) =&gt; Representational f No manual lifting required.
Not everything tho, JS supports functional programming at least a bit
But HIE uses a branch of ghcmod that works with 8.4.3 and also a similar branch of HaRe for refactoring support, and so on. That means, I would expect ghcmod to have a 8.4-compatible release.
This week's lsp-mode and hie work quite well, and if they don't you can fall back on dante for basic type checker feedback. Editing is in all cases supported by haskell-mode kinda like haskell-vim I think.
I started learning to code with Free Code Camp and JavaScript but found the language just didn’t click with me (nor did FCC seem to teach it that well). I ended up discovering The Haskell Book and self-teaching from that with no other Haskell or programming background and that resource alone was good enough to learn the language competently. In fact, it led me to a full-time job writing PureScript! The shorter version: all you need is one high quality resource like The Haskell Book to self-teach the language. Other things like the typed lambda calculus or category theory are nice enough but not necessary.
[JavaScript: Understanding the Weird Parts](https://www.udemy.com/understand-javascript/) is an absolute treasure! I just finished going back over his section on Functional programming and his Part 2 is basically just a pitch to read through Underscore's source code and use it and really let this stuff sink in.
Personally speaking, Category Theory has improved my Haskell understanding a lot. Knowing the motivations behind the patterns helps learn them and have them available for real problem solving. CT seems intimidating and sometimes suggesting its usefulness may be seen as annoying (from my experience), however I really think it describes what is going on in pure FP languages very well (if not *exactly* but that's a diff. topic)
GHC defines `$` with a levity-polymorphic type, but that's not what makes it work with `runST`. There is magic built into GHC that allows `$` to work with `runST`.
Hunh. I thought we dropped that special treatment in 8.x. TIL.
I'm having a lot of success with vim, ghcid, hoogle and stack. You can get tags straight from the repl (:ctags) for native navigation in vim. Stack can also build haddocks for dependencies. I also run poor man's CI with git hooks to run various tests and tools (hlint, weeder, documentation etc) This setup feels more UNIX and actually doesn't break when a plugin gets a minor version bump. YMMV
There is no such thing as "the full list of functions applicable to a type". I mean, there is theoretically but it's nowhere close to what it is in OO and that's not at all how I think about writing Haskell code. In OO languages, the dot operator has a very specific narrowing effect. You have defined a specific class as your scope and you are looking for a specific member of that class. In Haskell we don't have that. It's generalized function application, and that is not limited to narrowing operations. Let's for a moment simplify things by assuming that whitespace is not function application and that Haskell's `&amp;` operator (`x &amp; f = f x`) is the only thing we have. (Side note: if you're using the `&amp;` operator in the first place it probably means you're probably too attached to the OO paradigm and your code is likely not idiomatic Haskell. I'm only using it here as a thought experiment.) When I type `foo &amp;`, I am not just restricted to fields of the `Foo` data type. I have access to any function that I can pass `Foo` to. This list is ENORMOUS. It includes anything kind of structure that I can wrap a `Foo` in: `[]`, `Maybe`, `Either`, any of the tuple types, `Map.lookup`, `Map,insert`, `Set.member`, `Set,insert`, the list goes on and on. It's just not helpful to have a completion box that includes 80+% of all the functions in scope. ADDENDUM: When I showed this to a coworker they said "'the full list of functions applicable to a type'??? This person doesn't even understand what types are...or how they function." Later in the conversation, we had this exchange: Me: He has a `Foo`. He thinks he want to see the universe of functions that he can apply to a `Foo`. Coworker: He doesn't. It's meaningless. Because every single polymorphic function takes a Foo. I'm sure my coworker's reaction sounds a bit harsh. I'm not trying to slam you or make you feel bad. I'm relating this exchange because I think it gives you a better picture of how different Haskell is from what you're used to. Keep at it. It's a big mental shift, but it's very rewarding and way more productive. Just today I had lunch with someone who was telling me a story about how they had a project that had been going for 7 months and was mired on problems with a hard deadline bearing down on them. He convinced them to let him rewrite the whole thing in Haskell and had it done in less than a month.
Oh, I wasn't trying to rude at all! It was intended to be a wry statement about my own lack of knowledge, not about your writing :) 
Thanks, I will look in to it.
yeah, I kown ale, in spacevim we are using neomake. neomake also can async run hie. 
thanks, do you have any online guide for develop haskell in unix? 
If getting good at functional programming is your goal, I would suggest exploring Elm lang, it’s very beginner friendly with some good community. Syntax is inspired from Haskell.
Shameless plug for my preferred setup (which, crucially for me, supports navigate to library source code inside the IDE) https://gist.github.com/androidfred/a2bef54310c847f263343c529d32acd8 
Watched the latest video from the channel [here](https://www.twitch.tv/videos/308930805##). Some very cool stuff. Github link featured in the video [here](https://github.com/ocharles/zero-to-quake-3). I'm definitely interested in seeing future stuff from this project, I'd love to see some more games programmed in haskell.
Been watching this and I'm blown away.
I'm a logician who's toyed with a few programming languages, and picking up Haskell has been much easier for me than any other language I've tried (and afaict also easier than it is for many people with considerably more programming experience), exactly because it lets me use my familiarity and comfort with logic. Whether it's worth your time to study logic on the way to learning Haskell I don't know. But I can tell you from my own experience that if you just happened to know a bit about intuitionistic natural deduction and normalization anyhow, it would totally help. The first few chapters of Prawitz's book _Natural Deduction_ are a good source if you're interested.
What tool is he using for managing application windows? I am impressed... Like tmux for GUI applications? 
Perhaps xmonad?
You are right. Thanks.
Deal breakers are like the Either monad with short-circuiting behavior. If you keep continuing while simultaneously complaining, it's more like the These monad.
Maybe you will consider this answer as a joke, but this is a serious one. https://youtu.be/ycxqIGBYMoM
You mean inspecting a container without opening it? Neat, but I wish the container analogy would go away.
Fascinating. I recall seeing similar videos for sorting baby chicks, which was, erm, less wholesome.
Oh yeah I've seen that too. Not what I want to associate with Monads. Although it almost feels like there's a rubber-duck debugging joke in here somewhere. Nah...
They don't even *lift* the cans!
Uh, like a deal breaker in some kind of situation. (Here goes a terrible example...) Like say you're married and your spouse runs away with someone else then you'll have a divorce. If they are abusive towards your kids, then you'll have a divorce. Deal breakers. So -- yes, I realize this is not very realistic but you get the idea data Marriage a = Divorced e | Happy a Some people violate the monad laws by getting divorced and then remarried... 😅
wat not trying to be mean, I just don't get it, can you give it another shot with a different dealbreaker example?
I think it started with youtube playlists but the list parser works for lots of other sites too like twitch, vimeo, soundcloud... 
Oh wow, I never considered I could use \`where\` inline in functions...
As someone who works on a very large project that uses Nix quite extensively, my advise would be to be very, very careful. IMO, Nix is a siren song that promises a lot but turns out to be a huge pain in the neck if people blindly follow the "Nix is obvious answer" path. In addition, Nix is at as viral as the GNU GPL, and at least as difficult to get rid of. 
`Object` is a HashMap - it's all value level. `SomeJSON` would wrap a complex type representing whatever JSON was parsed and have an accompanying type class dictionary to discover properties of it. This would let you, for example, write code that works on JSON objects that have a key guaranteed at the *type* level, performing only as few checks as necessary to go from a wholly generic JSON object to a JSON object with statically known properties. That is, you might have something like `key :: forall k a v.(KnownJSON a, KnownSymbol k, Key k a ~ b) =&gt; a -&gt; b` used as `key @"somekey" some_jason == value_of_somekey` or perhaps `check :: (KnownJSON a, KnownJSON b) =&gt; a -&gt; Maybe b` used as `check some_jason :: Maybe (JSON ["num" := String, "key" := JSON Obj])` There are no instances declared or derived here, and no custom data type. If you can't really rely on the JSON data, this could be easier to work with, or also not, it depends.
SomeJSON is essentially the same as what Object already is, isn't it? This is like how SomeNat from GHC.TypeLits is isomorphic toS Natural from Numeric.Natural, and how SomeSymbol is isormophix to Text/String, or how `SomeSing a` from singletons is isomorphic to `a`. Working with an existential type under KnownJSON would be identical to working with just an Object. Unless you are talking about the actual kind of the KnownJSON itself; it looks a lot like row types and row polymorphism, similar to what we kind of have in libs like vinyl?
You can use `stack build --copy-compiler-tool ...` command to build and install any compiler-related tools (such as `hlint` and `ghc-mod`). Resulting binaries will not be installed globally but will be placed near the particular GHC version (and will be available using `stack exec` in any project that uses the same version of compiler). This way works fine with more than one version of GHC installed on the same machine (but you will need to install the tools for each version of compiler).
IIUC you weren't the one who set up nix for the project your are working on. Could you tell us some of most salient pain-in-the-neck aspect of nix?
You can move the `$` to the last parameter to get what you (might) want: ``` freqSort s = concat . sortBy (comparing length) $ (group . sort $ s) ``` this way you avoid the value `sortBy x y`, but get the function `sortBy x` which can be composed.
How about Qubits? They are like information stored in the Quantum Monad.
Deal breakers are fatal/unrecoverable issues you have with something.
any suggestions ?
I've found the cause! &lt;https://github.com/chrra/iCalendar/pull/30&gt; Thanks!
First of all, your \`treeFold\` definition is syntactically invalid: You have two equal signs in there, you are pattern matching on constructors not applied to arguments (e.g. JLeaf and JNode), you have written \`tree a\` instead of \`JTree a\`, ….. Secondly, the type of \`treeFold\` looks like you have copied the type from a fold function for a tree defined by \`data Tree a = Leaf a | Node (Tree a) (Tree a)\`. While you can implement a fold function of the type you have given, it necessarily looses structure, e.g., there is no way to differentiate between \`JLeaf x (Just y)\` and \`JNode Nothing (JLeaf x Nothing) (JLeaf y Nothing)\`. A more natural type for the fold function here would be \`treeFold :: (a -&gt; Maybe a -&gt; b) -&gt; (Maybe a -&gt; b -&gt; b -&gt; b) -&gt; JTree a -&gt; b\`. Finally, you can change the type of \`mTreeFilter\` to \`mTreeFilter :: (a -&gt; Bool) -&gt; JTree a -&gt; Maybe (JTree a)\` to handle the case where there are no elements left. You will also have to revisit the actual implementation (currently it operates on lists for some reason) but I think it makes more sense to do that once you’ve addressed the other points.
Thanks for the support, all! I'll keep plodding on. We've still got a long way to go, but things are starting to take shape :)
&gt;data JTree a = JLeaf a (Maybe a) | JNode (Maybe a) (JTree a) (JTree a) &amp;#x200B; can I redefine by use Just a ? 
I am not sure what you mean by that. \`Just\` is a data constructor not a type constructor. You need a type here.
because I see maybe can be (Nothing or Just ). &amp;#x200B;
If you always want the value to be present, then just remove the \`Maybe\` part and define the type as ```data JTree a = JLeaf a a | JNode a (JTree a) (JRree a)```
I can't help but chuckle over the irony of stack making the worst part of getting into haskell even worse.
Is this for graphics only or do yo plan to cover the game logic also?
&gt; It's just not helpful to have a completion box that includes 80+% of all the functions in scope. It's **incredibly** helpful. That's how it is in Scala, due to autocompletion also showing all the extension methods, they're just ranked with direct members and members with return types closer to the current expression being higher in the list. There's zero issues with doing the same in Haskell i.e. ranking applicable functions higher if they were imported from the same module as the data type, and ranking them higher if they more precisely fill the current hole – ghc does the latter already for explicit holes! &gt; Coworker: He doesn't. It's meaningless. Because every single polymorphic function takes a Foo. Except for all the monomorphic and constrained functions – which are the interesting ones. And if further ranked by the type of outer expression, the output will definitely be useful. If it's so, so meaningless to look at potentially everything in scope – why were typed holes added? &gt; I'm relating this exchange because I think it gives you a better picture of how different Haskell is from what you're used to. Keep at it. It's a big mental shift, but it's very rewarding and way more productive. That's a very nice assumption, but completely wrong. The shift happened in the other direction. I've learnt Haskell in 2011 as my first serious PL and has been shipping code in it for years. I've learnt Scala as my first OO language just a year ago, and the dot operator has been an absolutely insane productivity boost. When I was just starting with Scala, I've asked a coworker about reading javadocs – he said that he's never read any and never seen anyone read javadocs. I thought he was crazy because there was no way to do anything without haddocks in the Haskell world. But turns out, the view-library-source + dot completion features in IDE are enough to completely obviate the need to ever look at autogenerated documentation. So, no, I don't think that way is more productive, I don't think that scouring hackage/hoogle/haddocks is more productive than having all the information right in the IDE and having all the interesting functions immediately reachable via dot. &gt; (Side note: if you're using the &amp; operator in the first place it probably means you're probably too attached to the OO paradigm and your code is likely not idiomatic Haskell. I'm only using it here as a thought experiment.) Your pretentiousness is misplaced. Haskell is not god's gift to the world, it's just a view that gives less thought to modules (including first-class), than they deserve. 
Suppose I don't generally watch videos. Could someone describe, in text, what this is about? (In a bit more details than "Quake 3 in haskell" maybe.)
Yes, please! I also didn't find the very first video of these series. The earliest on with "basic setup" in title already suggests that there was another one before it... Did it expire?
I think using \`.extend\` is more natural than using \`composeExtensions\` and \`overrides\`.
It was unfortunately never recorded. I didn't realise you had to turn archiving on explicitly in Twitch. That one is lost for good I think!
I plan to go as far as I can. We'll see when burn out hits :)
We can’t even think of real world examples of lambda calculus! Sometimes concepts don’t apply to every day worldly examples. Bret Victor produced this idea of [“crocodiles and eggs”](http://worrydream.com/AlligatorEggs/) to model lambda calculus, except it leads to the absurd case of an egg hatching and producing another egg. At this point I would’ve given up on the analogy, but I suppose it serves as a good example of trying and failing to fit mathematical concepts into real world analogies. Sometimes they fit, other times it just doesn’t make sense. Something like quantum theory or the more abstract science fields might contain analogous things to other mathematical ideas but by that point you’re probably not “real world” in the typical sense of the world by that point.
Whoa... \`:ctags\`?! I never knew about this - I wonder how that would play with \`ghcid\` triggering a regen.
Does anyone use Nix for Haskell on OSX and how do you find it or what are your tips? I ask because I use NixOS on the server and find it great for that, I also use nix package manager on OSX with some success. I like the design principals behind NixOS etc etc. However the past weekend I tried to look at a Haskell project that has a Nix derivation that got out of date. I ended up going down a rabbit whole the entire weekend debugging the projects Nix derivation, then stack2nix which was used to generate the original derivation but stack2nix was failing to build so I had to debug that etc etc. Each change debugging resulted in hours of compilation as it rebuilt everything from source. At the end it felt like building Haskell on OSX still needs work, i had a bad experience or i'm doing something wrong.
Just so you know, a standard way to ease that burden is using a single newtype with a phantom type parameter and a bunch of type synonyms. Easiest way I've found is to use `Symbol` for the phantom with the type name as the string. newtype Wrapped (s :: Symbol) a = Wrapped a deriving (Show, Eq,...) type CustomerId = Wrapped "CustomerId" Int You can find one implementation in the [tagged](http://hackage.haskell.org/package/tagged) package, but I've found it preferable to define our own to avoid a bunch of orphan instances. I like your idea of having deriving be smart enough to see into a custom constraint and provide the instances if it knows how to implement the constituents, but I don't have a strong view on if its a good idea.
In macOS I use https://github.com/koekeishiya/chunkwm
I wish there was a formal treatment on this
ok, I opened an issue for that very important point 3: https://github.com/haskell/haskell-ide-engine/issues/821
I've never seen `.-` before. Maybe you're thinking of `.~` from the lens package, where it changes a value nested deeply within another data type?
There are [a few definitions](https://hayoo.fh-wedel.de/?query=.-) found using hayoo.
*Disclaimer: I am not a huge specialist with Nix (nor Haskell), and I never did the process all the way. It is my plan in case I will find myself in your shoes.* *Are there any huge differences for Nix or Haskell on OSX?* It seems to me that source of the problem is that Nix uses own snapshots of Hackage, not Stackage ones. AFAIC derivations generated by cabal2nix use Cabal under the hood. Although you can use Stack alongside Nix, you can't use it to build derivations. My guess in this situation is first building package with Stack. If it builds, then you can find last LTS it builds with, and then go to step two. If it does not (for example, some bindings have gone haywire), you go to step 2 immediately, with possibly lots and lots of compilation. Step two is pinning whole Nixpkgs for whole derivation to some commit which was around time of last time project was known to compile (try pin to some release branch, as it may still have binary caches lying around). If you managed to build project with stack, then date of the LTS is your reference point, if not, then it's probably date of last commit of the project. This might require a ton of time if binary caches not available. You also might want to go down the rabbit hole and pin not whole Nixpkgs, but specific Haskell packages. This might be better for future use, but ought to be painful since dependencies are usually numerous. You might want to read (in a case you did not already): * [https://nixos.wiki/wiki/FAQ/Pinning\_Nixpkgs](https://nixos.wiki/wiki/FAQ/Pinning_Nixpkgs) * [https://vaibhavsagar.com/blog/2018/05/27/quick-easy-nixpkgs-pinning/](https://vaibhavsagar.com/blog/2018/05/27/quick-easy-nixpkgs-pinning/) * [https://github.com/NixOS/nixpkgs/issues/9682](https://github.com/NixOS/nixpkgs/issues/9682)
Why do you need pattern synomym and just not use \`(,)\`. &amp;#x200B; case (,) (Just 5) "hello" of (,) Nothing (a:as) -&gt; \_ (,) (Just x) \_ -&gt; \_ &amp;#x200B; seems to work ...
\`cabal2nix\` doesn't use \`Cabal\` apart from to parse \`.cabal\` files. The nix package set is based on a LTS as well but also includes all of the packages on hackage.
My main job is in JS and I advocate for a functional style on my team but it's not the same. You can perform IO anywhere in a JS program which sets you up with that expectation when working in Haskell. I believe it has something to do with how our brains work. When you get to Haskell and adding logging is not as simple as importing a module and adding a few logging statements -- things can get a touch frustrating. Then you get pointed to mtl and suddenly have to restructure everything you wrote to fit into a giant monad stack and have no idea what's going on or why it's so much work compared to JS. ... and that's where the rub is: comparing everything to the first language you learned. My advice for avoiding that is to resist the temptation to believe you know anything about programming when you're learning Haskell. It's not like JS, C, Python, Ruby, C++, Java, etc, etc, etc. Those languages have threads of things in common. Haskell has none of it.
Not `cabal2nix`, but Nix expression it generates uses Cabal, no?
I agree, but I still don't think it's worth using. The issue with using \`.extend\` is that as soon as you do, you lose the ability to perform \`overrides\` afterwards. In addition, it seems like \`.extend\` can't edit everything that \`override\` can. To me, this is a big deal when working in a functional language, because it means you have to dig into how your input was defined to make sure you're allowed to perform certain operations, possibly needing to change the input definition to allow what you need to change. To steal a phrase from [this ticket](https://github.com/NixOS/nixpkgs/issues/26561), "extend is fundamentally worse, but ergonomically much better". 
If we're going to talk about physical monads, we had better have a physical category that we can put all of this in! How about this: Let's take the category where the objects are physical locations on the surface of Earth (excluding the north pole, to make some other stuff easier later on), and there is an arrow `p : A -&gt; B` for every path connecting location `A` to location `B`. You can compose a path `A -&gt; B` with a path `B -&gt; C` by concatenation. Now go out into the desert and draw a map of the earth, minus the north pole; it doesn't matter if distances are stretched or distorted. Given a physical location `A`, let's call the point on the map that represents that location `M A`. Also note that for every path `p : A -&gt; B` there is a corresponding path on the map `M p : M A -&gt; M B`, and if we concatenate two paths on Earth and then look at the mapped path, that's the same as if we mapped both paths first and then concatenate them. In other words, our map gives us a functor `M`. In fact, `M` is an endofunctor, because the map is drawn on the Earth itself! Is `M` a monad? To make things a little easier, let's first show that it has `pure :: a -&gt; m a` and `join :: m (m a) -&gt; m a`. Start with `pure`. We actually have to be a little bit careful about what the arrow means here: we can't just give a procedure for turning a location `A` into a mapped location `M A`. Instead, we have to give an arrow in our category: a physical path from `A` to `M A`. Let's just agree to use the shortest path that does not pass through the north pole. And that's our `pure`: the shortest north-pole-avoiding path from a point to it's representation on the map. To be unambiguous, let's say if there is more than one such path we will prefer the most southern one. What about `join`? Let's first think about what `M (M A)` is. That's the image on the map of a mapped point(!). So we can just do the same trick: `join` will be the shortest, most southern path from the doubly-mapped point `M (M A)` to its original point `M A` on the map. What if we wanted to think about `(&gt;&gt;=)` instead of `join`? In Haskell, we'd type that as `(&gt;&gt;=) :: m a -&gt; (a -&gt; m b) -&gt; m b`, but those arrows actually shouldn't all be the same. It should be more like (switching to `(=&lt;&lt;)` here): `(=&lt;&lt;) :: (a ~&gt; m b) -&gt; (m a ~&gt; m b)`. In our example, this means that bind converts a path from `A` to the mapped image of `B` into a path from the mapped image of `A` to the mapped image of `B`. But that's easy enough: first map the whole path to get a path from `M A -&gt; M (M B)`, then follow the path from `M (M B)` back to `M B` given by `join` above. In Haskellese, that's `(f =&lt;&lt;) = join . fmap f`.
Is there a library like gloss but for webgl or canvas? Something I could use with ghcjs
&gt; There is unmerged work making the GHC eventlog useful for such cases, but the state of this work is unclear. I agree; GHC could do better here. I introduced the ability to [dump heap profiler samples](http://downloads.haskell.org/~ghc/master/users-guide//eventlog-formats.html?highlight=profiling#heap-profiler-event-log-output) to the eventlog but we still lack much of the tooling that exists for `.hp` files. GHC has had [an RTS API](https://phabricator.haskell.org/D2934) since 8.2 for streaming eventlog data out of a process. However, we still don't have a great story for conveniently exposing this functionality to the user out-of-the-box. 
Burritos, obviously (/s)
This was awesome to see even for someone like me, with no prior knowledge of graphics programming. Great job and I hope you get the time to do more of these in the coming weeks! Noob questions for other graphic programmers: what's the best resource for learning Vulkan? Also, is it easier to first grok OpenGL and then move on to Vulkan?
Are you aware of any plans to add this `Representational` to `base`?
I have a bit of experience with graphics programming, and I think you should start with (modern) OpenGL. AFAIK one of the main points of Vulkan is to give the programmer more control over when exactly things happen. That's useful when you need extreme performance, but not so much when learning the basics.
I am not. There's also a potential class (forall a b. Coercible (f a) (f b)) =&gt; Phantom f instance (forall a b. Coercible (f a) (f b)) =&gt; Phantom f But there are actually lots more, for type constructors with multiple arguments. I don't know which such things are worth giving first-class status as classes in a `QuantifiedConstraints` world.
What can `overrides` do that `extend` can't?
How is Nix any more difficult to get rid of than any other way of configuring an equivalent build? I’m not seeing it. 
Debugging CI failures is a point in Nix’s favor imo. When a, say, stack build fails on CI, it’s kind of a pain to get my developer desktop in the same configuration as the CI server (which gets configured with some bash scripts). Whereas with Nix, I can nix-build to get a repro. And to debug, I can nix-shell to run the tests in ghci, but use Nix to allow me to get my shell in the same configuration as the CI build.
&gt; For instance if Nix is used as parts of your tests that run in CI and those tests fail, now your devs need to run Nix to debug the problem. I don't understand this. This is -- quite literally -- a feature and not a bug, and it is the only reason you can do things like guarantee a random developer machine will match the environment of a production machine. Being in the same state as the system was when it failed is extremely important. If you just completely hate Nix (which plenty of people do) and can't stand it, don't want to use it, and your complaint is basically "I, or anyone, have to use Nix at all, ever, and that's bad, and I hate it" then -- sure, that's fine. But your complaint otherwise seems to boil down to "Reproducing test failures might require running the system you use to run tests"... which... seems extremely obvious and exactly what you want in most cases.
Definitely hope you'll make a permanent archive of your stream, a la handmadehero.org.
Yes, I definitely agree with you. Haskell is completely different to OO or imperative languages
How about the List monad, where let's say you have a to-do list: to make it a proper Monad, let's say doing each item on the list can result in adding more to the list (a purely static list would be more like the Functor instance). Thing is, the above example is really just describing an algorithm, and of course you'll find mappings from algorithms to monads. An actual physical thing though, I can't think of, no more than for any other abstract mathematical concept. I suppose it comes down to how you choose your categories.
To be clear, I'm specifically talking about the work it would take to implement completion that gives you "the full list of functions applicable to a type". Completion that gives you the symbols exported by a module after you type the dot after a qualified module name is easy and presents a vastly smaller list and is perfectly fine. My argument is that calculating the "the full list of functions applicable to a type" is not worth the effort. It's not much smaller than the list of all symbols in scope. I would guess it's probably no smaller than a 50% reduction at best and will be dwarfed as soon as you type the first character of said function which will reduce the list by roughly a factor of 26. &gt; Except for all the monomorphic and constrained functions – which are the interesting ones. And if further ranked by the type of outer expression, the output will definitely be useful. If it's so, so meaningless to look at potentially everything in scope – why were typed holes added? I don't agree that the monomorphic and constrained functions are the interesting ones. I think they're probably used a relatively low percentage of the time. You're comparing to Scala which I think is too large a paradigm shift to be useful. It's not pretension so much as just a matter-of-fact assessment of how different things are when you have lots of purity and polymorphism. Type holes were added because they give you an easy way to find out the type of something, which is really valuable on its own.
Of course. I'm not disputing that. I'm talking about whether the effort is worthwhile above a simple prefix filtering which with just a single character will filter out significantly more things than sophisticated type analysis.
Ohhh boy:D
This is the wrong approach to solve the problem. You want to use Esqueleto (or a raw SQL query) to perform the join in SQL. This will give you a `SqlPersistM [(Entity a, Entity b)]` (or `ConduitT () (Entity a, Entity b) SqlPersistM ()`). From there, you'll create a `[(Entity a, [Entity b])]` by bucketing `b`s that have identitcal `a`s (the SQL query will return the one-to-many things in order that this works out).
To be honest appealing to the "real world" is a red flag for me when talking about programming. There are very few things that programming is similar to in the real world.
You might find an entity-component system like [apecs](https://hackage.haskell.org/package/apecs) or [ecstasy](https://github.com/isovector/ecstasy) interesting.
My idea would be having a typeclass DamageInflucencer (bad name) with a function computeDamage taking a context of monster, weapon etc which gets called for every factor required
Esqueleto is indeed a solution, but as I am learning \`conduit\` I am still interested in how to interleave sources in conduit. Also, one drawback of using a SQL join (using raw query at least), is that let's say on row of A correspond to N row of B. You get N copies of A : you get it sent N times, have to parse it N times etc ... &amp;#x200B; I use MySql, so maybe I shouldn't bother will conduit indeed . 
For that, I think you want [`ZipConduit`](http://hackage.haskell.org/package/conduit-1.3.0.3/docs/Data-Conduit.html#g:19), which lets you write `(,) &lt;$&gt; ZipConduit yourSource &lt;*&gt; ZipConduit otherSource`.
Suppose I'm defining a bunch of newtypes, all of the form: newtype Foo = Foo Int deriving ( ...everything Int has... ) Is there a way to abbreviate the deriving clause?
I should mention that there are some caveats about equality and the monad laws for this example. Caveat emptor.
I bet it'd be fun to try and build physical things that represent abstract ideas from the programming world. &amp;#x200B;
I thought I was clever: {-# Language ConstraintKinds, GeneralizedNewtypeDeriving #-} type IntLike a = ( Eq a , Ord a , Show a , Read a , Enum a , Num a , Real a , Bounded a , Integral a ) newtype T = T Int deriving (IntLike) But apparently that's not accepted. You might need to do some `TemplateHaskell` magic.
I thought I was clever: {-# Language ConstraintKinds, GeneralizedNewtypeDeriving #-} type IntLike a = ( Eq a , Ord a , Show a , Read a , Enum a , Num a , Real a , Bounded a , Integral a ) newtype T = T Int deriving (IntLike) But apparently that's not accepted. You might need to do some `TemplateHaskell` magic.
Your question made me think of this one blog post: http://www.haskellforall.com/2012/08/the-category-design-pattern.html
&gt; Also, are there any huge differences for Nix or Haskell on OSX? Nothing prevents you from pinning the same LTS :)
This is all well and good but the question was about the difference between `overrides` and `extend`.
Arrows? You could have the b component in Arrow a b be a foldable or traversable data structure representing combat state. Maybe. I'm not really well-versed in arrows.
&gt; This is -- quite literally -- a feature and not a bug This is only a feature if you have bought into the Nix-all-the-things ideology but I have not. So maybe that puts me in the "I just don't like Nix camp". &gt; Reproducing test failures might require running the system The problem is that if those tests had been written in just pure Haskell code, they would have been easier to understand, easier to debug and easier to work on. 
There isn't one except that `overrides` allows you to keep using `override` and `extend` removes the `override` function.
https://www.reddit.com/r/HaskellVideos/
I'd add that direct recursion is also way more likely to get you into trouble with non-termination. This is not a problem in simple cases, but using cata-morphisms ensures that your recursion is well-founded.
What exactly about the tests were written in Nix? In my experience, you’re still writing tests in Haskell, but using Nix to manage the test environment and potentially do some setup/cleanup.
If Nix is being used in a professional setting, then I’d expect developers to be professionals about the decision and get over it. Lots of people “hate” Haskell too you know. I’ve been in the situation you’ve described and the backlash to Nix came exclusively from developers who had no understanding of it and made little effort to gain such an understanding. Worse yet, they went out of their way to politic against Nix despite their lack of knowledge. So I guess the problem there *was* (like you said) bad-acting people :)
I write Haskell is to avoid sneaky, implicit mutation. So in the end, everything we want to talk about will somehow have to be an argument to the `computeDamage` function. It's just a question of what we pass in. I've only spent 20 minutes on this while drinking my morning tea, so let me know what you think. --- I think `computeDamage` is going to have the type `Player -&gt; Monster -&gt; Double` at the very least (the output type doesn't matter). When you say "modify the outcome of attacks", here's what I think: (Player -&gt; Monster -&gt; Double) -&gt; (Player -&gt; Monster -&gt; Double) So, given a way of computing damage, return a *modified* way of computing damage. If we have two damage modifiers `f` and `g`: f :: (Player -&gt; Monster -&gt; Double) -&gt; (Player -&gt; Monster -&gt; Double) g :: (Player -&gt; Monster -&gt; Double) -&gt; (Player -&gt; Monster -&gt; Double) then "apply `f`, then apply `g`" is `g . f`. There is also the "empty damage modifier"; the one that doesn't change the damage calculation. It's the identity function - `id :: a -&gt; a`. Functions of type `a -&gt; a` are called "endomaps", and they form a monoid, where the binary operation is function composition, and the unit is the identity function. In Haskell it's called [Endo](http://hackage.haskell.org/package/base-4.11.1.0/docs/Data-Monoid.html#t:Endo). We can also lean on operations in Data.Foldable, like [fold](http://hackage.haskell.org/package/base-4.11.1.0/docs/Data-Foldable.html#v:fold), to combine sequences of monoidal values. For `Endo`, `fold [Endo h, Endo g, Endo f] = Endo (h . g . f)` With that background out of the way, here's what I've written: type DamageModifier = Endo (Player -&gt; Monster -&gt; Double) -- | Computes how much damage a player should do to a monster computeDamage :: [DamageModifier] -&gt; Player -&gt; Monster -&gt; Double computeDamage externalMods = -- externalMods are applied first appEndo (fold $ modifiers &lt;&gt; externalMods) base where base :: Player -&gt; Monster -&gt; Double base _ _ = 0 modifiers :: [DamageModifier] modifiers = -- Step 3: Big monsters only take 90% damage [ Endo $ \cd p m -&gt; case monsterSize m of Big -&gt; 0.9 * cd p m _ -&gt; cd p m -- Step 2: The player's weapon has a modifier for damage , Endo $ \cd p m -&gt; playerWeapon p + cd p m -- Step 1: Player's level increases their damage by 5 * level , Endo $ \cd p m -&gt; playerLevel p * 5 + cd p m ]
That just sounds like a bad software development decision that happened to leverage Nix though? I don’t see how this is an argument against using Nix. It’s a good example of when to use it vs when to stay in Haskell-land, but that’s it. Using individual’s mistaken usage of Nix as an indictment of Nix as a whole is exactly the sort of political BS that annoys me :(
I agree that using async would’ve been wiser there. But maybe the “best” would take it further: abstract out async vs Nix and then have the tests runnable in both configurations for free! There’s merit to both probably. 
This is so cool! Would love to see it adapted for mandolin
&gt; I don’t see how this is an argument against using Nix Please re-read my initial comment. I did not say "don't use Nix". I said "be very, very careful" and i said "beware of its viral nature". In short, be careful about how you use it, when you do you use it and make sure its actually the best tool for the job. I can certainly see that Nix has advantages for service deployment. For that situation Nix or docker or something similar probably is the best tool for the job. For developers working on development machines (laptops/desktops) it can be useful for dependency management but that should be up to the developer using that machine. For developers not very experienced with Linux system admin, Nix may be great, but for others who have been maintaining their own Linux dev environments for years, Nix may not offer any advantages. &gt; Using individual’s mistaken usage of Nix as an indictment of Nix In this case its not an individual's mistaken usage, its an organisation's mistaken usage. See the "viral nature" I referred to earlier. 
that is true of `persistent-mysql`, but [persistent-mysql-haskell](https://hackage.haskell.org/package/persistent-mysql-haskell) does truly stream rows.
Okay, so I gave it some thought! Here's an example with **actual, real world objects.** &amp;#x200B; Lets take the category objects in are actual real world objects such as a laptop, a chicken, an apple, a stack of dollars etc. Morphisms are things you can do to the objects: you can eat an apple, you can sell your laptop for money, etc. Composition of morphisms is kind of obvious, if you can: 1. Sell your laptop and get money 2. Give your money in exchange for an apple then you can turn your laptop into an apple (and regrets). &amp;#x200B; **Now, put the laptop in a box.** That's our endofunctor W. It enables you to literally take something and place it in a box. Since it's an endofunctor, you can compose them and put a box in a box and so on. (A box is also a real world object! It's just boxed empty air.) &amp;#x200B; On morphisms, the functor acts in this way: if you know how to turn your laptop into money, you also know how to do the boxed version of that: you take your laptop out of the box, sell it and put the money back inside the box. &amp;#x200B; To show this is an actual Monad, we need to show that there's two natural transformations, pure and join: `pure :: a -&gt; m a` means that we just put something in a box. Since pure is a natural transformation, it means that there is a \_morphism\_ of putting something in a box. And there is! It's just a "thing you can do to the object", from the top of this post. Whatever you give me, I can box it. Even a boxed box. &amp;#x200B; `join :: m (m a) -&gt; m a` means that we ought to be able to take a box of a box of X and just turn it into a box of X. And we can! For any double boxed object X, there's a \_thing we can do\_ to that object: we just remove one of the boxes! &amp;#x200B; Tada! We also have to show a bunch of coherence conditions but to me it seems everything here behaves nicely! &amp;#x200B; &amp;#x200B;
The code is all in the post, why not be the one to do it and get some practice in the meantime? :)
I have actually thought about this! First off, the single defining characteristic of what makes a Monad unique and different is join. Not is more common friend &gt;&gt;= (bind), or even return. Join takes something in a Monad that’s _in_ a Monad (of the same type) and gives back something in a singe Monad. So Just (Just 7) becomes Just 7. We can use join anywhere where having two layers is redundant. Which kind translates to the real world... I like to think of double bagging at the supermarket. You have something in a bag, and then put the whole thing in another bag. When you get home and put away groceries, you don’t take the first bag out before you put away the second, you treat it like a single bag! If your double bag was empty, and someone asked you “what’s in the bag?” You wouldn’t say, “another bag.” No matter how many bags you add, it’s all just one in an abstract sense.
i prefer using \`fetchTarball\` (with nix &gt; 2.0) to fetch a pinned nixpkgs: &amp;#x200B; &amp;#x200B; ```nix pkgs ? import (fetchTarball { url = https://github.com/NixOS/nixpkgs-channels/archive/&lt;SHA1&gt;.tar.gz; sha256 = "&lt;SHA256&gt;"; }) {} ```
Haskell compiler to javascript that does not spit multimegabyte file for hello world. 
TOOLING! I have a small internal tool I want to write to simplify my and my team's daily workflow. My team knows I'm into Haskell and a few have expressed interest. So I figured hey, this would be a great opportunity to write a simple tool in Haskell and share it with the team! Then it took me nearly half a day to get a half-decent development environment set up from scratch on my work computer. And that's as someone who's done it before! At that point I abandoned the idea of introducing it to my team. I can't ask them to go through that, and I certainly can't be the office point of contact for things like "how the fuck do I add a dependency" and "ok, I added a dependency and now Stack is whining, what do I do?". 
I'd make the language more of a PITA so that I didn't mind so much programming in other languages.
Proper extensible records. Do them like Elm does (at least syntactically).
String !
I will cheat a bit here. I wish we had a common understanding, a shared set of standards, of the kind of libraries we should write. * Types are not substitutes for documentation. (Not everyone understands parametricity as well as you do, dear author) * Links to papers (however well written) are not a substitute for documentation. * Formal definitions are not a substitute for fuzzy intuition. * Explanation is not a substitute for code examples. * Module level documentation is not a substitute for an eagle eyed view of the package's organization. Perhaps that can be summarized as "all packages magically have so awesome documentation that you'd like to send a heartfelt note of thanks to all package authors".
Strings, maybe with a compiler switch not too unlike OCaml's safe-string (different purpose, I know).
totoally agree
Left-to-right . and $ so that function composition reads more like a pipeline
Not exactly what you wished for, but: import Control.Arrow ((&gt;&gt;&gt;)) import Data.Function ((&amp;)) main = do "Hello World" &amp; putStrLn show &gt;&gt;&gt; putStrLn $ True 
Yep, but it's not idiomatic unfortunately
+1 : Tooling ! And also memory management - so one can generate _small_ WebAssembly from haskell and use it in the browser without elm / ghcjs
The most annoying "feature" in the prelude that is String. As a close second, the partial functions in the prelude. A distant third, many insufficiently polymorphic functions - [] which should be Foldable/Traversable f, and Monad constraints that should be Applicative or Functor.
To be more friendly toward data scientists: &gt; As already mentioned, the tooling lacks, and it's the kind of thing that can really scare a community that really wants something that works well. It's really really sad IHaskell isn't available on windows yet &gt; Lack of libraries. No standard/easy way to plot, and largely limited compared to the competition. No equivalent for scipy's optimizations, no module for differential equations, no dataframe, etc &gt; This might be more controversial, but Haskell's syntax is sometimes very complex. It's so compact it sometimes need some deciphering. Which is a drawback when dealing with data and trying to limit coding errors. All of this is unfortunate because I'm convinced that Haskell could be a great language for data science, the type system, the purity, etc are really interesting features for that field
A single namespace for types and values. Kind promotion already requires quoting so you can distinguish, say, a product type and a pair of types, and it's just going to get worse when dependent Haskell gets here.
Thanks! This is a lot of detail and a lot of good ideas, which I really appreciate! The one thing I'm a bit worried about (and maybe this goes a little too far into general application architecture) is how to organize this type of code at a high level without the dependency graph getting very messy. In OO languages, it's common to have some concept of "discovery", where the central system might be able to say "get me a list of all of the DamageModifiers" so we don't need a bunch of centralized lists that we have to keep updated. Is there a good pattern for solving that problem in Haskell?
What did you set up and why didn't you just use the superior tool X?
It's quite all right. Be the change you want to see in the world
Tutorials that actually show how to build things, instead of blog post after blog post about some minor detail of the type system. Give me “how to build a Pokédex” or “how to view your holiday pictures, the Haskell way” or “create pong with too many monads” or whatever. I work with R a lot professionally and that community is all about building things with the language, which makes it easy to pick it up. Haskell, not so much. Which is a shame because I’ve not been able to overcome the learning curve yet. Maybe one day. 
This 100%. Typed languages were supposed to offer all sorts of refactoring and code analysis opportunities but that hasn’t turned into quality tools for the average Haskeller. Every time I switch back to OCaml they have better tools for build, editor support and package management. 
I don't know if you mean something specific by saying czmq as opposed to just zmq, but if not then https://hackage.haskell.org/package/zeromq4-haskell is probably what you want.
Modules, like OCaml ones. And they would be normal types with ability to declare typeclass instances over them.
&gt; where the central system might be able to say "get me a list of all of the DamageModifiers" In this case, isn't the system really accessing some *implicit* centralised list? If at any point in some function I can say "get me a list of all DamageModifiers", then it's not any different to having passed that list of DamageModifiers to the function (except that the former will be more difficult to debug). &gt; The one thing I'm a bit worried about (and maybe this goes a little too far into general application architecture) is how to organize this type of code at a high level without the dependency graph getting very messy. I don't really know what it means to worry about this. The kind of "discovery" mechanism you describe doesn't solve it either- the dependency graph is still the same, it's just that now some of the edges are less obvious. In general I think it's a difficult to talk about these hypothetical architectural questions, because pure functional programming is very different to other paradigms. Most (all?) OO idioms poorly translate to Haskell. I suggest doing the best "Haskell-y" solution you can with the tools you've got, and then asking for feedback. It's a lot easier to talk about architecture when there's actual code and concrete goals involved.
I've tried this but doesn't work unfortunately. `mergeSources` from the `conduit-merge` package seems to work but I just don't understand how.
Nitpick: You say there are two functions of type `forall a. a -&gt; a`, `id` and `undefined`. There is a third, `const undefined`, which `seq` can distinguish from regular `undefined`.
You might be interested in this package :) * https://github.com/haskell-backpack/backpack-str
- LISP syntax or at least ability to provide identifiers with dashes instead of camelcase. - default safe Prelude (the minimum in my opinion would be to get rid of all partial functions, I don't buy the argument about beginner friendliness) - slightly less lazy (typically `{-# LANGUAGE: Strict #-}` by default would feel more natural in most case while not really hurting laziness). - easier reproducible build between different dev env (a lot of effort was/is put there, but I always stumble upon problem due to lack of 3rd party lib, strange bugs, Linux vs OSX, etc... be it, cabal, stack or nix they all have their problems)
You might be interested in this package: * https://github.com/tfausak/flow#cheat-sheet
There is `|&gt;` and `&lt;|` defined somewhere IIRC.
As first starting point you can suggest blog posts like this one where the simple workflow with Haskell build tools is described: * https://kowainik.github.io/posts/2018-06-21-haskell-build-tools.html
As someone relatively new to haskell, yes. This. All the way.
Perfect.
But add syntax for generic update functions. That is, like how `.x` is shorthand for `\foo -&gt; foo.x`, have something like `=x` as shorthand for `\foo y -&gt; {foo | x = y}`.
Discovery, wether reflection based or somehow a part of the compiler/runtime, is a hack in an OOP language as much as it would be in Haskell. It probably wouldn't be too hard to do with TemplateHaskell or maybe even the C preprocessor.
💯 That's why I published this post yesterday: [https://adriansieber.com/ukulele-fingering-chart-cli-tool-in-haskell/](https://adriansieber.com/ukulele-fingering-chart-cli-tool-in-haskell/) I hope that's like it! 
Just one single thing: **Fix the damn tooling situation!** As everyone else I learned to hack in Haskell with Stack. Despite everyone saying never to use Cabal I recently gave it a try thanks to [this blogpost](https://kowainik.github.io/posts/2018-06-21-haskell-build-tools.html) and it wasn't as bad as I expected it to be. Cabal is showing great promise but doesn't feel as shiny as Stack but overall it seems like both tool support the same features but with different file formats and different UIs. In fact, I've started noticing projects with cabal.project files instead of stack.yaml files..... the format war has already begun! Unfortunately both tools suck differently! I frequently run into situations where I have to nuke my .stack to fix things and start over or recently Stack started choking on my projects with some inscrutable error about hoogle. Cabal on the other hand throws terrible error messages at you which often rather feel like debugging output than messages intended for users... What's the point of having two imperfect tools with basically the same purpose but with incompatible formats? Also some tooling only integrates with either Cabal or Stack but not both. This is very confusing and poses an unnecessary distraction especially if you're just starting out with Haskell. Seriously, just pick a "winner" among Stack and Cabal! Flip a coin or make a poll... it doesn't really matter which one we pick. Officially declare the loser as discontinued in favor of the winner and instead shift all resources into making the winner the best tool we can come up with.
As much as I agree with partial functions being mostly bad, I haven't found their presence in Prelude to be a real problem. I just don't use them.
I wasn't aware of that. That's brilliant ! Thanks
One of two things: - Type level programming. We've got lots of craziness to support type level programming. If you look at languages like idris, you can do a lot more with a lot less. - Compilation pipeline. For one, Haskell as a language seems like it'd benefit monstrously from link-time-optimization. Second, GHC's backend is somewhat hostile toward interpretation, template haskell, and cross compilation. So I'd replace everything after STG with a toolchain that supports better LTO and multi-targeting. Maybe even a VM, honestly; JITs can do some wonderful things.
Do you say this because you when you are solving problems, you find it helpful to be able to see how other people have solved similar problems and use that as a starting point? Do you find it motivating to see what people have built with the language? Do you enjoy a project-style of learning, where you have clearly defined goals, and the learning takes place in figuring out how to achieve those goals? I ask because personally I don't care for tutorials, but I want to know what sorts of people do and why they feel it is important.
I don't feel that I am writing less or more documentation in Haskell than in other language. In fact, I'm probably writing a bit more, because of haddoc : is there , and I don't have to chose an extra doc tool as I would have to do languages which don't have a built-in documentation tool. I think the documentation problem in the Haskell community is more due to the lack of resources. Some people are great at writing code, some are great at writing and really few are good at both. Most of the great doc you can find out there haven't probably been written by the coder itself, but by lots of people. Obviously, the less people working on the a package, the less chance to have good documentation and that's true in every language. At least with Haskell, types helps ... sometimes (I still haven't figured out what a indexed monad ...)
Regarding to the [comments](https://www.facebook.com/groups/programming.haskell/permalink/2242245609119761/) the book is not very interesting. 
Do data scientists care enough about code to devote time to learning Haskell? My understanding is that in this kind of interdisciplinary context the answer is often “no”...
A nice macro system which doesn't have a two stage restriction.
Im pretty sure that stack is the winner, it just takes time. Also, stack will generate .cabal files, which might account for some of the ones youve seen. Others may be old packages. 
I would enforce law abiding. Laws are crucial, have to be taken serious. Libraries that break the law should not compile in any circumstances. 
I don't understand what your goal is, but I can try to give some tipps. I think the key problem is "how to split off the first element off a source", i.e. a function like [`msplit`](http://hackage.haskell.org/package/logict-0.6.0.2/docs/Control-Monad-Logic-Class.html#v:msplit). The way they accomplish this in `mergeSources` is with a clever use of [`sealConduit`](http://hackage.haskell.org/package/conduit-1.3.0.3/docs/Data-Conduit.html#v:sealConduitT) and [`($$++)`](http://hackage.haskell.org/package/conduit-1.3.0.3/docs/Data-Conduit.html#v:-36--36--43--43-) and [`await`](http://hackage.haskell.org/package/conduit-1.3.0.3/docs/Data-Conduit.html#v:await). msplitSource :: Monad m =&gt; SealedConduitT () a m () -&gt; m (SealedConduitT () a m (), Maybe a) msplitSource source = source $$++ await This gives you maybe the first element and the rest of the source in the base monad.
Getting rid of exceptions that can be thrown anywhere but only caught in IO.
While we're waiting for first-class support for extensible records in Haskell, we can use packages like this one: * http://hackage.haskell.org/package/superrecord It's quite amazing actually how you can have such features in Haskell using only language. But, I guess, if extensible records were implemented by compiler, we could have nicer syntax and better performance.
Tested README getting started example. Like this: [https://github.com/nikita-volkov/hasql/pull/100](https://github.com/nikita-volkov/hasql/pull/100)
`string-&gt;text` would also be a valid identifier as well as: - `empty?` (instead of `isEmpty`) - `log!` (instead of `log` and checking its type is `IO ()` Also great advantage of list syntax is no more operator precedence problem. But I know I'm in the minority, that's just a personal preference.
Yeah, that would be really nice! LiterateHaskell + `markdown-unlit` contains huge power. In packages I'm working on I'm usually try to write such README with examples: * https://github.com/Holmusk/servant-hmac-auth#servant-hmac-auth * https://github.com/serokell/o-clock#example-how-to-make-your-own-time-unit
Ok, that actually makes sense. I am aware that I am missing some historical context and I do understand trade offs. However, since the those integration tests have grown over time and have become highly fragile and un-reliable, they are highly likely to be replaced and the replacement is unlikley to use Nix.
there are data scientists here https://gitter.im/dataHaskell/Lobby
Exceptions that actually get reflected in the type
Basically, I'm trying to read "freely" across two different sources, or if you prefer be able to `await` from one source or the other so I can do things along do a1 &lt;- awaitA a2 &lt;- awaitB if a1 == a2 then ... else do leftoverA a2 b &lt;- awaitB I think it indeed boils down to your `msplitSource` function. Could you explain a bit of it work ? 
I'd probably also split `Enum` and `Num` into separate parts. E.g having sensible implementations for `pred` and `succ` doesn't imply sensible implementations of `toEnum` and `fromEnum`
Also known as [https://en.wikipedia.org/wiki/Conway%27s\_law](https://en.wikipedia.org/wiki/Conway%27s_law)
Nice! I think having a "getting started", be it a blog post or a snippet in documentation would help. Also some sort of community status of eventlog, where it sits today and what is yet to be done to achieve X in practical manner. I'm happy to help here as I think it would help make production apps easier, but don't know where to start. I'll take a look at both links.
Yeah my main issues with library level solutions are: Can't get O(1) indexing particularly easily / without unsafe hacks. Can't create mutable records with O(1) indexing and update easily / without unsafe hacks. In general Haskell treats arbitrary contiguous data structures as second class citizens, making highly complex linked structures is easy, but contiguous ones must go via Data.Array or be basically hardcoded. No nice syntax such as literals. The entire Haskell ecosystem should more or less standardize on a single form of records, which goes against a library level solution. With that said superrecord and the like are fantastic and impressive libraries, and the baked in implementation should use them as a good reference. 
Which actually explains both what we have now and what the replacement is likely to look like :)
Precisely :)
Really? I haven't needed to nuke a `.stack-work` in at least a year, and the time I did it was because I was screwing with it. How did you get into this state of affairs?
Stack traces. The good news is –- HasCallStack has gon into one of previous versions, all that's left is to add stack info whenever you throw an exception (there's a ticket for that).
I think this is the wrong attitude; going against the grain is only asking for headaches both in terms of not growing familiar with the code other people are writing, and in terms of discouraging people from writing code with you.
Hm, I would personally consider this a small thing. I've seen it before and wouldn't be surprised.
But Reddit's markdown expects initial 4 spaces to enable code formatting. 
I fully agree with you. What I meant to say (but didn't say explicitly) was that these are all complements not substitutes (in the microeconomics sense of the words).
Get rid of cabal new-build and concentrate all forces on the Haskell Stack
You can't really escape crashes when you have non-termination: there's always `error` and `let !x = x in x`. It is a good idea to forbid partial patterns, record updates etc. You can do this with warnings.
Your are basically asking for a lot of features to be removed, a lot of behavior to be lined up with how JS does it and a move from lazy-to-strict by default. And PureScript delivers (Elm to some extend as well).
Are you sure that writing an open letter claiming that Haskell is a serious language will make Haskell look like a serious language ? Unfortunately, I don't think it will change anything.
`stack` started off as a solution to `cabal-install`'s issues, but it took an opinionated stance and ended up solving a slightly different problem -- reproducible builds (at least w.r.t. Haskell dependencies). The two are not substitutes for one another; the closest thing to `stack` is `cabal-install` with Stackage package sets, which is how `stack` eventually came about, anyway. This is also the reason why `stack` needs `.cabal` files to work -- the two are actually not in direct competition, IMO. That being said, I rejoiced back when I was learning Haskell when I found out that I could significantly ameliorate "`cabal` hell" by regularly nuking the `~/.cabal` directory. You would install `lens` as advised by some online tutorial you were reading, then install some arcane mathematical library to work on a Project Euler question, and then suddenly your GHC installation would be utterly unusable to to library conflicts (the ["butterfly effect"](https://cdsmith.wordpress.com/2011/01/17/the-butterfly-effect-in-cabal/)). Then `cabal-install` introduced sandboxes, and now `cabal new-build`, but `cabal-install` **still does the wrong thing by default** when you run `cabal install`, which is what 90% of tutorials and `Readme` files still incorrectly advise users to do. Although I haven't been a user of `cabal-install` for ages, I eagerly await the day when `cabal new-build` is the default behaviour, so we can get past this red-herring argument that `cabal-install` and `stack` are somehow in competition with each other...
That's actually a rather cool trick! Thank you! :-D 
Careful, though. `$` is [special-cased in the Haskell compiler code](https://stackoverflow.com/questions/9468963/runst-and-function-composition) (it is sometimes handled like a keyword with special semantics) -- it is not merely function application with extremely low operator precedence. I am not sure that a purely-library-based left-to-right version of `$` would behave as expected in all cases.
What exactly about tooling? It would be nice to list the problems they have and think how they could be solved
I was thinking more about the performance side of String. :)
You are describing something very similar to generic-lens: http://hackage.haskell.org/package/generic-lens
&gt; get rid of Template Haskell In favor of what new system? :) There are many uses of Template Haskell that cannot be gotten rid of without a ton of boilerplate.
Fair points !
Sorry to pick on vajrabum but the use of derivatives here has I think 0 to do with the use of limits in domain theory. If you work in a category of domains you won't ever have a bicartesian closed category.
Not sure where I first read this but I like the distinction of soft vs hard documentation. Soft documentation are tutorials, guides, blog posts and so in. They give intuition on how to use the api. Hard documentation is what you want while using an api. Type signatures, function levels documentation, implementation details and so in. Haskell is pretty good about hard documentation but soft documentation mostly only exists for framework-style libraries.
I didn't want to make this an argument about competition being the main problem. To me the bigger problem is the compatibility issue which is a direct consequence of what you refer to as an opinionated stance which causes Stack to intentionally diverge from its roots and make it harder to switch between the two tools. Stack wants you to use `package.yaml` and `stack.yaml` files whereas Cabal uses these confusingly named `project.cabal` and `cabal.project` files and whatnot. This forces Haskell users to make a choice whenever they start a new project. This is also somewhat an obstacle when you want to contribute to a Haskell project and that project's maintainer, to put it mildly, strongly prefers Cabal while you prefer Stack or vice versa. To me, **the build tool you use should be a minor detail and be interchangeable with each other!** but unfortunately with Stack and Cabal for whatever reasons this isn't the case yet.
I'm gonna be petty and say: change `::` to `:`!
Some comments here look useful: * https://www.reddit.com/r/haskell/comments/4f7fyn/what_are_row_types_exactly/
Your function signature is Char -&gt; Maybe Char, the input will always be a Char.
In your example `c` is always of type `Char`. This is what the type of your function (`Char -&gt; Maybe Char`) indicates, so GHC will enforce it. What are you trying to achieve? 
I see.
An example where given the input 3-tuple, the output *could* be Maybe Char.
Amazing, thank you.
What kind of input?
It would indeed be nice if you could get warnings to apply to library code as well.
Could you be more specific?
Write it :). The folds library is a good starting point. For something less state machine and more FRP, try reactive-banana or reflex.
More than 96 percent of American households buy bananas at least once a month. *** ^^^I'm&amp;#32;a&amp;#32;Bot&amp;#32;*bleep*&amp;#32;*bloop*&amp;#32;|&amp;#32;[&amp;#32;**Unsubscribe**](https://np.reddit.com/message/compose?to=BananaFactBot&amp;subject=I%20hate%20potassium&amp;message=If%20you%20would%20like%20to%20unsubscribe%20from%20banana%20facts%2C%20send%20this%20private%20message%20with%20the%20subject%20%27I%20hate%20potassium%27.%20)&amp;#32;|&amp;#32;[**🍌**](https://np.reddit.com/r/BananaFactBot/comments/8acmq6/banana/?st=jfof9k8d&amp;sh=acd80944)
Do you have a particular task in mind, or are you looking for any scenario where this makes sense? In the latter case, an easy answer is `firstChar :: String -&gt; Maybe Char`, as in: ``` Prelude Data.Char&gt; let firstchar s = case s of { "" -&gt; Nothing; x:_ -&gt; Just x } Prelude Data.Char&gt; firstchar "" Nothing Prelude Data.Char&gt; firstchar "abc" Just 'a' ``` This is a specialization of [headMay](https://hackage.haskell.org/package/safe-0.3.17/docs/Safe.html#v:headMay) to Strings with Chars in them. Another example would be taking the [arbitrary-sized Integer type](https://stackoverflow.com/questions/3429291/what-is-the-difference-between-int-and-integer/3429322#3429322) and converting it into a Char. You could only do so if the Integer in question fit into a Char, so any such function would be most naturally represented by `Integer -&gt; Maybe Char`. 
In terms of documentation Haskell needs to learn a lot from Elixir
&gt; I am coming around to the idea that maybe the boilerplate is preferable. Of course, the devil is in the details :). I think in certain cases boilerplate is preferable over using TH. How far do you go with it though? Are you willing to write all the lenses you need by hand? If you don't use lenses, are you open to decreeing that everyone who uses lenses write them by hand? Are you willing to write complex structures using a bunch of nested constructors and combinators instead of using quasiquotes? &gt; Or alternatively, some less integrated and cruder code generation, like what is done in Go (or hacked up in C via Makefiles). I disagree here. I think we shouldn't be "metaprogramming" with strings as the only data type. &gt; I'm not done pondering the issue, but I have the hunch that Template Haskell makes something really nasty much too accessible. Something that dirty should have a much higher inconvenience bar to use. I kinda' agree. There needs to be a better separation between innocuous uses (get me all nullary constructors of this data type) and performing arbitrary IO. Unfortunately I'm not very familiar with metaprogramming in other languages here to give any concrete suggestions for improvement.
I ran into the issue yesterday. You just needed to upgrade stack.
I think that generics is the right approach. As a rule in cinematography, the actor is never allowed to look directly into the camera, that is the meta level. Similarity, in programming one should never step out 
Could you recommend some examples of particularly good Elixir documentation apart from the standard library?
&gt;I think that generics is the right approach. I don't quite follow. Haskell already has parametric polymorphism. How can it replace metaprogramming facilities like TH though? &gt; As a rule in cinematography, the actor is never allowed to look directly into the camera, that is the meta level. Some movies wouldn't be as good if they didn't have the fourth wall breaking moments that they do today. &gt; Similarity, in programming, one should never step out If by "should never", you mean the same thing as "must never", then I have to disagree. It can be very useful to generate, reason about and manipulate code using code. If you do not provide proper mechanisms for metaprogramming, people will resort to using error-prone techniques with strings and build system hacks, whether you like it or not.
Ala Haskell, I think it's about the central importance of composition. The larger bodies of example code that you often find in tutorials are (usually) the best way to understand how language components and concepts compose to do something useful, which is often a shortcut to deeper understanding. FWIW, someone in the Rust subreddit posted a cross-sectional study about people learning programming languages which found that example code was the most important resource for new learners.
We're not able to help if you don't give us your context. Are you doing a homework exercise?
It's homework, although this is not relevant. It's about trying to understand when an input could produce Maybes.
I'm sure you can switch that ) for some other Unicode character...
I've been pondering writing an open letter entitled "An open letter to people who write open letters". But more seriously, I'm not worried about how seriously the language is taken or even how "the community" is perceived. I have met industry-focused Haskell users, and I've met theory-focused Haskell users, and I've met people who are somewhere in the middle. People who paint entire communities with broad brushstrokes don't actually care whether or not their generalizations are accurate. They just want something to grumble about.
Word of advice: don't take each twitter post full of ignorance as an attack, let it go :)
You could write a function that takes in text that's eg UTF and want to output "Just ASCII" if the input is well-formed ASCII, or Nothing if the input that can't be represented in ASCII. 
There is a way, but you probably don't want to do that. That way is to use the [`Typeable`](http://hackage.haskell.org/package/base-4.11.1.0/docs/Data-Typeable.html) typeclass: import Data.Typeable -- | -- &gt;&gt;&gt; isChar 'a' -- Just 'a' -- &gt;&gt;&gt; isChar "foo" -- Nothing isChar :: Typeable a =&gt; a -&gt; Maybe Char isChar = cast Note that my function can be called on values of any type `a`, as long as that `a` has a `Typeable` instance (most types do). In particular, it can be called on both a `Char` and a `String`, as shown above. Your function, on the other hand, can only be called on `Char`s, so you don't need to check that `c` is a `Char`; you already know that it is. And that's why you probably don't need `Typeable`: in Haskell, we already know the type of our input, so there is never a need to check which type it is! Even when we want to write a function which could either accept e.g. a `Char` or a `String`, we would not use `Typeable` to determine which of the two it is, we would instead ask for an `Either Char String` and pattern match on the `Left` and `Right` constructors in order to discover whether it was a `Char` or a `String` which was passed in. I can't even remember the last time I used `Typeable`, and I write a _lot_ of Haskell code!
I feel pretty much the same way. I'm also a beginner and I really like learning these new concepts and ways of approaching certain problems. However, when I think about all the things that ought to be learnt to "figure out" the language and put it to good use, I realize most of my peers would lose interest along the way. Besides, there's the problem that all the Haskell workflow is very Unix-like. That makes it a very hard sell for people used to GUIs. Most of the people I know are strangers to terminals (I'm at uni). Yes, I know, we are all very inexperienced. But how likely is it that a recently graduated person, that clicked their way throughout their entire college education, decides to learn a language such as Haskell? Not very likely, I'd say.
`⁾` 💩
\&gt; I don't quite follow. Haskell already has parametric polymorphism. How can it replace metaprogramming facilities like TH though? &amp;#x200B; [https://wiki.haskell.org/Generics](https://wiki.haskell.org/Generics)
`map . foldr` is the same as `(.) map foldr`. Take the type of `(.)`, substitute the type of `map` for its first argument, and the type of `foldr` for its second. Hint: `(a -&gt; b) -&gt; c -&gt; d` is the same as `(a -&gt; b) -&gt; (c -&gt; d)`.
Hint: `map . foldr = (.) map foldr`.
 $ stack --resolver ghc-8.4.3 exec ghci GHCi, version 8.4.3: http://www.haskell.org/ghc/ :? for help Loaded GHCi configuration from /Users/taylor/.ghc/ghci.conf &gt;&gt;&gt; :type (map . foldr) (map . foldr) :: Foldable t =&gt; (a1 -&gt; a2 -&gt; a2) -&gt; [a2] -&gt; [t a1 -&gt; a2]
`data Foo = Foo { foo, bar, baz :: Bar }` `data FooUpdate = FooUpdate { foo, bar, baz :: Bar, update :: Baz }` `convert Foo{..} = FooUpdate{ update = mkBaz, ..}`
Eh, I have a coworker who uses `&gt;&gt;&gt;` a lot and no one cares, and I sometimes use `&amp;` for readability, and no one minds.
True, but the most common case where this happens is `runST $ do {...}`, and in those cases, I would argue that putting the `do` block first is bad style anyway.
What do you do when someone presses `Ctrl-C`? For all that it's annoying, I haven't been able to think of a better design.
&lt;whining&gt; But I _like_ `data Foo = Foo`. Wah wah wah sob. &lt;/whining&gt;
Are there many [] specific ones left that could be generalized? Looking at the prelude there are - infinite list producers - zip &amp; unzip variants - stuff that probably shouldn't be generalized because it's O(n) like lookup/indexing - operations where list is invariant like scanl/take/etc - list specific stuff like head 
`$ stack --resolver ghc-8.4 exec -- ghc -e ':type +d (map . foldr)'` `(map . foldr) :: (a1 -&gt; a2 -&gt; a2) -&gt; [a2] -&gt; [[a1] -&gt; a2]`
You can check generic-lens on Hackage. If you go to the meta level what can be lost is the consistency of the world you've created in the artistic act. They knew it in ancient Greek and in other cultures as well. For sure this happen in powerful enough formal systems, we know that from Goedel. In this regard Template Haskell is not safe either
When I found out about Lisp conditions, I was like, "This is awesome!" Then I talked to my advisor, who had actually used Lisp, and he suggested that very few people actually use the condition system or miss it when they leave Lisp-land. It sounds cool, but it's too flexible and relies on ambient state to determine what a function will do. It's also relatively straightforward to make as a design pattern when you need it (see section on "Keepers" [here](http://joeduffyblog.com/2016/02/07/the-error-model/)).
I had to nuke mine yesterday. I had installed a package that relied on `text`, and then run `brew upgrade`, which upgraded `libicu`, and now I got linker errors trying to build anything that used `text`.
Get generics that work for GADTs and existentials; then maybe we'll talk. Honestly, though, I still wouldn't get rid of TH. Compile-time metaprogramming allows for eliminating boilerplate _and adding functionality_ that you literally can't explain to the compiler any other way.
http://www.staff.science.uu.nl/~f100183/fp/slides/fp-qa1.pdf, slide 15 ;)
I really enjoyed your post!
Maybe you could do something like 'isNumber' or 'isLetter'? Return a Nothing if it doesn't qualify, otherwise a Just with the character in it.
Thanks, this is interesting to know. I admit to not having actually used it in anger.
&gt; What do you do when someone presses Ctrl-C? Or you run out of memory, or you loop, or your lazy I/O read operation fails, or ... Sadly, exceptional situations are quite ubiquitous.
&gt; For one, Haskell as a language seems like it'd benefit monstrously from link-time-optimization. My understanding of LTO is that it is a bit of a hack around C's compilation model, which precludes inlining across compilation units. However, GHC's core simplifier already does aggressive inlining across modules. What do you think LTO will do that the core-to-core pipeline doesn't already give us?
It's not exactly magic. You can always use `Language.Haskell.TH.runQ` on a quasiquoter like `[d| ... |]` (`d` = declaration) to check what kind of general *shape* your template haskell code should have. So in Ghci: &gt;&gt;&gt; :set -XTemplateHaskell &gt;&gt;&gt; import Language.Haskell.TH &gt;&gt;&gt; runQ [d| newtype T = T Int deriving (Eq, Ord, Show) |] [NewtypeD [] T_0 [] Nothing (NormalC T_1 [(Bang NoSourceUnpackedness NoSourceStrictness,ConT GHC.Types.Int)]) [DerivClause Nothing [ConT GHC.Classes.Eq,ConT GHC.Classes.Ord,ConT GHC.Show.Show]]] This is of course not ideal, since the `DerivClause` is muddled with newtype-specific stuff. It's easiest to declare as little with th as possible. So in this case, I would use `-XStandAloneDeriving` and maybe `-XDerivingStrategies` to be in control over what happens. So what I would want to have is something like {-# LANGUAGE StandAloneDeriving, GeneralizedNewtypeDeriving, DerivingStrategies #-} newtype T = T Int deriving stock instance Eq T ... deriving newtype instance Num T ... And the standalone deriving clauses are readily created with TH. Example: &gt;&gt;&gt; runQ [d| deriving newtype instance Num T |] [StandaloneDerivD (Just NewtypeStrategy) [] (AppT (ConT GHC.Num.Num) (ConT Ghci2.T))] To do this programatically, just return a longer list than just a singleton list, and replace `GHC.Num.Num` etc. with the results from functions like `lookupTypeName`. What you would like to implement then is a function of type declareAllTheInstances :: Name -&gt; Q [Dec] You would then use it as such: newtype T = T Int declareAllTheInstances ''T
&gt; https://hackage.haskell.org/package/zeromq4-haskell Thanks but the zeromq bindings do not have `zauth_configure_curve` or any other [czmq](http://czmq.zeromq.org/) functions I'm seeking.
That's what I thought. Simon's book sets a ridiculously high bar, and it was free to read online last I checked.
- Projects based book or tutorial something like this https://xmonader.github.io/nimdays/ (I'm the author) the closest thing was real world Haskell and it's very outdated - seriously lots of packages documentations expect that types are documentation which isnt really the case - I'd love less hassle with stack .. stack.yml is so overwhelming - better names than intercalate nub.. etc - String text situation - First class tooling and 0 effort getting started and setting up development environment 
/u/Tysonzero, you might be aware of this comparison table: https://docs.google.com/spreadsheets/d/14MJEjiMVulTVzSU4Bg4cCYZVfkbgANCRlrOiRneNRv8/edit A relatively new addition to it is a package called [`row-types`](http://hackage.haskell.org/package/row-types). I know you'd like O(1) field access, but as /u/jaspervdj points out, it may not be possible in every general case. `row-types` seems to have `O(log n)` reads and writes, which may be sufficient for you. That's definitely better than even the built in record update which is `O(n)` due to copying. (Constants notwithstanding, could be interesting to benchmark.)
Also relevant and interesting: http://reasonablypolymorphic.com/blog/higher-kinded-data/
With respect to the language itself, I sometimes miss optional backtracking in constraint solving. Let me explain my thoughts about this: When selecting an instance of a typeclass, the compiler does not look at the context, just at the pattern of the instance head on the right side of the `=&gt;`. What is the reason for this design? Three reasons come to my mind, but neither is compulsory: 1. Having the solver commit to an instance and only then considering the context has its uses, for example by unifying a variable with a type term that would lead to overlapping instances with no most specific one, if it would be right of the `=&gt;`. However, I think we can have both: Split the context into two parts, one of which is solved first and causes backtracking when that fails and commitment to the instance if it can be solved for exactly one most specific instance. (Details below) 2. Backtracking can cause exponentially long compile times. But: only when the feature is actually used. Given that even standard Haskell allows us to write programs that make GHC consume exponential amounts of *memory*, this should not be a big deal. 3. Edward Kmett [argues](https://www.reddit.com/r/haskell/comments/3afi3t/the_constraint_trick_for_instances/) that backtracking is antimodular, because adding an instance can change the semantics of existing code elsewhere. But I don't see how this is worse than what the current form of overlapping instances does to modularity. Overlapping contexts can be resolved in basically the same way as ovelapping instance heads, can't they? Even if it is in some way worse than that, at least it should be possible to backtrack on type equality constraints, because for these no instances can be defined that would break existing code, and it would be useful in combination with type synonym families, which currently can't be applied right of `=&gt;`. The constraints permissible in the backtrackable part of the context could also safely include constraints for closed-world classes. Being able to backtrack would often be very useful, for example whan making every deriving an instance of one class for every defined instance of another class, for the same type, or for doing logical operations with constraints, see below. Here's my proposal, do you see any flaws with it?: Class instance declarations can now also be of the form instance classicContext | backtrackContext =&gt; instanceHead Only three places in the compiler should need a code change: First, the abstract syntax tree. Instead of a type signature, an instance declaration node now must store two constraints and one type without context. By renaming the data constructor for the instance node and creating a pattern synonym with the name of the old constructor that allows one to access the instance signature old-style as `(classicContext, backtrackContext) =&gt; instanceHead`, all code that only reads the type of the instance should continue working as before. Second, the parser, obviously, has to change. Besides supporting the new syntax, parsing an old style instance declarations must now yield one of the new AST nodes where `backtrackContext` is `()`. Third, to actually make any difference, the instance selection algorithm has to be extended as follows: Whenever normally an overlapping instances error would be caused, do this instead: Let *S* be the subset of all overlapping instances that are most specific according to their `instanceHead` part. Define the following partial order on the elements of *S*, which in the following will be the new definition of "more specific": *i* is more specific than *j* if these four conditions hold: 1. The `instanceHead` parts of *i* and *j* have a most general unifier *u* 2. Applying *u* to the `backtrackContext` part of *i* yields a set of constraint terms which is a superset of the set of constraint terms obtained by applying *u* to the `backtrackContext` part of *j*. 3. *i* is marked as *overlapping* 4. *j* is marked as *overlappable* For each instance, recursively try to solve all constraints its `backtrackContext` part. Let *S'* be the set of instances for which this succeeds. If *S'* is empty, report a failure to satisfy the target constraint to the caller due to nonexistence of suitable instances. The report should maybe include the failure reports from all instances in *S*, if very detailed error messages are desired. If *S'* is nonempty, let *S"* be the set of most specific elements of *S'*, that is *S"* is the maximal set where for no instance *i* in *S"* there is a different instance *j* in *S"* so that *j* is more specific than *i* in the sense defined above. If *S'* contains more than one coherent instance, report failure to satisfy the target constraint to the caller due to overlapping instances. Else commit to the coherent instance *i* or any instance *i* in *S"* if there is no coherent instance in *S"* and continue as usual by solving the constraints in the `classicContext` part of *i*. If they can be solved, report success to the caller, else forward the reason for failure to the caller. With this, we could define such goodies as: -- Choose between two constraints depending on whether a third constraint can be satisfied class ConstraintIf (c :: Constraint) (yesC :: Constraint) (noC :: Constraint) where constraintIfCase :: ((c, yesC)=&gt;r) -&gt; (noC =&gt; r) -&gt; r instance {-# OVERLAPPING #-}(yesC) | (c) =&gt; ConstraintIf c yesC noC where constraintIfCase ifYes ifNo = ifYes instance {-# OVERLAPPABLE #-}(noC) | () =&gt; ConstraintIf c yesC noC where constraintIfCase ifYes ifNo = ifNo type ConstraintNot c = ConstraintIf c (TypeError (Text "negated constraint could be solved")) () -- Disjunction of constraints class ConstraintOr (c1 :: Constraint) (c2 :: Constraint) where constraintOrCase :: ((c1)=&gt;r) -&gt; ((c2)=&gt;r) -&gt; ((c1, c2)=&gt;r) -&gt; r instance ()|(c1) =&gt; ConstraintOr c1 c2 where constraintOrCase if1 if2 ifBoth = if1 instance ()|(c2) =&gt; ConstraintOr c1 c2 where constraintOrCase if1 if2 ifBoth = if2 instance ()|(c1,c2) =&gt; ConstraintOr c1 c2 where constraintOrCase if1 if2 ifBoth = ifBoth This shows that compile-time constraint resolution can become a logic programming language, as it should be. It is similar to Prolog with cut, but nothing depends on ordering of clauses/instances. I think it plays nicely with other language features and extensions because for all purposes other than parsing and instance selection, instance a | b =&gt; c should behave the same as instance (a,b) =&gt; c Is there something I am missing? Am I imagining the constraint resolution process too simple, and it is actually more complex? I only had a quick glance at the GHC source code, so I'm not sure if it is really that easy to implement for someone knowledgeable in GHC internals. 
Functional Reactive Programming, FRP, would be the big one. It models time varying behavior declaratively. &gt; `Events` occur at some points in time and they carry a value. &gt; A `Behavior` is a container for a value, that changes over time. https://reflex-frp.org/ https://github.com/hansroland/reflex-dom-inbits/blob/master/tutorial.md
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [hansroland/reflex-dom-inbits/.../**tutorial.md** (master → 68bddd6)](https://github.com/hansroland/reflex-dom-inbits/blob/68bddd65789f601ea18c76bfba58eff9e619dfb0/tutorial.md) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply e5wsbli.)
Closed records are `O(1)` (which is all that Haskell can support), open records of the form {f1:T1,f2:T2,fn:Tn|r}` can be rewritten to n+1 arguments, the 0th argument being the record `r`, and the other n arguments being accessor functions that offer `O(1)` indexing, because the caller knows what type of record it has. This is equivalent to type class dictionary passing, and is likewise subject to inlining. The `Has x t r` construct is the same.
Add variables 
Pooling small contributions towards a bigger doc-writing goal might already be possible today via tools like Bountysource.
Doesn't necessarily need to be the authors, either, although there's question of who has responsibility to keep docs and code in sync and how that happens.
This is a good way to summon /u/edwardkmett see [previous discussion](https://www.reddit.com/r/haskell/comments/8sqzop/why_are_we_not_backtracking_when_picking_instances/e11j5rw/)
 (&gt;) = (&gt;&gt;&gt;) (&lt;) = (&lt;&lt;&lt;)
You fail at the innermost IO function. What does "the user pressed CTRL-C" even mean outside of IO? But I think the GP was complaining about exception you throw. There is little reason for making a "throw" function available if you can't also maske a "catch".
I am slightly annoyed how much using left-to-right composition improved my thinking, to the point where I say `(.)` was harmful to my Haskell learning solely based on the direction
I do still think this is better solved with tooling. We need the warnings to "spill" beyond module boundaries, but it does not require a change on the language.
You run a pure computation that takes 5 seconds to compute. I hit Ctrl+C 2.5 seconds in, so I can cancel it. Next question: Where does the catch occur? At the callsite of the pure function? You'd think so, and in this case, it would. But in general it's actually not even that simple because if *you* hand *me* a thunk that I don't evaluate until tomorrow, and *then* I evaluate it and it throws an exception -- the exception is thrown in a completely different context, at a completely different place, than the one that originally created the thunk to begin with. If this thunk is evaluated inside a library without proper exception handling, it can easily break internal variants for things like `MVar` (leaving e.g deadlocked writers because the reader was killed without proper clean.) So when you say you "fail at the innermost I/O" what you actually mean is "Every single I/O action must carefully consider exception safety in all possible cases because any random asynchronous event can interrupt the entire state of the system, including (but not limited to) thunk evaluation". Which is completely not the thing you were suggesting and actually quite a lot harder, and a lot harder when you mix safe concurrent resources (like handle management or lock acquisition). Many Haskell libraries screw up exception safety, I've done it a number of times. It's absolutely non-trivial and I absolutely have no better idea of how to handle it.
I postulate that this is not a novelty budget, but an education budget, and that the focus of that education should not be the "How to use this", and instead, it should be "Why should I use this", "What else could do this instead", "Why was this built in this way", "What specific problems was this engineered to solve", and most importantly, "What were the tradeoffs made during this design". If you understand how and why a technology works, and what tradeoffs it makes in order to accomplish it's objectives, then you're equipped to use it. If you merely know how to use it to accomplish some objective, you may not be. Our field is quite full of examples of people using technology they are quite comfortable with in places that it doesn't belong (See: all uses of the term "blockchain")
That's a good signature, but the real effort will be in trying to update the entire ecosystem to using it rather than String/Text/etc, then in creating a UTF8 Text implementation that's performant, etc. At least now we have the tools available to us to do this :)
Ah yes, I got confused. I should spend less time over at r/programming.
A lot of my confusion with the language would have gone away if Haskell had the following convention for data constructors: data Foo a = MkFoo a
&gt; it's inherently anti-multi-target Shipping IR is absolutely not less "anti-multi-target" -- because *all* object files emitted by any native code compiler, of any form, are inherently "anti-multi-target" -- they are created with knowledge that reflects the target platform the compiler has chosen at compilation time, and that matters deeply. LLVM bitcode is really no different than an object file in this regard, the only advantage is that its representation hasn't chosen a particular *instruction set*. But by the time you generate the bitcode, it's already a foregone conclusion, because `ppc64le-unknown-linux` bitcode isn't going to magically work on `x86_64-unknown-linux`. Now, there are a number of reasons for why this is the case, but, fundamentally -- systems like Clang/LLVM/bitcode etc are not "universal multi-target compilers" despite what everyone likes to think. They require intricate knowledge of each platform and require you to *know* the target at all points in the compilation pipeline.
That certainly seems like an interesting idea. I wonder how it would work though in terms of bounty scope/goal setting. With bugs, it is less of a grey area whether a bug is fixed or not. If a contributor takes on a doc bounty but the maintainer doesn't quite like the patch, then what should be done 🤷‍♂️
What is source plugin? 
I mistook you saying generics to mean generics in the Rust/Java/polymorphism sense, instead of GHC.Generics. Yes, I know about generic-lens, it is pretty cool. Can a similar mechanism be used to make quasiquotes work? I'm not saying that TH doesn't have its flaws, it certainly does. What I'm saying is: Afaik (admittedly I know very little), not all legitimate use cases of TH can be solved using GHC.Generics equally easily. I'd be more than happy to proven wrong 😃.
I wasn't talking about LLVM bitcode though. Core would only need a few changes to be target agnostic. The problem then becomes the frontend, with CPP allowing people to write platform specific things.
&gt; Shipping IR is absolutely not less "anti-multi-target" I believe the GP's point is that shipping platform-specific object code instead of an IR is inherently anti-multi-target. You're in violent agreement. 
I'm saying they're *both* anti-multi-target.
The Update typically has all Maybes so that you can choose not to touch a field.
"The problem" being the ability to embed any platform specific aspect of the target environment into your binary at any point *is* _the problem in its entirety_, however. Everything I said about LLVM IR is true of Core more or less. What happens, exactly, when `sizeof (undefined :: CInt)` is inlined into your core and it turns into the literal `4` very early in the compilation pipeline? How exactly do you handle this? Using an `#ifdef` at the source level is literally no different than any other system, it can easily be botched or missed (for example, because it might be generated by Template Haskell) and no IR is going to change the fact that the *source program* must be modified, or recompiled, to reflect this fact. All compiler IRs, and object forms, are inherently multi-target, and almost none of them work in any meaningful way across different compilation targets. You cannot escape it, but you can maybe hide it (and also screw it up.)
We have a few. Pandoc is a really good one, a lot of people have read that. Xmonad is also pretty solid. We could always use more though. Let me know if you have some good ideas.
Doesn’t necessarily imply moving to strict by default. All you need to add is another function arrow type that represents strict functions.
Enum should be a part of the ordering hierarchy imo. Num should be part of a ring + group hierarchy. 
??? You can use STRef/IORef if you really need mutable state.
I'm pretty sure you can have both the stack and the cabal files alongside each other. I remember switching back and forth between stack and nix + cabal and it was pretty painless. 
In general, we try to separate the decision and the calculation as separate components. This is seen in the 'smart constructor' paradigm, where the constructor essentially compartmentalizes deciding the state of the parameters from the actions performed on the parameters. It's very similar to using tagged enums and switch/case instead of big / if then trees, but Haskell makes this really easy to do without a bunch of boilerplate. Use of guards or if/then isn't quite a code smell, but it's much more idiomatic to use pattern matching and case statements to select code paths. The idea is that this eases maintenance because it lets you isolate the decision from the calculation. That gets hard to see the benefit of until you get used to it, but the first couple times you find a bug, you'll start understanding why it makes fixing or expanding the behavior easier. 
I'd suggest that defining what "novelty" means to the team might be a necessary precondition. For 99% of dev teams, using Haskell at all is novel to the extreme. Additionally, there will always be some technologies that some members of the team are more familiar with than others; presumably you hire a team not just to slot in more people with more hours, but to have a diversity of skillsets too. The trade-offs involved in any change to dev methodology and tools should be presented as up-front as possible, but there's still no silver bullets or universal slogans -- especially when dealing with other people.
Perhaps dead-code elimination? It is lamentable (in my opinion) that besides the (sometimes) unbearable long compile times we get (relatively) fat binaries. I'd be willing to spend even a "little" more time in an LTO build if the result were a small, minimal binary.
I personally quite like stack for GHC-only projects, but nix + cabal new-build (for incremental building) is far nicer for GHC+GHCJS projects which are 90% of what I do these days. 
YESSS! This is one thing Elm is desperately missing. Well, one of a few things...
&gt; What happens, exactly, when sizeof (undefined :: CInt) is inlined into your core and it turns into the literal 4 very early in the compilation pipeline? Why does that have to happen? Core can leave the evaluation of the size of CInt to the backend. E.g. Java class files are a good example of a multi target IR. There's nothing preventing core from existing at that level of abstraction, except the existing tooling. And the whole premise of the thread is throwing out whatever existing stuff you want, no matter how unreasonable
In a non-strict language, `(.)` is correct is in order of execution. In `f . g`, `f` is entered first and `g` might never even be entered.
Well compiling lazy ensures a lot of overhead. Lots of things need to be wrapped, thus resulting in a bigger JS blob.
Invoke the SIGINT signal handler, either the `IO ()` registered earlier, or the default one which exits the process (effectively killing all threads).
Not seeing every Haskell program enabling different esoteric experimental language features. It's as if every Haskell programmer is speaking a different dialect. Makes it very hard for a newcomer from read other's programs – all the smartypants custom operators don't help either – and really made me stop wanting to learn the language.
stack uses the project.cabal file(s) and adds an additional stack.yaml file to specify additional stuff. Some people like to write package.yaml files instead of project.cabal files, and stack has support for that built in. But it's optional.
I would say one that has more explicit staging, a la Racket.
Numeric class hierarchies are a _lot_ of work, and it can be the case that nicely laid out towers of abstractions slow down or impair the compiler too much. See https://github.com/tonyday567/numhask for a current example
I suppose exceptions are shaping up to be the next "billion-dollar mistake". However, I think it's worth mentioning that (async or impure-in-pure) exceptions *are* similar to signal handling[0] on POSIX, but there nobody really bats an eyelid when POSIX declares that "unhandled" means "program dies with no chance of cleanup". Of course things are somewhat different in language where internal consistency may depend on cleanup. On POSIX isn't really just things like files, explicitly shared memory, etc. This makes me wonder if there might be a design for a Haskell/Erlang hybrid where "spawn a thread" would be a first class construct and all sharing across threads would have to be done via messaging. [0] ... which has its own host of problems, but I digress.
In general no. But if you add some type class constraints then yes.
How to add type class constraints?
Upvote for being one the *hugely* most bang-per-buck suggestions. A lot of the other suggestions in this thread are -- at best -- "more research needed", but this seems like it should be within grasp. (Even if it were to be hardcoded -- there *are* advantages to 'hardcoding' as you pointed out yourself in a subcomment.)
Esoteric is in the mind of the beholder. I don't think most programs enable experimental extensions.
I don't know that we *have* to pull anything out of `base`. Just don't mark the troublesome things as `{-# ANN CheckTotal #-}` and possibly mark a few things that are total, but where the totality checker fails as `{-# ANN UnsafeAssertTotal #-}`. People that don't care (enough) about totality can use base same way they always have. People that want to check totality can add `{-# ANN CheckTotal #-}` to their bindings and in the definition restrict themselves to the total part of base. 
Just seeing this post now! Thanks for adding this to refined!
This looks neat. Does it come with bad performance?
Good points, but some of the examples of what's more novel than something else are a bit off base IMO. For example, is `servant` more novel than `yesod`? I think any architecture with an API will use servant instead of yesod. Yesod only if there is a simple website without an explicit API. Is `docker` more novel than `heroku`? I pretty confident that `docker` has more issues solved than heroku.
[Shellcheck](https://github.com/koalaman/shellcheck)!
I'm certain there's a knot tying joke here, just can't quite put my finger on it.
You might want to try a different subreddit: https://www.reddit.com/r/haskellquestions
`vinyl` supports [array-backed records](https://hackage.haskell.org/package/vinyl-0.10.0/docs/Data-Vinyl-ARec.html) with "the usual lists vs arrays" performance tradeoffs: O(1) access in exchange for expensive extension.According to the [readme](https://github.com/VinylRecords/Vinyl/blob/603ead3469986d9134b3eefd98e2442f9e236874/README.markdown), field access for either form is reasonably fast. And unless things have changed in the past couple of years, GHC can optimize non-persistent record usage pretty well.
`f :: (Num a, Num b, Num c) =&gt; (a,b,c) -&gt; Maybe b`
This is my number one feature ask for the language, and if I had infinite time it's something that I would like to propose to add to GHC. Has there ever been a concrete proposal to add this? Even if it was rejected it'd be good to know potential concerns, etc.
`map` is still a separate function, IIRC. There aren't any typeclasses that would make sense for zip or list producers in base. head isn't actually list-specific: `head = foldr const (error "Prelude.head: empty list")` is a valid implementation, and generalizes to any Foldable.
The issue with alternative preludes is that there are too many of them, and not enough people use them. They also tend to have rather large dependency footprints, which is awkward for me personally because my old PC already takes quite a while to compile.
Well, there is fmap also in the prelude. It could be argued that it should be an alias for fmap - and similar that concatMap should be an alias for `&gt;&gt;=` - but I don't think there is a huge advantage to that and it'd break huge amounts of code. Technically `zip` is a special case of Applicative but the default applicative for [] is cross product. Using Control.Applicative.ZipList for everything would be pretty unwieldily. 
A standardized, extensible format designed for storage on the filesystem specifically to support editor tooling, supported by the compiler, with a library in base for manipulation and extension. Like tags, but extensible and several orders of magnitude more powerful. Should be designed to support incremental and recoverable parsing. 
You probably aren't going to have that level of education in every part of your stack, though, and nor should you. Requiring a detailed understanding of how the kernel works in order to write a web app is just a non-starter - so, in the absence of knowing in detail the tradeoffs between using Linux and a custom unikernel on Xen, you should go for the more widely-used solution. Dealing with your inadequate understanding of the millions of lines of code your app rests on without knowing them in detail is exactly why this is a hard problem. &amp;#x200B;
I guess so, but for the purposes of the discussion in this post, it's another layer of indirection for the deriving clause.
Well, today every single I/O action must carefully consider exception safety in all possible cases. Having less places you can throw from does not make this any worse.
&gt; `^Foo` Is that the syntax we've settled on?
It gets murkier in a team environment though 
You'll need to decide what exactly you want to compare between them and then make some type classes or a higher order function for representing that comparison. What do you want to compare?
&gt; all the smartypants custom operators Well, the language doesn't *have* any built-in operators. So, you'll need to gets used to "custom" operators quickly. Sure there's a "standard set" in the Prelude and standard library, but not being limited to a small set of built-in operators is an intention design of Haskell that I think is an advantage.
We don't use ZMQ, we use AMQ, but for most of what we are doing it's in the clear with no authentication. When we do require authentication, we can afford the overhead of TCP/TLS and use client certs.
I hope he would at least agree that backtracking on equality constraints should be harmless.
If I had ultimate power I'd make Haskell [a total language](https://en.wikipedia.org/wiki/Total_functional_programming) :).
That looks like a continuation of the discussion in [your previous question](https://www.reddit.com/r/haskell/comments/9fhdxg/how_to_check_whether_input_is_of_certain_type/), in which you asked how to check whether an input has a certain type. I think you are doing yourself a dis-service by asking a brand new question which doesn't have the context of the previous one, because we're now going to have to ask the same questions as before in order to figure out what you _really_ want to know. Which is still not clear to me. Something about `Maybe`?
I fit this description. :) There are data scientists who are more or less interested in engineeringy/computer sciencey topics.
Interesting: not heard of that. Thanks!
Add reflection. Dump all the bullshit bug compatibility. Add native support for a call graph visualization with abstraction levels. Excommunicate PureScript and just fucking implement row types already. Get Haskell to compile to WebAssembly and disconnect from JS altogether. Stomp Elm to the ground. Burn the Haskell book.
No. You haven't given the compiler a way to know anything about a, b , or c, so it can't answer any questions about what functions you'd be allowed to call on them. Likewise you can't pattern match on them because you don't know their constructors. In fact, this signature can have only two possible implementations: f (_ , b , _ ) = Just b Or f (_ , b , _) = Nothing You're locked into either returning the second element of the tuple, or dropping it on the floor. This is what is meant by 'parametric polymorphism' - `a`, `b` or `c` can be anything you want them to be at the call site ( Including being the same type, (1,2,3) satisfies this just fine ), but `b` needs to be the same going in as it is coming out. Haskell knows what functions we can call on these types by supplying 'constraints' - f2 :: Eq b =&gt; (a -&gt;b) -&gt; (a,b,c) -&gt; Maybe b f2 convert (a,b,_) = if (convert a) == b then Just b else Nothing Here we're free to supply any function that takes the first element of that tuple and returns a type that's the same as whatever we pass to the second element of the tuple. We've said that the second element of the tuple needs to implement `Eq` (this is a Constraint), so we know we can use the `==` on it. But, we need to be sure that the first element is the same type, because `==` can only compare the same types, so we have to force it to be the same type. Note, that doesn't mean we have to change the first element of the tuple at all - We can do something like: `f2 id (2,2,5)` and get `Just 2`. That works because the result of passing an `Integer` `2` to the identity function is that we get an `Integer` back out, and the second element of our tuple is an `Integer`, so it satisfies all the rules we gave the compiler with our definition. Does that help illuminate the situation?
It's possible my experience is out of date there - I tried to deploy servant a year or two ago, and it was still missing a lot. Auth was pretty sketchy and a lot of the stuff you'd like to provide like cookies was missing - yes, it's mostly for APIs, but a lot of the time you're going to want a small actual-website attached. The point with docker vs heroku is more that using heroku makes it somebody else's problem. For my own private apps I tend to rsync a keter tarball to a directory. Pretty cavemanesque but it works reliably.
Yeah, part of the problem is that "novel" is always going to be part situational and part universal. Haskell's not much of a novelty for me personally, but I'm still going to pay the cost of there not being bindings to particular libraries - had to generate a Haskell API to a VMware WSDL, for instance. If we'd been using Java or Python, that would have come free. 
Seriously. I mean, what's it going to accomplish that a simple print $ [72,101,108,108,111,44,32,87,111,114,108,100] &gt;&gt;= return . chr won't?
https://i.imgur.com/eipugIu.jpg
I've been mulling over experimenting something similar with Conduit. I think I may have an answer for you. I present: [conduit-pandas](https://github.com/naushadh/conduit-pandas).
Not all preludes have big dependency footprints. Some of them depend only on `base` and some depend only on `text`, `bytestring`, `mtl`, `containers`. I'm personally working on such prelude. And what can I do is just make it better, write more tutorials and more documentation to help people use it.
I don't think it's settled yet. There's not even a formal proposal yet, to the best of my knowledge. I'm sure they'd be open to alternatives.
This is a good answer. To building further on this: It's easy to mix up the `a` and `b` when resolving types by hand. So I recommend before starting, that you substitute them for different groups of letters: ``` foldr :: (e -&gt; f -&gt; f) -&gt; (f -&gt; [e] -&gt; f) (.) :: (q -&gt; r) -&gt; (p -&gt; q) -&gt; p -&gt; r map :: (x -&gt; y) -&gt; ([x] -&gt; [y]) ``` From here you can make substitutions, such as: `p :: (e -&gt; f -&gt; f)` (because `foldr` is the second argument we passed to `(.)`, see?) Keep making these substitutions until you only have `e` and `f`. Then, figure out which is `a` and which is `b` in the supplied choices.
Neat, thanks! Here's some feedback. I only have experience with intero with spacemacs, not with intero for neovim, so my apologies if some of my comments are already well-known differences between intero for spacemacs and intero for neovim. Let me know if you would prefer if I created issues on your github project instead. intero doesn't start automatically, even if `let g:intero_start_immediately = 1` is set. Unlike in spacemacs, when the intero window is selected, I cannot use `&lt;ESC&gt;:` to type commands such as `:q`. This is especially annoying if I reach the buffer using `:bn`, because that means I cannot use `:bN` to go back to the previous buffer. Unlike in spacemacs, using `:q` to close the file with which the intero buffer is associated doesn't automatically close the intero buffer, instead the intero buffer buffer becomes the current buffer. I guess I just have to remember to quit with `:qa!` instead of `:q` from now on. Unlike in spacemacs, `:InteroReload` does not save the file before reloading intero. It was easy to make `\ir` do that though, by tweaking the provided `.vimrc` snippet. `:InteroGenericType` and `:InteroType` both worked fine, but `\it` and `\iT` did not. I notice that those two use `&lt;Plug&gt;` instead of `:` in your `.vimrc` snippet, what is this supposed to do? When I replace it with `:`, then `\it` and `\iT` both work fine too. In spacemacs, I can select an expression and ask for its type, whereas with intero-vim, it looks like I can only ask for the type of the identifier under the cursor? When `:InteroGoToDef`/`\ig` goes to a definition within the current file, I can't use `&lt;backtick&gt;&lt;backtick&gt;` nor `&lt;Ctrl-O&gt;` to go back to the previous position. The defaults seem to be different than in intero for neovim; the documentation implies that `set updatetime=1000` is the default, but when I enable `let g:intero_type_on_hover = 1`, the type appears after 4 seconds, not one. While `:InteroUses` does use [`:uses`](https://downloads.haskell.org/~ghc/8.2.2/docs/html/users_guide/ghci.html#ghci-cmd-:uses) in order to obtain the uses of the identifier under the cursor, it doesn't seem to do anything with that list; instead, it sets `:hsl` to highlight the search results, and searches for the current word, which highlights more occurrences than the `:uses` list. While recent versions of ghci support `:type-at`, `:loc-at`, and `:uses`, and setting `let g:intero_backend = {'command': 'stack ghci'}` does switch the backend from intero to ghci, intero for vim does not run `:set +c` before loading the file, and so all those features are disabled. Setting it manually partially fixes the problem: `:InteroGoToDef` and `:InteroUses`: work, but `:InteroType` and `:InteroGenericType` don't, because intero for vim doesn't quite give the right range arount the word under the cursor, and ghci is less forgiving than intero about those ranges.
I would like to treat Haskell code in as simple way as we treat data. Basically, code as data. When `module` is just a list of declarations. And if, for example, I have the same 10 lines of imports in every file, I can just write something like: :commonImports :: ModuleM () :commonImports = do :addImport Data.Traversable [for] :addImport MyPackage.Core [Id, Email] ... and so on And later I can just write `:commonImports` in import section. In other words, I would like to have better and simpler meta-programming system, where generating code can be done using the language itself. But `TemplateHaskell` has a lot of limitations. It's not possible to generate imports with `TemplateHaskell`. Having patterns as first-class objects would be really nice as well! Or, and local imports or local namespaces in other words. I would really appreciate this feature. I like Haskell because it allows to not keep big context in your head. So if I'm using single import statement only in one function from line 1450 to line 1521 then it would be really nice to write the import only near this function to make context clearer.
I agree with majority of the proposals here. It's nice to see how the language we all love and use can be improved and what people really miss! I've added my wishlist via the following comment: * https://www.reddit.com/r/haskell/comments/9fefoe/if_you_had_the_ultimate_power_and_could_change/e5y2uyb/
Because they are a cheap version of the real thing?? #BoomerangBoyz
Wow, thanks for the feedback! I'm rather sleepy at the moment so here's the plan: - I'm going to go over the differences you mentioned tomorrow and sort them based on whether they're intero-neovim concerns or intero-vim issues - I want to minimize the amount of "divergence" between the two versions at this point. The immediate goal is to have a similar experience across neovim and vim and make the plugins share as much code as possible. - Once that's in place all other concerns regarding behavior can be dealt with. You don't need to put stuff you've already written here on GitHub. If you have anything apart from this you'd like to point out I suppose it would be better to create an issue. Thanks again for being thorough!!
Would like to know _why_ is streamly using lower memory? I'm assuming both the libraries are built off the `forkIO` primitive, right? Is it that streamly is using more space-efficient data-structures for book-keeping on top of forkIO? And does the space-efficiency also result in time-efficiency because of lesser bytes that need to be copied around?
Split the UI into a pure "logic" part and a non-pure IO "update" part. I recommend searching for "Javascript React" to get ideas for how to do this in practive (UI innovations happen in the Javascript community. Not in the Haskell community).
A "serious" language doesn't need open letters. C/C++ runs the world. Without any open letters trying to convince anybody that those are "serious" languages. C/C++ programmers are too busy writing serious software to care about what others think.
My question wasn't about the format itself, as much as content. In Haskell, some libraries have very good documentation (e.g. anything written by Gabriel Gonzalez). The challenge is having that level of quality throughout the ecosystem. I understood your comment to mean that a large fraction of Elixir packages are well-documented. Was that correct or did you mean something different? 😅
Thanks! This is certainly a set of interesting wishes, more Lisp-y than the other ones here. The common imports can be solved by having re-exporting through a local prelude though? module MyPrelude (module Data.Traversable, module MyPackage.Core) where import Data.Traversable (for) import MyPackage.Core (Id, Email) Local imports seem like a relatively reasonable thing, I wonder why we don't already have them. Perhaps someone has already proposed them on GHC Trac at some point... Someone else in this thread suggested that we _shouldn't_ be metaprogramming at all (apart from using GHC.Generics) -- maybe you'd like to have a word with them :P
Ran into this one just now. https://github.com/BurntSushi/erd
You might also be interested in The Elm Architecture. It's been adapted into many languages and frameworks. https://guide.elm-lang.org/architecture/
Internally, it is represented exactly as you wrote. In the API interface, since I did not know how it will evolve, I started with the C style and have not been able to pay much attention to it yet though it is on the todo list. In fact there are three states to be represented Unlimited, Limited and Default. Why Default? This setting persists across the stream so the Default can be used to reset to the default without knowing what it is.
`Prelude` trick works to some extent. I'm using `base-noprelude` package and really helps to clean-up common imports. But you can't do this for modules in your package, unfortunately, because they depend on the `Prelude` already... But it's possible to create another module. Regarding local imports: I found only this proposal: * https://ghc.haskell.org/trac/ghc/wiki/Proposal/OpenImportExtension I think metaprogramming is too useful to drop :) Also, it's not enough to have GHC.Generics for another reasons: `GHC.Generics` introduce performance overhead for converting to/from generic representation. This is one of the reasons why people sometimes derive `ToJSON/FromJSON` instances using `TemplateHaskell` instead of anyclass deriving via `Generic`.
Maybe better: data Limit = LimitTo Natural | Unlimited 1. `Natural` instead of `Integer` to not have negative limit. 2. Put `Unlimited` as the second constructor so in derived `Ord` instance it always bigger than any `LimitTo smth`.
I wish I had multiple upvotes...
Agreed. But then we already have `IO (Either exception result)`. [unexceptionalio](http://hackage.haskell.org/package/unexceptionalio) is a good attempt.
Laziness... But then that would require Haskell to become a whole new language. The `Strict` pragma is just not enough
I promise to adapt and merge it soon! :)
I'm also curious about this. Btw there are other libraries that have better performance over base's concurrent data types, like [unagi-chan](http://hackage.haskell.org/package/unagi-chan). I haven't intestigated too deeply myself but I'm guessing that maybe base's concurrent types are not that hard to improve.
Why would `ghci` take such long time to do this? :t 1e1234111111111111111111111 Do it has something to do with haskell or ghci fails to optimize it?
Could you, please, elaborate on the reason why FRP is not a way to go?
There was a lot of work on [https://github.com/haskell-servant/servant-auth](https://github.com/haskell-servant/servant-auth) recently, which allows you to have cookies or API tokens. I do think proper documentation is missing, hoping to get that together on haskell exchange.
Thanks!
Good points!
Better than an open letter, write a serious application in Haskell or build a successful business on top of it. Basho building Riak in Erlang or WhatsApp using Erlang to build their business is more persuasive than any open letter. Look at all the serious things built in Go, a far more limited language than Haskell, eg Kubernetes/terraform/vault/consul. They’re getting used in all sorts of places and people are learning go or at least getting a go halo from them being written in it. 
Huh. I also get surprising behavior here (ghc 8.4.3). The process ate up a huge amount of memory, but never got anywhere. Eventually I got the following warning/error: GNU MP: Cannot allocate memory (size=93978265) I promptly killed the process at that point. Based on the memory consumption, my guess would be that ghci is actually trying to construct the term rather than just compute its type. Which is odd, since that's not what we asked it to do. And it shouldn't need to do much thinking anyway: typing a literal should be a fairly trivial affair. It seems to work as expected for smaller literals: &gt; :t 1e5 1e5 :: Fractional p =&gt; p But once I go past 8 digits after the `e`, it starts to get noticeably slower. This feels like a bug to me. I'd expect to be able to ask for the type of a potentially large term, without having to worry about ghci eating all my ram. After all, `:t [1..]` works. Not quite the same, as that is lazy and a big float lit probably isn't, but still. 
You mean like typed template haskell?
What uses for more than two stages do you have? 
Have you try changing type of glasses until they fit?
&gt; Eventually I got the following warning/error: &gt; GNU MP: Cannot allocate memory (size=93978265) Thanks for providing extra info. I lost my patience after the terminals hung up for half a minute and didn't see the memory allocation error. Bug reported here, https://ghc.haskell.org/trac/ghc/ticket/15646#ticket
Is TH not typed enough ?
I mean only one stage, so you can define and use things in the same file.
I don't think it's an "education budget". That's a different topic. The problem isn't education; that implies all the information you could need is already out there and all you have to do is just read it, which would be the case for a mature piece of technology. The problem is the unknowns that come with a _new_ technology: you can't document what hasn't been discovered yet. For example, on one Haskell project about 8 years ago, I needed to process PDF documents. I looked around at the existing sad crop of Haskell libraries. In the end, I picked a Java library, even though I'd never written a line of Java in my life, because I knew that (1) Java is stable, tried and tested, (2) the PDF library was industry standard and stable. What's the difference? The risk factor for the Haskell libraries was novelty (worst case: the whole thing has bugs), the risk factor for the Java libraries was education (worst case: I have to read through boring Java API docs). When a completely novel way of doing a system architecture comes up, you have novelty risk.
I don't know what Panda is but it seems that you are trying to join sources as I do. Are the joins in constant space ? It looks like using \`CC.length\` as to consume the full conduit, which defeat the object of using conduit initial.
One of the goals of NumHask is runtime performance and there are a number of benchmarks in place but I guess it also depends on how it's used. We'd all love to hear more user reports.
You might be looking for [IMGUI](https://caseymuratori.com/blog_0001) - although be aware that the original (dear imgui) implementation doesn't solve any layout problems and is therefore pretty awful to layout / work with - but that is a matter of implementation, the core principle of IMGUI still stands and IMGUI follows the principle of pure functions - however it mixes styling and data logic into one thing, which I regard as bad design. While I don't know Haskell (but I do like functional programming), I've built a functional (yet unfinished) UI framework called [azul](https://azul.rs/). In my opinion, many frameworks aren't as purely functional as they could theoretically be, since they are often hamstrung by the state of the browser and they have to take into account certain performance limitations. So I thought: since Rust is a pretty fast language I could trade in a bit of the performance in order to make GUI programming less awful, less OOP-centric, but still solve problems that IMGUI has mainly regarding seperating style and layout from the application logic. When I originally explored this idea, my idea of a GUI framework was pretty simple: 1. Initialize the app with a certain state 2. Ask the app state what the layout for this frame is (`AppState -&gt; Layout`) 3. Render the entire UI to the screen (this is stateful IO no matter what you do) 4. Hit-test the UI and call the necessary callbacks 5. The callbacks modify the application state (or: if you want to go pure-functional, you can also return a new AppState) (given the inputs for this frame, map `AppStateA -&gt; AppStateB`) 6. Go back to 2 and repeat. What I realized when actually implementing it however, is that it's not all as clean as it sounds on paper and there are many ways to do it, but that's the core "concept" of it. There is almost no mutation (in the Rust version I let callbacks modify the application state because of performance reasons, but again, you could do it in a pure way by simply returning a new application state), styling and layout is done via CSS and the stylesheet is parsed once at startup and then those "rules" are applied *on each frame* to the new layout - trading performance for ease of use. So my model that I've arrived at takes concepts from MVVM, IMGUI and retained-mode OOP models and combines them in some way - I've found that you can almost never follow some other persons model (let's say Elm) directly and you *need to experiment a lot*. There are a a lot of decisions in the implementation, ex. do I use FRP or not, do I render directly to the screen or not, do I cache texts, do I allow variables to be retained for performance reasons. In practical use, GUI frameworks are always impure in some way - for example, to render a font, you can't realistically expect to re-parse, re-load and re-render the entire font every frame, it would be dog slow. So in those cases, you are forced to use retained state, there's no way to get around this. There is a paradox in GUI development which I call the "visibility problem" and which I have not yet solved (and neither OOP nor IMGUI could solve it either): Let's say you have a text field and a label and you want to update the label when the text field is typed in. The label has to somehow be "visible" to the text field - the label has to know that it should re-render and the text field has to know that it should somehow "send" its input to the label. But how do they know about each other? OOP solves in in the simplest, yet most dangerous way: You have a container class that owns them both, and the text input and label hold mutable references to each other. The problem with this is that you have now adjusted your data model to fit the UI - let's say the input and the field are completely seperate from each other, you'd need to create huge classes that "know" about a lot of unrelated state to "connect" these two components. Then you have pub/sub and event-based models such as QT or Elm with events and message-passing. These systems can get pretty complex however regarding the routing of these messages, i.e. how you send messages from component A to component B. Since now you have a "router" which essentially knows about every component in your app, it's not that much better than an OOP superclass that contains all sub-components. The paradox is that in order for two elements to communicate, they either have to know about each other (either via mutable pointers or messages) - which is bad because it couples the two visual components with each other or they have to share memory that they both have access to which is bad, too because you cannot design a memory model that will fit both a label and a text input - the text input needs different things to render itself than the label. So pick your poison - I chose the latter, but I've also heard people swearing on FRP / message passing - both have their benefits, but also their downsides. So essentially, all callbacks share the entire application state and maintainability is done by writing unit tests, not by reducing the scope of access of the components - you can test that if you have state A and you call callback B, you get state C out. Also, regarding event handling: I've set it up so that the callbacks can "query" the framework for events, such as where the mouse is or similar. This way I can (in unit tests) set up the framework to "simulate" a certain event state and test how callbacks respond. A callback doesn't know what called it, if it was a mouse-hover event or a click or whatever. It can only query the current framework state and then respond accordingly. And lastly you have the problem of default behaviour: Generally when using a GUI framework I don't want to call the `update_text()` function manually, it should somehow happen automatically. OOP achieves this with subclassing, I achieved this with some tricks regarding void*, but maybe Haskell has better tools so that you can essentially implement behaviour on data types that update automatically without the user writing any code (automatic two-way data binding). In conclusion I think that a mix between IMGUI and FRP is the future of UI programming, but maybe you can come up with a better design, who knows. Hope this helps.
Hehehe.
If I understand correctly (based on [1](https://ghc.haskell.org/trac/ghc/wiki/Records/OverloadedRecordFields/MagicClasses), [2](https://github.com/adamgundry/ghc-proposals/blob/overloaded-record-fields/proposals/0000-overloaded-record-fields.rst), and probably some other sources I've forgotten), GHC 8.6 with a bunch of extensions will support record access with `#foo`, like ```haskell data ThingA { id :: ThingAId, val :: String } data ThingB { id :: ThingBId, val :: Float } getIds :: ThingA -&gt; ThingB -&gt; (ThingAId, ThingBId) getIds thingA thingB = (#id thingA, #id thingB) ``` But also, I think this is almost entirely supported in GHC 8.4. The only thing missing is an `IsLabel` instance for functions. And I can write that instance myself. ```haskell import GHC.Records (HasField(..)) import GHC.OverloadedLabels (IsLabel(..)) instance HasField x r a =&gt; IsLabel x (r -&gt; a) where fromLabel = getField @x ``` I've tested this, and it seems to work. If I import that module, I can use `#` for record accessing. But I'm worried something unexpected will bite me if I adopt it over my whole codebase. What things are likely (or even unlikely-but-possible) to bite me unexpectedly, if I use this?
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [adamgundry/ghc-proposals/.../**0000-overloaded-record-fields.rst** (overloaded-record-fields → 9f2ac92)](https://github.com/adamgundry/ghc-proposals/blob/9f2ac922e739f3db85f62017e4a8157515b7c2d3/proposals/0000-overloaded-record-fields.rst) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply e5yh9rk.)
Here is original Haskell Lens video I was surprised to see [https://www.youtube.com/watch?v=qfCd89enCXE](https://www.youtube.com/watch?v=qfCd89enCXE)
I mean the feature which is already implemented called "Typed template haskell" which has typed quotation and splicing. 
Is there a good reason that `mappend` has an INLINE pragma but `(&lt;&gt;)` does not?
No. Haskell is love, Haskell is life.
&gt; list of haskell apps: pandoc, xmonad So, same as 10 years ago? In all seriousness, why does no one mention [postgrest](https://github.com/PostgREST/postgrest)?
Haskell isn't a learning exercise to help you write better code in other languages. Build something with it.
Because almost every other language (Type Theory, SML, OCaml, Coq, Agda, Idris, Lean, Scala, Typescript, Rust, etc...) uses `:` for 'type of', and type ascription is used far more often than list cons.
[removed]
The problem with total languages is that their programs can not keep running until the user quits them.
Or a mostly total language with a single loop at the top level.
nothing specifically, it was just an example of 2 different avenues of learning something familiar and accepted by industry vs not. 
I read about Haskell for a few years but nothing started to sink in until I actually started doing things. Go create a website with Yesod or got an itch to build a script to do something script it with Haskell! Maybe even use [turtle] (http://hackage.haskell.org/package/turtle-1.5.10/docs/Turtle-Tutorial.html) ... in the end just try and do something more then a toy function as it that’s when it started to “click” for me because I started to see I really could program anything I wanted in haskell :)
There's always a trade-off when you decide to learn something, You have to invest the time and not learn something else. You could stay with a language you are comfortable with and learn certain domains that interest you and you might have more tutorials that use that language as well. Or you can explore something else and learn a new methodology of writing software along with a family of languages you can use for different things. Learning Haskell will also let you get familiar with new domains and it might also be a really good language to use in order to learn them. For example compilers, build systems and concurrent programming. I think today we have more/better resources for beginners and the learning curve is less steep which makes that time investment more sensible. If you feel like you can't build something with Haskell, ask for advice and share your concerns. Many use Haskell to build cool things and the required knowledge to start building your own stuff is lower than you might think.
Whatever you would have built in golang, just build that in Haskell instead. It'll be more enjoyable and you'll almost certainly end up with a more maintainable program.
[Cardano settlement layer](https://github.com/input-output-hk/cardano-sl). Here are the [top 50 haskell projects on github by stars](https://github.com/search?o=desc&amp;p=5&amp;q=language%3AHaskell+stars%3A%3E790&amp;ref=searchresults&amp;s=stars&amp;type=Repositories). [hledger](http://hledger.org) is steadily climbing the charts so I'll mention it too. :)
&gt; If you mean performance I don't mean performance, I mean "the work it would take to implement" compared to what you would get from much simpler approaches like simple string prefix completion. &gt; You say that most code is written with pure, unconstrained combinators, e.g. SKI. I don't think that's what you wanted to say; IMHO as soon as any constraint appears that produces a legible type hole output, the same output is at least worth putting into autocompletion list. It sounds like we're talking about different things. I'm talking about completion "as you type" on unfinished expressions, like what you get in OO languages after you type a dot. It sounds like you're talking about completing possibilities for a type hole in a completed expression? If that's the case, then I agree that there would be significantly fewer possibilities and completion would be correspondingly more useful.
This is an important and legitimate distinction that I wasn't making with my argument. 
WTF did I just see?
Extensions that increase the expressiveness of Haskell that I like: - RankNTypes - PolyKinds, DataKinds, GADTs, ExistentialQuantification, TypeInType. I consider all of these to be related. Often, they need to be used together. - TypeFamilies Extensions that do not increase the expressiveness of Haskell that I like: - OverloadedStrings - LambaCase You can go a long way with just Haskell98. Be careful with typeclasses. People use them to do weird stuff sometimes when plain old functions and data types would be a much more readable solution.
no problem! good luck what whatever you choose to do.
Not specific to `Go`, but faithfully translating (i.e. preserving complexity bounds) algorithms which involve a lot of mutation can get tricky. Sure, you can get it work with some amount of thought (or use ST/IO), but it can certainly be more challenging.
Might be handy to check out https://limperg.de/ghc-extensions/ and/or https://www.schoolofhaskell.com/school/to-infinity-and-beyond/pick-of-the-week/guide-to-ghc-extensions I'd echo that you can do a lot without these extensions. I think it's not so much that the core language is not powerful enough for "real" code, but more that it's powerful enough to be tantalizingly close to incredible, and extensions help bridge that gap, especially for library design.
Extensions that make the typeclass system work in less-surprising ways. There's nothing to learn to use these, except how and when to turn them on. * `MultiParamTypeClasses` * `FlexibleContexts`, `FlexibleInstances` * `UndecidableInstances` Turn on `MultiParamTypeClasses` when you want to make a type class with multiple parameters. Turn on `FlexibleContexts` and `FlexibleInstances` when you get a compiler error saying something doesn't work, did you mean to use `FlexibleContexts` or`FlexibleInstances`. Turn on `UndecidableInstances` when you get a compiler error saying something doesn't work without it. You're turning off the totality checking for instance resolution at compile time. The worst thing that can happen is the compiler runs forever.
Can you elaborate a bit? Count me in the camp of people who wish Haskell wasn't lazy, but I'm curious to hear more what exactly you want and why (or maybe it's simple and you're just saying you wish Haskell wasn't lazy :) ) My main reason for not wanting laziness is that I really wish I could write Haskell on the frontend (both mobile and browser) and laziness is often the biggest hurdle with getting GHCJS to cooperate effectively. That and the increase in debugging capability. Also, almost every example or argument I've seen showing laziness allowing for more expressiveness hasn't been that convincing to me (the strict equivalent often seems just as expressive/declarative/composable, with rare exception and especially in normal "commercial" code). To each their own on that last point though, but to me the sacrifice in getting to practically use Haskell on the frontend and the hit we take with debugging just isn't worth it 
All the `Derive` extensions. Use these when you want GHC to derive typeclass instances for types for you * `DeriveFunctor` derives `Functor` instances * `DeriveFoldable` derives `Foldable` instances * `DeriveTraversable` derives `Traversable` instances * `DeriveDataTypeable` derives `Data` and `Typeable` instances * `DeriveGeneric` derives `Generic` instances
It was the entire point of the blog post, though.
Is the benchmark fair when you are using a thread pool for streamy vs leaving it unconstrained for async? mapConcurrently will spawn as many light threads as elements in the provided list, which is in my experience wasteful. Maybe a note about this should be added in the README?
I think you're missing the point here. It's not about producing something beautiful or even enjoying what you do. It's about limiting the amount of risk you take on. Specifically, limiting the risk that your choices will make your task effectively impossible to complete under the constraints that you have. So yes, angular is huge and heavy and boring and feels like walking through sludge, but you can be sure that you'll never be forced to stop walking. You'll almost certainly never be forging new paths through uncharted mud. Stack overflow will be there to pull you through.
I second that, I was extremely happy with \`servant-auth\`. It handled as much as I would expect an auth module to handle, and it put me in charge of exactly the things I wanted to have control over. 
The only extensions I feel are 'must learn' are overloaded strings and the deriving X stuff. Everything else is completely optional. Super useful sometimes, really great, but not mandatory.
I have a slightly related question. Is there a difference performance wise between \`fold\` and \`mconcat\`. &amp;#x200B;
Partial evaluation. You simply make anything that is system dependent a stuck term, and defer it for later. This avoids moving optimizations around the pipeline by instead having not all of the expression progress through the pipeline together. It needs a huge refactor of GHC to implement the requisite incrementalism, but it's not conceptually innovative. CPP then is a problem not because it's at the source level, but because it is in general hard to partially evaluate since unbound identifiers are not recognizable in general. 
Also `GeneralizedNewtypeDeriving`.
I've found that in all jobs I've had there is room to secretly write something in Haskell. Any time you are going to write some bash to make something easier, write it in Haskell. If you get stuck you can always do things in IO and improve over time. Haskell is great for gluing things together in place of bash and as soon as it's more than a few functions it's easier to maintain.
Both are used in industry?
`(&lt;&gt;)` is a class method with no default definition, so `INLINE` couldn't do a thing for it. An instance definition for the `Semigroup` class, on the other hand, can specify `INLINE` on its specific definition. Note, however, that this instance definition could only be inlined in a context where the compiler can `SPECIALIZE` the `&lt;&gt;` call. 
No, I'm talking about "as you type" with implicit type hole assumed at cursor position. &gt; after you type a dot After any standard library combinator and after the user has stopped typing for a few seconds.
I suspect that the whole exception handling abilities of async do make a difference. Actually, that's one of my main reasons to use async. It's handling of asynchronous exceptions and therefore thread termination is really, well, exceptional compared to most other concurrency libraries and I have found it very useful. I just skimmed the streamly readme and there was no mention of the async exception handling, so I have no idea what streamly does in regard to exceptions and thread termination.
GHC will occasionally suggest `DeriveAnyClass`. You should usually ignore this advice.
[This guide](https://lexi-lambda.github.io/blog/2018/02/10/an-opinionated-guide-to-haskell-in-2018/) has a fairly extensive breakdown of which extensions the author considers useful. 
What's "inferred safe"? My natural interpretation isn't meaningfully distinct from "proven safe".
Many things that apply to `async` do not apply to `streamly`. Unlike `async`, there is no explicit notion of threads in streamly, and that's the beauty of it, it takes away all the complexity that the programmer has to deal with. No threads, no threadIds, no asynchronous exceptions. You can of course throw synch exceptions, there is a working `MonadThrow` instance. Catch (inside the streamly monad) is not implemented yet. Let me know if there is a specific aspect of exception handling that you are worried about or something that `async` does but `streamly` doesn't even though it applies to it. I will update the README/tutorial with details.
It would be really nice if pure code was total. It would be also nice if we got two versions of IO, one total and one Turing complete. This would bring some really non-wanted complexity into the type system if done wrong, and I can't even imagine what would look like if done right. But checked totality is a very nice feature.
I'm not missing that point. I'm disagreeing with the definition of failure implied by that definition of risk. Failure needs to mean something beyond just "we didn't ship." Building a complex monstrosity instead of a simpler foundation is also a form of failure, and preventing ourselves from building those monsters frequently isn't a question of avoiding technology or techniques that are new to us.
Sure they can. Limit the number of iterations to 2^(2^128) and you can guarantee they user will quit it before we get to the total number of iterations. Particularly short-lived iterations or long-lived users? Compile with --tower-of-power=3 to bump the limit to 2^2^2^128. Honestly, no computer we can manufacture today will last that many *CPU cycles*, although maybe if we've really learned from the mainframes, we can actually have a program that lasts longer than any single physical component it runs on. The second number (in plank time units) is far older than any nucleon will last.
`OverloadedStrings` isn't very experimental, and that is, as far as I can tell, the only language extension in use. I wouldn't be surprised if `OverloadedStrings` made it into the next report. We've had "`OverloadedNumericLiterals`" as part of the base language at least since the '98 report, and I think before that.
Yes.
A gui
This is extremely helpful! thanks!
Don't forget about `DerivingStrategies`. This gives the user more control over what is going on, and it documents which deriving strategy is being used.
That's what I thought. The use cases are a bit different. Streamly looks good for exactly the examples you provided. I use async for e.g. permanent threads to listen on a STM channel and then write to a socket while another watchdog thread checks the connection status of the socket. In traditional languages like C++ it is surprisingly hard to do this correctly and ensure a clean termination of the threads on request with proper closing of the protocol and socket. With the async library this is really easy and the cleanest approach I have seen until now thanks to it's handling of asynchronous exceptions (try that to do in C++, you'll get headaches as hell. Been there, done that). Also it's exception forwarding to the parent thread is sometimes really useful. I don't think, this is strictly necessary to implement in streamly. I think it is nice for the use cases it was designed to work (it looks to me, it is more designed for data processing and not for system programming tasks). As always: use the right tool for the right job. So anyway, it looks nice and useful, so thanks for your efforts!
You probably just have hacker-brain. Don't feel bad about wanting to learn what you're curious about. You'll be surprised at how all the different things you've learned in your spare time set you apart from others in a good way. A lot of programmers don't even have programming hobbies, hobbie projects, or bother to learn anything outside of work. This is the first thing I try to determine about someone when I interview them for a job.
That one has enough gotchas that you should know what you're doing first.
Functions map every element of A to one element of B. You have #B choices to map to for each element of A.
It's the same question in a team environment, except you have to consider what the team *as a whole is familiar* with rather than just yourself. Broader popularity *outside the team* is still not a high priority.
I feel guilty. Moreover, I was just in the same situation, spending a lot of time comparing language x vs y. At some point, I’ve realized that I need to make choice and start practicing, or I would become an armchair expert, who has written 0 lines of code but can argue about advantages and disadvantages of 100500 languages. And I feel guilty for not choosing Haskell. Haskell is definitely programming language that incorporates cutting-edge technologies, is very elegant and powerful. But there are some cons which made me choose another language. The main two: - Steep learning curve - Tooling Haskell is really complex language, with good amount of libraries and compiler extensions, I felt that I was not ready. That I needed more common, simple solution to get things done. I hope one day I’ll jump into learning this awesome language.
A function from a set of size N to a set of size M can be defined by N (input, output) pairs. If you keep the possible inputs in the same order and vary the outputs, you can also describe it as a list of N outputs, each with M options. For example if we decide to order Bool -&gt; Bool inputs as [**True**, *False*] then the output lists are [**True**, *True*], [**True**, *False*], [**False**, *True*], [**False**, *False*]. So for one input value, there are M possible output lists—one for each value of M, or M^1 . For two, there are M^2 possible output lists. For N, there are M^N .
Ah, I guess I've never really figured out when the empty standalone deriving will work either. The two cases that seem like it should work to me are when there are matching default implementations for all of the operations of the class or when there the class has no actual operations but just super-class constraints. Are there other scenarios where it will work?
You might find this useful: https://limperg.de/ghc-extensions/ Jannis divide then in to tracks. You can have a look at the "basic" track.
What kind of gotchas are you thinking of?
It can be done with streamly, if you can point me to a piece of code written with async I can write the equivalent code in streamly. I would be interested in discovering what scenarios, if any, cannot be expressed with it and find solutions if required.
(Wishing reddit had LaTeX support...) For normal non-depenent function you make a series of independnt choices of the output (selected from B) for each value of A. Counting this series is product(|B|, A) = |B|^(|A|) so they look expoential. For dependent functions you still make a series of independent choices of the output (selected from B_i) for each value (i) of A. Counting this series is product(|B_i|, A), but that doesn't simplify to an exponential unless you have more information about the various B_i.
And now `DerivingVia`!